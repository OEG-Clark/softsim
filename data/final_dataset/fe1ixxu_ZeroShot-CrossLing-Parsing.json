{"home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.None.evaluate.evaluate_from_args": [[29, 105], ["logging.basicConfig", "allennlp.models.archival.load_archive", "allennlp.common.util.prepare_environment", "model.eval", "config.pop", "logger.info", "DatasetReader.from_params.read", "dataset_reader.read.index_with", "config.pop", "allennlp.data.DataLoader.from_params", "allennlp.training.util.evaluate", "logger.info", "allennlp.common.util.dump_metrics", "logging.getLogger", "logging.getLogger", "logging.getLogger", "logging.getLogger", "logging.getLogger", "allennlp.data.dataset_readers.dataset_reader.DatasetReader.from_params", "allennlp.data.dataset_readers.dataset_reader.DatasetReader.from_params", "json.loads", "logger.info", "model.vocab.extend_from_instances", "model.extend_embedder_vocab", "config.pop", "dir", "range", "config.pop", "logging.info", "evaluate.get_iter_norm_mean_eval", "logger.info", "iter_mean_eval.append", "evaluate.degree_anisotropy", "embeddings.t"], "function", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.commands.iter_norm_evaluate.get_iter_norm_mean_eval", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.get_mapper.degree_anisotropy"], ["def", "evaluate_from_args", "(", "args", ":", "argparse", ".", "Namespace", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "# Disable some of the more verbose logging statements", "\n", "    ", "logging", ".", "getLogger", "(", "\"allennlp.common.params\"", ")", ".", "disabled", "=", "True", "\n", "logging", ".", "getLogger", "(", "\"allennlp.nn.initializers\"", ")", ".", "disabled", "=", "True", "\n", "logging", ".", "getLogger", "(", "\"transformers.modeling_utils\"", ")", ".", "disabled", "=", "True", "\n", "logging", ".", "getLogger", "(", "\"transformers.tokenization_utils\"", ")", ".", "disabled", "=", "True", "\n", "logging", ".", "getLogger", "(", "\"transformers.configuration_utils\"", ")", ".", "disabled", "=", "True", "\n", "logging", ".", "basicConfig", "(", "level", "=", "logging", ".", "INFO", ")", "\n", "\n", "# Load from archive", "\n", "archive", "=", "load_archive", "(", "\n", "args", ".", "archive_file", ",", "\n", "weights_file", "=", "args", ".", "weights_file", ",", "\n", "cuda_device", "=", "args", ".", "cuda_device", ",", "\n", "overrides", "=", "args", ".", "overrides", ",", "\n", ")", "\n", "config", "=", "archive", ".", "config", "\n", "prepare_environment", "(", "config", ")", "\n", "model", "=", "archive", ".", "model", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "# Load the evaluation data", "\n", "\n", "# Try to use the validation dataset reader if there is one - otherwise fall back", "\n", "# to the default dataset_reader used for both training and validation.", "\n", "validation_dataset_reader_params", "=", "config", ".", "pop", "(", "\"validation_dataset_reader\"", ",", "None", ")", "\n", "if", "validation_dataset_reader_params", "is", "not", "None", ":", "\n", "        ", "dataset_reader", "=", "DatasetReader", ".", "from_params", "(", "validation_dataset_reader_params", ")", "\n", "", "else", ":", "\n", "        ", "dataset_reader", "=", "DatasetReader", ".", "from_params", "(", "config", ".", "pop", "(", "\"dataset_reader\"", ")", ")", "\n", "", "evaluation_data_path", "=", "args", ".", "input_file", "\n", "logger", ".", "info", "(", "\"Reading evaluation data from %s\"", ",", "evaluation_data_path", ")", "\n", "instances", "=", "dataset_reader", ".", "read", "(", "evaluation_data_path", ")", "\n", "\n", "embedding_sources", "=", "(", "\n", "json", ".", "loads", "(", "args", ".", "embedding_sources_mapping", ")", "if", "args", ".", "embedding_sources_mapping", "else", "{", "}", "\n", ")", "\n", "\n", "if", "args", ".", "extend_vocab", ":", "\n", "        ", "logger", ".", "info", "(", "\"Vocabulary is being extended with test instances.\"", ")", "\n", "model", ".", "vocab", ".", "extend_from_instances", "(", "instances", "=", "instances", ")", "\n", "model", ".", "extend_embedder_vocab", "(", "embedding_sources", ")", "\n", "\n", "", "instances", ".", "index_with", "(", "model", ".", "vocab", ")", "\n", "data_loader_params", "=", "config", ".", "pop", "(", "\"validation_data_loader\"", ",", "None", ")", "\n", "if", "data_loader_params", "is", "None", ":", "\n", "        ", "data_loader_params", "=", "config", ".", "pop", "(", "\"data_loader\"", ")", "\n", "", "if", "args", ".", "batch_size", ":", "\n", "        ", "data_loader_params", "[", "\"batch_size\"", "]", "=", "args", ".", "batch_size", "\n", "", "data_loader", "=", "DataLoader", ".", "from_params", "(", "dataset", "=", "instances", ",", "params", "=", "data_loader_params", ")", "\n", "\n", "if", "\"iter_norm\"", "in", "dir", "(", "model", ".", "text_field_embedder", ".", "_token_embedders", "[", "'tokens'", "]", ")", ":", "\n", "        ", "iter_num", "=", "model", ".", "text_field_embedder", ".", "_token_embedders", "[", "'tokens'", "]", ".", "iter_norm", "\n", "", "else", ":", "\n", "        ", "iter_num", "=", "None", "\n", "\n", "", "if", "iter_num", ":", "\n", "# Obtrain evaluation info for iterative normalization:", "\n", "        ", "iter_mean_eval", "=", "[", "]", "\n", "for", "iter_norm_i", "in", "range", "(", "iter_num", ")", ":", "\n", "            ", "logging", ".", "info", "(", "\"This is the {} time during iterative normalization for evaluation\"", ".", "format", "(", "iter_norm_i", ")", ")", "\n", "mean", ",", "embeddings", "=", "get_iter_norm_mean_eval", "(", "model", ",", "data_loader", ",", "iter_mean_eval", ",", "args", ".", "cuda_device", ")", "\n", "logger", ".", "info", "(", "\"The degree of isotropy of vectors is {} \"", ".", "format", "(", "degree_anisotropy", "(", "embeddings", ".", "t", "(", ")", ",", "args", ".", "cuda_device", ")", ")", ")", "\n", "iter_mean_eval", ".", "append", "(", "mean", ")", "\n", "\n", "", "model", ".", "text_field_embedder", ".", "_token_embedders", "[", "'tokens'", "]", ".", "iter_norm", "=", "None", "\n", "model", ".", "text_field_embedder", ".", "_token_embedders", "[", "'tokens'", "]", ".", "_matched_embedder", ".", "mean_emb_eval", "=", "iter_mean_eval", "\n", "model", ".", "text_field_embedder", ".", "_token_embedders", "[", "'tokens'", "]", ".", "_matched_embedder", ".", "is_train", "=", "False", "\n", "\n", "", "metrics", "=", "evaluate", "(", "model", ",", "data_loader", ",", "args", ".", "cuda_device", ",", "args", ".", "batch_weight_key", ")", "\n", "\n", "logger", ".", "info", "(", "\"Finished evaluating.\"", ")", "\n", "\n", "dump_metrics", "(", "args", ".", "output_file", ",", "metrics", ",", "log", "=", "True", ")", "\n", "\n", "return", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.None.evaluate.get_iter_norm_mean_eval": [[106, 149], ["allennlp.common.checks.check_for_gpu", "torch.no_grad", "model.eval", "iter", "logger.info", "allennlp.common.Tqdm.tqdm", "torch.cat", "torch.cat.mean", "allennlp.nn.util.move_to_device", "model.forward_embeddings", "torch.cat.append"], "function", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.bert_biaffine_dependency_parser.BertBiaffineDependencyParser.forward_embeddings"], ["", "def", "get_iter_norm_mean_eval", "(", "\n", "model", ":", "Model", ",", "\n", "data_loader", ":", "DataLoader", ",", "\n", "mean", ":", "torch", ".", "Tensor", ",", "\n", "cuda_device", ":", "int", "=", "-", "1", "\n", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "    ", "\"\"\"\n    # Parameters\n\n    model : `Model`\n        The model to evaluate\n    data_loader : `DataLoader`\n        The `DataLoader` that will iterate over the evaluation data (data loaders already contain\n        their data).\n    cuda_device : `int`, optional (default=`-1`)\n        The cuda device to use for this evaluation.  The model is assumed to already be using this\n        device; this parameter is only used for moving the input data to the correct device.\n    batch_weight_key : `str`, optional (default=`None`)\n        If given, this is a key in the output dictionary for each batch that specifies how to weight\n        the loss for that batch.  If this is not given, we use a weight of 1 for every batch.\n    \"\"\"", "\n", "check_for_gpu", "(", "cuda_device", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "\n", "iterator", "=", "iter", "(", "data_loader", ")", "\n", "logger", ".", "info", "(", "\"Iterating over dataset\"", ")", "\n", "generator_tqdm", "=", "Tqdm", ".", "tqdm", "(", "iterator", ")", "\n", "\n", "# mean_embeddings: [torch.Tensor, int]", "\n", "# mean_embeddings = [torch.tensor([0.], device=cuda_device), 0]", "\n", "embeddings", "=", "[", "]", "\n", "for", "batch", "in", "generator_tqdm", ":", "\n", "            ", "batch", "=", "nn_util", ".", "move_to_device", "(", "batch", ",", "cuda_device", ")", "\n", "batch_embeddings", "=", "model", ".", "forward_embeddings", "(", "batch", "[", "'words'", "]", ",", "mean", ")", "\n", "# mean_embeddings[0] = (mean_embeddings[0] + batch_embeddings.sum(dim=0)) ", "\n", "# mean_embeddings[1] += batch_embeddings.shape[0]", "\n", "embeddings", ".", "append", "(", "batch_embeddings", ")", "\n", "\n", "# mean_embeddings[0] = mean_embeddings[0] / mean_embeddings[1]", "\n", "", "embeddings", "=", "torch", ".", "cat", "(", "embeddings", ",", "dim", "=", "0", ")", "\n", "\n", "", "return", "embeddings", ".", "mean", "(", "dim", "=", "0", ")", ",", "embeddings", "# mean_embeddings[0]", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.None.evaluate.degree_anisotropy": [[150, 159], ["random_indice_generator().cuda", "torch.triu", "torch.triu().t().cuda", "torch.where", "torch.mean", "evaluate.random_indice_generator", "vectors[].t", "torch.triu().t", "torch.triu", "torch.ones"], "function", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.get_mapper.random_indice_generator"], ["", "def", "degree_anisotropy", "(", "vectors", ",", "cuda_device", ")", ":", "\n", "    ", "index", "=", "random_indice_generator", "(", "1000", ",", "vectors", ".", "shape", "[", "1", "]", ")", ".", "cuda", "(", "cuda_device", ")", "\n", "vectors", "=", "torch", ".", "triu", "(", "vectors", "[", ":", ",", "index", "]", ".", "t", "(", ")", "@", "vectors", "[", ":", ",", "index", "]", ",", "1", ")", "\n", "filter_st", "=", "torch", ".", "triu", "(", "-", "100", "*", "torch", ".", "ones", "(", "vectors", ".", "shape", "[", "1", "]", ",", "vectors", ".", "shape", "[", "1", "]", ")", ")", ".", "t", "(", ")", ".", "cuda", "(", "cuda_device", ")", "\n", "vectors", "=", "torch", ".", "where", "(", "filter_st", "==", "-", "100", ",", "filter_st", ",", "vectors", ")", "\n", "ind", "=", "(", "vectors", "!=", "-", "100", ")", ".", "nonzero", "(", ")", "\n", "vectors", "=", "vectors", "[", "ind", "[", ":", ",", "0", "]", ",", "ind", "[", ":", ",", "1", "]", "]", "\n", "\n", "return", "torch", ".", "mean", "(", "vectors", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.None.evaluate.random_indice_generator": [[160, 165], ["list", "random.sample", "torch.tensor", "range"], "function", ["None"], ["", "def", "random_indice_generator", "(", "num", ",", "maxnum", ")", ":", "\n", "    ", "index", "=", "list", "(", "range", "(", "maxnum", ")", ")", "\n", "index", "=", "random", ".", "sample", "(", "index", ",", "num", ")", "\n", "index", "=", "torch", ".", "tensor", "(", "index", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "return", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.None.evaluate.main": [[166, 236], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_mutually_exclusive_group", "parser.add_mutually_exclusive_group.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "argparse.ArgumentParser.set_defaults", "evaluate.evaluate_from_args", "allennlp.common.util.import_module_and_submodules"], "function", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.commands.iter_norm_evaluate.evaluate_from_args"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"archive_file\"", ",", "type", "=", "str", ",", "help", "=", "\"path to an archived trained model\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"input_file\"", ",", "type", "=", "str", ",", "help", "=", "\"path to the file containing the evaluation data\"", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--output-file\"", ",", "type", "=", "str", ",", "help", "=", "\"path to output file\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--weights-file\"", ",", "type", "=", "str", ",", "help", "=", "\"a path that overrides which weights file to use\"", "\n", ")", "\n", "\n", "cuda_device", "=", "parser", ".", "add_mutually_exclusive_group", "(", "required", "=", "False", ")", "\n", "cuda_device", ".", "add_argument", "(", "\n", "\"--cuda-device\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "help", "=", "\"id of GPU to use (if any)\"", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"-o\"", ",", "\n", "\"--overrides\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"\"", ",", "\n", "help", "=", "\"a JSON structure used to override the experiment configuration\"", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--batch-size\"", ",", "type", "=", "int", ",", "help", "=", "\"If non-empty, the batch size to use during evaluation.\"", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--batch-weight-key\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"\"", ",", "\n", "help", "=", "\"If non-empty, name of metric used to weight the loss on a per-batch basis.\"", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--extend-vocab\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "default", "=", "False", ",", "\n", "help", "=", "\"if specified, we will use the instances in your new dataset to \"", "\n", "\"extend your vocabulary. If pretrained-file was used to initialize \"", "\n", "\"embedding layers, you may also need to pass --embedding-sources-mapping.\"", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--embedding-sources-mapping\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"\"", ",", "\n", "help", "=", "\"a JSON dict defining mapping from embedding module path to embedding \"", "\n", "\"pretrained-file used during training. If not passed, and embedding needs to be \"", "\n", "\"extended, we will try to use the original file paths used during training. If \"", "\n", "\"they are not available we will use random vectors for embedding extension.\"", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--include-package\"", ",", "\n", "type", "=", "str", ",", "\n", "action", "=", "\"append\"", ",", "\n", "default", "=", "[", "]", ",", "\n", "help", "=", "\"additional packages to include\"", ",", "\n", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "for", "package_name", "in", "args", ".", "include_package", ":", "\n", "        ", "import_module_and_submodules", "(", "package_name", ")", "\n", "", "parser", ".", "set_defaults", "(", "func", "=", "evaluate_from_args", ")", "\n", "evaluate_from_args", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.src.load_overrides.write_override_file": [[21, 45], ["overrides.replace.replace", "overrides.replace.replace", "overrides.replace.replace", "overrides.replace.replace", "overrides.replace.replace", "overrides.replace.replace", "overrides.replace.replace", "overrides.replace.replace", "overrides.replace.replace", "overrides.replace.replace", "overrides.replace.replace", "overrides.replace.replace", "overrides.replace.replace", "overrides.replace.replace", "overrides.replace.replace"], "function", ["None"], ["", "", "def", "write_override_file", "(", "args", ",", "overrides", ")", ":", "\n", "    ", "overrides", "=", "overrides", ".", "replace", "(", "\" \"", ",", "\"\"", ")", "\n", "overrides", "=", "overrides", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", "\n", "overrides", "=", "overrides", ".", "replace", "(", "\"\\t\"", ",", "\"\"", ")", "\n", "if", "args", ".", "lang", "==", "\"fi\"", ":", "\n", "        ", "overrides", "=", "overrides", ".", "replace", "(", "\"override_place_holder\"", ",", "\"TurkuNLP/bert-base-finnish-uncased-v1\"", ")", "\n", "overrides", "=", "overrides", ".", "replace", "(", "\"map_path_place_holder\"", ",", "args", ".", "mapping_path", "+", "\"/20-iter-norm-\"", "+", "args", ".", "type", "+", "\"_fi-en-1m.th\"", ")", "\n", "", "elif", "args", ".", "lang", "==", "\"el\"", ":", "\n", "        ", "overrides", "=", "overrides", ".", "replace", "(", "\"override_place_holder\"", ",", "\"nlpaueb/bert-base-greek-uncased-v1\"", ")", "\n", "overrides", "=", "overrides", ".", "replace", "(", "\"map_path_place_holder\"", ",", "args", ".", "mapping_path", "+", "\"/20-iter-norm-\"", "+", "args", ".", "type", "+", "\"_el-en-1m.th\"", ")", "\n", "", "elif", "args", ".", "lang", "==", "\"es\"", ":", "\n", "        ", "overrides", "=", "overrides", ".", "replace", "(", "\"override_place_holder\"", ",", "\"dccuchile/bert-base-spanish-wwm-uncased\"", ")", "\n", "overrides", "=", "overrides", ".", "replace", "(", "\"map_path_place_holder\"", ",", "args", ".", "mapping_path", "+", "\"/20-iter-norm-\"", "+", "args", ".", "type", "+", "\"_es-en-1m.th\"", ")", "\n", "", "elif", "args", ".", "lang", "==", "\"pl\"", ":", "\n", "        ", "overrides", "=", "overrides", ".", "replace", "(", "\"override_place_holder\"", ",", "\"dkleczek/bert-base-polish-uncased-v1\"", ")", "\n", "overrides", "=", "overrides", ".", "replace", "(", "\"map_path_place_holder\"", ",", "args", ".", "mapping_path", "+", "\"/15-iter-norm-\"", "+", "args", ".", "type", "+", "\"_pl-en-1m.th\"", ")", "\n", "", "elif", "args", ".", "lang", "==", "\"ro\"", ":", "\n", "        ", "overrides", "=", "overrides", ".", "replace", "(", "\"override_place_holder\"", ",", "\"dumitrescustefan/bert-base-romanian-uncased-v1\"", ")", "\n", "overrides", "=", "overrides", ".", "replace", "(", "\"map_path_place_holder\"", ",", "args", ".", "mapping_path", "+", "\"/20-iter-norm-\"", "+", "args", ".", "type", "+", "\"_ro-en-1m.th\"", ")", "\n", "", "elif", "args", ".", "lang", "==", "\"pt\"", ":", "\n", "        ", "overrides", "=", "overrides", ".", "replace", "(", "\"override_place_holder\"", ",", "\"neuralmind/bert-base-portuguese-cased\"", ")", "\n", "overrides", "=", "overrides", ".", "replace", "(", "\"map_path_place_holder\"", ",", "args", ".", "mapping_path", "+", "\"/20-iter-norm-\"", "+", "args", ".", "type", "+", "\"_pt-en-1m.th\"", ")", "\n", "\n", "", "return", "overrides", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.src.load_overrides.main": [[47, 65], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "print", "write_override_file.replace", "evaluate_file", "load_overrides.write_override_file"], "function", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.src.load_overrides.write_override_file"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--overrides\"", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--lang\"", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--mapping_path\"", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--type\"", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--ifstatic\"", ",", "type", "=", "str", ",", "default", "=", "False", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "ifstatic", "==", "\"yes\"", ":", "\n", "        ", "overrides", "=", "\"{'model':{'text_field_embedder':{'token_embedders':{'tokens':{'pretrained_file':'override_place_holder'}}}}}\"", "\n", "override_file", "=", "\"/export/b15/haoranxu/clce/fasttext/wiki.\"", "+", "args", ".", "lang", "+", "\".align.vec\"", "\n", "overrides", "=", "overrides", ".", "replace", "(", "\"override_place_holder\"", ",", "override_file", ")", "\n", "", "else", ":", "\n", "        ", "overrides", "=", "evaluate_file", "(", "args", ".", "overrides", ")", "\n", "overrides", "=", "write_override_file", "(", "args", ",", "overrides", ")", "\n", "\n", "", "print", "(", "overrides", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.src.get_vocab.main": [[4, 26], ["os.listdir", "set", "open", "open.writelines", "open.close", "open", "f.readline", "f.readline.split", "f.readline", "set.add", "open.writelines", "len", "print"], "function", ["None"], ["def", "main", "(", ")", ":", "\n", "    ", "file_list", "=", "os", ".", "listdir", "(", "\"/export/b15/haoranxu/clce/fasttext/\"", ")", "\n", "records", "=", "set", "(", ")", "\n", "num", "=", "0", "\n", "\n", "f_w", "=", "open", "(", "\"/export/b15/haoranxu/clce/fasttext/vocab.txt\"", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "\n", "f_w", ".", "writelines", "(", "[", "\"@@UNKNOWN@@\"", ",", "\"\\n\"", "]", ")", "\n", "for", "file", "in", "file_list", ":", "\n", "        ", "with", "open", "(", "\"/export/b15/haoranxu/clce/fasttext/\"", "+", "file", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "line", "=", "f", ".", "readline", "(", ")", "\n", "while", "(", "line", ")", ":", "\n", "                ", "line", "=", "line", ".", "split", "(", ")", "\n", "if", "len", "(", "line", ")", ">", "10", "and", "line", "[", "0", "]", "not", "in", "records", ":", "\n", "                    ", "word", "=", "line", "[", "0", "]", "\n", "records", ".", "add", "(", "word", ")", "\n", "f_w", ".", "writelines", "(", "[", "word", ",", "\"\\n\"", "]", ")", "\n", "num", "+=", "1", "\n", "if", "num", "%", "10000", "==", "0", ":", "\n", "                        ", "print", "(", "num", ")", "\n", "", "", "line", "=", "f", ".", "readline", "(", ")", "\n", "\n", "", "", "", "f_w", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.attachment_scores_cuda.AttachmentScoresCuda.__init__": [[24, 33], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "ignore_classes", ":", "List", "[", "int", "]", "=", "None", ")", "->", "None", ":", "\n", "        ", "self", ".", "_labeled_correct", "=", "0.0", "\n", "self", ".", "_unlabeled_correct", "=", "0.0", "\n", "self", ".", "_exact_labeled_correct", "=", "0.0", "\n", "self", ".", "_exact_unlabeled_correct", "=", "0.0", "\n", "self", ".", "_total_words", "=", "0.0", "\n", "self", ".", "_total_sentences", "=", "0.0", "\n", "\n", "self", ".", "_ignore_classes", ":", "List", "[", "int", "]", "=", "ignore_classes", "or", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.attachment_scores_cuda.AttachmentScoresCuda.__call__": [[34, 88], ["attachment_scores_cuda.AttachmentScoresCuda.detach_tensors", "torch.ones_like.long().cuda", "predicted_indices.long().cuda.long().cuda.long().cuda", "predicted_labels.long().cuda.long().cuda.long().cuda", "gold_indices.long().cuda.long().cuda.long().cuda", "gold_labels.long().cuda.long().cuda.long().cuda", "correct_indices.sum", "unlabeled_exact_match.sum", "correct_labels_and_indices.sum", "labeled_exact_match.sum", "correct_indices.size", "torch.ones_like", "gold_labels.long().cuda.long().cuda.eq", "predicted_indices.long().cuda.long().cuda.eq().long", "predicted_labels.long().cuda.long().cuda.eq().long", "correct_indices.numel", "torch.ones_like.long", "predicted_indices.long().cuda.long().cuda.long", "predicted_labels.long().cuda.long().cuda.long", "gold_indices.long().cuda.long().cuda.long", "gold_labels.long().cuda.long().cuda.long", "predicted_indices.long().cuda.long().cuda.eq", "predicted_labels.long().cuda.long().cuda.eq"], "methods", ["None"], ["", "def", "__call__", "(", "# type: ignore", "\n", "self", ",", "\n", "predicted_indices", ":", "torch", ".", "Tensor", ",", "\n", "predicted_labels", ":", "torch", ".", "Tensor", ",", "\n", "gold_indices", ":", "torch", ".", "Tensor", ",", "\n", "gold_labels", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        predicted_indices : `torch.Tensor`, required.\n            A tensor of head index predictions of shape (batch_size, timesteps).\n        predicted_labels : `torch.Tensor`, required.\n            A tensor of arc label predictions of shape (batch_size, timesteps).\n        gold_indices : `torch.Tensor`, required.\n            A tensor of the same shape as `predicted_indices`.\n        gold_labels : `torch.Tensor`, required.\n            A tensor of the same shape as `predicted_labels`.\n        mask : `torch.Tensor`, optional (default = None).\n            A tensor of the same shape as `predicted_indices`.\n        \"\"\"", "\n", "detached", "=", "self", ".", "detach_tensors", "(", "\n", "predicted_indices", ",", "predicted_labels", ",", "gold_indices", ",", "gold_labels", ",", "mask", "\n", ")", "\n", "predicted_indices", ",", "predicted_labels", ",", "gold_indices", ",", "gold_labels", ",", "mask", "=", "detached", "\n", "\n", "if", "mask", "is", "None", ":", "\n", "            ", "mask", "=", "torch", ".", "ones_like", "(", "predicted_indices", ")", "\n", "\n", "", "mask", "=", "mask", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n", "predicted_indices", "=", "predicted_indices", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n", "predicted_labels", "=", "predicted_labels", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n", "gold_indices", "=", "gold_indices", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n", "gold_labels", "=", "gold_labels", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n", "\n", "# Multiply by a mask denoting locations of", "\n", "# gold labels which we should ignore.", "\n", "for", "label", "in", "self", ".", "_ignore_classes", ":", "\n", "            ", "label_mask", "=", "gold_labels", ".", "eq", "(", "label", ")", "\n", "mask", "=", "mask", "*", "(", "~", "label_mask", ")", ".", "long", "(", ")", "\n", "\n", "", "correct_indices", "=", "predicted_indices", ".", "eq", "(", "gold_indices", ")", ".", "long", "(", ")", "*", "mask", "\n", "unlabeled_exact_match", "=", "(", "correct_indices", "+", "(", "1", "-", "mask", ")", ")", ".", "prod", "(", "dim", "=", "-", "1", ")", "\n", "correct_labels", "=", "predicted_labels", ".", "eq", "(", "gold_labels", ")", ".", "long", "(", ")", "*", "mask", "\n", "correct_labels_and_indices", "=", "correct_indices", "*", "correct_labels", "\n", "labeled_exact_match", "=", "(", "correct_labels_and_indices", "+", "(", "1", "-", "mask", ")", ")", ".", "prod", "(", "dim", "=", "-", "1", ")", "\n", "\n", "self", ".", "_unlabeled_correct", "+=", "correct_indices", ".", "sum", "(", ")", "\n", "self", ".", "_exact_unlabeled_correct", "+=", "unlabeled_exact_match", ".", "sum", "(", ")", "\n", "self", ".", "_labeled_correct", "+=", "correct_labels_and_indices", ".", "sum", "(", ")", "\n", "self", ".", "_exact_labeled_correct", "+=", "labeled_exact_match", ".", "sum", "(", ")", "\n", "self", ".", "_total_sentences", "+=", "correct_indices", ".", "size", "(", "0", ")", "\n", "self", ".", "_total_words", "+=", "correct_indices", ".", "numel", "(", ")", "-", "(", "1", "-", "mask", ")", ".", "sum", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.attachment_scores_cuda.AttachmentScoresCuda.get_metric": [[89, 114], ["attachment_scores_cuda.AttachmentScoresCuda.reset", "float", "float", "float", "float", "float", "float", "float", "float"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.attachment_scores_cuda.AttachmentScoresCuda.reset"], ["", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        # Returns\n\n        The accumulated metrics as a dictionary.\n        \"\"\"", "\n", "unlabeled_attachment_score", "=", "0.0", "\n", "labeled_attachment_score", "=", "0.0", "\n", "unlabeled_exact_match", "=", "0.0", "\n", "labeled_exact_match", "=", "0.0", "\n", "if", "self", ".", "_total_words", ">", "0.0", ":", "\n", "            ", "unlabeled_attachment_score", "=", "float", "(", "self", ".", "_unlabeled_correct", ")", "/", "float", "(", "self", ".", "_total_words", ")", "\n", "labeled_attachment_score", "=", "float", "(", "self", ".", "_labeled_correct", ")", "/", "float", "(", "self", ".", "_total_words", ")", "\n", "", "if", "self", ".", "_total_sentences", ">", "0", ":", "\n", "            ", "unlabeled_exact_match", "=", "float", "(", "self", ".", "_exact_unlabeled_correct", ")", "/", "float", "(", "\n", "self", ".", "_total_sentences", "\n", ")", "\n", "labeled_exact_match", "=", "float", "(", "self", ".", "_exact_labeled_correct", ")", "/", "float", "(", "self", ".", "_total_sentences", ")", "\n", "", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "{", "\n", "\"UAS\"", ":", "unlabeled_attachment_score", ",", "\n", "\"LAS\"", ":", "labeled_attachment_score", ",", "\n", "\"UEM\"", ":", "unlabeled_exact_match", ",", "\n", "\"LEM\"", ":", "labeled_exact_match", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.attachment_scores_cuda.AttachmentScoresCuda.reset": [[116, 124], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_labeled_correct", "=", "0.0", "\n", "self", ".", "_unlabeled_correct", "=", "0.0", "\n", "self", ".", "_exact_labeled_correct", "=", "0.0", "\n", "self", ".", "_exact_unlabeled_correct", "=", "0.0", "\n", "self", ".", "_total_words", "=", "0.0", "\n", "self", ".", "_total_sentences", "=", "0.0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.iter_norm_trainer.Iter_Norm_Trainer.__init__": [[204, 319], ["allennlp.training.trainer.Trainer.__init__", "allennlp.training.metric_tracker.MetricTracker", "iter_norm_trainer.Iter_Norm_Trainer._tensorboard.enable_activation_logging", "allennlp.training.checkpointer.Checkpointer", "allennlp.training.tensorboard_writer.TensorboardWriter", "amp.initialize", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "logger.warning", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "isinstance", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.evaluator.evaluator.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "model", ":", "Model", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "data_loader", ":", "torch", ".", "utils", ".", "data", ".", "DataLoader", ",", "\n", "patience", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "validation_metric", ":", "str", "=", "\"-loss\"", ",", "\n", "validation_data_loader", ":", "torch", ".", "utils", ".", "data", ".", "DataLoader", "=", "None", ",", "\n", "num_epochs", ":", "int", "=", "20", ",", "\n", "serialization_dir", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "checkpointer", ":", "Checkpointer", "=", "None", ",", "\n", "cuda_device", ":", "int", "=", "-", "1", ",", "\n", "grad_norm", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "grad_clipping", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "learning_rate_scheduler", ":", "Optional", "[", "LearningRateScheduler", "]", "=", "None", ",", "\n", "momentum_scheduler", ":", "Optional", "[", "MomentumScheduler", "]", "=", "None", ",", "\n", "tensorboard_writer", ":", "TensorboardWriter", "=", "None", ",", "\n", "moving_average", ":", "Optional", "[", "MovingAverage", "]", "=", "None", ",", "\n", "batch_callbacks", ":", "List", "[", "BatchCallback", "]", "=", "None", ",", "\n", "epoch_callbacks", ":", "List", "[", "EpochCallback", "]", "=", "None", ",", "\n", "distributed", ":", "bool", "=", "False", ",", "\n", "local_rank", ":", "int", "=", "0", ",", "\n", "world_size", ":", "int", "=", "1", ",", "\n", "num_gradient_accumulation_steps", ":", "int", "=", "1", ",", "\n", "opt_level", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "serialization_dir", ",", "cuda_device", ",", "distributed", ",", "local_rank", ",", "world_size", ")", "\n", "\n", "# I am not calling move_to_gpu here, because if the model is", "\n", "# not already on the GPU then the optimizer is going to be wrong.", "\n", "self", ".", "model", "=", "model", "\n", "\n", "self", ".", "data_loader", "=", "data_loader", "\n", "self", ".", "_validation_data_loader", "=", "validation_data_loader", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "iter_mean_train", "=", "[", "]", "\n", "self", ".", "iter_mean_eval", "=", "[", "]", "\n", "\n", "if", "patience", "is", "None", ":", "# no early stopping", "\n", "            ", "if", "validation_data_loader", "is", "not", "None", ":", "\n", "                ", "logger", ".", "warning", "(", "\n", "\"You provided a validation dataset but patience was set to None, \"", "\n", "\"meaning that early stopping is disabled\"", "\n", ")", "\n", "", "", "elif", "(", "not", "isinstance", "(", "patience", ",", "int", ")", ")", "or", "patience", "<=", "0", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "'{} is an invalid value for \"patience\": it must be a positive integer '", "\n", "\"or None (if you want to disable early stopping)\"", ".", "format", "(", "patience", ")", "\n", ")", "\n", "\n", "# For tracking is_best_so_far and should_stop_early", "\n", "", "self", ".", "_metric_tracker", "=", "MetricTracker", "(", "patience", ",", "validation_metric", ")", "\n", "# Get rid of + or -", "\n", "self", ".", "_validation_metric", "=", "validation_metric", "[", "1", ":", "]", "\n", "\n", "self", ".", "_num_epochs", "=", "num_epochs", "\n", "\n", "if", "checkpointer", "is", "not", "None", ":", "\n", "            ", "self", ".", "_checkpointer", "=", "checkpointer", "\n", "", "else", ":", "\n", "            ", "self", ".", "_checkpointer", "=", "Checkpointer", "(", "serialization_dir", ")", "\n", "\n", "", "self", ".", "_grad_norm", "=", "grad_norm", "\n", "self", ".", "_grad_clipping", "=", "grad_clipping", "\n", "\n", "self", ".", "_learning_rate_scheduler", "=", "learning_rate_scheduler", "\n", "self", ".", "_momentum_scheduler", "=", "momentum_scheduler", "\n", "self", ".", "_moving_average", "=", "moving_average", "\n", "self", ".", "_batch_callbacks", "=", "batch_callbacks", "or", "[", "]", "\n", "self", ".", "_epoch_callbacks", "=", "epoch_callbacks", "or", "[", "]", "\n", "\n", "# We keep the total batch number as an instance variable because it", "\n", "# is used inside a closure for the hook which logs activations in", "\n", "# `_enable_activation_logging`.", "\n", "self", ".", "_batch_num_total", "=", "0", "\n", "\n", "self", ".", "_tensorboard", "=", "tensorboard_writer", "or", "TensorboardWriter", "(", "serialization_dir", ")", "\n", "self", ".", "_tensorboard", ".", "get_batch_num_total", "=", "lambda", ":", "self", ".", "_batch_num_total", "\n", "self", ".", "_tensorboard", ".", "enable_activation_logging", "(", "self", ".", "model", ")", "\n", "\n", "self", ".", "_last_log", "=", "0.0", "# time of last logging", "\n", "\n", "self", ".", "_num_gradient_accumulation_steps", "=", "num_gradient_accumulation_steps", "\n", "\n", "# Enable automatic mixed precision training with NVIDIA Apex.", "\n", "self", ".", "_opt_level", "=", "opt_level", "\n", "if", "self", ".", "_opt_level", "is", "not", "None", ":", "\n", "            ", "if", "amp", "is", "None", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "(", "\n", "\"Apex not installed but opt_level was provided. Please install NVIDIA's Apex to enable\"", "\n", "\" automatic mixed precision (AMP) training. See: https://github.com/NVIDIA/apex.\"", "\n", ")", "\n", ")", "\n", "\n", "", "self", ".", "model", ",", "self", ".", "optimizer", "=", "amp", ".", "initialize", "(", "\n", "self", ".", "model", ",", "self", ".", "optimizer", ",", "opt_level", "=", "self", ".", "_opt_level", "\n", ")", "\n", "\n", "# Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its", "\n", "# usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`", "\n", "# will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.", "\n", "#", "\n", "# Hence a reference to Pytorch's object is maintained in the case of distributed training and in the", "\n", "# normal case, reference to `Model` is retained. This reference is only used in", "\n", "# these places: `model.__call__`, `model.train` and `model.eval`.", "\n", "", "if", "self", ".", "_distributed", ":", "\n", "            ", "self", ".", "_pytorch_model", "=", "DistributedDataParallel", "(", "\n", "self", ".", "model", ",", "\n", "device_ids", "=", "None", "if", "self", ".", "cuda_device", "==", "torch", ".", "device", "(", "\"cpu\"", ")", "else", "[", "self", ".", "cuda_device", "]", ",", "\n", "find_unused_parameters", "=", "True", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_pytorch_model", "=", "self", ".", "model", "\n", "", "self", ".", "iter_num", "=", "self", ".", "_pytorch_model", ".", "text_field_embedder", ".", "_token_embedders", "[", "'tokens'", "]", ".", "iter_norm", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.iter_norm_trainer.Iter_Norm_Trainer.rescale_gradients": [[320, 335], ["torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "amp.master_params", "iter_norm_trainer.Iter_Norm_Trainer.model.parameters"], "methods", ["None"], ["", "def", "rescale_gradients", "(", "self", ")", "->", "Optional", "[", "float", "]", ":", "\n", "        ", "\"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n        \"\"\"", "\n", "if", "self", ".", "_grad_norm", ":", "\n", "            ", "if", "self", ".", "_opt_level", "is", "not", "None", ":", "\n", "# See: https://nvidia.github.io/apex/advanced.html#gradient-clipping", "\n", "                ", "parameters_to_clip", "=", "[", "\n", "p", "for", "p", "in", "amp", ".", "master_params", "(", "self", ".", "optimizer", ")", "if", "p", ".", "grad", "is", "not", "None", "\n", "]", "\n", "", "else", ":", "\n", "                ", "parameters_to_clip", "=", "[", "p", "for", "p", "in", "self", ".", "model", ".", "parameters", "(", ")", "if", "p", ".", "grad", "is", "not", "None", "]", "\n", "", "return", "clip_grad_norm_", "(", "parameters_to_clip", ",", "self", ".", "_grad_norm", ")", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.iter_norm_trainer.Iter_Norm_Trainer.batch_outputs": [[336, 364], ["allennlp.nn.util.move_to_device", "iter_norm_trainer.Iter_Norm_Trainer._pytorch_model", "iter_norm_trainer.Iter_Norm_Trainer.model.get_regularization_penalty", "loss.new_full", "RuntimeError"], "methods", ["None"], ["", "", "def", "batch_outputs", "(", "self", ",", "batch", ":", "TensorDict", ",", "for_training", ":", "bool", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Does a forward pass on the given batch and returns the output dictionary that the model\n        returns, after adding any specified regularization penalty to the loss (if training).\n        \"\"\"", "\n", "batch", "=", "nn_util", ".", "move_to_device", "(", "batch", ",", "self", ".", "cuda_device", ")", "\n", "# print(self._pytorch_model.text_field_embedder._token_embedders['tokens'].iter_norm)", "\n", "output_dict", "=", "self", ".", "_pytorch_model", "(", "**", "batch", ")", "\n", "\n", "if", "for_training", ":", "\n", "            ", "try", ":", "\n", "                ", "regularization_penalty", "=", "self", ".", "model", ".", "get_regularization_penalty", "(", ")", "\n", "loss", "=", "output_dict", "[", "\"loss\"", "]", "\n", "\n", "# Handle model without regularization", "\n", "if", "regularization_penalty", "==", "0.0", ":", "\n", "                    ", "regularization_penalty", "=", "loss", ".", "new_full", "(", "size", "=", "[", "]", ",", "fill_value", "=", "0.0", ")", "\n", "\n", "", "output_dict", "[", "\"reg_loss\"", "]", "=", "regularization_penalty", "\n", "output_dict", "[", "\"loss\"", "]", "+=", "regularization_penalty", "\n", "", "except", "KeyError", ":", "\n", "                ", "if", "for_training", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\n", "\"The model you are trying to optimize does not contain a\"", "\n", "\" 'loss' key in the output of model.forward(inputs).\"", "\n", ")", "\n", "\n", "", "", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.iter_norm_trainer.Iter_Norm_Trainer._train_epoch": [[365, 553], ["logger.info", "allennlp.common.util.peak_memory_mb().items", "allennlp.common.util.gpu_memory_mb().items", "iter_norm_trainer.Iter_Norm_Trainer._pytorch_model.train", "iter", "allennlp.common.util.lazy_groups_of", "logger.info", "time.time", "allennlp.training.util.get_metrics", "cpu_memory_usage.append", "logger.info", "gpu_memory_usage.append", "logger.info", "len", "math.ceil", "allennlp.common.Tqdm.tqdm", "iter_norm_trainer.Iter_Norm_Trainer.optimizer.zero_grad", "iter_norm_trainer.Iter_Norm_Trainer.rescale_gradients", "allennlp.training.util.get_metrics", "logger.warning", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.tensor.item", "torch.tensor.item", "torch.tensor.item", "torch.barrier", "torch.barrier", "torch.barrier", "allennlp.common.util.peak_memory_mb", "allennlp.common.util.gpu_memory_mb", "float", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "iter_norm_trainer.Iter_Norm_Trainer.batch_outputs", "batch_group_outputs.append", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "loss.item", "reg_loss.item", "iter_norm_trainer.Iter_Norm_Trainer._learning_rate_scheduler.step_batch", "iter_norm_trainer.Iter_Norm_Trainer._momentum_scheduler.step_batch", "iter_norm_trainer.Iter_Norm_Trainer._tensorboard.should_log_histograms_this_batch", "iter_norm_trainer.Iter_Norm_Trainer.optimizer.step", "iter_norm_trainer.Iter_Norm_Trainer.model.named_parameters", "iter_norm_trainer.Iter_Norm_Trainer.optimizer.step", "iter_norm_trainer.Iter_Norm_Trainer._moving_average.apply", "allennlp.training.util.description_from_metrics", "allennlp.common.Tqdm.tqdm.set_description", "iter_norm_trainer.Iter_Norm_Trainer._tensorboard.log_batch", "iter_norm_trainer.Iter_Norm_Trainer._checkpointer.maybe_save_checkpoint", "callback", "torch.tensor.item", "torch.tensor.item", "torch.tensor.item", "logger.warning", "ValueError", "len", "len", "loss.backward", "param.detach().cpu().clone", "param_updates[].sub_", "amp.scale_loss", "scaled_loss.backward", "iter_norm_trainer.Iter_Norm_Trainer.model.named_parameters", "param.detach().cpu", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "param.detach().cpu", "str", "str", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "param.detach", "param.detach"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.iter_norm_trainer.Iter_Norm_Trainer.train", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser.get_metrics", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.iter_norm_trainer.Iter_Norm_Trainer.rescale_gradients", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser.get_metrics", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.iter_norm_trainer.Iter_Norm_Trainer.batch_outputs"], ["", "def", "_train_epoch", "(", "self", ",", "epoch", ":", "int", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"", "\n", "logger", ".", "info", "(", "\"Epoch %d/%d\"", ",", "epoch", ",", "self", ".", "_num_epochs", "-", "1", ")", "\n", "cpu_memory_usage", "=", "[", "]", "\n", "for", "worker", ",", "memory", "in", "common_util", ".", "peak_memory_mb", "(", ")", ".", "items", "(", ")", ":", "\n", "            ", "cpu_memory_usage", ".", "append", "(", "(", "worker", ",", "memory", ")", ")", "\n", "logger", ".", "info", "(", "f\"Worker {worker} memory usage MB: {memory}\"", ")", "\n", "", "gpu_memory_usage", "=", "[", "]", "\n", "for", "gpu", ",", "memory", "in", "common_util", ".", "gpu_memory_mb", "(", ")", ".", "items", "(", ")", ":", "\n", "            ", "gpu_memory_usage", ".", "append", "(", "(", "gpu", ",", "memory", ")", ")", "\n", "logger", ".", "info", "(", "f\"GPU {gpu} memory usage MB: {memory}\"", ")", "\n", "\n", "", "train_loss", "=", "0.0", "\n", "train_reg_loss", "=", "0.0", "\n", "# Set the model to \"train\" mode.", "\n", "self", ".", "_pytorch_model", ".", "train", "(", ")", "\n", "\n", "# Get tqdm for the training batches", "\n", "batch_generator", "=", "iter", "(", "self", ".", "data_loader", ")", "\n", "batch_group_generator", "=", "common_util", ".", "lazy_groups_of", "(", "\n", "batch_generator", ",", "self", ".", "_num_gradient_accumulation_steps", "\n", ")", "\n", "\n", "logger", ".", "info", "(", "\"Training\"", ")", "\n", "\n", "num_training_batches", ":", "Union", "[", "int", ",", "float", "]", "\n", "try", ":", "\n", "            ", "len_data_loader", "=", "len", "(", "self", ".", "data_loader", ")", "\n", "num_training_batches", "=", "math", ".", "ceil", "(", "\n", "len_data_loader", "/", "self", ".", "_num_gradient_accumulation_steps", "\n", ")", "\n", "", "except", "TypeError", ":", "\n", "            ", "num_training_batches", "=", "float", "(", "\"inf\"", ")", "\n", "\n", "# Having multiple tqdm bars in case of distributed training will be a mess. Hence only the master's", "\n", "# progress is shown", "\n", "", "if", "self", ".", "_master", ":", "\n", "            ", "batch_group_generator_tqdm", "=", "Tqdm", ".", "tqdm", "(", "\n", "batch_group_generator", ",", "total", "=", "num_training_batches", "\n", ")", "\n", "", "else", ":", "\n", "            ", "batch_group_generator_tqdm", "=", "batch_group_generator", "\n", "\n", "", "self", ".", "_last_log", "=", "time", ".", "time", "(", ")", "\n", "\n", "batches_this_epoch", "=", "0", "\n", "if", "self", ".", "_batch_num_total", "is", "None", ":", "\n", "            ", "self", ".", "_batch_num_total", "=", "0", "\n", "\n", "", "done_early", "=", "False", "\n", "for", "batch_group", "in", "batch_group_generator_tqdm", ":", "\n", "            ", "if", "self", ".", "_distributed", ":", "\n", "# Check whether the other workers have stopped already (due to differing amounts of", "\n", "# data in each). If so, we can't proceed because we would hang when we hit the", "\n", "# barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor", "\n", "# here because NCCL process groups apparently don't support BoolTensor.", "\n", "                ", "done", "=", "torch", ".", "tensor", "(", "0", ",", "device", "=", "self", ".", "cuda_device", ")", "\n", "torch", ".", "distributed", ".", "all_reduce", "(", "done", ",", "torch", ".", "distributed", ".", "ReduceOp", ".", "SUM", ")", "\n", "if", "done", ".", "item", "(", ")", ">", "0", ":", "\n", "                    ", "done_early", "=", "True", "\n", "logger", ".", "warning", "(", "\n", "f\"Worker {torch.distributed.get_rank()} finishing training early! \"", "\n", "\"This implies that there is an imbalance in your training \"", "\n", "\"data across the workers and that some amount of it will be \"", "\n", "\"ignored. A small amount of this is fine, but a major imbalance \"", "\n", "\"should be avoided. Note: This warning will appear unless your \"", "\n", "\"data is perfectly balanced.\"", "\n", ")", "\n", "break", "\n", "\n", "", "", "batches_this_epoch", "+=", "1", "\n", "self", ".", "_batch_num_total", "+=", "1", "\n", "batch_num_total", "=", "self", ".", "_batch_num_total", "\n", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "batch_group_outputs", "=", "[", "]", "\n", "for", "batch", "in", "batch_group", ":", "\n", "                ", "batch_outputs", "=", "self", ".", "batch_outputs", "(", "batch", ",", "for_training", "=", "True", ")", "\n", "batch_group_outputs", ".", "append", "(", "batch_outputs", ")", "\n", "loss", "=", "batch_outputs", "[", "\"loss\"", "]", "\n", "reg_loss", "=", "batch_outputs", "[", "\"reg_loss\"", "]", "\n", "if", "torch", ".", "isnan", "(", "loss", ")", ":", "\n", "                    ", "raise", "ValueError", "(", "\"nan loss encountered\"", ")", "\n", "", "loss", "=", "loss", "/", "len", "(", "batch_group", ")", "\n", "reg_loss", "=", "reg_loss", "/", "len", "(", "batch_group", ")", "\n", "if", "self", ".", "_opt_level", "is", "not", "None", ":", "\n", "                    ", "with", "amp", ".", "scale_loss", "(", "loss", ",", "self", ".", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                        ", "scaled_loss", ".", "backward", "(", ")", "\n", "", "", "else", ":", "\n", "                    ", "loss", ".", "backward", "(", ")", "\n", "", "train_loss", "+=", "loss", ".", "item", "(", ")", "\n", "train_reg_loss", "+=", "reg_loss", ".", "item", "(", ")", "\n", "\n", "", "batch_grad_norm", "=", "self", ".", "rescale_gradients", "(", ")", "\n", "\n", "# This does nothing if batch_num_total is None or you are using a", "\n", "# scheduler which doesn't update per batch.", "\n", "if", "self", ".", "_learning_rate_scheduler", ":", "\n", "                ", "self", ".", "_learning_rate_scheduler", ".", "step_batch", "(", "batch_num_total", ")", "\n", "", "if", "self", ".", "_momentum_scheduler", ":", "\n", "                ", "self", ".", "_momentum_scheduler", ".", "step_batch", "(", "batch_num_total", ")", "\n", "\n", "", "param_updates", "=", "None", "\n", "if", "self", ".", "_tensorboard", ".", "should_log_histograms_this_batch", "(", ")", "and", "self", ".", "_master", ":", "\n", "# Get the magnitude of parameter updates for logging.  We need to do some", "\n", "# computation before and after the optimizer step, and it's expensive because of", "\n", "# GPU/CPU copies (necessary for large models, and for shipping to tensorboard), so", "\n", "# we don't do this every batch, only when it's requested.", "\n", "                ", "param_updates", "=", "{", "\n", "name", ":", "param", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "clone", "(", ")", "\n", "for", "name", ",", "param", "in", "self", ".", "model", ".", "named_parameters", "(", ")", "\n", "}", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "for", "name", ",", "param", "in", "self", ".", "model", ".", "named_parameters", "(", ")", ":", "\n", "                    ", "param_updates", "[", "name", "]", ".", "sub_", "(", "param", ".", "detach", "(", ")", ".", "cpu", "(", ")", ")", "\n", "", "", "else", ":", "\n", "                ", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "# Update moving averages", "\n", "", "if", "self", ".", "_moving_average", "is", "not", "None", ":", "\n", "                ", "self", ".", "_moving_average", ".", "apply", "(", "batch_num_total", ")", "\n", "\n", "# Update the description with the latest metrics", "\n", "", "metrics", "=", "training_util", ".", "get_metrics", "(", "\n", "self", ".", "model", ",", "\n", "train_loss", ",", "\n", "train_reg_loss", ",", "\n", "batches_this_epoch", ",", "\n", "world_size", "=", "self", ".", "_world_size", ",", "\n", "cuda_device", "=", "self", ".", "cuda_device", ",", "\n", ")", "\n", "\n", "if", "self", ".", "_master", ":", "\n", "# Updating tqdm only for the master as the trainers wouldn't have one", "\n", "                ", "description", "=", "training_util", ".", "description_from_metrics", "(", "metrics", ")", "\n", "batch_group_generator_tqdm", ".", "set_description", "(", "description", ",", "refresh", "=", "False", ")", "\n", "self", ".", "_tensorboard", ".", "log_batch", "(", "\n", "self", ".", "model", ",", "\n", "self", ".", "optimizer", ",", "\n", "batch_grad_norm", ",", "\n", "metrics", ",", "\n", "batch_group", ",", "\n", "param_updates", ",", "\n", ")", "\n", "\n", "self", ".", "_checkpointer", ".", "maybe_save_checkpoint", "(", "self", ",", "epoch", ",", "batches_this_epoch", ")", "\n", "", "for", "callback", "in", "self", ".", "_batch_callbacks", ":", "\n", "                ", "callback", "(", "\n", "self", ",", "\n", "batch_group", ",", "\n", "batch_group_outputs", ",", "\n", "epoch", ",", "\n", "batches_this_epoch", ",", "\n", "is_training", "=", "True", ",", "\n", "is_master", "=", "self", ".", "_master", ",", "\n", ")", "\n", "\n", "", "", "if", "self", ".", "_distributed", "and", "not", "done_early", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "f\"Worker {torch.distributed.get_rank()} completed its entire epoch (training).\"", "\n", ")", "\n", "# Indicate that we're done so that any workers that have remaining data stop the epoch early.", "\n", "done", "=", "torch", ".", "tensor", "(", "1", ",", "device", "=", "self", ".", "cuda_device", ")", "\n", "torch", ".", "distributed", ".", "all_reduce", "(", "done", ",", "torch", ".", "distributed", ".", "ReduceOp", ".", "SUM", ")", "\n", "assert", "done", ".", "item", "(", ")", "\n", "\n", "# Let all workers finish their epoch before computing", "\n", "# the final statistics for the epoch.", "\n", "", "if", "self", ".", "_distributed", ":", "\n", "            ", "dist", ".", "barrier", "(", ")", "\n", "\n", "", "metrics", "=", "training_util", ".", "get_metrics", "(", "\n", "self", ".", "model", ",", "\n", "train_loss", ",", "\n", "train_reg_loss", ",", "\n", "batches_this_epoch", ",", "\n", "reset", "=", "True", ",", "\n", "world_size", "=", "self", ".", "_world_size", ",", "\n", "cuda_device", "=", "self", ".", "cuda_device", ",", "\n", ")", "\n", "for", "(", "worker", ",", "memory", ")", "in", "cpu_memory_usage", ":", "\n", "            ", "metrics", "[", "\"worker_\"", "+", "str", "(", "worker", ")", "+", "\"_memory_MB\"", "]", "=", "memory", "\n", "", "for", "(", "gpu_num", ",", "memory", ")", "in", "gpu_memory_usage", ":", "\n", "            ", "metrics", "[", "\"gpu_\"", "+", "str", "(", "gpu_num", ")", "+", "\"_memory_MB\"", "]", "=", "memory", "\n", "", "return", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.iter_norm_trainer.Iter_Norm_Trainer._get_iter_norm_mean_train": [[554, 616], ["iter", "allennlp.common.util.lazy_groups_of", "len", "math.ceil", "allennlp.common.Tqdm.tqdm", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "float", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "allennlp.nn.util.move_to_device", "iter_norm_trainer.Iter_Norm_Trainer._pytorch_model.forward_embeddings", "torch.tensor.item", "torch.tensor.item", "torch.tensor.item", "logger.warning", "iter_norm_trainer.Iter_Norm_Trainer.sum", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.bert_biaffine_dependency_parser.BertBiaffineDependencyParser.forward_embeddings"], ["", "def", "_get_iter_norm_mean_train", "(", "self", ",", "mean", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Traverse training data to get mean embeddings.\n        \"\"\"", "\n", "# Get tqdm for the training batches", "\n", "batch_generator", "=", "iter", "(", "self", ".", "data_loader", ")", "\n", "batch_group_generator", "=", "common_util", ".", "lazy_groups_of", "(", "\n", "batch_generator", ",", "self", ".", "_num_gradient_accumulation_steps", "\n", ")", "\n", "\n", "num_training_batches", ":", "Union", "[", "int", ",", "float", "]", "\n", "try", ":", "\n", "            ", "len_data_loader", "=", "len", "(", "self", ".", "data_loader", ")", "\n", "num_training_batches", "=", "math", ".", "ceil", "(", "\n", "len_data_loader", "/", "self", ".", "_num_gradient_accumulation_steps", "\n", ")", "\n", "", "except", "TypeError", ":", "\n", "            ", "num_training_batches", "=", "float", "(", "\"inf\"", ")", "\n", "\n", "# Having multiple tqdm bars in case of distributed training will be a mess. Hence only the master's", "\n", "# progress is shown", "\n", "", "if", "self", ".", "_master", ":", "\n", "            ", "batch_group_generator_tqdm", "=", "Tqdm", ".", "tqdm", "(", "\n", "batch_group_generator", ",", "total", "=", "num_training_batches", "\n", ")", "\n", "", "else", ":", "\n", "            ", "batch_group_generator_tqdm", "=", "batch_group_generator", "\n", "\n", "", "done_early", "=", "False", "\n", "\n", "mean_embeddings", ":", "[", "torch", ".", "Tensor", ",", "int", "]", "\n", "mean_embeddings", "=", "[", "torch", ".", "tensor", "(", "[", "0.", "]", ",", "device", "=", "self", ".", "cuda_device", ")", ",", "0", "]", "\n", "\n", "for", "batch_group", "in", "batch_group_generator_tqdm", ":", "\n", "            ", "if", "self", ".", "_distributed", ":", "\n", "# Check whether the other workers have stopped already (due to differing amounts of", "\n", "# data in each). If so, we can't proceed because we would hang when we hit the", "\n", "# barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor", "\n", "# here because NCCL process groups apparently don't support BoolTensor.", "\n", "                ", "done", "=", "torch", ".", "tensor", "(", "0", ",", "device", "=", "self", ".", "cuda_device", ")", "\n", "torch", ".", "distributed", ".", "all_reduce", "(", "done", ",", "torch", ".", "distributed", ".", "ReduceOp", ".", "SUM", ")", "\n", "if", "done", ".", "item", "(", ")", ">", "0", ":", "\n", "                    ", "done_early", "=", "True", "\n", "logger", ".", "warning", "(", "\n", "f\"Worker {torch.distributed.get_rank()} finishing training early! \"", "\n", "\"This implies that there is an imbalance in your training \"", "\n", "\"data across the workers and that some amount of it will be \"", "\n", "\"ignored. A small amount of this is fine, but a major imbalance \"", "\n", "\"should be avoided. Note: This warning will appear unless your \"", "\n", "\"data is perfectly balanced.\"", "\n", ")", "\n", "break", "\n", "\n", "", "", "for", "batch", "in", "batch_group", ":", "\n", "                ", "batch", "=", "nn_util", ".", "move_to_device", "(", "batch", ",", "self", ".", "cuda_device", ")", "\n", "batch_embeddings", "=", "self", ".", "_pytorch_model", ".", "forward_embeddings", "(", "batch", "[", "'words'", "]", ",", "mean", ")", "\n", "mean_embeddings", "[", "0", "]", "=", "(", "mean_embeddings", "[", "0", "]", "+", "batch_embeddings", ".", "sum", "(", "dim", "=", "0", ")", ")", "\n", "mean_embeddings", "[", "1", "]", "+=", "batch_embeddings", ".", "shape", "[", "0", "]", "\n", "\n", "", "", "mean_embeddings", "[", "0", "]", "=", "mean_embeddings", "[", "0", "]", "/", "mean_embeddings", "[", "1", "]", "\n", "\n", "return", "mean_embeddings", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.iter_norm_trainer.Iter_Norm_Trainer._get_iter_norm_mean_eval": [[617, 665], ["allennlp.common.Tqdm.tqdm", "iter_norm_trainer.Iter_Norm_Trainer._moving_average.assign_average_value", "allennlp.common.checks.ConfigurationError", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "allennlp.nn.util.move_to_device", "iter_norm_trainer.Iter_Norm_Trainer._pytorch_model.forward_embeddings", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "iter_norm_trainer.Iter_Norm_Trainer.sum", "torch.tensor.item", "torch.tensor.item", "torch.tensor.item", "logger.warning", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.bert_biaffine_dependency_parser.BertBiaffineDependencyParser.forward_embeddings"], ["", "def", "_get_iter_norm_mean_eval", "(", "self", ",", "mean", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Traverse training data to get mean embeddings.\n        \"\"\"", "\n", "# Replace parameter values with the shadow values from the moving averages.", "\n", "if", "self", ".", "_moving_average", "is", "not", "None", ":", "\n", "            ", "self", ".", "_moving_average", ".", "assign_average_value", "(", ")", "\n", "\n", "", "if", "self", ".", "_validation_data_loader", "is", "not", "None", ":", "\n", "            ", "validation_data_loader", "=", "self", ".", "_validation_data_loader", "\n", "", "else", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Validation results cannot be calculated without a validation_data_loader\"", "\n", ")", "\n", "\n", "", "val_generator_tqdm", "=", "Tqdm", ".", "tqdm", "(", "validation_data_loader", ")", "\n", "done_early", "=", "False", "\n", "mean_embeddings", ":", "[", "torch", ".", "Tensor", ",", "int", "]", "\n", "mean_embeddings", "=", "[", "torch", ".", "tensor", "(", "[", "0.", "]", ",", "device", "=", "self", ".", "cuda_device", ")", ",", "0", "]", "\n", "\n", "for", "batch", "in", "val_generator_tqdm", ":", "\n", "            ", "if", "self", ".", "_distributed", ":", "\n", "# Check whether the other workers have stopped already (due to differing amounts of", "\n", "# data in each). If so, we can't proceed because we would hang when we hit the", "\n", "# barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor", "\n", "# here because NCCL process groups apparently don't support BoolTensor.", "\n", "                ", "done", "=", "torch", ".", "tensor", "(", "0", ",", "device", "=", "self", ".", "cuda_device", ")", "\n", "torch", ".", "distributed", ".", "all_reduce", "(", "done", ",", "torch", ".", "distributed", ".", "ReduceOp", ".", "SUM", ")", "\n", "if", "done", ".", "item", "(", ")", ">", "0", ":", "\n", "                    ", "done_early", "=", "True", "\n", "logger", ".", "warning", "(", "\n", "f\"Worker {torch.distributed.get_rank()} finishing validation early! \"", "\n", "\"This implies that there is an imbalance in your validation \"", "\n", "\"data across the workers and that some amount of it will be \"", "\n", "\"ignored. A small amount of this is fine, but a major imbalance \"", "\n", "\"should be avoided. Note: This warning will appear unless your \"", "\n", "\"data is perfectly balanced.\"", "\n", ")", "\n", "break", "\n", "\n", "", "", "batch", "=", "nn_util", ".", "move_to_device", "(", "batch", ",", "self", ".", "cuda_device", ")", "\n", "batch_embeddings", "=", "self", ".", "_pytorch_model", ".", "forward_embeddings", "(", "batch", "[", "'words'", "]", ",", "mean", ")", "\n", "mean_embeddings", "[", "0", "]", "=", "(", "mean_embeddings", "[", "0", "]", "+", "batch_embeddings", ".", "sum", "(", "dim", "=", "0", ")", ")", "\n", "mean_embeddings", "[", "1", "]", "+=", "batch_embeddings", ".", "shape", "[", "0", "]", "\n", "\n", "", "mean_embeddings", "[", "0", "]", "=", "mean_embeddings", "[", "0", "]", "/", "mean_embeddings", "[", "1", "]", "\n", "\n", "return", "mean_embeddings", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.iter_norm_trainer.Iter_Norm_Trainer._validation_loss": [[666, 761], ["logger.info", "iter_norm_trainer.Iter_Norm_Trainer._pytorch_model.eval", "allennlp.common.Tqdm.tqdm", "iter_norm_trainer.Iter_Norm_Trainer._moving_average.assign_average_value", "allennlp.common.checks.ConfigurationError", "iter_norm_trainer.Iter_Norm_Trainer.batch_outputs", "iter_norm_trainer.Iter_Norm_Trainer.get", "iter_norm_trainer.Iter_Norm_Trainer.get", "allennlp.training.util.get_metrics", "allennlp.training.util.description_from_metrics", "allennlp.common.Tqdm.tqdm.set_description", "logger.warning", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.tensor.item", "torch.tensor.item", "torch.tensor.item", "iter_norm_trainer.Iter_Norm_Trainer._moving_average.restore", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "iter_norm_trainer.Iter_Norm_Trainer.get.detach().cpu().numpy", "callback", "torch.tensor.item", "torch.tensor.item", "torch.tensor.item", "logger.warning", "iter_norm_trainer.Iter_Norm_Trainer.get.detach().cpu().numpy", "iter_norm_trainer.Iter_Norm_Trainer.get.detach().cpu", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "iter_norm_trainer.Iter_Norm_Trainer.get.detach().cpu", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "iter_norm_trainer.Iter_Norm_Trainer.get.detach", "iter_norm_trainer.Iter_Norm_Trainer.get.detach"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.iter_norm_trainer.Iter_Norm_Trainer.batch_outputs", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser.get_metrics"], ["", "def", "_validation_loss", "(", "self", ",", "epoch", ":", "int", ")", "->", "Tuple", "[", "float", ",", "float", ",", "int", "]", ":", "\n", "        ", "\"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"", "\n", "logger", ".", "info", "(", "\"Validating\"", ")", "\n", "\n", "self", ".", "_pytorch_model", ".", "eval", "(", ")", "\n", "\n", "# Replace parameter values with the shadow values from the moving averages.", "\n", "if", "self", ".", "_moving_average", "is", "not", "None", ":", "\n", "            ", "self", ".", "_moving_average", ".", "assign_average_value", "(", ")", "\n", "\n", "", "if", "self", ".", "_validation_data_loader", "is", "not", "None", ":", "\n", "            ", "validation_data_loader", "=", "self", ".", "_validation_data_loader", "\n", "", "else", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Validation results cannot be calculated without a validation_data_loader\"", "\n", ")", "\n", "\n", "", "val_generator_tqdm", "=", "Tqdm", ".", "tqdm", "(", "validation_data_loader", ")", "\n", "batches_this_epoch", "=", "0", "\n", "val_loss", "=", "0", "\n", "val_reg_loss", "=", "0", "\n", "done_early", "=", "False", "\n", "for", "batch", "in", "val_generator_tqdm", ":", "\n", "            ", "if", "self", ".", "_distributed", ":", "\n", "# Check whether the other workers have stopped already (due to differing amounts of", "\n", "# data in each). If so, we can't proceed because we would hang when we hit the", "\n", "# barrier implicit in Model.forward. We use a IntTensor instead a BoolTensor", "\n", "# here because NCCL process groups apparently don't support BoolTensor.", "\n", "                ", "done", "=", "torch", ".", "tensor", "(", "0", ",", "device", "=", "self", ".", "cuda_device", ")", "\n", "torch", ".", "distributed", ".", "all_reduce", "(", "done", ",", "torch", ".", "distributed", ".", "ReduceOp", ".", "SUM", ")", "\n", "if", "done", ".", "item", "(", ")", ">", "0", ":", "\n", "                    ", "done_early", "=", "True", "\n", "logger", ".", "warning", "(", "\n", "f\"Worker {torch.distributed.get_rank()} finishing validation early! \"", "\n", "\"This implies that there is an imbalance in your validation \"", "\n", "\"data across the workers and that some amount of it will be \"", "\n", "\"ignored. A small amount of this is fine, but a major imbalance \"", "\n", "\"should be avoided. Note: This warning will appear unless your \"", "\n", "\"data is perfectly balanced.\"", "\n", ")", "\n", "break", "\n", "\n", "", "", "batch_outputs", "=", "self", ".", "batch_outputs", "(", "batch", ",", "for_training", "=", "False", ")", "\n", "loss", "=", "batch_outputs", ".", "get", "(", "\"loss\"", ")", "\n", "reg_loss", "=", "batch_outputs", ".", "get", "(", "\"reg_loss\"", ")", "\n", "if", "loss", "is", "not", "None", ":", "\n", "# You shouldn't necessarily have to compute a loss for validation, so we allow for", "\n", "# `loss` to be None.  We need to be careful, though - `batches_this_epoch` is", "\n", "# currently only used as the divisor for the loss function, so we can safely only", "\n", "# count those batches for which we actually have a loss.  If this variable ever", "\n", "# gets used for something else, we might need to change things around a bit.", "\n", "                ", "batches_this_epoch", "+=", "1", "\n", "val_loss", "+=", "loss", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "if", "reg_loss", "is", "not", "None", ":", "\n", "                    ", "val_reg_loss", "+=", "reg_loss", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "# Update the description with the latest metrics", "\n", "", "", "val_metrics", "=", "training_util", ".", "get_metrics", "(", "\n", "self", ".", "model", ",", "\n", "val_loss", ",", "\n", "val_reg_loss", ",", "\n", "batches_this_epoch", ",", "\n", "world_size", "=", "self", ".", "_world_size", ",", "\n", "cuda_device", "=", "self", ".", "cuda_device", ",", "\n", ")", "\n", "description", "=", "training_util", ".", "description_from_metrics", "(", "val_metrics", ")", "\n", "val_generator_tqdm", ".", "set_description", "(", "description", ",", "refresh", "=", "False", ")", "\n", "\n", "for", "callback", "in", "self", ".", "_batch_callbacks", ":", "\n", "                ", "callback", "(", "\n", "self", ",", "\n", "[", "batch", "]", ",", "\n", "[", "batch_outputs", "]", ",", "\n", "epoch", ",", "\n", "batches_this_epoch", ",", "\n", "is_training", "=", "False", ",", "\n", "is_master", "=", "self", ".", "_master", ",", "\n", ")", "\n", "\n", "", "", "if", "self", ".", "_distributed", "and", "not", "done_early", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "f\"Worker {torch.distributed.get_rank()} completed its entire epoch (validation).\"", "\n", ")", "\n", "# Indicate that we're done so that any workers that have remaining data stop validation early.", "\n", "done", "=", "torch", ".", "tensor", "(", "1", ",", "device", "=", "self", ".", "cuda_device", ")", "\n", "torch", ".", "distributed", ".", "all_reduce", "(", "done", ",", "torch", ".", "distributed", ".", "ReduceOp", ".", "SUM", ")", "\n", "assert", "done", ".", "item", "(", ")", "\n", "\n", "# Now restore the original parameter values.", "\n", "", "if", "self", ".", "_moving_average", "is", "not", "None", ":", "\n", "            ", "self", ".", "_moving_average", ".", "restore", "(", ")", "\n", "\n", "", "return", "val_loss", ",", "val_reg_loss", ",", "batches_this_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.iter_norm_trainer.Iter_Norm_Trainer.train": [[762, 928], ["allennlp.training.util.enable_gradient_clipping", "logger.info", "time.time", "iter_norm_trainer.Iter_Norm_Trainer._metric_tracker.best_epoch_metrics.items", "range", "iter_norm_trainer.Iter_Norm_Trainer._tensorboard.close", "iter_norm_trainer.Iter_Norm_Trainer._checkpointer.best_model_state", "iter_norm_trainer.Iter_Norm_Trainer._restore_checkpoint", "callback", "range", "range", "time.time", "iter_norm_trainer.Iter_Norm_Trainer._train_epoch", "iter_norm_trainer.Iter_Norm_Trainer.items", "str", "iter_norm_trainer.Iter_Norm_Trainer.items", "allennlp.training.util.get_metrics.items", "iter_norm_trainer.Iter_Norm_Trainer._metric_tracker.is_best_so_far", "logger.info", "iter_norm_trainer.Iter_Norm_Trainer.model.load_state_dict", "traceback.print_exc", "allennlp.common.checks.ConfigurationError", "logging.info", "iter_norm_trainer.Iter_Norm_Trainer._get_iter_norm_mean_train", "iter_norm_trainer.Iter_Norm_Trainer.iter_mean_train.append", "logging.info", "iter_norm_trainer.Iter_Norm_Trainer._get_iter_norm_mean_eval", "iter_norm_trainer.Iter_Norm_Trainer.iter_mean_eval.append", "iter_norm_trainer.Iter_Norm_Trainer._tensorboard.log_metrics", "time.time", "datetime.timedelta", "allennlp.training.util.get_metrics.items", "allennlp.common.util.dump_metrics", "iter_norm_trainer.Iter_Norm_Trainer._learning_rate_scheduler.step", "iter_norm_trainer.Iter_Norm_Trainer._momentum_scheduler.step", "iter_norm_trainer.Iter_Norm_Trainer._checkpointer.save_checkpoint", "torch.barrier", "torch.barrier", "torch.barrier", "callback", "time.time", "datetime.timedelta", "str", "logger.info", "key.startswith", "key.endswith", "max", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "iter_norm_trainer.Iter_Norm_Trainer._validation_loss", "allennlp.training.util.get_metrics", "iter_norm_trainer.Iter_Norm_Trainer._metric_tracker.add_metric", "iter_norm_trainer.Iter_Norm_Trainer._metric_tracker.should_stop_early", "os.path.join", "time.time", "datetime.timedelta", "metrics.get", "key.startswith", "key.endswith", "max", "torch.barrier", "torch.barrier", "torch.barrier", "logger.info", "iter_norm_trainer.Iter_Norm_Trainer._metric_tracker.is_best_so_far", "metrics.get", "float", "int"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.iter_norm_trainer.Iter_Norm_Trainer._restore_checkpoint", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.iter_norm_trainer.Iter_Norm_Trainer._train_epoch", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.iter_norm_trainer.Iter_Norm_Trainer._get_iter_norm_mean_train", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.iter_norm_trainer.Iter_Norm_Trainer._get_iter_norm_mean_eval", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.iter_norm_trainer.Iter_Norm_Trainer._validation_loss", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser.get_metrics"], ["", "def", "train", "(", "self", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "        ", "\"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"", "\n", "try", ":", "\n", "            ", "epoch_counter", "=", "self", ".", "_restore_checkpoint", "(", ")", "\n", "", "except", "RuntimeError", ":", "\n", "            ", "traceback", ".", "print_exc", "(", ")", "\n", "raise", "ConfigurationError", "(", "\n", "\"Could not recover training from the checkpoint.  Did you mean to output to \"", "\n", "\"a different serialization directory or delete the existing serialization \"", "\n", "\"directory?\"", "\n", ")", "\n", "\n", "", "training_util", ".", "enable_gradient_clipping", "(", "self", ".", "model", ",", "self", ".", "_grad_clipping", ")", "\n", "\n", "logger", ".", "info", "(", "\"Beginning training.\"", ")", "\n", "\n", "val_metrics", ":", "Dict", "[", "str", ",", "float", "]", "=", "{", "}", "\n", "this_epoch_val_metric", ":", "float", "=", "None", "\n", "metrics", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "}", "\n", "epochs_trained", "=", "0", "\n", "training_start_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "metrics", "[", "\"best_epoch\"", "]", "=", "self", ".", "_metric_tracker", ".", "best_epoch", "\n", "for", "key", ",", "value", "in", "self", ".", "_metric_tracker", ".", "best_epoch_metrics", ".", "items", "(", ")", ":", "\n", "            ", "metrics", "[", "\"best_validation_\"", "+", "key", "]", "=", "value", "\n", "\n", "", "for", "callback", "in", "self", ".", "_epoch_callbacks", ":", "\n", "            ", "callback", "(", "self", ",", "metrics", "=", "{", "}", ",", "epoch", "=", "-", "1", ",", "is_master", "=", "self", ".", "_master", ")", "\n", "\n", "", "if", "self", ".", "iter_num", ":", "\n", "\n", "# Obtrain training info for iterative normalization:", "\n", "            ", "self", ".", "iter_mean_train", "=", "[", "]", "\n", "for", "iter_norm_i", "in", "range", "(", "self", ".", "iter_num", ")", ":", "\n", "                ", "logging", ".", "info", "(", "\"This is the {} time during iterative normalization for training\"", ".", "format", "(", "iter_norm_i", ")", ")", "\n", "mean", "=", "self", ".", "_get_iter_norm_mean_train", "(", "self", ".", "iter_mean_train", ")", "\n", "self", ".", "iter_mean_train", ".", "append", "(", "mean", ")", "\n", "\n", "# Obtrain evaluation info for iterative normalization:", "\n", "", "self", ".", "iter_mean_eval", "=", "[", "]", "\n", "for", "iter_norm_i", "in", "range", "(", "self", ".", "iter_num", ")", ":", "\n", "                ", "logging", ".", "info", "(", "\"This is the {} time during iterative normalization for evaluation\"", ".", "format", "(", "iter_norm_i", ")", ")", "\n", "mean", "=", "self", ".", "_get_iter_norm_mean_eval", "(", "self", ".", "iter_mean_eval", ")", "\n", "self", ".", "iter_mean_eval", ".", "append", "(", "mean", ")", "\n", "\n", "", "self", ".", "_pytorch_model", ".", "text_field_embedder", ".", "_token_embedders", "[", "'tokens'", "]", ".", "iter_norm", "=", "None", "\n", "self", ".", "_pytorch_model", ".", "text_field_embedder", ".", "_token_embedders", "[", "'tokens'", "]", ".", "_matched_embedder", ".", "mean_emb_train", "=", "self", ".", "iter_mean_train", "\n", "self", ".", "_pytorch_model", ".", "text_field_embedder", ".", "_token_embedders", "[", "'tokens'", "]", ".", "_matched_embedder", ".", "mean_emb_eval", "=", "self", ".", "iter_mean_eval", "\n", "\n", "", "for", "epoch", "in", "range", "(", "epoch_counter", ",", "self", ".", "_num_epochs", ")", ":", "\n", "            ", "epoch_start_time", "=", "time", ".", "time", "(", ")", "\n", "if", "self", ".", "iter_num", ":", "\n", "                ", "self", ".", "_pytorch_model", ".", "text_field_embedder", ".", "_token_embedders", "[", "'tokens'", "]", ".", "_matched_embedder", ".", "is_train", "=", "True", "\n", "", "train_metrics", "=", "self", ".", "_train_epoch", "(", "epoch", ")", "\n", "\n", "# get peak of memory usage", "\n", "for", "key", ",", "value", "in", "train_metrics", ".", "items", "(", ")", ":", "\n", "                ", "if", "key", ".", "startswith", "(", "\"gpu_\"", ")", "and", "key", ".", "endswith", "(", "\"_memory_MB\"", ")", ":", "\n", "                    ", "metrics", "[", "\"peak_\"", "+", "key", "]", "=", "max", "(", "metrics", ".", "get", "(", "\"peak_\"", "+", "key", ",", "0", ")", ",", "value", ")", "\n", "", "elif", "key", ".", "startswith", "(", "\"worker_\"", ")", "and", "key", ".", "endswith", "(", "\"_memory_MB\"", ")", ":", "\n", "                    ", "metrics", "[", "\"peak_\"", "+", "key", "]", "=", "max", "(", "metrics", ".", "get", "(", "\"peak_\"", "+", "key", ",", "0", ")", ",", "value", ")", "\n", "\n", "", "", "if", "self", ".", "_validation_data_loader", "is", "not", "None", ":", "\n", "                ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# We have a validation set, so compute all the metrics on it.", "\n", "                    ", "if", "self", ".", "iter_num", ":", "\n", "                        ", "self", ".", "_pytorch_model", ".", "text_field_embedder", ".", "_token_embedders", "[", "'tokens'", "]", ".", "_matched_embedder", ".", "is_train", "=", "False", "\n", "", "val_loss", ",", "val_reg_loss", ",", "num_batches", "=", "self", ".", "_validation_loss", "(", "epoch", ")", "\n", "\n", "# It is safe again to wait till the validation is done. This is", "\n", "# important to get the metrics right.", "\n", "if", "self", ".", "_distributed", ":", "\n", "                        ", "dist", ".", "barrier", "(", ")", "\n", "\n", "", "val_metrics", "=", "training_util", ".", "get_metrics", "(", "\n", "self", ".", "model", ",", "\n", "val_loss", ",", "\n", "val_reg_loss", ",", "\n", "num_batches", ",", "\n", "reset", "=", "True", ",", "\n", "world_size", "=", "self", ".", "_world_size", ",", "\n", "cuda_device", "=", "self", ".", "cuda_device", ",", "\n", ")", "\n", "\n", "# Check validation metric for early stopping", "\n", "this_epoch_val_metric", "=", "val_metrics", "[", "self", ".", "_validation_metric", "]", "\n", "self", ".", "_metric_tracker", ".", "add_metric", "(", "this_epoch_val_metric", ")", "\n", "\n", "if", "self", ".", "_metric_tracker", ".", "should_stop_early", "(", ")", ":", "\n", "                        ", "logger", ".", "info", "(", "\"Ran out of patience.  Stopping training.\"", ")", "\n", "break", "\n", "\n", "", "", "", "if", "self", ".", "_master", ":", "\n", "                ", "self", ".", "_tensorboard", ".", "log_metrics", "(", "\n", "train_metrics", ",", "val_metrics", "=", "val_metrics", ",", "log_to_console", "=", "True", ",", "epoch", "=", "epoch", "+", "1", "\n", ")", "# +1 because tensorboard doesn't like 0", "\n", "\n", "# Create overall metrics dict", "\n", "", "training_elapsed_time", "=", "time", ".", "time", "(", ")", "-", "training_start_time", "\n", "metrics", "[", "\"training_duration\"", "]", "=", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "training_elapsed_time", ")", ")", "\n", "metrics", "[", "\"training_start_epoch\"", "]", "=", "epoch_counter", "\n", "metrics", "[", "\"training_epochs\"", "]", "=", "epochs_trained", "\n", "metrics", "[", "\"epoch\"", "]", "=", "epoch", "\n", "\n", "for", "key", ",", "value", "in", "train_metrics", ".", "items", "(", ")", ":", "\n", "                ", "metrics", "[", "\"training_\"", "+", "key", "]", "=", "value", "\n", "", "for", "key", ",", "value", "in", "val_metrics", ".", "items", "(", ")", ":", "\n", "                ", "metrics", "[", "\"validation_\"", "+", "key", "]", "=", "value", "\n", "\n", "", "if", "self", ".", "_metric_tracker", ".", "is_best_so_far", "(", ")", ":", "\n", "# Update all the best_ metrics.", "\n", "# (Otherwise they just stay the same as they were.)", "\n", "                ", "metrics", "[", "\"best_epoch\"", "]", "=", "epoch", "\n", "for", "key", ",", "value", "in", "val_metrics", ".", "items", "(", ")", ":", "\n", "                    ", "metrics", "[", "\"best_validation_\"", "+", "key", "]", "=", "value", "\n", "\n", "", "self", ".", "_metric_tracker", ".", "best_epoch_metrics", "=", "val_metrics", "\n", "\n", "", "if", "self", ".", "_serialization_dir", "and", "self", ".", "_master", ":", "\n", "                ", "common_util", ".", "dump_metrics", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "_serialization_dir", ",", "f\"metrics_epoch_{epoch}.json\"", ")", ",", "metrics", "\n", ")", "\n", "\n", "# The Scheduler API is agnostic to whether your schedule requires a validation metric -", "\n", "# if it doesn't, the validation metric passed here is ignored.", "\n", "", "if", "self", ".", "_learning_rate_scheduler", ":", "\n", "                ", "self", ".", "_learning_rate_scheduler", ".", "step", "(", "this_epoch_val_metric", ")", "\n", "", "if", "self", ".", "_momentum_scheduler", ":", "\n", "                ", "self", ".", "_momentum_scheduler", ".", "step", "(", "this_epoch_val_metric", ")", "\n", "\n", "", "if", "self", ".", "_master", ":", "\n", "                ", "self", ".", "_checkpointer", ".", "save_checkpoint", "(", "\n", "epoch", ",", "self", ",", "is_best_so_far", "=", "self", ".", "_metric_tracker", ".", "is_best_so_far", "(", ")", "\n", ")", "\n", "\n", "# Wait for the master to finish saving the checkpoint", "\n", "", "if", "self", ".", "_distributed", ":", "\n", "                ", "dist", ".", "barrier", "(", ")", "\n", "\n", "", "for", "callback", "in", "self", ".", "_epoch_callbacks", ":", "\n", "                ", "callback", "(", "self", ",", "metrics", "=", "metrics", ",", "epoch", "=", "epoch", ",", "is_master", "=", "self", ".", "_master", ")", "\n", "\n", "", "epoch_elapsed_time", "=", "time", ".", "time", "(", ")", "-", "epoch_start_time", "\n", "logger", ".", "info", "(", "\"Epoch duration: %s\"", ",", "datetime", ".", "timedelta", "(", "seconds", "=", "epoch_elapsed_time", ")", ")", "\n", "\n", "if", "epoch", "<", "self", ".", "_num_epochs", "-", "1", ":", "\n", "                ", "training_elapsed_time", "=", "time", ".", "time", "(", ")", "-", "training_start_time", "\n", "estimated_time_remaining", "=", "training_elapsed_time", "*", "(", "\n", "(", "self", ".", "_num_epochs", "-", "epoch_counter", ")", "/", "float", "(", "epoch", "-", "epoch_counter", "+", "1", ")", "-", "1", "\n", ")", "\n", "formatted_time", "=", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "estimated_time_remaining", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"Estimated training time remaining: %s\"", ",", "formatted_time", ")", "\n", "\n", "", "epochs_trained", "+=", "1", "\n", "\n", "# make sure pending events are flushed to disk and files are closed properly", "\n", "", "self", ".", "_tensorboard", ".", "close", "(", ")", "\n", "\n", "# Load the best model state before returning", "\n", "best_model_state", "=", "self", ".", "_checkpointer", ".", "best_model_state", "(", ")", "\n", "if", "best_model_state", ":", "\n", "            ", "self", ".", "model", ".", "load_state_dict", "(", "best_model_state", ")", "\n", "\n", "", "return", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.iter_norm_trainer.Iter_Norm_Trainer.get_checkpoint_state": [[929, 959], ["iter_norm_trainer.Iter_Norm_Trainer.model.state_dict", "iter_norm_trainer.Iter_Norm_Trainer._moving_average.assign_average_value", "iter_norm_trainer.Iter_Norm_Trainer._metric_tracker.state_dict", "iter_norm_trainer.Iter_Norm_Trainer.optimizer.state_dict", "iter_norm_trainer.Iter_Norm_Trainer._learning_rate_scheduler.state_dict", "iter_norm_trainer.Iter_Norm_Trainer._momentum_scheduler.state_dict", "amp.state_dict", "iter_norm_trainer.Iter_Norm_Trainer._moving_average.restore"], "methods", ["None"], ["", "@", "contextmanager", "\n", "def", "get_checkpoint_state", "(", "self", ")", "->", "Iterator", "[", "Tuple", "[", "Dict", "[", "str", ",", "Any", "]", ",", "Dict", "[", "str", ",", "Any", "]", "]", "]", ":", "\n", "        ", "if", "self", ".", "_moving_average", "is", "not", "None", ":", "\n", "# Assigning average value to model parameters.  The checkpointer will call", "\n", "# `restore_state_after_checkpointing` when it is done to put this back to what it was.", "\n", "            ", "self", ".", "_moving_average", ".", "assign_average_value", "(", ")", "\n", "\n", "", "model_state", "=", "self", ".", "model", ".", "state_dict", "(", ")", "\n", "\n", "# These are the training states we need to persist.", "\n", "training_states", "=", "{", "\n", "\"metric_tracker\"", ":", "self", ".", "_metric_tracker", ".", "state_dict", "(", ")", ",", "\n", "\"optimizer\"", ":", "self", ".", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "\"batch_num_total\"", ":", "self", ".", "_batch_num_total", ",", "\n", "}", "\n", "\n", "# If we have a learning rate or momentum scheduler, we should persist them too.", "\n", "if", "self", ".", "_learning_rate_scheduler", "is", "not", "None", ":", "\n", "            ", "training_states", "[", "\"learning_rate_scheduler\"", "]", "=", "self", ".", "_learning_rate_scheduler", ".", "state_dict", "(", ")", "\n", "", "if", "self", ".", "_momentum_scheduler", "is", "not", "None", ":", "\n", "            ", "training_states", "[", "\"momentum_scheduler\"", "]", "=", "self", ".", "_momentum_scheduler", ".", "state_dict", "(", ")", "\n", "# If model was trained with amp, we should persist the amp state.", "\n", "", "if", "self", ".", "_opt_level", "is", "not", "None", ":", "\n", "            ", "training_states", "[", "\"amp\"", "]", "=", "amp", ".", "state_dict", "(", ")", "\n", "\n", "", "try", ":", "\n", "            ", "yield", "model_state", ",", "training_states", "\n", "", "finally", ":", "\n", "            ", "if", "self", ".", "_moving_average", "is", "not", "None", ":", "\n", "                ", "self", ".", "_moving_average", ".", "restore", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.iter_norm_trainer.Iter_Norm_Trainer._restore_checkpoint": [[960, 1024], ["iter_norm_trainer.Iter_Norm_Trainer._checkpointer.restore_checkpoint", "iter_norm_trainer.Iter_Norm_Trainer.model.load_state_dict", "iter_norm_trainer.Iter_Norm_Trainer.optimizer.load_state_dict", "allennlp.training.util.move_optimizer_to_cuda", "isinstance", "training_state.get", "amp.initialize", "amp.load_state_dict", "iter_norm_trainer.Iter_Norm_Trainer._learning_rate_scheduler.load_state_dict", "iter_norm_trainer.Iter_Norm_Trainer._momentum_scheduler.load_state_dict", "iter_norm_trainer.Iter_Norm_Trainer._metric_tracker.load_state_dict", "iter_norm_trainer.Iter_Norm_Trainer._metric_tracker.clear", "iter_norm_trainer.Iter_Norm_Trainer._metric_tracker.add_metrics", "iter_norm_trainer.Iter_Norm_Trainer._metric_tracker.clear", "int", "training_state[].split"], "methods", ["None"], ["", "", "", "def", "_restore_checkpoint", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: `int`\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"", "\n", "model_state", ",", "training_state", "=", "self", ".", "_checkpointer", ".", "restore_checkpoint", "(", ")", "\n", "\n", "if", "not", "training_state", ":", "\n", "# No checkpoint to restore, start at 0", "\n", "            ", "return", "0", "\n", "\n", "# The apex docs recommend calling amp.initialize before calling load_state_dict.", "\n", "", "if", "self", ".", "_opt_level", "is", "not", "None", "and", "\"amp\"", "in", "training_state", ":", "\n", "            ", "self", ".", "model", ",", "self", ".", "optimizer", "=", "amp", ".", "initialize", "(", "\n", "self", ".", "model", ",", "self", ".", "optimizer", ",", "opt_level", "=", "self", ".", "_opt_level", "\n", ")", "\n", "amp", ".", "load_state_dict", "(", "training_state", "[", "\"amp\"", "]", ")", "\n", "", "self", ".", "model", ".", "load_state_dict", "(", "model_state", ")", "\n", "self", ".", "optimizer", ".", "load_state_dict", "(", "training_state", "[", "\"optimizer\"", "]", ")", "\n", "if", "(", "\n", "self", ".", "_learning_rate_scheduler", "is", "not", "None", "\n", "and", "\"learning_rate_scheduler\"", "in", "training_state", "\n", ")", ":", "\n", "            ", "self", ".", "_learning_rate_scheduler", ".", "load_state_dict", "(", "training_state", "[", "\"learning_rate_scheduler\"", "]", ")", "\n", "", "if", "self", ".", "_momentum_scheduler", "is", "not", "None", "and", "\"momentum_scheduler\"", "in", "training_state", ":", "\n", "            ", "self", ".", "_momentum_scheduler", ".", "load_state_dict", "(", "training_state", "[", "\"momentum_scheduler\"", "]", ")", "\n", "", "training_util", ".", "move_optimizer_to_cuda", "(", "self", ".", "optimizer", ")", "\n", "\n", "# Currently the `training_state` contains a serialized `MetricTracker`.", "\n", "if", "\"metric_tracker\"", "in", "training_state", ":", "\n", "            ", "self", ".", "_metric_tracker", ".", "load_state_dict", "(", "training_state", "[", "\"metric_tracker\"", "]", ")", "\n", "# It used to be the case that we tracked `val_metric_per_epoch`.", "\n", "", "elif", "\"val_metric_per_epoch\"", "in", "training_state", ":", "\n", "            ", "self", ".", "_metric_tracker", ".", "clear", "(", ")", "\n", "self", ".", "_metric_tracker", ".", "add_metrics", "(", "training_state", "[", "\"val_metric_per_epoch\"", "]", ")", "\n", "# And before that we didn't track anything.", "\n", "", "else", ":", "\n", "            ", "self", ".", "_metric_tracker", ".", "clear", "(", ")", "\n", "\n", "", "if", "isinstance", "(", "training_state", "[", "\"epoch\"", "]", ",", "int", ")", ":", "\n", "            ", "epoch_to_return", "=", "training_state", "[", "\"epoch\"", "]", "+", "1", "\n", "", "else", ":", "\n", "            ", "epoch_to_return", "=", "int", "(", "training_state", "[", "\"epoch\"", "]", ".", "split", "(", "\".\"", ")", "[", "0", "]", ")", "+", "1", "\n", "\n", "# For older checkpoints with batch_num_total missing, default to old behavior where", "\n", "# it is unchanged.", "\n", "", "batch_num_total", "=", "training_state", ".", "get", "(", "\"batch_num_total\"", ")", "\n", "if", "batch_num_total", "is", "not", "None", ":", "\n", "            ", "self", ".", "_batch_num_total", "=", "batch_num_total", "\n", "\n", "", "return", "epoch_to_return", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.iter_norm_trainer.Iter_Norm_Trainer.from_partial_objects": [[1025, 1127], ["allennlp.common.checks.check_for_gpu", "allennlp.common.util.log_frozen_and_tunable_parameter_names", "optimizer.construct", "moving_average.construct", "learning_rate_scheduler.construct", "momentum_scheduler.construct", "cls", "model.cuda.cuda.cuda", "model.cuda.cuda.named_parameters", "allennlp.training.optimizers.Optimizer.default", "len", "math.ceil", "checkpointer.construct", "allennlp.training.checkpointer.Checkpointer", "tensorboard_writer.construct", "allennlp.training.tensorboard_writer.TensorboardWriter", "any", "model.cuda.cuda.named_parameters", "parameter.requires_grad_", "re.search"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_partial_objects", "(", "\n", "cls", ",", "\n", "model", ":", "Model", ",", "\n", "serialization_dir", ":", "str", ",", "\n", "data_loader", ":", "DataLoader", ",", "\n", "validation_data_loader", ":", "DataLoader", "=", "None", ",", "\n", "local_rank", ":", "int", "=", "0", ",", "\n", "patience", ":", "int", "=", "None", ",", "\n", "validation_metric", ":", "str", "=", "\"-loss\"", ",", "\n", "num_epochs", ":", "int", "=", "20", ",", "\n", "cuda_device", ":", "int", "=", "-", "1", ",", "\n", "grad_norm", ":", "float", "=", "None", ",", "\n", "grad_clipping", ":", "float", "=", "None", ",", "\n", "distributed", ":", "bool", "=", "None", ",", "\n", "world_size", ":", "int", "=", "1", ",", "\n", "num_gradient_accumulation_steps", ":", "int", "=", "1", ",", "\n", "opt_level", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "no_grad", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "optimizer", ":", "Lazy", "[", "Optimizer", "]", "=", "None", ",", "\n", "learning_rate_scheduler", ":", "Lazy", "[", "LearningRateScheduler", "]", "=", "None", ",", "\n", "momentum_scheduler", ":", "Lazy", "[", "MomentumScheduler", "]", "=", "None", ",", "\n", "tensorboard_writer", ":", "Lazy", "[", "TensorboardWriter", "]", "=", "None", ",", "\n", "moving_average", ":", "Lazy", "[", "MovingAverage", "]", "=", "None", ",", "\n", "checkpointer", ":", "Lazy", "[", "Checkpointer", "]", "=", "None", ",", "\n", "batch_callbacks", ":", "List", "[", "BatchCallback", "]", "=", "None", ",", "\n", "epoch_callbacks", ":", "List", "[", "EpochCallback", "]", "=", "None", ",", "\n", ")", "->", "\"Trainer\"", ":", "\n", "        ", "\"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"", "\n", "\n", "check_for_gpu", "(", "cuda_device", ")", "\n", "if", "cuda_device", ">=", "0", ":", "\n", "# Moving model to GPU here so that the optimizer state gets constructed on", "\n", "# the right device.", "\n", "            ", "model", "=", "model", ".", "cuda", "(", "cuda_device", ")", "\n", "\n", "", "if", "no_grad", ":", "\n", "            ", "for", "name", ",", "parameter", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "                ", "if", "any", "(", "re", ".", "search", "(", "regex", ",", "name", ")", "for", "regex", "in", "no_grad", ")", ":", "\n", "                    ", "parameter", ".", "requires_grad_", "(", "False", ")", "\n", "\n", "", "", "", "common_util", ".", "log_frozen_and_tunable_parameter_names", "(", "model", ")", "\n", "\n", "parameters", "=", "[", "[", "n", ",", "p", "]", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "p", ".", "requires_grad", "]", "\n", "optimizer_", "=", "optimizer", ".", "construct", "(", "model_parameters", "=", "parameters", ")", "\n", "if", "not", "optimizer_", ":", "\n", "            ", "optimizer_", "=", "Optimizer", ".", "default", "(", "parameters", ")", "\n", "\n", "", "batches_per_epoch", ":", "Optional", "[", "int", "]", "\n", "try", ":", "\n", "            ", "batches_per_epoch", "=", "len", "(", "data_loader", ")", "\n", "batches_per_epoch", "=", "math", ".", "ceil", "(", "batches_per_epoch", "/", "num_gradient_accumulation_steps", ")", "\n", "", "except", "TypeError", ":", "\n", "            ", "batches_per_epoch", "=", "None", "\n", "\n", "", "moving_average_", "=", "moving_average", ".", "construct", "(", "parameters", "=", "parameters", ")", "\n", "learning_rate_scheduler_", "=", "learning_rate_scheduler", ".", "construct", "(", "\n", "optimizer", "=", "optimizer_", ",", "num_epochs", "=", "num_epochs", ",", "num_steps_per_epoch", "=", "batches_per_epoch", "\n", ")", "\n", "momentum_scheduler_", "=", "momentum_scheduler", ".", "construct", "(", "optimizer", "=", "optimizer_", ")", "\n", "\n", "checkpointer_", "=", "checkpointer", ".", "construct", "(", ")", "or", "Checkpointer", "(", "serialization_dir", ")", "\n", "tensorboard_writer_", "=", "tensorboard_writer", ".", "construct", "(", ")", "or", "TensorboardWriter", "(", "serialization_dir", ")", "\n", "\n", "return", "cls", "(", "\n", "model", ",", "\n", "optimizer_", ",", "\n", "data_loader", ",", "\n", "patience", "=", "patience", ",", "\n", "validation_metric", "=", "validation_metric", ",", "\n", "validation_data_loader", "=", "validation_data_loader", ",", "\n", "num_epochs", "=", "num_epochs", ",", "\n", "serialization_dir", "=", "serialization_dir", ",", "\n", "cuda_device", "=", "cuda_device", ",", "\n", "grad_norm", "=", "grad_norm", ",", "\n", "grad_clipping", "=", "grad_clipping", ",", "\n", "learning_rate_scheduler", "=", "learning_rate_scheduler_", ",", "\n", "momentum_scheduler", "=", "momentum_scheduler_", ",", "\n", "tensorboard_writer", "=", "tensorboard_writer_", ",", "\n", "checkpointer", "=", "checkpointer_", ",", "\n", "moving_average", "=", "moving_average_", ",", "\n", "batch_callbacks", "=", "batch_callbacks", ",", "\n", "epoch_callbacks", "=", "epoch_callbacks", ",", "\n", "distributed", "=", "distributed", ",", "\n", "local_rank", "=", "local_rank", ",", "\n", "world_size", "=", "world_size", ",", "\n", "num_gradient_accumulation_steps", "=", "num_gradient_accumulation_steps", ",", "\n", "opt_level", "=", "opt_level", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.iter_norm_trainer.degree_anisotropy": [[42, 51], ["torch.triu().cpu", "torch.triu().cpu", "torch.triu().cpu", "torch.triu().t().cpu", "torch.triu().t().cpu", "torch.triu().t().cpu", "torch.where", "torch.where", "torch.where", "torch.mean", "torch.mean", "torch.mean", "torch.triu", "torch.triu", "torch.triu", "torch.triu().t", "torch.triu().t", "torch.triu().t", "vectors[].t", "torch.triu", "torch.triu", "torch.triu", "torch.ones", "torch.ones", "torch.ones"], "function", ["None"], ["def", "degree_anisotropy", "(", "vectors", ",", "cuda_device", ")", ":", "\n", "# index = random_indice_generator(1000, vectors.shape[1]).cuda(cuda_device)", "\n", "    ", "vectors", "=", "torch", ".", "triu", "(", "vectors", "[", ":", ",", ":", "]", ".", "t", "(", ")", "@", "vectors", "[", ":", ",", ":", "]", ",", "1", ")", ".", "cpu", "(", ")", "\n", "filter_st", "=", "torch", ".", "triu", "(", "-", "100", "*", "torch", ".", "ones", "(", "vectors", ".", "shape", "[", "1", "]", ",", "vectors", ".", "shape", "[", "1", "]", ")", ")", ".", "t", "(", ")", ".", "cpu", "(", ")", "\n", "vectors", "=", "torch", ".", "where", "(", "filter_st", "==", "-", "100", ",", "filter_st", ",", "vectors", ")", "\n", "ind", "=", "(", "vectors", "!=", "-", "100", ")", ".", "nonzero", "(", ")", "\n", "vectors", "=", "vectors", "[", "ind", "[", ":", ",", "0", "]", ",", "ind", "[", ":", ",", "1", "]", "]", "\n", "\n", "return", "torch", ".", "mean", "(", "vectors", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder.__init__": [[41, 84], ["allennlp.modules.token_embedders.token_embedder.TokenEmbedder.__init__", "allennlp.data.tokenizers.PretrainedTransformerTokenizer", "len", "len", "transformers.AutoConfig.from_pretrained", "transformers.modeling_auto.AutoModel.from_pretrained", "transformers.modeling_auto.AutoModel.from_pretrained", "hasattr", "getattr", "static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder.transformer_model.eval"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.evaluator.evaluator.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "model_name", ":", "str", ",", "\n", "max_length", ":", "int", "=", "None", ",", "\n", "sub_module", ":", "str", "=", "None", ",", "\n", "train_parameters", ":", "bool", "=", "False", ",", "\n", "if_top_layers", ":", "bool", "=", "False", ",", "\n", "if_normalize", ":", "bool", "=", "False", ",", "\n", "map_path", ":", "str", "=", "None", ",", "\n", "iter_norm", ":", "int", "=", "None", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "if_top_layers", "=", "if_top_layers", "\n", "self", ".", "if_normalize", "=", "if_normalize", "\n", "self", ".", "map_path", "=", "map_path", "\n", "self", ".", "iter_norm", "=", "iter_norm", "\n", "if", "self", ".", "iter_norm", ":", "\n", "            ", "self", ".", "mean_emb_train", "=", "[", "]", "\n", "self", ".", "mean_emb_eval", "=", "[", "]", "\n", "self", ".", "is_train", "=", "None", "\n", "\n", "", "if", "if_top_layers", ":", "\n", "            ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "model_name", ",", "output_hidden_states", "=", "True", ")", "\n", "self", ".", "transformer_model", "=", "AutoModel", ".", "from_pretrained", "(", "model_name", ",", "config", "=", "config", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "transformer_model", "=", "AutoModel", ".", "from_pretrained", "(", "model_name", ")", "\n", "", "self", ".", "config", "=", "self", ".", "transformer_model", ".", "config", "\n", "\n", "if", "sub_module", ":", "\n", "            ", "assert", "hasattr", "(", "self", ".", "transformer_model", ",", "sub_module", ")", "\n", "self", ".", "transformer_model", "=", "getattr", "(", "self", ".", "transformer_model", ",", "sub_module", ")", "\n", "", "if", "not", "train_parameters", ":", "\n", "            ", "self", ".", "transformer_model", ".", "eval", "(", ")", "\n", "", "self", ".", "_max_length", "=", "max_length", "\n", "# I'm not sure if this works for all models; open an issue on github if you find a case", "\n", "# where it doesn't work.", "\n", "self", ".", "output_dim", "=", "self", ".", "config", ".", "hidden_size", "\n", "self", ".", "_train_parameters", "=", "train_parameters", "\n", "\n", "tokenizer", "=", "PretrainedTransformerTokenizer", "(", "model_name", ")", "\n", "self", ".", "_num_added_start_tokens", "=", "len", "(", "tokenizer", ".", "single_sequence_start_tokens", ")", "\n", "self", ".", "_num_added_end_tokens", "=", "len", "(", "tokenizer", ".", "single_sequence_end_tokens", ")", "\n", "self", ".", "_num_added_tokens", "=", "self", ".", "_num_added_start_tokens", "+", "self", ".", "_num_added_end_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder.get_output_dim": [[85, 88], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_output_dim", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder._number_of_token_type_embeddings": [[89, 96], ["isinstance", "hasattr"], "methods", ["None"], ["", "def", "_number_of_token_type_embeddings", "(", "self", ")", ":", "\n", "        ", "if", "isinstance", "(", "self", ".", "config", ",", "XLNetConfig", ")", ":", "\n", "            ", "return", "3", "# XLNet has 3 type ids", "\n", "", "elif", "hasattr", "(", "self", ".", "config", ",", "\"type_vocab_size\"", ")", ":", "\n", "            ", "return", "self", ".", "config", ".", "type_vocab_size", "\n", "", "else", ":", "\n", "            ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder.get_embeddings": [[97, 175], ["static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder.transformer_model.eval", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "type_ids.max", "token_ids.size", "static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder._fold_long_sequences", "transformer_mask.float", "torch.stack().permute", "torch.stack().permute", "torch.stack().permute", "torch.stack().permute", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder._unfold_long_sequences", "token_ids.size", "static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder.transformer_model", "static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder.transformer_model", "torch.norm().view", "torch.norm().view", "torch.norm().view", "torch.norm().view", "static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder._number_of_token_type_embeddings", "ValueError", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.norm", "torch.norm", "torch.norm", "torch.norm"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder._fold_long_sequences", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder._unfold_long_sequences", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder._number_of_token_type_embeddings"], ["", "", "def", "get_embeddings", "(", "\n", "self", ",", "\n", "token_ids", ":", "torch", ".", "LongTensor", ",", "\n", "mask", ":", "torch", ".", "BoolTensor", ",", "\n", "type_ids", ":", "Optional", "[", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "segment_concat_mask", ":", "Optional", "[", "torch", ".", "BoolTensor", "]", "=", "None", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "# type: ignore", "\n", "        ", "\"\"\"\n        # Parameters\n\n        token_ids: `torch.LongTensor`\n            Shape: `[batch_size, num_wordpieces if max_length is None else num_segment_concat_wordpieces]`.\n            num_segment_concat_wordpieces is num_wordpieces plus special tokens inserted in the\n            middle, e.g. the length of: \"[CLS] A B C [SEP] [CLS] D E F [SEP]\" (see indexer logic).\n        mask: `torch.BoolTensor`\n            Shape: [batch_size, num_wordpieces].\n        type_ids: `Optional[torch.LongTensor]`\n            Shape: `[batch_size, num_wordpieces if max_length is None else num_segment_concat_wordpieces]`.\n        segment_concat_mask: `Optional[torch.BoolTensor]`\n            Shape: `[batch_size, num_segment_concat_wordpieces]`.\n\n        # Returns\n\n        `torch.Tensor`\n            Shape: `[batch_size, num_wordpieces, embedding_size]`.\n\n        \"\"\"", "\n", "self", ".", "transformer_model", ".", "eval", "(", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "type_ids", "is", "not", "None", ":", "\n", "                ", "max_type_id", "=", "type_ids", ".", "max", "(", ")", "\n", "if", "max_type_id", "==", "0", ":", "\n", "                    ", "type_ids", "=", "None", "\n", "", "else", ":", "\n", "                    ", "if", "max_type_id", ">=", "self", ".", "_number_of_token_type_embeddings", "(", ")", ":", "\n", "                        ", "raise", "ValueError", "(", "\n", "\"Found type ids too large for the chosen transformer model.\"", "\n", ")", "\n", "", "assert", "token_ids", ".", "shape", "==", "type_ids", ".", "shape", "\n", "\n", "", "", "fold_long_sequences", "=", "(", "\n", "self", ".", "_max_length", "is", "not", "None", "and", "token_ids", ".", "size", "(", "1", ")", ">", "self", ".", "_max_length", "\n", ")", "\n", "if", "fold_long_sequences", ":", "\n", "                ", "batch_size", ",", "num_segment_concat_wordpieces", "=", "token_ids", ".", "size", "(", ")", "\n", "token_ids", ",", "segment_concat_mask", ",", "type_ids", "=", "self", ".", "_fold_long_sequences", "(", "\n", "token_ids", ",", "segment_concat_mask", ",", "type_ids", "\n", ")", "\n", "\n", "", "transformer_mask", "=", "segment_concat_mask", "if", "self", ".", "_max_length", "is", "not", "None", "else", "mask", "\n", "# Shape: [batch_size, num_wordpieces, embedding_size],", "\n", "# or if self._max_length is not None:", "\n", "# [batch_size * num_segments, self._max_length, embedding_size]", "\n", "\n", "# We call this with kwargs because some of the huggingface models don't have the", "\n", "# token_type_ids parameter and fail even when it's given as None.", "\n", "# Also, as of transformers v2.5.1, they are taking FloatTensor masks.", "\n", "parameters", "=", "{", "\"input_ids\"", ":", "token_ids", ",", "\"attention_mask\"", ":", "transformer_mask", ".", "float", "(", ")", "}", "\n", "if", "type_ids", "is", "not", "None", ":", "\n", "                ", "parameters", "[", "\"token_type_ids\"", "]", "=", "type_ids", "\n", "\n", "", "if", "self", ".", "if_top_layers", ":", "\n", "                ", "embeddings", "=", "self", ".", "transformer_model", "(", "**", "parameters", ")", "[", "2", "]", "\n", "embeddings", "=", "torch", ".", "stack", "(", "embeddings", ",", "dim", "=", "0", ")", ".", "permute", "(", "1", ",", "2", ",", "0", ",", "3", ")", "# [batch_size, tokens, layers, token_embeddings]", "\n", "embeddings", "=", "torch", ".", "sum", "(", "embeddings", "[", ":", ",", ":", ",", "-", "4", ":", ",", ":", "]", ",", "dim", "=", "2", ")", "\n", "", "else", ":", "\n", "                ", "embeddings", "=", "self", ".", "transformer_model", "(", "**", "parameters", ")", "[", "0", "]", "\n", "\n", "", "if", "self", ".", "if_normalize", ":", "\n", "                ", "embeddings", "=", "embeddings", "/", "torch", ".", "norm", "(", "embeddings", ",", "dim", "=", "2", ")", ".", "view", "(", "-", "1", ",", "embeddings", ".", "shape", "[", "1", "]", ",", "1", ")", "\n", "\n", "", "if", "fold_long_sequences", ":", "\n", "                ", "embeddings", "=", "self", ".", "_unfold_long_sequences", "(", "\n", "embeddings", ",", "segment_concat_mask", ",", "batch_size", ",", "num_segment_concat_wordpieces", "\n", ")", "\n", "\n", "", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder.forward": [[176, 277], ["static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder.transformer_model.eval", "torch.set_grad_enabled", "torch.set_grad_enabled", "torch.set_grad_enabled", "torch.set_grad_enabled", "type_ids.max", "token_ids.size", "static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder._fold_long_sequences", "transformer_mask.float", "torch.stack().permute", "torch.stack().permute", "torch.stack().permute", "torch.stack().permute", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "range", "torch.load().cuda", "torch.load().cuda", "torch.load().cuda", "torch.load().cuda", "static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder.permute", "static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder.permute", "static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder._unfold_long_sequences", "token_ids.size", "static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder.transformer_model", "static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder.transformer_model", "torch.norm().view", "torch.norm().view", "torch.norm().view", "torch.norm().view", "static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder._number_of_token_type_embeddings", "ValueError", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.norm().view", "torch.norm().view", "torch.norm().view", "torch.norm().view", "torch.load", "torch.load", "torch.load", "torch.load", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder._fold_long_sequences", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder._unfold_long_sequences", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder._number_of_token_type_embeddings"], ["", "", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "token_ids", ":", "torch", ".", "LongTensor", ",", "\n", "mask", ":", "torch", ".", "BoolTensor", ",", "\n", "type_ids", ":", "Optional", "[", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "segment_concat_mask", ":", "Optional", "[", "torch", ".", "BoolTensor", "]", "=", "None", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "# type: ignore", "\n", "        ", "\"\"\"\n        # Parameters\n\n        token_ids: `torch.LongTensor`\n            Shape: `[batch_size, num_wordpieces if max_length is None else num_segment_concat_wordpieces]`.\n            num_segment_concat_wordpieces is num_wordpieces plus special tokens inserted in the\n            middle, e.g. the length of: \"[CLS] A B C [SEP] [CLS] D E F [SEP]\" (see indexer logic).\n        mask: `torch.BoolTensor`\n            Shape: [batch_size, num_wordpieces].\n        type_ids: `Optional[torch.LongTensor]`\n            Shape: `[batch_size, num_wordpieces if max_length is None else num_segment_concat_wordpieces]`.\n        segment_concat_mask: `Optional[torch.BoolTensor]`\n            Shape: `[batch_size, num_segment_concat_wordpieces]`.\n\n        # Returns\n\n        `torch.Tensor`\n            Shape: `[batch_size, num_wordpieces, embedding_size]`.\n\n        \"\"\"", "\n", "if", "not", "self", ".", "_train_parameters", ":", "\n", "            ", "self", ".", "transformer_model", ".", "eval", "(", ")", "\n", "\n", "", "with", "torch", ".", "set_grad_enabled", "(", "self", ".", "_train_parameters", ")", ":", "\n", "# Some of the huggingface transformers don't support type ids at all and crash when you supply", "\n", "# them. For others, you can supply a tensor of zeros, and if you don't, they act as if you did.", "\n", "# There is no practical difference to the caller, so here we pretend that one case is the same", "\n", "# as another case.", "\n", "            ", "if", "type_ids", "is", "not", "None", ":", "\n", "                ", "max_type_id", "=", "type_ids", ".", "max", "(", ")", "\n", "if", "max_type_id", "==", "0", ":", "\n", "                    ", "type_ids", "=", "None", "\n", "", "else", ":", "\n", "                    ", "if", "max_type_id", ">=", "self", ".", "_number_of_token_type_embeddings", "(", ")", ":", "\n", "                        ", "raise", "ValueError", "(", "\n", "\"Found type ids too large for the chosen transformer model.\"", "\n", ")", "\n", "", "assert", "token_ids", ".", "shape", "==", "type_ids", ".", "shape", "\n", "\n", "", "", "fold_long_sequences", "=", "(", "\n", "self", ".", "_max_length", "is", "not", "None", "and", "token_ids", ".", "size", "(", "1", ")", ">", "self", ".", "_max_length", "\n", ")", "\n", "if", "fold_long_sequences", ":", "\n", "                ", "batch_size", ",", "num_segment_concat_wordpieces", "=", "token_ids", ".", "size", "(", ")", "\n", "token_ids", ",", "segment_concat_mask", ",", "type_ids", "=", "self", ".", "_fold_long_sequences", "(", "\n", "token_ids", ",", "segment_concat_mask", ",", "type_ids", "\n", ")", "\n", "\n", "", "transformer_mask", "=", "segment_concat_mask", "if", "self", ".", "_max_length", "is", "not", "None", "else", "mask", "\n", "# Shape: [batch_size, num_wordpieces, embedding_size],", "\n", "# or if self._max_length is not None:", "\n", "# [batch_size * num_segments, self._max_length, embedding_size]", "\n", "\n", "# We call this with kwargs because some of the huggingface models don't have the", "\n", "# token_type_ids parameter and fail even when it's given as None.", "\n", "# Also, as of transformers v2.5.1, they are taking FloatTensor masks.", "\n", "parameters", "=", "{", "\"input_ids\"", ":", "token_ids", ",", "\"attention_mask\"", ":", "transformer_mask", ".", "float", "(", ")", "}", "\n", "if", "type_ids", "is", "not", "None", ":", "\n", "                ", "parameters", "[", "\"token_type_ids\"", "]", "=", "type_ids", "\n", "\n", "", "if", "self", ".", "if_top_layers", ":", "\n", "                ", "embeddings", "=", "self", ".", "transformer_model", "(", "**", "parameters", ")", "[", "2", "]", "\n", "embeddings", "=", "torch", ".", "stack", "(", "embeddings", ",", "dim", "=", "0", ")", ".", "permute", "(", "1", ",", "2", ",", "0", ",", "3", ")", "# [batch_size, tokens, layers, token_embeddings]", "\n", "embeddings", "=", "torch", ".", "sum", "(", "embeddings", "[", ":", ",", ":", ",", "-", "4", ":", ",", ":", "]", ",", "dim", "=", "2", ")", "\n", "", "else", ":", "\n", "                ", "embeddings", "=", "self", ".", "transformer_model", "(", "**", "parameters", ")", "[", "0", "]", "\n", "\n", "", "if", "self", ".", "if_normalize", ":", "\n", "                ", "embeddings", "=", "embeddings", "/", "torch", ".", "norm", "(", "embeddings", ",", "dim", "=", "2", ")", ".", "view", "(", "-", "1", ",", "embeddings", ".", "shape", "[", "1", "]", ",", "1", ")", "\n", "\n", "", "if", "self", ".", "iter_norm", ":", "\n", "                ", "for", "i", "in", "range", "(", "self", ".", "iter_norm", ")", ":", "\n", "                    ", "if", "self", ".", "is_train", ":", "\n", "                        ", "embeddings", "=", "embeddings", "-", "self", ".", "mean_emb_train", "[", "i", "]", "\n", "", "else", ":", "\n", "                        ", "embeddings", "=", "embeddings", "-", "self", ".", "mean_emb_eval", "[", "i", "]", "\n", "", "embeddings", "=", "embeddings", "/", "torch", ".", "norm", "(", "embeddings", ",", "dim", "=", "2", ")", ".", "view", "(", "-", "1", ",", "embeddings", ".", "shape", "[", "1", "]", ",", "1", ")", "\n", "\n", "", "", "if", "self", ".", "map_path", ":", "\n", "# 1 load model", "\n", "                ", "map_matrix", "=", "torch", ".", "load", "(", "self", ".", "map_path", ")", ".", "cuda", "(", ")", "\n", "# map_matrix = torch.eye(768).cuda()", "\n", "# 2 multiply matrix", "\n", "embeddings", "=", "embeddings", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "# transpose the matrix in every batch", "\n", "embeddings", "=", "map_matrix", "@", "embeddings", "\n", "embeddings", "=", "embeddings", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "# transpose back", "\n", "\n", "", "if", "fold_long_sequences", ":", "\n", "                ", "embeddings", "=", "self", ".", "_unfold_long_sequences", "(", "\n", "embeddings", ",", "segment_concat_mask", ",", "batch_size", ",", "num_segment_concat_wordpieces", "\n", ")", "\n", "\n", "", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder._fold_long_sequences": [[278, 324], ["token_ids.size", "math.ceil", "torch.pad", "torch.pad", "torch.pad.reshape", "static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder._fold_long_sequences.fold"], "methods", ["None"], ["", "", "def", "_fold_long_sequences", "(", "\n", "self", ",", "\n", "token_ids", ":", "torch", ".", "LongTensor", ",", "\n", "mask", ":", "torch", ".", "BoolTensor", ",", "\n", "type_ids", ":", "Optional", "[", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "LongTensor", ",", "torch", ".", "LongTensor", ",", "Optional", "[", "torch", ".", "LongTensor", "]", "]", ":", "\n", "        ", "\"\"\"\n        We fold 1D sequences (for each element in batch), returned by `PretrainedTransformerIndexer`\n        that are in reality multiple segments concatenated together, to 2D tensors, e.g.\n\n        [ [CLS] A B C [SEP] [CLS] D E [SEP] ]\n        -> [ [ [CLS] A B C [SEP] ], [ [CLS] D E [SEP] [PAD] ] ]\n        The [PAD] positions can be found in the returned `mask`.\n\n        # Parameters\n\n        token_ids: `torch.LongTensor`\n            Shape: `[batch_size, num_segment_concat_wordpieces]`.\n            num_segment_concat_wordpieces is num_wordpieces plus special tokens inserted in the\n            middle, i.e. the length of: \"[CLS] A B C [SEP] [CLS] D E F [SEP]\" (see indexer logic).\n        mask: `torch.BoolTensor`\n            Shape: `[batch_size, num_segment_concat_wordpieces]`.\n            The mask for the concatenated segments of wordpieces. The same as `segment_concat_mask`\n            in `forward()`.\n        type_ids: `Optional[torch.LongTensor]`\n            Shape: [batch_size, num_segment_concat_wordpieces].\n\n        # Returns:\n\n        token_ids: `torch.LongTensor`\n            Shape: [batch_size * num_segments, self._max_length].\n        mask: `torch.BoolTensor`\n            Shape: [batch_size * num_segments, self._max_length].\n        \"\"\"", "\n", "num_segment_concat_wordpieces", "=", "token_ids", ".", "size", "(", "1", ")", "\n", "num_segments", "=", "math", ".", "ceil", "(", "num_segment_concat_wordpieces", "/", "self", ".", "_max_length", ")", "\n", "padded_length", "=", "num_segments", "*", "self", ".", "_max_length", "\n", "length_to_pad", "=", "padded_length", "-", "num_segment_concat_wordpieces", "\n", "\n", "def", "fold", "(", "tensor", ")", ":", "# Shape: [batch_size, num_segment_concat_wordpieces]", "\n", "# Shape: [batch_size, num_segments * self._max_length]", "\n", "            ", "tensor", "=", "F", ".", "pad", "(", "tensor", ",", "[", "0", ",", "length_to_pad", "]", ",", "value", "=", "0", ")", "\n", "# Shape: [batch_size * num_segments, self._max_length]", "\n", "return", "tensor", ".", "reshape", "(", "-", "1", ",", "self", ".", "_max_length", ")", "\n", "\n", "", "return", "fold", "(", "token_ids", ")", ",", "fold", "(", "mask", ")", ",", "fold", "(", "type_ids", ")", "if", "type_ids", "is", "not", "None", "else", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder._unfold_long_sequences": [[325, 425], ["int", "torch.cat.size", "torch.cat.size", "torch.cat.reshape", "torch.cat.reshape", "mask.reshape.reshape.reshape", "mask.reshape.reshape.sum", "allennlp.nn.util.batched_index_select", "torch.cat.reshape", "torch.cat.reshape", "torch.cat.reshape", "torch.cat.reshape", "num_removed_non_end_tokens.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.scatter_", "torch.cat.scatter_", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "ValueError", "end_token_indices.unsqueeze().expand_as", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "lengths.unsqueeze", "torch.cat.size", "torch.cat.size", "mask.reshape.sum.unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "lengths.size", "end_token_indices.unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder._unfold_long_sequences.lengths_to_mask"], "methods", ["None"], ["", "def", "_unfold_long_sequences", "(", "\n", "self", ",", "\n", "embeddings", ":", "torch", ".", "FloatTensor", ",", "\n", "mask", ":", "torch", ".", "BoolTensor", ",", "\n", "batch_size", ":", "int", ",", "\n", "num_segment_concat_wordpieces", ":", "int", ",", "\n", ")", "->", "torch", ".", "FloatTensor", ":", "\n", "        ", "\"\"\"\n        We take 2D segments of a long sequence and flatten them out to get the whole sequence\n        representation while remove unnecessary special tokens.\n\n        [ [ [CLS]_emb A_emb B_emb C_emb [SEP]_emb ], [ [CLS]_emb D_emb E_emb [SEP]_emb [PAD]_emb ] ]\n        -> [ [CLS]_emb A_emb B_emb C_emb D_emb E_emb [SEP]_emb ]\n\n        We truncate the start and end tokens for all segments, recombine the segments,\n        and manually add back the start and end tokens.\n\n        # Parameters\n\n        embeddings: `torch.FloatTensor`\n            Shape: [batch_size * num_segments, self._max_length, embedding_size].\n        mask: `torch.BoolTensor`\n            Shape: [batch_size * num_segments, self._max_length].\n            The mask for the concatenated segments of wordpieces. The same as `segment_concat_mask`\n            in `forward()`.\n        batch_size: `int`\n        num_segment_concat_wordpieces: `int`\n            The length of the original \"[ [CLS] A B C [SEP] [CLS] D E F [SEP] ]\", i.e.\n            the original `token_ids.size(1)`.\n\n        # Returns:\n\n        embeddings: `torch.FloatTensor`\n            Shape: [batch_size, self._num_wordpieces, embedding_size].\n        \"\"\"", "\n", "\n", "def", "lengths_to_mask", "(", "lengths", ",", "max_len", ",", "device", ")", ":", "\n", "            ", "return", "torch", ".", "arange", "(", "max_len", ",", "device", "=", "device", ")", ".", "expand", "(", "\n", "lengths", ".", "size", "(", "0", ")", ",", "max_len", "\n", ")", "<", "lengths", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "", "device", "=", "embeddings", ".", "device", "\n", "num_segments", "=", "int", "(", "embeddings", ".", "size", "(", "0", ")", "/", "batch_size", ")", "\n", "embedding_size", "=", "embeddings", ".", "size", "(", "2", ")", "\n", "\n", "# We want to remove all segment-level special tokens but maintain sequence-level ones", "\n", "num_wordpieces", "=", "num_segment_concat_wordpieces", "-", "(", "num_segments", "-", "1", ")", "*", "self", ".", "_num_added_tokens", "\n", "\n", "embeddings", "=", "embeddings", ".", "reshape", "(", "batch_size", ",", "num_segments", "*", "self", ".", "_max_length", ",", "embedding_size", ")", "\n", "mask", "=", "mask", ".", "reshape", "(", "batch_size", ",", "num_segments", "*", "self", ".", "_max_length", ")", "\n", "# We assume that all 1s in the mask precede all 0s, and add an assert for that.", "\n", "# Open an issue on GitHub if this breaks for you.", "\n", "# Shape: (batch_size,)", "\n", "seq_lengths", "=", "mask", ".", "sum", "(", "-", "1", ")", "\n", "if", "not", "(", "lengths_to_mask", "(", "seq_lengths", ",", "mask", ".", "size", "(", "1", ")", ",", "device", ")", "==", "mask", ")", ".", "all", "(", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Long sequence splitting only supports masks with all 1s preceding all 0s.\"", "\n", ")", "\n", "# Shape: (batch_size, self._num_added_end_tokens); this is a broadcast op", "\n", "", "end_token_indices", "=", "(", "\n", "seq_lengths", ".", "unsqueeze", "(", "-", "1", ")", "-", "torch", ".", "arange", "(", "self", ".", "_num_added_end_tokens", ",", "device", "=", "device", ")", "-", "1", "\n", ")", "\n", "\n", "# Shape: (batch_size, self._num_added_start_tokens, embedding_size)", "\n", "start_token_embeddings", "=", "embeddings", "[", ":", ",", ":", "self", ".", "_num_added_start_tokens", ",", ":", "]", "\n", "# Shape: (batch_size, self._num_added_end_tokens, embedding_size)", "\n", "end_token_embeddings", "=", "batched_index_select", "(", "embeddings", ",", "end_token_indices", ")", "\n", "\n", "embeddings", "=", "embeddings", ".", "reshape", "(", "batch_size", ",", "num_segments", ",", "self", ".", "_max_length", ",", "embedding_size", ")", "\n", "embeddings", "=", "embeddings", "[", "\n", ":", ",", ":", ",", "self", ".", "_num_added_start_tokens", ":", "-", "self", ".", "_num_added_end_tokens", ",", ":", "\n", "]", "# truncate segment-level start/end tokens", "\n", "embeddings", "=", "embeddings", ".", "reshape", "(", "batch_size", ",", "-", "1", ",", "embedding_size", ")", "# flatten", "\n", "\n", "# Now try to put end token embeddings back which is a little tricky.", "\n", "\n", "# The number of segment each sequence spans, excluding padding. Mimicking ceiling operation.", "\n", "# Shape: (batch_size,)", "\n", "num_effective_segments", "=", "(", "seq_lengths", "+", "self", ".", "_max_length", "-", "1", ")", "/", "self", ".", "_max_length", "\n", "# The number of indices that end tokens should shift back.", "\n", "num_removed_non_end_tokens", "=", "(", "\n", "num_effective_segments", "*", "self", ".", "_num_added_tokens", "-", "self", ".", "_num_added_end_tokens", "\n", ")", "\n", "# Shape: (batch_size, self._num_added_end_tokens)", "\n", "end_token_indices", "-=", "num_removed_non_end_tokens", ".", "unsqueeze", "(", "-", "1", ")", "\n", "assert", "(", "end_token_indices", ">=", "self", ".", "_num_added_start_tokens", ")", ".", "all", "(", ")", "\n", "# Add space for end embeddings", "\n", "embeddings", "=", "torch", ".", "cat", "(", "[", "embeddings", ",", "torch", ".", "zeros_like", "(", "end_token_embeddings", ")", "]", ",", "1", ")", "\n", "# Add end token embeddings back", "\n", "embeddings", ".", "scatter_", "(", "\n", "1", ",", "end_token_indices", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand_as", "(", "end_token_embeddings", ")", ",", "end_token_embeddings", "\n", ")", "\n", "\n", "# Now put back start tokens. We can do this before putting back end tokens, but then", "\n", "# we need to change `num_removed_non_end_tokens` a little.", "\n", "embeddings", "=", "torch", ".", "cat", "(", "[", "start_token_embeddings", ",", "embeddings", "]", ",", "1", ")", "\n", "\n", "# Truncate to original length", "\n", "embeddings", "=", "embeddings", "[", ":", ",", ":", "num_wordpieces", ",", ":", "]", "\n", "return", "embeddings", "\n", "", "", ""]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_mismatched_embedder.StaticPretrainedTransformerMismatchedEmbedder.__init__": [[33, 52], ["allennlp.modules.token_embedders.TokenEmbedder.__init__", "src.modules.token_embedders.StaticPretrainedTransformerEmbedder"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.evaluator.evaluator.__init__"], ["def", "__init__", "(", "\n", "self", ",", "model_name", ":", "str", ",", "\n", "max_length", ":", "int", "=", "None", ",", "\n", "train_parameters", ":", "bool", "=", "False", ",", "\n", "if_top_layers", ":", "bool", "=", "False", ",", "\n", "if_normalize", ":", "bool", "=", "False", ",", "\n", "map_path", ":", "str", "=", "None", ",", "\n", "iter_norm", ":", "int", "=", "None", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# The matched version v.s. mismatched", "\n", "self", ".", "iter_norm", "=", "iter_norm", "\n", "self", ".", "_matched_embedder", "=", "StaticPretrainedTransformerEmbedder", "(", "\n", "model_name", ",", "max_length", ",", "\n", "train_parameters", "=", "train_parameters", ",", "\n", "if_top_layers", "=", "if_top_layers", ",", "\n", "if_normalize", "=", "if_normalize", ",", "\n", "map_path", "=", "map_path", ",", "\n", "iter_norm", "=", "iter_norm", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_mismatched_embedder.StaticPretrainedTransformerMismatchedEmbedder.get_output_dim": [[54, 57], ["static_pretrained_transformer_mismatched_embedder.StaticPretrainedTransformerMismatchedEmbedder._matched_embedder.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_mismatched_embedder.StaticPretrainedTransformerMismatchedEmbedder.get_output_dim"], ["", "@", "overrides", "\n", "def", "get_output_dim", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_matched_embedder", ".", "get_output_dim", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_mismatched_embedder.StaticPretrainedTransformerMismatchedEmbedder.forward": [[58, 117], ["static_pretrained_transformer_mismatched_embedder.StaticPretrainedTransformerMismatchedEmbedder._matched_embedder", "allennlp.nn.util.batched_span_select", "span_mask.unsqueeze.unsqueeze.unsqueeze", "span_embeddings.sum", "span_mask.unsqueeze.unsqueeze.sum", "static_pretrained_transformer_mismatched_embedder.StaticPretrainedTransformerMismatchedEmbedder._matched_embedder.get_embeddings", "static_pretrained_transformer_mismatched_embedder.StaticPretrainedTransformerMismatchedEmbedder.contiguous"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_embedder.StaticPretrainedTransformerEmbedder.get_embeddings"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "token_ids", ":", "torch", ".", "LongTensor", ",", "\n", "mask", ":", "torch", ".", "BoolTensor", ",", "\n", "offsets", ":", "torch", ".", "LongTensor", ",", "\n", "wordpiece_mask", ":", "torch", ".", "BoolTensor", ",", "\n", "type_ids", ":", "Optional", "[", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "segment_concat_mask", ":", "Optional", "[", "torch", ".", "BoolTensor", "]", "=", "None", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "# type: ignore", "\n", "        ", "\"\"\"\n        # Parameters\n\n        token_ids: `torch.LongTensor`\n            Shape: [batch_size, num_wordpieces] (for exception see `PretrainedTransformerEmbedder`).\n        mask: `torch.BoolTensor`\n            Shape: [batch_size, num_orig_tokens].\n        offsets: `torch.LongTensor`\n            Shape: [batch_size, num_orig_tokens, 2].\n            Maps indices for the original tokens, i.e. those given as input to the indexer,\n            to a span in token_ids. `token_ids[i][offsets[i][j][0]:offsets[i][j][1] + 1]`\n            corresponds to the original j-th token from the i-th batch.\n        wordpiece_mask: `torch.BoolTensor`\n            Shape: [batch_size, num_wordpieces].\n        type_ids: `Optional[torch.LongTensor]`\n            Shape: [batch_size, num_wordpieces].\n        segment_concat_mask: `Optional[torch.BoolTensor]`\n            See `PretrainedTransformerEmbedder`.\n\n        # Returns\n\n        `torch.Tensor`\n            Shape: [batch_size, num_orig_tokens, embedding_size].\n        \"\"\"", "\n", "# Shape: [batch_size, num_wordpieces, embedding_size].", "\n", "if", "self", ".", "iter_norm", ":", "\n", "            ", "return", "self", ".", "_matched_embedder", ".", "get_embeddings", "(", "\n", "token_ids", ",", "wordpiece_mask", ",", "type_ids", "=", "type_ids", ",", "segment_concat_mask", "=", "segment_concat_mask", "\n", ")", "\n", "\n", "", "embeddings", "=", "self", ".", "_matched_embedder", "(", "\n", "token_ids", ",", "wordpiece_mask", ",", "type_ids", "=", "type_ids", ",", "segment_concat_mask", "=", "segment_concat_mask", "\n", ")", "\n", "\n", "# span_embeddings: (batch_size, num_orig_tokens, max_span_length, embedding_size)", "\n", "# span_mask: (batch_size, num_orig_tokens, max_span_length)", "\n", "span_embeddings", ",", "span_mask", "=", "util", ".", "batched_span_select", "(", "embeddings", ".", "contiguous", "(", ")", ",", "offsets", ")", "\n", "span_mask", "=", "span_mask", ".", "unsqueeze", "(", "-", "1", ")", "\n", "span_embeddings", "*=", "span_mask", "# zero out paddings", "\n", "\n", "span_embeddings_sum", "=", "span_embeddings", ".", "sum", "(", "2", ")", "\n", "span_embeddings_len", "=", "span_mask", ".", "sum", "(", "2", ")", "\n", "# Shape: (batch_size, num_orig_tokens, embedding_size)", "\n", "orig_embeddings", "=", "span_embeddings_sum", "/", "span_embeddings_len", "\n", "\n", "# All the places where the span length is zero, write in zeros.", "\n", "orig_embeddings", "[", "(", "span_embeddings_len", "==", "0", ")", ".", "expand", "(", "orig_embeddings", ".", "shape", ")", "]", "=", "0", "\n", "\n", "return", "orig_embeddings", "\n", "", "", ""]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.bert_biaffine_dependency_parser.BertBiaffineDependencyParser.__init__": [[95, 183], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "encoder.get_output_dim", "copy.deepcopy", "allennlp.modules.matrix_attention.bilinear_matrix_attention.BilinearMatrixAttention", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser.vocab.get_vocab_size", "copy.deepcopy", "torch.nn.modules.Bilinear", "torch.nn.modules.Bilinear", "torch.nn.modules.Bilinear", "torch.nn.modules.Bilinear", "allennlp.modules.InputVariationalDropout", "torch.nn.modules.Dropout", "torch.nn.modules.Dropout", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "text_field_embedder.get_output_dim", "allennlp.common.checks.check_dimensions_match", "allennlp.common.checks.check_dimensions_match", "allennlp.common.checks.check_dimensions_match", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser.vocab.get_token_to_index_vocabulary", "set", "logger.info", "src.training.AttachmentScoresCuda", "initializer", "allennlp.modules.FeedForward", "allennlp.modules.FeedForward", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "pos_tag_embedding.get_output_dim", "encoder.get_input_dim", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser.head_tag_feedforward.get_output_dim", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser.head_arc_feedforward.get_output_dim", "punctuation_tag_indices.values", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser.items", "allennlp.nn.Activation.by_name", "allennlp.nn.Activation.by_name", "encoder.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.evaluator.evaluator.__init__", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_mismatched_embedder.StaticPretrainedTransformerMismatchedEmbedder.get_output_dim", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_mismatched_embedder.StaticPretrainedTransformerMismatchedEmbedder.get_output_dim", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_mismatched_embedder.StaticPretrainedTransformerMismatchedEmbedder.get_output_dim", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_mismatched_embedder.StaticPretrainedTransformerMismatchedEmbedder.get_output_dim", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_mismatched_embedder.StaticPretrainedTransformerMismatchedEmbedder.get_output_dim", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_mismatched_embedder.StaticPretrainedTransformerMismatchedEmbedder.get_output_dim"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "text_field_embedder", ":", "TextFieldEmbedder", ",", "\n", "encoder", ":", "Seq2SeqEncoder", ",", "\n", "tag_representation_dim", ":", "int", ",", "\n", "arc_representation_dim", ":", "int", ",", "\n", "tag_feedforward", ":", "FeedForward", "=", "None", ",", "\n", "arc_feedforward", ":", "FeedForward", "=", "None", ",", "\n", "pos_tag_embedding", ":", "Embedding", "=", "None", ",", "\n", "use_mst_decoding_for_validation", ":", "bool", "=", "True", ",", "\n", "dropout", ":", "float", "=", "0.0", ",", "\n", "input_dropout", ":", "float", "=", "0.0", ",", "\n", "new_pretrained_embedder", ":", "str", "=", "None", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "vocab", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "text_field_embedder", "=", "text_field_embedder", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "new_pretrained_embedder", "=", "new_pretrained_embedder", "\n", "\n", "encoder_dim", "=", "encoder", ".", "get_output_dim", "(", ")", "\n", "\n", "self", ".", "head_arc_feedforward", "=", "arc_feedforward", "or", "FeedForward", "(", "\n", "encoder_dim", ",", "1", ",", "arc_representation_dim", ",", "Activation", ".", "by_name", "(", "\"elu\"", ")", "(", ")", "\n", ")", "\n", "self", ".", "child_arc_feedforward", "=", "copy", ".", "deepcopy", "(", "self", ".", "head_arc_feedforward", ")", "\n", "\n", "self", ".", "arc_attention", "=", "BilinearMatrixAttention", "(", "\n", "arc_representation_dim", ",", "arc_representation_dim", ",", "use_input_biases", "=", "True", "\n", ")", "\n", "\n", "num_labels", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "\"head_tags\"", ")", "\n", "\n", "self", ".", "head_tag_feedforward", "=", "tag_feedforward", "or", "FeedForward", "(", "\n", "encoder_dim", ",", "1", ",", "tag_representation_dim", ",", "Activation", ".", "by_name", "(", "\"elu\"", ")", "(", ")", "\n", ")", "\n", "self", ".", "child_tag_feedforward", "=", "copy", ".", "deepcopy", "(", "self", ".", "head_tag_feedforward", ")", "\n", "\n", "self", ".", "tag_bilinear", "=", "torch", ".", "nn", ".", "modules", ".", "Bilinear", "(", "\n", "tag_representation_dim", ",", "tag_representation_dim", ",", "num_labels", "\n", ")", "\n", "\n", "self", ".", "_pos_tag_embedding", "=", "pos_tag_embedding", "or", "None", "\n", "self", ".", "_dropout", "=", "InputVariationalDropout", "(", "dropout", ")", "\n", "self", ".", "_input_dropout", "=", "Dropout", "(", "input_dropout", ")", "\n", "self", ".", "_head_sentinel", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "[", "1", ",", "1", ",", "encoder", ".", "get_output_dim", "(", ")", "]", ")", ")", "\n", "\n", "representation_dim", "=", "text_field_embedder", ".", "get_output_dim", "(", ")", "\n", "if", "pos_tag_embedding", "is", "not", "None", ":", "\n", "            ", "representation_dim", "+=", "pos_tag_embedding", ".", "get_output_dim", "(", ")", "\n", "\n", "", "check_dimensions_match", "(", "\n", "representation_dim", ",", "\n", "encoder", ".", "get_input_dim", "(", ")", ",", "\n", "\"text field embedding dim\"", ",", "\n", "\"encoder input dim\"", ",", "\n", ")", "\n", "\n", "check_dimensions_match", "(", "\n", "tag_representation_dim", ",", "\n", "self", ".", "head_tag_feedforward", ".", "get_output_dim", "(", ")", ",", "\n", "\"tag representation dim\"", ",", "\n", "\"tag feedforward output dim\"", ",", "\n", ")", "\n", "check_dimensions_match", "(", "\n", "arc_representation_dim", ",", "\n", "self", ".", "head_arc_feedforward", ".", "get_output_dim", "(", ")", ",", "\n", "\"arc representation dim\"", ",", "\n", "\"arc feedforward output dim\"", ",", "\n", ")", "\n", "\n", "self", ".", "use_mst_decoding_for_validation", "=", "use_mst_decoding_for_validation", "\n", "\n", "tags", "=", "self", ".", "vocab", ".", "get_token_to_index_vocabulary", "(", "\"pos\"", ")", "\n", "punctuation_tag_indices", "=", "{", "\n", "tag", ":", "index", "for", "tag", ",", "index", "in", "tags", ".", "items", "(", ")", "if", "tag", "in", "POS_TO_IGNORE", "\n", "}", "\n", "self", ".", "_pos_to_ignore", "=", "set", "(", "punctuation_tag_indices", ".", "values", "(", ")", ")", "\n", "logger", ".", "info", "(", "\n", "f\"Found POS tags corresponding to the following punctuation : {punctuation_tag_indices}. \"", "\n", "\"Ignoring words with these POS tags for evaluation.\"", "\n", ")", "\n", "\n", "self", ".", "_attachment_scores", "=", "AttachmentScoresCuda", "(", ")", "\n", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.bert_biaffine_dependency_parser.BertBiaffineDependencyParser.forward": [[184, 285], ["bert_biaffine_dependency_parser.BertBiaffineDependencyParser.text_field_embedder().detach", "allennlp.nn.util.get_text_field_mask", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser._parse", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser._pos_tag_embedding", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser._get_mask_for_eval", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser._attachment_scores", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser.text_field_embedder", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._parse", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._get_mask_for_eval"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "# type: ignore", "\n", "words", ":", "TextFieldTensors", ",", "\n", "pos_tags", ":", "torch", ".", "LongTensor", ",", "\n", "metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", ",", "\n", "head_tags", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "head_indices", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n\n        words : TextFieldTensors, required\n            The output of `TextField.as_array()`, which should typically be passed directly to a\n            `TextFieldEmbedder`. This output is a dictionary mapping keys to `TokenIndexer`\n            tensors.  At its most basic, using a `SingleIdTokenIndexer` this is : `{\"tokens\":\n            Tensor(batch_size, sequence_length)}`. This dictionary will have the same keys as were used\n            for the `TokenIndexers` when you created the `TextField` representing your\n            sequence.  The dictionary is designed to be passed directly to a `TextFieldEmbedder`,\n            which knows how to combine different word representations into a single vector per\n            token in your input.\n        pos_tags : `torch.LongTensor`, required\n            The output of a `SequenceLabelField` containing POS tags.\n            POS tags are required regardless of whether they are used in the model,\n            because they are used to filter the evaluation metric to only consider\n            heads of words which are not punctuation.\n        metadata : List[Dict[str, Any]], optional (default=None)\n            A dictionary of metadata for each batch element which has keys:\n                words : `List[str]`, required.\n                    The tokens in the original sentence.\n                pos : `List[str]`, required.\n                    The dependencies POS tags for each word.\n        head_tags : torch.LongTensor, optional (default = None)\n            A torch tensor representing the sequence of integer gold class labels for the arcs\n            in the dependency parse. Has shape `(batch_size, sequence_length)`.\n        head_indices : torch.LongTensor, optional (default = None)\n            A torch tensor representing the sequence of integer indices denoting the parent of every\n            word in the dependency parse. Has shape `(batch_size, sequence_length)`.\n\n        # Returns\n\n        An output dictionary consisting of:\n        loss : `torch.FloatTensor`, optional\n            A scalar loss to be optimised.\n        arc_loss : `torch.FloatTensor`\n            The loss contribution from the unlabeled arcs.\n        loss : `torch.FloatTensor`, optional\n            The loss contribution from predicting the dependency\n            tags for the gold arcs.\n        heads : `torch.FloatTensor`\n            The predicted head indices for each word. A tensor\n            of shape (batch_size, sequence_length).\n        head_types : `torch.FloatTensor`\n            The predicted head types for each arc. A tensor\n            of shape (batch_size, sequence_length).\n        mask : `torch.LongTensor`\n            A mask denoting the padded elements in the batch.\n        \"\"\"", "\n", "# print(self.text_field_embedder._token_embedders, self.text_field_embedder._token_embedders.keys())", "\n", "\n", "embedded_text_input", "=", "self", ".", "text_field_embedder", "(", "words", ")", ".", "detach", "(", ")", "\n", "if", "pos_tags", "is", "not", "None", "and", "self", ".", "_pos_tag_embedding", "is", "not", "None", ":", "\n", "            ", "embedded_pos_tags", "=", "self", ".", "_pos_tag_embedding", "(", "pos_tags", ")", "\n", "embedded_text_input", "=", "torch", ".", "cat", "(", "[", "embedded_text_input", ",", "embedded_pos_tags", "]", ",", "-", "1", ")", "\n", "", "elif", "self", ".", "_pos_tag_embedding", "is", "not", "None", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"Model uses a POS embedding, but no POS tags were passed.\"", ")", "\n", "\n", "", "mask", "=", "get_text_field_mask", "(", "words", ")", "\n", "\n", "predicted_heads", ",", "predicted_head_tags", ",", "mask", ",", "arc_nll", ",", "tag_nll", "=", "self", ".", "_parse", "(", "\n", "embedded_text_input", ",", "mask", ",", "head_tags", ",", "head_indices", "\n", ")", "\n", "\n", "loss", "=", "arc_nll", "+", "tag_nll", "\n", "\n", "if", "head_indices", "is", "not", "None", "and", "head_tags", "is", "not", "None", ":", "\n", "            ", "evaluation_mask", "=", "self", ".", "_get_mask_for_eval", "(", "mask", "[", ":", ",", "1", ":", "]", ",", "pos_tags", ")", "\n", "# We calculate attatchment scores for the whole sentence", "\n", "# but excluding the symbolic ROOT token at the start,", "\n", "# which is why we start from the second element in the sequence.", "\n", "self", ".", "_attachment_scores", "(", "\n", "predicted_heads", "[", ":", ",", "1", ":", "]", ",", "\n", "predicted_head_tags", "[", ":", ",", "1", ":", "]", ",", "\n", "head_indices", ",", "\n", "head_tags", ",", "\n", "evaluation_mask", ",", "\n", ")", "\n", "\n", "", "output_dict", "=", "{", "\n", "\"heads\"", ":", "predicted_heads", ",", "\n", "\"head_tags\"", ":", "predicted_head_tags", ",", "\n", "\"arc_loss\"", ":", "arc_nll", ",", "\n", "\"tag_loss\"", ":", "tag_nll", ",", "\n", "\"loss\"", ":", "loss", ",", "\n", "\"mask\"", ":", "mask", ",", "\n", "\"words\"", ":", "[", "meta", "[", "\"words\"", "]", "for", "meta", "in", "metadata", "]", ",", "\n", "\"pos\"", ":", "[", "meta", "[", "\"pos\"", "]", "for", "meta", "in", "metadata", "]", ",", "\n", "}", "\n", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.bert_biaffine_dependency_parser.BertBiaffineDependencyParser.make_output_human_readable": [[286, 309], ["output_dict.pop().cpu().detach().numpy", "output_dict.pop().cpu().detach().numpy", "output_dict.pop", "allennlp.nn.util.get_lengths_from_binary_sequence_mask", "zip", "list", "head_tag_labels.append", "head_indices.append", "output_dict.pop().cpu().detach", "output_dict.pop().cpu().detach", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser.vocab.get_token_from_index", "output_dict.pop().cpu", "output_dict.pop().cpu", "output_dict.pop", "output_dict.pop"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "make_output_human_readable", "(", "\n", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "head_tags", "=", "output_dict", ".", "pop", "(", "\"head_tags\"", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "heads", "=", "output_dict", ".", "pop", "(", "\"heads\"", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "mask", "=", "output_dict", ".", "pop", "(", "\"mask\"", ")", "\n", "lengths", "=", "get_lengths_from_binary_sequence_mask", "(", "mask", ")", "\n", "head_tag_labels", "=", "[", "]", "\n", "head_indices", "=", "[", "]", "\n", "for", "instance_heads", ",", "instance_tags", ",", "length", "in", "zip", "(", "heads", ",", "head_tags", ",", "lengths", ")", ":", "\n", "            ", "instance_heads", "=", "list", "(", "instance_heads", "[", "1", ":", "length", "]", ")", "\n", "instance_tags", "=", "instance_tags", "[", "1", ":", "length", "]", "\n", "labels", "=", "[", "\n", "self", ".", "vocab", ".", "get_token_from_index", "(", "label", ",", "\"head_tags\"", ")", "for", "label", "in", "instance_tags", "\n", "]", "\n", "head_tag_labels", ".", "append", "(", "labels", ")", "\n", "head_indices", ".", "append", "(", "instance_heads", ")", "\n", "\n", "", "output_dict", "[", "\"predicted_dependencies\"", "]", "=", "head_tag_labels", "\n", "output_dict", "[", "\"predicted_heads\"", "]", "=", "head_indices", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.bert_biaffine_dependency_parser.BertBiaffineDependencyParser._parse": [[310, 377], ["bert_biaffine_dependency_parser.BertBiaffineDependencyParser._input_dropout", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser.encoder", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser.size", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser._head_sentinel.expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.float", "torch.cat.float", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser._dropout", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser._dropout", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser._dropout", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser._dropout", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser._dropout", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser.arc_attention", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser.head_arc_feedforward", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser.child_arc_feedforward", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser.head_tag_feedforward", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser.child_tag_feedforward", "minus_mask.unsqueeze", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser._greedy_decode", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser._mst_decode", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser._construct_loss", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser._construct_loss", "torch.cat.new_ones", "torch.cat.new_ones", "minus_mask.unsqueeze", "torch.cat.new_zeros", "torch.cat.new_zeros", "torch.cat.new_zeros", "torch.cat.new_zeros", "predicted_heads.long", "predicted_head_tags.long"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._greedy_decode", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._mst_decode", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._construct_loss", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._construct_loss"], ["", "def", "_parse", "(", "\n", "self", ",", "\n", "embedded_text_input", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "LongTensor", ",", "\n", "head_tags", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "head_indices", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "embedded_text_input", "=", "self", ".", "_input_dropout", "(", "embedded_text_input", ")", "\n", "encoded_text", "=", "self", ".", "encoder", "(", "embedded_text_input", ",", "mask", ")", "\n", "\n", "batch_size", ",", "_", ",", "encoding_dim", "=", "encoded_text", ".", "size", "(", ")", "\n", "\n", "head_sentinel", "=", "self", ".", "_head_sentinel", ".", "expand", "(", "batch_size", ",", "1", ",", "encoding_dim", ")", "\n", "# Concatenate the head sentinel onto the sentence representation.", "\n", "encoded_text", "=", "torch", ".", "cat", "(", "[", "head_sentinel", ",", "encoded_text", "]", ",", "1", ")", "\n", "mask", "=", "torch", ".", "cat", "(", "[", "mask", ".", "new_ones", "(", "batch_size", ",", "1", ")", ",", "mask", "]", ",", "1", ")", "\n", "if", "head_indices", "is", "not", "None", ":", "\n", "            ", "head_indices", "=", "torch", ".", "cat", "(", "[", "head_indices", ".", "new_zeros", "(", "batch_size", ",", "1", ")", ",", "head_indices", "]", ",", "1", ")", "\n", "", "if", "head_tags", "is", "not", "None", ":", "\n", "            ", "head_tags", "=", "torch", ".", "cat", "(", "[", "head_tags", ".", "new_zeros", "(", "batch_size", ",", "1", ")", ",", "head_tags", "]", ",", "1", ")", "\n", "", "float_mask", "=", "mask", ".", "float", "(", ")", "\n", "encoded_text", "=", "self", ".", "_dropout", "(", "encoded_text", ")", "\n", "\n", "# shape (batch_size, sequence_length, arc_representation_dim)", "\n", "head_arc_representation", "=", "self", ".", "_dropout", "(", "self", ".", "head_arc_feedforward", "(", "encoded_text", ")", ")", "\n", "child_arc_representation", "=", "self", ".", "_dropout", "(", "self", ".", "child_arc_feedforward", "(", "encoded_text", ")", ")", "\n", "\n", "# shape (batch_size, sequence_length, tag_representation_dim)", "\n", "head_tag_representation", "=", "self", ".", "_dropout", "(", "self", ".", "head_tag_feedforward", "(", "encoded_text", ")", ")", "\n", "child_tag_representation", "=", "self", ".", "_dropout", "(", "self", ".", "child_tag_feedforward", "(", "encoded_text", ")", ")", "\n", "# shape (batch_size, sequence_length, sequence_length)", "\n", "attended_arcs", "=", "self", ".", "arc_attention", "(", "head_arc_representation", ",", "child_arc_representation", ")", "\n", "\n", "minus_inf", "=", "-", "1e8", "\n", "minus_mask", "=", "(", "1", "-", "float_mask", ")", "*", "minus_inf", "\n", "attended_arcs", "=", "attended_arcs", "+", "minus_mask", ".", "unsqueeze", "(", "2", ")", "+", "minus_mask", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "if", "self", ".", "training", "or", "not", "self", ".", "use_mst_decoding_for_validation", ":", "\n", "            ", "predicted_heads", ",", "predicted_head_tags", "=", "self", ".", "_greedy_decode", "(", "\n", "head_tag_representation", ",", "child_tag_representation", ",", "attended_arcs", ",", "mask", "\n", ")", "\n", "", "else", ":", "\n", "            ", "predicted_heads", ",", "predicted_head_tags", "=", "self", ".", "_mst_decode", "(", "\n", "head_tag_representation", ",", "child_tag_representation", ",", "attended_arcs", ",", "mask", "\n", ")", "\n", "", "if", "head_indices", "is", "not", "None", "and", "head_tags", "is", "not", "None", ":", "\n", "\n", "            ", "arc_nll", ",", "tag_nll", "=", "self", ".", "_construct_loss", "(", "\n", "head_tag_representation", "=", "head_tag_representation", ",", "\n", "child_tag_representation", "=", "child_tag_representation", ",", "\n", "attended_arcs", "=", "attended_arcs", ",", "\n", "head_indices", "=", "head_indices", ",", "\n", "head_tags", "=", "head_tags", ",", "\n", "mask", "=", "mask", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "arc_nll", ",", "tag_nll", "=", "self", ".", "_construct_loss", "(", "\n", "head_tag_representation", "=", "head_tag_representation", ",", "\n", "child_tag_representation", "=", "child_tag_representation", ",", "\n", "attended_arcs", "=", "attended_arcs", ",", "\n", "head_indices", "=", "predicted_heads", ".", "long", "(", ")", ",", "\n", "head_tags", "=", "predicted_head_tags", ".", "long", "(", ")", ",", "\n", "mask", "=", "mask", ",", "\n", ")", "\n", "\n", "", "return", "predicted_heads", ",", "predicted_head_tags", ",", "mask", ",", "arc_nll", ",", "tag_nll", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.bert_biaffine_dependency_parser.BertBiaffineDependencyParser._construct_loss": [[378, 458], ["mask.float", "attended_arcs.size", "allennlp.nn.util.get_range_vector().unsqueeze", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser._get_head_tags", "allennlp.nn.util.get_range_vector", "allennlp.nn.util.get_range_vector.view().expand().long", "mask.float.unsqueeze", "allennlp.nn.util.masked_log_softmax", "mask.float.unsqueeze", "allennlp.nn.util.get_device_of", "mask.sum", "valid_positions.float", "valid_positions.float", "allennlp.nn.util.get_range_vector", "allennlp.nn.util.masked_log_softmax", "mask.float.unsqueeze", "mask.unsqueeze", "allennlp.nn.util.get_range_vector.view().expand", "arc_loss.sum", "tag_loss.sum", "allennlp.nn.util.get_device_of", "allennlp.nn.util.get_range_vector.view"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._get_head_tags"], ["", "def", "_construct_loss", "(", "\n", "self", ",", "\n", "head_tag_representation", ":", "torch", ".", "Tensor", ",", "\n", "child_tag_representation", ":", "torch", ".", "Tensor", ",", "\n", "attended_arcs", ":", "torch", ".", "Tensor", ",", "\n", "head_indices", ":", "torch", ".", "Tensor", ",", "\n", "head_tags", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Computes the arc and tag loss for a sequence given gold head indices and tags.\n\n        # Parameters\n\n        head_tag_representation : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length, tag_representation_dim),\n            which will be used to generate predictions for the dependency tags\n            for the given arcs.\n        child_tag_representation : `torch.Tensor`, required\n            A tensor of shape (batch_size, sequence_length, tag_representation_dim),\n            which will be used to generate predictions for the dependency tags\n            for the given arcs.\n        attended_arcs : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length, sequence_length) used to generate\n            a distribution over attachments of a given word to all other words.\n        head_indices : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length).\n            The indices of the heads for every word.\n        head_tags : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length).\n            The dependency labels of the heads for every word.\n        mask : `torch.Tensor`, required.\n            A mask of shape (batch_size, sequence_length), denoting unpadded\n            elements in the sequence.\n\n        # Returns\n\n        arc_nll : `torch.Tensor`, required.\n            The negative log likelihood from the arc loss.\n        tag_nll : `torch.Tensor`, required.\n            The negative log likelihood from the arc tag loss.\n        \"\"\"", "\n", "float_mask", "=", "mask", ".", "float", "(", ")", "\n", "batch_size", ",", "sequence_length", ",", "_", "=", "attended_arcs", ".", "size", "(", ")", "\n", "# shape (batch_size, 1)", "\n", "range_vector", "=", "get_range_vector", "(", "batch_size", ",", "get_device_of", "(", "attended_arcs", ")", ")", ".", "unsqueeze", "(", "1", ")", "\n", "# shape (batch_size, sequence_length, sequence_length)", "\n", "normalised_arc_logits", "=", "(", "\n", "masked_log_softmax", "(", "attended_arcs", ",", "mask", ")", "\n", "*", "float_mask", ".", "unsqueeze", "(", "2", ")", "\n", "*", "float_mask", ".", "unsqueeze", "(", "1", ")", "\n", ")", "\n", "\n", "# shape (batch_size, sequence_length, num_head_tags)", "\n", "head_tag_logits", "=", "self", ".", "_get_head_tags", "(", "\n", "head_tag_representation", ",", "child_tag_representation", ",", "head_indices", "\n", ")", "\n", "normalised_head_tag_logits", "=", "masked_log_softmax", "(", "\n", "head_tag_logits", ",", "mask", ".", "unsqueeze", "(", "-", "1", ")", "\n", ")", "*", "float_mask", ".", "unsqueeze", "(", "-", "1", ")", "\n", "# index matrix with shape (batch, sequence_length)", "\n", "timestep_index", "=", "get_range_vector", "(", "sequence_length", ",", "get_device_of", "(", "attended_arcs", ")", ")", "\n", "child_index", "=", "(", "\n", "timestep_index", ".", "view", "(", "1", ",", "sequence_length", ")", ".", "expand", "(", "batch_size", ",", "sequence_length", ")", ".", "long", "(", ")", "\n", ")", "\n", "# shape (batch_size, sequence_length)", "\n", "arc_loss", "=", "normalised_arc_logits", "[", "range_vector", ",", "child_index", ",", "head_indices", "]", "\n", "tag_loss", "=", "normalised_head_tag_logits", "[", "range_vector", ",", "child_index", ",", "head_tags", "]", "\n", "# We don't care about predictions for the symbolic ROOT token's head,", "\n", "# so we remove it from the loss.", "\n", "arc_loss", "=", "arc_loss", "[", ":", ",", "1", ":", "]", "\n", "tag_loss", "=", "tag_loss", "[", ":", ",", "1", ":", "]", "\n", "\n", "# The number of valid positions is equal to the number of unmasked elements minus", "\n", "# 1 per sequence in the batch, to account for the symbolic HEAD token.", "\n", "valid_positions", "=", "mask", ".", "sum", "(", ")", "-", "batch_size", "\n", "\n", "arc_nll", "=", "-", "arc_loss", ".", "sum", "(", ")", "/", "valid_positions", ".", "float", "(", ")", "\n", "tag_nll", "=", "-", "tag_loss", ".", "sum", "(", ")", "/", "valid_positions", ".", "float", "(", ")", "\n", "return", "arc_nll", ",", "tag_nll", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.bert_biaffine_dependency_parser.BertBiaffineDependencyParser._greedy_decode": [[459, 516], ["attended_arcs.max", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser._get_head_tags", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser.max", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "attended_arcs.masked_fill_", "attended_arcs.new().fill_", "attended_arcs.new", "mask.size"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._get_head_tags"], ["", "def", "_greedy_decode", "(", "\n", "self", ",", "\n", "head_tag_representation", ":", "torch", ".", "Tensor", ",", "\n", "child_tag_representation", ":", "torch", ".", "Tensor", ",", "\n", "attended_arcs", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Decodes the head and head tag predictions by decoding the unlabeled arcs\n        independently for each word and then again, predicting the head tags of\n        these greedily chosen arcs independently. Note that this method of decoding\n        is not guaranteed to produce trees (i.e. there maybe be multiple roots,\n        or cycles when children are attached to their parents).\n\n        # Parameters\n\n        head_tag_representation : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length, tag_representation_dim),\n            which will be used to generate predictions for the dependency tags\n            for the given arcs.\n        child_tag_representation : `torch.Tensor`, required\n            A tensor of shape (batch_size, sequence_length, tag_representation_dim),\n            which will be used to generate predictions for the dependency tags\n            for the given arcs.\n        attended_arcs : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length, sequence_length) used to generate\n            a distribution over attachments of a given word to all other words.\n\n        # Returns\n\n        heads : `torch.Tensor`\n            A tensor of shape (batch_size, sequence_length) representing the\n            greedily decoded heads of each word.\n        head_tags : `torch.Tensor`\n            A tensor of shape (batch_size, sequence_length) representing the\n            dependency tags of the greedily decoded heads of each word.\n        \"\"\"", "\n", "# Mask the diagonal, because the head of a word can't be itself.", "\n", "attended_arcs", "=", "attended_arcs", "+", "torch", ".", "diag", "(", "\n", "attended_arcs", ".", "new", "(", "mask", ".", "size", "(", "1", ")", ")", ".", "fill_", "(", "-", "numpy", ".", "inf", ")", "\n", ")", "\n", "# Mask padded tokens, because we only want to consider actual words as heads.", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "minus_mask", "=", "(", "~", "mask", ")", ".", "to", "(", "dtype", "=", "torch", ".", "bool", ")", ".", "unsqueeze", "(", "2", ")", "\n", "attended_arcs", ".", "masked_fill_", "(", "minus_mask", ",", "-", "numpy", ".", "inf", ")", "\n", "\n", "# Compute the heads greedily.", "\n", "# shape (batch_size, sequence_length)", "\n", "", "_", ",", "heads", "=", "attended_arcs", ".", "max", "(", "dim", "=", "2", ")", "\n", "\n", "# Given the greedily predicted heads, decode their dependency tags.", "\n", "# shape (batch_size, sequence_length, num_head_tags)", "\n", "head_tag_logits", "=", "self", ".", "_get_head_tags", "(", "\n", "head_tag_representation", ",", "child_tag_representation", ",", "heads", "\n", ")", "\n", "_", ",", "head_tags", "=", "head_tag_logits", ".", "max", "(", "dim", "=", "2", ")", "\n", "return", "heads", ",", "head_tags", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.bert_biaffine_dependency_parser.BertBiaffineDependencyParser._mst_decode": [[517, 590], ["head_tag_representation.expand().contiguous.expand().contiguous.size", "mask.data.sum().long().cpu().numpy", "head_tag_representation.expand().contiguous.expand().contiguous.unsqueeze", "head_tag_representation.expand().contiguous.expand().contiguous.expand().contiguous", "child_tag_representation.expand().contiguous.expand().contiguous.unsqueeze", "child_tag_representation.expand().contiguous.expand().contiguous.expand().contiguous", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser.tag_bilinear", "torch.log_softmax().permute", "torch.log_softmax().permute", "torch.log_softmax().transpose", "torch.log_softmax().transpose", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser._run_mst_decoding", "minus_mask.unsqueeze", "mask.data.sum().long().cpu", "head_tag_representation.expand().contiguous.expand().contiguous.expand", "child_tag_representation.expand().contiguous.expand().contiguous.expand", "torch.log_softmax", "torch.log_softmax", "mask.float", "minus_mask.unsqueeze", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax().transpose.unsqueeze", "mask.data.sum().long", "mask.data.sum"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._run_mst_decoding"], ["", "def", "_mst_decode", "(", "\n", "self", ",", "\n", "head_tag_representation", ":", "torch", ".", "Tensor", ",", "\n", "child_tag_representation", ":", "torch", ".", "Tensor", ",", "\n", "attended_arcs", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Decodes the head and head tag predictions using the Edmonds' Algorithm\n        for finding minimum spanning trees on directed graphs. Nodes in the\n        graph are the words in the sentence, and between each pair of nodes,\n        there is an edge in each direction, where the weight of the edge corresponds\n        to the most likely dependency label probability for that arc. The MST is\n        then generated from this directed graph.\n\n        # Parameters\n\n        head_tag_representation : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length, tag_representation_dim),\n            which will be used to generate predictions for the dependency tags\n            for the given arcs.\n        child_tag_representation : `torch.Tensor`, required\n            A tensor of shape (batch_size, sequence_length, tag_representation_dim),\n            which will be used to generate predictions for the dependency tags\n            for the given arcs.\n        attended_arcs : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length, sequence_length) used to generate\n            a distribution over attachments of a given word to all other words.\n\n        # Returns\n\n        heads : `torch.Tensor`\n            A tensor of shape (batch_size, sequence_length) representing the\n            greedily decoded heads of each word.\n        head_tags : `torch.Tensor`\n            A tensor of shape (batch_size, sequence_length) representing the\n            dependency tags of the optimally decoded heads of each word.\n        \"\"\"", "\n", "batch_size", ",", "sequence_length", ",", "tag_representation_dim", "=", "head_tag_representation", ".", "size", "(", ")", "\n", "\n", "lengths", "=", "mask", ".", "data", ".", "sum", "(", "dim", "=", "1", ")", ".", "long", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "expanded_shape", "=", "[", "batch_size", ",", "sequence_length", ",", "sequence_length", ",", "tag_representation_dim", "]", "\n", "head_tag_representation", "=", "head_tag_representation", ".", "unsqueeze", "(", "2", ")", "\n", "head_tag_representation", "=", "head_tag_representation", ".", "expand", "(", "*", "expanded_shape", ")", ".", "contiguous", "(", ")", "\n", "child_tag_representation", "=", "child_tag_representation", ".", "unsqueeze", "(", "1", ")", "\n", "child_tag_representation", "=", "child_tag_representation", ".", "expand", "(", "*", "expanded_shape", ")", ".", "contiguous", "(", ")", "\n", "# Shape (batch_size, sequence_length, sequence_length, num_head_tags)", "\n", "pairwise_head_logits", "=", "self", ".", "tag_bilinear", "(", "head_tag_representation", ",", "child_tag_representation", ")", "\n", "\n", "# Note that this log_softmax is over the tag dimension, and we don't consider pairs", "\n", "# of tags which are invalid (e.g are a pair which includes a padded element) anyway below.", "\n", "# Shape (batch, num_labels,sequence_length, sequence_length)", "\n", "normalized_pairwise_head_logits", "=", "F", ".", "log_softmax", "(", "pairwise_head_logits", ",", "dim", "=", "3", ")", ".", "permute", "(", "\n", "0", ",", "3", ",", "1", ",", "2", "\n", ")", "\n", "\n", "# Mask padded tokens, because we only want to consider actual words as heads.", "\n", "minus_inf", "=", "-", "1e8", "\n", "minus_mask", "=", "(", "1", "-", "mask", ".", "float", "(", ")", ")", "*", "minus_inf", "\n", "attended_arcs", "=", "attended_arcs", "+", "minus_mask", ".", "unsqueeze", "(", "2", ")", "+", "minus_mask", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# Shape (batch_size, sequence_length, sequence_length)", "\n", "normalized_arc_logits", "=", "F", ".", "log_softmax", "(", "attended_arcs", ",", "dim", "=", "2", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "# Shape (batch_size, num_head_tags, sequence_length, sequence_length)", "\n", "# This energy tensor expresses the following relation:", "\n", "# energy[i,j] = \"Score that i is the head of j\". In this", "\n", "# case, we have heads pointing to their children.", "\n", "batch_energy", "=", "torch", ".", "exp", "(", "\n", "normalized_arc_logits", ".", "unsqueeze", "(", "1", ")", "+", "normalized_pairwise_head_logits", "\n", ")", "\n", "return", "self", ".", "_run_mst_decoding", "(", "batch_energy", ",", "lengths", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.bert_biaffine_dependency_parser.BertBiaffineDependencyParser._run_mst_decoding": [[591, 620], ["zip", "batch_energy.detach().cpu", "energy.max", "allennlp.nn.chu_liu_edmonds.decode_mst", "enumerate", "heads.append", "head_tags.append", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "scores.numpy", "instance_head_tags.append", "numpy.stack", "numpy.stack", "batch_energy.detach", "tag_ids[].item"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_run_mst_decoding", "(", "\n", "batch_energy", ":", "torch", ".", "Tensor", ",", "lengths", ":", "torch", ".", "Tensor", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "heads", "=", "[", "]", "\n", "head_tags", "=", "[", "]", "\n", "for", "energy", ",", "length", "in", "zip", "(", "batch_energy", ".", "detach", "(", ")", ".", "cpu", "(", ")", ",", "lengths", ")", ":", "\n", "            ", "scores", ",", "tag_ids", "=", "energy", ".", "max", "(", "dim", "=", "0", ")", "\n", "# Although we need to include the root node so that the MST includes it,", "\n", "# we do not want any word to be the parent of the root node.", "\n", "# Here, we enforce this by setting the scores for all word -> ROOT edges", "\n", "# edges to be 0.", "\n", "scores", "[", "0", ",", ":", "]", "=", "0", "\n", "# Decode the heads. Because we modify the scores to prevent", "\n", "# adding in word -> ROOT edges, we need to find the labels ourselves.", "\n", "instance_heads", ",", "_", "=", "decode_mst", "(", "scores", ".", "numpy", "(", ")", ",", "length", ",", "has_labels", "=", "False", ")", "\n", "\n", "# Find the labels which correspond to the edges in the max spanning tree.", "\n", "instance_head_tags", "=", "[", "]", "\n", "for", "child", ",", "parent", "in", "enumerate", "(", "instance_heads", ")", ":", "\n", "                ", "instance_head_tags", ".", "append", "(", "tag_ids", "[", "parent", ",", "child", "]", ".", "item", "(", ")", ")", "\n", "# We don't care what the head or tag is for the root token, but by default it's", "\n", "# not necesarily the same in the batched vs unbatched case, which is annoying.", "\n", "# Here we'll just set them to zero.", "\n", "", "instance_heads", "[", "0", "]", "=", "0", "\n", "instance_head_tags", "[", "0", "]", "=", "0", "\n", "heads", ".", "append", "(", "instance_heads", ")", "\n", "head_tags", ".", "append", "(", "instance_head_tags", ")", "\n", "", "return", "torch", ".", "from_numpy", "(", "numpy", ".", "stack", "(", "heads", ")", ")", ",", "torch", ".", "from_numpy", "(", "numpy", ".", "stack", "(", "head_tags", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.bert_biaffine_dependency_parser.BertBiaffineDependencyParser._get_head_tags": [[621, 674], ["head_tag_representation.size", "allennlp.nn.util.get_range_vector().unsqueeze", "selected_head_tag_representations.contiguous.contiguous.contiguous", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser.tag_bilinear", "allennlp.nn.util.get_range_vector", "allennlp.nn.util.get_device_of"], "methods", ["None"], ["", "def", "_get_head_tags", "(", "\n", "self", ",", "\n", "head_tag_representation", ":", "torch", ".", "Tensor", ",", "\n", "child_tag_representation", ":", "torch", ".", "Tensor", ",", "\n", "head_indices", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Decodes the head tags given the head and child tag representations\n        and a tensor of head indices to compute tags for. Note that these are\n        either gold or predicted heads, depending on whether this function is\n        being called to compute the loss, or if it's being called during inference.\n\n        # Parameters\n\n        head_tag_representation : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length, tag_representation_dim),\n            which will be used to generate predictions for the dependency tags\n            for the given arcs.\n        child_tag_representation : `torch.Tensor`, required\n            A tensor of shape (batch_size, sequence_length, tag_representation_dim),\n            which will be used to generate predictions for the dependency tags\n            for the given arcs.\n        head_indices : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length). The indices of the heads\n            for every word.\n\n        # Returns\n\n        head_tag_logits : `torch.Tensor`\n            A tensor of shape (batch_size, sequence_length, num_head_tags),\n            representing logits for predicting a distribution over tags\n            for each arc.\n        \"\"\"", "\n", "batch_size", "=", "head_tag_representation", ".", "size", "(", "0", ")", "\n", "# shape (batch_size,)", "\n", "range_vector", "=", "get_range_vector", "(", "\n", "batch_size", ",", "get_device_of", "(", "head_tag_representation", ")", "\n", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# This next statement is quite a complex piece of indexing, which you really", "\n", "# need to read the docs to understand. See here:", "\n", "# https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html#advanced-indexing", "\n", "# In effect, we are selecting the indices corresponding to the heads of each word from the", "\n", "# sequence length dimension for each element in the batch.", "\n", "\n", "# shape (batch_size, sequence_length, tag_representation_dim)", "\n", "selected_head_tag_representations", "=", "head_tag_representation", "[", "range_vector", ",", "head_indices", "]", "\n", "selected_head_tag_representations", "=", "selected_head_tag_representations", ".", "contiguous", "(", ")", "\n", "# shape (batch_size, sequence_length, num_head_tags)", "\n", "head_tag_logits", "=", "self", ".", "tag_bilinear", "(", "\n", "selected_head_tag_representations", ",", "child_tag_representation", "\n", ")", "\n", "return", "head_tag_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.bert_biaffine_dependency_parser.BertBiaffineDependencyParser._get_mask_for_eval": [[675, 700], ["mask.detach", "pos_tags.eq().long", "pos_tags.eq"], "methods", ["None"], ["", "def", "_get_mask_for_eval", "(", "\n", "self", ",", "mask", ":", "torch", ".", "LongTensor", ",", "pos_tags", ":", "torch", ".", "LongTensor", "\n", ")", "->", "torch", ".", "LongTensor", ":", "\n", "        ", "\"\"\"\n        Dependency evaluation excludes words are punctuation.\n        Here, we create a new mask to exclude word indices which\n        have a \"punctuation-like\" part of speech tag.\n\n        # Parameters\n\n        mask : `torch.LongTensor`, required.\n            The original mask.\n        pos_tags : `torch.LongTensor`, required.\n            The pos tags for the sequence.\n\n        # Returns\n\n        A new mask, where any indices equal to labels\n        we should be ignoring are masked.\n        \"\"\"", "\n", "new_mask", "=", "mask", ".", "detach", "(", ")", "\n", "for", "label", "in", "self", ".", "_pos_to_ignore", ":", "\n", "            ", "label_mask", "=", "pos_tags", ".", "eq", "(", "label", ")", ".", "long", "(", ")", "\n", "new_mask", "=", "new_mask", "*", "(", "1", "-", "label_mask", ")", "\n", "", "return", "new_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.bert_biaffine_dependency_parser.BertBiaffineDependencyParser.get_metrics": [[701, 704], ["bert_biaffine_dependency_parser.BertBiaffineDependencyParser._attachment_scores.get_metric"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.attachment_scores_cuda.AttachmentScoresCuda.get_metric"], ["", "@", "overrides", "\n", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "return", "self", ".", "_attachment_scores", ".", "get_metric", "(", "reset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.bert_biaffine_dependency_parser.BertBiaffineDependencyParser._load": [[705, 793], ["os.path.join", "config.get", "config.get.pop_choice", "allennlp.data.Vocabulary.resolve_class_name", "vocab_class.from_files", "config.get", "config.get", "allennlp.models.model.remove_pretrained_embedding_params", "allennlp.models.model.Model.from_params", "amp.initialize.extend_embedder_vocab", "torch.load", "torch.load", "torch.load", "torch.load", "config.get", "config.pop", "amp.initialize.load_state_dict", "os.path.join", "allennlp.common.params.Params", "allennlp.data.Vocabulary.list_available", "config.get.get", "config.get.get", "allennlp.common.params.Params", "config.get.get", "amp.initialize.cuda", "amp.initialize.cpu", "cls.override_pretrain_embedder", "allennlp.common.params.Params", "logger.warning", "logger.warning", "amp.initialize", "allennlp.nn.util.device_mapping"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.bert_biaffine_dependency_parser.BertBiaffineDependencyParser.override_pretrain_embedder"], ["", "@", "classmethod", "\n", "def", "_load", "(", "\n", "cls", ",", "\n", "config", ":", "Params", ",", "\n", "serialization_dir", ":", "str", ",", "\n", "weights_file", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "cuda_device", ":", "int", "=", "-", "1", ",", "\n", "opt_level", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", ")", "->", "\"Model\"", ":", "\n", "        ", "\"\"\"\n        Instantiates an already-trained model, based on the experiment\n        configuration and some optional overrides.\n        \"\"\"", "\n", "weights_file", "=", "weights_file", "or", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "_DEFAULT_WEIGHTS", ")", "\n", "\n", "# Load vocabulary from file", "\n", "vocab_dir", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"vocabulary\"", ")", "\n", "# If the config specifies a vocabulary subclass, we need to use it.", "\n", "\n", "vocab_params", "=", "config", ".", "get", "(", "\"vocabulary\"", ",", "Params", "(", "{", "}", ")", ")", "\n", "vocab_choice", "=", "vocab_params", ".", "pop_choice", "(", "\"type\"", ",", "Vocabulary", ".", "list_available", "(", ")", ",", "True", ")", "\n", "vocab_class", ",", "_", "=", "Vocabulary", ".", "resolve_class_name", "(", "vocab_choice", ")", "\n", "vocab", "=", "vocab_class", ".", "from_files", "(", "\n", "vocab_dir", ",", "vocab_params", ".", "get", "(", "\"padding_token\"", ")", ",", "vocab_params", ".", "get", "(", "\"oov_token\"", ")", "\n", ")", "\n", "\n", "model_params", "=", "config", ".", "get", "(", "\"model\"", ")", "\n", "\n", "training_params", "=", "config", ".", "get", "(", "\"trainer\"", ",", "Params", "(", "{", "}", ")", ")", "\n", "opt_level", "=", "opt_level", "or", "training_params", ".", "get", "(", "\"opt_level\"", ")", "\n", "\n", "# The experiment config tells us how to _train_ a model, including where to get pre-trained", "\n", "# embeddings from.  We're now _loading_ the model, so those embeddings will already be", "\n", "# stored in our weights.  We don't need any pretrained weight file anymore, and we don't", "\n", "# want the code to look for it, so we remove it from the parameters here.", "\n", "remove_pretrained_embedding_params", "(", "model_params", ")", "\n", "model", "=", "Model", ".", "from_params", "(", "vocab", "=", "vocab", ",", "params", "=", "model_params", ")", "\n", "\n", "# Force model to cpu or gpu, as appropriate, to make sure that the embeddings are", "\n", "# in sync with the weights", "\n", "if", "cuda_device", ">=", "0", ":", "\n", "            ", "model", ".", "cuda", "(", "cuda_device", ")", "\n", "", "else", ":", "\n", "            ", "model", ".", "cpu", "(", ")", "\n", "\n", "# If opt_level is not None (i.e. it exists in the loaded models params or was provided", "\n", "# as argument to this method), call amp.initialize on the loaded model.", "\n", "# Log a warning if amp is not installed or we are loading onto the cpu so that these", "\n", "# cases do not pass silently.", "\n", "", "if", "opt_level", "is", "not", "None", ":", "\n", "            ", "if", "amp", "is", "None", ":", "\n", "                ", "logger", ".", "warning", "(", "\n", "(", "\n", "f\"Apex must be installed to enable mixed-precision via amp.\"", "\n", "f\" Got opt_level is not None (opt_level={opt_level}) but Apex is not installed.\"", "\n", "\" Any further training or inference will happen at full-precision.\"", "\n", ")", "\n", ")", "\n", "", "if", "cuda_device", "==", "-", "1", ":", "\n", "                ", "logger", ".", "warning", "(", "\n", "(", "\n", "f\"A CUDA device must be specified to enable mixed-precision via amp.\"", "\n", "f\" Got cuda_device=={cuda_device} but opt_level is not None (opt_level={opt_level}).\"", "\n", "\" Any further training or inference will happen at full-precision.\"", "\n", ")", "\n", ")", "\n", "", "if", "amp", "is", "not", "None", "and", "cuda_device", ">=", "0", ":", "\n", "                ", "model", "=", "amp", ".", "initialize", "(", "model", ",", "opt_level", "=", "opt_level", ")", "\n", "\n", "# If vocab+embedding extension was done, the model initialized from from_params", "\n", "# and one defined by state dict in weights_file might not have same embedding shapes.", "\n", "# Eg. when model embedder module was transferred along with vocab extension, the", "\n", "# initialized embedding weight shape would be smaller than one in the state_dict.", "\n", "# So calling model embedding extension is required before load_state_dict.", "\n", "# If vocab and model embeddings are in sync, following would be just a no-op.", "\n", "", "", "model", ".", "extend_embedder_vocab", "(", ")", "\n", "\n", "model_state", "=", "torch", ".", "load", "(", "weights_file", ",", "map_location", "=", "util", ".", "device_mapping", "(", "cuda_device", ")", ")", "\n", "\n", "# This is only applied to transformer-based pretrained model", "\n", "new_pretrained_embedder", "=", "config", ".", "get", "(", "\"new_pretrained_embedder\"", ",", "None", ")", "\n", "if", "new_pretrained_embedder", ":", "\n", "            ", "cls", ".", "override_pretrain_embedder", "(", "model_state", ",", "new_pretrained_embedder", ")", "\n", "", "config", ".", "pop", "(", "\"new_pretrained_embedder\"", ",", "Params", "(", "{", "}", ")", ")", "\n", "\n", "model", ".", "load_state_dict", "(", "model_state", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.bert_biaffine_dependency_parser.BertBiaffineDependencyParser.override_pretrain_embedder": [[794, 812], ["model_state.keys", "transformers.AutoModel.from_pretrained", "transformers.AutoModel.from_pretrained.state_dict", "AutoModel.from_pretrained.state_dict.items", "model_state.pop", "discard.append"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "override_pretrain_embedder", "(", "cls", ",", "\n", "model_state", ":", "Dict", ",", "\n", "new_pretrained_embedder", ":", "str", "\n", ")", "->", "Dict", ":", "\n", "\n", "        ", "discard", "=", "[", "]", "\n", "for", "module_name", "in", "model_state", ".", "keys", "(", ")", ":", "\n", "            ", "if", "PREFIX_MODULE_NAME", "in", "module_name", ":", "\n", "                ", "discard", ".", "append", "(", "module_name", ")", "\n", "", "", "for", "module_name", "in", "discard", ":", "\n", "            ", "model_state", ".", "pop", "(", "module_name", ")", "\n", "\n", "", "pretrained_model", "=", "AutoModel", ".", "from_pretrained", "(", "new_pretrained_embedder", ")", "\n", "states", "=", "pretrained_model", ".", "state_dict", "(", ")", "\n", "\n", "for", "module_name", ",", "module_weights", "in", "states", ".", "items", "(", ")", ":", "\n", "            ", "model_state", "[", "PREFIX_MODULE_NAME", "+", "module_name", "]", "=", "module_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.bert_biaffine_dependency_parser.BertBiaffineDependencyParser.forward_embeddings": [[813, 848], ["bert_biaffine_dependency_parser.BertBiaffineDependencyParser.text_field_embedder().detach", "torch.norm().view", "torch.norm().view", "torch.norm().view", "torch.norm().view", "bert_biaffine_dependency_parser.BertBiaffineDependencyParser.text_field_embedder", "torch.norm", "torch.norm", "torch.norm", "torch.norm"], "methods", ["None"], ["", "", "def", "forward_embeddings", "(", "\n", "self", ",", "# type: ignore", "\n", "words", ":", "TextFieldTensors", ",", "\n", "means", ":", "List", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n\n        words : TextFieldTensors, required\n            The output of `TextField.as_array()`, which should typically be passed directly to a\n            `TextFieldEmbedder`. This output is a dictionary mapping keys to `TokenIndexer`\n            tensors.  At its most basic, using a `SingleIdTokenIndexer` this is : `{\"tokens\":\n            Tensor(batch_size, sequence_length)}`. This dictionary will have the same keys as were used\n            for the `TokenIndexers` when you created the `TextField` representing your\n            sequence.  The dictionary is designed to be passed directly to a `TextFieldEmbedder`,\n            which knows how to combine different word representations into a single vector per\n            token in your input.\n\n        # Returns\n\n        An output dictionary consisting of:\n        batch_embeddings : `torch.Tensor`\n            An embedding tensor after the pre-trained transformer model without [CLS], [SEP], [PAD] and [MASK].\n        \"\"\"", "\n", "\n", "token_ids", "=", "words", "[", "'tokens'", "]", "[", "'token_ids'", "]", "\n", "mask", "=", "(", "token_ids", "!=", "CLS_ID", ")", "&", "(", "token_ids", "!=", "SEP_ID", ")", "&", "(", "token_ids", "!=", "PAD_ID", ")", "&", "(", "token_ids", "!=", "MASK_ID", ")", "\n", "batch_embeddings", "=", "self", ".", "text_field_embedder", "(", "words", ")", ".", "detach", "(", ")", "\n", "batch_embeddings", "=", "batch_embeddings", "[", "mask", "]", "\n", "for", "mean", "in", "means", ":", "\n", "            ", "batch_embeddings", "=", "batch_embeddings", "-", "mean", "\n", "batch_embeddings", "/=", "torch", ".", "norm", "(", "batch_embeddings", ",", "dim", "=", "1", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "\n", "", "return", "batch_embeddings", "\n", "", "", ""]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser.__init__": [[84, 170], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "encoder.get_output_dim", "copy.deepcopy", "allennlp.modules.matrix_attention.bilinear_matrix_attention.BilinearMatrixAttention", "biaffine_dependency_parser.BiaffineDependencyParser.vocab.get_vocab_size", "copy.deepcopy", "torch.nn.modules.Bilinear", "torch.nn.modules.Bilinear", "torch.nn.modules.Bilinear", "torch.nn.modules.Bilinear", "allennlp.modules.InputVariationalDropout", "torch.nn.modules.Dropout", "torch.nn.modules.Dropout", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "text_field_embedder.get_output_dim", "allennlp.common.checks.check_dimensions_match", "allennlp.common.checks.check_dimensions_match", "allennlp.common.checks.check_dimensions_match", "biaffine_dependency_parser.BiaffineDependencyParser.vocab.get_token_to_index_vocabulary", "set", "logger.info", "src.training.AttachmentScoresCuda", "initializer", "allennlp.modules.FeedForward", "allennlp.modules.FeedForward", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "pos_tag_embedding.get_output_dim", "encoder.get_input_dim", "biaffine_dependency_parser.BiaffineDependencyParser.head_tag_feedforward.get_output_dim", "biaffine_dependency_parser.BiaffineDependencyParser.head_arc_feedforward.get_output_dim", "punctuation_tag_indices.values", "biaffine_dependency_parser.BiaffineDependencyParser.items", "allennlp.nn.Activation.by_name", "allennlp.nn.Activation.by_name", "encoder.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.evaluator.evaluator.__init__", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_mismatched_embedder.StaticPretrainedTransformerMismatchedEmbedder.get_output_dim", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_mismatched_embedder.StaticPretrainedTransformerMismatchedEmbedder.get_output_dim", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_mismatched_embedder.StaticPretrainedTransformerMismatchedEmbedder.get_output_dim", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_mismatched_embedder.StaticPretrainedTransformerMismatchedEmbedder.get_output_dim", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_mismatched_embedder.StaticPretrainedTransformerMismatchedEmbedder.get_output_dim", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.token_embedders.static_pretrained_transformer_mismatched_embedder.StaticPretrainedTransformerMismatchedEmbedder.get_output_dim"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "text_field_embedder", ":", "TextFieldEmbedder", ",", "\n", "encoder", ":", "Seq2SeqEncoder", ",", "\n", "tag_representation_dim", ":", "int", ",", "\n", "arc_representation_dim", ":", "int", ",", "\n", "tag_feedforward", ":", "FeedForward", "=", "None", ",", "\n", "arc_feedforward", ":", "FeedForward", "=", "None", ",", "\n", "pos_tag_embedding", ":", "Embedding", "=", "None", ",", "\n", "use_mst_decoding_for_validation", ":", "bool", "=", "True", ",", "\n", "dropout", ":", "float", "=", "0.0", ",", "\n", "input_dropout", ":", "float", "=", "0.0", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "vocab", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "text_field_embedder", "=", "text_field_embedder", "\n", "self", ".", "encoder", "=", "encoder", "\n", "\n", "encoder_dim", "=", "encoder", ".", "get_output_dim", "(", ")", "\n", "\n", "self", ".", "head_arc_feedforward", "=", "arc_feedforward", "or", "FeedForward", "(", "\n", "encoder_dim", ",", "1", ",", "arc_representation_dim", ",", "Activation", ".", "by_name", "(", "\"elu\"", ")", "(", ")", "\n", ")", "\n", "self", ".", "child_arc_feedforward", "=", "copy", ".", "deepcopy", "(", "self", ".", "head_arc_feedforward", ")", "\n", "\n", "self", ".", "arc_attention", "=", "BilinearMatrixAttention", "(", "\n", "arc_representation_dim", ",", "arc_representation_dim", ",", "use_input_biases", "=", "True", "\n", ")", "\n", "\n", "num_labels", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "\"head_tags\"", ")", "\n", "\n", "self", ".", "head_tag_feedforward", "=", "tag_feedforward", "or", "FeedForward", "(", "\n", "encoder_dim", ",", "1", ",", "tag_representation_dim", ",", "Activation", ".", "by_name", "(", "\"elu\"", ")", "(", ")", "\n", ")", "\n", "self", ".", "child_tag_feedforward", "=", "copy", ".", "deepcopy", "(", "self", ".", "head_tag_feedforward", ")", "\n", "\n", "self", ".", "tag_bilinear", "=", "torch", ".", "nn", ".", "modules", ".", "Bilinear", "(", "\n", "tag_representation_dim", ",", "tag_representation_dim", ",", "num_labels", "\n", ")", "\n", "\n", "self", ".", "_pos_tag_embedding", "=", "pos_tag_embedding", "or", "None", "\n", "self", ".", "_dropout", "=", "InputVariationalDropout", "(", "dropout", ")", "\n", "self", ".", "_input_dropout", "=", "Dropout", "(", "input_dropout", ")", "\n", "self", ".", "_head_sentinel", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "[", "1", ",", "1", ",", "encoder", ".", "get_output_dim", "(", ")", "]", ")", ")", "\n", "\n", "representation_dim", "=", "text_field_embedder", ".", "get_output_dim", "(", ")", "\n", "if", "pos_tag_embedding", "is", "not", "None", ":", "\n", "            ", "representation_dim", "+=", "pos_tag_embedding", ".", "get_output_dim", "(", ")", "\n", "\n", "", "check_dimensions_match", "(", "\n", "representation_dim", ",", "\n", "encoder", ".", "get_input_dim", "(", ")", ",", "\n", "\"text field embedding dim\"", ",", "\n", "\"encoder input dim\"", ",", "\n", ")", "\n", "\n", "check_dimensions_match", "(", "\n", "tag_representation_dim", ",", "\n", "self", ".", "head_tag_feedforward", ".", "get_output_dim", "(", ")", ",", "\n", "\"tag representation dim\"", ",", "\n", "\"tag feedforward output dim\"", ",", "\n", ")", "\n", "check_dimensions_match", "(", "\n", "arc_representation_dim", ",", "\n", "self", ".", "head_arc_feedforward", ".", "get_output_dim", "(", ")", ",", "\n", "\"arc representation dim\"", ",", "\n", "\"arc feedforward output dim\"", ",", "\n", ")", "\n", "\n", "self", ".", "use_mst_decoding_for_validation", "=", "use_mst_decoding_for_validation", "\n", "\n", "tags", "=", "self", ".", "vocab", ".", "get_token_to_index_vocabulary", "(", "\"pos\"", ")", "\n", "punctuation_tag_indices", "=", "{", "\n", "tag", ":", "index", "for", "tag", ",", "index", "in", "tags", ".", "items", "(", ")", "if", "tag", "in", "POS_TO_IGNORE", "\n", "}", "\n", "self", ".", "_pos_to_ignore", "=", "set", "(", "punctuation_tag_indices", ".", "values", "(", ")", ")", "\n", "logger", ".", "info", "(", "\n", "f\"Found POS tags corresponding to the following punctuation : {punctuation_tag_indices}. \"", "\n", "\"Ignoring words with these POS tags for evaluation.\"", "\n", ")", "\n", "\n", "self", ".", "_attachment_scores", "=", "AttachmentScoresCuda", "(", ")", "\n", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser.forward": [[171, 271], ["biaffine_dependency_parser.BiaffineDependencyParser.text_field_embedder().detach", "allennlp.nn.util.get_text_field_mask", "biaffine_dependency_parser.BiaffineDependencyParser._parse", "biaffine_dependency_parser.BiaffineDependencyParser._pos_tag_embedding", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "biaffine_dependency_parser.BiaffineDependencyParser._get_mask_for_eval", "biaffine_dependency_parser.BiaffineDependencyParser._attachment_scores", "biaffine_dependency_parser.BiaffineDependencyParser.text_field_embedder", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._parse", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._get_mask_for_eval"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "# type: ignore", "\n", "words", ":", "TextFieldTensors", ",", "\n", "pos_tags", ":", "torch", ".", "LongTensor", ",", "\n", "metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", ",", "\n", "head_tags", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "head_indices", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n\n        words : TextFieldTensors, required\n            The output of `TextField.as_array()`, which should typically be passed directly to a\n            `TextFieldEmbedder`. This output is a dictionary mapping keys to `TokenIndexer`\n            tensors.  At its most basic, using a `SingleIdTokenIndexer` this is : `{\"tokens\":\n            Tensor(batch_size, sequence_length)}`. This dictionary will have the same keys as were used\n            for the `TokenIndexers` when you created the `TextField` representing your\n            sequence.  The dictionary is designed to be passed directly to a `TextFieldEmbedder`,\n            which knows how to combine different word representations into a single vector per\n            token in your input.\n        pos_tags : `torch.LongTensor`, required\n            The output of a `SequenceLabelField` containing POS tags.\n            POS tags are required regardless of whether they are used in the model,\n            because they are used to filter the evaluation metric to only consider\n            heads of words which are not punctuation.\n        metadata : List[Dict[str, Any]], optional (default=None)\n            A dictionary of metadata for each batch element which has keys:\n                words : `List[str]`, required.\n                    The tokens in the original sentence.\n                pos : `List[str]`, required.\n                    The dependencies POS tags for each word.\n        head_tags : torch.LongTensor, optional (default = None)\n            A torch tensor representing the sequence of integer gold class labels for the arcs\n            in the dependency parse. Has shape `(batch_size, sequence_length)`.\n        head_indices : torch.LongTensor, optional (default = None)\n            A torch tensor representing the sequence of integer indices denoting the parent of every\n            word in the dependency parse. Has shape `(batch_size, sequence_length)`.\n\n        # Returns\n\n        An output dictionary consisting of:\n        loss : `torch.FloatTensor`, optional\n            A scalar loss to be optimised.\n        arc_loss : `torch.FloatTensor`\n            The loss contribution from the unlabeled arcs.\n        loss : `torch.FloatTensor`, optional\n            The loss contribution from predicting the dependency\n            tags for the gold arcs.\n        heads : `torch.FloatTensor`\n            The predicted head indices for each word. A tensor\n            of shape (batch_size, sequence_length).\n        head_types : `torch.FloatTensor`\n            The predicted head types for each arc. A tensor\n            of shape (batch_size, sequence_length).\n        mask : `torch.LongTensor`\n            A mask denoting the padded elements in the batch.\n        \"\"\"", "\n", "\n", "embedded_text_input", "=", "self", ".", "text_field_embedder", "(", "words", ")", ".", "detach", "(", ")", "\n", "if", "pos_tags", "is", "not", "None", "and", "self", ".", "_pos_tag_embedding", "is", "not", "None", ":", "\n", "            ", "embedded_pos_tags", "=", "self", ".", "_pos_tag_embedding", "(", "pos_tags", ")", "\n", "embedded_text_input", "=", "torch", ".", "cat", "(", "[", "embedded_text_input", ",", "embedded_pos_tags", "]", ",", "-", "1", ")", "\n", "", "elif", "self", ".", "_pos_tag_embedding", "is", "not", "None", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"Model uses a POS embedding, but no POS tags were passed.\"", ")", "\n", "\n", "", "mask", "=", "get_text_field_mask", "(", "words", ")", "\n", "\n", "predicted_heads", ",", "predicted_head_tags", ",", "mask", ",", "arc_nll", ",", "tag_nll", "=", "self", ".", "_parse", "(", "\n", "embedded_text_input", ",", "mask", ",", "head_tags", ",", "head_indices", "\n", ")", "\n", "\n", "loss", "=", "arc_nll", "+", "tag_nll", "\n", "\n", "if", "head_indices", "is", "not", "None", "and", "head_tags", "is", "not", "None", ":", "\n", "            ", "evaluation_mask", "=", "self", ".", "_get_mask_for_eval", "(", "mask", "[", ":", ",", "1", ":", "]", ",", "pos_tags", ")", "\n", "# We calculate attatchment scores for the whole sentence", "\n", "# but excluding the symbolic ROOT token at the start,", "\n", "# which is why we start from the second element in the sequence.", "\n", "self", ".", "_attachment_scores", "(", "\n", "predicted_heads", "[", ":", ",", "1", ":", "]", ",", "\n", "predicted_head_tags", "[", ":", ",", "1", ":", "]", ",", "\n", "head_indices", ",", "\n", "head_tags", ",", "\n", "evaluation_mask", ",", "\n", ")", "\n", "\n", "", "output_dict", "=", "{", "\n", "\"heads\"", ":", "predicted_heads", ",", "\n", "\"head_tags\"", ":", "predicted_head_tags", ",", "\n", "\"arc_loss\"", ":", "arc_nll", ",", "\n", "\"tag_loss\"", ":", "tag_nll", ",", "\n", "\"loss\"", ":", "loss", ",", "\n", "\"mask\"", ":", "mask", ",", "\n", "\"words\"", ":", "[", "meta", "[", "\"words\"", "]", "for", "meta", "in", "metadata", "]", ",", "\n", "\"pos\"", ":", "[", "meta", "[", "\"pos\"", "]", "for", "meta", "in", "metadata", "]", ",", "\n", "}", "\n", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser.make_output_human_readable": [[272, 295], ["output_dict.pop().cpu().detach().numpy", "output_dict.pop().cpu().detach().numpy", "output_dict.pop", "allennlp.nn.util.get_lengths_from_binary_sequence_mask", "zip", "list", "head_tag_labels.append", "head_indices.append", "output_dict.pop().cpu().detach", "output_dict.pop().cpu().detach", "biaffine_dependency_parser.BiaffineDependencyParser.vocab.get_token_from_index", "output_dict.pop().cpu", "output_dict.pop().cpu", "output_dict.pop", "output_dict.pop"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "make_output_human_readable", "(", "\n", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "head_tags", "=", "output_dict", ".", "pop", "(", "\"head_tags\"", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "heads", "=", "output_dict", ".", "pop", "(", "\"heads\"", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "mask", "=", "output_dict", ".", "pop", "(", "\"mask\"", ")", "\n", "lengths", "=", "get_lengths_from_binary_sequence_mask", "(", "mask", ")", "\n", "head_tag_labels", "=", "[", "]", "\n", "head_indices", "=", "[", "]", "\n", "for", "instance_heads", ",", "instance_tags", ",", "length", "in", "zip", "(", "heads", ",", "head_tags", ",", "lengths", ")", ":", "\n", "            ", "instance_heads", "=", "list", "(", "instance_heads", "[", "1", ":", "length", "]", ")", "\n", "instance_tags", "=", "instance_tags", "[", "1", ":", "length", "]", "\n", "labels", "=", "[", "\n", "self", ".", "vocab", ".", "get_token_from_index", "(", "label", ",", "\"head_tags\"", ")", "for", "label", "in", "instance_tags", "\n", "]", "\n", "head_tag_labels", ".", "append", "(", "labels", ")", "\n", "head_indices", ".", "append", "(", "instance_heads", ")", "\n", "\n", "", "output_dict", "[", "\"predicted_dependencies\"", "]", "=", "head_tag_labels", "\n", "output_dict", "[", "\"predicted_heads\"", "]", "=", "head_indices", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._parse": [[296, 363], ["biaffine_dependency_parser.BiaffineDependencyParser._input_dropout", "biaffine_dependency_parser.BiaffineDependencyParser.encoder", "biaffine_dependency_parser.BiaffineDependencyParser.size", "biaffine_dependency_parser.BiaffineDependencyParser._head_sentinel.expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.float", "torch.cat.float", "biaffine_dependency_parser.BiaffineDependencyParser._dropout", "biaffine_dependency_parser.BiaffineDependencyParser._dropout", "biaffine_dependency_parser.BiaffineDependencyParser._dropout", "biaffine_dependency_parser.BiaffineDependencyParser._dropout", "biaffine_dependency_parser.BiaffineDependencyParser._dropout", "biaffine_dependency_parser.BiaffineDependencyParser.arc_attention", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "biaffine_dependency_parser.BiaffineDependencyParser.head_arc_feedforward", "biaffine_dependency_parser.BiaffineDependencyParser.child_arc_feedforward", "biaffine_dependency_parser.BiaffineDependencyParser.head_tag_feedforward", "biaffine_dependency_parser.BiaffineDependencyParser.child_tag_feedforward", "minus_mask.unsqueeze", "biaffine_dependency_parser.BiaffineDependencyParser._greedy_decode", "biaffine_dependency_parser.BiaffineDependencyParser._mst_decode", "biaffine_dependency_parser.BiaffineDependencyParser._construct_loss", "biaffine_dependency_parser.BiaffineDependencyParser._construct_loss", "torch.cat.new_ones", "torch.cat.new_ones", "minus_mask.unsqueeze", "torch.cat.new_zeros", "torch.cat.new_zeros", "torch.cat.new_zeros", "torch.cat.new_zeros", "predicted_heads.long", "predicted_head_tags.long"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._greedy_decode", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._mst_decode", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._construct_loss", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._construct_loss"], ["", "def", "_parse", "(", "\n", "self", ",", "\n", "embedded_text_input", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "LongTensor", ",", "\n", "head_tags", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "head_indices", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "embedded_text_input", "=", "self", ".", "_input_dropout", "(", "embedded_text_input", ")", "\n", "encoded_text", "=", "self", ".", "encoder", "(", "embedded_text_input", ",", "mask", ")", "\n", "\n", "batch_size", ",", "_", ",", "encoding_dim", "=", "encoded_text", ".", "size", "(", ")", "\n", "\n", "head_sentinel", "=", "self", ".", "_head_sentinel", ".", "expand", "(", "batch_size", ",", "1", ",", "encoding_dim", ")", "\n", "# Concatenate the head sentinel onto the sentence representation.", "\n", "encoded_text", "=", "torch", ".", "cat", "(", "[", "head_sentinel", ",", "encoded_text", "]", ",", "1", ")", "\n", "mask", "=", "torch", ".", "cat", "(", "[", "mask", ".", "new_ones", "(", "batch_size", ",", "1", ")", ",", "mask", "]", ",", "1", ")", "\n", "if", "head_indices", "is", "not", "None", ":", "\n", "            ", "head_indices", "=", "torch", ".", "cat", "(", "[", "head_indices", ".", "new_zeros", "(", "batch_size", ",", "1", ")", ",", "head_indices", "]", ",", "1", ")", "\n", "", "if", "head_tags", "is", "not", "None", ":", "\n", "            ", "head_tags", "=", "torch", ".", "cat", "(", "[", "head_tags", ".", "new_zeros", "(", "batch_size", ",", "1", ")", ",", "head_tags", "]", ",", "1", ")", "\n", "", "float_mask", "=", "mask", ".", "float", "(", ")", "\n", "encoded_text", "=", "self", ".", "_dropout", "(", "encoded_text", ")", "\n", "\n", "# shape (batch_size, sequence_length, arc_representation_dim)", "\n", "head_arc_representation", "=", "self", ".", "_dropout", "(", "self", ".", "head_arc_feedforward", "(", "encoded_text", ")", ")", "\n", "child_arc_representation", "=", "self", ".", "_dropout", "(", "self", ".", "child_arc_feedforward", "(", "encoded_text", ")", ")", "\n", "\n", "# shape (batch_size, sequence_length, tag_representation_dim)", "\n", "head_tag_representation", "=", "self", ".", "_dropout", "(", "self", ".", "head_tag_feedforward", "(", "encoded_text", ")", ")", "\n", "child_tag_representation", "=", "self", ".", "_dropout", "(", "self", ".", "child_tag_feedforward", "(", "encoded_text", ")", ")", "\n", "# shape (batch_size, sequence_length, sequence_length)", "\n", "attended_arcs", "=", "self", ".", "arc_attention", "(", "head_arc_representation", ",", "child_arc_representation", ")", "\n", "\n", "minus_inf", "=", "-", "1e8", "\n", "minus_mask", "=", "(", "1", "-", "float_mask", ")", "*", "minus_inf", "\n", "attended_arcs", "=", "attended_arcs", "+", "minus_mask", ".", "unsqueeze", "(", "2", ")", "+", "minus_mask", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "if", "self", ".", "training", "or", "not", "self", ".", "use_mst_decoding_for_validation", ":", "\n", "            ", "predicted_heads", ",", "predicted_head_tags", "=", "self", ".", "_greedy_decode", "(", "\n", "head_tag_representation", ",", "child_tag_representation", ",", "attended_arcs", ",", "mask", "\n", ")", "\n", "", "else", ":", "\n", "            ", "predicted_heads", ",", "predicted_head_tags", "=", "self", ".", "_mst_decode", "(", "\n", "head_tag_representation", ",", "child_tag_representation", ",", "attended_arcs", ",", "mask", "\n", ")", "\n", "", "if", "head_indices", "is", "not", "None", "and", "head_tags", "is", "not", "None", ":", "\n", "\n", "            ", "arc_nll", ",", "tag_nll", "=", "self", ".", "_construct_loss", "(", "\n", "head_tag_representation", "=", "head_tag_representation", ",", "\n", "child_tag_representation", "=", "child_tag_representation", ",", "\n", "attended_arcs", "=", "attended_arcs", ",", "\n", "head_indices", "=", "head_indices", ",", "\n", "head_tags", "=", "head_tags", ",", "\n", "mask", "=", "mask", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "arc_nll", ",", "tag_nll", "=", "self", ".", "_construct_loss", "(", "\n", "head_tag_representation", "=", "head_tag_representation", ",", "\n", "child_tag_representation", "=", "child_tag_representation", ",", "\n", "attended_arcs", "=", "attended_arcs", ",", "\n", "head_indices", "=", "predicted_heads", ".", "long", "(", ")", ",", "\n", "head_tags", "=", "predicted_head_tags", ".", "long", "(", ")", ",", "\n", "mask", "=", "mask", ",", "\n", ")", "\n", "\n", "", "return", "predicted_heads", ",", "predicted_head_tags", ",", "mask", ",", "arc_nll", ",", "tag_nll", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._construct_loss": [[364, 444], ["mask.float", "attended_arcs.size", "allennlp.nn.util.get_range_vector().unsqueeze", "biaffine_dependency_parser.BiaffineDependencyParser._get_head_tags", "allennlp.nn.util.get_range_vector", "allennlp.nn.util.get_range_vector.view().expand().long", "mask.float.unsqueeze", "allennlp.nn.util.masked_log_softmax", "mask.float.unsqueeze", "allennlp.nn.util.get_device_of", "mask.sum", "valid_positions.float", "valid_positions.float", "allennlp.nn.util.get_range_vector", "allennlp.nn.util.masked_log_softmax", "mask.float.unsqueeze", "mask.unsqueeze", "allennlp.nn.util.get_range_vector.view().expand", "arc_loss.sum", "tag_loss.sum", "allennlp.nn.util.get_device_of", "allennlp.nn.util.get_range_vector.view"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._get_head_tags"], ["", "def", "_construct_loss", "(", "\n", "self", ",", "\n", "head_tag_representation", ":", "torch", ".", "Tensor", ",", "\n", "child_tag_representation", ":", "torch", ".", "Tensor", ",", "\n", "attended_arcs", ":", "torch", ".", "Tensor", ",", "\n", "head_indices", ":", "torch", ".", "Tensor", ",", "\n", "head_tags", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Computes the arc and tag loss for a sequence given gold head indices and tags.\n\n        # Parameters\n\n        head_tag_representation : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length, tag_representation_dim),\n            which will be used to generate predictions for the dependency tags\n            for the given arcs.\n        child_tag_representation : `torch.Tensor`, required\n            A tensor of shape (batch_size, sequence_length, tag_representation_dim),\n            which will be used to generate predictions for the dependency tags\n            for the given arcs.\n        attended_arcs : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length, sequence_length) used to generate\n            a distribution over attachments of a given word to all other words.\n        head_indices : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length).\n            The indices of the heads for every word.\n        head_tags : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length).\n            The dependency labels of the heads for every word.\n        mask : `torch.Tensor`, required.\n            A mask of shape (batch_size, sequence_length), denoting unpadded\n            elements in the sequence.\n\n        # Returns\n\n        arc_nll : `torch.Tensor`, required.\n            The negative log likelihood from the arc loss.\n        tag_nll : `torch.Tensor`, required.\n            The negative log likelihood from the arc tag loss.\n        \"\"\"", "\n", "float_mask", "=", "mask", ".", "float", "(", ")", "\n", "batch_size", ",", "sequence_length", ",", "_", "=", "attended_arcs", ".", "size", "(", ")", "\n", "# shape (batch_size, 1)", "\n", "range_vector", "=", "get_range_vector", "(", "batch_size", ",", "get_device_of", "(", "attended_arcs", ")", ")", ".", "unsqueeze", "(", "1", ")", "\n", "# shape (batch_size, sequence_length, sequence_length)", "\n", "normalised_arc_logits", "=", "(", "\n", "masked_log_softmax", "(", "attended_arcs", ",", "mask", ")", "\n", "*", "float_mask", ".", "unsqueeze", "(", "2", ")", "\n", "*", "float_mask", ".", "unsqueeze", "(", "1", ")", "\n", ")", "\n", "\n", "# shape (batch_size, sequence_length, num_head_tags)", "\n", "head_tag_logits", "=", "self", ".", "_get_head_tags", "(", "\n", "head_tag_representation", ",", "child_tag_representation", ",", "head_indices", "\n", ")", "\n", "normalised_head_tag_logits", "=", "masked_log_softmax", "(", "\n", "head_tag_logits", ",", "mask", ".", "unsqueeze", "(", "-", "1", ")", "\n", ")", "*", "float_mask", ".", "unsqueeze", "(", "-", "1", ")", "\n", "# index matrix with shape (batch, sequence_length)", "\n", "timestep_index", "=", "get_range_vector", "(", "sequence_length", ",", "get_device_of", "(", "attended_arcs", ")", ")", "\n", "child_index", "=", "(", "\n", "timestep_index", ".", "view", "(", "1", ",", "sequence_length", ")", ".", "expand", "(", "batch_size", ",", "sequence_length", ")", ".", "long", "(", ")", "\n", ")", "\n", "# shape (batch_size, sequence_length)", "\n", "arc_loss", "=", "normalised_arc_logits", "[", "range_vector", ",", "child_index", ",", "head_indices", "]", "\n", "tag_loss", "=", "normalised_head_tag_logits", "[", "range_vector", ",", "child_index", ",", "head_tags", "]", "\n", "# We don't care about predictions for the symbolic ROOT token's head,", "\n", "# so we remove it from the loss.", "\n", "arc_loss", "=", "arc_loss", "[", ":", ",", "1", ":", "]", "\n", "tag_loss", "=", "tag_loss", "[", ":", ",", "1", ":", "]", "\n", "\n", "# The number of valid positions is equal to the number of unmasked elements minus", "\n", "# 1 per sequence in the batch, to account for the symbolic HEAD token.", "\n", "valid_positions", "=", "mask", ".", "sum", "(", ")", "-", "batch_size", "\n", "\n", "arc_nll", "=", "-", "arc_loss", ".", "sum", "(", ")", "/", "valid_positions", ".", "float", "(", ")", "\n", "tag_nll", "=", "-", "tag_loss", ".", "sum", "(", ")", "/", "valid_positions", ".", "float", "(", ")", "\n", "return", "arc_nll", ",", "tag_nll", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._greedy_decode": [[445, 502], ["attended_arcs.max", "biaffine_dependency_parser.BiaffineDependencyParser._get_head_tags", "biaffine_dependency_parser.BiaffineDependencyParser.max", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "attended_arcs.masked_fill_", "attended_arcs.new().fill_", "attended_arcs.new", "mask.size"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._get_head_tags"], ["", "def", "_greedy_decode", "(", "\n", "self", ",", "\n", "head_tag_representation", ":", "torch", ".", "Tensor", ",", "\n", "child_tag_representation", ":", "torch", ".", "Tensor", ",", "\n", "attended_arcs", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Decodes the head and head tag predictions by decoding the unlabeled arcs\n        independently for each word and then again, predicting the head tags of\n        these greedily chosen arcs independently. Note that this method of decoding\n        is not guaranteed to produce trees (i.e. there maybe be multiple roots,\n        or cycles when children are attached to their parents).\n\n        # Parameters\n\n        head_tag_representation : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length, tag_representation_dim),\n            which will be used to generate predictions for the dependency tags\n            for the given arcs.\n        child_tag_representation : `torch.Tensor`, required\n            A tensor of shape (batch_size, sequence_length, tag_representation_dim),\n            which will be used to generate predictions for the dependency tags\n            for the given arcs.\n        attended_arcs : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length, sequence_length) used to generate\n            a distribution over attachments of a given word to all other words.\n\n        # Returns\n\n        heads : `torch.Tensor`\n            A tensor of shape (batch_size, sequence_length) representing the\n            greedily decoded heads of each word.\n        head_tags : `torch.Tensor`\n            A tensor of shape (batch_size, sequence_length) representing the\n            dependency tags of the greedily decoded heads of each word.\n        \"\"\"", "\n", "# Mask the diagonal, because the head of a word can't be itself.", "\n", "attended_arcs", "=", "attended_arcs", "+", "torch", ".", "diag", "(", "\n", "attended_arcs", ".", "new", "(", "mask", ".", "size", "(", "1", ")", ")", ".", "fill_", "(", "-", "numpy", ".", "inf", ")", "\n", ")", "\n", "# Mask padded tokens, because we only want to consider actual words as heads.", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "minus_mask", "=", "(", "~", "mask", ")", ".", "to", "(", "dtype", "=", "torch", ".", "bool", ")", ".", "unsqueeze", "(", "2", ")", "\n", "attended_arcs", ".", "masked_fill_", "(", "minus_mask", ",", "-", "numpy", ".", "inf", ")", "\n", "\n", "# Compute the heads greedily.", "\n", "# shape (batch_size, sequence_length)", "\n", "", "_", ",", "heads", "=", "attended_arcs", ".", "max", "(", "dim", "=", "2", ")", "\n", "\n", "# Given the greedily predicted heads, decode their dependency tags.", "\n", "# shape (batch_size, sequence_length, num_head_tags)", "\n", "head_tag_logits", "=", "self", ".", "_get_head_tags", "(", "\n", "head_tag_representation", ",", "child_tag_representation", ",", "heads", "\n", ")", "\n", "_", ",", "head_tags", "=", "head_tag_logits", ".", "max", "(", "dim", "=", "2", ")", "\n", "return", "heads", ",", "head_tags", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._mst_decode": [[503, 576], ["head_tag_representation.expand().contiguous.expand().contiguous.size", "mask.data.sum().long().cpu().numpy", "head_tag_representation.expand().contiguous.expand().contiguous.unsqueeze", "head_tag_representation.expand().contiguous.expand().contiguous.expand().contiguous", "child_tag_representation.expand().contiguous.expand().contiguous.unsqueeze", "child_tag_representation.expand().contiguous.expand().contiguous.expand().contiguous", "biaffine_dependency_parser.BiaffineDependencyParser.tag_bilinear", "torch.log_softmax().permute", "torch.log_softmax().permute", "torch.log_softmax().transpose", "torch.log_softmax().transpose", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "biaffine_dependency_parser.BiaffineDependencyParser._run_mst_decoding", "minus_mask.unsqueeze", "mask.data.sum().long().cpu", "head_tag_representation.expand().contiguous.expand().contiguous.expand", "child_tag_representation.expand().contiguous.expand().contiguous.expand", "torch.log_softmax", "torch.log_softmax", "mask.float", "minus_mask.unsqueeze", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax().transpose.unsqueeze", "mask.data.sum().long", "mask.data.sum"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._run_mst_decoding"], ["", "def", "_mst_decode", "(", "\n", "self", ",", "\n", "head_tag_representation", ":", "torch", ".", "Tensor", ",", "\n", "child_tag_representation", ":", "torch", ".", "Tensor", ",", "\n", "attended_arcs", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Decodes the head and head tag predictions using the Edmonds' Algorithm\n        for finding minimum spanning trees on directed graphs. Nodes in the\n        graph are the words in the sentence, and between each pair of nodes,\n        there is an edge in each direction, where the weight of the edge corresponds\n        to the most likely dependency label probability for that arc. The MST is\n        then generated from this directed graph.\n\n        # Parameters\n\n        head_tag_representation : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length, tag_representation_dim),\n            which will be used to generate predictions for the dependency tags\n            for the given arcs.\n        child_tag_representation : `torch.Tensor`, required\n            A tensor of shape (batch_size, sequence_length, tag_representation_dim),\n            which will be used to generate predictions for the dependency tags\n            for the given arcs.\n        attended_arcs : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length, sequence_length) used to generate\n            a distribution over attachments of a given word to all other words.\n\n        # Returns\n\n        heads : `torch.Tensor`\n            A tensor of shape (batch_size, sequence_length) representing the\n            greedily decoded heads of each word.\n        head_tags : `torch.Tensor`\n            A tensor of shape (batch_size, sequence_length) representing the\n            dependency tags of the optimally decoded heads of each word.\n        \"\"\"", "\n", "batch_size", ",", "sequence_length", ",", "tag_representation_dim", "=", "head_tag_representation", ".", "size", "(", ")", "\n", "\n", "lengths", "=", "mask", ".", "data", ".", "sum", "(", "dim", "=", "1", ")", ".", "long", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "expanded_shape", "=", "[", "batch_size", ",", "sequence_length", ",", "sequence_length", ",", "tag_representation_dim", "]", "\n", "head_tag_representation", "=", "head_tag_representation", ".", "unsqueeze", "(", "2", ")", "\n", "head_tag_representation", "=", "head_tag_representation", ".", "expand", "(", "*", "expanded_shape", ")", ".", "contiguous", "(", ")", "\n", "child_tag_representation", "=", "child_tag_representation", ".", "unsqueeze", "(", "1", ")", "\n", "child_tag_representation", "=", "child_tag_representation", ".", "expand", "(", "*", "expanded_shape", ")", ".", "contiguous", "(", ")", "\n", "# Shape (batch_size, sequence_length, sequence_length, num_head_tags)", "\n", "pairwise_head_logits", "=", "self", ".", "tag_bilinear", "(", "head_tag_representation", ",", "child_tag_representation", ")", "\n", "\n", "# Note that this log_softmax is over the tag dimension, and we don't consider pairs", "\n", "# of tags which are invalid (e.g are a pair which includes a padded element) anyway below.", "\n", "# Shape (batch, num_labels,sequence_length, sequence_length)", "\n", "normalized_pairwise_head_logits", "=", "F", ".", "log_softmax", "(", "pairwise_head_logits", ",", "dim", "=", "3", ")", ".", "permute", "(", "\n", "0", ",", "3", ",", "1", ",", "2", "\n", ")", "\n", "\n", "# Mask padded tokens, because we only want to consider actual words as heads.", "\n", "minus_inf", "=", "-", "1e8", "\n", "minus_mask", "=", "(", "1", "-", "mask", ".", "float", "(", ")", ")", "*", "minus_inf", "\n", "attended_arcs", "=", "attended_arcs", "+", "minus_mask", ".", "unsqueeze", "(", "2", ")", "+", "minus_mask", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# Shape (batch_size, sequence_length, sequence_length)", "\n", "normalized_arc_logits", "=", "F", ".", "log_softmax", "(", "attended_arcs", ",", "dim", "=", "2", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "# Shape (batch_size, num_head_tags, sequence_length, sequence_length)", "\n", "# This energy tensor expresses the following relation:", "\n", "# energy[i,j] = \"Score that i is the head of j\". In this", "\n", "# case, we have heads pointing to their children.", "\n", "batch_energy", "=", "torch", ".", "exp", "(", "\n", "normalized_arc_logits", ".", "unsqueeze", "(", "1", ")", "+", "normalized_pairwise_head_logits", "\n", ")", "\n", "return", "self", ".", "_run_mst_decoding", "(", "batch_energy", ",", "lengths", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._run_mst_decoding": [[577, 606], ["zip", "batch_energy.detach().cpu", "energy.max", "allennlp.nn.chu_liu_edmonds.decode_mst", "enumerate", "heads.append", "head_tags.append", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "scores.numpy", "instance_head_tags.append", "numpy.stack", "numpy.stack", "batch_energy.detach", "tag_ids[].item"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_run_mst_decoding", "(", "\n", "batch_energy", ":", "torch", ".", "Tensor", ",", "lengths", ":", "torch", ".", "Tensor", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "heads", "=", "[", "]", "\n", "head_tags", "=", "[", "]", "\n", "for", "energy", ",", "length", "in", "zip", "(", "batch_energy", ".", "detach", "(", ")", ".", "cpu", "(", ")", ",", "lengths", ")", ":", "\n", "            ", "scores", ",", "tag_ids", "=", "energy", ".", "max", "(", "dim", "=", "0", ")", "\n", "# Although we need to include the root node so that the MST includes it,", "\n", "# we do not want any word to be the parent of the root node.", "\n", "# Here, we enforce this by setting the scores for all word -> ROOT edges", "\n", "# edges to be 0.", "\n", "scores", "[", "0", ",", ":", "]", "=", "0", "\n", "# Decode the heads. Because we modify the scores to prevent", "\n", "# adding in word -> ROOT edges, we need to find the labels ourselves.", "\n", "instance_heads", ",", "_", "=", "decode_mst", "(", "scores", ".", "numpy", "(", ")", ",", "length", ",", "has_labels", "=", "False", ")", "\n", "\n", "# Find the labels which correspond to the edges in the max spanning tree.", "\n", "instance_head_tags", "=", "[", "]", "\n", "for", "child", ",", "parent", "in", "enumerate", "(", "instance_heads", ")", ":", "\n", "                ", "instance_head_tags", ".", "append", "(", "tag_ids", "[", "parent", ",", "child", "]", ".", "item", "(", ")", ")", "\n", "# We don't care what the head or tag is for the root token, but by default it's", "\n", "# not necesarily the same in the batched vs unbatched case, which is annoying.", "\n", "# Here we'll just set them to zero.", "\n", "", "instance_heads", "[", "0", "]", "=", "0", "\n", "instance_head_tags", "[", "0", "]", "=", "0", "\n", "heads", ".", "append", "(", "instance_heads", ")", "\n", "head_tags", ".", "append", "(", "instance_head_tags", ")", "\n", "", "return", "torch", ".", "from_numpy", "(", "numpy", ".", "stack", "(", "heads", ")", ")", ",", "torch", ".", "from_numpy", "(", "numpy", ".", "stack", "(", "head_tags", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._get_head_tags": [[607, 660], ["head_tag_representation.size", "allennlp.nn.util.get_range_vector().unsqueeze", "selected_head_tag_representations.contiguous.contiguous.contiguous", "biaffine_dependency_parser.BiaffineDependencyParser.tag_bilinear", "allennlp.nn.util.get_range_vector", "allennlp.nn.util.get_device_of"], "methods", ["None"], ["", "def", "_get_head_tags", "(", "\n", "self", ",", "\n", "head_tag_representation", ":", "torch", ".", "Tensor", ",", "\n", "child_tag_representation", ":", "torch", ".", "Tensor", ",", "\n", "head_indices", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Decodes the head tags given the head and child tag representations\n        and a tensor of head indices to compute tags for. Note that these are\n        either gold or predicted heads, depending on whether this function is\n        being called to compute the loss, or if it's being called during inference.\n\n        # Parameters\n\n        head_tag_representation : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length, tag_representation_dim),\n            which will be used to generate predictions for the dependency tags\n            for the given arcs.\n        child_tag_representation : `torch.Tensor`, required\n            A tensor of shape (batch_size, sequence_length, tag_representation_dim),\n            which will be used to generate predictions for the dependency tags\n            for the given arcs.\n        head_indices : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length). The indices of the heads\n            for every word.\n\n        # Returns\n\n        head_tag_logits : `torch.Tensor`\n            A tensor of shape (batch_size, sequence_length, num_head_tags),\n            representing logits for predicting a distribution over tags\n            for each arc.\n        \"\"\"", "\n", "batch_size", "=", "head_tag_representation", ".", "size", "(", "0", ")", "\n", "# shape (batch_size,)", "\n", "range_vector", "=", "get_range_vector", "(", "\n", "batch_size", ",", "get_device_of", "(", "head_tag_representation", ")", "\n", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# This next statement is quite a complex piece of indexing, which you really", "\n", "# need to read the docs to understand. See here:", "\n", "# https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html#advanced-indexing", "\n", "# In effect, we are selecting the indices corresponding to the heads of each word from the", "\n", "# sequence length dimension for each element in the batch.", "\n", "\n", "# shape (batch_size, sequence_length, tag_representation_dim)", "\n", "selected_head_tag_representations", "=", "head_tag_representation", "[", "range_vector", ",", "head_indices", "]", "\n", "selected_head_tag_representations", "=", "selected_head_tag_representations", ".", "contiguous", "(", ")", "\n", "# shape (batch_size, sequence_length, num_head_tags)", "\n", "head_tag_logits", "=", "self", ".", "tag_bilinear", "(", "\n", "selected_head_tag_representations", ",", "child_tag_representation", "\n", ")", "\n", "return", "head_tag_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._get_mask_for_eval": [[661, 686], ["mask.detach", "pos_tags.eq().long", "pos_tags.eq"], "methods", ["None"], ["", "def", "_get_mask_for_eval", "(", "\n", "self", ",", "mask", ":", "torch", ".", "LongTensor", ",", "pos_tags", ":", "torch", ".", "LongTensor", "\n", ")", "->", "torch", ".", "LongTensor", ":", "\n", "        ", "\"\"\"\n        Dependency evaluation excludes words are punctuation.\n        Here, we create a new mask to exclude word indices which\n        have a \"punctuation-like\" part of speech tag.\n\n        # Parameters\n\n        mask : `torch.LongTensor`, required.\n            The original mask.\n        pos_tags : `torch.LongTensor`, required.\n            The pos tags for the sequence.\n\n        # Returns\n\n        A new mask, where any indices equal to labels\n        we should be ignoring are masked.\n        \"\"\"", "\n", "new_mask", "=", "mask", ".", "detach", "(", ")", "\n", "for", "label", "in", "self", ".", "_pos_to_ignore", ":", "\n", "            ", "label_mask", "=", "pos_tags", ".", "eq", "(", "label", ")", ".", "long", "(", ")", "\n", "new_mask", "=", "new_mask", "*", "(", "1", "-", "label_mask", ")", "\n", "", "return", "new_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser.get_metrics": [[687, 690], ["biaffine_dependency_parser.BiaffineDependencyParser._attachment_scores.get_metric"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.training.attachment_scores_cuda.AttachmentScoresCuda.get_metric"], ["", "@", "overrides", "\n", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "return", "self", ".", "_attachment_scores", ".", "get_metric", "(", "reset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.biaffine_dependency_parser.BiaffineDependencyParser._load": [[691, 777], ["os.path.join", "config.get", "config.get.pop_choice", "allennlp.data.Vocabulary.resolve_class_name", "vocab_class.from_files", "config.get", "config.get", "allennlp.models.model.Model.from_params", "amp.initialize.text_field_embedder._token_embedders[].weight.clone", "amp.initialize.extend_embedder_vocab", "torch.load", "torch.load", "torch.load", "torch.load", "amp.initialize.load_state_dict", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "os.path.join", "allennlp.common.params.Params", "allennlp.data.Vocabulary.list_available", "config.get.get", "config.get.get", "allennlp.common.params.Params", "config.get.get", "amp.initialize.cuda", "amp.initialize.cpu", "amp.initialize.cuda", "amp.initialize.cpu", "logger.warning", "logger.warning", "amp.initialize", "allennlp.nn.util.device_mapping"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "_load", "(", "\n", "cls", ",", "\n", "config", ":", "Params", ",", "\n", "serialization_dir", ":", "str", ",", "\n", "weights_file", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "cuda_device", ":", "int", "=", "-", "1", ",", "\n", "opt_level", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", ")", "->", "\"Model\"", ":", "\n", "        ", "\"\"\"\n        Instantiates an already-trained model, based on the experiment\n        configuration and some optional overrides.\n        \"\"\"", "\n", "weights_file", "=", "weights_file", "or", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "_DEFAULT_WEIGHTS", ")", "\n", "\n", "# Load vocabulary from file", "\n", "vocab_dir", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"vocabulary\"", ")", "\n", "# If the config specifies a vocabulary subclass, we need to use it.", "\n", "\n", "vocab_params", "=", "config", ".", "get", "(", "\"vocabulary\"", ",", "Params", "(", "{", "}", ")", ")", "\n", "vocab_choice", "=", "vocab_params", ".", "pop_choice", "(", "\"type\"", ",", "Vocabulary", ".", "list_available", "(", ")", ",", "True", ")", "\n", "vocab_class", ",", "_", "=", "Vocabulary", ".", "resolve_class_name", "(", "vocab_choice", ")", "\n", "vocab", "=", "vocab_class", ".", "from_files", "(", "\n", "vocab_dir", ",", "vocab_params", ".", "get", "(", "\"padding_token\"", ")", ",", "vocab_params", ".", "get", "(", "\"oov_token\"", ")", "\n", ")", "\n", "\n", "model_params", "=", "config", ".", "get", "(", "\"model\"", ")", "\n", "\n", "training_params", "=", "config", ".", "get", "(", "\"trainer\"", ",", "Params", "(", "{", "}", ")", ")", "\n", "opt_level", "=", "opt_level", "or", "training_params", ".", "get", "(", "\"opt_level\"", ")", "\n", "\n", "# remove_pretrained_embedding_params(model_params)", "\n", "model", "=", "Model", ".", "from_params", "(", "vocab", "=", "vocab", ",", "params", "=", "model_params", ")", "\n", "# model._attachment_scores = AttachmentScores()", "\n", "weight", "=", "model", ".", "text_field_embedder", ".", "_token_embedders", "[", "'tokens'", "]", ".", "weight", ".", "clone", "(", ")", "\n", "# Force model to cpu or gpu, as appropriate, to make sure that the embeddings are", "\n", "# in sync with the weights", "\n", "if", "cuda_device", ">=", "0", ":", "\n", "            ", "model", ".", "cuda", "(", "cuda_device", ")", "\n", "", "else", ":", "\n", "            ", "model", ".", "cpu", "(", ")", "\n", "\n", "# If opt_level is not None (i.e. it exists in the loaded models params or was provided", "\n", "# as argument to this method), call amp.initialize on the loaded model.", "\n", "# Log a warning if amp is not installed or we are loading onto the cpu so that these", "\n", "# cases do not pass silently.", "\n", "", "if", "opt_level", "is", "not", "None", ":", "\n", "            ", "if", "amp", "is", "None", ":", "\n", "                ", "logger", ".", "warning", "(", "\n", "(", "\n", "f\"Apex must be installed to enable mixed-precision via amp.\"", "\n", "f\" Got opt_level is not None (opt_level={opt_level}) but Apex is not installed.\"", "\n", "\" Any further training or inference will happen at full-precision.\"", "\n", ")", "\n", ")", "\n", "", "if", "cuda_device", "==", "-", "1", ":", "\n", "                ", "logger", ".", "warning", "(", "\n", "(", "\n", "f\"A CUDA device must be specified to enable mixed-precision via amp.\"", "\n", "f\" Got cuda_device=={cuda_device} but opt_level is not None (opt_level={opt_level}).\"", "\n", "\" Any further training or inference will happen at full-precision.\"", "\n", ")", "\n", ")", "\n", "", "if", "amp", "is", "not", "None", "and", "cuda_device", ">=", "0", ":", "\n", "                ", "model", "=", "amp", ".", "initialize", "(", "model", ",", "opt_level", "=", "opt_level", ")", "\n", "\n", "\n", "# If vocab+embedding extension was done, the model initialized from from_params", "\n", "# and one defined by state dict in weights_file might not have same embedding shapes.", "\n", "# Eg. when model embedder module was transferred along with vocab extension, the", "\n", "# initialized embedding weight shape would be smaller than one in the state_dict.", "\n", "# So calling model embedding extension is required before load_state_dict.", "\n", "# If vocab and model embeddings are in sync, following would be just a no-op.", "\n", "", "", "model", ".", "extend_embedder_vocab", "(", ")", "\n", "\n", "model_state", "=", "torch", ".", "load", "(", "weights_file", ",", "map_location", "=", "util", ".", "device_mapping", "(", "cuda_device", ")", ")", "\n", "model", ".", "load_state_dict", "(", "model_state", ")", "\n", "\n", "model", ".", "text_field_embedder", ".", "_token_embedders", "[", "'tokens'", "]", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "weight", ",", "requires_grad", "=", "False", ")", "\n", "\n", "if", "cuda_device", ">=", "0", ":", "\n", "            ", "model", ".", "cuda", "(", "cuda_device", ")", "\n", "", "else", ":", "\n", "            ", "model", ".", "cpu", "(", ")", "\n", "\n", "", "return", "model", "\n", "", "", ""]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.data.bert_token_sen.get_sentence": [[15, 27], ["len", "bert_token_sen.find_pound_key", "sentence.append", "tokens[].strip"], "function", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.find_pound_key"], ["def", "get_sentence", "(", "tokens", ")", ":", "\n", "    ", "sentence", "=", "[", "]", "\n", "length", "=", "len", "(", "tokens", ")", "\n", "i", "=", "0", "\n", "while", "(", "i", "<", "length", ")", ":", "\n", "        ", "index", "=", "[", "i", "]", "\n", "index", "=", "find_pound_key", "(", "tokens", ",", "i", ",", "index", ")", "\n", "i", "=", "index", "[", "-", "1", "]", "+", "1", "\n", "word", "=", "[", "tokens", "[", "j", "]", ".", "strip", "(", "\"##\"", ")", "for", "j", "in", "index", "]", "\n", "word", "=", "\"\"", ".", "join", "(", "word", ")", "\n", "sentence", ".", "append", "(", "word", ")", "\n", "", "return", "sentence", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.data.bert_token_sen.find_pound_key": [[28, 36], ["index.append", "bert_token_sen.find_pound_key", "len"], "function", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.find_pound_key"], ["", "def", "find_pound_key", "(", "tokens", ",", "i", ",", "index", ")", ":", "\n", "    ", "if", "i", "==", "len", "(", "tokens", ")", "-", "1", ":", "\n", "        ", "return", "index", "\n", "", "if", "\"##\"", "not", "in", "tokens", "[", "i", "+", "1", "]", ":", "\n", "        ", "return", "index", "\n", "", "if", "\"##\"", "in", "tokens", "[", "i", "+", "1", "]", ":", "\n", "        ", "index", ".", "append", "(", "i", "+", "1", ")", "\n", "return", "find_pound_key", "(", "tokens", ",", "i", "+", "1", ",", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.data.bert_token_sen.get_tokenizer": [[37, 55], ["transformers.BertTokenizer.from_pretrained", "transformers.BertTokenizer.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "transformers.BertTokenizer.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoTokenizer.from_pretrained"], "function", ["None"], ["", "", "def", "get_tokenizer", "(", "language", ")", ":", "\n", "    ", "assert", "language", "in", "[", "\"en\"", ",", "\"fi\"", ",", "\"el\"", ",", "\"es\"", ",", "\"pl\"", ",", "\"ro\"", ",", "\"pt\"", "]", "\n", "if", "language", "==", "\"en\"", ":", "\n", "        ", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", "\n", "", "elif", "language", "==", "\"fi\"", ":", "\n", "        ", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\"TurkuNLP/bert-base-finnish-uncased-v1\"", ")", "\n", "", "elif", "language", "==", "\"el\"", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\"nlpaueb/bert-base-greek-uncased-v1\"", ")", "\n", "", "elif", "language", "==", "\"es\"", ":", "\n", "        ", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\"dccuchile/bert-base-spanish-wwm-uncased\"", ")", "\n", "", "elif", "language", "==", "\"pl\"", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\"dkleczek/bert-base-polish-uncased-v1\"", ")", "\n", "", "elif", "language", "==", "\"ro\"", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\"dumitrescustefan/bert-base-romanian-uncased-v1\"", ",", "do_lower_case", "=", "True", ")", "\n", "", "elif", "language", "==", "\"pt\"", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\"neuralmind/bert-base-portuguese-cased\"", ",", "do_lower_case", "=", "True", ")", "\n", "\n", "", "return", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.dataset_readers.universal_dependencies.UniversalDependenciesDatasetReader.__init__": [[34, 45], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.evaluator.evaluator.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "use_language_specific_pos", ":", "bool", "=", "False", ",", "\n", "tokenizer", ":", "Tokenizer", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "use_language_specific_pos", "=", "use_language_specific_pos", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.dataset_readers.universal_dependencies.UniversalDependenciesDatasetReader._read": [[46, 70], ["allennlp.common.file_utils.cached_path", "open", "logger.info", "conllu.parse_incr", "universal_dependencies.UniversalDependenciesDatasetReader.text_to_instance", "isinstance", "list", "zip"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.dataset_readers.universal_dependencies.UniversalDependenciesDatasetReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "\n", "with", "open", "(", "file_path", ",", "\"r\"", ")", "as", "conllu_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"Reading UD instances from conllu dataset at: %s\"", ",", "file_path", ")", "\n", "\n", "for", "annotation", "in", "parse_incr", "(", "conllu_file", ")", ":", "\n", "# CoNLLU annotations sometimes add back in words that have been elided", "\n", "# in the original sentence; we remove these, as we're just predicting", "\n", "# dependencies for the original sentence.", "\n", "# We filter by integers here as elided words have a non-integer word id,", "\n", "# as parsed by the conllu python library.", "\n", "                ", "annotation", "=", "[", "x", "for", "x", "in", "annotation", "if", "isinstance", "(", "x", "[", "\"id\"", "]", ",", "int", ")", "]", "\n", "\n", "heads", "=", "[", "x", "[", "\"head\"", "]", "for", "x", "in", "annotation", "]", "\n", "tags", "=", "[", "x", "[", "\"deprel\"", "]", "for", "x", "in", "annotation", "]", "\n", "words", "=", "[", "x", "[", "\"form\"", "]", "for", "x", "in", "annotation", "]", "\n", "if", "self", ".", "use_language_specific_pos", ":", "\n", "                    ", "pos_tags", "=", "[", "x", "[", "\"xpostag\"", "]", "for", "x", "in", "annotation", "]", "\n", "", "else", ":", "\n", "                    ", "pos_tags", "=", "[", "x", "[", "\"upostag\"", "]", "for", "x", "in", "annotation", "]", "\n", "", "yield", "self", ".", "text_to_instance", "(", "words", ",", "pos_tags", ",", "list", "(", "zip", "(", "tags", ",", "heads", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.dataset_readers.universal_dependencies.UniversalDependenciesDatasetReader.text_to_instance": [[71, 118], ["allennlp.data.fields.TextField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "universal_dependencies.UniversalDependenciesDatasetReader.tokenizer.tokenize", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.tokenizers.Token"], "methods", ["None"], ["", "", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "\n", "self", ",", "# type: ignore", "\n", "words", ":", "List", "[", "str", "]", ",", "\n", "upos_tags", ":", "List", "[", "str", "]", ",", "\n", "dependencies", ":", "List", "[", "Tuple", "[", "str", ",", "int", "]", "]", "=", "None", ",", "\n", ")", "->", "Instance", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n\n        words : `List[str]`, required.\n            The words in the sentence to be encoded.\n        upos_tags : `List[str]`, required.\n            The universal dependencies POS tags for each word.\n        dependencies : `List[Tuple[str, int]]`, optional (default = None)\n            A list of  (head tag, head index) tuples. Indices are 1 indexed,\n            meaning an index of 0 corresponds to that word being the root of\n            the dependency tree.\n\n        # Returns\n\n        An instance containing words, upos tags, dependency head tags and head\n        indices as fields.\n        \"\"\"", "\n", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "\n", "if", "self", ".", "tokenizer", "is", "not", "None", ":", "\n", "            ", "tokens", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "\" \"", ".", "join", "(", "words", ")", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "=", "[", "Token", "(", "t", ")", "for", "t", "in", "words", "]", "\n", "\n", "", "text_field", "=", "TextField", "(", "tokens", ",", "self", ".", "_token_indexers", ")", "\n", "fields", "[", "\"words\"", "]", "=", "text_field", "\n", "fields", "[", "\"pos_tags\"", "]", "=", "SequenceLabelField", "(", "upos_tags", ",", "text_field", ",", "label_namespace", "=", "\"pos\"", ")", "\n", "if", "dependencies", "is", "not", "None", ":", "\n", "# We don't want to expand the label namespace with an additional dummy token, so we'll", "\n", "# always give the 'ROOT_HEAD' token a label of 'root'.", "\n", "            ", "fields", "[", "\"head_tags\"", "]", "=", "SequenceLabelField", "(", "\n", "[", "x", "[", "0", "]", "for", "x", "in", "dependencies", "]", ",", "text_field", ",", "label_namespace", "=", "\"head_tags\"", "\n", ")", "\n", "fields", "[", "\"head_indices\"", "]", "=", "SequenceLabelField", "(", "\n", "[", "x", "[", "1", "]", "for", "x", "in", "dependencies", "]", ",", "text_field", ",", "label_namespace", "=", "\"head_index_tags\"", "\n", ")", "\n", "\n", "", "fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "{", "\"words\"", ":", "words", ",", "\"pos\"", ":", "upos_tags", "}", ")", "\n", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.commands.iter_norm_evaluate.Iter_Norm_Evaluate.add_subparser": [[31, 96], ["parser.add_parser", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_mutually_exclusive_group", "parser.add_parser.add_mutually_exclusive_group.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.set_defaults"], "methods", ["None"], ["    ", "@", "overrides", "\n", "def", "add_subparser", "(", "self", ",", "parser", ":", "argparse", ".", "_SubParsersAction", ")", "->", "argparse", ".", "ArgumentParser", ":", "\n", "        ", "description", "=", "\"\"\"Evaluate the specified model + dataset\"\"\"", "\n", "subparser", "=", "parser", ".", "add_parser", "(", "\n", "self", ".", "name", ",", "description", "=", "description", ",", "help", "=", "\"Evaluate the specified model + dataset.\"", "\n", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\"archive_file\"", ",", "type", "=", "str", ",", "help", "=", "\"path to an archived trained model\"", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\n", "\"input_file\"", ",", "type", "=", "str", ",", "help", "=", "\"path to the file containing the evaluation data\"", "\n", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\"--output-file\"", ",", "type", "=", "str", ",", "help", "=", "\"path to output file\"", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\n", "\"--weights-file\"", ",", "type", "=", "str", ",", "help", "=", "\"a path that overrides which weights file to use\"", "\n", ")", "\n", "\n", "cuda_device", "=", "subparser", ".", "add_mutually_exclusive_group", "(", "required", "=", "False", ")", "\n", "cuda_device", ".", "add_argument", "(", "\n", "\"--cuda-device\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "help", "=", "\"id of GPU to use (if any)\"", "\n", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\n", "\"-o\"", ",", "\n", "\"--overrides\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"\"", ",", "\n", "help", "=", "\"a JSON structure used to override the experiment configuration\"", ",", "\n", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\n", "\"--batch-size\"", ",", "type", "=", "int", ",", "help", "=", "\"If non-empty, the batch size to use during evaluation.\"", "\n", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\n", "\"--batch-weight-key\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"\"", ",", "\n", "help", "=", "\"If non-empty, name of metric used to weight the loss on a per-batch basis.\"", ",", "\n", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\n", "\"--extend-vocab\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "default", "=", "False", ",", "\n", "help", "=", "\"if specified, we will use the instances in your new dataset to \"", "\n", "\"extend your vocabulary. If pretrained-file was used to initialize \"", "\n", "\"embedding layers, you may also need to pass --embedding-sources-mapping.\"", ",", "\n", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\n", "\"--embedding-sources-mapping\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"\"", ",", "\n", "help", "=", "\"a JSON dict defining mapping from embedding module path to embedding \"", "\n", "\"pretrained-file used during training. If not passed, and embedding needs to be \"", "\n", "\"extended, we will try to use the original file paths used during training. If \"", "\n", "\"they are not available we will use random vectors for embedding extension.\"", ",", "\n", ")", "\n", "\n", "subparser", ".", "set_defaults", "(", "func", "=", "evaluate_from_args", ")", "\n", "\n", "return", "subparser", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.commands.iter_norm_evaluate.evaluate_from_args": [[98, 167], ["logging.getLogger().setLevel", "allennlp.models.archival.load_archive", "allennlp.common.util.prepare_environment", "model.eval", "config.pop", "logger.info", "DatasetReader.from_params.read", "dataset_reader.read.index_with", "config.pop", "allennlp.data.DataLoader.from_params", "allennlp.training.util.evaluate", "logger.info", "allennlp.common.util.dump_metrics", "logging.getLogger", "logging.getLogger", "allennlp.data.dataset_readers.dataset_reader.DatasetReader.from_params", "allennlp.data.dataset_readers.dataset_reader.DatasetReader.from_params", "json.loads", "logger.info", "model.vocab.extend_from_instances", "model.extend_embedder_vocab", "config.pop", "torch.tensor", "range", "logging.getLogger", "config.pop", "logging.info", "iter_norm_evaluate.get_iter_norm_mean_eval", "iter_mean_eval.append"], "function", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.commands.iter_norm_evaluate.get_iter_norm_mean_eval"], ["", "", "def", "evaluate_from_args", "(", "args", ":", "argparse", ".", "Namespace", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "# Disable some of the more verbose logging statements", "\n", "    ", "logging", ".", "getLogger", "(", "\"allennlp.common.params\"", ")", ".", "disabled", "=", "True", "\n", "logging", ".", "getLogger", "(", "\"allennlp.nn.initializers\"", ")", ".", "disabled", "=", "True", "\n", "logging", ".", "getLogger", "(", "\"allennlp.modules.token_embedders.embedding\"", ")", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "\n", "# Load from archive", "\n", "archive", "=", "load_archive", "(", "\n", "args", ".", "archive_file", ",", "\n", "weights_file", "=", "args", ".", "weights_file", ",", "\n", "cuda_device", "=", "args", ".", "cuda_device", ",", "\n", "overrides", "=", "args", ".", "overrides", ",", "\n", ")", "\n", "config", "=", "archive", ".", "config", "\n", "prepare_environment", "(", "config", ")", "\n", "model", "=", "archive", ".", "model", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "# Load the evaluation data", "\n", "\n", "# Try to use the validation dataset reader if there is one - otherwise fall back", "\n", "# to the default dataset_reader used for both training and validation.", "\n", "validation_dataset_reader_params", "=", "config", ".", "pop", "(", "\"validation_dataset_reader\"", ",", "None", ")", "\n", "if", "validation_dataset_reader_params", "is", "not", "None", ":", "\n", "        ", "dataset_reader", "=", "DatasetReader", ".", "from_params", "(", "validation_dataset_reader_params", ")", "\n", "", "else", ":", "\n", "        ", "dataset_reader", "=", "DatasetReader", ".", "from_params", "(", "config", ".", "pop", "(", "\"dataset_reader\"", ")", ")", "\n", "", "evaluation_data_path", "=", "args", ".", "input_file", "\n", "logger", ".", "info", "(", "\"Reading evaluation data from %s\"", ",", "evaluation_data_path", ")", "\n", "instances", "=", "dataset_reader", ".", "read", "(", "evaluation_data_path", ")", "\n", "\n", "embedding_sources", "=", "(", "\n", "json", ".", "loads", "(", "args", ".", "embedding_sources_mapping", ")", "if", "args", ".", "embedding_sources_mapping", "else", "{", "}", "\n", ")", "\n", "\n", "if", "args", ".", "extend_vocab", ":", "\n", "        ", "logger", ".", "info", "(", "\"Vocabulary is being extended with test instances.\"", ")", "\n", "model", ".", "vocab", ".", "extend_from_instances", "(", "instances", "=", "instances", ")", "\n", "model", ".", "extend_embedder_vocab", "(", "embedding_sources", ")", "\n", "\n", "", "instances", ".", "index_with", "(", "model", ".", "vocab", ")", "\n", "data_loader_params", "=", "config", ".", "pop", "(", "\"validation_data_loader\"", ",", "None", ")", "\n", "if", "data_loader_params", "is", "None", ":", "\n", "        ", "data_loader_params", "=", "config", ".", "pop", "(", "\"data_loader\"", ")", "\n", "", "if", "args", ".", "batch_size", ":", "\n", "        ", "data_loader_params", "[", "\"batch_size\"", "]", "=", "args", ".", "batch_size", "\n", "", "data_loader", "=", "DataLoader", ".", "from_params", "(", "dataset", "=", "instances", ",", "params", "=", "data_loader_params", ")", "\n", "\n", "iter_num", "=", "model", ".", "text_field_embedder", ".", "_token_embedders", "[", "'tokens'", "]", ".", "iter_norm", "\n", "if", "iter_num", ":", "\n", "# Obtrain evaluation info for iterative normalization:", "\n", "        ", "iter_mean_eval", "=", "[", "]", "\n", "mean", "=", "torch", ".", "tensor", "(", "[", "0.", "]", ",", "device", "=", "args", ".", "cuda_device", ")", "#Initialize the embedding mean as 0", "\n", "for", "iter_norm_i", "in", "range", "(", "iter_num", ")", ":", "\n", "            ", "logging", ".", "info", "(", "\"This is the {} time during iterative normalization for evaluation\"", ".", "format", "(", "iter_norm_i", ")", ")", "\n", "mean", "=", "get_iter_norm_mean_eval", "(", "model", ",", "data_loader", ",", "mean", ",", "args", ".", "cuda_device", ")", "\n", "iter_mean_eval", ".", "append", "(", "mean", ")", "\n", "\n", "", "model", ".", "text_field_embedder", ".", "_token_embedders", "[", "'tokens'", "]", ".", "iter_norm", "=", "None", "\n", "model", ".", "text_field_embedder", ".", "_token_embedders", "[", "'tokens'", "]", ".", "_matched_embedder", ".", "mean_emb_eval", "=", "iter_mean_eval", "\n", "model", ".", "text_field_embedder", ".", "_token_embedders", "[", "'tokens'", "]", ".", "_matched_embedder", ".", "is_train", "=", "False", "\n", "\n", "", "metrics", "=", "evaluate", "(", "model", ",", "data_loader", ",", "args", ".", "cuda_device", ",", "args", ".", "batch_weight_key", ")", "\n", "\n", "logger", ".", "info", "(", "\"Finished evaluating.\"", ")", "\n", "\n", "dump_metrics", "(", "args", ".", "output_file", ",", "metrics", ",", "log", "=", "True", ")", "\n", "\n", "return", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.commands.iter_norm_evaluate.get_iter_norm_mean_eval": [[168, 206], ["allennlp.common.checks.check_for_gpu", "torch.no_grad", "model.eval", "iter", "logger.info", "allennlp.common.Tqdm.tqdm", "torch.tensor", "allennlp.nn.util.move_to_device", "model.forward_embeddings", "model.forward_embeddings.sum"], "function", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.models.bert_biaffine_dependency_parser.BertBiaffineDependencyParser.forward_embeddings"], ["", "def", "get_iter_norm_mean_eval", "(", "\n", "model", ":", "Model", ",", "\n", "data_loader", ":", "DataLoader", ",", "\n", "mean", ":", "torch", ".", "Tensor", ",", "\n", "cuda_device", ":", "int", "=", "-", "1", "\n", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "    ", "\"\"\"\n    # Parameters\n\n    model : `Model`\n        The model to evaluate\n    data_loader : `DataLoader`\n        The `DataLoader` that will iterate over the evaluation data (data loaders already contain\n        their data).\n    cuda_device : `int`, optional (default=`-1`)\n        The cuda device to use for this evaluation.  The model is assumed to already be using this\n        device; this parameter is only used for moving the input data to the correct device.\n    batch_weight_key : `str`, optional (default=`None`)\n        If given, this is a key in the output dictionary for each batch that specifies how to weight\n        the loss for that batch.  If this is not given, we use a weight of 1 for every batch.\n    \"\"\"", "\n", "check_for_gpu", "(", "cuda_device", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "\n", "iterator", "=", "iter", "(", "data_loader", ")", "\n", "logger", ".", "info", "(", "\"Iterating over dataset\"", ")", "\n", "generator_tqdm", "=", "Tqdm", ".", "tqdm", "(", "iterator", ")", "\n", "\n", "mean_embeddings", ":", "[", "torch", ".", "Tensor", ",", "int", "]", "\n", "mean_embeddings", "=", "[", "torch", ".", "tensor", "(", "[", "0.", "]", ",", "device", "=", "cuda_device", ")", ",", "0", "]", "\n", "for", "batch", "in", "generator_tqdm", ":", "\n", "            ", "batch", "=", "nn_util", ".", "move_to_device", "(", "batch", ",", "cuda_device", ")", "\n", "batch_embeddings", "=", "model", ".", "forward_embeddings", "(", "batch", "[", "'words'", "]", ",", "mean", ")", "\n", "mean_embeddings", "[", "0", "]", "=", "(", "mean_embeddings", "[", "0", "]", "+", "batch_embeddings", ".", "sum", "(", "dim", "=", "0", ")", ")", "/", "(", "mean_embeddings", "[", "1", "]", "+", "batch_embeddings", ".", "shape", "[", "0", "]", ")", "\n", "mean_embeddings", "[", "1", "]", "+=", "batch_embeddings", ".", "shape", "[", "0", "]", "\n", "\n", "", "", "return", "mean_embeddings", "[", "0", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.get_mapper.Mapper.__init__": [[25, 48], ["get_mapper.Mapper.reading_file"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.get_mapper.Mapper.reading_file"], ["    ", "def", "__init__", "(", "self", ",", "input_file", ",", "ifiter", ")", ":", "\n", "        ", "self", ".", "device", "=", "device", "\n", "self", ".", "ifiter", "=", "ifiter", "\n", "self", ".", "source", "=", "[", "]", "\n", "self", ".", "target", "=", "[", "]", "\n", "self", ".", "source_occur", "=", "[", "]", "\n", "self", ".", "entropy", "=", "[", "]", "\n", "self", ".", "source_vector", "=", "[", "]", "\n", "self", ".", "target_vector", "=", "[", "]", "\n", "self", ".", "info", "=", "None", "\n", "self", ".", "index", "=", "None", "\n", "self", ".", "occ_ratio", "=", "[", "]", "\n", "self", ".", "s_train", "=", "None", "\n", "self", ".", "t_train", "=", "None", "\n", "\n", "self", ".", "source_index", "=", "0", "\n", "self", ".", "target_index", "=", "1", "\n", "self", ".", "occur_index", "=", "2", "\n", "self", ".", "entropy_index", "=", "3", "\n", "self", ".", "source_vector_index", "=", "4", "\n", "self", ".", "target_vector_index", "=", "5", "\n", "\n", "self", ".", "reading_file", "(", "input_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.get_mapper.Mapper.reading_file": [[49, 92], ["get_mapper.Mapper.get_sort", "torch.stack().to", "torch.stack().to", "torch.stack().to", "torch.stack().to", "torch.norm().view", "torch.norm().view", "torch.norm().view", "torch.norm().view", "get_mapper.Mapper.source_vector.t", "torch.stack().to", "torch.stack().to", "torch.stack().to", "torch.stack().to", "torch.norm().view", "torch.norm().view", "torch.norm().view", "torch.norm().view", "get_mapper.Mapper.target_vector.t", "list", "open", "f.readline", "zip", "get_mapper.Mapper.iter_norm", "f.readline.split", "f.readline", "len", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "range", "int", "get_mapper.Mapper.source.append", "get_mapper.Mapper.target.append", "get_mapper.Mapper.source_occur.append", "get_mapper.Mapper.entropy.append", "numpy.array", "get_mapper.Mapper.source_vector.append", "numpy.array", "get_mapper.Mapper.target_vector.append", "len", "int", "float", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "temp.append", "tt.append", "sum", "line[].split", "line[].split", "len", "float", "float", "sum"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.get_mapper.Mapper.get_sort", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.get_mapper.Mapper.iter_norm"], ["", "def", "reading_file", "(", "self", ",", "input_file", ")", ":", "\n", "        ", "temp", "=", "[", "]", "\n", "tt", "=", "[", "]", "\n", "with", "open", "(", "input_file", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "line", "=", "f", ".", "readline", "(", ")", "\n", "while", "(", "line", ")", ":", "\n", "                ", "line", "=", "line", ".", "split", "(", "\"\\t\"", ")", "\n", "if", "int", "(", "line", "[", "self", ".", "occur_index", "]", ")", ">", "10", ":", "\n", "                    ", "self", ".", "source", ".", "append", "(", "line", "[", "self", ".", "source_index", "]", ")", "\n", "self", ".", "target", ".", "append", "(", "line", "[", "self", ".", "target_index", "]", ")", "\n", "self", ".", "source_occur", ".", "append", "(", "int", "(", "line", "[", "self", ".", "occur_index", "]", ")", ")", "\n", "self", ".", "entropy", ".", "append", "(", "float", "(", "line", "[", "self", ".", "entropy_index", "]", ")", ")", "\n", "vector_en", "=", "np", ".", "array", "(", "line", "[", "self", ".", "source_vector_index", "]", ".", "split", "(", ")", "[", ":", "768", "]", ",", "dtype", "=", "float", ")", "\n", "self", ".", "source_vector", ".", "append", "(", "torch", ".", "tensor", "(", "vector_en", ",", "dtype", "=", "torch", ".", "float", ")", ")", "\n", "vector_de", "=", "np", ".", "array", "(", "line", "[", "self", ".", "target_vector_index", "]", ".", "split", "(", ")", "[", ":", "768", "]", ",", "dtype", "=", "float", ")", "\n", "self", ".", "target_vector", ".", "append", "(", "torch", ".", "tensor", "(", "vector_de", ",", "dtype", "=", "torch", ".", "float", ")", ")", "\n", "\n", "if", "line", "[", "self", ".", "source_index", "]", "not", "in", "temp", ":", "\n", "                        ", "temp", ".", "append", "(", "line", "[", "self", ".", "source_index", "]", ")", "\n", "if", "len", "(", "tt", ")", "!=", "0", ":", "\n", "                            ", "tt", "=", "[", "t", "/", "sum", "(", "tt", ")", "for", "t", "in", "tt", "]", "\n", "", "self", ".", "occ_ratio", "+=", "tt", "\n", "tt", "=", "[", "float", "(", "line", "[", "self", ".", "occur_index", "]", ")", "]", "\n", "", "else", ":", "\n", "                        ", "tt", ".", "append", "(", "float", "(", "line", "[", "self", ".", "occur_index", "]", ")", ")", "\n", "", "", "line", "=", "f", ".", "readline", "(", ")", "\n", "\n", "", "if", "len", "(", "tt", ")", "!=", "0", ":", "\n", "                ", "tt", "=", "[", "t", "/", "sum", "(", "tt", ")", "for", "t", "in", "tt", "]", "\n", "", "self", ".", "occ_ratio", "+=", "tt", "\n", "\n", "", "self", ".", "get_sort", "(", ")", "\n", "\n", "self", ".", "source_vector", "=", "torch", ".", "stack", "(", "self", ".", "source_vector", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "source_vector", "/=", "torch", ".", "norm", "(", "self", ".", "source_vector", ",", "dim", "=", "1", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "self", ".", "source_vector", "=", "self", ".", "source_vector", ".", "t", "(", ")", "\n", "self", ".", "target_vector", "=", "torch", ".", "stack", "(", "self", ".", "target_vector", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "target_vector", "/=", "torch", ".", "norm", "(", "self", ".", "target_vector", ",", "dim", "=", "1", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "self", ".", "target_vector", "=", "self", ".", "target_vector", ".", "t", "(", ")", "\n", "self", ".", "info", "=", "list", "(", "zip", "(", "self", ".", "source", ",", "self", ".", "target", ",", "range", "(", "len", "(", "self", ".", "source", ")", ")", ")", ")", "\n", "\n", "if", "self", ".", "ifiter", ":", "\n", "            ", "self", ".", "iter_norm", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.get_mapper.Mapper.simple_procrustes": [[93, 100], ["torch.svd", "torch.svd", "torch.svd", "torch.svd", "V.t", "source.t"], "methods", ["None"], ["", "", "def", "simple_procrustes", "(", "self", ")", ":", "\n", "        ", "source", "=", "self", ".", "source_vector", "\n", "target", "=", "self", ".", "target_vector", "\n", "U", ",", "_", ",", "V", "=", "torch", ".", "svd", "(", "target", "@", "source", ".", "t", "(", ")", ")", "\n", "W", "=", "U", "@", "V", ".", "t", "(", ")", "\n", "aligned", "=", "W", "@", "self", ".", "source_vector", "\n", "return", "W", ",", "aligned", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.get_mapper.Mapper.get_sort": [[101, 111], ["list", "list.sort", "list", "list", "list", "list", "list", "list", "list", "zip", "map", "map", "map", "map", "map", "map", "map"], "methods", ["None"], ["", "def", "get_sort", "(", "self", ")", ":", "\n", "        ", "combine", "=", "list", "(", "zip", "(", "self", ".", "source", ",", "self", ".", "target", ",", "self", ".", "source_vector", ",", "self", ".", "target_vector", ",", "self", ".", "occ_ratio", ",", "self", ".", "entropy", ",", "self", ".", "source_occur", ")", ")", "\n", "combine", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "-", "1", "]", ",", "reverse", "=", "True", ")", "\n", "self", ".", "source", "=", "list", "(", "map", "(", "lambda", "x", ":", "x", "[", "0", "]", ",", "combine", ")", ")", "\n", "self", ".", "target", "=", "list", "(", "map", "(", "lambda", "x", ":", "x", "[", "1", "]", ",", "combine", ")", ")", "\n", "self", ".", "source_vector", "=", "list", "(", "map", "(", "lambda", "x", ":", "x", "[", "2", "]", ",", "combine", ")", ")", "\n", "self", ".", "target_vector", "=", "list", "(", "map", "(", "lambda", "x", ":", "x", "[", "3", "]", ",", "combine", ")", ")", "\n", "self", ".", "occ_ratio", "=", "list", "(", "map", "(", "lambda", "x", ":", "x", "[", "4", "]", ",", "combine", ")", ")", "\n", "self", ".", "entropy", "=", "list", "(", "map", "(", "lambda", "x", ":", "x", "[", "5", "]", ",", "combine", ")", ")", "\n", "self", ".", "source_occur", "=", "list", "(", "map", "(", "lambda", "x", ":", "x", "[", "-", "1", "]", ",", "combine", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.get_mapper.Mapper.iter_norm": [[112, 120], ["get_mapper.Mapper.salient_anisotropy", "range", "get_mapper.Mapper.source_vector.mean().view", "get_mapper.Mapper.target_vector.mean().view", "torch.norm().view", "torch.norm().view", "torch.norm().view", "torch.norm().view", "torch.norm().view", "torch.norm().view", "torch.norm().view", "torch.norm().view", "get_mapper.Mapper.salient_anisotropy", "get_mapper.Mapper.source_vector.mean", "get_mapper.Mapper.target_vector.mean", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.get_mapper.Mapper.salient_anisotropy", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.get_mapper.Mapper.salient_anisotropy"], ["", "def", "iter_norm", "(", "self", ",", "num", "=", "5", ")", ":", "\n", "        ", "self", ".", "salient_anisotropy", "(", ")", "\n", "for", "_", "in", "range", "(", "num", ")", ":", "\n", "            ", "self", ".", "source_vector", "-=", "self", ".", "source_vector", ".", "mean", "(", "dim", "=", "1", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "self", ".", "target_vector", "-=", "self", ".", "target_vector", ".", "mean", "(", "dim", "=", "1", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "self", ".", "source_vector", "/=", "torch", ".", "norm", "(", "self", ".", "source_vector", ",", "dim", "=", "0", ")", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "self", ".", "target_vector", "/=", "torch", ".", "norm", "(", "self", ".", "target_vector", ",", "dim", "=", "0", ")", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "self", ".", "salient_anisotropy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.get_mapper.Mapper.salient_anisotropy": [[121, 126], ["get_mapper.random_indice_generator", "get_mapper.degree_anisotropy", "get_mapper.degree_anisotropy", "print", "float", "float"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.get_mapper.random_indice_generator", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.get_mapper.degree_anisotropy", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.get_mapper.degree_anisotropy"], ["", "", "def", "salient_anisotropy", "(", "self", ")", ":", "\n", "            ", "index", "=", "random_indice_generator", "(", "1000", ",", "self", ".", "source_vector", ".", "shape", "[", "1", "]", ")", "\n", "an_s", "=", "degree_anisotropy", "(", "self", ".", "source_vector", ",", "index", ")", "\n", "an_t", "=", "degree_anisotropy", "(", "self", ".", "target_vector", ",", "index", ")", "\n", "print", "(", "\"anisotriopy for source lang is {}, for taget lang is {} \"", ".", "format", "(", "float", "(", "an_s", ")", ",", "float", "(", "an_t", ")", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.get_mapper.random_indice_generator": [[8, 13], ["list", "torch.tensor", "torch.tensor", "range", "random.sample"], "function", ["None"], ["def", "random_indice_generator", "(", "num", ",", "maxnum", ")", ":", "\n", "    ", "total", "=", "list", "(", "range", "(", "maxnum", ")", ")", "\n", "index", "=", "torch", ".", "tensor", "(", "random", ".", "sample", "(", "total", ",", "num", ")", ")", "\n", "\n", "return", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.get_mapper.degree_anisotropy": [[14, 22], ["torch.triu", "torch.triu", "torch.triu().t", "torch.triu().t", "torch.where", "torch.where", "torch.mean", "torch.mean", "torch.where.t", "torch.triu", "torch.triu", "torch.ones", "torch.ones"], "function", ["None"], ["", "def", "degree_anisotropy", "(", "vectors", ",", "index", ")", ":", "\n", "    ", "vectors", "=", "torch", ".", "triu", "(", "vectors", ".", "t", "(", ")", "@", "vectors", ",", "1", ")", "\n", "filter_st", "=", "torch", ".", "triu", "(", "-", "100", "*", "torch", ".", "ones", "(", "vectors", ".", "shape", "[", "1", "]", ",", "vectors", ".", "shape", "[", "1", "]", ")", ")", ".", "t", "(", ")", "\n", "vectors", "=", "torch", ".", "where", "(", "filter_st", "==", "-", "100", ",", "filter_st", ",", "vectors", ")", "\n", "ind", "=", "(", "vectors", "!=", "-", "100", ")", ".", "nonzero", "(", ")", "\n", "vectors", "=", "vectors", "[", "ind", "[", ":", ",", "0", "]", ",", "ind", "[", ":", ",", "1", "]", "]", "\n", "\n", "return", "torch", ".", "mean", "(", "vectors", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.get_mapping.main": [[7, 30], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "get_mapper.Mapper", "get_mapper.Mapper.simple_procrustes", "torch.save", "get_mapper.degree_anisotropy", "get_mapper.degree_anisotropy", "print", "float", "float", "parser.parse_args.input.split"], "function", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.get_mapper.Mapper.simple_procrustes", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.get_mapper.degree_anisotropy", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.get_mapper.degree_anisotropy"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "## Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--input\"", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--output\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--iter_norm\"", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--anisotropy\"", ",", "action", "=", "\"store_true\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "flag", "=", "args", ".", "iter_norm", "\n", "\n", "trainer", "=", "Mapper", "(", "args", ".", "input", ",", "flag", ")", "\n", "\n", "if", "args", ".", "anisotropy", ":", "\n", "        ", "an_s", "=", "degree_anisotropy", "(", "trainer", ".", "source_vector", ")", "\n", "an_t", "=", "degree_anisotropy", "(", "trainer", ".", "target_vector", ")", "\n", "print", "(", "\"anisotriopy for source lang is {}, for taget lang is {} \"", ".", "format", "(", "float", "(", "an_s", ")", ",", "float", "(", "an_t", ")", ")", ")", "\n", "return", "\n", "\n", "", "W", ",", "aligned", "=", "trainer", ".", "simple_procrustes", "(", ")", "\n", "\n", "torch", ".", "save", "(", "W", ",", "args", ".", "output", "+", "args", ".", "input", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "+", "\".th\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.cluster_vector.get_muti_mean_vector": [[19, 46], ["sklearn.cluster.KMeans().fit", "range", "numpy.vstack", "numpy.vstack", "collections.Counter", "collections.Counter.most_common", "cluster_entropy.append", "list", "list", "numpy.vstack().mean", "numpy.vstack().mean", "np.vstack.append", "np.vstack.append", "cluster_label.append", "sklearn.cluster.KMeans", "scipy.stats.entropy", "map", "map", "enumerate", "list", "enumerate", "numpy.vstack", "numpy.vstack", "map"], "function", ["None"], ["def", "get_muti_mean_vector", "(", "vectors_src_all", ",", "vectors", ",", "labels", ",", "cluster_num", ")", ":", "\n", "    ", "kmean", "=", "KMeans", "(", "n_clusters", "=", "cluster_num", ")", ".", "fit", "(", "vectors_src_all", ")", "\n", "# centers = kmean.cluster_centers_", "\n", "cluster_src", "=", "[", "]", "\n", "cluster_tgt", "=", "[", "]", "\n", "cluster_label", "=", "[", "]", "\n", "cluster_entropy", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "cluster_num", ")", ":", "\n", "        ", "cluster_temp", "=", "[", "labels", "[", "j", "]", "for", "j", ",", "l", "in", "enumerate", "(", "kmean", ".", "labels_", ")", "if", "l", "==", "i", "]", "\n", "count", "=", "Counter", "(", "cluster_temp", ")", "\n", "label_rank", "=", "count", ".", "most_common", "(", ")", "\n", "cluster_entropy", ".", "append", "(", "entropy", "(", "list", "(", "map", "(", "lambda", "x", ":", "x", "[", "1", "]", ",", "label_rank", ")", ")", ",", "base", "=", "2", ")", ")", "\n", "label", "=", "label_rank", "[", "0", "]", "\n", "\n", "vectors_temp", "=", "[", "v", "for", "j", ",", "v", "in", "enumerate", "(", "vectors", ")", "if", "kmean", ".", "labels_", "[", "j", "]", "==", "i", "]", "\n", "vectors_src", "=", "list", "(", "map", "(", "lambda", "x", ":", "x", "[", "1", "]", ",", "vectors_temp", ")", ")", "\n", "vectors_tgt", "=", "list", "(", "map", "(", "lambda", "x", ":", "x", "[", "2", "]", ",", "vectors_temp", ")", ")", "\n", "vectors_src", "=", "np", ".", "vstack", "(", "vectors_src", ")", ".", "mean", "(", "axis", "=", "0", ")", "\n", "vectors_tgt", "=", "np", ".", "vstack", "(", "vectors_tgt", ")", ".", "mean", "(", "axis", "=", "0", ")", "\n", "cluster_src", ".", "append", "(", "vectors_src", ")", "\n", "cluster_tgt", ".", "append", "(", "vectors_tgt", ")", "\n", "cluster_label", ".", "append", "(", "label", ")", "\n", "\n", "", "cluster_src", "=", "np", ".", "vstack", "(", "cluster_src", ")", "\n", "cluster_tgt", "=", "np", ".", "vstack", "(", "cluster_tgt", ")", "\n", "\n", "return", "cluster_src", ",", "cluster_tgt", ",", "cluster_label", ",", "cluster_entropy", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.cluster_vector.get_mean_vector": [[47, 61], ["collections.Counter", "collections.Counter.most_common", "scipy.stats.entropy", "list", "list", "list", "map", "map", "numpy.vstack().mean", "numpy.vstack().mean", "map", "numpy.vstack", "numpy.vstack"], "function", ["None"], ["", "def", "get_mean_vector", "(", "vectors", ",", "labels", ")", ":", "\n", "    ", "count", "=", "Counter", "(", "labels", ")", "\n", "label_rank", "=", "count", ".", "most_common", "(", ")", "\n", "cluster_entropy", "=", "entropy", "(", "list", "(", "map", "(", "lambda", "x", ":", "x", "[", "1", "]", ",", "label_rank", ")", ")", ",", "base", "=", "2", ")", "\n", "\n", "label", "=", "label_rank", "[", "0", "]", "\n", "vectors_src", "=", "list", "(", "map", "(", "lambda", "x", ":", "x", "[", "1", "]", ",", "vectors", ")", ")", "\n", "vectors_tgt", "=", "list", "(", "map", "(", "lambda", "x", ":", "x", "[", "2", "]", ",", "vectors", ")", ")", "\n", "cluster_src", "=", "np", ".", "vstack", "(", "vectors_src", ")", ".", "mean", "(", "axis", "=", "0", ")", "[", "np", ".", "newaxis", ",", ":", "]", "\n", "cluster_tgt", "=", "np", ".", "vstack", "(", "vectors_tgt", ")", ".", "mean", "(", "axis", "=", "0", ")", "[", "np", ".", "newaxis", ",", ":", "]", "\n", "\n", "cluster_label", "=", "[", "label", "]", "\n", "cluster_entropy", "=", "[", "cluster_entropy", "]", "\n", "return", "cluster_src", ",", "cluster_tgt", ",", "cluster_label", ",", "cluster_entropy", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.cluster_vector.write_file": [[62, 70], ["open", "fcntl.flock", "range", "fcntl.flock", "len", "f.writelines", "numpy.savetxt", "numpy.savetxt", "str", "str"], "function", ["None"], ["", "def", "write_file", "(", "cluster_src", ",", "cluster_tgt", ",", "cluster_label", ",", "cluster_entropy", ",", "write_mode", ",", "file_name", ",", "write_file", ")", ":", "\n", "    ", "with", "open", "(", "write_file", ",", "write_mode", ")", "as", "f", ":", "\n", "        ", "fcntl", ".", "flock", "(", "f", ",", "fcntl", ".", "LOCK_EX", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "cluster_label", ")", ")", ":", "\n", "            ", "f", ".", "writelines", "(", "[", "file_name", "[", ":", "-", "4", "]", ",", "'\\t'", ",", "cluster_label", "[", "i", "]", "[", "0", "]", ",", "'\\t'", ",", "str", "(", "cluster_label", "[", "i", "]", "[", "1", "]", ")", ",", "'\\t'", ",", "str", "(", "cluster_entropy", "[", "i", "]", ")", ",", "'\\t'", "]", ")", "\n", "np", ".", "savetxt", "(", "f", ",", "cluster_src", "[", "i", "]", "[", "np", ".", "newaxis", ",", ":", "]", ",", "fmt", "=", "'%.10f'", ",", "newline", "=", "\"\\t\"", ")", "\n", "np", ".", "savetxt", "(", "f", ",", "cluster_tgt", "[", "i", "]", "[", "np", ".", "newaxis", ",", ":", "]", ",", "fmt", "=", "'%.10f'", ",", "newline", "=", "\"\\n\"", ")", "\n", "", "fcntl", ".", "flock", "(", "f", ",", "fcntl", ".", "LOCK_UN", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.cluster_vector.checkspecial": [[72, 78], ["re.search"], "function", ["None"], ["", "", "def", "checkspecial", "(", "string", ")", ":", "\n", "    ", "test_str", "=", "re", ".", "search", "(", "r\"\\W\"", ",", "string", ")", "\n", "if", "test_str", "==", "None", ":", "\n", "        ", "return", "False", "\n", "", "else", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.cluster_vector.main": [[79, 146], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "os.listdir", "enumerate", "print", "print", "print", "print", "len", "tqdm.tqdm", "len", "len", "open", "f.readlines", "cluster_vector.write_file", "st.strip", "open", "f.readline", "len", "discard_word.append", "cluster_vector.checkspecial", "cluster_vector.get_mean_vector", "numpy.vstack", "yellowbrick.cluster.KElbowVisualizer", "yellowbrick.cluster.KElbowVisualizer.fit", "f.readline.split", "labels.append", "vectors.append", "f.readline", "len", "list", "sklearn.cluster.KMeans", "none_cluster.append", "cluster_vector.get_mean_vector", "cluster_vector.get_muti_mean_vector", "map", "numpy.array", "numpy.array", "line[].split", "line[].split"], "function", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.cluster_vector.write_file", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.checkspecial", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.cluster_vector.get_mean_vector", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.cluster_vector.get_mean_vector", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.cluster_vector.get_muti_mean_vector"], ["", "", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--input_file\"", ",", "type", "=", "str", ",", "required", "=", "True", ")", "# file we are reading", "\n", "parser", ".", "add_argument", "(", "\"--write_file\"", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--stopwords\"", ",", "default", "=", "None", ",", "type", "=", "str", ")", "# stopwords file name", "\n", "parser", ".", "add_argument", "(", "\"--min_threshold\"", ",", "type", "=", "int", ",", "default", "=", "10", ")", "# cluster the word whose appearence is larger than the threshold", "\n", "parser", ".", "add_argument", "(", "\"--min_num_words\"", ",", "type", "=", "int", ",", "default", "=", "3", ")", "# discard the word whose appearence is less than the threshold", "\n", "parser", ".", "add_argument", "(", "\"--start\"", ",", "default", "=", "0", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--end\"", ",", "default", "=", "-", "1", ",", "type", "=", "int", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "\n", "if", "args", ".", "stopwords", ":", "\n", "        ", "with", "open", "(", "args", ".", "stopwords", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "stopwords", "=", "f", ".", "readlines", "(", ")", "\n", "stopwords", "=", "[", "st", ".", "strip", "(", ")", "for", "st", "in", "stopwords", "]", "\n", "", "", "else", ":", "\n", "        ", "stopwords", "=", "[", "]", "\n", "\n", "", "rootdir", "=", "args", ".", "input_file", "\n", "writefile", "=", "args", ".", "write_file", "\n", "list_dir", "=", "os", ".", "listdir", "(", "rootdir", ")", "\n", "\n", "if", "args", ".", "end", "==", "-", "1", ":", "\n", "        ", "args", ".", "end", "=", "len", "(", "list_dir", ")", "\n", "\n", "", "none_cluster", "=", "[", "]", "\n", "discard_word", "=", "[", "]", "\n", "for", "file_index", ",", "file_name", "in", "enumerate", "(", "tqdm", "(", "list_dir", ")", ")", ":", "\n", "\n", "        ", "if", "not", "(", "file_index", ">=", "args", ".", "start", "and", "file_index", "<", "args", ".", "end", ")", ":", "\n", "            ", "continue", "\n", "\n", "", "write_mode", "=", "\"a\"", "\n", "labels", "=", "[", "]", "\n", "vectors", "=", "[", "]", "\n", "\n", "if", "file_name", "[", ":", "-", "4", "]", "!=", "''", "and", "\".txt\"", "in", "file_name", ":", "\n", "            ", "with", "open", "(", "rootdir", "+", "file_name", ")", "as", "f", ":", "\n", "                ", "line", "=", "f", ".", "readline", "(", ")", "\n", "while", "(", "line", ")", ":", "\n", "                    ", "line", "=", "line", ".", "split", "(", "\"\\t\"", ")", "\n", "labels", ".", "append", "(", "line", "[", "0", "]", ")", "\n", "vectors", ".", "append", "(", "[", "line", "[", "0", "]", ",", "np", ".", "array", "(", "line", "[", "1", "]", ".", "split", "(", "\" \"", ")", ",", "dtype", "=", "'float'", ")", ",", "np", ".", "array", "(", "line", "[", "2", "]", ".", "split", "(", "\" \"", ")", ",", "dtype", "=", "'float'", ")", "]", ")", "#[[label, vector_src, vector_tgt]]", "\n", "line", "=", "f", ".", "readline", "(", ")", "\n", "\n", "", "", "if", "len", "(", "vectors", ")", "<", "args", ".", "min_num_words", ":", "\n", "                ", "discard_word", ".", "append", "(", "file_name", "[", ":", "-", "4", "]", ")", "\n", "continue", "\n", "\n", "", "if", "len", "(", "vectors", ")", "<=", "args", ".", "min_threshold", "or", "checkspecial", "(", "file_name", "[", ":", "-", "4", "]", ")", "or", "file_name", "[", ":", "-", "4", "]", "in", "stopwords", ":", "\n", "                ", "cluster_src", ",", "cluster_tgt", ",", "cluster_label", ",", "cluster_entropy", "=", "get_mean_vector", "(", "vectors", ",", "labels", ")", "\n", "", "else", ":", "\n", "                ", "vectors_src_all", "=", "np", ".", "vstack", "(", "list", "(", "map", "(", "lambda", "x", ":", "x", "[", "1", "]", ",", "vectors", ")", ")", ")", "\n", "model", "=", "KElbowVisualizer", "(", "KMeans", "(", ")", ",", "k", "=", "(", "1", ",", "8", ")", ")", "\n", "model", ".", "fit", "(", "vectors_src_all", ")", "\n", "if", "model", ".", "elbow_value_", "==", "None", ":", "\n", "                    ", "none_cluster", ".", "append", "(", "file_name", "[", ":", "-", "4", "]", ")", "\n", "cluster_src", ",", "cluster_tgt", ",", "cluster_label", ",", "cluster_entropy", "=", "get_mean_vector", "(", "vectors", ",", "labels", ")", "\n", "", "else", ":", "\n", "                    ", "cluster_src", ",", "cluster_tgt", ",", "cluster_label", ",", "cluster_entropy", "=", "get_muti_mean_vector", "(", "vectors_src_all", ",", "vectors", ",", "labels", ",", "model", ".", "elbow_value_", ")", "\n", "", "", "write_file", "(", "cluster_src", ",", "cluster_tgt", ",", "cluster_label", ",", "cluster_entropy", ",", "write_mode", ",", "file_name", ",", "writefile", ")", "\n", "\n", "", "", "print", "(", "\"Number of None in clustering:\"", ",", "len", "(", "none_cluster", ")", ")", "\n", "print", "(", "\"Number of words that have been discarded:\"", ",", "len", "(", "discard_word", ")", ")", "\n", "print", "(", "\"List of file that not been clusted:\"", ",", "none_cluster", ")", "\n", "print", "(", "\"List of words that have been discarded:\"", ",", "discard_word", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.InputExample.__init__": [[54, 58], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "unique_id", ",", "text_a", ",", "text_b", ")", ":", "\n", "        ", "self", ".", "unique_id", "=", "unique_id", "\n", "self", ".", "text_a", "=", "text_a", "\n", "self", ".", "text_b", "=", "text_b", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.InputFeatures.__init__": [[63, 69], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "unique_id", ",", "tokens", ",", "input_ids", ",", "input_mask", ",", "input_type_ids", ")", ":", "\n", "        ", "self", ".", "unique_id", "=", "unique_id", "\n", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "input_type_ids", "=", "input_type_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.convert_examples_to_features": [[71, 153], ["enumerate", "tokenizer.tokenize", "tokens.append", "input_type_ids.append", "tokens.append", "input_type_ids.append", "tokenizer.convert_tokens_to_ids", "features.append", "tokenizer.tokenize", "getwordvectorsfrombert._truncate_seq_pair", "len", "tokens.append", "input_type_ids.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "input_type_ids.append", "len", "len", "len", "getwordvectorsfrombert.InputFeatures", "len", "tokens.append", "input_type_ids.append"], "function", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert._truncate_seq_pair"], ["", "", "def", "convert_examples_to_features", "(", "examples", ",", "seq_length", ",", "tokenizer", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputFeature`s.\"\"\"", "\n", "\n", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "tokens_a", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "text_a", ")", "\n", "\n", "tokens_b", "=", "None", "\n", "if", "example", ".", "text_b", ":", "\n", "            ", "tokens_b", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "text_b", ")", "\n", "\n", "", "if", "tokens_b", ":", "\n", "# Modifies `tokens_a` and `tokens_b` in place so that the total", "\n", "# length is less than the specified length.", "\n", "# Account for [CLS], [SEP], [SEP] with \"- 3\"", "\n", "            ", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "seq_length", "-", "3", ")", "\n", "", "else", ":", "\n", "# Account for [CLS] and [SEP] with \"- 2\"", "\n", "            ", "if", "len", "(", "tokens_a", ")", ">", "seq_length", "-", "2", ":", "\n", "                ", "tokens_a", "=", "tokens_a", "[", "0", ":", "(", "seq_length", "-", "2", ")", "]", "\n", "\n", "# The convention in BERT is:", "\n", "# (a) For sequence pairs:", "\n", "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]", "\n", "#  type_ids:   0   0  0    0    0     0      0   0    1  1  1   1  1   1", "\n", "# (b) For single sequences:", "\n", "#  tokens:   [CLS] the dog is hairy . [SEP]", "\n", "#  type_ids:   0   0   0   0  0     0   0", "\n", "#", "\n", "# Where \"type_ids\" are used to indicate whether this is the first", "\n", "# sequence or the second sequence. The embedding vectors for `type=0` and", "\n", "# `type=1` were learned during pre-training and are added to the wordpiece", "\n", "# embedding vector (and position vector). This is not *strictly* necessary", "\n", "# since the [SEP] token unambigiously separates the sequences, but it makes", "\n", "# it easier for the model to learn the concept of sequences.", "\n", "#", "\n", "# For classification tasks, the first vector (corresponding to [CLS]) is", "\n", "# used as as the \"sentence vector\". Note that this only makes sense because", "\n", "# the entire model is fine-tuned.", "\n", "", "", "tokens", "=", "[", "]", "\n", "input_type_ids", "=", "[", "]", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "\n", "tokens", "+=", "tokens_a", "\n", "input_type_ids", "+=", "[", "0", "]", "*", "len", "(", "tokens_a", ")", "\n", "\n", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "\n", "if", "tokens_b", ":", "\n", "            ", "for", "token", "in", "tokens_b", ":", "\n", "                ", "tokens", ".", "append", "(", "token", ")", "\n", "input_type_ids", ".", "append", "(", "1", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "input_type_ids", ".", "append", "(", "1", ")", "\n", "\n", "", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids", ")", "<", "seq_length", ":", "\n", "            ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_type_ids", ")", "==", "seq_length", "\n", "\n", "features", ".", "append", "(", "\n", "InputFeatures", "(", "\n", "unique_id", "=", "example", ".", "unique_id", ",", "\n", "tokens", "=", "tokens", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "input_type_ids", "=", "input_type_ids", ")", ")", "\n", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert._truncate_seq_pair": [[155, 170], ["len", "len", "len", "len", "tokens_a.pop", "tokens_b.pop"], "function", ["None"], ["", "def", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_length", ")", ":", "\n", "    ", "\"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"", "\n", "\n", "# This is a simple heuristic which will always truncate the longer sequence", "\n", "# one token at a time. This makes more sense than truncating an equal percent", "\n", "# of tokens from each, since if one sequence is very short then each token", "\n", "# that's truncated likely contains more information than a longer sequence.", "\n", "while", "True", ":", "\n", "        ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_length", ":", "\n", "            ", "break", "\n", "", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "            ", "tokens_a", ".", "pop", "(", ")", "\n", "", "else", ":", "\n", "            ", "tokens_b", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.read_examples": [[172, 194], ["open", "reader.readline", "line.strip.strip", "re.match", "examples.append", "re.match.group", "re.match.group", "getwordvectorsfrombert.InputExample"], "function", ["None"], ["", "", "", "def", "read_examples", "(", "input_file", ")", ":", "\n", "    ", "\"\"\"Read a list of `InputExample`s from an input file.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "unique_id", "=", "0", "\n", "with", "open", "(", "input_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "line", "=", "reader", ".", "readline", "(", ")", "\n", "if", "not", "line", ":", "\n", "                ", "break", "\n", "", "line", "=", "line", ".", "strip", "(", ")", "\n", "text_a", "=", "None", "\n", "text_b", "=", "None", "\n", "m", "=", "re", ".", "match", "(", "r\"^(.*) \\|\\|\\| (.*)$\"", ",", "line", ")", "\n", "if", "m", "is", "None", ":", "\n", "                ", "text_a", "=", "line", "\n", "", "else", ":", "\n", "                ", "text_a", "=", "m", ".", "group", "(", "1", ")", "\n", "text_b", "=", "m", ".", "group", "(", "2", ")", "\n", "", "examples", ".", "append", "(", "\n", "InputExample", "(", "unique_id", "=", "unique_id", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ")", ")", "\n", "unique_id", "+=", "1", "\n", "", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.get_tokenizer_model": [[196, 228], ["transformers.BertTokenizer.from_pretrained", "transformers.BertConfig.from_pretrained", "transformers.BertModel.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoConfig.from_pretrained", "transformers.AutoModel.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoConfig.from_pretrained", "transformers.AutoModel.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoConfig.from_pretrained", "transformers.AutoModel.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoConfig.from_pretrained", "transformers.AutoModel.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoConfig.from_pretrained", "transformers.AutoModel.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoConfig.from_pretrained", "transformers.AutoModel.from_pretrained"], "function", ["None"], ["", "def", "get_tokenizer_model", "(", "language", ")", ":", "\n", "    ", "assert", "language", "in", "[", "\"en\"", ",", "\"fi\"", ",", "\"el\"", ",", "\"es\"", ",", "\"pl\"", ",", "\"ro\"", ",", "\"pt\"", "]", "\n", "if", "language", "==", "\"en\"", ":", "\n", "        ", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", "\n", "config", "=", "BertConfig", ".", "from_pretrained", "(", "\"bert-base-uncased\"", ",", "output_hidden_states", "=", "True", ")", "\n", "model", "=", "BertModel", ".", "from_pretrained", "(", "'bert-base-uncased'", ",", "config", "=", "config", ")", "\n", "", "elif", "language", "==", "\"fi\"", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\"TurkuNLP/bert-base-finnish-uncased-v1\"", ")", "\n", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\"TurkuNLP/bert-base-finnish-uncased-v1\"", ",", "output_hidden_states", "=", "True", ")", "\n", "model", "=", "AutoModel", ".", "from_pretrained", "(", "\"TurkuNLP/bert-base-finnish-uncased-v1\"", ",", "config", "=", "config", ")", "\n", "", "elif", "language", "==", "\"el\"", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\"nlpaueb/bert-base-greek-uncased-v1\"", ")", "\n", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\"nlpaueb/bert-base-greek-uncased-v1\"", ",", "output_hidden_states", "=", "True", ")", "\n", "model", "=", "AutoModel", ".", "from_pretrained", "(", "\"nlpaueb/bert-base-greek-uncased-v1\"", ",", "config", "=", "config", ")", "\n", "", "elif", "language", "==", "\"es\"", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\"dccuchile/bert-base-spanish-wwm-uncased\"", ")", "\n", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\"dccuchile/bert-base-spanish-wwm-uncased\"", ",", "output_hidden_states", "=", "True", ")", "\n", "model", "=", "AutoModel", ".", "from_pretrained", "(", "\"dccuchile/bert-base-spanish-wwm-uncased\"", ",", "config", "=", "config", ")", "\n", "", "elif", "language", "==", "\"pl\"", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\"dkleczek/bert-base-polish-uncased-v1\"", ")", "\n", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\"dkleczek/bert-base-polish-uncased-v1\"", ",", "output_hidden_states", "=", "True", ")", "\n", "model", "=", "AutoModel", ".", "from_pretrained", "(", "\"dkleczek/bert-base-polish-uncased-v1\"", ",", "config", "=", "config", ")", "\n", "", "elif", "language", "==", "\"ro\"", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\"dumitrescustefan/bert-base-romanian-uncased-v1\"", ",", "do_lower_case", "=", "True", ")", "\n", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\"dumitrescustefan/bert-base-romanian-uncased-v1\"", ",", "output_hidden_states", "=", "True", ")", "\n", "model", "=", "AutoModel", ".", "from_pretrained", "(", "\"dumitrescustefan/bert-base-romanian-uncased-v1\"", ",", "config", "=", "config", ")", "\n", "", "elif", "language", "==", "\"pt\"", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\"neuralmind/bert-base-portuguese-cased\"", ",", "do_lower_case", "=", "True", ")", "\n", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\"neuralmind/bert-base-portuguese-cased\"", ",", "output_hidden_states", "=", "True", ")", "\n", "model", "=", "AutoModel", ".", "from_pretrained", "(", "\"neuralmind/bert-base-portuguese-cased\"", ",", "config", "=", "config", ")", "\n", "\n", "", "return", "tokenizer", ",", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.write_word": [[232, 242], ["open", "f.writelines", "numpy.savetxt", "numpy.savetxt"], "function", ["None"], ["", "def", "write_word", "(", "token", ",", "embds_src", ",", "embds_tgt", ",", "label", ",", "writetime", ",", "file_path", ")", ":", "\n", "    ", "write_mode", "=", "\"a\"", "if", "writetime", "==", "1", "else", "\"a\"", "\n", "with", "open", "(", "file_path", "+", "token", "+", "\".txt\"", ",", "write_mode", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "embds_src", "=", "embds_src", "[", "np", ".", "newaxis", ",", ":", "]", "\n", "embds_tgt", "=", "embds_tgt", "[", "np", ".", "newaxis", ",", ":", "]", "\n", "# if writetime == 1:", "\n", "#     f.writelines([token,'\\n'])", "\n", "f", ".", "writelines", "(", "[", "label", ",", "\"\\t\"", "]", ")", "\n", "np", ".", "savetxt", "(", "f", ",", "embds_src", ",", "fmt", "=", "'%.10f'", ",", "newline", "=", "\"\\t\"", ")", "\n", "np", ".", "savetxt", "(", "f", ",", "embds_tgt", ",", "fmt", "=", "'%.10f'", ",", "newline", "=", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.get_sentence": [[244, 256], ["len", "getwordvectorsfrombert.find_pound_key", "sentence.append", "tokens[].strip"], "function", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.find_pound_key"], ["", "", "def", "get_sentence", "(", "tokens", ")", ":", "\n", "    ", "sentence", "=", "[", "]", "\n", "length", "=", "len", "(", "tokens", ")", "\n", "i", "=", "0", "\n", "while", "(", "i", "<", "length", ")", ":", "\n", "        ", "index", "=", "[", "i", "]", "\n", "index", "=", "find_pound_key", "(", "tokens", ",", "i", ",", "index", ")", "\n", "i", "=", "index", "[", "-", "1", "]", "+", "1", "\n", "word", "=", "[", "tokens", "[", "j", "]", ".", "strip", "(", "\"##\"", ")", "for", "j", "in", "index", "]", "\n", "word", "=", "\"\"", ".", "join", "(", "word", ")", "\n", "sentence", ".", "append", "(", "[", "word", ",", "index", "]", ")", "\n", "", "return", "sentence", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.find_pound_key": [[258, 267], ["index.append", "getwordvectorsfrombert.find_pound_key"], "function", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.find_pound_key"], ["", "def", "find_pound_key", "(", "tokens", ",", "i", ",", "index", ")", ":", "\n", "    ", "if", "tokens", "[", "i", "]", "==", "\"[SEP]\"", ":", "\n", "        ", "return", "index", "\n", "", "if", "\"##\"", "not", "in", "tokens", "[", "i", "+", "1", "]", ":", "\n", "        ", "return", "index", "\n", "\n", "", "if", "\"##\"", "in", "tokens", "[", "i", "+", "1", "]", ":", "\n", "        ", "index", ".", "append", "(", "i", "+", "1", ")", "\n", "return", "find_pound_key", "(", "tokens", ",", "i", "+", "1", ",", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.get_align_dict": [[269, 296], ["align.items", "set", "couple.split.split", "int", "int", "align.pop", "discard_tgt.append", "seen_tgt.append", "set.append", "set.append"], "function", ["None"], ["", "", "def", "get_align_dict", "(", "line", ")", ":", "\n", "    ", "align", "=", "{", "}", "\n", "discard", "=", "[", "]", "\n", "discard_tgt", "=", "[", "]", "\n", "seen_tgt", "=", "[", "]", "\n", "\n", "for", "couple", "in", "line", ":", "\n", "        ", "couple", "=", "couple", ".", "split", "(", "'-'", ")", "\n", "c_0", "=", "int", "(", "couple", "[", "0", "]", ")", "\n", "c_1", "=", "int", "(", "couple", "[", "1", "]", ")", "\n", "if", "c_1", "in", "seen_tgt", ":", "\n", "            ", "discard_tgt", ".", "append", "(", "c_1", ")", "\n", "", "if", "c_0", "not", "in", "align", ":", "\n", "            ", "align", "[", "c_0", "]", "=", "c_1", "\n", "seen_tgt", ".", "append", "(", "c_1", ")", "\n", "", "else", ":", "\n", "            ", "discard", ".", "append", "(", "c_0", ")", "\n", "\n", "", "", "for", "src", ",", "tgt", "in", "align", ".", "items", "(", ")", ":", "\n", "        ", "if", "tgt", "in", "discard_tgt", ":", "\n", "            ", "discard", ".", "append", "(", "src", ")", "\n", "", "", "discard", "=", "set", "(", "discard", ")", "\n", "\n", "for", "dis", "in", "discard", ":", "\n", "        ", "align", ".", "pop", "(", "dis", ")", "\n", "\n", "", "return", "align", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.checknum": [[298, 305], ["re.compile", "re.compile.findall"], "function", ["None"], ["", "def", "checknum", "(", "string", ")", ":", "\n", "    ", "pattern", "=", "re", ".", "compile", "(", "'[0-9]+'", ")", "\n", "match", "=", "pattern", ".", "findall", "(", "string", ")", "\n", "if", "match", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.checkspecial": [[307, 313], ["re.search"], "function", ["None"], ["", "", "def", "checkspecial", "(", "string", ")", ":", "\n", "    ", "test_str", "=", "re", ".", "search", "(", "r\"\\W\"", ",", "string", ")", "\n", "if", "test_str", "==", "None", ":", "\n", "        ", "return", "False", "\n", "", "else", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.if_not_in_dict": [[315, 317], ["None"], "function", ["None"], ["", "", "def", "if_not_in_dict", "(", "i", ",", "align_dict", ")", ":", "\n", "    ", "return", "True", "if", "i", "not", "in", "align_dict", "else", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.if_special_number": [[319, 321], ["getwordvectorsfrombert.checkspecial", "getwordvectorsfrombert.checknum", "len", "len"], "function", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.checkspecial", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.checknum"], ["", "def", "if_special_number", "(", "token", ")", ":", "\n", "    ", "return", "True", "if", "(", "checkspecial", "(", "token", ")", "and", "len", "(", "token", ")", ">", "1", ")", "or", "(", "checknum", "(", "token", ")", "and", "len", "(", "token", ")", ">", "1", ")", "else", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.if_max": [[323, 328], ["None"], "function", ["None"], ["", "def", "if_max", "(", "token", ",", "token_list", ",", "max_num", ")", ":", "\n", "    ", "if", "token", "in", "token_list", ":", "\n", "        ", "if", "token_list", "[", "token", "]", ">=", "max_num", ":", "\n", "            ", "return", "True", "\n", "", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.if_OOV": [[330, 335], ["tokenizer.convert_tokens_to_ids"], "function", ["None"], ["", "def", "if_OOV", "(", "token", ",", "tokenizer", ",", "oov_ind", ")", ":", "\n", "    ", "w_id", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "token", ")", "\n", "if", "w_id", "==", "oov_ind", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.if_not_meet_requirement": [[337, 347], ["getwordvectorsfrombert.if_not_in_dict", "getwordvectorsfrombert.if_special_number", "getwordvectorsfrombert.if_max", "getwordvectorsfrombert.if_OOV"], "function", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.if_not_in_dict", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.if_special_number", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.if_max", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.if_OOV"], ["", "def", "if_not_meet_requirement", "(", "i", ",", "align_dict", ",", "token", ",", "token_list", ",", "tokenizer", ",", "max_num", ",", "oov_ind", ")", ":", "\n", "    ", "if", "if_not_in_dict", "(", "i", ",", "align_dict", ")", ":", "\n", "        ", "return", "True", "\n", "", "if", "if_special_number", "(", "token", ")", ":", "\n", "        ", "return", "True", "\n", "", "if", "if_max", "(", "token", ",", "token_list", ",", "max_num", ")", ":", "\n", "        ", "return", "True", "\n", "", "if", "if_OOV", "(", "token", ",", "tokenizer", ",", "oov_ind", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.main": [[349, 487], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "torch.device", "torch.cuda.device_count", "logger.info", "getwordvectorsfrombert.get_tokenizer_model", "model_src.to", "getwordvectorsfrombert.get_tokenizer_model", "model_tgt.to", "tokenizer_src.convert_tokens_to_ids", "tokenizer_tgt.convert_tokens_to_ids", "logger.info", "getwordvectorsfrombert.read_examples", "getwordvectorsfrombert.read_examples", "logger.info", "getwordvectorsfrombert.convert_examples_to_features", "getwordvectorsfrombert.convert_examples_to_features", "torch.tensor", "torch.tensor", "torch.arange", "torch.tensor", "torch.tensor", "torch.arange", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "open", "model_src.eval", "model_tgt.eval", "torch.tensor.size", "torch.tensor.size", "torch.no_grad", "zip", "open.close", "tqdm.tqdm", "input_ids_src.to.to", "input_mask_src.to.to", "input_ids_tgt.to.to", "input_mask_tgt.to.to", "torch.stack().permute", "torch.stack().permute", "enumerate", "torch.cuda.is_available", "model_src", "model_tgt", "open.readline().split", "getwordvectorsfrombert.get_sentence", "getwordvectorsfrombert.get_sentence", "len", "getwordvectorsfrombert.get_align_dict", "enumerate", "torch.stack", "torch.stack", "getwordvectorsfrombert.if_not_meet_requirement", "torch.sum().sum().detach().cpu().numpy", "torch.sum().sum().detach().cpu().numpy", "open.readline", "example_index.item", "example_index.item", "getwordvectorsfrombert.if_OOV", "getwordvectorsfrombert.if_special_number", "getwordvectorsfrombert.write_word", "torch.sum().sum().detach().cpu", "torch.sum().sum().detach().cpu", "getwordvectorsfrombert.write_word", "torch.sum().sum().detach", "torch.sum().sum().detach", "torch.sum().sum", "torch.sum().sum", "torch.sum", "torch.sum"], "function", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.get_tokenizer_model", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.get_tokenizer_model", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.read_examples", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.read_examples", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.convert_examples_to_features", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.convert_examples_to_features", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.get_sentence", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.get_sentence", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.get_align_dict", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.if_not_meet_requirement", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.if_OOV", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.if_special_number", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.write_word", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.getwordvectorsfrombert.write_word"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "## Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--src\"", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt\"", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--open_src_file\"", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--open_tgt_file\"", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--open_align_file\"", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--write_vectors_path\"", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "\n", "\n", "## Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "default", "=", "150", ",", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. Sequences longer \"", "\n", "\"than this will be truncated, and sequences shorter than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "default", "=", "128", ",", "type", "=", "int", ",", "help", "=", "\"Batch size for predictions.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_num_word\"", ",", "type", "=", "int", ",", "default", "=", "10000", ")", "\n", "parser", ".", "add_argument", "(", "\"--debug\"", ",", "action", "=", "\"store_true\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "logger", ".", "info", "(", "\"device: {} n_gpu: {} \"", ".", "format", "(", "device", ",", "n_gpu", ")", ")", "\n", "\n", "tokenizer_src", ",", "model_src", "=", "get_tokenizer_model", "(", "args", ".", "src", ")", "\n", "model_src", ".", "to", "(", "device", ")", "\n", "\n", "tokenizer_tgt", ",", "model_tgt", "=", "get_tokenizer_model", "(", "args", ".", "tgt", ")", "\n", "model_tgt", ".", "to", "(", "device", ")", "\n", "\n", "oov_ind_src", "=", "tokenizer_src", ".", "convert_tokens_to_ids", "(", "'[UNK]'", ")", "\n", "oov_ind_tgt", "=", "tokenizer_tgt", ".", "convert_tokens_to_ids", "(", "'[UNK]'", ")", "\n", "\n", "token_list", "=", "{", "}", "\n", "\n", "if", "args", ".", "debug", ":", "\n", "        ", "args", ".", "max_num_word", "=", "2000", "\n", "args", ".", "write_vectors_path", "=", "\"./word_vectors/debug/\"", "\n", "args", ".", "open_src_file", "=", "\"./data/en_split/debug.txt\"", "\n", "args", ".", "open_tgt_file", "=", "\"./data/de_split/debug.txt\"", "\n", "args", ".", "open_align_file", "=", "\"./data/forward_align_split/debug.txt\"", "\n", "\n", "# Split large file and process them one-by-one", "\n", "\n", "", "logger", ".", "info", "(", "\"reading examples from file \"", "+", "args", ".", "open_src_file", ")", "\n", "examples_src", "=", "read_examples", "(", "args", ".", "open_src_file", ")", "\n", "examples_tgt", "=", "read_examples", "(", "args", ".", "open_tgt_file", ")", "\n", "\n", "logger", ".", "info", "(", "\"finishing reading.\"", ")", "\n", "features_src", "=", "convert_examples_to_features", "(", "\n", "examples", "=", "examples_src", ",", "seq_length", "=", "args", ".", "max_seq_length", ",", "tokenizer", "=", "tokenizer_src", ")", "\n", "features_tgt", "=", "convert_examples_to_features", "(", "\n", "examples", "=", "examples_tgt", ",", "seq_length", "=", "args", ".", "max_seq_length", ",", "tokenizer", "=", "tokenizer_tgt", ")", "\n", "\n", "unique_id_to_feature", "=", "{", "}", "\n", "for", "feature", "in", "features_src", ":", "\n", "        ", "unique_id_to_feature", "[", "feature", ".", "unique_id", "]", "=", "feature", "\n", "\n", "", "all_input_ids_src", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "features_src", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask_src", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "features_src", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_example_index_src", "=", "torch", ".", "arange", "(", "all_input_ids_src", ".", "size", "(", "0", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "all_input_ids_tgt", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "features_tgt", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask_tgt", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "features_tgt", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_example_index_tgt", "=", "torch", ".", "arange", "(", "all_input_ids_tgt", ".", "size", "(", "0", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "eval_data_src", "=", "TensorDataset", "(", "all_input_ids_src", ",", "all_input_mask_src", ",", "all_example_index_src", ")", "\n", "eval_data_tgt", "=", "TensorDataset", "(", "all_input_ids_tgt", ",", "all_input_mask_tgt", ",", "all_example_index_tgt", ")", "\n", "\n", "eval_sampler_src", "=", "SequentialSampler", "(", "eval_data_src", ")", "\n", "eval_sampler_tgt", "=", "SequentialSampler", "(", "eval_data_tgt", ")", "\n", "\n", "eval_dataloader_src", "=", "DataLoader", "(", "eval_data_src", ",", "sampler", "=", "eval_sampler_src", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "eval_dataloader_tgt", "=", "DataLoader", "(", "eval_data_tgt", ",", "sampler", "=", "eval_sampler_tgt", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "\n", "f_align", "=", "open", "(", "args", ".", "open_align_file", ")", "\n", "\n", "model_src", ".", "eval", "(", ")", "\n", "model_tgt", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "(", "input_ids_src", ",", "input_mask_src", ",", "example_indices_src", ")", ",", "(", "input_ids_tgt", ",", "input_mask_tgt", ",", "example_indices_tgt", ")", "in", "zip", "(", "tqdm", "(", "eval_dataloader_src", ")", ",", "eval_dataloader_tgt", ")", ":", "\n", "                ", "input_ids_src", "=", "input_ids_src", ".", "to", "(", "device", ")", "\n", "input_mask_src", "=", "input_mask_src", ".", "to", "(", "device", ")", "\n", "input_ids_tgt", "=", "input_ids_tgt", ".", "to", "(", "device", ")", "\n", "input_mask_tgt", "=", "input_mask_tgt", ".", "to", "(", "device", ")", "\n", "\n", "all_encoder_layers_src", "=", "model_src", "(", "input_ids_src", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "input_mask_src", ")", "[", "2", "]", "\n", "all_encoder_layers_src", "=", "torch", ".", "stack", "(", "all_encoder_layers_src", ",", "dim", "=", "0", ")", ".", "permute", "(", "1", ",", "2", ",", "0", ",", "3", ")", "\n", "\n", "all_encoder_layers_tgt", "=", "model_tgt", "(", "input_ids_tgt", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "input_mask_tgt", ")", "[", "2", "]", "\n", "all_encoder_layers_tgt", "=", "torch", ".", "stack", "(", "all_encoder_layers_tgt", ",", "dim", "=", "0", ")", ".", "permute", "(", "1", ",", "2", ",", "0", ",", "3", ")", "\n", "\n", "for", "b", ",", "example_index", "in", "enumerate", "(", "example_indices_src", ")", ":", "\n", "\n", "                    ", "line", "=", "f_align", ".", "readline", "(", ")", ".", "split", "(", ")", "\n", "\n", "feature_src", "=", "features_src", "[", "example_index", ".", "item", "(", ")", "]", "\n", "sentence_src", "=", "get_sentence", "(", "feature_src", ".", "tokens", ")", "\n", "feature_tgt", "=", "features_tgt", "[", "example_index", ".", "item", "(", ")", "]", "\n", "sentence_tgt", "=", "get_sentence", "(", "feature_tgt", ".", "tokens", ")", "\n", "length_tgt", "=", "len", "(", "sentence_tgt", ")", "\n", "\n", "align_dict", "=", "get_align_dict", "(", "line", ")", "\n", "\n", "for", "(", "i", ",", "token_src", ")", "in", "enumerate", "(", "sentence_src", ")", ":", "\n", "                        ", "token_src", ",", "index_src", "=", "token_src", "[", "0", "]", ",", "token_src", "[", "1", "]", "\n", "\n", "if", "if_not_meet_requirement", "(", "i", "-", "1", ",", "align_dict", ",", "token_src", ",", "token_list", ",", "tokenizer_src", ",", "args", ".", "max_num_word", ",", "oov_ind_src", ")", ":", "\n", "                            ", "continue", "\n", "\n", "", "pos", "=", "align_dict", "[", "i", "-", "1", "]", "+", "1", "\n", "if", "pos", ">", "length_tgt", "-", "1", ":", "\n", "                            ", "continue", "\n", "", "token_tgt", "=", "sentence_tgt", "[", "pos", "]", "\n", "token_tgt", ",", "index_tgt", "=", "token_tgt", "[", "0", "]", ",", "token_tgt", "[", "1", "]", "\n", "\n", "if", "if_OOV", "(", "token_tgt", ",", "tokenizer_tgt", ",", "oov_ind_tgt", ")", "or", "if_special_number", "(", "token_tgt", ")", ":", "\n", "                            ", "continue", "\n", "", "values_src", "=", "torch", ".", "sum", "(", "all_encoder_layers_src", "[", "b", ",", "index_src", ",", "-", "4", ":", ",", ":", "]", ",", "dim", "=", "1", ")", ".", "sum", "(", "dim", "=", "0", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "values_tgt", "=", "torch", ".", "sum", "(", "all_encoder_layers_tgt", "[", "b", ",", "index_tgt", ",", "-", "4", ":", ",", ":", "]", ",", "dim", "=", "1", ")", ".", "sum", "(", "dim", "=", "0", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "if", "token_src", "not", "in", "token_list", ":", "\n", "                            ", "write_word", "(", "token_src", ",", "values_src", ",", "values_tgt", ",", "token_tgt", ",", "1", ",", "args", ".", "write_vectors_path", ")", "\n", "token_list", "[", "token_src", "]", "=", "1", "\n", "", "else", ":", "\n", "                            ", "if", "token_list", "[", "token_src", "]", "<", "args", ".", "max_num_word", ":", "\n", "                                ", "write_word", "(", "token_src", ",", "values_src", ",", "values_tgt", ",", "token_tgt", ",", "-", "1", ",", "args", ".", "write_vectors_path", ")", "\n", "token_list", "[", "token_src", "]", "+=", "1", "\n", "\n", "\n", "", "", "", "", "", "f_align", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.model.Aligner.__init__": [[5, 9], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Linear", "torch.Linear", "torch.eye", "torch.eye", "torch.eye", "torch.eye"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.evaluator.evaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "size", ")", ":", "\n", "        ", "super", "(", "Aligner", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "W", "=", "nn", ".", "Parameter", "(", "torch", ".", "eye", "(", "size", ")", ")", "\n", "self", ".", "sig", "=", "nn", ".", "Linear", "(", "3", ",", "1", ")", "\n", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.model.Aligner.forward": [[9, 11], ["x.t"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "(", "self", ".", "W", "@", "x", ".", "t", "(", ")", ")", ".", "t", "(", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.evaluator.evaluator.__init__": [[4, 11], ["evaluator.evaluator.reading_test_dictionary"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.evaluator.evaluator.reading_test_dictionary"], ["    ", "def", "__init__", "(", "self", ",", "trainer", ",", "file", ")", ":", "\n", "        ", "self", ".", "trainer", "=", "trainer", "\n", "self", ".", "test_dict", "=", "None", "\n", "self", ".", "index", "=", "[", "]", "\n", "self", ".", "mistake", "=", "[", "]", "\n", "\n", "self", ".", "reading_test_dictionary", "(", "file", ")", "\n", "# self.find_dictionary_words()", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.evaluator.evaluator.reading_test_dictionary": [[14, 32], ["list", "print", "open", "f.readline", "set", "f.readline.split", "f.readline", "len", "evaluator.evaluator.index.append", "evaluator.evaluator.trainer.source.index", "gold_dict[].append"], "methods", ["None"], ["", "def", "reading_test_dictionary", "(", "self", ",", "file", ")", ":", "\n", "        ", "assert", "self", ".", "index", "==", "[", "]", "\n", "gold_dict", "=", "{", "}", "\n", "with", "open", "(", "file", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "line", "=", "f", ".", "readline", "(", ")", "\n", "while", "(", "line", ")", ":", "\n", "                ", "line", "=", "line", ".", "split", "(", ")", "\n", "if", "line", "[", "0", "]", "in", "self", ".", "trainer", ".", "source", "and", "line", "[", "1", "]", "in", "self", ".", "trainer", ".", "target", ":", "\n", "                    ", "self", ".", "index", ".", "append", "(", "self", ".", "trainer", ".", "source", ".", "index", "(", "line", "[", "0", "]", ")", ")", "#find the first word whose occurance is highest", "\n", "# self.index = self.index + self.find_all_souce(line[0])", "\n", "if", "line", "[", "0", "]", "not", "in", "gold_dict", ":", "\n", "                        ", "gold_dict", "[", "line", "[", "0", "]", "]", "=", "[", "line", "[", "1", "]", "]", "\n", "", "else", ":", "\n", "                        ", "gold_dict", "[", "line", "[", "0", "]", "]", ".", "append", "(", "line", "[", "1", "]", ")", "\n", "", "", "line", "=", "f", ".", "readline", "(", ")", "\n", "", "", "self", ".", "test_dict", "=", "gold_dict", "\n", "self", ".", "index", "=", "list", "(", "set", "(", "self", ".", "index", ")", ")", "\n", "print", "(", "\"We are considering {} words in dictionary to test the accuracy\"", ".", "format", "(", "len", "(", "self", ".", "index", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.evaluator.evaluator.find_all_souce": [[33, 39], ["enumerate", "index.append"], "methods", ["None"], ["", "def", "find_all_souce", "(", "self", ",", "word", ")", ":", "\n", "        ", "index", "=", "[", "]", "\n", "for", "ind", ",", "src", "in", "enumerate", "(", "self", ".", "trainer", ".", "source", ")", ":", "\n", "            ", "if", "src", "==", "word", ":", "\n", "                ", "index", ".", "append", "(", "ind", ")", "\n", "", "", "return", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.evaluator.evaluator.calculate_accuracy": [[41, 81], ["print", "print", "print", "print", "print", "print", "evaluator.evaluator.get_knn_words", "evaluator.evaluator.get_csls_words", "len", "evaluator.evaluator.mistake.append", "len", "len", "len", "len", "len", "set", "len", "len", "set", "len", "len", "set", "len", "len", "set", "len", "len", "set", "len", "len", "set", "len", "len"], "methods", ["home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.evaluator.evaluator.get_knn_words", "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.evaluator.evaluator.get_csls_words"], ["", "def", "calculate_accuracy", "(", "self", ",", "aligned", ",", "csls_k", "=", "10", ",", "if_csls", "=", "True", ")", ":", "\n", "        ", "acc_nn", "=", "0", "\n", "acc_nn_5", "=", "0", "\n", "acc_nn_10", "=", "0", "\n", "acc_csls", "=", "0", "\n", "acc_csls_5", "=", "0", "\n", "acc_csls_10", "=", "0", "\n", "total", "=", "0", "\n", "\n", "for", "ind", "in", "self", ".", "index", ":", "\n", "            ", "source_vec", "=", "aligned", "[", ":", ",", "ind", "]", "\n", "words", "=", "self", ".", "get_knn_words", "(", "source_vec", ")", "\n", "if", "len", "(", "set", "(", "words", "[", ":", "1", "]", "+", "self", ".", "test_dict", "[", "self", ".", "trainer", ".", "source", "[", "ind", "]", "]", ")", ")", "<", "len", "(", "words", "[", ":", "1", "]", ")", "+", "len", "(", "self", ".", "test_dict", "[", "self", ".", "trainer", ".", "source", "[", "ind", "]", "]", ")", ":", "\n", "                ", "acc_nn", "+=", "1", "\n", "", "else", ":", "\n", "                ", "self", ".", "mistake", ".", "append", "(", "[", "self", ".", "trainer", ".", "source", "[", "ind", "]", ",", "words", "[", ":", "5", "]", ",", "self", ".", "test_dict", "[", "self", ".", "trainer", ".", "source", "[", "ind", "]", "]", "]", ")", "\n", "\n", "", "if", "len", "(", "set", "(", "words", "[", ":", "5", "]", "+", "self", ".", "test_dict", "[", "self", ".", "trainer", ".", "source", "[", "ind", "]", "]", ")", ")", "<", "len", "(", "words", "[", ":", "5", "]", ")", "+", "len", "(", "self", ".", "test_dict", "[", "self", ".", "trainer", ".", "source", "[", "ind", "]", "]", ")", ":", "\n", "                ", "acc_nn_5", "+=", "1", "\n", "", "if", "len", "(", "set", "(", "words", "[", ":", "10", "]", "+", "self", ".", "test_dict", "[", "self", ".", "trainer", ".", "source", "[", "ind", "]", "]", ")", ")", "<", "len", "(", "words", "[", ":", "10", "]", ")", "+", "len", "(", "self", ".", "test_dict", "[", "self", ".", "trainer", ".", "source", "[", "ind", "]", "]", ")", ":", "\n", "                ", "acc_nn_10", "+=", "1", "\n", "", "total", "+=", "1", "\n", "", "print", "(", "\"The accuracy of {}nn is {}%\"", ".", "format", "(", "1", ",", "acc_nn", "*", "100", "/", "total", ")", ")", "\n", "print", "(", "\"The accuracy of {}nn is {}%\"", ".", "format", "(", "5", ",", "acc_nn_5", "*", "100", "/", "total", ")", ")", "\n", "print", "(", "\"The accuracy of {}nn is {}%\"", ".", "format", "(", "10", ",", "acc_nn_10", "*", "100", "/", "total", ")", ")", "\n", "if", "if_csls", "==", "False", ":", "\n", "            ", "return", "\n", "\n", "", "for", "ind", "in", "self", ".", "index", ":", "\n", "            ", "source_vec", "=", "aligned", "[", ":", ",", "ind", "]", "\n", "words", "=", "self", ".", "get_csls_words", "(", "source_vec", ",", "aligned", ",", "csls_k", ")", "\n", "if", "len", "(", "set", "(", "words", "[", ":", "1", "]", "+", "self", ".", "test_dict", "[", "self", ".", "trainer", ".", "source", "[", "ind", "]", "]", ")", ")", "<", "len", "(", "words", "[", ":", "1", "]", ")", "+", "len", "(", "self", ".", "test_dict", "[", "self", ".", "trainer", ".", "source", "[", "ind", "]", "]", ")", ":", "\n", "                ", "acc_csls", "+=", "1", "\n", "", "if", "len", "(", "set", "(", "words", "[", ":", "5", "]", "+", "self", ".", "test_dict", "[", "self", ".", "trainer", ".", "source", "[", "ind", "]", "]", ")", ")", "<", "len", "(", "words", "[", ":", "5", "]", ")", "+", "len", "(", "self", ".", "test_dict", "[", "self", ".", "trainer", ".", "source", "[", "ind", "]", "]", ")", ":", "\n", "                ", "acc_csls_5", "+=", "1", "\n", "", "if", "len", "(", "set", "(", "words", "[", ":", "10", "]", "+", "self", ".", "test_dict", "[", "self", ".", "trainer", ".", "source", "[", "ind", "]", "]", ")", ")", "<", "len", "(", "words", "[", ":", "10", "]", ")", "+", "len", "(", "self", ".", "test_dict", "[", "self", ".", "trainer", ".", "source", "[", "ind", "]", "]", ")", ":", "\n", "                ", "acc_csls_10", "+=", "1", "\n", "", "", "print", "(", "\"The accuracy P@{} csls is {}%\"", ".", "format", "(", "1", ",", "acc_csls", "*", "100", "/", "total", ")", ")", "\n", "print", "(", "\"The accuracy P@{} csls is {}%\"", ".", "format", "(", "5", ",", "acc_csls_5", "*", "100", "/", "total", ")", ")", "\n", "print", "(", "\"The accuracy P@{} csls is {}%\"", ".", "format", "(", "10", ",", "acc_csls_10", "*", "100", "/", "total", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.evaluator.evaluator.get_knn_words": [[82, 87], ["torch.sort", "evaluator.evaluator.trainer.target_vector.t", "vec.view", "cos_sim.view"], "methods", ["None"], ["", "def", "get_knn_words", "(", "self", ",", "vec", ")", ":", "\n", "        ", "cos_sim", "=", "self", ".", "trainer", ".", "target_vector", ".", "t", "(", ")", "@", "vec", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "_", ",", "ind", "=", "torch", ".", "sort", "(", "cos_sim", ".", "view", "(", "-", "1", ")", ",", "descending", "=", "True", ")", "\n", "words", "=", "[", "self", ".", "trainer", ".", "target", "[", "i", "]", "for", "i", "in", "ind", "[", ":", "10", "]", "]", "\n", "return", "words", "\n", "\n"]], "home.repos.pwc.inspect_result.fe1ixxu_ZeroShot-CrossLing-Parsing.mapper.evaluator.evaluator.get_csls_words": [[88, 104], ["torch.sort", "cos_sim[].mean", "torch.sort", "r_t[].mean().view", "list", "list.sort", "list", "evaluator.evaluator.trainer.target_vector.t", "vec.view", "cos_sim.view", "aligned.t", "zip", "map", "r_t[].mean"], "methods", ["None"], ["", "def", "get_csls_words", "(", "self", ",", "vec", ",", "aligned", ",", "csls_k", ")", ":", "\n", "        ", "cos_sim", "=", "self", ".", "trainer", ".", "target_vector", ".", "t", "(", ")", "@", "vec", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "cos_sim", ",", "index", "=", "torch", ".", "sort", "(", "cos_sim", ".", "view", "(", "-", "1", ")", ",", "descending", "=", "True", ")", "\n", "index", "=", "index", "[", ":", "20", "]", "\n", "r_s", "=", "cos_sim", "[", ":", "csls_k", "]", ".", "mean", "(", "0", ")", "\n", "\n", "r_t", "=", "aligned", ".", "t", "(", ")", "@", "self", ".", "trainer", ".", "target_vector", "[", ":", ",", "index", "]", "\n", "r_t", ",", "_", "=", "torch", ".", "sort", "(", "r_t", ",", "dim", "=", "0", ",", "descending", "=", "True", ")", "\n", "r_t", "=", "r_t", "[", ":", "csls_k", "]", ".", "mean", "(", "dim", "=", "0", ")", ".", "view", "(", "-", "1", ")", "\n", "\n", "dis", "=", "2", "*", "cos_sim", "[", ":", "10", "]", "-", "r_s", "-", "r_t", "[", ":", "10", "]", "\n", "dis", "=", "list", "(", "zip", "(", "dis", ",", "index", ")", ")", "\n", "dis", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "dis", "=", "list", "(", "map", "(", "lambda", "x", ":", "x", "[", "1", "]", ",", "dis", ")", ")", "\n", "words", "=", "[", "self", ".", "trainer", ".", "target", "[", "i", "]", "for", "i", "in", "dis", "[", ":", "10", "]", "]", "\n", "return", "words", "\n", "", "", ""]]}