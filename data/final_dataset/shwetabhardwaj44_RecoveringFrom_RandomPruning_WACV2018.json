{"home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.config.get_data_files": [[46, 59], ["range", "tfrecords_filename.append"], "function", ["None"], ["def", "get_data_files", "(", "dataset_split_name", ")", ":", "\n", "    ", "tfrecords_filename", "=", "[", "]", "\n", "if", "dataset_split_name", "==", "'train'", ":", "\n", "        ", "length", "=", "1024", "\n", "data", "=", "\"-of-1024\"", "\n", "", "if", "dataset_split_name", "==", "'validation'", ":", "\n", "        ", "length", "=", "128", "\n", "data", "=", "\"-of-0128\"", "\n", "\n", "", "for", "k", "in", "range", "(", "length", ")", ":", "# Train data tfrecords", "\n", "        ", "j", "=", "\"-%04d\"", "%", "k", "# pad with 0's", "\n", "tfrecords_filename", ".", "append", "(", "data_dir", "+", "dataset_split_name", "+", "j", "+", "data", ")", "\n", "", "return", "tfrecords_filename", "\n", "\n"]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.vgg_preprocessing._crop": [[16, 57], ["tensorflow.shape", "tensorflow.Assert", "tensorflow.Assert", "tensorflow.to_int32", "tensorflow.reshape", "tensorflow.equal", "tensorflow.control_dependencies", "tensorflow.stack", "tensorflow.logical_and", "tensorflow.stack", "tensorflow.control_dependencies", "tensorflow.slice", "tensorflow.rank", "tensorflow.greater_equal", "tensorflow.greater_equal"], "function", ["None"], ["def", "_crop", "(", "image", ",", "offset_height", ",", "offset_width", ",", "crop_height", ",", "crop_width", ")", ":", "\n", "  ", "\"\"\"Crops the given image using the provided offsets and sizes.\n\n  Note that the method doesn't assume we know the input image size but it does\n  assume we know the input image rank.\n\n  Args:\n    image: an image of shape [height, width, channels].\n    offset_height: a scalar tensor indicating the height offset.\n    offset_width: a scalar tensor indicating the width offset.\n    crop_height: the height of the cropped image.\n    crop_width: the width of the cropped image.\n\n  Returns:\n    the cropped (and resized) image.\n\n  Raises:\n    InvalidArgumentError: if the rank is not 3 or if the image dimensions are\n      less than the crop size.\n  \"\"\"", "\n", "original_shape", "=", "tf", ".", "shape", "(", "image", ")", "\n", "\n", "rank_assertion", "=", "tf", ".", "Assert", "(", "\n", "tf", ".", "equal", "(", "tf", ".", "rank", "(", "image", ")", ",", "3", ")", ",", "\n", "[", "'Rank of image must be equal to 3.'", "]", ")", "\n", "with", "tf", ".", "control_dependencies", "(", "[", "rank_assertion", "]", ")", ":", "\n", "    ", "cropped_shape", "=", "tf", ".", "stack", "(", "[", "crop_height", ",", "crop_width", ",", "original_shape", "[", "2", "]", "]", ")", "\n", "\n", "", "size_assertion", "=", "tf", ".", "Assert", "(", "\n", "tf", ".", "logical_and", "(", "\n", "tf", ".", "greater_equal", "(", "original_shape", "[", "0", "]", ",", "crop_height", ")", ",", "\n", "tf", ".", "greater_equal", "(", "original_shape", "[", "1", "]", ",", "crop_width", ")", ")", ",", "\n", "[", "'Crop size greater than the image size.'", "]", ")", "\n", "\n", "offsets", "=", "tf", ".", "to_int32", "(", "tf", ".", "stack", "(", "[", "offset_height", ",", "offset_width", ",", "0", "]", ")", ")", "\n", "\n", "# Use tf.slice instead of crop_to_bounding box as it accepts tensors to", "\n", "# define the crop size.", "\n", "with", "tf", ".", "control_dependencies", "(", "[", "size_assertion", "]", ")", ":", "\n", "    ", "image", "=", "tf", ".", "slice", "(", "image", ",", "offsets", ",", "cropped_shape", ")", "\n", "", "return", "tf", ".", "reshape", "(", "image", ",", "cropped_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.vgg_preprocessing._random_crop": [[59, 140], ["range", "tensorflow.Assert", "range", "tensorflow.random_uniform", "tensorflow.random_uniform", "ValueError", "len", "tensorflow.rank", "tensorflow.Assert", "rank_assertions.append", "tensorflow.control_dependencies", "tensorflow.shape", "tensorflow.logical_and", "len", "asserts.append", "tensorflow.Assert", "tensorflow.Assert", "asserts.extend", "tensorflow.control_dependencies", "tensorflow.reshape", "tensorflow.control_dependencies", "tensorflow.reshape", "vgg_preprocessing._crop", "tensorflow.equal", "tensorflow.greater_equal", "tensorflow.greater_equal", "tensorflow.control_dependencies", "tensorflow.shape", "tensorflow.equal", "tensorflow.equal"], "function", ["home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.vgg_preprocessing._crop"], ["", "def", "_random_crop", "(", "image_list", ",", "crop_height", ",", "crop_width", ")", ":", "\n", "  ", "\"\"\"Crops the given list of images.\n\n  The function applies the same crop to each image in the list. This can be\n  effectively applied when there are multiple image inputs of the same\n  dimension such as:\n\n    image, depths, normals = _random_crop([image, depths, normals], 120, 150)\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the new height.\n    crop_width: the new width.\n\n  Returns:\n    the image_list with cropped images.\n\n  Raises:\n    ValueError: if there are multiple image inputs provided with different size\n      or the images are smaller than the crop dimensions.\n  \"\"\"", "\n", "if", "not", "image_list", ":", "\n", "    ", "raise", "ValueError", "(", "'Empty image_list.'", ")", "\n", "\n", "# Compute the rank assertions.", "\n", "", "rank_assertions", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "image_list", ")", ")", ":", "\n", "    ", "image_rank", "=", "tf", ".", "rank", "(", "image_list", "[", "i", "]", ")", "\n", "rank_assert", "=", "tf", ".", "Assert", "(", "\n", "tf", ".", "equal", "(", "image_rank", ",", "3", ")", ",", "\n", "[", "'Wrong rank for tensor  %s [expected] [actual]'", ",", "\n", "image_list", "[", "i", "]", ".", "name", ",", "3", ",", "image_rank", "]", ")", "\n", "rank_assertions", ".", "append", "(", "rank_assert", ")", "\n", "\n", "", "with", "tf", ".", "control_dependencies", "(", "[", "rank_assertions", "[", "0", "]", "]", ")", ":", "\n", "    ", "image_shape", "=", "tf", ".", "shape", "(", "image_list", "[", "0", "]", ")", "\n", "", "image_height", "=", "image_shape", "[", "0", "]", "\n", "image_width", "=", "image_shape", "[", "1", "]", "\n", "crop_size_assert", "=", "tf", ".", "Assert", "(", "\n", "tf", ".", "logical_and", "(", "\n", "tf", ".", "greater_equal", "(", "image_height", ",", "crop_height", ")", ",", "\n", "tf", ".", "greater_equal", "(", "image_width", ",", "crop_width", ")", ")", ",", "\n", "[", "'Crop size greater than the image size.'", "]", ")", "\n", "\n", "asserts", "=", "[", "rank_assertions", "[", "0", "]", ",", "crop_size_assert", "]", "\n", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "image_list", ")", ")", ":", "\n", "    ", "image", "=", "image_list", "[", "i", "]", "\n", "asserts", ".", "append", "(", "rank_assertions", "[", "i", "]", ")", "\n", "with", "tf", ".", "control_dependencies", "(", "[", "rank_assertions", "[", "i", "]", "]", ")", ":", "\n", "      ", "shape", "=", "tf", ".", "shape", "(", "image", ")", "\n", "", "height", "=", "shape", "[", "0", "]", "\n", "width", "=", "shape", "[", "1", "]", "\n", "\n", "height_assert", "=", "tf", ".", "Assert", "(", "\n", "tf", ".", "equal", "(", "height", ",", "image_height", ")", ",", "\n", "[", "'Wrong height for tensor %s [expected][actual]'", ",", "\n", "image", ".", "name", ",", "height", ",", "image_height", "]", ")", "\n", "width_assert", "=", "tf", ".", "Assert", "(", "\n", "tf", ".", "equal", "(", "width", ",", "image_width", ")", ",", "\n", "[", "'Wrong width for tensor %s [expected][actual]'", ",", "\n", "image", ".", "name", ",", "width", ",", "image_width", "]", ")", "\n", "asserts", ".", "extend", "(", "[", "height_assert", ",", "width_assert", "]", ")", "\n", "\n", "# Create a random bounding box.", "\n", "#", "\n", "# Use tf.random_uniform and not numpy.random.rand as doing the former would", "\n", "# generate random numbers at graph eval time, unlike the latter which", "\n", "# generates random numbers at graph definition time.", "\n", "", "with", "tf", ".", "control_dependencies", "(", "asserts", ")", ":", "\n", "    ", "max_offset_height", "=", "tf", ".", "reshape", "(", "image_height", "-", "crop_height", "+", "1", ",", "[", "]", ")", "\n", "", "with", "tf", ".", "control_dependencies", "(", "asserts", ")", ":", "\n", "    ", "max_offset_width", "=", "tf", ".", "reshape", "(", "image_width", "-", "crop_width", "+", "1", ",", "[", "]", ")", "\n", "", "offset_height", "=", "tf", ".", "random_uniform", "(", "\n", "[", "]", ",", "maxval", "=", "max_offset_height", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "offset_width", "=", "tf", ".", "random_uniform", "(", "\n", "[", "]", ",", "maxval", "=", "max_offset_width", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "return", "[", "_crop", "(", "image", ",", "offset_height", ",", "offset_width", ",", "\n", "crop_height", ",", "crop_width", ")", "for", "image", "in", "image_list", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.vgg_preprocessing._central_crop": [[142, 165], ["outputs.append", "tensorflow.shape", "tensorflow.shape", "vgg_preprocessing._crop"], "function", ["home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.vgg_preprocessing._crop"], ["", "def", "_central_crop", "(", "image_list", ",", "crop_height", ",", "crop_width", ")", ":", "\n", "  ", "\"\"\"Performs central crops of the given image list.\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the height of the image following the crop.\n    crop_width: the width of the image following the crop.\n\n  Returns:\n    the list of cropped images.\n  \"\"\"", "\n", "outputs", "=", "[", "]", "\n", "for", "image", "in", "image_list", ":", "\n", "    ", "image_height", "=", "tf", ".", "shape", "(", "image", ")", "[", "0", "]", "\n", "image_width", "=", "tf", ".", "shape", "(", "image", ")", "[", "1", "]", "\n", "\n", "offset_height", "=", "(", "image_height", "-", "crop_height", ")", "/", "2", "\n", "offset_width", "=", "(", "image_width", "-", "crop_width", ")", "/", "2", "\n", "\n", "outputs", ".", "append", "(", "_crop", "(", "image", ",", "offset_height", ",", "offset_width", ",", "\n", "crop_height", ",", "crop_width", ")", ")", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.vgg_preprocessing._mean_image_subtraction": [[167, 198], ["tensorflow.split", "range", "tensorflow.concat", "ValueError", "image.get_shape().as_list", "len", "ValueError", "image.get_shape", "image.get_shape"], "function", ["None"], ["", "def", "_mean_image_subtraction", "(", "image", ",", "means", ")", ":", "\n", "  ", "\"\"\"Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn't match the\n      number of values in `means`.\n  \"\"\"", "\n", "if", "image", ".", "get_shape", "(", ")", ".", "ndims", "!=", "3", ":", "\n", "    ", "raise", "ValueError", "(", "'Input must be of size [height, width, C>0]'", ")", "\n", "", "num_channels", "=", "image", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "\n", "if", "len", "(", "means", ")", "!=", "num_channels", ":", "\n", "    ", "raise", "ValueError", "(", "'len(means) must match the number of channels'", ")", "\n", "\n", "", "channels", "=", "tf", ".", "split", "(", "axis", "=", "2", ",", "num_or_size_splits", "=", "num_channels", ",", "value", "=", "image", ")", "\n", "for", "i", "in", "range", "(", "num_channels", ")", ":", "\n", "    ", "channels", "[", "i", "]", "-=", "means", "[", "i", "]", "\n", "", "return", "tf", ".", "concat", "(", "axis", "=", "2", ",", "values", "=", "channels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.vgg_preprocessing._smallest_size_at_least": [[200, 228], ["tensorflow.convert_to_tensor", "tensorflow.to_float", "tensorflow.to_float", "tensorflow.to_float", "tensorflow.cond", "tensorflow.to_int32", "tensorflow.to_int32", "tensorflow.greater"], "function", ["None"], ["", "def", "_smallest_size_at_least", "(", "height", ",", "width", ",", "smallest_side", ")", ":", "\n", "  ", "\"\"\"Computes new shape with the smallest side equal to `smallest_side`.\n\n  Computes new shape with the smallest side equal to `smallest_side` while\n  preserving the original aspect ratio.\n\n  Args:\n    height: an int32 scalar tensor indicating the current height.\n    width: an int32 scalar tensor indicating the current width.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    new_height: an int32 scalar tensor indicating the new height.\n    new_width: and int32 scalar tensor indicating the new width.\n  \"\"\"", "\n", "smallest_side", "=", "tf", ".", "convert_to_tensor", "(", "smallest_side", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "height", "=", "tf", ".", "to_float", "(", "height", ")", "\n", "width", "=", "tf", ".", "to_float", "(", "width", ")", "\n", "smallest_side", "=", "tf", ".", "to_float", "(", "smallest_side", ")", "\n", "\n", "scale", "=", "tf", ".", "cond", "(", "tf", ".", "greater", "(", "height", ",", "width", ")", ",", "\n", "lambda", ":", "smallest_side", "/", "width", ",", "\n", "lambda", ":", "smallest_side", "/", "height", ")", "\n", "new_height", "=", "tf", ".", "to_int32", "(", "height", "*", "scale", ")", "\n", "new_width", "=", "tf", ".", "to_int32", "(", "width", "*", "scale", ")", "\n", "return", "new_height", ",", "new_width", "\n", "\n"]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.vgg_preprocessing._aspect_preserving_resize": [[230, 253], ["tensorflow.convert_to_tensor", "tensorflow.shape", "vgg_preprocessing._smallest_size_at_least", "tensorflow.expand_dims", "tensorflow.image.resize_bilinear", "tensorflow.squeeze", "tf.squeeze.set_shape"], "function", ["home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.vgg_preprocessing._smallest_size_at_least"], ["", "def", "_aspect_preserving_resize", "(", "image", ",", "smallest_side", ")", ":", "\n", "  ", "\"\"\"Resize images preserving the original aspect ratio.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image.\n  \"\"\"", "\n", "smallest_side", "=", "tf", ".", "convert_to_tensor", "(", "smallest_side", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "shape", "=", "tf", ".", "shape", "(", "image", ")", "\n", "height", "=", "shape", "[", "0", "]", "\n", "width", "=", "shape", "[", "1", "]", "\n", "new_height", ",", "new_width", "=", "_smallest_size_at_least", "(", "height", ",", "width", ",", "smallest_side", ")", "\n", "image", "=", "tf", ".", "expand_dims", "(", "image", ",", "0", ")", "\n", "resized_image", "=", "tf", ".", "image", ".", "resize_bilinear", "(", "image", ",", "[", "new_height", ",", "new_width", "]", ",", "\n", "align_corners", "=", "False", ")", "\n", "resized_image", "=", "tf", ".", "squeeze", "(", "resized_image", ")", "\n", "resized_image", ".", "set_shape", "(", "[", "None", ",", "None", ",", "3", "]", ")", "\n", "return", "resized_image", "\n", "\n"]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.vgg_preprocessing.preprocess_for_train": [[255, 286], ["tensorflow.random_uniform", "vgg_preprocessing._aspect_preserving_resize", "tf.image.random_flip_left_right.set_shape", "tensorflow.to_float", "tensorflow.image.random_flip_left_right", "vgg_preprocessing._mean_image_subtraction", "vgg_preprocessing._random_crop"], "function", ["home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.vgg_preprocessing._aspect_preserving_resize", "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.vgg_preprocessing._mean_image_subtraction", "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.vgg_preprocessing._random_crop"], ["", "def", "preprocess_for_train", "(", "image", ",", "\n", "output_height", ",", "\n", "output_width", ",", "\n", "resize_side_min", "=", "_RESIZE_SIDE_MIN", ",", "\n", "resize_side_max", "=", "_RESIZE_SIDE_MAX", ")", ":", "\n", "  ", "\"\"\"Preprocesses the given image for training.\n\n  Note that the actual resizing scale is sampled from\n    [`resize_size_min`, `resize_size_max`].\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  \"\"\"", "\n", "resize_side", "=", "tf", ".", "random_uniform", "(", "\n", "[", "]", ",", "minval", "=", "resize_side_min", ",", "maxval", "=", "resize_side_max", "+", "1", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "image", "=", "_aspect_preserving_resize", "(", "image", ",", "resize_side", ")", "\n", "image", "=", "_random_crop", "(", "[", "image", "]", ",", "output_height", ",", "output_width", ")", "[", "0", "]", "\n", "image", ".", "set_shape", "(", "[", "output_height", ",", "output_width", ",", "3", "]", ")", "\n", "image", "=", "tf", ".", "to_float", "(", "image", ")", "\n", "image", "=", "tf", ".", "image", ".", "random_flip_left_right", "(", "image", ")", "\n", "return", "_mean_image_subtraction", "(", "image", ",", "[", "_R_MEAN", ",", "_G_MEAN", ",", "_B_MEAN", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.vgg_preprocessing.preprocess_for_eval": [[288, 305], ["vgg_preprocessing._aspect_preserving_resize", "tf.to_float.set_shape", "tensorflow.to_float", "vgg_preprocessing._mean_image_subtraction", "vgg_preprocessing._central_crop"], "function", ["home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.vgg_preprocessing._aspect_preserving_resize", "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.vgg_preprocessing._mean_image_subtraction", "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.vgg_preprocessing._central_crop"], ["", "def", "preprocess_for_eval", "(", "image", ",", "output_height", ",", "output_width", ",", "resize_side", ")", ":", "\n", "  ", "\"\"\"Preprocesses the given image for evaluation.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side: The smallest side of the image for aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  \"\"\"", "\n", "image", "=", "_aspect_preserving_resize", "(", "image", ",", "resize_side", ")", "\n", "image", "=", "_central_crop", "(", "[", "image", "]", ",", "output_height", ",", "output_width", ")", "[", "0", "]", "\n", "image", ".", "set_shape", "(", "[", "output_height", ",", "output_width", ",", "3", "]", ")", "\n", "image", "=", "tf", ".", "to_float", "(", "image", ")", "\n", "return", "_mean_image_subtraction", "(", "image", ",", "[", "_R_MEAN", ",", "_G_MEAN", ",", "_B_MEAN", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.vgg_preprocessing.preprocess_image": [[307, 335], ["vgg_preprocessing.preprocess_for_train", "vgg_preprocessing.preprocess_for_eval"], "function", ["home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.vgg_preprocessing.preprocess_for_train", "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.vgg_preprocessing.preprocess_for_eval"], ["", "def", "preprocess_image", "(", "image", ",", "output_height", ",", "output_width", ",", "is_training", "=", "False", ",", "\n", "resize_side_min", "=", "_RESIZE_SIDE_MIN", ",", "\n", "resize_side_max", "=", "_RESIZE_SIDE_MAX", ")", ":", "\n", "  ", "\"\"\"Preprocesses the given image.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    is_training: `True` if we're preprocessing the image for training and\n      `False` otherwise.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, then this value\n      is used for rescaling.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, this value is\n      ignored. Otherwise, the resize side is sampled from\n        [resize_size_min, resize_size_max].\n\n  Returns:\n    A preprocessed image.\n  \"\"\"", "\n", "if", "is_training", ":", "\n", "    ", "return", "preprocess_for_train", "(", "image", ",", "output_height", ",", "output_width", ",", "\n", "resize_side_min", ",", "resize_side_max", ")", "\n", "", "else", ":", "\n", "    ", "return", "preprocess_for_eval", "(", "image", ",", "output_height", ",", "output_width", ",", "\n", "resize_side_min", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.train_and_prune.parse_args": [[16, 30], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "print", "layer.replace().replace().replace", "layer.replace().replace", "layer.replace"], "function", ["home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.finetune_PrunedModel.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "  ", "global", "layer_name", ",", "checkpoint", ",", "prune_layers", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--layer_name\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--prune_layers\"", ",", "nargs", "=", "\"+\"", ",", "default", "=", "[", "]", ")", "\n", "parser", ".", "add_argument", "(", "\"--checkpoint\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_epochs\"", ",", "default", "=", "1", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "layer_name", "=", "args", ".", "layer_name", "\n", "checkpoint", "=", "args", ".", "checkpoint", "\n", "prune_layers", "=", "args", ".", "prune_layers", "\n", "prune_layers", "=", "[", "layer", ".", "replace", "(", "'['", ",", "''", ")", ".", "replace", "(", "']'", ",", "''", ")", ".", "replace", "(", "','", ",", "''", ")", "for", "layer", "in", "prune_layers", "]", "\n", "num_epochs", "=", "args", ".", "num_epochs", "\n", "print", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.train_and_prune.weight_variable": [[31, 33], ["tensorflow.get_variable", "tensorflow.contrib.layers.xavier_initializer"], "function", ["None"], ["", "def", "weight_variable", "(", "shape", ",", "name", ")", ":", "\n", "    ", "return", "tf", ".", "get_variable", "(", "name", ",", "shape", "=", "shape", ",", "initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.train_and_prune.bias_variable": [[34, 37], ["tensorflow.constant", "tensorflow.Variable"], "function", ["None"], ["", "def", "bias_variable", "(", "shape", ",", "name", ")", ":", "\n", "    ", "initial", "=", "tf", ".", "constant", "(", "0.001", ",", "shape", "=", "shape", ")", "\n", "return", "tf", ".", "Variable", "(", "initial", ",", "name", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.train_and_prune.conv2d": [[38, 40], ["tensorflow.nn.conv2d"], "function", ["home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.finetune_PrunedModel.conv2d"], ["", "def", "conv2d", "(", "x", ",", "W", ")", ":", "\n", "    ", "return", "tf", ".", "nn", ".", "conv2d", "(", "x", ",", "W", ",", "strides", "=", "[", "1", ",", "1", ",", "1", ",", "1", "]", ",", "padding", "=", "\"SAME\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.train_and_prune.max_pool_2x2": [[41, 43], ["tensorflow.nn.max_pool"], "function", ["None"], ["", "def", "max_pool_2x2", "(", "x", ")", ":", "\n", "    ", "return", "tf", ".", "nn", ".", "max_pool", "(", "x", ",", "ksize", "=", "[", "1", ",", "2", ",", "2", ",", "1", "]", ",", "strides", "=", "[", "1", ",", "2", ",", "2", ",", "1", "]", ",", "padding", "=", "\"SAME\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.train_and_prune.fetch_images_valid": [[44, 79], ["tensorflow.parse_single_example", "tensorflow.image.decode_jpeg", "tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "tensorflow.stack", "tensorflow.cast", "vgg_preprocessing.preprocess_image", "tensorflow.train.batch", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature"], "function", ["home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.vgg_preprocessing.preprocess_image"], ["", "def", "fetch_images_valid", "(", "serialized_example", ",", "IMAGE_HEIGHT", "=", "224", ",", "IMAGE_WIDTH", "=", "224", ",", "is_training", "=", "False", ")", ":", "\n", "    ", "features", "=", "tf", ".", "parse_single_example", "(", "\n", "serialized_example", ",", "features", "=", "{", "\n", "'image/encoded'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "(", ")", ",", "tf", ".", "string", ",", "default_value", "=", "''", ")", ",", "\n", "'image/format'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "(", ")", ",", "tf", ".", "string", ",", "default_value", "=", "'jpeg'", ")", ",", "\n", "'image/class/label'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ",", "default_value", "=", "-", "1", ")", ",", "\n", "'image/class/text'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "[", "]", ",", "dtype", "=", "tf", ".", "string", ",", "default_value", "=", "''", ")", ",", "\n", "'image/height'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "'image/width'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "'image/channels'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "# 'image/object/class/label': tf.VarLenFeature(", "\n", "#     dtype=tf.int64),", "\n", "}", ")", "\n", "\n", "image", "=", "tf", ".", "image", ".", "decode_jpeg", "(", "features", "[", "'image/encoded'", "]", ",", "channels", "=", "3", ")", "\n", "# image = tf.image.decode_image(features['image/encoded'], channels=3)", "\n", "label", "=", "tf", ".", "cast", "(", "features", "[", "'image/class/label'", "]", ",", "tf", ".", "int32", ")", "\n", "height", "=", "tf", ".", "cast", "(", "features", "[", "'image/height'", "]", ",", "tf", ".", "int32", ")", "\n", "width", "=", "tf", ".", "cast", "(", "features", "[", "'image/width'", "]", ",", "tf", ".", "int32", ")", "\n", "num_channels", "=", "tf", ".", "cast", "(", "features", "[", "'image/channels'", "]", ",", "tf", ".", "int32", ")", "\n", "image_shape", "=", "tf", ".", "stack", "(", "[", "height", ",", "width", ",", "num_channels", "]", ")", "\n", "image", "=", "tf", ".", "cast", "(", "image", ",", "tf", ".", "float32", ")", "\n", "preprocessed_image", "=", "preprocess", ".", "preprocess_image", "(", "image", ",", "IMAGE_HEIGHT", ",", "IMAGE_WIDTH", ",", "is_training", "=", "False", ")", "\n", "images", ",", "labels", "=", "tf", ".", "train", ".", "batch", "(", "[", "preprocessed_image", ",", "label", "]", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "capacity", "=", "batch_size", "+", "num_threads", "*", "batch_size", ",", "\n", "num_threads", "=", "num_threads", ")", "\n", "return", "images", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.train_and_prune.fetch_images_train": [[81, 119], ["tensorflow.parse_single_example", "tensorflow.image.decode_jpeg", "tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "tensorflow.reshape", "tensorflow.cast", "vgg_preprocessing.preprocess_image", "tensorflow.train.shuffle_batch", "tensorflow.stack", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature"], "function", ["home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.vgg_preprocessing.preprocess_image"], ["", "def", "fetch_images_train", "(", "serialized_example", ",", "IMAGE_HEIGHT", "=", "224", ",", "IMAGE_WIDTH", "=", "224", ",", "is_training", "=", "True", ")", ":", "\n", "    ", "features", "=", "tf", ".", "parse_single_example", "(", "\n", "serialized_example", ",", "features", "=", "{", "\n", "'image/encoded'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "(", ")", ",", "tf", ".", "string", ",", "default_value", "=", "''", ")", ",", "\n", "'image/format'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "(", ")", ",", "tf", ".", "string", ",", "default_value", "=", "'jpeg'", ")", ",", "\n", "'image/class/label'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ",", "default_value", "=", "-", "1", ")", ",", "\n", "'image/class/text'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "[", "]", ",", "dtype", "=", "tf", ".", "string", ",", "default_value", "=", "''", ")", ",", "\n", "'image/height'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "'image/width'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "'image/channels'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "# 'image/object/class/label': tf.VarLenFeature(", "\n", "#     dtype=tf.int64),", "\n", "}", ")", "\n", "image", "=", "tf", ".", "image", ".", "decode_jpeg", "(", "features", "[", "'image/encoded'", "]", ",", "channels", "=", "3", ")", "\n", "# image = tf.image.decode_image(features['image/encoded'], channels=3)", "\n", "\n", "label", "=", "tf", ".", "cast", "(", "features", "[", "'image/class/label'", "]", ",", "tf", ".", "int32", ")", "\n", "height", "=", "tf", ".", "cast", "(", "features", "[", "'image/height'", "]", ",", "tf", ".", "int32", ")", "\n", "width", "=", "tf", ".", "cast", "(", "features", "[", "'image/width'", "]", ",", "tf", ".", "int32", ")", "\n", "num_channels", "=", "tf", ".", "cast", "(", "features", "[", "'image/channels'", "]", ",", "tf", ".", "int32", ")", "\n", "\n", "image", "=", "tf", ".", "reshape", "(", "image", ",", "tf", ".", "stack", "(", "[", "height", ",", "width", ",", "num_channels", "]", ")", ")", "\n", "image", "=", "tf", ".", "cast", "(", "image", ",", "tf", ".", "float32", ")", "\n", "preprocessed_image", "=", "preprocess", ".", "preprocess_image", "(", "image", ",", "IMAGE_HEIGHT", ",", "IMAGE_WIDTH", ",", "is_training", "=", "is_training", ")", "\n", "images", ",", "labels", "=", "tf", ".", "train", ".", "shuffle_batch", "(", "[", "preprocessed_image", ",", "label", "]", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "capacity", "=", "batch_size", "+", "num_threads", "*", "batch_size", ",", "\n", "num_threads", "=", "num_threads", ",", "\n", "min_after_dequeue", "=", "batch_size", ")", "\n", "\n", "return", "images", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.train_and_prune.configure_lr": [[121, 127], ["int", "tensorflow.train.exponential_decay"], "function", ["None"], ["", "def", "configure_lr", "(", "global_step", ",", "learning_rate", ")", ":", "\n", "    ", "decay_steps", "=", "int", "(", "num_data_samples", "/", "batch_size", "*", "\n", "num_epochs_per_decay", ")", "\n", "return", "tf", ".", "train", ".", "exponential_decay", "(", "learning_rate", ",", "global_step", ",", "\n", "decay_steps", ",", "learning_rate_decay_factor", ",", "\n", "staircase", "=", "True", ",", "name", "=", "'exponential_decay_learning_rate'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.finetune_PrunedModel.parse_args": [[20, 29], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "layer.replace().replace().replace", "layer.replace().replace", "layer.replace"], "function", ["home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.finetune_PrunedModel.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "  ", "global", "prune_layers", ",", "checkpoint", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--prune_layers\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--checkpoint\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "prune_layers", "=", "args", ".", "prune_layers", "\n", "prune_layers", "=", "[", "layer", ".", "replace", "(", "'['", ",", "''", ")", ".", "replace", "(", "']'", ",", "''", ")", ".", "replace", "(", "','", ",", "''", ")", "for", "layer", "in", "prune_layers", "]", "\n", "checkpoint", "=", "args", ".", "checkpoint", "\n", "\n"]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.finetune_PrunedModel.weight_variable": [[30, 32], ["tensorflow.get_variable", "tensorflow.contrib.layers.xavier_initializer"], "function", ["None"], ["", "def", "weight_variable", "(", "shape", ",", "name", ")", ":", "\n", "    ", "return", "tf", ".", "get_variable", "(", "name", ",", "shape", "=", "shape", ",", "initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.finetune_PrunedModel.bias_variable": [[33, 36], ["tensorflow.constant", "tensorflow.Variable"], "function", ["None"], ["", "def", "bias_variable", "(", "shape", ",", "name", ")", ":", "\n", "    ", "initial", "=", "tf", ".", "constant", "(", "0.001", ",", "shape", "=", "shape", ")", "\n", "return", "tf", ".", "Variable", "(", "initial", ",", "name", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.finetune_PrunedModel.conv2d": [[37, 39], ["tensorflow.nn.conv2d"], "function", ["home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.finetune_PrunedModel.conv2d"], ["", "def", "conv2d", "(", "x", ",", "W", ")", ":", "\n", "    ", "return", "tf", ".", "nn", ".", "conv2d", "(", "x", ",", "W", ",", "strides", "=", "[", "1", ",", "1", ",", "1", ",", "1", "]", ",", "padding", "=", "\"SAME\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.finetune_PrunedModel.max_pool_2x2": [[40, 42], ["tensorflow.nn.max_pool"], "function", ["None"], ["", "def", "max_pool_2x2", "(", "x", ")", ":", "\n", "    ", "return", "tf", ".", "nn", ".", "max_pool", "(", "x", ",", "ksize", "=", "[", "1", ",", "2", ",", "2", ",", "1", "]", ",", "strides", "=", "[", "1", ",", "2", ",", "2", ",", "1", "]", ",", "padding", "=", "\"SAME\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.finetune_PrunedModel.fetch_images_valid": [[43, 77], ["tensorflow.parse_single_example", "tensorflow.image.decode_jpeg", "tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "tensorflow.stack", "tensorflow.cast", "vgg_preprocessing.preprocess_image", "tensorflow.train.batch", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.VarLenFeature"], "function", ["home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.vgg_preprocessing.preprocess_image"], ["", "def", "fetch_images_valid", "(", "serialized_example", ",", "IMAGE_HEIGHT", "=", "224", ",", "IMAGE_WIDTH", "=", "224", ",", "is_training", "=", "False", ")", ":", "\n", "    ", "features", "=", "tf", ".", "parse_single_example", "(", "\n", "serialized_example", ",", "features", "=", "{", "\n", "'image/encoded'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "(", ")", ",", "tf", ".", "string", ",", "default_value", "=", "''", ")", ",", "\n", "'image/format'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "(", ")", ",", "tf", ".", "string", ",", "default_value", "=", "'jpeg'", ")", ",", "\n", "'image/class/label'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ",", "default_value", "=", "-", "1", ")", ",", "\n", "'image/class/text'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "[", "]", ",", "dtype", "=", "tf", ".", "string", ",", "default_value", "=", "''", ")", ",", "\n", "'image/height'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "'image/width'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "'image/channels'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "'image/object/class/label'", ":", "tf", ".", "VarLenFeature", "(", "\n", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "}", ")", "\n", "\n", "image", "=", "tf", ".", "image", ".", "decode_jpeg", "(", "features", "[", "'image/encoded'", "]", ",", "channels", "=", "3", ")", "\n", "label", "=", "tf", ".", "cast", "(", "features", "[", "'image/class/label'", "]", ",", "tf", ".", "int32", ")", "\n", "height", "=", "tf", ".", "cast", "(", "features", "[", "'image/height'", "]", ",", "tf", ".", "int32", ")", "\n", "width", "=", "tf", ".", "cast", "(", "features", "[", "'image/width'", "]", ",", "tf", ".", "int32", ")", "\n", "num_channels", "=", "tf", ".", "cast", "(", "features", "[", "'image/channels'", "]", ",", "tf", ".", "int32", ")", "\n", "image_shape", "=", "tf", ".", "stack", "(", "[", "height", ",", "width", ",", "num_channels", "]", ")", "\n", "image", "=", "tf", ".", "cast", "(", "image", ",", "tf", ".", "float32", ")", "\n", "preprocessed_image", "=", "preprocess", ".", "preprocess_image", "(", "image", ",", "IMAGE_HEIGHT", ",", "IMAGE_WIDTH", ",", "is_training", "=", "False", ")", "\n", "images", ",", "labels", "=", "tf", ".", "train", ".", "batch", "(", "[", "preprocessed_image", ",", "label", "]", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "capacity", "=", "batch_size", "+", "num_threads", "*", "batch_size", ",", "\n", "num_threads", "=", "num_threads", ")", "\n", "return", "images", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.finetune_PrunedModel.fetch_images_train": [[78, 113], ["tensorflow.parse_single_example", "tensorflow.image.decode_jpeg", "tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "tensorflow.stack", "tensorflow.cast", "vgg_preprocessing.preprocess_image", "tensorflow.train.shuffle_batch", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.VarLenFeature"], "function", ["home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.vgg_preprocessing.preprocess_image"], ["", "def", "fetch_images_train", "(", "serialized_example", ",", "IMAGE_HEIGHT", "=", "224", ",", "IMAGE_WIDTH", "=", "224", ",", "is_training", "=", "True", ")", ":", "\n", "    ", "features", "=", "tf", ".", "parse_single_example", "(", "\n", "serialized_example", ",", "features", "=", "{", "\n", "'image/encoded'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "(", ")", ",", "tf", ".", "string", ",", "default_value", "=", "''", ")", ",", "\n", "'image/format'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "(", ")", ",", "tf", ".", "string", ",", "default_value", "=", "'jpeg'", ")", ",", "\n", "'image/class/label'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ",", "default_value", "=", "-", "1", ")", ",", "\n", "'image/class/text'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "[", "]", ",", "dtype", "=", "tf", ".", "string", ",", "default_value", "=", "''", ")", ",", "\n", "'image/height'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "'image/width'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "'image/channels'", ":", "tf", ".", "FixedLenFeature", "(", "\n", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "'image/object/class/label'", ":", "tf", ".", "VarLenFeature", "(", "\n", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "}", ")", "\n", "\n", "image", "=", "tf", ".", "image", ".", "decode_jpeg", "(", "features", "[", "'image/encoded'", "]", ",", "channels", "=", "3", ")", "\n", "label", "=", "tf", ".", "cast", "(", "features", "[", "'image/class/label'", "]", ",", "tf", ".", "int32", ")", "\n", "height", "=", "tf", ".", "cast", "(", "features", "[", "'image/height'", "]", ",", "tf", ".", "int32", ")", "\n", "width", "=", "tf", ".", "cast", "(", "features", "[", "'image/width'", "]", ",", "tf", ".", "int32", ")", "\n", "num_channels", "=", "tf", ".", "cast", "(", "features", "[", "'image/channels'", "]", ",", "tf", ".", "int32", ")", "\n", "image_shape", "=", "tf", ".", "stack", "(", "[", "height", ",", "width", ",", "num_channels", "]", ")", "\n", "image", "=", "tf", ".", "cast", "(", "image", ",", "tf", ".", "float32", ")", "\n", "preprocessed_image", "=", "preprocess", ".", "preprocess_image", "(", "image", ",", "IMAGE_HEIGHT", ",", "IMAGE_WIDTH", ",", "is_training", "=", "is_training", ")", "\n", "images", ",", "labels", "=", "tf", ".", "train", ".", "shuffle_batch", "(", "[", "preprocessed_image", ",", "label", "]", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "capacity", "=", "batch_size", "+", "num_threads", "*", "batch_size", ",", "\n", "num_threads", "=", "num_threads", ",", "\n", "min_after_dequeue", "=", "batch_size", ")", "\n", "return", "images", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.shwetabhardwaj44_RecoveringFrom_RandomPruning_WACV2018.None.finetune_PrunedModel.configure_lr": [[115, 121], ["int", "tensorflow.train.exponential_decay"], "function", ["None"], ["", "def", "configure_lr", "(", "global_step", ",", "learning_rate", ")", ":", "\n", "    ", "decay_steps", "=", "int", "(", "num_data_samples", "/", "batch_size", "*", "\n", "num_epochs_per_decay", ")", "\n", "return", "tf", ".", "train", ".", "exponential_decay", "(", "learning_rate", ",", "global_step", ",", "\n", "decay_steps", ",", "learning_rate_decay_factor", ",", "\n", "staircase", "=", "True", ",", "name", "=", "'exponential_decay_learning_rate'", ")", "\n", "\n"]]}