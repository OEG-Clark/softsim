{"home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.run_summarization.calc_running_avg_loss": [[82, 107], ["min", "tensorflow.Summary", "tf.Summary.value.add", "summary_writer.add_summary", "tensorflow.compat.v1.logging.info"], "function", ["None"], ["def", "calc_running_avg_loss", "(", "loss", ",", "running_avg_loss", ",", "summary_writer", ",", "step", ",", "decay", "=", "0.99", ")", ":", "\n", "  ", "\"\"\"Calculate the running average loss via exponential decay.\n  This is used to implement early stopping w.r.t. a more smooth loss curve than the raw loss curve.\n\n  Args:\n    loss: loss on the most recent eval step\n    running_avg_loss: running_avg_loss so far\n    summary_writer: FileWriter object to write for tensorboard\n    step: training iteration step\n    decay: rate of exponential decay, a float between 0 and 1. Larger is smoother.\n\n  Returns:\n    running_avg_loss: new running average loss\n  \"\"\"", "\n", "if", "running_avg_loss", "==", "0", ":", "# on the first iteration just take the loss", "\n", "    ", "running_avg_loss", "=", "loss", "\n", "", "else", ":", "\n", "    ", "running_avg_loss", "=", "running_avg_loss", "*", "decay", "+", "(", "1", "-", "decay", ")", "*", "loss", "\n", "", "running_avg_loss", "=", "min", "(", "running_avg_loss", ",", "12", ")", "# clip", "\n", "loss_sum", "=", "tf", ".", "Summary", "(", ")", "\n", "tag_name", "=", "'running_avg_loss/decay=%f'", "%", "(", "decay", ")", "\n", "loss_sum", ".", "value", ".", "add", "(", "tag", "=", "tag_name", ",", "simple_value", "=", "running_avg_loss", ")", "\n", "summary_writer", ".", "add_summary", "(", "loss_sum", ",", "step", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "'running_avg_loss: %f'", ",", "running_avg_loss", ")", "\n", "return", "running_avg_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.run_summarization.restore_best_model": [[109, 132], ["tensorflow.compat.v1.logging.info", "tensorflow.Session", "print", "tf.Session.run", "tensorflow.train.Saver", "print", "util.load_ckpt", "print", "[].replace", "os.path.join", "print", "tensorflow.train.Saver", "tf.train.Saver.save", "print", "exit", "tensorflow.initialize_all_variables", "util.get_config", "tensorflow.all_variables", "util.load_ckpt.split"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.util.load_ckpt", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.util.get_config"], ["", "def", "restore_best_model", "(", ")", ":", "\n", "  ", "\"\"\"Load bestmodel file from eval directory, add variables for adagrad, and save to train directory\"\"\"", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "\"Restoring bestmodel for training...\"", ")", "\n", "\n", "# Initialize all vars in the model", "\n", "sess", "=", "tf", ".", "Session", "(", "config", "=", "util", ".", "get_config", "(", ")", ")", "\n", "print", "(", "\"Initializing all variables...\"", ")", "\n", "sess", ".", "run", "(", "tf", ".", "initialize_all_variables", "(", ")", ")", "\n", "\n", "# Restore the best model from eval dir", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "[", "v", "for", "v", "in", "tf", ".", "all_variables", "(", ")", "if", "\"Adagrad\"", "not", "in", "v", ".", "name", "]", ")", "\n", "print", "(", "\"Restoring all non-adagrad variables from best model in eval dir...\"", ")", "\n", "curr_ckpt", "=", "util", ".", "load_ckpt", "(", "saver", ",", "sess", ",", "\"eval\"", ")", "\n", "print", "(", "\"Restored %s.\"", "%", "curr_ckpt", ")", "\n", "\n", "# Save this model to train dir and quit", "\n", "new_model_name", "=", "curr_ckpt", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ".", "replace", "(", "\"bestmodel\"", ",", "\"model\"", ")", "\n", "new_fname", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "log_root", ",", "\"train\"", ",", "new_model_name", ")", "\n", "print", "(", "\"Saving model to %s...\"", "%", "(", "new_fname", ")", ")", "\n", "new_saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "# this saver saves all variables that now exist, including Adagrad variables", "\n", "new_saver", ".", "save", "(", "sess", ",", "new_fname", ")", "\n", "print", "(", "\"Saved.\"", ")", "\n", "exit", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.run_summarization.convert_to_coverage_model": [[134, 156], ["tensorflow.compat.v1.logging.info", "tensorflow.Session", "print", "tf.Session.run", "tensorflow.train.Saver", "print", "util.load_ckpt", "print", "print", "tensorflow.train.Saver", "tf.train.Saver.save", "print", "exit", "tensorflow.global_variables_initializer", "util.get_config", "tensorflow.global_variables"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.util.load_ckpt", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.util.get_config"], ["", "def", "convert_to_coverage_model", "(", ")", ":", "\n", "  ", "\"\"\"Load non-coverage checkpoint, add initialized extra variables for coverage, and save as new checkpoint\"\"\"", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "\"converting non-coverage model to coverage model..\"", ")", "\n", "\n", "# initialize an entire coverage model from scratch", "\n", "sess", "=", "tf", ".", "Session", "(", "config", "=", "util", ".", "get_config", "(", ")", ")", "\n", "print", "(", "\"initializing everything...\"", ")", "\n", "sess", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "\n", "# load all non-coverage weights from checkpoint", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "[", "v", "for", "v", "in", "tf", ".", "global_variables", "(", ")", "if", "\"coverage\"", "not", "in", "v", ".", "name", "and", "\"Adagrad\"", "not", "in", "v", ".", "name", "]", ")", "\n", "print", "(", "\"restoring non-coverage variables...\"", ")", "\n", "curr_ckpt", "=", "util", ".", "load_ckpt", "(", "saver", ",", "sess", ")", "\n", "print", "(", "\"restored.\"", ")", "\n", "\n", "# save this model and quit", "\n", "new_fname", "=", "curr_ckpt", "+", "'_cov_init'", "\n", "print", "(", "\"saving model to %s...\"", "%", "(", "new_fname", ")", ")", "\n", "new_saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "# this one will save all variables that now exist", "\n", "new_saver", ".", "save", "(", "sess", ",", "new_fname", ")", "\n", "print", "(", "\"saved.\"", ")", "\n", "exit", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.run_summarization.setup_training": [[158, 187], ["os.path.join", "model.build_graph", "tensorflow.train.Saver", "tensorflow.train.Supervisor", "tensorflow.compat.v1.logging.info", "tf.train.Supervisor.prepare_or_wait_for_session", "tensorflow.compat.v1.logging.info", "os.path.exists", "os.makedirs", "run_summarization.convert_to_coverage_model", "run_summarization.restore_best_model", "run_summarization.run_training", "util.get_config", "tensorflow.compat.v1.logging.info", "tf.train.Supervisor.stop"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel.build_graph", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.run_summarization.convert_to_coverage_model", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.run_summarization.restore_best_model", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.run_summarization.run_training", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.util.get_config"], ["", "def", "setup_training", "(", "model", ",", "batcher", ",", "vocab", ",", "hps", ")", ":", "\n", "  ", "\"\"\"Does setup before starting training (run_training)\"\"\"", "\n", "train_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "log_root", ",", "\"train\"", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "train_dir", ")", ":", "os", ".", "makedirs", "(", "train_dir", ")", "\n", "\n", "model", ".", "build_graph", "(", ")", "# build the graph", "\n", "if", "FLAGS", ".", "convert_to_coverage_model", ":", "\n", "    ", "assert", "FLAGS", ".", "coverage", ",", "\"To convert your non-coverage model to a coverage model, run with convert_to_coverage_model=True and coverage=True\"", "\n", "convert_to_coverage_model", "(", ")", "\n", "", "if", "FLAGS", ".", "restore_best_model", ":", "\n", "    ", "restore_best_model", "(", ")", "\n", "", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "max_to_keep", "=", "3", ")", "# keep 3 checkpoints at a time", "\n", "\n", "sv", "=", "tf", ".", "train", ".", "Supervisor", "(", "logdir", "=", "train_dir", ",", "\n", "is_chief", "=", "True", ",", "\n", "saver", "=", "saver", ",", "\n", "summary_op", "=", "None", ",", "\n", "save_summaries_secs", "=", "600", ",", "# save summaries for tensorboard every 60 secs", "\n", "save_model_secs", "=", "None", ",", "# checkpoint every 60 secs", "\n", "global_step", "=", "model", ".", "global_step", ")", "\n", "summary_writer", "=", "sv", ".", "summary_writer", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "\"Preparing or waiting for session...\"", ")", "\n", "sess_context_manager", "=", "sv", ".", "prepare_or_wait_for_session", "(", "config", "=", "util", ".", "get_config", "(", ")", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "\"Created session.\"", ")", "\n", "try", ":", "\n", "    ", "run_training", "(", "model", ",", "batcher", ",", "sess_context_manager", ",", "sv", ",", "summary_writer", ",", "vocab", ",", "hps", ")", "# this is an infinite loop until interrupted", "\n", "", "except", "KeyboardInterrupt", ":", "\n", "    ", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "\"Caught keyboard interrupt on worker. Stopping supervisor...\"", ")", "\n", "sv", ".", "stop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.run_summarization.run_training": [[189, 257], ["tensorflow.compat.v1.logging.info", "os.path.join", "print", "batcher.Batcher", "time.time", "tensorflow.python.debug.LocalCLIDebugWrapperSession", "tf_debug.LocalCLIDebugWrapperSession.add_tensor_filter", "batcher.next_batch", "model.run_train_step", "tensorflow.compat.v1.logging.info", "tensorflow.compat.v1.logging.info", "summary_writer.add_summary", "print", "print", "tensorflow.compat.v1.logging.info", "numpy.isfinite", "Exception", "summary_writer.flush", "print", "time.time", "tensorflow.compat.v1.logging.info", "time.time", "batcher.Batcher", "run_summarization.run_eval", "print", "print", "print", "print", "print", "print", "saver.save"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Batcher.next_batch", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel.run_train_step", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.run_summarization.run_eval"], ["", "", "def", "run_training", "(", "model", ",", "batcher", ",", "sess_context_manager", ",", "sv", ",", "summary_writer", ",", "vocab", ",", "hps", ")", ":", "\n", "  ", "\"\"\"Repeatedly runs training iterations, logging loss to screen and writing summaries\"\"\"", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "\"starting run_training\"", ")", "\n", "train_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "log_root", ",", "\"train/model.ckpt\"", ")", "\n", "print", "(", "train_dir", ")", "\n", "saver", "=", "sv", ".", "saver", "\n", "with", "sess_context_manager", "as", "sess", ":", "\n", "    ", "if", "FLAGS", ".", "debug", ":", "# start the tensorflow debugger", "\n", "      ", "sess", "=", "tf_debug", ".", "LocalCLIDebugWrapperSession", "(", "sess", ")", "\n", "sess", ".", "add_tensor_filter", "(", "\"has_inf_or_nan\"", ",", "tf_debug", ".", "has_inf_or_nan", ")", "\n", "\n", "", "val_batcher", "=", "Batcher", "(", "FLAGS", ".", "valid_path", ",", "vocab", ",", "hps", ",", "single_pass", "=", "True", ")", "\n", "max_f1", ",", "max_acc", ",", "min_loss", "=", "0.0", ",", "0.0", ",", "20", "\n", "#max_f1, max_acc, min_loss = run_eval(model, sess, val_batcher)", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "while", "True", ":", "# repeats until interrupted", "\n", "      ", "batch", "=", "batcher", ".", "next_batch", "(", ")", "\n", "results", "=", "model", ".", "run_train_step", "(", "sess", ",", "batch", ")", "\n", "\n", "if", "FLAGS", ".", "coverage", ":", "\n", "        ", "print", "(", "results", "[", "'q_coverage'", "]", ")", "\n", "print", "(", "results", "[", "'r_coverage'", "]", ")", "\n", "coverage_loss", "=", "results", "[", "'coverage_loss'", "]", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "\"coverage_loss: %f\"", ",", "coverage_loss", ")", "# print the coverage loss to screen", "\n", "\n", "", "loss", "=", "results", "[", "'loss'", "]", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "'loss: %f'", ",", "loss", ")", "# print the loss to screen", "\n", "#tf.logging.info('accuracy: %f', accuracy) # print the loss to screen", "\n", "#print(results['losses'])", "\n", "if", "not", "np", ".", "isfinite", "(", "loss", ")", ":", "\n", "        ", "raise", "Exception", "(", "\"Loss is not finite. Stopping.\"", ")", "\n", "\n", "# get the summaries and iteration number so we can write summaries to tensorboard", "\n", "", "summaries", "=", "results", "[", "'summaries'", "]", "# we will write these summaries to tensorboard using summary_writer", "\n", "train_step", "=", "results", "[", "'global_step'", "]", "# we need this to update our running average loss", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "'train_step: %d'", ",", "train_step", ")", "\n", "\n", "summary_writer", ".", "add_summary", "(", "summaries", ",", "train_step", ")", "# write the summaries", "\n", "if", "train_step", "%", "100", "==", "0", ":", "# flush the summary writer every so often", "\n", "        ", "summary_writer", ".", "flush", "(", ")", "\n", "print", "(", "\"current best result: MACRO_F1 %s. ACC %s, Loss %s\"", "%", "(", "max_f1", ",", "max_acc", ",", "min_loss", ")", ")", "\n", "#print(split_acc)", "\n", "", "if", "train_step", "%", "5000", "==", "0", ":", "\n", "        ", "to_be_saved", "=", "False", "\n", "t1", "=", "time", ".", "time", "(", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "'seconds for training step: %.3f'", ",", "t1", "-", "t0", ")", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "val_batcher", "=", "Batcher", "(", "FLAGS", ".", "valid_path", ",", "vocab", ",", "hps", ",", "single_pass", "=", "True", ")", "\n", "f1", ",", "ACC", ",", "loss", "=", "run_eval", "(", "model", ",", "sess", ",", "val_batcher", ")", "\n", "if", "ACC", ">", "max_acc", ":", "\n", "          ", "max_acc", "=", "ACC", "\n", "#to_be_saved = True", "\n", "", "if", "f1", ">", "max_f1", ":", "\n", "          ", "max_f1", "=", "f1", "\n", "to_be_saved", "=", "True", "\n", "", "if", "loss", "<", "min_loss", ":", "\n", "          ", "min_loss", "=", "loss", "\n", "to_be_saved", "=", "True", "\n", "", "if", "to_be_saved", ":", "\n", "          ", "saver", ".", "save", "(", "sess", ",", "train_dir", ",", "global_step", "=", "train_step", ")", "\n", "", "print", "(", "\"current result: F1 %s\"", "%", "(", "f1", ")", ")", "\n", "print", "(", "\"current best result: MACRO_F1 %s\"", "%", "(", "max_f1", ")", ")", "\n", "print", "(", "\"current result: ACC %s\"", "%", "(", "ACC", ")", ")", "\n", "print", "(", "\"current best result: ACC %s\"", "%", "(", "max_acc", ")", ")", "\n", "print", "(", "\"current result: LOSS %s\"", "%", "(", "loss", ")", ")", "\n", "print", "(", "\"current best result: LOSS %s\"", "%", "(", "min_loss", ")", ")", "\n", "", "if", "train_step", ">", "150000", ":", "\n", "        ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.run_summarization.run_eval": [[260, 290], ["tensorflow.compat.v1.logging.info", "time.time", "time.time", "tensorflow.compat.v1.logging.info", "sklearn.metrics.precision_recall_fscore_support", "print", "f_class.mean", "val_batcher.next_batch", "model.run_eval_step", "print", "tensorflow.compat.v1.logging.info", "loss.append", "y_pred.extend", "y_true.extend", "sum", "float", "sum", "float", "len", "len", "numpy.asarray", "numpy.asarray"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Batcher.next_batch", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel.run_eval_step", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.beam_search.Hypothesis.extend", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.beam_search.Hypothesis.extend"], ["", "", "", "", "def", "run_eval", "(", "model", ",", "sess", ",", "val_batcher", ")", ":", "\n", "  ", "loss", "=", "[", "]", "\n", "y_pred", "=", "[", "]", "\n", "y_true", "=", "[", "]", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "'running validation step...'", ")", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "while", "True", ":", "\n", "    ", "val_batch", "=", "val_batcher", ".", "next_batch", "(", ")", "\n", "if", "not", "val_batch", ":", "\n", "      ", "break", "\n", "", "results", "=", "model", ".", "run_eval_step", "(", "sess", ",", "val_batch", ")", "\n", "accuracy", "=", "results", "[", "'accuracy'", "]", "\n", "l", "=", "results", "[", "'loss'", "]", "\n", "print", "(", "results", "[", "'y_prob'", "]", "[", "0", "]", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "'validation batch accuracy: %f'", ",", "accuracy", ")", "# print the accuracy to screen", "\n", "loss", ".", "append", "(", "l", ")", "\n", "y_pred", ".", "extend", "(", "results", "[", "'y_pred'", "]", ")", "\n", "y_true", ".", "extend", "(", "results", "[", "'y_true'", "]", ")", "\n", "\n", "", "t1", "=", "time", ".", "time", "(", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "'seconds for validation step: %.3f'", ",", "t1", "-", "t0", ")", "\n", "\n", "p_class", ",", "r_class", ",", "f_class", ",", "support_micro", "=", "precision_recall_fscore_support", "(", "\n", "y_true", "=", "y_true", ",", "y_pred", "=", "y_pred", ",", "labels", "=", "[", "0", ",", "1", ",", "2", "]", ",", "average", "=", "'macro'", ")", "\n", "print", "(", "f_class", ")", "\n", "F1", "=", "f_class", ".", "mean", "(", ")", "\n", "LOSS", "=", "sum", "(", "loss", ")", "/", "float", "(", "len", "(", "loss", ")", ")", "\n", "ACC", "=", "sum", "(", "np", ".", "asarray", "(", "y_pred", ")", "==", "np", ".", "asarray", "(", "y_true", ")", ")", "/", "float", "(", "len", "(", "y_pred", ")", ")", "\n", "\n", "return", "F1", ",", "ACC", ",", "LOSS", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.run_summarization.main": [[292, 345], ["tensorflow.compat.v1.logging.set_verbosity", "tensorflow.compat.v1.logging.info", "os.path.join", "data.Vocab", "FLAGS.__flags.items", "batcher.Batcher", "tensorflow.set_random_seed", "len", "Exception", "os.path.exists", "Exception", "collections.namedtuple", "print", "model.SummarizationModel", "run_summarization.setup_training", "os.makedirs", "Exception", "hps_dict.keys", "hps._replace", "model.SummarizationModel", "decode.BeamSearchDecoder", "decode.BeamSearchDecoder.decode", "ValueError"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.run_summarization.setup_training", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.BeamSearchDecoder.decode"], ["", "def", "main", "(", "unused_argv", ")", ":", "\n", "  ", "if", "len", "(", "unused_argv", ")", "!=", "1", ":", "# prints a message if you've entered flags incorrectly", "\n", "    ", "raise", "Exception", "(", "\"Problem with flags: %s\"", "%", "unused_argv", ")", "\n", "\n", "", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "INFO", ")", "# choose what level of logging you want", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "'Starting seq2seq_attention in %s mode...'", ",", "(", "FLAGS", ".", "mode", ")", ")", "\n", "\n", "# Change log_root to FLAGS.log_root/FLAGS.exp_name and create the dir if necessary", "\n", "FLAGS", ".", "log_root", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "log_root", ",", "FLAGS", ".", "exp_name", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "FLAGS", ".", "log_root", ")", ":", "\n", "    ", "if", "FLAGS", ".", "mode", "==", "\"train\"", ":", "\n", "      ", "os", ".", "makedirs", "(", "FLAGS", ".", "log_root", ")", "\n", "", "else", ":", "\n", "      ", "raise", "Exception", "(", "\"Logdir %s doesn't exist. Run in train mode to create it.\"", "%", "(", "FLAGS", ".", "log_root", ")", ")", "\n", "\n", "", "", "vocab", "=", "Vocab", "(", "FLAGS", ".", "vocab_path", ",", "FLAGS", ".", "vocab_size", ")", "# create a vocabulary", "\n", "\n", "# If in decode mode, set batch_size = beam_size", "\n", "# Reason: in decode mode, we decode one example at a time.", "\n", "# On each step, we have beam_size-many hypotheses in the beam, so we need to make a batch of these hypotheses.", "\n", "if", "FLAGS", ".", "mode", "==", "'decode'", ":", "\n", "    ", "FLAGS", ".", "batch_size", "=", "FLAGS", ".", "beam_size", "\n", "\n", "# If single_pass=True, check we're in decode mode", "\n", "", "if", "FLAGS", ".", "single_pass", "and", "FLAGS", ".", "mode", "!=", "'decode'", ":", "\n", "    ", "raise", "Exception", "(", "\"The single_pass flag should only be True in decode mode\"", ")", "\n", "\n", "# Make a namedtuple hps, containing the values of the hyperparameters that the model needs", "\n", "", "hparam_list", "=", "[", "'mode'", ",", "'lr'", ",", "'adagrad_init_acc'", ",", "'rand_unif_init_mag'", ",", "'trunc_norm_init_std'", ",", "'max_grad_norm'", ",", "'hidden_dim'", ",", "'emb_dim'", ",", "'review_num'", ",", "'batch_size'", ",", "'max_dec_steps'", ",", "'max_enc_steps'", ",", "'coverage'", ",", "'cov_loss_wt'", ",", "'sa_loss_wt'", ",", "'pointer_gen'", ",", "'glove'", "]", "\n", "hps_dict", "=", "{", "}", "\n", "for", "key", ",", "val", "in", "FLAGS", ".", "__flags", ".", "items", "(", ")", ":", "# for each flag", "\n", "    ", "if", "key", "in", "hparam_list", ":", "# if it's in the list", "\n", "      ", "hps_dict", "[", "key", "]", "=", "val", ".", "value", "# add it to the dict", "\n", "", "", "hps", "=", "namedtuple", "(", "\"HParams\"", ",", "hps_dict", ".", "keys", "(", ")", ")", "(", "**", "hps_dict", ")", "\n", "\n", "# Create a batcher object that will create minibatches of data", "\n", "batcher", "=", "Batcher", "(", "FLAGS", ".", "data_path", ",", "vocab", ",", "hps", ",", "single_pass", "=", "FLAGS", ".", "single_pass", ")", "\n", "\n", "\n", "tf", ".", "set_random_seed", "(", "111", ")", "# a seed value for randomness", "\n", "\n", "if", "hps", ".", "mode", "==", "'train'", ":", "\n", "    ", "print", "(", "\"creating model...\"", ")", "\n", "model", "=", "SummarizationModel", "(", "hps", ",", "vocab", ")", "\n", "setup_training", "(", "model", ",", "batcher", ",", "vocab", ",", "hps", ")", "\n", "", "elif", "hps", ".", "mode", "==", "'decode'", ":", "\n", "    ", "decode_model_hps", "=", "hps", "# This will be the hyperparameters for the decoder model", "\n", "decode_model_hps", "=", "hps", ".", "_replace", "(", "max_dec_steps", "=", "1", ")", "# The model is configured with max_dec_steps=1 because we only ever run one step of the decoder at a time (to do beam search). Note that the batcher is initialized with max_dec_steps equal to e.g. 100 because the batches need to contain the full summaries", "\n", "model", "=", "SummarizationModel", "(", "decode_model_hps", ",", "vocab", ")", "\n", "decoder", "=", "BeamSearchDecoder", "(", "model", ",", "batcher", ",", "vocab", ")", "\n", "decoder", ".", "decode", "(", ")", "# decode indefinitely (unless single_pass=True, in which case deocde the dataset exactly once)", "\n", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"The 'mode' flag must be one of train/eval/decode\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.attention_decoder.attention_decoder": [[27, 246], ["tensorflow.python.ops.variable_scope.variable_scope", "tensorflow.reshape", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.python.ops.variable_scope.get_variable", "tensorflow.python.ops.nn_ops.conv2d", "tensorflow.python.ops.variable_scope.get_variable", "tensorflow.python.ops.nn_ops.conv2d", "tensorflow.python.ops.variable_scope.get_variable", "tensorflow.python.ops.variable_scope.get_variable", "tensorflow.python.ops.array_ops.zeros", "array_ops.zeros.set_shape", "tensorflow.python.ops.array_ops.zeros", "array_ops.zeros.set_shape", "enumerate", "tensorflow.expand_dims", "tensorflow.expand_dims", "attention_decoder.attention_decoder.q_attention"], "function", ["None"], ["def", "attention_decoder", "(", "decoder_inputs", ",", "initial_state", ",", "question_states", ",", "review_states", ",", "sent_attention", ",", "num", ",", "q_padding_mask", ",", "r_padding_mask", ",", "cell", ",", "initial_state_attention", "=", "False", ",", "pointer_gen", "=", "True", ",", "use_coverage", "=", "False", ",", "q_prev_coverage", "=", "None", ",", "r_prev_coverage", "=", "None", ")", ":", "\n", "  ", "\"\"\"\n  Args:\n    decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n    initial_state: 2D Tensor [batch_size x cell.state_size].\n    question_states: 3D Tensor [batch_size x q_attn_length x attn_size].\n    review_states: 3D Tensor [batch_size x review_num*r_attn_length x attn_size].\n    q_padding_mask: 2D Tensor [batch_size x q_attn_length] containing 1s and 0s; indicates which of the question locations are padding (0) or a real token (1).\n    r_padding_mask: 3D Tensor [batch_size x review_num x r_attn_length] containing 1s and 0s; indicates which of the question locations are padding (0) or a real token (1).\n    cell: rnn_cell.RNNCell defining the cell function and size.\n    initial_state_attention:\n      Note that this attention decoder passes each decoder input through a linear layer with the previous step's context vector to get a modified version of the input. If initial_state_attention is False, on the first decoder step the \"previous context vector\" is just a zero vector. If initial_state_attention is True, we use initial_state to (re)calculate the previous step's context vector. We set this to False for train/eval mode (because we call attention_decoder once for all decoder steps) and True for decode mode (because we call attention_decoder once for each decoder step).\n    pointer_gen: boolean. If True, calculate the generation probability p_gen for each decoder step.\n    use_coverage: boolean. If True, use coverage mechanism.\n    prev_coverage:\n      If not None, a tensor with shape (batch_size, attn_length). The previous step's coverage vector. This is only not None in decode mode when using coverage.\n\n  Returns:\n    outputs: A list of the same length as decoder_inputs of 2D Tensors of\n      shape [batch_size x cell.output_size]. The output vectors.\n    state: The final state of the decoder. A tensor shape [batch_size x cell.state_size].\n    attn_dists: A list containing tensors of shape (batch_size,attn_length).\n      The attention distributions for each decoder step.\n    p_gens: List of scalars. The values of p_gen for each decoder step. Empty list if pointer_gen=False.\n    coverage: Coverage vector on the last step computed. None if use_coverage=False.\n  \"\"\"", "\n", "with", "variable_scope", ".", "variable_scope", "(", "\"attention_decoder\"", ")", "as", "scope", ":", "\n", "    ", "batch_size", "=", "question_states", ".", "get_shape", "(", ")", "[", "0", "]", ".", "value", "# if this line fails, it's because the batch size isn't defined", "\n", "attn_size", "=", "question_states", ".", "get_shape", "(", ")", "[", "2", "]", ".", "value", "# if this line fails, it's because the attention length isn't defined", "\n", "r_padding_mask", "=", "tf", ".", "reshape", "(", "r_padding_mask", ",", "[", "batch_size", ",", "-", "1", "]", ")", "\n", "\n", "# Reshape encoder_states (need to insert a dim)", "\n", "question_states", "=", "tf", ".", "expand_dims", "(", "question_states", ",", "axis", "=", "2", ")", "# now is shape (batch_size, attn_len, 1, attn_size)", "\n", "review_states", "=", "tf", ".", "expand_dims", "(", "review_states", ",", "axis", "=", "2", ")", "\n", "\n", "# To calculate attention, we calculate", "\n", "#   v^T tanh(W_h h_i + W_s s_t  + b_attn)", "\n", "# where h_i is an encoder state, s_t is a decoder state", "\n", "# attn_vec_size is the length of the vectors v, b_attn, (W_h h_i) and (W_s s_t).", "\n", "# We set it to be equal to the size of the encoder states.", "\n", "attention_vec_size", "=", "attn_size", "\n", "\n", "# Get the weight matrix W_h and apply it to each encoder state to get (W_h h_i), the encoder features", "\n", "W_h", "=", "variable_scope", ".", "get_variable", "(", "\"W_h\"", ",", "[", "1", ",", "1", ",", "attn_size", ",", "attention_vec_size", "]", ")", "\n", "question_features", "=", "nn_ops", ".", "conv2d", "(", "question_states", ",", "W_h", ",", "[", "1", ",", "1", ",", "1", ",", "1", "]", ",", "\"SAME\"", ")", "# shape (batch_size,attn_length,1,attention_vec_size)", "\n", "W_r", "=", "variable_scope", ".", "get_variable", "(", "\"W_r\"", ",", "[", "1", ",", "1", ",", "attn_size", ",", "attention_vec_size", "]", ")", "\n", "review_features", "=", "nn_ops", ".", "conv2d", "(", "review_states", ",", "W_r", ",", "[", "1", ",", "1", ",", "1", ",", "1", "]", ",", "\"SAME\"", ")", "# shape (batch_size,review_num*attn_length,1,attention_vec_size)", "\n", "\n", "# Get the weight vectors v and w_c (w_c is for coverage)", "\n", "v_q", "=", "variable_scope", ".", "get_variable", "(", "\"v_q\"", ",", "[", "attention_vec_size", "]", ")", "\n", "v_r", "=", "variable_scope", ".", "get_variable", "(", "\"v_r\"", ",", "[", "attention_vec_size", "]", ")", "\n", "if", "use_coverage", ":", "\n", "      ", "with", "variable_scope", ".", "variable_scope", "(", "\"coverage\"", ")", ":", "\n", "        ", "w_q_c", "=", "variable_scope", ".", "get_variable", "(", "\"w_q_c\"", ",", "[", "1", ",", "1", ",", "1", ",", "attention_vec_size", "]", ")", "\n", "w_r_c", "=", "variable_scope", ".", "get_variable", "(", "\"w_r_c\"", ",", "[", "1", ",", "1", ",", "1", ",", "attention_vec_size", "]", ")", "\n", "\n", "", "", "if", "q_prev_coverage", "is", "not", "None", ":", "# for beam search mode with coverage", "\n", "# reshape from (batch_size, attn_length) to (batch_size, attn_len, 1, 1)", "\n", "      ", "q_prev_coverage", "=", "tf", ".", "expand_dims", "(", "tf", ".", "expand_dims", "(", "q_prev_coverage", ",", "2", ")", ",", "3", ")", "\n", "", "if", "r_prev_coverage", "is", "not", "None", ":", "# for beam search mode with coverage", "\n", "# reshape from (batch_size, attn_length) to (batch_size, attn_len, 1, 1)", "\n", "      ", "r_prev_coverage", "=", "tf", ".", "expand_dims", "(", "tf", ".", "expand_dims", "(", "r_prev_coverage", ",", "2", ")", ",", "3", ")", "\n", "\n", "", "def", "q_attention", "(", "decoder_state", ",", "coverage", "=", "None", ")", ":", "\n", "      ", "\"\"\"Calculate the context vector and attention distribution from the decoder state.\n\n      Args:\n        decoder_state: state of the decoder\n        coverage: Optional. Previous timestep's coverage vector, shape (batch_size, attn_len, 1, 1).\n\n      Returns:\n        context_vector: weighted sum of encoder_states\n        attn_dist: attention distribution\n        coverage: new coverage vector. shape (batch_size, attn_len, 1, 1)\n      \"\"\"", "\n", "with", "variable_scope", ".", "variable_scope", "(", "\"Question_Attention\"", ")", ":", "\n", "# Pass the decoder state through a linear layer (this is W_s s_t + b_attn in the paper)", "\n", "        ", "decoder_features", "=", "linear", "(", "decoder_state", ",", "attention_vec_size", ",", "True", ")", "# shape (batch_size, attention_vec_size)", "\n", "decoder_features", "=", "tf", ".", "expand_dims", "(", "tf", ".", "expand_dims", "(", "decoder_features", ",", "1", ")", ",", "1", ")", "# reshape to (batch_size, 1, 1, attention_vec_size)", "\n", "\n", "if", "use_coverage", "and", "coverage", "is", "not", "None", ":", "# non-first step of coverage", "\n", "# Multiply coverage vector by w_c to get coverage_features.", "\n", "          ", "coverage_features", "=", "nn_ops", ".", "conv2d", "(", "coverage", ",", "w_q_c", ",", "[", "1", ",", "1", ",", "1", ",", "1", "]", ",", "\"SAME\"", ")", "# c has shape (batch_size, attn_length, 1, attention_vec_size)", "\n", "\n", "# Calculate v^T tanh(W_h h_i + W_s s_t + w_c c_i^t + b_attn)", "\n", "e", "=", "math_ops", ".", "reduce_sum", "(", "v_q", "*", "math_ops", ".", "tanh", "(", "question_features", "+", "decoder_features", "+", "coverage_features", ")", ",", "[", "2", ",", "3", "]", ")", "# shape (batch_size,attn_length)", "\n", "\n", "# Calculate attention distribution", "\n", "attn_dist", "=", "masked_attention_q", "(", "e", ",", "q_padding_mask", ")", "\n", "\n", "# Update coverage vector", "\n", "coverage", "+=", "array_ops", ".", "reshape", "(", "attn_dist", ",", "[", "batch_size", ",", "-", "1", ",", "1", ",", "1", "]", ")", "\n", "", "else", ":", "\n", "# Calculate v^T tanh(W_h h_i + W_s s_t + b_attn)", "\n", "          ", "e", "=", "math_ops", ".", "reduce_sum", "(", "v_q", "*", "math_ops", ".", "tanh", "(", "question_features", "+", "decoder_features", ")", ",", "[", "2", ",", "3", "]", ")", "# calculate e", "\n", "\n", "# Calculate attention distribution", "\n", "attn_dist", "=", "masked_attention_q", "(", "e", ",", "q_padding_mask", ")", "\n", "\n", "if", "use_coverage", ":", "# first step of training", "\n", "            ", "coverage", "=", "tf", ".", "expand_dims", "(", "tf", ".", "expand_dims", "(", "attn_dist", ",", "2", ")", ",", "2", ")", "# initialize coverage", "\n", "\n", "# Calculate the context vector from attn_dist and encoder_states", "\n", "", "", "context_vector", "=", "math_ops", ".", "reduce_sum", "(", "array_ops", ".", "reshape", "(", "attn_dist", ",", "[", "batch_size", ",", "-", "1", ",", "1", ",", "1", "]", ")", "*", "question_states", ",", "[", "1", ",", "2", "]", ")", "# shape (batch_size, attn_size).", "\n", "context_vector", "=", "array_ops", ".", "reshape", "(", "context_vector", ",", "[", "-", "1", ",", "attn_size", "]", ")", "\n", "\n", "", "return", "context_vector", ",", "attn_dist", ",", "coverage", "\n", "\n", "", "def", "r_attention", "(", "decoder_state", ",", "coverage", "=", "None", ")", ":", "\n", "      ", "\"\"\"Calculate the context vector and attention distribution from the decoder state.\n\n      Args:\n        decoder_state: state of the decoder\n        coverage: Optional. Previous timestep's coverage vector, shape (batch_size, attn_len, 1, 1).\n\n      Returns:\n        context_vector: weighted sum of encoder_states\n        attn_dist: attention distribution\n        coverage: new coverage vector. shape (batch_size, attn_len, 1, 1)\n      \"\"\"", "\n", "with", "variable_scope", ".", "variable_scope", "(", "\"Review_Attention\"", ")", ":", "\n", "# Pass the decoder state through a linear layer (this is W_s s_t + b_attn in the paper)", "\n", "        ", "decoder_features", "=", "linear", "(", "decoder_state", ",", "attention_vec_size", ",", "True", ")", "# shape (batch_size, attention_vec_size)", "\n", "decoder_features", "=", "tf", ".", "expand_dims", "(", "tf", ".", "expand_dims", "(", "decoder_features", ",", "1", ")", ",", "1", ")", "# reshape to (batch_size, 1, 1, attention_vec_size)", "\n", "\n", "if", "use_coverage", "and", "coverage", "is", "not", "None", ":", "# non-first step of coverage", "\n", "# Multiply coverage vector by w_c to get coverage_features.", "\n", "          ", "coverage_features", "=", "nn_ops", ".", "conv2d", "(", "coverage", ",", "w_r_c", ",", "[", "1", ",", "1", ",", "1", ",", "1", "]", ",", "\"SAME\"", ")", "# c has shape (batch_size, attn_length, 1, attention_vec_size)", "\n", "\n", "# Calculate v^T tanh(W_h h_i + W_s s_t + w_c c_i^t + b_attn)", "\n", "e", "=", "math_ops", ".", "reduce_sum", "(", "v_r", "*", "math_ops", ".", "tanh", "(", "review_features", "+", "decoder_features", "+", "coverage_features", ")", ",", "[", "2", ",", "3", "]", ")", "# shape (batch_size,attn_length)", "\n", "\n", "# Calculate attention distribution", "\n", "attn_dist", "=", "masked_attention_r", "(", "e", ",", "r_padding_mask", ",", "sent_attention", ",", "batch_size", ",", "num", ")", "\n", "\n", "# Update coverage vector", "\n", "coverage", "+=", "array_ops", ".", "reshape", "(", "attn_dist", ",", "[", "batch_size", ",", "-", "1", ",", "1", ",", "1", "]", ")", "\n", "", "else", ":", "\n", "# Calculate v^T tanh(W_h h_i + W_s s_t + b_attn)", "\n", "          ", "e", "=", "math_ops", ".", "reduce_sum", "(", "v_r", "*", "math_ops", ".", "tanh", "(", "review_features", "+", "decoder_features", ")", ",", "[", "2", ",", "3", "]", ")", "# calculate e", "\n", "\n", "# Calculate attention distribution", "\n", "attn_dist", "=", "masked_attention_r", "(", "e", ",", "r_padding_mask", ",", "sent_attention", ",", "batch_size", ",", "num", ")", "\n", "\n", "if", "use_coverage", ":", "# first step of training", "\n", "            ", "coverage", "=", "tf", ".", "expand_dims", "(", "tf", ".", "expand_dims", "(", "attn_dist", ",", "2", ")", ",", "2", ")", "# initialize coverage", "\n", "\n", "# Calculate the context vector from attn_dist and encoder_states", "\n", "", "", "context_vector", "=", "math_ops", ".", "reduce_sum", "(", "array_ops", ".", "reshape", "(", "attn_dist", ",", "[", "batch_size", ",", "-", "1", ",", "1", ",", "1", "]", ")", "*", "review_states", ",", "[", "1", ",", "2", "]", ")", "# shape (batch_size, attn_size).", "\n", "context_vector", "=", "array_ops", ".", "reshape", "(", "context_vector", ",", "[", "-", "1", ",", "attn_size", "]", ")", "\n", "\n", "", "return", "context_vector", ",", "attn_dist", ",", "coverage", "\n", "\n", "", "outputs", "=", "[", "]", "\n", "q_attn_dists", "=", "[", "]", "\n", "r_attn_dists", "=", "[", "]", "\n", "p_gens", "=", "[", "]", "\n", "state", "=", "initial_state", "\n", "output_states", "=", "[", "]", "\n", "q_coverage", "=", "q_prev_coverage", "# initialize coverage to None or whatever was passed in", "\n", "r_coverage", "=", "r_prev_coverage", "\n", "q_context_vector", "=", "array_ops", ".", "zeros", "(", "[", "batch_size", ",", "attn_size", "]", ")", "\n", "q_context_vector", ".", "set_shape", "(", "[", "None", ",", "attn_size", "]", ")", "# Ensure the second shape of attention vectors is set.", "\n", "r_context_vector", "=", "array_ops", ".", "zeros", "(", "[", "batch_size", ",", "attn_size", "]", ")", "\n", "r_context_vector", ".", "set_shape", "(", "[", "None", ",", "attn_size", "]", ")", "# Ensure the second shape of attention vectors is set.", "\n", "if", "initial_state_attention", ":", "# true in decode mode", "\n", "# Re-calculate the context vector from the previous step so that we can pass it through a linear layer with this step's input to get a modified version of the input", "\n", "      ", "q_context_vector", ",", "_", ",", "q_coverage", "=", "q_attention", "(", "initial_state", ",", "q_coverage", ")", "# in decode mode, this is what updates the coverage vector", "\n", "r_context_vector", ",", "_", ",", "r_coverage", "=", "r_attention", "(", "initial_state", ",", "r_coverage", ")", "\n", "", "for", "i", ",", "inp", "in", "enumerate", "(", "decoder_inputs", ")", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"Adding attention_decoder timestep %i of %i\"", ",", "i", ",", "len", "(", "decoder_inputs", ")", ")", "\n", "if", "i", ">", "0", ":", "\n", "        ", "variable_scope", ".", "get_variable_scope", "(", ")", ".", "reuse_variables", "(", ")", "\n", "\n", "# Merge input and previous attentions into one vector x of the same size as inp", "\n", "", "input_size", "=", "inp", ".", "get_shape", "(", ")", ".", "with_rank", "(", "2", ")", "[", "1", "]", "\n", "if", "input_size", ".", "value", "is", "None", ":", "\n", "        ", "raise", "ValueError", "(", "\"Could not infer input size from input: %s\"", "%", "inp", ".", "name", ")", "\n", "", "x", "=", "linear", "(", "[", "inp", "]", "+", "[", "q_context_vector", "]", "+", "[", "r_context_vector", "]", ",", "input_size", ",", "True", ")", "\n", "#x = inp", "\n", "\n", "# Run the decoder RNN cell. cell_output = decoder state", "\n", "cell_output", ",", "state", "=", "cell", "(", "x", ",", "state", ")", "\n", "output_states", ".", "append", "(", "cell_output", ")", "\n", "\n", "# Run the attention mechanism.", "\n", "if", "i", "==", "0", "and", "initial_state_attention", ":", "# always true in decode mode", "\n", "        ", "with", "variable_scope", ".", "variable_scope", "(", "variable_scope", ".", "get_variable_scope", "(", ")", ",", "reuse", "=", "True", ")", ":", "# you need this because you've already run the initial attention(...) call", "\n", "          ", "q_context_vector", ",", "q_attn_dist", ",", "_", "=", "q_attention", "(", "state", ",", "q_coverage", ")", "# don't allow coverage to update", "\n", "r_context_vector", ",", "r_attn_dist", ",", "_", "=", "r_attention", "(", "state", ",", "r_coverage", ")", "\n", "", "", "else", ":", "\n", "        ", "q_context_vector", ",", "q_attn_dist", ",", "q_coverage", "=", "q_attention", "(", "state", ",", "q_coverage", ")", "\n", "r_context_vector", ",", "r_attn_dist", ",", "r_coverage", "=", "r_attention", "(", "state", ",", "r_coverage", ")", "\n", "", "q_attn_dists", ".", "append", "(", "q_attn_dist", ")", "\n", "r_attn_dists", ".", "append", "(", "r_attn_dist", ")", "\n", "\n", "# Calculate p_gen", "\n", "if", "pointer_gen", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "'calculate_pgen'", ")", ":", "\n", "#p_gen = linear([q_context_vector, r_context_vector, state.c, state.h, x], 3, True) # a scalar", "\n", "          ", "p_gen", "=", "linear", "(", "[", "cell_output", "]", "+", "[", "q_context_vector", "]", "+", "[", "r_context_vector", "]", ",", "3", ",", "True", ")", "# a scalar", "\n", "p_gen", "=", "tf", ".", "nn", ".", "softmax", "(", "p_gen", ")", "\n", "p_gen", "=", "tf", ".", "split", "(", "p_gen", ",", "3", ",", "1", ")", "\n", "p_gens", ".", "append", "(", "p_gen", ")", "\n", "\n", "# Concatenate the cell_output (= decoder state) and the context vector, and pass them through a linear layer", "\n", "# This is V[s_t, h*_t] + b in the paper", "\n", "", "", "with", "variable_scope", ".", "variable_scope", "(", "\"AttnOutputProjection\"", ")", ":", "\n", "        ", "output", "=", "linear", "(", "[", "cell_output", "]", "+", "[", "q_context_vector", "]", "+", "[", "r_context_vector", "]", ",", "cell", ".", "output_size", ",", "True", ")", "\n", "#output = linear([cell_output], cell.output_size, True)", "\n", "", "outputs", ".", "append", "(", "output", ")", "\n", "\n", "# If using coverage, reshape it", "\n", "", "if", "r_coverage", "is", "not", "None", ":", "\n", "      ", "r_coverage", "=", "array_ops", ".", "reshape", "(", "r_coverage", ",", "[", "batch_size", ",", "-", "1", "]", ")", "\n", "", "if", "q_coverage", "is", "not", "None", ":", "\n", "      ", "q_coverage", "=", "array_ops", ".", "reshape", "(", "q_coverage", ",", "[", "batch_size", ",", "-", "1", "]", ")", "\n", "\n", "", "return", "outputs", ",", "output_states", ",", "state", ",", "q_attn_dists", ",", "r_attn_dists", ",", "p_gens", ",", "q_coverage", ",", "r_coverage", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.attention_decoder.masked_attention_q": [[247, 253], ["tensorflow.reduce_sum", "tensorflow.python.ops.nn_ops.softmax", "tensorflow.reshape"], "function", ["None"], ["", "", "def", "masked_attention_q", "(", "e", ",", "padding_mask", ")", ":", "\n", "  ", "\"\"\"Take softmax of e then apply enc_padding_mask and re-normalize\"\"\"", "\n", "attn_dist", "=", "nn_ops", ".", "softmax", "(", "e", ")", "+", "1e-8", "# take softmax. shape (batch_size, attn_length)", "\n", "attn_dist", "*=", "padding_mask", "# apply mask", "\n", "masked_sums", "=", "tf", ".", "reduce_sum", "(", "attn_dist", ",", "axis", "=", "1", ")", "# shape (batch_size)", "\n", "return", "attn_dist", "/", "tf", ".", "reshape", "(", "masked_sums", ",", "[", "-", "1", ",", "1", "]", ")", "# re-normalize", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.attention_decoder.masked_attention_r": [[269, 285], ["tensorflow.python.ops.nn_ops.softmax", "tensorflow.expand_dims", "tensorflow.reshape", "tensorflow.multiply", "tensorflow.reshape", "tensorflow.divide", "tensorflow.reshape", "tensorflow.reduce_sum", "tensorflow.reshape", "tensorflow.expand_dims", "tensorflow.reduce_sum"], "function", ["None"], ["def", "masked_attention_r", "(", "e", ",", "padding_mask", ",", "review_attention", ",", "batch_size", ",", "num", ")", ":", "\n", "  ", "attn_dist", "=", "nn_ops", ".", "softmax", "(", "e", ")", "#+1e-8 # take softmax. shape (batch_size, attn_length)", "\n", "\n", "review_attention", "=", "tf", ".", "expand_dims", "(", "review_attention", ",", "-", "1", ")", "\n", "attn_dist", "=", "tf", ".", "reshape", "(", "attn_dist", ",", "[", "batch_size", ",", "num", ",", "-", "1", "]", ")", "\n", "attn_dist", "=", "tf", ".", "multiply", "(", "attn_dist", ",", "review_attention", ")", "\n", "attn_norm", "=", "tf", ".", "reshape", "(", "attn_dist", ",", "[", "batch_size", ",", "-", "1", "]", ")", "\n", "attn_dist", "=", "tf", ".", "divide", "(", "attn_norm", ",", "tf", ".", "expand_dims", "(", "tf", ".", "reduce_sum", "(", "attn_norm", ",", "1", ")", ",", "-", "1", ")", ")", "\n", "\n", "#padding_mask *= review_attention", "\n", "padding_mask", "=", "tf", ".", "reshape", "(", "padding_mask", ",", "[", "batch_size", ",", "-", "1", "]", ")", "\n", "attn_dist", "+=", "1e-8", "\n", "attn_dist", "*=", "padding_mask", "# apply mask", "\n", "masked_sums", "=", "tf", ".", "reduce_sum", "(", "attn_dist", ",", "axis", "=", "1", ")", "# shape (batch_size)", "\n", "attn_dist", "/=", "tf", ".", "reshape", "(", "masked_sums", ",", "[", "-", "1", ",", "1", "]", ")", "# re-normalize", "\n", "return", "attn_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.attention_decoder.linear": [[286, 339], ["total_arg_sizes.append", "tensorflow.variable_scope", "range", "tensorflow.get_variable", "tensorflow.add_n", "ValueError", "isinstance", "a.get_shape().as_list", "len", "tensorflow.get_variable", "tensorflow.add_n", "isinstance", "len", "ValueError", "ValueError", "isinstance", "len", "res.append", "res.append", "tensorflow.constant_initializer", "a.get_shape", "str", "tensorflow.matmul", "tensorflow.matmul", "str", "str", "tensorflow.concat"], "function", ["None"], ["", "def", "linear", "(", "args", ",", "output_size", ",", "bias", ",", "bias_start", "=", "0.0", ",", "scope", "=", "None", ")", ":", "\n", "  ", "\"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n\n  Args:\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n    output_size: int, second dimension of W[i].\n    bias: boolean, whether to add a bias term or not.\n    bias_start: starting value to initialize the bias; 0 by default.\n    scope: VariableScope for the created subgraph; defaults to \"Linear\".\n\n  Returns:\n    A 2D Tensor with shape [batch x output_size] equal to\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n\n  Raises:\n    ValueError: if some of the arguments has unspecified or wrong shape.\n  \"\"\"", "\n", "total_arg_sizes", "=", "[", "]", "\n", "for", "arg", "in", "args", ":", "\n", "    ", "if", "arg", "is", "None", "or", "(", "isinstance", "(", "arg", ",", "(", "list", ",", "tuple", ")", ")", "and", "not", "arg", ")", ":", "\n", "      ", "raise", "ValueError", "(", "\"`arg` must be specified\"", ")", "\n", "", "if", "not", "isinstance", "(", "arg", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "      ", "arg", "=", "[", "arg", "]", "\n", "\n", "# Calculate the total size of arguments on dimension 1.", "\n", "", "total_arg_size", "=", "0", "\n", "shapes", "=", "[", "a", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "for", "a", "in", "arg", "]", "\n", "for", "shape", "in", "shapes", ":", "\n", "      ", "if", "len", "(", "shape", ")", "!=", "2", ":", "\n", "        ", "raise", "ValueError", "(", "\"Linear is expecting 2D arguments: %s\"", "%", "str", "(", "shapes", ")", ")", "\n", "", "if", "not", "shape", "[", "1", "]", ":", "\n", "        ", "raise", "ValueError", "(", "\"Linear expects shape[1] of arguments: %s\"", "%", "str", "(", "shapes", ")", ")", "\n", "", "else", ":", "\n", "        ", "total_arg_size", "+=", "shape", "[", "1", "]", "\n", "", "", "total_arg_sizes", ".", "append", "(", "total_arg_size", ")", "\n", "\n", "# Now the computation.", "\n", "", "with", "tf", ".", "variable_scope", "(", "scope", "or", "\"Linear\"", ")", ":", "\n", "    ", "res", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "args", ")", ")", ":", "\n", "      ", "matrix", "=", "tf", ".", "get_variable", "(", "\"Matrix\"", "+", "str", "(", "i", ")", ",", "[", "total_arg_sizes", "[", "i", "]", ",", "output_size", "]", ")", "\n", "arg", "=", "args", "[", "i", "]", "\n", "if", "not", "isinstance", "(", "arg", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "arg", "=", "[", "arg", "]", "\n", "", "if", "len", "(", "arg", ")", "==", "1", ":", "\n", "        ", "res", ".", "append", "(", "tf", ".", "matmul", "(", "arg", "[", "0", "]", ",", "matrix", ")", ")", "\n", "", "else", ":", "\n", "        ", "res", ".", "append", "(", "tf", ".", "matmul", "(", "tf", ".", "concat", "(", "axis", "=", "1", ",", "values", "=", "arg", ")", ",", "matrix", ")", ")", "\n", "", "", "if", "not", "bias", ":", "\n", "      ", "return", "tf", ".", "add_n", "(", "res", ")", "\n", "", "bias_term", "=", "tf", ".", "get_variable", "(", "\n", "\"Bias\"", ",", "[", "output_size", "]", ",", "initializer", "=", "tf", ".", "constant_initializer", "(", "bias_start", ")", ")", "\n", "", "return", "tf", ".", "add_n", "(", "res", ")", "+", "bias_term", "\n", "", ""]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.beam_search.Hypothesis.__init__": [[28, 47], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "tokens", ",", "log_probs", ",", "state", ",", "q_attn_dists", ",", "r_attn_dists", ",", "p_gens", ",", "q_coverage", ",", "r_coverage", ")", ":", "\n", "    ", "\"\"\"Hypothesis constructor.\n\n    Args:\n      tokens: List of integers. The ids of the tokens that form the summary so far.\n      log_probs: List, same length as tokens, of floats, giving the log probabilities of the tokens so far.\n      state: Current state of the decoder, a LSTMStateTuple.\n      attn_dists: List, same length as tokens, of numpy arrays with shape (attn_length). These are the attention distributions so far.\n      p_gens: List, same length as tokens, of floats, or None if not using pointer-generator model. The values of the generation probability so far.\n      coverage: Numpy array of shape (attn_length), or None if not using coverage. The current coverage vector.\n    \"\"\"", "\n", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "log_probs", "=", "log_probs", "\n", "self", ".", "state", "=", "state", "\n", "self", ".", "q_attn_dists", "=", "q_attn_dists", "\n", "self", ".", "r_attn_dists", "=", "r_attn_dists", "\n", "self", ".", "p_gens", "=", "p_gens", "\n", "self", ".", "q_coverage", "=", "q_coverage", "\n", "self", ".", "r_coverage", "=", "r_coverage", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.beam_search.Hypothesis.extend": [[48, 69], ["beam_search.Hypothesis"], "methods", ["None"], ["", "def", "extend", "(", "self", ",", "token", ",", "log_prob", ",", "state", ",", "q_attn_dist", ",", "r_attn_dist", ",", "p_gen", ",", "q_coverage", ",", "r_coverage", ")", ":", "\n", "    ", "\"\"\"Return a NEW hypothesis, extended with the information from the latest step of beam search.\n\n    Args:\n      token: Integer. Latest token produced by beam search.\n      log_prob: Float. Log prob of the latest token.\n      state: Current decoder state, a LSTMStateTuple.\n      attn_dist: Attention distribution from latest step. Numpy array shape (attn_length).\n      p_gen: Generation probability on latest step. Float.\n      coverage: Latest coverage vector. Numpy array shape (attn_length), or None if not using coverage.\n    Returns:\n      New Hypothesis for next step.\n    \"\"\"", "\n", "return", "Hypothesis", "(", "tokens", "=", "self", ".", "tokens", "+", "[", "token", "]", ",", "\n", "log_probs", "=", "self", ".", "log_probs", "+", "[", "log_prob", "]", ",", "\n", "state", "=", "state", ",", "\n", "q_attn_dists", "=", "self", ".", "q_attn_dists", "+", "[", "q_attn_dist", "]", ",", "\n", "r_attn_dists", "=", "self", ".", "r_attn_dists", "+", "[", "r_attn_dist", "]", ",", "\n", "p_gens", "=", "self", ".", "p_gens", "+", "[", "p_gen", "]", ",", "\n", "q_coverage", "=", "q_coverage", ",", "\n", "r_coverage", "=", "r_coverage", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.beam_search.Hypothesis.latest_token": [[70, 73], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "latest_token", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "tokens", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.beam_search.Hypothesis.log_prob": [[74, 78], ["sum"], "methods", ["None"], ["", "@", "property", "\n", "def", "log_prob", "(", "self", ")", ":", "\n", "# the log probability of the hypothesis so far is the sum of the log probabilities of the tokens so far", "\n", "    ", "return", "sum", "(", "self", ".", "log_probs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.beam_search.Hypothesis.avg_log_prob": [[79, 83], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "avg_log_prob", "(", "self", ")", ":", "\n", "# normalize log probability by number of tokens (otherwise longer sequences always have lower probability)", "\n", "    ", "return", "self", ".", "log_prob", "/", "len", "(", "self", ".", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.beam_search.run_beam_search": [[85, 174], ["model.run_encoder", "beam_search.sort_hyps", "beam_search.Hypothesis", "model.decode_onestep", "range", "beam_search.sort_hyps", "len", "range", "len", "len", "range", "numpy.zeros", "numpy.zeros", "vocab.word2id", "h.extend", "all_hyps.append", "vocab.word2id", "hyps.append", "vocab.word2id", "range", "results.append", "len", "len", "vocab.size"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel.run_encoder", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.beam_search.sort_hyps", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel.decode_onestep", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.beam_search.sort_hyps", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.beam_search.Hypothesis.extend", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.size"], ["", "", "def", "run_beam_search", "(", "sess", ",", "model", ",", "vocab", ",", "batch", ")", ":", "\n", "  ", "\"\"\"Performs beam search decoding on the given example.\n\n  Args:\n    sess: a tf.Session\n    model: a seq2seq model\n    vocab: Vocabulary object\n    batch: Batch object that is the same example repeated across the batch\n\n  Returns:\n    best_hyp: Hypothesis object; the best hypothesis found by beam search.\n  \"\"\"", "\n", "# Run the encoder to get the encoder hidden states and decoder initial state", "\n", "dec_in_state", ",", "dec_question_inputs", ",", "dec_review_inputs", ",", "y_pred", "=", "model", ".", "run_encoder", "(", "sess", ",", "batch", ")", "\n", "# dec_in_state is a LSTMStateTuple", "\n", "# enc_states has shape [batch_size, <=max_enc_steps, 2*hidden_dim].", "\n", "\n", "# Initialize beam_size-many hyptheses", "\n", "hyps", "=", "[", "Hypothesis", "(", "tokens", "=", "[", "vocab", ".", "word2id", "(", "data", ".", "START_DECODING", ")", "]", ",", "\n", "log_probs", "=", "[", "0.0", "]", ",", "\n", "state", "=", "dec_in_state", ",", "\n", "q_attn_dists", "=", "[", "]", ",", "\n", "r_attn_dists", "=", "[", "]", ",", "\n", "p_gens", "=", "[", "]", ",", "\n", "q_coverage", "=", "np", ".", "zeros", "(", "[", "batch", ".", "q_batch", ".", "shape", "[", "1", "]", "]", ")", ",", "# zero vector of length attention_length", "\n", "r_coverage", "=", "np", ".", "zeros", "(", "[", "batch", ".", "r_batch", ".", "shape", "[", "1", "]", "*", "batch", ".", "r_batch", ".", "shape", "[", "2", "]", "]", ")", "\n", ")", "for", "_", "in", "range", "(", "FLAGS", ".", "beam_size", ")", "]", "\n", "results", "=", "[", "]", "# this will contain finished hypotheses (those that have emitted the [STOP] token)", "\n", "\n", "steps", "=", "0", "\n", "while", "steps", "<", "FLAGS", ".", "max_dec_steps", "and", "len", "(", "results", ")", "<", "FLAGS", ".", "beam_size", ":", "\n", "    ", "latest_tokens", "=", "[", "h", ".", "latest_token", "for", "h", "in", "hyps", "]", "# latest token produced by each hypothesis", "\n", "latest_tokens", "=", "[", "t", "if", "t", "in", "range", "(", "vocab", ".", "size", "(", ")", ")", "else", "vocab", ".", "word2id", "(", "data", ".", "UNKNOWN_TOKEN", ")", "for", "t", "in", "latest_tokens", "]", "# change any in-article temporary OOV ids to [UNK] id, so that we can lookup word embeddings", "\n", "states", "=", "[", "h", ".", "state", "for", "h", "in", "hyps", "]", "# list of current decoder states of the hypotheses", "\n", "q_prev_coverage", "=", "[", "h", ".", "q_coverage", "for", "h", "in", "hyps", "]", "# list of coverage vectors (or None)", "\n", "r_prev_coverage", "=", "[", "h", ".", "r_coverage", "for", "h", "in", "hyps", "]", "\n", "\n", "# Run one step of the decoder to get the new info", "\n", "(", "topk_ids", ",", "topk_log_probs", ",", "new_states", ",", "q_attn_dists", ",", "r_attn_dists", ",", "p_gens", ",", "q_new_coverage", ",", "r_new_coverage", ")", "=", "model", ".", "decode_onestep", "(", "sess", "=", "sess", ",", "\n", "batch", "=", "batch", ",", "\n", "latest_tokens", "=", "latest_tokens", ",", "\n", "dec_init_states", "=", "states", ",", "\n", "dec_question_inputs", "=", "dec_question_inputs", ",", "\n", "dec_review_inputs", "=", "dec_review_inputs", ",", "\n", "q_prev_coverage", "=", "q_prev_coverage", ",", "\n", "r_prev_coverage", "=", "r_prev_coverage", ")", "\n", "\n", "# Extend each hypothesis and collect them all in all_hyps", "\n", "all_hyps", "=", "[", "]", "\n", "num_orig_hyps", "=", "1", "if", "steps", "==", "0", "else", "len", "(", "hyps", ")", "# On the first step, we only had one original hypothesis (the initial hypothesis). On subsequent steps, all original hypotheses are distinct.", "\n", "for", "i", "in", "range", "(", "num_orig_hyps", ")", ":", "\n", "      ", "h", ",", "new_state", ",", "q_attn_dist", ",", "r_attn_dist", ",", "p_gen", ",", "q_new_coverage_i", ",", "r_new_coverage_i", "=", "hyps", "[", "i", "]", ",", "new_states", "[", "i", "]", ",", "q_attn_dists", "[", "i", "]", ",", "r_attn_dists", "[", "i", "]", ",", "p_gens", "[", "i", "]", ",", "q_new_coverage", "[", "i", "]", ",", "r_new_coverage", "[", "i", "]", "# take the ith hypothesis and new decoder state info", "\n", "for", "j", "in", "range", "(", "FLAGS", ".", "beam_size", "*", "2", ")", ":", "# for each of the top 2*beam_size hyps:", "\n", "# Extend the ith hypothesis with the jth option", "\n", "        ", "new_hyp", "=", "h", ".", "extend", "(", "token", "=", "topk_ids", "[", "i", ",", "j", "]", ",", "\n", "log_prob", "=", "topk_log_probs", "[", "i", ",", "j", "]", ",", "\n", "state", "=", "new_state", ",", "\n", "q_attn_dist", "=", "q_attn_dist", ",", "\n", "r_attn_dist", "=", "r_attn_dist", ",", "\n", "p_gen", "=", "p_gen", ",", "\n", "q_coverage", "=", "q_new_coverage_i", ",", "\n", "r_coverage", "=", "r_new_coverage_i", ")", "\n", "all_hyps", ".", "append", "(", "new_hyp", ")", "\n", "\n", "# Filter and collect any hypotheses that have produced the end token.", "\n", "", "", "hyps", "=", "[", "]", "# will contain hypotheses for the next step", "\n", "for", "h", "in", "sort_hyps", "(", "all_hyps", ")", ":", "# in order of most likely h", "\n", "      ", "if", "h", ".", "latest_token", "==", "vocab", ".", "word2id", "(", "data", ".", "STOP_DECODING", ")", ":", "# if stop token is reached...", "\n", "# If this hypothesis is sufficiently long, put in results. Otherwise discard.", "\n", "        ", "if", "steps", ">=", "FLAGS", ".", "min_dec_steps", ":", "\n", "          ", "results", ".", "append", "(", "h", ")", "\n", "", "", "else", ":", "# hasn't reached stop token, so continue to extend this hypothesis", "\n", "        ", "hyps", ".", "append", "(", "h", ")", "\n", "", "if", "len", "(", "hyps", ")", "==", "FLAGS", ".", "beam_size", "or", "len", "(", "results", ")", "==", "FLAGS", ".", "beam_size", ":", "\n", "# Once we've collected beam_size-many hypotheses for the next step, or beam_size-many complete hypotheses, stop.", "\n", "        ", "break", "\n", "\n", "", "", "steps", "+=", "1", "\n", "\n", "# At this point, either we've got beam_size results, or we've reached maximum decoder steps", "\n", "\n", "", "if", "len", "(", "results", ")", "==", "0", ":", "# if we don't have any complete results, add all current hypotheses (incomplete summaries) to results", "\n", "    ", "results", "=", "hyps", "\n", "\n", "# Sort hypotheses by average log probability", "\n", "", "hyps_sorted", "=", "sort_hyps", "(", "results", ")", "\n", "\n", "# Return the hypothesis with highest average log prob", "\n", "return", "hyps_sorted", "[", "0", "]", ",", "y_pred", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.beam_search.sort_hyps": [[175, 178], ["sorted"], "function", ["None"], ["", "def", "sort_hyps", "(", "hyps", ")", ":", "\n", "  ", "\"\"\"Return a list of Hypothesis objects, sorted by descending average log probability\"\"\"", "\n", "return", "sorted", "(", "hyps", ",", "key", "=", "lambda", "h", ":", "h", ".", "avg_log_prob", ",", "reverse", "=", "True", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.decode.BeamSearchDecoder.__init__": [[37, 76], ["decode.BeamSearchDecoder._model.build_graph", "tensorflow.train.Saver", "tensorflow.Session", "util.load_ckpt", "os.path.join", "os.path.exists", "os.path.join", "os.path.exists", "os.mkdir", "os.path.join", "os.path.join", "os.path.join", "util.get_config", "decode.get_decode_dir_name", "Exception", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir", "util.load_ckpt.split"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel.build_graph", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.util.load_ckpt", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.util.get_config", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.get_decode_dir_name"], ["def", "__init__", "(", "self", ",", "model", ",", "batcher", ",", "vocab", ")", ":", "\n", "    ", "\"\"\"Initialize decoder.\n\n    Args:\n      model: a Seq2SeqAttentionModel object.\n      batcher: a Batcher object.\n      vocab: Vocabulary object\n    \"\"\"", "\n", "self", ".", "_model", "=", "model", "\n", "self", ".", "_model", ".", "build_graph", "(", ")", "\n", "self", ".", "_batcher", "=", "batcher", "\n", "self", ".", "_vocab", "=", "vocab", "\n", "self", ".", "_saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "# we use this to load checkpoints for decoding", "\n", "self", ".", "_sess", "=", "tf", ".", "Session", "(", "config", "=", "util", ".", "get_config", "(", ")", ")", "\n", "\n", "# Load an initial checkpoint to use for decoding", "\n", "ckpt_path", "=", "util", ".", "load_ckpt", "(", "self", ".", "_saver", ",", "self", ".", "_sess", ")", "\n", "\n", "if", "FLAGS", ".", "single_pass", ":", "\n", "# Make a descriptive decode directory name", "\n", "      ", "ckpt_name", "=", "\"ckpt-\"", "+", "ckpt_path", ".", "split", "(", "'-'", ")", "[", "-", "1", "]", "# this is something of the form \"ckpt-123456\"", "\n", "self", ".", "_decode_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "log_root", ",", "get_decode_dir_name", "(", "ckpt_name", ")", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "self", ".", "_decode_dir", ")", ":", "\n", "        ", "raise", "Exception", "(", "\"single_pass decode directory %s should not already exist\"", "%", "self", ".", "_decode_dir", ")", "\n", "\n", "", "", "else", ":", "# Generic decode dir name", "\n", "      ", "self", ".", "_decode_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "log_root", ",", "\"decode\"", ")", "\n", "\n", "# Make the decode dir if necessary", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "_decode_dir", ")", ":", "os", ".", "mkdir", "(", "self", ".", "_decode_dir", ")", "\n", "\n", "if", "FLAGS", ".", "single_pass", ":", "\n", "# Make the dirs to contain output written in the correct format for pyrouge", "\n", "      ", "self", ".", "_rouge_ref_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_decode_dir", ",", "\"reference\"", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "_rouge_ref_dir", ")", ":", "os", ".", "mkdir", "(", "self", ".", "_rouge_ref_dir", ")", "\n", "self", ".", "_rouge_dec_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_decode_dir", ",", "\"decoded\"", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "_rouge_dec_dir", ")", ":", "os", ".", "mkdir", "(", "self", ".", "_rouge_dec_dir", ")", "\n", "self", ".", "_rouge_cqa_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_decode_dir", ",", "\"cqa\"", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "_rouge_cqa_dir", ")", ":", "os", ".", "mkdir", "(", "self", ".", "_rouge_cqa_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.decode.BeamSearchDecoder.decode": [[78, 128], ["time.time", "decode.BeamSearchDecoder._batcher.next_batch", "data.show_art_oovs", "data.show_abs_oovs", "beam_search.run_beam_search", "data.outputids2words", "tensorflow.logging.info", "tensorflow.logging.info", "data.show_art_oovs", "int", "data.outputids2words.index", "decode.BeamSearchDecoder.write_for_eval", "decode.print_results", "decode.BeamSearchDecoder.write_for_attnvis", "time.time", "tensorflow.logging.info", "util.load_ckpt", "time.time"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Batcher.next_batch", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.show_art_oovs", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.show_abs_oovs", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.beam_search.run_beam_search", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.outputids2words", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.show_art_oovs", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.BeamSearchDecoder.write_for_eval", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.print_results", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.BeamSearchDecoder.write_for_attnvis", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.util.load_ckpt"], ["", "", "def", "decode", "(", "self", ")", ":", "\n", "    ", "\"\"\"Decode examples until data is exhausted (if FLAGS.single_pass) and return, or decode indefinitely, loading latest checkpoint at regular intervals\"\"\"", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "counter", "=", "0", "\n", "while", "True", ":", "\n", "      ", "batch", "=", "self", ".", "_batcher", ".", "next_batch", "(", ")", "# 1 example repeated across batch", "\n", "if", "batch", "is", "None", ":", "# finished decoding dataset in single_pass mode", "\n", "        ", "assert", "FLAGS", ".", "single_pass", ",", "\"Dataset exhausted, but we are not in single_pass mode\"", "\n", "tf", ".", "logging", ".", "info", "(", "\"Decoder has finished reading dataset for single_pass.\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Output has been saved in %s and %s. Now starting ROUGE eval...\"", ",", "self", ".", "_rouge_ref_dir", ",", "self", ".", "_rouge_dec_dir", ")", "\n", "return", "\n", "\n", "", "original_reviews", "=", "batch", ".", "original_reviews", "[", "0", "]", "# list", "\n", "original_answer", "=", "batch", ".", "original_answers", "[", "0", "]", "# string", "\n", "original_answers_sents", "=", "batch", ".", "original_answers_sents", "[", "0", "]", "# list of strings", "\n", "original_question", "=", "batch", ".", "original_questions", "[", "0", "]", "\n", "y_target", "=", "batch", ".", "y_target_batch", "[", "0", "]", "\n", "\n", "review_withunks", "=", "[", "data", ".", "show_art_oovs", "(", "original_review", ",", "self", ".", "_vocab", ")", "for", "original_review", "in", "original_reviews", "]", "# string", "\n", "question_withunks", "=", "data", ".", "show_art_oovs", "(", "original_question", ",", "self", ".", "_vocab", ")", "# string", "\n", "answer_withunks", "=", "data", ".", "show_abs_oovs", "(", "original_answer", ",", "self", ".", "_vocab", ",", "(", "batch", ".", "oovs", "[", "0", "]", "if", "FLAGS", ".", "pointer_gen", "else", "None", ")", ")", "# string", "\n", "\n", "# Run beam search to get best Hypothesis", "\n", "best_hyp", ",", "y_pred", "=", "beam_search", ".", "run_beam_search", "(", "self", ".", "_sess", ",", "self", ".", "_model", ",", "self", ".", "_vocab", ",", "batch", ")", "\n", "\n", "# Extract the output ids from the hypothesis and convert back to words", "\n", "output_ids", "=", "[", "int", "(", "t", ")", "for", "t", "in", "best_hyp", ".", "tokens", "[", "1", ":", "]", "]", "\n", "decoded_words", "=", "data", ".", "outputids2words", "(", "output_ids", ",", "self", ".", "_vocab", ",", "(", "batch", ".", "oovs", "[", "0", "]", "if", "FLAGS", ".", "pointer_gen", "else", "None", ")", ")", "\n", "\n", "# Remove the [STOP] token from decoded_words, if necessary", "\n", "try", ":", "\n", "        ", "fst_stop_idx", "=", "decoded_words", ".", "index", "(", "data", ".", "STOP_DECODING", ")", "# index of the (first) [STOP] symbol", "\n", "decoded_words", "=", "decoded_words", "[", ":", "fst_stop_idx", "]", "\n", "", "except", "ValueError", ":", "\n", "        ", "decoded_words", "=", "decoded_words", "\n", "", "decoded_output", "=", "' '", ".", "join", "(", "decoded_words", ")", "# single string", "\n", "\n", "if", "FLAGS", ".", "single_pass", ":", "\n", "        ", "self", ".", "write_for_eval", "(", "original_answers_sents", ",", "decoded_words", ",", "original_question", ",", "y_target", ",", "y_pred", ",", "counter", ")", "# write ref summary and decoded summary to file, to eval with pyrouge later", "\n", "counter", "+=", "1", "# this is how many examples we've decoded", "\n", "", "else", ":", "\n", "        ", "print_results", "(", "review_withunks", ",", "answer_withunks", ",", "decoded_output", ",", "original_question", ",", "y_target", ",", "y_pred", ")", "# log output to screen", "\n", "self", ".", "write_for_attnvis", "(", "question_withunks", ",", "review_withunks", ",", "decoded_words", ",", "best_hyp", ".", "r_attn_dists", ",", "best_hyp", ".", "p_gens", ")", "# write info to .json file for visualization tool", "\n", "\n", "# Check if SECS_UNTIL_NEW_CKPT has elapsed; if so return so we can load a new checkpoint", "\n", "t1", "=", "time", ".", "time", "(", ")", "\n", "if", "t1", "-", "t0", ">", "SECS_UNTIL_NEW_CKPT", ":", "\n", "          ", "tf", ".", "logging", ".", "info", "(", "'We\\'ve been decoding with same checkpoint for %i seconds. Time to load new checkpoint'", ",", "t1", "-", "t0", ")", "\n", "_", "=", "util", ".", "load_ckpt", "(", "self", ".", "_saver", ",", "self", ".", "_sess", ")", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.decode.BeamSearchDecoder.write_for_eval": [[130, 172], ["os.path.join", "os.path.join", "os.path.join", "tensorflow.logging.info", "len", "decoded_sents.append", "decode.make_html_safe", "decode.make_html_safe", "open", "enumerate", "open", "enumerate", "open", "f.write", "decoded_words.index", "len", "f.write", "f.write", "f.write", "f.write", "str", "len", "len", "map"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.make_html_safe", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.make_html_safe"], ["", "", "", "", "def", "write_for_eval", "(", "self", ",", "reference_sents", ",", "decoded_words", ",", "question", ",", "target", ",", "prediction", ",", "ex_index", ")", ":", "\n", "    ", "\"\"\"Write output to file in correct format for eval with pyrouge. This is called in single_pass mode.\n\n    Args:\n      reference_sents: list of strings\n      decoded_words: list of strings\n      ex_index: int, the index with which to label the files\n    \"\"\"", "\n", "# First, divide decoded output into sentences", "\n", "decoded_sents", "=", "[", "]", "\n", "while", "len", "(", "decoded_words", ")", ">", "0", ":", "\n", "      ", "try", ":", "\n", "        ", "fst_period_idx", "=", "decoded_words", ".", "index", "(", "\".\"", ")", "\n", "", "except", "ValueError", ":", "# there is text remaining that doesn't end in \".\"", "\n", "        ", "fst_period_idx", "=", "len", "(", "decoded_words", ")", "\n", "", "sent", "=", "decoded_words", "[", ":", "fst_period_idx", "+", "1", "]", "# sentence up to and including the period", "\n", "decoded_words", "=", "decoded_words", "[", "fst_period_idx", "+", "1", ":", "]", "# everything else", "\n", "decoded_sents", ".", "append", "(", "' '", ".", "join", "(", "sent", ")", ")", "\n", "\n", "# pyrouge calls a perl script that puts the data into HTML files.", "\n", "# Therefore we need to make our output HTML safe.", "\n", "", "decoded_sents", "=", "[", "make_html_safe", "(", "w", ")", "for", "w", "in", "decoded_sents", "]", "\n", "reference_sents", "=", "[", "make_html_safe", "(", "w", ")", "for", "w", "in", "reference_sents", "]", "\n", "\n", "# cqa results", "\n", "result", "=", "[", "question", ",", "target", ",", "prediction", "]", "\n", "\n", "# Write to file", "\n", "ref_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_rouge_ref_dir", ",", "\"%06d_reference.txt\"", "%", "ex_index", ")", "\n", "decoded_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_rouge_dec_dir", ",", "\"%06d_decoded.txt\"", "%", "ex_index", ")", "\n", "cqa_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_rouge_cqa_dir", ",", "\"%06d_op.txt\"", "%", "ex_index", ")", "\n", "\n", "with", "open", "(", "ref_file", ",", "\"w\"", ")", "as", "f", ":", "\n", "      ", "for", "idx", ",", "sent", "in", "enumerate", "(", "reference_sents", ")", ":", "\n", "        ", "f", ".", "write", "(", "sent", ")", "if", "idx", "==", "len", "(", "reference_sents", ")", "-", "1", "else", "f", ".", "write", "(", "sent", "+", "\"\\n\"", ")", "\n", "", "", "with", "open", "(", "decoded_file", ",", "\"w\"", ")", "as", "f", ":", "\n", "      ", "for", "idx", ",", "sent", "in", "enumerate", "(", "decoded_sents", ")", ":", "\n", "        ", "f", ".", "write", "(", "sent", ")", "if", "idx", "==", "len", "(", "decoded_sents", ")", "-", "1", "else", "f", ".", "write", "(", "sent", "+", "\"\\n\"", ")", "\n", "", "", "with", "open", "(", "cqa_file", ",", "\"w\"", ")", "as", "f", ":", "\n", "      ", "f", ".", "write", "(", "'%s\\t%s\\t%s\\n'", "%", "(", "result", "[", "0", "]", ",", "str", "(", "result", "[", "1", "]", ")", ",", "' '", ".", "join", "(", "map", "(", "str", ",", "result", "[", "2", "]", ")", ")", ")", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"Wrote example %i to file\"", "%", "ex_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.decode.BeamSearchDecoder.write_for_attnvis": [[174, 199], ["article.split", "os.path.join", "tensorflow.logging.info", "decode.make_html_safe", "open", "json.dump", "decode.make_html_safe", "decode.make_html_safe"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.make_html_safe", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.make_html_safe", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.make_html_safe"], ["", "def", "write_for_attnvis", "(", "self", ",", "article", ",", "abstract", ",", "decoded_words", ",", "attn_dists", ",", "p_gens", ")", ":", "\n", "    ", "\"\"\"Write some data to json file, which can be read into the in-browser attention visualizer tool:\n      https://github.com/abisee/attn_vis\n\n    Args:\n      article: The original article string.\n      abstract: The human (correct) abstract string.\n      attn_dists: List of arrays; the attention distributions.\n      decoded_words: List of strings; the words of the generated summary.\n      p_gens: List of scalars; the p_gen values. If not running in pointer-generator mode, list of None.\n    \"\"\"", "\n", "article_lst", "=", "article", ".", "split", "(", ")", "# list of words", "\n", "decoded_lst", "=", "decoded_words", "# list of decoded words", "\n", "to_write", "=", "{", "\n", "'article_lst'", ":", "[", "make_html_safe", "(", "t", ")", "for", "t", "in", "article_lst", "]", ",", "\n", "'decoded_lst'", ":", "[", "make_html_safe", "(", "t", ")", "for", "t", "in", "decoded_lst", "]", ",", "\n", "'abstract_str'", ":", "make_html_safe", "(", "abstract", ")", ",", "\n", "'attn_dists'", ":", "attn_dists", "\n", "}", "\n", "if", "FLAGS", ".", "pointer_gen", ":", "\n", "      ", "to_write", "[", "'p_gens'", "]", "=", "p_gens", "\n", "", "output_fname", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_decode_dir", ",", "'attn_vis_data.json'", ")", "\n", "with", "open", "(", "output_fname", ",", "'w'", ")", "as", "output_file", ":", "\n", "      ", "json", ".", "dump", "(", "to_write", ",", "output_file", ")", "\n", "", "tf", ".", "logging", ".", "info", "(", "'Wrote visualization data to %s'", ",", "output_fname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.decode.print_results": [[201, 210], ["print", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "print"], "function", ["None"], ["", "", "def", "print_results", "(", "article", ",", "abstract", ",", "decoded_output", ",", "question", ",", "target", ",", "prediction", ")", ":", "\n", "  ", "\"\"\"Prints the article, the reference summmary and the decoded summary to screen\"\"\"", "\n", "print", "(", "\"---------------------------------------------------------------------------\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'ARTICLE:  %s'", ",", "article", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'REFERENCE SUMMARY: %s'", ",", "abstract", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'GENERATED SUMMARY: %s'", ",", "decoded_output", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'QUESTION: %s'", ",", "question", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'TARGET#PREDICTION: %s#%s'", ",", "(", "target", ",", "prediction", "[", "1", "]", ")", ")", "\n", "print", "(", "\"---------------------------------------------------------------------------\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.decode.make_html_safe": [[212, 217], ["s.replace", "s.replace"], "function", ["None"], ["", "def", "make_html_safe", "(", "s", ")", ":", "\n", "  ", "\"\"\"Replace any angled brackets in string s to avoid interfering with HTML attention visualizer.\"\"\"", "\n", "s", ".", "replace", "(", "\"<\"", ",", "\"&lt;\"", ")", "\n", "s", ".", "replace", "(", "\">\"", ",", "\"&gt;\"", ")", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.decode.get_decode_dir_name": [[219, 230], ["ValueError"], "function", ["None"], ["", "def", "get_decode_dir_name", "(", "ckpt_name", ")", ":", "\n", "  ", "\"\"\"Make a descriptive name for the decode dir, including the name of the checkpoint we use to decode. This is called in single_pass mode.\"\"\"", "\n", "\n", "if", "\"train\"", "in", "FLAGS", ".", "data_path", ":", "dataset", "=", "\"train\"", "\n", "elif", "\"val\"", "in", "FLAGS", ".", "data_path", ":", "dataset", "=", "\"val\"", "\n", "elif", "\"test\"", "in", "FLAGS", ".", "data_path", ":", "dataset", "=", "\"test\"", "\n", "else", ":", "raise", "ValueError", "(", "\"FLAGS.data_path %s should contain one of train, val or test\"", "%", "(", "FLAGS", ".", "data_path", ")", ")", "\n", "dirname", "=", "\"decode_%s_%imaxenc_%ibeam_%imindec_%imaxdec\"", "%", "(", "dataset", ",", "FLAGS", ".", "max_enc_steps", ",", "FLAGS", ".", "beam_size", ",", "FLAGS", ".", "min_dec_steps", ",", "FLAGS", ".", "max_dec_steps", ")", "\n", "if", "ckpt_name", "is", "not", "None", ":", "\n", "    ", "dirname", "+=", "\"_%s\"", "%", "ckpt_name", "\n", "", "return", "dirname", "\n", "", ""]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.batcher.Example.__init__": [[31, 103], ["vocab.word2id", "vocab.word2id", "answer.split", "question.split", "len", "batcher.Example.get_dec_inp_targ_seqs", "len", "review.split", "reviews_words.append", "batcher.Example.r_lens.append", "batcher.Example.r_batch.append", "vocab.word2id", "vocab.word2id", "data.article2ids", "data.abstract2ids", "batcher.Example.get_dec_inp_targ_seqs", "len", "len", "data.article2ids", "batcher.Example.reviews_extend_vocab.append", "vocab.word2id"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Example.get_dec_inp_targ_seqs", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.article2ids", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.abstract2ids", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Example.get_dec_inp_targ_seqs", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.article2ids", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id"], ["def", "__init__", "(", "self", ",", "reviews", ",", "ratings", ",", "answer_sentences", ",", "question", ",", "label", ",", "vocab", ",", "hps", ")", ":", "\n", "    ", "\"\"\"Initializes the Example, performing tokenization and truncation to produce the encoder, decoder and target sequences, which are stored in self.\n\n    Args:\n      reviews: review text; a list.\n      ratings: alist.\n      answer_sentences: list of strings, one per abstract sentence. In each sentence, each token is separated by a single space.\n      vocab: Vocabulary object\n      hps: hyperparameters\n    \"\"\"", "\n", "self", ".", "hps", "=", "hps", "\n", "\n", "# Get ids of special tokens", "\n", "start_decoding", "=", "vocab", ".", "word2id", "(", "data", ".", "START_DECODING", ")", "\n", "stop_decoding", "=", "vocab", ".", "word2id", "(", "data", ".", "STOP_DECODING", ")", "\n", "\n", "\n", "\n", "# Process the reviews", "\n", "self", ".", "r_lens", "=", "[", "]", "\n", "self", ".", "r_batch", "=", "[", "]", "\n", "self", ".", "rating_batch", "=", "ratings", "\n", "reviews_words", "=", "[", "]", "\n", "for", "review", "in", "reviews", ":", "\n", "      ", "review_words", "=", "review", ".", "split", "(", ")", "\n", "if", "len", "(", "review_words", ")", ">", "hps", ".", "max_enc_steps", ":", "\n", "        ", "review_words", "=", "review_words", "[", ":", "hps", ".", "max_enc_steps", "]", "\n", "", "reviews_words", ".", "append", "(", "review_words", ")", "\n", "self", ".", "r_lens", ".", "append", "(", "len", "(", "review_words", ")", ")", "# store the length after truncation but before padding", "\n", "self", ".", "r_batch", ".", "append", "(", "[", "vocab", ".", "word2id", "(", "w", ")", "for", "w", "in", "review_words", "]", ")", "# list of word ids; OOVs are represented by the id for UNK token", "\n", "\n", "# Process the abstract", "\n", "", "answer", "=", "' '", ".", "join", "(", "answer_sentences", ")", "# string", "\n", "answer_words", "=", "answer", ".", "split", "(", ")", "# list of strings", "\n", "ans_ids", "=", "[", "vocab", ".", "word2id", "(", "w", ")", "for", "w", "in", "answer_words", "]", "# list of word ids; OOVs are represented by the id for UNK token", "\n", "\n", "# Process the question", "\n", "question_words", "=", "question", ".", "split", "(", ")", "\n", "self", ".", "q_lens", "=", "len", "(", "question_words", ")", "\n", "self", ".", "q_batch", "=", "[", "vocab", ".", "word2id", "(", "w", ")", "for", "w", "in", "question_words", "]", "\n", "\n", "# Process the label", "\n", "self", ".", "y_target", "=", "label", "\n", "\n", "# Get the decoder input sequence and target sequence", "\n", "self", ".", "dec_input", ",", "self", ".", "target", "=", "self", ".", "get_dec_inp_targ_seqs", "(", "ans_ids", ",", "hps", ".", "max_dec_steps", ",", "start_decoding", ",", "stop_decoding", ")", "\n", "self", ".", "dec_len", "=", "len", "(", "self", ".", "dec_input", ")", "\n", "assert", "self", ".", "dec_len", ">", "0", "\n", "\n", "# If using pointer-generator mode, we need to store some extra info", "\n", "if", "hps", ".", "pointer_gen", ":", "\n", "      ", "self", ".", "oovs", "=", "[", "]", "\n", "# Store a version of the enc_input where in-article OOVs are represented by their temporary OOV id; also store the in-article OOVs words themselves", "\n", "self", ".", "reviews_extend_vocab", "=", "[", "]", "\n", "for", "review_words", "in", "reviews_words", ":", "\n", "        ", "review_extend_vocab", ",", "self", ".", "oovs", "=", "data", ".", "article2ids", "(", "review_words", ",", "vocab", ",", "self", ".", "oovs", ")", "\n", "self", ".", "reviews_extend_vocab", ".", "append", "(", "review_extend_vocab", ")", "\n", "\n", "# question OOV id.", "\n", "", "self", ".", "question_extend_vocab", ",", "self", ".", "oovs", "=", "data", ".", "article2ids", "(", "question_words", ",", "vocab", ",", "self", ".", "oovs", ")", "\n", "\n", "# Get a verison of the reference summary where in-article OOVs are represented by their temporary article OOV id", "\n", "ans_ids_extend_vocab", "=", "data", ".", "abstract2ids", "(", "answer_words", ",", "vocab", ",", "self", ".", "oovs", ")", "\n", "\n", "# Overwrite decoder target sequence so it uses the temp article OOV ids", "\n", "_", ",", "self", ".", "target", "=", "self", ".", "get_dec_inp_targ_seqs", "(", "ans_ids_extend_vocab", ",", "hps", ".", "max_dec_steps", ",", "start_decoding", ",", "stop_decoding", ")", "\n", "\n", "# Store the original strings", "\n", "", "self", ".", "original_reviews", "=", "reviews", "\n", "self", ".", "original_answer", "=", "answer", "\n", "self", ".", "original_answer_sents", "=", "answer_sentences", "\n", "self", ".", "original_question", "=", "question", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.batcher.Example.get_dec_inp_targ_seqs": [[105, 127], ["len", "target.append", "len", "len"], "methods", ["None"], ["", "def", "get_dec_inp_targ_seqs", "(", "self", ",", "sequence", ",", "max_len", ",", "start_id", ",", "stop_id", ")", ":", "\n", "    ", "\"\"\"Given the reference summary as a sequence of tokens, return the input sequence for the decoder, and the target sequence which we will use to calculate loss. The sequence will be truncated if it is longer than max_len. The input sequence must start with the start_id and the target sequence must end with the stop_id (but not if it's been truncated).\n\n    Args:\n      sequence: List of ids (integers)\n      max_len: integer\n      start_id: integer\n      stop_id: integer\n\n    Returns:\n      inp: sequence length <=max_len starting with start_id\n      target: sequence same length as input, ending with stop_id only if there was no truncation\n    \"\"\"", "\n", "inp", "=", "[", "start_id", "]", "+", "sequence", "[", ":", "]", "\n", "target", "=", "sequence", "[", ":", "]", "\n", "if", "len", "(", "inp", ")", ">", "max_len", ":", "# truncate", "\n", "      ", "inp", "=", "inp", "[", ":", "max_len", "]", "\n", "target", "=", "target", "[", ":", "max_len", "]", "# no end_token", "\n", "", "else", ":", "# no truncation", "\n", "      ", "target", ".", "append", "(", "stop_id", ")", "# end token", "\n", "", "assert", "len", "(", "inp", ")", "==", "len", "(", "target", ")", "\n", "return", "inp", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.batcher.Example.pad_decoder_inp_targ": [[129, 135], ["len", "batcher.Example.dec_input.append", "len", "batcher.Example.target.append"], "methods", ["None"], ["", "def", "pad_decoder_inp_targ", "(", "self", ",", "max_len", ",", "pad_id", ")", ":", "\n", "    ", "\"\"\"Pad decoder input and target sequences with pad_id up to max_len.\"\"\"", "\n", "while", "len", "(", "self", ".", "dec_input", ")", "<", "max_len", ":", "\n", "      ", "self", ".", "dec_input", ".", "append", "(", "pad_id", ")", "\n", "", "while", "len", "(", "self", ".", "target", ")", "<", "max_len", ":", "\n", "      ", "self", ".", "target", ".", "append", "(", "pad_id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.batcher.Example.pad_reviews": [[137, 146], ["range", "len", "range", "len", "batcher.Example.r_batch[].append", "len", "len", "batcher.Example.reviews_extend_vocab[].append"], "methods", ["None"], ["", "", "def", "pad_reviews", "(", "self", ",", "max_len", ",", "pad_id", ")", ":", "\n", "    ", "\"\"\"Pad the encoder input sequence with pad_id up to max_len.\"\"\"", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "r_batch", ")", ")", ":", "\n", "      ", "while", "len", "(", "self", ".", "r_batch", "[", "i", "]", ")", "<", "max_len", ":", "\n", "        ", "self", ".", "r_batch", "[", "i", "]", ".", "append", "(", "pad_id", ")", "\n", "", "", "if", "self", ".", "hps", ".", "pointer_gen", ":", "\n", "      ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "reviews_extend_vocab", ")", ")", ":", "\n", "        ", "while", "len", "(", "self", ".", "reviews_extend_vocab", "[", "i", "]", ")", "<", "max_len", ":", "\n", "          ", "self", ".", "reviews_extend_vocab", "[", "i", "]", ".", "append", "(", "pad_id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.batcher.Example.pad_question": [[148, 155], ["len", "batcher.Example.q_batch.append", "len", "batcher.Example.question_extend_vocab.append"], "methods", ["None"], ["", "", "", "", "def", "pad_question", "(", "self", ",", "max_len", ",", "pad_id", ")", ":", "\n", "    ", "\"\"\"Pad the encoder input sequence with pad_id up to max_len.\"\"\"", "\n", "while", "len", "(", "self", ".", "q_batch", ")", "<", "max_len", ":", "\n", "      ", "self", ".", "q_batch", ".", "append", "(", "pad_id", ")", "\n", "", "if", "self", ".", "hps", ".", "pointer_gen", ":", "\n", "      ", "while", "len", "(", "self", ".", "question_extend_vocab", ")", "<", "max_len", ":", "\n", "        ", "self", ".", "question_extend_vocab", ".", "append", "(", "pad_id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.batcher.Batch.__init__": [[160, 178], ["vocab.word2id", "batcher.Batch.init_encoder_seq", "batcher.Batch.init_decoder_seq", "batcher.Batch.store_orig_strings"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Batch.init_encoder_seq", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Batch.init_decoder_seq", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Batch.store_orig_strings"], ["def", "__init__", "(", "self", ",", "example_list", ",", "hps", ",", "vocab", ")", ":", "\n", "    ", "\"\"\"Turns the example_list into a Batch object.\n\n    # Pad the encoder input sequences up to the length of the longest sequence\n    for ex in example_list:\n      ex.pad_reviews(max_r_seq_len, self.pad_id)\n      ex.pad_question(max_q_seq_len, self.pad_id)\n\n\n    Args:\n       example_list: List of Example objects\n       hps: hyperparameters\n       vocab: Vocabulary object\n    \"\"\"", "\n", "self", ".", "pad_id", "=", "vocab", ".", "word2id", "(", "data", ".", "PAD_TOKEN", ")", "# id of the PAD token used to pad sequences", "\n", "self", ".", "init_encoder_seq", "(", "example_list", ",", "hps", ")", "# initialize the input to the encoder", "\n", "self", ".", "init_decoder_seq", "(", "example_list", ",", "hps", ")", "# initialize the input and targets for the decoder", "\n", "self", ".", "store_orig_strings", "(", "example_list", ")", "# store the original strings", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.batcher.Batch.init_encoder_seq": [[179, 241], ["max", "max", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "enumerate", "ex.pad_reviews", "ex.pad_question", "range", "range", "max", "numpy.zeros", "numpy.zeros", "enumerate", "enumerate", "max", "len", "range", "len"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Example.pad_reviews", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Example.pad_question"], ["", "def", "init_encoder_seq", "(", "self", ",", "example_list", ",", "hps", ")", ":", "\n", "    ", "\"\"\"Initializes the following:\n        self.enc_batch:\n          numpy array of shape (batch_size, <=max_enc_steps) containing integer ids (all OOVs represented by UNK id), padded to length of longest sequence in the batch\n        self.enc_lens:\n          numpy array of shape (batch_size) containing integers. The (truncated) length of each encoder input sequence (pre-padding).\n        self.enc_padding_mask:\n          numpy array of shape (batch_size, <=max_enc_steps), containing 1s and 0s. 1s correspond to real tokens in enc_batch and target_batch; 0s correspond to padding.\n\n      If hps.pointer_gen, additionally initializes the following:\n        self.max_art_oovs:\n          maximum number of in-article OOVs in the batch\n        self.art_oovs:\n          list of list of in-article OOVs (strings), for each example in the batch\n        self.enc_batch_extend_vocab:\n          Same as self.enc_batch, but in-article OOVs are represented by their temporary article OOV number.\n    \"\"\"", "\n", "# Determine the maximum length of the encoder input sequence in this batch", "\n", "max_r_seq_len", "=", "max", "(", "[", "max", "(", "ex", ".", "r_lens", ")", "for", "ex", "in", "example_list", "]", ")", "\n", "max_q_seq_len", "=", "max", "(", "[", "ex", ".", "q_lens", "for", "ex", "in", "example_list", "]", ")", "\n", "\n", "# Pad the encoder input sequences up to the length of the longest sequence", "\n", "for", "ex", "in", "example_list", ":", "\n", "      ", "ex", ".", "pad_reviews", "(", "max_r_seq_len", ",", "self", ".", "pad_id", ")", "\n", "ex", ".", "pad_question", "(", "max_q_seq_len", ",", "self", ".", "pad_id", ")", "\n", "\n", "# Initialize the numpy arrays", "\n", "# Note: our enc_batch can have different length (second dimension) for each batch because we use dynamic_rnn for the encoder.", "\n", "", "self", ".", "r_batch", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ",", "hps", ".", "review_num", ",", "max_r_seq_len", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "self", ".", "q_batch", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ",", "max_q_seq_len", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "self", ".", "r_lens", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ",", "hps", ".", "review_num", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "self", ".", "q_lens", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "self", ".", "r_padding_mask", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ",", "hps", ".", "review_num", ",", "max_r_seq_len", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "q_padding_mask", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ",", "max_q_seq_len", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "rating_batch", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ",", "hps", ".", "review_num", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "\n", "# Fill in the numpy arrays", "\n", "for", "i", ",", "ex", "in", "enumerate", "(", "example_list", ")", ":", "\n", "      ", "self", ".", "r_batch", "[", "i", ",", ":", "]", "=", "ex", ".", "r_batch", "[", ":", "]", "\n", "self", ".", "r_lens", "[", "i", ",", ":", "]", "=", "ex", ".", "r_lens", "[", ":", "]", "\n", "self", ".", "q_batch", "[", "i", ",", ":", "]", "=", "ex", ".", "q_batch", "[", ":", "]", "\n", "self", ".", "q_lens", "[", "i", "]", "=", "ex", ".", "q_lens", "\n", "self", ".", "rating_batch", "[", "i", ",", ":", "]", "=", "ex", ".", "rating_batch", "[", ":", "]", "\n", "for", "j", "in", "range", "(", "len", "(", "ex", ".", "r_lens", ")", ")", ":", "\n", "        ", "for", "k", "in", "range", "(", "ex", ".", "r_lens", "[", "j", "]", ")", ":", "\n", "          ", "self", ".", "r_padding_mask", "[", "i", "]", "[", "j", "]", "[", "k", "]", "=", "1", "\n", "", "", "for", "j", "in", "range", "(", "ex", ".", "q_lens", ")", ":", "\n", "        ", "self", ".", "q_padding_mask", "[", "i", "]", "[", "j", "]", "=", "1", "\n", "\n", "# For pointer-generator mode, need to store some extra info", "\n", "", "", "if", "hps", ".", "pointer_gen", ":", "\n", "# Determine the max number of in-article OOVs in this batch", "\n", "      ", "self", ".", "max_oovs", "=", "max", "(", "[", "len", "(", "ex", ".", "oovs", ")", "for", "ex", "in", "example_list", "]", ")", "\n", "# Store the in-article OOVs themselves", "\n", "self", ".", "oovs", "=", "[", "ex", ".", "oovs", "for", "ex", "in", "example_list", "]", "\n", "# Store the version of the enc_batch that uses the article OOV ids", "\n", "self", ".", "r_batch_extend_vocab", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ",", "hps", ".", "review_num", ",", "max_r_seq_len", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "self", ".", "q_batch_extend_vocab", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ",", "max_q_seq_len", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "for", "i", ",", "ex", "in", "enumerate", "(", "example_list", ")", ":", "\n", "        ", "self", ".", "r_batch_extend_vocab", "[", "i", ",", ":", "]", "=", "ex", ".", "reviews_extend_vocab", "[", ":", "]", "\n", "", "for", "i", ",", "ex", "in", "enumerate", "(", "example_list", ")", ":", "\n", "        ", "self", ".", "q_batch_extend_vocab", "[", "i", ",", ":", "]", "=", "ex", ".", "question_extend_vocab", "[", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.batcher.Batch.init_decoder_seq": [[242, 269], ["numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "enumerate", "ex.pad_decoder_inp_targ", "range"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Example.pad_decoder_inp_targ"], ["", "", "", "def", "init_decoder_seq", "(", "self", ",", "example_list", ",", "hps", ")", ":", "\n", "    ", "\"\"\"Initializes the following:\n        self.dec_batch:\n          numpy array of shape (batch_size, max_dec_steps), containing integer ids as input for the decoder, padded to max_dec_steps length.\n        self.target_batch:\n          numpy array of shape (batch_size, max_dec_steps), containing integer ids for the target sequence, padded to max_dec_steps length.\n        self.dec_padding_mask:\n          numpy array of shape (batch_size, max_dec_steps), containing 1s and 0s. 1s correspond to real tokens in dec_batch and target_batch; 0s correspond to padding.\n        \"\"\"", "\n", "# Pad the inputs and targets", "\n", "for", "ex", "in", "example_list", ":", "\n", "      ", "ex", ".", "pad_decoder_inp_targ", "(", "hps", ".", "max_dec_steps", ",", "self", ".", "pad_id", ")", "\n", "\n", "# Initialize the numpy arrays.", "\n", "# Note: our decoder inputs and targets must be the same length for each batch (second dimension = max_dec_steps) because we do not use a dynamic_rnn for decoding. However I believe this is possible, or will soon be possible, with Tensorflow 1.0, in which case it may be best to upgrade to that.", "\n", "", "self", ".", "dec_batch", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ",", "hps", ".", "max_dec_steps", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "self", ".", "target_batch", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ",", "hps", ".", "max_dec_steps", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "self", ".", "y_target_batch", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "self", ".", "dec_padding_mask", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ",", "hps", ".", "max_dec_steps", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "# Fill in the numpy arrays", "\n", "for", "i", ",", "ex", "in", "enumerate", "(", "example_list", ")", ":", "\n", "      ", "self", ".", "dec_batch", "[", "i", ",", ":", "]", "=", "ex", ".", "dec_input", "[", ":", "]", "\n", "self", ".", "target_batch", "[", "i", ",", ":", "]", "=", "ex", ".", "target", "[", ":", "]", "\n", "self", ".", "y_target_batch", "[", "i", "]", "=", "ex", ".", "y_target", "\n", "for", "j", "in", "range", "(", "ex", ".", "dec_len", ")", ":", "\n", "        ", "self", ".", "dec_padding_mask", "[", "i", "]", "[", "j", "]", "=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.batcher.Batch.store_orig_strings": [[270, 276], ["None"], "methods", ["None"], ["", "", "", "def", "store_orig_strings", "(", "self", ",", "example_list", ")", ":", "\n", "    ", "\"\"\"Store the original article and abstract strings in the Batch object\"\"\"", "\n", "self", ".", "original_reviews", "=", "[", "ex", ".", "original_reviews", "for", "ex", "in", "example_list", "]", "# list of lists", "\n", "self", ".", "original_answers", "=", "[", "ex", ".", "original_answer", "for", "ex", "in", "example_list", "]", "# list of lists", "\n", "self", ".", "original_questions", "=", "[", "ex", ".", "original_question", "for", "ex", "in", "example_list", "]", "# list of lists", "\n", "self", ".", "original_answers_sents", "=", "[", "ex", ".", "original_answer_sents", "for", "ex", "in", "example_list", "]", "# list of list of lists", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.batcher.Batcher.__init__": [[283, 329], ["queue.Queue", "queue.Queue", "range", "range", "batcher.Batcher._example_q_threads.append", "batcher.Batcher._example_q_threads[].start", "batcher.Batcher._batch_q_threads.append", "batcher.Batcher._batch_q_threads[].start", "threading.Thread", "batcher.Batcher._watch_thread.start", "threading.Thread", "threading.Thread"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "data_path", ",", "vocab", ",", "hps", ",", "single_pass", ")", ":", "\n", "    ", "\"\"\"Initialize the batcher. Start threads that process the data into batches.\n\n    Args:\n      data_path: tf.Example filepattern.\n      vocab: Vocabulary object\n      hps: hyperparameters\n      single_pass: If True, run through the dataset exactly once (useful for when you want to run evaluation on the dev or test set). Otherwise generate random batches indefinitely (useful for training).\n    \"\"\"", "\n", "self", ".", "_data_path", "=", "data_path", "\n", "self", ".", "_vocab", "=", "vocab", "\n", "self", ".", "_hps", "=", "hps", "\n", "self", ".", "_single_pass", "=", "single_pass", "\n", "\n", "# Initialize a queue of Batches waiting to be used, and a queue of Examples waiting to be batched", "\n", "self", ".", "_batch_queue", "=", "Queue", ".", "Queue", "(", "self", ".", "BATCH_QUEUE_MAX", ")", "\n", "self", ".", "_example_queue", "=", "Queue", ".", "Queue", "(", "self", ".", "BATCH_QUEUE_MAX", "*", "self", ".", "_hps", ".", "batch_size", ")", "\n", "\n", "# Different settings depending on whether we're in single_pass mode or not", "\n", "if", "single_pass", ":", "\n", "      ", "self", ".", "_num_example_q_threads", "=", "1", "# just one thread, so we read through the dataset just once", "\n", "self", ".", "_num_batch_q_threads", "=", "1", "# just one thread to batch examples", "\n", "self", ".", "_bucketing_cache_size", "=", "1", "# only load one batch's worth of examples before bucketing; this essentially means no bucketing", "\n", "self", ".", "_finished_reading", "=", "False", "# this will tell us when we're finished reading the dataset", "\n", "", "else", ":", "\n", "      ", "self", ".", "_num_example_q_threads", "=", "16", "# num threads to fill example queue", "\n", "self", ".", "_num_batch_q_threads", "=", "4", "# num threads to fill batch queue", "\n", "self", ".", "_bucketing_cache_size", "=", "100", "# how many batches-worth of examples to load into cache before bucketing", "\n", "\n", "# Start the threads that load the queues", "\n", "", "self", ".", "_example_q_threads", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "self", ".", "_num_example_q_threads", ")", ":", "\n", "      ", "self", ".", "_example_q_threads", ".", "append", "(", "Thread", "(", "target", "=", "self", ".", "fill_example_queue", ")", ")", "\n", "self", ".", "_example_q_threads", "[", "-", "1", "]", ".", "daemon", "=", "True", "\n", "self", ".", "_example_q_threads", "[", "-", "1", "]", ".", "start", "(", ")", "\n", "", "self", ".", "_batch_q_threads", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "self", ".", "_num_batch_q_threads", ")", ":", "\n", "      ", "self", ".", "_batch_q_threads", ".", "append", "(", "Thread", "(", "target", "=", "self", ".", "fill_batch_queue", ")", ")", "\n", "self", ".", "_batch_q_threads", "[", "-", "1", "]", ".", "daemon", "=", "True", "\n", "self", ".", "_batch_q_threads", "[", "-", "1", "]", ".", "start", "(", ")", "\n", "\n", "# Start a thread that watches the other threads and restarts them if they're dead", "\n", "", "if", "not", "single_pass", ":", "# We don't want a watcher in single_pass mode because the threads shouldn't run forever", "\n", "      ", "self", ".", "_watch_thread", "=", "Thread", "(", "target", "=", "self", ".", "watch_threads", ")", "\n", "self", ".", "_watch_thread", ".", "daemon", "=", "True", "\n", "self", ".", "_watch_thread", ".", "start", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.batcher.Batcher.next_batch": [[331, 349], ["batcher.Batcher._batch_queue.get", "batcher.Batcher._batch_queue.qsize", "tensorflow.compat.v1.logging.warning", "print", "batcher.Batcher._batch_queue.qsize", "batcher.Batcher._example_queue.qsize", "tensorflow.logging.info"], "methods", ["None"], ["", "", "def", "next_batch", "(", "self", ")", ":", "\n", "    ", "\"\"\"Return a Batch from the batch queue.\n\n    If mode='decode' then each batch contains a single example repeated beam_size-many times; this is necessary for beam search.\n\n    Returns:\n      batch: a Batch object, or None if we're in single_pass mode and we've exhausted the dataset.\n    \"\"\"", "\n", "# If the batch queue is empty, print a warning", "\n", "if", "self", ".", "_batch_queue", ".", "qsize", "(", ")", "==", "0", ":", "\n", "      ", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "warning", "(", "'Bucket input queue is empty when calling next_batch. Bucket queue size: %i, Input queue size: %i'", ",", "self", ".", "_batch_queue", ".", "qsize", "(", ")", ",", "self", ".", "_example_queue", ".", "qsize", "(", ")", ")", "\n", "print", "(", "self", ".", "_single_pass", ",", "self", ".", "_finished_reading", ")", "\n", "if", "self", ".", "_single_pass", "and", "self", ".", "_finished_reading", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"Finished reading dataset in single_pass mode.\"", ")", "\n", "return", "None", "\n", "\n", "", "", "batch", "=", "self", ".", "_batch_queue", ".", "get", "(", ")", "# get the next Batch", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.batcher.Batcher.fill_example_queue": [[350, 371], ["batcher.Batcher.text_generator", "data.example_generator", "batcher.Example", "batcher.Batcher._example_queue.put", "next", "sent.strip", "tensorflow.logging.info", "data.abstract2sents", "tensorflow.logging.info", "Exception"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Batcher.text_generator", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.example_generator", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.abstract2sents"], ["", "def", "fill_example_queue", "(", "self", ")", ":", "\n", "    ", "\"\"\"Reads data from file and processes into Examples which are then placed into the example queue.\"\"\"", "\n", "\n", "input_gen", "=", "self", ".", "text_generator", "(", "data", ".", "example_generator", "(", "self", ".", "_data_path", ",", "self", ".", "_single_pass", ")", ")", "\n", "\n", "while", "True", ":", "\n", "      ", "try", ":", "\n", "        ", "(", "reviews", ",", "ratings", ",", "answer", ",", "question", ",", "label", ")", "=", "next", "(", "input_gen", ")", "# read the next example from file. article and abstract are both strings.", "\n", "#except StopIteration: # if there are no more examples:", "\n", "", "except", "Exception", "as", "e", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"The example generator for this example queue filling thread has exhausted data.\"", ")", "\n", "if", "self", ".", "_single_pass", ":", "\n", "          ", "tf", ".", "logging", ".", "info", "(", "\"single_pass mode is on, so we've finished reading dataset. This thread is stopping.\"", ")", "\n", "self", ".", "_finished_reading", "=", "True", "\n", "break", "\n", "", "else", ":", "\n", "          ", "raise", "Exception", "(", "\"single_pass mode is off but the example generator is out of data; error.\"", ")", "\n", "\n", "", "", "answer_sentences", "=", "[", "sent", ".", "strip", "(", ")", "for", "sent", "in", "data", ".", "abstract2sents", "(", "answer", ")", "]", "# Use the <s> and </s> tags in abstract to get a list of sentences.", "\n", "example", "=", "Example", "(", "reviews", ",", "ratings", ",", "answer_sentences", ",", "question", ",", "label", ",", "self", ".", "_vocab", ",", "self", ".", "_hps", ")", "# Process into an Example.", "\n", "self", ".", "_example_queue", ".", "put", "(", "example", ")", "# place the Example in the example queue.", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.batcher.Batcher.fill_batch_queue": [[373, 399], ["range", "sorted", "range", "batcher.Batcher._example_queue.get", "batcher.Batcher._batch_queue.put", "sorted.append", "len", "batches.append", "random.shuffle", "batcher.Batcher._batch_queue.put", "batcher.Batch", "batcher.Batcher._example_queue.get", "batcher.Batch", "range"], "methods", ["None"], ["", "", "def", "fill_batch_queue", "(", "self", ")", ":", "\n", "    ", "\"\"\"Takes Examples out of example queue, sorts them by encoder sequence length, processes into Batches and places them in the batch queue.\n\n    In decode mode, makes batches that each contain a single example repeated.\n    \"\"\"", "\n", "while", "True", ":", "\n", "      ", "if", "self", ".", "_hps", ".", "mode", "!=", "'decode'", ":", "\n", "# Get bucketing_cache_size-many batches of Examples into a list, then sort", "\n", "        ", "inputs", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "self", ".", "_hps", ".", "batch_size", "*", "self", ".", "_bucketing_cache_size", ")", ":", "\n", "          ", "inputs", ".", "append", "(", "self", ".", "_example_queue", ".", "get", "(", ")", ")", "\n", "", "inputs", "=", "sorted", "(", "inputs", ",", "key", "=", "lambda", "inp", ":", "inp", ".", "q_lens", ")", "# sort by length of encoder sequence", "\n", "\n", "# Group the sorted Examples into batches, optionally shuffle the batches, and place in the batch queue.", "\n", "batches", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "inputs", ")", ",", "self", ".", "_hps", ".", "batch_size", ")", ":", "\n", "          ", "batches", ".", "append", "(", "inputs", "[", "i", ":", "i", "+", "self", ".", "_hps", ".", "batch_size", "]", ")", "\n", "", "if", "not", "self", ".", "_single_pass", ":", "\n", "          ", "shuffle", "(", "batches", ")", "\n", "", "for", "b", "in", "batches", ":", "# each b is a list of Example objects", "\n", "          ", "self", ".", "_batch_queue", ".", "put", "(", "Batch", "(", "b", ",", "self", ".", "_hps", ",", "self", ".", "_vocab", ")", ")", "\n", "\n", "", "", "else", ":", "# beam search decode mode", "\n", "        ", "ex", "=", "self", ".", "_example_queue", ".", "get", "(", ")", "\n", "b", "=", "[", "ex", "for", "_", "in", "range", "(", "self", ".", "_hps", ".", "batch_size", ")", "]", "\n", "self", ".", "_batch_queue", ".", "put", "(", "Batch", "(", "b", ",", "self", ".", "_hps", ",", "self", ".", "_vocab", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.batcher.Batcher.watch_threads": [[401, 419], ["time.sleep", "enumerate", "enumerate", "t.is_alive", "tensorflow.logging.error", "threading.Thread", "threading.Thread.start", "t.is_alive", "tensorflow.logging.error", "threading.Thread", "threading.Thread.start"], "methods", ["None"], ["", "", "", "def", "watch_threads", "(", "self", ")", ":", "\n", "    ", "\"\"\"Watch example queue and batch queue threads and restart if dead.\"\"\"", "\n", "while", "True", ":", "\n", "      ", "time", ".", "sleep", "(", "60", ")", "\n", "for", "idx", ",", "t", "in", "enumerate", "(", "self", ".", "_example_q_threads", ")", ":", "\n", "        ", "if", "not", "t", ".", "is_alive", "(", ")", ":", "# if the thread is dead", "\n", "          ", "tf", ".", "logging", ".", "error", "(", "'Found example queue thread dead. Restarting.'", ")", "\n", "new_t", "=", "Thread", "(", "target", "=", "self", ".", "fill_example_queue", ")", "\n", "self", ".", "_example_q_threads", "[", "idx", "]", "=", "new_t", "\n", "new_t", ".", "daemon", "=", "True", "\n", "new_t", ".", "start", "(", ")", "\n", "", "", "for", "idx", ",", "t", "in", "enumerate", "(", "self", ".", "_batch_q_threads", ")", ":", "\n", "        ", "if", "not", "t", ".", "is_alive", "(", ")", ":", "# if the thread is dead", "\n", "          ", "tf", ".", "logging", ".", "error", "(", "'Found batch queue thread dead. Restarting.'", ")", "\n", "new_t", "=", "Thread", "(", "target", "=", "self", ".", "fill_batch_queue", ")", "\n", "self", ".", "_batch_q_threads", "[", "idx", "]", "=", "new_t", "\n", "new_t", ".", "daemon", "=", "True", "\n", "new_t", ".", "start", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.batcher.Batcher.text_generator": [[421, 441], ["next", "eval", "eval", "next.features.feature[].bytes_list.value[].decode", "next.features.feature[].bytes_list.value[].decode", "next.features.feature[].bytes_list.value[].decode", "len", "tensorflow.logging.warning", "next.features.feature[].bytes_list.value[].decode", "next.features.feature[].bytes_list.value[].decode", "tensorflow.logging.error"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.BeamSearchDecoder.decode", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.BeamSearchDecoder.decode", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.BeamSearchDecoder.decode", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.BeamSearchDecoder.decode", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.BeamSearchDecoder.decode"], ["", "", "", "", "def", "text_generator", "(", "self", ",", "example_generator", ")", ":", "\n", "    ", "\"\"\"Generates article and abstract text from tf.Example.\n\n    Args:\n      example_generator: a generator of tf.Examples from file. See data.example_generator\"\"\"", "\n", "while", "True", ":", "\n", "      ", "e", "=", "next", "(", "example_generator", ")", "# e is a tf.Example", "\n", "try", ":", "\n", "        ", "reviews", "=", "eval", "(", "e", ".", "features", ".", "feature", "[", "'reviews'", "]", ".", "bytes_list", ".", "value", "[", "0", "]", ".", "decode", "(", ")", ")", "# the article text was saved under the key 'article' in the data files", "\n", "ratings", "=", "eval", "(", "e", ".", "features", ".", "feature", "[", "'ratings'", "]", ".", "bytes_list", ".", "value", "[", "0", "]", ".", "decode", "(", ")", ")", "\n", "answer_text", "=", "e", ".", "features", ".", "feature", "[", "'answer'", "]", ".", "bytes_list", ".", "value", "[", "0", "]", ".", "decode", "(", ")", "# the abstract text was saved under the key 'abstract' in the data files", "\n", "question_text", "=", "e", ".", "features", ".", "feature", "[", "'question'", "]", ".", "bytes_list", ".", "value", "[", "0", "]", ".", "decode", "(", ")", "# the question text was saved under the key 'question' in the data files", "\n", "answer_label", "=", "e", ".", "features", ".", "feature", "[", "'label'", "]", ".", "bytes_list", ".", "value", "[", "0", "]", ".", "decode", "(", ")", "# the answer label was saved under the key 'label' in the data files", "\n", "", "except", "ValueError", ":", "\n", "        ", "tf", ".", "logging", ".", "error", "(", "'Failed to get article or abstract from example'", ")", "\n", "continue", "\n", "", "if", "len", "(", "question_text", ")", "==", "0", ":", "\n", "        ", "tf", ".", "logging", ".", "warning", "(", "'Found an example with empty article text. Skipping it.'", ")", "\n", "", "else", ":", "\n", "        ", "yield", "(", "reviews", ",", "ratings", ",", "answer_text", ",", "question_text", ",", "answer_label", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.data.Vocab.__init__": [[44, 80], ["print", "open", "line.split", "len", "print", "Exception", "Exception", "print"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "max_size", ")", ":", "\n", "    ", "\"\"\"Creates a vocab of up to max_size words, reading from the vocab_file. If max_size is 0, reads the entire vocab file.\n\n    Args:\n      vocab_file: path to the vocab file, which is assumed to contain \"<word> <frequency>\" on each line, sorted with most frequent word first. This code doesn't actually use the frequencies, though.\n      max_size: integer. The maximum size of the resulting Vocabulary.\"\"\"", "\n", "self", ".", "_word_to_id", "=", "{", "}", "\n", "self", ".", "_id_to_word", "=", "{", "}", "\n", "self", ".", "_count", "=", "0", "# keeps track of total number of words in the Vocab", "\n", "\n", "# [UNK], [PAD], [START] and [STOP] get the ids 0,1,2,3.", "\n", "for", "w", "in", "[", "UNKNOWN_TOKEN", ",", "PAD_TOKEN", ",", "START_DECODING", ",", "STOP_DECODING", "]", ":", "\n", "      ", "self", ".", "_word_to_id", "[", "w", "]", "=", "self", ".", "_count", "\n", "self", ".", "_id_to_word", "[", "self", ".", "_count", "]", "=", "w", "\n", "self", ".", "_count", "+=", "1", "\n", "\n", "# Read the vocab file and add words up to max_size", "\n", "", "with", "open", "(", "vocab_file", ",", "'r'", ")", "as", "vocab_f", ":", "\n", "      ", "for", "line", "in", "vocab_f", ":", "\n", "        ", "pieces", "=", "line", ".", "split", "(", ")", "\n", "if", "len", "(", "pieces", ")", "!=", "2", ":", "\n", "          ", "print", "(", "'Warning: incorrectly formatted line in vocabulary file: %s\\n'", "%", "line", ")", "\n", "continue", "\n", "", "w", "=", "pieces", "[", "0", "]", "\n", "if", "w", "in", "[", "SENTENCE_START", ",", "SENTENCE_END", ",", "UNKNOWN_TOKEN", ",", "PAD_TOKEN", ",", "START_DECODING", ",", "STOP_DECODING", "]", ":", "\n", "          ", "raise", "Exception", "(", "'<s>, </s>, [UNK], [PAD], [START] and [STOP] shouldn\\'t be in the vocab file, but %s is'", "%", "w", ")", "\n", "", "if", "w", "in", "self", ".", "_word_to_id", ":", "\n", "          ", "raise", "Exception", "(", "'Duplicated word in vocabulary file: %s'", "%", "w", ")", "\n", "", "self", ".", "_word_to_id", "[", "w", "]", "=", "self", ".", "_count", "\n", "self", ".", "_id_to_word", "[", "self", ".", "_count", "]", "=", "w", "\n", "self", ".", "_count", "+=", "1", "\n", "if", "max_size", "!=", "0", "and", "self", ".", "_count", ">=", "max_size", ":", "\n", "          ", "print", "(", "\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\"", "%", "(", "max_size", ",", "self", ".", "_count", ")", ")", "\n", "break", "\n", "\n", "", "", "", "print", "(", "\"Finished constructing vocabulary of %i total words. Last word added: %s\"", "%", "(", "self", ".", "_count", ",", "self", ".", "_id_to_word", "[", "self", ".", "_count", "-", "1", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.data.Vocab.word2id": [[81, 86], ["None"], "methods", ["None"], ["", "def", "word2id", "(", "self", ",", "word", ")", ":", "\n", "    ", "\"\"\"Returns the id (integer) of a word (string). Returns [UNK] id if word is OOV.\"\"\"", "\n", "if", "word", "not", "in", "self", ".", "_word_to_id", ":", "\n", "      ", "return", "self", ".", "_word_to_id", "[", "UNKNOWN_TOKEN", "]", "\n", "", "return", "self", ".", "_word_to_id", "[", "word", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.data.Vocab.id2word": [[87, 92], ["ValueError"], "methods", ["None"], ["", "def", "id2word", "(", "self", ",", "word_id", ")", ":", "\n", "    ", "\"\"\"Returns the word (string) corresponding to an id (integer).\"\"\"", "\n", "if", "word_id", "not", "in", "self", ".", "_id_to_word", ":", "\n", "      ", "raise", "ValueError", "(", "'Id not found in vocab: %d'", "%", "word_id", ")", "\n", "", "return", "self", ".", "_id_to_word", "[", "word_id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.data.Vocab.size": [[93, 96], ["None"], "methods", ["None"], ["", "def", "size", "(", "self", ")", ":", "\n", "    ", "\"\"\"Returns the total size of the vocabulary\"\"\"", "\n", "return", "self", ".", "_count", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.data.Vocab.write_metadata": [[97, 110], ["print", "open", "csv.DictWriter", "range", "data.Vocab.size", "csv.DictWriter.writerow"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.size"], ["", "def", "write_metadata", "(", "self", ",", "fpath", ")", ":", "\n", "    ", "\"\"\"Writes metadata file for Tensorboard word embedding visualizer as described here:\n      https://www.tensorflow.org/get_started/embedding_viz\n\n    Args:\n      fpath: place to write the metadata file\n    \"\"\"", "\n", "print", "(", "\"Writing word embedding metadata file to %s...\"", "%", "(", "fpath", ")", ")", "\n", "with", "open", "(", "fpath", ",", "\"w\"", ")", "as", "f", ":", "\n", "      ", "fieldnames", "=", "[", "'word'", "]", "\n", "writer", "=", "csv", ".", "DictWriter", "(", "f", ",", "delimiter", "=", "\"\\t\"", ",", "fieldnames", "=", "fieldnames", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "size", "(", ")", ")", ":", "\n", "        ", "writer", ".", "writerow", "(", "{", "\"word\"", ":", "self", ".", "_id_to_word", "[", "i", "]", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.data.example_generator": [[112, 146], ["glob.glob", "sorted", "random.shuffle", "open", "print", "open.read", "struct.unpack", "struct.unpack", "tensorflow.core.example.example_pb2.Example.FromString", "open.read"], "function", ["None"], ["", "", "", "", "def", "example_generator", "(", "data_path", ",", "single_pass", ")", ":", "\n", "  ", "\"\"\"Generates tf.Examples from data files.\n\n    Binary data format: <length><blob>. <length> represents the byte size\n    of <blob>. <blob> is serialized tf.Example proto. The tf.Example contains\n    the tokenized article text and summary.\n\n  Args:\n    data_path:\n      Path to tf.Example data files. Can include wildcards, e.g. if you have several training data chunk files train_001.bin, train_002.bin, etc, then pass data_path=train_* to access them all.\n    single_pass:\n      Boolean. If True, go through the dataset exactly once, generating examples in the order they appear, then return. Otherwise, generate random examples indefinitely.\n\n  Yields:\n    Deserialized tf.Example.\n  \"\"\"", "\n", "while", "True", ":", "\n", "    ", "filelist", "=", "glob", ".", "glob", "(", "data_path", ")", "# get the list of datafiles", "\n", "assert", "filelist", ",", "(", "'Error: Empty filelist at %s'", "%", "data_path", ")", "# check filelist isn't empty", "\n", "if", "single_pass", ":", "\n", "      ", "filelist", "=", "sorted", "(", "filelist", ")", "\n", "", "else", ":", "\n", "      ", "random", ".", "shuffle", "(", "filelist", ")", "\n", "", "for", "f", "in", "filelist", ":", "\n", "      ", "reader", "=", "open", "(", "f", ",", "'rb'", ")", "\n", "while", "True", ":", "\n", "        ", "len_bytes", "=", "reader", ".", "read", "(", "8", ")", "\n", "if", "not", "len_bytes", ":", "break", "# finished reading this file", "\n", "str_len", "=", "struct", ".", "unpack", "(", "'q'", ",", "len_bytes", ")", "[", "0", "]", "\n", "example_str", "=", "struct", ".", "unpack", "(", "'%ds'", "%", "str_len", ",", "reader", ".", "read", "(", "str_len", ")", ")", "[", "0", "]", "\n", "yield", "example_pb2", ".", "Example", ".", "FromString", "(", "example_str", ")", "\n", "", "", "if", "single_pass", ":", "\n", "      ", "print", "(", "\"example_generator completed reading all datafiles. No more data.\"", ")", "\n", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.data.article2ids": [[148, 172], ["vocab.word2id", "vocab.word2id", "oovs.index", "ids.append", "ids.append", "oovs.append", "vocab.size"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.size"], ["", "", "", "def", "article2ids", "(", "article_words", ",", "vocab", ",", "oovs", ")", ":", "\n", "  ", "\"\"\"Map the article words to their ids. Also return a list of OOVs in the article.\n\n  Args:\n    article_words: list of words (strings)\n    vocab: Vocabulary object\n\n  Returns:\n    ids:\n      A list of word ids (integers); OOVs are represented by their temporary article OOV number. If the vocabulary size is 50k and the article has 3 OOVs, then these temporary OOV numbers will be 50000, 50001, 50002.\n    oovs:\n      A list of the OOV words in the article (strings), in the order corresponding to their temporary article OOV numbers.\"\"\"", "\n", "ids", "=", "[", "]", "\n", "unk_id", "=", "vocab", ".", "word2id", "(", "UNKNOWN_TOKEN", ")", "\n", "for", "w", "in", "article_words", ":", "\n", "    ", "i", "=", "vocab", ".", "word2id", "(", "w", ")", "\n", "if", "i", "==", "unk_id", ":", "# If w is OOV", "\n", "      ", "if", "w", "not", "in", "oovs", ":", "# Add to list of OOVs", "\n", "        ", "oovs", ".", "append", "(", "w", ")", "\n", "", "oov_num", "=", "oovs", ".", "index", "(", "w", ")", "# This is 0 for the first article OOV, 1 for the second article OOV...", "\n", "ids", ".", "append", "(", "vocab", ".", "size", "(", ")", "+", "oov_num", ")", "# This is e.g. 50000 for the first article OOV, 50001 for the second...", "\n", "", "else", ":", "\n", "      ", "ids", ".", "append", "(", "i", ")", "\n", "", "", "return", "ids", ",", "oovs", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.data.abstract2ids": [[174, 197], ["vocab.word2id", "vocab.word2id", "ids.append", "ids.append", "ids.append", "vocab.size", "article_oovs.index"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.size"], ["", "def", "abstract2ids", "(", "abstract_words", ",", "vocab", ",", "article_oovs", ")", ":", "\n", "  ", "\"\"\"Map the abstract words to their ids. In-article OOVs are mapped to their temporary OOV numbers.\n\n  Args:\n    abstract_words: list of words (strings)\n    vocab: Vocabulary object\n    article_oovs: list of in-article OOV words (strings), in the order corresponding to their temporary article OOV numbers\n\n  Returns:\n    ids: List of ids (integers). In-article OOV words are mapped to their temporary OOV numbers. Out-of-article OOV words are mapped to the UNK token id.\"\"\"", "\n", "ids", "=", "[", "]", "\n", "unk_id", "=", "vocab", ".", "word2id", "(", "UNKNOWN_TOKEN", ")", "\n", "for", "w", "in", "abstract_words", ":", "\n", "    ", "i", "=", "vocab", ".", "word2id", "(", "w", ")", "\n", "if", "i", "==", "unk_id", ":", "# If w is an OOV word", "\n", "      ", "if", "w", "in", "article_oovs", ":", "# If w is an in-article OOV", "\n", "        ", "vocab_idx", "=", "vocab", ".", "size", "(", ")", "+", "article_oovs", ".", "index", "(", "w", ")", "# Map to its temporary article OOV number", "\n", "ids", ".", "append", "(", "vocab_idx", ")", "\n", "", "else", ":", "# If w is an out-of-article OOV", "\n", "        ", "ids", ".", "append", "(", "unk_id", ")", "# Map to the UNK token id", "\n", "", "", "else", ":", "\n", "      ", "ids", ".", "append", "(", "i", ")", "\n", "", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.data.outputids2words": [[199, 223], ["words.append", "vocab.id2word", "vocab.size", "ValueError", "len"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.id2word", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.size"], ["", "def", "outputids2words", "(", "id_list", ",", "vocab", ",", "article_oovs", ")", ":", "\n", "  ", "\"\"\"Maps output ids to words, including mapping in-article OOVs from their temporary ids to the original OOV string (applicable in pointer-generator mode).\n\n  Args:\n    id_list: list of ids (integers)\n    vocab: Vocabulary object\n    article_oovs: list of OOV words (strings) in the order corresponding to their temporary article OOV ids (that have been assigned in pointer-generator mode), or None (in baseline mode)\n\n  Returns:\n    words: list of words (strings)\n  \"\"\"", "\n", "words", "=", "[", "]", "\n", "for", "i", "in", "id_list", ":", "\n", "    ", "try", ":", "\n", "      ", "w", "=", "vocab", ".", "id2word", "(", "i", ")", "# might be [UNK]", "\n", "", "except", "ValueError", "as", "e", ":", "# w is OOV", "\n", "      ", "assert", "article_oovs", "is", "not", "None", ",", "\"Error: model produced a word ID that isn't in the vocabulary. This should not happen in baseline (no pointer-generator) mode\"", "\n", "article_oov_idx", "=", "i", "-", "vocab", ".", "size", "(", ")", "\n", "try", ":", "\n", "        ", "w", "=", "article_oovs", "[", "article_oov_idx", "]", "\n", "", "except", "ValueError", "as", "e", ":", "# i doesn't correspond to an article oov", "\n", "        ", "raise", "ValueError", "(", "'Error: model produced word ID %i which corresponds to article OOV %i but this example only has %i article OOVs'", "%", "(", "i", ",", "article_oov_idx", ",", "len", "(", "article_oovs", ")", ")", ")", "\n", "", "", "words", ".", "append", "(", "w", ")", "\n", "", "return", "words", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.data.abstract2sents": [[225, 244], ["abstract.index", "abstract.index", "sents.append", "len", "len"], "function", ["None"], ["", "def", "abstract2sents", "(", "abstract", ")", ":", "\n", "  ", "\"\"\"Splits abstract text from datafile into list of sentences.\n\n  Args:\n    abstract: string containing <s> and </s> tags for starts and ends of sentences\n\n  Returns:\n    sents: List of sentence strings (no tags)\"\"\"", "\n", "cur", "=", "0", "\n", "sents", "=", "[", "abstract", "]", "\n", "return", "sents", "\n", "while", "True", ":", "\n", "    ", "try", ":", "\n", "      ", "start_p", "=", "abstract", ".", "index", "(", "SENTENCE_START", ",", "cur", ")", "\n", "end_p", "=", "abstract", ".", "index", "(", "SENTENCE_END", ",", "start_p", "+", "1", ")", "\n", "cur", "=", "end_p", "+", "len", "(", "SENTENCE_END", ")", "\n", "sents", ".", "append", "(", "abstract", "[", "start_p", "+", "len", "(", "SENTENCE_START", ")", ":", "end_p", "]", ")", "\n", "", "except", "ValueError", "as", "e", ":", "# no more sentences", "\n", "      ", "return", "sents", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.data.show_art_oovs": [[246, 253], ["vocab.word2id", "article.split", "vocab.word2id"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id"], ["", "", "", "def", "show_art_oovs", "(", "article", ",", "vocab", ")", ":", "\n", "  ", "\"\"\"Returns the article string, highlighting the OOVs by placing __underscores__ around them\"\"\"", "\n", "unk_token", "=", "vocab", ".", "word2id", "(", "UNKNOWN_TOKEN", ")", "\n", "words", "=", "article", ".", "split", "(", "' '", ")", "\n", "words", "=", "[", "(", "\"__%s__\"", "%", "w", ")", "if", "vocab", ".", "word2id", "(", "w", ")", "==", "unk_token", "else", "w", "for", "w", "in", "words", "]", "\n", "out_str", "=", "' '", ".", "join", "(", "words", ")", "\n", "return", "out_str", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.data.show_abs_oovs": [[255, 281], ["vocab.word2id", "abstract.split", "vocab.word2id", "new_words.append", "new_words.append", "new_words.append", "new_words.append"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id"], ["", "def", "show_abs_oovs", "(", "abstract", ",", "vocab", ",", "article_oovs", ")", ":", "\n", "  ", "\"\"\"Returns the abstract string, highlighting the article OOVs with __underscores__.\n\n  If a list of article_oovs is provided, non-article OOVs are differentiated like !!__this__!!.\n\n  Args:\n    abstract: string\n    vocab: Vocabulary object\n    article_oovs: list of words (strings), or None (in baseline mode)\n  \"\"\"", "\n", "unk_token", "=", "vocab", ".", "word2id", "(", "UNKNOWN_TOKEN", ")", "\n", "words", "=", "abstract", ".", "split", "(", "' '", ")", "\n", "new_words", "=", "[", "]", "\n", "for", "w", "in", "words", ":", "\n", "    ", "if", "vocab", ".", "word2id", "(", "w", ")", "==", "unk_token", ":", "# w is oov", "\n", "      ", "if", "article_oovs", "is", "None", ":", "# baseline mode", "\n", "        ", "new_words", ".", "append", "(", "\"__%s__\"", "%", "w", ")", "\n", "", "else", ":", "# pointer-generator mode", "\n", "        ", "if", "w", "in", "article_oovs", ":", "\n", "          ", "new_words", ".", "append", "(", "\"__%s__\"", "%", "w", ")", "\n", "", "else", ":", "\n", "          ", "new_words", ".", "append", "(", "\"!!__%s__!!\"", "%", "w", ")", "\n", "", "", "", "else", ":", "# w is in-vocab word", "\n", "      ", "new_words", ".", "append", "(", "w", ")", "\n", "", "", "out_str", "=", "' '", ".", "join", "(", "new_words", ")", "\n", "return", "out_str", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.data.get_init_embeddings": [[283, 304], ["gensim.test.utils.get_tmpfile", "gensim.scripts.glove2word2vec.glove2word2vec", "print", "gensim.models.keyedvectors.KeyedVectors.load_word2vec_format", "list", "sorted", "numpy.random.normal", "numpy.random.normal", "numpy.array", "str", "reversed_dict.items", "list.append", "KeyedVectors.load_word2vec_format.word_vec", "numpy.zeros"], "function", ["None"], ["", "def", "get_init_embeddings", "(", "reversed_dict", ",", "embedding_size", ")", ":", "\n", "    ", "glove_file", "=", "\"embed/glove.6B.%sd.txt\"", "%", "str", "(", "embedding_size", ")", "\n", "word2vec_file", "=", "get_tmpfile", "(", "\"word2vec_format.vec\"", ")", "\n", "glove2word2vec", "(", "glove_file", ",", "word2vec_file", ")", "\n", "print", "(", "\"Loading Glove vectors from %s\"", "%", "glove_file", ")", "\n", "word_vectors", "=", "KeyedVectors", ".", "load_word2vec_format", "(", "word2vec_file", ")", "\n", "\n", "word_vec_list", "=", "list", "(", ")", "\n", "for", "_", ",", "word", "in", "sorted", "(", "reversed_dict", ".", "items", "(", ")", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "word_vec", "=", "word_vectors", ".", "word_vec", "(", "word", ")", "\n", "", "except", "KeyError", ":", "\n", "            ", "word_vec", "=", "np", ".", "zeros", "(", "[", "embedding_size", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "", "word_vec_list", ".", "append", "(", "word_vec", ")", "\n", "\n", "# Assign random vector to <s>, </s> token", "\n", "", "word_vec_list", "[", "2", "]", "=", "np", ".", "random", ".", "normal", "(", "0", ",", "1", ",", "embedding_size", ")", "\n", "word_vec_list", "[", "3", "]", "=", "np", ".", "random", ".", "normal", "(", "0", ",", "1", ",", "embedding_size", ")", "\n", "\n", "return", "np", ".", "array", "(", "word_vec_list", ",", "dtype", "=", "np", ".", "float32", ")", "", "", ""]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.model.SummarizationModel.__init__": [[32, 35], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "hps", ",", "vocab", ")", ":", "\n", "    ", "self", ".", "_hps", "=", "hps", "\n", "self", ".", "_vocab", "=", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.model.SummarizationModel._add_placeholders": [[36, 62], ["tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder"], "methods", ["None"], ["", "def", "_add_placeholders", "(", "self", ")", ":", "\n", "    ", "\"\"\"Add placeholders to the graph. These are entry points for any input data.\"\"\"", "\n", "hps", "=", "self", ".", "_hps", "\n", "\n", "# encoder part", "\n", "self", ".", "_r_batch", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "hps", ".", "batch_size", ",", "hps", ".", "review_num", ",", "None", "]", ",", "name", "=", "'r_batch'", ")", "\n", "self", ".", "_r_lens", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "hps", ".", "batch_size", ",", "hps", ".", "review_num", "]", ",", "name", "=", "'r_lens'", ")", "\n", "self", ".", "_q_batch", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "hps", ".", "batch_size", ",", "None", "]", ",", "name", "=", "'q_batch'", ")", "\n", "self", ".", "_q_lens", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "hps", ".", "batch_size", "]", ",", "name", "=", "'q_lens'", ")", "\n", "self", ".", "_rating_batch", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "hps", ".", "batch_size", ",", "hps", ".", "review_num", "]", ",", "name", "=", "'rating_batch'", ")", "\n", "self", ".", "_r_padding_mask", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "hps", ".", "batch_size", ",", "hps", ".", "review_num", ",", "None", "]", ",", "name", "=", "'r_padding_mask'", ")", "\n", "self", ".", "_q_padding_mask", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "hps", ".", "batch_size", ",", "None", "]", ",", "name", "=", "'q_padding_mask'", ")", "\n", "if", "FLAGS", ".", "pointer_gen", ":", "\n", "      ", "self", ".", "_r_batch_extend_vocab", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "hps", ".", "batch_size", ",", "hps", ".", "review_num", ",", "None", "]", ",", "name", "=", "'r_batch_extend_vocab'", ")", "\n", "self", ".", "_q_batch_extend_vocab", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "hps", ".", "batch_size", ",", "None", "]", ",", "name", "=", "'q_batch_extend_vocab'", ")", "\n", "self", ".", "_max_oovs", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "]", ",", "name", "=", "'max_oovs'", ")", "\n", "\n", "# decoder part", "\n", "", "self", ".", "_dec_batch", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "hps", ".", "batch_size", ",", "hps", ".", "max_dec_steps", "]", ",", "name", "=", "'dec_batch'", ")", "\n", "self", ".", "_target_batch", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "hps", ".", "batch_size", ",", "hps", ".", "max_dec_steps", "]", ",", "name", "=", "'target_batch'", ")", "\n", "self", ".", "_y_target_batch", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "hps", ".", "batch_size", "]", ",", "name", "=", "'y_target_batch'", ")", "\n", "self", ".", "_dec_padding_mask", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "hps", ".", "batch_size", ",", "hps", ".", "max_dec_steps", "]", ",", "name", "=", "'dec_padding_mask'", ")", "\n", "\n", "if", "hps", ".", "mode", "==", "\"decode\"", "and", "hps", ".", "coverage", ":", "\n", "      ", "self", ".", "q_prev_coverage", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "hps", ".", "batch_size", ",", "None", "]", ",", "name", "=", "'q_prev_coverage'", ")", "\n", "self", ".", "r_prev_coverage", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "hps", ".", "batch_size", ",", "None", "]", ",", "name", "=", "'r_prev_coverage'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.model.SummarizationModel._make_feed_dict": [[64, 89], ["None"], "methods", ["None"], ["", "", "def", "_make_feed_dict", "(", "self", ",", "batch", ",", "just_enc", "=", "False", ")", ":", "\n", "    ", "\"\"\"Make a feed dictionary mapping parts of the batch to the appropriate placeholders.\n\n    Args:\n      batch: Batch object\n      just_enc: Boolean. If True, only feed the parts needed for the encoder.\n    \"\"\"", "\n", "feed_dict", "=", "{", "}", "\n", "feed_dict", "[", "self", ".", "_r_batch", "]", "=", "batch", ".", "r_batch", "\n", "feed_dict", "[", "self", ".", "_r_lens", "]", "=", "batch", ".", "r_lens", "\n", "feed_dict", "[", "self", ".", "_q_batch", "]", "=", "batch", ".", "q_batch", "\n", "feed_dict", "[", "self", ".", "_q_lens", "]", "=", "batch", ".", "q_lens", "\n", "feed_dict", "[", "self", ".", "_rating_batch", "]", "=", "batch", ".", "rating_batch", "\n", "feed_dict", "[", "self", ".", "_r_padding_mask", "]", "=", "batch", ".", "r_padding_mask", "\n", "feed_dict", "[", "self", ".", "_q_padding_mask", "]", "=", "batch", ".", "q_padding_mask", "\n", "if", "FLAGS", ".", "pointer_gen", ":", "\n", "      ", "feed_dict", "[", "self", ".", "_r_batch_extend_vocab", "]", "=", "batch", ".", "r_batch_extend_vocab", "\n", "feed_dict", "[", "self", ".", "_q_batch_extend_vocab", "]", "=", "batch", ".", "q_batch_extend_vocab", "\n", "feed_dict", "[", "self", ".", "_max_oovs", "]", "=", "batch", ".", "max_oovs", "\n", "", "if", "not", "just_enc", ":", "\n", "      ", "feed_dict", "[", "self", ".", "_dec_batch", "]", "=", "batch", ".", "dec_batch", "\n", "feed_dict", "[", "self", ".", "_target_batch", "]", "=", "batch", ".", "target_batch", "\n", "feed_dict", "[", "self", ".", "_y_target_batch", "]", "=", "batch", ".", "y_target_batch", "\n", "feed_dict", "[", "self", ".", "_dec_padding_mask", "]", "=", "batch", ".", "dec_padding_mask", "\n", "", "return", "feed_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.model.SummarizationModel._add_encoder": [[90, 111], ["tensorflow.compat.v1.variable_scope", "tensorflow.contrib.rnn.LSTMCell", "tensorflow.contrib.rnn.DropoutWrapper", "tensorflow.contrib.rnn.LSTMCell", "tensorflow.contrib.rnn.DropoutWrapper", "tensorflow.nn.bidirectional_dynamic_rnn", "tensorflow.concat"], "methods", ["None"], ["", "def", "_add_encoder", "(", "self", ",", "encoder_inputs", ",", "seq_len", ",", "scope", ",", "dropout", "=", "0.8", ")", ":", "\n", "    ", "\"\"\"Add a single-layer bidirectional LSTM encoder to the graph.\n\n    Args:\n      encoder_inputs: A tensor of shape [batch_size, <=max_enc_steps, emb_size].\n      seq_len: Lengths of encoder_inputs (before padding). A tensor of shape [batch_size].\n\n    Returns:\n      encoder_outputs:\n        A tensor of shape [batch_size, <=max_enc_steps, 2*hidden_dim]. It's 2*hidden_dim because it's the concatenation of the forwards and backwards states.\n      fw_state, bw_state:\n        Each are LSTMStateTuples of shape ([batch_size,hidden_dim],[batch_size,hidden_dim])\n    \"\"\"", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "'encoder'", "+", "scope", ")", ":", "\n", "      ", "cell_fw", "=", "tf", ".", "contrib", ".", "rnn", ".", "LSTMCell", "(", "self", ".", "_hps", ".", "hidden_dim", ",", "initializer", "=", "self", ".", "rand_unif_init", ",", "state_is_tuple", "=", "True", ")", "\n", "cell_fw", "=", "tf", ".", "contrib", ".", "rnn", ".", "DropoutWrapper", "(", "cell_fw", ",", "output_keep_prob", "=", "dropout", ")", "\n", "cell_bw", "=", "tf", ".", "contrib", ".", "rnn", ".", "LSTMCell", "(", "self", ".", "_hps", ".", "hidden_dim", ",", "initializer", "=", "self", ".", "rand_unif_init", ",", "state_is_tuple", "=", "True", ")", "\n", "cell_bw", "=", "tf", ".", "contrib", ".", "rnn", ".", "DropoutWrapper", "(", "cell_bw", ",", "output_keep_prob", "=", "dropout", ")", "\n", "(", "encoder_outputs", ",", "(", "fw_st", ",", "bw_st", ")", ")", "=", "tf", ".", "nn", ".", "bidirectional_dynamic_rnn", "(", "cell_fw", ",", "cell_bw", ",", "encoder_inputs", ",", "dtype", "=", "tf", ".", "float32", ",", "sequence_length", "=", "seq_len", ",", "swap_memory", "=", "True", ")", "\n", "encoder_outputs", "=", "tf", ".", "concat", "(", "axis", "=", "2", ",", "values", "=", "encoder_outputs", ")", "# concatenate the forwards and backwards states", "\n", "", "return", "encoder_outputs", ",", "fw_st", ",", "bw_st", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.model.SummarizationModel._reduce_states": [[113, 139], ["op.get_shape", "tensorflow.compat.v1.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.concat", "tensorflow.concat", "tensorflow.nn.relu", "tensorflow.nn.relu", "tensorflow.contrib.rnn.LSTMStateTuple", "tensorflow.matmul", "tensorflow.matmul"], "methods", ["None"], ["", "def", "_reduce_states", "(", "self", ",", "fw_st", ",", "bw_st", ",", "op", ")", ":", "\n", "    ", "\"\"\"Add to the graph a linear layer to reduce the encoder's final FW and BW state into a single initial state for the decoder. This is needed because the encoder is bidirectional but the decoder is not.\n\n    Args:\n      fw_st: LSTMStateTuple with hidden_dim units.\n      bw_st: LSTMStateTuple with hidden_dim units.\n\n    Returns:\n      state: LSTMStateTuple with hidden_dim units.\n    \"\"\"", "\n", "hidden_dim", "=", "self", ".", "_hps", ".", "hidden_dim", "\n", "op_dim", "=", "op", ".", "get_shape", "(", ")", "[", "1", "]", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "'reduce_final_st'", ")", ":", "\n", "\n", "# Define weights and biases to reduce the cell and reduce the state", "\n", "      ", "w_reduce_c", "=", "tf", ".", "get_variable", "(", "'w_reduce_c'", ",", "[", "hidden_dim", "*", "2", "+", "op_dim", ",", "hidden_dim", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "initializer", "=", "self", ".", "trunc_norm_init", ")", "\n", "w_reduce_h", "=", "tf", ".", "get_variable", "(", "'w_reduce_h'", ",", "[", "hidden_dim", "*", "2", "+", "op_dim", ",", "hidden_dim", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "initializer", "=", "self", ".", "trunc_norm_init", ")", "\n", "bias_reduce_c", "=", "tf", ".", "get_variable", "(", "'bias_reduce_c'", ",", "[", "hidden_dim", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "initializer", "=", "self", ".", "trunc_norm_init", ")", "\n", "bias_reduce_h", "=", "tf", ".", "get_variable", "(", "'bias_reduce_h'", ",", "[", "hidden_dim", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "initializer", "=", "self", ".", "trunc_norm_init", ")", "\n", "\n", "# Apply linear layer", "\n", "old_c", "=", "tf", ".", "concat", "(", "axis", "=", "1", ",", "values", "=", "[", "fw_st", ".", "c", ",", "bw_st", ".", "c", ",", "op", "]", ")", "# Concatenation of fw and bw cell", "\n", "old_h", "=", "tf", ".", "concat", "(", "axis", "=", "1", ",", "values", "=", "[", "fw_st", ".", "h", ",", "bw_st", ".", "h", ",", "op", "]", ")", "# Concatenation of fw and bw state", "\n", "new_c", "=", "tf", ".", "nn", ".", "relu", "(", "tf", ".", "matmul", "(", "old_c", ",", "w_reduce_c", ")", "+", "bias_reduce_c", ")", "# Get new cell from old cell", "\n", "new_h", "=", "tf", ".", "nn", ".", "relu", "(", "tf", ".", "matmul", "(", "old_h", ",", "w_reduce_h", ")", "+", "bias_reduce_h", ")", "# Get new state from old state", "\n", "return", "tf", ".", "contrib", ".", "rnn", ".", "LSTMStateTuple", "(", "new_c", ",", "new_h", ")", "# Return new cell and state", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.model.SummarizationModel._add_decoder": [[141, 163], ["tensorflow.contrib.rnn.LSTMCell", "attention_decoder.attention_decoder.attention_decoder"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.attention_decoder.attention_decoder"], ["", "", "def", "_add_decoder", "(", "self", ",", "inputs", ")", ":", "\n", "    ", "\"\"\"Add attention decoder to the graph. In train or eval mode, you call this once to get output on ALL steps. In decode (beam search) mode, you call this once for EACH decoder step.\n\n    Args:\n      inputs: inputs to the decoder (word embeddings). A list of tensors shape (batch_size, emb_dim)\n\n    Returns:\n      outputs: List of tensors; the outputs of the decoder\n      out_state: The final state of the decoder\n      attn_dists: A list of tensors; the attention distributions\n      p_gens: A list of scalar tensors; the generation probabilities\n      coverage: A tensor, the current coverage vector\n    \"\"\"", "\n", "hps", "=", "self", ".", "_hps", "\n", "cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "LSTMCell", "(", "hps", ".", "hidden_dim", ",", "state_is_tuple", "=", "True", ",", "initializer", "=", "self", ".", "rand_unif_init", ")", "\n", "\n", "q_prev_coverage", "=", "self", ".", "q_prev_coverage", "if", "hps", ".", "mode", "==", "\"decode\"", "and", "hps", ".", "coverage", "else", "None", "# In decode mode, we run attention_decoder one step at a time and so need to pass in the previous step's coverage vector each time", "\n", "r_prev_coverage", "=", "self", ".", "r_prev_coverage", "if", "hps", ".", "mode", "==", "\"decode\"", "and", "hps", ".", "coverage", "else", "None", "\n", "\n", "outputs", ",", "output_states", ",", "out_state", ",", "q_attn_dists", ",", "r_attn_dists", ",", "p_gens", ",", "q_coverage", ",", "r_coverage", "=", "attention_decoder", "(", "inputs", ",", "self", ".", "_dec_in_state", ",", "self", ".", "dec_question_inputs", ",", "self", ".", "dec_review_inputs", ",", "self", ".", "review_attention", ",", "hps", ".", "review_num", ",", "self", ".", "_q_padding_mask", ",", "self", ".", "_r_padding_mask", ",", "cell", ",", "initial_state_attention", "=", "(", "hps", ".", "mode", "==", "\"decode\"", ")", ",", "pointer_gen", "=", "hps", ".", "pointer_gen", ",", "use_coverage", "=", "hps", ".", "coverage", ",", "q_prev_coverage", "=", "q_prev_coverage", ",", "r_prev_coverage", "=", "r_prev_coverage", ")", "\n", "\n", "return", "outputs", ",", "output_states", ",", "out_state", ",", "q_attn_dists", ",", "r_attn_dists", ",", "p_gens", ",", "q_coverage", ",", "r_coverage", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.model.SummarizationModel._calc_final_dist": [[164, 208], ["tensorflow.compat.v1.variable_scope", "tensorflow.zeros", "tensorflow.layers.flatten", "tensorflow.range", "tensorflow.expand_dims", "tensorflow.tile", "tensorflow.tile", "tensorflow.stack", "tensorflow.stack", "model.SummarizationModel._vocab.size", "tensorflow.concat", "tensorflow.shape", "tensorflow.scatter_nd", "tensorflow.scatter_nd", "zip", "zip", "zip", "tensorflow.shape", "tensorflow.shape", "zip"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.size"], ["", "def", "_calc_final_dist", "(", "self", ",", "vocab_dists", ",", "q_attn_dists", ",", "r_attn_dists", ")", ":", "\n", "    ", "\"\"\"Calculate the final distribution, for the pointer-generator model\n\n    Args:\n      vocab_dists: The vocabulary distributions. List length max_dec_steps of (batch_size, vsize) arrays. The words are in the order they appear in the vocabulary file.\n      attn_dists: The attention distributions. List length max_dec_steps of (batch_size, attn_len) arrays\n\n    Returns:\n      final_dists: The final distributions. List length max_dec_steps of (batch_size, extended_vsize) arrays.\n    \"\"\"", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "'final_distribution'", ")", ":", "\n", "# Multiply vocab dists by p_gen and attention dists by (1-p_gen)", "\n", "      ", "vocab_dists", "=", "[", "p_gen", "[", "0", "]", "*", "dist", "for", "(", "p_gen", ",", "dist", ")", "in", "zip", "(", "self", ".", "p_gens", ",", "vocab_dists", ")", "]", "\n", "q_attn_dists", "=", "[", "p_gen", "[", "1", "]", "*", "dist", "for", "(", "p_gen", ",", "dist", ")", "in", "zip", "(", "self", ".", "p_gens", ",", "q_attn_dists", ")", "]", "\n", "r_attn_dists", "=", "[", "p_gen", "[", "2", "]", "*", "dist", "for", "(", "p_gen", ",", "dist", ")", "in", "zip", "(", "self", ".", "p_gens", ",", "r_attn_dists", ")", "]", "\n", "\n", "# Concatenate some zeros to each vocabulary dist, to hold the probabilities for in-article OOV words", "\n", "extended_vsize", "=", "self", ".", "_vocab", ".", "size", "(", ")", "+", "self", ".", "_max_oovs", "# the maximum (over the batch) size of the extended vocabulary", "\n", "extra_zeros", "=", "tf", ".", "zeros", "(", "(", "self", ".", "_hps", ".", "batch_size", ",", "self", ".", "_max_oovs", ")", ")", "\n", "vocab_dists_extended", "=", "[", "tf", ".", "concat", "(", "axis", "=", "1", ",", "values", "=", "[", "dist", ",", "extra_zeros", "]", ")", "for", "dist", "in", "vocab_dists", "]", "# list length max_dec_steps of shape (batch_size, extended_vsize)", "\n", "\n", "# Project the values in the attention distributions onto the appropriate entries in the final distributions", "\n", "# This means that if a_i = 0.1 and the ith encoder word is w, and w has index 500 in the vocabulary, then we add 0.1 onto the 500th entry of the final distribution", "\n", "# This is done for each decoder timestep.", "\n", "# This is fiddly; we use tf.scatter_nd to do the projection", "\n", "r_batch_extend_vocab", "=", "tf", ".", "layers", ".", "flatten", "(", "self", ".", "_r_batch_extend_vocab", ")", "\n", "batch_nums", "=", "tf", ".", "range", "(", "0", ",", "limit", "=", "self", ".", "_hps", ".", "batch_size", ")", "# shape (batch_size)", "\n", "batch_nums", "=", "tf", ".", "expand_dims", "(", "batch_nums", ",", "1", ")", "# shape (batch_size, 1)", "\n", "r_attn_len", "=", "tf", ".", "shape", "(", "self", ".", "_r_batch_extend_vocab", ")", "[", "1", "]", "*", "tf", ".", "shape", "(", "self", ".", "_r_batch_extend_vocab", ")", "[", "2", "]", "# number of states we attend over", "\n", "q_attn_len", "=", "tf", ".", "shape", "(", "self", ".", "_q_batch_extend_vocab", ")", "[", "1", "]", "\n", "r_batch_nums", "=", "tf", ".", "tile", "(", "batch_nums", ",", "[", "1", ",", "r_attn_len", "]", ")", "# shape (batch_size, attn_len)", "\n", "q_batch_nums", "=", "tf", ".", "tile", "(", "batch_nums", ",", "[", "1", ",", "q_attn_len", "]", ")", "\n", "r_indices", "=", "tf", ".", "stack", "(", "(", "r_batch_nums", ",", "r_batch_extend_vocab", ")", ",", "axis", "=", "2", ")", "# shape (batch_size, enc_t, 2)", "\n", "q_indices", "=", "tf", ".", "stack", "(", "(", "q_batch_nums", ",", "self", ".", "_q_batch_extend_vocab", ")", ",", "axis", "=", "2", ")", "\n", "shape", "=", "[", "self", ".", "_hps", ".", "batch_size", ",", "extended_vsize", "]", "\n", "r_attn_dists_projected", "=", "[", "tf", ".", "scatter_nd", "(", "r_indices", ",", "copy_dist", ",", "shape", ")", "for", "copy_dist", "in", "r_attn_dists", "]", "# list length max_dec_steps (batch_size, extended_vsize)", "\n", "q_attn_dists_projected", "=", "[", "tf", ".", "scatter_nd", "(", "q_indices", ",", "copy_dist", ",", "shape", ")", "for", "copy_dist", "in", "q_attn_dists", "]", "\n", "\n", "# Add the vocab distributions and the copy distributions together to get the final distributions", "\n", "# final_dists is a list length max_dec_steps; each entry is a tensor shape (batch_size, extended_vsize) giving the final distribution for that decoder timestep", "\n", "# Note that for decoder timesteps and examples corresponding to a [PAD] token, this is junk - ignore.", "\n", "final_dists", "=", "[", "vocab_dist", "+", "r_copy_dist", "+", "q_copy_dist", "for", "(", "vocab_dist", ",", "r_copy_dist", ",", "q_copy_dist", ")", "in", "zip", "(", "vocab_dists_extended", ",", "r_attn_dists_projected", ",", "q_attn_dists_projected", ")", "]", "\n", "\n", "return", "final_dists", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.model.SummarizationModel._add_emb_vis": [[209, 222], ["os.path.join", "os.path.join", "model.SummarizationModel._vocab.write_metadata", "tensorflow.summary.FileWriter", "tensorflow.contrib.tensorboard.plugins.projector.ProjectorConfig", "tensorflow.contrib.tensorboard.plugins.projector.ProjectorConfig.embeddings.add", "tensorflow.contrib.tensorboard.plugins.projector.visualize_embeddings"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.write_metadata"], ["", "", "def", "_add_emb_vis", "(", "self", ",", "embedding_var", ")", ":", "\n", "    ", "\"\"\"Do setup so that we can view word embedding visualization in Tensorboard, as described here:\n    https://www.tensorflow.org/get_started/embedding_viz\n    Make the vocab metadata file, then make the projector config file pointing to it.\"\"\"", "\n", "train_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "log_root", ",", "\"train\"", ")", "\n", "vocab_metadata_path", "=", "os", ".", "path", ".", "join", "(", "train_dir", ",", "\"vocab_metadata.tsv\"", ")", "\n", "self", ".", "_vocab", ".", "write_metadata", "(", "vocab_metadata_path", ")", "# write metadata file", "\n", "summary_writer", "=", "tf", ".", "summary", ".", "FileWriter", "(", "train_dir", ")", "\n", "config", "=", "projector", ".", "ProjectorConfig", "(", ")", "\n", "embedding", "=", "config", ".", "embeddings", ".", "add", "(", ")", "\n", "embedding", ".", "tensor_name", "=", "embedding_var", ".", "name", "\n", "embedding", ".", "metadata_path", "=", "vocab_metadata_path", "\n", "projector", ".", "visualize_embeddings", "(", "summary_writer", ",", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.model.SummarizationModel._co_attention": [[224, 267], ["int", "int", "tensorflow.get_variable", "tensorflow.expand_dims", "tensorflow.tile", "tensorflow.expand_dims", "tensorflow.tile", "tensorflow.reshape", "tensorflow.einsum", "tensorflow.matmul", "tensorflow.nn.softmax", "tensorflow.reshape", "tensorflow.reduce_sum", "tensorflow.expand_dims", "tensorflow.nn.softmax", "tensorflow.reshape", "tensorflow.reduce_sum", "tensorflow.expand_dims", "tensorflow.multiply", "tensorflow.multiply", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reduce_mean", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.concat", "tensorflow.reshape", "tensorflow.transpose", "tensorflow.reduce_max", "tensorflow.reshape", "tensorflow.reduce_max", "tensorflow.reshape", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reshape.get_shape", "reviews.get_shape", "tensorflow.contrib.layers.xavier_initializer"], "methods", ["None"], ["", "def", "_co_attention", "(", "self", ",", "question", ",", "reviews", ",", "batch_size", ",", "num", ")", ":", "\n", "    ", "dim1", "=", "int", "(", "question", ".", "get_shape", "(", ")", "[", "2", "]", ")", "\n", "dim2", "=", "int", "(", "reviews", ".", "get_shape", "(", ")", "[", "2", "]", ")", "\n", "U", "=", "tf", ".", "get_variable", "(", "\n", "\"U\"", ",", "\n", "shape", "=", "[", "dim1", ",", "dim2", "]", ",", "\n", "initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer", "(", ")", ")", "\n", "\n", "question", "=", "tf", ".", "expand_dims", "(", "question", ",", "1", ")", "\n", "question", "=", "tf", ".", "tile", "(", "question", ",", "(", "1", ",", "num", ",", "1", ",", "1", ")", ")", "\n", "q_padding_mask", "=", "tf", ".", "expand_dims", "(", "self", ".", "_q_padding_mask", ",", "1", ")", "\n", "q_padding_mask", "=", "tf", ".", "tile", "(", "q_padding_mask", ",", "(", "1", ",", "num", ",", "1", ")", ")", "\n", "\n", "question", "=", "tf", ".", "reshape", "(", "question", ",", "[", "batch_size", "*", "num", ",", "-", "1", ",", "dim1", "]", ")", "\n", "transform_left", "=", "tf", ".", "einsum", "(", "'ijk,kl->ijl'", ",", "question", ",", "U", ")", "\n", "att_mat", "=", "tf", ".", "matmul", "(", "transform_left", ",", "tf", ".", "transpose", "(", "reviews", ",", "[", "0", ",", "2", ",", "1", "]", ")", ")", "\n", "\n", "r_attn", "=", "tf", ".", "nn", ".", "softmax", "(", "tf", ".", "reduce_max", "(", "att_mat", ",", "axis", "=", "1", ")", ")", "# (batch_size*review_num, r_len)", "\n", "r_attn", "*=", "tf", ".", "reshape", "(", "self", ".", "_r_padding_mask", ",", "[", "batch_size", "*", "num", ",", "-", "1", "]", ")", "\n", "masked_sums", "=", "tf", ".", "reduce_sum", "(", "r_attn", ",", "axis", "=", "1", ")", "# (batch_size*review_num)", "\n", "r_attn", "=", "r_attn", "/", "tf", ".", "reshape", "(", "masked_sums", ",", "[", "-", "1", ",", "1", "]", ")", "# re-normalize", "\n", "r_attn", "=", "tf", ".", "expand_dims", "(", "r_attn", ",", "-", "1", ",", "name", "=", "'review_attention'", ")", "\n", "\n", "q_attn", "=", "tf", ".", "nn", ".", "softmax", "(", "tf", ".", "reduce_max", "(", "att_mat", ",", "axis", "=", "2", ")", ")", "# (batch_size*review_num, q_len)", "\n", "q_attn", "*=", "tf", ".", "reshape", "(", "q_padding_mask", ",", "[", "batch_size", "*", "num", ",", "-", "1", "]", ")", "\n", "masked_sums", "=", "tf", ".", "reduce_sum", "(", "q_attn", ",", "axis", "=", "1", ")", "# (batch_size*review_num)", "\n", "q_attn", "=", "q_attn", "/", "tf", ".", "reshape", "(", "masked_sums", ",", "[", "-", "1", ",", "1", "]", ")", "# re-normalize", "\n", "q_attn", "=", "tf", ".", "expand_dims", "(", "q_attn", ",", "-", "1", ",", "name", "=", "'question_attention'", ")", "\n", "\n", "dec_review_inputs", "=", "tf", ".", "multiply", "(", "reviews", ",", "r_attn", ")", "# (batch_size * review_num, r_len, dim)", "\n", "dec_question_inputs", "=", "tf", ".", "multiply", "(", "question", ",", "q_attn", ")", "# (batch_size * review_num, q_len, dim)", "\n", "att_review_outputs", "=", "tf", ".", "reshape", "(", "tf", ".", "reduce_sum", "(", "dec_review_inputs", ",", "1", ")", ",", "[", "-", "1", ",", "dim2", "]", ")", "\n", "att_question_outputs", "=", "tf", ".", "reshape", "(", "tf", ".", "reduce_sum", "(", "dec_question_inputs", ",", "1", ")", ",", "[", "-", "1", ",", "dim1", "]", ")", "\n", "\n", "dec_question_inputs", "=", "tf", ".", "reshape", "(", "dec_question_inputs", ",", "[", "batch_size", ",", "num", ",", "-", "1", ",", "dim1", "]", ")", "\n", "dec_question_inputs", "=", "tf", ".", "reduce_mean", "(", "dec_question_inputs", ",", "1", ")", "# (batch_size, q_len, dim)", "\n", "dec_review_inputs", "=", "tf", ".", "reshape", "(", "dec_review_inputs", ",", "[", "batch_size", ",", "num", ",", "-", "1", ",", "dim2", "]", ")", "\n", "dec_review_inputs", "=", "tf", ".", "reshape", "(", "dec_review_inputs", ",", "[", "batch_size", ",", "-", "1", ",", "dim2", "]", ")", "# (batch_size, review_num * r_len, dim)", "\n", "hidden_output", "=", "tf", ".", "concat", "(", "[", "att_question_outputs", ",", "att_review_outputs", "]", ",", "1", ")", "# (batch_size * review_num, dim * 2)", "\n", "#hidden_output = att_review_outputs  # (batch_size * review_num, dim)", "\n", "hidden_output", "=", "tf", ".", "reshape", "(", "hidden_output", ",", "[", "batch_size", ",", "num", ",", "-", "1", "]", ")", "\n", "\n", "return", "hidden_output", ",", "dec_question_inputs", ",", "dec_review_inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.model.SummarizationModel._overlap": [[268, 285], ["tensorflow.expand_dims", "tensorflow.tile", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.matmul", "tensorflow.expand_dims", "tensorflow.matmul", "tensorflow.expand_dims", "tensorflow.concat", "tensorflow.concat", "tensorflow.transpose", "tensorflow.reduce_max", "tensorflow.transpose", "tensorflow.reduce_max"], "methods", ["None"], ["", "def", "_overlap", "(", "self", ",", "q_embed", ",", "r_embed", ",", "batch_size", ",", "num", ",", "emb_dim", ")", ":", "\n", "    ", "q_embed", "=", "tf", ".", "expand_dims", "(", "q_embed", ",", "1", ")", "\n", "q_embed", "=", "tf", ".", "tile", "(", "q_embed", ",", "(", "1", ",", "num", ",", "1", ",", "1", ")", ")", "\n", "q_embed", "=", "tf", ".", "reshape", "(", "q_embed", ",", "(", "batch_size", "*", "num", ",", "-", "1", ",", "emb_dim", ")", ")", "\n", "r_embed", "=", "tf", ".", "reshape", "(", "r_embed", ",", "(", "batch_size", "*", "num", ",", "-", "1", ",", "emb_dim", ")", ")", "\n", "\n", "overlap1", "=", "tf", ".", "matmul", "(", "q_embed", ",", "tf", ".", "transpose", "(", "r_embed", ",", "[", "0", ",", "2", ",", "1", "]", ")", ")", "\n", "overlap1", "=", "tf", ".", "expand_dims", "(", "tf", ".", "reduce_max", "(", "overlap1", ",", "axis", "=", "2", ")", ",", "-", "1", ")", "\n", "\n", "overlap2", "=", "tf", ".", "matmul", "(", "r_embed", ",", "tf", ".", "transpose", "(", "q_embed", ",", "[", "0", ",", "2", ",", "1", "]", ")", ")", "\n", "overlap2", "=", "tf", ".", "expand_dims", "(", "tf", ".", "reduce_max", "(", "overlap2", ",", "axis", "=", "2", ")", ",", "-", "1", ")", "\n", "q_embed", "=", "tf", ".", "concat", "(", "[", "q_embed", ",", "overlap1", "]", ",", "2", ")", "\n", "r_embed", "=", "tf", ".", "concat", "(", "[", "r_embed", ",", "overlap2", "]", ",", "2", ")", "\n", "\n", "#q_embed = tf.reshape(q_embed, (batch_size, num, -1, emb_dim + 1))", "\n", "#r_embed = tf.reshape(r_embed, (batch_size, num, -1, emb_dim + 1))", "\n", "return", "q_embed", ",", "r_embed", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.model.SummarizationModel._self_matching": [[286, 319], ["tensorflow.get_variable", "tensorflow.tanh", "tensorflow.get_variable", "tensorflow.nn.softmax", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.xw_plus_b", "tensorflow.nn.softmax", "sentiment_input.get_shape", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.transpose", "tensorflow.reshape", "tensorflow.contrib.layers.xavier_initializer", "tensorflow.contrib.layers.xavier_initializer", "tensorflow.matmul", "tensorflow.contrib.layers.xavier_initializer", "tensorflow.constant_initializer", "tensorflow.reshape.get_shape"], "methods", ["None"], ["", "def", "_self_matching", "(", "self", ",", "sentiment_input", ",", "batch_size", ",", "hidden_size", "=", "256", ",", "dropout_keep_prob", "=", "0.8", ")", ":", "\n", "    ", "dim1", "=", "sentiment_input", ".", "get_shape", "(", ")", "[", "-", "1", "]", "\n", "W", "=", "tf", ".", "get_variable", "(", "\n", "\"W\"", ",", "\n", "shape", "=", "[", "dim1", ",", "hidden_size", "]", ",", "\n", "initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer", "(", ")", ")", "\n", "U", "=", "tf", ".", "tanh", "(", "tf", ".", "matmul", "(", "sentiment_input", ",", "W", ")", ")", "\n", "w", "=", "tf", ".", "get_variable", "(", "\n", "\"w\"", ",", "\n", "shape", "=", "[", "hidden_size", ",", "1", "]", ",", "\n", "initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer", "(", ")", ")", "\n", "review_attention", "=", "tf", ".", "nn", ".", "softmax", "(", "tf", ".", "reshape", "(", "tf", ".", "matmul", "(", "U", ",", "w", ")", ",", "[", "batch_size", ",", "-", "1", "]", ")", ")", "\n", "decode_in_state", "=", "tf", ".", "matmul", "(", "tf", ".", "transpose", "(", "sentiment_input", ",", "[", "0", ",", "2", ",", "1", "]", ")", ",", "tf", ".", "reshape", "(", "review_attention", ",", "[", "batch_size", ",", "-", "1", ",", "1", "]", ")", ")", "\n", "decode_in_state", "=", "tf", ".", "reshape", "(", "decode_in_state", ",", "[", "batch_size", ",", "-", "1", "]", ")", "\n", "'''\n    W_h = tf.get_variable(\n      \"W_hidden\",\n      shape=[decode_in_state.get_shape()[1], hidden_size],\n      initializer=tf.contrib.layers.xavier_initializer())\n    b_h = tf.get_variable(\"b_hidden\", [hidden_size], initializer=tf.constant_initializer(0.1))\n    hidden_output = tf.nn.relu(tf.nn.xw_plus_b(decode_in_state, W_h, b_h, name=\"hidden_output\"))\n\n    h_drop = tf.nn.dropout(hidden_output, dropout_keep_prob, name=\"hidden_output_drop\")\n    '''", "\n", "W_o", "=", "tf", ".", "get_variable", "(", "\n", "\"W_output\"", ",", "\n", "shape", "=", "[", "decode_in_state", ".", "get_shape", "(", ")", "[", "1", "]", ",", "3", "]", ",", "\n", "initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer", "(", ")", ")", "\n", "b_o", "=", "tf", ".", "get_variable", "(", "\"b_output\"", ",", "[", "3", "]", ",", "initializer", "=", "tf", ".", "constant_initializer", "(", "0.1", ")", ")", "\n", "prob", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "decode_in_state", ",", "W_o", ",", "b_o", ")", "\n", "soft_prob", "=", "tf", ".", "nn", ".", "softmax", "(", "prob", ",", "name", "=", "'distance'", ")", "\n", "\n", "return", "prob", ",", "soft_prob", ",", "review_attention", ",", "decode_in_state", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.model.SummarizationModel._combine_attn": [[321, 331], ["tensorflow.expand_dims", "tensorflow.reshape", "tensorflow.multiply", "tensorflow.reshape", "tensorflow.divide", "combined_attn_dists.append", "tensorflow.expand_dims", "tensorflow.reduce_sum"], "methods", ["None"], ["", "def", "_combine_attn", "(", "self", ",", "attn_dists", ",", "review_attention", ",", "batch_size", ",", "num", ")", ":", "\n", "    ", "combined_attn_dists", "=", "[", "]", "\n", "review_attention", "=", "tf", ".", "expand_dims", "(", "review_attention", ",", "-", "1", ")", "\n", "for", "attn_dist", "in", "attn_dists", ":", "\n", "      ", "attn_dist", "=", "tf", ".", "reshape", "(", "attn_dist", ",", "[", "batch_size", ",", "num", ",", "-", "1", "]", ")", "\n", "attn_dist", "=", "tf", ".", "multiply", "(", "attn_dist", ",", "review_attention", ")", "\n", "attn_norm", "=", "tf", ".", "reshape", "(", "attn_dist", ",", "[", "batch_size", ",", "-", "1", "]", ")", "\n", "attn_dist", "=", "tf", ".", "divide", "(", "attn_norm", ",", "tf", ".", "expand_dims", "(", "tf", ".", "reduce_sum", "(", "attn_norm", ",", "1", ")", ",", "-", "1", ")", ")", "\n", "combined_attn_dists", ".", "append", "(", "attn_dist", ")", "\n", "", "return", "combined_attn_dists", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.model.SummarizationModel._add_seq2seq": [[333, 452], ["model.SummarizationModel._vocab.size", "tensorflow.compat.v1.variable_scope", "tensorflow.random_uniform_initializer", "tensorflow.truncated_normal_initializer", "tensorflow.reshape", "tensorflow.reshape", "model.SummarizationModel._add_encoder", "model.SummarizationModel._add_encoder", "model.SummarizationModel._reduce_states", "tensorflow.nn.top_k", "tensorflow.log", "tensorflow.compat.v1.variable_scope", "tensorflow.nn.embedding_lookup", "tensorflow.nn.embedding_lookup", "tensorflow.compat.v1.variable_scope", "model.SummarizationModel._co_attention", "tensorflow.compat.v1.variable_scope", "tensorflow.one_hot", "tensorflow.concat", "model.SummarizationModel._self_matching", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.reduce_mean", "tensorflow.compat.v1.variable_scope", "model.SummarizationModel._add_decoder", "tensorflow.compat.v1.variable_scope", "tensorflow.cast", "tensorflow.equal", "tensorflow.cast", "tensorflow.reduce_mean", "len", "data.get_init_embeddings", "tensorflow.get_variable", "tensorflow.get_variable", "model.SummarizationModel._add_emb_vis", "tensorflow.nn.embedding_lookup", "tensorflow.argmax", "tensorflow.compat.v1.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "enumerate", "model.SummarizationModel._calc_final_dist", "tensorflow.compat.v1.variable_scope", "tensorflow.summary.scalar", "tensorflow.unstack", "vocab_scores.append", "tensorflow.nn.softmax", "tensorflow.range", "enumerate", "model._mask_and_avg", "tensorflow.contrib.seq2seq.sequence_loss", "tensorflow.summary.scalar", "tensorflow.get_variable_scope().reuse_variables", "tensorflow.nn.xw_plus_b", "tensorflow.stack", "tensorflow.gather_nd", "loss_per_step.append", "tensorflow.stack", "tensorflow.variable_scope", "model._coverage_loss", "tensorflow.summary.scalar", "tensorflow.log", "tensorflow.get_variable_scope"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.size", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._add_encoder", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._add_encoder", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._reduce_states", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._co_attention", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._self_matching", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._add_decoder", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.get_init_embeddings", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._add_emb_vis", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._calc_final_dist", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model._mask_and_avg", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model._coverage_loss"], ["", "def", "_add_seq2seq", "(", "self", ")", ":", "\n", "    ", "\"\"\"Add the whole sequence-to-sequence model to the graph.\"\"\"", "\n", "hps", "=", "self", ".", "_hps", "\n", "vsize", "=", "self", ".", "_vocab", ".", "size", "(", ")", "# size of the vocabulary", "\n", "vocab", "=", "self", ".", "_vocab", "\n", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "'seq2seq'", ")", ":", "\n", "# Some initializers", "\n", "      ", "self", ".", "rand_unif_init", "=", "tf", ".", "random_uniform_initializer", "(", "-", "hps", ".", "rand_unif_init_mag", ",", "hps", ".", "rand_unif_init_mag", ",", "seed", "=", "123", ")", "\n", "self", ".", "trunc_norm_init", "=", "tf", ".", "truncated_normal_initializer", "(", "stddev", "=", "hps", ".", "trunc_norm_init_std", ")", "\n", "\n", "# Add embedding matrix (shared by the encoder and decoder inputs)", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "'embedding'", ")", ":", "\n", "        ", "if", "hps", ".", "glove", ":", "\n", "          ", "init_embeddings", "=", "data", ".", "get_init_embeddings", "(", "vocab", ".", "_id_to_word", ",", "hps", ".", "emb_dim", ")", "\n", "embedding", "=", "tf", ".", "get_variable", "(", "\"embedding\"", ",", "initializer", "=", "init_embeddings", ")", "\n", "", "else", ":", "\n", "          ", "embedding", "=", "tf", ".", "get_variable", "(", "'embedding'", ",", "[", "vsize", ",", "hps", ".", "emb_dim", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "initializer", "=", "self", ".", "trunc_norm_init", ")", "\n", "", "if", "hps", ".", "mode", "==", "\"train\"", ":", "self", ".", "_add_emb_vis", "(", "embedding", ")", "# add to tensorboard", "\n", "emb_enc_inputs", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "embedding", ",", "self", ".", "_r_batch", ")", "# tensor with shape (batch_size, review_num, max_enc_steps, emb_size)", "\n", "emb_dec_inputs", "=", "[", "tf", ".", "nn", ".", "embedding_lookup", "(", "embedding", ",", "x", ")", "for", "x", "in", "tf", ".", "unstack", "(", "self", ".", "_dec_batch", ",", "axis", "=", "1", ")", "]", "# list length max_dec_steps containing shape (batch_size, emb_size)", "\n", "emb_q_inputs", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "embedding", ",", "self", ".", "_q_batch", ")", "# tensor with shape (batch_size, max_enc_steps, emb_size)", "\n", "#emb_q_inputs, emb_enc_inputs = self._overlap(emb_q_inputs, emb_enc_inputs, hps.batch_size, hps.review_num, hps.emb_dim)", "\n", "\n", "\n", "# Add the encoder.", "\n", "", "r_lens", "=", "tf", ".", "reshape", "(", "self", ".", "_r_lens", ",", "[", "-", "1", "]", ")", "\n", "emb_enc_inputs", "=", "tf", ".", "reshape", "(", "emb_enc_inputs", ",", "[", "hps", ".", "batch_size", "*", "hps", ".", "review_num", ",", "-", "1", ",", "hps", ".", "emb_dim", "]", ")", "\n", "enc_outputs", ",", "fw_st", ",", "bw_st", "=", "self", ".", "_add_encoder", "(", "emb_enc_inputs", ",", "r_lens", ",", "\"/review\"", ")", "# (batch_size*review_num, enc_lens, emb_size)", "\n", "q_outputs", ",", "q_fw_st", ",", "q_bw_st", "=", "self", ".", "_add_encoder", "(", "emb_q_inputs", ",", "self", ".", "_q_lens", ",", "\"/question\"", ")", "# (batch_size, q_lens, emb_size)", "\n", "\n", "# Co-attention layer", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "'co-attention'", ")", ":", "\n", "        ", "hidden_output", ",", "self", ".", "dec_question_inputs", ",", "self", ".", "dec_review_inputs", "=", "self", ".", "_co_attention", "(", "q_outputs", ",", "enc_outputs", ",", "hps", ".", "batch_size", ",", "hps", ".", "review_num", ")", "\n", "\n", "# Sentiment layer", "\n", "", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "'sentiment_layer'", ")", ":", "\n", "        ", "rating_batch", "=", "tf", ".", "one_hot", "(", "self", ".", "_rating_batch", ",", "depth", "=", "5", ",", "axis", "=", "-", "1", ")", "\n", "sentiment_input", "=", "tf", ".", "concat", "(", "[", "hidden_output", ",", "rating_batch", "]", ",", "2", ")", "\n", "self", ".", "y_prob", ",", "self", ".", "y_pred", ",", "self", ".", "review_attention", ",", "decode_in_state", "=", "self", ".", "_self_matching", "(", "sentiment_input", ",", "hps", ".", "batch_size", ")", "\n", "# Sentiment loss", "\n", "losses", "=", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "logits", "=", "self", ".", "y_prob", ",", "labels", "=", "self", ".", "_y_target_batch", ")", "\n", "self", ".", "_loss_sa", "=", "tf", ".", "reduce_mean", "(", "losses", ")", "\n", "\n", "# Our encoder is bidirectional and our decoder is unidirectional so we need to reduce the final encoder hidden state to the right size to be the initial decoder hidden state", "\n", "", "self", ".", "_dec_in_state", "=", "self", ".", "_reduce_states", "(", "q_fw_st", ",", "q_bw_st", ",", "decode_in_state", ")", "\n", "\n", "# Add the decoder.", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "'decoder'", ")", ":", "\n", "        ", "decoder_outputs", ",", "output_states", ",", "self", ".", "_dec_out_state", ",", "self", ".", "q_attn_dists", ",", "self", ".", "r_attn_dists", ",", "self", ".", "p_gens", ",", "self", ".", "q_coverage", ",", "self", ".", "r_coverage", "=", "self", ".", "_add_decoder", "(", "emb_dec_inputs", ")", "\n", "\n", "# Accuracy", "\n", "", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "\"accuracy\"", ")", ":", "\n", "        ", "self", ".", "predictions", "=", "tf", ".", "cast", "(", "tf", ".", "argmax", "(", "self", ".", "y_pred", ",", "1", ",", "name", "=", "\"predictions\"", ")", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "correct_predictions", "=", "tf", ".", "equal", "(", "self", ".", "predictions", ",", "self", ".", "_y_target_batch", ")", "\n", "self", ".", "batch_accuracy", "=", "tf", ".", "cast", "(", "correct_predictions", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "self", ".", "accuracy", "=", "tf", ".", "reduce_mean", "(", "self", ".", "batch_accuracy", ",", "name", "=", "\"accuracy\"", ")", "\n", "\n", "", "if", "hps", ".", "mode", "in", "[", "'train'", ",", "'decode'", "]", ":", "\n", "# Add the output projection to obtain the vocabulary distribution", "\n", "        ", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "'output_projection'", ")", ":", "\n", "          ", "w", "=", "tf", ".", "get_variable", "(", "'w'", ",", "[", "hps", ".", "hidden_dim", ",", "vsize", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "initializer", "=", "self", ".", "trunc_norm_init", ")", "\n", "v", "=", "tf", ".", "get_variable", "(", "'v'", ",", "[", "vsize", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "initializer", "=", "self", ".", "trunc_norm_init", ")", "\n", "vocab_scores", "=", "[", "]", "# vocab_scores is the vocabulary distribution before applying softmax. Each entry on the list corresponds to one decoder step", "\n", "for", "i", ",", "output", "in", "enumerate", "(", "decoder_outputs", ")", ":", "\n", "            ", "if", "i", ">", "0", ":", "\n", "              ", "tf", ".", "get_variable_scope", "(", ")", ".", "reuse_variables", "(", ")", "\n", "", "vocab_scores", ".", "append", "(", "tf", ".", "nn", ".", "xw_plus_b", "(", "output", ",", "w", ",", "v", ")", ")", "# apply the linear layer", "\n", "\n", "", "vocab_dists", "=", "[", "tf", ".", "nn", ".", "softmax", "(", "s", ")", "for", "s", "in", "vocab_scores", "]", "# The vocabulary distributions. List length max_dec_steps of (batch_size, vsize) arrays. The words are in the order they appear in the vocabulary file.", "\n", "\n", "\n", "# For pointer-generator model, calc final distribution from copy distribution and vocabulary distribution", "\n", "# Combined Attention", "\n", "#self.r_attn_dists = self._combine_attn(self.r_attn_dists, review_attention, hps.batch_size, hps.review_num)", "\n", "", "if", "FLAGS", ".", "pointer_gen", ":", "\n", "          ", "final_dists", "=", "self", ".", "_calc_final_dist", "(", "vocab_dists", ",", "self", ".", "q_attn_dists", ",", "self", ".", "r_attn_dists", ")", "\n", "", "else", ":", "# final distribution is just vocabulary distribution", "\n", "          ", "final_dists", "=", "vocab_dists", "\n", "\n", "", "", "if", "hps", ".", "mode", "in", "[", "'train'", "]", ":", "\n", "# Calculate the loss", "\n", "        ", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "'loss'", ")", ":", "\n", "          ", "if", "FLAGS", ".", "pointer_gen", ":", "\n", "# Calculate the loss per step", "\n", "# This is fiddly; we use tf.gather_nd to pick out the probabilities of the gold target words", "\n", "            ", "loss_per_step", "=", "[", "]", "# will be list length max_dec_steps containing shape (batch_size)", "\n", "batch_nums", "=", "tf", ".", "range", "(", "0", ",", "limit", "=", "hps", ".", "batch_size", ")", "# shape (batch_size)", "\n", "for", "dec_step", ",", "dist", "in", "enumerate", "(", "final_dists", ")", ":", "\n", "              ", "targets", "=", "self", ".", "_target_batch", "[", ":", ",", "dec_step", "]", "# The indices of the target words. shape (batch_size)", "\n", "indices", "=", "tf", ".", "stack", "(", "(", "batch_nums", ",", "targets", ")", ",", "axis", "=", "1", ")", "# shape (batch_size, 2)", "\n", "gold_probs", "=", "tf", ".", "gather_nd", "(", "dist", ",", "indices", ")", "# shape (batch_size). prob of correct words on this step", "\n", "losses", "=", "-", "tf", ".", "log", "(", "gold_probs", ")", "\n", "loss_per_step", ".", "append", "(", "losses", ")", "\n", "\n", "# Apply dec_padding_mask and get loss", "\n", "", "self", ".", "_loss_sum", "=", "_mask_and_avg", "(", "loss_per_step", ",", "self", ".", "_dec_padding_mask", ")", "\n", "\n", "", "else", ":", "# baseline model", "\n", "            ", "self", ".", "_loss_sum", "=", "tf", ".", "contrib", ".", "seq2seq", ".", "sequence_loss", "(", "tf", ".", "stack", "(", "vocab_scores", ",", "axis", "=", "1", ")", ",", "self", ".", "_target_batch", ",", "self", ".", "_dec_padding_mask", ")", "# this applies softmax internally", "\n", "\n", "", "self", ".", "_loss", "=", "hps", ".", "sa_loss_wt", "*", "self", ".", "_loss_sa", "+", "self", ".", "_loss_sum", "\n", "tf", ".", "summary", ".", "scalar", "(", "'loss'", ",", "self", ".", "_loss", ")", "\n", "\n", "# Calculate coverage loss from the attention distributions", "\n", "if", "hps", ".", "coverage", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "'coverage_loss'", ")", ":", "\n", "              ", "self", ".", "_q_coverage_loss", ",", "self", ".", "_r_coverage_loss", "=", "_coverage_loss", "(", "self", ".", "q_attn_dists", ",", "self", ".", "r_attn_dists", ",", "self", ".", "p_gens", ",", "self", ".", "_dec_padding_mask", ")", "\n", "self", ".", "_coverage_loss", "=", "self", ".", "_q_coverage_loss", "+", "self", ".", "_r_coverage_loss", "\n", "tf", ".", "summary", ".", "scalar", "(", "'coverage_loss'", ",", "self", ".", "_coverage_loss", ")", "\n", "", "self", ".", "_total_loss", "=", "self", ".", "_loss", "+", "hps", ".", "cov_loss_wt", "*", "self", ".", "_coverage_loss", "\n", "tf", ".", "summary", ".", "scalar", "(", "'total_loss'", ",", "self", ".", "_total_loss", ")", "\n", "\n", "", "", "", "", "if", "hps", ".", "mode", "==", "\"decode\"", ":", "\n", "# We run decode beam search mode one decoder step at a time", "\n", "      ", "assert", "len", "(", "final_dists", ")", "==", "1", "# final_dists is a singleton list containing shape (batch_size, extended_vsize)", "\n", "final_dists", "=", "final_dists", "[", "0", "]", "\n", "topk_probs", ",", "self", ".", "_topk_ids", "=", "tf", ".", "nn", ".", "top_k", "(", "final_dists", ",", "hps", ".", "batch_size", "*", "2", ")", "# take the k largest probs. note batch_size=beam_size in decode mode", "\n", "self", ".", "_topk_log_probs", "=", "tf", ".", "log", "(", "topk_probs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.model.SummarizationModel._add_train_op": [[454, 472], ["tensorflow.trainable_variables", "tensorflow.gradients", "tensorflow.train.AdagradOptimizer", "tensorflow.device", "tensorflow.clip_by_global_norm", "tensorflow.device", "tensorflow.train.AdagradOptimizer.apply_gradients", "zip"], "methods", ["None"], ["", "", "def", "_add_train_op", "(", "self", ")", ":", "\n", "    ", "\"\"\"Sets self._train_op, the op to run for training.\"\"\"", "\n", "# Take gradients of the trainable variables w.r.t. the loss function to minimize", "\n", "loss_to_minimize", "=", "self", ".", "_total_loss", "if", "self", ".", "_hps", ".", "coverage", "else", "self", ".", "_loss", "\n", "tvars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "gradients", "=", "tf", ".", "gradients", "(", "loss_to_minimize", ",", "tvars", ",", "aggregation_method", "=", "tf", ".", "AggregationMethod", ".", "EXPERIMENTAL_TREE", ")", "\n", "\n", "# Clip the gradients", "\n", "with", "tf", ".", "device", "(", "\"/gpu:0\"", ")", ":", "\n", "      ", "grads", ",", "_", "=", "tf", ".", "clip_by_global_norm", "(", "gradients", ",", "self", ".", "_hps", ".", "max_grad_norm", ")", "\n", "\n", "# Add a summary", "\n", "#tf.summary.scalar('global_norm', global_norm)", "\n", "\n", "# Apply adagrad optimizer", "\n", "", "optimizer", "=", "tf", ".", "train", ".", "AdagradOptimizer", "(", "self", ".", "_hps", ".", "lr", ",", "initial_accumulator_value", "=", "self", ".", "_hps", ".", "adagrad_init_acc", ")", "\n", "with", "tf", ".", "device", "(", "\"/gpu:0\"", ")", ":", "\n", "      ", "self", ".", "_train_op", "=", "optimizer", ".", "apply_gradients", "(", "zip", "(", "grads", ",", "tvars", ")", ",", "global_step", "=", "self", ".", "global_step", ",", "name", "=", "'train_step'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.model.SummarizationModel.build_graph": [[474, 487], ["tensorflow.compat.v1.logging.info", "time.time", "model.SummarizationModel._add_placeholders", "tensorflow.Variable", "tensorflow.summary.merge_all", "time.time", "tensorflow.compat.v1.logging.info", "tensorflow.device", "model.SummarizationModel._add_seq2seq", "model.SummarizationModel._add_train_op"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._add_placeholders", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._add_seq2seq", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._add_train_op"], ["", "", "def", "build_graph", "(", "self", ")", ":", "\n", "    ", "\"\"\"Add the placeholders, model, global step, train_op and summaries to the graph\"\"\"", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "'Building graph...'", ")", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "self", ".", "_add_placeholders", "(", ")", "\n", "with", "tf", ".", "device", "(", "\"/gpu:0\"", ")", ":", "\n", "      ", "self", ".", "_add_seq2seq", "(", ")", "\n", "", "self", ".", "global_step", "=", "tf", ".", "Variable", "(", "0", ",", "name", "=", "'global_step'", ",", "trainable", "=", "False", ")", "\n", "if", "self", ".", "_hps", ".", "mode", "==", "'train'", ":", "\n", "      ", "self", ".", "_add_train_op", "(", ")", "\n", "", "self", ".", "_summaries", "=", "tf", ".", "summary", ".", "merge_all", "(", ")", "\n", "t1", "=", "time", ".", "time", "(", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "'Time to build graph: %i seconds'", ",", "t1", "-", "t0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.model.SummarizationModel.run_train_step": [[488, 504], ["model.SummarizationModel._make_feed_dict", "sess.run"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._make_feed_dict"], ["", "def", "run_train_step", "(", "self", ",", "sess", ",", "batch", ")", ":", "\n", "    ", "\"\"\"Runs one training iteration. Returns a dictionary containing train op, summaries, loss, global_step and (optionally) coverage loss.\"\"\"", "\n", "feed_dict", "=", "self", ".", "_make_feed_dict", "(", "batch", ")", "\n", "to_return", "=", "{", "\n", "'train_op'", ":", "self", ".", "_train_op", ",", "\n", "'summaries'", ":", "self", ".", "_summaries", ",", "\n", "'loss'", ":", "self", ".", "_loss", ",", "\n", "'losses'", ":", "self", ".", "y_prob", ",", "\n", "'global_step'", ":", "self", ".", "global_step", ",", "\n", "'accuracy'", ":", "self", ".", "accuracy", ",", "\n", "}", "\n", "if", "self", ".", "_hps", ".", "coverage", ":", "\n", "      ", "to_return", "[", "'q_coverage'", "]", "=", "self", ".", "_q_coverage_loss", ",", "\n", "to_return", "[", "'r_coverage'", "]", "=", "self", ".", "_r_coverage_loss", ",", "\n", "to_return", "[", "'coverage_loss'", "]", "=", "self", ".", "_coverage_loss", "\n", "", "return", "sess", ".", "run", "(", "to_return", ",", "feed_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.model.SummarizationModel.run_eval_step": [[505, 520], ["model.SummarizationModel._make_feed_dict", "sess.run"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._make_feed_dict"], ["", "def", "run_eval_step", "(", "self", ",", "sess", ",", "batch", ")", ":", "\n", "    ", "\"\"\"Runs one evaluation iteration. Returns a dictionary containing summaries, loss, global_step and (optionally) coverage loss.\"\"\"", "\n", "feed_dict", "=", "self", ".", "_make_feed_dict", "(", "batch", ")", "\n", "to_return", "=", "{", "\n", "'loss'", ":", "self", ".", "_loss_sum", ",", "\n", "'y_prob'", ":", "self", ".", "y_pred", ",", "\n", "'y_pred'", ":", "self", ".", "predictions", ",", "\n", "'y_true'", ":", "self", ".", "_y_target_batch", ",", "\n", "'accuracy'", ":", "self", ".", "accuracy", ",", "\n", "'batch_accuracy'", ":", "self", ".", "batch_accuracy", "\n", "}", "\n", "\n", "if", "self", ".", "_hps", ".", "coverage", ":", "\n", "      ", "to_return", "[", "'coverage_loss'", "]", "=", "self", ".", "_coverage_loss", "\n", "", "return", "sess", ".", "run", "(", "to_return", ",", "feed_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.model.SummarizationModel.run_encoder": [[521, 540], ["model.SummarizationModel._make_feed_dict", "sess.run", "tensorflow.contrib.rnn.LSTMStateTuple"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._make_feed_dict"], ["", "def", "run_encoder", "(", "self", ",", "sess", ",", "batch", ")", ":", "\n", "    ", "\"\"\"For beam search decoding. Run the encoder on the batch and return the encoder states and decoder initial state.\n\n    Args:\n      sess: Tensorflow session.\n      batch: Batch object that is the same example repeated across the batch (for beam search)\n\n    Returns:\n      enc_states: The encoder states. A tensor of shape [batch_size, <=max_enc_steps, 2*hidden_dim].\n      dec_in_state: A LSTMStateTuple of shape ([1,hidden_dim],[1,hidden_dim])\n    \"\"\"", "\n", "feed_dict", "=", "self", ".", "_make_feed_dict", "(", "batch", ",", "just_enc", "=", "True", ")", "# feed the batch into the placeholders", "\n", "(", "dec_in_state", ",", "dec_question_inputs", ",", "dec_review_inputs", ",", "y_pred", ",", "global_step", ")", "=", "sess", ".", "run", "(", "[", "self", ".", "_dec_in_state", ",", "self", ".", "dec_question_inputs", ",", "self", ".", "dec_review_inputs", ",", "self", ".", "y_pred", ",", "self", ".", "global_step", "]", ",", "feed_dict", ")", "# run the encoder", "\n", "\n", "# dec_in_state is LSTMStateTuple shape ([batch_size,hidden_dim],[batch_size,hidden_dim])", "\n", "# Given that the batch is a single example repeated, dec_in_state is identical across the batch so we just take the top row.", "\n", "dec_in_state", "=", "tf", ".", "contrib", ".", "rnn", ".", "LSTMStateTuple", "(", "dec_in_state", ".", "c", "[", "0", "]", ",", "dec_in_state", ".", "h", "[", "0", "]", ")", "\n", "return", "dec_in_state", ",", "dec_question_inputs", ",", "dec_review_inputs", ",", "y_pred", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.model.SummarizationModel.decode_onestep": [[542, 635], ["len", "numpy.concatenate", "numpy.concatenate", "tensorflow.contrib.rnn.LSTMStateTuple", "sess.run", "[].tolist", "[].tolist", "numpy.expand_dims", "numpy.expand_dims", "numpy.transpose", "numpy.stack", "numpy.stack", "tensorflow.contrib.rnn.LSTMStateTuple", "len", "len", "[].tolist", "results[].tolist", "results[].tolist", "numpy.array", "range", "len", "len", "len", "range", "range", "range", "numpy.transpose", "numpy.asarray"], "methods", ["None"], ["", "def", "decode_onestep", "(", "self", ",", "sess", ",", "batch", ",", "latest_tokens", ",", "dec_init_states", ",", "dec_question_inputs", ",", "dec_review_inputs", ",", "q_prev_coverage", ",", "r_prev_coverage", ")", ":", "\n", "    ", "\"\"\"For beam search decoding. Run the decoder for one step.\n\n    Args:\n      sess: Tensorflow session.\n      batch: Batch object containing single example repeated across the batch\n      latest_tokens: Tokens to be fed as input into the decoder for this timestep\n      enc_states: The encoder states.\n      dec_init_states: List of beam_size LSTMStateTuples; the decoder states from the previous timestep\n      prev_coverage: List of np arrays. The coverage vectors from the previous timestep. List of None if not using coverage.\n\n    Returns:\n      ids: top 2k ids. shape [beam_size, 2*beam_size]\n      probs: top 2k log probabilities. shape [beam_size, 2*beam_size]\n      new_states: new states of the decoder. a list length beam_size containing\n        LSTMStateTuples each of shape ([hidden_dim,],[hidden_dim,])\n      attn_dists: List length beam_size containing lists length attn_length.\n      p_gens: Generation probabilities for this step. A list length beam_size. List of None if in baseline mode.\n      new_coverage: Coverage vectors for this step. A list of arrays. List of None if coverage is not turned on.\n    \"\"\"", "\n", "\n", "beam_size", "=", "len", "(", "dec_init_states", ")", "\n", "\n", "# Turn dec_init_states (a list of LSTMStateTuples) into a single LSTMStateTuple for the batch", "\n", "cells", "=", "[", "np", ".", "expand_dims", "(", "state", ".", "c", ",", "axis", "=", "0", ")", "for", "state", "in", "dec_init_states", "]", "\n", "hiddens", "=", "[", "np", ".", "expand_dims", "(", "state", ".", "h", ",", "axis", "=", "0", ")", "for", "state", "in", "dec_init_states", "]", "\n", "new_c", "=", "np", ".", "concatenate", "(", "cells", ",", "axis", "=", "0", ")", "# shape [batch_size,hidden_dim]", "\n", "new_h", "=", "np", ".", "concatenate", "(", "hiddens", ",", "axis", "=", "0", ")", "# shape [batch_size,hidden_dim]", "\n", "new_dec_in_state", "=", "tf", ".", "contrib", ".", "rnn", ".", "LSTMStateTuple", "(", "new_c", ",", "new_h", ")", "\n", "\n", "feed", "=", "{", "\n", "self", ".", "dec_question_inputs", ":", "dec_question_inputs", ",", "\n", "self", ".", "dec_review_inputs", ":", "dec_review_inputs", ",", "\n", "self", ".", "_r_padding_mask", ":", "batch", ".", "r_padding_mask", ",", "\n", "self", ".", "_q_padding_mask", ":", "batch", ".", "q_padding_mask", ",", "\n", "self", ".", "_q_lens", ":", "batch", ".", "q_lens", ",", "\n", "self", ".", "_q_batch", ":", "batch", ".", "q_batch", ",", "\n", "self", ".", "_r_batch", ":", "batch", ".", "r_batch", ",", "\n", "self", ".", "_r_lens", ":", "batch", ".", "r_lens", ",", "\n", "self", ".", "_rating_batch", ":", "batch", ".", "rating_batch", ",", "\n", "self", ".", "_dec_in_state", ":", "new_dec_in_state", ",", "\n", "self", ".", "_dec_batch", ":", "np", ".", "transpose", "(", "np", ".", "array", "(", "[", "latest_tokens", "]", ")", ")", ",", "\n", "}", "\n", "\n", "to_return", "=", "{", "\n", "\"ids\"", ":", "self", ".", "_topk_ids", ",", "\n", "\"probs\"", ":", "self", ".", "_topk_log_probs", ",", "\n", "\"states\"", ":", "self", ".", "_dec_out_state", ",", "\n", "\"q_attn_dists\"", ":", "self", ".", "q_attn_dists", ",", "\n", "\"r_attn_dists\"", ":", "self", ".", "r_attn_dists", ",", "\n", "}", "\n", "\n", "if", "FLAGS", ".", "pointer_gen", ":", "\n", "      ", "feed", "[", "self", ".", "_q_batch_extend_vocab", "]", "=", "batch", ".", "q_batch_extend_vocab", "\n", "feed", "[", "self", ".", "_r_batch_extend_vocab", "]", "=", "batch", ".", "r_batch_extend_vocab", "\n", "feed", "[", "self", ".", "_max_oovs", "]", "=", "batch", ".", "max_oovs", "\n", "to_return", "[", "'p_gens'", "]", "=", "self", ".", "p_gens", "\n", "\n", "", "if", "self", ".", "_hps", ".", "coverage", ":", "\n", "      ", "feed", "[", "self", ".", "q_prev_coverage", "]", "=", "np", ".", "stack", "(", "q_prev_coverage", ",", "axis", "=", "0", ")", "\n", "feed", "[", "self", ".", "r_prev_coverage", "]", "=", "np", ".", "stack", "(", "r_prev_coverage", ",", "axis", "=", "0", ")", "\n", "to_return", "[", "'q_coverage'", "]", "=", "self", ".", "q_coverage", "\n", "to_return", "[", "'r_coverage'", "]", "=", "self", ".", "r_coverage", "\n", "\n", "", "results", "=", "sess", ".", "run", "(", "to_return", ",", "feed_dict", "=", "feed", ")", "# run the decoder step", "\n", "\n", "# Convert results['states'] (a single LSTMStateTuple) into a list of LSTMStateTuple -- one for each hypothesis", "\n", "new_states", "=", "[", "tf", ".", "contrib", ".", "rnn", ".", "LSTMStateTuple", "(", "results", "[", "'states'", "]", ".", "c", "[", "i", ",", ":", "]", ",", "results", "[", "'states'", "]", ".", "h", "[", "i", ",", ":", "]", ")", "for", "i", "in", "range", "(", "beam_size", ")", "]", "\n", "\n", "# Convert singleton list containing a tensor to a list of k arrays", "\n", "assert", "len", "(", "results", "[", "'q_attn_dists'", "]", ")", "==", "1", "\n", "q_attn_dists", "=", "results", "[", "'q_attn_dists'", "]", "[", "0", "]", ".", "tolist", "(", ")", "\n", "assert", "len", "(", "results", "[", "'r_attn_dists'", "]", ")", "==", "1", "\n", "r_attn_dists", "=", "results", "[", "'r_attn_dists'", "]", "[", "0", "]", ".", "tolist", "(", ")", "\n", "\n", "if", "FLAGS", ".", "pointer_gen", ":", "\n", "# Convert singleton list containing a tensor to a list of k arrays", "\n", "      ", "assert", "len", "(", "results", "[", "'p_gens'", "]", ")", "==", "1", "\n", "p_gens", "=", "np", ".", "transpose", "(", "np", ".", "asarray", "(", "results", "[", "'p_gens'", "]", "[", "0", "]", ")", ")", "[", "0", "]", ".", "tolist", "(", ")", "\n", "", "else", ":", "\n", "      ", "p_gens", "=", "[", "None", "for", "_", "in", "range", "(", "beam_size", ")", "]", "\n", "\n", "# Convert the coverage tensor to a list length k containing the coverage vector for each hypothesis", "\n", "", "if", "FLAGS", ".", "coverage", ":", "\n", "      ", "q_new_coverage", "=", "results", "[", "'q_coverage'", "]", ".", "tolist", "(", ")", "\n", "r_new_coverage", "=", "results", "[", "'r_coverage'", "]", ".", "tolist", "(", ")", "\n", "assert", "len", "(", "q_new_coverage", ")", "==", "beam_size", "\n", "assert", "len", "(", "r_new_coverage", ")", "==", "beam_size", "\n", "", "else", ":", "\n", "      ", "q_new_coverage", "=", "[", "None", "for", "_", "in", "range", "(", "beam_size", ")", "]", "\n", "r_new_coverage", "=", "[", "None", "for", "_", "in", "range", "(", "beam_size", ")", "]", "\n", "\n", "", "return", "results", "[", "'ids'", "]", ",", "results", "[", "'probs'", "]", ",", "new_states", ",", "q_attn_dists", ",", "r_attn_dists", ",", "p_gens", ",", "q_new_coverage", ",", "r_new_coverage", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.model._mask_and_avg": [[637, 652], ["tensorflow.reduce_sum", "tensorflow.reduce_mean", "sum", "enumerate"], "function", ["None"], ["", "", "def", "_mask_and_avg", "(", "values", ",", "padding_mask", ")", ":", "\n", "  ", "\"\"\"Applies mask to values then returns overall average (a scalar)\n\n  Args:\n    values: a list length max_dec_steps containing arrays shape (batch_size).\n    padding_mask: tensor shape (batch_size, max_dec_steps) containing 1s and 0s.\n\n  Returns:\n    a scalar\n  \"\"\"", "\n", "\n", "dec_lens", "=", "tf", ".", "reduce_sum", "(", "padding_mask", ",", "axis", "=", "1", ")", "# shape batch_size. float32", "\n", "values_per_step", "=", "[", "v", "*", "padding_mask", "[", ":", ",", "dec_step", "]", "for", "dec_step", ",", "v", "in", "enumerate", "(", "values", ")", "]", "\n", "values_per_ex", "=", "sum", "(", "values_per_step", ")", "/", "dec_lens", "# shape (batch_size); normalized value for each batch member", "\n", "return", "tf", ".", "reduce_mean", "(", "values_per_ex", ")", "# overall average", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.model._coverage_loss": [[654, 682], ["tensorflow.zeros_like", "model._mask_and_avg", "tensorflow.zeros_like", "model._mask_and_avg", "tensorflow.reduce_sum", "q_covlosses.append", "tensorflow.reduce_sum", "r_covlosses.append", "zip", "zip", "tensorflow.minimum", "tensorflow.minimum"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model._mask_and_avg", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model._mask_and_avg"], ["", "def", "_coverage_loss", "(", "q_attn_dists", ",", "r_attn_dists", ",", "p_gens", ",", "padding_mask", ")", ":", "\n", "  ", "\"\"\"Calculates the coverage loss from the attention distributions.\n\n  Args:\n    attn_dists: The attention distributions for each decoder timestep. A list length max_dec_steps containing shape (batch_size, attn_length)\n    padding_mask: shape (batch_size, max_dec_steps).\n\n  Returns:\n    coverage_loss: scalar\n  \"\"\"", "\n", "q_attn_dists", "=", "[", "p_gen", "[", "1", "]", "/", "(", "p_gen", "[", "1", "]", "+", "p_gen", "[", "2", "]", ")", "*", "dist", "for", "(", "p_gen", ",", "dist", ")", "in", "zip", "(", "p_gens", ",", "q_attn_dists", ")", "]", "\n", "r_attn_dists", "=", "[", "p_gen", "[", "2", "]", "/", "(", "p_gen", "[", "1", "]", "+", "p_gen", "[", "2", "]", ")", "*", "dist", "for", "(", "p_gen", ",", "dist", ")", "in", "zip", "(", "p_gens", ",", "r_attn_dists", ")", "]", "\n", "q_coverage", "=", "tf", ".", "zeros_like", "(", "q_attn_dists", "[", "0", "]", ")", "# shape (batch_size, attn_length). Initial coverage is zero.", "\n", "q_covlosses", "=", "[", "]", "# Coverage loss per decoder timestep. Will be list length max_dec_steps containing shape (batch_size).", "\n", "for", "a", "in", "q_attn_dists", ":", "\n", "    ", "covloss", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "minimum", "(", "a", ",", "q_coverage", ")", ",", "[", "1", "]", ")", "# calculate the coverage loss for this step", "\n", "q_covlosses", ".", "append", "(", "covloss", ")", "\n", "q_coverage", "+=", "a", "# update the coverage vector", "\n", "", "q_coverage_loss", "=", "_mask_and_avg", "(", "q_covlosses", ",", "padding_mask", ")", "\n", "r_coverage", "=", "tf", ".", "zeros_like", "(", "r_attn_dists", "[", "0", "]", ")", "# shape (batch_size, attn_length). Initial coverage is zero.", "\n", "r_covlosses", "=", "[", "]", "# Coverage loss per decoder timestep. Will be list length max_dec_steps containing shape (batch_size).", "\n", "for", "a", "in", "r_attn_dists", ":", "\n", "    ", "covloss", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "minimum", "(", "a", ",", "r_coverage", ")", ",", "[", "1", "]", ")", "# calculate the coverage loss for this step", "\n", "r_covlosses", ".", "append", "(", "covloss", ")", "\n", "r_coverage", "+=", "a", "# update the coverage vector", "\n", "", "r_coverage_loss", "=", "_mask_and_avg", "(", "r_covlosses", ",", "padding_mask", ")", "\n", "\n", "return", "q_coverage_loss", ",", "r_coverage_loss", "\n", "", ""]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.util.get_config": [[24, 29], ["tensorflow.ConfigProto"], "function", ["None"], ["def", "get_config", "(", ")", ":", "\n", "  ", "\"\"\"Returns config for tf.session\"\"\"", "\n", "config", "=", "tf", ".", "ConfigProto", "(", "allow_soft_placement", "=", "True", ")", "\n", "config", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-static.util.load_ckpt": [[30, 43], ["os.path.join", "tensorflow.train.get_checkpoint_state", "tensorflow.logging.info", "saver.restore", "tensorflow.logging.info", "time.sleep"], "function", ["None"], ["", "def", "load_ckpt", "(", "saver", ",", "sess", ",", "ckpt_dir", "=", "\"train\"", ")", ":", "\n", "  ", "\"\"\"Load checkpoint from the ckpt_dir (if unspecified, this is train dir) and restore it to saver and sess, waiting 10 secs in the case of failure. Also returns checkpoint name.\"\"\"", "\n", "while", "True", ":", "\n", "    ", "try", ":", "\n", "      ", "latest_filename", "=", "\"checkpoint_best\"", "if", "ckpt_dir", "==", "\"eval\"", "else", "None", "\n", "ckpt_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "log_root", ",", "ckpt_dir", ")", "\n", "ckpt_state", "=", "tf", ".", "train", ".", "get_checkpoint_state", "(", "ckpt_dir", ",", "latest_filename", "=", "latest_filename", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Loading checkpoint %s'", ",", "ckpt_state", ".", "model_checkpoint_path", ")", "\n", "saver", ".", "restore", "(", "sess", ",", "ckpt_state", ".", "model_checkpoint_path", ")", "\n", "return", "ckpt_state", ".", "model_checkpoint_path", "\n", "", "except", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"Failed to load checkpoint from %s. Sleeping for %i secs...\"", ",", "ckpt_dir", ",", "10", ")", "\n", "time", ".", "sleep", "(", "10", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.run_summarization.calc_running_avg_loss": [[82, 107], ["min", "tensorflow.Summary", "tf.Summary.value.add", "summary_writer.add_summary", "tensorflow.compat.v1.logging.info"], "function", ["None"], ["def", "calc_running_avg_loss", "(", "loss", ",", "running_avg_loss", ",", "summary_writer", ",", "step", ",", "decay", "=", "0.99", ")", ":", "\n", "  ", "\"\"\"Calculate the running average loss via exponential decay.\n  This is used to implement early stopping w.r.t. a more smooth loss curve than the raw loss curve.\n\n  Args:\n    loss: loss on the most recent eval step\n    running_avg_loss: running_avg_loss so far\n    summary_writer: FileWriter object to write for tensorboard\n    step: training iteration step\n    decay: rate of exponential decay, a float between 0 and 1. Larger is smoother.\n\n  Returns:\n    running_avg_loss: new running average loss\n  \"\"\"", "\n", "if", "running_avg_loss", "==", "0", ":", "# on the first iteration just take the loss", "\n", "    ", "running_avg_loss", "=", "loss", "\n", "", "else", ":", "\n", "    ", "running_avg_loss", "=", "running_avg_loss", "*", "decay", "+", "(", "1", "-", "decay", ")", "*", "loss", "\n", "", "running_avg_loss", "=", "min", "(", "running_avg_loss", ",", "12", ")", "# clip", "\n", "loss_sum", "=", "tf", ".", "Summary", "(", ")", "\n", "tag_name", "=", "'running_avg_loss/decay=%f'", "%", "(", "decay", ")", "\n", "loss_sum", ".", "value", ".", "add", "(", "tag", "=", "tag_name", ",", "simple_value", "=", "running_avg_loss", ")", "\n", "summary_writer", ".", "add_summary", "(", "loss_sum", ",", "step", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "'running_avg_loss: %f'", ",", "running_avg_loss", ")", "\n", "return", "running_avg_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.run_summarization.restore_best_model": [[109, 132], ["tensorflow.compat.v1.logging.info", "tensorflow.Session", "print", "tf.Session.run", "tensorflow.train.Saver", "print", "util.load_ckpt", "print", "[].replace", "os.path.join", "print", "tensorflow.train.Saver", "tf.train.Saver.save", "print", "exit", "tensorflow.initialize_all_variables", "util.get_config", "tensorflow.all_variables", "util.load_ckpt.split"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.util.load_ckpt", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.util.get_config"], ["", "def", "restore_best_model", "(", ")", ":", "\n", "  ", "\"\"\"Load bestmodel file from eval directory, add variables for adagrad, and save to train directory\"\"\"", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "\"Restoring bestmodel for training...\"", ")", "\n", "\n", "# Initialize all vars in the model", "\n", "sess", "=", "tf", ".", "Session", "(", "config", "=", "util", ".", "get_config", "(", ")", ")", "\n", "print", "(", "\"Initializing all variables...\"", ")", "\n", "sess", ".", "run", "(", "tf", ".", "initialize_all_variables", "(", ")", ")", "\n", "\n", "# Restore the best model from eval dir", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "[", "v", "for", "v", "in", "tf", ".", "all_variables", "(", ")", "if", "\"Adagrad\"", "not", "in", "v", ".", "name", "]", ")", "\n", "print", "(", "\"Restoring all non-adagrad variables from best model in eval dir...\"", ")", "\n", "curr_ckpt", "=", "util", ".", "load_ckpt", "(", "saver", ",", "sess", ",", "\"eval\"", ")", "\n", "print", "(", "\"Restored %s.\"", "%", "curr_ckpt", ")", "\n", "\n", "# Save this model to train dir and quit", "\n", "new_model_name", "=", "curr_ckpt", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ".", "replace", "(", "\"bestmodel\"", ",", "\"model\"", ")", "\n", "new_fname", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "log_root", ",", "\"train\"", ",", "new_model_name", ")", "\n", "print", "(", "\"Saving model to %s...\"", "%", "(", "new_fname", ")", ")", "\n", "new_saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "# this saver saves all variables that now exist, including Adagrad variables", "\n", "new_saver", ".", "save", "(", "sess", ",", "new_fname", ")", "\n", "print", "(", "\"Saved.\"", ")", "\n", "exit", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.run_summarization.convert_to_coverage_model": [[134, 156], ["tensorflow.compat.v1.logging.info", "tensorflow.Session", "print", "tf.Session.run", "tensorflow.train.Saver", "print", "util.load_ckpt", "print", "print", "tensorflow.train.Saver", "tf.train.Saver.save", "print", "exit", "tensorflow.global_variables_initializer", "util.get_config", "tensorflow.global_variables"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.util.load_ckpt", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.util.get_config"], ["", "def", "convert_to_coverage_model", "(", ")", ":", "\n", "  ", "\"\"\"Load non-coverage checkpoint, add initialized extra variables for coverage, and save as new checkpoint\"\"\"", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "\"converting non-coverage model to coverage model..\"", ")", "\n", "\n", "# initialize an entire coverage model from scratch", "\n", "sess", "=", "tf", ".", "Session", "(", "config", "=", "util", ".", "get_config", "(", ")", ")", "\n", "print", "(", "\"initializing everything...\"", ")", "\n", "sess", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "\n", "# load all non-coverage weights from checkpoint", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "[", "v", "for", "v", "in", "tf", ".", "global_variables", "(", ")", "if", "\"coverage\"", "not", "in", "v", ".", "name", "and", "\"Adagrad\"", "not", "in", "v", ".", "name", "]", ")", "\n", "print", "(", "\"restoring non-coverage variables...\"", ")", "\n", "curr_ckpt", "=", "util", ".", "load_ckpt", "(", "saver", ",", "sess", ")", "\n", "print", "(", "\"restored.\"", ")", "\n", "\n", "# save this model and quit", "\n", "new_fname", "=", "curr_ckpt", "+", "'_cov_init'", "\n", "print", "(", "\"saving model to %s...\"", "%", "(", "new_fname", ")", ")", "\n", "new_saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "# this one will save all variables that now exist", "\n", "new_saver", ".", "save", "(", "sess", ",", "new_fname", ")", "\n", "print", "(", "\"saved.\"", ")", "\n", "exit", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.run_summarization.setup_training": [[158, 187], ["os.path.join", "model.build_graph", "tensorflow.train.Saver", "tensorflow.train.Supervisor", "tensorflow.compat.v1.logging.info", "tf.train.Supervisor.prepare_or_wait_for_session", "tensorflow.compat.v1.logging.info", "os.path.exists", "os.makedirs", "run_summarization.convert_to_coverage_model", "run_summarization.restore_best_model", "run_summarization.run_training", "util.get_config", "tensorflow.compat.v1.logging.info", "tf.train.Supervisor.stop"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel.build_graph", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.run_summarization.convert_to_coverage_model", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.run_summarization.restore_best_model", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.run_summarization.run_training", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.util.get_config"], ["", "def", "setup_training", "(", "model", ",", "batcher", ",", "vocab", ",", "hps", ")", ":", "\n", "  ", "\"\"\"Does setup before starting training (run_training)\"\"\"", "\n", "train_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "log_root", ",", "\"train\"", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "train_dir", ")", ":", "os", ".", "makedirs", "(", "train_dir", ")", "\n", "\n", "model", ".", "build_graph", "(", ")", "# build the graph", "\n", "if", "FLAGS", ".", "convert_to_coverage_model", ":", "\n", "    ", "assert", "FLAGS", ".", "coverage", ",", "\"To convert your non-coverage model to a coverage model, run with convert_to_coverage_model=True and coverage=True\"", "\n", "convert_to_coverage_model", "(", ")", "\n", "", "if", "FLAGS", ".", "restore_best_model", ":", "\n", "    ", "restore_best_model", "(", ")", "\n", "", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "max_to_keep", "=", "3", ")", "# keep 3 checkpoints at a time", "\n", "\n", "sv", "=", "tf", ".", "train", ".", "Supervisor", "(", "logdir", "=", "train_dir", ",", "\n", "is_chief", "=", "True", ",", "\n", "saver", "=", "saver", ",", "\n", "summary_op", "=", "None", ",", "\n", "save_summaries_secs", "=", "600", ",", "# save summaries for tensorboard every 60 secs", "\n", "save_model_secs", "=", "None", ",", "# checkpoint every 60 secs", "\n", "global_step", "=", "model", ".", "global_step", ")", "\n", "summary_writer", "=", "sv", ".", "summary_writer", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "\"Preparing or waiting for session...\"", ")", "\n", "sess_context_manager", "=", "sv", ".", "prepare_or_wait_for_session", "(", "config", "=", "util", ".", "get_config", "(", ")", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "\"Created session.\"", ")", "\n", "try", ":", "\n", "    ", "run_training", "(", "model", ",", "batcher", ",", "sess_context_manager", ",", "sv", ",", "summary_writer", ",", "vocab", ",", "hps", ")", "# this is an infinite loop until interrupted", "\n", "", "except", "KeyboardInterrupt", ":", "\n", "    ", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "\"Caught keyboard interrupt on worker. Stopping supervisor...\"", ")", "\n", "sv", ".", "stop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.run_summarization.run_training": [[189, 257], ["tensorflow.compat.v1.logging.info", "os.path.join", "print", "batcher.Batcher", "time.time", "tensorflow.python.debug.LocalCLIDebugWrapperSession", "tf_debug.LocalCLIDebugWrapperSession.add_tensor_filter", "batcher.next_batch", "model.run_train_step", "tensorflow.compat.v1.logging.info", "tensorflow.compat.v1.logging.info", "summary_writer.add_summary", "print", "print", "tensorflow.compat.v1.logging.info", "numpy.isfinite", "Exception", "summary_writer.flush", "print", "time.time", "tensorflow.compat.v1.logging.info", "time.time", "batcher.Batcher", "run_summarization.run_eval", "print", "print", "print", "print", "print", "print", "saver.save"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Batcher.next_batch", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel.run_train_step", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.run_summarization.run_eval"], ["", "", "def", "run_training", "(", "model", ",", "batcher", ",", "sess_context_manager", ",", "sv", ",", "summary_writer", ",", "vocab", ",", "hps", ")", ":", "\n", "  ", "\"\"\"Repeatedly runs training iterations, logging loss to screen and writing summaries\"\"\"", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "\"starting run_training\"", ")", "\n", "train_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "log_root", ",", "\"train/model.ckpt\"", ")", "\n", "print", "(", "train_dir", ")", "\n", "saver", "=", "sv", ".", "saver", "\n", "with", "sess_context_manager", "as", "sess", ":", "\n", "    ", "if", "FLAGS", ".", "debug", ":", "# start the tensorflow debugger", "\n", "      ", "sess", "=", "tf_debug", ".", "LocalCLIDebugWrapperSession", "(", "sess", ")", "\n", "sess", ".", "add_tensor_filter", "(", "\"has_inf_or_nan\"", ",", "tf_debug", ".", "has_inf_or_nan", ")", "\n", "\n", "", "val_batcher", "=", "Batcher", "(", "FLAGS", ".", "valid_path", ",", "vocab", ",", "hps", ",", "single_pass", "=", "True", ")", "\n", "max_f1", ",", "max_acc", ",", "min_loss", "=", "0.0", ",", "0.0", ",", "20", "\n", "#max_f1, max_acc, min_loss = run_eval(model, sess, val_batcher)", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "while", "True", ":", "# repeats until interrupted", "\n", "      ", "batch", "=", "batcher", ".", "next_batch", "(", ")", "\n", "results", "=", "model", ".", "run_train_step", "(", "sess", ",", "batch", ")", "\n", "\n", "if", "FLAGS", ".", "coverage", ":", "\n", "        ", "print", "(", "results", "[", "'q_coverage'", "]", ")", "\n", "print", "(", "results", "[", "'r_coverage'", "]", ")", "\n", "coverage_loss", "=", "results", "[", "'coverage_loss'", "]", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "\"coverage_loss: %f\"", ",", "coverage_loss", ")", "# print the coverage loss to screen", "\n", "\n", "", "loss", "=", "results", "[", "'loss'", "]", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "'loss: %f'", ",", "loss", ")", "# print the loss to screen", "\n", "#tf.logging.info('accuracy: %f', accuracy) # print the loss to screen", "\n", "#print(results['losses'])", "\n", "if", "not", "np", ".", "isfinite", "(", "loss", ")", ":", "\n", "        ", "raise", "Exception", "(", "\"Loss is not finite. Stopping.\"", ")", "\n", "\n", "# get the summaries and iteration number so we can write summaries to tensorboard", "\n", "", "summaries", "=", "results", "[", "'summaries'", "]", "# we will write these summaries to tensorboard using summary_writer", "\n", "train_step", "=", "results", "[", "'global_step'", "]", "# we need this to update our running average loss", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "'train_step: %d'", ",", "train_step", ")", "\n", "\n", "summary_writer", ".", "add_summary", "(", "summaries", ",", "train_step", ")", "# write the summaries", "\n", "if", "train_step", "%", "100", "==", "0", ":", "# flush the summary writer every so often", "\n", "        ", "summary_writer", ".", "flush", "(", ")", "\n", "print", "(", "\"current best result: MACRO_F1 %s. ACC %s, Loss %s\"", "%", "(", "max_f1", ",", "max_acc", ",", "min_loss", ")", ")", "\n", "#print(split_acc)", "\n", "", "if", "train_step", "%", "5000", "==", "0", ":", "\n", "        ", "to_be_saved", "=", "False", "\n", "t1", "=", "time", ".", "time", "(", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "'seconds for training step: %.3f'", ",", "t1", "-", "t0", ")", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "val_batcher", "=", "Batcher", "(", "FLAGS", ".", "valid_path", ",", "vocab", ",", "hps", ",", "single_pass", "=", "True", ")", "\n", "f1", ",", "ACC", ",", "loss", "=", "run_eval", "(", "model", ",", "sess", ",", "val_batcher", ")", "\n", "if", "ACC", ">", "max_acc", ":", "\n", "          ", "max_acc", "=", "ACC", "\n", "#to_be_saved = True", "\n", "", "if", "f1", ">", "max_f1", ":", "\n", "          ", "max_f1", "=", "f1", "\n", "to_be_saved", "=", "True", "\n", "", "if", "loss", "<", "min_loss", ":", "\n", "          ", "min_loss", "=", "loss", "\n", "to_be_saved", "=", "True", "\n", "", "if", "to_be_saved", ":", "\n", "          ", "saver", ".", "save", "(", "sess", ",", "train_dir", ",", "global_step", "=", "train_step", ")", "\n", "", "print", "(", "\"current result: F1 %s\"", "%", "(", "f1", ")", ")", "\n", "print", "(", "\"current best result: MACRO_F1 %s\"", "%", "(", "max_f1", ")", ")", "\n", "print", "(", "\"current result: ACC %s\"", "%", "(", "ACC", ")", ")", "\n", "print", "(", "\"current best result: ACC %s\"", "%", "(", "max_acc", ")", ")", "\n", "print", "(", "\"current result: LOSS %s\"", "%", "(", "loss", ")", ")", "\n", "print", "(", "\"current best result: LOSS %s\"", "%", "(", "min_loss", ")", ")", "\n", "", "if", "train_step", ">", "150000", ":", "\n", "        ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.run_summarization.run_eval": [[260, 290], ["tensorflow.compat.v1.logging.info", "time.time", "time.time", "tensorflow.compat.v1.logging.info", "sklearn.metrics.precision_recall_fscore_support", "print", "f_class.mean", "val_batcher.next_batch", "model.run_eval_step", "print", "tensorflow.compat.v1.logging.info", "loss.append", "y_pred.extend", "y_true.extend", "sum", "float", "sum", "float", "len", "len", "numpy.asarray", "numpy.asarray"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Batcher.next_batch", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel.run_eval_step", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.beam_search.Hypothesis.extend", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.beam_search.Hypothesis.extend"], ["", "", "", "", "def", "run_eval", "(", "model", ",", "sess", ",", "val_batcher", ")", ":", "\n", "  ", "loss", "=", "[", "]", "\n", "y_pred", "=", "[", "]", "\n", "y_true", "=", "[", "]", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "'running validation step...'", ")", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "while", "True", ":", "\n", "    ", "val_batch", "=", "val_batcher", ".", "next_batch", "(", ")", "\n", "if", "not", "val_batch", ":", "\n", "      ", "break", "\n", "", "results", "=", "model", ".", "run_eval_step", "(", "sess", ",", "val_batch", ")", "\n", "accuracy", "=", "results", "[", "'accuracy'", "]", "\n", "l", "=", "results", "[", "'loss'", "]", "\n", "print", "(", "results", "[", "'y_prob'", "]", "[", "0", "]", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "'validation batch accuracy: %f'", ",", "accuracy", ")", "# print the accuracy to screen", "\n", "loss", ".", "append", "(", "l", ")", "\n", "y_pred", ".", "extend", "(", "results", "[", "'y_pred'", "]", ")", "\n", "y_true", ".", "extend", "(", "results", "[", "'y_true'", "]", ")", "\n", "\n", "", "t1", "=", "time", ".", "time", "(", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "'seconds for validation step: %.3f'", ",", "t1", "-", "t0", ")", "\n", "\n", "p_class", ",", "r_class", ",", "f_class", ",", "support_micro", "=", "precision_recall_fscore_support", "(", "\n", "y_true", "=", "y_true", ",", "y_pred", "=", "y_pred", ",", "labels", "=", "[", "0", ",", "1", ",", "2", "]", ",", "average", "=", "'macro'", ")", "\n", "print", "(", "f_class", ")", "\n", "F1", "=", "f_class", ".", "mean", "(", ")", "\n", "LOSS", "=", "sum", "(", "loss", ")", "/", "float", "(", "len", "(", "loss", ")", ")", "\n", "ACC", "=", "sum", "(", "np", ".", "asarray", "(", "y_pred", ")", "==", "np", ".", "asarray", "(", "y_true", ")", ")", "/", "float", "(", "len", "(", "y_pred", ")", ")", "\n", "\n", "return", "F1", ",", "ACC", ",", "LOSS", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.run_summarization.main": [[292, 345], ["tensorflow.compat.v1.logging.set_verbosity", "tensorflow.compat.v1.logging.info", "os.path.join", "data.Vocab", "FLAGS.__flags.items", "batcher.Batcher", "tensorflow.set_random_seed", "len", "Exception", "os.path.exists", "Exception", "collections.namedtuple", "print", "model.SummarizationModel", "run_summarization.setup_training", "os.makedirs", "Exception", "hps_dict.keys", "hps._replace", "model.SummarizationModel", "decode.BeamSearchDecoder", "decode.BeamSearchDecoder.decode", "ValueError"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.run_summarization.setup_training", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.BeamSearchDecoder.decode"], ["", "def", "main", "(", "unused_argv", ")", ":", "\n", "  ", "if", "len", "(", "unused_argv", ")", "!=", "1", ":", "# prints a message if you've entered flags incorrectly", "\n", "    ", "raise", "Exception", "(", "\"Problem with flags: %s\"", "%", "unused_argv", ")", "\n", "\n", "", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "INFO", ")", "# choose what level of logging you want", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "'Starting seq2seq_attention in %s mode...'", ",", "(", "FLAGS", ".", "mode", ")", ")", "\n", "\n", "# Change log_root to FLAGS.log_root/FLAGS.exp_name and create the dir if necessary", "\n", "FLAGS", ".", "log_root", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "log_root", ",", "FLAGS", ".", "exp_name", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "FLAGS", ".", "log_root", ")", ":", "\n", "    ", "if", "FLAGS", ".", "mode", "==", "\"train\"", ":", "\n", "      ", "os", ".", "makedirs", "(", "FLAGS", ".", "log_root", ")", "\n", "", "else", ":", "\n", "      ", "raise", "Exception", "(", "\"Logdir %s doesn't exist. Run in train mode to create it.\"", "%", "(", "FLAGS", ".", "log_root", ")", ")", "\n", "\n", "", "", "vocab", "=", "Vocab", "(", "FLAGS", ".", "vocab_path", ",", "FLAGS", ".", "vocab_size", ")", "# create a vocabulary", "\n", "\n", "# If in decode mode, set batch_size = beam_size", "\n", "# Reason: in decode mode, we decode one example at a time.", "\n", "# On each step, we have beam_size-many hypotheses in the beam, so we need to make a batch of these hypotheses.", "\n", "if", "FLAGS", ".", "mode", "==", "'decode'", ":", "\n", "    ", "FLAGS", ".", "batch_size", "=", "FLAGS", ".", "beam_size", "\n", "\n", "# If single_pass=True, check we're in decode mode", "\n", "", "if", "FLAGS", ".", "single_pass", "and", "FLAGS", ".", "mode", "!=", "'decode'", ":", "\n", "    ", "raise", "Exception", "(", "\"The single_pass flag should only be True in decode mode\"", ")", "\n", "\n", "# Make a namedtuple hps, containing the values of the hyperparameters that the model needs", "\n", "", "hparam_list", "=", "[", "'mode'", ",", "'lr'", ",", "'adagrad_init_acc'", ",", "'rand_unif_init_mag'", ",", "'trunc_norm_init_std'", ",", "'max_grad_norm'", ",", "'hidden_dim'", ",", "'emb_dim'", ",", "'review_num'", ",", "'batch_size'", ",", "'max_dec_steps'", ",", "'max_enc_steps'", ",", "'coverage'", ",", "'cov_loss_wt'", ",", "'sa_loss_wt'", ",", "'pointer_gen'", ",", "'glove'", "]", "\n", "hps_dict", "=", "{", "}", "\n", "for", "key", ",", "val", "in", "FLAGS", ".", "__flags", ".", "items", "(", ")", ":", "# for each flag", "\n", "    ", "if", "key", "in", "hparam_list", ":", "# if it's in the list", "\n", "      ", "hps_dict", "[", "key", "]", "=", "val", ".", "value", "# add it to the dict", "\n", "", "", "hps", "=", "namedtuple", "(", "\"HParams\"", ",", "hps_dict", ".", "keys", "(", ")", ")", "(", "**", "hps_dict", ")", "\n", "\n", "# Create a batcher object that will create minibatches of data", "\n", "batcher", "=", "Batcher", "(", "FLAGS", ".", "data_path", ",", "vocab", ",", "hps", ",", "single_pass", "=", "FLAGS", ".", "single_pass", ")", "\n", "\n", "\n", "tf", ".", "set_random_seed", "(", "111", ")", "# a seed value for randomness", "\n", "\n", "if", "hps", ".", "mode", "==", "'train'", ":", "\n", "    ", "print", "(", "\"creating model...\"", ")", "\n", "model", "=", "SummarizationModel", "(", "hps", ",", "vocab", ")", "\n", "setup_training", "(", "model", ",", "batcher", ",", "vocab", ",", "hps", ")", "\n", "", "elif", "hps", ".", "mode", "==", "'decode'", ":", "\n", "    ", "decode_model_hps", "=", "hps", "# This will be the hyperparameters for the decoder model", "\n", "decode_model_hps", "=", "hps", ".", "_replace", "(", "max_dec_steps", "=", "1", ")", "# The model is configured with max_dec_steps=1 because we only ever run one step of the decoder at a time (to do beam search). Note that the batcher is initialized with max_dec_steps equal to e.g. 100 because the batches need to contain the full summaries", "\n", "model", "=", "SummarizationModel", "(", "decode_model_hps", ",", "vocab", ")", "\n", "decoder", "=", "BeamSearchDecoder", "(", "model", ",", "batcher", ",", "vocab", ")", "\n", "decoder", ".", "decode", "(", ")", "# decode indefinitely (unless single_pass=True, in which case deocde the dataset exactly once)", "\n", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"The 'mode' flag must be one of train/eval/decode\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.attention_decoder.attention_decoder": [[27, 265], ["tensorflow.python.ops.variable_scope.variable_scope", "tensorflow.reshape", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.python.ops.variable_scope.get_variable", "tensorflow.python.ops.nn_ops.conv2d", "tensorflow.python.ops.variable_scope.get_variable", "tensorflow.python.ops.nn_ops.conv2d", "tensorflow.python.ops.variable_scope.get_variable", "tensorflow.python.ops.nn_ops.conv2d", "tensorflow.python.ops.variable_scope.get_variable", "tensorflow.python.ops.variable_scope.get_variable", "tensorflow.python.ops.variable_scope.get_variable", "tensorflow.python.ops.array_ops.zeros", "array_ops.zeros.set_shape", "tensorflow.python.ops.array_ops.zeros", "array_ops.zeros.set_shape", "enumerate", "tensorflow.expand_dims", "tensorflow.expand_dims", "attention_decoder.attention_decoder.op_attention"], "function", ["None"], ["def", "attention_decoder", "(", "decoder_inputs", ",", "initial_state", ",", "question_states", ",", "review_states", ",", "sent_attention", ",", "num", ",", "q_padding_mask", ",", "r_padding_mask", ",", "cell", ",", "initial_state_attention", "=", "False", ",", "pointer_gen", "=", "True", ",", "use_coverage", "=", "False", ",", "q_prev_coverage", "=", "None", ",", "r_prev_coverage", "=", "None", ")", ":", "\n", "  ", "\"\"\"\n  Args:\n    decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n    initial_state: 2D Tensor [batch_size x cell.state_size].\n    question_states: 3D Tensor [batch_size x q_attn_length x attn_size].\n    review_states: 3D Tensor [batch_size x review_num*r_attn_length x attn_size].\n    q_padding_mask: 2D Tensor [batch_size x q_attn_length] containing 1s and 0s; indicates which of the question locations are padding (0) or a real token (1).\n    r_padding_mask: 3D Tensor [batch_size x review_num x r_attn_length] containing 1s and 0s; indicates which of the question locations are padding (0) or a real token (1).\n    cell: rnn_cell.RNNCell defining the cell function and size.\n    initial_state_attention:\n      Note that this attention decoder passes each decoder input through a linear layer with the previous step's context vector to get a modified version of the input. If initial_state_attention is False, on the first decoder step the \"previous context vector\" is just a zero vector. If initial_state_attention is True, we use initial_state to (re)calculate the previous step's context vector. We set this to False for train/eval mode (because we call attention_decoder once for all decoder steps) and True for decode mode (because we call attention_decoder once for each decoder step).\n    pointer_gen: boolean. If True, calculate the generation probability p_gen for each decoder step.\n    use_coverage: boolean. If True, use coverage mechanism.\n    prev_coverage:\n      If not None, a tensor with shape (batch_size, attn_length). The previous step's coverage vector. This is only not None in decode mode when using coverage.\n\n  Returns:\n    outputs: A list of the same length as decoder_inputs of 2D Tensors of\n      shape [batch_size x cell.output_size]. The output vectors.\n    state: The final state of the decoder. A tensor shape [batch_size x cell.state_size].\n    attn_dists: A list containing tensors of shape (batch_size,attn_length).\n      The attention distributions for each decoder step.\n    p_gens: List of scalars. The values of p_gen for each decoder step. Empty list if pointer_gen=False.\n    coverage: Coverage vector on the last step computed. None if use_coverage=False.\n  \"\"\"", "\n", "with", "variable_scope", ".", "variable_scope", "(", "\"attention_decoder\"", ")", "as", "scope", ":", "\n", "    ", "batch_size", "=", "question_states", ".", "get_shape", "(", ")", "[", "0", "]", ".", "value", "# if this line fails, it's because the batch size isn't defined", "\n", "attn_size", "=", "question_states", ".", "get_shape", "(", ")", "[", "2", "]", ".", "value", "# if this line fails, it's because the attention length isn't defined", "\n", "r_padding_mask", "=", "tf", ".", "reshape", "(", "r_padding_mask", ",", "[", "batch_size", ",", "-", "1", "]", ")", "\n", "\n", "# Reshape encoder_states (need to insert a dim)", "\n", "question_states", "=", "tf", ".", "expand_dims", "(", "question_states", ",", "axis", "=", "2", ")", "# now is shape (batch_size, attn_len, 1, attn_size)", "\n", "review_states", "=", "tf", ".", "expand_dims", "(", "review_states", ",", "axis", "=", "2", ")", "\n", "\n", "# To calculate attention, we calculate", "\n", "#   v^T tanh(W_h h_i + W_s s_t  + b_attn)", "\n", "# where h_i is an encoder state, s_t is a decoder state", "\n", "# attn_vec_size is the length of the vectors v, b_attn, (W_h h_i) and (W_s s_t).", "\n", "# We set it to be equal to the size of the encoder states.", "\n", "attention_vec_size", "=", "attn_size", "\n", "\n", "# Get the weight matrix W_h and apply it to each encoder state to get (W_h h_i), the encoder features", "\n", "W_h", "=", "variable_scope", ".", "get_variable", "(", "\"W_h\"", ",", "[", "1", ",", "1", ",", "attn_size", ",", "attention_vec_size", "]", ")", "\n", "question_features", "=", "nn_ops", ".", "conv2d", "(", "question_states", ",", "W_h", ",", "[", "1", ",", "1", ",", "1", ",", "1", "]", ",", "\"SAME\"", ")", "# shape (batch_size,attn_length,1,attention_vec_size)", "\n", "W_r", "=", "variable_scope", ".", "get_variable", "(", "\"W_r\"", ",", "[", "1", ",", "1", ",", "attn_size", ",", "attention_vec_size", "]", ")", "\n", "review_features", "=", "nn_ops", ".", "conv2d", "(", "review_states", ",", "W_r", ",", "[", "1", ",", "1", ",", "1", ",", "1", "]", ",", "\"SAME\"", ")", "# shape (batch_size,review_num*attn_length,1,attention_vec_size)", "\n", "\n", "# Get the weight vectors v and w_c (w_c is for coverage)", "\n", "v_q", "=", "variable_scope", ".", "get_variable", "(", "\"v_q\"", ",", "[", "attention_vec_size", "]", ")", "\n", "v_r", "=", "variable_scope", ".", "get_variable", "(", "\"v_r\"", ",", "[", "attention_vec_size", "]", ")", "\n", "if", "use_coverage", ":", "\n", "      ", "with", "variable_scope", ".", "variable_scope", "(", "\"coverage\"", ")", ":", "\n", "        ", "w_q_c", "=", "variable_scope", ".", "get_variable", "(", "\"w_q_c\"", ",", "[", "1", ",", "1", ",", "1", ",", "attention_vec_size", "]", ")", "\n", "w_r_c", "=", "variable_scope", ".", "get_variable", "(", "\"w_r_c\"", ",", "[", "1", ",", "1", ",", "1", ",", "attention_vec_size", "]", ")", "\n", "\n", "", "", "if", "q_prev_coverage", "is", "not", "None", ":", "# for beam search mode with coverage", "\n", "# reshape from (batch_size, attn_length) to (batch_size, attn_len, 1, 1)", "\n", "      ", "q_prev_coverage", "=", "tf", ".", "expand_dims", "(", "tf", ".", "expand_dims", "(", "q_prev_coverage", ",", "2", ")", ",", "3", ")", "\n", "", "if", "r_prev_coverage", "is", "not", "None", ":", "# for beam search mode with coverage", "\n", "# reshape from (batch_size, attn_length) to (batch_size, attn_len, 1, 1)", "\n", "      ", "r_prev_coverage", "=", "tf", ".", "expand_dims", "(", "tf", ".", "expand_dims", "(", "r_prev_coverage", ",", "2", ")", ",", "3", ")", "\n", "\n", "", "def", "q_attention", "(", "decoder_state", ",", "coverage", "=", "None", ")", ":", "\n", "      ", "\"\"\"Calculate the context vector and attention distribution from the decoder state.\n\n      Args:\n        decoder_state: state of the decoder\n        coverage: Optional. Previous timestep's coverage vector, shape (batch_size, attn_len, 1, 1).\n\n      Returns:\n        context_vector: weighted sum of encoder_states\n        attn_dist: attention distribution\n        coverage: new coverage vector. shape (batch_size, attn_len, 1, 1)\n      \"\"\"", "\n", "with", "variable_scope", ".", "variable_scope", "(", "\"Question_Attention\"", ")", ":", "\n", "# Pass the decoder state through a linear layer (this is W_s s_t + b_attn in the paper)", "\n", "        ", "decoder_features", "=", "linear", "(", "decoder_state", ",", "attention_vec_size", ",", "True", ")", "# shape (batch_size, attention_vec_size)", "\n", "decoder_features", "=", "tf", ".", "expand_dims", "(", "tf", ".", "expand_dims", "(", "decoder_features", ",", "1", ")", ",", "1", ")", "# reshape to (batch_size, 1, 1, attention_vec_size)", "\n", "\n", "if", "use_coverage", "and", "coverage", "is", "not", "None", ":", "# non-first step of coverage", "\n", "# Multiply coverage vector by w_c to get coverage_features.", "\n", "          ", "coverage_features", "=", "nn_ops", ".", "conv2d", "(", "coverage", ",", "w_q_c", ",", "[", "1", ",", "1", ",", "1", ",", "1", "]", ",", "\"SAME\"", ")", "# c has shape (batch_size, attn_length, 1, attention_vec_size)", "\n", "\n", "# Calculate v^T tanh(W_h h_i + W_s s_t + w_c c_i^t + b_attn)", "\n", "e", "=", "math_ops", ".", "reduce_sum", "(", "v_q", "*", "math_ops", ".", "tanh", "(", "question_features", "+", "decoder_features", "+", "coverage_features", ")", ",", "[", "2", ",", "3", "]", ")", "# shape (batch_size,attn_length)", "\n", "\n", "# Calculate attention distribution", "\n", "attn_dist", "=", "masked_attention_q", "(", "e", ",", "q_padding_mask", ")", "\n", "\n", "# Update coverage vector", "\n", "coverage", "+=", "array_ops", ".", "reshape", "(", "attn_dist", ",", "[", "batch_size", ",", "-", "1", ",", "1", ",", "1", "]", ")", "\n", "", "else", ":", "\n", "# Calculate v^T tanh(W_h h_i + W_s s_t + b_attn)", "\n", "          ", "e", "=", "math_ops", ".", "reduce_sum", "(", "v_q", "*", "math_ops", ".", "tanh", "(", "question_features", "+", "decoder_features", ")", ",", "[", "2", ",", "3", "]", ")", "# calculate e", "\n", "\n", "# Calculate attention distribution", "\n", "attn_dist", "=", "masked_attention_q", "(", "e", ",", "q_padding_mask", ")", "\n", "\n", "if", "use_coverage", ":", "# first step of training", "\n", "            ", "coverage", "=", "tf", ".", "expand_dims", "(", "tf", ".", "expand_dims", "(", "attn_dist", ",", "2", ")", ",", "2", ")", "# initialize coverage", "\n", "\n", "# Calculate the context vector from attn_dist and encoder_states", "\n", "", "", "context_vector", "=", "math_ops", ".", "reduce_sum", "(", "array_ops", ".", "reshape", "(", "attn_dist", ",", "[", "batch_size", ",", "-", "1", ",", "1", ",", "1", "]", ")", "*", "question_states", ",", "[", "1", ",", "2", "]", ")", "# shape (batch_size, attn_size).", "\n", "context_vector", "=", "array_ops", ".", "reshape", "(", "context_vector", ",", "[", "-", "1", ",", "attn_size", "]", ")", "\n", "\n", "", "return", "context_vector", ",", "attn_dist", ",", "coverage", "\n", "\n", "", "def", "r_attention", "(", "decoder_state", ",", "coverage", "=", "None", ")", ":", "\n", "      ", "\"\"\"Calculate the context vector and attention distribution from the decoder state.\n\n      Args:\n        decoder_state: state of the decoder\n        coverage: Optional. Previous timestep's coverage vector, shape (batch_size, attn_len, 1, 1).\n\n      Returns:\n        context_vector: weighted sum of encoder_states\n        attn_dist: attention distribution\n        coverage: new coverage vector. shape (batch_size, attn_len, 1, 1)\n      \"\"\"", "\n", "with", "variable_scope", ".", "variable_scope", "(", "\"Review_Attention\"", ")", ":", "\n", "# Pass the decoder state through a linear layer (this is W_s s_t + b_attn in the paper)", "\n", "        ", "decoder_features", "=", "linear", "(", "decoder_state", ",", "attention_vec_size", ",", "True", ")", "# shape (batch_size, attention_vec_size)", "\n", "decoder_features", "=", "tf", ".", "expand_dims", "(", "tf", ".", "expand_dims", "(", "decoder_features", ",", "1", ")", ",", "1", ")", "# reshape to (batch_size, 1, 1, attention_vec_size)", "\n", "\n", "if", "use_coverage", "and", "coverage", "is", "not", "None", ":", "# non-first step of coverage", "\n", "# Multiply coverage vector by w_c to get coverage_features.", "\n", "          ", "coverage_features", "=", "nn_ops", ".", "conv2d", "(", "coverage", ",", "w_r_c", ",", "[", "1", ",", "1", ",", "1", ",", "1", "]", ",", "\"SAME\"", ")", "# c has shape (batch_size, attn_length, 1, attention_vec_size)", "\n", "\n", "# Calculate v^T tanh(W_h h_i + W_s s_t + w_c c_i^t + b_attn)", "\n", "e", "=", "math_ops", ".", "reduce_sum", "(", "v_r", "*", "math_ops", ".", "tanh", "(", "review_features", "+", "decoder_features", "+", "coverage_features", ")", ",", "[", "2", ",", "3", "]", ")", "# shape (batch_size,attn_length)", "\n", "\n", "# Calculate attention distribution", "\n", "attn_dist", "=", "masked_attention_r", "(", "e", ",", "r_padding_mask", ",", "sent_attention", ",", "batch_size", ",", "num", ")", "\n", "\n", "# Update coverage vector", "\n", "coverage", "+=", "array_ops", ".", "reshape", "(", "attn_dist", ",", "[", "batch_size", ",", "-", "1", ",", "1", ",", "1", "]", ")", "\n", "", "else", ":", "\n", "# Calculate v^T tanh(W_h h_i + W_s s_t + b_attn)", "\n", "          ", "e", "=", "math_ops", ".", "reduce_sum", "(", "v_r", "*", "math_ops", ".", "tanh", "(", "review_features", "+", "decoder_features", ")", ",", "[", "2", ",", "3", "]", ")", "# calculate e", "\n", "\n", "# Calculate attention distribution", "\n", "attn_dist", "=", "masked_attention_r", "(", "e", ",", "r_padding_mask", ",", "sent_attention", ",", "batch_size", ",", "num", ")", "\n", "\n", "if", "use_coverage", ":", "# first step of training", "\n", "            ", "coverage", "=", "tf", ".", "expand_dims", "(", "tf", ".", "expand_dims", "(", "attn_dist", ",", "2", ")", ",", "2", ")", "# initialize coverage", "\n", "\n", "# Calculate the context vector from attn_dist and encoder_states", "\n", "", "", "context_vector", "=", "math_ops", ".", "reduce_sum", "(", "array_ops", ".", "reshape", "(", "attn_dist", ",", "[", "batch_size", ",", "-", "1", ",", "1", ",", "1", "]", ")", "*", "review_states", ",", "[", "1", ",", "2", "]", ")", "# shape (batch_size, attn_size).", "\n", "context_vector", "=", "array_ops", ".", "reshape", "(", "context_vector", ",", "[", "-", "1", ",", "attn_size", "]", ")", "\n", "\n", "", "return", "context_vector", ",", "attn_dist", ",", "coverage", "\n", "\n", "", "outputs", "=", "[", "]", "\n", "q_attn_dists", "=", "[", "]", "\n", "r_attn_dists", "=", "[", "]", "\n", "p_gens", "=", "[", "]", "\n", "state", "=", "initial_state", "\n", "output_states", "=", "[", "]", "\n", "q_coverage", "=", "q_prev_coverage", "# initialize coverage to None or whatever was passed in", "\n", "r_coverage", "=", "r_prev_coverage", "\n", "q_context_vector", "=", "array_ops", ".", "zeros", "(", "[", "batch_size", ",", "attn_size", "]", ")", "\n", "q_context_vector", ".", "set_shape", "(", "[", "None", ",", "attn_size", "]", ")", "# Ensure the second shape of attention vectors is set.", "\n", "r_context_vector", "=", "array_ops", ".", "zeros", "(", "[", "batch_size", ",", "attn_size", "]", ")", "\n", "r_context_vector", ".", "set_shape", "(", "[", "None", ",", "attn_size", "]", ")", "# Ensure the second shape of attention vectors is set.", "\n", "if", "initial_state_attention", ":", "# true in decode mode", "\n", "# Re-calculate the context vector from the previous step so that we can pass it through a linear layer with this step's input to get a modified version of the input", "\n", "      ", "q_context_vector", ",", "_", ",", "q_coverage", "=", "q_attention", "(", "initial_state", ",", "q_coverage", ")", "# in decode mode, this is what updates the coverage vector", "\n", "r_context_vector", ",", "_", ",", "r_coverage", "=", "r_attention", "(", "initial_state", ",", "r_coverage", ")", "\n", "", "for", "i", ",", "inp", "in", "enumerate", "(", "decoder_inputs", ")", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"Adding attention_decoder timestep %i of %i\"", ",", "i", ",", "len", "(", "decoder_inputs", ")", ")", "\n", "if", "i", ">", "0", ":", "\n", "        ", "variable_scope", ".", "get_variable_scope", "(", ")", ".", "reuse_variables", "(", ")", "\n", "\n", "# Merge input and previous attentions into one vector x of the same size as inp", "\n", "", "input_size", "=", "inp", ".", "get_shape", "(", ")", ".", "with_rank", "(", "2", ")", "[", "1", "]", "\n", "if", "input_size", ".", "value", "is", "None", ":", "\n", "        ", "raise", "ValueError", "(", "\"Could not infer input size from input: %s\"", "%", "inp", ".", "name", ")", "\n", "", "x", "=", "linear", "(", "[", "inp", "]", "+", "[", "q_context_vector", "]", "+", "[", "r_context_vector", "]", ",", "input_size", ",", "True", ")", "\n", "#x = inp", "\n", "\n", "# Run the decoder RNN cell. cell_output = decoder state", "\n", "cell_output", ",", "state", "=", "cell", "(", "x", ",", "state", ")", "\n", "output_states", ".", "append", "(", "cell_output", ")", "\n", "\n", "# Run the attention mechanism.", "\n", "if", "i", "==", "0", "and", "initial_state_attention", ":", "# always true in decode mode", "\n", "        ", "with", "variable_scope", ".", "variable_scope", "(", "variable_scope", ".", "get_variable_scope", "(", ")", ",", "reuse", "=", "True", ")", ":", "# you need this because you've already run the initial attention(...) call", "\n", "          ", "q_context_vector", ",", "q_attn_dist", ",", "_", "=", "q_attention", "(", "state", ",", "q_coverage", ")", "# don't allow coverage to update", "\n", "r_context_vector", ",", "r_attn_dist", ",", "_", "=", "r_attention", "(", "state", ",", "r_coverage", ")", "\n", "", "", "else", ":", "\n", "        ", "q_context_vector", ",", "q_attn_dist", ",", "q_coverage", "=", "q_attention", "(", "state", ",", "q_coverage", ")", "\n", "r_context_vector", ",", "r_attn_dist", ",", "r_coverage", "=", "r_attention", "(", "state", ",", "r_coverage", ")", "\n", "", "q_attn_dists", ".", "append", "(", "q_attn_dist", ")", "\n", "r_attn_dists", ".", "append", "(", "r_attn_dist", ")", "\n", "\n", "# Calculate p_gen", "\n", "if", "pointer_gen", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "'calculate_pgen'", ")", ":", "\n", "#p_gen = linear([q_context_vector, r_context_vector, state.c, state.h, x], 3, True) # a scalar", "\n", "          ", "p_gen", "=", "linear", "(", "[", "cell_output", "]", "+", "[", "q_context_vector", "]", "+", "[", "r_context_vector", "]", ",", "3", ",", "True", ")", "# a scalar", "\n", "p_gen", "=", "tf", ".", "nn", ".", "softmax", "(", "p_gen", ")", "\n", "p_gen", "=", "tf", ".", "split", "(", "p_gen", ",", "3", ",", "1", ")", "\n", "p_gens", ".", "append", "(", "p_gen", ")", "\n", "\n", "# Concatenate the cell_output (= decoder state) and the context vector, and pass them through a linear layer", "\n", "# This is V[s_t, h*_t] + b in the paper", "\n", "", "", "with", "variable_scope", ".", "variable_scope", "(", "\"AttnOutputProjection\"", ")", ":", "\n", "        ", "output", "=", "linear", "(", "[", "cell_output", "]", "+", "[", "q_context_vector", "]", "+", "[", "r_context_vector", "]", ",", "cell", ".", "output_size", ",", "True", ")", "\n", "#output = linear([cell_output], cell.output_size, True)", "\n", "", "outputs", ".", "append", "(", "output", ")", "\n", "\n", "# If using coverage, reshape it", "\n", "", "if", "r_coverage", "is", "not", "None", ":", "\n", "      ", "r_coverage", "=", "array_ops", ".", "reshape", "(", "r_coverage", ",", "[", "batch_size", ",", "-", "1", "]", ")", "\n", "", "if", "q_coverage", "is", "not", "None", ":", "\n", "      ", "q_coverage", "=", "array_ops", ".", "reshape", "(", "q_coverage", ",", "[", "batch_size", ",", "-", "1", "]", ")", "\n", "\n", "", "return", "outputs", ",", "output_states", ",", "state", ",", "q_attn_dists", ",", "r_attn_dists", ",", "p_gens", ",", "q_coverage", ",", "r_coverage", "\n", "\n", "", "", "def", "masked_attention_q", "(", "e", ",", "padding_mask", ")", ":", "\n", "  ", "\"\"\"Take softmax of e then apply enc_padding_mask and re-normalize\"\"\"", "\n", "attn_dist", "=", "nn_ops", ".", "softmax", "(", "e", ")", "+", "1e-8", "# take softmax. shape (batch_size, attn_length)", "\n", "attn_dist", "*=", "padding_mask", "# apply mask", "\n", "masked_sums", "=", "tf", ".", "reduce_sum", "(", "attn_dist", ",", "axis", "=", "1", ")", "# shape (batch_size)", "\n", "return", "attn_dist", "/", "tf", ".", "reshape", "(", "masked_sums", ",", "[", "-", "1", ",", "1", "]", ")", "# re-normalize", "\n", "\n", ""]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.attention_decoder.masked_attention_q": [[266, 272], ["tensorflow.reduce_sum", "tensorflow.python.ops.nn_ops.softmax", "tensorflow.reshape"], "function", ["None"], ["\n", "\n", "def", "masked_attention_r", "(", "e", ",", "padding_mask", ",", "review_attention", ",", "batch_size", ",", "num", ")", ":", "\n", "  ", "attn_dist", "=", "nn_ops", ".", "softmax", "(", "e", ")", "#+1e-8 # take softmax. shape (batch_size, attn_length)", "\n", "\n", "review_attention", "=", "tf", ".", "expand_dims", "(", "review_attention", ",", "-", "1", ")", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.attention_decoder.masked_attention_r": [[288, 304], ["tensorflow.python.ops.nn_ops.softmax", "tensorflow.expand_dims", "tensorflow.reshape", "tensorflow.multiply", "tensorflow.reshape", "tensorflow.divide", "tensorflow.reshape", "tensorflow.reduce_sum", "tensorflow.reshape", "tensorflow.expand_dims", "tensorflow.reduce_sum"], "function", ["None"], ["\n", "total_arg_sizes", "=", "[", "]", "\n", "for", "arg", "in", "args", ":", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.attention_decoder.linear": [[305, 358], ["total_arg_sizes.append", "tensorflow.variable_scope", "range", "tensorflow.get_variable", "tensorflow.add_n", "ValueError", "isinstance", "a.get_shape().as_list", "len", "tensorflow.get_variable", "tensorflow.add_n", "isinstance", "len", "ValueError", "ValueError", "isinstance", "len", "res.append", "res.append", "tensorflow.constant_initializer", "a.get_shape", "str", "tensorflow.matmul", "tensorflow.matmul", "str", "str", "tensorflow.concat"], "function", ["None"], ["    ", "if", "arg", "is", "None", "or", "(", "isinstance", "(", "arg", ",", "(", "list", ",", "tuple", ")", ")", "and", "not", "arg", ")", ":", "\n", "      ", "raise", "ValueError", "(", "\"`arg` must be specified\"", ")", "\n", "", "if", "not", "isinstance", "(", "arg", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "      ", "arg", "=", "[", "arg", "]", "\n", "\n", "# Calculate the total size of arguments on dimension 1.", "\n", "", "total_arg_size", "=", "0", "\n", "shapes", "=", "[", "a", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "for", "a", "in", "arg", "]", "\n", "for", "shape", "in", "shapes", ":", "\n", "      ", "if", "len", "(", "shape", ")", "!=", "2", ":", "\n", "        ", "raise", "ValueError", "(", "\"Linear is expecting 2D arguments: %s\"", "%", "str", "(", "shapes", ")", ")", "\n", "", "if", "not", "shape", "[", "1", "]", ":", "\n", "        ", "raise", "ValueError", "(", "\"Linear expects shape[1] of arguments: %s\"", "%", "str", "(", "shapes", ")", ")", "\n", "", "else", ":", "\n", "        ", "total_arg_size", "+=", "shape", "[", "1", "]", "\n", "", "", "total_arg_sizes", ".", "append", "(", "total_arg_size", ")", "\n", "\n", "# Now the computation.", "\n", "", "with", "tf", ".", "variable_scope", "(", "scope", "or", "\"Linear\"", ")", ":", "\n", "    ", "res", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "args", ")", ")", ":", "\n", "      ", "matrix", "=", "tf", ".", "get_variable", "(", "\"Matrix\"", "+", "str", "(", "i", ")", ",", "[", "total_arg_sizes", "[", "i", "]", ",", "output_size", "]", ")", "\n", "arg", "=", "args", "[", "i", "]", "\n", "if", "not", "isinstance", "(", "arg", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "arg", "=", "[", "arg", "]", "\n", "", "if", "len", "(", "arg", ")", "==", "1", ":", "\n", "        ", "res", ".", "append", "(", "tf", ".", "matmul", "(", "arg", "[", "0", "]", ",", "matrix", ")", ")", "\n", "", "else", ":", "\n", "        ", "res", ".", "append", "(", "tf", ".", "matmul", "(", "tf", ".", "concat", "(", "axis", "=", "1", ",", "values", "=", "arg", ")", ",", "matrix", ")", ")", "\n", "", "", "if", "not", "bias", ":", "\n", "      ", "return", "tf", ".", "add_n", "(", "res", ")", "\n", "", "bias_term", "=", "tf", ".", "get_variable", "(", "\n", "\"Bias\"", ",", "[", "output_size", "]", ",", "initializer", "=", "tf", ".", "constant_initializer", "(", "bias_start", ")", ")", "\n", "", "return", "tf", ".", "add_n", "(", "res", ")", "+", "bias_term", "\n", "", ""]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.beam_search.Hypothesis.__init__": [[28, 47], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "tokens", ",", "log_probs", ",", "state", ",", "q_attn_dists", ",", "r_attn_dists", ",", "p_gens", ",", "q_coverage", ",", "r_coverage", ")", ":", "\n", "    ", "\"\"\"Hypothesis constructor.\n\n    Args:\n      tokens: List of integers. The ids of the tokens that form the summary so far.\n      log_probs: List, same length as tokens, of floats, giving the log probabilities of the tokens so far.\n      state: Current state of the decoder, a LSTMStateTuple.\n      attn_dists: List, same length as tokens, of numpy arrays with shape (attn_length). These are the attention distributions so far.\n      p_gens: List, same length as tokens, of floats, or None if not using pointer-generator model. The values of the generation probability so far.\n      coverage: Numpy array of shape (attn_length), or None if not using coverage. The current coverage vector.\n    \"\"\"", "\n", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "log_probs", "=", "log_probs", "\n", "self", ".", "state", "=", "state", "\n", "self", ".", "q_attn_dists", "=", "q_attn_dists", "\n", "self", ".", "r_attn_dists", "=", "r_attn_dists", "\n", "self", ".", "p_gens", "=", "p_gens", "\n", "self", ".", "q_coverage", "=", "q_coverage", "\n", "self", ".", "r_coverage", "=", "r_coverage", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.beam_search.Hypothesis.extend": [[48, 73], ["beam_search.Hypothesis"], "methods", ["None"], ["", "def", "extend", "(", "self", ",", "token", ",", "log_prob", ",", "state", ",", "q_attn_dist", ",", "r_attn_dist", ",", "p_gen", ",", "q_coverage", ",", "r_coverage", ")", ":", "\n", "    ", "\"\"\"Return a NEW hypothesis, extended with the information from the latest step of beam search.\n\n    Args:\n      token: Integer. Latest token produced by beam search.\n      log_prob: Float. Log prob of the latest token.\n      state: Current decoder state, a LSTMStateTuple.\n      attn_dist: Attention distribution from latest step. Numpy array shape (attn_length).\n      p_gen: Generation probability on latest step. Float.\n      coverage: Latest coverage vector. Numpy array shape (attn_length), or None if not using coverage.\n    Returns:\n      New Hypothesis for next step.\n    \"\"\"", "\n", "return", "Hypothesis", "(", "tokens", "=", "self", ".", "tokens", "+", "[", "token", "]", ",", "\n", "log_probs", "=", "self", ".", "log_probs", "+", "[", "log_prob", "]", ",", "\n", "state", "=", "state", ",", "\n", "q_attn_dists", "=", "self", ".", "q_attn_dists", "+", "[", "q_attn_dist", "]", ",", "\n", "r_attn_dists", "=", "self", ".", "r_attn_dists", "+", "[", "r_attn_dist", "]", ",", "\n", "p_gens", "=", "self", ".", "p_gens", "+", "[", "p_gen", "]", ",", "\n", "q_coverage", "=", "q_coverage", ",", "\n", "r_coverage", "=", "r_coverage", ")", "\n", "\n", "", "@", "property", "\n", "def", "latest_token", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "tokens", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.beam_search.Hypothesis.latest_token": [[74, 77], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "log_prob", "(", "self", ")", ":", "\n", "# the log probability of the hypothesis so far is the sum of the log probabilities of the tokens so far", "\n", "    ", "return", "sum", "(", "self", ".", "log_probs", ")", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.beam_search.Hypothesis.log_prob": [[78, 82], ["sum"], "methods", ["None"], ["\n", "", "@", "property", "\n", "def", "avg_log_prob", "(", "self", ")", ":", "\n", "# normalize log probability by number of tokens (otherwise longer sequences always have lower probability)", "\n", "    ", "return", "self", ".", "log_prob", "/", "len", "(", "self", ".", "tokens", ")", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.beam_search.Hypothesis.avg_log_prob": [[83, 87], ["len"], "methods", ["None"], ["\n", "\n", "", "", "def", "run_beam_search", "(", "sess", ",", "model", ",", "vocab", ",", "batch", ")", ":", "\n", "  "]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.beam_search.run_beam_search": [[89, 178], ["model.run_encoder", "beam_search.sort_hyps", "beam_search.Hypothesis", "model.decode_onestep", "range", "beam_search.sort_hyps", "len", "range", "len", "len", "range", "numpy.zeros", "numpy.zeros", "vocab.word2id", "h.extend", "all_hyps.append", "vocab.word2id", "hyps.append", "vocab.word2id", "range", "results.append", "len", "len", "vocab.size"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel.run_encoder", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.beam_search.sort_hyps", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel.decode_onestep", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.beam_search.sort_hyps", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.beam_search.Hypothesis.extend", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.size"], ["\n", "# Run the encoder to get the encoder hidden states and decoder initial state", "\n", "dec_in_state", ",", "dec_question_inputs", ",", "dec_review_inputs", ",", "y_pred", "=", "model", ".", "run_encoder", "(", "sess", ",", "batch", ")", "\n", "# dec_in_state is a LSTMStateTuple", "\n", "# enc_states has shape [batch_size, <=max_enc_steps, 2*hidden_dim].", "\n", "\n", "# Initialize beam_size-many hyptheses", "\n", "hyps", "=", "[", "Hypothesis", "(", "tokens", "=", "[", "vocab", ".", "word2id", "(", "data", ".", "START_DECODING", ")", "]", ",", "\n", "log_probs", "=", "[", "0.0", "]", ",", "\n", "state", "=", "dec_in_state", ",", "\n", "q_attn_dists", "=", "[", "]", ",", "\n", "r_attn_dists", "=", "[", "]", ",", "\n", "p_gens", "=", "[", "]", ",", "\n", "q_coverage", "=", "np", ".", "zeros", "(", "[", "batch", ".", "q_batch", ".", "shape", "[", "1", "]", "]", ")", ",", "# zero vector of length attention_length", "\n", "r_coverage", "=", "np", ".", "zeros", "(", "[", "batch", ".", "r_batch", ".", "shape", "[", "1", "]", "*", "batch", ".", "r_batch", ".", "shape", "[", "2", "]", "]", ")", "\n", ")", "for", "_", "in", "range", "(", "FLAGS", ".", "beam_size", ")", "]", "\n", "results", "=", "[", "]", "# this will contain finished hypotheses (those that have emitted the [STOP] token)", "\n", "\n", "steps", "=", "0", "\n", "while", "steps", "<", "FLAGS", ".", "max_dec_steps", "and", "len", "(", "results", ")", "<", "FLAGS", ".", "beam_size", ":", "\n", "    ", "latest_tokens", "=", "[", "h", ".", "latest_token", "for", "h", "in", "hyps", "]", "# latest token produced by each hypothesis", "\n", "latest_tokens", "=", "[", "t", "if", "t", "in", "range", "(", "vocab", ".", "size", "(", ")", ")", "else", "vocab", ".", "word2id", "(", "data", ".", "UNKNOWN_TOKEN", ")", "for", "t", "in", "latest_tokens", "]", "# change any in-article temporary OOV ids to [UNK] id, so that we can lookup word embeddings", "\n", "states", "=", "[", "h", ".", "state", "for", "h", "in", "hyps", "]", "# list of current decoder states of the hypotheses", "\n", "q_prev_coverage", "=", "[", "h", ".", "q_coverage", "for", "h", "in", "hyps", "]", "# list of coverage vectors (or None)", "\n", "r_prev_coverage", "=", "[", "h", ".", "r_coverage", "for", "h", "in", "hyps", "]", "\n", "\n", "# Run one step of the decoder to get the new info", "\n", "(", "topk_ids", ",", "topk_log_probs", ",", "new_states", ",", "q_attn_dists", ",", "r_attn_dists", ",", "p_gens", ",", "q_new_coverage", ",", "r_new_coverage", ")", "=", "model", ".", "decode_onestep", "(", "sess", "=", "sess", ",", "\n", "batch", "=", "batch", ",", "\n", "latest_tokens", "=", "latest_tokens", ",", "\n", "dec_init_states", "=", "states", ",", "\n", "dec_question_inputs", "=", "dec_question_inputs", ",", "\n", "dec_review_inputs", "=", "dec_review_inputs", ",", "\n", "q_prev_coverage", "=", "q_prev_coverage", ",", "\n", "r_prev_coverage", "=", "r_prev_coverage", ")", "\n", "\n", "# Extend each hypothesis and collect them all in all_hyps", "\n", "all_hyps", "=", "[", "]", "\n", "num_orig_hyps", "=", "1", "if", "steps", "==", "0", "else", "len", "(", "hyps", ")", "# On the first step, we only had one original hypothesis (the initial hypothesis). On subsequent steps, all original hypotheses are distinct.", "\n", "for", "i", "in", "range", "(", "num_orig_hyps", ")", ":", "\n", "      ", "h", ",", "new_state", ",", "q_attn_dist", ",", "r_attn_dist", ",", "p_gen", ",", "q_new_coverage_i", ",", "r_new_coverage_i", "=", "hyps", "[", "i", "]", ",", "new_states", "[", "i", "]", ",", "q_attn_dists", "[", "i", "]", ",", "r_attn_dists", "[", "i", "]", ",", "p_gens", "[", "i", "]", ",", "q_new_coverage", "[", "i", "]", ",", "r_new_coverage", "[", "i", "]", "# take the ith hypothesis and new decoder state info", "\n", "for", "j", "in", "range", "(", "FLAGS", ".", "beam_size", "*", "2", ")", ":", "# for each of the top 2*beam_size hyps:", "\n", "# Extend the ith hypothesis with the jth option", "\n", "        ", "new_hyp", "=", "h", ".", "extend", "(", "token", "=", "topk_ids", "[", "i", ",", "j", "]", ",", "\n", "log_prob", "=", "topk_log_probs", "[", "i", ",", "j", "]", ",", "\n", "state", "=", "new_state", ",", "\n", "q_attn_dist", "=", "q_attn_dist", ",", "\n", "r_attn_dist", "=", "r_attn_dist", ",", "\n", "p_gen", "=", "p_gen", ",", "\n", "q_coverage", "=", "q_new_coverage_i", ",", "\n", "r_coverage", "=", "r_new_coverage_i", ")", "\n", "all_hyps", ".", "append", "(", "new_hyp", ")", "\n", "\n", "# Filter and collect any hypotheses that have produced the end token.", "\n", "", "", "hyps", "=", "[", "]", "# will contain hypotheses for the next step", "\n", "for", "h", "in", "sort_hyps", "(", "all_hyps", ")", ":", "# in order of most likely h", "\n", "      ", "if", "h", ".", "latest_token", "==", "vocab", ".", "word2id", "(", "data", ".", "STOP_DECODING", ")", ":", "# if stop token is reached...", "\n", "# If this hypothesis is sufficiently long, put in results. Otherwise discard.", "\n", "        ", "if", "steps", ">=", "FLAGS", ".", "min_dec_steps", ":", "\n", "          ", "results", ".", "append", "(", "h", ")", "\n", "", "", "else", ":", "# hasn't reached stop token, so continue to extend this hypothesis", "\n", "        ", "hyps", ".", "append", "(", "h", ")", "\n", "", "if", "len", "(", "hyps", ")", "==", "FLAGS", ".", "beam_size", "or", "len", "(", "results", ")", "==", "FLAGS", ".", "beam_size", ":", "\n", "# Once we've collected beam_size-many hypotheses for the next step, or beam_size-many complete hypotheses, stop.", "\n", "        ", "break", "\n", "\n", "", "", "steps", "+=", "1", "\n", "\n", "# At this point, either we've got beam_size results, or we've reached maximum decoder steps", "\n", "\n", "", "if", "len", "(", "results", ")", "==", "0", ":", "# if we don't have any complete results, add all current hypotheses (incomplete summaries) to results", "\n", "    ", "results", "=", "hyps", "\n", "\n", "# Sort hypotheses by average log probability", "\n", "", "hyps_sorted", "=", "sort_hyps", "(", "results", ")", "\n", "\n", "# Return the hypothesis with highest average log prob", "\n", "return", "hyps_sorted", "[", "0", "]", ",", "y_pred", "[", "0", "]", "\n", "\n", "", "def", "sort_hyps", "(", "hyps", ")", ":", "\n", "  ", "\"\"\"Return a list of Hypothesis objects, sorted by descending average log probability\"\"\"", "\n", "return", "sorted", "(", "hyps", ",", "key", "=", "lambda", "h", ":", "h", ".", "avg_log_prob", ",", "reverse", "=", "True", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.beam_search.sort_hyps": [[179, 182], ["sorted"], "function", ["None"], []], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.BeamSearchDecoder.__init__": [[37, 76], ["decode.BeamSearchDecoder._model.build_graph", "tensorflow.train.Saver", "tensorflow.Session", "util.load_ckpt", "os.path.join", "os.path.exists", "os.path.join", "os.path.exists", "os.mkdir", "os.path.join", "os.path.join", "os.path.join", "util.get_config", "decode.get_decode_dir_name", "Exception", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir", "util.load_ckpt.split"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel.build_graph", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.util.load_ckpt", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.util.get_config", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.get_decode_dir_name"], ["def", "__init__", "(", "self", ",", "model", ",", "batcher", ",", "vocab", ")", ":", "\n", "    ", "\"\"\"Initialize decoder.\n\n    Args:\n      model: a Seq2SeqAttentionModel object.\n      batcher: a Batcher object.\n      vocab: Vocabulary object\n    \"\"\"", "\n", "self", ".", "_model", "=", "model", "\n", "self", ".", "_model", ".", "build_graph", "(", ")", "\n", "self", ".", "_batcher", "=", "batcher", "\n", "self", ".", "_vocab", "=", "vocab", "\n", "self", ".", "_saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "# we use this to load checkpoints for decoding", "\n", "self", ".", "_sess", "=", "tf", ".", "Session", "(", "config", "=", "util", ".", "get_config", "(", ")", ")", "\n", "\n", "# Load an initial checkpoint to use for decoding", "\n", "ckpt_path", "=", "util", ".", "load_ckpt", "(", "self", ".", "_saver", ",", "self", ".", "_sess", ")", "\n", "\n", "if", "FLAGS", ".", "single_pass", ":", "\n", "# Make a descriptive decode directory name", "\n", "      ", "ckpt_name", "=", "\"ckpt-\"", "+", "ckpt_path", ".", "split", "(", "'-'", ")", "[", "-", "1", "]", "# this is something of the form \"ckpt-123456\"", "\n", "self", ".", "_decode_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "log_root", ",", "get_decode_dir_name", "(", "ckpt_name", ")", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "self", ".", "_decode_dir", ")", ":", "\n", "        ", "raise", "Exception", "(", "\"single_pass decode directory %s should not already exist\"", "%", "self", ".", "_decode_dir", ")", "\n", "\n", "", "", "else", ":", "# Generic decode dir name", "\n", "      ", "self", ".", "_decode_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "log_root", ",", "\"decode\"", ")", "\n", "\n", "# Make the decode dir if necessary", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "_decode_dir", ")", ":", "os", ".", "mkdir", "(", "self", ".", "_decode_dir", ")", "\n", "\n", "if", "FLAGS", ".", "single_pass", ":", "\n", "# Make the dirs to contain output written in the correct format for pyrouge", "\n", "      ", "self", ".", "_rouge_ref_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_decode_dir", ",", "\"reference\"", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "_rouge_ref_dir", ")", ":", "os", ".", "mkdir", "(", "self", ".", "_rouge_ref_dir", ")", "\n", "self", ".", "_rouge_dec_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_decode_dir", ",", "\"decoded\"", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "_rouge_dec_dir", ")", ":", "os", ".", "mkdir", "(", "self", ".", "_rouge_dec_dir", ")", "\n", "self", ".", "_rouge_cqa_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_decode_dir", ",", "\"cqa\"", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "_rouge_cqa_dir", ")", ":", "os", ".", "mkdir", "(", "self", ".", "_rouge_cqa_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.BeamSearchDecoder.decode": [[78, 128], ["time.time", "decode.BeamSearchDecoder._batcher.next_batch", "data.show_art_oovs", "data.show_abs_oovs", "beam_search.run_beam_search", "data.outputids2words", "tensorflow.logging.info", "tensorflow.logging.info", "data.show_art_oovs", "int", "data.outputids2words.index", "decode.BeamSearchDecoder.write_for_eval", "decode.print_results", "decode.BeamSearchDecoder.write_for_attnvis", "time.time", "tensorflow.logging.info", "util.load_ckpt", "time.time"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Batcher.next_batch", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.show_art_oovs", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.show_abs_oovs", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.beam_search.run_beam_search", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.outputids2words", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.show_art_oovs", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.BeamSearchDecoder.write_for_eval", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.print_results", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.BeamSearchDecoder.write_for_attnvis", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.util.load_ckpt"], ["", "", "def", "decode", "(", "self", ")", ":", "\n", "    ", "\"\"\"Decode examples until data is exhausted (if FLAGS.single_pass) and return, or decode indefinitely, loading latest checkpoint at regular intervals\"\"\"", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "counter", "=", "0", "\n", "while", "True", ":", "\n", "      ", "batch", "=", "self", ".", "_batcher", ".", "next_batch", "(", ")", "# 1 example repeated across batch", "\n", "if", "batch", "is", "None", ":", "# finished decoding dataset in single_pass mode", "\n", "        ", "assert", "FLAGS", ".", "single_pass", ",", "\"Dataset exhausted, but we are not in single_pass mode\"", "\n", "tf", ".", "logging", ".", "info", "(", "\"Decoder has finished reading dataset for single_pass.\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Output has been saved in %s and %s. Now starting ROUGE eval...\"", ",", "self", ".", "_rouge_ref_dir", ",", "self", ".", "_rouge_dec_dir", ")", "\n", "return", "\n", "\n", "", "original_reviews", "=", "batch", ".", "original_reviews", "[", "0", "]", "# list", "\n", "original_answer", "=", "batch", ".", "original_answers", "[", "0", "]", "# string", "\n", "original_answers_sents", "=", "batch", ".", "original_answers_sents", "[", "0", "]", "# list of strings", "\n", "original_question", "=", "batch", ".", "original_questions", "[", "0", "]", "\n", "y_target", "=", "batch", ".", "y_target_batch", "[", "0", "]", "\n", "\n", "review_withunks", "=", "[", "data", ".", "show_art_oovs", "(", "original_review", ",", "self", ".", "_vocab", ")", "for", "original_review", "in", "original_reviews", "]", "# string", "\n", "question_withunks", "=", "data", ".", "show_art_oovs", "(", "original_question", ",", "self", ".", "_vocab", ")", "# string", "\n", "answer_withunks", "=", "data", ".", "show_abs_oovs", "(", "original_answer", ",", "self", ".", "_vocab", ",", "(", "batch", ".", "oovs", "[", "0", "]", "if", "FLAGS", ".", "pointer_gen", "else", "None", ")", ")", "# string", "\n", "\n", "# Run beam search to get best Hypothesis", "\n", "best_hyp", ",", "y_pred", "=", "beam_search", ".", "run_beam_search", "(", "self", ".", "_sess", ",", "self", ".", "_model", ",", "self", ".", "_vocab", ",", "batch", ")", "\n", "\n", "# Extract the output ids from the hypothesis and convert back to words", "\n", "output_ids", "=", "[", "int", "(", "t", ")", "for", "t", "in", "best_hyp", ".", "tokens", "[", "1", ":", "]", "]", "\n", "decoded_words", "=", "data", ".", "outputids2words", "(", "output_ids", ",", "self", ".", "_vocab", ",", "(", "batch", ".", "oovs", "[", "0", "]", "if", "FLAGS", ".", "pointer_gen", "else", "None", ")", ")", "\n", "\n", "# Remove the [STOP] token from decoded_words, if necessary", "\n", "try", ":", "\n", "        ", "fst_stop_idx", "=", "decoded_words", ".", "index", "(", "data", ".", "STOP_DECODING", ")", "# index of the (first) [STOP] symbol", "\n", "decoded_words", "=", "decoded_words", "[", ":", "fst_stop_idx", "]", "\n", "", "except", "ValueError", ":", "\n", "        ", "decoded_words", "=", "decoded_words", "\n", "", "decoded_output", "=", "' '", ".", "join", "(", "decoded_words", ")", "# single string", "\n", "\n", "if", "FLAGS", ".", "single_pass", ":", "\n", "        ", "self", ".", "write_for_eval", "(", "original_answers_sents", ",", "decoded_words", ",", "original_question", ",", "y_target", ",", "y_pred", ",", "counter", ")", "# write ref summary and decoded summary to file, to eval with pyrouge later", "\n", "counter", "+=", "1", "# this is how many examples we've decoded", "\n", "", "else", ":", "\n", "        ", "print_results", "(", "review_withunks", ",", "answer_withunks", ",", "decoded_output", ",", "original_question", ",", "y_target", ",", "y_pred", ")", "# log output to screen", "\n", "self", ".", "write_for_attnvis", "(", "question_withunks", ",", "review_withunks", ",", "decoded_words", ",", "best_hyp", ".", "r_attn_dists", ",", "best_hyp", ".", "p_gens", ")", "# write info to .json file for visualization tool", "\n", "\n", "# Check if SECS_UNTIL_NEW_CKPT has elapsed; if so return so we can load a new checkpoint", "\n", "t1", "=", "time", ".", "time", "(", ")", "\n", "if", "t1", "-", "t0", ">", "SECS_UNTIL_NEW_CKPT", ":", "\n", "          ", "tf", ".", "logging", ".", "info", "(", "'We\\'ve been decoding with same checkpoint for %i seconds. Time to load new checkpoint'", ",", "t1", "-", "t0", ")", "\n", "_", "=", "util", ".", "load_ckpt", "(", "self", ".", "_saver", ",", "self", ".", "_sess", ")", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.BeamSearchDecoder.write_for_eval": [[130, 172], ["os.path.join", "os.path.join", "os.path.join", "tensorflow.logging.info", "len", "decoded_sents.append", "decode.make_html_safe", "decode.make_html_safe", "open", "enumerate", "open", "enumerate", "open", "f.write", "decoded_words.index", "len", "f.write", "f.write", "f.write", "f.write", "str", "len", "len", "map"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.make_html_safe", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.make_html_safe"], ["", "", "", "", "def", "write_for_eval", "(", "self", ",", "reference_sents", ",", "decoded_words", ",", "question", ",", "target", ",", "prediction", ",", "ex_index", ")", ":", "\n", "    ", "\"\"\"Write output to file in correct format for eval with pyrouge. This is called in single_pass mode.\n\n    Args:\n      reference_sents: list of strings\n      decoded_words: list of strings\n      ex_index: int, the index with which to label the files\n    \"\"\"", "\n", "# First, divide decoded output into sentences", "\n", "decoded_sents", "=", "[", "]", "\n", "while", "len", "(", "decoded_words", ")", ">", "0", ":", "\n", "      ", "try", ":", "\n", "        ", "fst_period_idx", "=", "decoded_words", ".", "index", "(", "\".\"", ")", "\n", "", "except", "ValueError", ":", "# there is text remaining that doesn't end in \".\"", "\n", "        ", "fst_period_idx", "=", "len", "(", "decoded_words", ")", "\n", "", "sent", "=", "decoded_words", "[", ":", "fst_period_idx", "+", "1", "]", "# sentence up to and including the period", "\n", "decoded_words", "=", "decoded_words", "[", "fst_period_idx", "+", "1", ":", "]", "# everything else", "\n", "decoded_sents", ".", "append", "(", "' '", ".", "join", "(", "sent", ")", ")", "\n", "\n", "# pyrouge calls a perl script that puts the data into HTML files.", "\n", "# Therefore we need to make our output HTML safe.", "\n", "", "decoded_sents", "=", "[", "make_html_safe", "(", "w", ")", "for", "w", "in", "decoded_sents", "]", "\n", "reference_sents", "=", "[", "make_html_safe", "(", "w", ")", "for", "w", "in", "reference_sents", "]", "\n", "\n", "# cqa results", "\n", "result", "=", "[", "question", ",", "target", ",", "prediction", "]", "\n", "\n", "# Write to file", "\n", "ref_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_rouge_ref_dir", ",", "\"%06d_reference.txt\"", "%", "ex_index", ")", "\n", "decoded_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_rouge_dec_dir", ",", "\"%06d_decoded.txt\"", "%", "ex_index", ")", "\n", "cqa_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_rouge_cqa_dir", ",", "\"%06d_op.txt\"", "%", "ex_index", ")", "\n", "\n", "with", "open", "(", "ref_file", ",", "\"w\"", ")", "as", "f", ":", "\n", "      ", "for", "idx", ",", "sent", "in", "enumerate", "(", "reference_sents", ")", ":", "\n", "        ", "f", ".", "write", "(", "sent", ")", "if", "idx", "==", "len", "(", "reference_sents", ")", "-", "1", "else", "f", ".", "write", "(", "sent", "+", "\"\\n\"", ")", "\n", "", "", "with", "open", "(", "decoded_file", ",", "\"w\"", ")", "as", "f", ":", "\n", "      ", "for", "idx", ",", "sent", "in", "enumerate", "(", "decoded_sents", ")", ":", "\n", "        ", "f", ".", "write", "(", "sent", ")", "if", "idx", "==", "len", "(", "decoded_sents", ")", "-", "1", "else", "f", ".", "write", "(", "sent", "+", "\"\\n\"", ")", "\n", "", "", "with", "open", "(", "cqa_file", ",", "\"w\"", ")", "as", "f", ":", "\n", "      ", "f", ".", "write", "(", "'%s\\t%s\\t%s\\n'", "%", "(", "result", "[", "0", "]", ",", "str", "(", "result", "[", "1", "]", ")", ",", "' '", ".", "join", "(", "map", "(", "str", ",", "result", "[", "2", "]", ")", ")", ")", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"Wrote example %i to file\"", "%", "ex_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.BeamSearchDecoder.write_for_attnvis": [[174, 199], ["article.split", "os.path.join", "tensorflow.logging.info", "decode.make_html_safe", "open", "json.dump", "decode.make_html_safe", "decode.make_html_safe"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.make_html_safe", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.make_html_safe", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.make_html_safe"], ["", "def", "write_for_attnvis", "(", "self", ",", "article", ",", "abstract", ",", "decoded_words", ",", "attn_dists", ",", "p_gens", ")", ":", "\n", "    ", "\"\"\"Write some data to json file, which can be read into the in-browser attention visualizer tool:\n      https://github.com/abisee/attn_vis\n\n    Args:\n      article: The original article string.\n      abstract: The human (correct) abstract string.\n      attn_dists: List of arrays; the attention distributions.\n      decoded_words: List of strings; the words of the generated summary.\n      p_gens: List of scalars; the p_gen values. If not running in pointer-generator mode, list of None.\n    \"\"\"", "\n", "article_lst", "=", "article", ".", "split", "(", ")", "# list of words", "\n", "decoded_lst", "=", "decoded_words", "# list of decoded words", "\n", "to_write", "=", "{", "\n", "'article_lst'", ":", "[", "make_html_safe", "(", "t", ")", "for", "t", "in", "article_lst", "]", ",", "\n", "'decoded_lst'", ":", "[", "make_html_safe", "(", "t", ")", "for", "t", "in", "decoded_lst", "]", ",", "\n", "'abstract_str'", ":", "make_html_safe", "(", "abstract", ")", ",", "\n", "'attn_dists'", ":", "attn_dists", "\n", "}", "\n", "if", "FLAGS", ".", "pointer_gen", ":", "\n", "      ", "to_write", "[", "'p_gens'", "]", "=", "p_gens", "\n", "", "output_fname", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_decode_dir", ",", "'attn_vis_data.json'", ")", "\n", "with", "open", "(", "output_fname", ",", "'w'", ")", "as", "output_file", ":", "\n", "      ", "json", ".", "dump", "(", "to_write", ",", "output_file", ")", "\n", "", "tf", ".", "logging", ".", "info", "(", "'Wrote visualization data to %s'", ",", "output_fname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.print_results": [[201, 210], ["print", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "print"], "function", ["None"], ["", "", "def", "print_results", "(", "article", ",", "abstract", ",", "decoded_output", ",", "question", ",", "target", ",", "prediction", ")", ":", "\n", "  ", "\"\"\"Prints the article, the reference summmary and the decoded summary to screen\"\"\"", "\n", "print", "(", "\"---------------------------------------------------------------------------\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'ARTICLE:  %s'", ",", "article", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'REFERENCE SUMMARY: %s'", ",", "abstract", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'GENERATED SUMMARY: %s'", ",", "decoded_output", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'QUESTION: %s'", ",", "question", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'TARGET#PREDICTION: %s#%s'", ",", "(", "target", ",", "prediction", "[", "1", "]", ")", ")", "\n", "print", "(", "\"---------------------------------------------------------------------------\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.make_html_safe": [[212, 217], ["s.replace", "s.replace"], "function", ["None"], ["", "def", "make_html_safe", "(", "s", ")", ":", "\n", "  ", "\"\"\"Replace any angled brackets in string s to avoid interfering with HTML attention visualizer.\"\"\"", "\n", "s", ".", "replace", "(", "\"<\"", ",", "\"&lt;\"", ")", "\n", "s", ".", "replace", "(", "\">\"", ",", "\"&gt;\"", ")", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.get_decode_dir_name": [[218, 229], ["ValueError"], "function", ["None"], ["\n", "", "def", "get_decode_dir_name", "(", "ckpt_name", ")", ":", "\n", "  ", "\"\"\"Make a descriptive name for the decode dir, including the name of the checkpoint we use to decode. This is called in single_pass mode.\"\"\"", "\n", "\n", "if", "\"train\"", "in", "FLAGS", ".", "data_path", ":", "dataset", "=", "\"train\"", "\n", "elif", "\"val\"", "in", "FLAGS", ".", "data_path", ":", "dataset", "=", "\"val\"", "\n", "elif", "\"test\"", "in", "FLAGS", ".", "data_path", ":", "dataset", "=", "\"test\"", "\n", "else", ":", "raise", "ValueError", "(", "\"FLAGS.data_path %s should contain one of train, val or test\"", "%", "(", "FLAGS", ".", "data_path", ")", ")", "\n", "dirname", "=", "\"decode_%s_%imaxenc_%ibeam_%imindec_%imaxdec\"", "%", "(", "dataset", ",", "FLAGS", ".", "max_enc_steps", ",", "FLAGS", ".", "beam_size", ",", "FLAGS", ".", "min_dec_steps", ",", "FLAGS", ".", "max_dec_steps", ")", "\n", "if", "ckpt_name", "is", "not", "None", ":", "\n", "    ", "dirname", "+=", "\"_%s\"", "%", "ckpt_name", "\n", "", "return", "dirname", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Example.__init__": [[31, 103], ["vocab.word2id", "vocab.word2id", "answer.split", "question.split", "len", "batcher.Example.get_dec_inp_targ_seqs", "len", "review.split", "reviews_words.append", "batcher.Example.r_lens.append", "batcher.Example.r_batch.append", "vocab.word2id", "vocab.word2id", "data.article2ids", "data.abstract2ids", "batcher.Example.get_dec_inp_targ_seqs", "len", "len", "data.article2ids", "batcher.Example.reviews_extend_vocab.append", "vocab.word2id"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Example.get_dec_inp_targ_seqs", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.article2ids", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.abstract2ids", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Example.get_dec_inp_targ_seqs", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.article2ids", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id"], ["def", "__init__", "(", "self", ",", "reviews", ",", "ratings", ",", "answer_sentences", ",", "question", ",", "label", ",", "vocab", ",", "hps", ")", ":", "\n", "    ", "\"\"\"Initializes the Example, performing tokenization and truncation to produce the encoder, decoder and target sequences, which are stored in self.\n\n    Args:\n      reviews: review text; a list.\n      ratings: alist.\n      answer_sentences: list of strings, one per abstract sentence. In each sentence, each token is separated by a single space.\n      vocab: Vocabulary object\n      hps: hyperparameters\n    \"\"\"", "\n", "self", ".", "hps", "=", "hps", "\n", "\n", "# Get ids of special tokens", "\n", "start_decoding", "=", "vocab", ".", "word2id", "(", "data", ".", "START_DECODING", ")", "\n", "stop_decoding", "=", "vocab", ".", "word2id", "(", "data", ".", "STOP_DECODING", ")", "\n", "\n", "\n", "\n", "# Process the reviews", "\n", "self", ".", "r_lens", "=", "[", "]", "\n", "self", ".", "r_batch", "=", "[", "]", "\n", "self", ".", "rating_batch", "=", "ratings", "\n", "reviews_words", "=", "[", "]", "\n", "for", "review", "in", "reviews", ":", "\n", "      ", "review_words", "=", "review", ".", "split", "(", ")", "\n", "if", "len", "(", "review_words", ")", ">", "hps", ".", "max_enc_steps", ":", "\n", "        ", "review_words", "=", "review_words", "[", ":", "hps", ".", "max_enc_steps", "]", "\n", "", "reviews_words", ".", "append", "(", "review_words", ")", "\n", "self", ".", "r_lens", ".", "append", "(", "len", "(", "review_words", ")", ")", "# store the length after truncation but before padding", "\n", "self", ".", "r_batch", ".", "append", "(", "[", "vocab", ".", "word2id", "(", "w", ")", "for", "w", "in", "review_words", "]", ")", "# list of word ids; OOVs are represented by the id for UNK token", "\n", "\n", "# Process the abstract", "\n", "", "answer", "=", "' '", ".", "join", "(", "answer_sentences", ")", "# string", "\n", "answer_words", "=", "answer", ".", "split", "(", ")", "# list of strings", "\n", "ans_ids", "=", "[", "vocab", ".", "word2id", "(", "w", ")", "for", "w", "in", "answer_words", "]", "# list of word ids; OOVs are represented by the id for UNK token", "\n", "\n", "# Process the question", "\n", "question_words", "=", "question", ".", "split", "(", ")", "\n", "self", ".", "q_lens", "=", "len", "(", "question_words", ")", "\n", "self", ".", "q_batch", "=", "[", "vocab", ".", "word2id", "(", "w", ")", "for", "w", "in", "question_words", "]", "\n", "\n", "# Process the label", "\n", "self", ".", "y_target", "=", "label", "\n", "\n", "# Get the decoder input sequence and target sequence", "\n", "self", ".", "dec_input", ",", "self", ".", "target", "=", "self", ".", "get_dec_inp_targ_seqs", "(", "ans_ids", ",", "hps", ".", "max_dec_steps", ",", "start_decoding", ",", "stop_decoding", ")", "\n", "self", ".", "dec_len", "=", "len", "(", "self", ".", "dec_input", ")", "\n", "assert", "self", ".", "dec_len", ">", "0", "\n", "\n", "# If using pointer-generator mode, we need to store some extra info", "\n", "if", "hps", ".", "pointer_gen", ":", "\n", "      ", "self", ".", "oovs", "=", "[", "]", "\n", "# Store a version of the enc_input where in-article OOVs are represented by their temporary OOV id; also store the in-article OOVs words themselves", "\n", "self", ".", "reviews_extend_vocab", "=", "[", "]", "\n", "for", "review_words", "in", "reviews_words", ":", "\n", "        ", "review_extend_vocab", ",", "self", ".", "oovs", "=", "data", ".", "article2ids", "(", "review_words", ",", "vocab", ",", "self", ".", "oovs", ")", "\n", "self", ".", "reviews_extend_vocab", ".", "append", "(", "review_extend_vocab", ")", "\n", "\n", "# question OOV id.", "\n", "", "self", ".", "question_extend_vocab", ",", "self", ".", "oovs", "=", "data", ".", "article2ids", "(", "question_words", ",", "vocab", ",", "self", ".", "oovs", ")", "\n", "\n", "# Get a verison of the reference summary where in-article OOVs are represented by their temporary article OOV id", "\n", "ans_ids_extend_vocab", "=", "data", ".", "abstract2ids", "(", "answer_words", ",", "vocab", ",", "self", ".", "oovs", ")", "\n", "\n", "# Overwrite decoder target sequence so it uses the temp article OOV ids", "\n", "_", ",", "self", ".", "target", "=", "self", ".", "get_dec_inp_targ_seqs", "(", "ans_ids_extend_vocab", ",", "hps", ".", "max_dec_steps", ",", "start_decoding", ",", "stop_decoding", ")", "\n", "\n", "# Store the original strings", "\n", "", "self", ".", "original_reviews", "=", "reviews", "\n", "self", ".", "original_answer", "=", "answer", "\n", "self", ".", "original_answer_sents", "=", "answer_sentences", "\n", "self", ".", "original_question", "=", "question", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Example.get_dec_inp_targ_seqs": [[105, 127], ["len", "target.append", "len", "len"], "methods", ["None"], ["", "def", "get_dec_inp_targ_seqs", "(", "self", ",", "sequence", ",", "max_len", ",", "start_id", ",", "stop_id", ")", ":", "\n", "    ", "\"\"\"Given the reference summary as a sequence of tokens, return the input sequence for the decoder, and the target sequence which we will use to calculate loss. The sequence will be truncated if it is longer than max_len. The input sequence must start with the start_id and the target sequence must end with the stop_id (but not if it's been truncated).\n\n    Args:\n      sequence: List of ids (integers)\n      max_len: integer\n      start_id: integer\n      stop_id: integer\n\n    Returns:\n      inp: sequence length <=max_len starting with start_id\n      target: sequence same length as input, ending with stop_id only if there was no truncation\n    \"\"\"", "\n", "inp", "=", "[", "start_id", "]", "+", "sequence", "[", ":", "]", "\n", "target", "=", "sequence", "[", ":", "]", "\n", "if", "len", "(", "inp", ")", ">", "max_len", ":", "# truncate", "\n", "      ", "inp", "=", "inp", "[", ":", "max_len", "]", "\n", "target", "=", "target", "[", ":", "max_len", "]", "# no end_token", "\n", "", "else", ":", "# no truncation", "\n", "      ", "target", ".", "append", "(", "stop_id", ")", "# end token", "\n", "", "assert", "len", "(", "inp", ")", "==", "len", "(", "target", ")", "\n", "return", "inp", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Example.pad_decoder_inp_targ": [[129, 135], ["len", "batcher.Example.dec_input.append", "len", "batcher.Example.target.append"], "methods", ["None"], ["", "def", "pad_decoder_inp_targ", "(", "self", ",", "max_len", ",", "pad_id", ")", ":", "\n", "    ", "\"\"\"Pad decoder input and target sequences with pad_id up to max_len.\"\"\"", "\n", "while", "len", "(", "self", ".", "dec_input", ")", "<", "max_len", ":", "\n", "      ", "self", ".", "dec_input", ".", "append", "(", "pad_id", ")", "\n", "", "while", "len", "(", "self", ".", "target", ")", "<", "max_len", ":", "\n", "      ", "self", ".", "target", ".", "append", "(", "pad_id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Example.pad_reviews": [[137, 146], ["range", "len", "range", "len", "batcher.Example.r_batch[].append", "len", "len", "batcher.Example.reviews_extend_vocab[].append"], "methods", ["None"], ["", "", "def", "pad_reviews", "(", "self", ",", "max_len", ",", "pad_id", ")", ":", "\n", "    ", "\"\"\"Pad the encoder input sequence with pad_id up to max_len.\"\"\"", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "r_batch", ")", ")", ":", "\n", "      ", "while", "len", "(", "self", ".", "r_batch", "[", "i", "]", ")", "<", "max_len", ":", "\n", "        ", "self", ".", "r_batch", "[", "i", "]", ".", "append", "(", "pad_id", ")", "\n", "", "", "if", "self", ".", "hps", ".", "pointer_gen", ":", "\n", "      ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "reviews_extend_vocab", ")", ")", ":", "\n", "        ", "while", "len", "(", "self", ".", "reviews_extend_vocab", "[", "i", "]", ")", "<", "max_len", ":", "\n", "          ", "self", ".", "reviews_extend_vocab", "[", "i", "]", ".", "append", "(", "pad_id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Example.pad_question": [[148, 155], ["len", "batcher.Example.q_batch.append", "len", "batcher.Example.question_extend_vocab.append"], "methods", ["None"], ["", "", "", "", "def", "pad_question", "(", "self", ",", "max_len", ",", "pad_id", ")", ":", "\n", "    ", "\"\"\"Pad the encoder input sequence with pad_id up to max_len.\"\"\"", "\n", "while", "len", "(", "self", ".", "q_batch", ")", "<", "max_len", ":", "\n", "      ", "self", ".", "q_batch", ".", "append", "(", "pad_id", ")", "\n", "", "if", "self", ".", "hps", ".", "pointer_gen", ":", "\n", "      ", "while", "len", "(", "self", ".", "question_extend_vocab", ")", "<", "max_len", ":", "\n", "        ", "self", ".", "question_extend_vocab", ".", "append", "(", "pad_id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Batch.__init__": [[160, 178], ["vocab.word2id", "batcher.Batch.init_encoder_seq", "batcher.Batch.init_decoder_seq", "batcher.Batch.store_orig_strings"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Batch.init_encoder_seq", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Batch.init_decoder_seq", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Batch.store_orig_strings"], ["def", "__init__", "(", "self", ",", "example_list", ",", "hps", ",", "vocab", ")", ":", "\n", "    ", "\"\"\"Turns the example_list into a Batch object.\n\n    # Pad the encoder input sequences up to the length of the longest sequence\n    for ex in example_list:\n      ex.pad_reviews(max_r_seq_len, self.pad_id)\n      ex.pad_question(max_q_seq_len, self.pad_id)\n\n\n    Args:\n       example_list: List of Example objects\n       hps: hyperparameters\n       vocab: Vocabulary object\n    \"\"\"", "\n", "self", ".", "pad_id", "=", "vocab", ".", "word2id", "(", "data", ".", "PAD_TOKEN", ")", "# id of the PAD token used to pad sequences", "\n", "self", ".", "init_encoder_seq", "(", "example_list", ",", "hps", ")", "# initialize the input to the encoder", "\n", "self", ".", "init_decoder_seq", "(", "example_list", ",", "hps", ")", "# initialize the input and targets for the decoder", "\n", "self", ".", "store_orig_strings", "(", "example_list", ")", "# store the original strings", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Batch.init_encoder_seq": [[179, 241], ["max", "max", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "enumerate", "ex.pad_reviews", "ex.pad_question", "range", "range", "max", "numpy.zeros", "numpy.zeros", "enumerate", "enumerate", "max", "len", "range", "len"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Example.pad_reviews", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Example.pad_question"], ["", "def", "init_encoder_seq", "(", "self", ",", "example_list", ",", "hps", ")", ":", "\n", "    ", "\"\"\"Initializes the following:\n        self.enc_batch:\n          numpy array of shape (batch_size, <=max_enc_steps) containing integer ids (all OOVs represented by UNK id), padded to length of longest sequence in the batch\n        self.enc_lens:\n          numpy array of shape (batch_size) containing integers. The (truncated) length of each encoder input sequence (pre-padding).\n        self.enc_padding_mask:\n          numpy array of shape (batch_size, <=max_enc_steps), containing 1s and 0s. 1s correspond to real tokens in enc_batch and target_batch; 0s correspond to padding.\n\n      If hps.pointer_gen, additionally initializes the following:\n        self.max_art_oovs:\n          maximum number of in-article OOVs in the batch\n        self.art_oovs:\n          list of list of in-article OOVs (strings), for each example in the batch\n        self.enc_batch_extend_vocab:\n          Same as self.enc_batch, but in-article OOVs are represented by their temporary article OOV number.\n    \"\"\"", "\n", "# Determine the maximum length of the encoder input sequence in this batch", "\n", "max_r_seq_len", "=", "max", "(", "[", "max", "(", "ex", ".", "r_lens", ")", "for", "ex", "in", "example_list", "]", ")", "\n", "max_q_seq_len", "=", "max", "(", "[", "ex", ".", "q_lens", "for", "ex", "in", "example_list", "]", ")", "\n", "\n", "# Pad the encoder input sequences up to the length of the longest sequence", "\n", "for", "ex", "in", "example_list", ":", "\n", "      ", "ex", ".", "pad_reviews", "(", "max_r_seq_len", ",", "self", ".", "pad_id", ")", "\n", "ex", ".", "pad_question", "(", "max_q_seq_len", ",", "self", ".", "pad_id", ")", "\n", "\n", "# Initialize the numpy arrays", "\n", "# Note: our enc_batch can have different length (second dimension) for each batch because we use dynamic_rnn for the encoder.", "\n", "", "self", ".", "r_batch", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ",", "hps", ".", "review_num", ",", "max_r_seq_len", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "self", ".", "q_batch", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ",", "max_q_seq_len", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "self", ".", "r_lens", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ",", "hps", ".", "review_num", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "self", ".", "q_lens", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "self", ".", "r_padding_mask", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ",", "hps", ".", "review_num", ",", "max_r_seq_len", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "q_padding_mask", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ",", "max_q_seq_len", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "rating_batch", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ",", "hps", ".", "review_num", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "\n", "# Fill in the numpy arrays", "\n", "for", "i", ",", "ex", "in", "enumerate", "(", "example_list", ")", ":", "\n", "      ", "self", ".", "r_batch", "[", "i", ",", ":", "]", "=", "ex", ".", "r_batch", "[", ":", "]", "\n", "self", ".", "r_lens", "[", "i", ",", ":", "]", "=", "ex", ".", "r_lens", "[", ":", "]", "\n", "self", ".", "q_batch", "[", "i", ",", ":", "]", "=", "ex", ".", "q_batch", "[", ":", "]", "\n", "self", ".", "q_lens", "[", "i", "]", "=", "ex", ".", "q_lens", "\n", "self", ".", "rating_batch", "[", "i", ",", ":", "]", "=", "ex", ".", "rating_batch", "[", ":", "]", "\n", "for", "j", "in", "range", "(", "len", "(", "ex", ".", "r_lens", ")", ")", ":", "\n", "        ", "for", "k", "in", "range", "(", "ex", ".", "r_lens", "[", "j", "]", ")", ":", "\n", "          ", "self", ".", "r_padding_mask", "[", "i", "]", "[", "j", "]", "[", "k", "]", "=", "1", "\n", "", "", "for", "j", "in", "range", "(", "ex", ".", "q_lens", ")", ":", "\n", "        ", "self", ".", "q_padding_mask", "[", "i", "]", "[", "j", "]", "=", "1", "\n", "\n", "# For pointer-generator mode, need to store some extra info", "\n", "", "", "if", "hps", ".", "pointer_gen", ":", "\n", "# Determine the max number of in-article OOVs in this batch", "\n", "      ", "self", ".", "max_oovs", "=", "max", "(", "[", "len", "(", "ex", ".", "oovs", ")", "for", "ex", "in", "example_list", "]", ")", "\n", "# Store the in-article OOVs themselves", "\n", "self", ".", "oovs", "=", "[", "ex", ".", "oovs", "for", "ex", "in", "example_list", "]", "\n", "# Store the version of the enc_batch that uses the article OOV ids", "\n", "self", ".", "r_batch_extend_vocab", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ",", "hps", ".", "review_num", ",", "max_r_seq_len", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "self", ".", "q_batch_extend_vocab", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ",", "max_q_seq_len", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "for", "i", ",", "ex", "in", "enumerate", "(", "example_list", ")", ":", "\n", "        ", "self", ".", "r_batch_extend_vocab", "[", "i", ",", ":", "]", "=", "ex", ".", "reviews_extend_vocab", "[", ":", "]", "\n", "", "for", "i", ",", "ex", "in", "enumerate", "(", "example_list", ")", ":", "\n", "        ", "self", ".", "q_batch_extend_vocab", "[", "i", ",", ":", "]", "=", "ex", ".", "question_extend_vocab", "[", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Batch.init_decoder_seq": [[242, 269], ["numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "enumerate", "ex.pad_decoder_inp_targ", "range"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Example.pad_decoder_inp_targ"], ["", "", "", "def", "init_decoder_seq", "(", "self", ",", "example_list", ",", "hps", ")", ":", "\n", "    ", "\"\"\"Initializes the following:\n        self.dec_batch:\n          numpy array of shape (batch_size, max_dec_steps), containing integer ids as input for the decoder, padded to max_dec_steps length.\n        self.target_batch:\n          numpy array of shape (batch_size, max_dec_steps), containing integer ids for the target sequence, padded to max_dec_steps length.\n        self.dec_padding_mask:\n          numpy array of shape (batch_size, max_dec_steps), containing 1s and 0s. 1s correspond to real tokens in dec_batch and target_batch; 0s correspond to padding.\n        \"\"\"", "\n", "# Pad the inputs and targets", "\n", "for", "ex", "in", "example_list", ":", "\n", "      ", "ex", ".", "pad_decoder_inp_targ", "(", "hps", ".", "max_dec_steps", ",", "self", ".", "pad_id", ")", "\n", "\n", "# Initialize the numpy arrays.", "\n", "# Note: our decoder inputs and targets must be the same length for each batch (second dimension = max_dec_steps) because we do not use a dynamic_rnn for decoding. However I believe this is possible, or will soon be possible, with Tensorflow 1.0, in which case it may be best to upgrade to that.", "\n", "", "self", ".", "dec_batch", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ",", "hps", ".", "max_dec_steps", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "self", ".", "target_batch", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ",", "hps", ".", "max_dec_steps", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "self", ".", "y_target_batch", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "self", ".", "dec_padding_mask", "=", "np", ".", "zeros", "(", "(", "hps", ".", "batch_size", ",", "hps", ".", "max_dec_steps", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "# Fill in the numpy arrays", "\n", "for", "i", ",", "ex", "in", "enumerate", "(", "example_list", ")", ":", "\n", "      ", "self", ".", "dec_batch", "[", "i", ",", ":", "]", "=", "ex", ".", "dec_input", "[", ":", "]", "\n", "self", ".", "target_batch", "[", "i", ",", ":", "]", "=", "ex", ".", "target", "[", ":", "]", "\n", "self", ".", "y_target_batch", "[", "i", "]", "=", "ex", ".", "y_target", "\n", "for", "j", "in", "range", "(", "ex", ".", "dec_len", ")", ":", "\n", "        ", "self", ".", "dec_padding_mask", "[", "i", "]", "[", "j", "]", "=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Batch.store_orig_strings": [[270, 276], ["None"], "methods", ["None"], ["", "", "", "def", "store_orig_strings", "(", "self", ",", "example_list", ")", ":", "\n", "    ", "\"\"\"Store the original article and abstract strings in the Batch object\"\"\"", "\n", "self", ".", "original_reviews", "=", "[", "ex", ".", "original_reviews", "for", "ex", "in", "example_list", "]", "# list of lists", "\n", "self", ".", "original_answers", "=", "[", "ex", ".", "original_answer", "for", "ex", "in", "example_list", "]", "# list of lists", "\n", "self", ".", "original_questions", "=", "[", "ex", ".", "original_question", "for", "ex", "in", "example_list", "]", "# list of lists", "\n", "self", ".", "original_answers_sents", "=", "[", "ex", ".", "original_answer_sents", "for", "ex", "in", "example_list", "]", "# list of list of lists", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Batcher.__init__": [[283, 329], ["queue.Queue", "queue.Queue", "range", "range", "batcher.Batcher._example_q_threads.append", "batcher.Batcher._example_q_threads[].start", "batcher.Batcher._batch_q_threads.append", "batcher.Batcher._batch_q_threads[].start", "threading.Thread", "batcher.Batcher._watch_thread.start", "threading.Thread", "threading.Thread"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "data_path", ",", "vocab", ",", "hps", ",", "single_pass", ")", ":", "\n", "    ", "\"\"\"Initialize the batcher. Start threads that process the data into batches.\n\n    Args:\n      data_path: tf.Example filepattern.\n      vocab: Vocabulary object\n      hps: hyperparameters\n      single_pass: If True, run through the dataset exactly once (useful for when you want to run evaluation on the dev or test set). Otherwise generate random batches indefinitely (useful for training).\n    \"\"\"", "\n", "self", ".", "_data_path", "=", "data_path", "\n", "self", ".", "_vocab", "=", "vocab", "\n", "self", ".", "_hps", "=", "hps", "\n", "self", ".", "_single_pass", "=", "single_pass", "\n", "\n", "# Initialize a queue of Batches waiting to be used, and a queue of Examples waiting to be batched", "\n", "self", ".", "_batch_queue", "=", "Queue", ".", "Queue", "(", "self", ".", "BATCH_QUEUE_MAX", ")", "\n", "self", ".", "_example_queue", "=", "Queue", ".", "Queue", "(", "self", ".", "BATCH_QUEUE_MAX", "*", "self", ".", "_hps", ".", "batch_size", ")", "\n", "\n", "# Different settings depending on whether we're in single_pass mode or not", "\n", "if", "single_pass", ":", "\n", "      ", "self", ".", "_num_example_q_threads", "=", "1", "# just one thread, so we read through the dataset just once", "\n", "self", ".", "_num_batch_q_threads", "=", "1", "# just one thread to batch examples", "\n", "self", ".", "_bucketing_cache_size", "=", "1", "# only load one batch's worth of examples before bucketing; this essentially means no bucketing", "\n", "self", ".", "_finished_reading", "=", "False", "# this will tell us when we're finished reading the dataset", "\n", "", "else", ":", "\n", "      ", "self", ".", "_num_example_q_threads", "=", "16", "# num threads to fill example queue", "\n", "self", ".", "_num_batch_q_threads", "=", "4", "# num threads to fill batch queue", "\n", "self", ".", "_bucketing_cache_size", "=", "100", "# how many batches-worth of examples to load into cache before bucketing", "\n", "\n", "# Start the threads that load the queues", "\n", "", "self", ".", "_example_q_threads", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "self", ".", "_num_example_q_threads", ")", ":", "\n", "      ", "self", ".", "_example_q_threads", ".", "append", "(", "Thread", "(", "target", "=", "self", ".", "fill_example_queue", ")", ")", "\n", "self", ".", "_example_q_threads", "[", "-", "1", "]", ".", "daemon", "=", "True", "\n", "self", ".", "_example_q_threads", "[", "-", "1", "]", ".", "start", "(", ")", "\n", "", "self", ".", "_batch_q_threads", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "self", ".", "_num_batch_q_threads", ")", ":", "\n", "      ", "self", ".", "_batch_q_threads", ".", "append", "(", "Thread", "(", "target", "=", "self", ".", "fill_batch_queue", ")", ")", "\n", "self", ".", "_batch_q_threads", "[", "-", "1", "]", ".", "daemon", "=", "True", "\n", "self", ".", "_batch_q_threads", "[", "-", "1", "]", ".", "start", "(", ")", "\n", "\n", "# Start a thread that watches the other threads and restarts them if they're dead", "\n", "", "if", "not", "single_pass", ":", "# We don't want a watcher in single_pass mode because the threads shouldn't run forever", "\n", "      ", "self", ".", "_watch_thread", "=", "Thread", "(", "target", "=", "self", ".", "watch_threads", ")", "\n", "self", ".", "_watch_thread", ".", "daemon", "=", "True", "\n", "self", ".", "_watch_thread", ".", "start", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Batcher.next_batch": [[331, 349], ["batcher.Batcher._batch_queue.get", "batcher.Batcher._batch_queue.qsize", "tensorflow.compat.v1.logging.warning", "print", "batcher.Batcher._batch_queue.qsize", "batcher.Batcher._example_queue.qsize", "tensorflow.logging.info"], "methods", ["None"], ["", "", "def", "next_batch", "(", "self", ")", ":", "\n", "    ", "\"\"\"Return a Batch from the batch queue.\n\n    If mode='decode' then each batch contains a single example repeated beam_size-many times; this is necessary for beam search.\n\n    Returns:\n      batch: a Batch object, or None if we're in single_pass mode and we've exhausted the dataset.\n    \"\"\"", "\n", "# If the batch queue is empty, print a warning", "\n", "if", "self", ".", "_batch_queue", ".", "qsize", "(", ")", "==", "0", ":", "\n", "      ", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "warning", "(", "'Bucket input queue is empty when calling next_batch. Bucket queue size: %i, Input queue size: %i'", ",", "self", ".", "_batch_queue", ".", "qsize", "(", ")", ",", "self", ".", "_example_queue", ".", "qsize", "(", ")", ")", "\n", "print", "(", "self", ".", "_single_pass", ",", "self", ".", "_finished_reading", ")", "\n", "if", "self", ".", "_single_pass", "and", "self", ".", "_finished_reading", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"Finished reading dataset in single_pass mode.\"", ")", "\n", "return", "None", "\n", "\n", "", "", "batch", "=", "self", ".", "_batch_queue", ".", "get", "(", ")", "# get the next Batch", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Batcher.fill_example_queue": [[350, 371], ["batcher.Batcher.text_generator", "data.example_generator", "batcher.Example", "batcher.Batcher._example_queue.put", "next", "sent.strip", "tensorflow.logging.info", "data.abstract2sents", "tensorflow.logging.info", "Exception"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Batcher.text_generator", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.example_generator", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.abstract2sents"], ["", "def", "fill_example_queue", "(", "self", ")", ":", "\n", "    ", "\"\"\"Reads data from file and processes into Examples which are then placed into the example queue.\"\"\"", "\n", "\n", "input_gen", "=", "self", ".", "text_generator", "(", "data", ".", "example_generator", "(", "self", ".", "_data_path", ",", "self", ".", "_single_pass", ")", ")", "\n", "\n", "while", "True", ":", "\n", "      ", "try", ":", "\n", "        ", "(", "reviews", ",", "ratings", ",", "answer", ",", "question", ",", "label", ")", "=", "next", "(", "input_gen", ")", "# read the next example from file. article and abstract are both strings.", "\n", "#except StopIteration: # if there are no more examples:", "\n", "", "except", "Exception", "as", "e", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"The example generator for this example queue filling thread has exhausted data.\"", ")", "\n", "if", "self", ".", "_single_pass", ":", "\n", "          ", "tf", ".", "logging", ".", "info", "(", "\"single_pass mode is on, so we've finished reading dataset. This thread is stopping.\"", ")", "\n", "self", ".", "_finished_reading", "=", "True", "\n", "break", "\n", "", "else", ":", "\n", "          ", "raise", "Exception", "(", "\"single_pass mode is off but the example generator is out of data; error.\"", ")", "\n", "\n", "", "", "answer_sentences", "=", "[", "sent", ".", "strip", "(", ")", "for", "sent", "in", "data", ".", "abstract2sents", "(", "answer", ")", "]", "# Use the <s> and </s> tags in abstract to get a list of sentences.", "\n", "example", "=", "Example", "(", "reviews", ",", "ratings", ",", "answer_sentences", ",", "question", ",", "label", ",", "self", ".", "_vocab", ",", "self", ".", "_hps", ")", "# Process into an Example.", "\n", "self", ".", "_example_queue", ".", "put", "(", "example", ")", "# place the Example in the example queue.", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Batcher.fill_batch_queue": [[373, 399], ["range", "sorted", "range", "batcher.Batcher._example_queue.get", "batcher.Batcher._batch_queue.put", "sorted.append", "len", "batches.append", "random.shuffle", "batcher.Batcher._batch_queue.put", "batcher.Batch", "batcher.Batcher._example_queue.get", "batcher.Batch", "range"], "methods", ["None"], ["", "", "def", "fill_batch_queue", "(", "self", ")", ":", "\n", "    ", "\"\"\"Takes Examples out of example queue, sorts them by encoder sequence length, processes into Batches and places them in the batch queue.\n\n    In decode mode, makes batches that each contain a single example repeated.\n    \"\"\"", "\n", "while", "True", ":", "\n", "      ", "if", "self", ".", "_hps", ".", "mode", "!=", "'decode'", ":", "\n", "# Get bucketing_cache_size-many batches of Examples into a list, then sort", "\n", "        ", "inputs", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "self", ".", "_hps", ".", "batch_size", "*", "self", ".", "_bucketing_cache_size", ")", ":", "\n", "          ", "inputs", ".", "append", "(", "self", ".", "_example_queue", ".", "get", "(", ")", ")", "\n", "", "inputs", "=", "sorted", "(", "inputs", ",", "key", "=", "lambda", "inp", ":", "inp", ".", "q_lens", ")", "# sort by length of encoder sequence", "\n", "\n", "# Group the sorted Examples into batches, optionally shuffle the batches, and place in the batch queue.", "\n", "batches", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "inputs", ")", ",", "self", ".", "_hps", ".", "batch_size", ")", ":", "\n", "          ", "batches", ".", "append", "(", "inputs", "[", "i", ":", "i", "+", "self", ".", "_hps", ".", "batch_size", "]", ")", "\n", "", "if", "not", "self", ".", "_single_pass", ":", "\n", "          ", "shuffle", "(", "batches", ")", "\n", "", "for", "b", "in", "batches", ":", "# each b is a list of Example objects", "\n", "          ", "self", ".", "_batch_queue", ".", "put", "(", "Batch", "(", "b", ",", "self", ".", "_hps", ",", "self", ".", "_vocab", ")", ")", "\n", "\n", "", "", "else", ":", "# beam search decode mode", "\n", "        ", "ex", "=", "self", ".", "_example_queue", ".", "get", "(", ")", "\n", "b", "=", "[", "ex", "for", "_", "in", "range", "(", "self", ".", "_hps", ".", "batch_size", ")", "]", "\n", "self", ".", "_batch_queue", ".", "put", "(", "Batch", "(", "b", ",", "self", ".", "_hps", ",", "self", ".", "_vocab", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Batcher.watch_threads": [[401, 419], ["time.sleep", "enumerate", "enumerate", "t.is_alive", "tensorflow.logging.error", "threading.Thread", "threading.Thread.start", "t.is_alive", "tensorflow.logging.error", "threading.Thread", "threading.Thread.start"], "methods", ["None"], ["", "", "", "def", "watch_threads", "(", "self", ")", ":", "\n", "    ", "\"\"\"Watch example queue and batch queue threads and restart if dead.\"\"\"", "\n", "while", "True", ":", "\n", "      ", "time", ".", "sleep", "(", "60", ")", "\n", "for", "idx", ",", "t", "in", "enumerate", "(", "self", ".", "_example_q_threads", ")", ":", "\n", "        ", "if", "not", "t", ".", "is_alive", "(", ")", ":", "# if the thread is dead", "\n", "          ", "tf", ".", "logging", ".", "error", "(", "'Found example queue thread dead. Restarting.'", ")", "\n", "new_t", "=", "Thread", "(", "target", "=", "self", ".", "fill_example_queue", ")", "\n", "self", ".", "_example_q_threads", "[", "idx", "]", "=", "new_t", "\n", "new_t", ".", "daemon", "=", "True", "\n", "new_t", ".", "start", "(", ")", "\n", "", "", "for", "idx", ",", "t", "in", "enumerate", "(", "self", ".", "_batch_q_threads", ")", ":", "\n", "        ", "if", "not", "t", ".", "is_alive", "(", ")", ":", "# if the thread is dead", "\n", "          ", "tf", ".", "logging", ".", "error", "(", "'Found batch queue thread dead. Restarting.'", ")", "\n", "new_t", "=", "Thread", "(", "target", "=", "self", ".", "fill_batch_queue", ")", "\n", "self", ".", "_batch_q_threads", "[", "idx", "]", "=", "new_t", "\n", "new_t", ".", "daemon", "=", "True", "\n", "new_t", ".", "start", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.batcher.Batcher.text_generator": [[421, 441], ["next", "eval", "eval", "next.features.feature[].bytes_list.value[].decode", "next.features.feature[].bytes_list.value[].decode", "next.features.feature[].bytes_list.value[].decode", "len", "tensorflow.logging.warning", "next.features.feature[].bytes_list.value[].decode", "next.features.feature[].bytes_list.value[].decode", "tensorflow.logging.error"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.BeamSearchDecoder.decode", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.BeamSearchDecoder.decode", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.BeamSearchDecoder.decode", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.BeamSearchDecoder.decode", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.decode.BeamSearchDecoder.decode"], ["", "", "", "", "def", "text_generator", "(", "self", ",", "example_generator", ")", ":", "\n", "    ", "\"\"\"Generates article and abstract text from tf.Example.\n\n    Args:\n      example_generator: a generator of tf.Examples from file. See data.example_generator\"\"\"", "\n", "while", "True", ":", "\n", "      ", "e", "=", "next", "(", "example_generator", ")", "# e is a tf.Example", "\n", "try", ":", "\n", "        ", "reviews", "=", "eval", "(", "e", ".", "features", ".", "feature", "[", "'reviews'", "]", ".", "bytes_list", ".", "value", "[", "0", "]", ".", "decode", "(", ")", ")", "# the article text was saved under the key 'article' in the data files", "\n", "ratings", "=", "eval", "(", "e", ".", "features", ".", "feature", "[", "'ratings'", "]", ".", "bytes_list", ".", "value", "[", "0", "]", ".", "decode", "(", ")", ")", "\n", "answer_text", "=", "e", ".", "features", ".", "feature", "[", "'answer'", "]", ".", "bytes_list", ".", "value", "[", "0", "]", ".", "decode", "(", ")", "# the abstract text was saved under the key 'abstract' in the data files", "\n", "question_text", "=", "e", ".", "features", ".", "feature", "[", "'question'", "]", ".", "bytes_list", ".", "value", "[", "0", "]", ".", "decode", "(", ")", "# the question text was saved under the key 'question' in the data files", "\n", "answer_label", "=", "e", ".", "features", ".", "feature", "[", "'label'", "]", ".", "bytes_list", ".", "value", "[", "0", "]", ".", "decode", "(", ")", "# the answer label was saved under the key 'label' in the data files", "\n", "", "except", "ValueError", ":", "\n", "        ", "tf", ".", "logging", ".", "error", "(", "'Failed to get article or abstract from example'", ")", "\n", "continue", "\n", "", "if", "len", "(", "question_text", ")", "==", "0", ":", "\n", "        ", "tf", ".", "logging", ".", "warning", "(", "'Found an example with empty article text. Skipping it.'", ")", "\n", "", "else", ":", "\n", "        ", "yield", "(", "reviews", ",", "ratings", ",", "answer_text", ",", "question_text", ",", "answer_label", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.__init__": [[44, 80], ["print", "open", "line.split", "len", "print", "Exception", "Exception", "print"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "max_size", ")", ":", "\n", "    ", "\"\"\"Creates a vocab of up to max_size words, reading from the vocab_file. If max_size is 0, reads the entire vocab file.\n\n    Args:\n      vocab_file: path to the vocab file, which is assumed to contain \"<word> <frequency>\" on each line, sorted with most frequent word first. This code doesn't actually use the frequencies, though.\n      max_size: integer. The maximum size of the resulting Vocabulary.\"\"\"", "\n", "self", ".", "_word_to_id", "=", "{", "}", "\n", "self", ".", "_id_to_word", "=", "{", "}", "\n", "self", ".", "_count", "=", "0", "# keeps track of total number of words in the Vocab", "\n", "\n", "# [UNK], [PAD], [START] and [STOP] get the ids 0,1,2,3.", "\n", "for", "w", "in", "[", "UNKNOWN_TOKEN", ",", "PAD_TOKEN", ",", "START_DECODING", ",", "STOP_DECODING", "]", ":", "\n", "      ", "self", ".", "_word_to_id", "[", "w", "]", "=", "self", ".", "_count", "\n", "self", ".", "_id_to_word", "[", "self", ".", "_count", "]", "=", "w", "\n", "self", ".", "_count", "+=", "1", "\n", "\n", "# Read the vocab file and add words up to max_size", "\n", "", "with", "open", "(", "vocab_file", ",", "'r'", ")", "as", "vocab_f", ":", "\n", "      ", "for", "line", "in", "vocab_f", ":", "\n", "        ", "pieces", "=", "line", ".", "split", "(", ")", "\n", "if", "len", "(", "pieces", ")", "!=", "2", ":", "\n", "          ", "print", "(", "'Warning: incorrectly formatted line in vocabulary file: %s\\n'", "%", "line", ")", "\n", "continue", "\n", "", "w", "=", "pieces", "[", "0", "]", "\n", "if", "w", "in", "[", "SENTENCE_START", ",", "SENTENCE_END", ",", "UNKNOWN_TOKEN", ",", "PAD_TOKEN", ",", "START_DECODING", ",", "STOP_DECODING", "]", ":", "\n", "          ", "raise", "Exception", "(", "'<s>, </s>, [UNK], [PAD], [START] and [STOP] shouldn\\'t be in the vocab file, but %s is'", "%", "w", ")", "\n", "", "if", "w", "in", "self", ".", "_word_to_id", ":", "\n", "          ", "raise", "Exception", "(", "'Duplicated word in vocabulary file: %s'", "%", "w", ")", "\n", "", "self", ".", "_word_to_id", "[", "w", "]", "=", "self", ".", "_count", "\n", "self", ".", "_id_to_word", "[", "self", ".", "_count", "]", "=", "w", "\n", "self", ".", "_count", "+=", "1", "\n", "if", "max_size", "!=", "0", "and", "self", ".", "_count", ">=", "max_size", ":", "\n", "          ", "print", "(", "\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\"", "%", "(", "max_size", ",", "self", ".", "_count", ")", ")", "\n", "break", "\n", "\n", "", "", "", "print", "(", "\"Finished constructing vocabulary of %i total words. Last word added: %s\"", "%", "(", "self", ".", "_count", ",", "self", ".", "_id_to_word", "[", "self", ".", "_count", "-", "1", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id": [[81, 86], ["None"], "methods", ["None"], ["", "def", "word2id", "(", "self", ",", "word", ")", ":", "\n", "    ", "\"\"\"Returns the id (integer) of a word (string). Returns [UNK] id if word is OOV.\"\"\"", "\n", "if", "word", "not", "in", "self", ".", "_word_to_id", ":", "\n", "      ", "return", "self", ".", "_word_to_id", "[", "UNKNOWN_TOKEN", "]", "\n", "", "return", "self", ".", "_word_to_id", "[", "word", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.id2word": [[87, 92], ["ValueError"], "methods", ["None"], ["", "def", "id2word", "(", "self", ",", "word_id", ")", ":", "\n", "    ", "\"\"\"Returns the word (string) corresponding to an id (integer).\"\"\"", "\n", "if", "word_id", "not", "in", "self", ".", "_id_to_word", ":", "\n", "      ", "raise", "ValueError", "(", "'Id not found in vocab: %d'", "%", "word_id", ")", "\n", "", "return", "self", ".", "_id_to_word", "[", "word_id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.size": [[93, 96], ["None"], "methods", ["None"], ["", "def", "size", "(", "self", ")", ":", "\n", "    ", "\"\"\"Returns the total size of the vocabulary\"\"\"", "\n", "return", "self", ".", "_count", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.write_metadata": [[97, 110], ["print", "open", "csv.DictWriter", "range", "data.Vocab.size", "csv.DictWriter.writerow"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.size"], ["", "def", "write_metadata", "(", "self", ",", "fpath", ")", ":", "\n", "    ", "\"\"\"Writes metadata file for Tensorboard word embedding visualizer as described here:\n      https://www.tensorflow.org/get_started/embedding_viz\n\n    Args:\n      fpath: place to write the metadata file\n    \"\"\"", "\n", "print", "(", "\"Writing word embedding metadata file to %s...\"", "%", "(", "fpath", ")", ")", "\n", "with", "open", "(", "fpath", ",", "\"w\"", ")", "as", "f", ":", "\n", "      ", "fieldnames", "=", "[", "'word'", "]", "\n", "writer", "=", "csv", ".", "DictWriter", "(", "f", ",", "delimiter", "=", "\"\\t\"", ",", "fieldnames", "=", "fieldnames", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "size", "(", ")", ")", ":", "\n", "        ", "writer", ".", "writerow", "(", "{", "\"word\"", ":", "self", ".", "_id_to_word", "[", "i", "]", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.example_generator": [[112, 146], ["glob.glob", "sorted", "random.shuffle", "open", "print", "open.read", "struct.unpack", "struct.unpack", "tensorflow.core.example.example_pb2.Example.FromString", "open.read"], "function", ["None"], ["", "", "", "", "def", "example_generator", "(", "data_path", ",", "single_pass", ")", ":", "\n", "  ", "\"\"\"Generates tf.Examples from data files.\n\n    Binary data format: <length><blob>. <length> represents the byte size\n    of <blob>. <blob> is serialized tf.Example proto. The tf.Example contains\n    the tokenized article text and summary.\n\n  Args:\n    data_path:\n      Path to tf.Example data files. Can include wildcards, e.g. if you have several training data chunk files train_001.bin, train_002.bin, etc, then pass data_path=train_* to access them all.\n    single_pass:\n      Boolean. If True, go through the dataset exactly once, generating examples in the order they appear, then return. Otherwise, generate random examples indefinitely.\n\n  Yields:\n    Deserialized tf.Example.\n  \"\"\"", "\n", "while", "True", ":", "\n", "    ", "filelist", "=", "glob", ".", "glob", "(", "data_path", ")", "# get the list of datafiles", "\n", "assert", "filelist", ",", "(", "'Error: Empty filelist at %s'", "%", "data_path", ")", "# check filelist isn't empty", "\n", "if", "single_pass", ":", "\n", "      ", "filelist", "=", "sorted", "(", "filelist", ")", "\n", "", "else", ":", "\n", "      ", "random", ".", "shuffle", "(", "filelist", ")", "\n", "", "for", "f", "in", "filelist", ":", "\n", "      ", "reader", "=", "open", "(", "f", ",", "'rb'", ")", "\n", "while", "True", ":", "\n", "        ", "len_bytes", "=", "reader", ".", "read", "(", "8", ")", "\n", "if", "not", "len_bytes", ":", "break", "# finished reading this file", "\n", "str_len", "=", "struct", ".", "unpack", "(", "'q'", ",", "len_bytes", ")", "[", "0", "]", "\n", "example_str", "=", "struct", ".", "unpack", "(", "'%ds'", "%", "str_len", ",", "reader", ".", "read", "(", "str_len", ")", ")", "[", "0", "]", "\n", "yield", "example_pb2", ".", "Example", ".", "FromString", "(", "example_str", ")", "\n", "", "", "if", "single_pass", ":", "\n", "      ", "print", "(", "\"example_generator completed reading all datafiles. No more data.\"", ")", "\n", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.article2ids": [[148, 172], ["vocab.word2id", "vocab.word2id", "oovs.index", "ids.append", "ids.append", "oovs.append", "vocab.size"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.size"], ["", "", "", "def", "article2ids", "(", "article_words", ",", "vocab", ",", "oovs", ")", ":", "\n", "  ", "\"\"\"Map the article words to their ids. Also return a list of OOVs in the article.\n\n  Args:\n    article_words: list of words (strings)\n    vocab: Vocabulary object\n\n  Returns:\n    ids:\n      A list of word ids (integers); OOVs are represented by their temporary article OOV number. If the vocabulary size is 50k and the article has 3 OOVs, then these temporary OOV numbers will be 50000, 50001, 50002.\n    oovs:\n      A list of the OOV words in the article (strings), in the order corresponding to their temporary article OOV numbers.\"\"\"", "\n", "ids", "=", "[", "]", "\n", "unk_id", "=", "vocab", ".", "word2id", "(", "UNKNOWN_TOKEN", ")", "\n", "for", "w", "in", "article_words", ":", "\n", "    ", "i", "=", "vocab", ".", "word2id", "(", "w", ")", "\n", "if", "i", "==", "unk_id", ":", "# If w is OOV", "\n", "      ", "if", "w", "not", "in", "oovs", ":", "# Add to list of OOVs", "\n", "        ", "oovs", ".", "append", "(", "w", ")", "\n", "", "oov_num", "=", "oovs", ".", "index", "(", "w", ")", "# This is 0 for the first article OOV, 1 for the second article OOV...", "\n", "ids", ".", "append", "(", "vocab", ".", "size", "(", ")", "+", "oov_num", ")", "# This is e.g. 50000 for the first article OOV, 50001 for the second...", "\n", "", "else", ":", "\n", "      ", "ids", ".", "append", "(", "i", ")", "\n", "", "", "return", "ids", ",", "oovs", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.abstract2ids": [[174, 197], ["vocab.word2id", "vocab.word2id", "ids.append", "ids.append", "ids.append", "vocab.size", "article_oovs.index"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.size"], ["", "def", "abstract2ids", "(", "abstract_words", ",", "vocab", ",", "article_oovs", ")", ":", "\n", "  ", "\"\"\"Map the abstract words to their ids. In-article OOVs are mapped to their temporary OOV numbers.\n\n  Args:\n    abstract_words: list of words (strings)\n    vocab: Vocabulary object\n    article_oovs: list of in-article OOV words (strings), in the order corresponding to their temporary article OOV numbers\n\n  Returns:\n    ids: List of ids (integers). In-article OOV words are mapped to their temporary OOV numbers. Out-of-article OOV words are mapped to the UNK token id.\"\"\"", "\n", "ids", "=", "[", "]", "\n", "unk_id", "=", "vocab", ".", "word2id", "(", "UNKNOWN_TOKEN", ")", "\n", "for", "w", "in", "abstract_words", ":", "\n", "    ", "i", "=", "vocab", ".", "word2id", "(", "w", ")", "\n", "if", "i", "==", "unk_id", ":", "# If w is an OOV word", "\n", "      ", "if", "w", "in", "article_oovs", ":", "# If w is an in-article OOV", "\n", "        ", "vocab_idx", "=", "vocab", ".", "size", "(", ")", "+", "article_oovs", ".", "index", "(", "w", ")", "# Map to its temporary article OOV number", "\n", "ids", ".", "append", "(", "vocab_idx", ")", "\n", "", "else", ":", "# If w is an out-of-article OOV", "\n", "        ", "ids", ".", "append", "(", "unk_id", ")", "# Map to the UNK token id", "\n", "", "", "else", ":", "\n", "      ", "ids", ".", "append", "(", "i", ")", "\n", "", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.outputids2words": [[199, 223], ["words.append", "vocab.id2word", "vocab.size", "ValueError", "len"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.id2word", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.size"], ["", "def", "outputids2words", "(", "id_list", ",", "vocab", ",", "article_oovs", ")", ":", "\n", "  ", "\"\"\"Maps output ids to words, including mapping in-article OOVs from their temporary ids to the original OOV string (applicable in pointer-generator mode).\n\n  Args:\n    id_list: list of ids (integers)\n    vocab: Vocabulary object\n    article_oovs: list of OOV words (strings) in the order corresponding to their temporary article OOV ids (that have been assigned in pointer-generator mode), or None (in baseline mode)\n\n  Returns:\n    words: list of words (strings)\n  \"\"\"", "\n", "words", "=", "[", "]", "\n", "for", "i", "in", "id_list", ":", "\n", "    ", "try", ":", "\n", "      ", "w", "=", "vocab", ".", "id2word", "(", "i", ")", "# might be [UNK]", "\n", "", "except", "ValueError", "as", "e", ":", "# w is OOV", "\n", "      ", "assert", "article_oovs", "is", "not", "None", ",", "\"Error: model produced a word ID that isn't in the vocabulary. This should not happen in baseline (no pointer-generator) mode\"", "\n", "article_oov_idx", "=", "i", "-", "vocab", ".", "size", "(", ")", "\n", "try", ":", "\n", "        ", "w", "=", "article_oovs", "[", "article_oov_idx", "]", "\n", "", "except", "ValueError", "as", "e", ":", "# i doesn't correspond to an article oov", "\n", "        ", "raise", "ValueError", "(", "'Error: model produced word ID %i which corresponds to article OOV %i but this example only has %i article OOVs'", "%", "(", "i", ",", "article_oov_idx", ",", "len", "(", "article_oovs", ")", ")", ")", "\n", "", "", "words", ".", "append", "(", "w", ")", "\n", "", "return", "words", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.abstract2sents": [[225, 244], ["abstract.index", "abstract.index", "sents.append", "len", "len"], "function", ["None"], ["", "def", "abstract2sents", "(", "abstract", ")", ":", "\n", "  ", "\"\"\"Splits abstract text from datafile into list of sentences.\n\n  Args:\n    abstract: string containing <s> and </s> tags for starts and ends of sentences\n\n  Returns:\n    sents: List of sentence strings (no tags)\"\"\"", "\n", "cur", "=", "0", "\n", "sents", "=", "[", "abstract", "]", "\n", "return", "sents", "\n", "while", "True", ":", "\n", "    ", "try", ":", "\n", "      ", "start_p", "=", "abstract", ".", "index", "(", "SENTENCE_START", ",", "cur", ")", "\n", "end_p", "=", "abstract", ".", "index", "(", "SENTENCE_END", ",", "start_p", "+", "1", ")", "\n", "cur", "=", "end_p", "+", "len", "(", "SENTENCE_END", ")", "\n", "sents", ".", "append", "(", "abstract", "[", "start_p", "+", "len", "(", "SENTENCE_START", ")", ":", "end_p", "]", ")", "\n", "", "except", "ValueError", "as", "e", ":", "# no more sentences", "\n", "      ", "return", "sents", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.show_art_oovs": [[246, 253], ["vocab.word2id", "article.split", "vocab.word2id"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id"], ["", "", "", "def", "show_art_oovs", "(", "article", ",", "vocab", ")", ":", "\n", "  ", "\"\"\"Returns the article string, highlighting the OOVs by placing __underscores__ around them\"\"\"", "\n", "unk_token", "=", "vocab", ".", "word2id", "(", "UNKNOWN_TOKEN", ")", "\n", "words", "=", "article", ".", "split", "(", "' '", ")", "\n", "words", "=", "[", "(", "\"__%s__\"", "%", "w", ")", "if", "vocab", ".", "word2id", "(", "w", ")", "==", "unk_token", "else", "w", "for", "w", "in", "words", "]", "\n", "out_str", "=", "' '", ".", "join", "(", "words", ")", "\n", "return", "out_str", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.show_abs_oovs": [[255, 281], ["vocab.word2id", "abstract.split", "vocab.word2id", "new_words.append", "new_words.append", "new_words.append", "new_words.append"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.word2id"], ["", "def", "show_abs_oovs", "(", "abstract", ",", "vocab", ",", "article_oovs", ")", ":", "\n", "  ", "\"\"\"Returns the abstract string, highlighting the article OOVs with __underscores__.\n\n  If a list of article_oovs is provided, non-article OOVs are differentiated like !!__this__!!.\n\n  Args:\n    abstract: string\n    vocab: Vocabulary object\n    article_oovs: list of words (strings), or None (in baseline mode)\n  \"\"\"", "\n", "unk_token", "=", "vocab", ".", "word2id", "(", "UNKNOWN_TOKEN", ")", "\n", "words", "=", "abstract", ".", "split", "(", "' '", ")", "\n", "new_words", "=", "[", "]", "\n", "for", "w", "in", "words", ":", "\n", "    ", "if", "vocab", ".", "word2id", "(", "w", ")", "==", "unk_token", ":", "# w is oov", "\n", "      ", "if", "article_oovs", "is", "None", ":", "# baseline mode", "\n", "        ", "new_words", ".", "append", "(", "\"__%s__\"", "%", "w", ")", "\n", "", "else", ":", "# pointer-generator mode", "\n", "        ", "if", "w", "in", "article_oovs", ":", "\n", "          ", "new_words", ".", "append", "(", "\"__%s__\"", "%", "w", ")", "\n", "", "else", ":", "\n", "          ", "new_words", ".", "append", "(", "\"!!__%s__!!\"", "%", "w", ")", "\n", "", "", "", "else", ":", "# w is in-vocab word", "\n", "      ", "new_words", ".", "append", "(", "w", ")", "\n", "", "", "out_str", "=", "' '", ".", "join", "(", "new_words", ")", "\n", "return", "out_str", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.get_init_embeddings": [[283, 304], ["gensim.test.utils.get_tmpfile", "gensim.scripts.glove2word2vec.glove2word2vec", "print", "gensim.models.keyedvectors.KeyedVectors.load_word2vec_format", "list", "sorted", "numpy.random.normal", "numpy.random.normal", "numpy.array", "str", "reversed_dict.items", "list.append", "KeyedVectors.load_word2vec_format.word_vec", "numpy.zeros"], "function", ["None"], ["", "def", "get_init_embeddings", "(", "reversed_dict", ",", "embedding_size", ")", ":", "\n", "    ", "glove_file", "=", "\"embed/glove.6B.%sd.txt\"", "%", "str", "(", "embedding_size", ")", "\n", "word2vec_file", "=", "get_tmpfile", "(", "\"word2vec_format.vec\"", ")", "\n", "glove2word2vec", "(", "glove_file", ",", "word2vec_file", ")", "\n", "print", "(", "\"Loading Glove vectors from %s\"", "%", "glove_file", ")", "\n", "word_vectors", "=", "KeyedVectors", ".", "load_word2vec_format", "(", "word2vec_file", ")", "\n", "\n", "word_vec_list", "=", "list", "(", ")", "\n", "for", "_", ",", "word", "in", "sorted", "(", "reversed_dict", ".", "items", "(", ")", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "word_vec", "=", "word_vectors", ".", "word_vec", "(", "word", ")", "\n", "", "except", "KeyError", ":", "\n", "            ", "word_vec", "=", "np", ".", "zeros", "(", "[", "embedding_size", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "", "word_vec_list", ".", "append", "(", "word_vec", ")", "\n", "\n", "# Assign random vector to <s>, </s> token", "\n", "", "word_vec_list", "[", "2", "]", "=", "np", ".", "random", ".", "normal", "(", "0", ",", "1", ",", "embedding_size", ")", "\n", "word_vec_list", "[", "3", "]", "=", "np", ".", "random", ".", "normal", "(", "0", ",", "1", ",", "embedding_size", ")", "\n", "\n", "return", "np", ".", "array", "(", "word_vec_list", ",", "dtype", "=", "np", ".", "float32", ")", "", "", ""]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel.__init__": [[32, 35], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "hps", ",", "vocab", ")", ":", "\n", "    ", "self", ".", "_hps", "=", "hps", "\n", "self", ".", "_vocab", "=", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._add_placeholders": [[36, 62], ["tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder"], "methods", ["None"], ["", "def", "_add_placeholders", "(", "self", ")", ":", "\n", "    ", "\"\"\"Add placeholders to the graph. These are entry points for any input data.\"\"\"", "\n", "hps", "=", "self", ".", "_hps", "\n", "\n", "# encoder part", "\n", "self", ".", "_r_batch", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "hps", ".", "batch_size", ",", "hps", ".", "review_num", ",", "None", "]", ",", "name", "=", "'r_batch'", ")", "\n", "self", ".", "_r_lens", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "hps", ".", "batch_size", ",", "hps", ".", "review_num", "]", ",", "name", "=", "'r_lens'", ")", "\n", "self", ".", "_q_batch", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "hps", ".", "batch_size", ",", "None", "]", ",", "name", "=", "'q_batch'", ")", "\n", "self", ".", "_q_lens", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "hps", ".", "batch_size", "]", ",", "name", "=", "'q_lens'", ")", "\n", "self", ".", "_rating_batch", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "hps", ".", "batch_size", ",", "hps", ".", "review_num", "]", ",", "name", "=", "'rating_batch'", ")", "\n", "self", ".", "_r_padding_mask", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "hps", ".", "batch_size", ",", "hps", ".", "review_num", ",", "None", "]", ",", "name", "=", "'r_padding_mask'", ")", "\n", "self", ".", "_q_padding_mask", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "hps", ".", "batch_size", ",", "None", "]", ",", "name", "=", "'q_padding_mask'", ")", "\n", "if", "FLAGS", ".", "pointer_gen", ":", "\n", "      ", "self", ".", "_r_batch_extend_vocab", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "hps", ".", "batch_size", ",", "hps", ".", "review_num", ",", "None", "]", ",", "name", "=", "'r_batch_extend_vocab'", ")", "\n", "self", ".", "_q_batch_extend_vocab", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "hps", ".", "batch_size", ",", "None", "]", ",", "name", "=", "'q_batch_extend_vocab'", ")", "\n", "self", ".", "_max_oovs", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "]", ",", "name", "=", "'max_oovs'", ")", "\n", "\n", "# decoder part", "\n", "", "self", ".", "_dec_batch", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "hps", ".", "batch_size", ",", "hps", ".", "max_dec_steps", "]", ",", "name", "=", "'dec_batch'", ")", "\n", "self", ".", "_target_batch", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "hps", ".", "batch_size", ",", "hps", ".", "max_dec_steps", "]", ",", "name", "=", "'target_batch'", ")", "\n", "self", ".", "_y_target_batch", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "hps", ".", "batch_size", "]", ",", "name", "=", "'y_target_batch'", ")", "\n", "self", ".", "_dec_padding_mask", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "hps", ".", "batch_size", ",", "hps", ".", "max_dec_steps", "]", ",", "name", "=", "'dec_padding_mask'", ")", "\n", "\n", "if", "hps", ".", "mode", "==", "\"decode\"", "and", "hps", ".", "coverage", ":", "\n", "      ", "self", ".", "q_prev_coverage", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "hps", ".", "batch_size", ",", "None", "]", ",", "name", "=", "'q_prev_coverage'", ")", "\n", "self", ".", "r_prev_coverage", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "hps", ".", "batch_size", ",", "None", "]", ",", "name", "=", "'r_prev_coverage'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._make_feed_dict": [[64, 89], ["None"], "methods", ["None"], ["", "", "def", "_make_feed_dict", "(", "self", ",", "batch", ",", "just_enc", "=", "False", ")", ":", "\n", "    ", "\"\"\"Make a feed dictionary mapping parts of the batch to the appropriate placeholders.\n\n    Args:\n      batch: Batch object\n      just_enc: Boolean. If True, only feed the parts needed for the encoder.\n    \"\"\"", "\n", "feed_dict", "=", "{", "}", "\n", "feed_dict", "[", "self", ".", "_r_batch", "]", "=", "batch", ".", "r_batch", "\n", "feed_dict", "[", "self", ".", "_r_lens", "]", "=", "batch", ".", "r_lens", "\n", "feed_dict", "[", "self", ".", "_q_batch", "]", "=", "batch", ".", "q_batch", "\n", "feed_dict", "[", "self", ".", "_q_lens", "]", "=", "batch", ".", "q_lens", "\n", "feed_dict", "[", "self", ".", "_rating_batch", "]", "=", "batch", ".", "rating_batch", "\n", "feed_dict", "[", "self", ".", "_r_padding_mask", "]", "=", "batch", ".", "r_padding_mask", "\n", "feed_dict", "[", "self", ".", "_q_padding_mask", "]", "=", "batch", ".", "q_padding_mask", "\n", "if", "FLAGS", ".", "pointer_gen", ":", "\n", "      ", "feed_dict", "[", "self", ".", "_r_batch_extend_vocab", "]", "=", "batch", ".", "r_batch_extend_vocab", "\n", "feed_dict", "[", "self", ".", "_q_batch_extend_vocab", "]", "=", "batch", ".", "q_batch_extend_vocab", "\n", "feed_dict", "[", "self", ".", "_max_oovs", "]", "=", "batch", ".", "max_oovs", "\n", "", "if", "not", "just_enc", ":", "\n", "      ", "feed_dict", "[", "self", ".", "_dec_batch", "]", "=", "batch", ".", "dec_batch", "\n", "feed_dict", "[", "self", ".", "_target_batch", "]", "=", "batch", ".", "target_batch", "\n", "feed_dict", "[", "self", ".", "_y_target_batch", "]", "=", "batch", ".", "y_target_batch", "\n", "feed_dict", "[", "self", ".", "_dec_padding_mask", "]", "=", "batch", ".", "dec_padding_mask", "\n", "", "return", "feed_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._add_encoder": [[90, 111], ["tensorflow.compat.v1.variable_scope", "tensorflow.contrib.rnn.LSTMCell", "tensorflow.contrib.rnn.DropoutWrapper", "tensorflow.contrib.rnn.LSTMCell", "tensorflow.contrib.rnn.DropoutWrapper", "tensorflow.nn.bidirectional_dynamic_rnn", "tensorflow.concat"], "methods", ["None"], ["", "def", "_add_encoder", "(", "self", ",", "encoder_inputs", ",", "seq_len", ",", "scope", ",", "dropout", "=", "0.8", ")", ":", "\n", "    ", "\"\"\"Add a single-layer bidirectional LSTM encoder to the graph.\n\n    Args:\n      encoder_inputs: A tensor of shape [batch_size, <=max_enc_steps, emb_size].\n      seq_len: Lengths of encoder_inputs (before padding). A tensor of shape [batch_size].\n\n    Returns:\n      encoder_outputs:\n        A tensor of shape [batch_size, <=max_enc_steps, 2*hidden_dim]. It's 2*hidden_dim because it's the concatenation of the forwards and backwards states.\n      fw_state, bw_state:\n        Each are LSTMStateTuples of shape ([batch_size,hidden_dim],[batch_size,hidden_dim])\n    \"\"\"", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "'encoder'", "+", "scope", ")", ":", "\n", "      ", "cell_fw", "=", "tf", ".", "contrib", ".", "rnn", ".", "LSTMCell", "(", "self", ".", "_hps", ".", "hidden_dim", ",", "initializer", "=", "self", ".", "rand_unif_init", ",", "state_is_tuple", "=", "True", ")", "\n", "cell_fw", "=", "tf", ".", "contrib", ".", "rnn", ".", "DropoutWrapper", "(", "cell_fw", ",", "output_keep_prob", "=", "dropout", ")", "\n", "cell_bw", "=", "tf", ".", "contrib", ".", "rnn", ".", "LSTMCell", "(", "self", ".", "_hps", ".", "hidden_dim", ",", "initializer", "=", "self", ".", "rand_unif_init", ",", "state_is_tuple", "=", "True", ")", "\n", "cell_bw", "=", "tf", ".", "contrib", ".", "rnn", ".", "DropoutWrapper", "(", "cell_bw", ",", "output_keep_prob", "=", "dropout", ")", "\n", "(", "encoder_outputs", ",", "(", "fw_st", ",", "bw_st", ")", ")", "=", "tf", ".", "nn", ".", "bidirectional_dynamic_rnn", "(", "cell_fw", ",", "cell_bw", ",", "encoder_inputs", ",", "dtype", "=", "tf", ".", "float32", ",", "sequence_length", "=", "seq_len", ",", "swap_memory", "=", "True", ")", "\n", "encoder_outputs", "=", "tf", ".", "concat", "(", "axis", "=", "2", ",", "values", "=", "encoder_outputs", ")", "# concatenate the forwards and backwards states", "\n", "", "return", "encoder_outputs", ",", "fw_st", ",", "bw_st", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._reduce_states": [[113, 139], ["op.get_shape", "tensorflow.compat.v1.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.concat", "tensorflow.concat", "tensorflow.nn.relu", "tensorflow.nn.relu", "tensorflow.contrib.rnn.LSTMStateTuple", "tensorflow.matmul", "tensorflow.matmul"], "methods", ["None"], ["", "def", "_reduce_states", "(", "self", ",", "fw_st", ",", "bw_st", ",", "op", ")", ":", "\n", "    ", "\"\"\"Add to the graph a linear layer to reduce the encoder's final FW and BW state into a single initial state for the decoder. This is needed because the encoder is bidirectional but the decoder is not.\n\n    Args:\n      fw_st: LSTMStateTuple with hidden_dim units.\n      bw_st: LSTMStateTuple with hidden_dim units.\n\n    Returns:\n      state: LSTMStateTuple with hidden_dim units.\n    \"\"\"", "\n", "hidden_dim", "=", "self", ".", "_hps", ".", "hidden_dim", "\n", "op_dim", "=", "op", ".", "get_shape", "(", ")", "[", "1", "]", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "'reduce_final_st'", ")", ":", "\n", "\n", "# Define weights and biases to reduce the cell and reduce the state", "\n", "      ", "w_reduce_c", "=", "tf", ".", "get_variable", "(", "'w_reduce_c'", ",", "[", "hidden_dim", "*", "2", "+", "op_dim", ",", "hidden_dim", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "initializer", "=", "self", ".", "trunc_norm_init", ")", "\n", "w_reduce_h", "=", "tf", ".", "get_variable", "(", "'w_reduce_h'", ",", "[", "hidden_dim", "*", "2", "+", "op_dim", ",", "hidden_dim", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "initializer", "=", "self", ".", "trunc_norm_init", ")", "\n", "bias_reduce_c", "=", "tf", ".", "get_variable", "(", "'bias_reduce_c'", ",", "[", "hidden_dim", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "initializer", "=", "self", ".", "trunc_norm_init", ")", "\n", "bias_reduce_h", "=", "tf", ".", "get_variable", "(", "'bias_reduce_h'", ",", "[", "hidden_dim", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "initializer", "=", "self", ".", "trunc_norm_init", ")", "\n", "\n", "# Apply linear layer", "\n", "old_c", "=", "tf", ".", "concat", "(", "axis", "=", "1", ",", "values", "=", "[", "fw_st", ".", "c", ",", "bw_st", ".", "c", ",", "op", "]", ")", "# Concatenation of fw and bw cell", "\n", "old_h", "=", "tf", ".", "concat", "(", "axis", "=", "1", ",", "values", "=", "[", "fw_st", ".", "h", ",", "bw_st", ".", "h", ",", "op", "]", ")", "# Concatenation of fw and bw state", "\n", "new_c", "=", "tf", ".", "nn", ".", "relu", "(", "tf", ".", "matmul", "(", "old_c", ",", "w_reduce_c", ")", "+", "bias_reduce_c", ")", "# Get new cell from old cell", "\n", "new_h", "=", "tf", ".", "nn", ".", "relu", "(", "tf", ".", "matmul", "(", "old_h", ",", "w_reduce_h", ")", "+", "bias_reduce_h", ")", "# Get new state from old state", "\n", "return", "tf", ".", "contrib", ".", "rnn", ".", "LSTMStateTuple", "(", "new_c", ",", "new_h", ")", "# Return new cell and state", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._add_decoder": [[141, 163], ["tensorflow.contrib.rnn.LSTMCell", "attention_decoder.attention_decoder.attention_decoder"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.attention_decoder.attention_decoder"], ["", "", "def", "_add_decoder", "(", "self", ",", "inputs", ")", ":", "\n", "    ", "\"\"\"Add attention decoder to the graph. In train or eval mode, you call this once to get output on ALL steps. In decode (beam search) mode, you call this once for EACH decoder step.\n\n    Args:\n      inputs: inputs to the decoder (word embeddings). A list of tensors shape (batch_size, emb_dim)\n\n    Returns:\n      outputs: List of tensors; the outputs of the decoder\n      out_state: The final state of the decoder\n      attn_dists: A list of tensors; the attention distributions\n      p_gens: A list of scalar tensors; the generation probabilities\n      coverage: A tensor, the current coverage vector\n    \"\"\"", "\n", "hps", "=", "self", ".", "_hps", "\n", "cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "LSTMCell", "(", "hps", ".", "hidden_dim", ",", "state_is_tuple", "=", "True", ",", "initializer", "=", "self", ".", "rand_unif_init", ")", "\n", "\n", "q_prev_coverage", "=", "self", ".", "q_prev_coverage", "if", "hps", ".", "mode", "==", "\"decode\"", "and", "hps", ".", "coverage", "else", "None", "# In decode mode, we run attention_decoder one step at a time and so need to pass in the previous step's coverage vector each time", "\n", "r_prev_coverage", "=", "self", ".", "r_prev_coverage", "if", "hps", ".", "mode", "==", "\"decode\"", "and", "hps", ".", "coverage", "else", "None", "\n", "\n", "outputs", ",", "output_states", ",", "out_state", ",", "q_attn_dists", ",", "r_attn_dists", ",", "p_gens", ",", "q_coverage", ",", "r_coverage", "=", "attention_decoder", "(", "inputs", ",", "self", ".", "_dec_in_state", ",", "self", ".", "dec_question_inputs", ",", "self", ".", "dec_review_inputs", ",", "self", ".", "review_attention", ",", "hps", ".", "review_num", ",", "self", ".", "_q_padding_mask", ",", "self", ".", "_r_padding_mask", ",", "cell", ",", "initial_state_attention", "=", "(", "hps", ".", "mode", "==", "\"decode\"", ")", ",", "pointer_gen", "=", "hps", ".", "pointer_gen", ",", "use_coverage", "=", "hps", ".", "coverage", ",", "q_prev_coverage", "=", "q_prev_coverage", ",", "r_prev_coverage", "=", "r_prev_coverage", ")", "\n", "\n", "return", "outputs", ",", "output_states", ",", "out_state", ",", "q_attn_dists", ",", "r_attn_dists", ",", "p_gens", ",", "q_coverage", ",", "r_coverage", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._calc_final_dist": [[164, 208], ["tensorflow.compat.v1.variable_scope", "tensorflow.zeros", "tensorflow.layers.flatten", "tensorflow.range", "tensorflow.expand_dims", "tensorflow.tile", "tensorflow.tile", "tensorflow.stack", "tensorflow.stack", "model.SummarizationModel._vocab.size", "tensorflow.concat", "tensorflow.shape", "tensorflow.scatter_nd", "tensorflow.scatter_nd", "zip", "zip", "zip", "tensorflow.shape", "tensorflow.shape", "zip"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.size"], ["", "def", "_calc_final_dist", "(", "self", ",", "vocab_dists", ",", "q_attn_dists", ",", "r_attn_dists", ")", ":", "\n", "    ", "\"\"\"Calculate the final distribution, for the pointer-generator model\n\n    Args:\n      vocab_dists: The vocabulary distributions. List length max_dec_steps of (batch_size, vsize) arrays. The words are in the order they appear in the vocabulary file.\n      attn_dists: The attention distributions. List length max_dec_steps of (batch_size, attn_len) arrays\n\n    Returns:\n      final_dists: The final distributions. List length max_dec_steps of (batch_size, extended_vsize) arrays.\n    \"\"\"", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "'final_distribution'", ")", ":", "\n", "# Multiply vocab dists by p_gen and attention dists by (1-p_gen)", "\n", "      ", "vocab_dists", "=", "[", "p_gen", "[", "0", "]", "*", "dist", "for", "(", "p_gen", ",", "dist", ")", "in", "zip", "(", "self", ".", "p_gens", ",", "vocab_dists", ")", "]", "\n", "q_attn_dists", "=", "[", "p_gen", "[", "1", "]", "*", "dist", "for", "(", "p_gen", ",", "dist", ")", "in", "zip", "(", "self", ".", "p_gens", ",", "q_attn_dists", ")", "]", "\n", "r_attn_dists", "=", "[", "p_gen", "[", "2", "]", "*", "dist", "for", "(", "p_gen", ",", "dist", ")", "in", "zip", "(", "self", ".", "p_gens", ",", "r_attn_dists", ")", "]", "\n", "\n", "# Concatenate some zeros to each vocabulary dist, to hold the probabilities for in-article OOV words", "\n", "extended_vsize", "=", "self", ".", "_vocab", ".", "size", "(", ")", "+", "self", ".", "_max_oovs", "# the maximum (over the batch) size of the extended vocabulary", "\n", "extra_zeros", "=", "tf", ".", "zeros", "(", "(", "self", ".", "_hps", ".", "batch_size", ",", "self", ".", "_max_oovs", ")", ")", "\n", "vocab_dists_extended", "=", "[", "tf", ".", "concat", "(", "axis", "=", "1", ",", "values", "=", "[", "dist", ",", "extra_zeros", "]", ")", "for", "dist", "in", "vocab_dists", "]", "# list length max_dec_steps of shape (batch_size, extended_vsize)", "\n", "\n", "# Project the values in the attention distributions onto the appropriate entries in the final distributions", "\n", "# This means that if a_i = 0.1 and the ith encoder word is w, and w has index 500 in the vocabulary, then we add 0.1 onto the 500th entry of the final distribution", "\n", "# This is done for each decoder timestep.", "\n", "# This is fiddly; we use tf.scatter_nd to do the projection", "\n", "r_batch_extend_vocab", "=", "tf", ".", "layers", ".", "flatten", "(", "self", ".", "_r_batch_extend_vocab", ")", "\n", "batch_nums", "=", "tf", ".", "range", "(", "0", ",", "limit", "=", "self", ".", "_hps", ".", "batch_size", ")", "# shape (batch_size)", "\n", "batch_nums", "=", "tf", ".", "expand_dims", "(", "batch_nums", ",", "1", ")", "# shape (batch_size, 1)", "\n", "r_attn_len", "=", "tf", ".", "shape", "(", "self", ".", "_r_batch_extend_vocab", ")", "[", "1", "]", "*", "tf", ".", "shape", "(", "self", ".", "_r_batch_extend_vocab", ")", "[", "2", "]", "# number of states we attend over", "\n", "q_attn_len", "=", "tf", ".", "shape", "(", "self", ".", "_q_batch_extend_vocab", ")", "[", "1", "]", "\n", "r_batch_nums", "=", "tf", ".", "tile", "(", "batch_nums", ",", "[", "1", ",", "r_attn_len", "]", ")", "# shape (batch_size, attn_len)", "\n", "q_batch_nums", "=", "tf", ".", "tile", "(", "batch_nums", ",", "[", "1", ",", "q_attn_len", "]", ")", "\n", "r_indices", "=", "tf", ".", "stack", "(", "(", "r_batch_nums", ",", "r_batch_extend_vocab", ")", ",", "axis", "=", "2", ")", "# shape (batch_size, enc_t, 2)", "\n", "q_indices", "=", "tf", ".", "stack", "(", "(", "q_batch_nums", ",", "self", ".", "_q_batch_extend_vocab", ")", ",", "axis", "=", "2", ")", "\n", "shape", "=", "[", "self", ".", "_hps", ".", "batch_size", ",", "extended_vsize", "]", "\n", "r_attn_dists_projected", "=", "[", "tf", ".", "scatter_nd", "(", "r_indices", ",", "copy_dist", ",", "shape", ")", "for", "copy_dist", "in", "r_attn_dists", "]", "# list length max_dec_steps (batch_size, extended_vsize)", "\n", "q_attn_dists_projected", "=", "[", "tf", ".", "scatter_nd", "(", "q_indices", ",", "copy_dist", ",", "shape", ")", "for", "copy_dist", "in", "q_attn_dists", "]", "\n", "\n", "# Add the vocab distributions and the copy distributions together to get the final distributions", "\n", "# final_dists is a list length max_dec_steps; each entry is a tensor shape (batch_size, extended_vsize) giving the final distribution for that decoder timestep", "\n", "# Note that for decoder timesteps and examples corresponding to a [PAD] token, this is junk - ignore.", "\n", "final_dists", "=", "[", "vocab_dist", "+", "r_copy_dist", "+", "q_copy_dist", "for", "(", "vocab_dist", ",", "r_copy_dist", ",", "q_copy_dist", ")", "in", "zip", "(", "vocab_dists_extended", ",", "r_attn_dists_projected", ",", "q_attn_dists_projected", ")", "]", "\n", "\n", "return", "final_dists", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._add_emb_vis": [[209, 222], ["os.path.join", "os.path.join", "model.SummarizationModel._vocab.write_metadata", "tensorflow.summary.FileWriter", "tensorflow.contrib.tensorboard.plugins.projector.ProjectorConfig", "tensorflow.contrib.tensorboard.plugins.projector.ProjectorConfig.embeddings.add", "tensorflow.contrib.tensorboard.plugins.projector.visualize_embeddings"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.write_metadata"], ["", "", "def", "_add_emb_vis", "(", "self", ",", "embedding_var", ")", ":", "\n", "    ", "\"\"\"Do setup so that we can view word embedding visualization in Tensorboard, as described here:\n    https://www.tensorflow.org/get_started/embedding_viz\n    Make the vocab metadata file, then make the projector config file pointing to it.\"\"\"", "\n", "train_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "log_root", ",", "\"train\"", ")", "\n", "vocab_metadata_path", "=", "os", ".", "path", ".", "join", "(", "train_dir", ",", "\"vocab_metadata.tsv\"", ")", "\n", "self", ".", "_vocab", ".", "write_metadata", "(", "vocab_metadata_path", ")", "# write metadata file", "\n", "summary_writer", "=", "tf", ".", "summary", ".", "FileWriter", "(", "train_dir", ")", "\n", "config", "=", "projector", ".", "ProjectorConfig", "(", ")", "\n", "embedding", "=", "config", ".", "embeddings", ".", "add", "(", ")", "\n", "embedding", ".", "tensor_name", "=", "embedding_var", ".", "name", "\n", "embedding", ".", "metadata_path", "=", "vocab_metadata_path", "\n", "projector", ".", "visualize_embeddings", "(", "summary_writer", ",", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._co_attention": [[224, 267], ["int", "int", "tensorflow.get_variable", "tensorflow.expand_dims", "tensorflow.tile", "tensorflow.expand_dims", "tensorflow.tile", "tensorflow.reshape", "tensorflow.einsum", "tensorflow.matmul", "tensorflow.nn.softmax", "tensorflow.reshape", "tensorflow.reduce_sum", "tensorflow.expand_dims", "tensorflow.nn.softmax", "tensorflow.reshape", "tensorflow.reduce_sum", "tensorflow.expand_dims", "tensorflow.multiply", "tensorflow.multiply", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reduce_mean", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.concat", "tensorflow.reshape", "tensorflow.transpose", "tensorflow.reduce_max", "tensorflow.reshape", "tensorflow.reduce_max", "tensorflow.reshape", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reshape.get_shape", "reviews.get_shape", "tensorflow.contrib.layers.xavier_initializer"], "methods", ["None"], ["", "def", "_co_attention", "(", "self", ",", "question", ",", "reviews", ",", "batch_size", ",", "num", ")", ":", "\n", "    ", "dim1", "=", "int", "(", "question", ".", "get_shape", "(", ")", "[", "2", "]", ")", "\n", "dim2", "=", "int", "(", "reviews", ".", "get_shape", "(", ")", "[", "2", "]", ")", "\n", "U", "=", "tf", ".", "get_variable", "(", "\n", "\"U\"", ",", "\n", "shape", "=", "[", "dim1", ",", "dim2", "]", ",", "\n", "initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer", "(", ")", ")", "\n", "\n", "question", "=", "tf", ".", "expand_dims", "(", "question", ",", "1", ")", "\n", "question", "=", "tf", ".", "tile", "(", "question", ",", "(", "1", ",", "num", ",", "1", ",", "1", ")", ")", "\n", "q_padding_mask", "=", "tf", ".", "expand_dims", "(", "self", ".", "_q_padding_mask", ",", "1", ")", "\n", "q_padding_mask", "=", "tf", ".", "tile", "(", "q_padding_mask", ",", "(", "1", ",", "num", ",", "1", ")", ")", "\n", "\n", "question", "=", "tf", ".", "reshape", "(", "question", ",", "[", "batch_size", "*", "num", ",", "-", "1", ",", "dim1", "]", ")", "\n", "transform_left", "=", "tf", ".", "einsum", "(", "'ijk,kl->ijl'", ",", "question", ",", "U", ")", "\n", "att_mat", "=", "tf", ".", "matmul", "(", "transform_left", ",", "tf", ".", "transpose", "(", "reviews", ",", "[", "0", ",", "2", ",", "1", "]", ")", ")", "\n", "\n", "r_attn", "=", "tf", ".", "nn", ".", "softmax", "(", "tf", ".", "reduce_max", "(", "att_mat", ",", "axis", "=", "1", ")", ")", "# (batch_size*review_num, r_len)", "\n", "r_attn", "*=", "tf", ".", "reshape", "(", "self", ".", "_r_padding_mask", ",", "[", "batch_size", "*", "num", ",", "-", "1", "]", ")", "\n", "masked_sums", "=", "tf", ".", "reduce_sum", "(", "r_attn", ",", "axis", "=", "1", ")", "# (batch_size*review_num)", "\n", "r_attn", "=", "r_attn", "/", "tf", ".", "reshape", "(", "masked_sums", ",", "[", "-", "1", ",", "1", "]", ")", "# re-normalize", "\n", "r_attn", "=", "tf", ".", "expand_dims", "(", "r_attn", ",", "-", "1", ",", "name", "=", "'review_attention'", ")", "\n", "\n", "q_attn", "=", "tf", ".", "nn", ".", "softmax", "(", "tf", ".", "reduce_max", "(", "att_mat", ",", "axis", "=", "2", ")", ")", "# (batch_size*review_num, q_len)", "\n", "q_attn", "*=", "tf", ".", "reshape", "(", "q_padding_mask", ",", "[", "batch_size", "*", "num", ",", "-", "1", "]", ")", "\n", "masked_sums", "=", "tf", ".", "reduce_sum", "(", "q_attn", ",", "axis", "=", "1", ")", "# (batch_size*review_num)", "\n", "q_attn", "=", "q_attn", "/", "tf", ".", "reshape", "(", "masked_sums", ",", "[", "-", "1", ",", "1", "]", ")", "# re-normalize", "\n", "q_attn", "=", "tf", ".", "expand_dims", "(", "q_attn", ",", "-", "1", ",", "name", "=", "'question_attention'", ")", "\n", "\n", "dec_review_inputs", "=", "tf", ".", "multiply", "(", "reviews", ",", "r_attn", ")", "# (batch_size * review_num, r_len, dim)", "\n", "dec_question_inputs", "=", "tf", ".", "multiply", "(", "question", ",", "q_attn", ")", "# (batch_size * review_num, q_len, dim)", "\n", "att_review_outputs", "=", "tf", ".", "reshape", "(", "tf", ".", "reduce_sum", "(", "dec_review_inputs", ",", "1", ")", ",", "[", "-", "1", ",", "dim2", "]", ")", "\n", "att_question_outputs", "=", "tf", ".", "reshape", "(", "tf", ".", "reduce_sum", "(", "dec_question_inputs", ",", "1", ")", ",", "[", "-", "1", ",", "dim1", "]", ")", "\n", "\n", "dec_question_inputs", "=", "tf", ".", "reshape", "(", "dec_question_inputs", ",", "[", "batch_size", ",", "num", ",", "-", "1", ",", "dim1", "]", ")", "\n", "dec_question_inputs", "=", "tf", ".", "reduce_mean", "(", "dec_question_inputs", ",", "1", ")", "# (batch_size, q_len, dim)", "\n", "dec_review_inputs", "=", "tf", ".", "reshape", "(", "dec_review_inputs", ",", "[", "batch_size", ",", "num", ",", "-", "1", ",", "dim2", "]", ")", "\n", "dec_review_inputs", "=", "tf", ".", "reshape", "(", "dec_review_inputs", ",", "[", "batch_size", ",", "-", "1", ",", "dim2", "]", ")", "# (batch_size, review_num * r_len, dim)", "\n", "hidden_output", "=", "tf", ".", "concat", "(", "[", "att_question_outputs", ",", "att_review_outputs", "]", ",", "1", ")", "# (batch_size * review_num, dim * 2)", "\n", "#hidden_output = att_review_outputs  # (batch_size * review_num, dim)", "\n", "hidden_output", "=", "tf", ".", "reshape", "(", "hidden_output", ",", "[", "batch_size", ",", "num", ",", "-", "1", "]", ")", "\n", "\n", "return", "hidden_output", ",", "dec_question_inputs", ",", "dec_review_inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._overlap": [[268, 285], ["tensorflow.expand_dims", "tensorflow.tile", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.matmul", "tensorflow.expand_dims", "tensorflow.matmul", "tensorflow.expand_dims", "tensorflow.concat", "tensorflow.concat", "tensorflow.transpose", "tensorflow.reduce_max", "tensorflow.transpose", "tensorflow.reduce_max"], "methods", ["None"], ["", "def", "_overlap", "(", "self", ",", "q_embed", ",", "r_embed", ",", "batch_size", ",", "num", ",", "emb_dim", ")", ":", "\n", "    ", "q_embed", "=", "tf", ".", "expand_dims", "(", "q_embed", ",", "1", ")", "\n", "q_embed", "=", "tf", ".", "tile", "(", "q_embed", ",", "(", "1", ",", "num", ",", "1", ",", "1", ")", ")", "\n", "q_embed", "=", "tf", ".", "reshape", "(", "q_embed", ",", "(", "batch_size", "*", "num", ",", "-", "1", ",", "emb_dim", ")", ")", "\n", "r_embed", "=", "tf", ".", "reshape", "(", "r_embed", ",", "(", "batch_size", "*", "num", ",", "-", "1", ",", "emb_dim", ")", ")", "\n", "\n", "overlap1", "=", "tf", ".", "matmul", "(", "q_embed", ",", "tf", ".", "transpose", "(", "r_embed", ",", "[", "0", ",", "2", ",", "1", "]", ")", ")", "\n", "overlap1", "=", "tf", ".", "expand_dims", "(", "tf", ".", "reduce_max", "(", "overlap1", ",", "axis", "=", "2", ")", ",", "-", "1", ")", "\n", "\n", "overlap2", "=", "tf", ".", "matmul", "(", "r_embed", ",", "tf", ".", "transpose", "(", "q_embed", ",", "[", "0", ",", "2", ",", "1", "]", ")", ")", "\n", "overlap2", "=", "tf", ".", "expand_dims", "(", "tf", ".", "reduce_max", "(", "overlap2", ",", "axis", "=", "2", ")", ",", "-", "1", ")", "\n", "q_embed", "=", "tf", ".", "concat", "(", "[", "q_embed", ",", "overlap1", "]", ",", "2", ")", "\n", "r_embed", "=", "tf", ".", "concat", "(", "[", "r_embed", ",", "overlap2", "]", ",", "2", ")", "\n", "\n", "#q_embed = tf.reshape(q_embed, (batch_size, num, -1, emb_dim + 1))", "\n", "#r_embed = tf.reshape(r_embed, (batch_size, num, -1, emb_dim + 1))", "\n", "return", "q_embed", ",", "r_embed", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._self_matching": [[286, 320], ["tensorflow.get_variable", "tensorflow.tanh", "tensorflow.get_variable", "tensorflow.nn.softmax", "tensorflow.multiply", "tensorflow.reshape", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.xw_plus_b", "tensorflow.nn.softmax", "sentiment_input.get_shape", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.expand_dims", "tensorflow.reduce_sum", "tensorflow.contrib.layers.xavier_initializer", "tensorflow.contrib.layers.xavier_initializer", "tensorflow.matmul", "tensorflow.contrib.layers.xavier_initializer", "tensorflow.constant_initializer", "tensorflow.reshape.get_shape"], "methods", ["None"], ["", "def", "_self_matching", "(", "self", ",", "sentiment_input", ",", "batch_size", ",", "hidden_size", "=", "256", ",", "dropout_keep_prob", "=", "0.8", ")", ":", "\n", "    ", "dim1", "=", "sentiment_input", ".", "get_shape", "(", ")", "[", "-", "1", "]", "\n", "W", "=", "tf", ".", "get_variable", "(", "\n", "\"W\"", ",", "\n", "shape", "=", "[", "dim1", ",", "hidden_size", "]", ",", "\n", "initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer", "(", ")", ")", "\n", "U", "=", "tf", ".", "tanh", "(", "tf", ".", "matmul", "(", "sentiment_input", ",", "W", ")", ")", "\n", "w", "=", "tf", ".", "get_variable", "(", "\n", "\"w\"", ",", "\n", "shape", "=", "[", "hidden_size", ",", "1", "]", ",", "\n", "initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer", "(", ")", ")", "\n", "review_attention", "=", "tf", ".", "nn", ".", "softmax", "(", "tf", ".", "reshape", "(", "tf", ".", "matmul", "(", "U", ",", "w", ")", ",", "[", "batch_size", ",", "-", "1", "]", ")", ")", "\n", "decode_in_state", "=", "tf", ".", "matmul", "(", "tf", ".", "transpose", "(", "sentiment_input", ",", "[", "0", ",", "2", ",", "1", "]", ")", ",", "tf", ".", "reshape", "(", "review_attention", ",", "[", "batch_size", ",", "-", "1", ",", "1", "]", ")", ")", "\n", "decode_in_state", "=", "tf", ".", "reshape", "(", "decode_in_state", ",", "[", "batch_size", ",", "-", "1", "]", ")", "\n", "'''\n    W_h = tf.get_variable(\n      \"W_hidden\",\n      shape=[decode_in_state.get_shape()[1], hidden_size],\n      initializer=tf.contrib.layers.xavier_initializer())\n    b_h = tf.get_variable(\"b_hidden\", [hidden_size], initializer=tf.constant_initializer(0.1))\n    hidden_output = tf.nn.relu(tf.nn.xw_plus_b(decode_in_state, W_h, b_h, name=\"hidden_output\"))\n\n    h_drop = tf.nn.dropout(hidden_output, dropout_keep_prob, name=\"hidden_output_drop\")\n    '''", "\n", "W_o", "=", "tf", ".", "get_variable", "(", "\n", "\"W_output\"", ",", "\n", "shape", "=", "[", "decode_in_state", ".", "get_shape", "(", ")", "[", "1", "]", ",", "3", "]", ",", "\n", "initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer", "(", ")", ")", "\n", "b_o", "=", "tf", ".", "get_variable", "(", "\"b_output\"", ",", "[", "3", "]", ",", "initializer", "=", "tf", ".", "constant_initializer", "(", "0.1", ")", ")", "\n", "prob", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "decode_in_state", ",", "W_o", ",", "b_o", ")", "\n", "soft_prob", "=", "tf", ".", "nn", ".", "softmax", "(", "prob", ",", "name", "=", "'distance'", ")", "\n", "\n", "return", "prob", ",", "soft_prob", ",", "review_attention", ",", "decode_in_state", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._add_seq2seq": [[323, 442], ["model.SummarizationModel._vocab.size", "tensorflow.compat.v1.variable_scope", "tensorflow.random_uniform_initializer", "tensorflow.truncated_normal_initializer", "tensorflow.reshape", "tensorflow.reshape", "model.SummarizationModel._add_encoder", "model.SummarizationModel._add_encoder", "model.SummarizationModel._reduce_states", "tensorflow.nn.top_k", "tensorflow.log", "tensorflow.compat.v1.variable_scope", "tensorflow.nn.embedding_lookup", "tensorflow.nn.embedding_lookup", "tensorflow.compat.v1.variable_scope", "model.SummarizationModel._co_attention", "tensorflow.compat.v1.variable_scope", "tensorflow.one_hot", "tensorflow.concat", "model.SummarizationModel._self_matching", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.reduce_mean", "tensorflow.compat.v1.variable_scope", "model.SummarizationModel._add_decoder", "tensorflow.compat.v1.variable_scope", "tensorflow.cast", "tensorflow.equal", "tensorflow.cast", "tensorflow.reduce_mean", "len", "data.get_init_embeddings", "tensorflow.get_variable", "tensorflow.get_variable", "model.SummarizationModel._add_emb_vis", "tensorflow.nn.embedding_lookup", "tensorflow.argmax", "tensorflow.compat.v1.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "enumerate", "model.SummarizationModel._calc_final_dist", "tensorflow.compat.v1.variable_scope", "tensorflow.summary.scalar", "tensorflow.unstack", "vocab_scores.append", "tensorflow.nn.softmax", "tensorflow.range", "enumerate", "model._mask_and_avg", "tensorflow.contrib.seq2seq.sequence_loss", "tensorflow.summary.scalar", "tensorflow.get_variable_scope().reuse_variables", "tensorflow.nn.xw_plus_b", "tensorflow.stack", "tensorflow.gather_nd", "loss_per_step.append", "tensorflow.stack", "tensorflow.variable_scope", "model._coverage_loss", "tensorflow.summary.scalar", "tensorflow.log", "tensorflow.get_variable_scope"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.Vocab.size", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._add_encoder", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._add_encoder", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._reduce_states", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._co_attention", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._self_matching", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._add_decoder", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.data.get_init_embeddings", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._add_emb_vis", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._calc_final_dist", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model._mask_and_avg", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model._coverage_loss"], ["review_attention", "=", "tf", ".", "expand_dims", "(", "review_attention", ",", "-", "1", ")", "\n", "for", "attn_dist", "in", "attn_dists", ":", "\n", "      ", "attn_dist", "=", "tf", ".", "reshape", "(", "attn_dist", ",", "[", "batch_size", ",", "num", ",", "-", "1", "]", ")", "\n", "attn_dist", "=", "tf", ".", "multiply", "(", "attn_dist", ",", "review_attention", ")", "\n", "attn_norm", "=", "tf", ".", "reshape", "(", "attn_dist", ",", "[", "batch_size", ",", "-", "1", "]", ")", "\n", "attn_dist", "=", "tf", ".", "divide", "(", "attn_norm", ",", "tf", ".", "expand_dims", "(", "tf", ".", "reduce_sum", "(", "attn_norm", ",", "1", ")", ",", "-", "1", ")", ")", "\n", "combined_attn_dists", ".", "append", "(", "attn_dist", ")", "\n", "", "return", "combined_attn_dists", "\n", "\n", "\n", "", "def", "_add_seq2seq", "(", "self", ")", ":", "\n", "    ", "\"\"\"Add the whole sequence-to-sequence model to the graph.\"\"\"", "\n", "hps", "=", "self", ".", "_hps", "\n", "vsize", "=", "self", ".", "_vocab", ".", "size", "(", ")", "# size of the vocabulary", "\n", "vocab", "=", "self", ".", "_vocab", "\n", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "'seq2seq'", ")", ":", "\n", "# Some initializers", "\n", "      ", "self", ".", "rand_unif_init", "=", "tf", ".", "random_uniform_initializer", "(", "-", "hps", ".", "rand_unif_init_mag", ",", "hps", ".", "rand_unif_init_mag", ",", "seed", "=", "123", ")", "\n", "self", ".", "trunc_norm_init", "=", "tf", ".", "truncated_normal_initializer", "(", "stddev", "=", "hps", ".", "trunc_norm_init_std", ")", "\n", "\n", "# Add embedding matrix (shared by the encoder and decoder inputs)", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "'embedding'", ")", ":", "\n", "        ", "if", "hps", ".", "glove", ":", "\n", "          ", "init_embeddings", "=", "data", ".", "get_init_embeddings", "(", "vocab", ".", "_id_to_word", ",", "hps", ".", "emb_dim", ")", "\n", "embedding", "=", "tf", ".", "get_variable", "(", "\"embedding\"", ",", "initializer", "=", "init_embeddings", ")", "\n", "", "else", ":", "\n", "          ", "embedding", "=", "tf", ".", "get_variable", "(", "'embedding'", ",", "[", "vsize", ",", "hps", ".", "emb_dim", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "initializer", "=", "self", ".", "trunc_norm_init", ")", "\n", "", "if", "hps", ".", "mode", "==", "\"train\"", ":", "self", ".", "_add_emb_vis", "(", "embedding", ")", "# add to tensorboard", "\n", "emb_enc_inputs", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "embedding", ",", "self", ".", "_r_batch", ")", "# tensor with shape (batch_size, review_num, max_enc_steps, emb_size)", "\n", "emb_dec_inputs", "=", "[", "tf", ".", "nn", ".", "embedding_lookup", "(", "embedding", ",", "x", ")", "for", "x", "in", "tf", ".", "unstack", "(", "self", ".", "_dec_batch", ",", "axis", "=", "1", ")", "]", "# list length max_dec_steps containing shape (batch_size, emb_size)", "\n", "emb_q_inputs", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "embedding", ",", "self", ".", "_q_batch", ")", "# tensor with shape (batch_size, max_enc_steps, emb_size)", "\n", "#emb_q_inputs, emb_enc_inputs = self._overlap(emb_q_inputs, emb_enc_inputs, hps.batch_size, hps.review_num, hps.emb_dim)", "\n", "\n", "\n", "# Add the encoder.", "\n", "", "r_lens", "=", "tf", ".", "reshape", "(", "self", ".", "_r_lens", ",", "[", "-", "1", "]", ")", "\n", "emb_enc_inputs", "=", "tf", ".", "reshape", "(", "emb_enc_inputs", ",", "[", "hps", ".", "batch_size", "*", "hps", ".", "review_num", ",", "-", "1", ",", "hps", ".", "emb_dim", "]", ")", "\n", "enc_outputs", ",", "fw_st", ",", "bw_st", "=", "self", ".", "_add_encoder", "(", "emb_enc_inputs", ",", "r_lens", ",", "\"/review\"", ")", "# (batch_size*review_num, enc_lens, emb_size)", "\n", "q_outputs", ",", "q_fw_st", ",", "q_bw_st", "=", "self", ".", "_add_encoder", "(", "emb_q_inputs", ",", "self", ".", "_q_lens", ",", "\"/question\"", ")", "# (batch_size, q_lens, emb_size)", "\n", "\n", "# Co-attention layer", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "'co-attention'", ")", ":", "\n", "        ", "hidden_output", ",", "self", ".", "dec_question_inputs", ",", "self", ".", "dec_review_inputs", "=", "self", ".", "_co_attention", "(", "q_outputs", ",", "enc_outputs", ",", "hps", ".", "batch_size", ",", "hps", ".", "review_num", ")", "\n", "\n", "# Sentiment layer", "\n", "", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "'sentiment_layer'", ")", ":", "\n", "        ", "rating_batch", "=", "tf", ".", "one_hot", "(", "self", ".", "_rating_batch", ",", "depth", "=", "5", ",", "axis", "=", "-", "1", ")", "\n", "sentiment_input", "=", "tf", ".", "concat", "(", "[", "hidden_output", ",", "rating_batch", "]", ",", "2", ")", "\n", "self", ".", "y_prob", ",", "self", ".", "y_pred", ",", "self", ".", "review_attention", ",", "decode_in_state", "=", "self", ".", "_self_matching", "(", "sentiment_input", ",", "hps", ".", "batch_size", ")", "\n", "# Sentiment loss", "\n", "losses", "=", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "logits", "=", "self", ".", "y_prob", ",", "labels", "=", "self", ".", "_y_target_batch", ")", "\n", "self", ".", "_loss_sa", "=", "tf", ".", "reduce_mean", "(", "losses", ")", "\n", "\n", "# Our encoder is bidirectional and our decoder is unidirectional so we need to reduce the final encoder hidden state to the right size to be the initial decoder hidden state", "\n", "", "self", ".", "_dec_in_state", "=", "self", ".", "_reduce_states", "(", "q_fw_st", ",", "q_bw_st", ",", "decode_in_state", ")", "\n", "\n", "# Add the decoder.", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "'decoder'", ")", ":", "\n", "        ", "decoder_outputs", ",", "output_states", ",", "self", ".", "_dec_out_state", ",", "self", ".", "q_attn_dists", ",", "self", ".", "r_attn_dists", ",", "self", ".", "p_gens", ",", "self", ".", "q_coverage", ",", "self", ".", "r_coverage", "=", "self", ".", "_add_decoder", "(", "emb_dec_inputs", ")", "\n", "\n", "# Accuracy", "\n", "", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "\"accuracy\"", ")", ":", "\n", "        ", "self", ".", "predictions", "=", "tf", ".", "cast", "(", "tf", ".", "argmax", "(", "self", ".", "y_pred", ",", "1", ",", "name", "=", "\"predictions\"", ")", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "correct_predictions", "=", "tf", ".", "equal", "(", "self", ".", "predictions", ",", "self", ".", "_y_target_batch", ")", "\n", "self", ".", "batch_accuracy", "=", "tf", ".", "cast", "(", "correct_predictions", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "self", ".", "accuracy", "=", "tf", ".", "reduce_mean", "(", "self", ".", "batch_accuracy", ",", "name", "=", "\"accuracy\"", ")", "\n", "\n", "", "if", "hps", ".", "mode", "in", "[", "'train'", ",", "'decode'", "]", ":", "\n", "# Add the output projection to obtain the vocabulary distribution", "\n", "        ", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "'output_projection'", ")", ":", "\n", "          ", "w", "=", "tf", ".", "get_variable", "(", "'w'", ",", "[", "hps", ".", "hidden_dim", ",", "vsize", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "initializer", "=", "self", ".", "trunc_norm_init", ")", "\n", "v", "=", "tf", ".", "get_variable", "(", "'v'", ",", "[", "vsize", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "initializer", "=", "self", ".", "trunc_norm_init", ")", "\n", "vocab_scores", "=", "[", "]", "# vocab_scores is the vocabulary distribution before applying softmax. Each entry on the list corresponds to one decoder step", "\n", "for", "i", ",", "output", "in", "enumerate", "(", "decoder_outputs", ")", ":", "\n", "            ", "if", "i", ">", "0", ":", "\n", "              ", "tf", ".", "get_variable_scope", "(", ")", ".", "reuse_variables", "(", ")", "\n", "", "vocab_scores", ".", "append", "(", "tf", ".", "nn", ".", "xw_plus_b", "(", "output", ",", "w", ",", "v", ")", ")", "# apply the linear layer", "\n", "\n", "", "vocab_dists", "=", "[", "tf", ".", "nn", ".", "softmax", "(", "s", ")", "for", "s", "in", "vocab_scores", "]", "# The vocabulary distributions. List length max_dec_steps of (batch_size, vsize) arrays. The words are in the order they appear in the vocabulary file.", "\n", "\n", "\n", "# For pointer-generator model, calc final distribution from copy distribution and vocabulary distribution", "\n", "# Combined Attention", "\n", "#self.r_attn_dists = self._combine_attn(self.r_attn_dists, review_attention, hps.batch_size, hps.review_num)", "\n", "", "if", "FLAGS", ".", "pointer_gen", ":", "\n", "          ", "final_dists", "=", "self", ".", "_calc_final_dist", "(", "vocab_dists", ",", "self", ".", "q_attn_dists", ",", "self", ".", "r_attn_dists", ")", "\n", "", "else", ":", "# final distribution is just vocabulary distribution", "\n", "          ", "final_dists", "=", "vocab_dists", "\n", "\n", "", "", "if", "hps", ".", "mode", "in", "[", "'train'", "]", ":", "\n", "# Calculate the loss", "\n", "        ", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "'loss'", ")", ":", "\n", "          ", "if", "FLAGS", ".", "pointer_gen", ":", "\n", "# Calculate the loss per step", "\n", "# This is fiddly; we use tf.gather_nd to pick out the probabilities of the gold target words", "\n", "            ", "loss_per_step", "=", "[", "]", "# will be list length max_dec_steps containing shape (batch_size)", "\n", "batch_nums", "=", "tf", ".", "range", "(", "0", ",", "limit", "=", "hps", ".", "batch_size", ")", "# shape (batch_size)", "\n", "for", "dec_step", ",", "dist", "in", "enumerate", "(", "final_dists", ")", ":", "\n", "              ", "targets", "=", "self", ".", "_target_batch", "[", ":", ",", "dec_step", "]", "# The indices of the target words. shape (batch_size)", "\n", "indices", "=", "tf", ".", "stack", "(", "(", "batch_nums", ",", "targets", ")", ",", "axis", "=", "1", ")", "# shape (batch_size, 2)", "\n", "gold_probs", "=", "tf", ".", "gather_nd", "(", "dist", ",", "indices", ")", "# shape (batch_size). prob of correct words on this step", "\n", "losses", "=", "-", "tf", ".", "log", "(", "gold_probs", ")", "\n", "loss_per_step", ".", "append", "(", "losses", ")", "\n", "\n", "# Apply dec_padding_mask and get loss", "\n", "", "self", ".", "_loss_sum", "=", "_mask_and_avg", "(", "loss_per_step", ",", "self", ".", "_dec_padding_mask", ")", "\n", "\n", "", "else", ":", "# baseline model", "\n", "            ", "self", ".", "_loss_sum", "=", "tf", ".", "contrib", ".", "seq2seq", ".", "sequence_loss", "(", "tf", ".", "stack", "(", "vocab_scores", ",", "axis", "=", "1", ")", ",", "self", ".", "_target_batch", ",", "self", ".", "_dec_padding_mask", ")", "# this applies softmax internally", "\n", "\n", "", "self", ".", "_loss", "=", "hps", ".", "sa_loss_wt", "*", "self", ".", "_loss_sa", "+", "self", ".", "_loss_sum", "\n", "tf", ".", "summary", ".", "scalar", "(", "'loss'", ",", "self", ".", "_loss", ")", "\n", "\n", "# Calculate coverage loss from the attention distributions", "\n", "if", "hps", ".", "coverage", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "'coverage_loss'", ")", ":", "\n", "              ", "self", ".", "_q_coverage_loss", ",", "self", ".", "_r_coverage_loss", "=", "_coverage_loss", "(", "self", ".", "q_attn_dists", ",", "self", ".", "r_attn_dists", ",", "self", ".", "p_gens", ",", "self", ".", "_dec_padding_mask", ")", "\n", "self", ".", "_coverage_loss", "=", "self", ".", "_q_coverage_loss", "+", "self", ".", "_r_coverage_loss", "\n", "tf", ".", "summary", ".", "scalar", "(", "'coverage_loss'", ",", "self", ".", "_coverage_loss", ")", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._add_train_op": [[444, 462], ["tensorflow.trainable_variables", "tensorflow.gradients", "tensorflow.train.AdagradOptimizer", "tensorflow.device", "tensorflow.clip_by_global_norm", "tensorflow.device", "tensorflow.train.AdagradOptimizer.apply_gradients", "zip"], "methods", ["None"], ["tf", ".", "summary", ".", "scalar", "(", "'total_loss'", ",", "self", ".", "_total_loss", ")", "\n", "\n", "", "", "", "", "if", "hps", ".", "mode", "==", "\"decode\"", ":", "\n", "# We run decode beam search mode one decoder step at a time", "\n", "      ", "assert", "len", "(", "final_dists", ")", "==", "1", "# final_dists is a singleton list containing shape (batch_size, extended_vsize)", "\n", "final_dists", "=", "final_dists", "[", "0", "]", "\n", "topk_probs", ",", "self", ".", "_topk_ids", "=", "tf", ".", "nn", ".", "top_k", "(", "final_dists", ",", "hps", ".", "batch_size", "*", "2", ")", "# take the k largest probs. note batch_size=beam_size in decode mode", "\n", "self", ".", "_topk_log_probs", "=", "tf", ".", "log", "(", "topk_probs", ")", "\n", "\n", "\n", "", "", "def", "_add_train_op", "(", "self", ")", ":", "\n", "    ", "\"\"\"Sets self._train_op, the op to run for training.\"\"\"", "\n", "# Take gradients of the trainable variables w.r.t. the loss function to minimize", "\n", "loss_to_minimize", "=", "self", ".", "_total_loss", "if", "self", ".", "_hps", ".", "coverage", "else", "self", ".", "_loss", "\n", "tvars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "gradients", "=", "tf", ".", "gradients", "(", "loss_to_minimize", ",", "tvars", ",", "aggregation_method", "=", "tf", ".", "AggregationMethod", ".", "EXPERIMENTAL_TREE", ")", "\n", "\n", "# Clip the gradients", "\n", "with", "tf", ".", "device", "(", "\"/gpu:0\"", ")", ":", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel.build_graph": [[464, 477], ["tensorflow.compat.v1.logging.info", "time.time", "model.SummarizationModel._add_placeholders", "tensorflow.Variable", "tensorflow.summary.merge_all", "time.time", "tensorflow.compat.v1.logging.info", "tensorflow.device", "model.SummarizationModel._add_seq2seq", "model.SummarizationModel._add_train_op"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._add_placeholders", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._add_seq2seq", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._add_train_op"], ["\n", "# Add a summary", "\n", "#tf.summary.scalar('global_norm', global_norm)", "\n", "\n", "# Apply adagrad optimizer", "\n", "", "optimizer", "=", "tf", ".", "train", ".", "AdagradOptimizer", "(", "self", ".", "_hps", ".", "lr", ",", "initial_accumulator_value", "=", "self", ".", "_hps", ".", "adagrad_init_acc", ")", "\n", "with", "tf", ".", "device", "(", "\"/gpu:0\"", ")", ":", "\n", "      ", "self", ".", "_train_op", "=", "optimizer", ".", "apply_gradients", "(", "zip", "(", "grads", ",", "tvars", ")", ",", "global_step", "=", "self", ".", "global_step", ",", "name", "=", "'train_step'", ")", "\n", "\n", "\n", "", "", "def", "build_graph", "(", "self", ")", ":", "\n", "    ", "\"\"\"Add the placeholders, model, global step, train_op and summaries to the graph\"\"\"", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "'Building graph...'", ")", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel.run_train_step": [[478, 494], ["model.SummarizationModel._make_feed_dict", "sess.run"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._make_feed_dict"], ["self", ".", "_add_placeholders", "(", ")", "\n", "with", "tf", ".", "device", "(", "\"/gpu:0\"", ")", ":", "\n", "      ", "self", ".", "_add_seq2seq", "(", ")", "\n", "", "self", ".", "global_step", "=", "tf", ".", "Variable", "(", "0", ",", "name", "=", "'global_step'", ",", "trainable", "=", "False", ")", "\n", "if", "self", ".", "_hps", ".", "mode", "==", "'train'", ":", "\n", "      ", "self", ".", "_add_train_op", "(", ")", "\n", "", "self", ".", "_summaries", "=", "tf", ".", "summary", ".", "merge_all", "(", ")", "\n", "t1", "=", "time", ".", "time", "(", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "logging", ".", "info", "(", "'Time to build graph: %i seconds'", ",", "t1", "-", "t0", ")", "\n", "\n", "", "def", "run_train_step", "(", "self", ",", "sess", ",", "batch", ")", ":", "\n", "    ", "\"\"\"Runs one training iteration. Returns a dictionary containing train op, summaries, loss, global_step and (optionally) coverage loss.\"\"\"", "\n", "feed_dict", "=", "self", ".", "_make_feed_dict", "(", "batch", ")", "\n", "to_return", "=", "{", "\n", "'train_op'", ":", "self", ".", "_train_op", ",", "\n", "'summaries'", ":", "self", ".", "_summaries", ",", "\n", "'loss'", ":", "self", ".", "_loss", ",", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel.run_eval_step": [[495, 510], ["model.SummarizationModel._make_feed_dict", "sess.run"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._make_feed_dict"], ["'losses'", ":", "self", ".", "y_prob", ",", "\n", "'global_step'", ":", "self", ".", "global_step", ",", "\n", "'accuracy'", ":", "self", ".", "accuracy", ",", "\n", "}", "\n", "if", "self", ".", "_hps", ".", "coverage", ":", "\n", "      ", "to_return", "[", "'q_coverage'", "]", "=", "self", ".", "_q_coverage_loss", ",", "\n", "to_return", "[", "'r_coverage'", "]", "=", "self", ".", "_r_coverage_loss", ",", "\n", "to_return", "[", "'coverage_loss'", "]", "=", "self", ".", "_coverage_loss", "\n", "", "return", "sess", ".", "run", "(", "to_return", ",", "feed_dict", ")", "\n", "\n", "", "def", "run_eval_step", "(", "self", ",", "sess", ",", "batch", ")", ":", "\n", "    ", "\"\"\"Runs one evaluation iteration. Returns a dictionary containing summaries, loss, global_step and (optionally) coverage loss.\"\"\"", "\n", "feed_dict", "=", "self", ".", "_make_feed_dict", "(", "batch", ")", "\n", "to_return", "=", "{", "\n", "'loss'", ":", "self", ".", "_loss_sum", ",", "\n", "'y_prob'", ":", "self", ".", "y_pred", ",", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel.run_encoder": [[511, 530], ["model.SummarizationModel._make_feed_dict", "sess.run", "tensorflow.contrib.rnn.LSTMStateTuple"], "methods", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel._make_feed_dict"], ["'y_pred'", ":", "self", ".", "predictions", ",", "\n", "'y_true'", ":", "self", ".", "_y_target_batch", ",", "\n", "'accuracy'", ":", "self", ".", "accuracy", ",", "\n", "'batch_accuracy'", ":", "self", ".", "batch_accuracy", "\n", "}", "\n", "\n", "if", "self", ".", "_hps", ".", "coverage", ":", "\n", "      ", "to_return", "[", "'coverage_loss'", "]", "=", "self", ".", "_coverage_loss", "\n", "", "return", "sess", ".", "run", "(", "to_return", ",", "feed_dict", ")", "\n", "\n", "", "def", "run_encoder", "(", "self", ",", "sess", ",", "batch", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model.SummarizationModel.decode_onestep": [[532, 625], ["len", "numpy.concatenate", "numpy.concatenate", "tensorflow.contrib.rnn.LSTMStateTuple", "sess.run", "[].tolist", "[].tolist", "numpy.expand_dims", "numpy.expand_dims", "numpy.transpose", "numpy.stack", "numpy.stack", "tensorflow.contrib.rnn.LSTMStateTuple", "len", "len", "[].tolist", "results[].tolist", "results[].tolist", "numpy.array", "range", "len", "len", "len", "range", "range", "range", "numpy.transpose", "numpy.asarray"], "methods", ["None"], ["feed_dict", "=", "self", ".", "_make_feed_dict", "(", "batch", ",", "just_enc", "=", "True", ")", "# feed the batch into the placeholders", "\n", "(", "dec_in_state", ",", "dec_question_inputs", ",", "dec_review_inputs", ",", "y_pred", ",", "global_step", ")", "=", "sess", ".", "run", "(", "[", "self", ".", "_dec_in_state", ",", "self", ".", "dec_question_inputs", ",", "self", ".", "dec_review_inputs", ",", "self", ".", "y_pred", ",", "self", ".", "global_step", "]", ",", "feed_dict", ")", "# run the encoder", "\n", "\n", "# dec_in_state is LSTMStateTuple shape ([batch_size,hidden_dim],[batch_size,hidden_dim])", "\n", "# Given that the batch is a single example repeated, dec_in_state is identical across the batch so we just take the top row.", "\n", "dec_in_state", "=", "tf", ".", "contrib", ".", "rnn", ".", "LSTMStateTuple", "(", "dec_in_state", ".", "c", "[", "0", "]", ",", "dec_in_state", ".", "h", "[", "0", "]", ")", "\n", "return", "dec_in_state", ",", "dec_question_inputs", ",", "dec_review_inputs", ",", "y_pred", "\n", "\n", "\n", "", "def", "decode_onestep", "(", "self", ",", "sess", ",", "batch", ",", "latest_tokens", ",", "dec_init_states", ",", "dec_question_inputs", ",", "dec_review_inputs", ",", "q_prev_coverage", ",", "r_prev_coverage", ")", ":", "\n", "    ", "\"\"\"For beam search decoding. Run the decoder for one step.\n\n    Args:\n      sess: Tensorflow session.\n      batch: Batch object containing single example repeated across the batch\n      latest_tokens: Tokens to be fed as input into the decoder for this timestep\n      enc_states: The encoder states.\n      dec_init_states: List of beam_size LSTMStateTuples; the decoder states from the previous timestep\n      prev_coverage: List of np arrays. The coverage vectors from the previous timestep. List of None if not using coverage.\n\n    Returns:\n      ids: top 2k ids. shape [beam_size, 2*beam_size]\n      probs: top 2k log probabilities. shape [beam_size, 2*beam_size]\n      new_states: new states of the decoder. a list length beam_size containing\n        LSTMStateTuples each of shape ([hidden_dim,],[hidden_dim,])\n      attn_dists: List length beam_size containing lists length attn_length.\n      p_gens: Generation probabilities for this step. A list length beam_size. List of None if in baseline mode.\n      new_coverage: Coverage vectors for this step. A list of arrays. List of None if coverage is not turned on.\n    \"\"\"", "\n", "\n", "beam_size", "=", "len", "(", "dec_init_states", ")", "\n", "\n", "# Turn dec_init_states (a list of LSTMStateTuples) into a single LSTMStateTuple for the batch", "\n", "cells", "=", "[", "np", ".", "expand_dims", "(", "state", ".", "c", ",", "axis", "=", "0", ")", "for", "state", "in", "dec_init_states", "]", "\n", "hiddens", "=", "[", "np", ".", "expand_dims", "(", "state", ".", "h", ",", "axis", "=", "0", ")", "for", "state", "in", "dec_init_states", "]", "\n", "new_c", "=", "np", ".", "concatenate", "(", "cells", ",", "axis", "=", "0", ")", "# shape [batch_size,hidden_dim]", "\n", "new_h", "=", "np", ".", "concatenate", "(", "hiddens", ",", "axis", "=", "0", ")", "# shape [batch_size,hidden_dim]", "\n", "new_dec_in_state", "=", "tf", ".", "contrib", ".", "rnn", ".", "LSTMStateTuple", "(", "new_c", ",", "new_h", ")", "\n", "\n", "feed", "=", "{", "\n", "self", ".", "dec_question_inputs", ":", "dec_question_inputs", ",", "\n", "self", ".", "dec_review_inputs", ":", "dec_review_inputs", ",", "\n", "self", ".", "_r_padding_mask", ":", "batch", ".", "r_padding_mask", ",", "\n", "self", ".", "_q_padding_mask", ":", "batch", ".", "q_padding_mask", ",", "\n", "self", ".", "_q_lens", ":", "batch", ".", "q_lens", ",", "\n", "self", ".", "_q_batch", ":", "batch", ".", "q_batch", ",", "\n", "self", ".", "_r_batch", ":", "batch", ".", "r_batch", ",", "\n", "self", ".", "_r_lens", ":", "batch", ".", "r_lens", ",", "\n", "self", ".", "_rating_batch", ":", "batch", ".", "rating_batch", ",", "\n", "self", ".", "_dec_in_state", ":", "new_dec_in_state", ",", "\n", "self", ".", "_dec_batch", ":", "np", ".", "transpose", "(", "np", ".", "array", "(", "[", "latest_tokens", "]", ")", ")", ",", "\n", "}", "\n", "\n", "to_return", "=", "{", "\n", "\"ids\"", ":", "self", ".", "_topk_ids", ",", "\n", "\"probs\"", ":", "self", ".", "_topk_log_probs", ",", "\n", "\"states\"", ":", "self", ".", "_dec_out_state", ",", "\n", "\"q_attn_dists\"", ":", "self", ".", "q_attn_dists", ",", "\n", "\"r_attn_dists\"", ":", "self", ".", "r_attn_dists", ",", "\n", "}", "\n", "\n", "if", "FLAGS", ".", "pointer_gen", ":", "\n", "      ", "feed", "[", "self", ".", "_q_batch_extend_vocab", "]", "=", "batch", ".", "q_batch_extend_vocab", "\n", "feed", "[", "self", ".", "_r_batch_extend_vocab", "]", "=", "batch", ".", "r_batch_extend_vocab", "\n", "feed", "[", "self", ".", "_max_oovs", "]", "=", "batch", ".", "max_oovs", "\n", "to_return", "[", "'p_gens'", "]", "=", "self", ".", "p_gens", "\n", "\n", "", "if", "self", ".", "_hps", ".", "coverage", ":", "\n", "      ", "feed", "[", "self", ".", "q_prev_coverage", "]", "=", "np", ".", "stack", "(", "q_prev_coverage", ",", "axis", "=", "0", ")", "\n", "feed", "[", "self", ".", "r_prev_coverage", "]", "=", "np", ".", "stack", "(", "r_prev_coverage", ",", "axis", "=", "0", ")", "\n", "to_return", "[", "'q_coverage'", "]", "=", "self", ".", "q_coverage", "\n", "to_return", "[", "'r_coverage'", "]", "=", "self", ".", "r_coverage", "\n", "\n", "", "results", "=", "sess", ".", "run", "(", "to_return", ",", "feed_dict", "=", "feed", ")", "# run the decoder step", "\n", "\n", "# Convert results['states'] (a single LSTMStateTuple) into a list of LSTMStateTuple -- one for each hypothesis", "\n", "new_states", "=", "[", "tf", ".", "contrib", ".", "rnn", ".", "LSTMStateTuple", "(", "results", "[", "'states'", "]", ".", "c", "[", "i", ",", ":", "]", ",", "results", "[", "'states'", "]", ".", "h", "[", "i", ",", ":", "]", ")", "for", "i", "in", "range", "(", "beam_size", ")", "]", "\n", "\n", "# Convert singleton list containing a tensor to a list of k arrays", "\n", "assert", "len", "(", "results", "[", "'q_attn_dists'", "]", ")", "==", "1", "\n", "q_attn_dists", "=", "results", "[", "'q_attn_dists'", "]", "[", "0", "]", ".", "tolist", "(", ")", "\n", "assert", "len", "(", "results", "[", "'r_attn_dists'", "]", ")", "==", "1", "\n", "r_attn_dists", "=", "results", "[", "'r_attn_dists'", "]", "[", "0", "]", ".", "tolist", "(", ")", "\n", "\n", "if", "FLAGS", ".", "pointer_gen", ":", "\n", "# Convert singleton list containing a tensor to a list of k arrays", "\n", "      ", "assert", "len", "(", "results", "[", "'p_gens'", "]", ")", "==", "1", "\n", "p_gens", "=", "np", ".", "transpose", "(", "np", ".", "asarray", "(", "results", "[", "'p_gens'", "]", "[", "0", "]", ")", ")", "[", "0", "]", ".", "tolist", "(", ")", "\n", "", "else", ":", "\n", "      ", "p_gens", "=", "[", "None", "for", "_", "in", "range", "(", "beam_size", ")", "]", "\n", "\n", "# Convert the coverage tensor to a list length k containing the coverage vector for each hypothesis", "\n", "", "if", "FLAGS", ".", "coverage", ":", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model._mask_and_avg": [[627, 642], ["tensorflow.reduce_sum", "tensorflow.reduce_mean", "sum", "enumerate"], "function", ["None"], ["r_new_coverage", "=", "results", "[", "'r_coverage'", "]", ".", "tolist", "(", ")", "\n", "assert", "len", "(", "q_new_coverage", ")", "==", "beam_size", "\n", "assert", "len", "(", "r_new_coverage", ")", "==", "beam_size", "\n", "", "else", ":", "\n", "      ", "q_new_coverage", "=", "[", "None", "for", "_", "in", "range", "(", "beam_size", ")", "]", "\n", "r_new_coverage", "=", "[", "None", "for", "_", "in", "range", "(", "beam_size", ")", "]", "\n", "\n", "", "return", "results", "[", "'ids'", "]", ",", "results", "[", "'probs'", "]", ",", "new_states", ",", "q_attn_dists", ",", "r_attn_dists", ",", "p_gens", ",", "q_new_coverage", ",", "r_new_coverage", "\n", "\n", "\n", "", "", "def", "_mask_and_avg", "(", "values", ",", "padding_mask", ")", ":", "\n", "  "]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model._coverage_loss": [[644, 672], ["tensorflow.zeros_like", "model._mask_and_avg", "tensorflow.zeros_like", "model._mask_and_avg", "tensorflow.reduce_sum", "q_covlosses.append", "tensorflow.reduce_sum", "r_covlosses.append", "zip", "zip", "tensorflow.minimum", "tensorflow.minimum"], "function", ["home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model._mask_and_avg", "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.model._mask_and_avg"], ["\n", "\n", "dec_lens", "=", "tf", ".", "reduce_sum", "(", "padding_mask", ",", "axis", "=", "1", ")", "# shape batch_size. float32", "\n", "values_per_step", "=", "[", "v", "*", "padding_mask", "[", ":", ",", "dec_step", "]", "for", "dec_step", ",", "v", "in", "enumerate", "(", "values", ")", "]", "\n", "values_per_ex", "=", "sum", "(", "values_per_step", ")", "/", "dec_lens", "# shape (batch_size); normalized value for each batch member", "\n", "return", "tf", ".", "reduce_mean", "(", "values_per_ex", ")", "# overall average", "\n", "\n", "\n", "", "def", "_coverage_loss", "(", "q_attn_dists", ",", "r_attn_dists", ",", "p_gens", ",", "padding_mask", ")", ":", "\n", "  ", "\"\"\"Calculates the coverage loss from the attention distributions.\n\n  Args:\n    attn_dists: The attention distributions for each decoder timestep. A list length max_dec_steps containing shape (batch_size, attn_length)\n    padding_mask: shape (batch_size, max_dec_steps).\n\n  Returns:\n    coverage_loss: scalar\n  \"\"\"", "\n", "q_attn_dists", "=", "[", "p_gen", "[", "1", "]", "/", "(", "p_gen", "[", "1", "]", "+", "p_gen", "[", "2", "]", ")", "*", "dist", "for", "(", "p_gen", ",", "dist", ")", "in", "zip", "(", "p_gens", ",", "q_attn_dists", ")", "]", "\n", "r_attn_dists", "=", "[", "p_gen", "[", "2", "]", "/", "(", "p_gen", "[", "1", "]", "+", "p_gen", "[", "2", "]", ")", "*", "dist", "for", "(", "p_gen", ",", "dist", ")", "in", "zip", "(", "p_gens", ",", "r_attn_dists", ")", "]", "\n", "q_coverage", "=", "tf", ".", "zeros_like", "(", "q_attn_dists", "[", "0", "]", ")", "# shape (batch_size, attn_length). Initial coverage is zero.", "\n", "q_covlosses", "=", "[", "]", "# Coverage loss per decoder timestep. Will be list length max_dec_steps containing shape (batch_size).", "\n", "for", "a", "in", "q_attn_dists", ":", "\n", "    ", "covloss", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "minimum", "(", "a", ",", "q_coverage", ")", ",", "[", "1", "]", ")", "# calculate the coverage loss for this step", "\n", "q_covlosses", ".", "append", "(", "covloss", ")", "\n", "q_coverage", "+=", "a", "# update the coverage vector", "\n", "", "q_coverage_loss", "=", "_mask_and_avg", "(", "q_covlosses", ",", "padding_mask", ")", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.util.get_config": [[24, 29], ["tensorflow.ConfigProto"], "function", ["None"], ["def", "get_config", "(", ")", ":", "\n", "  ", "\"\"\"Returns config for tf.session\"\"\"", "\n", "config", "=", "tf", ".", "ConfigProto", "(", "allow_soft_placement", "=", "True", ")", "\n", "config", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.dengyang17_OAAG.oaag-dy.util.load_ckpt": [[30, 43], ["os.path.join", "tensorflow.train.get_checkpoint_state", "tensorflow.logging.info", "saver.restore", "tensorflow.logging.info", "time.sleep"], "function", ["None"], ["", "def", "load_ckpt", "(", "saver", ",", "sess", ",", "ckpt_dir", "=", "\"train\"", ")", ":", "\n", "  ", "\"\"\"Load checkpoint from the ckpt_dir (if unspecified, this is train dir) and restore it to saver and sess, waiting 10 secs in the case of failure. Also returns checkpoint name.\"\"\"", "\n", "while", "True", ":", "\n", "    ", "try", ":", "\n", "      ", "latest_filename", "=", "\"checkpoint_best\"", "if", "ckpt_dir", "==", "\"eval\"", "else", "None", "\n", "ckpt_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "log_root", ",", "ckpt_dir", ")", "\n", "ckpt_state", "=", "tf", ".", "train", ".", "get_checkpoint_state", "(", "ckpt_dir", ",", "latest_filename", "=", "latest_filename", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Loading checkpoint %s'", ",", "ckpt_state", ".", "model_checkpoint_path", ")", "\n", "saver", ".", "restore", "(", "sess", ",", "ckpt_state", ".", "model_checkpoint_path", ")", "\n", "return", "ckpt_state", ".", "model_checkpoint_path", "\n", "", "except", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"Failed to load checkpoint from %s. Sleeping for %i secs...\"", ",", "ckpt_dir", ",", "10", ")", "\n", "time", ".", "sleep", "(", "10", ")", "\n", "", "", "", ""]]}