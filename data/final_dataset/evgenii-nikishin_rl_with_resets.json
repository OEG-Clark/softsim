{"home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.None.train_dense.main": [[38, 112], ["os.makedirs", "continuous_control.utils.make_env", "continuous_control.utils.make_env", "numpy.random.seed", "random.seed", "FLAGS.flag_values_dict", "FLAGS.flag_values_dict.update", "dict", "dict.pop", "dict.pop", "continuous_control.agents.SACLearner", "continuous_control.datasets.ReplayBuffer", "tqdm.tqdm", "os.path.join", "os.path.join", "FLAGS.flag_values_dict.pop", "dict.pop", "continuous_control.utils.make_env.reset", "range", "continuous_control.utils.make_env.step", "continuous_control.datasets.ReplayBuffer.insert", "continuous_control.utils.make_env.observation_space.sample", "continuous_control.utils.make_env.action_space.sample", "continuous_control.utils.make_env.action_space.sample", "continuous_control.agents.SACLearner.sample_actions", "float", "range", "continuous_control.evaluation.evaluate", "eval_returns.append", "numpy.savetxt", "continuous_control.agents.SACLearner", "continuous_control.utils.make_env.reset", "continuous_control.datasets.ReplayBuffer.sample", "continuous_control.agents.SACLearner.update", "os.path.join", "continuous_control.utils.make_env.observation_space.sample", "continuous_control.utils.make_env.action_space.sample"], "function", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.continuous_control.utils.make_env", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.continuous_control.utils.make_env", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.temperature.update", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.absorbing_states.AbsorbingStatesWrapper.reset", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.step", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.datasets.replay_buffer.ReplayBuffer.insert", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.sample", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.sample", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.sample", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.policies.sample_actions", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.continuous_control.evaluation.evaluate", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.absorbing_states.AbsorbingStatesWrapper.reset", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.sample", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.temperature.update", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.sample", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.sample"], ["def", "main", "(", "_", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "FLAGS", ".", "save_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "if", "FLAGS", ".", "save_video", ":", "\n", "        ", "video_train_folder", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "save_dir", ",", "'video'", ",", "'train'", ")", "\n", "video_eval_folder", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "save_dir", ",", "'video'", ",", "'eval'", ")", "\n", "", "else", ":", "\n", "        ", "video_train_folder", "=", "None", "\n", "video_eval_folder", "=", "None", "\n", "\n", "", "env", "=", "make_env", "(", "FLAGS", ".", "env_name", ",", "FLAGS", ".", "seed", ",", "video_train_folder", ")", "\n", "eval_env", "=", "make_env", "(", "FLAGS", ".", "env_name", ",", "FLAGS", ".", "seed", "+", "42", ",", "video_eval_folder", ")", "\n", "\n", "np", ".", "random", ".", "seed", "(", "FLAGS", ".", "seed", ")", "\n", "random", ".", "seed", "(", "FLAGS", ".", "seed", ")", "\n", "\n", "all_kwargs", "=", "FLAGS", ".", "flag_values_dict", "(", ")", "\n", "all_kwargs", ".", "update", "(", "all_kwargs", ".", "pop", "(", "'config'", ")", ")", "\n", "\n", "kwargs", "=", "dict", "(", "FLAGS", ".", "config", ")", "\n", "assert", "kwargs", ".", "pop", "(", "'algo'", ")", "==", "'sac'", "\n", "updates_per_step", "=", "kwargs", ".", "pop", "(", "'updates_per_step'", ")", "\n", "replay_buffer_size", "=", "kwargs", ".", "pop", "(", "'replay_buffer_size'", ")", "\n", "\n", "agent", "=", "SACLearner", "(", "FLAGS", ".", "seed", ",", "\n", "env", ".", "observation_space", ".", "sample", "(", ")", "[", "np", ".", "newaxis", "]", ",", "\n", "env", ".", "action_space", ".", "sample", "(", ")", "[", "np", ".", "newaxis", "]", ",", "**", "kwargs", ")", "\n", "\n", "action_dim", "=", "env", ".", "action_space", ".", "shape", "[", "0", "]", "\n", "replay_buffer", "=", "ReplayBuffer", "(", "env", ".", "observation_space", ",", "action_dim", ",", "\n", "replay_buffer_size", "or", "FLAGS", ".", "max_steps", ")", "\n", "\n", "eval_returns", "=", "[", "]", "\n", "observation", ",", "done", "=", "env", ".", "reset", "(", ")", ",", "False", "\n", "for", "i", "in", "tqdm", ".", "tqdm", "(", "range", "(", "1", ",", "FLAGS", ".", "max_steps", "+", "1", ")", ",", "\n", "smoothing", "=", "0.1", ",", "\n", "disable", "=", "not", "FLAGS", ".", "tqdm", ")", ":", "\n", "        ", "if", "i", "<", "FLAGS", ".", "start_training", ":", "\n", "            ", "action", "=", "env", ".", "action_space", ".", "sample", "(", ")", "\n", "", "else", ":", "\n", "            ", "action", "=", "agent", ".", "sample_actions", "(", "observation", ")", "\n", "", "next_observation", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "action", ")", "\n", "\n", "if", "not", "done", "or", "'TimeLimit.truncated'", "in", "info", ":", "\n", "            ", "mask", "=", "1.0", "\n", "", "else", ":", "\n", "            ", "mask", "=", "0.0", "\n", "\n", "", "replay_buffer", ".", "insert", "(", "observation", ",", "action", ",", "reward", ",", "mask", ",", "float", "(", "done", ")", ",", "\n", "next_observation", ")", "\n", "observation", "=", "next_observation", "\n", "\n", "if", "done", ":", "\n", "            ", "observation", ",", "done", "=", "env", ".", "reset", "(", ")", ",", "False", "\n", "\n", "", "if", "i", ">=", "FLAGS", ".", "start_training", ":", "\n", "            ", "for", "_", "in", "range", "(", "updates_per_step", ")", ":", "\n", "                ", "batch", "=", "replay_buffer", ".", "sample", "(", "FLAGS", ".", "batch_size", ")", "\n", "agent", ".", "update", "(", "batch", ")", "\n", "\n", "", "", "if", "i", "%", "FLAGS", ".", "eval_interval", "==", "0", ":", "\n", "            ", "eval_stats", "=", "evaluate", "(", "agent", ",", "eval_env", ",", "FLAGS", ".", "eval_episodes", ")", "\n", "\n", "eval_returns", ".", "append", "(", "\n", "(", "info", "[", "'total'", "]", "[", "'timesteps'", "]", ",", "eval_stats", "[", "'return'", "]", ")", ")", "\n", "np", ".", "savetxt", "(", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "save_dir", ",", "f'{FLAGS.seed}.txt'", ")", ",", "\n", "eval_returns", ",", "\n", "fmt", "=", "[", "'%d'", ",", "'%.1f'", "]", ")", "\n", "\n", "", "if", "FLAGS", ".", "resets", "and", "i", "%", "FLAGS", ".", "reset_interval", "==", "0", ":", "\n", "# create a completely new agent", "\n", "            ", "agent", "=", "SACLearner", "(", "FLAGS", ".", "seed", "+", "i", ",", "\n", "env", ".", "observation_space", ".", "sample", "(", ")", "[", "np", ".", "newaxis", "]", ",", "\n", "env", ".", "action_space", ".", "sample", "(", ")", "[", "np", ".", "newaxis", "]", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.None.train_pixels.main": [[54, 176], ["os.makedirs", "FLAGS.flag_values_dict", "FLAGS.flag_values_dict.update", "dict", "dict.pop", "dict.pop", "train_pixels.main.make_pixel_env"], "function", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.temperature.update"], ["def", "main", "(", "_", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "FLAGS", ".", "save_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "if", "FLAGS", ".", "save_video", ":", "\n", "        ", "video_train_folder", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "save_dir", ",", "'video'", ",", "'train'", ")", "\n", "video_eval_folder", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "save_dir", ",", "'video'", ",", "'eval'", ")", "\n", "", "else", ":", "\n", "        ", "video_train_folder", "=", "None", "\n", "video_eval_folder", "=", "None", "\n", "\n", "", "if", "FLAGS", ".", "action_repeat", "is", "not", "None", ":", "\n", "        ", "action_repeat", "=", "FLAGS", ".", "action_repeat", "\n", "", "else", ":", "\n", "        ", "action_repeat", "=", "PLANET_ACTION_REPEAT", ".", "get", "(", "FLAGS", ".", "env_name", ",", "2", ")", "\n", "\n", "", "all_kwargs", "=", "FLAGS", ".", "flag_values_dict", "(", ")", "\n", "all_kwargs", ".", "update", "(", "all_kwargs", ".", "pop", "(", "'config'", ")", ")", "\n", "kwargs", "=", "dict", "(", "FLAGS", ".", "config", ")", "\n", "\n", "gray_scale", "=", "kwargs", ".", "pop", "(", "'gray_scale'", ")", "\n", "image_size", "=", "kwargs", ".", "pop", "(", "'image_size'", ")", "\n", "\n", "def", "make_pixel_env", "(", "seed", ",", "video_folder", ")", ":", "\n", "        ", "return", "make_env", "(", "FLAGS", ".", "env_name", ",", "\n", "seed", ",", "\n", "video_folder", ",", "\n", "action_repeat", "=", "action_repeat", ",", "\n", "image_size", "=", "image_size", ",", "\n", "frame_stack", "=", "3", ",", "\n", "from_pixels", "=", "True", ",", "\n", "gray_scale", "=", "gray_scale", ")", "\n", "\n", "", "env", "=", "make_pixel_env", "(", "FLAGS", ".", "seed", ",", "video_train_folder", ")", "\n", "eval_env", "=", "make_pixel_env", "(", "FLAGS", ".", "seed", "+", "42", ",", "video_eval_folder", ")", "\n", "\n", "np", ".", "random", ".", "seed", "(", "FLAGS", ".", "seed", ")", "\n", "random", ".", "seed", "(", "FLAGS", ".", "seed", ")", "\n", "\n", "assert", "kwargs", ".", "pop", "(", "'algo'", ")", "==", "'drq'", "\n", "replay_buffer_size", "=", "kwargs", ".", "pop", "(", "'replay_buffer_size'", ")", "\n", "\n", "obs_demo", "=", "env", ".", "observation_space", ".", "sample", "(", ")", "\n", "action_demo", "=", "env", ".", "action_space", ".", "sample", "(", ")", "\n", "agent", "=", "DrQLearner", "(", "FLAGS", ".", "seed", ",", "\n", "obs_demo", "[", "np", ".", "newaxis", "]", ",", "\n", "action_demo", "[", "np", ".", "newaxis", "]", ",", "**", "kwargs", ")", "\n", "\n", "action_dim", "=", "env", ".", "action_space", ".", "shape", "[", "0", "]", "\n", "replay_buffer", "=", "ReplayBuffer", "(", "env", ".", "observation_space", ",", "action_dim", ",", "\n", "replay_buffer_size", "or", "FLAGS", ".", "max_steps", ")", "\n", "\n", "eval_returns", "=", "[", "]", "\n", "observation", ",", "done", "=", "env", ".", "reset", "(", ")", ",", "False", "\n", "for", "i", "in", "tqdm", ".", "tqdm", "(", "range", "(", "1", ",", "FLAGS", ".", "max_steps", "//", "action_repeat", "+", "1", ")", ",", "\n", "smoothing", "=", "0.1", ",", "\n", "disable", "=", "not", "FLAGS", ".", "tqdm", ")", ":", "\n", "        ", "if", "i", "<", "FLAGS", ".", "start_training", ":", "\n", "            ", "action", "=", "env", ".", "action_space", ".", "sample", "(", ")", "\n", "", "else", ":", "\n", "            ", "action", "=", "agent", ".", "sample_actions", "(", "observation", ")", "\n", "", "next_observation", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "action", ")", "\n", "\n", "if", "not", "done", "or", "'TimeLimit.truncated'", "in", "info", ":", "\n", "            ", "mask", "=", "1.0", "\n", "", "else", ":", "\n", "            ", "mask", "=", "0.0", "\n", "\n", "", "replay_buffer", ".", "insert", "(", "observation", ",", "action", ",", "reward", ",", "mask", ",", "float", "(", "done", ")", ",", "\n", "next_observation", ")", "\n", "observation", "=", "next_observation", "\n", "\n", "if", "done", ":", "\n", "            ", "observation", ",", "done", "=", "env", ".", "reset", "(", ")", ",", "False", "\n", "\n", "", "if", "i", ">=", "FLAGS", ".", "start_training", ":", "\n", "            ", "batch", "=", "replay_buffer", ".", "sample", "(", "FLAGS", ".", "batch_size", ")", "\n", "agent", ".", "update", "(", "batch", ")", "\n", "\n", "", "if", "i", "%", "FLAGS", ".", "eval_interval", "==", "0", ":", "\n", "            ", "eval_stats", "=", "evaluate", "(", "agent", ",", "eval_env", ",", "FLAGS", ".", "eval_episodes", ")", "\n", "\n", "eval_returns", ".", "append", "(", "\n", "(", "info", "[", "'total'", "]", "[", "'timesteps'", "]", ",", "eval_stats", "[", "'return'", "]", ")", ")", "\n", "np", ".", "savetxt", "(", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "save_dir", ",", "f'{FLAGS.seed}.txt'", ")", ",", "\n", "eval_returns", ",", "\n", "fmt", "=", "[", "'%d'", ",", "'%.1f'", "]", ")", "\n", "\n", "", "if", "FLAGS", ".", "resets", "and", "i", "%", "FLAGS", ".", "reset_interval", "==", "0", ":", "\n", "# shared enc params: 388416", "\n", "# critic head(s) params: 366232", "\n", "# actor head params: 286882", "\n", "# so we reset roughtly half of the agent (both layer and param wise)", "\n", "\n", "# save encoder parameters", "\n", "            ", "old_critic_enc", "=", "agent", ".", "critic", ".", "params", "[", "'SharedEncoder'", "]", "\n", "# target critic has its own copy of encoder", "\n", "old_target_critic_enc", "=", "agent", ".", "target_critic", ".", "params", "[", "'SharedEncoder'", "]", "\n", "# save encoder optimizer statistics", "\n", "old_critic_enc_opt", "=", "agent", ".", "critic", ".", "opt_state_enc", "\n", "\n", "# create new agent: note that the temperature is new as well", "\n", "agent", "=", "DrQLearner", "(", "FLAGS", ".", "seed", "+", "i", ",", "\n", "env", ".", "observation_space", ".", "sample", "(", ")", "[", "np", ".", "newaxis", "]", ",", "\n", "env", ".", "action_space", ".", "sample", "(", ")", "[", "np", ".", "newaxis", "]", ",", "**", "kwargs", ")", "\n", "\n", "# resetting critic: copy encoder parameters and optimizer statistics", "\n", "new_critic_params", "=", "agent", ".", "critic", ".", "params", ".", "copy", "(", "\n", "add_or_replace", "=", "{", "'SharedEncoder'", ":", "old_critic_enc", "}", ")", "\n", "agent", ".", "critic", "=", "agent", ".", "critic", ".", "replace", "(", "params", "=", "new_critic_params", ",", "\n", "opt_state_enc", "=", "old_critic_enc_opt", ")", "\n", "\n", "# resetting actor: actor in DrQ uses critic's encoder", "\n", "# note we could have copied enc optimizer here but actor does not affect enc", "\n", "new_actor_params", "=", "agent", ".", "actor", ".", "params", ".", "copy", "(", "\n", "add_or_replace", "=", "{", "'SharedEncoder'", ":", "old_critic_enc", "}", ")", "\n", "agent", ".", "actor", "=", "agent", ".", "actor", ".", "replace", "(", "params", "=", "new_actor_params", ")", "\n", "\n", "# resetting target critic", "\n", "new_target_critic_params", "=", "agent", ".", "target_critic", ".", "params", ".", "copy", "(", "\n", "add_or_replace", "=", "{", "'SharedEncoder'", ":", "old_target_critic_enc", "}", ")", "\n", "agent", ".", "target_critic", "=", "agent", ".", "target_critic", ".", "replace", "(", "\n", "params", "=", "new_target_critic_params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.continuous_control.utils.make_env": [[11, 80], ["gym.envs.registry.all", "gym.wrappers.RescaleAction", "wrappers.RGB2Gray.seed", "wrappers.RGB2Gray.action_space.seed", "wrappers.RGB2Gray.observation_space.seed", "gym.make", "env_name.split", "continuous_control.wrappers.DMCEnv", "isinstance", "gym.wrappers.FlattenObservation", "continuous_control.wrappers.EpisodeMonitor", "continuous_control.wrappers.RepeatAction", "continuous_control.wrappers.VideoRecorder", "gym.wrappers.pixel_observation.PixelObservationWrapper", "continuous_control.wrappers.TakeKey", "continuous_control.wrappers.SinglePrecision", "continuous_control.wrappers.FrameStack", "continuous_control.wrappers.StickyActionEnv", "continuous_control.wrappers.RGB2Gray"], "function", ["None"], ["def", "make_env", "(", "env_name", ":", "str", ",", "\n", "seed", ":", "int", ",", "\n", "save_folder", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "add_episode_monitor", ":", "bool", "=", "True", ",", "\n", "action_repeat", ":", "int", "=", "1", ",", "\n", "frame_stack", ":", "int", "=", "1", ",", "\n", "from_pixels", ":", "bool", "=", "False", ",", "\n", "pixels_only", ":", "bool", "=", "True", ",", "\n", "image_size", ":", "int", "=", "84", ",", "\n", "sticky", ":", "bool", "=", "False", ",", "\n", "gray_scale", ":", "bool", "=", "False", ",", "\n", "flatten", ":", "bool", "=", "True", ")", "->", "gym", ".", "Env", ":", "\n", "# Check if the env is in gym.", "\n", "    ", "all_envs", "=", "gym", ".", "envs", ".", "registry", ".", "all", "(", ")", "\n", "env_ids", "=", "[", "env_spec", ".", "id", "for", "env_spec", "in", "all_envs", "]", "\n", "\n", "if", "env_name", "in", "env_ids", ":", "\n", "        ", "env", "=", "gym", ".", "make", "(", "env_name", ")", "\n", "", "else", ":", "\n", "        ", "domain_name", ",", "task_name", "=", "env_name", ".", "split", "(", "'-'", ")", "\n", "env", "=", "wrappers", ".", "DMCEnv", "(", "domain_name", "=", "domain_name", ",", "\n", "task_name", "=", "task_name", ",", "\n", "task_kwargs", "=", "{", "'random'", ":", "seed", "}", ")", "\n", "\n", "", "if", "flatten", "and", "isinstance", "(", "env", ".", "observation_space", ",", "gym", ".", "spaces", ".", "Dict", ")", ":", "\n", "        ", "env", "=", "gym", ".", "wrappers", ".", "FlattenObservation", "(", "env", ")", "\n", "\n", "", "if", "add_episode_monitor", ":", "\n", "        ", "env", "=", "wrappers", ".", "EpisodeMonitor", "(", "env", ")", "\n", "\n", "", "if", "action_repeat", ">", "1", ":", "\n", "        ", "env", "=", "wrappers", ".", "RepeatAction", "(", "env", ",", "action_repeat", ")", "\n", "\n", "", "env", "=", "RescaleAction", "(", "env", ",", "-", "1.0", ",", "1.0", ")", "\n", "\n", "if", "save_folder", "is", "not", "None", ":", "\n", "        ", "env", "=", "VideoRecorder", "(", "env", ",", "save_folder", "=", "save_folder", ")", "\n", "\n", "", "if", "from_pixels", ":", "\n", "        ", "if", "env_name", "in", "env_ids", ":", "\n", "            ", "camera_id", "=", "0", "\n", "", "else", ":", "\n", "            ", "camera_id", "=", "2", "if", "domain_name", "==", "'quadruped'", "else", "0", "\n", "", "env", "=", "PixelObservationWrapper", "(", "env", ",", "\n", "pixels_only", "=", "pixels_only", ",", "\n", "render_kwargs", "=", "{", "\n", "'pixels'", ":", "{", "\n", "'height'", ":", "image_size", ",", "\n", "'width'", ":", "image_size", ",", "\n", "'camera_id'", ":", "camera_id", "\n", "}", "\n", "}", ")", "\n", "env", "=", "wrappers", ".", "TakeKey", "(", "env", ",", "take_key", "=", "'pixels'", ")", "\n", "if", "gray_scale", ":", "\n", "            ", "env", "=", "wrappers", ".", "RGB2Gray", "(", "env", ")", "\n", "", "", "else", ":", "\n", "        ", "env", "=", "wrappers", ".", "SinglePrecision", "(", "env", ")", "\n", "\n", "", "if", "frame_stack", ">", "1", ":", "\n", "        ", "env", "=", "wrappers", ".", "FrameStack", "(", "env", ",", "num_stack", "=", "frame_stack", ")", "\n", "\n", "", "if", "sticky", ":", "\n", "        ", "env", "=", "wrappers", ".", "StickyActionEnv", "(", "env", ")", "\n", "\n", "", "env", ".", "seed", "(", "seed", ")", "\n", "env", ".", "action_space", ".", "seed", "(", "seed", ")", "\n", "env", ".", "observation_space", ".", "seed", "(", "seed", ")", "\n", "\n", "return", "env", "\n", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.continuous_control.evaluation.evaluate": [[8, 31], ["range", "stats.items", "stats.keys", "numpy.mean", "env.reset", "agent.sample_actions", "env.step", "stats[].append"], "function", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.absorbing_states.AbsorbingStatesWrapper.reset", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.policies.sample_actions", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.step"], ["def", "evaluate", "(", "agent", ":", "nn", ".", "Module", ",", "env", ":", "gym", ".", "Env", ",", "\n", "num_episodes", ":", "int", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "    ", "stats", "=", "{", "'return'", ":", "[", "]", ",", "'length'", ":", "[", "]", "}", "\n", "successes", "=", "None", "\n", "for", "_", "in", "range", "(", "num_episodes", ")", ":", "\n", "        ", "observation", ",", "done", "=", "env", ".", "reset", "(", ")", ",", "False", "\n", "while", "not", "done", ":", "\n", "            ", "action", "=", "agent", ".", "sample_actions", "(", "observation", ",", "temperature", "=", "0.0", ")", "\n", "observation", ",", "_", ",", "done", ",", "info", "=", "env", ".", "step", "(", "action", ")", "\n", "", "for", "k", "in", "stats", ".", "keys", "(", ")", ":", "\n", "            ", "stats", "[", "k", "]", ".", "append", "(", "info", "[", "'episode'", "]", "[", "k", "]", ")", "\n", "\n", "", "if", "'is_success'", "in", "info", ":", "\n", "            ", "if", "successes", "is", "None", ":", "\n", "                ", "successes", "=", "0.0", "\n", "", "successes", "+=", "info", "[", "'is_success'", "]", "\n", "\n", "", "", "for", "k", ",", "v", "in", "stats", ".", "items", "(", ")", ":", "\n", "        ", "stats", "[", "k", "]", "=", "np", ".", "mean", "(", "v", ")", "\n", "\n", "", "if", "successes", "is", "not", "None", ":", "\n", "        ", "stats", "[", "'success'", "]", "=", "successes", "/", "num_episodes", "\n", "", "return", "stats", "\n", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.datasets.dataset_utils.make_env_and_dataset": [[11, 23], ["continuous_control.utils.make_env", "D4RLDataset", "AWACDataset", "NotImplementedError"], "function", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.continuous_control.utils.make_env"], ["def", "make_env_and_dataset", "(", "env_name", ":", "str", ",", "seed", ":", "int", ",", "dataset_name", ":", "str", ",", "\n", "video_save_folder", ":", "str", ")", "->", "Tuple", "[", "gym", ".", "Env", ",", "Dataset", "]", ":", "\n", "    ", "env", "=", "make_env", "(", "env_name", ",", "seed", ",", "video_save_folder", ")", "\n", "\n", "if", "'d4rl'", "in", "dataset_name", ":", "\n", "        ", "dataset", "=", "D4RLDataset", "(", "env", ")", "\n", "", "elif", "'awac'", "in", "dataset_name", ":", "\n", "        ", "dataset", "=", "AWACDataset", "(", "env_name", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "f'{dataset_name} is not available!'", ")", "\n", "\n", "", "return", "env", ",", "dataset", "\n", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.datasets.replay_buffer.ReplayBuffer.__init__": [[12, 39], ["numpy.empty", "numpy.empty", "numpy.empty", "numpy.empty", "numpy.empty", "numpy.empty", "continuous_control.datasets.dataset.Dataset.__init__"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.__init__"], ["    ", "def", "__init__", "(", "self", ",", "observation_space", ":", "gym", ".", "spaces", ".", "Box", ",", "action_dim", ":", "int", ",", "\n", "capacity", ":", "int", ")", ":", "\n", "\n", "        ", "observations", "=", "np", ".", "empty", "(", "(", "capacity", ",", "*", "observation_space", ".", "shape", ")", ",", "\n", "dtype", "=", "observation_space", ".", "dtype", ")", "\n", "actions", "=", "np", ".", "empty", "(", "(", "capacity", ",", "action_dim", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "rewards", "=", "np", ".", "empty", "(", "(", "capacity", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "masks", "=", "np", ".", "empty", "(", "(", "capacity", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "dones_float", "=", "np", ".", "empty", "(", "(", "capacity", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "next_observations", "=", "np", ".", "empty", "(", "(", "capacity", ",", "*", "observation_space", ".", "shape", ")", ",", "\n", "dtype", "=", "observation_space", ".", "dtype", ")", "\n", "super", "(", ")", ".", "__init__", "(", "observations", "=", "observations", ",", "\n", "actions", "=", "actions", ",", "\n", "rewards", "=", "rewards", ",", "\n", "masks", "=", "masks", ",", "\n", "dones_float", "=", "dones_float", ",", "\n", "next_observations", "=", "next_observations", ",", "\n", "size", "=", "0", ")", "\n", "\n", "self", ".", "size", "=", "0", "\n", "\n", "self", ".", "insert_index", "=", "0", "\n", "self", ".", "capacity", "=", "capacity", "\n", "\n", "# for saving the buffer", "\n", "self", ".", "n_parts", "=", "4", "\n", "assert", "self", ".", "capacity", "%", "self", ".", "n_parts", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.datasets.replay_buffer.ReplayBuffer.initialize_with_dataset": [[40, 68], ["len", "min", "numpy.random.permutation", "numpy.arange"], "methods", ["None"], ["", "def", "initialize_with_dataset", "(", "self", ",", "dataset", ":", "Dataset", ",", "\n", "num_samples", ":", "Optional", "[", "int", "]", ")", ":", "\n", "        ", "assert", "self", ".", "insert_index", "==", "0", ",", "'Can insert a batch online in an empty replay buffer.'", "\n", "\n", "dataset_size", "=", "len", "(", "dataset", ".", "observations", ")", "\n", "\n", "if", "num_samples", "is", "None", ":", "\n", "            ", "num_samples", "=", "dataset_size", "\n", "", "else", ":", "\n", "            ", "num_samples", "=", "min", "(", "dataset_size", ",", "num_samples", ")", "\n", "", "assert", "self", ".", "capacity", ">=", "num_samples", ",", "'Dataset cannot be larger than the replay buffer capacity.'", "\n", "\n", "if", "num_samples", "<", "dataset_size", ":", "\n", "            ", "perm", "=", "np", ".", "random", ".", "permutation", "(", "dataset_size", ")", "\n", "indices", "=", "perm", "[", ":", "num_samples", "]", "\n", "", "else", ":", "\n", "            ", "indices", "=", "np", ".", "arange", "(", "num_samples", ")", "\n", "\n", "", "self", ".", "observations", "[", ":", "num_samples", "]", "=", "dataset", ".", "observations", "[", "indices", "]", "\n", "self", ".", "actions", "[", ":", "num_samples", "]", "=", "dataset", ".", "actions", "[", "indices", "]", "\n", "self", ".", "rewards", "[", ":", "num_samples", "]", "=", "dataset", ".", "rewards", "[", "indices", "]", "\n", "self", ".", "masks", "[", ":", "num_samples", "]", "=", "dataset", ".", "masks", "[", "indices", "]", "\n", "self", ".", "dones_float", "[", ":", "num_samples", "]", "=", "dataset", ".", "dones_float", "[", "indices", "]", "\n", "self", ".", "next_observations", "[", ":", "num_samples", "]", "=", "dataset", ".", "next_observations", "[", "\n", "indices", "]", "\n", "\n", "self", ".", "insert_index", "=", "num_samples", "\n", "self", ".", "size", "=", "num_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.datasets.replay_buffer.ReplayBuffer.insert": [[69, 81], ["min"], "methods", ["None"], ["", "def", "insert", "(", "self", ",", "observation", ":", "np", ".", "ndarray", ",", "action", ":", "np", ".", "ndarray", ",", "\n", "reward", ":", "float", ",", "mask", ":", "float", ",", "done_float", ":", "float", ",", "\n", "next_observation", ":", "np", ".", "ndarray", ")", ":", "\n", "        ", "self", ".", "observations", "[", "self", ".", "insert_index", "]", "=", "observation", "\n", "self", ".", "actions", "[", "self", ".", "insert_index", "]", "=", "action", "\n", "self", ".", "rewards", "[", "self", ".", "insert_index", "]", "=", "reward", "\n", "self", ".", "masks", "[", "self", ".", "insert_index", "]", "=", "mask", "\n", "self", ".", "dones_float", "[", "self", ".", "insert_index", "]", "=", "done_float", "\n", "self", ".", "next_observations", "[", "self", ".", "insert_index", "]", "=", "next_observation", "\n", "\n", "self", ".", "insert_index", "=", "(", "self", ".", "insert_index", "+", "1", ")", "%", "self", ".", "capacity", "\n", "self", ".", "size", "=", "min", "(", "self", ".", "size", "+", "1", ",", "self", ".", "capacity", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.datasets.replay_buffer.ReplayBuffer.save": [[82, 101], ["os.makedirs", "range", "os.path.dirname", "data_path.split", "pickle.dump", "open"], "methods", ["None"], ["", "def", "save", "(", "self", ",", "data_path", ":", "str", ")", ":", "\n", "# because of memory limits, we will dump the buffer into multiple files", "\n", "        ", "os", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "data_path", ")", ",", "exist_ok", "=", "True", ")", "\n", "chunk_size", "=", "self", ".", "capacity", "//", "self", ".", "n_parts", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "n_parts", ")", ":", "\n", "            ", "data_chunk", "=", "[", "\n", "self", ".", "observations", "[", "i", "*", "chunk_size", ":", "(", "i", "+", "1", ")", "*", "chunk_size", "]", ",", "\n", "self", ".", "actions", "[", "i", "*", "chunk_size", ":", "(", "i", "+", "1", ")", "*", "chunk_size", "]", ",", "\n", "self", ".", "rewards", "[", "i", "*", "chunk_size", ":", "(", "i", "+", "1", ")", "*", "chunk_size", "]", ",", "\n", "self", ".", "masks", "[", "i", "*", "chunk_size", ":", "(", "i", "+", "1", ")", "*", "chunk_size", "]", ",", "\n", "self", ".", "dones_float", "[", "i", "*", "chunk_size", ":", "(", "i", "+", "1", ")", "*", "chunk_size", "]", ",", "\n", "self", ".", "next_observations", "[", "i", "*", "chunk_size", ":", "(", "i", "+", "1", ")", "*", "chunk_size", "]", "\n", "]", "\n", "\n", "data_path_splitted", "=", "data_path", ".", "split", "(", "'buffer'", ")", "\n", "data_path_splitted", "[", "-", "1", "]", "=", "f'_chunk_{i}{data_path_splitted[-1]}'", "\n", "data_path_chunk", "=", "'buffer'", ".", "join", "(", "data_path_splitted", ")", "\n", "pickle", ".", "dump", "(", "data_chunk", ",", "open", "(", "data_path_chunk", ",", "'wb'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.datasets.replay_buffer.ReplayBuffer.load": [[102, 124], ["range", "data_path.split", "pickle.load", "len", "print", "open"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.load"], ["", "", "def", "load", "(", "self", ",", "data_path", ":", "str", ")", ":", "\n", "        ", "chunk_size", "=", "self", ".", "capacity", "//", "self", ".", "n_parts", "\n", "total_size", "=", "0", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "n_parts", ")", ":", "\n", "            ", "data_path_splitted", "=", "data_path", ".", "split", "(", "'buffer'", ")", "\n", "data_path_splitted", "[", "-", "1", "]", "=", "f'_chunk_{i}{data_path_splitted[-1]}'", "\n", "data_path_chunk", "=", "'buffer'", ".", "join", "(", "data_path_splitted", ")", "\n", "data_chunk", "=", "pickle", ".", "load", "(", "open", "(", "data_path_chunk", ",", "\"rb\"", ")", ")", "\n", "total_size", "+=", "len", "(", "data_chunk", "[", "0", "]", ")", "\n", "\n", "self", ".", "observations", "[", "i", "*", "chunk_size", ":", "(", "i", "+", "1", ")", "*", "chunk_size", "]", ",", "self", ".", "actions", "[", "i", "*", "chunk_size", ":", "(", "i", "+", "1", ")", "*", "chunk_size", "]", ",", "self", ".", "rewards", "[", "i", "*", "chunk_size", ":", "(", "i", "+", "1", ")", "*", "chunk_size", "]", ",", "self", ".", "masks", "[", "i", "*", "chunk_size", ":", "(", "i", "+", "1", ")", "*", "chunk_size", "]", ",", "self", ".", "dones_float", "[", "i", "*", "chunk_size", ":", "(", "i", "+", "1", ")", "*", "chunk_size", "]", ",", "self", ".", "next_observations", "[", "i", "*", "chunk_size", ":", "(", "i", "+", "1", ")", "*", "chunk_size", "]", "=", "data_chunk", "\n", "\n", "", "if", "self", ".", "capacity", "!=", "total_size", ":", "\n", "            ", "print", "(", "'WARNING: buffer capacity does not match size of loaded data!'", ")", "\n", "", "self", ".", "insert_index", "=", "0", "\n", "self", ".", "size", "=", "total_size", "\n", "", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.datasets.dataset.Dataset.__init__": [[48, 59], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "observations", ":", "np", ".", "ndarray", ",", "actions", ":", "np", ".", "ndarray", ",", "\n", "rewards", ":", "np", ".", "ndarray", ",", "masks", ":", "np", ".", "ndarray", ",", "\n", "dones_float", ":", "np", ".", "ndarray", ",", "next_observations", ":", "np", ".", "ndarray", ",", "\n", "size", ":", "int", ")", ":", "\n", "        ", "self", ".", "observations", "=", "observations", "\n", "self", ".", "actions", "=", "actions", "\n", "self", ".", "rewards", "=", "rewards", "\n", "self", ".", "masks", "=", "masks", "\n", "self", ".", "dones_float", "=", "dones_float", "\n", "self", ".", "next_observations", "=", "next_observations", "\n", "self", ".", "size", "=", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.datasets.dataset.Dataset.sample": [[60, 67], ["numpy.random.randint", "Batch"], "methods", ["None"], ["", "def", "sample", "(", "self", ",", "batch_size", ":", "int", ")", "->", "Batch", ":", "\n", "        ", "indx", "=", "np", ".", "random", ".", "randint", "(", "self", ".", "size", ",", "size", "=", "batch_size", ")", "\n", "return", "Batch", "(", "observations", "=", "self", ".", "observations", "[", "indx", "]", ",", "\n", "actions", "=", "self", ".", "actions", "[", "indx", "]", ",", "\n", "rewards", "=", "self", ".", "rewards", "[", "indx", "]", ",", "\n", "masks", "=", "self", ".", "masks", "[", "indx", "]", ",", "\n", "next_observations", "=", "self", ".", "next_observations", "[", "indx", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.datasets.dataset.Dataset.get_initial_states": [[68, 100], ["dataset.split_into_trajectories", "split_into_trajectories.sort", "numpy.stack", "numpy.stack.append", "numpy.stack", "numpy.stack.append"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.datasets.dataset.split_into_trajectories"], ["", "def", "get_initial_states", "(", "\n", "self", ",", "\n", "and_action", ":", "bool", "=", "False", "\n", ")", "->", "Union", "[", "np", ".", "ndarray", ",", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", "]", ":", "\n", "        ", "states", "=", "[", "]", "\n", "if", "and_action", ":", "\n", "            ", "actions", "=", "[", "]", "\n", "", "trajs", "=", "split_into_trajectories", "(", "self", ".", "observations", ",", "self", ".", "actions", ",", "\n", "self", ".", "rewards", ",", "self", ".", "masks", ",", "\n", "self", ".", "dones_float", ",", "\n", "self", ".", "next_observations", ")", "\n", "\n", "def", "compute_returns", "(", "traj", ")", ":", "\n", "            ", "episode_return", "=", "0", "\n", "for", "_", ",", "_", ",", "rew", ",", "_", ",", "_", ",", "_", "in", "traj", ":", "\n", "                ", "episode_return", "+=", "rew", "\n", "\n", "", "return", "episode_return", "\n", "\n", "", "trajs", ".", "sort", "(", "key", "=", "compute_returns", ")", "\n", "\n", "for", "traj", "in", "trajs", ":", "\n", "            ", "states", ".", "append", "(", "traj", "[", "0", "]", "[", "0", "]", ")", "\n", "if", "and_action", ":", "\n", "                ", "actions", ".", "append", "(", "traj", "[", "0", "]", "[", "1", "]", ")", "\n", "\n", "", "", "states", "=", "np", ".", "stack", "(", "states", ",", "0", ")", "\n", "if", "and_action", ":", "\n", "            ", "actions", "=", "np", ".", "stack", "(", "actions", ",", "0", ")", "\n", "return", "states", ",", "actions", "\n", "", "else", ":", "\n", "            ", "return", "states", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.datasets.dataset.Dataset.get_monte_carlo_returns": [[101, 114], ["dataset.split_into_trajectories", "numpy.asarray", "enumerate", "mc_returns.append"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.datasets.dataset.split_into_trajectories"], ["", "", "def", "get_monte_carlo_returns", "(", "self", ",", "discount", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "trajs", "=", "split_into_trajectories", "(", "self", ".", "observations", ",", "self", ".", "actions", ",", "\n", "self", ".", "rewards", ",", "self", ".", "masks", ",", "\n", "self", ".", "dones_float", ",", "\n", "self", ".", "next_observations", ")", "\n", "mc_returns", "=", "[", "]", "\n", "for", "traj", "in", "trajs", ":", "\n", "            ", "mc_return", "=", "0.0", "\n", "for", "i", ",", "(", "_", ",", "_", ",", "reward", ",", "_", ",", "_", ",", "_", ")", "in", "enumerate", "(", "traj", ")", ":", "\n", "                ", "mc_return", "+=", "reward", "*", "(", "discount", "**", "i", ")", "\n", "", "mc_returns", ".", "append", "(", "mc_return", ")", "\n", "\n", "", "return", "np", ".", "asarray", "(", "mc_returns", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.datasets.dataset.Dataset.take_top": [[115, 141], ["dataset.split_into_trajectories", "split_into_trajectories.sort", "int", "max", "dataset.merge_trajectories", "len", "len"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.datasets.dataset.split_into_trajectories", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.datasets.dataset.merge_trajectories"], ["", "def", "take_top", "(", "self", ",", "percentile", ":", "float", "=", "100.0", ")", ":", "\n", "        ", "assert", "percentile", ">", "0.0", "and", "percentile", "<=", "100.0", "\n", "\n", "trajs", "=", "split_into_trajectories", "(", "self", ".", "observations", ",", "self", ".", "actions", ",", "\n", "self", ".", "rewards", ",", "self", ".", "masks", ",", "\n", "self", ".", "dones_float", ",", "\n", "self", ".", "next_observations", ")", "\n", "\n", "def", "compute_returns", "(", "traj", ")", ":", "\n", "            ", "episode_return", "=", "0", "\n", "for", "_", ",", "_", ",", "rew", ",", "_", ",", "_", ",", "_", "in", "traj", ":", "\n", "                ", "episode_return", "+=", "rew", "\n", "\n", "", "return", "episode_return", "\n", "\n", "", "trajs", ".", "sort", "(", "key", "=", "compute_returns", ")", "\n", "\n", "N", "=", "int", "(", "len", "(", "trajs", ")", "*", "percentile", "/", "100", ")", "\n", "N", "=", "max", "(", "1", ",", "N", ")", "\n", "\n", "trajs", "=", "trajs", "[", "-", "N", ":", "]", "\n", "\n", "(", "self", ".", "observations", ",", "self", ".", "actions", ",", "self", ".", "rewards", ",", "self", ".", "masks", ",", "\n", "self", ".", "dones_float", ",", "self", ".", "next_observations", ")", "=", "merge_trajectories", "(", "trajs", ")", "\n", "\n", "self", ".", "size", "=", "len", "(", "self", ".", "observations", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.datasets.dataset.Dataset.take_random": [[142, 160], ["dataset.split_into_trajectories", "numpy.random.shuffle", "int", "max", "dataset.merge_trajectories", "len", "len"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.datasets.dataset.split_into_trajectories", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.datasets.dataset.merge_trajectories"], ["", "def", "take_random", "(", "self", ",", "percentage", ":", "float", "=", "100.0", ")", ":", "\n", "        ", "assert", "percentage", ">", "0.0", "and", "percentage", "<=", "100.0", "\n", "\n", "trajs", "=", "split_into_trajectories", "(", "self", ".", "observations", ",", "self", ".", "actions", ",", "\n", "self", ".", "rewards", ",", "self", ".", "masks", ",", "\n", "self", ".", "dones_float", ",", "\n", "self", ".", "next_observations", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "trajs", ")", "\n", "\n", "N", "=", "int", "(", "len", "(", "trajs", ")", "*", "percentage", "/", "100", ")", "\n", "N", "=", "max", "(", "1", ",", "N", ")", "\n", "\n", "trajs", "=", "trajs", "[", "-", "N", ":", "]", "\n", "\n", "(", "self", ".", "observations", ",", "self", ".", "actions", ",", "self", ".", "rewards", ",", "self", ".", "masks", ",", "\n", "self", ".", "dones_float", ",", "self", ".", "next_observations", ")", "=", "merge_trajectories", "(", "trajs", ")", "\n", "\n", "self", ".", "size", "=", "len", "(", "self", ".", "observations", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.datasets.dataset.split_into_trajectories": [[12, 23], ["tqdm.tqdm", "range", "trajs[].append", "len", "trajs.append", "len"], "function", ["None"], ["def", "split_into_trajectories", "(", "observations", ",", "actions", ",", "rewards", ",", "masks", ",", "dones_float", ",", "\n", "next_observations", ")", ":", "\n", "    ", "trajs", "=", "[", "[", "]", "]", "\n", "\n", "for", "i", "in", "tqdm", "(", "range", "(", "len", "(", "observations", ")", ")", ")", ":", "\n", "        ", "trajs", "[", "-", "1", "]", ".", "append", "(", "(", "observations", "[", "i", "]", ",", "actions", "[", "i", "]", ",", "rewards", "[", "i", "]", ",", "masks", "[", "i", "]", ",", "\n", "dones_float", "[", "i", "]", ",", "next_observations", "[", "i", "]", ")", ")", "\n", "if", "dones_float", "[", "i", "]", "==", "1.0", "and", "i", "+", "1", "<", "len", "(", "observations", ")", ":", "\n", "            ", "trajs", ".", "append", "(", "[", "]", ")", "\n", "\n", "", "", "return", "trajs", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.datasets.dataset.merge_trajectories": [[25, 45], ["numpy.stack", "numpy.stack", "numpy.stack", "numpy.stack", "numpy.stack", "numpy.stack", "observations.append", "actions.append", "rewards.append", "masks.append", "dones_float.append", "next_observations.append"], "function", ["None"], ["", "def", "merge_trajectories", "(", "trajs", ")", ":", "\n", "    ", "observations", "=", "[", "]", "\n", "actions", "=", "[", "]", "\n", "rewards", "=", "[", "]", "\n", "masks", "=", "[", "]", "\n", "dones_float", "=", "[", "]", "\n", "next_observations", "=", "[", "]", "\n", "\n", "for", "traj", "in", "trajs", ":", "\n", "        ", "for", "(", "obs", ",", "act", ",", "rew", ",", "mask", ",", "done", ",", "next_obs", ")", "in", "traj", ":", "\n", "            ", "observations", ".", "append", "(", "obs", ")", "\n", "actions", ".", "append", "(", "act", ")", "\n", "rewards", ".", "append", "(", "rew", ")", "\n", "masks", ".", "append", "(", "mask", ")", "\n", "dones_float", ".", "append", "(", "done", ")", "\n", "next_observations", ".", "append", "(", "next_obs", ")", "\n", "\n", "", "", "return", "np", ".", "stack", "(", "observations", ")", ",", "np", ".", "stack", "(", "actions", ")", ",", "np", ".", "stack", "(", "\n", "rewards", ")", ",", "np", ".", "stack", "(", "masks", ")", ",", "np", ".", "stack", "(", "dones_float", ")", ",", "np", ".", "stack", "(", "\n", "next_observations", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent.__init__": [[317, 445], ["absl.logging.info", "absl.logging.info", "absl.logging.info", "absl.logging.info", "absl.logging.info", "absl.logging.info", "absl.logging.info", "absl.logging.info", "absl.logging.info", "absl.logging.info", "absl.logging.info", "absl.logging.info", "absl.logging.info", "float", "jax.linspace", "jax.linspace", "int", "int", "int", "int", "int", "int", "dopamine.jax.agents.dqn.dqn_agent.JaxDQNAgent.__init__", "functools.partial"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.__init__"], ["def", "__init__", "(", "self", ",", "\n", "num_actions", ",", "\n", "noisy", "=", "False", ",", "\n", "dueling", "=", "False", ",", "\n", "double_dqn", "=", "False", ",", "\n", "distributional", "=", "True", ",", "\n", "data_augmentation", "=", "False", ",", "\n", "network", "=", "networks", ".", "RainbowDQNNetwork", ",", "\n", "num_atoms", "=", "51", ",", "\n", "vmax", "=", "10.", ",", "\n", "vmin", "=", "None", ",", "\n", "jumps", "=", "5", ",", "\n", "spr_weight", "=", "5", ",", "\n", "batch_size", "=", "32", ",", "\n", "replay_ratio", "=", "64", ",", "\n", "log_every", "=", "100", ",", "\n", "epsilon_fn", "=", "dqn_agent", ".", "linearly_decaying_epsilon", ",", "\n", "replay_scheme", "=", "'prioritized'", ",", "\n", "replay_type", "=", "'deterministic'", ",", "\n", "reset_every", "=", "-", "1", ",", "\n", "reset_offset", "=", "0", ",", "\n", "total_resets", "=", "0", ",", "\n", "encoder_warmup", "=", "0", ",", "\n", "head_warmup", "=", "0", ",", "\n", "reset_head", "=", "False", ",", "\n", "reset_projection", "=", "False", ",", "\n", "reset_encoder", "=", "False", ",", "\n", "reset_noise", "=", "False", ",", "\n", "updates_on_reset", "=", "0", ",", "\n", "summary_writer", "=", "None", ",", "\n", "seed", "=", "None", ")", ":", "\n", "        ", "\"\"\"Initializes the agent and constructs the necessary components.\n\n        Args:\n            num_actions: int, number of actions the agent can take at any state.\n            noisy: bool, Whether to use noisy networks or not.\n            dueling: bool, Whether to use dueling network architecture or not.\n            double_dqn: bool, Whether to use Double DQN or not.\n            distributional: bool, whether to use distributional RL or not.\n            data_augmentation: bool, Whether to use data augmentation or not.\n            network: flax.linen Module, neural network used by the agent initialized\n                by shape in _create_network below. See\n                dopamine.jax.networks.RainbowNetwork as an example.\n            num_atoms: int, the number of buckets of the value function distribution.\n            vmax: float, the value distribution support is [vmin, vmax].\n            vmin: float, the value distribution support is [vmin, vmax]. If vmin is\n                None, it is set to -vmax.\n            jumps: int, how many steps to use for SPR. 5 is original.\n            spr_weight: float, what weight to give SPR loss. 5 is original.\n            batch_size: int, batch size for training.\n            log_every: int, how often to log\n            epsilon_fn: function expecting 4 parameters: (decay_period, step,\n                warmup_steps, epsilon). This function should return the epsilon value\n                used for exploration during training.\n            replay_scheme: str, 'prioritized' or 'uniform', the sampling scheme of the\n                replay memory.\n            replay_type: str, 'deterministic' or 'regular', specifies the type of\n                replay buffer to create.\n            reset_every: int, how often to reset.\n            reset_offset: int, offsets reset period.\n            total_resets: int, how many resets to perform during training.\n            encoder_warmup: int, LR warmup steps for encoder after resets.\n            head_warmup: int, LR warmup steps for head after resets.\n            reset_head: bool, whether or not to reset final layer\n            reset_projection: bool, reset penultimate layer\n            reset_encoder: bool, reset CNN encoder\n            reset_noise: bool, reset noisy nets parameters in head.\n            updates_on_reset: int, how many offline updates to perform after\n                each reset.\n            summary_writer: SummaryWriter object, for outputting training statistics.\n            seed: int, a seed for Jax RNG and initialization.\n        \"\"\"", "\n", "logging", ".", "info", "(", "'Creating %s agent with the following parameters:'", ",", "\n", "self", ".", "__class__", ".", "__name__", ")", "\n", "logging", ".", "info", "(", "'\\t double_dqn: %s'", ",", "double_dqn", ")", "\n", "logging", ".", "info", "(", "'\\t noisy_networks: %s'", ",", "noisy", ")", "\n", "logging", ".", "info", "(", "'\\t dueling_dqn: %s'", ",", "dueling", ")", "\n", "logging", ".", "info", "(", "'\\t distributional: %s'", ",", "distributional", ")", "\n", "logging", ".", "info", "(", "'\\t data_augmentation: %s'", ",", "data_augmentation", ")", "\n", "logging", ".", "info", "(", "'\\t replay_scheme: %s'", ",", "replay_scheme", ")", "\n", "logging", ".", "info", "(", "'\\t total_resets: %s'", ",", "total_resets", ")", "\n", "logging", ".", "info", "(", "'\\t reset_every: %s'", ",", "reset_every", ")", "\n", "logging", ".", "info", "(", "'\\t reset_encoder: %s'", ",", "reset_encoder", ")", "\n", "logging", ".", "info", "(", "'\\t reset_noise: %s'", ",", "reset_noise", ")", "\n", "logging", ".", "info", "(", "'\\t reset_projection: %s'", ",", "reset_projection", ")", "\n", "logging", ".", "info", "(", "'\\t updates_on_reset: %s'", ",", "updates_on_reset", ")", "\n", "# We need this because some tools convert round floats into ints.", "\n", "vmax", "=", "float", "(", "vmax", ")", "\n", "self", ".", "_num_atoms", "=", "num_atoms", "\n", "vmin", "=", "vmin", "if", "vmin", "else", "-", "vmax", "\n", "self", ".", "_support", "=", "jnp", ".", "linspace", "(", "vmin", ",", "vmax", ",", "num_atoms", ")", "\n", "self", ".", "_replay_scheme", "=", "replay_scheme", "\n", "self", ".", "_replay_type", "=", "replay_type", "\n", "self", ".", "_double_dqn", "=", "double_dqn", "\n", "self", ".", "_noisy", "=", "noisy", "\n", "self", ".", "_dueling", "=", "dueling", "\n", "self", ".", "_distributional", "=", "distributional", "\n", "self", ".", "_data_augmentation", "=", "data_augmentation", "\n", "self", ".", "_replay_ratio", "=", "replay_ratio", "\n", "self", ".", "_batch_size", "=", "batch_size", "\n", "self", ".", "_jumps", "=", "jumps", "\n", "self", ".", "spr_weight", "=", "spr_weight", "\n", "self", ".", "log_every", "=", "log_every", "\n", "\n", "self", ".", "reset_every", "=", "int", "(", "reset_every", ")", "\n", "self", ".", "reset_offset", "=", "int", "(", "reset_offset", ")", "\n", "self", ".", "reset_head", "=", "reset_head", "\n", "self", ".", "reset_projection", "=", "reset_projection", "\n", "self", ".", "reset_encoder", "=", "reset_encoder", "\n", "self", ".", "reset_noise", "=", "reset_noise", "\n", "self", ".", "updates_on_reset", "=", "int", "(", "updates_on_reset", ")", "\n", "self", ".", "remaining_resets", "=", "int", "(", "total_resets", ")", "\n", "\n", "self", ".", "encoder_warmup", "=", "int", "(", "encoder_warmup", ")", "\n", "self", ".", "head_warmup", "=", "int", "(", "head_warmup", ")", "\n", "\n", "self", ".", "replay_elements", "=", "None", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "num_actions", "=", "num_actions", ",", "\n", "network", "=", "functools", ".", "partial", "(", "\n", "network", ",", "num_atoms", "=", "num_atoms", ",", "\n", "noisy", "=", "self", ".", "_noisy", ",", "\n", "dueling", "=", "self", ".", "_dueling", ",", "\n", "distributional", "=", "self", ".", "_distributional", ")", ",", "\n", "epsilon_fn", "=", "epsilon_fn", ",", "\n", "summary_writer", "=", "summary_writer", ",", "\n", "seed", "=", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent._build_networks_and_optimizer": [[446, 472], ["jax.random.split", "jax.random.split", "jax.random.split", "jax.random.split", "dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent.network_def.init", "rainbow_agent.create_optimizer", "rainbow_agent.create_optimizer", "flax.core.frozen_dict.FrozenDict", "flax.core.frozen_dict.FrozenDict", "optax.chain", "dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent.optimizer.init", "copy.deepcopy", "optax.masked", "optax.masked", "jax.zeros", "jax.zeros"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.create_optimizer", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.create_optimizer"], ["", "def", "_build_networks_and_optimizer", "(", "self", ")", ":", "\n", "        ", "self", ".", "_rng", ",", "rng", "=", "jax", ".", "random", ".", "split", "(", "self", ".", "_rng", ")", "\n", "self", ".", "state_shape", "=", "self", ".", "state", ".", "shape", "\n", "self", ".", "online_params", "=", "self", ".", "network_def", ".", "init", "(", "rng", ",", "x", "=", "self", ".", "state", ",", "\n", "actions", "=", "jnp", ".", "zeros", "(", "(", "5", ",", ")", ")", ",", "\n", "do_rollout", "=", "self", ".", "spr_weight", ">", "0", ",", "\n", "support", "=", "self", ".", "_support", ")", "\n", "optimizer", "=", "create_optimizer", "(", "self", ".", "_optimizer_name", ",", "warmup", "=", "self", ".", "head_warmup", ")", "\n", "encoder_optimizer", "=", "create_optimizer", "(", "self", ".", "_optimizer_name", ",", "warmup", "=", "self", ".", "encoder_warmup", ")", "\n", "\n", "self", ".", "encoder_mask", "=", "FrozenDict", "(", "\n", "{", "\"params\"", ":", "{", "\"encoder\"", ":", "True", ",", "\"transition_model\"", ":", "True", ",", "\n", "\"head\"", ":", "False", ",", "\"projection\"", ":", "False", ",", "\"predictor\"", ":", "False", "}", "}", "\n", ")", "\n", "self", ".", "head_mask", "=", "FrozenDict", "(", "\n", "{", "\"params\"", ":", "{", "\"encoder\"", ":", "False", ",", "\"transition_model\"", ":", "False", ",", "\n", "\"head\"", ":", "True", ",", "\"projection\"", ":", "True", ",", "\"predictor\"", ":", "True", "}", "}", "\n", ")", "\n", "\n", "self", ".", "optimizer", "=", "optax", ".", "chain", "(", "\n", "optax", ".", "masked", "(", "encoder_optimizer", ",", "self", ".", "encoder_mask", ")", ",", "\n", "optax", ".", "masked", "(", "optimizer", ",", "self", ".", "head_mask", ")", ",", "\n", ")", "\n", "\n", "self", ".", "optimizer_state", "=", "self", ".", "optimizer", ".", "init", "(", "self", ".", "online_params", ")", "\n", "self", ".", "target_network_params", "=", "copy", ".", "deepcopy", "(", "self", ".", "online_params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent._build_replay_buffer": [[473, 513], ["time.time", "print", "print", "print", "print", "ValueError", "ValueError", "discrete_control.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer", "discrete_control.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer"], "methods", ["None"], ["", "def", "_build_replay_buffer", "(", "self", ")", ":", "\n", "        ", "\"\"\"Creates the replay buffer used by the agent.\"\"\"", "\n", "if", "self", ".", "_replay_scheme", "not", "in", "[", "'uniform'", ",", "'prioritized'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "'Invalid replay scheme: {}'", ".", "format", "(", "self", ".", "_replay_scheme", ")", ")", "\n", "", "if", "self", ".", "_replay_type", "not", "in", "[", "'deterministic'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "'Invalid replay type: {}'", ".", "format", "(", "self", ".", "_replay_type", ")", ")", "\n", "", "if", "self", ".", "_replay_scheme", "==", "\"prioritized\"", ":", "\n", "            ", "buffer", "=", "tdrbs", ".", "PrioritizedJaxSubsequenceParallelEnvReplayBuffer", "(", "\n", "observation_shape", "=", "self", ".", "observation_shape", ",", "\n", "stack_size", "=", "self", ".", "stack_size", ",", "\n", "update_horizon", "=", "self", ".", "update_horizon", ",", "\n", "gamma", "=", "self", ".", "gamma", ",", "\n", "subseq_len", "=", "self", ".", "_jumps", "+", "1", ",", "\n", "observation_dtype", "=", "self", ".", "observation_dtype", ",", ")", "\n", "", "else", ":", "\n", "            ", "buffer", "=", "tdrbs", ".", "JaxSubsequenceParallelEnvReplayBuffer", "(", "\n", "observation_shape", "=", "self", ".", "observation_shape", ",", "\n", "stack_size", "=", "self", ".", "stack_size", ",", "\n", "update_horizon", "=", "self", ".", "update_horizon", ",", "\n", "gamma", "=", "self", ".", "gamma", ",", "\n", "subseq_len", "=", "self", ".", "_jumps", "+", "1", ",", "\n", "observation_dtype", "=", "self", ".", "observation_dtype", ",", ")", "\n", "\n", "", "self", ".", "_batch_size", "=", "buffer", ".", "_batch_size", "\n", "self", ".", "n_envs", "=", "buffer", ".", "n_envs", "\n", "self", ".", "start", "=", "time", ".", "time", "(", ")", "\n", "\n", "print", "(", "\"Operating with {} environments, batch size {} and replay ratio {}\"", ".", "format", "(", "self", ".", "n_envs", ",", "self", ".", "_batch_size", ",", "\n", "self", ".", "_replay_ratio", ")", ")", "\n", "self", ".", "_num_updates_per_train_step", "=", "self", ".", "_replay_ratio", "*", "self", ".", "n_envs", "//", "self", ".", "_batch_size", "\n", "print", "(", "\"Calculated {} updates per step\"", ".", "format", "(", "self", ".", "_num_updates_per_train_step", ")", ")", "\n", "\n", "print", "(", "\"Setting min_replay_history to {} from {}\"", ".", "format", "(", "self", ".", "min_replay_history", "/", "self", ".", "n_envs", ",", "\n", "self", ".", "min_replay_history", ")", ")", "\n", "print", "(", "\"Setting epsilon_decay_period to {} from {}\"", ".", "format", "(", "self", ".", "epsilon_decay_period", "/", "self", ".", "n_envs", ",", "\n", "self", ".", "epsilon_decay_period", ")", ")", "\n", "self", ".", "min_replay_history", "=", "(", "self", ".", "min_replay_history", "/", "self", ".", "n_envs", ")", "\n", "self", ".", "epsilon_decay_period", "=", "(", "self", ".", "epsilon_decay_period", "/", "self", ".", "n_envs", ")", "\n", "\n", "return", "buffer", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent._sample_from_replay_buffer": [[514, 521], ["jax.random.split", "jax.random.split", "jax.random.split", "jax.random.split", "dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent._replay.sample_transition_batch", "dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent._replay.get_transition_elements", "collections.OrderedDict", "zip"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.sample_transition_batch", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.get_transition_elements"], ["", "def", "_sample_from_replay_buffer", "(", "self", ")", ":", "\n", "        ", "self", ".", "_rng", ",", "rng", "=", "jax", ".", "random", ".", "split", "(", "self", ".", "_rng", ")", "\n", "samples", "=", "self", ".", "_replay", ".", "sample_transition_batch", "(", "rng", ")", "\n", "types", "=", "self", ".", "_replay", ".", "get_transition_elements", "(", ")", "\n", "self", ".", "replay_elements", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "element", ",", "element_type", "in", "zip", "(", "samples", ",", "types", ")", ":", "\n", "            ", "self", ".", "replay_elements", "[", "element_type", ".", "name", "]", "=", "element", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent.reset_weights": [[522, 566], ["jax.random.split", "jax.random.split", "jax.random.split", "jax.random.split", "dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent.network_def.init", "dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent.optimizer.init", "range", "tuple", "flax.core.frozen_dict.FrozenDict", "print", "range", "print", "print", "len", "len", "dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent.state[].reshape", "dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent.state.reshape", "len", "rainbow_agent.copy_params", "flax.core.frozen_dict.FrozenDict", "updated_optim_state.append", "rainbow_agent.copy_params", "dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent._train_step", "jax.zeros", "jax.zeros", "dict", "dict", "optim_state[]._replace", "dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent.optimizer_state[]._asdict", "optim_state[]._asdict"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.copy_params", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.copy_params", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent._train_step"], ["", "", "def", "reset_weights", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "remaining_resets", "<=", "0", ":", "\n", "            ", "print", "(", "\"All resets completed, skipping\"", ")", "\n", "return", "\n", "", "else", ":", "\n", "            ", "self", ".", "remaining_resets", "-=", "1", "\n", "print", "(", "\"Resetting weights; {} resets remaining\"", ".", "format", "(", "self", ".", "remaining_resets", ")", ")", "\n", "\n", "", "self", ".", "_rng", ",", "rng", "=", "jax", ".", "random", ".", "split", "(", "self", ".", "_rng", ")", "\n", "if", "len", "(", "self", ".", "state_shape", ")", "<", "len", "(", "self", ".", "state", ".", "shape", ")", ":", "\n", "            ", "state", "=", "self", ".", "state", "[", "0", "]", ".", "reshape", "(", "*", "self", ".", "state_shape", ")", "\n", "", "else", ":", "\n", "            ", "state", "=", "self", ".", "state", ".", "reshape", "(", "*", "self", ".", "state_shape", ")", "\n", "", "online_network_params", "=", "self", ".", "network_def", ".", "init", "(", "rng", ",", "x", "=", "state", ",", "\n", "actions", "=", "jnp", ".", "zeros", "(", "(", "5", ",", ")", ")", ",", "\n", "do_rollout", "=", "self", ".", "spr_weight", ">", "0", ",", "\n", "support", "=", "self", ".", "_support", ")", "\n", "optim_state", "=", "self", ".", "optimizer", ".", "init", "(", "self", ".", "online_params", ")", "\n", "\n", "keys_to_copy", "=", "[", "]", "\n", "if", "not", "self", ".", "reset_projection", ":", "\n", "            ", "keys_to_copy", "+=", "[", "\"projection\"", ",", "\"predictor\"", "]", "\n", "", "if", "not", "self", ".", "reset_encoder", ":", "\n", "            ", "keys_to_copy", "+=", "[", "\"encoder\"", ",", "\"transition_model\"", "]", "\n", "", "if", "not", "self", ".", "reset_noise", ":", "\n", "            ", "keys_to_copy", "+=", "[", "\"kernell\"", ",", "\"biass\"", "]", "\n", "\n", "", "updated_optim_state", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "optim_state", ")", ")", ":", "\n", "            ", "optim_to_copy", "=", "copy_params", "(", "dict", "(", "self", ".", "optimizer_state", "[", "i", "]", ".", "_asdict", "(", ")", ")", ",", "\n", "dict", "(", "optim_state", "[", "i", "]", ".", "_asdict", "(", ")", ")", ",", "\n", "keys", "=", "keys_to_copy", ")", "\n", "optim_to_copy", "=", "FrozenDict", "(", "optim_to_copy", ")", "\n", "updated_optim_state", ".", "append", "(", "optim_state", "[", "i", "]", ".", "_replace", "(", "**", "optim_to_copy", ")", ")", "\n", "\n", "", "self", ".", "optimizer_state", "=", "tuple", "(", "updated_optim_state", ")", "\n", "self", ".", "online_params", "=", "FrozenDict", "(", "copy_params", "(", "self", ".", "online_params", ",", "\n", "online_network_params", ",", "\n", "keys", "=", "keys_to_copy", ")", ")", "\n", "\n", "print", "(", "\"Running {} steps after reset\"", ".", "format", "(", "self", ".", "updates_on_reset", ")", ")", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "updates_on_reset", ")", ":", "\n", "            ", "self", ".", "_train_step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent.preprocess_states": [[567, 577], ["jax.random.split", "jax.random.split", "jax.random.split", "jax.random.split", "discrete_control.networks.process_inputs", "discrete_control.networks.process_inputs"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.process_inputs", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.process_inputs"], ["", "", "def", "preprocess_states", "(", "self", ")", ":", "\n", "        ", "self", ".", "_rng", ",", "rng1", ",", "rng2", "=", "jax", ".", "random", ".", "split", "(", "self", ".", "_rng", ",", "num", "=", "3", ")", "\n", "self", ".", "replay_elements", "[", "'state'", "]", "=", "networks", ".", "process_inputs", "(", "\n", "self", ".", "replay_elements", "[", "'state'", "]", ",", "\n", "rng", "=", "rng1", ",", "\n", "data_augmentation", "=", "self", ".", "_data_augmentation", ")", "\n", "self", ".", "replay_elements", "[", "'next_state'", "]", "=", "networks", ".", "process_inputs", "(", "\n", "self", ".", "replay_elements", "[", "'next_state'", "]", "[", ":", ",", "0", "]", ",", "\n", "rng", "=", "rng2", ",", "\n", "data_augmentation", "=", "self", ".", "_data_augmentation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent._training_step_update": [[578, 665], ["time.time", "time.time", "time.time", "time.time", "rainbow_agent.train", "dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent._sample_from_replay_buffer", "dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent.preprocess_states", "time.time", "dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent._sample_from_replay_buffer", "dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent.preprocess_states", "time.time", "time.time", "jax.max", "jax.max", "jax.ones", "jax.ones", "dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent._replay.set_priority", "time.time", "jax.sqrt", "jax.sqrt", "jax.sqrt", "jax.sqrt", "tensorflow.compat.v1.Summary", "dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent.summary_writer.add_summary", "tensorflow.compat.v1.Summary.Value", "tensorflow.compat.v1.Summary.Value", "tensorflow.compat.v1.Summary.Value", "tensorflow.compat.v1.Summary.Value", "tensorflow.compat.v1.Summary.Value", "tensorflow.compat.v1.Summary.Value", "tensorflow.compat.v1.Summary.Value", "tensorflow.compat.v1.Summary.Value", "tensorflow.compat.v1.Summary.Value", "tensorflow.compat.v1.Summary.Value", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "dqn_loss.mean", "rainbow_agent.tree_norm", "td_errors.mean", "spr_loss.mean"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.train", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent._sample_from_replay_buffer", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent.preprocess_states", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent._sample_from_replay_buffer", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent.preprocess_states", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.set_priority", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.tree_norm"], ["", "def", "_training_step_update", "(", "self", ",", "step_index", "=", "0", ")", ":", "\n", "        ", "\"\"\"Gradient update during every training step.\"\"\"", "\n", "# print(\"Inter-batch time {}\".format(time.time() - self.start))", "\n", "interbatch_time", "=", "time", ".", "time", "(", ")", "-", "self", ".", "start", "\n", "self", ".", "start", "=", "time", ".", "time", "(", ")", "\n", "train_start", "=", "time", ".", "time", "(", ")", "\n", "if", "self", ".", "replay_elements", "is", "None", ":", "\n", "            ", "self", ".", "_sample_from_replay_buffer", "(", ")", "\n", "self", ".", "preprocess_states", "(", ")", "\n", "# print(\"Sampling took {}\".format(time.time() - train_start))", "\n", "", "sampling_time", "=", "time", ".", "time", "(", ")", "-", "train_start", "\n", "train_start", "=", "time", ".", "time", "(", ")", "\n", "\n", "aug_time", "=", "time", ".", "time", "(", ")", "-", "train_start", "\n", "train_start", "=", "time", ".", "time", "(", ")", "\n", "\n", "if", "self", ".", "_replay_scheme", "==", "'prioritized'", ":", "\n", "# The original prioritized experience replay uses a linear exponent", "\n", "# schedule 0.4 -> 1.0. Comparing the schedule to a fixed exponent of", "\n", "# 0.5 on 5 games (Asterix, Pong, Q*Bert, Seaquest, Space Invaders)", "\n", "# suggested a fixed exponent actually performs better, except on Pong.", "\n", "            ", "probs", "=", "self", ".", "replay_elements", "[", "'sampling_probabilities'", "]", "\n", "probs", "=", "probs", "\n", "# Weight the loss by the inverse priorities.", "\n", "loss_weights", "=", "1.0", "/", "jnp", ".", "sqrt", "(", "probs", "+", "1e-10", ")", "\n", "loss_weights", "/=", "jnp", ".", "max", "(", "loss_weights", ")", "\n", "", "else", ":", "\n", "# Uniform weights if not using prioritized replay.", "\n", "            ", "loss_weights", "=", "jnp", ".", "ones", "(", "self", ".", "replay_elements", "[", "'state'", "]", ".", "shape", "[", "0", "]", ")", "\n", "\n", "", "self", ".", "optimizer_state", ",", "self", ".", "online_params", ",", "loss", ",", "mean_loss", ",", "dqn_loss", ",", "spr_loss", ",", "grad_norm", ",", "td_errors", ",", "self", ".", "_rng", "=", "train", "(", "\n", "self", ".", "network_def", ",", "self", ".", "online_params", ",", "self", ".", "target_network_params", ",", "\n", "self", ".", "optimizer", ",", "self", ".", "optimizer_state", ",", "\n", "self", ".", "replay_elements", "[", "'state'", "]", ",", "\n", "self", ".", "replay_elements", "[", "'action'", "]", ",", "\n", "self", ".", "replay_elements", "[", "'next_state'", "]", ",", "\n", "self", ".", "replay_elements", "[", "'reward'", "]", "[", ":", ",", "0", "]", ",", "\n", "self", ".", "replay_elements", "[", "'terminal'", "]", "[", ":", ",", "0", "]", ",", "\n", "self", ".", "replay_elements", "[", "'same_trajectory'", "]", "[", ":", ",", "1", ":", "]", ",", "loss_weights", ",", "\n", "self", ".", "_support", ",", "self", ".", "cumulative_gamma", ",", "self", ".", "_double_dqn", ",", "\n", "self", ".", "_distributional", ",", "self", ".", "_rng", ",", "self", ".", "spr_weight", "\n", ")", "\n", "# Jax will happily run train asynchronously unless we block on its", "\n", "# output, so we can sample a new batch while it's running.", "\n", "self", ".", "_sample_from_replay_buffer", "(", ")", "\n", "self", ".", "preprocess_states", "(", ")", "\n", "\n", "if", "self", ".", "_replay_scheme", "==", "'prioritized'", ":", "\n", "# Rainbow and prioritized replay are parametrized by an exponent", "\n", "# alpha, but in both cases it is set to 0.5 - for simplicity's sake we", "\n", "# leave it as is here, using the more direct sqrt(). Taking the square", "\n", "# root \"makes sense\", as we are dealing with a squared loss.  Add a", "\n", "# small nonzero value to the loss to avoid 0 priority items. While", "\n", "# technically this may be okay, setting all items to 0 priority will", "\n", "# cause troubles, and also result in 1.0 / 0.0 = NaN correction terms.", "\n", "            ", "self", ".", "_replay", ".", "set_priority", "(", "self", ".", "replay_elements", "[", "'indices'", "]", ",", "\n", "jnp", ".", "sqrt", "(", "dqn_loss", "+", "1e-10", ")", ")", "\n", "#", "\n", "# print(\"Training took {}\".format(time.time() - train_start))", "\n", "", "training_time", "=", "time", ".", "time", "(", ")", "-", "train_start", "\n", "if", "self", ".", "training_steps", "%", "self", ".", "log_every", "==", "0", "and", "step_index", "==", "0", ":", "\n", "            ", "if", "self", ".", "summary_writer", "is", "not", "None", ":", "\n", "                ", "summary", "=", "tf", ".", "compat", ".", "v1", ".", "Summary", "(", "value", "=", "[", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "'TotalLoss'", ",", "simple_value", "=", "float", "(", "mean_loss", ")", ")", ",", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "'DQNLoss'", ",", "simple_value", "=", "float", "(", "dqn_loss", ".", "mean", "(", ")", ")", ")", ",", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "'GradNorm'", ",", "simple_value", "=", "float", "(", "grad_norm", ")", ")", ",", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "'PNorm'", ",", "simple_value", "=", "float", "(", "tree_norm", "(", "self", ".", "online_params", ")", ")", ")", ",", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "'TD Error'", ",", "simple_value", "=", "float", "(", "td_errors", ".", "mean", "(", ")", ")", ")", ",", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "'SPRLoss'", ",", "simple_value", "=", "float", "(", "spr_loss", ".", "mean", "(", ")", ")", ")", ",", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "'Inter-batch time'", ",", "simple_value", "=", "float", "(", "interbatch_time", ")", ")", ",", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "'Sampling time'", ",", "simple_value", "=", "float", "(", "sampling_time", ")", ")", ",", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "'Augmentation time'", ",", "simple_value", "=", "float", "(", "aug_time", ")", ")", ",", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "'Training time'", ",", "simple_value", "=", "float", "(", "training_time", ")", ")", ",", "\n", "]", ")", "\n", "self", ".", "summary_writer", ".", "add_summary", "(", "summary", ",", "self", ".", "training_steps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent._store_transition": [[666, 696], ["isinstance", "isinstance", "numpy.ones", "dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent._replay.add", "numpy.ones.fill"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.add"], ["", "", "", "def", "_store_transition", "(", "self", ",", "\n", "last_observation", ",", "\n", "action", ",", "\n", "reward", ",", "\n", "is_terminal", ",", "\n", "*", "args", ",", "\n", "priority", "=", "None", ",", "\n", "episode_end", "=", "False", ")", ":", "\n", "        ", "\"\"\"Stores a transition when in training mode.\"\"\"", "\n", "is_prioritized", "=", "(", "\n", "isinstance", "(", "self", ".", "_replay", ",", "\n", "prioritized_replay_buffer", ".", "OutOfGraphPrioritizedReplayBuffer", ")", "\n", "or", "isinstance", "(", "self", ".", "_replay", ",", "\n", "tdrbs", ".", "PrioritizedJaxSubsequenceParallelEnvReplayBuffer", ")", ")", "\n", "if", "is_prioritized", "and", "priority", "is", "None", ":", "\n", "            ", "priority", "=", "onp", ".", "ones", "(", "(", "last_observation", ".", "shape", "[", "0", "]", ")", ")", "\n", "if", "self", ".", "_replay_scheme", "==", "'uniform'", ":", "\n", "                ", "pass", "# Already 1, doesn't matter", "\n", "", "else", ":", "\n", "                ", "priority", ".", "fill", "(", "self", ".", "_replay", ".", "sum_tree", ".", "max_recorded_priority", ")", "\n", "\n", "", "", "if", "not", "self", ".", "eval_mode", ":", "\n", "            ", "self", ".", "_replay", ".", "add", "(", "\n", "last_observation", ",", "\n", "action", ",", "\n", "reward", ",", "\n", "is_terminal", ",", "\n", "*", "args", ",", "\n", "priority", "=", "priority", ",", "\n", "episode_end", "=", "episode_end", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent._train_step": [[697, 721], ["range", "dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent._sync_weights", "print", "dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent.reset_weights", "dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent._training_step_update"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent.reset_weights", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent._training_step_update"], ["", "", "def", "_train_step", "(", "self", ")", ":", "\n", "        ", "\"\"\"Runs a single training step.\n\n        Runs training if both:\n            (1) A minimum number of frames have been added to the replay buffer.\n            (2) `training_steps` is a multiple of `update_period`.\n\n        Also, syncs weights from online_network_params to target_network_params if\n        training steps is a multiple of target update period.\n        \"\"\"", "\n", "if", "self", ".", "_replay", ".", "add_count", ">", "self", ".", "min_replay_history", ":", "\n", "            ", "if", "self", ".", "training_steps", "%", "self", ".", "update_period", "==", "0", ":", "\n", "                ", "for", "i", "in", "range", "(", "self", ".", "_num_updates_per_train_step", ")", ":", "\n", "                    ", "self", ".", "_training_step_update", "(", "i", ")", "\n", "\n", "", "", "if", "self", ".", "training_steps", "%", "self", ".", "target_update_period", "==", "0", ":", "\n", "                ", "self", ".", "_sync_weights", "(", ")", "\n", "\n", "", "if", "(", "self", ".", "training_steps", "+", "1", ")", "%", "(", "self", ".", "reset_every", "+", "self", ".", "updates_on_reset", ")", "==", "self", ".", "reset_offset", "and", "self", ".", "reset_every", ">", "0", ":", "\n", "                ", "print", "(", "\"Resetting weights at {}\"", ".", "format", "(", "self", ".", "training_steps", ")", ")", "\n", "self", ".", "reset_weights", "(", ")", "\n", "\n", "", "", "self", ".", "training_steps", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent._reset_state": [[722, 725], ["numpy.zeros"], "methods", ["None"], ["", "def", "_reset_state", "(", "self", ",", "n_envs", ")", ":", "\n", "        ", "\"\"\"Resets the agent state by filling it with zeros.\"\"\"", "\n", "self", ".", "state", "=", "onp", ".", "zeros", "(", "n_envs", ",", "*", "self", ".", "state_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent._record_observation": [[726, 747], ["observation.squeeze.squeeze.squeeze", "numpy.roll", "len", "len", "numpy.reshape", "numpy.reshape"], "methods", ["None"], ["", "def", "_record_observation", "(", "self", ",", "observation", ")", ":", "\n", "        ", "\"\"\"Records an observation and update state.\n\n        Extracts a frame from the observation vector and overwrites the oldest\n        frame in the state buffer.\n\n        Args:\n          observation: numpy array, an observation from the environment.\n        \"\"\"", "\n", "# Set current observation. We do the reshaping to handle environments", "\n", "# without frame stacking.", "\n", "observation", "=", "observation", ".", "squeeze", "(", "-", "1", ")", "\n", "if", "len", "(", "observation", ".", "shape", ")", "==", "len", "(", "self", ".", "observation_shape", ")", ":", "\n", "            ", "self", ".", "_observation", "=", "onp", ".", "reshape", "(", "observation", ",", "self", ".", "observation_shape", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_observation", "=", "onp", ".", "reshape", "(", "observation", ",", "\n", "(", "observation", ".", "shape", "[", "0", "]", ",", "\n", "*", "self", ".", "observation_shape", ")", ")", "\n", "# Swap out the oldest frame with the current frame.", "\n", "", "self", ".", "state", "=", "onp", ".", "roll", "(", "self", ".", "state", ",", "-", "1", ",", "axis", "=", "-", "1", ")", "\n", "self", ".", "state", "[", "...", ",", "-", "1", "]", "=", "self", ".", "_observation", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent.reset_all": [[748, 753], ["numpy.zeros", "dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent._record_observation"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent._record_observation"], ["", "def", "reset_all", "(", "self", ",", "new_obs", ")", ":", "\n", "        ", "\"\"\"Resets the agent state by filling it with zeros.\"\"\"", "\n", "n_envs", "=", "new_obs", ".", "shape", "[", "0", "]", "\n", "self", ".", "state", "=", "onp", ".", "zeros", "(", "(", "n_envs", ",", "*", "self", ".", "state_shape", ")", ")", "\n", "self", ".", "_record_observation", "(", "new_obs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent.reset_one": [[754, 756], ["dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent.state[].fill"], "methods", ["None"], ["", "def", "reset_one", "(", "self", ",", "env_id", ")", ":", "\n", "        ", "self", ".", "state", "[", "env_id", "]", ".", "fill", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent.delete_one": [[757, 760], ["numpy.concatenate"], "methods", ["None"], ["", "def", "delete_one", "(", "self", ",", "env_id", ")", ":", "\n", "        ", "self", ".", "state", "=", "onp", ".", "concatenate", "(", "[", "self", ".", "state", "[", ":", "env_id", "]", ",", "\n", "self", ".", "state", "[", "env_id", "+", "1", ":", "]", "]", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent.cache_train_state": [[761, 765], ["copy.deepcopy", "copy.deepcopy", "copy.deepcopy"], "methods", ["None"], ["", "def", "cache_train_state", "(", "self", ")", ":", "\n", "        ", "self", ".", "training_state", "=", "(", "copy", ".", "deepcopy", "(", "self", ".", "state", ")", ",", "\n", "copy", ".", "deepcopy", "(", "self", ".", "_last_observation", ")", ",", "\n", "copy", ".", "deepcopy", "(", "self", ".", "_observation", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent.restore_train_state": [[766, 770], ["None"], "methods", ["None"], ["", "def", "restore_train_state", "(", "self", ")", ":", "\n", "        ", "self", ".", "state", ",", "self", ".", "_last_observation", ",", "self", ".", "_observation", "=", "self", ".", "training_state", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent.log_transition": [[771, 778], ["dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent._record_observation", "dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent._store_transition"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent._record_observation", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent._store_transition"], ["", "def", "log_transition", "(", "self", ",", "observation", ",", "action", ",", "reward", ",", "terminal", ",", "episode_end", ")", ":", "\n", "        ", "self", ".", "_last_observation", "=", "self", ".", "_observation", "\n", "self", ".", "_record_observation", "(", "observation", ")", "\n", "\n", "if", "not", "self", ".", "eval_mode", ":", "\n", "            ", "self", ".", "_store_transition", "(", "self", ".", "_last_observation", ",", "action", ",", "reward", ",", "terminal", ",", "\n", "episode_end", "=", "episode_end", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent.step": [[779, 791], ["discrete_control.networks.process_inputs", "rainbow_agent.select_action", "numpy.asarray", "dopamine.jax.agents.rainbow.rainbow_agent.JaxSPRAgent._train_step"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.process_inputs", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.select_action", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent._train_step"], ["", "", "def", "step", "(", "self", ")", ":", "\n", "        ", "\"\"\"Records the most recent transition, returns the agent's next action, and trains if appropriate.\"\"\"", "\n", "if", "not", "self", ".", "eval_mode", ":", "\n", "            ", "self", ".", "_train_step", "(", ")", "\n", "", "state", "=", "networks", ".", "process_inputs", "(", "self", ".", "state", ",", "data_augmentation", "=", "False", ")", "\n", "self", ".", "_rng", ",", "self", ".", "action", "=", "select_action", "(", "\n", "self", ".", "network_def", ",", "self", ".", "online_params", ",", "state", ",", "self", ".", "_rng", ",", "\n", "self", ".", "num_actions", ",", "self", ".", "eval_mode", ",", "self", ".", "epsilon_eval", ",", "self", ".", "epsilon_train", ",", "\n", "self", ".", "epsilon_decay_period", ",", "self", ".", "training_steps", ",", "self", ".", "min_replay_history", ",", "\n", "self", ".", "epsilon_fn", ",", "self", ".", "_support", ")", "\n", "self", ".", "action", "=", "onp", ".", "asarray", "(", "self", ".", "action", ")", "\n", "return", "self", ".", "action", "\n", "", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.copy_within_frozen_tree": [[55, 58], ["old[].copy", "old.copy"], "function", ["None"], ["def", "copy_within_frozen_tree", "(", "old", ",", "new", ",", "prefix", ")", ":", "\n", "    ", "new_entry", "=", "old", "[", "prefix", "]", ".", "copy", "(", "add_or_replace", "=", "new", ")", "\n", "return", "old", ".", "copy", "(", "add_or_replace", "=", "{", "prefix", ":", "new_entry", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.copy_params": [[59, 70], ["isinstance", "isinstance", "isinstance", "old.items", "rainbow_agent.copy_params"], "function", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.copy_params"], ["", "def", "copy_params", "(", "old", ",", "new", ",", "keys", "=", "(", "\"encoder\"", ",", "\"transition_model\"", ")", ")", ":", "\n", "    ", "if", "isinstance", "(", "old", ",", "dict", ")", "or", "isinstance", "(", "old", ",", "OrderedDict", ")", "or", "isinstance", "(", "old", ",", "FrozenDict", ")", ":", "\n", "        ", "fresh_dict", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "old", ".", "items", "(", ")", ":", "\n", "            ", "if", "k", "in", "keys", ":", "\n", "                ", "fresh_dict", "[", "k", "]", "=", "v", "\n", "", "else", ":", "\n", "                ", "fresh_dict", "[", "k", "]", "=", "copy_params", "(", "old", "[", "k", "]", ",", "new", "[", "k", "]", ",", "keys", ")", "\n", "", "", "return", "fresh_dict", "\n", "", "else", ":", "\n", "        ", "return", "new", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.identity_epsilon": [[71, 75], ["None"], "function", ["None"], ["", "", "@", "gin", ".", "configurable", "\n", "def", "identity_epsilon", "(", "unused_decay_period", ",", "unused_step", ",", "unused_warmup_steps", ",", "\n", "epsilon", ")", ":", "\n", "  ", "return", "epsilon", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.tree_norm": [[76, 78], ["jax.sqrt", "sum", "jax.tree_leaves", "jax.tree_leaves"], "function", ["None"], ["", "def", "tree_norm", "(", "tree", ")", ":", "\n", "    ", "return", "jnp", ".", "sqrt", "(", "sum", "(", "(", "x", "**", "2", ")", ".", "sum", "(", ")", "for", "x", "in", "jax", ".", "tree_leaves", "(", "tree", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.select_action": [[79, 105], ["functools.partial", "jax.where", "jax.random.split", "jax.random.split", "jax.random.uniform", "jax.random.uniform", "rainbow_agent.get_q_values_no_actions", "jax.argmax", "jax.where", "epsilon_fn", "network_def.apply", "jax.random.randint", "jax.random.randint"], "function", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.get_q_values_no_actions", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.apply"], ["", "@", "functools", ".", "partial", "(", "jax", ".", "jit", ",", "static_argnums", "=", "(", "0", ",", "4", ",", "5", ",", "6", ",", "7", ",", "8", ",", "10", ",", "11", ")", ")", "\n", "def", "select_action", "(", "network_def", ",", "params", ",", "state", ",", "rng", ",", "num_actions", ",", "eval_mode", ",", "\n", "epsilon_eval", ",", "epsilon_train", ",", "epsilon_decay_period", ",", "\n", "training_steps", ",", "min_replay_history", ",", "epsilon_fn", ",", "support", ")", ":", "\n", "    ", "\"\"\"Select an action from the set of available actions.\"\"\"", "\n", "epsilon", "=", "jnp", ".", "where", "(", "\n", "eval_mode", ",", "epsilon_eval", ",", "\n", "epsilon_fn", "(", "epsilon_decay_period", ",", "training_steps", ",", "min_replay_history", ",", "\n", "epsilon_train", ")", ")", "\n", "\n", "def", "q_online", "(", "state", ",", "key", ",", "actions", "=", "None", ",", "do_rollout", "=", "False", ")", ":", "\n", "        ", "return", "network_def", ".", "apply", "(", "params", ",", "state", ",", "actions", "=", "actions", ",", "\n", "do_rollout", "=", "do_rollout", ",", "key", "=", "key", ",", "\n", "support", "=", "support", ",", "mutable", "=", "[", "\"batch_stats\"", "]", ")", "\n", "\n", "", "rng", ",", "rng1", ",", "rng2", ",", "rng3", "=", "jax", ".", "random", ".", "split", "(", "rng", ",", "num", "=", "4", ")", "\n", "p", "=", "jax", ".", "random", ".", "uniform", "(", "rng1", ",", "shape", "=", "(", "state", ".", "shape", "[", "0", "]", ",", ")", ")", "\n", "q_values", "=", "get_q_values_no_actions", "(", "q_online", ",", "state", ",", "rng2", ")", "\n", "# q_values = network_def.apply(params, state, key=rng2, eval_mode=eval_mode,", "\n", "#                              support=support).q_values", "\n", "\n", "best_actions", "=", "jnp", ".", "argmax", "(", "q_values", ",", "axis", "=", "-", "1", ")", "\n", "new_actions", "=", "jnp", ".", "where", "(", "p", "<=", "epsilon", ",", "\n", "jax", ".", "random", ".", "randint", "(", "rng3", ",", "(", "state", ".", "shape", "[", "0", "]", ",", ")", ",", "0", ",", "num_actions", ",", ")", ",", "\n", "best_actions", ")", "\n", "return", "rng", ",", "new_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.get_q_values_no_actions": [[106, 110], ["functools.partial", "model"], "function", ["None"], ["", "@", "functools", ".", "partial", "(", "jax", ".", "vmap", ",", "in_axes", "=", "(", "None", ",", "0", ",", "None", ")", ",", "axis_name", "=", "\"batch\"", ")", "\n", "def", "get_q_values_no_actions", "(", "model", ",", "states", ",", "rng", ")", ":", "\n", "    ", "results", "=", "model", "(", "states", ",", "actions", "=", "None", ",", "do_rollout", "=", "False", ",", "key", "=", "rng", ")", "[", "0", "]", "\n", "return", "results", ".", "q_values", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.get_logits": [[111, 115], ["functools.partial", "model"], "function", ["None"], ["", "@", "functools", ".", "partial", "(", "jax", ".", "vmap", ",", "in_axes", "=", "(", "None", ",", "0", ",", "0", ",", "None", ",", "None", ")", ",", "axis_name", "=", "\"batch\"", ")", "\n", "def", "get_logits", "(", "model", ",", "states", ",", "actions", ",", "do_rollout", ",", "rng", ")", ":", "\n", "    ", "results", "=", "model", "(", "states", ",", "actions", "=", "actions", ",", "do_rollout", "=", "do_rollout", ",", "key", "=", "rng", ")", "[", "0", "]", "\n", "return", "results", ".", "logits", ",", "results", ".", "latent", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.get_q_values": [[117, 121], ["functools.partial", "model"], "function", ["None"], ["", "@", "functools", ".", "partial", "(", "jax", ".", "vmap", ",", "in_axes", "=", "(", "None", ",", "0", ",", "0", ",", "None", ",", "None", ")", ",", "axis_name", "=", "\"batch\"", ")", "\n", "def", "get_q_values", "(", "model", ",", "states", ",", "actions", ",", "do_rollout", ",", "rng", ")", ":", "\n", "    ", "results", "=", "model", "(", "states", ",", "actions", "=", "actions", ",", "do_rollout", "=", "do_rollout", ",", "key", "=", "rng", ")", "[", "0", "]", "\n", "return", "results", ".", "q_values", ",", "results", ".", "latent", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.get_spr_targets": [[123, 127], ["functools.partial", "model"], "function", ["None"], ["", "@", "functools", ".", "partial", "(", "jax", ".", "vmap", ",", "in_axes", "=", "(", "None", ",", "0", ",", "None", ")", ",", "axis_name", "=", "\"batch\"", ")", "\n", "def", "get_spr_targets", "(", "model", ",", "states", ",", "key", ")", ":", "\n", "    ", "results", "=", "model", "(", "states", ",", "key", ")", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.train": [[129, 241], ["functools.partial", "jax.random.split", "jax.random.split", "jax.value_and_grad", "jax.value_and_grad", "rainbow_agent.target_output", "jax.value_and_grad.", "rainbow_agent.tree_norm", "optimizer.update", "optax.apply_updates", "network_def.apply", "network_def.apply", "network_def.apply", "latent.reshape.reshape", "network_def.apply", "jax.mean", "rainbow_agent.get_spr_targets", "spr_targets.reshape().transpose.reshape().transpose", "network_def.apply", "rainbow_agent.get_logits", "jax.squeeze", "rainbow_agent.get_q_values", "jax.squeeze", "spr_predictions.transpose.transpose", "jax.power().sum", "future_states.reshape", "jax.vmap", "jax.vmap", "jax.vmap", "jax.vmap", "jax.nan_to_num().sum", "jax.vmap", "jax.vmap", "jax.vmap", "jax.vmap", "jax.linalg.norm", "jax.linalg.norm", "spr_targets.reshape().transpose.reshape", "jax.power", "jax.nan_to_num", "same_traj_mask.transpose", "jax.log"], "function", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.target_output", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.tree_norm", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.temperature.update", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.apply", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.apply", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.apply", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.apply", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.get_spr_targets", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.apply", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.get_logits", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.get_q_values"], ["", "@", "functools", ".", "partial", "(", "jax", ".", "jit", ",", "static_argnums", "=", "(", "0", ",", "3", ",", "13", ",", "14", ",", "15", ",", "17", ")", ")", "\n", "def", "train", "(", "network_def", ",", "online_params", ",", "target_params", ",", "optimizer", ",", "optimizer_state", ",", "states", ",", "actions", ",", "next_states", ",", "\n", "rewards", ",", "terminals", ",", "same_traj_mask", ",", "loss_weights", ",", "support", ",", "\n", "cumulative_gamma", ",", "double_dqn", ",", "distributional", ",", "rng", ",", "spr_weight", ")", ":", "\n", "  ", "\"\"\"Run a training step.\"\"\"", "\n", "\n", "current_state", "=", "states", "[", ":", ",", "0", "]", "\n", "online_params", "=", "online_params", "\n", "# Split the current rng into 2 for updating the rng after this call", "\n", "rng", ",", "rng1", ",", "rng2", "=", "jax", ".", "random", ".", "split", "(", "rng", ",", "num", "=", "3", ")", "\n", "use_spr", "=", "spr_weight", ">", "0", "\n", "\n", "def", "q_online", "(", "state", ",", "key", ",", "actions", "=", "None", ",", "do_rollout", "=", "False", ")", ":", "\n", "    ", "return", "network_def", ".", "apply", "(", "\n", "online_params", ",", "\n", "state", ",", "\n", "actions", "=", "actions", ",", "\n", "do_rollout", "=", "do_rollout", ",", "\n", "key", "=", "key", ",", "\n", "support", "=", "support", ",", "\n", "mutable", "=", "[", "\"batch_stats\"", "]", ")", "\n", "\n", "", "def", "q_target", "(", "state", ",", "key", ")", ":", "\n", "    ", "return", "network_def", ".", "apply", "(", "\n", "target_params", ",", "state", ",", "key", "=", "key", ",", "support", "=", "support", ",", "mutable", "=", "[", "\"batch_stats\"", "]", ")", "\n", "\n", "", "def", "encode_project", "(", "state", ",", "key", ")", ":", "\n", "    ", "latent", ",", "_", "=", "network_def", ".", "apply", "(", "\n", "target_params", ",", "\n", "state", ",", "\n", "method", "=", "network_def", ".", "encode", ",", "\n", "mutable", "=", "[", "\"batch_stats\"", "]", ")", "\n", "latent", "=", "latent", ".", "reshape", "(", "-", "1", ")", "\n", "return", "network_def", ".", "apply", "(", "\n", "target_params", ",", "\n", "latent", ",", "\n", "key", "=", "key", ",", "\n", "eval_mode", "=", "True", ",", "\n", "method", "=", "network_def", ".", "project", ")", "\n", "\n", "", "def", "loss_fn", "(", "params", ",", "target", ",", "spr_targets", ",", "loss_multipliers", ")", ":", "\n", "    ", "\"\"\"Computes the distributional loss for C51 or huber loss for DQN.\"\"\"", "\n", "\n", "def", "q_online", "(", "state", ",", "key", ",", "actions", "=", "None", ",", "do_rollout", "=", "False", ")", ":", "\n", "      ", "return", "network_def", ".", "apply", "(", "\n", "params", ",", "\n", "state", ",", "\n", "actions", "=", "actions", ",", "\n", "do_rollout", "=", "do_rollout", ",", "\n", "key", "=", "key", ",", "\n", "support", "=", "support", ",", "\n", "mutable", "=", "[", "\"batch_stats\"", "]", ")", "\n", "\n", "", "if", "distributional", ":", "\n", "      ", "(", "logits", ",", "spr_predictions", ")", "=", "get_logits", "(", "q_online", ",", "current_state", ",", "\n", "actions", "[", ":", ",", ":", "-", "1", "]", ",", "use_spr", ",", "rng", ")", "\n", "logits", "=", "jnp", ".", "squeeze", "(", "logits", ")", "\n", "# Fetch the logits for its selected action. We use vmap to perform this", "\n", "# indexing across the batch.", "\n", "chosen_action_logits", "=", "jax", ".", "vmap", "(", "lambda", "x", ",", "y", ":", "x", "[", "y", "]", ")", "(", "logits", ",", "actions", "[", ":", ",", "0", "]", ")", "\n", "dqn_loss", "=", "jax", ".", "vmap", "(", "losses", ".", "softmax_cross_entropy_loss_with_logits", ")", "(", "\n", "target", ",", "chosen_action_logits", ")", "\n", "td_error", "=", "dqn_loss", "+", "jnp", ".", "nan_to_num", "(", "target", "*", "jnp", ".", "log", "(", "target", ")", ")", ".", "sum", "(", "-", "1", ")", "\n", "", "else", ":", "\n", "      ", "q_values", ",", "spr_predictions", "=", "get_q_values", "(", "q_online", ",", "current_state", ",", "\n", "actions", "[", ":", ",", ":", "-", "1", "]", ",", "use_spr", ",", "rng", ")", "\n", "q_values", "=", "jnp", ".", "squeeze", "(", "q_values", ")", "\n", "replay_chosen_q", "=", "jax", ".", "vmap", "(", "lambda", "x", ",", "y", ":", "x", "[", "y", "]", ")", "(", "q_values", ",", "actions", "[", ":", ",", "0", "]", ")", "\n", "dqn_loss", "=", "jax", ".", "vmap", "(", "losses", ".", "huber_loss", ")", "(", "target", ",", "replay_chosen_q", ")", "\n", "td_error", "=", "dqn_loss", "\n", "\n", "", "if", "use_spr", ":", "\n", "      ", "spr_predictions", "=", "spr_predictions", ".", "transpose", "(", "1", ",", "0", ",", "2", ")", "\n", "spr_predictions", "=", "spr_predictions", "/", "jnp", ".", "linalg", ".", "norm", "(", "\n", "spr_predictions", ",", "2", ",", "-", "1", ",", "keepdims", "=", "True", ")", "\n", "spr_targets", "=", "spr_targets", "/", "jnp", ".", "linalg", ".", "norm", "(", "\n", "spr_targets", ",", "2", ",", "-", "1", ",", "keepdims", "=", "True", ")", "\n", "spr_loss", "=", "jnp", ".", "power", "(", "spr_predictions", "-", "spr_targets", ",", "2", ")", ".", "sum", "(", "-", "1", ")", "\n", "spr_loss", "=", "(", "spr_loss", "*", "same_traj_mask", ".", "transpose", "(", "1", ",", "0", ")", ")", ".", "mean", "(", "0", ")", "\n", "", "else", ":", "\n", "      ", "spr_loss", "=", "0", "\n", "\n", "", "loss", "=", "dqn_loss", "+", "spr_weight", "*", "spr_loss", "\n", "\n", "mean_loss", "=", "jnp", ".", "mean", "(", "loss_multipliers", "*", "loss", ")", "\n", "return", "mean_loss", ",", "(", "loss", ",", "dqn_loss", ",", "spr_loss", ",", "td_error", ")", "\n", "\n", "# Use the weighted mean loss for gradient computation.", "\n", "", "grad_fn", "=", "jax", ".", "value_and_grad", "(", "loss_fn", ",", "has_aux", "=", "True", ")", "\n", "target", "=", "target_output", "(", "q_online", ",", "q_target", ",", "next_states", ",", "rewards", ",", "terminals", ",", "\n", "support", ",", "cumulative_gamma", ",", "double_dqn", ",", "distributional", ",", "\n", "rng1", ")", "\n", "\n", "if", "use_spr", ":", "\n", "    ", "future_states", "=", "states", "[", ":", ",", "1", ":", "]", "\n", "spr_targets", "=", "get_spr_targets", "(", "\n", "encode_project", ",", "future_states", ".", "reshape", "(", "-", "1", ",", "*", "future_states", ".", "shape", "[", "2", ":", "]", ")", ",", "\n", "rng1", ")", "\n", "spr_targets", "=", "spr_targets", ".", "reshape", "(", "*", "future_states", ".", "shape", "[", ":", "2", "]", ",", "\n", "*", "spr_targets", ".", "shape", "[", "1", ":", "]", ")", ".", "transpose", "(", "\n", "1", ",", "0", ",", "2", ")", "\n", "", "else", ":", "\n", "    ", "spr_targets", "=", "None", "\n", "\n", "# Get the unweighted loss without taking its mean for updating priorities.", "\n", "", "(", "mean_loss", ",", "(", "loss", ",", "dqn_loss", ",", "spr_loss", ",", "td_error", ")", ")", ",", "grad", "=", "grad_fn", "(", "online_params", ",", "target", ",", "spr_targets", ",", "loss_weights", ")", "\n", "grad_norm", "=", "tree_norm", "(", "grad", ")", "\n", "updates", ",", "optimizer_state", "=", "optimizer", ".", "update", "(", "grad", ",", "optimizer_state", ")", "\n", "online_params", "=", "optax", ".", "apply_updates", "(", "online_params", ",", "updates", ")", "\n", "return", "optimizer_state", ",", "online_params", ",", "loss", ",", "mean_loss", ",", "dqn_loss", ",", "spr_loss", ",", "grad_norm", ",", "td_error", ",", "rng2", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.target_output": [[243, 275], ["functools.partial", "target_network", "jax.squeeze", "jax.argmax", "jax.lax.stop_gradient", "jax.lax.stop_gradient", "terminals.astype", "model", "jax.squeeze", "dopamine.jax.agents.rainbow.rainbow_agent.project_distribution", "jax.squeeze"], "function", ["None"], ["", "@", "functools", ".", "partial", "(", "\n", "jax", ".", "vmap", ",", "in_axes", "=", "(", "None", ",", "None", ",", "0", ",", "0", ",", "0", ",", "None", ",", "None", ",", "None", ",", "None", ",", "None", ")", ",", "axis_name", "=", "\"batch\"", ")", "\n", "def", "target_output", "(", "model", ",", "target_network", ",", "next_states", ",", "rewards", ",", "terminals", ",", "\n", "support", ",", "cumulative_gamma", ",", "double_dqn", ",", "distributional", ",", "rng", ")", ":", "\n", "    ", "\"\"\"Builds the C51 target distribution or DQN target Q-values.\"\"\"", "\n", "is_terminal_multiplier", "=", "1.", "-", "terminals", ".", "astype", "(", "jnp", ".", "float32", ")", "\n", "# Incorporate terminal state to discount factor.", "\n", "gamma_with_terminal", "=", "cumulative_gamma", "*", "is_terminal_multiplier", "\n", "\n", "target_network_dist", ",", "_", "=", "target_network", "(", "next_states", ",", "key", "=", "rng", ")", "\n", "if", "double_dqn", ":", "\n", "# Use the current network for the action selection", "\n", "        ", "next_state_target_outputs", ",", "_", "=", "model", "(", "next_states", ",", "key", "=", "rng", ")", "\n", "", "else", ":", "\n", "        ", "next_state_target_outputs", "=", "target_network_dist", "\n", "# Action selection using Q-values for next-state", "\n", "", "q_values", "=", "jnp", ".", "squeeze", "(", "next_state_target_outputs", ".", "q_values", ")", "\n", "next_qt_argmax", "=", "jnp", ".", "argmax", "(", "q_values", ")", "\n", "\n", "if", "distributional", ":", "\n", "# Compute the target Q-value distribution", "\n", "        ", "probabilities", "=", "jnp", ".", "squeeze", "(", "target_network_dist", ".", "probabilities", ")", "\n", "next_probabilities", "=", "probabilities", "[", "next_qt_argmax", "]", "\n", "target_support", "=", "rewards", "+", "gamma_with_terminal", "*", "support", "\n", "target", "=", "dopamine_rainbow_agent", ".", "project_distribution", "(", "target_support", ",", "next_probabilities", ",", "support", ")", "\n", "", "else", ":", "\n", "# Compute the target Q-value", "\n", "        ", "next_q_values", "=", "jnp", ".", "squeeze", "(", "target_network_dist", ".", "q_values", ")", "\n", "replay_next_qt_max", "=", "next_q_values", "[", "next_qt_argmax", "]", "\n", "target", "=", "rewards", "+", "gamma_with_terminal", "*", "replay_next_qt_max", "\n", "\n", "", "return", "jax", ".", "lax", ".", "stop_gradient", "(", "target", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.create_optimizer": [[276, 311], ["absl.logging.info", "optax.adam", "optax.inject_hyperparams", "absl.logging.info", "ValueError", "optax.linear_schedule", "optax.rmsprop", "optax.inject_hyperparams", "optax.linear_schedule"], "function", ["None"], ["", "@", "gin", ".", "configurable", "\n", "def", "create_optimizer", "(", "name", "=", "'adam'", ",", "learning_rate", "=", "6.25e-5", ",", "beta1", "=", "0.9", ",", "beta2", "=", "0.999", ",", "\n", "eps", "=", "1.5e-4", ",", "centered", "=", "False", ",", "warmup", "=", "0", ")", ":", "\n", "  ", "\"\"\"Create an optimizer for training.\n\n  Currently, only the Adam and RMSProp optimizers are supported.\n\n  Args:\n    name: str, name of the optimizer to create.\n    learning_rate: float, learning rate to use in the optimizer.\n    beta1: float, beta1 parameter for the optimizer.\n    beta2: float, beta2 parameter for the optimizer.\n    eps: float, epsilon parameter for the optimizer.\n    centered: bool, centered parameter for RMSProp.\n    warmup: int, warmup steps for learning rate.\n\n  Returns:\n    A flax optimizer.\n  \"\"\"", "\n", "if", "name", "==", "'adam'", ":", "\n", "    ", "logging", ".", "info", "(", "'Creating Adam optimizer with settings lr=%f, beta1=%f, '", "\n", "'beta2=%f, eps=%f'", ",", "learning_rate", ",", "beta1", ",", "beta2", ",", "eps", ")", "\n", "if", "warmup", "==", "0", ":", "\n", "        ", "return", "optax", ".", "adam", "(", "learning_rate", ",", "b1", "=", "beta1", ",", "b2", "=", "beta2", ",", "eps", "=", "eps", ")", "\n", "", "return", "optax", ".", "inject_hyperparams", "(", "optax", ".", "adam", ")", "(", "learning_rate", "=", "optax", ".", "linear_schedule", "(", "0", ",", "learning_rate", ",", "warmup", ")", ",", "b1", "=", "beta1", ",", "b2", "=", "beta2", ",", "eps", "=", "eps", ")", "\n", "", "elif", "name", "==", "'rmsprop'", ":", "\n", "    ", "logging", ".", "info", "(", "'Creating RMSProp optimizer with settings lr=%f, beta2=%f, '", "\n", "'eps=%f'", ",", "learning_rate", ",", "beta2", ",", "eps", ")", "\n", "if", "warmup", "==", "0", ":", "\n", "        ", "return", "optax", ".", "rmsprop", "(", "learning_rate", ",", "decay", "=", "beta2", ",", "eps", "=", "eps", ",", "\n", "centered", "=", "centered", ")", "\n", "", "return", "optax", ".", "inject_hyperparams", "(", "optax", ".", "rmsprop", ")", "(", "learning_rate", "=", "optax", ".", "linear_schedule", "(", "0", ",", "learning_rate", ",", "warmup", ")", ",", "decay", "=", "beta2", ",", "eps", "=", "eps", ",", "\n", "centered", "=", "centered", ")", "\n", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "'Unsupported optimizer {}'", ".", "format", "(", "name", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.drq.networks.Encoder.__call__": [[20, 38], ["zip", "len", "len", "observations.astype", "flax.relu", "len", "x.reshape.reshape.reshape", "x.reshape.reshape.reshape", "flax.Conv", "continuous_control.networks.common.default_init"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.default_init"], ["@", "nn", ".", "compact", "\n", "def", "__call__", "(", "self", ",", "observations", ":", "jnp", ".", "ndarray", ")", "->", "jnp", ".", "ndarray", ":", "\n", "        ", "assert", "len", "(", "self", ".", "features", ")", "==", "len", "(", "self", ".", "strides", ")", "\n", "\n", "x", "=", "observations", ".", "astype", "(", "jnp", ".", "float32", ")", "/", "255.0", "\n", "for", "features", ",", "stride", "in", "zip", "(", "self", ".", "features", ",", "self", ".", "strides", ")", ":", "\n", "            ", "x", "=", "nn", ".", "Conv", "(", "features", ",", "\n", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "\n", "strides", "=", "(", "stride", ",", "stride", ")", ",", "\n", "kernel_init", "=", "default_init", "(", ")", ",", "\n", "padding", "=", "self", ".", "padding", ")", "(", "x", ")", "\n", "x", "=", "nn", ".", "relu", "(", "x", ")", "\n", "\n", "", "if", "len", "(", "x", ".", "shape", ")", "==", "4", ":", "\n", "            ", "x", "=", "x", ".", "reshape", "(", "[", "x", ".", "shape", "[", "0", "]", ",", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "x", ".", "reshape", "(", "[", "-", "1", "]", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.drq.networks.DrQDoubleCritic.__call__": [[47, 60], ["flax.tanh", "networks.Encoder", "flax.Dense", "flax.LayerNorm", "continuous_control.networks.critic_net.DoubleCritic"], "methods", ["None"], ["@", "nn", ".", "compact", "\n", "def", "__call__", "(", "self", ",", "observations", ":", "jnp", ".", "ndarray", ",", "\n", "actions", ":", "jnp", ".", "ndarray", ")", "->", "Tuple", "[", "jnp", ".", "ndarray", ",", "jnp", ".", "ndarray", "]", ":", "\n", "        ", "x", "=", "Encoder", "(", "self", ".", "cnn_features", ",", "\n", "self", ".", "cnn_strides", ",", "\n", "self", ".", "cnn_padding", ",", "\n", "name", "=", "'SharedEncoder'", ")", "(", "observations", ")", "\n", "\n", "x", "=", "nn", ".", "Dense", "(", "self", ".", "latent_dim", ")", "(", "x", ")", "\n", "x", "=", "nn", ".", "LayerNorm", "(", ")", "(", "x", ")", "\n", "x", "=", "nn", ".", "tanh", "(", "x", ")", "\n", "\n", "return", "DoubleCritic", "(", "self", ".", "hidden_dims", ")", "(", "x", ",", "actions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.drq.networks.DrQPolicy.__call__": [[70, 88], ["jax.lax.stop_gradient", "jax.lax.stop_gradient", "tensorflow_probability.substrates.jax.lax.stop_gradient", "jax.lax.stop_gradient", "jax.lax.stop_gradient", "tensorflow_probability.substrates.jax.lax.stop_gradient", "tensorflow_probability.substrates.jax.lax.stop_gradient", "flax.tanh", "networks.Encoder", "flax.Dense", "flax.LayerNorm", "continuous_control.networks.policies.NormalTanhPolicy"], "methods", ["None"], ["@", "nn", ".", "compact", "\n", "def", "__call__", "(", "self", ",", "\n", "observations", ":", "jnp", ".", "ndarray", ",", "\n", "temperature", ":", "float", "=", "1.0", ")", "->", "tfd", ".", "Distribution", ":", "\n", "        ", "x", "=", "Encoder", "(", "self", ".", "cnn_features", ",", "\n", "self", ".", "cnn_strides", ",", "\n", "self", ".", "cnn_padding", ",", "\n", "name", "=", "'SharedEncoder'", ")", "(", "observations", ")", "\n", "\n", "# We do not update conv layers with policy gradients.", "\n", "x", "=", "jax", ".", "lax", ".", "stop_gradient", "(", "x", ")", "\n", "\n", "x", "=", "nn", ".", "Dense", "(", "self", ".", "latent_dim", ")", "(", "x", ")", "\n", "x", "=", "nn", ".", "LayerNorm", "(", ")", "(", "x", ")", "\n", "x", "=", "nn", ".", "tanh", "(", "x", ")", "\n", "\n", "return", "NormalTanhPolicy", "(", "self", ".", "hidden_dims", ",", "self", ".", "action_dim", ")", "(", "x", ",", "\n", "temperature", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.drq.drq_learner.DrQLearner.__init__": [[70, 129], ["jax.random.PRNGKey", "jax.random.PRNGKey", "jax.random.PRNGKey", "jax.random.PRNGKey", "jax.random.split", "jax.random.split", "jax.random.split", "jax.random.split", "continuous_control.agents.drq.networks.DrQPolicy", "continuous_control.networks.common.Model.create", "continuous_control.agents.drq.networks.DrQDoubleCritic", "continuous_control.networks.common.ModelDecoupleOpt.create", "continuous_control.networks.common.Model.create", "continuous_control.networks.common.Model.create", "continuous_control.agents.sac.temperature.Temperature", "optax.adam", "optax.adam", "optax.adam", "optax.adam"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.create", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.create", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.create", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.create"], ["    ", "def", "__init__", "(", "self", ",", "\n", "seed", ":", "int", ",", "\n", "observations", ":", "jnp", ".", "ndarray", ",", "\n", "actions", ":", "jnp", ".", "ndarray", ",", "\n", "actor_lr", ":", "float", "=", "3e-4", ",", "\n", "critic_lr", ":", "float", "=", "3e-4", ",", "\n", "temp_lr", ":", "float", "=", "3e-4", ",", "\n", "hidden_dims", ":", "Sequence", "[", "int", "]", "=", "(", "256", ",", "256", ")", ",", "\n", "cnn_features", ":", "Sequence", "[", "int", "]", "=", "(", "32", ",", "32", ",", "32", ",", "32", ")", ",", "\n", "cnn_strides", ":", "Sequence", "[", "int", "]", "=", "(", "2", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "cnn_padding", ":", "str", "=", "'VALID'", ",", "\n", "latent_dim", ":", "int", "=", "50", ",", "\n", "discount", ":", "float", "=", "0.99", ",", "\n", "tau", ":", "float", "=", "0.005", ",", "\n", "target_update_period", ":", "int", "=", "1", ",", "\n", "target_entropy", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "init_temperature", ":", "float", "=", "0.1", ")", ":", "\n", "\n", "        ", "action_dim", "=", "actions", ".", "shape", "[", "-", "1", "]", "\n", "\n", "if", "target_entropy", "is", "None", ":", "\n", "            ", "self", ".", "target_entropy", "=", "-", "action_dim", "\n", "", "else", ":", "\n", "            ", "self", ".", "target_entropy", "=", "target_entropy", "\n", "\n", "", "self", ".", "tau", "=", "tau", "\n", "self", ".", "target_update_period", "=", "target_update_period", "\n", "self", ".", "discount", "=", "discount", "\n", "\n", "rng", "=", "jax", ".", "random", ".", "PRNGKey", "(", "seed", ")", "\n", "rng", ",", "actor_key", ",", "critic_key", ",", "temp_key", "=", "jax", ".", "random", ".", "split", "(", "rng", ",", "4", ")", "\n", "\n", "actor_def", "=", "DrQPolicy", "(", "hidden_dims", ",", "action_dim", ",", "cnn_features", ",", "\n", "cnn_strides", ",", "cnn_padding", ",", "latent_dim", ")", "\n", "actor", "=", "Model", ".", "create", "(", "actor_def", ",", "\n", "inputs", "=", "[", "actor_key", ",", "observations", "]", ",", "\n", "tx", "=", "optax", ".", "adam", "(", "learning_rate", "=", "actor_lr", ")", ")", "\n", "\n", "critic_def", "=", "DrQDoubleCritic", "(", "hidden_dims", ",", "cnn_features", ",", "cnn_strides", ",", "\n", "cnn_padding", ",", "latent_dim", ")", "\n", "\n", "critic", "=", "ModelDecoupleOpt", ".", "create", "(", "critic_def", ",", "\n", "inputs", "=", "[", "critic_key", ",", "observations", ",", "actions", "]", ",", "\n", "tx", "=", "optax", ".", "adam", "(", "learning_rate", "=", "critic_lr", ")", ",", "\n", "tx_enc", "=", "optax", ".", "adam", "(", "learning_rate", "=", "critic_lr", ")", ")", "\n", "\n", "target_critic", "=", "Model", ".", "create", "(", "\n", "critic_def", ",", "inputs", "=", "[", "critic_key", ",", "observations", ",", "actions", "]", ")", "\n", "\n", "temp", "=", "Model", ".", "create", "(", "temperature", ".", "Temperature", "(", "init_temperature", ")", ",", "\n", "inputs", "=", "[", "temp_key", "]", ",", "\n", "tx", "=", "optax", ".", "adam", "(", "learning_rate", "=", "temp_lr", ")", ")", "\n", "\n", "self", ".", "actor", "=", "actor", "\n", "self", ".", "critic", "=", "critic", "\n", "self", ".", "target_critic", "=", "target_critic", "\n", "self", ".", "temp", "=", "temp", "\n", "self", ".", "rng", "=", "rng", "\n", "self", ".", "step", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.drq.drq_learner.DrQLearner.sample_actions": [[130, 141], ["continuous_control.networks.policies.sample_actions", "numpy.asarray", "numpy.clip"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.policies.sample_actions"], ["", "def", "sample_actions", "(", "self", ",", "\n", "observations", ":", "np", ".", "ndarray", ",", "\n", "temperature", ":", "float", "=", "1.0", ")", "->", "jnp", ".", "ndarray", ":", "\n", "        ", "rng", ",", "actions", "=", "policies", ".", "sample_actions", "(", "self", ".", "rng", ",", "self", ".", "actor", ".", "apply_fn", ",", "\n", "self", ".", "actor", ".", "params", ",", "observations", ",", "\n", "temperature", ")", "\n", "\n", "self", ".", "rng", "=", "rng", "\n", "\n", "actions", "=", "np", ".", "asarray", "(", "actions", ")", "\n", "return", "np", ".", "clip", "(", "actions", ",", "-", "1", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.drq.drq_learner.DrQLearner.update": [[142, 156], ["drq_learner._update_jit"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.sac_learner._update_jit"], ["", "def", "update", "(", "self", ",", "batch", ":", "Batch", ")", "->", "InfoDict", ":", "\n", "        ", "self", ".", "step", "+=", "1", "\n", "new_rng", ",", "new_actor", ",", "new_critic", ",", "new_target_critic", ",", "new_temp", ",", "info", "=", "_update_jit", "(", "\n", "self", ".", "rng", ",", "self", ".", "actor", ",", "self", ".", "critic", ",", "self", ".", "target_critic", ",", "self", ".", "temp", ",", "\n", "batch", ",", "self", ".", "discount", ",", "self", ".", "tau", ",", "self", ".", "target_entropy", ",", "\n", "self", ".", "step", "%", "self", ".", "target_update_period", "==", "0", ")", "\n", "\n", "self", ".", "rng", "=", "new_rng", "\n", "self", ".", "actor", "=", "new_actor", "\n", "self", ".", "critic", "=", "new_critic", "\n", "self", ".", "target_critic", "=", "new_target_critic", "\n", "self", ".", "temp", "=", "new_temp", "\n", "\n", "return", "info", "\n", "", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.drq.drq_learner._update_jit": [[23, 66], ["functools.partial", "jax.random.split", "jax.random.split", "continuous_control.agents.drq.augmentations.batched_random_crop", "jax.random.split", "jax.random.split", "continuous_control.agents.drq.augmentations.batched_random_crop", "batch._replace._replace", "jax.random.split", "jax.random.split", "continuous_control.agents.sac.critic.update", "actor.replace.params.copy", "actor.replace.replace", "jax.random.split", "jax.random.split", "continuous_control.agents.sac.actor.update", "continuous_control.agents.sac.temperature.update", "continuous_control.agents.sac.critic.target_update"], "function", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.drq.augmentations.batched_random_crop", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.drq.augmentations.batched_random_crop", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.temperature.update", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.temperature.update", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.temperature.update", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.critic.target_update"], ["@", "functools", ".", "partial", "(", "jax", ".", "jit", ",", "static_argnames", "=", "(", "'update_target'", ")", ")", "\n", "def", "_update_jit", "(", "\n", "rng", ":", "PRNGKey", ",", "actor", ":", "Model", ",", "critic", ":", "Model", ",", "target_critic", ":", "Model", ",", "\n", "temp", ":", "Model", ",", "batch", ":", "Batch", ",", "discount", ":", "float", ",", "tau", ":", "float", ",", "\n", "target_entropy", ":", "float", ",", "update_target", ":", "bool", "\n", ")", "->", "Tuple", "[", "PRNGKey", ",", "Model", ",", "Model", ",", "Model", ",", "Model", ",", "InfoDict", "]", ":", "\n", "\n", "    ", "rng", ",", "key", "=", "jax", ".", "random", ".", "split", "(", "rng", ")", "\n", "observations", "=", "batched_random_crop", "(", "key", ",", "batch", ".", "observations", ")", "\n", "rng", ",", "key", "=", "jax", ".", "random", ".", "split", "(", "rng", ")", "\n", "next_observations", "=", "batched_random_crop", "(", "key", ",", "batch", ".", "next_observations", ")", "\n", "\n", "batch", "=", "batch", ".", "_replace", "(", "observations", "=", "observations", ",", "\n", "next_observations", "=", "next_observations", ")", "\n", "\n", "rng", ",", "key", "=", "jax", ".", "random", ".", "split", "(", "rng", ")", "\n", "new_critic", ",", "critic_info", "=", "update_critic", "(", "key", ",", "\n", "actor", ",", "\n", "critic", ",", "\n", "target_critic", ",", "\n", "temp", ",", "\n", "batch", ",", "\n", "discount", ",", "\n", "soft_critic", "=", "True", ")", "\n", "if", "update_target", ":", "\n", "        ", "new_target_critic", "=", "target_update", "(", "new_critic", ",", "target_critic", ",", "tau", ")", "\n", "", "else", ":", "\n", "        ", "new_target_critic", "=", "target_critic", "\n", "\n", "# Use critic conv layers in actor:", "\n", "", "new_actor_params", "=", "actor", ".", "params", ".", "copy", "(", "\n", "add_or_replace", "=", "{", "'SharedEncoder'", ":", "new_critic", ".", "params", "[", "'SharedEncoder'", "]", "}", ")", "\n", "actor", "=", "actor", ".", "replace", "(", "params", "=", "new_actor_params", ")", "\n", "\n", "rng", ",", "key", "=", "jax", ".", "random", ".", "split", "(", "rng", ")", "\n", "new_actor", ",", "actor_info", "=", "update_actor", "(", "key", ",", "actor", ",", "new_critic", ",", "temp", ",", "batch", ")", "\n", "new_temp", ",", "alpha_info", "=", "temperature", ".", "update", "(", "temp", ",", "actor_info", "[", "'entropy'", "]", ",", "\n", "target_entropy", ")", "\n", "\n", "return", "rng", ",", "new_actor", ",", "new_critic", ",", "new_target_critic", ",", "new_temp", ",", "{", "\n", "**", "critic_info", ",", "\n", "**", "actor_info", ",", "\n", "**", "alpha_info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.drq.augmentations.random_crop": [[5, 11], ["jax.random.randint", "jax.random.randint", "jax.concatenate", "jax.pad", "jax.lax.dynamic_slice", "jax.lax.dynamic_slice", "jax.zeros"], "function", ["None"], ["def", "random_crop", "(", "key", ",", "img", ",", "padding", ")", ":", "\n", "    ", "crop_from", "=", "jax", ".", "random", ".", "randint", "(", "key", ",", "(", "2", ",", ")", ",", "0", ",", "2", "*", "padding", "+", "1", ")", "\n", "crop_from", "=", "jnp", ".", "concatenate", "(", "[", "crop_from", ",", "jnp", ".", "zeros", "(", "(", "1", ",", ")", ",", "dtype", "=", "jnp", ".", "int32", ")", "]", ")", "\n", "padded_img", "=", "jnp", ".", "pad", "(", "img", ",", "(", "(", "padding", ",", "padding", ")", ",", "(", "padding", ",", "padding", ")", ",", "(", "0", ",", "0", ")", ")", ",", "\n", "mode", "=", "'edge'", ")", "\n", "return", "jax", ".", "lax", ".", "dynamic_slice", "(", "padded_img", ",", "crop_from", ",", "img", ".", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.drq.augmentations.batched_random_crop": [[13, 16], ["jax.random.split", "jax.random.split", "jax.vmap", "jax.vmap"], "function", ["None"], ["", "def", "batched_random_crop", "(", "key", ",", "imgs", ",", "padding", "=", "4", ")", ":", "\n", "    ", "keys", "=", "jax", ".", "random", ".", "split", "(", "key", ",", "imgs", ".", "shape", "[", "0", "]", ")", "\n", "return", "jax", ".", "vmap", "(", "random_crop", ",", "(", "0", ",", "0", ",", "None", ")", ")", "(", "keys", ",", "imgs", ",", "padding", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.sac_learner.SACLearner.__init__": [[54, 107], ["jax.random.PRNGKey", "jax.random.PRNGKey", "jax.random.PRNGKey", "jax.random.PRNGKey", "jax.random.split", "jax.random.split", "jax.random.split", "jax.random.split", "continuous_control.networks.policies.NormalTanhPolicy", "continuous_control.networks.common.Model.create", "continuous_control.networks.critic_net.DoubleCritic", "continuous_control.networks.common.Model.create", "continuous_control.networks.common.Model.create", "continuous_control.networks.common.Model.create", "continuous_control.agents.sac.temperature.Temperature", "optax.adam", "optax.adam", "optax.adam"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.create", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.create", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.create", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.create"], ["    ", "def", "__init__", "(", "self", ",", "\n", "seed", ":", "int", ",", "\n", "observations", ":", "jnp", ".", "ndarray", ",", "\n", "actions", ":", "jnp", ".", "ndarray", ",", "\n", "actor_lr", ":", "float", "=", "3e-4", ",", "\n", "critic_lr", ":", "float", "=", "3e-4", ",", "\n", "temp_lr", ":", "float", "=", "3e-4", ",", "\n", "hidden_dims", ":", "Sequence", "[", "int", "]", "=", "(", "256", ",", "256", ")", ",", "\n", "discount", ":", "float", "=", "0.99", ",", "\n", "tau", ":", "float", "=", "0.005", ",", "\n", "target_update_period", ":", "int", "=", "1", ",", "\n", "target_entropy", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "init_temperature", ":", "float", "=", "1.0", ")", ":", "\n", "        ", "\"\"\"\n        An implementation of the version of Soft-Actor-Critic described in https://arxiv.org/abs/1812.05905\n        \"\"\"", "\n", "\n", "action_dim", "=", "actions", ".", "shape", "[", "-", "1", "]", "\n", "\n", "if", "target_entropy", "is", "None", ":", "\n", "            ", "self", ".", "target_entropy", "=", "-", "action_dim", "/", "2", "\n", "", "else", ":", "\n", "            ", "self", ".", "target_entropy", "=", "target_entropy", "\n", "\n", "", "self", ".", "tau", "=", "tau", "\n", "self", ".", "target_update_period", "=", "target_update_period", "\n", "self", ".", "discount", "=", "discount", "\n", "\n", "rng", "=", "jax", ".", "random", ".", "PRNGKey", "(", "seed", ")", "\n", "rng", ",", "actor_key", ",", "critic_key", ",", "temp_key", "=", "jax", ".", "random", ".", "split", "(", "rng", ",", "4", ")", "\n", "actor_def", "=", "policies", ".", "NormalTanhPolicy", "(", "hidden_dims", ",", "action_dim", ")", "\n", "actor", "=", "Model", ".", "create", "(", "actor_def", ",", "\n", "inputs", "=", "[", "actor_key", ",", "observations", "]", ",", "\n", "tx", "=", "optax", ".", "adam", "(", "learning_rate", "=", "actor_lr", ")", ")", "\n", "\n", "critic_def", "=", "critic_net", ".", "DoubleCritic", "(", "hidden_dims", ")", "\n", "critic", "=", "Model", ".", "create", "(", "critic_def", ",", "\n", "inputs", "=", "[", "critic_key", ",", "observations", ",", "actions", "]", ",", "\n", "tx", "=", "optax", ".", "adam", "(", "learning_rate", "=", "critic_lr", ")", ")", "\n", "target_critic", "=", "Model", ".", "create", "(", "\n", "critic_def", ",", "inputs", "=", "[", "critic_key", ",", "observations", ",", "actions", "]", ")", "\n", "\n", "temp", "=", "Model", ".", "create", "(", "temperature", ".", "Temperature", "(", "init_temperature", ")", ",", "\n", "inputs", "=", "[", "temp_key", "]", ",", "\n", "tx", "=", "optax", ".", "adam", "(", "learning_rate", "=", "temp_lr", ")", ")", "\n", "\n", "self", ".", "actor", "=", "actor", "\n", "self", ".", "critic", "=", "critic", "\n", "self", ".", "target_critic", "=", "target_critic", "\n", "self", ".", "temp", "=", "temp", "\n", "self", ".", "rng", "=", "rng", "\n", "\n", "self", ".", "step", "=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.sac_learner.SACLearner.sample_actions": [[108, 118], ["continuous_control.networks.policies.sample_actions", "numpy.asarray", "numpy.clip"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.policies.sample_actions"], ["", "def", "sample_actions", "(", "self", ",", "\n", "observations", ":", "np", ".", "ndarray", ",", "\n", "temperature", ":", "float", "=", "1.0", ")", "->", "jnp", ".", "ndarray", ":", "\n", "        ", "rng", ",", "actions", "=", "policies", ".", "sample_actions", "(", "self", ".", "rng", ",", "self", ".", "actor", ".", "apply_fn", ",", "\n", "self", ".", "actor", ".", "params", ",", "observations", ",", "\n", "temperature", ")", "\n", "self", ".", "rng", "=", "rng", "\n", "\n", "actions", "=", "np", ".", "asarray", "(", "actions", ")", "\n", "return", "np", ".", "clip", "(", "actions", ",", "-", "1", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.sac_learner.SACLearner.update": [[119, 134], ["sac_learner._update_jit"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.sac_learner._update_jit"], ["", "def", "update", "(", "self", ",", "batch", ":", "Batch", ")", "->", "InfoDict", ":", "\n", "        ", "self", ".", "step", "+=", "1", "\n", "\n", "new_rng", ",", "new_actor", ",", "new_critic", ",", "new_target_critic", ",", "new_temp", ",", "info", "=", "_update_jit", "(", "\n", "self", ".", "rng", ",", "self", ".", "actor", ",", "self", ".", "critic", ",", "self", ".", "target_critic", ",", "self", ".", "temp", ",", "\n", "batch", ",", "self", ".", "discount", ",", "self", ".", "tau", ",", "self", ".", "target_entropy", ",", "\n", "self", ".", "step", "%", "self", ".", "target_update_period", "==", "0", ")", "\n", "\n", "self", ".", "rng", "=", "new_rng", "\n", "self", ".", "actor", "=", "new_actor", "\n", "self", ".", "critic", "=", "new_critic", "\n", "self", ".", "target_critic", "=", "new_target_critic", "\n", "self", ".", "temp", "=", "new_temp", "\n", "\n", "return", "info", "\n", "", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.sac_learner._update_jit": [[20, 50], ["functools.partial", "jax.random.split", "jax.random.split", "continuous_control.agents.sac.critic.update", "jax.random.split", "jax.random.split", "continuous_control.agents.sac.actor.update", "continuous_control.agents.sac.temperature.update", "continuous_control.agents.sac.critic.target_update"], "function", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.temperature.update", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.temperature.update", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.temperature.update", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.critic.target_update"], ["@", "functools", ".", "partial", "(", "jax", ".", "jit", ",", "static_argnames", "=", "(", "'update_target'", ")", ")", "\n", "def", "_update_jit", "(", "\n", "rng", ":", "PRNGKey", ",", "actor", ":", "Model", ",", "critic", ":", "Model", ",", "target_critic", ":", "Model", ",", "\n", "temp", ":", "Model", ",", "batch", ":", "Batch", ",", "discount", ":", "float", ",", "tau", ":", "float", ",", "\n", "target_entropy", ":", "float", ",", "update_target", ":", "bool", "\n", ")", "->", "Tuple", "[", "PRNGKey", ",", "Model", ",", "Model", ",", "Model", ",", "Model", ",", "InfoDict", "]", ":", "\n", "\n", "    ", "rng", ",", "key", "=", "jax", ".", "random", ".", "split", "(", "rng", ")", "\n", "new_critic", ",", "critic_info", "=", "update_critic", "(", "key", ",", "\n", "actor", ",", "\n", "critic", ",", "\n", "target_critic", ",", "\n", "temp", ",", "\n", "batch", ",", "\n", "discount", ",", "\n", "soft_critic", "=", "True", ")", "\n", "if", "update_target", ":", "\n", "        ", "new_target_critic", "=", "target_update", "(", "new_critic", ",", "target_critic", ",", "tau", ")", "\n", "", "else", ":", "\n", "        ", "new_target_critic", "=", "target_critic", "\n", "\n", "", "rng", ",", "key", "=", "jax", ".", "random", ".", "split", "(", "rng", ")", "\n", "new_actor", ",", "actor_info", "=", "update_actor", "(", "key", ",", "actor", ",", "new_critic", ",", "temp", ",", "batch", ")", "\n", "new_temp", ",", "alpha_info", "=", "temperature", ".", "update", "(", "temp", ",", "actor_info", "[", "'entropy'", "]", ",", "\n", "target_entropy", ")", "\n", "\n", "return", "rng", ",", "new_actor", ",", "new_critic", ",", "new_target_critic", ",", "new_temp", ",", "{", "\n", "**", "critic_info", ",", "\n", "**", "actor_info", ",", "\n", "**", "alpha_info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.critic.target_update": [[10, 16], ["jax.tree_multimap", "jax.tree_multimap", "target_critic.replace"], "function", ["None"], ["def", "target_update", "(", "critic", ":", "Model", ",", "target_critic", ":", "Model", ",", "tau", ":", "float", ")", "->", "Model", ":", "\n", "    ", "new_target_params", "=", "jax", ".", "tree_multimap", "(", "\n", "lambda", "p", ",", "tp", ":", "p", "*", "tau", "+", "tp", "*", "(", "1", "-", "tau", ")", ",", "critic", ".", "params", ",", "\n", "target_critic", ".", "params", ")", "\n", "\n", "return", "target_critic", ".", "replace", "(", "params", "=", "new_target_params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.critic.update": [[18, 58], ["actor", "actor.sample", "actor.log_prob", "target_critic", "jax.minimum", "critic.apply_gradient", "info.pop", "critic.apply", "critic_fn", "jax.value_and_grad", "jax.value_and_grad", "temp", "q1.mean", "q2.mean", "batch.rewards.mean", "continuous_control.networks.common.tree_norm", "jax.sqrt().mean", "jax.sqrt"], "function", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.sample", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.apply_gradient", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.apply", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.tree_norm"], ["", "def", "update", "(", "key", ":", "PRNGKey", ",", "actor", ":", "Model", ",", "critic", ":", "Model", ",", "target_critic", ":", "Model", ",", "\n", "temp", ":", "Model", ",", "batch", ":", "Batch", ",", "discount", ":", "float", ",", "\n", "soft_critic", ":", "bool", ")", "->", "Tuple", "[", "Model", ",", "InfoDict", "]", ":", "\n", "    ", "dist", "=", "actor", "(", "batch", ".", "next_observations", ")", "\n", "next_actions", "=", "dist", ".", "sample", "(", "seed", "=", "key", ")", "\n", "next_log_probs", "=", "dist", ".", "log_prob", "(", "next_actions", ")", "\n", "next_q1", ",", "next_q2", "=", "target_critic", "(", "batch", ".", "next_observations", ",", "next_actions", ")", "\n", "next_q", "=", "jnp", ".", "minimum", "(", "next_q1", ",", "next_q2", ")", "\n", "\n", "target_q", "=", "batch", ".", "rewards", "+", "discount", "*", "batch", ".", "masks", "*", "next_q", "\n", "\n", "if", "soft_critic", ":", "\n", "        ", "target_q", "-=", "discount", "*", "batch", ".", "masks", "*", "temp", "(", ")", "*", "next_log_probs", "\n", "\n", "", "def", "critic_loss_fn", "(", "critic_params", ":", "Params", ")", "->", "Tuple", "[", "jnp", ".", "ndarray", ",", "InfoDict", "]", ":", "\n", "# q1, q2 = critic.apply({'params': critic_params}, batch.observations,", "\n", "#                       batch.actions)", "\n", "        ", "critic_fn", "=", "lambda", "actions", ":", "critic", ".", "apply", "(", "{", "'params'", ":", "critic_params", "}", ",", "\n", "batch", ".", "observations", ",", "actions", ")", "\n", "def", "_critic_fn", "(", "actions", ")", ":", "\n", "            ", "q1", ",", "q2", "=", "critic_fn", "(", "actions", ")", "\n", "return", "0.5", "*", "(", "q1", "+", "q2", ")", ".", "mean", "(", ")", ",", "(", "q1", ",", "q2", ")", "\n", "\n", "", "(", "_", ",", "(", "q1", ",", "q2", ")", ")", ",", "action_grad", "=", "jax", ".", "value_and_grad", "(", "_critic_fn", ",", "\n", "has_aux", "=", "True", ")", "(", "\n", "batch", ".", "actions", ")", "\n", "critic_loss", "=", "(", "(", "q1", "-", "target_q", ")", "**", "2", "+", "(", "q2", "-", "target_q", ")", "**", "2", ")", ".", "mean", "(", ")", "\n", "return", "critic_loss", ",", "{", "\n", "'critic_loss'", ":", "critic_loss", ",", "\n", "'q1'", ":", "q1", ".", "mean", "(", ")", ",", "\n", "'q2'", ":", "q2", ".", "mean", "(", ")", ",", "\n", "'r'", ":", "batch", ".", "rewards", ".", "mean", "(", ")", ",", "\n", "'critic_pnorm'", ":", "tree_norm", "(", "critic_params", ")", ",", "\n", "'critic_agnorm'", ":", "jnp", ".", "sqrt", "(", "(", "action_grad", "**", "2", ")", ".", "sum", "(", "-", "1", ")", ")", ".", "mean", "(", "0", ")", "\n", "}", "\n", "\n", "", "new_critic", ",", "info", "=", "critic", ".", "apply_gradient", "(", "critic_loss_fn", ")", "\n", "info", "[", "'critic_gnorm'", "]", "=", "info", ".", "pop", "(", "'grad_norm'", ")", "\n", "\n", "return", "new_critic", ",", "info", "\n", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.actor.update": [[10, 30], ["actor.apply_gradient", "info.pop", "actor.apply", "actor.apply.sample", "actor.apply.log_prob", "critic", "jax.minimum", "continuous_control.networks.common.tree_norm", "jax.mean", "dist.log_prob.mean", "jax.abs", "temp"], "function", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.apply_gradient", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.apply", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.sample", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.tree_norm"], ["def", "update", "(", "key", ":", "PRNGKey", ",", "actor", ":", "Model", ",", "critic", ":", "Model", ",", "temp", ":", "Model", ",", "\n", "batch", ":", "Batch", ")", "->", "Tuple", "[", "Model", ",", "InfoDict", "]", ":", "\n", "    ", "def", "actor_loss_fn", "(", "actor_params", ":", "Params", ")", "->", "Tuple", "[", "jnp", ".", "ndarray", ",", "InfoDict", "]", ":", "\n", "        ", "dist", "=", "actor", ".", "apply", "(", "{", "'params'", ":", "actor_params", "}", ",", "batch", ".", "observations", ")", "\n", "actions", "=", "dist", ".", "sample", "(", "seed", "=", "key", ")", "\n", "log_probs", "=", "dist", ".", "log_prob", "(", "actions", ")", "\n", "q1", ",", "q2", "=", "critic", "(", "batch", ".", "observations", ",", "actions", ")", "\n", "q", "=", "jnp", ".", "minimum", "(", "q1", ",", "q2", ")", "\n", "actor_loss", "=", "(", "log_probs", "*", "temp", "(", ")", "-", "q", ")", ".", "mean", "(", ")", "\n", "return", "actor_loss", ",", "{", "\n", "'actor_loss'", ":", "actor_loss", ",", "\n", "'entropy'", ":", "-", "log_probs", ".", "mean", "(", ")", ",", "\n", "'actor_pnorm'", ":", "tree_norm", "(", "actor_params", ")", ",", "\n", "'actor_action'", ":", "jnp", ".", "mean", "(", "jnp", ".", "abs", "(", "actions", ")", ")", "\n", "}", "\n", "\n", "", "new_actor", ",", "info", "=", "actor", ".", "apply_gradient", "(", "actor_loss_fn", ")", "\n", "info", "[", "'actor_gnorm'", "]", "=", "info", ".", "pop", "(", "'grad_norm'", ")", "\n", "\n", "return", "new_actor", ",", "info", "\n", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.temperature.Temperature.__call__": [[12, 18], ["temperature.Temperature.param", "jax.exp", "jax.full", "jax.log"], "methods", ["None"], ["@", "nn", ".", "compact", "\n", "def", "__call__", "(", "self", ")", "->", "jnp", ".", "ndarray", ":", "\n", "        ", "log_temp", "=", "self", ".", "param", "(", "'log_temp'", ",", "\n", "init_fn", "=", "lambda", "key", ":", "jnp", ".", "full", "(", "\n", "(", ")", ",", "jnp", ".", "log", "(", "self", ".", "initial_temperature", ")", ")", ")", "\n", "return", "jnp", ".", "exp", "(", "log_temp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.temperature.update": [[20, 31], ["temp.apply_gradient", "info.pop", "temp.apply"], "function", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.apply_gradient", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.apply"], ["", "", "def", "update", "(", "temp", ":", "Model", ",", "entropy", ":", "float", ",", "\n", "target_entropy", ":", "float", ")", "->", "Tuple", "[", "Model", ",", "InfoDict", "]", ":", "\n", "    ", "def", "temperature_loss_fn", "(", "temp_params", ")", ":", "\n", "        ", "temperature", "=", "temp", ".", "apply", "(", "{", "'params'", ":", "temp_params", "}", ")", "\n", "temp_loss", "=", "temperature", "*", "(", "entropy", "-", "target_entropy", ")", ".", "mean", "(", ")", "\n", "return", "temp_loss", ",", "{", "'temperature'", ":", "temperature", ",", "'temp_loss'", ":", "temp_loss", "}", "\n", "\n", "", "new_temp", ",", "info", "=", "temp", ".", "apply_gradient", "(", "temperature_loss_fn", ")", "\n", "info", ".", "pop", "(", "'grad_norm'", ")", "\n", "\n", "return", "new_temp", ",", "info", "\n", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.policies.MSEPolicy.__call__": [[24, 37], ["flax.tanh", "continuous_control.networks.common.MLP", "flax.Dense", "continuous_control.networks.common.default_init"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.default_init"], ["@", "nn", ".", "compact", "\n", "def", "__call__", "(", "self", ",", "\n", "observations", ":", "jnp", ".", "ndarray", ",", "\n", "temperature", ":", "float", "=", "1.0", ",", "\n", "training", ":", "bool", "=", "False", ")", "->", "jnp", ".", "ndarray", ":", "\n", "        ", "outputs", "=", "MLP", "(", "self", ".", "hidden_dims", ",", "\n", "activate_final", "=", "True", ",", "\n", "dropout_rate", "=", "self", ".", "dropout_rate", ")", "(", "observations", ",", "\n", "training", "=", "training", ")", "\n", "\n", "actions", "=", "nn", ".", "Dense", "(", "self", ".", "action_dim", ",", "\n", "kernel_init", "=", "default_init", "(", ")", ")", "(", "outputs", ")", "\n", "return", "nn", ".", "tanh", "(", "actions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.policies.NormalTanhPolicy.__call__": [[49, 86], ["tfd.MultivariateNormalDiag", "continuous_control.networks.common.MLP", "flax.Dense", "policies.NormalTanhPolicy.param", "flax.tanh", "tfd.TransformedDistribution", "flax.Dense", "continuous_control.networks.common.default_init", "flax.tanh", "jax.exp", "jax.exp", "tensorflow_probability.substrates.jax.exp", "tfb.Tanh", "continuous_control.networks.common.default_init"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.default_init", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.default_init"], ["@", "nn", ".", "compact", "\n", "def", "__call__", "(", "self", ",", "\n", "observations", ":", "jnp", ".", "ndarray", ",", "\n", "temperature", ":", "float", "=", "1.0", ",", "\n", "training", ":", "bool", "=", "False", ")", "->", "tfd", ".", "Distribution", ":", "\n", "        ", "outputs", "=", "MLP", "(", "self", ".", "hidden_dims", ",", "\n", "activate_final", "=", "True", ",", "\n", "dropout_rate", "=", "self", ".", "dropout_rate", ")", "(", "observations", ",", "\n", "training", "=", "training", ")", "\n", "\n", "means", "=", "nn", ".", "Dense", "(", "self", ".", "action_dim", ",", "kernel_init", "=", "default_init", "(", ")", ")", "(", "outputs", ")", "\n", "\n", "if", "self", ".", "state_dependent_std", ":", "\n", "            ", "log_stds", "=", "nn", ".", "Dense", "(", "self", ".", "action_dim", ",", "\n", "kernel_init", "=", "default_init", "(", "\n", "self", ".", "log_std_scale", ")", ")", "(", "outputs", ")", "\n", "", "else", ":", "\n", "            ", "log_stds", "=", "self", ".", "param", "(", "'log_stds'", ",", "nn", ".", "initializers", ".", "zeros", ",", "\n", "(", "self", ".", "action_dim", ",", ")", ")", "\n", "\n", "", "log_std_min", "=", "self", ".", "log_std_min", "or", "LOG_STD_MIN", "\n", "log_std_max", "=", "self", ".", "log_std_max", "or", "LOG_STD_MAX", "\n", "#log_stds = jnp.clip(log_stds, log_std_min, log_std_max)", "\n", "# suggested by Ilya for stability", "\n", "log_stds", "=", "log_std_min", "+", "(", "log_std_max", "-", "log_std_min", ")", "*", "0.5", "*", "(", "1", "+", "nn", ".", "tanh", "(", "log_stds", ")", ")", "\n", "\n", "if", "not", "self", ".", "tanh_squash_distribution", ":", "\n", "            ", "means", "=", "nn", ".", "tanh", "(", "means", ")", "\n", "\n", "", "base_dist", "=", "tfd", ".", "MultivariateNormalDiag", "(", "loc", "=", "means", ",", "\n", "scale_diag", "=", "jnp", ".", "exp", "(", "log_stds", ")", "*", "\n", "temperature", ")", "\n", "if", "self", ".", "tanh_squash_distribution", ":", "\n", "            ", "return", "tfd", ".", "TransformedDistribution", "(", "distribution", "=", "base_dist", ",", "\n", "bijector", "=", "tfb", ".", "Tanh", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "return", "base_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.policies.NormalTanhMixturePolicy.__call__": [[94, 131], ["jax.reshape", "jax.reshape", "tensorflow_probability.substrates.jax.reshape", "jax.reshape", "jax.reshape", "tensorflow_probability.substrates.jax.reshape", "jax.reshape", "jax.reshape", "tensorflow_probability.substrates.jax.reshape", "jax.clip", "jax.clip", "tensorflow_probability.substrates.jax.clip", "tfd.Normal", "tfd.MixtureSameFamily", "tfd.TransformedDistribution", "tfd.Independent", "continuous_control.networks.common.MLP", "flax.Dense", "flax.Dense", "flax.Dense", "list", "tfd.Categorical", "tfb.Tanh", "continuous_control.networks.common.default_init", "continuous_control.networks.common.default_init", "flax.initializers.normal", "continuous_control.networks.common.default_init", "jax.exp", "jax.exp", "tensorflow_probability.substrates.jax.exp"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.default_init", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.default_init", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.default_init"], ["@", "nn", ".", "compact", "\n", "def", "__call__", "(", "self", ",", "\n", "observations", ":", "jnp", ".", "ndarray", ",", "\n", "temperature", ":", "float", "=", "1.0", ",", "\n", "training", ":", "bool", "=", "False", ")", "->", "tfd", ".", "Distribution", ":", "\n", "        ", "outputs", "=", "MLP", "(", "self", ".", "hidden_dims", ",", "\n", "activate_final", "=", "True", ",", "\n", "dropout_rate", "=", "self", ".", "dropout_rate", ")", "(", "observations", ",", "\n", "training", "=", "training", ")", "\n", "\n", "logits", "=", "nn", ".", "Dense", "(", "self", ".", "action_dim", "*", "self", ".", "num_components", ",", "\n", "kernel_init", "=", "default_init", "(", ")", ")", "(", "outputs", ")", "\n", "means", "=", "nn", ".", "Dense", "(", "self", ".", "action_dim", "*", "self", ".", "num_components", ",", "\n", "kernel_init", "=", "default_init", "(", ")", ",", "\n", "bias_init", "=", "nn", ".", "initializers", ".", "normal", "(", "stddev", "=", "1.0", ")", ")", "(", "outputs", ")", "\n", "log_stds", "=", "nn", ".", "Dense", "(", "self", ".", "action_dim", "*", "self", ".", "num_components", ",", "\n", "kernel_init", "=", "default_init", "(", ")", ")", "(", "outputs", ")", "\n", "\n", "shape", "=", "list", "(", "observations", ".", "shape", "[", ":", "-", "1", "]", ")", "+", "[", "-", "1", ",", "self", ".", "num_components", "]", "\n", "logits", "=", "jnp", ".", "reshape", "(", "logits", ",", "shape", ")", "\n", "mu", "=", "jnp", ".", "reshape", "(", "means", ",", "shape", ")", "\n", "log_stds", "=", "jnp", ".", "reshape", "(", "log_stds", ",", "shape", ")", "\n", "\n", "log_stds", "=", "jnp", ".", "clip", "(", "log_stds", ",", "LOG_STD_MIN", ",", "LOG_STD_MAX", ")", "\n", "\n", "components_distribution", "=", "tfd", ".", "Normal", "(", "loc", "=", "mu", ",", "\n", "scale", "=", "jnp", ".", "exp", "(", "log_stds", ")", "*", "\n", "temperature", ")", "\n", "\n", "base_dist", "=", "tfd", ".", "MixtureSameFamily", "(", "\n", "mixture_distribution", "=", "tfd", ".", "Categorical", "(", "logits", "=", "logits", ")", ",", "\n", "components_distribution", "=", "components_distribution", ")", "\n", "\n", "dist", "=", "tfd", ".", "TransformedDistribution", "(", "distribution", "=", "base_dist", ",", "\n", "bijector", "=", "tfb", ".", "Tanh", "(", ")", ")", "\n", "\n", "return", "tfd", ".", "Independent", "(", "dist", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.policies._sample_actions": [[133, 149], ["functools.partial", "actor_def.apply", "jax.random.split", "jax.random.split", "tensorflow_probability.substrates.jax.random.split", "actor_def.apply", "actor_def.apply.sample"], "function", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.apply", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.apply", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.sample"], ["", "", "@", "functools", ".", "partial", "(", "jax", ".", "jit", ",", "static_argnames", "=", "(", "'actor_def'", ",", "'distribution'", ")", ")", "\n", "def", "_sample_actions", "(", "\n", "rng", ":", "PRNGKey", ",", "\n", "actor_def", ":", "nn", ".", "Module", ",", "\n", "actor_params", ":", "Params", ",", "\n", "observations", ":", "np", ".", "ndarray", ",", "\n", "temperature", ":", "float", "=", "1.0", ",", "\n", "distribution", ":", "str", "=", "'log_prob'", ")", "->", "Tuple", "[", "PRNGKey", ",", "jnp", ".", "ndarray", "]", ":", "\n", "    ", "if", "distribution", "==", "'det'", ":", "\n", "        ", "return", "rng", ",", "actor_def", ".", "apply", "(", "{", "'params'", ":", "actor_params", "}", ",", "observations", ",", "\n", "temperature", ")", "\n", "", "else", ":", "\n", "        ", "dist", "=", "actor_def", ".", "apply", "(", "{", "'params'", ":", "actor_params", "}", ",", "observations", ",", "\n", "temperature", ")", "\n", "rng", ",", "key", "=", "jax", ".", "random", ".", "split", "(", "rng", ")", "\n", "return", "rng", ",", "dist", ".", "sample", "(", "seed", "=", "key", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.policies.sample_actions": [[151, 160], ["policies._sample_actions"], "function", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.policies._sample_actions"], ["", "", "def", "sample_actions", "(", "\n", "rng", ":", "PRNGKey", ",", "\n", "actor_def", ":", "nn", ".", "Module", ",", "\n", "actor_params", ":", "Params", ",", "\n", "observations", ":", "np", ".", "ndarray", ",", "\n", "temperature", ":", "float", "=", "1.0", ",", "\n", "distribution", ":", "str", "=", "'log_prob'", ")", "->", "Tuple", "[", "PRNGKey", ",", "jnp", ".", "ndarray", "]", ":", "\n", "    ", "return", "_sample_actions", "(", "rng", ",", "actor_def", ",", "actor_params", ",", "observations", ",", "\n", "temperature", ",", "distribution", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.MLP.__call__": [[31, 41], ["enumerate", "flax.Dense", "flax.Dense", "common.MLP.activations", "len", "common.default_init", "flax.Dropout", "flax.Dropout"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.default_init"], ["@", "nn", ".", "compact", "\n", "def", "__call__", "(", "self", ",", "x", ":", "jnp", ".", "ndarray", ",", "training", ":", "bool", "=", "False", ")", "->", "jnp", ".", "ndarray", ":", "\n", "        ", "for", "i", ",", "size", "in", "enumerate", "(", "self", ".", "hidden_dims", ")", ":", "\n", "            ", "x", "=", "nn", ".", "Dense", "(", "size", ",", "kernel_init", "=", "default_init", "(", ")", ")", "(", "x", ")", "\n", "if", "i", "+", "1", "<", "len", "(", "self", ".", "hidden_dims", ")", "or", "self", ".", "activate_final", ":", "\n", "                ", "x", "=", "self", ".", "activations", "(", "x", ")", "\n", "if", "self", ".", "dropout_rate", "is", "not", "None", ":", "\n", "                    ", "x", "=", "nn", ".", "Dropout", "(", "rate", "=", "self", ".", "dropout_rate", ")", "(", "\n", "x", ",", "deterministic", "=", "not", "training", ")", "\n", "", "", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.Model.create": [[52, 71], ["model_def.init", "model_def.init.pop", "cls", "tx.init"], "methods", ["None"], ["@", "classmethod", "\n", "def", "create", "(", "cls", ",", "\n", "model_def", ":", "nn", ".", "Module", ",", "\n", "inputs", ":", "Sequence", "[", "jnp", ".", "ndarray", "]", ",", "\n", "tx", ":", "Optional", "[", "optax", ".", "GradientTransformation", "]", "=", "None", ")", "->", "'Model'", ":", "\n", "        ", "variables", "=", "model_def", ".", "init", "(", "*", "inputs", ")", "\n", "\n", "_", ",", "params", "=", "variables", ".", "pop", "(", "'params'", ")", "\n", "\n", "if", "tx", "is", "not", "None", ":", "\n", "            ", "opt_state", "=", "tx", ".", "init", "(", "params", ")", "\n", "", "else", ":", "\n", "            ", "opt_state", "=", "None", "\n", "\n", "", "return", "cls", "(", "step", "=", "1", ",", "\n", "apply_fn", "=", "model_def", ",", "\n", "params", "=", "params", ",", "\n", "tx", "=", "tx", ",", "\n", "opt_state", "=", "opt_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.Model.__call__": [[72, 74], ["common.Model.apply_fn.apply"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.apply"], ["", "def", "__call__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "apply_fn", ".", "apply", "(", "{", "'params'", ":", "self", ".", "params", "}", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.Model.apply": [[75, 77], ["common.Model.apply_fn.apply"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.apply"], ["", "def", "apply", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "apply_fn", ".", "apply", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.Model.apply_gradient": [[78, 91], ["jax.grad", "jax.grad", "jax.grad", "jax.grad", "jax.grad.", "jax.grad.", "common.tree_norm", "common.Model.tx.update", "optax.apply_updates", "common.Model.replace"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.tree_norm", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.temperature.update"], ["", "def", "apply_gradient", "(", "self", ",", "loss_fn", ")", "->", "Tuple", "[", "Any", ",", "'Model'", "]", ":", "\n", "        ", "grad_fn", "=", "jax", ".", "grad", "(", "loss_fn", ",", "has_aux", "=", "True", ")", "\n", "grads", ",", "info", "=", "grad_fn", "(", "self", ".", "params", ")", "\n", "grad_norm", "=", "tree_norm", "(", "grads", ")", "\n", "info", "[", "'grad_norm'", "]", "=", "grad_norm", "\n", "\n", "updates", ",", "new_opt_state", "=", "self", ".", "tx", ".", "update", "(", "grads", ",", "self", ".", "opt_state", ",", "\n", "self", ".", "params", ")", "\n", "new_params", "=", "optax", ".", "apply_updates", "(", "self", ".", "params", ",", "updates", ")", "\n", "\n", "return", "self", ".", "replace", "(", "step", "=", "self", ".", "step", "+", "1", ",", "\n", "params", "=", "new_params", ",", "\n", "opt_state", "=", "new_opt_state", ")", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.Model.save": [[92, 96], ["os.makedirs", "os.path.dirname", "open", "f.write", "flax.serialization.to_bytes", "flax.serialization.to_bytes", "flax.serialization.to_bytes", "flax.serialization.to_bytes"], "methods", ["None"], ["", "def", "save", "(", "self", ",", "save_path", ":", "str", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "save_path", ")", ",", "exist_ok", "=", "True", ")", "\n", "with", "open", "(", "save_path", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "flax", ".", "serialization", ".", "to_bytes", "(", "self", ".", "params", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.Model.load": [[97, 101], ["common.Model.replace", "open", "flax.serialization.from_bytes", "flax.serialization.from_bytes", "flax.serialization.from_bytes", "flax.serialization.from_bytes", "f.read"], "methods", ["None"], ["", "", "def", "load", "(", "self", ",", "load_path", ":", "str", ")", "->", "'Model'", ":", "\n", "        ", "with", "open", "(", "load_path", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "params", "=", "flax", ".", "serialization", ".", "from_bytes", "(", "self", ".", "params", ",", "f", ".", "read", "(", ")", ")", "\n", "", "return", "self", ".", "replace", "(", "params", "=", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.create": [[124, 150], ["model_def.init", "model_def.init.pop", "cls", "common.split_tree", "tx_enc.init", "tx.init"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.split_tree"], ["@", "classmethod", "\n", "def", "create", "(", "cls", ",", "\n", "model_def", ":", "nn", ".", "Module", ",", "\n", "inputs", ":", "Sequence", "[", "jnp", ".", "ndarray", "]", ",", "\n", "tx", ":", "Optional", "[", "optax", ".", "GradientTransformation", "]", "=", "None", ",", "\n", "tx_enc", ":", "Optional", "[", "optax", ".", "GradientTransformation", "]", "=", "None", ")", "->", "'ModelDecoupleOpt'", ":", "\n", "        ", "variables", "=", "model_def", ".", "init", "(", "*", "inputs", ")", "\n", "\n", "_", ",", "params", "=", "variables", ".", "pop", "(", "'params'", ")", "\n", "\n", "if", "tx", "is", "not", "None", ":", "\n", "            ", "if", "tx_enc", "is", "None", ":", "\n", "                ", "tx_enc", "=", "tx", "\n", "", "params_enc", ",", "params_head", "=", "split_tree", "(", "params", ",", "'SharedEncoder'", ")", "\n", "opt_state_enc", "=", "tx_enc", ".", "init", "(", "params_enc", ")", "\n", "opt_state_head", "=", "tx", ".", "init", "(", "params_head", ")", "\n", "", "else", ":", "\n", "            ", "opt_state_enc", ",", "opt_state_head", "=", "None", ",", "None", "\n", "\n", "", "return", "cls", "(", "step", "=", "1", ",", "\n", "apply_fn", "=", "model_def", ",", "\n", "params", "=", "params", ",", "\n", "tx", "=", "tx", ",", "\n", "tx_enc", "=", "tx_enc", ",", "\n", "opt_state_enc", "=", "opt_state_enc", ",", "\n", "opt_state_head", "=", "opt_state_head", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.__call__": [[151, 153], ["common.ModelDecoupleOpt.apply_fn.apply"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.apply"], ["", "def", "__call__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "apply_fn", ".", "apply", "(", "{", "'params'", ":", "self", ".", "params", "}", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.apply": [[154, 156], ["common.ModelDecoupleOpt.apply_fn.apply"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.apply"], ["", "def", "apply", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "apply_fn", ".", "apply", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.apply_gradient": [[157, 179], ["jax.grad", "jax.grad", "jax.grad", "jax.grad", "jax.grad.", "jax.grad.", "common.tree_norm", "common.split_tree", "common.split_tree", "common.ModelDecoupleOpt.tx_enc.update", "optax.apply_updates", "common.ModelDecoupleOpt.tx.update", "optax.apply_updates", "flax.core.FrozenDict", "flax.core.FrozenDict", "flax.core.FrozenDict", "flax.core.FrozenDict", "common.ModelDecoupleOpt.replace"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.tree_norm", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.split_tree", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.split_tree", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.temperature.update", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.temperature.update"], ["", "def", "apply_gradient", "(", "self", ",", "loss_fn", ")", "->", "Tuple", "[", "Any", ",", "'Model'", "]", ":", "\n", "        ", "grad_fn", "=", "jax", ".", "grad", "(", "loss_fn", ",", "has_aux", "=", "True", ")", "\n", "grads", ",", "info", "=", "grad_fn", "(", "self", ".", "params", ")", "\n", "grad_norm", "=", "tree_norm", "(", "grads", ")", "\n", "info", "[", "'grad_norm'", "]", "=", "grad_norm", "\n", "\n", "params_enc", ",", "params_head", "=", "split_tree", "(", "self", ".", "params", ",", "'SharedEncoder'", ")", "\n", "grads_enc", ",", "grads_head", "=", "split_tree", "(", "grads", ",", "'SharedEncoder'", ")", "\n", "\n", "updates_enc", ",", "new_opt_state_enc", "=", "self", ".", "tx_enc", ".", "update", "(", "grads_enc", ",", "self", ".", "opt_state_enc", ",", "\n", "params_enc", ")", "\n", "new_params_enc", "=", "optax", ".", "apply_updates", "(", "params_enc", ",", "updates_enc", ")", "\n", "\n", "updates_head", ",", "new_opt_state_head", "=", "self", ".", "tx", ".", "update", "(", "grads_head", ",", "self", ".", "opt_state_head", ",", "\n", "params_head", ")", "\n", "new_params_head", "=", "optax", ".", "apply_updates", "(", "params_head", ",", "updates_head", ")", "\n", "\n", "new_params", "=", "flax", ".", "core", ".", "FrozenDict", "(", "{", "**", "new_params_head", ",", "'SharedEncoder'", ":", "new_params_enc", "}", ")", "\n", "return", "self", ".", "replace", "(", "step", "=", "self", ".", "step", "+", "1", ",", "\n", "params", "=", "new_params", ",", "\n", "opt_state_enc", "=", "new_opt_state_enc", ",", "\n", "opt_state_head", "=", "new_opt_state_head", ")", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.save": [[180, 184], ["os.makedirs", "os.path.dirname", "open", "f.write", "flax.serialization.to_bytes", "flax.serialization.to_bytes", "flax.serialization.to_bytes", "flax.serialization.to_bytes"], "methods", ["None"], ["", "def", "save", "(", "self", ",", "save_path", ":", "str", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "save_path", ")", ",", "exist_ok", "=", "True", ")", "\n", "with", "open", "(", "save_path", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "flax", ".", "serialization", ".", "to_bytes", "(", "self", ".", "params", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.ModelDecoupleOpt.load": [[185, 189], ["common.ModelDecoupleOpt.replace", "open", "flax.serialization.from_bytes", "flax.serialization.from_bytes", "flax.serialization.from_bytes", "flax.serialization.from_bytes", "f.read"], "methods", ["None"], ["", "", "def", "load", "(", "self", ",", "load_path", ":", "str", ")", "->", "'Model'", ":", "\n", "        ", "with", "open", "(", "load_path", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "params", "=", "flax", ".", "serialization", ".", "from_bytes", "(", "self", ".", "params", ",", "f", ".", "read", "(", ")", ")", "\n", "", "return", "self", ".", "replace", "(", "params", "=", "params", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.tree_norm": [[10, 12], ["jax.sqrt", "sum", "jax.tree_leaves", "jax.tree_leaves"], "function", ["None"], ["def", "tree_norm", "(", "tree", ")", ":", "\n", "    ", "return", "jnp", ".", "sqrt", "(", "sum", "(", "(", "x", "**", "2", ")", ".", "sum", "(", ")", "for", "x", "in", "jax", ".", "tree_leaves", "(", "tree", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.default_init": [[13, 15], ["jax.sqrt", "flax.initializers.orthogonal"], "function", ["None"], ["", "def", "default_init", "(", "scale", ":", "Optional", "[", "float", "]", "=", "jnp", ".", "sqrt", "(", "2", ")", ")", ":", "\n", "    ", "return", "nn", ".", "initializers", ".", "orthogonal", "(", "scale", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.common.split_tree": [[103, 109], ["tree.unfreeze", "flax.core.FrozenDict.pop", "flax.core.FrozenDict", "flax.core.FrozenDict", "flax.core.FrozenDict", "flax.core.FrozenDict"], "function", ["None"], ["", "", "def", "split_tree", "(", "tree", ",", "key", ")", ":", "\n", "    ", "tree_head", "=", "tree", ".", "unfreeze", "(", ")", "\n", "tree_enc", "=", "tree_head", ".", "pop", "(", "key", ")", "\n", "tree_head", "=", "flax", ".", "core", ".", "FrozenDict", "(", "tree_head", ")", "\n", "tree_enc", "=", "flax", ".", "core", ".", "FrozenDict", "(", "tree_enc", ")", "\n", "return", "tree_enc", ",", "tree_head", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.critic_net.ValueCritic.__call__": [[14, 18], ["jax.squeeze", "continuous_control.networks.common.MLP"], "methods", ["None"], ["@", "nn", ".", "compact", "\n", "def", "__call__", "(", "self", ",", "observations", ":", "jnp", ".", "ndarray", ")", "->", "jnp", ".", "ndarray", ":", "\n", "        ", "critic", "=", "MLP", "(", "(", "*", "self", ".", "hidden_dims", ",", "1", ")", ")", "(", "observations", ")", "\n", "return", "jnp", ".", "squeeze", "(", "critic", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.critic_net.Critic.__call__": [[24, 31], ["jax.concatenate", "jax.squeeze", "continuous_control.networks.common.MLP"], "methods", ["None"], ["@", "nn", ".", "compact", "\n", "def", "__call__", "(", "self", ",", "observations", ":", "jnp", ".", "ndarray", ",", "\n", "actions", ":", "jnp", ".", "ndarray", ")", "->", "jnp", ".", "ndarray", ":", "\n", "        ", "inputs", "=", "jnp", ".", "concatenate", "(", "[", "observations", ",", "actions", "]", ",", "-", "1", ")", "\n", "critic", "=", "MLP", "(", "(", "*", "self", ".", "hidden_dims", ",", "1", ")", ",", "\n", "activations", "=", "self", ".", "activations", ")", "(", "inputs", ")", "\n", "return", "jnp", ".", "squeeze", "(", "critic", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.networks.critic_net.DoubleCritic.__call__": [[37, 45], ["critic_net.Critic", "critic_net.Critic"], "methods", ["None"], ["@", "nn", ".", "compact", "\n", "def", "__call__", "(", "self", ",", "observations", ":", "jnp", ".", "ndarray", ",", "\n", "actions", ":", "jnp", ".", "ndarray", ")", "->", "Tuple", "[", "jnp", ".", "ndarray", ",", "jnp", ".", "ndarray", "]", ":", "\n", "        ", "critic1", "=", "Critic", "(", "self", ".", "hidden_dims", ",", "\n", "activations", "=", "self", ".", "activations", ")", "(", "observations", ",", "actions", ")", "\n", "critic2", "=", "Critic", "(", "self", ".", "hidden_dims", ",", "\n", "activations", "=", "self", ".", "activations", ")", "(", "observations", ",", "actions", ")", "\n", "return", "critic1", ",", "critic2", "\n", "", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.video_recorder.VideoRecorder.__init__": [[13, 32], ["gym.Wrapper.__init__", "os.makedirs"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.__init__"], ["    ", "def", "__init__", "(", "self", ",", "\n", "env", ":", "gym", ".", "Env", ",", "\n", "save_folder", ":", "str", "=", "''", ",", "\n", "height", ":", "int", "=", "128", ",", "\n", "width", ":", "int", "=", "128", ",", "\n", "fps", ":", "int", "=", "30", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "env", ")", "\n", "\n", "self", ".", "current_episode", "=", "0", "\n", "self", ".", "save_folder", "=", "save_folder", "\n", "self", ".", "height", "=", "height", "\n", "self", ".", "width", "=", "width", "\n", "self", ".", "fps", "=", "fps", "\n", "self", ".", "frames", "=", "[", "]", "\n", "\n", "try", ":", "\n", "            ", "os", ".", "makedirs", "(", "save_folder", ",", "exist_ok", "=", "True", ")", "\n", "", "except", ":", "\n", "            ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.video_recorder.VideoRecorder.step": [[33, 60], ["video_recorder.VideoRecorder.env.render", "video_recorder.VideoRecorder.frames.append", "video_recorder.VideoRecorder.env.step", "os.path.join", "imageio.mimsave", "video_recorder.VideoRecorder.sim.render", "numpy.flipud", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.dmc_env.DMCEnv.render", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.step", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.dmc_env.DMCEnv.render"], ["", "", "def", "step", "(", "self", ",", "action", ":", "np", ".", "ndarray", ")", "->", "TimeStep", ":", "\n", "\n", "        ", "frame", "=", "self", ".", "env", ".", "render", "(", "mode", "=", "'rgb_array'", ",", "\n", "height", "=", "self", ".", "height", ",", "\n", "width", "=", "self", ".", "width", ")", "\n", "\n", "if", "frame", "is", "None", ":", "\n", "            ", "try", ":", "\n", "                ", "frame", "=", "self", ".", "sim", ".", "render", "(", "width", "=", "self", ".", "width", ",", "\n", "height", "=", "self", ".", "height", ",", "\n", "mode", "=", "'offscreen'", ")", "\n", "frame", "=", "np", ".", "flipud", "(", "frame", ")", "\n", "", "except", ":", "\n", "                ", "raise", "NotImplementedError", "(", "'Rendering is not implemented.'", ")", "\n", "\n", "", "", "self", ".", "frames", ".", "append", "(", "frame", ")", "\n", "\n", "observation", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "\n", "if", "done", ":", "\n", "            ", "save_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "save_folder", ",", "\n", "f'{self.current_episode}.mp4'", ")", "\n", "imageio", ".", "mimsave", "(", "save_file", ",", "self", ".", "frames", ",", "fps", "=", "self", ".", "fps", ")", "\n", "self", ".", "frames", "=", "[", "]", "\n", "self", ".", "current_episode", "+=", "1", "\n", "\n", "", "return", "observation", ",", "reward", ",", "done", ",", "info", "\n", "", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.rgb2gray.RGB2Gray.__init__": [[6, 14], ["gym.ObservationWrapper.__init__", "gym.spaces.Box"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "env", ")", "\n", "\n", "obs_shape", "=", "env", ".", "observation_space", ".", "shape", "\n", "self", ".", "observation_space", "=", "gym", ".", "spaces", ".", "Box", "(", "low", "=", "0", ",", "\n", "high", "=", "255", ",", "\n", "shape", "=", "(", "*", "obs_shape", "[", ":", "2", "]", ",", "1", ")", ",", "\n", "dtype", "=", "np", ".", "uint8", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.rgb2gray.RGB2Gray.observation": [[15, 18], ["numpy.dot", "numpy.dot.astype"], "methods", ["None"], ["", "def", "observation", "(", "self", ",", "observation", ")", ":", "\n", "        ", "observation", "=", "np", ".", "dot", "(", "observation", ",", "[", "[", "0.299", "]", ",", "[", "0.587", "]", ",", "[", "0.114", "]", "]", ")", "\n", "return", "observation", ".", "astype", "(", "np", ".", "uint8", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.single_precision.SinglePrecision.__init__": [[9, 23], ["gym.ObservationWrapper.__init__", "isinstance", "gym.spaces.Box", "isinstance", "copy.copy", "copy.copy.items", "gym.spaces.Dict", "gym.spaces.Box"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "env", ")", "\n", "\n", "if", "isinstance", "(", "self", ".", "observation_space", ",", "Box", ")", ":", "\n", "            ", "obs_space", "=", "self", ".", "observation_space", "\n", "self", ".", "observation_space", "=", "Box", "(", "obs_space", ".", "low", ",", "obs_space", ".", "high", ",", "\n", "obs_space", ".", "shape", ")", "\n", "", "elif", "isinstance", "(", "self", ".", "observation_space", ",", "Dict", ")", ":", "\n", "            ", "obs_spaces", "=", "copy", ".", "copy", "(", "self", ".", "observation_space", ".", "spaces", ")", "\n", "for", "k", ",", "v", "in", "obs_spaces", ".", "items", "(", ")", ":", "\n", "                ", "obs_spaces", "[", "k", "]", "=", "Box", "(", "v", ".", "low", ",", "v", ".", "high", ",", "v", ".", "shape", ")", "\n", "", "self", ".", "observation_space", "=", "Dict", "(", "obs_spaces", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.single_precision.SinglePrecision.observation": [[24, 32], ["isinstance", "copy.copy.astype", "isinstance", "copy.copy", "copy.copy.items", "v.astype"], "methods", ["None"], ["", "", "def", "observation", "(", "self", ",", "observation", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "if", "isinstance", "(", "observation", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "return", "observation", ".", "astype", "(", "np", ".", "float32", ")", "\n", "", "elif", "isinstance", "(", "observation", ",", "dict", ")", ":", "\n", "            ", "observation", "=", "copy", ".", "copy", "(", "observation", ")", "\n", "for", "k", ",", "v", "in", "observation", ".", "items", "(", ")", ":", "\n", "                ", "observation", "[", "k", "]", "=", "v", ".", "astype", "(", "np", ".", "float32", ")", "\n", "", "return", "observation", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.frame_stack.FrameStack.__init__": [[9, 23], ["gym.Wrapper.__init__", "collections.deque", "numpy.repeat", "numpy.repeat", "gym.spaces.Box"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "num_stack", ":", "int", ",", "stack_axis", "=", "-", "1", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "env", ")", "\n", "self", ".", "_num_stack", "=", "num_stack", "\n", "self", ".", "_stack_axis", "=", "stack_axis", "\n", "\n", "self", ".", "_frames", "=", "collections", ".", "deque", "(", "[", "]", ",", "maxlen", "=", "num_stack", ")", "\n", "\n", "low", "=", "np", ".", "repeat", "(", "self", ".", "observation_space", ".", "low", ",", "num_stack", ",", "axis", "=", "stack_axis", ")", "\n", "high", "=", "np", ".", "repeat", "(", "self", ".", "observation_space", ".", "high", ",", "\n", "num_stack", ",", "\n", "axis", "=", "stack_axis", ")", "\n", "self", ".", "observation_space", "=", "Box", "(", "low", "=", "low", ",", "\n", "high", "=", "high", ",", "\n", "dtype", "=", "self", ".", "observation_space", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.frame_stack.FrameStack.reset": [[24, 29], ["frame_stack.FrameStack.env.reset", "range", "frame_stack.FrameStack._get_obs", "frame_stack.FrameStack._frames.append"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.absorbing_states.AbsorbingStatesWrapper.reset", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.frame_stack.FrameStack._get_obs"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "obs", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "for", "_", "in", "range", "(", "self", ".", "_num_stack", ")", ":", "\n", "            ", "self", ".", "_frames", ".", "append", "(", "obs", ")", "\n", "", "return", "self", ".", "_get_obs", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.frame_stack.FrameStack.step": [[30, 34], ["frame_stack.FrameStack.env.step", "frame_stack.FrameStack._frames.append", "frame_stack.FrameStack._get_obs"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.step", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.frame_stack.FrameStack._get_obs"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "obs", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "self", ".", "_frames", ".", "append", "(", "obs", ")", "\n", "return", "self", ".", "_get_obs", "(", ")", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.frame_stack.FrameStack._get_obs": [[35, 38], ["numpy.concatenate", "len", "list"], "methods", ["None"], ["", "def", "_get_obs", "(", "self", ")", ":", "\n", "        ", "assert", "len", "(", "self", ".", "_frames", ")", "==", "self", ".", "_num_stack", "\n", "return", "np", ".", "concatenate", "(", "list", "(", "self", ".", "_frames", ")", ",", "axis", "=", "self", ".", "_stack_axis", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.take_key.TakeKey.__init__": [[7, 13], ["gym.ObservationWrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "take_key", ")", ":", "\n", "        ", "super", "(", "TakeKey", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "self", ".", "_take_key", "=", "take_key", "\n", "\n", "assert", "take_key", "in", "self", ".", "observation_space", ".", "spaces", "\n", "self", ".", "observation_space", "=", "self", ".", "env", ".", "observation_space", "[", "take_key", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.take_key.TakeKey.observation": [[14, 19], ["copy.copy", "copy.copy.pop"], "methods", ["None"], ["", "def", "observation", "(", "self", ",", "observation", ")", ":", "\n", "        ", "observation", "=", "copy", ".", "copy", "(", "observation", ")", "\n", "taken_observation", "=", "observation", ".", "pop", "(", "self", ".", "_take_key", ")", "\n", "self", ".", "_ignored_observations", "=", "observation", "\n", "return", "taken_observation", "\n", "", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.sticky_actions.StickyActionEnv.__init__": [[11, 15], ["gym.Wrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "p", "=", "0.25", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "env", ")", "\n", "self", ".", "p", "=", "p", "\n", "self", ".", "last_action", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.sticky_actions.StickyActionEnv.step": [[16, 22], ["sticky_actions.StickyActionEnv.env.step", "numpy.random.uniform"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.step"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "if", "np", ".", "random", ".", "uniform", "(", ")", "<", "self", ".", "p", ":", "\n", "            ", "action", "=", "self", ".", "last_action", "\n", "", "self", ".", "last_action", "=", "action", "\n", "obs", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "return", "obs", ",", "reward", ",", "done", ",", "info", "", "", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.episode_monitor.EpisodeMonitor.__init__": [[11, 15], ["gym.ActionWrapper.__init__", "episode_monitor.EpisodeMonitor._reset_stats"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.__init__", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.episode_monitor.EpisodeMonitor._reset_stats"], ["def", "__init__", "(", "self", ",", "env", ":", "gym", ".", "Env", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "env", ")", "\n", "self", ".", "_reset_stats", "(", ")", "\n", "self", ".", "total_timesteps", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.episode_monitor.EpisodeMonitor._reset_stats": [[16, 20], ["time.time"], "methods", ["None"], ["", "def", "_reset_stats", "(", "self", ")", ":", "\n", "        ", "self", ".", "reward_sum", "=", "0.0", "\n", "self", ".", "episode_length", "=", "0", "\n", "self", ".", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.episode_monitor.EpisodeMonitor.step": [[21, 40], ["episode_monitor.EpisodeMonitor.env.step", "hasattr", "time.time", "episode_monitor.EpisodeMonitor.get_normalized_score"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.step"], ["", "def", "step", "(", "self", ",", "action", ":", "np", ".", "ndarray", ")", "->", "TimeStep", ":", "\n", "        ", "observation", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "\n", "self", ".", "reward_sum", "+=", "reward", "\n", "self", ".", "episode_length", "+=", "1", "\n", "self", ".", "total_timesteps", "+=", "1", "\n", "info", "[", "'total'", "]", "=", "{", "'timesteps'", ":", "self", ".", "total_timesteps", "}", "\n", "\n", "if", "done", ":", "\n", "            ", "info", "[", "'episode'", "]", "=", "{", "}", "\n", "info", "[", "'episode'", "]", "[", "'return'", "]", "=", "self", ".", "reward_sum", "\n", "info", "[", "'episode'", "]", "[", "'length'", "]", "=", "self", ".", "episode_length", "\n", "info", "[", "'episode'", "]", "[", "'duration'", "]", "=", "time", ".", "time", "(", ")", "-", "self", ".", "start_time", "\n", "\n", "if", "hasattr", "(", "self", ",", "'get_normalized_score'", ")", ":", "\n", "                ", "info", "[", "'episode'", "]", "[", "'return'", "]", "=", "self", ".", "get_normalized_score", "(", "\n", "info", "[", "'episode'", "]", "[", "'return'", "]", ")", "*", "100.0", "\n", "\n", "", "", "return", "observation", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.episode_monitor.EpisodeMonitor.reset": [[41, 44], ["episode_monitor.EpisodeMonitor._reset_stats", "episode_monitor.EpisodeMonitor.env.reset"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.episode_monitor.EpisodeMonitor._reset_stats", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.absorbing_states.AbsorbingStatesWrapper.reset"], ["", "def", "reset", "(", "self", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "self", ".", "_reset_stats", "(", ")", "\n", "return", "self", ".", "env", ".", "reset", "(", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.repeat_action.RepeatAction.__init__": [[8, 11], ["gym.Wrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "action_repeat", "=", "4", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "env", ")", "\n", "self", ".", "_action_repeat", "=", "action_repeat", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.repeat_action.RepeatAction.step": [[12, 25], ["range", "repeat_action.RepeatAction.env.step", "combined_info.update"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.step", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.sac.temperature.update"], ["", "def", "step", "(", "self", ",", "action", ":", "np", ".", "ndarray", ")", "->", "TimeStep", ":", "\n", "        ", "total_reward", "=", "0.0", "\n", "done", "=", "None", "\n", "combined_info", "=", "{", "}", "\n", "\n", "for", "_", "in", "range", "(", "self", ".", "_action_repeat", ")", ":", "\n", "            ", "obs", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "total_reward", "+=", "reward", "\n", "combined_info", ".", "update", "(", "info", ")", "\n", "if", "done", ":", "\n", "                ", "break", "\n", "\n", "", "", "return", "obs", ",", "total_reward", ",", "done", ",", "combined_info", "\n", "", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.dmc_env.DMCEnv.__init__": [[37, 54], ["dm_control.suite.load", "dmc_env.dmc_spec2gym_space", "dmc_env.dmc_spec2gym_space", "dmc_env.DMCEnv.seed", "dmc_env.DMCEnv._env.action_spec", "dmc_env.DMCEnv._env.observation_spec"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.load", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.dmc_env.dmc_spec2gym_space", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.dmc_env.dmc_spec2gym_space"], ["    ", "def", "__init__", "(", "self", ",", "\n", "domain_name", ":", "str", ",", "\n", "task_name", ":", "str", ",", "\n", "task_kwargs", ":", "Optional", "[", "Dict", "]", "=", "{", "}", ",", "\n", "environment_kwargs", "=", "None", ")", ":", "\n", "        ", "assert", "'random'", "in", "task_kwargs", ",", "'please specify a seed, for deterministic behaviour'", "\n", "\n", "self", ".", "_env", "=", "suite", ".", "load", "(", "domain_name", "=", "domain_name", ",", "\n", "task_name", "=", "task_name", ",", "\n", "task_kwargs", "=", "task_kwargs", ",", "\n", "environment_kwargs", "=", "environment_kwargs", ")", "\n", "self", ".", "action_space", "=", "dmc_spec2gym_space", "(", "self", ".", "_env", ".", "action_spec", "(", ")", ")", "\n", "\n", "self", ".", "observation_space", "=", "dmc_spec2gym_space", "(", "\n", "self", ".", "_env", ".", "observation_spec", "(", ")", ")", "\n", "\n", "self", ".", "seed", "(", "seed", "=", "task_kwargs", "[", "'random'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.dmc_env.DMCEnv.__getattr__": [[55, 57], ["getattr"], "methods", ["None"], ["", "def", "__getattr__", "(", "self", ",", "name", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ".", "_env", ",", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.dmc_env.DMCEnv.step": [[58, 71], ["dmc_env.DMCEnv.action_space.contains", "dmc_env.DMCEnv._env.step", "dmc_env.DMCEnv.last"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.step"], ["", "def", "step", "(", "self", ",", "action", ":", "np", ".", "ndarray", ")", "->", "TimeStep", ":", "\n", "        ", "assert", "self", ".", "action_space", ".", "contains", "(", "action", ")", "\n", "\n", "time_step", "=", "self", ".", "_env", ".", "step", "(", "action", ")", "\n", "reward", "=", "time_step", ".", "reward", "or", "0", "\n", "done", "=", "time_step", ".", "last", "(", ")", "\n", "obs", "=", "time_step", ".", "observation", "\n", "\n", "info", "=", "{", "}", "\n", "if", "done", "and", "time_step", ".", "discount", "==", "1.0", ":", "\n", "            ", "info", "[", "'TimeLimit.truncated'", "]", "=", "True", "\n", "\n", "", "return", "obs", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.dmc_env.DMCEnv.reset": [[72, 75], ["dmc_env.DMCEnv._env.reset"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.absorbing_states.AbsorbingStatesWrapper.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "time_step", "=", "self", ".", "_env", ".", "reset", "(", ")", "\n", "return", "time_step", ".", "observation", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.dmc_env.DMCEnv.render": [[76, 85], ["dmc_env.DMCEnv._env.physics.render"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.dmc_env.DMCEnv.render"], ["", "def", "render", "(", "self", ",", "\n", "mode", "=", "'rgb_array'", ",", "\n", "height", ":", "int", "=", "84", ",", "\n", "width", ":", "int", "=", "84", ",", "\n", "camera_id", ":", "int", "=", "0", ")", ":", "\n", "        ", "assert", "mode", "==", "'rgb_array'", ",", "'only support rgb_array mode, given %s'", "%", "mode", "\n", "return", "self", ".", "_env", ".", "physics", ".", "render", "(", "height", "=", "height", ",", "\n", "width", "=", "width", ",", "\n", "camera_id", "=", "camera_id", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.dmc_env.dmc_spec2gym_space": [[16, 34], ["isinstance", "copy.copy", "copy.copy.items", "gym.spaces.Dict", "isinstance", "dmc_env.dmc_spec2gym_space", "gym.spaces.Box", "isinstance", "gym.spaces.Box", "float", "float"], "function", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.dmc_env.dmc_spec2gym_space"], ["def", "dmc_spec2gym_space", "(", "spec", ")", ":", "\n", "    ", "if", "isinstance", "(", "spec", ",", "OrderedDict", ")", ":", "\n", "        ", "spec", "=", "copy", ".", "copy", "(", "spec", ")", "\n", "for", "k", ",", "v", "in", "spec", ".", "items", "(", ")", ":", "\n", "            ", "spec", "[", "k", "]", "=", "dmc_spec2gym_space", "(", "v", ")", "\n", "", "return", "spaces", ".", "Dict", "(", "spec", ")", "\n", "", "elif", "isinstance", "(", "spec", ",", "specs", ".", "BoundedArray", ")", ":", "\n", "        ", "return", "spaces", ".", "Box", "(", "low", "=", "spec", ".", "minimum", ",", "\n", "high", "=", "spec", ".", "maximum", ",", "\n", "shape", "=", "spec", ".", "shape", ",", "\n", "dtype", "=", "spec", ".", "dtype", ")", "\n", "", "elif", "isinstance", "(", "spec", ",", "specs", ".", "Array", ")", ":", "\n", "        ", "return", "spaces", ".", "Box", "(", "low", "=", "-", "float", "(", "'inf'", ")", ",", "\n", "high", "=", "float", "(", "'inf'", ")", ",", "\n", "shape", "=", "spec", ".", "shape", ",", "\n", "dtype", "=", "spec", ".", "dtype", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.absorbing_states.AbsorbingStatesWrapper.__init__": [[11, 21], ["gym.Wrapper.__init__", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "gym.spaces.Box", "numpy.zeros_like"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "env", ")", "\n", "low", "=", "env", ".", "observation_space", ".", "low", "\n", "high", "=", "env", ".", "observation_space", ".", "high", "\n", "self", ".", "_absorbing_state", "=", "np", ".", "concatenate", "(", "[", "np", ".", "zeros_like", "(", "low", ")", ",", "[", "1.0", "]", "]", ",", "0", ")", "\n", "low", "=", "np", ".", "concatenate", "(", "[", "low", ",", "[", "0", "]", "]", ",", "0", ")", "\n", "high", "=", "np", ".", "concatenate", "(", "[", "high", ",", "[", "1", "]", "]", ",", "0", ")", "\n", "\n", "self", ".", "observation_space", "=", "gym", ".", "spaces", ".", "Box", "(", "\n", "low", "=", "low", ",", "high", "=", "high", ",", "dtype", "=", "env", ".", "observation_space", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.absorbing_states.AbsorbingStatesWrapper.reset": [[22, 27], ["absorbing_states.make_non_absorbing", "absorbing_states.AbsorbingStatesWrapper.env.reset"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.absorbing_states.make_non_absorbing", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.absorbing_states.AbsorbingStatesWrapper.reset"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "_done", "=", "False", "\n", "self", ".", "_absorbing", "=", "False", "\n", "self", ".", "_info", "=", "{", "}", "\n", "return", "make_non_absorbing", "(", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.absorbing_states.AbsorbingStatesWrapper.step": [[28, 42], ["absorbing_states.AbsorbingStatesWrapper.env.step", "absorbing_states.make_non_absorbing"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.step", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.absorbing_states.make_non_absorbing"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "if", "not", "self", ".", "_done", ":", "\n", "            ", "observation", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "observation", "=", "make_non_absorbing", "(", "observation", ")", "\n", "self", ".", "_done", "=", "done", "\n", "self", ".", "_info", "=", "info", "\n", "truncated_done", "=", "'TimeLimit.truncated'", "in", "info", "\n", "return", "observation", ",", "reward", ",", "truncated_done", ",", "info", "\n", "", "else", ":", "\n", "            ", "if", "not", "self", ".", "_absorbing", ":", "\n", "                ", "self", ".", "_absorbing", "=", "True", "\n", "return", "self", ".", "_absorbing_state", ",", "0.0", ",", "False", ",", "self", ".", "_info", "\n", "", "else", ":", "\n", "                ", "return", "self", ".", "_absorbing_state", ",", "0.0", ",", "True", ",", "self", ".", "_info", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.absorbing_states.make_non_absorbing": [[6, 8], ["numpy.concatenate"], "function", ["None"], ["def", "make_non_absorbing", "(", "observation", ")", ":", "\n", "    ", "return", "np", ".", "concatenate", "(", "[", "observation", ",", "[", "0.0", "]", "]", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.NoisyNetwork.sample_noise": [[112, 115], ["jax.random.normal", "jax.random.normal", "jax.random.normal"], "methods", ["None"], []], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.NoisyNetwork.f": [[116, 120], ["jax.multiply", "jax.multiply", "jax.multiply", "jax.sign", "jax.sign", "jax.sign", "jax.power", "jax.power", "jax.power", "jax.abs", "jax.abs", "jax.abs"], "methods", ["None"], []], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.NoisyNetwork.__call__": [[121, 163], ["networks.NoisyNetwork.sample_noise", "networks.NoisyNetwork.sample_noise", "networks.NoisyNetwork.f", "networks.NoisyNetwork.f", "jax.squeeze", "jax.squeeze", "jax.squeeze", "networks.NoisyNetwork.param", "networks.NoisyNetwork.param", "jax.where", "jax.where", "jax.where", "jax.matmul", "jax.matmul", "jax.matmul", "jax.where", "jax.where", "jax.where", "networks.NoisyNetwork.param", "networks.NoisyNetwork.param", "jax.where", "jax.where", "jax.where", "jax.random.uniform", "jax.random.uniform", "jax.random.uniform", "numpy.zeros", "jax.multiply", "jax.multiply", "jax.multiply", "numpy.zeros", "jax.multiply", "jax.multiply", "jax.multiply", "jax.power", "jax.power", "jax.power", "jax.power", "jax.power", "jax.power", "jax.ones", "jax.ones", "jax.ones", "numpy.sqrt"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.NoisyNetwork.sample_noise", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.NoisyNetwork.sample_noise", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.NoisyNetwork.f", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.NoisyNetwork.f"], []], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.NoStatsBatchNorm.__call__": [[201, 255], ["jax.asarray", "jax.asarray", "jax.asarray", "networks._absolute_dims", "tuple", "tuple", "tuple", "networks.NoStatsBatchNorm.is_mutable_collection", "jax.mean", "jax.mean", "jax.mean", "jax.mean", "jax.mean", "jax.mean", "jax.lax.rsqrt", "jax.lax.rsqrt", "jax.lax.rsqrt", "jax.asarray", "jax.asarray", "jax.asarray", "isinstance", "jax.lax.square", "jax.lax.square", "jax.lax.square", "jax.concatenate", "jax.concatenate", "jax.concatenate", "jax.split", "jax.split", "jax.split", "jax.lax.square", "jax.lax.square", "jax.lax.square", "jax.mean.reshape", "networks.NoStatsBatchNorm.param().reshape", "networks.NoStatsBatchNorm.param().reshape", "jax.lax.pmean", "jax.lax.pmean", "jax.lax.pmean", "enumerate", "enumerate", "range", "networks.NoStatsBatchNorm.param", "networks.NoStatsBatchNorm.param"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks._absolute_dims"], []], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.FeatureLayer.setup": [[261, 266], ["networks.NoisyNetwork", "flax.linen.Dense", "flax.linen.initializers.xavier_uniform"], "methods", ["None"], []], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.FeatureLayer.__call__": [[267, 272], ["networks.FeatureLayer.net", "networks.FeatureLayer.net"], "methods", ["None"], []], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.LinearHead.setup": [[280, 286], ["networks.FeatureLayer", "networks.FeatureLayer", "networks.FeatureLayer"], "methods", ["None"], []], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.LinearHead.__call__": [[287, 301], ["networks.LinearHead.advantage", "networks.LinearHead.value", "adv.reshape.reshape.reshape", "value.reshape.reshape.reshape", "networks.LinearHead.advantage", "networks.LinearHead.reshape", "jax.mean", "jax.mean", "jax.mean"], "methods", ["None"], []], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.ConvTMCell.setup": [[331, 334], ["networks.NoStatsBatchNorm"], "methods", ["None"], []], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.ConvTMCell.__call__": [[335, 363], ["jax.nn.one_hot", "jax.nn.one_hot", "jax.nn.one_hot", "jax.nn.one_hot", "jax.nn.one_hot", "jax.nn.one_hot", "jax.nn.one_hot", "jax.nn.one_hot", "jax.nn.one_hot", "jax.lax.broadcast", "jax.lax.broadcast", "jax.lax.broadcast", "jax.lax.broadcast", "jax.lax.broadcast", "jax.lax.broadcast", "jax.lax.broadcast", "jax.lax.broadcast", "jax.lax.broadcast", "jax.concatenate", "jax.concatenate", "jax.concatenate", "range", "flax.linen.relu", "flax.linen.relu", "flax.linen.Conv", "networks.renormalize", "flax.linen.Conv", "flax.linen.initializers.xavier_uniform", "flax.linen.initializers.xavier_uniform"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.renormalize"], []], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.RainbowCNN.__call__": [[368, 383], ["range", "flax.linen.relu", "flax.linen.Conv", "flax.linen.initializers.xavier_uniform"], "methods", ["None"], []], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.TransitionModel.__call__": [[390, 399], ["scan", "flax.linen.scan"], "methods", ["None"], []], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.RainbowDQNNetwork.setup": [[420, 433], ["networks.TransitionModel", "networks.FeatureLayer", "flax.linen.Dense", "networks.RainbowCNN", "networks.LinearHead"], "methods", ["None"], []], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.RainbowDQNNetwork.encode": [[435, 440], ["networks.RainbowDQNNetwork.encoder", "networks.renormalize"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.renormalize"], []], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.RainbowDQNNetwork.project": [[441, 443], ["networks.RainbowDQNNetwork.projection"], "methods", ["None"], []], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.RainbowDQNNetwork.spr_predict": [[444, 447], ["functools.partial", "networks.RainbowDQNNetwork.predictor", "networks.RainbowDQNNetwork.projection"], "methods", ["None"], []], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.RainbowDQNNetwork.spr_rollout": [[448, 452], ["networks.RainbowDQNNetwork.transition_model", "networks.RainbowDQNNetwork.spr_predict", "pred_latents.reshape"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.RainbowDQNNetwork.spr_predict"], []], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.RainbowDQNNetwork.__call__": [[453, 489], ["networks.RainbowDQNNetwork.encode", "networks.RainbowDQNNetwork.projection", "flax.linen.relu", "networks.RainbowDQNNetwork.head", "jax.squeeze", "jax.squeeze", "jax.squeeze", "SPROutputType", "jax.random.PRNGKey", "jax.random.PRNGKey", "jax.random.PRNGKey", "networks.RainbowDQNNetwork.reshape", "networks.RainbowDQNNetwork.spr_rollout", "jax.squeeze", "jax.squeeze", "jax.squeeze", "jax.squeeze", "jax.squeeze", "jax.squeeze", "SPROutputType", "int", "flax.linen.softmax", "jax.sum", "jax.sum", "jax.sum", "time.time"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.RainbowDQNNetwork.encode", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.RainbowDQNNetwork.spr_rollout"], []], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks._absolute_dims": [[44, 46], ["tuple"], "function", ["None"], ["cnn_padding", ":", "str", "=", "'VALID'", "\n", "latent_dim", ":", "int", "=", "50", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks._random_crop": [[54, 61], ["jax.random.split", "jax.random.randint", "jax.random.randint"], "function", ["None"], ["\n", "x", "=", "nn", ".", "Dense", "(", "self", ".", "latent_dim", ")", "(", "x", ")", "\n", "x", "=", "nn", ".", "LayerNorm", "(", ")", "(", "x", ")", "\n", "x", "=", "nn", ".", "tanh", "(", "x", ")", "\n", "\n", "return", "DoubleCritic", "(", "self", ".", "hidden_dims", ")", "(", "x", ",", "actions", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks._crop_with_indices": [[64, 68], ["functools.partial", "jax.lax.dynamic_slice", "jax.lax.dynamic_slice", "jax.lax.dynamic_slice"], "function", ["None"], ["action_dim", ":", "int", "\n", "cnn_features", ":", "Sequence", "[", "int", "]", "=", "(", "32", ",", "32", ",", "32", ",", "32", ")", "\n", "cnn_strides", ":", "Sequence", "[", "int", "]", "=", "(", "2", ",", "1", ",", "1", ",", "1", ")", "\n", "cnn_padding", ":", "str", "=", "'VALID'", "\n", "latent_dim", ":", "int", "=", "50", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks._per_image_random_crop": [[70, 79], ["jax.random.split", "jax.random.randint", "jax.random.randint", "networks._crop_with_indices"], "function", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks._crop_with_indices"], ["@", "nn", ".", "compact", "\n", "def", "__call__", "(", "self", ",", "\n", "observations", ":", "jnp", ".", "ndarray", ",", "\n", "temperature", ":", "float", "=", "1.0", ")", "->", "tfd", ".", "Distribution", ":", "\n", "        ", "x", "=", "Encoder", "(", "self", ".", "cnn_features", ",", "\n", "self", ".", "cnn_strides", ",", "\n", "self", ".", "cnn_padding", ",", "\n", "name", "=", "'SharedEncoder'", ")", "(", "observations", ")", "\n", "\n", "# We do not update conv layers with policy gradients.", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks._intensity_aug": [[81, 86], ["jax.random.normal", "jax.clip"], "function", ["None"], ["\n", "x", "=", "nn", ".", "Dense", "(", "self", ".", "latent_dim", ")", "(", "x", ")", "\n", "x", "=", "nn", ".", "LayerNorm", "(", ")", "(", "x", ")", "\n", "x", "=", "nn", ".", "tanh", "(", "x", ")", "\n", "\n", "return", "NormalTanhPolicy", "(", "self", ".", "hidden_dims", ",", "self", ".", "action_dim", ")", "(", "x", ",", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.drq_image_aug": [[88, 102], ["obs.reshape", "jax.pad", "jax.random.split", "networks._per_image_random_crop", "networks._intensity_aug", "_intensity_aug.reshape"], "function", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks._per_image_random_crop", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks._intensity_aug"], ["", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.process_inputs": [[303, 311], ["x.astype", "networks.drq_image_aug", "ValueError"], "function", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.drq_image_aug"], []], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.networks.renormalize": [[313, 321], ["jnp.expand_dims.reshape", "jax.max", "jax.min", "jax.expand_dims"], "function", ["None"], []], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.DataEfficientAtariRunner.__init__": [[83, 120], ["dopamine.discrete_domains.run_experiment.Runner.__init__", "absl.logging.info", "eval_run_experiment.create_env_wrapper", "eval_run_experiment.DataEfficientAtariRunner._agent.reset_all", "eval_run_experiment.DataEfficientAtariRunner._agent.cache_train_state", "eval_run_experiment.DataEfficientAtariRunner.create_environment_fn", "eval_run_experiment.DataEfficientAtariRunner._initialize_episode", "hasattr", "range", "eval_run_experiment.DataEfficientAtariRunner.train_envs[].environment._game.lower().replace().replace", "eval_run_experiment.DataEfficientAtariRunner.train_envs[].environment.game.lower().replace().replace", "eval_run_experiment.DataEfficientAtariRunner.train_envs[].environment.env._game.lower().replace().replace", "eval_run_experiment.DataEfficientAtariRunner.train_envs[].environment._game.lower().replace", "eval_run_experiment.DataEfficientAtariRunner.train_envs[].environment.game.lower().replace", "eval_run_experiment.DataEfficientAtariRunner.train_envs[].environment.env._game.lower().replace", "eval_run_experiment.DataEfficientAtariRunner.train_envs[].environment._game.lower", "eval_run_experiment.DataEfficientAtariRunner.train_envs[].environment.game.lower", "eval_run_experiment.DataEfficientAtariRunner.train_envs[].environment.env._game.lower"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.__init__", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.create_env_wrapper", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent.reset_all", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent.cache_train_state", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.DataEfficientAtariRunner._initialize_episode"], ["def", "__init__", "(", "self", ",", "base_dir", ",", "\n", "create_agent_fn", ",", "\n", "create_environment_fn", "=", "atari_lib", ".", "create_atari_environment", ",", "\n", "num_eval_episodes", "=", "100", ",", "\n", "max_noops", "=", "30", ",", "\n", "parallel_eval", "=", "True", ",", "\n", "num_eval_envs", "=", "100", ",", "\n", "num_train_envs", "=", "4", ",", "\n", "eval_one_to_one", "=", "True", "\n", ")", ":", "\n", "        ", "\"\"\"Specify the number of evaluation episodes.\"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "base_dir", ",", "create_agent_fn", ",", "create_environment_fn", "=", "create_environment_fn", ")", "\n", "self", ".", "_num_eval_episodes", "=", "num_eval_episodes", "\n", "logging", ".", "info", "(", "'Num evaluation episodes: %d'", ",", "num_eval_episodes", ")", "\n", "self", ".", "_evaluation_steps", "=", "None", "\n", "self", ".", "num_steps", "=", "0", "\n", "self", ".", "total_steps", "=", "self", ".", "_training_steps", "*", "self", ".", "_num_iterations", "\n", "self", ".", "create_environment_fn", "=", "create_env_wrapper", "(", "create_environment_fn", ")", "\n", "\n", "self", ".", "max_noops", "=", "max_noops", "\n", "self", ".", "parallel_eval", "=", "parallel_eval", "\n", "self", ".", "num_eval_envs", "=", "num_eval_envs", "\n", "self", ".", "num_train_envs", "=", "num_train_envs", "\n", "self", ".", "eval_one_to_one", "=", "eval_one_to_one", "\n", "\n", "self", ".", "train_envs", "=", "[", "self", ".", "create_environment_fn", "(", ")", "for", "i", "in", "range", "(", "num_train_envs", ")", "]", "\n", "self", ".", "train_state", "=", "None", "\n", "self", ".", "_agent", ".", "reset_all", "(", "self", ".", "_initialize_episode", "(", "self", ".", "train_envs", ")", ")", "\n", "self", ".", "_agent", ".", "cache_train_state", "(", ")", "\n", "\n", "try", ":", "\n", "            ", "if", "hasattr", "(", "self", ".", "train_envs", "[", "0", "]", ".", "environment", ",", "\"_game\"", ")", ":", "\n", "                ", "self", ".", "game", "=", "self", ".", "train_envs", "[", "0", "]", ".", "environment", ".", "_game", ".", "lower", "(", ")", ".", "replace", "(", "\"_\"", ",", "\"\"", ")", ".", "replace", "(", "\" \"", ",", "\"\"", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "game", "=", "self", ".", "train_envs", "[", "0", "]", ".", "environment", ".", "game", ".", "lower", "(", ")", ".", "replace", "(", "\"_\"", ",", "\"\"", ")", ".", "replace", "(", "\" \"", ",", "\"\"", ")", "\n", "", "", "except", ":", "\n", "            ", "self", ".", "game", "=", "self", ".", "train_envs", "[", "0", "]", ".", "environment", ".", "env", ".", "_game", ".", "lower", "(", ")", ".", "replace", "(", "\"_\"", ",", "\"\"", ")", ".", "replace", "(", "\" \"", ",", "\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.DataEfficientAtariRunner._run_one_phase": [[121, 173], ["eval_run_experiment.DataEfficientAtariRunner._run_parallel", "zip", "statistics.append", "sys.stdout.flush", "tensorflow.compat.v1.Summary", "eval_run_experiment.DataEfficientAtariRunner._summary_writer.add_summary", "tensorflow.compat.v1.Summary.Value", "tensorflow.compat.v1.Summary.Value", "float", "float"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.DataEfficientAtariRunner._run_parallel"], ["", "", "def", "_run_one_phase", "(", "self", ",", "envs", ",", "steps", ",", "max_episodes", ",", "\n", "statistics", ",", "run_mode_str", ",", "\n", "needs_reset", "=", "False", ",", "one_to_one", "=", "False", ",", "resume_state", "=", "None", ")", ":", "\n", "        ", "\"\"\"Runs the agent/environment loop until a desired number of steps.\n\n        We follow the Machado et al., 2017 convention of running full episodes,\n        and terminating once we've run a minimum number of steps.\n\n        Args:\n          min_steps: int, minimum number of steps to generate in this phase.\n          max_steps: int, maximum number of steps to generate in this phase.\n          max_episodes: int, maximum number of episodes to generate in this phase.\n          statistics: `IterationStatistics` object which records the experimental\n            results.\n          run_mode_str: str, describes the run mode for this agent.\n\n        Returns:\n          Tuple containing the number of steps taken in this phase (int), the sum of\n            returns (float), and the number of episodes performed (int).\n        \"\"\"", "\n", "step_count", "=", "0", "\n", "num_episodes", "=", "0", "\n", "sum_returns", "=", "0.", "\n", "\n", "episode_lengths", ",", "episode_returns", ",", "state", ",", "envs", "=", "self", ".", "_run_parallel", "(", "episodes", "=", "max_episodes", ",", "envs", "=", "envs", ",", "\n", "one_to_one", "=", "one_to_one", ",", "\n", "needs_reset", "=", "needs_reset", ",", "\n", "resume_state", "=", "resume_state", ",", "\n", "max_steps", "=", "steps", ")", "\n", "\n", "for", "episode_length", ",", "episode_return", "in", "zip", "(", "episode_lengths", ",", "episode_returns", ")", ":", "\n", "            ", "statistics", ".", "append", "(", "{", "\n", "'{}_episode_lengths'", ".", "format", "(", "run_mode_str", ")", ":", "episode_length", ",", "\n", "'{}_episode_returns'", ".", "format", "(", "run_mode_str", ")", ":", "episode_return", "\n", "}", ")", "\n", "if", "run_mode_str", "==", "\"train\"", ":", "\n", "# we use one extra frame at the starting", "\n", "                ", "self", ".", "num_steps", "+=", "episode_length", "\n", "", "step_count", "+=", "episode_length", "\n", "sum_returns", "+=", "episode_return", "\n", "num_episodes", "+=", "1", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "if", "self", ".", "_summary_writer", "is", "not", "None", ":", "\n", "                ", "summary", "=", "tf", ".", "compat", ".", "v1", ".", "Summary", "(", "value", "=", "[", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "'train_episode_returns'", ",", "simple_value", "=", "float", "(", "episode_return", ")", ")", ",", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "'train_episode_lengths'", ",", "simple_value", "=", "float", "(", "episode_length", ")", ")", ",", "\n", "]", ")", "\n", "self", ".", "_summary_writer", ".", "add_summary", "(", "summary", ",", "self", ".", "num_steps", ")", "\n", "", "", "return", "step_count", ",", "sum_returns", ",", "num_episodes", ",", "state", ",", "envs", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.DataEfficientAtariRunner._initialize_episode": [[174, 194], ["numpy.stack", "env.reset", "observations.append", "jax.random.split", "jax.random.randint", "range", "env.step", "env.reset"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.absorbing_states.AbsorbingStatesWrapper.reset", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.step", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.wrappers.absorbing_states.AbsorbingStatesWrapper.reset"], ["", "def", "_initialize_episode", "(", "self", ",", "envs", ")", ":", "\n", "        ", "\"\"\"Initialization for a new episode.\n\n        Returns:\n          action: int, the initial action chosen by the agent.\n        \"\"\"", "\n", "observations", "=", "[", "]", "\n", "for", "env", "in", "envs", ":", "\n", "            ", "initial_observation", "=", "env", ".", "reset", "(", ")", "\n", "if", "self", ".", "max_noops", ">", "0", ":", "\n", "                ", "self", ".", "_agent", ".", "_rng", ",", "rng", "=", "jax", ".", "random", ".", "split", "(", "self", ".", "_agent", ".", "_rng", ")", "\n", "num_noops", "=", "jax", ".", "random", ".", "randint", "(", "rng", ",", "(", ")", ",", "0", ",", "self", ".", "max_noops", ")", "\n", "for", "i", "in", "range", "(", "num_noops", ")", ":", "\n", "                    ", "initial_observation", ",", "_", ",", "terminal", ",", "_", "=", "env", ".", "step", "(", "0", ")", "\n", "if", "terminal", ":", "\n", "                        ", "initial_observation", "=", "env", ".", "reset", "(", ")", "\n", "", "", "", "observations", ".", "append", "(", "initial_observation", ")", "\n", "", "initial_observation", "=", "np", ".", "stack", "(", "observations", ",", "0", ")", "\n", "\n", "return", "initial_observation", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.DataEfficientAtariRunner._run_parallel": [[195, 305], ["list", "range", "eval_run_experiment.DataEfficientAtariRunner._initialize_episode", "numpy.zeros", "eval_run_experiment.DataEfficientAtariRunner._agent.reset_all", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros.fill", "len", "eval_run_experiment.DataEfficientAtariRunner._agent.step", "eval_run_experiment.DataEfficientAtariRunner._agent.log_transition", "len", "len", "envs[].step", "numpy.clip", "len", "len", "len", "len", "len", "cum_rewards.append", "cum_lengths.append", "eval_run_experiment.normalize_score", "print", "print", "eval_run_experiment.DataEfficientAtariRunner._maybe_save_single_summary", "len", "eval_run_experiment.delete_ind_from_array", "eval_run_experiment.delete_ind_from_array", "eval_run_experiment.delete_ind_from_array", "eval_run_experiment.delete_ind_from_array", "eval_run_experiment.DataEfficientAtariRunner._agent.delete_one", "eval_run_experiment.DataEfficientAtariRunner._initialize_episode", "eval_run_experiment.DataEfficientAtariRunner._agent.reset_one", "eval_run_experiment.DataEfficientAtariRunner._agent.reset_one", "numpy.round", "len"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.DataEfficientAtariRunner._initialize_episode", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent.reset_all", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.step", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent.log_transition", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.step", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.normalize_score", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.DataEfficientAtariRunner._maybe_save_single_summary", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.delete_ind_from_array", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.delete_ind_from_array", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.delete_ind_from_array", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.delete_ind_from_array", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent.delete_one", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.DataEfficientAtariRunner._initialize_episode", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent.reset_one", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent.reset_one"], ["", "def", "_run_parallel", "(", "self", ",", "envs", ",", "episodes", "=", "None", ",", "max_steps", "=", "None", ",", "\n", "one_to_one", "=", "False", ",", "needs_reset", "=", "True", ",", "resume_state", "=", "None", ")", ":", "\n", "        ", "\"\"\"Executes a full trajectory of the agent interacting with the environment.\n\n        Returns:\n          The number of steps taken and the total reward.\n        \"\"\"", "\n", "# You can't ask for 200 episodes run one-to-one on 100 envs", "\n", "if", "one_to_one", ":", "\n", "            ", "assert", "episodes", "is", "None", "or", "episodes", "==", "len", "(", "envs", ")", "\n", "\n", "# Create envs", "\n", "", "live_envs", "=", "list", "(", "range", "(", "len", "(", "envs", ")", ")", ")", "\n", "\n", "if", "needs_reset", ":", "\n", "            ", "new_obs", "=", "self", ".", "_initialize_episode", "(", "envs", ")", "\n", "new_obses", "=", "np", ".", "zeros", "(", "(", "2", ",", "len", "(", "envs", ")", ",", "*", "self", ".", "_agent", ".", "observation_shape", ",", "1", ")", ")", "\n", "self", ".", "_agent", ".", "reset_all", "(", "new_obs", ")", "\n", "\n", "rewards", "=", "np", ".", "zeros", "(", "(", "len", "(", "envs", ")", ",", ")", ")", "\n", "terminals", "=", "np", ".", "zeros", "(", "(", "len", "(", "envs", ")", ",", ")", ")", "\n", "episode_end", "=", "np", ".", "zeros", "(", "(", "len", "(", "envs", ")", ",", ")", ")", "\n", "\n", "cum_rewards", "=", "[", "]", "\n", "cum_lengths", "=", "[", "]", "\n", "", "else", ":", "\n", "            ", "assert", "resume_state", "is", "not", "None", "\n", "new_obses", ",", "rewards", ",", "terminals", ",", "episode_end", ",", "cum_rewards", ",", "cum_lengths", "=", "resume_state", "\n", "\n", "", "total_steps", "=", "0", "\n", "total_episodes", "=", "0", "\n", "max_steps", "=", "np", ".", "inf", "if", "max_steps", "is", "None", "else", "max_steps", "\n", "step", "=", "0", "\n", "\n", "# Keep interacting until we reach a terminal state.", "\n", "while", "True", ":", "\n", "            ", "b", "=", "0", "\n", "step", "+=", "1", "\n", "episode_end", ".", "fill", "(", "0", ")", "\n", "total_steps", "+=", "len", "(", "live_envs", ")", "\n", "actions", "=", "self", ".", "_agent", ".", "step", "(", ")", "\n", "\n", "# The agent may be hanging on to the previous new_obs, so we don't", "\n", "# want to change it yet.", "\n", "# By alternating, we can make sure we don't end up logging", "\n", "# with an offset.", "\n", "new_obs", "=", "new_obses", "[", "step", "%", "2", "]", "\n", "\n", "# don't want to do a for-loop since live envs may change", "\n", "while", "b", "<", "len", "(", "live_envs", ")", ":", "\n", "                ", "env_id", "=", "live_envs", "[", "b", "]", "\n", "obs", ",", "reward", ",", "d", ",", "env_info", "=", "envs", "[", "env_id", "]", ".", "step", "(", "actions", "[", "b", "]", ")", "\n", "envs", "[", "env_id", "]", ".", "cum_length", "+=", "1", "\n", "envs", "[", "env_id", "]", ".", "cum_reward", "+=", "reward", "\n", "new_obs", "[", "b", "]", "=", "obs", "\n", "rewards", "[", "b", "]", "=", "reward", "\n", "terminals", "[", "b", "]", "=", "d", "\n", "\n", "if", "(", "envs", "[", "env_id", "]", ".", "game_over", "or", "envs", "[", "env_id", "]", ".", "cum_length", "==", "\n", "self", ".", "_max_steps_per_episode", ")", ":", "\n", "                    ", "total_episodes", "+=", "1", "\n", "cum_rewards", ".", "append", "(", "envs", "[", "env_id", "]", ".", "cum_reward", ")", "\n", "cum_lengths", ".", "append", "(", "envs", "[", "env_id", "]", ".", "cum_length", ")", "\n", "envs", "[", "env_id", "]", ".", "cum_length", "=", "0", "\n", "envs", "[", "env_id", "]", ".", "cum_reward", "=", "0", "\n", "\n", "human_norm_ret", "=", "normalize_score", "(", "cum_rewards", "[", "-", "1", "]", ",", "self", ".", "game", ")", "\n", "\n", "print", "(", ")", "\n", "print", "(", "'Steps executed: {} '", ".", "format", "(", "total_steps", ")", "+", "\n", "'Num episodes: {} '", ".", "format", "(", "len", "(", "cum_rewards", ")", ")", "+", "\n", "'Episode length: {} '", ".", "format", "(", "cum_lengths", "[", "-", "1", "]", ")", "+", "\n", "'Return: {} '", ".", "format", "(", "cum_rewards", "[", "-", "1", "]", ")", "+", "\n", "'Normalized Return: {}'", ".", "format", "(", "np", ".", "round", "(", "human_norm_ret", ",", "3", ")", ")", ")", "\n", "self", ".", "_maybe_save_single_summary", "(", "self", ".", "num_steps", "+", "total_steps", ",", "\n", "cum_rewards", "[", "-", "1", "]", ",", "\n", "cum_lengths", "[", "-", "1", "]", ")", "\n", "\n", "if", "one_to_one", ":", "\n", "                        ", "new_obses", "=", "delete_ind_from_array", "(", "new_obses", ",", "b", ",", "axis", "=", "1", ")", "\n", "new_obs", "=", "new_obses", "[", "step", "%", "2", "]", "\n", "actions", "=", "delete_ind_from_array", "(", "actions", ",", "b", ")", "\n", "rewards", "=", "delete_ind_from_array", "(", "rewards", ",", "b", ")", "\n", "terminals", "=", "delete_ind_from_array", "(", "terminals", ",", "b", ")", "\n", "self", ".", "_agent", ".", "delete_one", "(", "b", ")", "\n", "del", "live_envs", "[", "b", "]", "\n", "b", "-=", "1", "# live_envs[b] is now the next env, so go back one.", "\n", "", "else", ":", "\n", "                        ", "episode_end", "[", "b", "]", "=", "1", "\n", "new_obs", "[", "b", "]", "=", "self", ".", "_initialize_episode", "(", "[", "envs", "[", "env_id", "]", "]", ")", "\n", "self", ".", "_agent", ".", "reset_one", "(", "env_id", "=", "b", ")", "\n", "", "", "elif", "d", ":", "\n", "                    ", "self", ".", "_agent", ".", "reset_one", "(", "env_id", "=", "b", ")", "\n", "\n", "", "b", "+=", "1", "\n", "\n", "", "if", "self", ".", "_clip_rewards", ":", "\n", "# Perform reward clipping.", "\n", "                ", "rewards", "=", "np", ".", "clip", "(", "rewards", ",", "-", "1", ",", "1", ")", "\n", "\n", "", "self", ".", "_agent", ".", "log_transition", "(", "new_obs", ",", "actions", ",", "rewards", ",", "terminals", ",", "episode_end", ")", "\n", "\n", "if", "len", "(", "live_envs", ")", "==", "0", "or", "(", "max_steps", "is", "not", "None", "and", "total_steps", ">", "max_steps", ")", "or", "(", "episodes", "is", "not", "None", "and", "total_episodes", ">", "episodes", ")", ":", "\n", "                ", "break", "\n", "\n", "", "", "state", "=", "(", "new_obses", ",", "rewards", ",", "terminals", ",", "\n", "episode_end", ",", "cum_rewards", ",", "cum_lengths", ")", "\n", "return", "cum_lengths", ",", "cum_rewards", ",", "state", ",", "envs", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.DataEfficientAtariRunner._run_train_phase": [[306, 343], ["eval_run_experiment.DataEfficientAtariRunner._agent.restore_train_state", "time.time", "eval_run_experiment.DataEfficientAtariRunner._run_one_phase", "statistics.append", "eval_run_experiment.normalize_score", "statistics.append", "statistics.append", "absl.logging.info", "absl.logging.info", "absl.logging.info", "eval_run_experiment.DataEfficientAtariRunner._agent.cache_train_state", "time.time"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent.restore_train_state", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.DataEfficientAtariRunner._run_one_phase", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.normalize_score", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent.cache_train_state"], ["", "def", "_run_train_phase", "(", "self", ",", "statistics", ")", ":", "\n", "        ", "\"\"\"Run training phase.\n\n        Args:\n          statistics: `IterationStatistics` object which records the experimental\n            results. Note - This object is modified by this method.\n\n        Returns:\n          num_episodes: int, The number of episodes run in this phase.\n          average_reward: float, The average reward generated in this phase.\n          average_steps_per_second: float, The average number of steps per second.\n        \"\"\"", "\n", "# Perform the training phase, during which the agent learns.", "\n", "self", ".", "_agent", ".", "eval_mode", "=", "False", "\n", "self", ".", "_agent", ".", "restore_train_state", "(", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "number_steps", ",", "sum_returns", ",", "num_episodes", ",", "self", ".", "train_state", ",", "self", ".", "train_envs", "=", "self", ".", "_run_one_phase", "(", "\n", "self", ".", "train_envs", ",", "\n", "self", ".", "_training_steps", ",", "max_episodes", "=", "None", ",", "\n", "statistics", "=", "statistics", ",", "run_mode_str", "=", "'train'", ",", "\n", "needs_reset", "=", "self", ".", "train_state", "is", "None", ",", "resume_state", "=", "self", ".", "train_state", ")", "\n", "average_return", "=", "sum_returns", "/", "num_episodes", "if", "num_episodes", ">", "0", "else", "0.0", "\n", "statistics", ".", "append", "(", "{", "'train_average_return'", ":", "average_return", "}", ")", "\n", "human_norm_ret", "=", "normalize_score", "(", "average_return", ",", "self", ".", "game", ")", "\n", "statistics", ".", "append", "(", "{", "'train_average_normalized_score'", ":", "human_norm_ret", "}", ")", "\n", "time_delta", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "average_steps_per_second", "=", "number_steps", "/", "time_delta", "\n", "statistics", ".", "append", "(", "\n", "{", "'train_average_steps_per_second'", ":", "average_steps_per_second", "}", ")", "\n", "logging", ".", "info", "(", "'Average undiscounted return per training episode: %.2f'", ",", "\n", "average_return", ")", "\n", "logging", ".", "info", "(", "'Average normalized return per training episode: %.2f'", ",", "\n", "human_norm_ret", ")", "\n", "logging", ".", "info", "(", "'Average training steps per second: %.2f'", ",", "\n", "average_steps_per_second", ")", "\n", "self", ".", "_agent", ".", "cache_train_state", "(", ")", "\n", "return", "num_episodes", ",", "average_return", ",", "average_steps_per_second", ",", "human_norm_ret", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.DataEfficientAtariRunner._run_eval_phase": [[344, 375], ["eval_run_experiment.DataEfficientAtariRunner._run_one_phase", "absl.logging.info", "statistics.append", "eval_run_experiment.normalize_score", "statistics.append", "absl.logging.info", "eval_run_experiment.DataEfficientAtariRunner.create_environment_fn", "range"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.DataEfficientAtariRunner._run_one_phase", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.normalize_score"], ["", "def", "_run_eval_phase", "(", "self", ",", "statistics", ")", ":", "\n", "        ", "\"\"\"Run evaluation phase.\n\n        Args:\n            statistics: `IterationStatistics` object which records the experimental\n                results. Note - This object is modified by this method.\n\n        Returns:\n            num_episodes: int, The number of episodes run in this phase.\n            average_reward: float, The average reward generated in this phase.\n        \"\"\"", "\n", "# Perform the evaluation phase -- no learning.", "\n", "self", ".", "_agent", ".", "eval_mode", "=", "True", "\n", "eval_envs", "=", "[", "self", ".", "create_environment_fn", "(", ")", "for", "i", "in", "range", "(", "self", ".", "num_eval_envs", ")", "]", "\n", "_", ",", "sum_returns", ",", "num_episodes", ",", "_", ",", "_", "=", "self", ".", "_run_one_phase", "(", "\n", "eval_envs", ",", "steps", "=", "None", ",", "\n", "max_episodes", "=", "self", ".", "_num_eval_episodes", ",", "\n", "statistics", "=", "statistics", ",", "\n", "needs_reset", "=", "True", ",", "\n", "resume_state", "=", "None", ",", "\n", "one_to_one", "=", "self", ".", "eval_one_to_one", ",", "\n", "run_mode_str", "=", "'eval'", ")", "\n", "average_return", "=", "sum_returns", "/", "num_episodes", "if", "num_episodes", ">", "0", "else", "0.0", "\n", "logging", ".", "info", "(", "'Average undiscounted return per evaluation episode: %.2f'", ",", "\n", "average_return", ")", "\n", "statistics", ".", "append", "(", "{", "'eval_average_return'", ":", "average_return", "}", ")", "\n", "human_norm_return", "=", "normalize_score", "(", "average_return", ",", "self", ".", "game", ")", "\n", "statistics", ".", "append", "(", "{", "'train_average_normalized_score'", ":", "human_norm_return", "}", ")", "\n", "logging", ".", "info", "(", "'Average normalized return per evaluation episode: %.2f'", ",", "\n", "human_norm_return", ")", "\n", "return", "num_episodes", ",", "average_return", ",", "human_norm_return", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.DataEfficientAtariRunner._run_one_iteration": [[376, 391], ["dopamine.discrete_domains.iteration_statistics.IterationStatistics", "absl.logging.info", "eval_run_experiment.DataEfficientAtariRunner._run_train_phase", "eval_run_experiment.DataEfficientAtariRunner._run_eval_phase", "eval_run_experiment.DataEfficientAtariRunner._save_tensorboard_summaries"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.OfflineMaxEpisodeEvalRunner._run_train_phase", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.DataEfficientAtariRunner._run_eval_phase", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.OfflineMaxEpisodeEvalRunner._save_tensorboard_summaries"], ["", "def", "_run_one_iteration", "(", "self", ",", "iteration", ")", ":", "\n", "        ", "\"\"\"Runs one iteration of agent/environment interaction.\"\"\"", "\n", "statistics", "=", "iteration_statistics", ".", "IterationStatistics", "(", ")", "\n", "logging", ".", "info", "(", "'Starting iteration %d'", ",", "iteration", ")", "\n", "num_episodes_train", ",", "average_reward_train", ",", "average_steps_per_second", ",", "norm_score_train", "=", "self", ".", "_run_train_phase", "(", "statistics", ")", "\n", "num_episodes_eval", ",", "average_reward_eval", ",", "human_norm_eval", "=", "self", ".", "_run_eval_phase", "(", "statistics", ")", "\n", "self", ".", "_save_tensorboard_summaries", "(", "iteration", ",", "\n", "num_episodes_train", ",", "\n", "average_reward_train", ",", "\n", "norm_score_train", ",", "\n", "num_episodes_eval", ",", "\n", "average_reward_eval", ",", "\n", "human_norm_eval", ",", "\n", "average_steps_per_second", ",", ")", "\n", "return", "statistics", ".", "data_lists", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.DataEfficientAtariRunner._maybe_save_single_summary": [[392, 405], ["eval_run_experiment.normalize_score", "tensorflow.compat.v1.Summary", "eval_run_experiment.DataEfficientAtariRunner._summary_writer.add_summary", "tensorflow.compat.v1.Summary.Value", "tensorflow.compat.v1.Summary.Value", "tensorflow.compat.v1.Summary.Value"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.normalize_score"], ["", "def", "_maybe_save_single_summary", "(", "self", ",", "iteration", ",", "ep_return", ",", "length", ",", "save_if_eval", "=", "False", ")", ":", "\n", "        ", "prefix", "=", "\"Train/\"", "if", "not", "self", ".", "_agent", ".", "eval_mode", "else", "\"Eval/\"", "\n", "if", "not", "self", ".", "_agent", ".", "eval_mode", "or", "save_if_eval", ":", "\n", "            ", "normalized_score", "=", "normalize_score", "(", "ep_return", ",", "self", ".", "game", ")", "\n", "summary", "=", "tf", ".", "compat", ".", "v1", ".", "Summary", "(", "value", "=", "[", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "prefix", "+", "'EpisodeLength'", ",", "simple_value", "=", "length", ")", ",", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "prefix", "+", "'EpisodeReturn'", ",", "simple_value", "=", "ep_return", ")", ",", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "prefix", "+", "'EpisodeNormalizedScore'", ",", "simple_value", "=", "normalized_score", ")", ",", "\n", "]", ")", "\n", "self", ".", "_summary_writer", ".", "add_summary", "(", "summary", ",", "iteration", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.DataEfficientAtariRunner._save_tensorboard_summaries": [[406, 442], ["tensorflow.compat.v1.Summary", "eval_run_experiment.DataEfficientAtariRunner._summary_writer.add_summary", "tensorflow.compat.v1.Summary.Value", "tensorflow.compat.v1.Summary.Value", "tensorflow.compat.v1.Summary.Value", "tensorflow.compat.v1.Summary.Value", "tensorflow.compat.v1.Summary.Value", "tensorflow.compat.v1.Summary.Value", "tensorflow.compat.v1.Summary.Value"], "methods", ["None"], ["", "", "def", "_save_tensorboard_summaries", "(", "self", ",", "iteration", ",", "\n", "num_episodes_train", ",", "\n", "average_reward_train", ",", "\n", "norm_score_train", ",", "\n", "num_episodes_eval", ",", "\n", "average_reward_eval", ",", "\n", "norm_score_eval", ",", "\n", "average_steps_per_second", ")", ":", "\n", "        ", "\"\"\"Save statistics as tensorboard summaries.\n\n        Args:\n          iteration: int, The current iteration number.\n          num_episodes_train: int, number of training episodes run.\n          average_reward_train: float, The average training reward.\n          num_episodes_eval: int, number of evaluation episodes run.\n          average_reward_eval: float, The average evaluation reward.\n          average_steps_per_second: float, The average number of steps per second.\n        \"\"\"", "\n", "summary", "=", "tf", ".", "compat", ".", "v1", ".", "Summary", "(", "value", "=", "[", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "'Train/NumEpisodes'", ",", "simple_value", "=", "num_episodes_train", ")", ",", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "'Train/AverageReturns'", ",", "simple_value", "=", "average_reward_train", ")", ",", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "'Train/AverageNormalizedScore'", ",", "simple_value", "=", "norm_score_train", ")", ",", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "'Train/AverageStepsPerSecond'", ",", "\n", "simple_value", "=", "average_steps_per_second", ")", ",", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "'Eval/NumEpisodes'", ",", "simple_value", "=", "num_episodes_eval", ")", ",", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "'Eval/AverageReturns'", ",", "simple_value", "=", "average_reward_eval", ")", ",", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "'Eval/NormalizedScore'", ",", "simple_value", "=", "norm_score_eval", ")", "\n", "]", ")", "\n", "self", ".", "_summary_writer", ".", "add_summary", "(", "summary", ",", "iteration", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.DataEfficientAtariRunner.run_experiment": [[443, 456], ["absl.logging.info", "range", "eval_run_experiment.DataEfficientAtariRunner._summary_writer.flush", "absl.logging.warning", "eval_run_experiment.DataEfficientAtariRunner._run_one_iteration", "eval_run_experiment.DataEfficientAtariRunner._log_experiment", "eval_run_experiment.DataEfficientAtariRunner._checkpoint_experiment"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.OfflineMaxEpisodeEvalRunner._run_one_iteration"], ["", "def", "run_experiment", "(", "self", ")", ":", "\n", "        ", "\"\"\"Runs a full experiment, spread over multiple iterations.\"\"\"", "\n", "logging", ".", "info", "(", "'Beginning training...'", ")", "\n", "if", "self", ".", "_num_iterations", "<=", "self", ".", "_start_iteration", ":", "\n", "            ", "logging", ".", "warning", "(", "'num_iterations (%d) < start_iteration(%d)'", ",", "\n", "self", ".", "_num_iterations", ",", "self", ".", "_start_iteration", ")", "\n", "return", "\n", "\n", "", "for", "iteration", "in", "range", "(", "self", ".", "_start_iteration", ",", "self", ".", "_num_iterations", ")", ":", "\n", "            ", "statistics", "=", "self", ".", "_run_one_iteration", "(", "iteration", ")", "\n", "self", ".", "_log_experiment", "(", "iteration", ",", "statistics", ")", "\n", "self", ".", "_checkpoint_experiment", "(", "iteration", ")", "\n", "", "self", ".", "_summary_writer", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.LoggedDataEfficientAtariRunner.__init__": [[461, 471], ["eval_run_experiment.DataEfficientAtariRunner.__init__", "absl.logging.info", "absl.logging.info"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.__init__"], ["def", "__init__", "(", "self", ",", "\n", "base_dir", ",", "\n", "create_agent_fn", ",", "\n", "load_replay_dir", "=", "None", ",", "\n", "save_replay", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "base_dir", ",", "create_agent_fn", ")", "\n", "self", ".", "_load_replay_dir", "=", "load_replay_dir", "\n", "self", ".", "_save_replay", "=", "save_replay", "\n", "logging", ".", "info", "(", "'Load fixed replay from directory: %s'", ",", "load_replay_dir", ")", "\n", "logging", ".", "info", "(", "'Save replay: %s'", ",", "save_replay", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.LoggedDataEfficientAtariRunner.run_experiment": [[472, 480], ["eval_run_experiment.DataEfficientAtariRunner.run_experiment", "eval_run_experiment.LoggedDataEfficientAtariRunner._agent.load_fixed_replay", "os.path.join", "eval_run_experiment.LoggedDataEfficientAtariRunner._agent.save_replay"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.LoggedDataEfficientAtariRunner.run_experiment"], ["", "def", "run_experiment", "(", "self", ")", ":", "\n", "        ", "\"\"\"Runs a full experiment, spread over multiple iterations.\"\"\"", "\n", "if", "self", ".", "_load_replay_dir", "is", "not", "None", ":", "\n", "            ", "self", ".", "_agent", ".", "load_fixed_replay", "(", "self", ".", "_load_replay_dir", ")", "\n", "", "super", "(", ")", ".", "run_experiment", "(", ")", "\n", "if", "self", ".", "_save_replay", ":", "\n", "            ", "save_replay_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_base_dir", ",", "'replay_logs'", ")", "\n", "self", ".", "_agent", ".", "save_replay", "(", "save_replay_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.OfflineMaxEpisodeEvalRunner._run_train_phase": [[486, 499], ["time.time", "range", "statistics.append", "absl.logging.info", "eval_run_experiment.OfflineMaxEpisodeEvalRunner._agent._train_step", "time.time"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.agents.rainbow_agent.JaxSPRAgent._train_step"], ["def", "_run_train_phase", "(", "self", ",", "statistics", ")", ":", "\n", "        ", "\"\"\"Run training phase with offline dataset.\"\"\"", "\n", "self", ".", "_agent", ".", "eval_mode", "=", "False", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "for", "_", "in", "range", "(", "self", ".", "_training_steps", ")", ":", "\n", "            ", "self", ".", "_agent", ".", "_train_step", "(", ")", "\n", "", "time_delta", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "average_steps_per_second", "=", "self", ".", "_training_steps", "/", "time_delta", "\n", "statistics", ".", "append", "(", "\n", "{", "'train_average_steps_per_second'", ":", "average_steps_per_second", "}", ")", "\n", "logging", ".", "info", "(", "'Average training steps per second: %.2f'", ",", "\n", "average_steps_per_second", ")", "\n", "return", "average_steps_per_second", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.OfflineMaxEpisodeEvalRunner._run_one_iteration": [[500, 511], ["dopamine.discrete_domains.iteration_statistics.IterationStatistics", "absl.logging.info", "eval_run_experiment.OfflineMaxEpisodeEvalRunner._run_train_phase", "eval_run_experiment.OfflineMaxEpisodeEvalRunner._run_eval_phase", "eval_run_experiment.OfflineMaxEpisodeEvalRunner._save_tensorboard_summaries"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.OfflineMaxEpisodeEvalRunner._run_train_phase", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.DataEfficientAtariRunner._run_eval_phase", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.OfflineMaxEpisodeEvalRunner._save_tensorboard_summaries"], ["", "def", "_run_one_iteration", "(", "self", ",", "iteration", ")", ":", "\n", "        ", "\"\"\"Runs one iteration of agent/environment interaction.\"\"\"", "\n", "statistics", "=", "iteration_statistics", ".", "IterationStatistics", "(", ")", "\n", "logging", ".", "info", "(", "'Starting iteration %d'", ",", "iteration", ")", "\n", "average_steps_per_second", "=", "self", ".", "_run_train_phase", "(", "statistics", ")", "\n", "num_episodes_eval", ",", "average_reward_eval", ",", "human_norm_eval", "=", "self", ".", "_run_eval_phase", "(", "statistics", ")", "\n", "self", ".", "_save_tensorboard_summaries", "(", "iteration", ",", "num_episodes_eval", ",", "\n", "average_reward_eval", ",", "\n", "human_norm_eval", ",", "\n", "average_steps_per_second", ")", "\n", "return", "statistics", ".", "data_lists", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.OfflineMaxEpisodeEvalRunner._save_tensorboard_summaries": [[512, 529], ["tensorflow.compat.v1.Summary", "eval_run_experiment.OfflineMaxEpisodeEvalRunner._summary_writer.add_summary", "tensorflow.compat.v1.Summary.Value", "tensorflow.compat.v1.Summary.Value", "tensorflow.compat.v1.Summary.Value", "tensorflow.compat.v1.Summary.Value"], "methods", ["None"], ["", "def", "_save_tensorboard_summaries", "(", "self", ",", "iteration", ",", "num_episodes_eval", ",", "\n", "average_reward_eval", ",", "\n", "human_norm_eval", ",", "\n", "average_steps_per_second", ")", ":", "\n", "        ", "\"\"\"Save statistics as tensorboard summaries.\"\"\"", "\n", "summary", "=", "tf", ".", "compat", ".", "v1", ".", "Summary", "(", "value", "=", "[", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "'Train/AverageStepsPerSecond'", ",", "\n", "simple_value", "=", "average_steps_per_second", ")", ",", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "'Eval/NumEpisodes'", ",", "simple_value", "=", "num_episodes_eval", ")", ",", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "'Eval/AverageReturns'", ",", "simple_value", "=", "average_reward_eval", ")", ",", "\n", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "'Eval/NormalizedScore'", ",", "simple_value", "=", "human_norm_eval", ")", "\n", "]", ")", "\n", "self", ".", "_summary_writer", ".", "add_summary", "(", "summary", ",", "iteration", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.normalize_score": [[64, 67], ["None"], "function", ["None"], ["def", "normalize_score", "(", "ret", ",", "game", ",", "by", "=", "atari_human_scores", ")", ":", "\n", "    ", "return", "(", "ret", "-", "atari_random_scores", "[", "game", "]", ")", "/", "(", "atari_human_scores", "[", "game", "]", "-", "atari_random_scores", "[", "game", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.create_env_wrapper": [[68, 75], ["create_env_fn"], "function", ["None"], ["", "def", "create_env_wrapper", "(", "create_env_fn", ")", ":", "\n", "    ", "def", "inner_create", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "env", "=", "create_env_fn", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "env", ".", "cum_length", "=", "0", "\n", "env", ".", "cum_reward", "=", "0", "\n", "return", "env", "\n", "", "return", "inner_create", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.delete_ind_from_array": [[531, 536], ["tuple", "tuple", "numpy.concatenate", "slice", "slice", "slice", "slice"], "function", ["None"], ["", "", "def", "delete_ind_from_array", "(", "array", ",", "ind", ",", "axis", "=", "0", ")", ":", "\n", "    ", "start", "=", "tuple", "(", "(", "[", "slice", "(", "None", ")", "]", "*", "axis", ")", "+", "[", "slice", "(", "0", ",", "ind", ")", "]", ")", "\n", "end", "=", "tuple", "(", "(", "[", "slice", "(", "None", ")", "]", "*", "axis", ")", "+", "[", "slice", "(", "ind", "+", "1", ",", "array", ".", "shape", "[", "axis", "]", "+", "1", ")", "]", ")", "\n", "tensor", "=", "np", ".", "concatenate", "(", "[", "array", "[", "start", "]", ",", "array", "[", "end", "]", "]", ",", "axis", ")", "\n", "return", "tensor", "\n", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.train.load_gin_configs": [[59, 71], ["gin.parse_config_files_and_bindings"], "function", ["None"], ["def", "load_gin_configs", "(", "gin_files", ",", "gin_bindings", ")", ":", "\n", "  ", "\"\"\"Loads gin configuration files.\n\n  Args:\n    gin_files: list, of paths to the gin configuration files for this\n      experiment.\n    gin_bindings: list, of gin parameter bindings to override the values in\n      the config files.\n  \"\"\"", "\n", "gin", ".", "parse_config_files_and_bindings", "(", "gin_files", ",", "\n", "bindings", "=", "gin_bindings", ",", "\n", "skip_unknown", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.train.init_wandb": [[72, 82], ["os.makedirs", "wandb.init", "config._CONFIG.items"], "function", ["None"], ["", "def", "init_wandb", "(", "base_dir", ",", "seed", ",", "tag", "=", "None", ",", "agent", "=", "None", ")", ":", "\n", "    ", "os", ".", "environ", "[", "'WANDB_MODE'", "]", "=", "'offline'", "\n", "os", ".", "makedirs", "(", "base_dir", ",", "exist_ok", "=", "True", ")", "\n", "import", "wandb", "\n", "from", "gin", "import", "config", "\n", "clean_cfg", "=", "{", "k", "[", "1", "]", ":", "v", "for", "k", ",", "v", "in", "config", ".", "_CONFIG", ".", "items", "(", ")", "}", "\n", "clean_cfg", "[", "\"seed\"", "]", "=", "seed", "\n", "clean_cfg", "[", "\"tag\"", "]", "=", "tag", "\n", "clean_cfg", "[", "\"agent\"", "]", "=", "agent", "\n", "wandb", ".", "init", "(", "config", "=", "clean_cfg", ",", "sync_tensorboard", "=", "True", ",", "dir", "=", "base_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.train.create_load_replay_dir": [[84, 101], ["xm_params.items", "param.endswith", "os.path.join", "param.endswith", "str", "str"], "function", ["None"], ["", "def", "create_load_replay_dir", "(", "xm_params", ")", ":", "\n", "    ", "\"\"\"Creates the directory for loading fixed replay data.\"\"\"", "\n", "problem_name", ",", "run_number", "=", "''", ",", "''", "\n", "for", "param", ",", "value", "in", "xm_params", ".", "items", "(", ")", ":", "\n", "        ", "if", "param", ".", "endswith", "(", "'game_name'", ")", ":", "\n", "            ", "problem_name", "=", "value", "\n", "", "elif", "param", ".", "endswith", "(", "'run_number'", ")", ":", "\n", "            ", "run_number", "=", "str", "(", "value", ")", "\n", "", "", "replay_dir", "=", "FLAGS", ".", "load_replay_dir", "\n", "if", "replay_dir", ":", "\n", "        ", "if", "FLAGS", ".", "load_replay_number", ":", "\n", "            ", "replay_number", "=", "str", "(", "FLAGS", ".", "load_replay_number", ")", "\n", "", "else", ":", "\n", "            ", "replay_number", "=", "run_number", "\n", "", "replay_dir", "=", "os", ".", "path", ".", "join", "(", "replay_dir", ",", "problem_name", ",", "replay_number", ",", "\n", "'replay_logs'", ")", "\n", "", "return", "replay_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.train.create_agent": [[103, 111], ["discrete_control.agents.rainbow_agent.JaxSPRAgent"], "function", ["None"], ["", "def", "create_agent", "(", "sess", ",", "environment", ",", "\n", "seed", ",", "\n", "summary_writer", "=", "None", ")", ":", "\n", "    ", "\"\"\"Helper function for creating agent.\"\"\"", "\n", "return", "rainbow_agent", ".", "JaxSPRAgent", "(", "\n", "num_actions", "=", "environment", ".", "action_space", ".", "n", ",", "\n", "seed", "=", "seed", ",", "\n", "summary_writer", "=", "summary_writer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.train.set_random_seed": [[113, 119], ["absl.logging.info", "str", "tensorflow.random.set_seed", "numpy.random.seed"], "function", ["None"], ["", "def", "set_random_seed", "(", "seed", ")", ":", "\n", "    ", "\"\"\"Set random seed for reproducibility.\"\"\"", "\n", "logging", ".", "info", "(", "'Setting random seed: %d'", ",", "seed", ")", "\n", "os", ".", "environ", "[", "'PYTHONHASHSEED'", "]", "=", "str", "(", "seed", ")", "\n", "tf", ".", "random", ".", "set_seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.train.main": [[121, 150], ["absl.logging.set_verbosity", "tensorflow.compat.v1.disable_v2_behavior", "train.set_random_seed", "dopamine.discrete_domains.run_experiment.load_gin_configs", "functools.partial", "run_experiment.Runner.run_experiment", "train.init_wandb", "absl.logging.info", "runner_fn", "dopamine.discrete_domains.run_experiment.Runner"], "function", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.train.set_random_seed", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.train.load_gin_configs", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.eval_run_experiment.LoggedDataEfficientAtariRunner.run_experiment", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.discrete_control.train.init_wandb"], ["", "def", "main", "(", "unused_argv", ")", ":", "\n", "    ", "\"\"\"Main method.\n\n    Args:\n        unused_argv: Arguments (unused).\n    \"\"\"", "\n", "logging", ".", "set_verbosity", "(", "logging", ".", "INFO", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "disable_v2_behavior", "(", ")", "\n", "\n", "base_dir", "=", "FLAGS", ".", "base_dir", "\n", "gin_files", "=", "FLAGS", ".", "gin_files", "\n", "gin_bindings", "=", "FLAGS", ".", "gin_bindings", "\n", "# Add code for setting random seed using the run_number", "\n", "set_random_seed", "(", "FLAGS", ".", "run_number", ")", "\n", "run_experiment", ".", "load_gin_configs", "(", "gin_files", ",", "gin_bindings", ")", "\n", "if", "FLAGS", ".", "wandb", ":", "\n", "        ", "init_wandb", "(", "base_dir", ",", "FLAGS", ".", "run_number", ",", "FLAGS", ".", "tag", ",", "FLAGS", ".", "agent", ")", "\n", "# Set the Jax agent seed", "\n", "", "seed", "=", "FLAGS", ".", "run_number", "if", "not", "FLAGS", ".", "agent_seed", "else", "FLAGS", ".", "agent_seed", "\n", "create_agent_fn", "=", "functools", ".", "partial", "(", "\n", "create_agent", ",", "seed", "=", "seed", ")", "\n", "if", "FLAGS", ".", "max_episode_eval", ":", "\n", "        ", "runner_fn", "=", "eval_run_experiment", ".", "DataEfficientAtariRunner", "\n", "logging", ".", "info", "(", "'Using MaxEpisodeEvalRunner for evaluation.'", ")", "\n", "kwargs", "=", "{", "}", "# No additional flags should be passed.", "\n", "runner", "=", "runner_fn", "(", "base_dir", ",", "create_agent_fn", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "        ", "runner", "=", "run_experiment", ".", "Runner", "(", "base_dir", ",", "create_agent_fn", ")", "\n", "", "runner", ".", "run_experiment", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.__init__": [[100, 197], ["isinstance", "absl.logging.info", "absl.logging.info", "absl.logging.info", "absl.logging.info", "absl.logging.info", "absl.logging.info", "absl.logging.info", "absl.logging.info", "absl.logging.info", "int", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._create_storage", "numpy.array", "numpy.zeros", "numpy.array", "set", "ValueError", "str", "str", "str", "math.pow", "range"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._create_storage", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.set"], ["def", "__init__", "(", "self", ",", "\n", "observation_shape", ",", "\n", "stack_size", ",", "\n", "replay_capacity", ",", "\n", "batch_size", ",", "\n", "subseq_len", ",", "\n", "n_envs", "=", "1", ",", "\n", "update_horizon", "=", "1", ",", "\n", "gamma", "=", "0.99", ",", "\n", "max_sample_attempts", "=", "1000", ",", "\n", "extra_storage_types", "=", "None", ",", "\n", "observation_dtype", "=", "np", ".", "uint8", ",", "\n", "terminal_dtype", "=", "np", ".", "uint8", ",", "\n", "action_shape", "=", "(", ")", ",", "\n", "action_dtype", "=", "np", ".", "int32", ",", "\n", "reward_shape", "=", "(", ")", ",", "\n", "reward_dtype", "=", "np", ".", "float32", ")", ":", "\n", "        ", "\"\"\"Initializes OutOfGraphReplayBuffer.\n        Args:\n            observation_shape: tuple of ints.\n            stack_size: int, number of frames to use in state stack.\n            replay_capacity: int, number of transitions to keep in memory.\n            batch_size: int.\n            update_horizon: int, length of update ('n' in n-step update).\n            gamma: int, the discount factor.\n            max_sample_attempts: int, the maximum number of attempts allowed to\n                get a sample.\n            extra_storage_types: list of ReplayElements defining the type of the extra\n                contents that will be stored and returned by sample_transition_batch.\n            observation_dtype: np.dtype, type of the observations. Defaults to\n                np.uint8 for Atari 2600.\n            terminal_dtype: np.dtype, type of the terminals. Defaults to np.uint8 for\n                Atari 2600.\n            action_shape: tuple of ints, the shape for the action vector. Empty tuple\n                means the action is a scalar.\n            action_dtype: np.dtype, type of elements in the action.\n            reward_shape: tuple of ints, the shape of the reward vector. Empty tuple\n                means the reward is a scalar.\n            reward_dtype: np.dtype, type of elements in the reward.\n        Raises:\n            ValueError: If replay_capacity is too small to hold at least one\n                transition.\n        \"\"\"", "\n", "assert", "isinstance", "(", "observation_shape", ",", "tuple", ")", "\n", "if", "replay_capacity", "<", "update_horizon", "+", "stack_size", ":", "\n", "            ", "raise", "ValueError", "(", "'There is not enough capacity to cover '", "\n", "'update_horizon and stack_size.'", ")", "\n", "\n", "", "logging", ".", "info", "(", "\n", "'Creating a %s replay memory with the following parameters:'", ",", "\n", "self", ".", "__class__", ".", "__name__", ")", "\n", "logging", ".", "info", "(", "'\\t observation_shape: %s'", ",", "str", "(", "observation_shape", ")", ")", "\n", "logging", ".", "info", "(", "'\\t observation_dtype: %s'", ",", "str", "(", "observation_dtype", ")", ")", "\n", "logging", ".", "info", "(", "'\\t terminal_dtype: %s'", ",", "str", "(", "terminal_dtype", ")", ")", "\n", "logging", ".", "info", "(", "'\\t stack_size: %d'", ",", "stack_size", ")", "\n", "logging", ".", "info", "(", "'\\t replay_capacity: %d'", ",", "replay_capacity", ")", "\n", "logging", ".", "info", "(", "'\\t batch_size: %d'", ",", "batch_size", ")", "\n", "logging", ".", "info", "(", "'\\t update_horizon: %d'", ",", "update_horizon", ")", "\n", "logging", ".", "info", "(", "'\\t gamma: %f'", ",", "gamma", ")", "\n", "\n", "self", ".", "_action_shape", "=", "action_shape", "\n", "self", ".", "_action_dtype", "=", "action_dtype", "\n", "self", ".", "_reward_shape", "=", "reward_shape", "\n", "self", ".", "_reward_dtype", "=", "reward_dtype", "\n", "self", ".", "_observation_shape", "=", "observation_shape", "\n", "self", ".", "_stack_size", "=", "stack_size", "\n", "self", ".", "_state_shape", "=", "self", ".", "_observation_shape", "+", "(", "self", ".", "_stack_size", ",", ")", "\n", "self", ".", "_batch_size", "=", "batch_size", "\n", "self", ".", "_update_horizon", "=", "update_horizon", "\n", "self", ".", "_gamma", "=", "gamma", "\n", "self", ".", "_observation_dtype", "=", "observation_dtype", "\n", "self", ".", "_terminal_dtype", "=", "terminal_dtype", "\n", "self", ".", "_max_sample_attempts", "=", "max_sample_attempts", "\n", "self", ".", "_subseq_len", "=", "subseq_len", "\n", "\n", "self", ".", "n_envs", "=", "n_envs", "\n", "self", ".", "_replay_length", "=", "int", "(", "replay_capacity", "//", "self", ".", "n_envs", ")", "\n", "\n", "# Gotta round this down, since the matrix is rectangular.", "\n", "self", ".", "_replay_capacity", "=", "self", ".", "_replay_length", "*", "self", ".", "n_envs", "\n", "\n", "self", ".", "total_steps", "=", "0", "\n", "\n", "if", "extra_storage_types", ":", "\n", "            ", "self", ".", "_extra_storage_types", "=", "extra_storage_types", "\n", "", "else", ":", "\n", "            ", "self", ".", "_extra_storage_types", "=", "[", "]", "\n", "", "self", ".", "_create_storage", "(", ")", "\n", "self", ".", "add_count", "=", "np", ".", "array", "(", "0", ")", "\n", "self", ".", "invalid_range", "=", "np", ".", "zeros", "(", "(", "self", ".", "_stack_size", ")", ")", "\n", "# When the horizon is > 1, we compute the sum of discounted rewards as a dot", "\n", "# product using the precomputed vector <gamma^0, gamma^1, ..., gamma^{n-1}>.", "\n", "self", ".", "_cumulative_discount_vector", "=", "np", ".", "array", "(", "\n", "[", "math", ".", "pow", "(", "self", ".", "_gamma", ",", "n", ")", "for", "n", "in", "range", "(", "update_horizon", ")", "]", ",", "\n", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "_next_experience_is_episode_start", "=", "True", "\n", "self", ".", "_episode_end_indices", "=", "set", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._create_storage": [[198, 206], ["batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.get_storage_signature", "numpy.empty", "list"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.get_storage_signature"], ["", "def", "_create_storage", "(", "self", ")", ":", "\n", "        ", "\"\"\"Creates the numpy arrays used to store transitions.\n        \"\"\"", "\n", "self", ".", "_store", "=", "{", "}", "\n", "for", "storage_element", "in", "self", ".", "get_storage_signature", "(", ")", ":", "\n", "            ", "array_shape", "=", "[", "self", ".", "_replay_length", ",", "self", ".", "n_envs", "]", "+", "list", "(", "storage_element", ".", "shape", ")", "\n", "self", ".", "_store", "[", "storage_element", ".", "name", "]", "=", "np", ".", "empty", "(", "\n", "array_shape", ",", "dtype", "=", "storage_element", ".", "type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.get_add_args_signature": [[207, 215], ["batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.get_storage_signature"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.get_storage_signature"], ["", "", "def", "get_add_args_signature", "(", "self", ")", ":", "\n", "        ", "\"\"\"The signature of the add function.\n        Note - Derived classes may return a different signature.\n        Returns:\n            list of ReplayElements defining the type of the argument signature needed\n                by the add function.\n        \"\"\"", "\n", "return", "self", ".", "get_storage_signature", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.get_storage_signature": [[216, 233], ["ReplayElement", "ReplayElement", "ReplayElement", "ReplayElement", "storage_elements.append"], "methods", ["None"], ["", "def", "get_storage_signature", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns a default list of elements to be stored in this replay memory.\n        Note - Derived classes may return a different signature.\n        Returns:\n            list of ReplayElements defining the type of the contents stored.\n        \"\"\"", "\n", "storage_elements", "=", "[", "\n", "ReplayElement", "(", "'observation'", ",", "self", ".", "_observation_shape", ",", "\n", "self", ".", "_observation_dtype", ")", ",", "\n", "ReplayElement", "(", "'action'", ",", "self", ".", "_action_shape", ",", "self", ".", "_action_dtype", ")", ",", "\n", "ReplayElement", "(", "'reward'", ",", "self", ".", "_reward_shape", ",", "self", ".", "_reward_dtype", ")", ",", "\n", "ReplayElement", "(", "'terminal'", ",", "(", ")", ",", "self", ".", "_terminal_dtype", ")", "\n", "]", "\n", "\n", "for", "extra_replay_element", "in", "self", ".", "_extra_storage_types", ":", "\n", "            ", "storage_elements", ".", "append", "(", "extra_replay_element", ")", "\n", "", "return", "storage_elements", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._add_zero_transition": [[234, 243], ["batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.get_add_args_signature", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._episode_end_indices.discard", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._add", "zero_transition.append", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.cursor", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.get_add_args_signature", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer._add", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.cursor"], ["", "def", "_add_zero_transition", "(", "self", ")", ":", "\n", "        ", "\"\"\"Adds a padding transition filled with zeros (Used in episode beginnings).\n        \"\"\"", "\n", "zero_transition", "=", "[", "]", "\n", "for", "element_type", "in", "self", ".", "get_add_args_signature", "(", ")", ":", "\n", "            ", "zero_transition", ".", "append", "(", "\n", "np", ".", "zeros", "(", "element_type", ".", "shape", ",", "dtype", "=", "element_type", ".", "type", ")", ")", "\n", "", "self", ".", "_episode_end_indices", ".", "discard", "(", "self", ".", "cursor", "(", ")", ")", "# If present", "\n", "self", ".", "_add", "(", "*", "zero_transition", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.add": [[244, 297], ["batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._check_add_types", "range", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._add", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._episode_end_indices.add", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._episode_end_indices.discard", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.cursor", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.cursor"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._check_add_types", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer._add", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.add", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.cursor", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.cursor"], ["", "def", "add", "(", "self", ",", "\n", "observation", ",", "\n", "action", ",", "\n", "reward", ",", "\n", "terminal", ",", "\n", "*", "args", ",", "\n", "priority", "=", "None", ",", "\n", "episode_end", "=", "False", ")", ":", "\n", "        ", "\"\"\"Adds a transition to the replay memory.\n        This function checks the types and handles the padding at the beginning of\n        an episode. Then it calls the _add function.\n        Since the next_observation in the transition will be the observation added\n        next there is no need to pass it.\n        If the replay memory is at capacity the oldest transition will be discarded.\n        Args:\n            observation: np.array with shape observation_shape.\n            action: int, the action in the transition.\n            reward: float, the reward received in the transition.\n            terminal: np.dtype, acts as a boolean indicating whether the transition\n                                was terminal (1) or not (0).\n            *args: extra contents with shapes and dtypes according to\n                extra_storage_types.\n            priority: float, unused in the circular replay buffer, but may be used\n                in child classes like PrioritizedReplayBuffer.\n            episode_end: bool, whether this experience is the last experience in\n                the episode. This is useful for tasks that terminate due to time-out,\n                but do not end on a terminal state. Overloading 'terminal' may not\n                be sufficient in this case, since 'terminal' is passed to the agent\n                for training. 'episode_end' allows the replay buffer to determine\n                episode boundaries without passing that information to the agent.\n        \"\"\"", "\n", "if", "priority", "is", "not", "None", ":", "\n", "            ", "args", "=", "args", "+", "(", "priority", ",", ")", "\n", "\n", "", "self", ".", "total_steps", "+=", "self", ".", "n_envs", "\n", "\n", "self", ".", "_check_add_types", "(", "observation", ",", "action", ",", "reward", ",", "terminal", ",", "*", "args", ")", "\n", "# if self._next_experience_is_episode_start:", "\n", "# for _ in range(self._stack_size - 1):", "\n", "#     # Child classes can rely on the padding transitions being filled with", "\n", "#     # zeros. This is useful when there is a priority argument.", "\n", "#     self._add_zero_transition()", "\n", "# self._next_experience_is_episode_start = False", "\n", "\n", "resets", "=", "episode_end", "+", "terminal", "\n", "for", "i", "in", "range", "(", "resets", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "if", "resets", "[", "i", "]", ":", "\n", "                ", "self", ".", "_episode_end_indices", ".", "add", "(", "(", "self", ".", "cursor", "(", ")", ",", "i", ")", ")", "\n", "# self._next_experience_is_episode_start = True", "\n", "", "else", ":", "\n", "                ", "self", ".", "_episode_end_indices", ".", "discard", "(", "(", "self", ".", "cursor", "(", ")", ",", "i", ")", ")", "# If present", "\n", "\n", "", "", "self", ".", "_add", "(", "observation", ",", "action", ",", "reward", ",", "terminal", ",", "*", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._add": [[298, 307], ["batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._check_args_length", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._add_transition", "enumerate", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.get_add_args_signature"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._check_args_length", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._add_transition", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.get_add_args_signature"], ["", "def", "_add", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "\"\"\"Internal add method to add to the storage arrays.\n        Args:\n            *args: All the elements in a transition.\n        \"\"\"", "\n", "self", ".", "_check_args_length", "(", "*", "args", ")", "\n", "transition", "=", "{", "e", ".", "name", ":", "args", "[", "idx", "]", "\n", "for", "idx", ",", "e", "in", "enumerate", "(", "self", ".", "get_add_args_signature", "(", ")", ")", "}", "\n", "self", ".", "_add_transition", "(", "transition", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._add_transition": [[308, 324], ["batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.cursor", "batched_buffer.invalid_range", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.cursor"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.cursor", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.invalid_range", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.cursor"], ["", "def", "_add_transition", "(", "self", ",", "transition", ")", ":", "\n", "        ", "\"\"\"Internal add method to add transition dictionary to storage arrays.\n        Args:\n            transition: The dictionary of names and values of the transition\n                        to add to the storage.\n                        Each tensor should have leading dim equal to the number\n                        of environments used by the buffer.\n        \"\"\"", "\n", "cursor", "=", "self", ".", "cursor", "(", ")", "\n", "for", "arg_name", "in", "transition", ":", "\n", "            ", "self", ".", "_store", "[", "arg_name", "]", "[", "cursor", "]", "=", "transition", "[", "arg_name", "]", "\n", "\n", "", "self", ".", "add_count", "+=", "1", "\n", "self", ".", "invalid_range", "=", "invalid_range", "(", "\n", "self", ".", "cursor", "(", ")", ",", "self", ".", "_replay_length", ",", "self", ".", "_stack_size", ",", "\n", "self", ".", "_update_horizon", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._check_args_length": [[325, 335], ["len", "len", "ValueError", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.get_add_args_signature", "len", "len", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.get_add_args_signature"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.get_add_args_signature", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.get_add_args_signature"], ["", "def", "_check_args_length", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "\"\"\"Check if args passed to the add method have the same length as storage.\n        Args:\n            *args: Args for elements used in storage.\n        Raises:\n            ValueError: If args have wrong length.\n        \"\"\"", "\n", "if", "len", "(", "args", ")", "!=", "len", "(", "self", ".", "get_add_args_signature", "(", ")", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'Add expects {} elements, received {}'", ".", "format", "(", "\n", "len", "(", "self", ".", "get_add_args_signature", "(", ")", ")", ",", "len", "(", "args", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._check_add_types": [[336, 359], ["batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._check_args_length", "enumerate", "zip", "isinstance", "tuple", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.get_add_args_signature", "ValueError", "isinstance", "isinstance", "tuple", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._check_args_length", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.get_add_args_signature"], ["", "", "def", "_check_add_types", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "\"\"\"Checks if args passed to the add method match those of the storage.\n        Args:\n            *args: Args whose types need to be validated.\n        Raises:\n            ValueError: If args have wrong shape or dtype.\n        \"\"\"", "\n", "self", ".", "_check_args_length", "(", "*", "args", ")", "\n", "for", "i", ",", "(", "arg_element", ",", "store_element", ")", "in", "enumerate", "(", "zip", "(", "args", ",", "self", ".", "get_add_args_signature", "(", ")", ")", ")", ":", "\n", "            ", "if", "isinstance", "(", "arg_element", ",", "np", ".", "ndarray", ")", ":", "\n", "                ", "arg_shape", "=", "arg_element", ".", "shape", "\n", "", "elif", "isinstance", "(", "arg_element", ",", "tuple", ")", "or", "isinstance", "(", "arg_element", ",", "list", ")", ":", "\n", "# TODO(b/80536437). This is not efficient when arg_element is a list.", "\n", "                ", "arg_shape", "=", "np", ".", "array", "(", "arg_element", ")", ".", "shape", "\n", "", "else", ":", "\n", "# Assume it is scalar.", "\n", "                ", "arg_shape", "=", "tuple", "(", ")", "\n", "", "store_element_shape", "=", "tuple", "(", "store_element", ".", "shape", ")", "\n", "assert", "arg_shape", "[", "0", "]", "==", "self", ".", "n_envs", "\n", "arg_shape", "=", "arg_shape", "[", "1", ":", "]", "\n", "if", "arg_shape", "!=", "store_element_shape", ":", "\n", "                ", "raise", "ValueError", "(", "'arg {} has shape {}, expected {}'", ".", "format", "(", "i", ",", "\n", "arg_shape", ",", "store_element_shape", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.is_empty": [[360, 363], ["None"], "methods", ["None"], ["", "", "", "def", "is_empty", "(", "self", ")", ":", "\n", "        ", "\"\"\"Is the Replay Buffer empty?\"\"\"", "\n", "return", "self", ".", "add_count", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.is_full": [[364, 367], ["None"], "methods", ["None"], ["", "def", "is_full", "(", "self", ")", ":", "\n", "        ", "\"\"\"Is the Replay Buffer full?\"\"\"", "\n", "return", "self", ".", "add_count", ">=", "self", ".", "_replay_length", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.ravel_indices": [[368, 372], ["numpy.ravel_multi_index"], "methods", ["None"], ["", "def", "ravel_indices", "(", "self", ",", "indices_t", ",", "indices_b", ")", ":", "\n", "        ", "return", "np", ".", "ravel_multi_index", "(", "(", "indices_t", ",", "indices_b", ")", ",", "\n", "(", "self", ".", "_replay_length", ",", "self", ".", "n_envs", ")", ",", "\n", "mode", "=", "\"wrap\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.unravel_indices": [[373, 375], ["numpy.unravel_index"], "methods", ["None"], ["", "def", "unravel_indices", "(", "self", ",", "indices", ")", ":", "\n", "        ", "return", "np", ".", "unravel_index", "(", "indices", ",", "(", "self", ".", "_replay_length", ",", "self", ".", "n_envs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.get_from_store": [[376, 379], ["None"], "methods", ["None"], ["", "def", "get_from_store", "(", "self", ",", "element_name", ",", "indices_t", ",", "indices_b", ")", ":", "\n", "        ", "array", "=", "self", ".", "_store", "[", "element_name", "]", "\n", "return", "array", "[", "indices_t", ",", "indices_b", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.cursor": [[380, 383], ["None"], "methods", ["None"], ["", "def", "cursor", "(", "self", ")", ":", "\n", "        ", "\"\"\"Index to the location where the next transition will be written.\"\"\"", "\n", "return", "self", ".", "add_count", "%", "self", ".", "_replay_length", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.parallel_get_stack": [[384, 393], ["indices_b[].repeat", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.get_from_store", "mask.reshape.reshape.reshape", "numpy.moveaxis", "numpy.arange", "len"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.get_from_store"], ["", "def", "parallel_get_stack", "(", "self", ",", "element_name", ",", "indices_t", ",", "indices_b", ",", "first_valid", ")", ":", "\n", "        ", "indices_t", "=", "np", ".", "arange", "(", "-", "self", ".", "_stack_size", "+", "1", ",", "1", ")", "[", ":", ",", "None", "]", "+", "indices_t", "[", "None", ",", ":", "]", "\n", "indices_b", "=", "indices_b", "[", "None", ",", ":", "]", ".", "repeat", "(", "self", ".", "_stack_size", ",", "axis", "=", "0", ")", "\n", "mask", "=", "indices_t", ">=", "first_valid", "\n", "result", "=", "self", ".", "get_from_store", "(", "element_name", ",", "indices_t", "%", "self", ".", "_replay_length", ",", "indices_b", ")", "\n", "mask", "=", "mask", ".", "reshape", "(", "*", "mask", ".", "shape", ",", "*", "(", "[", "1", "]", "*", "(", "len", "(", "result", ".", "shape", ")", "-", "2", ")", ")", ")", "\n", "result", "=", "result", "*", "mask", "\n", "result", "=", "np", ".", "moveaxis", "(", "result", ",", "0", ",", "-", "1", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.get_terminal_stack": [[394, 396], ["batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.parallel_get_stack"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.parallel_get_stack"], ["", "def", "get_terminal_stack", "(", "self", ",", "index_t", ",", "index_b", ")", ":", "\n", "        ", "return", "self", ".", "parallel_get_stack", "(", "\"terminal\"", ",", "index_t", ",", "index_b", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.is_valid_transition": [[397, 437], ["terminals.any", "batched_buffer.modulo_range", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.is_full", "set", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.get_terminal_stack", "terminals.argmax", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.cursor", "i.item", "index_b.item"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.modulo_range", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.is_full", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.set", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.get_terminal_stack", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.cursor"], ["", "def", "is_valid_transition", "(", "self", ",", "index_t", ",", "index_b", ")", ":", "\n", "        ", "\"\"\"Checks if the index contains a valid transition.\n        Checks for collisions with the end of episodes and the current position\n        of the cursor.\n        Args:\n            index: int, the index to the state in the transition.\n        Returns:\n            Is the index valid: Boolean.\n            Start of the current episode (if within our stack size): Integer.\n        \"\"\"", "\n", "# Check the index is in the valid range", "\n", "if", "index_t", "<", "0", "or", "index_t", ">=", "self", ".", "_replay_length", ":", "\n", "            ", "return", "False", ",", "0", "\n", "", "if", "not", "self", ".", "is_full", "(", ")", ":", "\n", "# The indices and next_indices must be smaller than the cursor.", "\n", "            ", "if", "index_t", ">=", "self", ".", "cursor", "(", ")", "-", "self", ".", "_update_horizon", "-", "self", ".", "_subseq_len", ":", "\n", "                ", "return", "False", ",", "0", "\n", "# The first few indices contain the padding states of the first episode.", "\n", "", "if", "index_t", "<", "self", ".", "_stack_size", "-", "1", ":", "\n", "                ", "return", "False", ",", "0", "\n", "\n", "# Skip transitions that straddle the cursor.", "\n", "", "", "if", "index_t", "[", "0", "]", "in", "set", "(", "self", ".", "invalid_range", ")", ":", "\n", "            ", "return", "False", ",", "0", "\n", "\n", "# If there are terminal flags in any other frame other than the last one", "\n", "# the stack is not valid, so don't sample it.", "\n", "", "terminals", "=", "self", ".", "get_terminal_stack", "(", "index_t", ",", "index_b", ")", "[", "0", ",", ":", "-", "1", "]", "\n", "if", "terminals", ".", "any", "(", ")", ":", "\n", "            ", "ep_start", "=", "index_t", "-", "self", ".", "_stack_size", "+", "terminals", ".", "argmax", "(", ")", "+", "2", "\n", "", "else", ":", "\n", "            ", "ep_start", "=", "0", "\n", "\n", "# If the episode ends before the update horizon, without a terminal signal,", "\n", "# it is invalid.", "\n", "", "for", "i", "in", "modulo_range", "(", "index_t", ",", "self", ".", "_update_horizon", ",", "self", ".", "_replay_length", ")", ":", "\n", "            ", "if", "(", "i", ".", "item", "(", ")", ",", "index_b", ".", "item", "(", ")", ")", "in", "self", ".", "_episode_end_indices", "and", "not", "self", ".", "_store", "[", "'terminal'", "]", "[", "i", ",", "index_b", "]", ":", "\n", "                ", "return", "False", ",", "0", "\n", "\n", "", "", "return", "True", ",", "ep_start", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._create_batch_arrays": [[438, 454], ["batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.get_transition_elements", "tuple", "batch_arrays.append", "numpy.empty"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.get_transition_elements"], ["", "def", "_create_batch_arrays", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "\"\"\"Create a tuple of arrays with the type of get_transition_elements.\n        When using the WrappedReplayBuffer with staging enabled it is important to\n        create new arrays every sample because StaginArea keeps a pointer to the\n        returned arrays.\n        Args:\n            batch_size: (int) number of transitions returned. If None the default\n                batch_size will be used.\n        Returns:\n            Tuple of np.arrays with the shape and type of get_transition_elements.\n        \"\"\"", "\n", "transition_elements", "=", "self", ".", "get_transition_elements", "(", "batch_size", ")", "\n", "batch_arrays", "=", "[", "]", "\n", "for", "element", "in", "transition_elements", ":", "\n", "            ", "batch_arrays", ".", "append", "(", "np", ".", "empty", "(", "element", ".", "shape", ",", "dtype", "=", "element", ".", "type", ")", ")", "\n", "", "return", "tuple", "(", "batch_arrays", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.sample_index_batch": [[455, 509], ["jax.random.split", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.is_full", "jax.random.randint", "numpy.array", "numpy.zeros_like", "range", "jax.random.randint", "len", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.is_valid_transition", "RuntimeError", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.cursor", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.cursor", "RuntimeError", "jax.random.split", "jax.random.randint", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.is_valid_transition", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.cursor", "jax.random.randint"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.is_full", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.is_valid_transition", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.cursor", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.cursor", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.is_valid_transition", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.cursor"], ["", "def", "sample_index_batch", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "\"\"\"Returns a batch of valid indices sampled uniformly.\n\n        Args:\n            batch_size: int, number of indices returned.\n\n        Returns:\n            list of ints, a batch of valid indices sampled uniformly.\n\n        Raises:\n            RuntimeError: If the batch was not constructed after maximum number of\n                tries.\n        \"\"\"", "\n", "self", ".", "_rng", ",", "rng", "=", "jax", ".", "random", ".", "split", "(", "self", ".", "_rng", ")", "\n", "if", "self", ".", "is_full", "(", ")", ":", "\n", "# add_count >= self._replay_capacity > self._stack_size", "\n", "            ", "min_id", "=", "self", ".", "cursor", "(", ")", "-", "self", ".", "_replay_length", "+", "self", ".", "_stack_size", "-", "1", "\n", "max_id", "=", "self", ".", "cursor", "(", ")", "-", "self", ".", "_update_horizon", "-", "self", ".", "_subseq_len", "\n", "", "else", ":", "\n", "# add_count < self._replay_capacity", "\n", "            ", "min_id", "=", "self", ".", "_stack_size", "-", "1", "\n", "max_id", "=", "self", ".", "cursor", "(", ")", "-", "self", ".", "_update_horizon", "-", "self", ".", "_subseq_len", "\n", "if", "max_id", "<=", "min_id", ":", "\n", "                ", "raise", "RuntimeError", "(", "'Cannot sample a batch with fewer than stack size '", "\n", "'({}) + update_horizon ({}) transitions.'", ".", "\n", "format", "(", "self", ".", "_stack_size", ",", "self", ".", "_update_horizon", ")", ")", "\n", "", "", "t_indices", "=", "jax", ".", "random", ".", "randint", "(", "rng", ",", "(", "batch_size", ",", ")", ",", "min_id", ",", "max_id", ")", "%", "self", ".", "_replay_length", "\n", "b_indices", "=", "jax", ".", "random", ".", "randint", "(", "rng", ",", "(", "batch_size", ",", ")", ",", "0", ",", "self", ".", "n_envs", ")", "\n", "allowed_attempts", "=", "self", ".", "_max_sample_attempts", "\n", "t_indices", "=", "np", ".", "array", "(", "t_indices", ")", "\n", "censor_before", "=", "np", ".", "zeros_like", "(", "t_indices", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "t_indices", ")", ")", ":", "\n", "            ", "is_valid", ",", "ep_start", "=", "self", ".", "is_valid_transition", "(", "t_indices", "[", "i", ":", "i", "+", "1", "]", ",", "b_indices", "[", "i", ":", "i", "+", "1", "]", ")", "\n", "censor_before", "[", "i", "]", "=", "ep_start", "\n", "if", "not", "is_valid", ":", "\n", "                ", "if", "allowed_attempts", "==", "0", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\n", "'Max sample attempts: Tried {} times but only sampled {}'", "\n", "' valid indices. Batch size is {}'", ".", "\n", "format", "(", "self", ".", "_max_sample_attempts", ",", "i", ",", "batch_size", ")", ")", "\n", "", "while", "not", "is_valid", "and", "allowed_attempts", ">", "0", ":", "\n", "# If index i is not valid keep sampling others. Note that this", "\n", "# is not stratified.", "\n", "                    ", "self", ".", "_rng", ",", "rng", "=", "jax", ".", "random", ".", "split", "(", "self", ".", "_rng", ")", "\n", "t_index", "=", "jax", ".", "random", ".", "randint", "(", "rng", ",", "(", "1", ",", ")", ",", "min_id", ",", "max_id", ")", "%", "self", ".", "_replay_length", "\n", "b_index", "=", "jax", ".", "random", ".", "randint", "(", "rng", ",", "(", "1", ",", ")", ",", "0", ",", "self", ".", "n_envs", ")", "\n", "allowed_attempts", "-=", "1", "\n", "t_indices", "[", "i", "]", "=", "t_index", "\n", "b_indices", "[", "i", "]", "=", "b_index", "\n", "is_valid", ",", "first_valid", "=", "self", ".", "is_valid_transition", "(", "t_indices", "[", "i", ":", "i", "+", "1", "]", ",", "\n", "b_indices", "[", "i", ":", "i", "+", "1", "]", ")", "\n", "censor_before", "[", "i", "]", "=", "ep_start", "\n", "", "", "", "return", "t_indices", ",", "b_indices", ",", "censor_before", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.restore_leading_dims": [[510, 512], ["tensor.reshape"], "methods", ["None"], ["", "def", "restore_leading_dims", "(", "self", ",", "batch_size", ",", "jumps", ",", "tensor", ")", ":", "\n", "        ", "return", "tensor", ".", "reshape", "(", "batch_size", ",", "jumps", ",", "*", "tensor", ".", "shape", "[", "1", ":", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.sample": [[513, 515], ["batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.sample_transition_batch"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.sample_transition_batch"], ["", "def", "sample", "(", "self", ",", "rng", "=", "None", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "sample_transition_batch", "(", "rng", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.sample_transition_batch": [[516, 612], ["batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.get_transition_elements", "state_indices.reshape.reshape.reshape", "b_indices[].repeat().reshape", "censor_before[].repeat().reshape", "b_indices[].repeat", "trajectory_terminals.any", "numpy.sum", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.sample_index_batch", "len", "len", "outputs.append", "numpy.arange", "b_indices[].repeat", "censor_before[].repeat", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.parallel_get_stack", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.restore_leading_dims", "numpy.arange", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.restore_leading_dims", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.parallel_get_stack", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.restore_leading_dims", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.restore_leading_dims", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.restore_leading_dims", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.restore_leading_dims", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.ravel_indices().astype", "name.lstrip", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.restore_leading_dims", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._store.keys", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.restore_leading_dims", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.ravel_indices"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.get_transition_elements", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.sample_index_batch", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.parallel_get_stack", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.restore_leading_dims", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.restore_leading_dims", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.parallel_get_stack", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.restore_leading_dims", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.restore_leading_dims", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.restore_leading_dims", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.restore_leading_dims", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.restore_leading_dims", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.restore_leading_dims", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.ravel_indices"], ["", "def", "sample_transition_batch", "(", "self", ",", "rng", "=", "None", ",", "batch_size", "=", "None", ",", "indices", "=", "None", ",", "jumps", "=", "None", ")", ":", "\n", "        ", "\"\"\"Returns a batch of transitions (including any extra contents).\n        If get_transition_elements has been overridden and defines elements not\n        stored in self._store, an empty array will be returned and it will be\n        left to the child class to fill it. For example, for the child class\n        OutOfGraphPrioritizedReplayBuffer, the contents of the\n        sampling_probabilities are stored separately in a sum tree.\n        When the transition is terminal next_state_batch has undefined contents.\n        NOTE: This transition contains the indices of the sampled elements. These\n        are only valid during the call to sample_transition_batch, i.e. they may\n        be used by subclasses of this replay buffer but may point to different data\n        as soon as sampling is done.\n        Args:\n            batch_size: int, number of transitions returned. If None, the default\n                batch_size will be used.\n            indices: None or list of ints, the indices of every transition in the\n                batch. If None, sample the indices uniformly.\n        Returns:\n            transition_batch: tuple of np.arrays with the shape and type as in\n                get_transition_elements().\n        Raises:\n            ValueError: If an element to be sampled is missing from the replay buffer.\n        \"\"\"", "\n", "self", ".", "_rng", "=", "rng", "if", "rng", "is", "not", "None", "else", "self", ".", "_rng", "\n", "if", "batch_size", "is", "None", ":", "\n", "            ", "batch_size", "=", "self", ".", "_batch_size", "\n", "", "if", "jumps", "is", "None", ":", "\n", "            ", "jumps", "=", "self", ".", "_subseq_len", "\n", "", "if", "indices", "is", "None", ":", "\n", "            ", "t_indices", ",", "b_indices", ",", "censor_before", "=", "self", ".", "sample_index_batch", "(", "batch_size", ")", "\n", "", "assert", "len", "(", "t_indices", ")", "==", "batch_size", "\n", "assert", "len", "(", "b_indices", ")", "==", "batch_size", "\n", "transition_elements", "=", "self", ".", "get_transition_elements", "(", "batch_size", ")", "\n", "state_indices", "=", "t_indices", "[", ":", ",", "None", "]", "+", "np", ".", "arange", "(", "jumps", ")", "[", "None", ",", ":", "]", "\n", "state_indices", "=", "state_indices", ".", "reshape", "(", "batch_size", "*", "jumps", ")", "\n", "b_indices", "=", "b_indices", "[", ":", ",", "None", "]", ".", "repeat", "(", "jumps", ",", "axis", "=", "1", ")", ".", "reshape", "(", "batch_size", "*", "jumps", ")", "\n", "censor_before", "=", "censor_before", "[", ":", ",", "None", "]", ".", "repeat", "(", "jumps", ",", "axis", "=", "1", ")", ".", "reshape", "(", "batch_size", "*", "jumps", ")", "\n", "\n", "# shape: horizon X batch_size*jumps", "\n", "# Offset by one; a `d", "\n", "trajectory_indices", "=", "(", "np", ".", "arange", "(", "-", "1", ",", "self", ".", "_update_horizon", "-", "1", ")", "[", ":", ",", "None", "]", "+", "\n", "state_indices", "[", "None", ",", ":", "]", ")", "%", "self", ".", "_replay_length", "\n", "trajectory_b_indices", "=", "b_indices", "[", "None", ",", "]", ".", "repeat", "(", "self", ".", "_update_horizon", ",", "axis", "=", "0", ")", "\n", "trajectory_terminals", "=", "self", ".", "_store", "[", "\"terminal\"", "]", "[", "trajectory_indices", ",", "\n", "trajectory_b_indices", "]", "\n", "trajectory_terminals", "[", "0", ",", ":", "]", "=", "0", "\n", "is_terminal_transition", "=", "trajectory_terminals", ".", "any", "(", "0", ")", "\n", "valid_mask", "=", "(", "1", "-", "trajectory_terminals", ")", ".", "cumprod", "(", "0", ")", "\n", "trajectory_discount_vector", "=", "valid_mask", "*", "self", ".", "_cumulative_discount_vector", "[", ":", ",", "None", "]", "\n", "trajectory_rewards", "=", "self", ".", "_store", "[", "'reward'", "]", "[", "(", "trajectory_indices", "+", "1", ")", "%", "self", ".", "_replay_length", ",", "\n", "trajectory_b_indices", "]", "\n", "returns", "=", "np", ".", "sum", "(", "trajectory_discount_vector", "*", "trajectory_rewards", ",", "axis", "=", "0", ")", "\n", "\n", "next_indices", "=", "(", "state_indices", "+", "self", ".", "_update_horizon", ")", "%", "self", ".", "_replay_length", "\n", "outputs", "=", "[", "]", "\n", "for", "element", "in", "transition_elements", ":", "\n", "            ", "name", "=", "element", ".", "name", "\n", "if", "name", "==", "'state'", ":", "\n", "                ", "output", "=", "self", ".", "parallel_get_stack", "(", "\"observation\"", ",", "\n", "state_indices", ",", "\n", "b_indices", ",", "\n", "censor_before", ",", ")", "\n", "output", "=", "self", ".", "restore_leading_dims", "(", "batch_size", ",", "jumps", ",", "output", ")", "\n", "", "elif", "name", "==", "'reward'", ":", "\n", "# compute the discounted sum of rewards in the trajectory.", "\n", "                ", "output", "=", "returns", "\n", "output", "=", "self", ".", "restore_leading_dims", "(", "batch_size", ",", "jumps", ",", "output", ")", "\n", "", "elif", "name", "==", "'next_state'", ":", "\n", "                ", "output", "=", "self", ".", "parallel_get_stack", "(", "\"observation\"", ",", "\n", "next_indices", ",", "b_indices", ",", "\n", "censor_before", ",", ")", "\n", "output", "=", "self", ".", "restore_leading_dims", "(", "batch_size", ",", "jumps", ",", "output", ")", "\n", "# elif name == 'valid':", "\n", "#     output = np.array([self.is_valid_transition(i) for i in state_indices])", "\n", "#     output = self.restore_leading_dims(batch_size, jumps, output)", "\n", "", "elif", "name", "==", "\"same_trajectory\"", ":", "\n", "                ", "output", "=", "self", ".", "_store", "[", "\"terminal\"", "]", "[", "state_indices", ",", "b_indices", "]", "\n", "output", "=", "self", ".", "restore_leading_dims", "(", "batch_size", ",", "jumps", ",", "output", ")", "\n", "output", "[", "0", ",", ":", "]", "=", "0", "\n", "output", "=", "(", "1", "-", "output", ")", ".", "cumprod", "(", "1", ")", "\n", "", "elif", "name", "in", "(", "'next_action'", ",", "'next_reward'", ")", ":", "\n", "                ", "output", "=", "self", ".", "_store", "[", "name", ".", "lstrip", "(", "'next_'", ")", "]", "[", "next_indices", ",", "b_indices", "]", "\n", "output", "=", "self", ".", "restore_leading_dims", "(", "batch_size", ",", "jumps", ",", "output", ")", "\n", "", "elif", "element", ".", "name", "==", "'terminal'", ":", "\n", "                ", "output", "=", "is_terminal_transition", "\n", "output", "=", "self", ".", "restore_leading_dims", "(", "batch_size", ",", "jumps", ",", "output", ")", "\n", "", "elif", "name", "==", "'indices'", ":", "\n", "                ", "output", "=", "self", ".", "ravel_indices", "(", "state_indices", ",", "b_indices", ")", ".", "astype", "(", "\"int32\"", ")", "\n", "output", "=", "self", ".", "restore_leading_dims", "(", "batch_size", ",", "jumps", ",", "output", ")", "[", ":", ",", "0", "]", "\n", "", "elif", "name", "in", "self", ".", "_store", ".", "keys", "(", ")", ":", "\n", "                ", "output", "=", "self", ".", "_store", "[", "name", "]", "[", "state_indices", ",", "b_indices", "]", "\n", "output", "=", "self", ".", "restore_leading_dims", "(", "batch_size", ",", "jumps", ",", "output", ")", "\n", "", "else", ":", "\n", "                ", "continue", "\n", "", "outputs", ".", "append", "(", "output", ")", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.sanitize_batch": [[613, 622], ["None"], "methods", ["None"], ["", "def", "sanitize_batch", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Sanitize a batch.  All entries in the subtrajectories that belong to\n        episodes other than the current one will be replaced with default values\n        (0 for reward and terminal, the closest valid observation for the\n        observations).\n\n        :return:\n        \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.get_transition_elements": [[623, 657], ["ReplayElement", "ReplayElement", "ReplayElement", "ReplayElement", "ReplayElement", "ReplayElement", "ReplayElement", "ReplayElement", "ReplayElement", "transition_elements.append", "ReplayElement", "tuple"], "methods", ["None"], ["", "def", "get_transition_elements", "(", "self", ",", "batch_size", "=", "None", ",", "jumps", "=", "None", ")", ":", "\n", "        ", "\"\"\"Returns a 'type signature' for sample_transition_batch.\n        Args:\n            batch_size: int, number of transitions returned. If None, the default\n                batch_size will be used.\n        Returns:\n            signature: A namedtuple describing the method's return type signature.\n        \"\"\"", "\n", "jumps", "=", "self", ".", "_subseq_len", "if", "jumps", "is", "None", "else", "jumps", "\n", "batch_size", "=", "self", ".", "_batch_size", "if", "batch_size", "is", "None", "else", "batch_size", "\n", "\n", "transition_elements", "=", "[", "\n", "ReplayElement", "(", "'state'", ",", "(", "batch_size", ",", "jumps", ")", "+", "self", ".", "_state_shape", ",", "\n", "self", ".", "_observation_dtype", ")", ",", "\n", "ReplayElement", "(", "'action'", ",", "(", "batch_size", ",", "jumps", ")", "+", "self", ".", "_action_shape", ",", "\n", "self", ".", "_action_dtype", ")", ",", "\n", "ReplayElement", "(", "'reward'", ",", "(", "batch_size", ",", "jumps", ")", "+", "self", ".", "_reward_shape", ",", "\n", "self", ".", "_reward_dtype", ")", ",", "\n", "ReplayElement", "(", "'next_state'", ",", "(", "batch_size", ",", "jumps", ")", "+", "self", ".", "_state_shape", ",", "\n", "self", ".", "_observation_dtype", ")", ",", "\n", "ReplayElement", "(", "'next_action'", ",", "(", "batch_size", ",", "jumps", ")", "+", "self", ".", "_action_shape", ",", "\n", "self", ".", "_action_dtype", ")", ",", "\n", "ReplayElement", "(", "'next_reward'", ",", "(", "batch_size", ",", "jumps", ")", "+", "self", ".", "_reward_shape", ",", "\n", "self", ".", "_reward_dtype", ")", ",", "\n", "ReplayElement", "(", "'terminal'", ",", "(", "batch_size", ",", "jumps", ")", ",", "self", ".", "_terminal_dtype", ")", ",", "\n", "ReplayElement", "(", "'same_trajectory'", ",", "(", "batch_size", ",", "jumps", ")", ",", "self", ".", "_terminal_dtype", ")", ",", "\n", "# ReplayElement('valid', (batch_size, jumps), self._terminal_dtype),", "\n", "ReplayElement", "(", "'indices'", ",", "(", "batch_size", ",", ")", ",", "np", ".", "int32", ")", "\n", "]", "\n", "for", "element", "in", "self", ".", "_extra_storage_types", ":", "\n", "            ", "transition_elements", ".", "append", "(", "\n", "ReplayElement", "(", "element", ".", "name", ",", "(", "batch_size", ",", "jumps", ")", "+", "tuple", "(", "element", ".", "shape", ")", ",", "\n", "element", ".", "type", ")", ")", "\n", "", "return", "transition_elements", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._generate_filename": [[658, 660], ["os.path.join"], "methods", ["None"], ["", "def", "_generate_filename", "(", "self", ",", "checkpoint_dir", ",", "name", ",", "suffix", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "join", "(", "checkpoint_dir", ",", "'{}_ckpt.{}.gz'", ".", "format", "(", "name", ",", "suffix", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._return_checkpointable_elements": [[661, 675], ["batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.__dict__.items", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._store.items", "member_name.startswith"], "methods", ["None"], ["", "def", "_return_checkpointable_elements", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the dict of elements of the class for checkpointing.\n        Returns:\n            checkpointable_elements: dict containing all non private (starting with\n            _) members + all the arrays inside self._store.\n        \"\"\"", "\n", "checkpointable_elements", "=", "{", "}", "\n", "for", "member_name", ",", "member", "in", "self", ".", "__dict__", ".", "items", "(", ")", ":", "\n", "            ", "if", "member_name", "==", "'_store'", ":", "\n", "                ", "for", "array_name", ",", "array", "in", "self", ".", "_store", ".", "items", "(", ")", ":", "\n", "                    ", "checkpointable_elements", "[", "STORE_FILENAME_PREFIX", "+", "array_name", "]", "=", "array", "\n", "", "", "elif", "not", "member_name", ".", "startswith", "(", "'_'", ")", ":", "\n", "                ", "checkpointable_elements", "[", "member_name", "]", "=", "member", "\n", "", "", "return", "checkpointable_elements", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.save": [[676, 717], ["batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._return_checkpointable_elements", "tensorflow.io.gfile.exists", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._generate_filename", "tensorflow.io.gfile.GFile", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._generate_filename", "gzip.GzipFile", "attr.startswith", "tensorflow.io.gfile.remove", "numpy.save", "isinstance", "numpy.save", "pickle.dump", "len"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._return_checkpointable_elements", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._generate_filename", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._generate_filename", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.save", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.save"], ["", "def", "save", "(", "self", ",", "checkpoint_dir", ",", "iteration_number", ")", ":", "\n", "        ", "\"\"\"Save the OutOfGraphReplayBuffer attributes into a file.\n        This method will save all the replay buffer's state in a single file.\n        Args:\n            checkpoint_dir: str, the directory where numpy checkpoint files should be\n                saved.\n            iteration_number: int, iteration_number to use as a suffix in naming\n                numpy checkpoint files.\n        \"\"\"", "\n", "if", "not", "tf", ".", "io", ".", "gfile", ".", "exists", "(", "checkpoint_dir", ")", ":", "\n", "            ", "return", "\n", "\n", "", "checkpointable_elements", "=", "self", ".", "_return_checkpointable_elements", "(", ")", "\n", "\n", "for", "attr", "in", "checkpointable_elements", ":", "\n", "            ", "filename", "=", "self", ".", "_generate_filename", "(", "checkpoint_dir", ",", "attr", ",", "iteration_number", ")", "\n", "with", "tf", ".", "io", ".", "gfile", ".", "GFile", "(", "filename", ",", "'wb'", ")", "as", "f", ":", "\n", "                ", "with", "gzip", ".", "GzipFile", "(", "fileobj", "=", "f", ")", "as", "outfile", ":", "\n", "# Checkpoint the np arrays in self._store with np.save instead of", "\n", "# pickling the dictionary is critical for file size and performance.", "\n", "# STORE_FILENAME_PREFIX indicates that the variable is contained in", "\n", "# self._store.", "\n", "                    ", "if", "attr", ".", "startswith", "(", "STORE_FILENAME_PREFIX", ")", ":", "\n", "                        ", "array_name", "=", "attr", "[", "len", "(", "STORE_FILENAME_PREFIX", ")", ":", "]", "\n", "np", ".", "save", "(", "outfile", ",", "self", ".", "_store", "[", "array_name", "]", ",", "allow_pickle", "=", "False", ")", "\n", "# Some numpy arrays might not be part of storage", "\n", "", "elif", "isinstance", "(", "self", ".", "__dict__", "[", "attr", "]", ",", "np", ".", "ndarray", ")", ":", "\n", "                        ", "np", ".", "save", "(", "outfile", ",", "self", ".", "__dict__", "[", "attr", "]", ",", "allow_pickle", "=", "False", ")", "\n", "", "else", ":", "\n", "                        ", "pickle", ".", "dump", "(", "self", ".", "__dict__", "[", "attr", "]", ",", "outfile", ")", "\n", "\n", "# After writing a checkpoint file, we garbage collect the checkpoint file", "\n", "# that is four versions old.", "\n", "", "", "", "stale_iteration_number", "=", "iteration_number", "-", "CHECKPOINT_DURATION", "\n", "if", "stale_iteration_number", ">=", "0", ":", "\n", "                ", "stale_filename", "=", "self", ".", "_generate_filename", "(", "checkpoint_dir", ",", "attr", ",", "\n", "stale_iteration_number", ")", "\n", "try", ":", "\n", "                    ", "tf", ".", "io", ".", "gfile", ".", "remove", "(", "stale_filename", ")", "\n", "", "except", "tf", ".", "errors", ".", "NotFoundError", ":", "\n", "                    ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.load": [[718, 748], ["batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._return_checkpointable_elements", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._generate_filename", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._generate_filename", "tensorflow.io.gfile.exists", "tensorflow.errors.NotFoundError", "tensorflow.io.gfile.GFile", "gzip.GzipFile", "attr.startswith", "numpy.load", "isinstance", "numpy.load", "pickle.load", "len"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._return_checkpointable_elements", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._generate_filename", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._generate_filename", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.load", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.load", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.load"], ["", "", "", "", "def", "load", "(", "self", ",", "checkpoint_dir", ",", "suffix", ")", ":", "\n", "        ", "\"\"\"Restores the object from bundle_dictionary and numpy checkpoints.\n        Args:\n            checkpoint_dir: str, the directory where to read the numpy checkpointed\n                files from.\n            suffix: str, the suffix to use in numpy checkpoint files.\n        Raises:\n            NotFoundError: If not all expected files are found in directory.\n        \"\"\"", "\n", "save_elements", "=", "self", ".", "_return_checkpointable_elements", "(", ")", "\n", "# We will first make sure we have all the necessary files available to avoid", "\n", "# loading a partially-specified (i.e. corrupted) replay buffer.", "\n", "for", "attr", "in", "save_elements", ":", "\n", "            ", "filename", "=", "self", ".", "_generate_filename", "(", "checkpoint_dir", ",", "attr", ",", "suffix", ")", "\n", "if", "not", "tf", ".", "io", ".", "gfile", ".", "exists", "(", "filename", ")", ":", "\n", "                ", "raise", "tf", ".", "errors", ".", "NotFoundError", "(", "None", ",", "None", ",", "\n", "'Missing file: {}'", ".", "format", "(", "filename", ")", ")", "\n", "# If we've reached this point then we have verified that all expected files", "\n", "# are available.", "\n", "", "", "for", "attr", "in", "save_elements", ":", "\n", "            ", "filename", "=", "self", ".", "_generate_filename", "(", "checkpoint_dir", ",", "attr", ",", "suffix", ")", "\n", "with", "tf", ".", "io", ".", "gfile", ".", "GFile", "(", "filename", ",", "'rb'", ")", "as", "f", ":", "\n", "                ", "with", "gzip", ".", "GzipFile", "(", "fileobj", "=", "f", ")", "as", "infile", ":", "\n", "                    ", "if", "attr", ".", "startswith", "(", "STORE_FILENAME_PREFIX", ")", ":", "\n", "                        ", "array_name", "=", "attr", "[", "len", "(", "STORE_FILENAME_PREFIX", ")", ":", "]", "\n", "self", ".", "_store", "[", "array_name", "]", "=", "np", ".", "load", "(", "infile", ",", "allow_pickle", "=", "False", ")", "\n", "", "elif", "isinstance", "(", "self", ".", "__dict__", "[", "attr", "]", ",", "np", ".", "ndarray", ")", ":", "\n", "                        ", "self", ".", "__dict__", "[", "attr", "]", "=", "np", ".", "load", "(", "infile", ",", "allow_pickle", "=", "False", ")", "\n", "", "else", ":", "\n", "                        ", "self", ".", "__dict__", "[", "attr", "]", "=", "pickle", ".", "load", "(", "infile", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.__init__": [[754, 790], ["batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.__init__", "deterministic_sum_tree.DeterministicSumTree"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.__init__"], ["def", "__init__", "(", "self", ",", "\n", "observation_shape", ",", "\n", "stack_size", ",", "\n", "replay_capacity", ",", "\n", "batch_size", ",", "\n", "update_horizon", "=", "1", ",", "\n", "subseq_len", "=", "0", ",", "\n", "n_envs", "=", "1", ",", "\n", "gamma", "=", "0.99", ",", "\n", "max_sample_attempts", "=", "1000", ",", "\n", "extra_storage_types", "=", "None", ",", "\n", "observation_dtype", "=", "np", ".", "uint8", ",", "\n", "terminal_dtype", "=", "np", ".", "uint8", ",", "\n", "action_shape", "=", "(", ")", ",", "\n", "action_dtype", "=", "np", ".", "int32", ",", "\n", "reward_shape", "=", "(", ")", ",", "\n", "reward_dtype", "=", "np", ".", "float32", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "observation_shape", "=", "observation_shape", ",", "\n", "stack_size", "=", "stack_size", ",", "\n", "replay_capacity", "=", "replay_capacity", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "update_horizon", "=", "update_horizon", ",", "\n", "gamma", "=", "gamma", ",", "\n", "max_sample_attempts", "=", "max_sample_attempts", ",", "\n", "extra_storage_types", "=", "extra_storage_types", ",", "\n", "observation_dtype", "=", "observation_dtype", ",", "\n", "terminal_dtype", "=", "terminal_dtype", ",", "\n", "subseq_len", "=", "subseq_len", ",", "\n", "n_envs", "=", "n_envs", ",", "\n", "action_shape", "=", "action_shape", ",", "\n", "action_dtype", "=", "action_dtype", ",", "\n", "reward_shape", "=", "reward_shape", ",", "\n", "reward_dtype", "=", "reward_dtype", ")", "\n", "\n", "self", ".", "sum_tree", "=", "sum_tree", ".", "DeterministicSumTree", "(", "replay_capacity", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.get_add_args_signature": [[791, 798], ["batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.get_add_args_signature", "ReplayElement"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.get_add_args_signature"], ["", "def", "get_add_args_signature", "(", "self", ")", ":", "\n", "        ", "\"\"\"The signature of the add function.\"\"\"", "\n", "parent_add_signature", "=", "super", "(", ")", ".", "get_add_args_signature", "(", ")", "\n", "add_signature", "=", "parent_add_signature", "+", "[", "\n", "ReplayElement", "(", "'priority'", ",", "(", ")", ",", "np", ".", "float32", ")", "\n", "]", "\n", "return", "add_signature", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer._add": [[799, 819], ["batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer._check_args_length", "enumerate", "numpy.ravel_multi_index", "batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._add_transition", "batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.get_add_args_signature", "batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.sum_tree.set", "numpy.arange", "range", "numpy.ones", "batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.cursor", "len"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._check_args_length", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer._add_transition", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.get_add_args_signature", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.set", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.cursor"], ["", "def", "_add", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "\"\"\"Internal add method to add to the underlying memory arrays.\"\"\"", "\n", "self", ".", "_check_args_length", "(", "*", "args", ")", "\n", "\n", "# Use Schaul et al.'s (2015) scheme of setting the priority of new elements", "\n", "# to the maximum priority so far.", "\n", "# Picks out 'priority' from arguments and adds it to the sum_tree.", "\n", "transition", "=", "{", "}", "\n", "for", "i", ",", "element", "in", "enumerate", "(", "self", ".", "get_add_args_signature", "(", ")", ")", ":", "\n", "            ", "if", "element", ".", "name", "==", "'priority'", ":", "\n", "                ", "priority", "=", "args", "[", "i", "]", "\n", "", "else", ":", "\n", "                ", "transition", "[", "element", ".", "name", "]", "=", "args", "[", "i", "]", "\n", "\n", "", "", "indices", "=", "np", ".", "ravel_multi_index", "(", "(", "np", ".", "ones", "(", "(", "1", ",", ")", ",", "dtype", "=", "\"int32\"", ")", "*", "self", ".", "cursor", "(", ")", ",", "\n", "np", ".", "arange", "(", "self", ".", "n_envs", ")", ")", ",", "\n", "(", "self", ".", "_replay_length", ",", "self", ".", "n_envs", ")", ")", "\n", "\n", "[", "self", ".", "sum_tree", ".", "set", "(", "indices", "[", "i", "]", ",", "priority", "[", "i", "]", ")", "for", "i", "in", "range", "(", "len", "(", "indices", ")", ")", "]", "\n", "super", "(", ")", ".", "_add_transition", "(", "transition", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.sample_index_batch": [[820, 853], ["batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.sum_tree.stratified_sample", "numpy.array", "batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.unravel_indices", "numpy.zeros_like", "range", "len", "batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.is_valid_transition", "RuntimeError", "jax.random.split", "int", "batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.unravel_indices", "batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.is_valid_transition", "batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.sum_tree.sample"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.stratified_sample", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.unravel_indices", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.is_valid_transition", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.unravel_indices", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.is_valid_transition", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.sample"], ["", "def", "sample_index_batch", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "\"\"\"Returns a batch of valid indices sampled as in Schaul et al. (2015).\"\"\"", "\n", "# Sample stratified indices. Some of them might be invalid.", "\n", "# start = time.time()", "\n", "indices", "=", "self", ".", "sum_tree", ".", "stratified_sample", "(", "batch_size", ",", "self", ".", "_rng", ")", "\n", "indices", "=", "np", ".", "array", "(", "indices", ")", "\n", "# print(\"Sampling from sum tree took {}\".format(time.time() - start))", "\n", "allowed_attempts", "=", "self", ".", "_max_sample_attempts", "\n", "\n", "t_indices", ",", "b_indices", "=", "self", ".", "unravel_indices", "(", "indices", ")", "\n", "censor_before", "=", "np", ".", "zeros_like", "(", "t_indices", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "indices", ")", ")", ":", "\n", "            ", "is_valid", ",", "ep_start", "=", "self", ".", "is_valid_transition", "(", "t_indices", "[", "i", ":", "i", "+", "1", "]", ",", "b_indices", "[", "i", ":", "i", "+", "1", "]", ")", "\n", "censor_before", "[", "i", "]", "=", "ep_start", "\n", "if", "not", "is_valid", ":", "\n", "                ", "if", "allowed_attempts", "==", "0", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\n", "'Max sample attempts: Tried {} times but only sampled {}'", "\n", "' valid indices. Batch size is {}'", ".", "\n", "format", "(", "self", ".", "_max_sample_attempts", ",", "i", ",", "batch_size", ")", ")", "\n", "", "while", "(", "not", "is_valid", ")", "and", "allowed_attempts", ">", "0", ":", "\n", "# If index i is not valid keep sampling others. Note that this", "\n", "# is not stratified.", "\n", "                    ", "self", ".", "_rng", ",", "rng", "=", "jax", ".", "random", ".", "split", "(", "self", ".", "_rng", ")", "\n", "index", "=", "int", "(", "self", ".", "sum_tree", ".", "sample", "(", "rng", "=", "rng", ")", ")", "\n", "t_index", ",", "b_index", "=", "self", ".", "unravel_indices", "(", "index", ")", "\n", "allowed_attempts", "-=", "1", "\n", "t_indices", "[", "i", "]", "=", "t_index", "\n", "b_indices", "[", "i", "]", "=", "b_index", "\n", "is_valid", ",", "ep_start", "=", "self", ".", "is_valid_transition", "(", "t_indices", "[", "i", ":", "i", "+", "1", "]", ",", "\n", "b_indices", "[", "i", ":", "i", "+", "1", "]", ")", "\n", "censor_before", "[", "i", "]", "=", "ep_start", "\n", "", "", "", "return", "t_indices", ",", "b_indices", ",", "censor_before", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.sample_transition_batch": [[854, 859], ["batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.sample_transition_batch", "super().sample_transition_batch.append", "batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.get_priority"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.sample_transition_batch", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.get_priority"], ["", "def", "sample_transition_batch", "(", "self", ",", "rng", ",", "batch_size", "=", "None", ",", "indices", "=", "None", ")", ":", "\n", "        ", "\"\"\"Returns a batch of transitions with extra storage and the priorities.\"\"\"", "\n", "transition", "=", "super", "(", ")", ".", "sample_transition_batch", "(", "rng", ",", "batch_size", ",", "indices", ")", "\n", "transition", ".", "append", "(", "self", ".", "get_priority", "(", "transition", "[", "-", "1", "]", ")", ")", "\n", "return", "transition", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.set_priority": [[860, 866], ["zip", "batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.sum_tree.set"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.set"], ["", "def", "set_priority", "(", "self", ",", "indices", ",", "priorities", ")", ":", "\n", "        ", "\"\"\"Sets the priority of the given elements according to Schaul et al.\"\"\"", "\n", "assert", "indices", ".", "dtype", "==", "np", ".", "int32", ",", "(", "'Indices must be integers, '", "\n", "'given: {}'", ".", "format", "(", "indices", ".", "dtype", ")", ")", "\n", "for", "index", ",", "priority", "in", "zip", "(", "indices", ",", "priorities", ")", ":", "\n", "            ", "self", ".", "sum_tree", ".", "set", "(", "index", ",", "priority", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.get_priority": [[867, 874], ["batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.sum_tree.get"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.get"], ["", "", "def", "get_priority", "(", "self", ",", "indices", ")", ":", "\n", "        ", "\"\"\"Fetches the priorities correspond to a batch of memory indices.\"\"\"", "\n", "assert", "indices", ".", "shape", ",", "'Indices must be an array.'", "\n", "assert", "indices", ".", "dtype", "==", "np", ".", "int32", ",", "(", "'Indices must be int32s, '", "\n", "'given: {}'", ".", "format", "(", "indices", ".", "dtype", ")", ")", "\n", "priority_batch", "=", "self", ".", "sum_tree", ".", "get", "(", "indices", ")", "\n", "return", "priority_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.get_transition_elements": [[875, 883], ["batched_buffer.JaxSubsequenceParallelEnvReplayBuffer.get_transition_elements", "ReplayElement"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.PrioritizedJaxSubsequenceParallelEnvReplayBuffer.get_transition_elements"], ["", "def", "get_transition_elements", "(", "self", ",", "batch_size", "=", "None", ")", ":", "\n", "        ", "\"\"\"Returns a 'type signature' for sample_transition_batch.\"\"\"", "\n", "parent_transition_type", "=", "(", "\n", "super", "(", ")", ".", "get_transition_elements", "(", "batch_size", ")", ")", "\n", "probablilities_type", "=", "[", "\n", "ReplayElement", "(", "'sampling_probabilities'", ",", "(", "batch_size", ",", ")", ",", "np", ".", "float32", ")", "\n", "]", "\n", "return", "parent_transition_type", "+", "probablilities_type", "\n", "", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.modulo_range": [[56, 59], ["range"], "function", ["None"], ["def", "modulo_range", "(", "start", ",", "length", ",", "modulo", ")", ":", "\n", "    ", "for", "i", "in", "range", "(", "length", ")", ":", "\n", "        ", "yield", "(", "start", "+", "i", ")", "%", "modulo", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.batched_buffer.invalid_range": [[61, 83], ["numpy.array", "range"], "function", ["None"], ["", "", "def", "invalid_range", "(", "cursor", ",", "replay_capacity", ",", "stack_size", ",", "update_horizon", ")", ":", "\n", "    ", "\"\"\"Returns a array with the indices of cursor-related invalid transitions.\n    There are update_horizon + stack_size invalid indices:\n        - The update_horizon indices before the cursor, because we do not have a\n            valid N-step transition (including the next state).\n        - The stack_size indices on or immediately after the cursor.\n    If N = update_horizon, K = stack_size, and the cursor is at c, invalid\n    indices are:\n        c - N, c - N + 1, ..., c, c + 1, ..., c + K - 1.\n    It handles special cases in a circular buffer in the beginning and the end.\n    Args:\n        cursor: int, the position of the cursor.\n        replay_capacity: int, the size of the replay memory.\n        stack_size: int, the size of the stacks returned by the replay memory.\n        update_horizon: int, the agent's update horizon.\n    Returns:\n        np.array of size stack_size with the invalid indices.\n    \"\"\"", "\n", "assert", "cursor", "<", "replay_capacity", "\n", "return", "np", ".", "array", "(", "\n", "[", "(", "cursor", "-", "update_horizon", "+", "i", ")", "%", "replay_capacity", "\n", "for", "i", "in", "range", "(", "stack_size", "+", "update_horizon", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.__init__": [[64, 87], ["isinstance", "int", "jax.numpy.zeros", "ValueError", "jax.numpy.ceil", "jax.numpy.log2"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "capacity", ")", ":", "\n", "        ", "\"\"\"Creates the sum tree data structure for the given replay capacity.\n        Args:\n          capacity: int, the maximum number of elements that can be stored in this\n            data structure.\n        Raises:\n          ValueError: If requested capacity is not positive.\n        \"\"\"", "\n", "assert", "isinstance", "(", "capacity", ",", "int", ")", "\n", "if", "capacity", "<=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "'Sum tree capacity should be positive. Got: {}'", ".", "\n", "format", "(", "capacity", ")", ")", "\n", "\n", "", "self", ".", "nodes", "=", "[", "]", "\n", "self", ".", "depth", "=", "int", "(", "np", ".", "ceil", "(", "np", ".", "log2", "(", "capacity", ")", ")", ")", "\n", "self", ".", "low_idx", "=", "(", "2", "**", "self", ".", "depth", ")", "-", "1", "# pri_idx + low_idx -> tree_idx", "\n", "self", ".", "high_idx", "=", "capacity", "+", "self", ".", "low_idx", "\n", "self", ".", "nodes", "=", "np", ".", "zeros", "(", "2", "**", "(", "self", ".", "depth", "+", "1", ")", "-", "1", ")", "# Double precision.", "\n", "self", ".", "capacity", "=", "capacity", "\n", "\n", "self", ".", "highest_set", "=", "0", "\n", "\n", "self", ".", "max_recorded_priority", "=", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree._total_priority": [[88, 94], ["None"], "methods", ["None"], ["", "def", "_total_priority", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the sum of all priorities stored in this sum tree.\n        Returns:\n          float, sum of priorities stored in this sum tree.\n        \"\"\"", "\n", "return", "self", ".", "nodes", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.sample": [[95, 134], ["jax.numpy.array", "deterministic_sum_tree.DeterministicSumTree._total_priority", "jax.lax.fori_loop", "jax.numpy.minimum", "jax.random.uniform"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree._total_priority"], ["", "def", "sample", "(", "self", ",", "rng", ",", "query_value", "=", "None", ")", ":", "\n", "        ", "\"\"\"Samples an element from the sum tree.\"\"\"", "\n", "# start = time.time()", "\n", "# if self._total_priority() == 0.0:", "\n", "#     raise Exception('Cannot sample from an empty sum tree.')", "\n", "#", "\n", "# if query_value and (query_value < 0. or query_value > 1.):", "\n", "#     raise ValueError('query_value must be in [0, 1].')", "\n", "\n", "# Sample a value in range [0, R), where R is the value stored at the root.", "\n", "nodes", "=", "jnp", ".", "array", "(", "self", ".", "nodes", ")", "\n", "query_value", "=", "(", "\n", "jax", ".", "random", ".", "uniform", "(", "rng", ")", "if", "query_value", "is", "None", "else", "query_value", ")", "\n", "query_value", "*=", "self", ".", "_total_priority", "(", ")", "\n", "\n", "# Now traverse the sum tree.", "\n", "# print(\"Sum tree sampling took {}\".format(time.time() - start))", "\n", "\n", "_", ",", "index", ",", "_", "=", "jax", ".", "lax", ".", "fori_loop", "(", "0", ",", "self", ".", "depth", ",", "\n", "step", ",", "\n", "(", "query_value", ",", "0", ",", "nodes", ")", ")", "\n", "\n", "# for nodes_at_this_depth in self.nodes[1:]:", "\n", "#     # Compute children of previous depth's node.", "\n", "#     left_child = node_index * 2", "\n", "#", "\n", "#     left_sum = nodes_at_this_depth[left_child]", "\n", "#     # Each subtree describes a range [0, a), where a is its value.", "\n", "#     if query_value < left_sum:    # Recurse into left subtree.", "\n", "#         node_index = left_child", "\n", "#     else:    # Recurse into right subtree.", "\n", "#         node_index = left_child + 1", "\n", "#         # Adjust query to be relative to right subtree.", "\n", "#         query_value -= left_sum", "\n", "# print(\"Sum tree traversal took {}\".format(time.time() - start))", "\n", "\n", "# Possible to get nasty errors due to numerical issues, so make sure", "\n", "# we return something that's actually in the buffer.", "\n", "return", "np", ".", "minimum", "(", "index", "-", "self", ".", "low_idx", ",", "self", ".", "highest_set", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.stratified_sample": [[135, 148], ["deterministic_sum_tree.parallel_stratified_sample", "jax.numpy.minimum", "deterministic_sum_tree.DeterministicSumTree._total_priority", "Exception", "jax.numpy.arange"], "methods", ["home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.parallel_stratified_sample", "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree._total_priority"], ["", "def", "stratified_sample", "(", "self", ",", "batch_size", ",", "rng", ")", ":", "\n", "        ", "\"\"\"Performs stratified sampling using the sum tree.\"\"\"", "\n", "if", "self", ".", "_total_priority", "(", ")", "==", "0.0", ":", "\n", "            ", "raise", "Exception", "(", "'Cannot sample from an empty sum tree.'", ")", "\n", "\n", "# bounds = np.linspace(0., 1., batch_size + 1)", "\n", "# assert len(bounds) == batch_size + 1", "\n", "# segments = [(bounds[i], bounds[i+1]) for i in range(batch_size)]", "\n", "# query_values = [", "\n", "#         jax.random.uniform(rng, minval=x[0], maxval=x[1]) for x in segments]", "\n", "\n", "", "indices", "=", "parallel_stratified_sample", "(", "rng", ",", "self", ".", "nodes", ",", "jnp", ".", "arange", "(", "batch_size", ")", ",", "batch_size", ",", "self", ".", "depth", ")", "\n", "return", "np", ".", "minimum", "(", "indices", "-", "self", ".", "low_idx", ",", "self", ".", "highest_set", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.get": [[149, 157], ["None"], "methods", ["None"], ["", "def", "get", "(", "self", ",", "node_index", ")", ":", "\n", "        ", "\"\"\"Returns the value of the leaf node corresponding to the index.\n        Args:\n            node_index: The index of the leaf node.\n        Returns:\n            The value of the leaf node.\n        \"\"\"", "\n", "return", "self", ".", "nodes", "[", "node_index", "+", "self", ".", "low_idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.DeterministicSumTree.set": [[158, 186], ["max", "max", "reversed", "ValueError", "range"], "methods", ["None"], ["", "def", "set", "(", "self", ",", "node_index", ",", "value", ")", ":", "\n", "        ", "\"\"\"Sets the value of a leaf node and updates internal nodes accordingly.\n        This operation takes O(log(capacity)).\n        Args:\n            node_index: int, the index of the leaf node to be updated.\n            value: float, the value which we assign to the node. This value must be\n                nonnegative. Setting value = 0 will cause the element to never be\n                sampled.\n        Raises:\n            ValueError: If the given value is negative.\n        \"\"\"", "\n", "if", "value", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "'Sum tree values should be nonnegative. Got {}'", ".", "\n", "format", "(", "value", ")", ")", "\n", "", "self", ".", "highest_set", "=", "max", "(", "node_index", ",", "self", ".", "highest_set", ")", "\n", "node_index", "=", "node_index", "+", "self", ".", "low_idx", "\n", "self", ".", "max_recorded_priority", "=", "max", "(", "value", ",", "self", ".", "max_recorded_priority", ")", "\n", "\n", "delta_value", "=", "value", "-", "self", ".", "nodes", "[", "node_index", "]", "\n", "\n", "# Now traverse back the tree, adjusting all sums along the way.", "\n", "for", "depth", "in", "reversed", "(", "range", "(", "self", ".", "depth", ")", ")", ":", "\n", "# Note: Adding a delta leads to some tolerable numerical inaccuracies.", "\n", "            ", "self", ".", "nodes", "[", "node_index", "]", "+=", "delta_value", "\n", "node_index", "=", "(", "node_index", "-", "1", ")", "//", "2", "\n", "\n", "", "self", ".", "nodes", "[", "node_index", "]", "+=", "delta_value", "\n", "assert", "node_index", "==", "0", ",", "(", "'Sum tree traversal failed, final node index '", "\n", "'is not 0.'", ")", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.step": [[26, 40], ["jax.lax.cond", "jax.lax.cond"], "function", ["None"], ["@", "jax", ".", "jit", "\n", "def", "step", "(", "i", ",", "args", ")", ":", "\n", "    ", "query_value", ",", "index", ",", "nodes", "=", "args", "\n", "left_child", "=", "index", "*", "2", "+", "1", "\n", "left_sum", "=", "nodes", "[", "left_child", "]", "\n", "index", "=", "jax", ".", "lax", ".", "cond", "(", "query_value", "<", "left_sum", ",", "\n", "lambda", "x", ":", "x", ",", "\n", "lambda", "x", ":", "x", "+", "1", ",", "\n", "left_child", ")", "\n", "query_value", "=", "jax", ".", "lax", ".", "cond", "(", "query_value", "<", "left_sum", ",", "\n", "lambda", "x", ":", "x", ",", "\n", "lambda", "x", ":", "x", "-", "left_sum", ",", "\n", "query_value", ")", "\n", "return", "query_value", ",", "index", ",", "nodes", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.replay_memory.deterministic_sum_tree.parallel_stratified_sample": [[42, 54], ["functools.partial", "jax.random.fold_in", "jax.random.uniform", "jax.lax.fori_loop"], "function", ["None"], ["", "@", "jax", ".", "jit", "\n", "@", "functools", ".", "partial", "(", "jax", ".", "vmap", ",", "in_axes", "=", "(", "None", ",", "None", ",", "0", ",", "None", ",", "None", ")", ")", "\n", "def", "parallel_stratified_sample", "(", "rng", ",", "nodes", ",", "i", ",", "n", ",", "depth", ")", ":", "\n", "    ", "rng", "=", "jax", ".", "random", ".", "fold_in", "(", "rng", ",", "i", ")", "\n", "total_priority", "=", "nodes", "[", "0", "]", "\n", "upper_bound", "=", "(", "i", "+", "1", ")", "/", "n", "\n", "lower_bound", "=", "i", "/", "n", "\n", "query", "=", "jax", ".", "random", ".", "uniform", "(", "rng", ",", "minval", "=", "lower_bound", ",", "maxval", "=", "upper_bound", ")", "\n", "_", ",", "index", ",", "_", "=", "jax", ".", "lax", ".", "fori_loop", "(", "0", ",", "depth", ",", "\n", "step", ",", "\n", "(", "query", "*", "total_priority", ",", "0", ",", "nodes", ")", ")", "\n", "return", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.configs.sac.get_config": [[4, 27], ["ml_collections.ConfigDict", "int"], "function", ["None"], ["def", "get_config", "(", ")", ":", "\n", "    ", "config", "=", "ml_collections", ".", "ConfigDict", "(", ")", "\n", "\n", "config", ".", "algo", "=", "'sac'", "\n", "\n", "config", ".", "actor_lr", "=", "3e-4", "\n", "config", ".", "critic_lr", "=", "3e-4", "\n", "config", ".", "temp_lr", "=", "3e-4", "\n", "\n", "config", ".", "hidden_dims", "=", "(", "256", ",", "256", ")", "\n", "\n", "config", ".", "discount", "=", "0.99", "\n", "\n", "config", ".", "tau", "=", "0.005", "\n", "config", ".", "target_update_period", "=", "1", "\n", "config", ".", "updates_per_step", "=", "1", "\n", "\n", "config", ".", "init_temperature", "=", "1.0", "\n", "config", ".", "target_entropy", "=", "None", "\n", "\n", "config", ".", "replay_buffer_size", "=", "int", "(", "1e6", ")", "\n", "\n", "return", "config", "\n", "", ""]], "home.repos.pwc.inspect_result.evgenii-nikishin_rl_with_resets.configs.drq.get_config": [[4, 34], ["ml_collections.ConfigDict"], "function", ["None"], ["def", "get_config", "(", ")", ":", "\n", "    ", "config", "=", "ml_collections", ".", "ConfigDict", "(", ")", "\n", "\n", "config", ".", "algo", "=", "'drq'", "\n", "\n", "config", ".", "actor_lr", "=", "3e-4", "\n", "config", ".", "critic_lr", "=", "3e-4", "\n", "config", ".", "temp_lr", "=", "3e-4", "\n", "\n", "config", ".", "hidden_dims", "=", "(", "256", ",", "256", ")", "\n", "\n", "config", ".", "cnn_features", "=", "(", "32", ",", "64", ",", "128", ",", "256", ")", "\n", "config", ".", "cnn_strides", "=", "(", "2", ",", "2", ",", "2", ",", "2", ")", "\n", "config", ".", "cnn_padding", "=", "'SAME'", "\n", "config", ".", "latent_dim", "=", "50", "\n", "\n", "config", ".", "discount", "=", "0.99", "\n", "\n", "config", ".", "tau", "=", "0.005", "\n", "config", ".", "target_update_period", "=", "1", "\n", "\n", "config", ".", "init_temperature", "=", "0.1", "\n", "config", ".", "target_entropy", "=", "None", "\n", "\n", "config", ".", "replay_buffer_size", "=", "100_000", "\n", "\n", "config", ".", "gray_scale", "=", "True", "\n", "config", ".", "image_size", "=", "64", "\n", "\n", "return", "config", "\n", "", ""]]}