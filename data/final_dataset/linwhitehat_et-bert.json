{"home.repos.pwc.inspect_result.linwhitehat_et-bert.None.preprocess.main": [[13, 79], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "dataset.build_and_save"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.Dataset.build_and_save"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "formatter_class", "=", "argparse", ".", "ArgumentDefaultsHelpFormatter", ")", "\n", "\n", "# Path options.", "\n", "parser", ".", "add_argument", "(", "\"--corpus_path\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of the corpus for pretraining.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--vocab_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Path of the vocabulary file.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--spm_model_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Path of the sentence piece model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_vocab_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Path of the target vocabulary file.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_spm_model_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Path of the target sentence piece model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dataset_path\"", ",", "type", "=", "str", ",", "default", "=", "\"dataset.pt\"", ",", "\n", "help", "=", "\"Path of the preprocessed dataset.\"", ")", "\n", "\n", "# Preprocess options.", "\n", "parser", ".", "add_argument", "(", "\"--tokenizer\"", ",", "choices", "=", "[", "\"bert\"", ",", "\"char\"", ",", "\"space\"", "]", ",", "default", "=", "\"bert\"", ",", "\n", "help", "=", "\"Specify the tokenizer.\"", "\n", "\"Original Google BERT uses bert tokenizer on Chinese corpus.\"", "\n", "\"Char tokenizer segments sentences into characters.\"", "\n", "\"Space tokenizer segments sentences into words according to space.\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_tokenizer\"", ",", "choices", "=", "[", "\"bert\"", ",", "\"char\"", ",", "\"space\"", "]", ",", "default", "=", "\"bert\"", ",", "\n", "help", "=", "\"Specify the tokenizer.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--processes_num\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Split the whole dataset into `processes_num` parts, \"", "\n", "\"and each part is fed to a single process in training step.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--target\"", ",", "choices", "=", "[", "\"bert\"", ",", "\"lm\"", ",", "\"mlm\"", ",", "\"bilm\"", ",", "\"albert\"", ",", "\"seq2seq\"", ",", "\"t5\"", ",", "\"cls\"", ",", "\"prefixlm\"", "]", ",", "default", "=", "\"bert\"", ",", "\n", "help", "=", "\"The training target of the pretraining model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--docs_buffer_size\"", ",", "type", "=", "int", ",", "default", "=", "100000", ",", "\n", "help", "=", "\"The buffer size of documents in memory, specific to targets that require negative sampling.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--seq_length\"", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "\"Sequence length of instances.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_seq_length\"", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "\"Target sequence length of instances.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dup_factor\"", ",", "type", "=", "int", ",", "default", "=", "5", ",", "\n", "help", "=", "\"Duplicate instances multiple times.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--short_seq_prob\"", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "\n", "help", "=", "\"Probability of truncating sequence.\"", "\n", "\"The larger value, the higher probability of using short (truncated) sequence.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--full_sentences\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Full sentences.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "type", "=", "int", ",", "default", "=", "7", ",", "help", "=", "\"Random seed.\"", ")", "\n", "\n", "# Masking options.", "\n", "parser", ".", "add_argument", "(", "\"--dynamic_masking\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Dynamic masking.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--whole_word_masking\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Whole word masking.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--span_masking\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Span masking.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--span_geo_prob\"", ",", "type", "=", "float", ",", "default", "=", "0.2", ",", "\n", "help", "=", "\"Hyperparameter of geometric distribution for span masking.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--span_max_length\"", ",", "type", "=", "int", ",", "default", "=", "10", ",", "\n", "help", "=", "\"Max length for span masking.\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# Dynamic masking.", "\n", "if", "args", ".", "dynamic_masking", ":", "\n", "        ", "args", ".", "dup_factor", "=", "1", "\n", "\n", "# Build tokenizer.", "\n", "", "tokenizer", "=", "str2tokenizer", "[", "args", ".", "tokenizer", "]", "(", "args", ")", "\n", "if", "args", ".", "target", "==", "\"seq2seq\"", ":", "\n", "        ", "args", ".", "tgt_tokenizer", "=", "str2tokenizer", "[", "args", ".", "tgt_tokenizer", "]", "(", "args", ",", "False", ")", "\n", "\n", "# Build and save dataset.", "\n", "", "dataset", "=", "str2dataset", "[", "args", ".", "target", "]", "(", "args", ",", "tokenizer", ".", "vocab", ",", "tokenizer", ")", "\n", "dataset", ".", "build_and_save", "(", "args", ".", "processes_num", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.fine-tuning.run_classifier.Classifier.__init__": [[22, 32], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "len"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "Classifier", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embedding", "=", "str2embedding", "[", "args", ".", "embedding", "]", "(", "args", ",", "len", "(", "args", ".", "tokenizer", ".", "vocab", ")", ")", "\n", "self", ".", "encoder", "=", "str2encoder", "[", "args", ".", "encoder", "]", "(", "args", ")", "\n", "self", ".", "labels_num", "=", "args", ".", "labels_num", "\n", "self", ".", "pooling", "=", "args", ".", "pooling", "\n", "self", ".", "soft_targets", "=", "args", ".", "soft_targets", "\n", "self", ".", "soft_alpha", "=", "args", ".", "soft_alpha", "\n", "self", ".", "output_layer_1", "=", "nn", ".", "Linear", "(", "args", ".", "hidden_size", ",", "args", ".", "hidden_size", ")", "\n", "self", ".", "output_layer_2", "=", "nn", ".", "Linear", "(", "args", ".", "hidden_size", ",", "self", ".", "labels_num", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.fine-tuning.run_classifier.Classifier.forward": [[33, 65], ["run_classifier.Classifier.embedding", "run_classifier.Classifier.encoder", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "run_classifier.Classifier.output_layer_2", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "run_classifier.Classifier.output_layer_1", "torch.max", "torch.max", "torch.max", "torch.max", "torch.NLLLoss", "torch.NLLLoss", "tgt.view", "torch.LogSoftmax", "torch.LogSoftmax", "torch.MSELoss", "torch.MSELoss", "torch.NLLLoss", "torch.NLLLoss", "tgt.view", "torch.LogSoftmax", "torch.LogSoftmax"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src", ",", "tgt", ",", "seg", ",", "soft_tgt", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            src: [batch_size x seq_length]\n            tgt: [batch_size]\n            seg: [batch_size x seq_length]\n        \"\"\"", "\n", "# Embedding.", "\n", "emb", "=", "self", ".", "embedding", "(", "src", ",", "seg", ")", "\n", "# Encoder.", "\n", "output", "=", "self", ".", "encoder", "(", "emb", ",", "seg", ")", "\n", "temp_output", "=", "output", "\n", "# Target.", "\n", "if", "self", ".", "pooling", "==", "\"mean\"", ":", "\n", "            ", "output", "=", "torch", ".", "mean", "(", "output", ",", "dim", "=", "1", ")", "\n", "", "elif", "self", ".", "pooling", "==", "\"max\"", ":", "\n", "            ", "output", "=", "torch", ".", "max", "(", "output", ",", "dim", "=", "1", ")", "[", "0", "]", "\n", "", "elif", "self", ".", "pooling", "==", "\"last\"", ":", "\n", "            ", "output", "=", "output", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "", "else", ":", "\n", "            ", "output", "=", "output", "[", ":", ",", "0", ",", ":", "]", "\n", "", "output", "=", "torch", ".", "tanh", "(", "self", ".", "output_layer_1", "(", "output", ")", ")", "\n", "logits", "=", "self", ".", "output_layer_2", "(", "output", ")", "\n", "if", "tgt", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "soft_targets", "and", "soft_tgt", "is", "not", "None", ":", "\n", "                ", "loss", "=", "self", ".", "soft_alpha", "*", "nn", ".", "MSELoss", "(", ")", "(", "logits", ",", "soft_tgt", ")", "+", "(", "1", "-", "self", ".", "soft_alpha", ")", "*", "nn", ".", "NLLLoss", "(", ")", "(", "nn", ".", "LogSoftmax", "(", "dim", "=", "-", "1", ")", "(", "logits", ")", ",", "tgt", ".", "view", "(", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "nn", ".", "NLLLoss", "(", ")", "(", "nn", ".", "LogSoftmax", "(", "dim", "=", "-", "1", ")", "(", "logits", ")", ",", "tgt", ".", "view", "(", "-", "1", ")", ")", "\n", "", "return", "loss", ",", "logits", "\n", "", "else", ":", "\n", "            ", "return", "None", ",", "logits", "\n", "#return temp_output, logits", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.fine-tuning.run_classifier.count_labels_num": [[68, 80], ["len", "set", "open", "enumerate", "line.strip().split.strip().split", "int", "labels_set.add", "enumerate", "line.strip().split.strip().split", "line.strip().split.strip", "line.strip().split.strip"], "function", ["None"], ["", "", "", "def", "count_labels_num", "(", "path", ")", ":", "\n", "    ", "labels_set", ",", "columns", "=", "set", "(", ")", ",", "{", "}", "\n", "with", "open", "(", "path", ",", "mode", "=", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "for", "line_id", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "            ", "if", "line_id", "==", "0", ":", "\n", "                ", "for", "i", ",", "column_name", "in", "enumerate", "(", "line", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", ")", ":", "\n", "                    ", "columns", "[", "column_name", "]", "=", "i", "\n", "", "continue", "\n", "", "line", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "label", "=", "int", "(", "line", "[", "columns", "[", "\"label\"", "]", "]", ")", "\n", "labels_set", ".", "add", "(", "label", ")", "\n", "", "", "return", "len", "(", "labels_set", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.fine-tuning.run_classifier.load_or_initialize_parameters": [[82, 91], ["model.load_state_dict", "list", "torch.load", "torch.load", "model.named_parameters", "p.data.normal_"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.load", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.load"], ["", "def", "load_or_initialize_parameters", "(", "args", ",", "model", ")", ":", "\n", "    ", "if", "args", ".", "pretrained_model_path", "is", "not", "None", ":", "\n", "# Initialize with pretrained model.", "\n", "        ", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "args", ".", "pretrained_model_path", ",", "map_location", "=", "{", "'cuda:1'", ":", "'cuda:0'", ",", "'cuda:2'", ":", "'cuda:0'", ",", "'cuda:3'", ":", "'cuda:0'", "}", ")", ",", "strict", "=", "False", ")", "\n", "", "else", ":", "\n", "# Initialize with normal distribution.", "\n", "        ", "for", "n", ",", "p", "in", "list", "(", "model", ".", "named_parameters", "(", ")", ")", ":", "\n", "            ", "if", "\"gamma\"", "not", "in", "n", "and", "\"beta\"", "not", "in", "n", ":", "\n", "                ", "p", ".", "data", ".", "normal_", "(", "0", ",", "0.02", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.fine-tuning.run_classifier.build_optimizer": [[93, 112], ["list", "model.named_parameters", "any", "any"], "function", ["None"], ["", "", "", "", "def", "build_optimizer", "(", "args", ",", "model", ")", ":", "\n", "    ", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'gamma'", ",", "'beta'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay_rate'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay_rate'", ":", "0.0", "}", "\n", "]", "\n", "if", "args", ".", "optimizer", "in", "[", "\"adamw\"", "]", ":", "\n", "        ", "optimizer", "=", "str2optimizer", "[", "args", ".", "optimizer", "]", "(", "optimizer_grouped_parameters", ",", "lr", "=", "args", ".", "learning_rate", ",", "correct_bias", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "optimizer", "=", "str2optimizer", "[", "args", ".", "optimizer", "]", "(", "optimizer_grouped_parameters", ",", "lr", "=", "args", ".", "learning_rate", ",", "\n", "scale_parameter", "=", "False", ",", "relative_step", "=", "False", ")", "\n", "", "if", "args", ".", "scheduler", "in", "[", "\"constant\"", "]", ":", "\n", "        ", "scheduler", "=", "str2scheduler", "[", "args", ".", "scheduler", "]", "(", "optimizer", ")", "\n", "", "elif", "args", ".", "scheduler", "in", "[", "\"constant_with_warmup\"", "]", ":", "\n", "        ", "scheduler", "=", "str2scheduler", "[", "args", ".", "scheduler", "]", "(", "optimizer", ",", "args", ".", "train_steps", "*", "args", ".", "warmup", ")", "\n", "", "else", ":", "\n", "        ", "scheduler", "=", "str2scheduler", "[", "args", ".", "scheduler", "]", "(", "optimizer", ",", "args", ".", "train_steps", "*", "args", ".", "warmup", ",", "args", ".", "train_steps", ")", "\n", "", "return", "optimizer", ",", "scheduler", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.fine-tuning.run_classifier.batch_loader": [[114, 134], ["range", "src.size"], "function", ["None"], ["", "def", "batch_loader", "(", "batch_size", ",", "src", ",", "tgt", ",", "seg", ",", "soft_tgt", "=", "None", ")", ":", "\n", "    ", "instances_num", "=", "src", ".", "size", "(", ")", "[", "0", "]", "\n", "for", "i", "in", "range", "(", "instances_num", "//", "batch_size", ")", ":", "\n", "        ", "src_batch", "=", "src", "[", "i", "*", "batch_size", ":", "(", "i", "+", "1", ")", "*", "batch_size", ",", ":", "]", "\n", "tgt_batch", "=", "tgt", "[", "i", "*", "batch_size", ":", "(", "i", "+", "1", ")", "*", "batch_size", "]", "\n", "seg_batch", "=", "seg", "[", "i", "*", "batch_size", ":", "(", "i", "+", "1", ")", "*", "batch_size", ",", ":", "]", "\n", "if", "soft_tgt", "is", "not", "None", ":", "\n", "            ", "soft_tgt_batch", "=", "soft_tgt", "[", "i", "*", "batch_size", ":", "(", "i", "+", "1", ")", "*", "batch_size", ",", ":", "]", "\n", "yield", "src_batch", ",", "tgt_batch", ",", "seg_batch", ",", "soft_tgt_batch", "\n", "", "else", ":", "\n", "            ", "yield", "src_batch", ",", "tgt_batch", ",", "seg_batch", ",", "None", "\n", "", "", "if", "instances_num", ">", "instances_num", "//", "batch_size", "*", "batch_size", ":", "\n", "        ", "src_batch", "=", "src", "[", "instances_num", "//", "batch_size", "*", "batch_size", ":", ",", ":", "]", "\n", "tgt_batch", "=", "tgt", "[", "instances_num", "//", "batch_size", "*", "batch_size", ":", "]", "\n", "seg_batch", "=", "seg", "[", "instances_num", "//", "batch_size", "*", "batch_size", ":", ",", ":", "]", "\n", "if", "soft_tgt", "is", "not", "None", ":", "\n", "            ", "soft_tgt_batch", "=", "soft_tgt", "[", "instances_num", "//", "batch_size", "*", "batch_size", ":", ",", ":", "]", "\n", "yield", "src_batch", ",", "tgt_batch", ",", "seg_batch", ",", "soft_tgt_batch", "\n", "", "else", ":", "\n", "            ", "yield", "src_batch", ",", "tgt_batch", ",", "seg_batch", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.fine-tuning.run_classifier.read_dataset": [[136, 171], ["open", "enumerate", "line[].split", "int", "enumerate", "args.tokenizer.convert_tokens_to_ids", "args.tokenizer.convert_tokens_to_ids", "args.tokenizer.convert_tokens_to_ids", "len", "len", "args.tokenizer.convert_tokens_to_ids.append", "seg.append", "dataset.append", "dataset.append", "line[].split.strip().split", "columns.keys", "float", "len", "columns.keys", "line[].split", "args.tokenizer.tokenize", "args.tokenizer.tokenize", "len", "len", "line[].split.strip", "args.tokenizer.tokenize"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_tokens_to_ids", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_tokens_to_ids", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_tokens_to_ids", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.WordpieceTokenizer.tokenize"], ["", "", "", "def", "read_dataset", "(", "args", ",", "path", ")", ":", "\n", "    ", "dataset", ",", "columns", "=", "[", "]", ",", "{", "}", "\n", "with", "open", "(", "path", ",", "mode", "=", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "for", "line_id", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "            ", "if", "line_id", "==", "0", ":", "\n", "                ", "for", "i", ",", "column_name", "in", "enumerate", "(", "line", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", ")", ":", "\n", "                    ", "columns", "[", "column_name", "]", "=", "i", "\n", "", "continue", "\n", "", "line", "=", "line", "[", ":", "-", "1", "]", ".", "split", "(", "\"\\t\"", ")", "\n", "tgt", "=", "int", "(", "line", "[", "columns", "[", "\"label\"", "]", "]", ")", "\n", "if", "args", ".", "soft_targets", "and", "\"logits\"", "in", "columns", ".", "keys", "(", ")", ":", "\n", "                ", "soft_tgt", "=", "[", "float", "(", "value", ")", "for", "value", "in", "line", "[", "columns", "[", "\"logits\"", "]", "]", ".", "split", "(", "\" \"", ")", "]", "\n", "", "if", "\"text_b\"", "not", "in", "columns", ":", "# Sentence classification.", "\n", "                ", "text_a", "=", "line", "[", "columns", "[", "\"text_a\"", "]", "]", "\n", "src", "=", "args", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "CLS_TOKEN", "]", "+", "args", ".", "tokenizer", ".", "tokenize", "(", "text_a", ")", ")", "\n", "seg", "=", "[", "1", "]", "*", "len", "(", "src", ")", "\n", "", "else", ":", "# Sentence-pair classification.", "\n", "                ", "text_a", ",", "text_b", "=", "line", "[", "columns", "[", "\"text_a\"", "]", "]", ",", "line", "[", "columns", "[", "\"text_b\"", "]", "]", "\n", "src_a", "=", "args", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "CLS_TOKEN", "]", "+", "args", ".", "tokenizer", ".", "tokenize", "(", "text_a", ")", "+", "[", "SEP_TOKEN", "]", ")", "\n", "src_b", "=", "args", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "args", ".", "tokenizer", ".", "tokenize", "(", "text_b", ")", "+", "[", "SEP_TOKEN", "]", ")", "\n", "src", "=", "src_a", "+", "src_b", "\n", "seg", "=", "[", "1", "]", "*", "len", "(", "src_a", ")", "+", "[", "2", "]", "*", "len", "(", "src_b", ")", "\n", "\n", "", "if", "len", "(", "src", ")", ">", "args", ".", "seq_length", ":", "\n", "                ", "src", "=", "src", "[", ":", "args", ".", "seq_length", "]", "\n", "seg", "=", "seg", "[", ":", "args", ".", "seq_length", "]", "\n", "", "while", "len", "(", "src", ")", "<", "args", ".", "seq_length", ":", "\n", "                ", "src", ".", "append", "(", "0", ")", "\n", "seg", ".", "append", "(", "0", ")", "\n", "", "if", "args", ".", "soft_targets", "and", "\"logits\"", "in", "columns", ".", "keys", "(", ")", ":", "\n", "                ", "dataset", ".", "append", "(", "(", "src", ",", "tgt", ",", "seg", ",", "soft_tgt", ")", ")", "\n", "", "else", ":", "\n", "                ", "dataset", ".", "append", "(", "(", "src", ",", "tgt", ",", "seg", ")", ")", "\n", "\n", "", "", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.fine-tuning.run_classifier.train_model": [[173, 196], ["model.zero_grad", "src_batch.to.to", "tgt_batch.to.to", "seg_batch.to.to", "model", "optimizer.step", "scheduler.step", "soft_tgt_batch.to.to", "torch.cuda.device_count", "torch.cuda.device_count", "torch.mean", "torch.mean", "torch.mean.backward", "args.amp.scale_loss", "scaled_loss.backward"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.optimizers.Adafactor.step", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.optimizers.Adafactor.step"], ["", "def", "train_model", "(", "args", ",", "model", ",", "optimizer", ",", "scheduler", ",", "src_batch", ",", "tgt_batch", ",", "seg_batch", ",", "soft_tgt_batch", "=", "None", ")", ":", "\n", "    ", "model", ".", "zero_grad", "(", ")", "\n", "\n", "src_batch", "=", "src_batch", ".", "to", "(", "args", ".", "device", ")", "\n", "tgt_batch", "=", "tgt_batch", ".", "to", "(", "args", ".", "device", ")", "\n", "seg_batch", "=", "seg_batch", ".", "to", "(", "args", ".", "device", ")", "\n", "if", "soft_tgt_batch", "is", "not", "None", ":", "\n", "        ", "soft_tgt_batch", "=", "soft_tgt_batch", ".", "to", "(", "args", ".", "device", ")", "\n", "\n", "", "loss", ",", "_", "=", "model", "(", "src_batch", ",", "tgt_batch", ",", "seg_batch", ",", "soft_tgt_batch", ")", "\n", "if", "torch", ".", "cuda", ".", "device_count", "(", ")", ">", "1", ":", "\n", "        ", "loss", "=", "torch", ".", "mean", "(", "loss", ")", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "        ", "with", "args", ".", "amp", ".", "scale_loss", "(", "loss", ",", "optimizer", ")", "as", "scaled_loss", ":", "\n", "            ", "scaled_loss", ".", "backward", "(", ")", "\n", "", "", "else", ":", "\n", "        ", "loss", ".", "backward", "(", ")", "\n", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "scheduler", ".", "step", "(", ")", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.fine-tuning.run_classifier.evaluate": [[198, 243], ["torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.zeros", "torch.zeros", "args.model.eval", "enumerate", "print", "run_classifier.batch_loader", "src_batch.to.to", "tgt_batch.to.to", "seg_batch.to.to", "torch.argmax", "torch.argmax", "range", "torch.sum().item", "torch.sum().item", "print", "print", "torch.zeros.numpy", "print", "range", "torch.no_grad", "torch.no_grad", "args.model", "open", "print", "len", "len", "torch.Softmax", "torch.argmax.size", "torch.sum", "torch.sum", "f.write", "torch.zeros.size", "confusion[].item", "confusion[].item", "len", "confusion[].sum().item", "confusion[].sum().item", "str", "confusion[].sum", "confusion[].sum"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.inference.run_classifier_infer.batch_loader"], ["", "def", "evaluate", "(", "args", ",", "dataset", ",", "print_confusion_matrix", "=", "False", ")", ":", "\n", "    ", "src", "=", "torch", ".", "LongTensor", "(", "[", "sample", "[", "0", "]", "for", "sample", "in", "dataset", "]", ")", "\n", "tgt", "=", "torch", ".", "LongTensor", "(", "[", "sample", "[", "1", "]", "for", "sample", "in", "dataset", "]", ")", "\n", "seg", "=", "torch", ".", "LongTensor", "(", "[", "sample", "[", "2", "]", "for", "sample", "in", "dataset", "]", ")", "\n", "\n", "batch_size", "=", "args", ".", "batch_size", "\n", "\n", "correct", "=", "0", "\n", "# Confusion matrix.", "\n", "confusion", "=", "torch", ".", "zeros", "(", "args", ".", "labels_num", ",", "args", ".", "labels_num", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "args", ".", "model", ".", "eval", "(", ")", "\n", "\n", "for", "i", ",", "(", "src_batch", ",", "tgt_batch", ",", "seg_batch", ",", "_", ")", "in", "enumerate", "(", "batch_loader", "(", "batch_size", ",", "src", ",", "tgt", ",", "seg", ")", ")", ":", "\n", "        ", "src_batch", "=", "src_batch", ".", "to", "(", "args", ".", "device", ")", "\n", "tgt_batch", "=", "tgt_batch", ".", "to", "(", "args", ".", "device", ")", "\n", "seg_batch", "=", "seg_batch", ".", "to", "(", "args", ".", "device", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "_", ",", "logits", "=", "args", ".", "model", "(", "src_batch", ",", "tgt_batch", ",", "seg_batch", ")", "\n", "", "pred", "=", "torch", ".", "argmax", "(", "nn", ".", "Softmax", "(", "dim", "=", "1", ")", "(", "logits", ")", ",", "dim", "=", "1", ")", "\n", "gold", "=", "tgt_batch", "\n", "for", "j", "in", "range", "(", "pred", ".", "size", "(", ")", "[", "0", "]", ")", ":", "\n", "            ", "confusion", "[", "pred", "[", "j", "]", ",", "gold", "[", "j", "]", "]", "+=", "1", "\n", "", "correct", "+=", "torch", ".", "sum", "(", "pred", "==", "gold", ")", ".", "item", "(", ")", "\n", "\n", "", "if", "print_confusion_matrix", ":", "\n", "        ", "print", "(", "\"Confusion matrix:\"", ")", "\n", "print", "(", "confusion", ")", "\n", "cf_array", "=", "confusion", ".", "numpy", "(", ")", "\n", "with", "open", "(", "\"/data2/lxj/pre-train/results/confusion_matrix\"", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "for", "cf_a", "in", "cf_array", ":", "\n", "                ", "f", ".", "write", "(", "str", "(", "cf_a", ")", "+", "'\\n'", ")", "\n", "", "", "print", "(", "\"Report precision, recall, and f1:\"", ")", "\n", "eps", "=", "1e-9", "\n", "for", "i", "in", "range", "(", "confusion", ".", "size", "(", ")", "[", "0", "]", ")", ":", "\n", "            ", "p", "=", "confusion", "[", "i", ",", "i", "]", ".", "item", "(", ")", "/", "(", "confusion", "[", "i", ",", ":", "]", ".", "sum", "(", ")", ".", "item", "(", ")", "+", "eps", ")", "\n", "r", "=", "confusion", "[", "i", ",", "i", "]", ".", "item", "(", ")", "/", "(", "confusion", "[", ":", ",", "i", "]", ".", "sum", "(", ")", ".", "item", "(", ")", "+", "eps", ")", "\n", "if", "(", "p", "+", "r", ")", "==", "0", ":", "\n", "                ", "f1", "=", "0", "\n", "", "else", ":", "\n", "                ", "f1", "=", "2", "*", "p", "*", "r", "/", "(", "p", "+", "r", ")", "\n", "", "print", "(", "\"Label {}: {:.3f}, {:.3f}, {:.3f}\"", ".", "format", "(", "i", ",", "p", ",", "r", ",", "f1", ")", ")", "\n", "\n", "", "", "print", "(", "\"Acc. (Correct/Total): {:.4f} ({}/{}) \"", ".", "format", "(", "correct", "/", "len", "(", "dataset", ")", ",", "correct", ",", "len", "(", "dataset", ")", ")", ")", "\n", "return", "correct", "/", "len", "(", "dataset", ")", ",", "confusion", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.fine-tuning.run_classifier.main": [[245, 347], ["argparse.ArgumentParser", "uer.opts.finetune_opts", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "uer.utils.config.load_hyperparam", "uer.utils.seed.set_seed", "run_classifier.count_labels_num", "run_classifier.Classifier", "run_classifier.load_or_initialize_parameters", "torch.device", "torch.device", "torch.nn.DataParallel.to", "run_classifier.read_dataset", "random.shuffle", "len", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "print", "print", "run_classifier.build_optimizer", "print", "tqdm.tqdm", "torch.FloatTensor", "torch.FloatTensor", "int", "amp.initialize", "torch.cuda.device_count", "torch.cuda.device_count", "print", "torch.nn.DataParallel", "torch.nn.DataParallel", "range", "torch.nn.DataParallel.train", "enumerate", "run_classifier.evaluate", "print", "run_classifier.evaluate", "torch.cuda.is_available", "torch.cuda.is_available", "run_classifier.batch_loader", "run_classifier.train_model", "train_model.item", "run_classifier.read_dataset", "uer.model_saver.save_model", "torch.cuda.device_count", "torch.cuda.device_count", "torch.nn.DataParallel.module.load_state_dict", "torch.nn.DataParallel.load_state_dict", "run_classifier.read_dataset", "ImportError", "torch.cuda.device_count", "torch.cuda.device_count", "print", "torch.load", "torch.load", "torch.load", "torch.load"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.opts.finetune_opts", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.config.load_hyperparam", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.seed.set_seed", "home.repos.pwc.inspect_result.linwhitehat_et-bert.fine-tuning.run_classifier.count_labels_num", "home.repos.pwc.inspect_result.linwhitehat_et-bert.fine-tuning.run_classifier.load_or_initialize_parameters", "home.repos.pwc.inspect_result.linwhitehat_et-bert.inference.run_classifier_infer.read_dataset", "home.repos.pwc.inspect_result.linwhitehat_et-bert.fine-tuning.run_classifier.build_optimizer", "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.Trainer.train", "home.repos.pwc.inspect_result.linwhitehat_et-bert.fine-tuning.run_classifier.evaluate", "home.repos.pwc.inspect_result.linwhitehat_et-bert.fine-tuning.run_classifier.evaluate", "home.repos.pwc.inspect_result.linwhitehat_et-bert.inference.run_classifier_infer.batch_loader", "home.repos.pwc.inspect_result.linwhitehat_et-bert.fine-tuning.run_classifier.train_model", "home.repos.pwc.inspect_result.linwhitehat_et-bert.inference.run_classifier_infer.read_dataset", "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.model_saver.save_model", "home.repos.pwc.inspect_result.linwhitehat_et-bert.inference.run_classifier_infer.read_dataset", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.load", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.load", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.load", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.load"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "formatter_class", "=", "argparse", ".", "ArgumentDefaultsHelpFormatter", ")", "\n", "\n", "finetune_opts", "(", "parser", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--pooling\"", ",", "choices", "=", "[", "\"mean\"", ",", "\"max\"", ",", "\"first\"", ",", "\"last\"", "]", ",", "default", "=", "\"first\"", ",", "\n", "help", "=", "\"Pooling type.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--tokenizer\"", ",", "choices", "=", "[", "\"bert\"", ",", "\"char\"", ",", "\"space\"", "]", ",", "default", "=", "\"bert\"", ",", "\n", "help", "=", "\"Specify the tokenizer.\"", "\n", "\"Original Google BERT uses bert tokenizer on Chinese corpus.\"", "\n", "\"Char tokenizer segments sentences into characters.\"", "\n", "\"Space tokenizer segments sentences into words according to space.\"", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--soft_targets\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Train model with logits.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--soft_alpha\"", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "\n", "help", "=", "\"Weight of the soft targets loss.\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# Load the hyperparameters from the config file.", "\n", "args", "=", "load_hyperparam", "(", "args", ")", "\n", "\n", "set_seed", "(", "args", ".", "seed", ")", "\n", "\n", "# Count the number of labels.", "\n", "args", ".", "labels_num", "=", "count_labels_num", "(", "args", ".", "train_path", ")", "\n", "\n", "# Build tokenizer.", "\n", "args", ".", "tokenizer", "=", "str2tokenizer", "[", "args", ".", "tokenizer", "]", "(", "args", ")", "\n", "\n", "# Build classification model.", "\n", "model", "=", "Classifier", "(", "args", ")", "\n", "\n", "# Load or initialize parameters.", "\n", "load_or_initialize_parameters", "(", "args", ",", "model", ")", "\n", "\n", "args", ".", "device", "=", "torch", ".", "device", "(", "\"cuda:0\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "model", "=", "model", ".", "to", "(", "args", ".", "device", ")", "\n", "\n", "# Training phase.", "\n", "trainset", "=", "read_dataset", "(", "args", ",", "args", ".", "train_path", ")", "\n", "random", ".", "shuffle", "(", "trainset", ")", "\n", "instances_num", "=", "len", "(", "trainset", ")", "\n", "batch_size", "=", "args", ".", "batch_size", "\n", "\n", "src", "=", "torch", ".", "LongTensor", "(", "[", "example", "[", "0", "]", "for", "example", "in", "trainset", "]", ")", "\n", "tgt", "=", "torch", ".", "LongTensor", "(", "[", "example", "[", "1", "]", "for", "example", "in", "trainset", "]", ")", "\n", "seg", "=", "torch", ".", "LongTensor", "(", "[", "example", "[", "2", "]", "for", "example", "in", "trainset", "]", ")", "\n", "if", "args", ".", "soft_targets", ":", "\n", "        ", "soft_tgt", "=", "torch", ".", "FloatTensor", "(", "[", "example", "[", "3", "]", "for", "example", "in", "trainset", "]", ")", "\n", "", "else", ":", "\n", "        ", "soft_tgt", "=", "None", "\n", "\n", "", "args", ".", "train_steps", "=", "int", "(", "instances_num", "*", "args", ".", "epochs_num", "/", "batch_size", ")", "+", "1", "\n", "\n", "print", "(", "\"Batch size: \"", ",", "batch_size", ")", "\n", "print", "(", "\"The number of training instances:\"", ",", "instances_num", ")", "\n", "\n", "optimizer", ",", "scheduler", "=", "build_optimizer", "(", "args", ",", "model", ")", "\n", "\n", "if", "args", ".", "fp16", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", "import", "amp", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"", ")", "\n", "", "model", ",", "optimizer", "=", "amp", ".", "initialize", "(", "model", ",", "optimizer", ",", "opt_level", "=", "args", ".", "fp16_opt_level", ")", "\n", "args", ".", "amp", "=", "amp", "\n", "\n", "", "if", "torch", ".", "cuda", ".", "device_count", "(", ")", ">", "1", ":", "\n", "        ", "print", "(", "\"{} GPUs are available. Let's use them.\"", ".", "format", "(", "torch", ".", "cuda", ".", "device_count", "(", ")", ")", ")", "\n", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "", "args", ".", "model", "=", "model", "\n", "\n", "total_loss", ",", "result", ",", "best_result", "=", "0.0", ",", "0.0", ",", "0.0", "\n", "\n", "print", "(", "\"Start training.\"", ")", "\n", "\n", "for", "epoch", "in", "tqdm", ".", "tqdm", "(", "range", "(", "1", ",", "args", ".", "epochs_num", "+", "1", ")", ")", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "for", "i", ",", "(", "src_batch", ",", "tgt_batch", ",", "seg_batch", ",", "soft_tgt_batch", ")", "in", "enumerate", "(", "batch_loader", "(", "batch_size", ",", "src", ",", "tgt", ",", "seg", ",", "soft_tgt", ")", ")", ":", "\n", "            ", "loss", "=", "train_model", "(", "args", ",", "model", ",", "optimizer", ",", "scheduler", ",", "src_batch", ",", "tgt_batch", ",", "seg_batch", ",", "soft_tgt_batch", ")", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "if", "(", "i", "+", "1", ")", "%", "args", ".", "report_steps", "==", "0", ":", "\n", "                ", "print", "(", "\"Epoch id: {}, Training steps: {}, Avg loss: {:.3f}\"", ".", "format", "(", "epoch", ",", "i", "+", "1", ",", "total_loss", "/", "args", ".", "report_steps", ")", ")", "\n", "total_loss", "=", "0.0", "\n", "\n", "", "", "result", "=", "evaluate", "(", "args", ",", "read_dataset", "(", "args", ",", "args", ".", "dev_path", ")", ")", "\n", "if", "result", "[", "0", "]", ">", "best_result", ":", "\n", "            ", "best_result", "=", "result", "[", "0", "]", "\n", "save_model", "(", "model", ",", "args", ".", "output_model_path", ")", "\n", "\n", "# Evaluation phase.", "\n", "", "", "if", "args", ".", "test_path", "is", "not", "None", ":", "\n", "        ", "print", "(", "\"Test set evaluation.\"", ")", "\n", "if", "torch", ".", "cuda", ".", "device_count", "(", ")", ">", "1", ":", "\n", "            ", "model", ".", "module", ".", "load_state_dict", "(", "torch", ".", "load", "(", "args", ".", "output_model_path", ")", ")", "\n", "", "else", ":", "\n", "            ", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "args", ".", "output_model_path", ")", ")", "\n", "", "evaluate", "(", "args", ",", "read_dataset", "(", "args", ",", "args", ".", "test_path", ")", ",", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.pre-training.pretrain.main": [[8, 122], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "model_opts", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "optimization_opts", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "len", "uer.train_and_validate", "uer.utils.config.load_hyperparam", "torch.cuda.is_available", "print", "torch.cuda.device_count", "torch.cuda.is_available", "print", "print", "torch.cuda.device_count"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.opts.model_opts", "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.opts.optimization_opts", "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.train_and_validate", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.config.load_hyperparam"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "formatter_class", "=", "argparse", ".", "ArgumentDefaultsHelpFormatter", ")", "\n", "\n", "# Path options.", "\n", "parser", ".", "add_argument", "(", "\"--dataset_path\"", ",", "type", "=", "str", ",", "default", "=", "\"dataset.pt\"", ",", "\n", "help", "=", "\"Path of the preprocessed dataset.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--vocab_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Path of the vocabulary file.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--spm_model_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Path of the sentence piece model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_vocab_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Path of the target vocabulary file.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_spm_model_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Path of the target sentence piece model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--pretrained_model_path\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"Path of the pretrained model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_model_path\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of the output model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--config_path\"", ",", "type", "=", "str", ",", "default", "=", "\"models/bert/base_config.json\"", ",", "\n", "help", "=", "\"Config file of model hyper-parameters.\"", ")", "\n", "\n", "# Training and saving options. ", "\n", "parser", ".", "add_argument", "(", "\"--total_steps\"", ",", "type", "=", "int", ",", "default", "=", "100000", ",", "\n", "help", "=", "\"Total training steps.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_checkpoint_steps\"", ",", "type", "=", "int", ",", "default", "=", "10000", ",", "\n", "help", "=", "\"Specific steps to save model checkpoint.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--report_steps\"", ",", "type", "=", "int", ",", "default", "=", "100", ",", "\n", "help", "=", "\"Specific steps to print prompt.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--accumulation_steps\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Specific steps to accumulate gradient.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ",", "default", "=", "32", ",", "\n", "help", "=", "\"Training batch size. The actual batch_size is [batch_size x world_size x accumulation_steps].\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--instances_buffer_size\"", ",", "type", "=", "int", ",", "default", "=", "25600", ",", "\n", "help", "=", "\"The buffer size of instances in memory.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--labels_num\"", ",", "type", "=", "int", ",", "required", "=", "False", ",", "\n", "help", "=", "\"Number of prediction labels.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dropout\"", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "help", "=", "\"Dropout value.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "type", "=", "int", ",", "default", "=", "7", ",", "help", "=", "\"Random seed.\"", ")", "\n", "\n", "# Preprocess options.", "\n", "parser", ".", "add_argument", "(", "\"--tokenizer\"", ",", "choices", "=", "[", "\"bert\"", ",", "\"char\"", ",", "\"space\"", "]", ",", "default", "=", "\"bert\"", ",", "\n", "help", "=", "\"Specify the tokenizer.\"", "\n", "\"Original Google BERT uses bert tokenizer on Chinese corpus.\"", "\n", "\"Char tokenizer segments sentences into characters.\"", "\n", "\"Space tokenizer segments sentences into words according to space.\"", "\n", ")", "\n", "\n", "# Model options.", "\n", "model_opts", "(", "parser", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_embedding\"", ",", "choices", "=", "[", "\"word\"", ",", "\"word_pos\"", ",", "\"word_pos_seg\"", ",", "\"word_sinusoidalpos\"", "]", ",", "default", "=", "\"word_pos_seg\"", ",", "\n", "help", "=", "\"Target embedding type.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--decoder\"", ",", "choices", "=", "[", "\"transformer\"", "]", ",", "default", "=", "\"transformer\"", ",", "help", "=", "\"Decoder type.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--pooling\"", ",", "choices", "=", "[", "\"mean\"", ",", "\"max\"", ",", "\"first\"", ",", "\"last\"", "]", ",", "default", "=", "\"first\"", ",", "\n", "help", "=", "\"Pooling type.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--target\"", ",", "choices", "=", "[", "\"bert\"", ",", "\"lm\"", ",", "\"mlm\"", ",", "\"bilm\"", ",", "\"albert\"", ",", "\"seq2seq\"", ",", "\"t5\"", ",", "\"cls\"", ",", "\"prefixlm\"", "]", ",", "default", "=", "\"bert\"", ",", "\n", "help", "=", "\"The training target of the pretraining model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tie_weights\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Tie the word embedding and softmax weights.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--has_lmtarget_bias\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Add bias on output_layer for lm target.\"", ")", "\n", "\n", "# Masking options.", "\n", "parser", ".", "add_argument", "(", "\"--whole_word_masking\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Whole word masking.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--span_masking\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Span masking.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--span_geo_prob\"", ",", "type", "=", "float", ",", "default", "=", "0.2", ",", "\n", "help", "=", "\"Hyperparameter of geometric distribution for span masking.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--span_max_length\"", ",", "type", "=", "int", ",", "default", "=", "10", ",", "\n", "help", "=", "\"Max length for span masking.\"", ")", "\n", "\n", "# Optimizer options.", "\n", "optimization_opts", "(", "parser", ")", "\n", "\n", "# GPU options.", "\n", "parser", ".", "add_argument", "(", "\"--world_size\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "\"Total number of processes (GPUs) for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--gpu_ranks\"", ",", "default", "=", "[", "]", ",", "nargs", "=", "'+'", ",", "type", "=", "int", ",", "help", "=", "\"List of ranks of each process.\"", "\n", "\" Each process has a unique integer rank whose value is in the interval [0, world_size), and runs in a single GPU.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--master_ip\"", ",", "default", "=", "\"tcp://localhost:12345\"", ",", "type", "=", "str", ",", "help", "=", "\"IP-Port of master for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--backend\"", ",", "choices", "=", "[", "\"nccl\"", ",", "\"gloo\"", "]", ",", "default", "=", "\"nccl\"", ",", "type", "=", "str", ",", "help", "=", "\"Distributed backend.\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "target", "==", "\"cls\"", ":", "\n", "        ", "assert", "args", ".", "labels_num", "is", "not", "None", ",", "\"Cls target needs the denotation of the number of labels.\"", "\n", "\n", "# Load hyper-parameters from config file. ", "\n", "", "if", "args", ".", "config_path", ":", "\n", "        ", "load_hyperparam", "(", "args", ")", "\n", "\n", "", "ranks_num", "=", "len", "(", "args", ".", "gpu_ranks", ")", "\n", "\n", "if", "args", ".", "world_size", ">", "1", ":", "\n", "# Multiprocessing distributed mode.", "\n", "        ", "assert", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"No available GPUs.\"", "\n", "assert", "ranks_num", "<=", "args", ".", "world_size", ",", "\"Started processes exceed `world_size` upper limit.\"", "\n", "assert", "ranks_num", "<=", "torch", ".", "cuda", ".", "device_count", "(", ")", ",", "\"Started processes exceeds the available GPUs.\"", "\n", "args", ".", "dist_train", "=", "True", "\n", "args", ".", "ranks_num", "=", "ranks_num", "\n", "print", "(", "\"Using distributed mode for training.\"", ")", "\n", "", "elif", "args", ".", "world_size", "==", "1", "and", "ranks_num", "==", "1", ":", "\n", "# Single GPU mode.", "\n", "        ", "assert", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"No available GPUs.\"", "\n", "args", ".", "gpu_id", "=", "args", ".", "gpu_ranks", "[", "0", "]", "\n", "assert", "args", ".", "gpu_id", "<", "torch", ".", "cuda", ".", "device_count", "(", ")", ",", "\"Invalid specified GPU device.\"", "\n", "args", ".", "dist_train", "=", "False", "\n", "args", ".", "single_gpu", "=", "True", "\n", "print", "(", "\"Using GPU %d for training.\"", "%", "args", ".", "gpu_id", ")", "\n", "", "else", ":", "\n", "# CPU mode.", "\n", "        ", "assert", "ranks_num", "==", "0", ",", "\"GPUs are specified, please check the arguments.\"", "\n", "args", ".", "dist_train", "=", "False", "\n", "args", ".", "single_gpu", "=", "False", "\n", "print", "(", "\"Using CPU mode for training.\"", ")", "\n", "\n", "", "trainer", ".", "train_and_validate", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.vocab_process.main.pcap_preprocess": [[29, 42], ["print", "print", "main.preprocess", "str"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.vocab_process.main.preprocess"], ["def", "pcap_preprocess", "(", ")", ":", "\n", "\n", "    ", "start_date", "=", "tls_date", "[", "0", "]", "\n", "end_date", "=", "tls_date", "[", "1", "]", "\n", "packet_num", "=", "0", "\n", "while", "start_date", "<=", "end_date", ":", "\n", "        ", "data_dir", "=", "tls13_pcap_dir", "+", "str", "(", "start_date", ")", "+", "\"\\\\\"", "\n", "p_num", "=", "preprocess", "(", "data_dir", ")", "\n", "packet_num", "+=", "p_num", "\n", "start_date", "+=", "1", "\n", "", "print", "(", "\"used packets %d\"", "%", "packet_num", ")", "\n", "print", "(", "\"finish generating tls13 pretrain dataset.\\n please check in %s\"", "%", "word_dir", ")", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.vocab_process.main.preprocess": [[43, 94], ["print", "os.walk", "print", "print", "scapy.rdpcap", "open", "open.close", "p.copy", "binascii.hexlify", "len", "main.cut", "words_txt.append", "open.write", "bytes", "binascii.hexlify.decode", "int", "main.cut", "range", "words_txt.append", "len", "words_txt.append", "words_txt.append", "len"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.cut", "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.cut"], ["", "def", "preprocess", "(", "pcap_dir", ")", ":", "\n", "    ", "print", "(", "\"now pre-process pcap_dir is %s\"", "%", "pcap_dir", ")", "\n", "\n", "packet_num", "=", "0", "\n", "n", "=", "0", "\n", "\n", "for", "parent", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "pcap_dir", ")", ":", "\n", "        ", "for", "file", "in", "files", ":", "\n", "            ", "if", "\"pcapng\"", "not", "in", "file", "and", "tls13_name", "in", "file", ":", "\n", "                ", "n", "+=", "1", "\n", "pcap_name", "=", "parent", "+", "\"\\\\\"", "+", "file", "\n", "print", "(", "\"No.%d pacp is processed ... %s ...\"", "%", "(", "n", ",", "file", ")", ")", "\n", "packets", "=", "scapy", ".", "rdpcap", "(", "pcap_name", ")", "\n", "#word_packet = b''", "\n", "words_txt", "=", "[", "]", "\n", "\n", "for", "p", "in", "packets", ":", "\n", "                    ", "packet_num", "+=", "1", "\n", "word_packet", "=", "p", ".", "copy", "(", ")", "\n", "words", "=", "(", "binascii", ".", "hexlify", "(", "bytes", "(", "word_packet", ")", ")", ")", "\n", "\n", "words_string", "=", "words", ".", "decode", "(", ")", "[", "76", ":", "]", "\n", "# print(words_string)", "\n", "length", "=", "len", "(", "words_string", ")", "\n", "if", "length", "<", "10", ":", "\n", "                        ", "continue", "\n", "", "for", "string_txt", "in", "cut", "(", "words_string", ",", "int", "(", "length", "/", "2", ")", ")", ":", "\n", "                        ", "token_count", "=", "0", "\n", "sentence", "=", "cut", "(", "string_txt", ",", "1", ")", "\n", "for", "sub_string_index", "in", "range", "(", "len", "(", "sentence", ")", ")", ":", "\n", "                            ", "if", "sub_string_index", "!=", "(", "len", "(", "sentence", ")", "-", "1", ")", ":", "\n", "                                ", "token_count", "+=", "1", "\n", "if", "token_count", ">", "256", ":", "\n", "                                    ", "break", "\n", "", "else", ":", "\n", "                                    ", "merge_word_bigram", "=", "sentence", "[", "sub_string_index", "]", "+", "sentence", "[", "\n", "sub_string_index", "+", "1", "]", "\n", "", "", "else", ":", "\n", "                                ", "break", "\n", "", "words_txt", ".", "append", "(", "merge_word_bigram", ")", "\n", "words_txt", ".", "append", "(", "' '", ")", "\n", "", "words_txt", ".", "append", "(", "\"\\n\"", ")", "\n", "", "words_txt", ".", "append", "(", "\"\\n\"", ")", "\n", "\n", "\n", "", "result_file", "=", "open", "(", "word_dir", "+", "word_name", ",", "'a'", ")", "\n", "for", "words", "in", "words_txt", ":", "\n", "                    ", "result_file", ".", "write", "(", "words", ")", "\n", "", "result_file", ".", "close", "(", ")", "\n", "", "", "", "print", "(", "\"finish preprocessing %d pcaps\"", "%", "n", ")", "\n", "return", "packet_num", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.vocab_process.main.cut": [[95, 103], ["len", "range", "len", "range", "len"], "function", ["None"], ["", "def", "cut", "(", "obj", ",", "sec", ")", ":", "\n", "    ", "result", "=", "[", "obj", "[", "i", ":", "i", "+", "sec", "]", "for", "i", "in", "range", "(", "0", ",", "len", "(", "obj", ")", ",", "sec", ")", "]", "\n", "remanent_count", "=", "len", "(", "result", "[", "0", "]", ")", "%", "4", "\n", "if", "remanent_count", "==", "0", ":", "\n", "        ", "pass", "\n", "", "else", ":", "\n", "        ", "result", "=", "[", "obj", "[", "i", ":", "i", "+", "sec", "+", "remanent_count", "]", "for", "i", "in", "range", "(", "0", ",", "len", "(", "obj", ")", ",", "sec", "+", "remanent_count", ")", "]", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.vocab_process.main.build_BPE": [[104, 132], ["tokenizers.Tokenizer", "tokenizers.pre_tokenizers.BertPreTokenizer", "tokenizers.decoders.WordPiece", "tokenizers.processors.BertProcessing", "tokenizers.trainers.WordPieceTrainer", "tokenizers.Tokenizer.train", "tokenizers.Tokenizer.save", "tokenizers.models.WordPiece"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.Trainer.train", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.save"], ["", "def", "build_BPE", "(", ")", ":", "\n", "# generate source dictionary,0-65535", "\n", "    ", "num_count", "=", "65536", "\n", "not_change_string_count", "=", "5", "\n", "i", "=", "0", "\n", "source_dictionary", "=", "{", "}", "\n", "tuple_sep", "=", "(", ")", "\n", "tuple_cls", "=", "(", ")", "\n", "#'PAD':0,'UNK':1,'CLS':2,'SEP':3,'MASK':4", "\n", "while", "i", "<", "num_count", ":", "\n", "        ", "temp_string", "=", "'{:04x}'", ".", "format", "(", "i", ")", "\n", "source_dictionary", "[", "temp_string", "]", "=", "i", "\n", "i", "+=", "1", "\n", "# Initialize a tokenizer", "\n", "", "tokenizer", "=", "Tokenizer", "(", "models", ".", "WordPiece", "(", "vocab", "=", "source_dictionary", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "4", ")", ")", "\n", "\n", "# Customize pre-tokenization and decoding", "\n", "tokenizer", ".", "pre_tokenizer", "=", "pre_tokenizers", ".", "BertPreTokenizer", "(", ")", "\n", "tokenizer", ".", "decoder", "=", "decoders", ".", "WordPiece", "(", ")", "\n", "tokenizer", ".", "post_processor", "=", "processors", ".", "BertProcessing", "(", "sep", "=", "(", "\"[SEP]\"", ",", "1", ")", ",", "cls", "=", "(", "'[CLS]'", ",", "2", ")", ")", "\n", "\n", "# And then train", "\n", "trainer", "=", "trainers", ".", "WordPieceTrainer", "(", "vocab_size", "=", "65536", ",", "min_frequency", "=", "2", ")", "\n", "tokenizer", ".", "train", "(", "[", "word_dir", "+", "word_name", ",", "word_dir", "+", "word_name", "]", ",", "trainer", "=", "trainer", ")", "\n", "\n", "# And Save it", "\n", "tokenizer", ".", "save", "(", "\"wordpiece.tokenizer.json\"", ",", "pretty", "=", "True", ")", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.vocab_process.main.build_vocab": [[133, 145], ["open", "open.read", "open.close", "json.loads", "vocab_txt.append", "open", "f.write"], "function", ["None"], ["", "def", "build_vocab", "(", ")", ":", "\n", "    ", "json_file", "=", "open", "(", "\"wordpiece.tokenizer.json\"", ",", "'r'", ")", "\n", "json_content", "=", "json_file", ".", "read", "(", ")", "\n", "json_file", ".", "close", "(", ")", "\n", "vocab_json", "=", "json", ".", "loads", "(", "json_content", ")", "\n", "vocab_txt", "=", "[", "\"[PAD]\"", ",", "\"[SEP]\"", ",", "\"[CLS]\"", ",", "\"[UNK]\"", ",", "\"[MASK]\"", "]", "\n", "for", "item", "in", "vocab_json", "[", "'model'", "]", "[", "'vocab'", "]", ":", "\n", "        ", "vocab_txt", ".", "append", "(", "item", ")", "# append key of vocab_json", "\n", "", "with", "open", "(", "vocab_dir", "+", "vocab_name", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "word", "in", "vocab_txt", ":", "\n", "            ", "f", ".", "write", "(", "word", "+", "\"\\n\"", ")", "\n", "", "", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.vocab_process.main.bigram_generation": [[146, 165], ["main.cut", "range", "len", "result.rstrip.rstrip", "len"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.cut"], ["", "def", "bigram_generation", "(", "packet_string", ",", "flag", "=", "False", ")", ":", "\n", "    ", "result", "=", "''", "\n", "sentence", "=", "cut", "(", "packet_string", ",", "1", ")", "\n", "token_count", "=", "0", "\n", "for", "sub_string_index", "in", "range", "(", "len", "(", "sentence", ")", ")", ":", "\n", "        ", "if", "sub_string_index", "!=", "(", "len", "(", "sentence", ")", "-", "1", ")", ":", "\n", "            ", "token_count", "+=", "1", "\n", "if", "token_count", ">", "256", ":", "\n", "                ", "break", "\n", "", "else", ":", "\n", "                ", "merge_word_bigram", "=", "sentence", "[", "sub_string_index", "]", "+", "sentence", "[", "sub_string_index", "+", "1", "]", "\n", "", "", "else", ":", "\n", "            ", "break", "\n", "", "result", "+=", "merge_word_bigram", "\n", "result", "+=", "' '", "\n", "", "if", "flag", "==", "True", ":", "\n", "        ", "result", "=", "result", ".", "rstrip", "(", ")", "\n", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.vocab_process.main.read_pcap_feature": [[166, 173], ["flowcontainer.extractor.extract", "flowcontainer.extractor.extract.keys", "packet_length_feature.append"], "function", ["None"], ["", "def", "read_pcap_feature", "(", "pcap_file", ")", ":", "\n", "    ", "packet_length_feature", "=", "[", "]", "\n", "feature_result", "=", "extract", "(", "pcap_file", ",", "filter", "=", "'tcp'", ")", "\n", "for", "key", "in", "feature_result", ".", "keys", "(", ")", ":", "\n", "        ", "value", "=", "feature_result", "[", "key", "]", "\n", "packet_length_feature", ".", "append", "(", "value", ".", "ip_lengths", ")", "\n", "", "return", "packet_length_feature", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.vocab_process.main.read_pcap_flow": [[174, 199], ["scapy.rdpcap", "print", "len", "print", "packet.copy", "binascii.hexlify", "binascii.hexlify.decode", "main.bigram_generation", "packet.copy", "binascii.hexlify", "binascii.hexlify.decode", "main.bigram_generation", "bytes", "bytes"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.bigram_generation", "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.bigram_generation"], ["", "def", "read_pcap_flow", "(", "pcap_file", ")", ":", "\n", "    ", "packets", "=", "scapy", ".", "rdpcap", "(", "pcap_file", ")", "\n", "\n", "packet_count", "=", "0", "\n", "flow_data_string", "=", "''", "\n", "\n", "if", "len", "(", "packets", ")", "<", "5", ":", "\n", "        ", "print", "(", "\"preprocess flow %s but this flow has less than 5 packets.\"", "%", "pcap_file", ")", "\n", "return", "-", "1", "\n", "\n", "", "print", "(", "\"preprocess flow %s\"", "%", "pcap_file", ")", "\n", "for", "packet", "in", "packets", ":", "\n", "        ", "packet_count", "+=", "1", "\n", "if", "packet_count", "==", "5", ":", "\n", "            ", "packet_data", "=", "packet", ".", "copy", "(", ")", "\n", "data", "=", "(", "binascii", ".", "hexlify", "(", "bytes", "(", "packet_data", ")", ")", ")", "\n", "packet_string", "=", "data", ".", "decode", "(", ")", "\n", "flow_data_string", "+=", "bigram_generation", "(", "packet_string", ",", "flag", "=", "True", ")", "\n", "break", "\n", "", "else", ":", "\n", "            ", "packet_data", "=", "packet", ".", "copy", "(", ")", "\n", "data", "=", "(", "binascii", ".", "hexlify", "(", "bytes", "(", "packet_data", ")", ")", ")", "\n", "packet_string", "=", "data", ".", "decode", "(", ")", "\n", "flow_data_string", "+=", "bigram_generation", "(", "packet_string", ")", "\n", "", "", "return", "flow_data_string", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.vocab_process.main.split_cap": [[200, 205], ["os.system"], "function", ["None"], ["", "def", "split_cap", "(", "pcap_file", ",", "pcap_name", ")", ":", "\n", "    ", "cmd", "=", "\"I:\\\\SplitCap.exe -r %s -s session -o I:\\\\split_pcaps\\\\\"", "+", "pcap_name", "\n", "command", "=", "cmd", "%", "pcap_file", "\n", "os", ".", "system", "(", "command", ")", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.open_dataset_deal.fix_dataset": [[8, 19], ["os.walk", "print", "label.split"], "function", ["None"], ["def", "fix_dataset", "(", "method", ")", ":", "\n", "    ", "dataset_path", "=", "\"F:\\\\dataset\\\\cstnet-tls1.3\\\\\"", "\n", "\n", "comand", "=", "\"I:\\\\mergecap.exe -w I:\\\\dataset\\\\%s.pcap I:\\\\%s\\\\*.pcap\"", "\n", "for", "p", ",", "d", ",", "f", "in", "os", ".", "walk", "(", "dataset_path", ")", ":", "\n", "        ", "for", "label", "in", "d", ":", "\n", "            ", "if", "label", "!=", "\"0_merge_datas\"", ":", "\n", "                ", "label_domain", "=", "label", ".", "split", "(", "\".\"", ")", "[", "0", "]", "\n", "print", "(", "comand", "%", "(", "label_domain", ",", "label", ")", ")", "\n", "\n", "", "", "", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.open_dataset_deal.reverse_dir2file": [[20, 26], ["os.walk", "shutil.move"], "function", ["None"], ["", "def", "reverse_dir2file", "(", ")", ":", "\n", "    ", "path", "=", "\"F:\\\\dataset\\\\\"", "\n", "for", "p", ",", "d", ",", "f", "in", "os", ".", "walk", "(", "path", ")", ":", "\n", "        ", "for", "file", "in", "f", ":", "\n", "            ", "shutil", ".", "move", "(", "p", "+", "\"\\\\\"", "+", "file", ",", "path", ")", "\n", "", "", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.open_dataset_deal.dataset_file2dir": [[27, 34], ["os.walk", "os.mkdir", "shutil.move", "file.split"], "function", ["None"], ["", "def", "dataset_file2dir", "(", "file_path", ")", ":", "\n", "    ", "for", "parent", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "file_path", ")", ":", "\n", "        ", "for", "file", "in", "files", ":", "\n", "            ", "label_name", "=", "file", ".", "split", "(", "\".pcap\"", ")", "[", "0", "]", "\n", "os", ".", "mkdir", "(", "parent", "+", "\"\\\\\"", "+", "label_name", ")", "\n", "shutil", ".", "move", "(", "parent", "+", "\"\\\\\"", "+", "file", ",", "parent", "+", "\"\\\\\"", "+", "label_name", "+", "\"\\\\\"", ")", "\n", "", "", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.open_dataset_deal.file_2_pcap": [[35, 40], ["os.system"], "function", ["None"], ["", "def", "file_2_pcap", "(", "source_file", ",", "target_file", ")", ":", "\n", "    ", "cmd", "=", "\"I:\\\\tshark.exe -F pcap -r %s -w %s\"", "\n", "command", "=", "cmd", "%", "(", "source_file", ",", "target_file", ")", "\n", "os", ".", "system", "(", "command", ")", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.open_dataset_deal.clean_pcap": [[41, 48], ["source_file.replace", "os.system"], "function", ["None"], ["", "def", "clean_pcap", "(", "source_file", ")", ":", "\n", "    ", "target_file", "=", "source_file", ".", "replace", "(", "'.pcap'", ",", "'_clean.pcap'", ")", "\n", "clean_protocols", "=", "'\"not arp and not dns and not stun and not dhcpv6 and not icmpv6 and not icmp and not dhcp and not llmnr and not nbns and not ntp and not igmp and frame.len > 80\"'", "\n", "cmd", "=", "\"I:\\\\tshark.exe -F pcap -r %s -Y %s -w %s\"", "\n", "command", "=", "cmd", "%", "(", "source_file", ",", "clean_protocols", ",", "target_file", ")", "\n", "os", ".", "system", "(", "command", ")", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.open_dataset_deal.statistic_dataset_sample_count": [[49, 83], ["os.walk", "print", "print", "dataset_label.extend", "os.walk", "dataset_lengths.extend", "temp.append", "len", "os.walk", "dataset_lengths.extend", "temp.append", "p.split", "len", "p.split"], "function", ["None"], ["", "def", "statistic_dataset_sample_count", "(", "data_path", ")", ":", "\n", "    ", "dataset_label", "=", "[", "]", "\n", "dataset_lengths", "=", "[", "]", "\n", "\n", "tls13_flag", "=", "1", "\n", "\n", "temp", "=", "[", "]", "\n", "for", "p", ",", "d", ",", "f", "in", "os", ".", "walk", "(", "data_path", ")", ":", "\n", "        ", "if", "p", "==", "data_path", ":", "\n", "            ", "dataset_label", ".", "extend", "(", "d", ")", "\n", "", "elif", "f", "==", "[", "]", ":", "\n", "\n", "            ", "if", "(", "p", ".", "split", "(", "\"\\\\\"", ")", "[", "-", "1", "]", "not", "in", "dataset_label", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "file_num", "=", "0", "\n", "for", "pp", ",", "dd", ",", "ff", "in", "os", ".", "walk", "(", "p", ")", ":", "\n", "                ", "file_num", "+=", "len", "(", "ff", ")", "\n", "", "dataset_lengths", ".", "extend", "(", "[", "file_num", "]", ")", "\n", "temp", ".", "append", "(", "p", ")", "\n", "", "else", ":", "\n", "            ", "if", "tls13_flag", "==", "1", ":", "\n", "                ", "if", "(", "p", ".", "split", "(", "\"\\\\\"", ")", "[", "-", "1", "]", "not", "in", "dataset_label", ")", ":", "\n", "                    ", "continue", "\n", "\n", "", "file_num", "=", "0", "\n", "for", "pp", ",", "dd", ",", "ff", "in", "os", ".", "walk", "(", "p", ")", ":", "\n", "                    ", "file_num", "+=", "len", "(", "ff", ")", "\n", "", "dataset_lengths", ".", "extend", "(", "[", "file_num", "]", ")", "\n", "temp", ".", "append", "(", "p", ")", "\n", "\n", "", "", "", "print", "(", "\"label samples: \"", ",", "dataset_lengths", ")", "\n", "print", "(", "\"labels: \"", ",", "dataset_label", ")", "\n", "return", "dataset_lengths", ",", "dataset_label", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.convert_pcapng_2_pcap": [[26, 33], ["os.system", "pcapng_file.replace"], "function", ["None"], ["def", "convert_pcapng_2_pcap", "(", "pcapng_path", ",", "pcapng_file", ",", "output_path", ")", ":", "\n", "\n", "    ", "pcap_file", "=", "output_path", "+", "pcapng_file", ".", "replace", "(", "'pcapng'", ",", "'pcap'", ")", "\n", "cmd", "=", "\"I:\\\\editcap.exe -F pcap %s %s\"", "\n", "command", "=", "cmd", "%", "(", "pcapng_path", "+", "pcapng_file", ",", "pcap_file", ")", "\n", "os", ".", "system", "(", "command", ")", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.split_cap": [[34, 56], ["os.system", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir"], "function", ["None"], ["", "def", "split_cap", "(", "pcap_path", ",", "pcap_file", ",", "pcap_name", ",", "pcap_label", "=", "''", ",", "dataset_level", "=", "'flow'", ")", ":", "\n", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "pcap_path", "+", "\"\\\\splitcap\"", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "pcap_path", "+", "\"\\\\splitcap\"", ")", "\n", "", "if", "pcap_label", "!=", "''", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "pcap_path", "+", "\"\\\\splitcap\\\\\"", "+", "pcap_label", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "pcap_path", "+", "\"\\\\splitcap\\\\\"", "+", "pcap_label", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "pcap_path", "+", "\"\\\\splitcap\\\\\"", "+", "pcap_label", "+", "\"\\\\\"", "+", "pcap_name", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "pcap_path", "+", "\"\\\\splitcap\\\\\"", "+", "pcap_label", "+", "\"\\\\\"", "+", "pcap_name", ")", "\n", "\n", "", "output_path", "=", "pcap_path", "+", "\"\\\\splitcap\\\\\"", "+", "pcap_label", "+", "\"\\\\\"", "+", "pcap_name", "\n", "", "else", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "pcap_path", "+", "\"\\\\splitcap\\\\\"", "+", "pcap_name", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "pcap_path", "+", "\"\\\\splitcap\\\\\"", "+", "pcap_name", ")", "\n", "", "output_path", "=", "pcap_path", "+", "\"\\\\splitcap\\\\\"", "+", "pcap_name", "\n", "", "if", "dataset_level", "==", "'flow'", ":", "\n", "        ", "cmd", "=", "\"I:\\\\SplitCap.exe -r %s -s session -o \"", "+", "output_path", "\n", "", "elif", "dataset_level", "==", "'packet'", ":", "\n", "        ", "cmd", "=", "\"I:\\\\SplitCap.exe -r %s -s packets 1 -o \"", "+", "output_path", "\n", "", "command", "=", "cmd", "%", "pcap_file", "\n", "os", ".", "system", "(", "command", ")", "\n", "return", "output_path", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.cut": [[57, 69], ["range", "len", "print", "len", "range", "len"], "function", ["None"], ["", "def", "cut", "(", "obj", ",", "sec", ")", ":", "\n", "    ", "result", "=", "[", "obj", "[", "i", ":", "i", "+", "sec", "]", "for", "i", "in", "range", "(", "0", ",", "len", "(", "obj", ")", ",", "sec", ")", "]", "\n", "try", ":", "\n", "        ", "remanent_count", "=", "len", "(", "result", "[", "0", "]", ")", "%", "4", "\n", "", "except", "Exception", "as", "e", ":", "\n", "        ", "remanent_count", "=", "0", "\n", "print", "(", "1", ")", "\n", "", "if", "remanent_count", "==", "0", ":", "\n", "        ", "pass", "\n", "", "else", ":", "\n", "        ", "result", "=", "[", "obj", "[", "i", ":", "i", "+", "sec", "+", "remanent_count", "]", "for", "i", "in", "range", "(", "0", ",", "len", "(", "obj", ")", ",", "sec", "+", "remanent_count", ")", "]", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.bigram_generation": [[70, 89], ["dataset_generation.cut", "range", "len", "result.rstrip.rstrip", "len"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.cut"], ["", "def", "bigram_generation", "(", "packet_datagram", ",", "packet_len", "=", "64", ",", "flag", "=", "True", ")", ":", "\n", "    ", "result", "=", "''", "\n", "generated_datagram", "=", "cut", "(", "packet_datagram", ",", "1", ")", "\n", "token_count", "=", "0", "\n", "for", "sub_string_index", "in", "range", "(", "len", "(", "generated_datagram", ")", ")", ":", "\n", "        ", "if", "sub_string_index", "!=", "(", "len", "(", "generated_datagram", ")", "-", "1", ")", ":", "\n", "            ", "token_count", "+=", "1", "\n", "if", "token_count", ">", "packet_len", ":", "\n", "                ", "break", "\n", "", "else", ":", "\n", "                ", "merge_word_bigram", "=", "generated_datagram", "[", "sub_string_index", "]", "+", "generated_datagram", "[", "sub_string_index", "+", "1", "]", "\n", "", "", "else", ":", "\n", "            ", "break", "\n", "", "result", "+=", "merge_word_bigram", "\n", "result", "+=", "' '", "\n", "", "if", "flag", "==", "True", ":", "\n", "        ", "result", "=", "result", ".", "rstrip", "(", ")", "\n", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.get_burst_feature": [[90, 138], ["scapy.rdpcap", "flowcontainer.extractor.extract", "flowcontainer.extractor.extract.keys", "len", "len", "range", "len", "packets[].copy", "binascii.hexlify", "open", "f.write", "abs", "bytes", "binascii.hexlify.decode", "len", "dataset_generation.cut", "len", "dataset_generation.cut", "int", "dataset_generation.bigram_generation", "len", "int", "dataset_generation.bigram_generation", "len", "len"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.cut", "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.cut", "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.bigram_generation", "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.bigram_generation"], ["", "def", "get_burst_feature", "(", "label_pcap", ",", "payload_len", ")", ":", "\n", "    ", "feature_data", "=", "[", "]", "\n", "\n", "packets", "=", "scapy", ".", "rdpcap", "(", "label_pcap", ")", "\n", "\n", "packet_direction", "=", "[", "]", "\n", "feature_result", "=", "extract", "(", "label_pcap", ")", "\n", "for", "key", "in", "feature_result", ".", "keys", "(", ")", ":", "\n", "        ", "value", "=", "feature_result", "[", "key", "]", "\n", "packet_direction", "=", "[", "x", "//", "abs", "(", "x", ")", "for", "x", "in", "value", ".", "ip_lengths", "]", "\n", "\n", "", "if", "len", "(", "packet_direction", ")", "==", "len", "(", "packets", ")", ":", "\n", "\n", "        ", "burst_data_string", "=", "''", "\n", "\n", "burst_txt", "=", "''", "\n", "\n", "for", "packet_index", "in", "range", "(", "len", "(", "packets", ")", ")", ":", "\n", "            ", "packet_data", "=", "packets", "[", "packet_index", "]", ".", "copy", "(", ")", "\n", "data", "=", "(", "binascii", ".", "hexlify", "(", "bytes", "(", "packet_data", ")", ")", ")", "\n", "\n", "packet_string", "=", "data", ".", "decode", "(", ")", "[", ":", "2", "*", "payload_len", "]", "\n", "\n", "if", "packet_index", "==", "0", ":", "\n", "                ", "burst_data_string", "+=", "packet_string", "\n", "", "else", ":", "\n", "                ", "if", "packet_direction", "[", "packet_index", "]", "!=", "packet_direction", "[", "packet_index", "-", "1", "]", ":", "\n", "\n", "                    ", "length", "=", "len", "(", "burst_data_string", ")", "\n", "for", "string_txt", "in", "cut", "(", "burst_data_string", ",", "int", "(", "length", "/", "2", ")", ")", ":", "\n", "                        ", "burst_txt", "+=", "bigram_generation", "(", "string_txt", ",", "packet_len", "=", "len", "(", "string_txt", ")", ")", "\n", "burst_txt", "+=", "'\\n'", "\n", "", "burst_txt", "+=", "'\\n'", "\n", "\n", "burst_data_string", "=", "''", "\n", "\n", "", "burst_data_string", "+=", "packet_string", "\n", "if", "packet_index", "==", "len", "(", "packets", ")", "-", "1", ":", "\n", "\n", "                    ", "length", "=", "len", "(", "burst_data_string", ")", "\n", "for", "string_txt", "in", "cut", "(", "burst_data_string", ",", "int", "(", "length", "/", "2", ")", ")", ":", "\n", "                        ", "burst_txt", "+=", "bigram_generation", "(", "string_txt", ",", "packet_len", "=", "len", "(", "string_txt", ")", ")", "\n", "burst_txt", "+=", "'\\n'", "\n", "", "burst_txt", "+=", "'\\n'", "\n", "\n", "", "", "", "with", "open", "(", "word_dir", "+", "word_name", ",", "'a'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "burst_txt", ")", "\n", "", "", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.get_feature_packet": [[139, 157], ["scapy.rdpcap", "feature_data.append", "packet.copy", "binascii.hexlify", "binascii.hexlify.decode", "dataset_generation.bigram_generation", "bytes"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.bigram_generation"], ["", "def", "get_feature_packet", "(", "label_pcap", ",", "payload_len", ")", ":", "\n", "    ", "feature_data", "=", "[", "]", "\n", "\n", "packets", "=", "scapy", ".", "rdpcap", "(", "label_pcap", ")", "\n", "packet_data_string", "=", "''", "\n", "\n", "for", "packet", "in", "packets", ":", "\n", "            ", "packet_data", "=", "packet", ".", "copy", "(", ")", "\n", "data", "=", "(", "binascii", ".", "hexlify", "(", "bytes", "(", "packet_data", ")", ")", ")", "\n", "\n", "packet_string", "=", "data", ".", "decode", "(", ")", "\n", "\n", "new_packet_string", "=", "packet_string", "[", "76", ":", "]", "\n", "\n", "packet_data_string", "+=", "bigram_generation", "(", "new_packet_string", ",", "packet_len", "=", "payload_len", ",", "flag", "=", "True", ")", "\n", "\n", "", "feature_data", ".", "append", "(", "packet_data_string", ")", "\n", "return", "feature_data", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.get_feature_flow": [[158, 322], ["scapy.rdpcap", "flowcontainer.extractor.extract", "len", "flowcontainer.extractor.extract.keys", "feature_data.append", "feature_data.append", "feature_data.append", "feature_data.append", "feature_data.append", "len", "flowcontainer.extractor.extract", "flowcontainer.extractor.extract.keys", "flowcontainer.extractor.extract.keys", "packet_length.append", "packet_time.append", "range", "extension_dict.keys", "len", "list", "len", "print", "len", "print", "len", "print", "print", "flowcontainer.extractor.extract.keys", "len", "len", "extensions.keys", "extensions.keys", "packet_message_type.append", "extension_dict.keys", "packet_message_type.append", "packet_direction.append", "packet_direction.append", "packet.copy", "binascii.hexlify", "dataset_generation.bigram_generation", "packet.copy", "binascii.hexlify", "dataset_generation.bigram_generation", "flowcontainer.extractor.extract.keys", "len", "len", "extensions.keys", "bytes", "binascii.hexlify.decode", "bytes", "binascii.hexlify.decode", "len", "print", "sum", "sum", "len", "ms_type.extend", "ms_type.append", "len", "range", "len", "ms_type.extend", "ms_type.append", "extension_dict.keys", "extension_dict[].extend", "record_content[].split", "extension_dict.keys", "len", "ms_type.extend", "ms_type.append", "record_opaque[].split", "len", "len", "len", "tls_handshake[].split", "range", "set", "print", "functools.reduce", "len", "len", "int", "extension_dict[].insert", "int", "open", "f.write", "status.split"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.bigram_generation", "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.bigram_generation"], ["", "def", "get_feature_flow", "(", "label_pcap", ",", "payload_len", ",", "payload_pac", ")", ":", "\n", "\n", "    ", "feature_data", "=", "[", "]", "\n", "packets", "=", "scapy", ".", "rdpcap", "(", "label_pcap", ")", "\n", "packet_count", "=", "0", "\n", "flow_data_string", "=", "''", "\n", "\n", "feature_result", "=", "extract", "(", "label_pcap", ",", "filter", "=", "'tcp'", ",", "extension", "=", "[", "'tls.record.content_type'", ",", "'tls.record.opaque_type'", ",", "'tls.handshake.type'", "]", ")", "\n", "if", "len", "(", "feature_result", ")", "==", "0", ":", "\n", "        ", "feature_result", "=", "extract", "(", "label_pcap", ",", "filter", "=", "'udp'", ")", "\n", "if", "len", "(", "feature_result", ")", "==", "0", ":", "\n", "            ", "return", "-", "1", "\n", "", "extract_keys", "=", "list", "(", "feature_result", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "if", "len", "(", "feature_result", "[", "label_pcap", ",", "extract_keys", "[", "1", "]", ",", "extract_keys", "[", "2", "]", "]", ".", "ip_lengths", ")", "<", "3", ":", "\n", "            ", "print", "(", "\"preprocess flow %s but this flow has less than 3 packets.\"", "%", "label_pcap", ")", "\n", "return", "-", "1", "\n", "", "", "elif", "len", "(", "packets", ")", "<", "3", ":", "\n", "        ", "print", "(", "\"preprocess flow %s but this flow has less than 3 packets.\"", "%", "label_pcap", ")", "\n", "return", "-", "1", "\n", "", "try", ":", "\n", "        ", "if", "len", "(", "feature_result", "[", "label_pcap", ",", "'tcp'", ",", "'0'", "]", ".", "ip_lengths", ")", "<", "3", ":", "\n", "            ", "print", "(", "\"preprocess flow %s but this flow has less than 3 packets.\"", "%", "label_pcap", ")", "\n", "return", "-", "1", "\n", "", "", "except", "Exception", "as", "e", ":", "\n", "        ", "print", "(", "\"*** this flow begings from 1 or other numbers than 0.\"", ")", "\n", "for", "key", "in", "feature_result", ".", "keys", "(", ")", ":", "\n", "            ", "if", "len", "(", "feature_result", "[", "key", "]", ".", "ip_lengths", ")", "<", "3", ":", "\n", "                ", "print", "(", "\"preprocess flow %s but this flow has less than 3 packets.\"", "%", "label_pcap", ")", "\n", "return", "-", "1", "\n", "\n", "", "", "", "if", "feature_result", ".", "keys", "(", ")", "==", "{", "}", ".", "keys", "(", ")", ":", "\n", "        ", "return", "-", "1", "\n", "\n", "", "packet_length", "=", "[", "]", "\n", "packet_time", "=", "[", "]", "\n", "packet_direction", "=", "[", "]", "\n", "packet_message_type", "=", "[", "]", "\n", "\n", "if", "feature_result", "==", "{", "}", ":", "\n", "        ", "return", "-", "1", "\n", "", "feature_result_lens", "=", "len", "(", "feature_result", ".", "keys", "(", ")", ")", "\n", "for", "key", "in", "feature_result", ".", "keys", "(", ")", ":", "\n", "        ", "value", "=", "feature_result", "[", "key", "]", "\n", "packet_length", ".", "append", "(", "value", ".", "ip_lengths", ")", "\n", "packet_time", ".", "append", "(", "value", ".", "ip_timestamps", ")", "\n", "\n", "if", "len", "(", "packet_length", ")", "<", "feature_result_lens", ":", "\n", "            ", "continue", "\n", "", "elif", "len", "(", "packet_length", ")", "==", "1", ":", "\n", "            ", "pass", "\n", "", "else", ":", "\n", "            ", "packet_length", "=", "[", "sum", "(", "packet_length", ",", "[", "]", ")", "]", "\n", "packet_time", "=", "[", "sum", "(", "packet_time", ",", "[", "]", ")", "]", "\n", "\n", "", "extension_dict", "=", "{", "}", "\n", "\n", "for", "len_index", "in", "range", "(", "len", "(", "packet_length", ")", ")", ":", "\n", "            ", "extension_list", "=", "[", "0", "]", "*", "(", "len", "(", "packet_length", "[", "len_index", "]", ")", ")", "\n", "\n", "", "extensions", "=", "value", ".", "extension", "\n", "\n", "if", "'tls.record.content_type'", "in", "extensions", ".", "keys", "(", ")", ":", "\n", "            ", "for", "record_content", "in", "extensions", "[", "'tls.record.content_type'", "]", ":", "\n", "                ", "packet_index", "=", "record_content", "[", "1", "]", "\n", "ms_type", "=", "[", "]", "\n", "\n", "if", "len", "(", "record_content", "[", "0", "]", ")", ">", "2", ":", "\n", "                    ", "ms_type", ".", "extend", "(", "record_content", "[", "0", "]", ".", "split", "(", "','", ")", ")", "\n", "", "else", ":", "\n", "                    ", "ms_type", ".", "append", "(", "record_content", "[", "0", "]", ")", "\n", "\n", "", "extension_dict", "[", "packet_index", "]", "=", "ms_type", "\n", "\n", "", "if", "'tls.handshake.type'", "in", "extensions", ".", "keys", "(", ")", ":", "\n", "                ", "for", "tls_handshake", "in", "extensions", "[", "'tls.handshake.type'", "]", ":", "\n", "                    ", "packet_index", "=", "tls_handshake", "[", "1", "]", "\n", "if", "packet_index", "not", "in", "extension_dict", ".", "keys", "(", ")", ":", "\n", "                        ", "continue", "\n", "", "ms_type", "=", "[", "]", "\n", "if", "len", "(", "tls_handshake", "[", "0", "]", ")", ">", "2", ":", "\n", "                        ", "ms_type", ".", "extend", "(", "tls_handshake", "[", "0", "]", ".", "split", "(", "','", ")", ")", "\n", "", "else", ":", "\n", "                        ", "ms_type", ".", "append", "(", "tls_handshake", "[", "0", "]", ")", "\n", "", "source_length", "=", "len", "(", "extension_dict", "[", "packet_index", "]", ")", "\n", "for", "record_index", "in", "range", "(", "source_length", ")", ":", "\n", "                        ", "if", "extension_dict", "[", "packet_index", "]", "[", "record_index", "]", "==", "'22'", ":", "\n", "                            ", "for", "handshake_type_index", "in", "range", "(", "len", "(", "ms_type", ")", ")", ":", "\n", "                                ", "extension_dict", "[", "packet_index", "]", "[", "record_index", "]", "=", "'22:'", "+", "ms_type", "[", "handshake_type_index", "]", "\n", "if", "handshake_type_index", ">", "0", ":", "\n", "                                    ", "extension_dict", "[", "packet_index", "]", ".", "insert", "(", "handshake_type_index", ",", "\n", "(", "'22:'", "+", "ms_type", "[", "handshake_type_index", "]", ")", ")", "\n", "", "", "break", "\n", "", "", "", "", "", "if", "'tls.record.opaque_type'", "in", "extensions", ".", "keys", "(", ")", ":", "\n", "            ", "for", "record_opaque", "in", "extensions", "[", "'tls.record.opaque_type'", "]", ":", "\n", "                ", "packet_index", "=", "record_opaque", "[", "1", "]", "\n", "ms_type", "=", "[", "]", "\n", "if", "len", "(", "record_opaque", "[", "0", "]", ")", ">", "2", ":", "\n", "                    ", "ms_type", ".", "extend", "(", "record_opaque", "[", "0", "]", ".", "split", "(", "\",\"", ")", ")", "\n", "", "else", ":", "\n", "                    ", "ms_type", ".", "append", "(", "record_opaque", "[", "0", "]", ")", "\n", "", "if", "packet_index", "not", "in", "extension_dict", ".", "keys", "(", ")", ":", "\n", "                    ", "extension_dict", "[", "packet_index", "]", "=", "ms_type", "\n", "", "else", ":", "\n", "                    ", "extension_dict", "[", "packet_index", "]", ".", "extend", "(", "ms_type", ")", "\n", "\n", "", "", "", "extension_string_dict", "=", "{", "}", "\n", "for", "key", "in", "extension_dict", ".", "keys", "(", ")", ":", "\n", "            ", "temp_string", "=", "''", "\n", "for", "status", "in", "extension_dict", "[", "key", "]", ":", "\n", "                ", "temp_string", "+=", "status", "+", "','", "\n", "", "temp_string", "=", "temp_string", "[", ":", "-", "1", "]", "\n", "extension_string_dict", "[", "key", "]", "=", "temp_string", "\n", "\n", "", "is_source", "=", "0", "\n", "if", "is_source", ":", "\n", "            ", "packet_message_type", ".", "append", "(", "extension_string_dict", ")", "\n", "", "else", ":", "\n", "            ", "for", "key", "in", "extension_dict", ".", "keys", "(", ")", ":", "\n", "                ", "if", "len", "(", "set", "(", "extension_dict", "[", "key", "]", ")", ")", "==", "1", "and", "len", "(", "extension_dict", "[", "key", "]", ")", ">", "1", ":", "\n", "                    ", "try", ":", "\n", "                        ", "extension_list", "[", "key", "]", "+=", "len", "(", "extension_dict", "[", "key", "]", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "                        ", "print", "(", "key", ")", "\n", "", "", "else", ":", "\n", "                    ", "for", "status", "in", "extension_dict", "[", "key", "]", ":", "\n", "                        ", "if", "':'", "in", "status", ":", "\n", "\n", "                            ", "extension_list", "[", "key", "-", "1", "]", "+=", "reduce", "(", "operator", ".", "mul", ",", "[", "int", "(", "x", ")", "for", "x", "in", "status", ".", "split", "(", "':'", ")", "]", ",", "1", ")", "\n", "", "else", ":", "\n", "\n", "                            ", "if", "key", "<=", "len", "(", "packet_length", "[", "0", "]", ")", ":", "\n", "                                ", "extension_list", "[", "key", "-", "1", "]", "+=", "int", "(", "status", ")", "\n", "", "else", ":", "\n", "                                ", "with", "open", "(", "\"error_while_writin_record\"", ",", "\"a\"", ")", "as", "f", ":", "\n", "                                    ", "f", ".", "write", "(", "label_pcap", "+", "'\\n'", ")", "\n", "", "continue", "\n", "", "", "", "", "", "packet_message_type", ".", "append", "(", "extension_list", ")", "\n", "", "", "for", "length", "in", "packet_length", "[", "0", "]", ":", "\n", "        ", "if", "length", ">", "0", ":", "\n", "            ", "packet_direction", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "packet_direction", ".", "append", "(", "-", "1", ")", "\n", "\n", "", "", "packet_index", "=", "0", "\n", "for", "packet", "in", "packets", ":", "\n", "        ", "packet_count", "+=", "1", "\n", "if", "packet_count", "==", "payload_pac", ":", "\n", "            ", "packet_data", "=", "packet", ".", "copy", "(", ")", "\n", "data", "=", "(", "binascii", ".", "hexlify", "(", "bytes", "(", "packet_data", ")", ")", ")", "\n", "packet_string", "=", "data", ".", "decode", "(", ")", "[", "76", ":", "]", "\n", "flow_data_string", "+=", "bigram_generation", "(", "packet_string", ",", "packet_len", "=", "payload_len", ",", "flag", "=", "True", ")", "\n", "break", "\n", "", "else", ":", "\n", "            ", "packet_data", "=", "packet", ".", "copy", "(", ")", "\n", "data", "=", "(", "binascii", ".", "hexlify", "(", "bytes", "(", "packet_data", ")", ")", ")", "\n", "packet_string", "=", "data", ".", "decode", "(", ")", "[", "76", ":", "]", "\n", "flow_data_string", "+=", "bigram_generation", "(", "packet_string", ",", "packet_len", "=", "payload_len", ",", "flag", "=", "True", ")", "\n", "", "", "feature_data", ".", "append", "(", "flow_data_string", ")", "\n", "feature_data", ".", "append", "(", "packet_length", "[", "0", "]", ")", "\n", "feature_data", ".", "append", "(", "packet_time", "[", "0", "]", ")", "\n", "feature_data", ".", "append", "(", "packet_direction", ")", "\n", "feature_data", ".", "append", "(", "packet_message_type", "[", "0", "]", ")", "\n", "\n", "return", "feature_data", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.generation": [[323, 512], ["os.path.exists", "os.walk", "range", "print", "tqdm.tqdm", "range", "print", "dataset_generation.obtain_data", "print", "dataset_generation.obtain_data", "len", "session_pcap_path.keys", "random.sample", "len", "print", "open", "open", "json.dump", "print", "range", "label_name_list.extend", "os.walk", "r_file_record.append", "p_f.write", "p_f.write", "open", "json.load", "print", "str", "len", "json.load.pop", "open", "json.dump", "os.renames", "range", "os.path.getsize", "os.walk", "dataset_generation.get_feature_flow", "len", "json.load.pop", "range", "open", "json.load", "open", "f.read().split", "len", "source_samples[].split", "open", "json.dump", "open", "f.read().split", "shutil.copyfile", "os.walk", "dataset_generation.get_feature_packet", "[].keys", "int", "print", "os.path.exists", "os.mkdir", "os.path.join", "dataset_generation.split_cap", "float", "os.walk", "str", "f.read", "f.read", "file.split", "file.split", "dataset_generation.size_format", "os.remove", "print", "str", "str", "str", "str", "str", "str", "file.split", "os.path.getsize", "os.path.getsize", "os.remove", "print", "scapy.rdpcap", "float", "dataset_generation.size_format", "str", "os.path.getsize", "str", "print", "os.remove", "print", "os.remove", "print", "str", "os.remove", "print"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.obtain_data", "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.obtain_data", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.load", "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.get_feature_flow", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.load", "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.get_feature_packet", "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.split_cap", "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.size_format", "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.size_format"], ["", "def", "generation", "(", "pcap_path", ",", "samples", ",", "features", ",", "splitcap", "=", "False", ",", "payload_length", "=", "128", ",", "payload_packet", "=", "5", ",", "dataset_save_path", "=", "\"I:\\\\ex_results\\\\\"", ",", "dataset_level", "=", "\"flow\"", ")", ":", "\n", "    ", "if", "os", ".", "path", ".", "exists", "(", "dataset_save_path", "+", "\"dataset.json\"", ")", ":", "\n", "        ", "print", "(", "\"the pcap file of %s is finished generating.\"", "%", "pcap_path", ")", "\n", "\n", "clean_dataset", "=", "0", "\n", "\n", "re_write", "=", "0", "\n", "\n", "if", "clean_dataset", ":", "\n", "            ", "with", "open", "(", "dataset_save_path", "+", "\"\\\\dataset.json\"", ",", "\"r\"", ")", "as", "f", ":", "\n", "                ", "new_dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "pop_keys", "=", "[", "'1'", ",", "'10'", ",", "'16'", ",", "'23'", ",", "'25'", ",", "'71'", "]", "\n", "print", "(", "\"delete domains.\"", ")", "\n", "for", "p_k", "in", "pop_keys", ":", "\n", "                ", "print", "(", "new_dataset", ".", "pop", "(", "p_k", ")", ")", "\n", "\n", "", "change_keys", "=", "[", "str", "(", "x", ")", "for", "x", "in", "range", "(", "113", ",", "119", ")", "]", "\n", "relation_dict", "=", "{", "}", "\n", "for", "c_k_index", "in", "range", "(", "len", "(", "change_keys", ")", ")", ":", "\n", "                ", "relation_dict", "[", "change_keys", "[", "c_k_index", "]", "]", "=", "pop_keys", "[", "c_k_index", "]", "\n", "new_dataset", "[", "pop_keys", "[", "c_k_index", "]", "]", "=", "new_dataset", ".", "pop", "(", "change_keys", "[", "c_k_index", "]", ")", "\n", "", "with", "open", "(", "dataset_save_path", "+", "\"\\\\dataset.json\"", ",", "\"w\"", ")", "as", "f", ":", "\n", "                ", "json", ".", "dump", "(", "new_dataset", ",", "fp", "=", "f", ",", "ensure_ascii", "=", "False", ",", "indent", "=", "4", ")", "\n", "", "", "elif", "re_write", ":", "\n", "            ", "with", "open", "(", "dataset_save_path", "+", "\"\\\\dataset.json\"", ",", "\"r\"", ")", "as", "f", ":", "\n", "                ", "old_dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "os", ".", "renames", "(", "dataset_save_path", "+", "\"\\\\dataset.json\"", ",", "dataset_save_path", "+", "\"\\\\old_dataset.json\"", ")", "\n", "with", "open", "(", "dataset_save_path", "+", "\"\\\\new-samples.txt\"", ",", "\"r\"", ")", "as", "f", ":", "\n", "                ", "source_samples", "=", "f", ".", "read", "(", ")", ".", "split", "(", "'\\n'", ")", "\n", "", "new_dataset", "=", "{", "}", "\n", "samples_count", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "source_samples", ")", ")", ":", "\n", "                ", "current_class", "=", "source_samples", "[", "i", "]", ".", "split", "(", "'\\t'", ")", "\n", "if", "int", "(", "current_class", "[", "1", "]", ")", ">", "9", ":", "\n", "                    ", "new_dataset", "[", "str", "(", "samples_count", ")", "]", "=", "old_dataset", "[", "str", "(", "i", ")", "]", "\n", "samples_count", "+=", "1", "\n", "print", "(", "old_dataset", "[", "str", "(", "i", ")", "]", "[", "'samples'", "]", ")", "\n", "", "", "with", "open", "(", "dataset_save_path", "+", "\"\\\\dataset.json\"", ",", "\"w\"", ")", "as", "f", ":", "\n", "                ", "json", ".", "dump", "(", "new_dataset", ",", "fp", "=", "f", ",", "ensure_ascii", "=", "False", ",", "indent", "=", "4", ")", "\n", "", "", "X", ",", "Y", "=", "obtain_data", "(", "pcap_path", ",", "samples", ",", "features", ",", "dataset_save_path", ")", "\n", "return", "X", ",", "Y", "\n", "\n", "", "dataset", "=", "{", "}", "\n", "\n", "label_name_list", "=", "[", "]", "\n", "\n", "session_pcap_path", "=", "{", "}", "\n", "\n", "for", "parent", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "pcap_path", ")", ":", "\n", "        ", "if", "label_name_list", "==", "[", "]", ":", "\n", "            ", "label_name_list", ".", "extend", "(", "dirs", ")", "\n", "\n", "", "tls13", "=", "0", "\n", "if", "tls13", ":", "\n", "            ", "record_file", "=", "\"I:\\\\ex_results\\\\picked_file_record\"", "\n", "target_path", "=", "\"I:\\\\ex_results\\\\packet_splitcap\\\\\"", "\n", "if", "not", "os", ".", "path", ".", "getsize", "(", "target_path", ")", ":", "\n", "                ", "with", "open", "(", "record_file", ",", "'r'", ")", "as", "f", ":", "\n", "                    ", "record_files", "=", "f", ".", "read", "(", ")", ".", "split", "(", "'\\n'", ")", "\n", "", "for", "file", "in", "record_files", "[", ":", "-", "2", "]", ":", "\n", "                    ", "current_path", "=", "target_path", "+", "file", ".", "split", "(", "'\\\\'", ")", "[", "5", "]", "\n", "new_name", "=", "'_'", ".", "join", "(", "file", ".", "split", "(", "'\\\\'", ")", "[", "6", ":", "]", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "current_path", ")", ":", "\n", "                        ", "os", ".", "mkdir", "(", "current_path", ")", "\n", "", "shutil", ".", "copyfile", "(", "file", ",", "os", ".", "path", ".", "join", "(", "current_path", ",", "new_name", ")", ")", "\n", "\n", "", "", "", "for", "dir", "in", "label_name_list", ":", "\n", "            ", "for", "p", ",", "dd", ",", "ff", "in", "os", ".", "walk", "(", "parent", "+", "\"\\\\\"", "+", "dir", ")", ":", "\n", "\n", "                ", "if", "splitcap", ":", "\n", "                    ", "for", "file", "in", "ff", ":", "\n", "                        ", "session_path", "=", "(", "split_cap", "(", "pcap_path", ",", "p", "+", "\"\\\\\"", "+", "file", ",", "file", ".", "split", "(", "\".\"", ")", "[", "-", "2", "]", ",", "dir", ",", "dataset_level", "=", "dataset_level", ")", ")", "\n", "", "session_pcap_path", "[", "dir", "]", "=", "pcap_path", "+", "\"\\\\splitcap\\\\\"", "+", "dir", "\n", "", "else", ":", "\n", "                    ", "session_pcap_path", "[", "dir", "]", "=", "pcap_path", "+", "dir", "\n", "", "", "", "break", "\n", "\n", "", "label_id", "=", "{", "}", "\n", "for", "index", "in", "range", "(", "len", "(", "label_name_list", ")", ")", ":", "\n", "        ", "label_id", "[", "label_name_list", "[", "index", "]", "]", "=", "index", "\n", "\n", "", "r_file_record", "=", "[", "]", "\n", "print", "(", "\"\\nBegin to generate features.\"", ")", "\n", "\n", "label_count", "=", "0", "\n", "for", "key", "in", "tqdm", ".", "tqdm", "(", "session_pcap_path", ".", "keys", "(", ")", ")", ":", "\n", "\n", "        ", "if", "dataset_level", "==", "\"flow\"", ":", "\n", "            ", "if", "splitcap", ":", "\n", "                ", "for", "p", ",", "d", ",", "f", "in", "os", ".", "walk", "(", "session_pcap_path", "[", "key", "]", ")", ":", "\n", "                    ", "for", "file", "in", "f", ":", "\n", "                        ", "file_size", "=", "float", "(", "size_format", "(", "os", ".", "path", ".", "getsize", "(", "p", "+", "\"\\\\\"", "+", "file", ")", ")", ")", "\n", "# 2KB", "\n", "if", "file_size", "<", "5", ":", "\n", "                            ", "os", ".", "remove", "(", "p", "+", "\"\\\\\"", "+", "file", ")", "\n", "print", "(", "\"remove sample: %s for its size is less than 5 KB.\"", "%", "(", "p", "+", "\"\\\\\"", "+", "file", ")", ")", "\n", "\n", "", "", "", "", "if", "label_id", "[", "key", "]", "not", "in", "dataset", ":", "\n", "                ", "dataset", "[", "label_id", "[", "key", "]", "]", "=", "{", "\n", "\"samples\"", ":", "0", ",", "\n", "\"payload\"", ":", "{", "}", ",", "\n", "\"length\"", ":", "{", "}", ",", "\n", "\"time\"", ":", "{", "}", ",", "\n", "\"direction\"", ":", "{", "}", ",", "\n", "\"message_type\"", ":", "{", "}", "\n", "}", "\n", "", "", "elif", "dataset_level", "==", "\"packet\"", ":", "\n", "            ", "if", "splitcap", ":", "# not splitcap", "\n", "                ", "for", "p", ",", "d", ",", "f", "in", "os", ".", "walk", "(", "session_pcap_path", "[", "key", "]", ")", ":", "\n", "                    ", "for", "file", "in", "f", ":", "\n", "                        ", "current_file", "=", "p", "+", "\"\\\\\"", "+", "file", "\n", "if", "not", "os", ".", "path", ".", "getsize", "(", "current_file", ")", ":", "\n", "                            ", "os", ".", "remove", "(", "current_file", ")", "\n", "print", "(", "\"current pcap %s is 0KB and delete\"", "%", "current_file", ")", "\n", "", "else", ":", "\n", "                            ", "current_packet", "=", "scapy", ".", "rdpcap", "(", "p", "+", "\"\\\\\"", "+", "file", ")", "\n", "file_size", "=", "float", "(", "size_format", "(", "os", ".", "path", ".", "getsize", "(", "p", "+", "\"\\\\\"", "+", "file", ")", ")", ")", "\n", "try", ":", "\n", "                                ", "if", "'TCP'", "in", "str", "(", "current_packet", ".", "res", ")", ":", "\n", "# 0.12KB", "\n", "                                    ", "if", "file_size", "<", "0.14", ":", "\n", "                                        ", "os", ".", "remove", "(", "p", "+", "\"\\\\\"", "+", "file", ")", "\n", "print", "(", "\"remove TCP sample: %s for its size is less than 0.14KB.\"", "%", "(", "\n", "p", "+", "\"\\\\\"", "+", "file", ")", ")", "\n", "", "", "elif", "'UDP'", "in", "str", "(", "current_packet", ".", "res", ")", ":", "\n", "                                    ", "if", "file_size", "<", "0.1", ":", "\n", "                                        ", "os", ".", "remove", "(", "p", "+", "\"\\\\\"", "+", "file", ")", "\n", "print", "(", "\"remove UDP sample: %s for its size is less than 0.1KB.\"", "%", "(", "\n", "p", "+", "\"\\\\\"", "+", "file", ")", ")", "\n", "", "", "", "except", "Exception", "as", "e", ":", "\n", "                                ", "print", "(", "\"error in data_generation 611: scapy read pcap and analyse error\"", ")", "\n", "os", ".", "remove", "(", "p", "+", "\"\\\\\"", "+", "file", ")", "\n", "print", "(", "\"remove packet sample: %s for reading error.\"", "%", "(", "p", "+", "\"\\\\\"", "+", "file", ")", ")", "\n", "", "", "", "", "", "if", "label_id", "[", "key", "]", "not", "in", "dataset", ":", "\n", "                ", "dataset", "[", "label_id", "[", "key", "]", "]", "=", "{", "\n", "\"samples\"", ":", "0", ",", "\n", "\"payload\"", ":", "{", "}", "\n", "}", "\n", "", "", "if", "splitcap", ":", "\n", "            ", "continue", "\n", "\n", "", "target_all_files", "=", "[", "x", "[", "0", "]", "+", "\"\\\\\"", "+", "y", "for", "x", "in", "[", "(", "p", ",", "f", ")", "for", "p", ",", "d", ",", "f", "in", "os", ".", "walk", "(", "session_pcap_path", "[", "key", "]", ")", "]", "for", "y", "in", "x", "[", "1", "]", "]", "\n", "r_files", "=", "random", ".", "sample", "(", "target_all_files", ",", "samples", "[", "label_count", "]", ")", "\n", "label_count", "+=", "1", "\n", "for", "r_f", "in", "r_files", ":", "\n", "            ", "if", "dataset_level", "==", "\"flow\"", ":", "\n", "                ", "feature_data", "=", "get_feature_flow", "(", "r_f", ",", "payload_len", "=", "payload_length", ",", "payload_pac", "=", "payload_packet", ")", "\n", "", "elif", "dataset_level", "==", "\"packet\"", ":", "\n", "                ", "feature_data", "=", "get_feature_packet", "(", "r_f", ",", "payload_len", "=", "payload_length", ")", "\n", "\n", "", "if", "feature_data", "==", "-", "1", ":", "\n", "                ", "continue", "\n", "", "r_file_record", ".", "append", "(", "r_f", ")", "\n", "dataset", "[", "label_id", "[", "key", "]", "]", "[", "\"samples\"", "]", "+=", "1", "\n", "if", "len", "(", "dataset", "[", "label_id", "[", "key", "]", "]", "[", "\"payload\"", "]", ".", "keys", "(", ")", ")", ">", "0", ":", "\n", "                ", "dataset", "[", "label_id", "[", "key", "]", "]", "[", "\"payload\"", "]", "[", "str", "(", "dataset", "[", "label_id", "[", "key", "]", "]", "[", "\"samples\"", "]", ")", "]", "=", "feature_data", "[", "0", "]", "\n", "if", "dataset_level", "==", "\"flow\"", ":", "\n", "                    ", "dataset", "[", "label_id", "[", "key", "]", "]", "[", "\"length\"", "]", "[", "str", "(", "dataset", "[", "label_id", "[", "key", "]", "]", "[", "\"samples\"", "]", ")", "]", "=", "feature_data", "[", "1", "]", "\n", "dataset", "[", "label_id", "[", "key", "]", "]", "[", "\"time\"", "]", "[", "str", "(", "dataset", "[", "label_id", "[", "key", "]", "]", "[", "\"samples\"", "]", ")", "]", "=", "feature_data", "[", "2", "]", "\n", "dataset", "[", "label_id", "[", "key", "]", "]", "[", "\"direction\"", "]", "[", "str", "(", "dataset", "[", "label_id", "[", "key", "]", "]", "[", "\"samples\"", "]", ")", "]", "=", "feature_data", "[", "3", "]", "\n", "dataset", "[", "label_id", "[", "key", "]", "]", "[", "\"message_type\"", "]", "[", "str", "(", "dataset", "[", "label_id", "[", "key", "]", "]", "[", "\"samples\"", "]", ")", "]", "=", "feature_data", "[", "4", "]", "\n", "", "", "else", ":", "\n", "                ", "dataset", "[", "label_id", "[", "key", "]", "]", "[", "\"payload\"", "]", "[", "\"1\"", "]", "=", "feature_data", "[", "0", "]", "\n", "if", "dataset_level", "==", "\"flow\"", ":", "\n", "                    ", "dataset", "[", "label_id", "[", "key", "]", "]", "[", "\"length\"", "]", "[", "\"1\"", "]", "=", "feature_data", "[", "1", "]", "\n", "dataset", "[", "label_id", "[", "key", "]", "]", "[", "\"time\"", "]", "[", "\"1\"", "]", "=", "feature_data", "[", "2", "]", "\n", "dataset", "[", "label_id", "[", "key", "]", "]", "[", "\"direction\"", "]", "[", "\"1\"", "]", "=", "feature_data", "[", "3", "]", "\n", "dataset", "[", "label_id", "[", "key", "]", "]", "[", "\"message_type\"", "]", "[", "\"1\"", "]", "=", "feature_data", "[", "4", "]", "\n", "\n", "", "", "", "", "all_data_number", "=", "0", "\n", "for", "index", "in", "range", "(", "len", "(", "label_name_list", ")", ")", ":", "\n", "        ", "print", "(", "\"%s\\t%s\\t%d\"", "%", "(", "label_id", "[", "label_name_list", "[", "index", "]", "]", ",", "label_name_list", "[", "index", "]", ",", "dataset", "[", "label_id", "[", "label_name_list", "[", "index", "]", "]", "]", "[", "\"samples\"", "]", ")", ")", "\n", "all_data_number", "+=", "dataset", "[", "label_id", "[", "label_name_list", "[", "index", "]", "]", "]", "[", "\"samples\"", "]", "\n", "", "print", "(", "\"all\\t%d\"", "%", "(", "all_data_number", ")", ")", "\n", "\n", "with", "open", "(", "dataset_save_path", "+", "\"\\\\picked_file_record\"", ",", "\"w\"", ")", "as", "p_f", ":", "\n", "        ", "for", "i", "in", "r_file_record", ":", "\n", "            ", "p_f", ".", "write", "(", "i", ")", "\n", "p_f", ".", "write", "(", "\"\\n\"", ")", "\n", "", "", "with", "open", "(", "dataset_save_path", "+", "\"\\\\dataset.json\"", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "dataset", ",", "fp", "=", "f", ",", "ensure_ascii", "=", "False", ",", "indent", "=", "4", ")", "\n", "\n", "", "X", ",", "Y", "=", "obtain_data", "(", "pcap_path", ",", "samples", ",", "features", ",", "dataset_save_path", ",", "json_data", "=", "dataset", ")", "\n", "return", "X", ",", "Y", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.read_data_from_json": [[513, 544], ["range", "len", "json_data.keys", "X.append", "random.sample", "x.append", "[].keys", "x.append", "Y.append", "Y.append", "list", "x_label.append", "x_label.append", "[].keys"], "function", ["None"], ["", "def", "read_data_from_json", "(", "json_data", ",", "features", ",", "samples", ")", ":", "\n", "    ", "X", ",", "Y", "=", "[", "]", ",", "[", "]", "\n", "ablation_flag", "=", "0", "\n", "for", "feature_index", "in", "range", "(", "len", "(", "features", ")", ")", ":", "\n", "        ", "x", "=", "[", "]", "\n", "label_count", "=", "0", "\n", "for", "label", "in", "json_data", ".", "keys", "(", ")", ":", "\n", "            ", "sample_num", "=", "json_data", "[", "label", "]", "[", "\"samples\"", "]", "\n", "if", "X", "==", "[", "]", ":", "\n", "                ", "if", "not", "ablation_flag", ":", "\n", "                    ", "y", "=", "[", "label", "]", "*", "sample_num", "\n", "Y", ".", "append", "(", "y", ")", "\n", "", "else", ":", "\n", "                    ", "if", "sample_num", ">", "1500", ":", "\n", "                        ", "y", "=", "[", "label", "]", "*", "1500", "\n", "", "else", ":", "\n", "                        ", "y", "=", "[", "label", "]", "*", "sample_num", "\n", "", "Y", ".", "append", "(", "y", ")", "\n", "", "", "if", "samples", "[", "label_count", "]", "<", "sample_num", ":", "\n", "                ", "x_label", "=", "[", "]", "\n", "for", "sample_index", "in", "random", ".", "sample", "(", "list", "(", "json_data", "[", "label", "]", "[", "features", "[", "feature_index", "]", "]", ".", "keys", "(", ")", ")", ",", "1500", ")", ":", "\n", "                    ", "x_label", ".", "append", "(", "json_data", "[", "label", "]", "[", "features", "[", "feature_index", "]", "]", "[", "sample_index", "]", ")", "\n", "", "x", ".", "append", "(", "x_label", ")", "\n", "", "else", ":", "\n", "                ", "x_label", "=", "[", "]", "\n", "for", "sample_index", "in", "json_data", "[", "label", "]", "[", "features", "[", "feature_index", "]", "]", ".", "keys", "(", ")", ":", "\n", "                    ", "x_label", ".", "append", "(", "json_data", "[", "label", "]", "[", "features", "[", "feature_index", "]", "]", "[", "sample_index", "]", ")", "\n", "", "x", ".", "append", "(", "x_label", ")", "\n", "", "label_count", "+=", "1", "\n", "", "X", ".", "append", "(", "x", ")", "\n", "", "return", "X", ",", "Y", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.obtain_data": [[545, 561], ["range", "dataset_generation.read_data_from_json", "print", "dataset_generation.read_data_from_json", "len", "open", "json.load", "len", "len", "print", "print", "len", "len"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.read_data_from_json", "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.read_data_from_json", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.load"], ["", "def", "obtain_data", "(", "pcap_path", ",", "samples", ",", "features", ",", "dataset_save_path", ",", "json_data", "=", "None", ")", ":", "\n", "\n", "    ", "if", "json_data", ":", "\n", "        ", "X", ",", "Y", "=", "read_data_from_json", "(", "json_data", ",", "features", ",", "samples", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"read dataset from json file.\"", ")", "\n", "with", "open", "(", "dataset_save_path", "+", "\"\\\\dataset.json\"", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "X", ",", "Y", "=", "read_data_from_json", "(", "dataset", ",", "features", ",", "samples", ")", "\n", "\n", "", "for", "index", "in", "range", "(", "len", "(", "X", ")", ")", ":", "\n", "        ", "if", "len", "(", "X", "[", "index", "]", ")", "!=", "len", "(", "Y", ")", ":", "\n", "            ", "print", "(", "\"data and labels are not properly associated.\"", ")", "\n", "print", "(", "\"x:%s\\ty:%s\"", "%", "(", "len", "(", "X", "[", "index", "]", ")", ",", "len", "(", "Y", ")", ")", ")", "\n", "return", "-", "1", "\n", "", "", "return", "X", ",", "Y", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.combine_dataset_json": [[562, 583], ["range", "json.load.keys", "open", "json.dump", "open", "json.load", "print", "str", "dataset.keys", "int", "int"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.load"], ["", "def", "combine_dataset_json", "(", ")", ":", "\n", "    ", "dataset_name", "=", "\"I:\\\\traffic_pcap\\\\splitcap\\\\dataset-\"", "\n", "# dataset vocab", "\n", "dataset", "=", "{", "}", "\n", "# progress", "\n", "progress_num", "=", "8", "\n", "for", "i", "in", "range", "(", "progress_num", ")", ":", "\n", "        ", "dataset_file", "=", "dataset_name", "+", "str", "(", "i", ")", "+", "\".json\"", "\n", "with", "open", "(", "dataset_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "json_data", "=", "json", ".", "load", "(", "f", ")", "\n", "", "for", "key", "in", "json_data", ".", "keys", "(", ")", ":", "\n", "            ", "if", "i", ">", "1", ":", "\n", "                ", "new_key", "=", "int", "(", "key", ")", "+", "9", "*", "1", "+", "6", "*", "(", "i", "-", "1", ")", "\n", "", "else", ":", "\n", "                ", "new_key", "=", "int", "(", "key", ")", "+", "9", "*", "i", "\n", "", "print", "(", "new_key", ")", "\n", "if", "new_key", "not", "in", "dataset", ".", "keys", "(", ")", ":", "\n", "                ", "dataset", "[", "new_key", "]", "=", "json_data", "[", "key", "]", "\n", "", "", "", "with", "open", "(", "\"I:\\\\traffic_pcap\\\\splitcap\\\\dataset.json\"", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "dataset", ",", "fp", "=", "f", ",", "ensure_ascii", "=", "False", ",", "indent", "=", "4", ")", "\n", "", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.pretrain_dataset_generation": [[584, 610], ["print", "os.walk", "os.listdir", "print", "os.walk", "os.path.exists", "print", "os.walk", "dataset_generation.get_burst_feature", "dataset_generation.split_cap", "dataset_generation.convert_pcapng_2_pcap", "shutil.copy"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.get_burst_feature", "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.split_cap", "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.convert_pcapng_2_pcap"], ["", "def", "pretrain_dataset_generation", "(", "pcap_path", ")", ":", "\n", "    ", "output_split_path", "=", "\"I:\\\\dataset\\\\\"", "\n", "pcap_output_path", "=", "\"I:\\\\dataset\\\\\"", "\n", "\n", "if", "not", "os", ".", "listdir", "(", "pcap_output_path", ")", ":", "\n", "        ", "print", "(", "\"Begin to convert pcapng to pcap.\"", ")", "\n", "for", "_parent", ",", "_dirs", ",", "files", "in", "os", ".", "walk", "(", "pcap_path", ")", ":", "\n", "            ", "for", "file", "in", "files", ":", "\n", "                ", "if", "'pcapng'", "in", "file", ":", "\n", "#print(_parent + file)", "\n", "                    ", "convert_pcapng_2_pcap", "(", "_parent", ",", "file", ",", "pcap_output_path", ")", "\n", "", "else", ":", "\n", "                    ", "shutil", ".", "copy", "(", "_parent", "+", "\"\\\\\"", "+", "file", ",", "pcap_output_path", "+", "file", ")", "\n", "\n", "", "", "", "", "if", "not", "os", ".", "path", ".", "exists", "(", "output_split_path", "+", "\"splitcap\"", ")", ":", "\n", "        ", "print", "(", "\"Begin to split pcap as session flows.\"", ")", "\n", "\n", "for", "_p", ",", "_d", ",", "files", "in", "os", ".", "walk", "(", "pcap_output_path", ")", ":", "\n", "            ", "for", "file", "in", "files", ":", "\n", "                ", "split_cap", "(", "output_split_path", ",", "_p", "+", "file", ",", "file", ")", "\n", "", "", "", "print", "(", "\"Begin to generate burst dataset.\"", ")", "\n", "# burst sample", "\n", "for", "_p", ",", "_d", ",", "files", "in", "os", ".", "walk", "(", "output_split_path", "+", "\"splitcap\"", ")", ":", "\n", "        ", "for", "file", "in", "files", ":", "\n", "            ", "get_burst_feature", "(", "_p", "+", "\"\\\\\"", "+", "file", ",", "payload_len", "=", "64", ")", "\n", "", "", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.size_format": [[611, 615], ["float"], "function", ["None"], ["", "def", "size_format", "(", "size", ")", ":", "\n", "# 'KB'", "\n", "    ", "file_size", "=", "'%.3f'", "%", "float", "(", "size", "/", "1000", ")", "\n", "return", "file_size", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.data_preprocess.combine_data_json": [[9, 50], ["json.load.keys", "random.sample", "json.load.keys", "random.sample", "open", "json.load", "open", "json.load", "[].keys", "[].keys", "[].keys", "[].keys", "open", "json.dump", "len", "len", "len", "len", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.load", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.load"], ["def", "combine_data_json", "(", ")", ":", "\n", "    ", "target_path", "=", "\"F:\\\\dataset\\\\\"", "\n", "target_file1", "=", "target_path", "+", "\"dataset_1.json\"", "\n", "target_file2", "=", "target_path", "+", "\"dataset_2.json\"", "\n", "save_file", "=", "target_path", "+", "\"dataset.json\"", "\n", "\n", "result_samples", "=", "{", "'0'", ":", "{", "'payload'", ":", "{", "}", "}", ",", "'1'", ":", "{", "'payload'", ":", "{", "}", "}", "}", "\n", "with", "open", "(", "target_file1", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "file1_json", "=", "json", ".", "load", "(", "f", ")", "\n", "", "with", "open", "(", "target_file2", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "file2_json", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "count", "=", "0", "\n", "for", "key", "in", "file1_json", ".", "keys", "(", ")", ":", "\n", "        ", "for", "item_key", "in", "file1_json", "[", "key", "]", "[", "'payload'", "]", ".", "keys", "(", ")", ":", "\n", "            ", "if", "len", "(", "file1_json", "[", "key", "]", "[", "'payload'", "]", "[", "item_key", "]", ")", ">", "100", ":", "\n", "                ", "count", "+=", "1", "\n", "result_samples", "[", "'0'", "]", "[", "'payload'", "]", "[", "str", "(", "count", ")", "]", "=", "file1_json", "[", "key", "]", "[", "'payload'", "]", "[", "item_key", "]", "\n", "", "", "", "file1_random_samples", "=", "random", ".", "sample", "(", "result_samples", "[", "'0'", "]", "[", "'payload'", "]", ".", "keys", "(", ")", ",", "5000", ")", "\n", "\n", "count", "=", "0", "\n", "for", "key", "in", "file2_json", ".", "keys", "(", ")", ":", "\n", "        ", "for", "item_key", "in", "file2_json", "[", "key", "]", "[", "'payload'", "]", ".", "keys", "(", ")", ":", "\n", "            ", "if", "len", "(", "file2_json", "[", "key", "]", "[", "'payload'", "]", "[", "item_key", "]", ")", ">", "100", ":", "\n", "                ", "count", "+=", "1", "\n", "result_samples", "[", "'1'", "]", "[", "'payload'", "]", "[", "str", "(", "count", ")", "]", "=", "file2_json", "[", "key", "]", "[", "'payload'", "]", "[", "item_key", "]", "\n", "", "", "", "file2_random_samples", "=", "random", ".", "sample", "(", "result_samples", "[", "'1'", "]", "[", "'payload'", "]", ".", "keys", "(", ")", ",", "5000", ")", "\n", "\n", "combined_result", "=", "{", "'0'", ":", "{", "'samples'", ":", "len", "(", "file1_random_samples", ")", ",", "'payload'", ":", "{", "}", "}", ",", "'1'", ":", "{", "'samples'", ":", "len", "(", "file2_random_samples", ")", ",", "'payload'", ":", "{", "}", "}", "}", "\n", "count", "=", "0", "\n", "for", "i", "in", "file1_random_samples", ":", "\n", "        ", "count", "+=", "1", "\n", "combined_result", "[", "'0'", "]", "[", "'payload'", "]", "[", "str", "(", "count", ")", "]", "=", "result_samples", "[", "'0'", "]", "[", "'payload'", "]", "[", "i", "]", "\n", "", "count", "=", "0", "\n", "for", "i", "in", "file2_random_samples", ":", "\n", "        ", "count", "+=", "1", "\n", "combined_result", "[", "'1'", "]", "[", "'payload'", "]", "[", "str", "(", "count", ")", "]", "=", "result_samples", "[", "'1'", "]", "[", "'payload'", "]", "[", "i", "]", "\n", "\n", "", "with", "open", "(", "save_file", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "combined_result", ",", "f", ",", "indent", "=", "4", ",", "ensure_ascii", "=", "False", ")", "\n", "", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.data_preprocess.basic_process_1": [[51, 96], ["range", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "len", "range", "len", "[].tolist", "[].tolist", "len", "data_preprocess.time_process_1", "bind_len_data_new[].append", "bind_time_data_new[].append", "[].tolist.remove", "[].tolist.remove"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.data_preprocess.time_process_1"], ["", "def", "basic_process_1", "(", "y_dict", ",", "x_dict", ")", ":", "\n", "    ", "x_len_train_new", "=", "[", "]", "\n", "x_len_test_new", "=", "[", "]", "\n", "x_len_valid_new", "=", "[", "]", "\n", "x_time_train_new", "=", "[", "]", "\n", "x_time_test_new", "=", "[", "]", "\n", "x_time_valid_new", "=", "[", "]", "\n", "\n", "bind_data", "=", "[", "y_dict", "[", "'train'", "]", ",", "y_dict", "[", "'test'", "]", ",", "y_dict", "[", "'valid'", "]", "]", "\n", "bind_len_data", "=", "[", "x_dict", "[", "'len_train'", "]", ",", "x_dict", "[", "'len_test'", "]", ",", "x_dict", "[", "'len_valid'", "]", "]", "\n", "bind_len_data_new", "=", "[", "x_len_train_new", ",", "x_len_test_new", ",", "x_len_valid_new", "]", "\n", "bind_time_data", "=", "[", "x_dict", "[", "'time_train'", "]", ",", "x_dict", "[", "'time_test'", "]", ",", "x_dict", "[", "'time_valid'", "]", "]", "\n", "bind_time_data_new", "=", "[", "x_time_train_new", ",", "x_time_test_new", ",", "x_time_valid_new", "]", "\n", "\n", "for", "data_index", "in", "range", "(", "len", "(", "bind_data", ")", ")", ":", "\n", "        ", "for", "item_index", "in", "range", "(", "len", "(", "bind_data", "[", "data_index", "]", ")", ")", ":", "\n", "\n", "            ", "i", "=", "0", "\n", "\n", "len_data", "=", "bind_len_data", "[", "data_index", "]", "[", "item_index", "]", ".", "tolist", "(", ")", "\n", "time_data", "=", "bind_time_data", "[", "data_index", "]", "[", "item_index", "]", ".", "tolist", "(", ")", "\n", "length", "=", "len", "(", "len_data", ")", "\n", "while", "i", "<", "length", ":", "\n", "                ", "if", "len_data", "[", "i", "]", "==", "0", ":", "\n", "                    ", "len_data", ".", "remove", "(", "0", ")", "\n", "if", "0", "in", "time_data", ":", "\n", "                        ", "time_data", ".", "remove", "(", "0", ")", "\n", "", "i", "-=", "1", "\n", "length", "-=", "1", "\n", "", "i", "+=", "1", "\n", "\n", "", "time_data_new", "=", "time_process_1", "(", "time_data", ")", "\n", "\n", "bind_len_data_new", "[", "data_index", "]", ".", "append", "(", "len_data", ")", "\n", "bind_time_data_new", "[", "data_index", "]", ".", "append", "(", "time_data_new", ")", "\n", "\n", "", "", "x_len_train_np", "=", "np", ".", "array", "(", "x_len_train_new", ")", "\n", "x_len_test_np", "=", "np", ".", "array", "(", "x_len_test_new", ")", "\n", "x_len_valid_np", "=", "np", ".", "array", "(", "x_len_valid_new", ")", "\n", "x_time_train_np", "=", "np", ".", "array", "(", "x_time_train_new", ")", "\n", "x_time_test_np", "=", "np", ".", "array", "(", "x_time_test_new", ")", "\n", "x_time_valid_np", "=", "np", ".", "array", "(", "x_time_valid_new", ")", "\n", "\n", "x_result", "=", "{", "'len_train'", ":", "x_len_train_np", ",", "'len_test'", ":", "x_len_test_np", ",", "'len_valid'", ":", "x_len_valid_np", ",", "'time_train'", ":", "x_time_train_np", ",", "'time_test'", ":", "x_time_test_np", ",", "'time_valid'", ":", "x_time_valid_np", "}", "\n", "return", "x_result", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.data_preprocess.time_process_1": [[97, 102], ["time_data_new.extend", "range", "len"], "function", ["None"], ["", "def", "time_process_1", "(", "time_data", ")", ":", "\n", "\n", "    ", "time_data_new", "=", "[", "0.0", "]", "\n", "time_data_new", ".", "extend", "(", "[", "time_data", "[", "time_index", "]", "-", "time_data", "[", "time_index", "-", "1", "]", "for", "time_index", "in", "range", "(", "1", ",", "len", "(", "time_data", ")", ")", "]", ")", "\n", "return", "time_data_new", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_cleanning.deal_label": [[6, 11], ["lower_label.extend", "lower_label.sort"], "function", ["None"], ["def", "deal_label", "(", ")", ":", "\n", "    ", "lower_label", "=", "[", "23", ",", "24", ",", "35", ",", "44", ",", "76", ",", "94", ",", "95", "]", "\n", "lower_label", ".", "extend", "(", "[", "22", ",", "28", ",", "52", ",", "62", ",", "67", ",", "102", ",", "104", "]", ")", "\n", "lower_label", ".", "sort", "(", ")", "\n", "return", "lower_label", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_cleanning.deal_finetuning": [[12, 85], ["input", "open", "open", "open", "range", "enumerate", "range", "enumerate", "range", "enumerate", "range", "range", "range", "open", "f.write", "open", "f.write", "open", "f.write", "ml_classifier.unlabel_data", "f.read().split", "f.read().split", "f.read().split", "len", "train_data.pop", "len", "valid_data.pop", "len", "test_data.pop", "len", "len", "len", "f.write", "f.write", "f.write", "train_pop_index.append", "valid_pop_index.append", "test_pop_index.append", "data.replace", "valid_data[].replace", "test_data[].replace", "f.read", "f.read", "f.read", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str"], "function", ["None"], ["", "def", "deal_finetuning", "(", "excluding_label", ")", ":", "\n", "    ", "dataset_path", "=", "\"I:\\\\datasets\\\\cstnet-tls1.3\\\\\"", "\n", "save_dataset_path", "=", "dataset_path", "\n", "with", "open", "(", "dataset_path", "+", "\"train_dataset.tsv\"", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "train_data", "=", "f", ".", "read", "(", ")", ".", "split", "(", "'\\n'", ")", "[", "1", ":", "]", "\n", "", "with", "open", "(", "dataset_path", "+", "\"valid_dataset.tsv\"", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "valid_data", "=", "f", ".", "read", "(", ")", ".", "split", "(", "'\\n'", ")", "[", "1", ":", "]", "\n", "", "with", "open", "(", "dataset_path", "+", "\"test_dataset.tsv\"", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "test_data", "=", "f", ".", "read", "(", ")", ".", "split", "(", "'\\n'", ")", "[", "1", ":", "]", "\n", "", "for", "label_number", "in", "excluding_label", ":", "\n", "        ", "train_pop_index", "=", "[", "]", "\n", "valid_pop_index", "=", "[", "]", "\n", "test_pop_index", "=", "[", "]", "\n", "for", "index", "in", "range", "(", "len", "(", "train_data", ")", ")", ":", "\n", "            ", "if", "str", "(", "label_number", ")", "+", "'\\t'", "in", "train_data", "[", "index", "]", ":", "\n", "                ", "train_pop_index", ".", "append", "(", "index", ")", "\n", "", "", "for", "counter", ",", "index", "in", "enumerate", "(", "train_pop_index", ")", ":", "\n", "            ", "index", "=", "index", "-", "counter", "\n", "train_data", ".", "pop", "(", "index", ")", "\n", "\n", "", "for", "index", "in", "range", "(", "len", "(", "valid_data", ")", ")", ":", "\n", "            ", "if", "str", "(", "label_number", ")", "+", "'\\t'", "in", "valid_data", "[", "index", "]", ":", "\n", "                ", "valid_pop_index", ".", "append", "(", "index", ")", "\n", "", "", "for", "counter", ",", "index", "in", "enumerate", "(", "valid_pop_index", ")", ":", "\n", "            ", "index", "=", "index", "-", "counter", "\n", "valid_data", ".", "pop", "(", "index", ")", "\n", "\n", "", "for", "index", "in", "range", "(", "len", "(", "test_data", ")", ")", ":", "\n", "            ", "if", "str", "(", "label_number", ")", "+", "'\\t'", "in", "test_data", "[", "index", "]", ":", "\n", "                ", "test_pop_index", ".", "append", "(", "index", ")", "\n", "", "", "for", "counter", ",", "index", "in", "enumerate", "(", "test_pop_index", ")", ":", "\n", "            ", "index", "=", "index", "-", "counter", "\n", "test_data", ".", "pop", "(", "index", ")", "\n", "\n", "", "", "label_number", "=", "120", "\n", "count", "=", "0", "\n", "while", "label_number", ">", "105", ":", "\n", "        ", "for", "index", "in", "range", "(", "len", "(", "train_data", ")", ")", ":", "\n", "            ", "data", "=", "train_data", "[", "index", "]", "\n", "if", "str", "(", "label_number", ")", "+", "'\\t'", "in", "data", ":", "\n", "                ", "new_data", "=", "data", ".", "replace", "(", "str", "(", "label_number", ")", "+", "'\\t'", ",", "str", "(", "excluding_label", "[", "count", "]", ")", "+", "'\\t'", ")", "\n", "train_data", "[", "index", "]", "=", "new_data", "\n", "\n", "", "", "for", "index", "in", "range", "(", "len", "(", "valid_data", ")", ")", ":", "\n", "            ", "if", "str", "(", "label_number", ")", "+", "'\\t'", "in", "valid_data", "[", "index", "]", ":", "\n", "                ", "new_data", "=", "valid_data", "[", "index", "]", ".", "replace", "(", "str", "(", "label_number", ")", "+", "'\\t'", ",", "str", "(", "excluding_label", "[", "count", "]", ")", "+", "'\\t'", ")", "\n", "valid_data", "[", "index", "]", "=", "new_data", "\n", "\n", "", "", "for", "index", "in", "range", "(", "len", "(", "test_data", ")", ")", ":", "\n", "            ", "if", "str", "(", "label_number", ")", "+", "'\\t'", "in", "test_data", "[", "index", "]", ":", "\n", "                ", "new_data", "=", "test_data", "[", "index", "]", ".", "replace", "(", "str", "(", "label_number", ")", "+", "'\\t'", ",", "str", "(", "excluding_label", "[", "count", "]", ")", "+", "'\\t'", ")", "\n", "test_data", "[", "index", "]", "=", "new_data", "\n", "\n", "", "", "label_number", "-=", "1", "\n", "count", "+=", "1", "\n", "\n", "", "with", "open", "(", "save_dataset_path", "+", "\"train_dataset.tsv\"", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "\"label\\ttext_a\\n\"", ")", "\n", "for", "data", "in", "train_data", ":", "\n", "            ", "f", ".", "write", "(", "data", "+", "'\\n'", ")", "\n", "", "", "with", "open", "(", "save_dataset_path", "+", "\"valid_dataset.tsv\"", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "\"label\\ttext_a\\n\"", ")", "\n", "for", "data", "in", "valid_data", ":", "\n", "            ", "f", ".", "write", "(", "data", "+", "'\\n'", ")", "\n", "", "", "with", "open", "(", "save_dataset_path", "+", "\"test_dataset.tsv\"", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "\"label\\ttext_a\\n\"", ")", "\n", "for", "data", "in", "test_data", ":", "\n", "            ", "f", ".", "write", "(", "data", "+", "'\\n'", ")", "\n", "\n", "", "", "deal_result", "=", "input", "(", "\"please delete the last blank line in %s and input '1'\"", "%", "(", "save_dataset_path", "+", "\"test_dataset.tsv\"", ")", ")", "\n", "if", "deal_result", "==", "'1'", ":", "\n", "        ", "unlabel_data", "(", "save_dataset_path", "+", "\"test_dataset.tsv\"", ")", "\n", "", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.inference.run_classifier_infer.batch_loader": [[23, 33], ["range", "src.size"], "function", ["None"], ["def", "batch_loader", "(", "batch_size", ",", "src", ",", "seg", ")", ":", "\n", "    ", "instances_num", "=", "src", ".", "size", "(", ")", "[", "0", "]", "\n", "for", "i", "in", "range", "(", "instances_num", "//", "batch_size", ")", ":", "\n", "        ", "src_batch", "=", "src", "[", "i", "*", "batch_size", ":", "(", "i", "+", "1", ")", "*", "batch_size", ",", ":", "]", "\n", "seg_batch", "=", "seg", "[", "i", "*", "batch_size", ":", "(", "i", "+", "1", ")", "*", "batch_size", ",", ":", "]", "\n", "yield", "src_batch", ",", "seg_batch", "\n", "", "if", "instances_num", ">", "instances_num", "//", "batch_size", "*", "batch_size", ":", "\n", "        ", "src_batch", "=", "src", "[", "instances_num", "//", "batch_size", "*", "batch_size", ":", ",", ":", "]", "\n", "seg_batch", "=", "seg", "[", "instances_num", "//", "batch_size", "*", "batch_size", ":", ",", ":", "]", "\n", "yield", "src_batch", ",", "seg_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.inference.run_classifier_infer.read_dataset": [[35, 65], ["open", "enumerate", "line.strip().split.strip().split", "dataset.append", "line.strip().split.strip().split", "enumerate", "args.tokenizer.convert_tokens_to_ids", "args.tokenizer.convert_tokens_to_ids", "args.tokenizer.convert_tokens_to_ids", "len", "len", "args.tokenizer.convert_tokens_to_ids.append", "seg.append", "line.strip().split.strip", "len", "line.strip().split.strip", "args.tokenizer.tokenize", "args.tokenizer.tokenize", "len", "len", "args.tokenizer.tokenize"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_tokens_to_ids", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_tokens_to_ids", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_tokens_to_ids", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.WordpieceTokenizer.tokenize"], ["", "", "def", "read_dataset", "(", "args", ",", "path", ")", ":", "\n", "    ", "dataset", ",", "columns", "=", "[", "]", ",", "{", "}", "\n", "with", "open", "(", "path", ",", "mode", "=", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "for", "line_id", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "            ", "if", "line_id", "==", "0", ":", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "for", "i", ",", "column_name", "in", "enumerate", "(", "line", ")", ":", "\n", "                    ", "columns", "[", "column_name", "]", "=", "i", "\n", "", "continue", "\n", "", "line", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "if", "\"text_b\"", "not", "in", "columns", ":", "# Sentence classification.", "\n", "                ", "text_a", "=", "line", "[", "columns", "[", "\"text_a\"", "]", "]", "\n", "src", "=", "args", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "CLS_TOKEN", "]", "+", "args", ".", "tokenizer", ".", "tokenize", "(", "text_a", ")", ")", "\n", "seg", "=", "[", "1", "]", "*", "len", "(", "src", ")", "\n", "", "else", ":", "# Sentence pair classification.", "\n", "                ", "text_a", ",", "text_b", "=", "line", "[", "columns", "[", "\"text_a\"", "]", "]", ",", "line", "[", "columns", "[", "\"text_b\"", "]", "]", "\n", "src_a", "=", "args", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "CLS_TOKEN", "]", "+", "args", ".", "tokenizer", ".", "tokenize", "(", "text_a", ")", "+", "[", "SEP_TOKEN", "]", ")", "\n", "src_b", "=", "args", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "args", ".", "tokenizer", ".", "tokenize", "(", "text_b", ")", "+", "[", "SEP_TOKEN", "]", ")", "\n", "src", "=", "src_a", "+", "src_b", "\n", "seg", "=", "[", "1", "]", "*", "len", "(", "src_a", ")", "+", "[", "2", "]", "*", "len", "(", "src_b", ")", "\n", "\n", "", "if", "len", "(", "src", ")", ">", "args", ".", "seq_length", ":", "\n", "                ", "src", "=", "src", "[", ":", "args", ".", "seq_length", "]", "\n", "seg", "=", "seg", "[", ":", "args", ".", "seq_length", "]", "\n", "", "while", "len", "(", "src", ")", "<", "args", ".", "seq_length", ":", "\n", "                ", "src", ".", "append", "(", "0", ")", "\n", "seg", ".", "append", "(", "0", ")", "\n", "", "dataset", ".", "append", "(", "(", "src", ",", "seg", ")", ")", "\n", "\n", "", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.inference.run_classifier_infer.main": [[67, 146], ["argparse.ArgumentParser", "uer.opts.infer_opts", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "uer.utils.config.load_hyperparam", "run_classifier.Classifier", "uer.model_loader.load_model", "torch.device", "torch.device", "torch.nn.DataParallel.to", "run_classifier_infer.read_dataset", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "print", "torch.nn.DataParallel.eval", "torch.cuda.device_count", "torch.cuda.device_count", "print", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.LongTensor.size", "open", "f.write", "f.write", "enumerate", "torch.cuda.is_available", "torch.cuda.is_available", "f.write", "f.write", "run_classifier_infer.batch_loader", "src_batch.to.to", "seg_batch.to.to", "torch.argmax", "torch.argmax", "pred.cpu().numpy().tolist.cpu().numpy().tolist", "logits.cpu().numpy().tolist.cpu().numpy().tolist", "prob.cpu().numpy().tolist.cpu().numpy().tolist", "range", "torch.cuda.device_count", "torch.cuda.device_count", "torch.no_grad", "torch.no_grad", "torch.nn.DataParallel.", "torch.Softmax", "len", "f.write", "f.write", "pred.cpu().numpy().tolist.cpu().numpy", "logits.cpu().numpy().tolist.cpu().numpy", "prob.cpu().numpy().tolist.cpu().numpy", "str", "f.write", "f.write", "pred.cpu().numpy().tolist.cpu", "logits.cpu().numpy().tolist.cpu", "prob.cpu().numpy().tolist.cpu", "str", "str"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.opts.infer_opts", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.config.load_hyperparam", "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.model_loader.load_model", "home.repos.pwc.inspect_result.linwhitehat_et-bert.inference.run_classifier_infer.read_dataset", "home.repos.pwc.inspect_result.linwhitehat_et-bert.inference.run_classifier_infer.batch_loader"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "formatter_class", "=", "argparse", ".", "ArgumentDefaultsHelpFormatter", ")", "\n", "\n", "infer_opts", "(", "parser", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--pooling\"", ",", "choices", "=", "[", "\"mean\"", ",", "\"max\"", ",", "\"first\"", ",", "\"last\"", "]", ",", "default", "=", "\"first\"", ",", "\n", "help", "=", "\"Pooling type.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--labels_num\"", ",", "type", "=", "int", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Number of prediction labels.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--tokenizer\"", ",", "choices", "=", "[", "\"bert\"", ",", "\"char\"", ",", "\"space\"", "]", ",", "default", "=", "\"bert\"", ",", "\n", "help", "=", "\"Specify the tokenizer.\"", "\n", "\"Original Google BERT uses bert tokenizer on Chinese corpus.\"", "\n", "\"Char tokenizer segments sentences into characters.\"", "\n", "\"Space tokenizer segments sentences into words according to space.\"", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--output_logits\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Write logits to output file.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_prob\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Write probabilities to output file.\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# Load the hyperparameters from the config file.", "\n", "args", "=", "load_hyperparam", "(", "args", ")", "\n", "\n", "# Build tokenizer.", "\n", "args", ".", "tokenizer", "=", "str2tokenizer", "[", "args", ".", "tokenizer", "]", "(", "args", ")", "\n", "\n", "# Build classification model and load parameters.", "\n", "args", ".", "soft_targets", ",", "args", ".", "soft_alpha", "=", "False", ",", "False", "\n", "model", "=", "Classifier", "(", "args", ")", "\n", "model", "=", "load_model", "(", "model", ",", "args", ".", "load_model_path", ")", "\n", "\n", "# For simplicity, we use DataParallel wrapper to use multiple GPUs.", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "if", "torch", ".", "cuda", ".", "device_count", "(", ")", ">", "1", ":", "\n", "        ", "print", "(", "\"{} GPUs are available. Let's use them.\"", ".", "format", "(", "torch", ".", "cuda", ".", "device_count", "(", ")", ")", ")", "\n", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "", "dataset", "=", "read_dataset", "(", "args", ",", "args", ".", "test_path", ")", "\n", "\n", "src", "=", "torch", ".", "LongTensor", "(", "[", "sample", "[", "0", "]", "for", "sample", "in", "dataset", "]", ")", "\n", "seg", "=", "torch", ".", "LongTensor", "(", "[", "sample", "[", "1", "]", "for", "sample", "in", "dataset", "]", ")", "\n", "\n", "batch_size", "=", "args", ".", "batch_size", "\n", "instances_num", "=", "src", ".", "size", "(", ")", "[", "0", "]", "\n", "\n", "print", "(", "\"The number of prediction instances: \"", ",", "instances_num", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "with", "open", "(", "args", ".", "prediction_path", ",", "mode", "=", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "\"label\"", ")", "\n", "if", "args", ".", "output_logits", ":", "\n", "            ", "f", ".", "write", "(", "\"\\t\"", "+", "\"logits\"", ")", "\n", "", "if", "args", ".", "output_prob", ":", "\n", "            ", "f", ".", "write", "(", "\"\\t\"", "+", "\"prob\"", ")", "\n", "", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "for", "i", ",", "(", "src_batch", ",", "seg_batch", ")", "in", "enumerate", "(", "batch_loader", "(", "batch_size", ",", "src", ",", "seg", ")", ")", ":", "\n", "            ", "src_batch", "=", "src_batch", ".", "to", "(", "device", ")", "\n", "seg_batch", "=", "seg_batch", ".", "to", "(", "device", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "_", ",", "logits", "=", "model", "(", "src_batch", ",", "None", ",", "seg_batch", ")", "\n", "\n", "", "pred", "=", "torch", ".", "argmax", "(", "logits", ",", "dim", "=", "1", ")", "\n", "pred", "=", "pred", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "\n", "prob", "=", "nn", ".", "Softmax", "(", "dim", "=", "1", ")", "(", "logits", ")", "\n", "logits", "=", "logits", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "\n", "prob", "=", "prob", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "\n", "\n", "for", "j", "in", "range", "(", "len", "(", "pred", ")", ")", ":", "\n", "                ", "f", ".", "write", "(", "str", "(", "pred", "[", "j", "]", ")", ")", "\n", "if", "args", ".", "output_logits", ":", "\n", "                    ", "f", ".", "write", "(", "\"\\t\"", "+", "\" \"", ".", "join", "(", "[", "str", "(", "v", ")", "for", "v", "in", "logits", "[", "j", "]", "]", ")", ")", "\n", "", "if", "args", ".", "output_prob", ":", "\n", "                    ", "f", ".", "write", "(", "\"\\t\"", "+", "\" \"", ".", "join", "(", "[", "str", "(", "v", ")", "for", "v", "in", "prob", "[", "j", "]", "]", ")", ")", "\n", "", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.opts.model_opts": [[1, 31], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "function", ["None"], ["def", "model_opts", "(", "parser", ")", ":", "\n", "    ", "parser", ".", "add_argument", "(", "\"--embedding\"", ",", "choices", "=", "[", "\"word\"", ",", "\"word_pos\"", ",", "\"word_pos_seg\"", ",", "\"word_sinusoidalpos\"", "]", ",", "default", "=", "\"word_pos_seg\"", ",", "\n", "help", "=", "\"Emebdding type.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "type", "=", "int", ",", "default", "=", "512", ",", "\n", "help", "=", "\"Max sequence length for word embedding.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--relative_position_embedding\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Use relative position embedding.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--relative_attention_buckets_num\"", ",", "type", "=", "int", ",", "default", "=", "32", ",", "\n", "help", "=", "\"Buckets num of relative position embedding.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--remove_embedding_layernorm\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Remove layernorm on embedding.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--remove_attention_scale\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Remove attention scale.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--encoder\"", ",", "choices", "=", "[", "\"transformer\"", ",", "\"rnn\"", ",", "\"lstm\"", ",", "\"gru\"", ",", "\n", "\"birnn\"", ",", "\"bilstm\"", ",", "\"bigru\"", ",", "\n", "\"gatedcnn\"", "]", ",", "\n", "default", "=", "\"transformer\"", ",", "help", "=", "\"Encoder type.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mask\"", ",", "choices", "=", "[", "\"fully_visible\"", ",", "\"causal\"", ",", "\"causal_with_prefix\"", "]", ",", "default", "=", "\"fully_visible\"", ",", "\n", "help", "=", "\"Mask type.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--layernorm_positioning\"", ",", "choices", "=", "[", "\"pre\"", ",", "\"post\"", "]", ",", "default", "=", "\"post\"", ",", "\n", "help", "=", "\"Layernorm positioning.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--feed_forward\"", ",", "choices", "=", "[", "\"dense\"", ",", "\"gated\"", "]", ",", "default", "=", "\"dense\"", ",", "\n", "help", "=", "\"Feed forward type, specific to transformer model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--remove_transformer_bias\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Remove bias on transformer layers.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--layernorm\"", ",", "choices", "=", "[", "\"normal\"", ",", "\"t5\"", "]", ",", "default", "=", "\"normal\"", ",", "\n", "help", "=", "\"Layernorm type.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bidirectional\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Specific to recurrent model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--factorized_embedding_parameterization\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Factorized embedding parameterization.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--parameter_sharing\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Parameter sharing.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.opts.optimization_opts": [[33, 49], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "function", ["None"], ["", "def", "optimization_opts", "(", "parser", ")", ":", "\n", "    ", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "type", "=", "float", ",", "default", "=", "2e-5", ",", "\n", "help", "=", "\"Learning rate.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup\"", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "\n", "help", "=", "\"Warm up value.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--fp16\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--fp16_opt_level\"", ",", "choices", "=", "[", "\"O0\"", ",", "\"O1\"", ",", "\"O2\"", ",", "\"O3\"", "]", ",", "default", "=", "'O1'", ",", "\n", "help", "=", "\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"", "\n", "\"See details at https://nvidia.github.io/apex/amp.html\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--optimizer\"", ",", "choices", "=", "[", "\"adamw\"", ",", "\"adafactor\"", "]", ",", "\n", "default", "=", "\"adamw\"", ",", "\n", "help", "=", "\"Optimizer type.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--scheduler\"", ",", "choices", "=", "[", "\"linear\"", ",", "\"cosine\"", ",", "\"cosine_with_restarts\"", ",", "\"polynomial\"", ",", "\n", "\"constant\"", ",", "\"constant_with_warmup\"", "]", ",", "\n", "default", "=", "\"linear\"", ",", "help", "=", "\"Scheduler type.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.opts.training_opts": [[51, 64], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "function", ["None"], ["", "def", "training_opts", "(", "parser", ")", ":", "\n", "    ", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ",", "default", "=", "32", ",", "\n", "help", "=", "\"Batch size.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--seq_length\"", ",", "type", "=", "int", ",", "default", "=", "128", ",", "\n", "help", "=", "\"Sequence length.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dropout\"", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "\n", "help", "=", "\"Dropout.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--epochs_num\"", ",", "type", "=", "int", ",", "default", "=", "3", ",", "\n", "help", "=", "\"Number of epochs.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--report_steps\"", ",", "type", "=", "int", ",", "default", "=", "100", ",", "\n", "help", "=", "\"Specific steps to print prompt.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "type", "=", "int", ",", "default", "=", "7", ",", "\n", "help", "=", "\"Random seed.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.opts.finetune_opts": [[66, 93], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "opts.model_opts", "opts.optimization_opts", "opts.training_opts"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.opts.model_opts", "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.opts.optimization_opts", "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.opts.training_opts"], ["", "def", "finetune_opts", "(", "parser", ")", ":", "\n", "# Path options.", "\n", "    ", "parser", ".", "add_argument", "(", "\"--pretrained_model_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Path of the pretrained model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_model_path\"", ",", "default", "=", "\"models/finetuned_model.bin\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Path of the output model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--vocab_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Path of the vocabulary file.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--spm_model_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Path of the sentence piece model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_path\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of the trainset.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dev_path\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of the devset.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--test_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Path of the testset.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--config_path\"", ",", "default", "=", "\"models/bert/base_config.json\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Path of the config file.\"", ")", "\n", "\n", "# Model options.", "\n", "model_opts", "(", "parser", ")", "\n", "\n", "# Optimization options.", "\n", "optimization_opts", "(", "parser", ")", "\n", "\n", "# Training options.", "\n", "training_opts", "(", "parser", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.opts.infer_opts": [[95, 118], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "opts.model_opts", "parser.add_argument", "parser.add_argument"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.opts.model_opts"], ["", "def", "infer_opts", "(", "parser", ")", ":", "\n", "# Path options.", "\n", "    ", "parser", ".", "add_argument", "(", "\"--load_model_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Path of the input model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--vocab_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Path of the vocabulary file.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--spm_model_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Path of the sentence piece model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--test_path\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of the testset.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--prediction_path\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of the prediction file.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--config_path\"", ",", "default", "=", "\"models/bert/base_config.json\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Path of the config file.\"", ")", "\n", "\n", "# Model options.", "\n", "model_opts", "(", "parser", ")", "\n", "\n", "# Inference options.", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ",", "default", "=", "64", ",", "\n", "help", "=", "\"Batch size.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--seq_length\"", ",", "type", "=", "int", ",", "default", "=", "128", ",", "\n", "help", "=", "\"Sequence length.\"", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.Trainer.__init__": [[68, 83], ["time.time"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "self", ".", "current_step", "=", "1", "\n", "self", ".", "total_steps", "=", "args", ".", "total_steps", "\n", "self", ".", "accumulation_steps", "=", "args", ".", "accumulation_steps", "\n", "self", ".", "report_steps", "=", "args", ".", "report_steps", "\n", "self", ".", "save_checkpoint_steps", "=", "args", ".", "save_checkpoint_steps", "\n", "\n", "self", ".", "output_model_path", "=", "args", ".", "output_model_path", "\n", "\n", "self", ".", "start_time", "=", "time", ".", "time", "(", ")", "\n", "self", ".", "total_loss", "=", "0.0", "\n", "\n", "self", ".", "dist_train", "=", "args", ".", "dist_train", "\n", "self", ".", "batch_size", "=", "args", ".", "batch_size", "\n", "self", ".", "world_size", "=", "args", ".", "world_size", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.Trainer.forward_propagation": [[84, 87], ["None"], "methods", ["None"], ["", "def", "forward_propagation", "(", "self", ",", "batch", ",", "model", ")", ":", "\n", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.Trainer.report_and_reset_stats": [[88, 91], ["None"], "methods", ["None"], ["", "def", "report_and_reset_stats", "(", "self", ")", ":", "\n", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.Trainer.train": [[92, 127], ["model.train", "iter", "list", "batch[].size", "trainer.Trainer.forward_propagation", "next", "range", "trainer.Trainer.backward", "optimizer.step", "scheduler.step", "model.zero_grad", "trainer.Trainer.report_and_reset_stats", "time.time", "uer.model_saver.save_model", "len", "batch[].cuda", "args.amp.scale_loss", "scaled_loss.backward", "str"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.Trainer.train", "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.Seq2seqTrainer.forward_propagation", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.optimizers.Adafactor.step", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.optimizers.Adafactor.step", "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.Seq2seqTrainer.report_and_reset_stats", "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.model_saver.save_model"], ["", "def", "train", "(", "self", ",", "args", ",", "gpu_id", ",", "rank", ",", "loader", ",", "model", ",", "optimizer", ",", "scheduler", ")", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "loader_iter", "=", "iter", "(", "loader", ")", "\n", "while", "True", ":", "\n", "            ", "if", "self", ".", "current_step", "==", "self", ".", "total_steps", "+", "1", ":", "\n", "                ", "break", "\n", "", "batch", "=", "list", "(", "next", "(", "loader_iter", ")", ")", "\n", "self", ".", "seq_length", "=", "batch", "[", "0", "]", ".", "size", "(", "1", ")", "\n", "if", "gpu_id", "is", "not", "None", ":", "\n", "                ", "for", "i", "in", "range", "(", "len", "(", "batch", ")", ")", ":", "\n", "                    ", "batch", "[", "i", "]", "=", "batch", "[", "i", "]", ".", "cuda", "(", "gpu_id", ")", "\n", "\n", "", "", "loss", "=", "self", ".", "forward_propagation", "(", "batch", ",", "model", ")", "\n", "\n", "if", "args", ".", "fp16", ":", "\n", "                ", "with", "args", ".", "amp", ".", "scale_loss", "(", "loss", ",", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                    ", "scaled_loss", ".", "backward", "(", ")", "\n", "", "", "else", ":", "\n", "                ", "loss", ".", "backward", "(", ")", "\n", "\n", "", "if", "self", ".", "current_step", "%", "self", ".", "accumulation_steps", "==", "0", ":", "\n", "                ", "optimizer", ".", "step", "(", ")", "\n", "scheduler", ".", "step", "(", ")", "\n", "model", ".", "zero_grad", "(", ")", "\n", "\n", "", "if", "self", ".", "current_step", "%", "self", ".", "report_steps", "==", "0", "and", "(", "not", "self", ".", "dist_train", "or", "(", "self", ".", "dist_train", "and", "rank", "==", "0", ")", ")", ":", "\n", "                ", "self", ".", "report_and_reset_stats", "(", ")", "\n", "self", ".", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "", "if", "self", ".", "current_step", "%", "self", ".", "save_checkpoint_steps", "==", "0", "and", "(", "not", "self", ".", "dist_train", "or", "(", "self", ".", "dist_train", "and", "rank", "==", "0", ")", ")", ":", "\n", "                ", "save_model", "(", "model", ",", "self", ".", "output_model_path", "+", "\"-\"", "+", "str", "(", "self", ".", "current_step", ")", ")", "\n", "\n", "", "self", ".", "current_step", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.MlmTrainer.__init__": [[130, 134], ["trainer.Trainer.__init__"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "MlmTrainer", ",", "self", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "total_correct", "=", "0.0", "\n", "self", ".", "total_denominator", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.MlmTrainer.forward_propagation": [[135, 144], ["model", "loss.item", "correct.item", "denominator.item"], "methods", ["None"], ["", "def", "forward_propagation", "(", "self", ",", "batch", ",", "model", ")", ":", "\n", "        ", "src", ",", "tgt", ",", "seg", "=", "batch", "\n", "loss_info", "=", "model", "(", "src", ",", "tgt", ",", "seg", ")", "\n", "loss", ",", "correct", ",", "denominator", "=", "loss_info", "\n", "self", ".", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "self", ".", "total_correct", "+=", "correct", ".", "item", "(", ")", "\n", "self", ".", "total_denominator", "+=", "denominator", ".", "item", "(", ")", "\n", "loss", "=", "loss", "/", "self", ".", "accumulation_steps", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.MlmTrainer.report_and_reset_stats": [[145, 162], ["print", "time.time"], "methods", ["None"], ["", "def", "report_and_reset_stats", "(", "self", ")", ":", "\n", "        ", "done_tokens", "=", "self", ".", "batch_size", "*", "self", ".", "seq_length", "*", "self", ".", "report_steps", "\n", "if", "self", ".", "dist_train", ":", "\n", "            ", "done_tokens", "*=", "self", ".", "world_size", "\n", "", "print", "(", "\"| {:8d}/{:8d} steps\"", "\n", "\"| {:8.2f} tokens/s\"", "\n", "\"| loss {:7.2f}\"", "\n", "\"| acc: {:3.3f}\"", ".", "format", "(", "\n", "self", ".", "current_step", ",", "\n", "self", ".", "total_steps", ",", "\n", "done_tokens", "/", "(", "time", ".", "time", "(", ")", "-", "self", ".", "start_time", ")", ",", "\n", "self", ".", "total_loss", "/", "self", ".", "report_steps", ",", "\n", "self", ".", "total_correct", "/", "self", ".", "total_denominator", ")", ")", "\n", "\n", "self", ".", "total_loss", "=", "0.0", "\n", "self", ".", "total_correct", "=", "0.0", "\n", "self", ".", "total_denominator", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.BertTrainer.__init__": [[165, 174], ["trainer.Trainer.__init__"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "BertTrainer", ",", "self", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "total_loss_sp", "=", "0.0", "\n", "self", ".", "total_correct_sp", "=", "0.0", "\n", "self", ".", "total_instances", "=", "0.0", "\n", "\n", "self", ".", "total_loss_mlm", "=", "0.0", "\n", "self", ".", "total_correct_mlm", "=", "0.0", "\n", "self", ".", "total_denominator", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.BertTrainer.forward_propagation": [[175, 190], ["model", "loss.item", "loss_mlm.item", "loss_sp.item", "correct_mlm.item", "correct_sp.item", "denominator.item", "src.size"], "methods", ["None"], ["", "def", "forward_propagation", "(", "self", ",", "batch", ",", "model", ")", ":", "\n", "        ", "src", ",", "tgt_mlm", ",", "tgt_sp", ",", "seg", "=", "batch", "\n", "loss_info", "=", "model", "(", "src", ",", "(", "tgt_mlm", ",", "tgt_sp", ")", ",", "seg", ")", "\n", "loss_mlm", ",", "loss_sp", ",", "correct_mlm", ",", "correct_sp", ",", "denominator", "=", "loss_info", "\n", "loss", "=", "loss_mlm", "/", "10", "+", "loss_sp", "\n", "self", ".", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "self", ".", "total_loss_mlm", "+=", "loss_mlm", ".", "item", "(", ")", "\n", "self", ".", "total_loss_sp", "+=", "loss_sp", ".", "item", "(", ")", "\n", "self", ".", "total_correct_mlm", "+=", "correct_mlm", ".", "item", "(", ")", "\n", "self", ".", "total_correct_sp", "+=", "correct_sp", ".", "item", "(", ")", "\n", "self", ".", "total_denominator", "+=", "denominator", ".", "item", "(", ")", "\n", "self", ".", "total_instances", "+=", "src", ".", "size", "(", "0", ")", "\n", "loss", "=", "loss", "/", "self", ".", "accumulation_steps", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.BertTrainer.report_and_reset_stats": [[191, 215], ["print", "time.time"], "methods", ["None"], ["", "def", "report_and_reset_stats", "(", "self", ")", ":", "\n", "        ", "done_tokens", "=", "self", ".", "batch_size", "*", "self", ".", "seq_length", "*", "self", ".", "report_steps", "\n", "if", "self", ".", "dist_train", ":", "\n", "            ", "done_tokens", "*=", "self", ".", "world_size", "\n", "\n", "", "print", "(", "\"| {:8d}/{:8d} steps\"", "\n", "\"| {:8.2f} tokens/s\"", "\n", "\"| loss {:7.2f}\"", "\n", "\"| loss_mlm: {:3.3f}\"", "\n", "\"| loss_sp: {:3.3f}\"", "\n", "\"| acc_mlm: {:3.3f}\"", "\n", "\"| acc_sp: {:3.3f}\"", ".", "format", "(", "\n", "self", ".", "current_step", ",", "\n", "self", ".", "total_steps", ",", "\n", "done_tokens", "/", "(", "time", ".", "time", "(", ")", "-", "self", ".", "start_time", ")", ",", "\n", "self", ".", "total_loss", "/", "self", ".", "report_steps", ",", "\n", "self", ".", "total_loss_mlm", "/", "self", ".", "report_steps", ",", "\n", "self", ".", "total_loss_sp", "/", "self", ".", "report_steps", ",", "\n", "self", ".", "total_correct_mlm", "/", "self", ".", "total_denominator", ",", "\n", "self", ".", "total_correct_sp", "/", "self", ".", "total_instances", ")", ")", "\n", "\n", "self", ".", "total_loss", ",", "self", ".", "total_loss_mlm", ",", "self", ".", "total_loss_sp", "=", "0.0", ",", "0.0", ",", "0.0", "\n", "self", ".", "total_correct_mlm", ",", "self", ".", "total_denominator", "=", "0.0", ",", "0.0", "\n", "self", ".", "total_correct_sp", ",", "self", ".", "total_instances", "=", "0.0", ",", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.BilmTrainer.__init__": [[226, 231], ["trainer.Trainer.__init__"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "BilmTrainer", ",", "self", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "total_loss_forward", ",", "self", ".", "total_loss_backward", "=", "0.0", ",", "0.0", "\n", "self", ".", "total_correct_forward", ",", "self", ".", "total_correct_backward", "=", "0.0", ",", "0.0", "\n", "self", ".", "total_denominator", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.BilmTrainer.forward_propagation": [[232, 245], ["model", "loss.item", "loss_forward.item", "loss_backward.item", "correct_forward.item", "correct_backward.item", "denominator.item"], "methods", ["None"], ["", "def", "forward_propagation", "(", "self", ",", "batch", ",", "model", ")", ":", "\n", "        ", "src", ",", "tgt_forward", ",", "tgt_backward", ",", "seg", "=", "batch", "\n", "loss_info", "=", "model", "(", "src", ",", "(", "tgt_forward", ",", "tgt_backward", ")", ",", "seg", ")", "\n", "loss_forward", ",", "loss_backward", ",", "correct_forward", ",", "correct_backward", ",", "denominator", "=", "loss_info", "\n", "loss", "=", "loss_forward", "+", "loss_backward", "\n", "self", ".", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "self", ".", "total_loss_forward", "+=", "loss_forward", ".", "item", "(", ")", "\n", "self", ".", "total_loss_backward", "+=", "loss_backward", ".", "item", "(", ")", "\n", "self", ".", "total_correct_forward", "+=", "correct_forward", ".", "item", "(", ")", "\n", "self", ".", "total_correct_backward", "+=", "correct_backward", ".", "item", "(", ")", "\n", "self", ".", "total_denominator", "+=", "denominator", ".", "item", "(", ")", "\n", "loss", "=", "loss", "/", "self", ".", "accumulation_steps", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.BilmTrainer.report_and_reset_stats": [[246, 268], ["print", "time.time"], "methods", ["None"], ["", "def", "report_and_reset_stats", "(", "self", ")", ":", "\n", "        ", "done_tokens", "=", "self", ".", "batch_size", "*", "self", ".", "seq_length", "*", "self", ".", "report_steps", "\n", "if", "self", ".", "dist_train", ":", "\n", "            ", "done_tokens", "*=", "self", ".", "world_size", "\n", "", "print", "(", "\"| {:8d}/{:8d} steps\"", "\n", "\"| {:8.2f} tokens/s\"", "\n", "\"| loss {:7.2f}\"", "\n", "\"| loss_forward {:3.3f}\"", "\n", "\"| loss_backward {:3.3f}\"", "\n", "\"| acc_forward: {:3.3f}\"", "\n", "\"| acc_backward: {:3.3f}\"", ".", "format", "(", "\n", "self", ".", "current_step", ",", "\n", "self", ".", "total_steps", ",", "\n", "done_tokens", "/", "(", "time", ".", "time", "(", ")", "-", "self", ".", "start_time", ")", ",", "\n", "self", ".", "total_loss", "/", "self", ".", "report_steps", ",", "\n", "self", ".", "total_loss_forward", "/", "self", ".", "report_steps", ",", "\n", "self", ".", "total_loss_backward", "/", "self", ".", "report_steps", ",", "\n", "self", ".", "total_correct_forward", "/", "self", ".", "total_denominator", ",", "\n", "self", ".", "total_correct_backward", "/", "self", ".", "total_denominator", ")", ")", "\n", "\n", "self", ".", "total_loss", ",", "self", ".", "total_loss_forward", ",", "self", ".", "total_loss_backward", "=", "0.0", ",", "0.0", ",", "0.0", "\n", "self", ".", "total_correct_forward", ",", "self", ".", "total_correct_backward", ",", "self", ".", "total_denominator", "=", "0.0", ",", "0.0", ",", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.ClsTrainer.__init__": [[271, 275], ["trainer.Trainer.__init__"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "ClsTrainer", ",", "self", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "total_correct", "=", "0.0", "\n", "self", ".", "total_instances", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.ClsTrainer.forward_propagation": [[276, 285], ["model", "loss.item", "correct.item", "src.size"], "methods", ["None"], ["", "def", "forward_propagation", "(", "self", ",", "batch", ",", "model", ")", ":", "\n", "        ", "src", ",", "tgt", ",", "seg", "=", "batch", "\n", "loss_info", "=", "model", "(", "src", ",", "tgt", ",", "seg", ")", "\n", "loss", ",", "correct", "=", "loss_info", "\n", "self", ".", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "self", ".", "total_correct", "+=", "correct", ".", "item", "(", ")", "\n", "self", ".", "total_instances", "+=", "src", ".", "size", "(", "0", ")", "\n", "loss", "=", "loss", "/", "self", ".", "accumulation_steps", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.ClsTrainer.report_and_reset_stats": [[286, 303], ["print", "time.time"], "methods", ["None"], ["", "def", "report_and_reset_stats", "(", "self", ")", ":", "\n", "        ", "done_tokens", "=", "self", ".", "batch_size", "*", "self", ".", "seq_length", "*", "self", ".", "report_steps", "\n", "if", "self", ".", "dist_train", ":", "\n", "            ", "done_tokens", "*=", "self", ".", "world_size", "\n", "", "print", "(", "\"| {:8d}/{:8d} steps\"", "\n", "\"| {:8.2f} tokens/s\"", "\n", "\"| loss {:7.2f}\"", "\n", "\"| acc: {:3.3f}\"", ".", "format", "(", "\n", "self", ".", "current_step", ",", "\n", "self", ".", "total_steps", ",", "\n", "done_tokens", "/", "(", "time", ".", "time", "(", ")", "-", "self", ".", "start_time", ")", ",", "\n", "self", ".", "total_loss", "/", "self", ".", "report_steps", ",", "\n", "self", ".", "total_correct", "/", "self", ".", "total_instances", ")", ")", "\n", "\n", "self", ".", "total_loss", "=", "0.0", "\n", "self", ".", "total_correct", "=", "0.0", "\n", "self", ".", "total_instances", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.Seq2seqTrainer.__init__": [[306, 310], ["trainer.Trainer.__init__"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "Seq2seqTrainer", ",", "self", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "total_correct", "=", "0.0", "\n", "self", ".", "total_denominator", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.Seq2seqTrainer.forward_propagation": [[311, 322], ["model", "loss.item", "correct.item", "denominator.item"], "methods", ["None"], ["", "def", "forward_propagation", "(", "self", ",", "batch", ",", "model", ")", ":", "\n", "        ", "src", ",", "tgt_in", ",", "tgt_out", ",", "seg", "=", "batch", "\n", "loss_info", "=", "model", "(", "src", ",", "(", "tgt_in", ",", "tgt_out", ",", "src", ")", ",", "seg", ")", "\n", "loss", ",", "correct", ",", "denominator", "=", "loss_info", "\n", "self", ".", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "self", ".", "total_correct", "+=", "correct", ".", "item", "(", ")", "\n", "self", ".", "total_denominator", "+=", "denominator", ".", "item", "(", ")", "\n", "\n", "loss", "=", "loss", "/", "self", ".", "accumulation_steps", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.Seq2seqTrainer.report_and_reset_stats": [[323, 341], ["print", "time.time"], "methods", ["None"], ["", "def", "report_and_reset_stats", "(", "self", ")", ":", "\n", "        ", "done_tokens", "=", "self", ".", "batch_size", "*", "self", ".", "seq_length", "*", "self", ".", "report_steps", "\n", "if", "self", ".", "dist_train", ":", "\n", "            ", "done_tokens", "*=", "self", ".", "world_size", "\n", "\n", "", "print", "(", "\"| {:8d}/{:8d} steps\"", "\n", "\"| {:8.2f} tokens/s\"", "\n", "\"| loss {:7.2f}\"", "\n", "\"| acc: {:3.3f}\"", ".", "format", "(", "\n", "self", ".", "current_step", ",", "\n", "self", ".", "total_steps", ",", "\n", "done_tokens", "/", "(", "time", ".", "time", "(", ")", "-", "self", ".", "start_time", ")", ",", "\n", "self", ".", "total_loss", "/", "self", ".", "report_steps", ",", "\n", "self", ".", "total_correct", "/", "self", ".", "total_denominator", ")", ")", "\n", "\n", "self", ".", "total_loss", "=", "0.0", "\n", "self", ".", "total_correct", "=", "0.0", "\n", "self", ".", "total_denominator", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.train_and_validate": [[15, 65], ["uer.utils.seed.set_seed", "uer.model_builder.build_model", "spm.SentencePieceProcessor", "spm.SentencePieceProcessor.Load", "uer.model_loader.load_model", "list", "torch.spawn", "spm.SentencePieceProcessor.IdToPiece", "spm.SentencePieceProcessor", "spm.SentencePieceProcessor.Load", "uer.utils.vocab.Vocab", "uer.utils.vocab.Vocab.load", "uer.model_loader.load_model.named_parameters", "trainer.worker", "trainer.worker", "ImportError", "range", "spm.SentencePieceProcessor.IdToPiece", "p.data.normal_", "spm.SentencePieceProcessor.GetPieceSize", "range", "spm.SentencePieceProcessor.GetPieceSize"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.seed.set_seed", "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.model_builder.build_model", "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.model_loader.load_model", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.load", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.worker", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.worker"], ["def", "train_and_validate", "(", "args", ")", ":", "\n", "    ", "set_seed", "(", "args", ".", "seed", ")", "\n", "\n", "# Load vocabulary.", "\n", "if", "args", ".", "spm_model_path", ":", "\n", "        ", "try", ":", "\n", "            ", "import", "sentencepiece", "as", "spm", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"You need to install SentencePiece to use XLNetTokenizer: https://github.com/google/sentencepiece\"", "\n", "\"pip install sentencepiece\"", ")", "\n", "", "sp_model", "=", "spm", ".", "SentencePieceProcessor", "(", ")", "\n", "sp_model", ".", "Load", "(", "args", ".", "spm_model_path", ")", "\n", "args", ".", "vocab", "=", "{", "sp_model", ".", "IdToPiece", "(", "i", ")", ":", "i", "for", "i", "\n", "in", "range", "(", "sp_model", ".", "GetPieceSize", "(", ")", ")", "}", "\n", "args", ".", "tokenizer", "=", "str2tokenizer", "[", "args", ".", "tokenizer", "]", "(", "args", ")", "\n", "if", "args", ".", "target", "==", "\"seq2seq\"", ":", "\n", "            ", "tgt_sp_model", "=", "spm", ".", "SentencePieceProcessor", "(", ")", "\n", "tgt_sp_model", ".", "Load", "(", "args", ".", "tgt_spm_model_path", ")", "\n", "args", ".", "tgt_vocab", "=", "{", "tgt_sp_model", ".", "IdToPiece", "(", "i", ")", ":", "i", "for", "i", "\n", "in", "range", "(", "tgt_sp_model", ".", "GetPieceSize", "(", ")", ")", "}", "\n", "", "", "else", ":", "\n", "        ", "args", ".", "tokenizer", "=", "str2tokenizer", "[", "args", ".", "tokenizer", "]", "(", "args", ")", "\n", "args", ".", "vocab", "=", "args", ".", "tokenizer", ".", "vocab", "\n", "if", "args", ".", "target", "==", "\"seq2seq\"", ":", "\n", "            ", "tgt_vocab", "=", "Vocab", "(", ")", "\n", "tgt_vocab", ".", "load", "(", "args", ".", "tgt_vocab_path", ")", "\n", "args", ".", "tgt_vocab", "=", "tgt_vocab", ".", "w2i", "\n", "\n", "# Build model.", "\n", "", "", "model", "=", "build_model", "(", "args", ")", "\n", "\n", "# Load or initialize parameters.", "\n", "if", "args", ".", "pretrained_model_path", "is", "not", "None", ":", "\n", "# Initialize with pretrained model.", "\n", "        ", "model", "=", "load_model", "(", "model", ",", "args", ".", "pretrained_model_path", ")", "\n", "", "else", ":", "\n", "# Initialize with normal distribution.", "\n", "        ", "for", "n", ",", "p", "in", "list", "(", "model", ".", "named_parameters", "(", ")", ")", ":", "\n", "            ", "if", "\"gamma\"", "not", "in", "n", "and", "\"beta\"", "not", "in", "n", ":", "\n", "                ", "p", ".", "data", ".", "normal_", "(", "0", ",", "0.02", ")", "\n", "\n", "", "", "", "if", "args", ".", "dist_train", ":", "\n", "# Multiprocessing distributed mode.", "\n", "        ", "mp", ".", "spawn", "(", "worker", ",", "nprocs", "=", "args", ".", "ranks_num", ",", "args", "=", "(", "args", ".", "gpu_ranks", ",", "args", ",", "model", ")", ",", "daemon", "=", "False", ")", "\n", "", "elif", "args", ".", "single_gpu", ":", "\n", "# Single GPU mode.", "\n", "        ", "worker", "(", "args", ".", "gpu_id", ",", "None", ",", "args", ",", "model", ")", "\n", "", "else", ":", "\n", "# CPU mode.", "\n", "        ", "worker", "(", "None", ",", "None", ",", "args", ",", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.worker": [[355, 423], ["uer.utils.seed.set_seed", "list", "trainer.train", "torch.cuda.set_device", "torch.cuda.set_device", "torch.cuda.set_device", "torch.nn.parallel.DistributedDataParallel.cuda", "torch.nn.parallel.DistributedDataParallel.named_parameters", "amp.initialize", "torch.init_process_group", "torch.nn.parallel.DistributedDataParallel", "print", "print", "ImportError", "any", "any"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.seed.set_seed", "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.trainer.Trainer.train"], ["def", "worker", "(", "proc_id", ",", "gpu_ranks", ",", "args", ",", "model", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        proc_id: The id of GPU for single GPU mode;\n                 The id of process (and GPU) for multiprocessing distributed mode.\n        gpu_ranks: List of ranks of each process.\n    \"\"\"", "\n", "set_seed", "(", "args", ".", "seed", ")", "\n", "\n", "if", "args", ".", "dist_train", ":", "\n", "        ", "rank", "=", "gpu_ranks", "[", "proc_id", "]", "\n", "gpu_id", "=", "proc_id", "\n", "", "elif", "args", ".", "single_gpu", ":", "\n", "        ", "rank", "=", "None", "\n", "gpu_id", "=", "proc_id", "\n", "", "else", ":", "\n", "        ", "rank", "=", "None", "\n", "gpu_id", "=", "None", "\n", "\n", "", "if", "args", ".", "dist_train", ":", "\n", "        ", "train_loader", "=", "str2dataloader", "[", "args", ".", "target", "]", "(", "args", ",", "args", ".", "dataset_path", ",", "args", ".", "batch_size", ",", "rank", ",", "args", ".", "world_size", ",", "True", ")", "\n", "", "else", ":", "\n", "        ", "train_loader", "=", "str2dataloader", "[", "args", ".", "target", "]", "(", "args", ",", "args", ".", "dataset_path", ",", "args", ".", "batch_size", ",", "0", ",", "1", ",", "True", ")", "\n", "\n", "", "if", "gpu_id", "is", "not", "None", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "gpu_id", ")", "\n", "model", ".", "cuda", "(", "gpu_id", ")", "\n", "\n", "# Build optimizer.", "\n", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "\"bias\"", ",", "\"gamma\"", ",", "\"beta\"", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\"weight_decay_rate\"", ":", "0.01", "}", ",", "\n", "{", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\"weight_decay_rate\"", ":", "0.0", "}", "\n", "]", "\n", "if", "args", ".", "optimizer", "in", "[", "\"adamw\"", "]", ":", "\n", "        ", "optimizer", "=", "str2optimizer", "[", "args", ".", "optimizer", "]", "(", "optimizer_grouped_parameters", ",", "lr", "=", "args", ".", "learning_rate", ",", "correct_bias", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "optimizer", "=", "str2optimizer", "[", "args", ".", "optimizer", "]", "(", "optimizer_grouped_parameters", ",", "lr", "=", "args", ".", "learning_rate", ",", "\n", "scale_parameter", "=", "False", ",", "relative_step", "=", "False", ")", "\n", "", "if", "args", ".", "scheduler", "in", "[", "\"constant\"", "]", ":", "\n", "        ", "scheduler", "=", "str2scheduler", "[", "args", ".", "scheduler", "]", "(", "optimizer", ")", "\n", "", "elif", "args", ".", "scheduler", "in", "[", "\"constant_with_warmup\"", "]", ":", "\n", "        ", "scheduler", "=", "str2scheduler", "[", "args", ".", "scheduler", "]", "(", "optimizer", ",", "args", ".", "total_steps", "*", "args", ".", "warmup", ")", "\n", "", "else", ":", "\n", "        ", "scheduler", "=", "str2scheduler", "[", "args", ".", "scheduler", "]", "(", "optimizer", ",", "args", ".", "total_steps", "*", "args", ".", "warmup", ",", "args", ".", "total_steps", ")", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", "import", "amp", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"", ")", "\n", "", "model", ",", "optimizer", "=", "amp", ".", "initialize", "(", "model", ",", "optimizer", ",", "opt_level", "=", "args", ".", "fp16_opt_level", ")", "\n", "args", ".", "amp", "=", "amp", "\n", "\n", "", "if", "args", ".", "dist_train", ":", "\n", "# Initialize multiprocessing distributed training environment.", "\n", "        ", "dist", ".", "init_process_group", "(", "backend", "=", "args", ".", "backend", ",", "\n", "init_method", "=", "args", ".", "master_ip", ",", "\n", "world_size", "=", "args", ".", "world_size", ",", "\n", "rank", "=", "rank", ")", "\n", "model", "=", "DistributedDataParallel", "(", "model", ",", "device_ids", "=", "[", "gpu_id", "]", ",", "find_unused_parameters", "=", "True", ")", "\n", "print", "(", "\"Worker %d is training ... \"", "%", "rank", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Worker is training ...\"", ")", "\n", "\n", "", "trainer", "=", "str2trainer", "[", "args", ".", "target", "]", "(", "args", ")", "\n", "trainer", ".", "train", "(", "args", ",", "gpu_id", ",", "rank", ",", "train_loader", ",", "model", ",", "optimizer", ",", "scheduler", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.model_builder.build_model": [[8, 23], ["uer.models.model.Model", "len", "len"], "function", ["None"], ["def", "build_model", "(", "args", ")", ":", "\n", "    ", "\"\"\"\n    Build universial encoder representations models.\n    The combinations of different embedding, encoder, \n    and target layers yield pretrained models of different \n    properties. \n    We could select suitable one for downstream tasks.\n    \"\"\"", "\n", "\n", "embedding", "=", "str2embedding", "[", "args", ".", "embedding", "]", "(", "args", ",", "len", "(", "args", ".", "vocab", ")", ")", "\n", "encoder", "=", "str2encoder", "[", "args", ".", "encoder", "]", "(", "args", ")", "\n", "target", "=", "str2target", "[", "args", ".", "target", "]", "(", "args", ",", "len", "(", "args", ".", "vocab", ")", ")", "\n", "model", "=", "Model", "(", "args", ",", "embedding", ",", "encoder", ",", "target", ")", "\n", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.model_loader.load_model": [[4, 10], ["hasattr", "model.module.load_state_dict", "model.load_state_dict", "torch.load", "torch.load"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.load", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.load"], ["def", "load_model", "(", "model", ",", "model_path", ")", ":", "\n", "    ", "if", "hasattr", "(", "model", ",", "\"module\"", ")", ":", "\n", "        ", "model", ".", "module", ".", "load_state_dict", "(", "torch", ".", "load", "(", "model_path", ",", "map_location", "=", "\"cpu\"", ")", ",", "strict", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "model_path", ",", "map_location", "=", "\"cpu\"", ")", ",", "strict", "=", "False", ")", "\n", "", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.uer.model_saver.save_model": [[4, 9], ["hasattr", "torch.save", "torch.save", "model.module.state_dict", "model.state_dict"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.save", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.save"], ["def", "save_model", "(", "model", ",", "model_path", ")", ":", "\n", "    ", "if", "hasattr", "(", "model", ",", "\"module\"", ")", ":", "\n", "        ", "torch", ".", "save", "(", "model", ".", "module", ".", "state_dict", "(", ")", ",", "model_path", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "model_path", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.config.load_hyperparam": [[5, 14], ["vars", "vars.update", "argparse.Namespace", "open", "json.load"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.load"], ["def", "load_hyperparam", "(", "args", ")", ":", "\n", "    ", "with", "open", "(", "args", ".", "config_path", ",", "mode", "=", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "param", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "args_dict", "=", "vars", "(", "args", ")", "\n", "args_dict", ".", "update", "(", "param", ")", "\n", "args", "=", "Namespace", "(", "**", "args_dict", ")", "\n", "\n", "return", "args", "\n", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.Dataset.__init__": [[186, 200], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "vocab", ",", "tokenizer", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "corpus_path", "=", "args", ".", "corpus_path", "\n", "self", ".", "dataset_path", "=", "args", ".", "dataset_path", "\n", "self", ".", "seq_length", "=", "args", ".", "seq_length", "\n", "self", ".", "seed", "=", "args", ".", "seed", "\n", "self", ".", "dynamic_masking", "=", "args", ".", "dynamic_masking", "\n", "self", ".", "whole_word_masking", "=", "args", ".", "whole_word_masking", "\n", "self", ".", "span_masking", "=", "args", ".", "span_masking", "\n", "self", ".", "span_geo_prob", "=", "args", ".", "span_geo_prob", "\n", "self", ".", "span_max_length", "=", "args", ".", "span_max_length", "\n", "self", ".", "docs_buffer_size", "=", "args", ".", "docs_buffer_size", "\n", "self", ".", "dup_factor", "=", "args", ".", "dup_factor", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.Dataset.build_and_save": [[201, 222], ["uer.utils.misc.count_lines", "print", "data.merge_dataset", "data.Dataset.worker", "multiprocessing.Pool", "range", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "multiprocessing.Pool.apply_async"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.misc.count_lines", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.merge_dataset", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.worker"], ["", "def", "build_and_save", "(", "self", ",", "workers_num", ")", ":", "\n", "        ", "\"\"\"\n        Build dataset from the given corpus.\n        Start workers_num processes and each process deals with a part of data.\n        \"\"\"", "\n", "lines_num", "=", "count_lines", "(", "self", ".", "corpus_path", ")", "\n", "print", "(", "\"Starting %d workers for building datasets ... \"", "%", "workers_num", ")", "\n", "assert", "(", "workers_num", ">=", "1", ")", "\n", "if", "workers_num", "==", "1", ":", "\n", "            ", "self", ".", "worker", "(", "0", ",", "0", ",", "lines_num", ")", "\n", "", "else", ":", "\n", "            ", "pool", "=", "Pool", "(", "workers_num", ")", "\n", "for", "i", "in", "range", "(", "workers_num", ")", ":", "\n", "                ", "start", "=", "i", "*", "lines_num", "//", "workers_num", "\n", "end", "=", "(", "i", "+", "1", ")", "*", "lines_num", "//", "workers_num", "\n", "pool", ".", "apply_async", "(", "func", "=", "self", ".", "worker", ",", "args", "=", "[", "i", ",", "start", ",", "end", "]", ")", "\n", "", "pool", ".", "close", "(", ")", "\n", "pool", ".", "join", "(", ")", "\n", "\n", "# Merge datasets.", "\n", "", "merge_dataset", "(", "self", ".", "dataset_path", ",", "workers_num", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.Dataset.worker": [[223, 225], ["NotImplementedError"], "methods", ["None"], ["", "def", "worker", "(", "self", ",", "proc_id", ",", "start", ",", "end", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.DataLoader.__init__": [[228, 245], ["open"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "dataset_path", ",", "batch_size", ",", "proc_id", ",", "proc_num", ",", "shuffle", "=", "False", ")", ":", "\n", "        ", "self", ".", "tokenizer", "=", "args", ".", "tokenizer", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "instances_buffer_size", "=", "args", ".", "instances_buffer_size", "\n", "self", ".", "proc_id", "=", "proc_id", "\n", "self", ".", "proc_num", "=", "proc_num", "\n", "self", ".", "shuffle", "=", "shuffle", "\n", "self", ".", "dataset_reader", "=", "open", "(", "dataset_path", ",", "\"rb\"", ")", "\n", "self", ".", "read_count", "=", "0", "\n", "self", ".", "start", "=", "0", "\n", "self", ".", "end", "=", "0", "\n", "self", ".", "buffer", "=", "[", "]", "\n", "self", ".", "vocab", "=", "args", ".", "vocab", "\n", "self", ".", "whole_word_masking", "=", "args", ".", "whole_word_masking", "\n", "self", ".", "span_masking", "=", "args", ".", "span_masking", "\n", "self", ".", "span_geo_prob", "=", "args", ".", "span_geo_prob", "\n", "self", ".", "span_max_length", "=", "args", ".", "span_max_length", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.DataLoader._fill_buf": [[246, 264], ["len", "random.shuffle", "pickle.load", "data.DataLoader.dataset_reader.seek", "data.DataLoader.buffer.append", "len"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.load"], ["", "def", "_fill_buf", "(", "self", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "self", ".", "buffer", "=", "[", "]", "\n", "while", "True", ":", "\n", "                ", "instance", "=", "pickle", ".", "load", "(", "self", ".", "dataset_reader", ")", "\n", "self", ".", "read_count", "+=", "1", "\n", "if", "(", "self", ".", "read_count", "-", "1", ")", "%", "self", ".", "proc_num", "==", "self", ".", "proc_id", ":", "\n", "                    ", "self", ".", "buffer", ".", "append", "(", "instance", ")", "\n", "if", "len", "(", "self", ".", "buffer", ")", ">=", "self", ".", "instances_buffer_size", ":", "\n", "                        ", "break", "\n", "", "", "", "", "except", "EOFError", ":", "\n", "# Reach file end.", "\n", "            ", "self", ".", "dataset_reader", ".", "seek", "(", "0", ")", "\n", "\n", "", "if", "self", ".", "shuffle", ":", "\n", "            ", "random", ".", "shuffle", "(", "self", ".", "buffer", ")", "\n", "", "self", ".", "start", "=", "0", "\n", "self", ".", "end", "=", "len", "(", "self", ".", "buffer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.DataLoader._empty": [[265, 267], ["None"], "methods", ["None"], ["", "def", "_empty", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "start", ">=", "self", ".", "end", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.DataLoader.__del__": [[268, 270], ["data.DataLoader.dataset_reader.close"], "methods", ["None"], ["", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "self", ".", "dataset_reader", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.BertDataset.__init__": [[280, 283], ["data.Dataset.__init__"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "vocab", ",", "tokenizer", ")", ":", "\n", "        ", "super", "(", "BertDataset", ",", "self", ")", ".", "__init__", "(", "args", ",", "vocab", ",", "tokenizer", ")", "\n", "self", ".", "short_seq_prob", "=", "args", ".", "short_seq_prob", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.BertDataset.worker": [[284, 324], ["print", "uer.utils.seed.set_seed", "open", "open.close", "open", "f.readline", "f.readline", "data.BertDataset.tokenizer.convert_tokens_to_ids", "str", "f.readline.strip", "data.BertDataset.tokenizer.tokenize", "len", "document.append", "len", "data.BertDataset.build_instances", "len", "docs_buffer.append", "len", "data.BertDataset.build_instances", "pickle.dump", "pickle.dump"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.seed.set_seed", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_tokens_to_ids", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.AlbertDataset.build_instances", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.AlbertDataset.build_instances"], ["", "def", "worker", "(", "self", ",", "proc_id", ",", "start", ",", "end", ")", ":", "\n", "        ", "print", "(", "\"Worker %d is building dataset ... \"", "%", "proc_id", ")", "\n", "set_seed", "(", "self", ".", "seed", ")", "\n", "docs_buffer", "=", "[", "]", "\n", "document", "=", "[", "]", "\n", "pos", "=", "0", "\n", "dataset_writer", "=", "open", "(", "\"dataset-tmp-\"", "+", "str", "(", "proc_id", ")", "+", "\".pt\"", ",", "\"wb\"", ")", "\n", "with", "open", "(", "self", ".", "corpus_path", ",", "mode", "=", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "while", "pos", "<", "start", ":", "\n", "                ", "f", ".", "readline", "(", ")", "\n", "pos", "+=", "1", "\n", "", "while", "True", ":", "\n", "                ", "line", "=", "f", ".", "readline", "(", ")", "\n", "pos", "+=", "1", "\n", "\n", "if", "pos", ">=", "end", ":", "\n", "                    ", "if", "len", "(", "docs_buffer", ")", ">", "0", ":", "\n", "                        ", "instances", "=", "self", ".", "build_instances", "(", "docs_buffer", ")", "\n", "for", "instance", "in", "instances", ":", "\n", "                            ", "pickle", ".", "dump", "(", "instance", ",", "dataset_writer", ")", "\n", "", "", "break", "\n", "\n", "", "if", "not", "line", ".", "strip", "(", ")", ":", "\n", "                    ", "if", "len", "(", "document", ")", ">=", "1", ":", "\n", "                        ", "docs_buffer", ".", "append", "(", "document", ")", "\n", "", "document", "=", "[", "]", "\n", "if", "len", "(", "docs_buffer", ")", "==", "self", ".", "docs_buffer_size", ":", "\n", "# Build instances from documents.", "\n", "                        ", "instances", "=", "self", ".", "build_instances", "(", "docs_buffer", ")", "\n", "# Save instances.", "\n", "for", "instance", "in", "instances", ":", "\n", "                            ", "pickle", ".", "dump", "(", "instance", ",", "dataset_writer", ")", "\n", "# Clear buffer.", "\n", "", "docs_buffer", "=", "[", "]", "\n", "", "continue", "\n", "", "sentence", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "line", ")", ")", "\n", "if", "len", "(", "sentence", ")", ">", "0", ":", "\n", "                    ", "document", ".", "append", "(", "sentence", ")", "\n", "\n", "", "", "", "dataset_writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.BertDataset.build_instances": [[325, 331], ["range", "range", "len", "instances.extend", "data.BertDataset.create_ins_from_doc"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.AlbertDataset.create_ins_from_doc"], ["", "def", "build_instances", "(", "self", ",", "all_documents", ")", ":", "\n", "        ", "instances", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "self", ".", "dup_factor", ")", ":", "\n", "            ", "for", "doc_index", "in", "range", "(", "len", "(", "all_documents", ")", ")", ":", "\n", "                ", "instances", ".", "extend", "(", "self", ".", "create_ins_from_doc", "(", "all_documents", ",", "doc_index", ")", ")", "\n", "", "", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.BertDataset.create_ins_from_doc": [[332, 408], ["random.random", "random.randint", "len", "current_chunk.append", "len", "range", "data.truncate_seq_pair", "src.append", "src.extend", "src.append", "src.extend", "src.append", "seg_pos.append", "instances.append", "len", "len", "random.randint", "tokens_a.extend", "range", "random.randint", "range", "range", "data.BertDataset.vocab.get", "data.BertDataset.vocab.get", "len", "data.BertDataset.vocab.get", "len", "len", "src.append", "data.mask_seq", "len", "random.random", "len", "random.randint", "len", "tokens_b.extend", "len", "len", "tokens_b.extend", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.truncate_seq_pair", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.mask_seq"], ["", "def", "create_ins_from_doc", "(", "self", ",", "all_documents", ",", "document_index", ")", ":", "\n", "        ", "document", "=", "all_documents", "[", "document_index", "]", "\n", "max_num_tokens", "=", "self", ".", "seq_length", "-", "3", "\n", "target_seq_length", "=", "max_num_tokens", "\n", "if", "random", ".", "random", "(", ")", "<", "self", ".", "short_seq_prob", ":", "\n", "            ", "target_seq_length", "=", "random", ".", "randint", "(", "2", ",", "max_num_tokens", ")", "\n", "", "instances", "=", "[", "]", "\n", "current_chunk", "=", "[", "]", "\n", "current_length", "=", "0", "\n", "i", "=", "0", "\n", "while", "i", "<", "len", "(", "document", ")", ":", "\n", "            ", "segment", "=", "document", "[", "i", "]", "\n", "current_chunk", ".", "append", "(", "segment", ")", "\n", "current_length", "+=", "len", "(", "segment", ")", "\n", "if", "i", "==", "len", "(", "document", ")", "-", "1", "or", "current_length", ">=", "target_seq_length", ":", "\n", "                ", "if", "current_chunk", ":", "\n", "                    ", "a_end", "=", "1", "\n", "if", "len", "(", "current_chunk", ")", ">=", "2", ":", "\n", "                        ", "a_end", "=", "random", ".", "randint", "(", "1", ",", "len", "(", "current_chunk", ")", "-", "1", ")", "\n", "\n", "", "tokens_a", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "a_end", ")", ":", "\n", "                        ", "tokens_a", ".", "extend", "(", "current_chunk", "[", "j", "]", ")", "\n", "\n", "", "tokens_b", "=", "[", "]", "\n", "is_random_next", "=", "0", "\n", "\n", "if", "len", "(", "current_chunk", ")", "==", "1", "or", "random", ".", "random", "(", ")", "<", "0.5", ":", "\n", "                        ", "is_random_next", "=", "1", "\n", "target_b_length", "=", "target_seq_length", "-", "len", "(", "tokens_a", ")", "\n", "\n", "for", "_", "in", "range", "(", "10", ")", ":", "\n", "                            ", "random_document_index", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "all_documents", ")", "-", "1", ")", "\n", "if", "random_document_index", "!=", "document_index", ":", "\n", "                                ", "break", "\n", "\n", "", "", "random_document", "=", "all_documents", "[", "random_document_index", "]", "\n", "random_start", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "random_document", ")", "-", "1", ")", "\n", "for", "j", "in", "range", "(", "random_start", ",", "len", "(", "random_document", ")", ")", ":", "\n", "                            ", "tokens_b", ".", "extend", "(", "random_document", "[", "j", "]", ")", "\n", "if", "len", "(", "tokens_b", ")", ">=", "target_b_length", ":", "\n", "                                ", "break", "\n", "\n", "", "", "num_unused_segments", "=", "len", "(", "current_chunk", ")", "-", "a_end", "\n", "i", "-=", "num_unused_segments", "\n", "\n", "", "else", ":", "\n", "                        ", "is_random_next", "=", "0", "\n", "for", "j", "in", "range", "(", "a_end", ",", "len", "(", "current_chunk", ")", ")", ":", "\n", "                            ", "tokens_b", ".", "extend", "(", "current_chunk", "[", "j", "]", ")", "\n", "\n", "", "", "truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_num_tokens", ")", "\n", "\n", "src", "=", "[", "]", "\n", "src", ".", "append", "(", "self", ".", "vocab", ".", "get", "(", "CLS_TOKEN", ")", ")", "\n", "src", ".", "extend", "(", "tokens_a", ")", "\n", "src", ".", "append", "(", "self", ".", "vocab", ".", "get", "(", "SEP_TOKEN", ")", ")", "\n", "seg_pos", "=", "[", "len", "(", "src", ")", "]", "\n", "src", ".", "extend", "(", "tokens_b", ")", "\n", "src", ".", "append", "(", "self", ".", "vocab", ".", "get", "(", "SEP_TOKEN", ")", ")", "\n", "seg_pos", ".", "append", "(", "len", "(", "src", ")", ")", "\n", "\n", "while", "len", "(", "src", ")", "!=", "self", ".", "seq_length", ":", "\n", "                        ", "src", ".", "append", "(", "PAD_ID", ")", "\n", "\n", "", "if", "not", "self", ".", "dynamic_masking", ":", "\n", "                        ", "src", ",", "tgt_mlm", "=", "mask_seq", "(", "src", ",", "self", ".", "tokenizer", ",", "self", ".", "whole_word_masking", ",", "self", ".", "span_masking", ",", "self", ".", "span_geo_prob", ",", "self", ".", "span_max_length", ")", "\n", "instance", "=", "(", "src", ",", "tgt_mlm", ",", "is_random_next", ",", "seg_pos", ")", "\n", "", "else", ":", "\n", "                        ", "instance", "=", "(", "src", ",", "is_random_next", ",", "seg_pos", ")", "\n", "\n", "", "instances", ".", "append", "(", "instance", ")", "\n", "", "current_chunk", "=", "[", "]", "\n", "current_length", "=", "0", "\n", "", "i", "+=", "1", "\n", "", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.BertDataLoader.__iter__": [[411, 455], ["data.BertDataLoader._empty", "data.BertDataLoader._fill_buf", "len", "src.append", "len", "tgt_mlm.append", "is_next.append", "seg.append", "data.mask_seq", "len", "src.append", "tgt_mlm.append", "is_next.append", "seg.append", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.DataLoader._empty", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.DataLoader._fill_buf", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.mask_seq"], ["    ", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "while", "self", ".", "_empty", "(", ")", ":", "\n", "                ", "self", ".", "_fill_buf", "(", ")", "\n", "", "if", "self", ".", "start", "+", "self", ".", "batch_size", ">=", "self", ".", "end", ":", "\n", "                ", "instances", "=", "self", ".", "buffer", "[", "self", ".", "start", ":", "]", "\n", "", "else", ":", "\n", "                ", "instances", "=", "self", ".", "buffer", "[", "self", ".", "start", ":", "self", ".", "start", "+", "self", ".", "batch_size", "]", "\n", "\n", "", "self", ".", "start", "+=", "self", ".", "batch_size", "\n", "\n", "src", "=", "[", "]", "\n", "tgt_mlm", "=", "[", "]", "\n", "is_next", "=", "[", "]", "\n", "seg", "=", "[", "]", "\n", "\n", "masked_words_num", "=", "0", "\n", "\n", "for", "ins", "in", "instances", ":", "\n", "                ", "if", "len", "(", "ins", ")", "==", "4", ":", "\n", "                    ", "src", ".", "append", "(", "ins", "[", "0", "]", ")", "\n", "masked_words_num", "+=", "len", "(", "ins", "[", "1", "]", ")", "\n", "tgt_mlm", ".", "append", "(", "[", "0", "]", "*", "len", "(", "ins", "[", "0", "]", ")", ")", "\n", "for", "mask", "in", "ins", "[", "1", "]", ":", "\n", "                        ", "tgt_mlm", "[", "-", "1", "]", "[", "mask", "[", "0", "]", "]", "=", "mask", "[", "1", "]", "\n", "", "is_next", ".", "append", "(", "ins", "[", "2", "]", ")", "\n", "seg", ".", "append", "(", "[", "1", "]", "*", "ins", "[", "3", "]", "[", "0", "]", "+", "[", "2", "]", "*", "(", "ins", "[", "3", "]", "[", "1", "]", "-", "ins", "[", "3", "]", "[", "0", "]", ")", "+", "[", "PAD_ID", "]", "*", "(", "len", "(", "ins", "[", "0", "]", ")", "-", "ins", "[", "3", "]", "[", "1", "]", ")", ")", "\n", "", "else", ":", "\n", "                    ", "src_single", ",", "tgt_mlm_single", "=", "mask_seq", "(", "ins", "[", "0", "]", ",", "self", ".", "tokenizer", ",", "self", ".", "whole_word_masking", ",", "self", ".", "span_masking", ",", "self", ".", "span_geo_prob", ",", "self", ".", "span_max_length", ")", "\n", "masked_words_num", "+=", "len", "(", "tgt_mlm_single", ")", "\n", "src", ".", "append", "(", "src_single", ")", "\n", "tgt_mlm", ".", "append", "(", "[", "0", "]", "*", "len", "(", "ins", "[", "0", "]", ")", ")", "\n", "for", "mask", "in", "tgt_mlm_single", ":", "\n", "                        ", "tgt_mlm", "[", "-", "1", "]", "[", "mask", "[", "0", "]", "]", "=", "mask", "[", "1", "]", "\n", "", "is_next", ".", "append", "(", "ins", "[", "1", "]", ")", "\n", "seg", ".", "append", "(", "[", "1", "]", "*", "ins", "[", "2", "]", "[", "0", "]", "+", "[", "2", "]", "*", "(", "ins", "[", "2", "]", "[", "1", "]", "-", "ins", "[", "2", "]", "[", "0", "]", ")", "+", "[", "PAD_ID", "]", "*", "(", "len", "(", "ins", "[", "0", "]", ")", "-", "ins", "[", "2", "]", "[", "1", "]", ")", ")", "\n", "\n", "", "", "if", "masked_words_num", "==", "0", ":", "\n", "                ", "continue", "\n", "\n", "", "yield", "torch", ".", "LongTensor", "(", "src", ")", ",", "torch", ".", "LongTensor", "(", "tgt_mlm", ")", ",", "torch", ".", "LongTensor", "(", "is_next", ")", ",", "torch", ".", "LongTensor", "(", "seg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.MlmDataset.__init__": [[458, 461], ["data.Dataset.__init__"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "vocab", ",", "tokenizer", ")", ":", "\n", "        ", "super", "(", "MlmDataset", ",", "self", ")", ".", "__init__", "(", "args", ",", "vocab", ",", "tokenizer", ")", "\n", "self", ".", "full_sentences", "=", "args", ".", "full_sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.MlmDataset.worker": [[462, 510], ["print", "uer.utils.seed.set_seed", "open", "range", "open.close", "open", "str", "f.readline", "f.readline", "data.MlmDataset.tokenizer.convert_tokens_to_ids", "data.MlmDataset.vocab.get", "len", "docs_buffer.append", "len", "data.MlmDataset.concatenate_docs", "data.MlmDataset.build_instances", "len", "data.MlmDataset.build_instances", "data.MlmDataset.vocab.get", "data.MlmDataset.tokenizer.tokenize", "pickle.dump", "len", "data.MlmDataset.concatenate_docs", "data.MlmDataset.build_instances", "pickle.dump", "pickle.dump"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.seed.set_seed", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_tokens_to_ids", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.MlmDataset.concatenate_docs", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.AlbertDataset.build_instances", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.AlbertDataset.build_instances", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.MlmDataset.concatenate_docs", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.AlbertDataset.build_instances"], ["", "def", "worker", "(", "self", ",", "proc_id", ",", "start", ",", "end", ")", ":", "\n", "        ", "print", "(", "\"Worker %d is building dataset ... \"", "%", "proc_id", ")", "\n", "set_seed", "(", "self", ".", "seed", ")", "\n", "dataset_writer", "=", "open", "(", "\"dataset-tmp-\"", "+", "str", "(", "proc_id", ")", "+", "\".pt\"", ",", "\"wb\"", ")", "\n", "docs_buffer", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "self", ".", "dup_factor", ")", ":", "\n", "            ", "pos", "=", "0", "\n", "with", "open", "(", "self", ".", "corpus_path", ",", "mode", "=", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                ", "while", "pos", "<", "start", ":", "\n", "                    ", "f", ".", "readline", "(", ")", "\n", "pos", "+=", "1", "\n", "", "while", "True", ":", "\n", "                    ", "line", "=", "f", ".", "readline", "(", ")", "\n", "pos", "+=", "1", "\n", "\n", "document", "=", "[", "self", ".", "vocab", ".", "get", "(", "CLS_TOKEN", ")", "]", "+", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "line", ")", ")", "+", "[", "self", ".", "vocab", ".", "get", "(", "SEP_TOKEN", ")", "]", "\n", "\n", "if", "self", ".", "full_sentences", ":", "\n", "                        ", "if", "len", "(", "document", ")", ">", "0", ":", "\n", "                            ", "docs_buffer", ".", "append", "(", "document", ")", "\n", "", "if", "len", "(", "docs_buffer", ")", "==", "self", ".", "docs_buffer_size", ":", "\n", "# Build instances from documents.", "\n", "                            ", "all_documents", "=", "self", ".", "concatenate_docs", "(", "docs_buffer", ")", "\n", "instances", "=", "self", ".", "build_instances", "(", "all_documents", ")", "\n", "# Save instances.", "\n", "for", "instance", "in", "instances", ":", "\n", "                                ", "pickle", ".", "dump", "(", "instance", ",", "dataset_writer", ")", "\n", "# Clear buffer.", "\n", "", "docs_buffer", "=", "[", "]", "\n", "", "if", "pos", ">=", "end", ":", "\n", "                            ", "if", "len", "(", "docs_buffer", ")", ">", "0", ":", "\n", "                                ", "all_documents", "=", "self", ".", "concatenate_docs", "(", "docs_buffer", ")", "\n", "instances", "=", "self", ".", "build_instances", "(", "all_documents", ")", "\n", "# Save instances.", "\n", "for", "instance", "in", "instances", ":", "\n", "                                    ", "pickle", ".", "dump", "(", "instance", ",", "dataset_writer", ")", "\n", "", "", "break", "\n", "", "", "else", ":", "\n", "                        ", "if", "len", "(", "document", ")", ">", "0", ":", "\n", "                            ", "instances", "=", "self", ".", "build_instances", "(", "document", ")", "\n", "# Save instances.", "\n", "for", "instance", "in", "instances", ":", "\n", "                                ", "pickle", ".", "dump", "(", "instance", ",", "dataset_writer", ")", "\n", "\n", "", "", "", "if", "pos", ">=", "end", ":", "\n", "                        ", "break", "\n", "\n", "", "", "", "", "dataset_writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.MlmDataset.concatenate_docs": [[511, 516], ["range", "len"], "methods", ["None"], ["", "def", "concatenate_docs", "(", "self", ",", "docs_buffer", ")", ":", "\n", "        ", "all_documents", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "docs_buffer", ")", ")", ":", "\n", "            ", "all_documents", "+=", "docs_buffer", "[", "i", "]", "\n", "", "return", "all_documents", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.MlmDataset.build_instances": [[517, 546], ["range", "instances.append", "len", "instances.append", "len", "len", "src.append", "data.mask_seq", "len", "data.mask_seq"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.mask_seq", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.mask_seq"], ["", "def", "build_instances", "(", "self", ",", "all_documents", ")", ":", "\n", "        ", "instances", "=", "[", "]", "\n", "instances_num", "=", "len", "(", "all_documents", ")", "//", "self", ".", "seq_length", "\n", "for", "i", "in", "range", "(", "instances_num", ")", ":", "\n", "            ", "src", "=", "all_documents", "[", "i", "*", "self", ".", "seq_length", ":", "(", "i", "+", "1", ")", "*", "self", ".", "seq_length", "]", "\n", "seg_pos", "=", "[", "len", "(", "src", ")", "]", "\n", "\n", "if", "not", "self", ".", "dynamic_masking", ":", "\n", "                ", "src", ",", "tgt", "=", "mask_seq", "(", "src", ",", "self", ".", "tokenizer", ",", "self", ".", "whole_word_masking", ",", "self", ".", "span_masking", ",", "self", ".", "span_geo_prob", ",", "self", ".", "span_max_length", ")", "\n", "instance", "=", "(", "src", ",", "tgt", ",", "seg_pos", ")", "\n", "", "else", ":", "\n", "                ", "instance", "=", "(", "src", ",", "seg_pos", ")", "\n", "\n", "", "instances", ".", "append", "(", "instance", ")", "\n", "\n", "", "src", "=", "all_documents", "[", "instances_num", "*", "self", ".", "seq_length", ":", "]", "\n", "seg_pos", "=", "[", "len", "(", "src", ")", "]", "\n", "\n", "while", "len", "(", "src", ")", "!=", "self", ".", "seq_length", ":", "\n", "            ", "src", ".", "append", "(", "PAD_ID", ")", "\n", "\n", "", "if", "not", "self", ".", "dynamic_masking", ":", "\n", "            ", "src", ",", "tgt", "=", "mask_seq", "(", "src", ",", "self", ".", "tokenizer", ",", "self", ".", "whole_word_masking", ",", "self", ".", "span_masking", ",", "self", ".", "span_geo_prob", ",", "self", ".", "span_max_length", ")", "\n", "instance", "=", "(", "src", ",", "tgt", ",", "seg_pos", ")", "\n", "", "else", ":", "\n", "            ", "instance", "=", "(", "src", ",", "seg_pos", ")", "\n", "\n", "", "instances", ".", "append", "(", "instance", ")", "\n", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.MlmDataLoader.__iter__": [[549, 589], ["data.MlmDataLoader._empty", "data.MlmDataLoader._fill_buf", "len", "src.append", "len", "tgt.append", "seg.append", "data.mask_seq", "len", "src.append", "tgt.append", "seg.append", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.DataLoader._empty", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.DataLoader._fill_buf", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.mask_seq"], ["    ", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "while", "self", ".", "_empty", "(", ")", ":", "\n", "                ", "self", ".", "_fill_buf", "(", ")", "\n", "", "if", "self", ".", "start", "+", "self", ".", "batch_size", ">=", "self", ".", "end", ":", "\n", "                ", "instances", "=", "self", ".", "buffer", "[", "self", ".", "start", ":", "]", "\n", "", "else", ":", "\n", "                ", "instances", "=", "self", ".", "buffer", "[", "self", ".", "start", ":", "self", ".", "start", "+", "self", ".", "batch_size", "]", "\n", "\n", "", "self", ".", "start", "+=", "self", ".", "batch_size", "\n", "\n", "src", "=", "[", "]", "\n", "tgt", "=", "[", "]", "\n", "seg", "=", "[", "]", "\n", "\n", "masked_words_num", "=", "0", "\n", "\n", "for", "ins", "in", "instances", ":", "\n", "                ", "if", "len", "(", "ins", ")", "==", "3", ":", "\n", "                    ", "src", ".", "append", "(", "ins", "[", "0", "]", ")", "\n", "masked_words_num", "+=", "len", "(", "ins", "[", "1", "]", ")", "\n", "tgt", ".", "append", "(", "[", "0", "]", "*", "len", "(", "ins", "[", "0", "]", ")", ")", "\n", "for", "mask", "in", "ins", "[", "1", "]", ":", "\n", "                        ", "tgt", "[", "-", "1", "]", "[", "mask", "[", "0", "]", "]", "=", "mask", "[", "1", "]", "\n", "", "seg", ".", "append", "(", "[", "1", "]", "*", "ins", "[", "2", "]", "[", "0", "]", "+", "[", "PAD_ID", "]", "*", "(", "len", "(", "ins", "[", "0", "]", ")", "-", "ins", "[", "2", "]", "[", "0", "]", ")", ")", "\n", "", "else", ":", "\n", "                    ", "src_single", ",", "tgt_single", "=", "mask_seq", "(", "ins", "[", "0", "]", ",", "self", ".", "tokenizer", ",", "self", ".", "whole_word_masking", ",", "self", ".", "span_masking", ",", "self", ".", "span_geo_prob", ",", "self", ".", "span_max_length", ")", "\n", "masked_words_num", "+=", "len", "(", "tgt_single", ")", "\n", "src", ".", "append", "(", "src_single", ")", "\n", "tgt", ".", "append", "(", "[", "0", "]", "*", "len", "(", "ins", "[", "0", "]", ")", ")", "\n", "for", "mask", "in", "tgt_single", ":", "\n", "                        ", "tgt", "[", "-", "1", "]", "[", "mask", "[", "0", "]", "]", "=", "mask", "[", "1", "]", "\n", "", "seg", ".", "append", "(", "[", "1", "]", "*", "ins", "[", "1", "]", "[", "0", "]", "+", "[", "PAD_ID", "]", "*", "(", "len", "(", "ins", "[", "0", "]", ")", "-", "ins", "[", "1", "]", "[", "0", "]", ")", ")", "\n", "\n", "", "", "if", "masked_words_num", "==", "0", ":", "\n", "                ", "continue", "\n", "\n", "", "yield", "torch", ".", "LongTensor", "(", "src", ")", ",", "torch", ".", "LongTensor", "(", "tgt", ")", ",", "torch", ".", "LongTensor", "(", "seg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.AlbertDataset.__init__": [[599, 602], ["data.Dataset.__init__"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "vocab", ",", "tokenizer", ")", ":", "\n", "        ", "super", "(", "AlbertDataset", ",", "self", ")", ".", "__init__", "(", "args", ",", "vocab", ",", "tokenizer", ")", "\n", "self", ".", "short_seq_prob", "=", "args", ".", "short_seq_prob", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.AlbertDataset.worker": [[603, 633], ["print", "uer.utils.seed.set_seed", "open", "range", "open.close", "open", "str", "f.readline", "f.readline", "data.AlbertDataset.tokenizer.convert_tokens_to_ids", "f.readline.strip", "data.AlbertDataset.tokenizer.tokenize", "len", "document.append", "len", "data.AlbertDataset.build_instances", "len", "data.AlbertDataset.build_instances", "pickle.dump", "pickle.dump"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.seed.set_seed", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_tokens_to_ids", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.AlbertDataset.build_instances", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.AlbertDataset.build_instances"], ["", "def", "worker", "(", "self", ",", "proc_id", ",", "start", ",", "end", ")", ":", "\n", "        ", "print", "(", "\"Worker %d is building dataset ... \"", "%", "proc_id", ")", "\n", "set_seed", "(", "self", ".", "seed", ")", "\n", "document", "=", "[", "]", "\n", "dataset_writer", "=", "open", "(", "\"dataset-tmp-\"", "+", "str", "(", "proc_id", ")", "+", "\".pt\"", ",", "\"wb\"", ")", "\n", "for", "_", "in", "range", "(", "self", ".", "dup_factor", ")", ":", "\n", "            ", "pos", "=", "0", "\n", "with", "open", "(", "self", ".", "corpus_path", ",", "mode", "=", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                ", "while", "pos", "<", "start", ":", "\n", "                    ", "f", ".", "readline", "(", ")", "\n", "pos", "+=", "1", "\n", "", "while", "True", ":", "\n", "                    ", "line", "=", "f", ".", "readline", "(", ")", "\n", "pos", "+=", "1", "\n", "if", "not", "line", ".", "strip", "(", ")", ":", "\n", "                        ", "if", "len", "(", "document", ")", ">=", "1", ":", "\n", "                            ", "instances", "=", "self", ".", "build_instances", "(", "document", ")", "\n", "for", "instance", "in", "instances", ":", "\n", "                                ", "pickle", ".", "dump", "(", "instance", ",", "dataset_writer", ")", "\n", "", "", "document", "=", "[", "]", "\n", "", "sentence", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "line", ")", ")", "\n", "if", "len", "(", "sentence", ")", ">", "0", ":", "\n", "                        ", "document", ".", "append", "(", "sentence", ")", "\n", "", "if", "pos", ">=", "end", "-", "1", ":", "\n", "                        ", "if", "len", "(", "document", ")", ">=", "1", ":", "\n", "                            ", "instances", "=", "self", ".", "build_instances", "(", "document", ")", "\n", "for", "instance", "in", "instances", ":", "\n", "                                ", "pickle", ".", "dump", "(", "instance", ",", "dataset_writer", ")", "\n", "", "", "break", "\n", "", "", "", "", "dataset_writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.AlbertDataset.build_instances": [[634, 638], ["instances.extend", "data.AlbertDataset.create_ins_from_doc"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.AlbertDataset.create_ins_from_doc"], ["", "def", "build_instances", "(", "self", ",", "document", ")", ":", "\n", "        ", "instances", "=", "[", "]", "\n", "instances", ".", "extend", "(", "self", ".", "create_ins_from_doc", "(", "document", ")", ")", "\n", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.AlbertDataset.create_ins_from_doc": [[639, 698], ["random.random", "random.randint", "len", "current_chunk.append", "len", "range", "range", "data.truncate_seq_pair", "src.append", "src.extend", "src.append", "src.extend", "src.append", "seg_pos.append", "instances.append", "len", "len", "random.randint", "tokens_a.extend", "len", "tokens_b.extend", "random.random", "data.AlbertDataset.vocab.get", "data.AlbertDataset.vocab.get", "len", "data.AlbertDataset.vocab.get", "len", "len", "src.append", "data.mask_seq", "len"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.truncate_seq_pair", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.mask_seq"], ["", "def", "create_ins_from_doc", "(", "self", ",", "document", ")", ":", "\n", "        ", "max_num_tokens", "=", "self", ".", "seq_length", "-", "3", "\n", "target_seq_length", "=", "max_num_tokens", "\n", "if", "random", ".", "random", "(", ")", "<", "self", ".", "short_seq_prob", ":", "\n", "            ", "target_seq_length", "=", "random", ".", "randint", "(", "2", ",", "max_num_tokens", ")", "\n", "", "instances", "=", "[", "]", "\n", "current_chunk", "=", "[", "]", "\n", "current_length", "=", "0", "\n", "i", "=", "0", "\n", "while", "i", "<", "len", "(", "document", ")", ":", "\n", "            ", "segment", "=", "document", "[", "i", "]", "\n", "current_chunk", ".", "append", "(", "segment", ")", "\n", "current_length", "+=", "len", "(", "segment", ")", "\n", "if", "i", "==", "len", "(", "document", ")", "-", "1", "or", "current_length", ">=", "target_seq_length", ":", "\n", "                ", "if", "current_chunk", ":", "\n", "                    ", "a_end", "=", "1", "\n", "if", "len", "(", "current_chunk", ")", ">=", "2", ":", "\n", "                        ", "a_end", "=", "random", ".", "randint", "(", "1", ",", "len", "(", "current_chunk", ")", "-", "1", ")", "\n", "\n", "", "tokens_a", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "a_end", ")", ":", "\n", "                        ", "tokens_a", ".", "extend", "(", "current_chunk", "[", "j", "]", ")", "\n", "\n", "", "tokens_b", "=", "[", "]", "\n", "is_wrong_order", "=", "0", "\n", "for", "j", "in", "range", "(", "a_end", ",", "len", "(", "current_chunk", ")", ")", ":", "\n", "                        ", "tokens_b", ".", "extend", "(", "current_chunk", "[", "j", "]", ")", "\n", "\n", "", "if", "random", ".", "random", "(", ")", "<", "0.5", ":", "\n", "                        ", "is_wrong_order", "=", "1", "\n", "tmp", "=", "tokens_a", "\n", "tokens_a", "=", "tokens_b", "\n", "tokens_b", "=", "tmp", "\n", "\n", "", "truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_num_tokens", ")", "\n", "\n", "src", "=", "[", "]", "\n", "src", ".", "append", "(", "self", ".", "vocab", ".", "get", "(", "CLS_TOKEN", ")", ")", "\n", "src", ".", "extend", "(", "tokens_a", ")", "\n", "src", ".", "append", "(", "self", ".", "vocab", ".", "get", "(", "SEP_TOKEN", ")", ")", "\n", "seg_pos", "=", "[", "len", "(", "src", ")", "]", "\n", "src", ".", "extend", "(", "tokens_b", ")", "\n", "src", ".", "append", "(", "self", ".", "vocab", ".", "get", "(", "SEP_TOKEN", ")", ")", "\n", "seg_pos", ".", "append", "(", "len", "(", "src", ")", ")", "\n", "\n", "while", "len", "(", "src", ")", "!=", "self", ".", "seq_length", ":", "\n", "                        ", "src", ".", "append", "(", "PAD_ID", ")", "\n", "\n", "", "if", "not", "self", ".", "dynamic_masking", ":", "\n", "                        ", "src", ",", "tgt_mlm", "=", "mask_seq", "(", "src", ",", "self", ".", "tokenizer", ",", "self", ".", "whole_word_masking", ",", "self", ".", "span_masking", ",", "self", ".", "span_geo_prob", ",", "self", ".", "span_max_length", ")", "\n", "instance", "=", "(", "src", ",", "tgt_mlm", ",", "is_wrong_order", ",", "seg_pos", ")", "\n", "", "else", ":", "\n", "                        ", "instance", "=", "(", "src", ",", "is_wrong_order", ",", "seg_pos", ")", "\n", "\n", "", "instances", ".", "append", "(", "instance", ")", "\n", "", "current_chunk", "=", "[", "]", "\n", "current_length", "=", "0", "\n", "", "i", "+=", "1", "\n", "", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.LmDataset.worker": [[708, 741], ["print", "uer.utils.seed.set_seed", "open", "open.close", "open", "f.readline", "f.readline", "data.LmDataset.tokenizer.convert_tokens_to_ids", "range", "str", "data.LmDataset.tokenizer.tokenize", "len", "pickle.dump", "len", "len", "pickle.dump", "data.LmDataset.vocab.get", "len", "src.append", "data.LmDataset.vocab.get"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.seed.set_seed", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_tokens_to_ids", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get"], ["    ", "def", "worker", "(", "self", ",", "proc_id", ",", "start", ",", "end", ")", ":", "\n", "        ", "print", "(", "\"Worker %d is building dataset ... \"", "%", "proc_id", ")", "\n", "set_seed", "(", "self", ".", "seed", ")", "\n", "dataset_writer", "=", "open", "(", "\"dataset-tmp-\"", "+", "str", "(", "proc_id", ")", "+", "\".pt\"", ",", "\"wb\"", ")", "\n", "pos", "=", "0", "\n", "with", "open", "(", "self", ".", "corpus_path", ",", "mode", "=", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "while", "pos", "<", "start", ":", "\n", "                ", "f", ".", "readline", "(", ")", "\n", "pos", "+=", "1", "\n", "", "while", "True", ":", "\n", "                ", "line", "=", "f", ".", "readline", "(", ")", "\n", "pos", "+=", "1", "\n", "\n", "document", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "line", ")", ")", "\n", "document", "=", "[", "self", ".", "vocab", ".", "get", "(", "CLS_TOKEN", ")", "]", "+", "document", "+", "[", "self", ".", "vocab", ".", "get", "(", "SEP_TOKEN", ")", "]", "\n", "\n", "instances_num", "=", "len", "(", "document", ")", "//", "(", "self", ".", "seq_length", "+", "1", ")", "\n", "for", "i", "in", "range", "(", "instances_num", ")", ":", "\n", "                    ", "src", "=", "document", "[", "i", "*", "(", "self", ".", "seq_length", "+", "1", ")", ":", "(", "i", "+", "1", ")", "*", "(", "self", ".", "seq_length", "+", "1", ")", "]", "\n", "seg_pos", "=", "self", ".", "seq_length", "\n", "pickle", ".", "dump", "(", "(", "src", ",", "seg_pos", ")", ",", "dataset_writer", ")", "\n", "\n", "", "src", "=", "document", "[", "instances_num", "*", "(", "self", ".", "seq_length", "+", "1", ")", ":", "]", "\n", "if", "len", "(", "src", ")", ">", "0", ":", "\n", "                    ", "seg_pos", "=", "len", "(", "src", ")", "\n", "while", "len", "(", "src", ")", "!=", "self", ".", "seq_length", "+", "1", ":", "\n", "                        ", "src", ".", "append", "(", "PAD_ID", ")", "\n", "", "pickle", ".", "dump", "(", "(", "src", ",", "seg_pos", ")", ",", "dataset_writer", ")", "\n", "\n", "", "if", "pos", ">=", "end", ":", "\n", "                    ", "break", "\n", "\n", "", "", "", "dataset_writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.LmDataLoader.__iter__": [[744, 770], ["data.LmDataLoader._empty", "data.LmDataLoader._fill_buf", "src.append", "tgt.append", "len", "seg.append", "seg.append", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "len"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.DataLoader._empty", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.DataLoader._fill_buf"], ["    ", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "while", "self", ".", "_empty", "(", ")", ":", "\n", "                ", "self", ".", "_fill_buf", "(", ")", "\n", "", "if", "self", ".", "start", "+", "self", ".", "batch_size", ">=", "self", ".", "end", ":", "\n", "                ", "instances", "=", "self", ".", "buffer", "[", "self", ".", "start", ":", "]", "\n", "", "else", ":", "\n", "                ", "instances", "=", "self", ".", "buffer", "[", "self", ".", "start", ":", "self", ".", "start", "+", "self", ".", "batch_size", "]", "\n", "\n", "", "self", ".", "start", "+=", "self", ".", "batch_size", "\n", "\n", "src", "=", "[", "]", "\n", "tgt", "=", "[", "]", "\n", "seg", "=", "[", "]", "\n", "\n", "for", "ins", "in", "instances", ":", "\n", "                ", "src", ".", "append", "(", "ins", "[", "0", "]", "[", ":", "-", "1", "]", ")", "\n", "tgt", ".", "append", "(", "ins", "[", "0", "]", "[", "1", ":", "]", ")", "\n", "if", "ins", "[", "1", "]", "==", "len", "(", "ins", "[", "0", "]", ")", ":", "\n", "                    ", "seg", ".", "append", "(", "[", "1", "]", "*", "(", "ins", "[", "1", "]", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                    ", "seg", ".", "append", "(", "[", "1", "]", "*", "ins", "[", "1", "]", "+", "[", "PAD_ID", "]", "*", "(", "len", "(", "ins", "[", "0", "]", ")", "-", "1", "-", "ins", "[", "1", "]", ")", ")", "\n", "\n", "", "", "yield", "torch", ".", "LongTensor", "(", "src", ")", ",", "torch", ".", "LongTensor", "(", "tgt", ")", ",", "torch", ".", "LongTensor", "(", "seg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.BilmDataset.worker": [[773, 813], ["print", "uer.utils.seed.set_seed", "open", "open.close", "open", "f.readline", "f.readline", "data.BilmDataset.tokenizer.convert_tokens_to_ids", "range", "pickle.dump", "str", "data.BilmDataset.tokenizer.tokenize", "len", "pickle.dump", "len", "len", "len", "src.append", "tgt_forward.append", "tgt_backward.append", "seg.append", "len", "data.BilmDataset.vocab.get", "data.BilmDataset.vocab.get", "data.BilmDataset.vocab.get", "data.BilmDataset.vocab.get"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.seed.set_seed", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_tokens_to_ids", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get"], ["    ", "def", "worker", "(", "self", ",", "proc_id", ",", "start", ",", "end", ")", ":", "\n", "        ", "print", "(", "\"Worker %d is building dataset ... \"", "%", "proc_id", ")", "\n", "set_seed", "(", "self", ".", "seed", ")", "\n", "dataset_writer", "=", "open", "(", "\"dataset-tmp-\"", "+", "str", "(", "proc_id", ")", "+", "\".pt\"", ",", "\"wb\"", ")", "\n", "pos", "=", "0", "\n", "with", "open", "(", "self", ".", "corpus_path", ",", "mode", "=", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "while", "pos", "<", "start", ":", "\n", "                ", "f", ".", "readline", "(", ")", "\n", "pos", "+=", "1", "\n", "", "while", "True", ":", "\n", "                ", "line", "=", "f", ".", "readline", "(", ")", "\n", "pos", "+=", "1", "\n", "\n", "document", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "line", ")", ")", "\n", "\n", "instances_num", "=", "len", "(", "document", ")", "//", "self", ".", "seq_length", "\n", "for", "i", "in", "range", "(", "instances_num", ")", ":", "\n", "                    ", "src", "=", "document", "[", "i", "*", "self", ".", "seq_length", ":", "(", "i", "+", "1", ")", "*", "self", ".", "seq_length", "]", "\n", "tgt_forward", "=", "src", "[", "1", ":", "]", "+", "[", "self", ".", "vocab", ".", "get", "(", "SEP_TOKEN", ")", "]", "\n", "tgt_backward", "=", "[", "self", ".", "vocab", ".", "get", "(", "CLS_TOKEN", ")", "]", "+", "src", "[", ":", "-", "1", "]", "\n", "seg", "=", "[", "1", "]", "*", "len", "(", "src", ")", "\n", "pickle", ".", "dump", "(", "(", "src", ",", "tgt_forward", ",", "tgt_backward", ",", "seg", ")", ",", "dataset_writer", ")", "\n", "\n", "", "src", "=", "document", "[", "instances_num", "*", "self", ".", "seq_length", ":", "]", "\n", "if", "len", "(", "src", ")", "<", "1", ":", "\n", "                    ", "continue", "\n", "", "tgt_forward", "=", "src", "[", "1", ":", "]", "+", "[", "self", ".", "vocab", ".", "get", "(", "SEP_TOKEN", ")", "]", "\n", "tgt_backward", "=", "[", "self", ".", "vocab", ".", "get", "(", "CLS_TOKEN", ")", "]", "+", "src", "[", ":", "-", "1", "]", "\n", "seg", "=", "[", "1", "]", "*", "len", "(", "src", ")", "\n", "while", "len", "(", "src", ")", "!=", "self", ".", "seq_length", ":", "\n", "                    ", "src", ".", "append", "(", "PAD_ID", ")", "\n", "tgt_forward", ".", "append", "(", "PAD_ID", ")", "\n", "tgt_backward", ".", "append", "(", "PAD_ID", ")", "\n", "seg", ".", "append", "(", "PAD_ID", ")", "\n", "", "pickle", ".", "dump", "(", "(", "src", ",", "tgt_forward", ",", "tgt_backward", ",", "seg", ")", ",", "dataset_writer", ")", "\n", "\n", "if", "pos", ">=", "end", "-", "1", ":", "\n", "                    ", "break", "\n", "\n", "", "", "", "dataset_writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.BilmDataLoader.__iter__": [[816, 842], ["data.BilmDataLoader._empty", "data.BilmDataLoader._fill_buf", "src.append", "tgt_forward.append", "tgt_backward.append", "seg.append", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.DataLoader._empty", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.DataLoader._fill_buf"], ["    ", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "while", "self", ".", "_empty", "(", ")", ":", "\n", "                ", "self", ".", "_fill_buf", "(", ")", "\n", "", "if", "self", ".", "start", "+", "self", ".", "batch_size", ">=", "self", ".", "end", ":", "\n", "                ", "instances", "=", "self", ".", "buffer", "[", "self", ".", "start", ":", "]", "\n", "", "else", ":", "\n", "                ", "instances", "=", "self", ".", "buffer", "[", "self", ".", "start", ":", "self", ".", "start", "+", "self", ".", "batch_size", "]", "\n", "\n", "", "self", ".", "start", "+=", "self", ".", "batch_size", "\n", "\n", "src", "=", "[", "]", "\n", "tgt_forward", "=", "[", "]", "\n", "tgt_backward", "=", "[", "]", "\n", "seg", "=", "[", "]", "\n", "\n", "for", "ins", "in", "instances", ":", "\n", "                ", "src", ".", "append", "(", "ins", "[", "0", "]", ")", "\n", "tgt_forward", ".", "append", "(", "ins", "[", "1", "]", ")", "\n", "tgt_backward", ".", "append", "(", "ins", "[", "2", "]", ")", "\n", "seg", ".", "append", "(", "ins", "[", "3", "]", ")", "\n", "\n", "", "yield", "torch", ".", "LongTensor", "(", "src", ")", ",", "torch", ".", "LongTensor", "(", "tgt_forward", ")", ",", "torch", ".", "LongTensor", "(", "tgt_backward", ")", ",", "torch", ".", "LongTensor", "(", "seg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.Seq2seqDataset.__init__": [[845, 851], ["data.Dataset.__init__"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "vocab", ",", "tokenizer", ")", ":", "\n", "        ", "super", "(", "Seq2seqDataset", ",", "self", ")", ".", "__init__", "(", "args", ",", "vocab", ",", "tokenizer", ")", "\n", "self", ".", "tgt_seq_length", "=", "args", ".", "tgt_seq_length", "\n", "self", ".", "src_vocab", ",", "self", ".", "src_tokenizer", "=", "vocab", ",", "tokenizer", "\n", "self", ".", "tgt_tokenizer", "=", "args", ".", "tgt_tokenizer", "\n", "self", ".", "tgt_vocab", "=", "self", ".", "tgt_tokenizer", ".", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.Seq2seqDataset.worker": [[852, 889], ["print", "uer.utils.seed.set_seed", "open", "open", "open.close", "f.readline", "f.readline", "f.readline.strip().split", "data.Seq2seqDataset.src_tokenizer.convert_tokens_to_ids", "data.Seq2seqDataset.tgt_tokenizer.convert_tokens_to_ids", "pickle.dump", "str", "len", "data.Seq2seqDataset.src_tokenizer.tokenize", "data.Seq2seqDataset.tgt_tokenizer.tokenize", "len", "len", "data.Seq2seqDataset.append", "seg.append", "len", "data.Seq2seqDataset.append", "f.readline.strip().split", "f.readline.strip", "data.Seq2seqDataset.src_vocab.get", "data.Seq2seqDataset.tgt_vocab.get", "data.Seq2seqDataset.src_vocab.get", "data.Seq2seqDataset.tgt_vocab.get", "f.readline.strip"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.seed.set_seed", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_tokens_to_ids", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_tokens_to_ids", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get"], ["", "def", "worker", "(", "self", ",", "proc_id", ",", "start", ",", "end", ")", ":", "\n", "        ", "print", "(", "\"Worker %d is building dataset ... \"", "%", "proc_id", ")", "\n", "set_seed", "(", "self", ".", "seed", ")", "\n", "dataset_writer", "=", "open", "(", "\"dataset-tmp-\"", "+", "str", "(", "proc_id", ")", "+", "\".pt\"", ",", "\"wb\"", ")", "\n", "pos", "=", "0", "\n", "with", "open", "(", "self", ".", "corpus_path", ",", "mode", "=", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "while", "pos", "<", "start", ":", "\n", "                ", "f", ".", "readline", "(", ")", "\n", "pos", "+=", "1", "\n", "", "while", "True", ":", "\n", "                ", "line", "=", "f", ".", "readline", "(", ")", "\n", "pos", "+=", "1", "\n", "\n", "if", "len", "(", "line", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", ")", "!=", "2", ":", "\n", "                    ", "if", "pos", ">=", "end", ":", "\n", "                        ", "break", "\n", "", "continue", "\n", "", "document_src", ",", "document_tgt", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "src", "=", "self", ".", "src_tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "src_tokenizer", ".", "tokenize", "(", "document_src", ")", ")", "\n", "tgt", "=", "self", ".", "tgt_tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tgt_tokenizer", ".", "tokenize", "(", "document_tgt", ")", ")", "\n", "\n", "src", "=", "[", "self", ".", "src_vocab", ".", "get", "(", "CLS_TOKEN", ")", "]", "+", "src", "+", "[", "self", ".", "src_vocab", ".", "get", "(", "SEP_TOKEN", ")", "]", "\n", "tgt", "=", "[", "self", ".", "tgt_vocab", ".", "get", "(", "CLS_TOKEN", ")", "]", "+", "tgt", "+", "[", "self", ".", "tgt_vocab", ".", "get", "(", "SEP_TOKEN", ")", "]", "\n", "seg", "=", "[", "1", "]", "*", "len", "(", "src", ")", "\n", "\n", "src", ",", "tgt", ",", "seg", "=", "src", "[", ":", "self", ".", "seq_length", "]", ",", "tgt", "[", ":", "self", ".", "tgt_seq_length", "+", "1", "]", ",", "seg", "[", ":", "self", ".", "seq_length", "]", "\n", "while", "len", "(", "src", ")", "!=", "self", ".", "seq_length", ":", "\n", "                    ", "src", ".", "append", "(", "PAD_ID", ")", "\n", "seg", ".", "append", "(", "PAD_ID", ")", "\n", "", "while", "len", "(", "tgt", ")", "!=", "self", ".", "tgt_seq_length", "+", "1", ":", "\n", "                    ", "tgt", ".", "append", "(", "PAD_ID", ")", "\n", "", "pickle", ".", "dump", "(", "(", "src", ",", "tgt", ",", "seg", ")", ",", "dataset_writer", ")", "\n", "\n", "if", "pos", ">=", "end", ":", "\n", "                    ", "break", "\n", "\n", "", "", "dataset_writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.Seq2seqDataLoader.__iter__": [[892, 918], ["data.Seq2seqDataLoader._empty", "data.Seq2seqDataLoader._fill_buf", "src.append", "tgt_in.append", "tgt_out.append", "seg.append", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.DataLoader._empty", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.DataLoader._fill_buf"], ["    ", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "while", "self", ".", "_empty", "(", ")", ":", "\n", "                ", "self", ".", "_fill_buf", "(", ")", "\n", "", "if", "self", ".", "start", "+", "self", ".", "batch_size", ">=", "self", ".", "end", ":", "\n", "                ", "instances", "=", "self", ".", "buffer", "[", "self", ".", "start", ":", "]", "\n", "", "else", ":", "\n", "                ", "instances", "=", "self", ".", "buffer", "[", "self", ".", "start", ":", "self", ".", "start", "+", "self", ".", "batch_size", "]", "\n", "\n", "", "self", ".", "start", "+=", "self", ".", "batch_size", "\n", "\n", "src", "=", "[", "]", "\n", "tgt_in", "=", "[", "]", "\n", "tgt_out", "=", "[", "]", "\n", "seg", "=", "[", "]", "\n", "\n", "for", "ins", "in", "instances", ":", "\n", "                ", "src", ".", "append", "(", "ins", "[", "0", "]", ")", "\n", "tgt_in", ".", "append", "(", "ins", "[", "1", "]", "[", ":", "-", "1", "]", ")", "\n", "tgt_out", ".", "append", "(", "ins", "[", "1", "]", "[", "1", ":", "]", ")", "\n", "seg", ".", "append", "(", "ins", "[", "2", "]", ")", "\n", "\n", "", "yield", "torch", ".", "LongTensor", "(", "src", ")", ",", "torch", ".", "LongTensor", "(", "tgt_in", ")", ",", "torch", ".", "LongTensor", "(", "tgt_out", ")", ",", "torch", ".", "LongTensor", "(", "seg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.T5DataLoader.__iter__": [[928, 999], ["data.T5DataLoader._empty", "enumerate", "range", "data.T5DataLoader._fill_buf", "data.T5DataLoader.vocab.get", "data.T5DataLoader.vocab.get", "tgt_in_single.append", "tgt_in_single.append", "src.append", "tgt_in.append", "tgt_out.append", "len", "len", "seg.append", "data.mask_seq", "seg.append", "data.T5DataLoader.vocab.get", "data.T5DataLoader.vocab.get", "len", "len", "src_with_sentinel.append", "len", "len", "len", "tgt_in[].append", "tgt_out[].append", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "tgt_in_single.append", "src_with_sentinel.append", "src_with_sentinel.append", "tgt_in_single.append", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.DataLoader._empty", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.DataLoader._fill_buf", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.mask_seq", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get"], ["    ", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "while", "self", ".", "_empty", "(", ")", ":", "\n", "                ", "self", ".", "_fill_buf", "(", ")", "\n", "", "if", "self", ".", "start", "+", "self", ".", "batch_size", ">=", "self", ".", "end", ":", "\n", "                ", "instances", "=", "self", ".", "buffer", "[", "self", ".", "start", ":", "]", "\n", "", "else", ":", "\n", "                ", "instances", "=", "self", ".", "buffer", "[", "self", ".", "start", ":", "self", ".", "start", "+", "self", ".", "batch_size", "]", "\n", "\n", "", "self", ".", "start", "+=", "self", ".", "batch_size", "\n", "\n", "src", "=", "[", "]", "\n", "tgt_in", "=", "[", "]", "\n", "tgt_out", "=", "[", "]", "\n", "seg", "=", "[", "]", "\n", "\n", "tgt_seq_length", "=", "0", "\n", "\n", "for", "_", ",", "ins", "in", "enumerate", "(", "instances", ")", ":", "\n", "                ", "if", "len", "(", "ins", ")", "==", "3", ":", "\n", "                    ", "src_single", "=", "ins", "[", "0", "]", "\n", "tgt_single", "=", "ins", "[", "1", "]", "\n", "seg", ".", "append", "(", "[", "1", "]", "*", "ins", "[", "2", "]", "[", "0", "]", "+", "[", "PAD_ID", "]", "*", "(", "len", "(", "ins", "[", "0", "]", ")", "-", "ins", "[", "2", "]", "[", "0", "]", ")", ")", "\n", "", "else", ":", "\n", "                    ", "src_single", ",", "tgt_single", "=", "mask_seq", "(", "ins", "[", "0", "]", ",", "self", ".", "tokenizer", ",", "self", ".", "whole_word_masking", ",", "self", ".", "span_masking", ",", "self", ".", "span_geo_prob", ",", "self", ".", "span_max_length", ")", "\n", "seg", ".", "append", "(", "[", "1", "]", "*", "ins", "[", "1", "]", "[", "0", "]", "+", "[", "PAD_ID", "]", "*", "(", "len", "(", "ins", "[", "0", "]", ")", "-", "ins", "[", "1", "]", "[", "0", "]", ")", ")", "\n", "\n", "", "MASK_ID", "=", "self", ".", "vocab", ".", "get", "(", "MASK_TOKEN", ")", "\n", "SENTINEL_ID", "=", "self", ".", "vocab", ".", "get", "(", "SENTINEL_TOKEN", ")", "\n", "\n", "for", "src_index", ",", "_", "in", "tgt_single", ":", "\n", "                    ", "if", "src_single", "[", "src_index", "]", "!=", "MASK_ID", ":", "\n", "                        ", "src_single", "[", "src_index", "]", "=", "MASK_ID", "\n", "\n", "", "", "tgt_in_single", "=", "[", "self", ".", "vocab", ".", "get", "(", "CLS_TOKEN", ")", "]", "\n", "mask_index", "=", "0", "\n", "src_with_sentinel", "=", "[", "]", "\n", "for", "token_id", "in", "src_single", ":", "\n", "                    ", "if", "token_id", "==", "MASK_ID", ":", "\n", "                        ", "if", "len", "(", "src_with_sentinel", ")", ">", "0", "and", "src_with_sentinel", "[", "-", "1", "]", "==", "(", "SENTINEL_ID", "-", "1", ")", ":", "\n", "                            ", "pass", "\n", "", "else", ":", "\n", "                            ", "src_with_sentinel", ".", "append", "(", "SENTINEL_ID", ")", "\n", "tgt_in_single", ".", "append", "(", "SENTINEL_ID", ")", "\n", "SENTINEL_ID", "+=", "1", "\n", "", "tgt_in_single", ".", "append", "(", "tgt_single", "[", "mask_index", "]", "[", "1", "]", ")", "\n", "mask_index", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "src_with_sentinel", ".", "append", "(", "token_id", ")", "\n", "", "", "tgt_in_single", ".", "append", "(", "SENTINEL_ID", ")", "\n", "tgt_in_single", ".", "append", "(", "self", ".", "vocab", ".", "get", "(", "SEP_TOKEN", ")", ")", "\n", "\n", "while", "len", "(", "src_with_sentinel", ")", "<", "len", "(", "src_single", ")", ":", "\n", "                    ", "src_with_sentinel", ".", "append", "(", "PAD_ID", ")", "\n", "\n", "", "if", "len", "(", "tgt_in_single", ")", ">", "tgt_seq_length", ":", "\n", "                    ", "tgt_seq_length", "=", "len", "(", "tgt_in_single", ")", "\n", "\n", "", "src", ".", "append", "(", "src_with_sentinel", ")", "\n", "tgt_in", ".", "append", "(", "tgt_in_single", ")", "\n", "tgt_out", ".", "append", "(", "tgt_in", "[", "-", "1", "]", "[", "1", ":", "]", "+", "[", "PAD_ID", "]", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "tgt_in", ")", ")", ":", "\n", "                ", "while", "len", "(", "tgt_in", "[", "i", "]", ")", "!=", "tgt_seq_length", ":", "\n", "                    ", "tgt_in", "[", "i", "]", ".", "append", "(", "PAD_ID", ")", "\n", "tgt_out", "[", "i", "]", ".", "append", "(", "PAD_ID", ")", "\n", "\n", "", "", "yield", "torch", ".", "LongTensor", "(", "src", ")", ",", "torch", ".", "LongTensor", "(", "tgt_in", ")", ",", "torch", ".", "LongTensor", "(", "tgt_out", ")", ",", "torch", ".", "LongTensor", "(", "seg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.ClsDataset.worker": [[1002, 1058], ["print", "uer.utils.seed.set_seed", "open", "open.close", "open", "f.readline", "f.readline", "line.strip().split.strip().split.strip().split", "str", "len", "int", "pickle.dump", "line.strip().split.strip().split.strip", "data.ClsDataset.vocab.get", "len", "len", "len", "int", "pickle.dump", "data.ClsDataset.tokenizer.tokenize", "data.ClsDataset.vocab.get", "len", "src.append", "seg.append", "data.ClsDataset.vocab.get", "data.ClsDataset.vocab.get", "len", "data.ClsDataset.tokenizer.tokenize", "data.ClsDataset.vocab.get", "data.ClsDataset.tokenizer.tokenize", "data.ClsDataset.vocab.get", "len", "len", "len", "src.append", "seg.append", "data.ClsDataset.vocab.get"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.seed.set_seed", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get"], ["    ", "def", "worker", "(", "self", ",", "proc_id", ",", "start", ",", "end", ")", ":", "\n", "        ", "print", "(", "\"Worker %d is building dataset ... \"", "%", "proc_id", ")", "\n", "set_seed", "(", "self", ".", "seed", ")", "\n", "f_write", "=", "open", "(", "\"dataset-tmp-\"", "+", "str", "(", "proc_id", ")", "+", "\".pt\"", ",", "\"wb\"", ")", "\n", "pos", "=", "0", "\n", "with", "open", "(", "self", ".", "corpus_path", ",", "mode", "=", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "while", "pos", "<", "start", ":", "\n", "                ", "line", "=", "f", ".", "readline", "(", ")", "\n", "pos", "+=", "1", "\n", "", "while", "True", ":", "\n", "                ", "line", "=", "f", ".", "readline", "(", ")", "\n", "pos", "+=", "1", "\n", "\n", "line", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "if", "len", "(", "line", ")", "==", "2", ":", "\n", "                    ", "label", "=", "int", "(", "line", "[", "0", "]", ")", "\n", "text", "=", "\" \"", ".", "join", "(", "line", "[", "1", ":", "]", ")", "\n", "src", "=", "[", "self", ".", "vocab", ".", "get", "(", "t", ")", "for", "t", "in", "self", ".", "tokenizer", ".", "tokenize", "(", "text", ")", "]", "\n", "src", "=", "[", "self", ".", "vocab", ".", "get", "(", "CLS_TOKEN", ")", "]", "+", "src", "\n", "tgt", "=", "label", "\n", "seg", "=", "[", "1", "]", "*", "len", "(", "src", ")", "\n", "if", "len", "(", "src", ")", ">=", "self", ".", "seq_length", ":", "\n", "                        ", "src", "=", "src", "[", ":", "self", ".", "seq_length", "]", "\n", "seg", "=", "seg", "[", ":", "self", ".", "seq_length", "]", "\n", "", "else", ":", "\n", "                        ", "while", "len", "(", "src", ")", "!=", "self", ".", "seq_length", ":", "\n", "                            ", "src", ".", "append", "(", "PAD_ID", ")", "\n", "seg", ".", "append", "(", "PAD_ID", ")", "\n", "", "", "pickle", ".", "dump", "(", "(", "src", ",", "tgt", ",", "seg", ")", ",", "f_write", ")", "\n", "", "elif", "len", "(", "line", ")", "==", "3", ":", "# For sentence pair input.", "\n", "                    ", "label", "=", "int", "(", "line", "[", "0", "]", ")", "\n", "text_a", ",", "text_b", "=", "line", "[", "1", "]", ",", "line", "[", "2", "]", "\n", "\n", "src_a", "=", "[", "self", ".", "vocab", ".", "get", "(", "t", ")", "for", "t", "in", "self", ".", "tokenizer", ".", "tokenize", "(", "text_a", ")", "]", "\n", "src_a", "=", "[", "self", ".", "vocab", ".", "get", "(", "CLS_TOKEN", ")", "]", "+", "src_a", "+", "[", "self", ".", "vocab", ".", "get", "(", "SEP_TOKEN", ")", "]", "\n", "src_b", "=", "[", "self", ".", "vocab", ".", "get", "(", "t", ")", "for", "t", "in", "self", ".", "tokenizer", ".", "tokenize", "(", "text_b", ")", "]", "\n", "src_b", "=", "src_b", "+", "[", "self", ".", "vocab", ".", "get", "(", "SEP_TOKEN", ")", "]", "\n", "\n", "src", "=", "src_a", "+", "src_b", "\n", "seg", "=", "[", "1", "]", "*", "len", "(", "src_a", ")", "+", "[", "2", "]", "*", "len", "(", "src_b", ")", "\n", "\n", "if", "len", "(", "src", ")", ">=", "self", ".", "seq_length", ":", "\n", "                        ", "src", "=", "src", "[", ":", "self", ".", "seq_length", "]", "\n", "seg", "=", "seg", "[", ":", "self", ".", "seq_length", "]", "\n", "", "else", ":", "\n", "                        ", "while", "len", "(", "src", ")", "!=", "self", ".", "seq_length", ":", "\n", "                            ", "src", ".", "append", "(", "PAD_ID", ")", "\n", "seg", ".", "append", "(", "PAD_ID", ")", "\n", "", "", "pickle", ".", "dump", "(", "(", "src", ",", "tgt", ",", "seg", ")", ",", "f_write", ")", "\n", "", "else", ":", "\n", "                    ", "pass", "\n", "\n", "", "if", "pos", ">=", "end", "-", "1", ":", "\n", "                    ", "break", "\n", "\n", "", "", "", "f_write", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.ClsDataLoader.__iter__": [[1061, 1084], ["data.ClsDataLoader._empty", "data.ClsDataLoader._fill_buf", "src.append", "tgt.append", "seg.append", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.DataLoader._empty", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.DataLoader._fill_buf"], ["    ", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "while", "self", ".", "_empty", "(", ")", ":", "\n", "                ", "self", ".", "_fill_buf", "(", ")", "\n", "", "if", "self", ".", "start", "+", "self", ".", "batch_size", ">=", "self", ".", "end", ":", "\n", "                ", "instances", "=", "self", ".", "buffer", "[", "self", ".", "start", ":", "]", "\n", "", "else", ":", "\n", "                ", "instances", "=", "self", ".", "buffer", "[", "self", ".", "start", ":", "self", ".", "start", "+", "self", ".", "batch_size", "]", "\n", "\n", "", "self", ".", "start", "+=", "self", ".", "batch_size", "\n", "\n", "src", "=", "[", "]", "\n", "tgt", "=", "[", "]", "\n", "seg", "=", "[", "]", "\n", "\n", "for", "ins", "in", "instances", ":", "\n", "                ", "src", ".", "append", "(", "ins", "[", "0", "]", ")", "\n", "tgt", ".", "append", "(", "ins", "[", "1", "]", ")", "\n", "seg", ".", "append", "(", "ins", "[", "2", "]", ")", "\n", "\n", "", "yield", "torch", ".", "LongTensor", "(", "src", ")", ",", "torch", ".", "LongTensor", "(", "tgt", ")", ",", "torch", ".", "LongTensor", "(", "seg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.PrefixlmDataset.worker": [[1088, 1131], ["print", "uer.utils.seed.set_seed", "open", "open", "open.close", "f.readline", "f.readline", "f.readline.strip().split", "data.PrefixlmDataset.tokenizer.convert_tokens_to_ids", "data.PrefixlmDataset.tokenizer.convert_tokens_to_ids", "seg_pos.append", "pickle.dump", "str", "len", "data.PrefixlmDataset.tokenizer.tokenize", "data.PrefixlmDataset.tokenizer.tokenize", "len", "len", "len", "data.PrefixlmDataset.append", "data.PrefixlmDataset.append", "f.readline.strip().split", "f.readline.strip", "data.PrefixlmDataset.vocab.get", "data.PrefixlmDataset.vocab.get", "data.PrefixlmDataset.vocab.get", "f.readline.strip"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.seed.set_seed", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_tokens_to_ids", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_tokens_to_ids", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get"], ["    ", "def", "worker", "(", "self", ",", "proc_id", ",", "start", ",", "end", ")", ":", "\n", "        ", "print", "(", "\"Worker %d is building dataset ... \"", "%", "proc_id", ")", "\n", "set_seed", "(", "self", ".", "seed", ")", "\n", "dataset_writer", "=", "open", "(", "\"dataset-tmp-\"", "+", "str", "(", "proc_id", ")", "+", "\".pt\"", ",", "\"wb\"", ")", "\n", "pos", "=", "0", "\n", "with", "open", "(", "self", ".", "corpus_path", ",", "mode", "=", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "while", "pos", "<", "start", ":", "\n", "                ", "f", ".", "readline", "(", ")", "\n", "pos", "+=", "1", "\n", "", "while", "True", ":", "\n", "                ", "line", "=", "f", ".", "readline", "(", ")", "\n", "pos", "+=", "1", "\n", "\n", "if", "len", "(", "line", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", ")", "!=", "2", ":", "\n", "                    ", "if", "pos", ">=", "end", ":", "\n", "                        ", "break", "\n", "", "continue", "\n", "", "document_src", ",", "document_tgt", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "src", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "document_src", ")", ")", "\n", "tgt", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "document_tgt", ")", ")", "\n", "src", "=", "[", "self", ".", "vocab", ".", "get", "(", "CLS_TOKEN", ")", "]", "+", "src", "+", "[", "self", ".", "vocab", ".", "get", "(", "SEP_TOKEN", ")", "]", "\n", "tgt", "=", "tgt", "+", "[", "self", ".", "vocab", ".", "get", "(", "SEP_TOKEN", ")", "]", "\n", "seg_pos", "=", "[", "len", "(", "src", ")", "]", "\n", "\n", "if", "seg_pos", "[", "0", "]", ">=", "self", ".", "seq_length", ":", "\n", "                    ", "continue", "\n", "\n", "", "src", "=", "src", "+", "tgt", "\n", "tgt", "=", "[", "0", "]", "*", "seg_pos", "[", "0", "]", "+", "tgt", "[", "1", ":", "]", "+", "[", "PAD_ID", "]", "\n", "seg_pos", ".", "append", "(", "len", "(", "src", ")", ")", "\n", "src", ",", "tgt", "=", "src", "[", ":", "self", ".", "seq_length", "]", ",", "tgt", "[", ":", "self", ".", "seq_length", "]", "\n", "while", "len", "(", "src", ")", "!=", "self", ".", "seq_length", ":", "\n", "                    ", "src", ".", "append", "(", "PAD_ID", ")", "\n", "tgt", ".", "append", "(", "PAD_ID", ")", "\n", "", "if", "seg_pos", "[", "1", "]", ">", "self", ".", "seq_length", ":", "\n", "                    ", "seg_pos", "[", "1", "]", "=", "self", ".", "seq_length", "\n", "\n", "", "pickle", ".", "dump", "(", "(", "src", ",", "tgt", ",", "seg_pos", ")", ",", "dataset_writer", ")", "\n", "\n", "if", "pos", ">=", "end", ":", "\n", "                    ", "break", "\n", "\n", "", "", "dataset_writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.PrefixlmDataLoader.__iter__": [[1134, 1157], ["data.PrefixlmDataLoader._empty", "data.PrefixlmDataLoader._fill_buf", "src.append", "tgt.append", "seg.append", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "len"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.DataLoader._empty", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.DataLoader._fill_buf"], ["    ", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "while", "self", ".", "_empty", "(", ")", ":", "\n", "                ", "self", ".", "_fill_buf", "(", ")", "\n", "", "if", "self", ".", "start", "+", "self", ".", "batch_size", ">=", "self", ".", "end", ":", "\n", "                ", "instances", "=", "self", ".", "buffer", "[", "self", ".", "start", ":", "]", "\n", "", "else", ":", "\n", "                ", "instances", "=", "self", ".", "buffer", "[", "self", ".", "start", ":", "self", ".", "start", "+", "self", ".", "batch_size", "]", "\n", "\n", "", "self", ".", "start", "+=", "self", ".", "batch_size", "\n", "\n", "src", "=", "[", "]", "\n", "tgt", "=", "[", "]", "\n", "seg", "=", "[", "]", "\n", "\n", "for", "ins", "in", "instances", ":", "\n", "                ", "src", ".", "append", "(", "ins", "[", "0", "]", ")", "\n", "tgt", ".", "append", "(", "ins", "[", "1", "]", ")", "\n", "seg", ".", "append", "(", "[", "1", "]", "*", "ins", "[", "2", "]", "[", "0", "]", "+", "[", "2", "]", "*", "(", "ins", "[", "2", "]", "[", "1", "]", "-", "ins", "[", "2", "]", "[", "0", "]", ")", "+", "[", "PAD_ID", "]", "*", "(", "len", "(", "ins", "[", "0", "]", ")", "-", "ins", "[", "2", "]", "[", "1", "]", ")", ")", "\n", "\n", "", "yield", "torch", ".", "LongTensor", "(", "src", ")", ",", "torch", ".", "LongTensor", "(", "tgt", ")", ",", "torch", ".", "LongTensor", "(", "seg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.mask_seq": [[12, 85], ["range", "data.create_index", "random.shuffle", "max", "sorted", "len", "len", "int", "len", "round", "len", "range", "sorted.append", "random.random", "range", "random.random", "sorted.append", "random.random", "len", "len", "len", "len", "vocab.get", "sorted.append", "range", "vocab.get", "len", "vocab.get", "range", "random.randint", "random.randint", "random.randint", "len", "vocab.get", "vocab.get", "vocab.get", "len", "vocab.get", "vocab.get", "vocab.get", "len", "vocab.get", "vocab.get", "vocab.get"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.create_index", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get"], ["def", "mask_seq", "(", "src", ",", "tokenizer", ",", "whole_word_masking", ",", "span_masking", ",", "span_geo_prob", ",", "span_max_length", ")", ":", "\n", "    ", "vocab", "=", "tokenizer", ".", "vocab", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "src", ")", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "        ", "if", "src", "[", "i", "]", "!=", "PAD_ID", ":", "\n", "            ", "break", "\n", "", "", "src_no_pad", "=", "src", "[", ":", "i", "+", "1", "]", "\n", "\n", "tokens_index", ",", "src_no_pad", "=", "create_index", "(", "src_no_pad", ",", "tokenizer", ",", "whole_word_masking", ",", "span_masking", ",", "span_geo_prob", ",", "span_max_length", ")", "\n", "if", "len", "(", "src_no_pad", ")", "<", "len", "(", "src", ")", ":", "\n", "        ", "src", "=", "src_no_pad", "+", "(", "len", "(", "src", ")", "-", "len", "(", "src_no_pad", ")", ")", "*", "[", "PAD_ID", "]", "\n", "", "else", ":", "\n", "        ", "src", "=", "src_no_pad", "\n", "\n", "", "random", ".", "shuffle", "(", "tokens_index", ")", "\n", "num_to_predict", "=", "max", "(", "1", ",", "int", "(", "round", "(", "len", "(", "src_no_pad", ")", "*", "0.15", ")", ")", ")", "\n", "tgt_mlm", "=", "[", "]", "\n", "for", "index_set", "in", "tokens_index", ":", "\n", "        ", "if", "len", "(", "tgt_mlm", ")", ">=", "num_to_predict", ":", "\n", "            ", "break", "\n", "", "if", "whole_word_masking", ":", "\n", "            ", "i", "=", "index_set", "[", "0", "]", "\n", "mask_len", "=", "index_set", "[", "1", "]", "\n", "if", "len", "(", "tgt_mlm", ")", "+", "mask_len", ">", "num_to_predict", ":", "\n", "                ", "continue", "\n", "\n", "", "for", "j", "in", "range", "(", "mask_len", ")", ":", "\n", "                ", "token", "=", "src", "[", "i", "+", "j", "]", "\n", "tgt_mlm", ".", "append", "(", "(", "i", "+", "j", ",", "token", ")", ")", "\n", "prob", "=", "random", ".", "random", "(", ")", "\n", "if", "prob", "<", "0.8", ":", "\n", "                    ", "src", "[", "i", "+", "j", "]", "=", "vocab", ".", "get", "(", "MASK_TOKEN", ")", "\n", "", "elif", "prob", "<", "0.9", ":", "\n", "                    ", "while", "True", ":", "\n", "                        ", "rdi", "=", "random", ".", "randint", "(", "1", ",", "len", "(", "vocab", ")", "-", "1", ")", "\n", "if", "rdi", "not", "in", "[", "vocab", ".", "get", "(", "CLS_TOKEN", ")", ",", "vocab", ".", "get", "(", "SEP_TOKEN", ")", ",", "vocab", ".", "get", "(", "MASK_TOKEN", ")", ",", "PAD_ID", "]", ":", "\n", "                            ", "break", "\n", "", "", "src", "[", "i", "+", "j", "]", "=", "rdi", "\n", "", "", "", "elif", "span_masking", ":", "\n", "            ", "i", "=", "index_set", "[", "0", "]", "\n", "span_len", "=", "index_set", "[", "1", "]", "\n", "if", "len", "(", "tgt_mlm", ")", "+", "span_len", ">", "num_to_predict", ":", "\n", "                ", "continue", "\n", "\n", "", "for", "j", "in", "range", "(", "span_len", ")", ":", "\n", "                ", "token", "=", "src", "[", "i", "+", "j", "]", "\n", "tgt_mlm", ".", "append", "(", "(", "i", "+", "j", ",", "token", ")", ")", "\n", "", "prob", "=", "random", ".", "random", "(", ")", "\n", "if", "prob", "<", "0.8", ":", "\n", "                ", "for", "j", "in", "range", "(", "span_len", ")", ":", "\n", "                    ", "src", "[", "i", "+", "j", "]", "=", "vocab", ".", "get", "(", "MASK_TOKEN", ")", "\n", "", "", "elif", "prob", "<", "0.9", ":", "\n", "                ", "for", "j", "in", "range", "(", "span_len", ")", ":", "\n", "                    ", "while", "True", ":", "\n", "                        ", "rdi", "=", "random", ".", "randint", "(", "1", ",", "len", "(", "vocab", ")", "-", "1", ")", "\n", "if", "rdi", "not", "in", "[", "vocab", ".", "get", "(", "CLS_TOKEN", ")", ",", "vocab", ".", "get", "(", "SEP_TOKEN", ")", ",", "vocab", ".", "get", "(", "MASK_TOKEN", ")", ",", "PAD_ID", "]", ":", "\n", "                            ", "break", "\n", "", "", "src", "[", "i", "+", "j", "]", "=", "rdi", "\n", "", "", "", "else", ":", "\n", "            ", "i", "=", "index_set", "[", "0", "]", "\n", "token", "=", "src", "[", "i", "]", "\n", "tgt_mlm", ".", "append", "(", "(", "i", ",", "token", ")", ")", "\n", "prob", "=", "random", ".", "random", "(", ")", "\n", "if", "prob", "<", "0.8", ":", "\n", "                ", "src", "[", "i", "]", "=", "vocab", ".", "get", "(", "MASK_TOKEN", ")", "\n", "", "elif", "prob", "<", "0.9", ":", "\n", "                ", "while", "True", ":", "\n", "                    ", "rdi", "=", "random", ".", "randint", "(", "1", ",", "len", "(", "vocab", ")", "-", "1", ")", "\n", "if", "rdi", "not", "in", "[", "vocab", ".", "get", "(", "CLS_TOKEN", ")", ",", "vocab", ".", "get", "(", "SEP_TOKEN", ")", ",", "vocab", ".", "get", "(", "MASK_TOKEN", ")", ",", "PAD_ID", "]", ":", "\n", "                        ", "break", "\n", "", "", "src", "[", "i", "]", "=", "rdi", "\n", "", "", "", "tgt_mlm", "=", "sorted", "(", "tgt_mlm", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "return", "src", ",", "tgt_mlm", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.create_index": [[87, 132], ["len", "jieba.cut", "enumerate", "vocab.get", "vocab.get", "len", "tokenizer.convert_tokens_to_ids", "len", "vocab.get", "tokenizer.tokenize", "len", "tokens_index.append", "vocab.get", "tokens_index.append", "data.get_span_len", "tokens_index.append", "vocab.get", "vocab.get", "len", "len", "tokenizer.convert_ids_to_tokens", "len"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.data_process.dataset_generation.cut", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_tokens_to_ids", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.get_span_len", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_ids_to_tokens"], ["", "def", "create_index", "(", "src", ",", "tokenizer", ",", "whole_word_masking", ",", "span_masking", ",", "span_geo_prob", ",", "span_max_length", ")", ":", "\n", "    ", "tokens_index", "=", "[", "]", "\n", "span_end_position", "=", "-", "1", "\n", "vocab", "=", "tokenizer", ".", "vocab", "\n", "if", "whole_word_masking", ":", "\n", "        ", "src_wwm", "=", "[", "]", "\n", "src_length", "=", "len", "(", "src", ")", "\n", "has_cls", ",", "has_sep", "=", "False", ",", "False", "\n", "if", "src", "[", "0", "]", "==", "vocab", ".", "get", "(", "CLS_TOKEN", ")", ":", "\n", "            ", "src", "=", "src", "[", "1", ":", "]", "\n", "has_cls", "=", "True", "\n", "", "if", "src", "[", "-", "1", "]", "==", "vocab", ".", "get", "(", "SEP_TOKEN", ")", ":", "\n", "            ", "src", "=", "src", "[", ":", "-", "1", "]", "\n", "has_sep", "=", "True", "\n", "", "sentence", "=", "\"\"", ".", "join", "(", "tokenizer", ".", "convert_ids_to_tokens", "(", "src", ")", ")", ".", "replace", "(", "'[UNK]'", ",", "''", ")", ".", "replace", "(", "'##'", ",", "''", ")", "\n", "import", "jieba", "\n", "wordlist", "=", "jieba", ".", "cut", "(", "sentence", ")", "\n", "if", "has_cls", ":", "\n", "            ", "src_wwm", "+=", "[", "vocab", ".", "get", "(", "CLS_TOKEN", ")", "]", "\n", "", "for", "word", "in", "wordlist", ":", "\n", "            ", "position", "=", "len", "(", "src_wwm", ")", "\n", "src_wwm", "+=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenizer", ".", "tokenize", "(", "word", ")", ")", "\n", "if", "len", "(", "src_wwm", ")", "<", "src_length", ":", "\n", "                ", "tokens_index", ".", "append", "(", "[", "position", ",", "len", "(", "src_wwm", ")", "-", "position", "]", ")", "\n", "", "", "if", "has_sep", ":", "\n", "            ", "src_wwm", "+=", "[", "vocab", ".", "get", "(", "SEP_TOKEN", ")", "]", "\n", "", "if", "len", "(", "src_wwm", ")", ">", "src_length", ":", "\n", "            ", "src", "=", "src_wwm", "[", ":", "src_length", "]", "\n", "", "else", ":", "\n", "            ", "src", "=", "src_wwm", "\n", "", "", "else", ":", "\n", "        ", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "src", ")", ":", "\n", "            ", "if", "token", "==", "vocab", ".", "get", "(", "CLS_TOKEN", ")", "or", "token", "==", "vocab", ".", "get", "(", "SEP_TOKEN", ")", "or", "token", "==", "PAD_ID", ":", "\n", "                ", "continue", "\n", "", "if", "not", "span_masking", ":", "\n", "                ", "tokens_index", ".", "append", "(", "[", "i", "]", ")", "\n", "", "else", ":", "\n", "                ", "if", "i", "<", "span_end_position", ":", "\n", "                    ", "continue", "\n", "", "span_len", "=", "get_span_len", "(", "span_max_length", ",", "span_geo_prob", ")", "\n", "span_end_position", "=", "i", "+", "span_len", "\n", "if", "span_end_position", ">", "len", "(", "src", ")", ":", "\n", "                    ", "span_len", "=", "len", "(", "src", ")", "-", "i", "\n", "", "tokens_index", ".", "append", "(", "[", "i", ",", "span_len", "]", ")", "\n", "", "", "", "return", "tokens_index", ",", "src", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.get_span_len": [[134, 152], ["range", "range", "random.random", "geo_prob_cum.append", "geo_prob_cum.append", "len"], "function", ["None"], ["", "def", "get_span_len", "(", "max_span_len", ",", "p", ")", ":", "\n", "    ", "geo_prob_cum", "=", "[", "0.0", "]", "\n", "geo_prob", "=", "1.0", "\n", "for", "i", "in", "range", "(", "max_span_len", "+", "1", ")", ":", "\n", "        ", "if", "i", "==", "0", ":", "\n", "            ", "continue", "\n", "", "if", "i", "==", "1", ":", "\n", "            ", "geo_prob", "*=", "p", "\n", "geo_prob_cum", ".", "append", "(", "geo_prob_cum", "[", "-", "1", "]", "+", "geo_prob", ")", "\n", "", "else", ":", "\n", "            ", "geo_prob", "*=", "(", "1", "-", "p", ")", "\n", "geo_prob_cum", ".", "append", "(", "geo_prob_cum", "[", "-", "1", "]", "+", "geo_prob", ")", "\n", "\n", "", "", "prob", "=", "geo_prob_cum", "[", "-", "1", "]", "*", "random", ".", "random", "(", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "geo_prob_cum", ")", "-", "1", ")", ":", "\n", "        ", "if", "prob", ">=", "geo_prob_cum", "[", "i", "]", "and", "prob", "<", "geo_prob_cum", "[", "i", "+", "1", "]", ":", "\n", "            ", "current_span_len", "=", "i", "+", "1", "\n", "", "", "return", "current_span_len", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.merge_dataset": [[154, 168], ["open", "range", "open.close", "open", "open.close", "os.remove", "open.read", "open.write", "str", "str"], "function", ["None"], ["", "def", "merge_dataset", "(", "dataset_path", ",", "workers_num", ")", ":", "\n", "# Merge datasets.", "\n", "    ", "dataset_writer", "=", "open", "(", "dataset_path", ",", "\"wb\"", ")", "\n", "for", "i", "in", "range", "(", "workers_num", ")", ":", "\n", "        ", "tmp_dataset_reader", "=", "open", "(", "\"dataset-tmp-\"", "+", "str", "(", "i", ")", "+", "\".pt\"", ",", "\"rb\"", ")", "\n", "while", "True", ":", "\n", "            ", "tmp_data", "=", "tmp_dataset_reader", ".", "read", "(", "2", "^", "20", ")", "\n", "if", "tmp_data", ":", "\n", "                ", "dataset_writer", ".", "write", "(", "tmp_data", ")", "\n", "", "else", ":", "\n", "                ", "break", "\n", "", "", "tmp_dataset_reader", ".", "close", "(", ")", "\n", "os", ".", "remove", "(", "\"dataset-tmp-\"", "+", "str", "(", "i", ")", "+", "\".pt\"", ")", "\n", "", "dataset_writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.data.truncate_seq_pair": [[170, 183], ["len", "len", "random.random", "trunc_tokens.pop", "len", "len"], "function", ["None"], ["", "def", "truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_num_tokens", ")", ":", "\n", "    ", "\"\"\" truncate sequence pair to specific length \"\"\"", "\n", "while", "True", ":", "\n", "        ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_num_tokens", ":", "\n", "            ", "break", "\n", "\n", "", "trunc_tokens", "=", "tokens_a", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", "else", "tokens_b", "\n", "\n", "if", "random", ".", "random", "(", ")", "<", "0.5", ":", "\n", "            ", "del", "trunc_tokens", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "trunc_tokens", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.Tokenizer.__init__": [[12, 37], ["spm.SentencePieceProcessor", "tokenizers.Tokenizer.sp_model.Load", "uer.utils.vocab.Vocab", "tokenizers.Tokenizer.vocab.load", "tokenizers.Tokenizer.sp_model.IdToPiece", "tokenizers.Tokenizer.vocab.items", "ImportError", "range", "tokenizers.Tokenizer.sp_model.GetPieceSize"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.load"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "is_src", "=", "True", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "None", "\n", "self", ".", "sp_model", "=", "None", "\n", "if", "is_src", "==", "True", ":", "\n", "            ", "spm_model_path", "=", "args", ".", "spm_model_path", "\n", "vocab_path", "=", "args", ".", "vocab_path", "\n", "", "else", ":", "\n", "            ", "spm_model_path", "=", "args", ".", "tgt_spm_model_path", "\n", "vocab_path", "=", "args", ".", "tgt_vocab_path", "\n", "\n", "", "if", "spm_model_path", ":", "\n", "            ", "try", ":", "\n", "                ", "import", "sentencepiece", "as", "spm", "\n", "", "except", "ImportError", ":", "\n", "                ", "raise", "ImportError", "(", "\"You need to install SentencePiece to use XLNetTokenizer: https://github.com/google/sentencepiece\"", "\n", "\"pip install sentencepiece\"", ")", "\n", "", "self", ".", "sp_model", "=", "spm", ".", "SentencePieceProcessor", "(", ")", "\n", "self", ".", "sp_model", ".", "Load", "(", "spm_model_path", ")", "\n", "self", ".", "vocab", "=", "{", "self", ".", "sp_model", ".", "IdToPiece", "(", "i", ")", ":", "i", "for", "i", "\n", "in", "range", "(", "self", ".", "sp_model", ".", "GetPieceSize", "(", ")", ")", "}", "\n", "", "else", ":", "\n", "            ", "self", ".", "vocab", "=", "Vocab", "(", ")", "\n", "self", ".", "vocab", ".", "load", "(", "vocab_path", ",", "is_quiet", "=", "True", ")", "\n", "self", ".", "vocab", "=", "self", ".", "vocab", ".", "w2i", "\n", "", "self", ".", "inv_vocab", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "vocab", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.Tokenizer.tokenize": [[38, 40], ["None"], "methods", ["None"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.Tokenizer.convert_tokens_to_ids": [[41, 47], ["tokenizers.convert_by_vocab", "tokenizers.Tokenizer.sp_model.PieceToId", "tokenizers.printable_text"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_by_vocab", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.printable_text"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "if", "self", ".", "sp_model", ":", "\n", "            ", "return", "[", "self", ".", "sp_model", ".", "PieceToId", "(", "\n", "printable_text", "(", "token", ")", ")", "for", "token", "in", "tokens", "]", "\n", "", "else", ":", "\n", "            ", "return", "convert_by_vocab", "(", "self", ".", "vocab", ",", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.Tokenizer.convert_ids_to_tokens": [[48, 53], ["tokenizers.convert_by_vocab", "tokenizers.Tokenizer.sp_model.IdToPiece"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_by_vocab"], ["", "", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "        ", "if", "self", ".", "sp_model", ":", "\n", "            ", "return", "[", "self", ".", "sp_model", ".", "IdToPiece", "(", "id_", ")", "for", "id_", "in", "ids", "]", "\n", "", "else", ":", "\n", "            ", "return", "convert_by_vocab", "(", "self", ".", "inv_vocab", ",", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.CharTokenizer.__init__": [[57, 59], ["tokenizers.Tokenizer.__init__"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "is_src", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "is_src", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.CharTokenizer.tokenize": [[60, 65], ["list", "list", "text.strip", "text.strip"], "methods", ["None"], ["", "def", "tokenize", "(", "self", ",", "text", ",", "use_vocab", "=", "True", ")", ":", "\n", "        ", "if", "use_vocab", ":", "\n", "            ", "return", "[", "token", "if", "token", "in", "self", ".", "vocab", "else", "\"[UNK]\"", "for", "token", "in", "list", "(", "text", ".", "strip", "(", ")", ")", "]", "\n", "", "else", ":", "\n", "            ", "return", "[", "token", "for", "token", "in", "list", "(", "text", ".", "strip", "(", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.SpaceTokenizer.__init__": [[69, 71], ["tokenizers.Tokenizer.__init__"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "is_src", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "is_src", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.SpaceTokenizer.tokenize": [[72, 77], ["text.strip().split", "text.strip().split", "text.strip", "text.strip"], "methods", ["None"], ["", "def", "tokenize", "(", "self", ",", "text", ",", "use_vocab", "=", "True", ")", ":", "\n", "        ", "if", "use_vocab", ":", "\n", "            ", "return", "[", "token", "if", "token", "in", "self", ".", "vocab", "else", "\"[UNK]\"", "for", "token", "in", "text", ".", "strip", "(", ")", ".", "split", "(", "\" \"", ")", "]", "\n", "", "else", ":", "\n", "            ", "return", "[", "token", "for", "token", "in", "text", ".", "strip", "(", ")", ".", "split", "(", "\" \"", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.BertTokenizer.__init__": [[217, 222], ["tokenizers.Tokenizer.__init__", "tokenizers.BasicTokenizer", "tokenizers.WordpieceTokenizer"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "is_src", "=", "True", ",", "do_lower_case", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "is_src", ")", "\n", "if", "not", "args", ".", "spm_model_path", ":", "\n", "            ", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.BertTokenizer.tokenize": [[223, 233], ["tokenizers.encode_pieces", "tokenizers.BertTokenizer.basic_tokenizer.tokenize", "tokenizers.BertTokenizer.wordpiece_tokenizer.tokenize", "encode_pieces.append"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.encode_pieces", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.WordpieceTokenizer.tokenize"], ["", "", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "if", "self", ".", "sp_model", ":", "\n", "            ", "split_tokens", "=", "encode_pieces", "(", "self", ".", "sp_model", ",", "text", ",", "return_unicode", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", ":", "\n", "                ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                    ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.BasicTokenizer.__init__": [[238, 244], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "do_lower_case", "=", "True", ")", ":", "\n", "        ", "\"\"\"Constructs a BasicTokenizer.\n        Args:\n            do_lower_case: Whether to lower case the input.\n        \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.BasicTokenizer.tokenize": [[245, 268], ["tokenizers.convert_to_unicode", "tokenizers.BasicTokenizer._clean_text", "tokenizers.BasicTokenizer._tokenize_chinese_chars", "tokenizers.whitespace_tokenize", "tokenizers.whitespace_tokenize", "split_tokens.extend", "tokenizers.BasicTokenizer.lower", "tokenizers.BasicTokenizer._run_strip_accents", "tokenizers.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_to_unicode", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.BasicTokenizer._tokenize_chinese_chars", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.whitespace_tokenize", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.whitespace_tokenize", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n", "# and generally don't have any Chinese data in them (there are Chinese", "\n", "# characters in the vocabulary because Wikipedia does have some Chinese", "\n", "# words in the English Wikipedia.).", "\n", "text", "=", "self", ".", "_tokenize_chinese_chars", "(", "text", ")", "\n", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "            ", "if", "self", ".", "do_lower_case", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.BasicTokenizer._run_strip_accents": [[269, 279], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["None"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "                ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.BasicTokenizer._run_split_on_punc": [[280, 299], ["list", "len", "tokenizers._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "            ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "start_new_word", ":", "\n", "                    ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.BasicTokenizer._tokenize_chinese_chars": [[300, 312], ["ord", "tokenizers.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.BasicTokenizer._is_chinese_char"], ["", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.BasicTokenizer._is_chinese_char": [[313, 334], ["None"], "methods", ["None"], ["", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "        ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "            ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.BasicTokenizer._clean_text": [[335, 347], ["ord", "tokenizers._is_whitespace", "tokenizers._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers._is_whitespace", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "                ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.WordpieceTokenizer.__init__": [[352, 356], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "200", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.WordpieceTokenizer.tokenize": [[357, 405], ["tokenizers.convert_to_unicode", "tokenizers.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend", "six.ensure_str"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_to_unicode", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.whitespace_tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text into its word pieces.\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n        For example:\n            input = \"unaffable\"\n            output = [\"un\", \"##aff\", \"##able\"]\n        Args:\n            text: A single token or whitespace separated tokens. This should have\n                already been passed through `BasicTokenizer.\n        Returns:\n            A list of wordpiece tokens.\n        \"\"\"", "\n", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "            ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "                ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "                    ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "                        ", "substr", "=", "\"##\"", "+", "six", ".", "ensure_str", "(", "substr", ")", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "                        ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "                    ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.preprocess_text": [[82, 100], ["unicodedata.normalize", "isinstance", "six.ensure_text.lower", "inputs.strip().split", "six.ensure_text", "six.ensure_text", "inputs.strip", "unicodedata.combining"], "function", ["None"], ["def", "preprocess_text", "(", "inputs", ",", "remove_space", "=", "True", ",", "lower", "=", "False", ")", ":", "\n", "    ", "\"\"\"preprocess data by removing extra space and normalize data.\"\"\"", "\n", "outputs", "=", "inputs", "\n", "if", "remove_space", ":", "\n", "        ", "outputs", "=", "\" \"", ".", "join", "(", "inputs", ".", "strip", "(", ")", ".", "split", "(", ")", ")", "\n", "\n", "", "if", "six", ".", "PY2", "and", "isinstance", "(", "outputs", ",", "str", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "outputs", "=", "six", ".", "ensure_text", "(", "outputs", ",", "\"utf-8\"", ")", "\n", "", "except", "UnicodeDecodeError", ":", "\n", "            ", "outputs", "=", "six", ".", "ensure_text", "(", "outputs", ",", "\"latin-1\"", ")", "\n", "\n", "", "", "outputs", "=", "unicodedata", ".", "normalize", "(", "\"NFKD\"", ",", "outputs", ")", "\n", "outputs", "=", "\"\"", ".", "join", "(", "[", "c", "for", "c", "in", "outputs", "if", "not", "unicodedata", ".", "combining", "(", "c", ")", "]", ")", "\n", "if", "lower", ":", "\n", "        ", "outputs", "=", "outputs", ".", "lower", "(", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.encode_pieces": [[102, 138], ["isinstance", "six.ensure_binary", "sp_model.EncodeAsPieces", "sp_model.SampleEncodeAsPieces", "tokenizers.printable_text", "piece[].isdigit", "sp_model.EncodeAsPieces", "sp_model.EncodeAsPieces.append", "new_pieces.extend", "new_pieces.append", "isinstance", "ret_pieces.append", "len", "six.ensure_binary().replace", "six.ensure_text", "len", "six.ensure_binary"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.printable_text"], ["", "def", "encode_pieces", "(", "sp_model", ",", "text", ",", "return_unicode", "=", "True", ",", "sample", "=", "False", ")", ":", "\n", "    ", "\"\"\"turn sentences into word pieces.\"\"\"", "\n", "\n", "if", "six", ".", "PY2", "and", "isinstance", "(", "text", ",", "six", ".", "text_type", ")", ":", "\n", "        ", "text", "=", "six", ".", "ensure_binary", "(", "text", ",", "\"utf-8\"", ")", "\n", "\n", "", "if", "not", "sample", ":", "\n", "        ", "pieces", "=", "sp_model", ".", "EncodeAsPieces", "(", "text", ")", "\n", "", "else", ":", "\n", "        ", "pieces", "=", "sp_model", ".", "SampleEncodeAsPieces", "(", "text", ",", "64", ",", "0.1", ")", "\n", "", "new_pieces", "=", "[", "]", "\n", "for", "piece", "in", "pieces", ":", "\n", "        ", "piece", "=", "printable_text", "(", "piece", ")", "\n", "if", "len", "(", "piece", ")", ">", "1", "and", "piece", "[", "-", "1", "]", "==", "\",\"", "and", "piece", "[", "-", "2", "]", ".", "isdigit", "(", ")", ":", "\n", "            ", "cur_pieces", "=", "sp_model", ".", "EncodeAsPieces", "(", "\n", "six", ".", "ensure_binary", "(", "piece", "[", ":", "-", "1", "]", ")", ".", "replace", "(", "SPIECE_UNDERLINE", ",", "b\"\"", ")", ")", "\n", "if", "piece", "[", "0", "]", "!=", "SPIECE_UNDERLINE", "and", "cur_pieces", "[", "0", "]", "[", "0", "]", "==", "SPIECE_UNDERLINE", ":", "\n", "                ", "if", "len", "(", "cur_pieces", "[", "0", "]", ")", "==", "1", ":", "\n", "                    ", "cur_pieces", "=", "cur_pieces", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "                    ", "cur_pieces", "[", "0", "]", "=", "cur_pieces", "[", "0", "]", "[", "1", ":", "]", "\n", "", "", "cur_pieces", ".", "append", "(", "piece", "[", "-", "1", "]", ")", "\n", "new_pieces", ".", "extend", "(", "cur_pieces", ")", "\n", "", "else", ":", "\n", "            ", "new_pieces", ".", "append", "(", "piece", ")", "\n", "\n", "# note(zhiliny): convert back to unicode for py2", "\n", "", "", "if", "six", ".", "PY2", "and", "return_unicode", ":", "\n", "        ", "ret_pieces", "=", "[", "]", "\n", "for", "piece", "in", "new_pieces", ":", "\n", "            ", "if", "isinstance", "(", "piece", ",", "str", ")", ":", "\n", "                ", "piece", "=", "six", ".", "ensure_text", "(", "piece", ",", "\"utf-8\"", ")", "\n", "", "ret_pieces", ".", "append", "(", "piece", ")", "\n", "", "new_pieces", "=", "ret_pieces", "\n", "\n", "", "return", "new_pieces", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.encode_ids": [[140, 144], ["tokenizers.encode_pieces", "sp_model.PieceToId"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.encode_pieces"], ["", "def", "encode_ids", "(", "sp_model", ",", "text", ",", "sample", "=", "False", ")", ":", "\n", "    ", "pieces", "=", "encode_pieces", "(", "sp_model", ",", "text", ",", "return_unicode", "=", "False", ",", "sample", "=", "sample", ")", "\n", "ids", "=", "[", "sp_model", ".", "PieceToId", "(", "piece", ")", "for", "piece", "in", "pieces", "]", "\n", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_to_unicode": [[146, 164], ["isinstance", "isinstance", "isinstance", "ValueError", "six.ensure_text", "ValueError", "six.ensure_text", "isinstance", "ValueError", "type", "type"], "function", ["None"], ["", "def", "convert_to_unicode", "(", "text", ")", ":", "\n", "    ", "\"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"", "\n", "if", "six", ".", "PY3", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "            ", "return", "six", ".", "ensure_text", "(", "text", ",", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "six", ".", "ensure_text", "(", "text", ",", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "elif", "isinstance", "(", "text", ",", "six", ".", "text_type", ")", ":", "\n", "            ", "return", "text", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.printable_text": [[166, 187], ["isinstance", "isinstance", "isinstance", "ValueError", "six.ensure_text", "ValueError", "isinstance", "six.ensure_binary", "ValueError", "type", "type"], "function", ["None"], ["", "", "def", "printable_text", "(", "text", ")", ":", "\n", "    ", "\"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"", "\n", "\n", "# These functions want `str` for both Python2 and Python3, but in one case", "\n", "# it's a Unicode string and in the other it's a byte string.", "\n", "if", "six", ".", "PY3", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "            ", "return", "six", ".", "ensure_text", "(", "text", ",", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "six", ".", "text_type", ")", ":", "\n", "            ", "return", "six", ".", "ensure_binary", "(", "text", ",", "\"utf-8\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_by_vocab": [[189, 195], ["output.append"], "function", ["None"], ["", "", "def", "convert_by_vocab", "(", "vocab", ",", "items", ")", ":", "\n", "    ", "\"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "item", "in", "items", ":", "\n", "        ", "output", ".", "append", "(", "vocab", "[", "item", "]", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_tokens_to_ids": [[197, 199], ["tokenizers.convert_by_vocab"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_by_vocab"], ["", "def", "convert_tokens_to_ids", "(", "vocab", ",", "tokens", ")", ":", "\n", "    ", "return", "convert_by_vocab", "(", "vocab", ",", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_ids_to_tokens": [[201, 203], ["tokenizers.convert_by_vocab"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.convert_by_vocab"], ["", "def", "convert_ids_to_tokens", "(", "inv_vocab", ",", "ids", ")", ":", "\n", "    ", "return", "convert_by_vocab", "(", "inv_vocab", ",", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.whitespace_tokenize": [[205, 212], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "        ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers._is_whitespace": [[407, 417], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically control characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers._is_control": [[419, 429], ["unicodedata.category"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "in", "(", "\"Cc\"", ",", "\"Cf\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers._is_punctuation": [[431, 445], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.misc.count_lines": [[6, 15], ["open", "f.read", "f.read.count"], "function", ["None"], ["def", "count_lines", "(", "file_path", ")", ":", "\n", "    ", "lines_num", "=", "0", "\n", "with", "open", "(", "file_path", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "data", "=", "f", ".", "read", "(", "2", "^", "20", ")", "\n", "if", "not", "data", ":", "\n", "                ", "break", "\n", "", "lines_num", "+=", "data", ".", "count", "(", "b'\\n'", ")", "\n", "", "", "return", "lines_num", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.misc.flip": [[17, 22], ["torch.arange", "torch.arange", "x.dim", "slice", "x.size", "tuple"], "function", ["None"], ["", "def", "flip", "(", "x", ",", "dim", ")", ":", "\n", "    ", "indices", "=", "[", "slice", "(", "None", ")", "]", "*", "x", ".", "dim", "(", ")", "\n", "indices", "[", "dim", "]", "=", "torch", ".", "arange", "(", "x", ".", "size", "(", "dim", ")", "-", "1", ",", "-", "1", ",", "-", "1", ",", "\n", "dtype", "=", "torch", ".", "long", ",", "device", "=", "x", ".", "device", ")", "\n", "return", "x", "[", "tuple", "(", "indices", ")", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.seed.set_seed": [[6, 13], ["random.seed", "str", "numpy.random.seed", "torch.manual_seed", "torch.cuda.manual_seed"], "function", ["None"], ["def", "set_seed", "(", "seed", "=", "7", ")", ":", "\n", "    ", "random", ".", "seed", "(", "seed", ")", "\n", "os", ".", "environ", "[", "'PYTHONHASHSEED'", "]", "=", "str", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed", "(", "seed", ")", "\n", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.subword.word2sub": [[7, 21], ["word_ids.contiguous().view().tolist.size", "word_ids.contiguous().view().tolist.contiguous().view().tolist", "max", "torch.zeros().to", "torch.zeros().to", "range", "len", "enumerate", "word_ids.contiguous().view().tolist.contiguous().view", "len", "torch.zeros", "torch.zeros", "sub_vocab.w2i.get", "word_ids.contiguous().view().tolist.contiguous", "len"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get"], ["def", "word2sub", "(", "word_ids", ",", "vocab", ",", "sub_vocab", ",", "subword_type", ")", ":", "\n", "    ", "'''\n    word_ids: batch_size, seq_length\n    '''", "\n", "batch_size", ",", "seq_length", "=", "word_ids", ".", "size", "(", ")", "\n", "device", "=", "word_ids", ".", "device", "\n", "word_ids", "=", "word_ids", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", ".", "tolist", "(", ")", "\n", "words", "=", "[", "vocab", ".", "i2w", "[", "i", "]", "for", "i", "in", "word_ids", "]", "\n", "max_length", "=", "max", "(", "[", "len", "(", "w", ")", "for", "w", "in", "words", "]", ")", "\n", "sub_ids", "=", "torch", ".", "zeros", "(", "(", "len", "(", "words", ")", ",", "max_length", ")", ",", "dtype", "=", "torch", ".", "long", ")", ".", "to", "(", "device", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "words", ")", ")", ":", "\n", "        ", "for", "j", ",", "c", "in", "enumerate", "(", "words", "[", "i", "]", ")", ":", "\n", "            ", "sub_ids", "[", "i", ",", "j", "]", "=", "sub_vocab", ".", "w2i", ".", "get", "(", "c", ",", "UNK_ID", ")", "\n", "", "", "return", "sub_ids", "\n", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.act_fun.gelu": [[7, 9], ["torch.erf", "torch.erf", "math.sqrt"], "function", ["None"], ["def", "gelu", "(", "x", ")", ":", "\n", "    ", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.act_fun.gelu_fast": [[10, 12], ["torch.tanh", "torch.tanh", "math.sqrt", "torch.pow", "torch.pow"], "function", ["None"], ["", "def", "gelu_fast", "(", "x", ")", ":", "\n", "    ", "return", "0.5", "*", "x", "*", "(", "1.0", "+", "torch", ".", "tanh", "(", "math", ".", "sqrt", "(", "2.0", "/", "math", ".", "pi", ")", "*", "(", "x", "+", "0.044715", "*", "torch", ".", "pow", "(", "x", ",", "3.0", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.act_fun.relu": [[13, 15], ["torch.relu"], "function", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.act_fun.relu"], ["", "def", "relu", "(", "x", ")", ":", "\n", "    ", "return", "F", ".", "relu", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.act_fun.linear": [[16, 18], ["None"], "function", ["None"], ["", "def", "linear", "(", "x", ")", ":", "\n", "    ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.act_fun._silu_python": [[19, 28], ["torch.sigmoid", "torch.sigmoid"], "function", ["None"], ["", "def", "_silu_python", "(", "x", ")", ":", "\n", "    ", "\"\"\"\n    See Gaussian Error Linear Units (Hendrycks et al., https://arxiv.org/abs/1606.08415) where the SiLU (Sigmoid Linear\n    Unit) was originally introduced and coined, and see Sigmoid-Weighted Linear Units for Neural Network Function\n    Approximation in Reinforcement Learning (Elfwing et al., https://arxiv.org/abs/1702.03118) and Swish: a Self-Gated\n    Activation Function (Ramachandran et al., https://arxiv.org/abs/1710.05941v1) where the SiLU was experimented with\n    later.\n    \"\"\"", "\n", "return", "x", "*", "torch", ".", "sigmoid", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.optimizers.AdamW.__init__": [[218, 237], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "params", ":", "Iterable", "[", "torch", ".", "nn", ".", "parameter", ".", "Parameter", "]", ",", "\n", "lr", ":", "float", "=", "1e-3", ",", "\n", "betas", ":", "Tuple", "[", "float", ",", "float", "]", "=", "(", "0.9", ",", "0.999", ")", ",", "\n", "eps", ":", "float", "=", "1e-6", ",", "\n", "weight_decay", ":", "float", "=", "0.0", ",", "\n", "correct_bias", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "if", "lr", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {} - should be >= 0.0\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {} - should be >= 0.0\"", ".", "format", "(", "eps", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ",", "correct_bias", "=", "correct_bias", ")", "\n", "super", "(", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.optimizers.AdamW.step": [[238, 297], ["closure", "exp_avg.mul_().add_", "exp_avg_sq.mul_().addcmul_", "exp_avg_sq.sqrt().add_", "p.data.addcdiv_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "p.data.add_", "exp_avg.mul_", "exp_avg_sq.mul_", "exp_avg_sq.sqrt", "math.sqrt"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", ":", "Callable", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Performs a single optimization step.\n        Arguments:\n            closure (:obj:`Callable`, `optional`): A closure that reevaluates the model and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "\"params\"", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\"Adam does not support sparse gradients, please consider SparseAdam instead\"", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "\"step\"", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "\"exp_avg\"", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "\"exp_avg_sq\"", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "\"exp_avg\"", "]", ",", "state", "[", "\"exp_avg_sq\"", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "\"betas\"", "]", "\n", "\n", "state", "[", "\"step\"", "]", "+=", "1", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "# In-place operations to update the averages at the same time", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "1.0", "-", "beta1", ")", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "grad", ",", "grad", ",", "value", "=", "1.0", "-", "beta2", ")", "\n", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "\"eps\"", "]", ")", "\n", "\n", "step_size", "=", "group", "[", "\"lr\"", "]", "\n", "if", "group", "[", "\"correct_bias\"", "]", ":", "# No bias correction for Bert", "\n", "                    ", "bias_correction1", "=", "1.0", "-", "beta1", "**", "state", "[", "\"step\"", "]", "\n", "bias_correction2", "=", "1.0", "-", "beta2", "**", "state", "[", "\"step\"", "]", "\n", "step_size", "=", "step_size", "*", "math", ".", "sqrt", "(", "bias_correction2", ")", "/", "bias_correction1", "\n", "\n", "", "p", ".", "data", ".", "addcdiv_", "(", "exp_avg", ",", "denom", ",", "value", "=", "-", "step_size", ")", "\n", "\n", "# Just adding the square of the weights to the loss function is *not*", "\n", "# the correct way of using L2 regularization/weight decay with Adam,", "\n", "# since that will interact with the m and v parameters in strange ways.", "\n", "#", "\n", "# Instead we want to decay the weights in a manner that doesn't interact", "\n", "# with the m/v parameters. This is equivalent to adding the square", "\n", "# of the weights to the loss with plain (non-momentum) SGD.", "\n", "# Add weight decay at the end (fixed version)", "\n", "if", "group", "[", "\"weight_decay\"", "]", ">", "0.0", ":", "\n", "                    ", "p", ".", "data", ".", "add_", "(", "p", ".", "data", ",", "alpha", "=", "-", "group", "[", "\"lr\"", "]", "*", "group", "[", "\"weight_decay\"", "]", ")", "\n", "\n", "", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.optimizers.Adafactor.__init__": [[354, 384], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "params", ",", "\n", "lr", "=", "None", ",", "\n", "eps", "=", "(", "1e-30", ",", "1e-3", ")", ",", "\n", "clip_threshold", "=", "1.0", ",", "\n", "decay_rate", "=", "-", "0.8", ",", "\n", "beta1", "=", "None", ",", "\n", "weight_decay", "=", "0.0", ",", "\n", "scale_parameter", "=", "True", ",", "\n", "relative_step", "=", "True", ",", "\n", "warmup_init", "=", "False", ",", "\n", ")", ":", "\n", "        ", "if", "lr", "is", "not", "None", "and", "relative_step", ":", "\n", "            ", "raise", "ValueError", "(", "\"Cannot combine manual lr and relative_step options\"", ")", "\n", "", "if", "warmup_init", "and", "not", "relative_step", ":", "\n", "            ", "raise", "ValueError", "(", "\"warmup_init requires relative_step=True\"", ")", "\n", "\n", "", "defaults", "=", "dict", "(", "\n", "lr", "=", "lr", ",", "\n", "eps", "=", "eps", ",", "\n", "clip_threshold", "=", "clip_threshold", ",", "\n", "decay_rate", "=", "decay_rate", ",", "\n", "beta1", "=", "beta1", ",", "\n", "weight_decay", "=", "weight_decay", ",", "\n", "scale_parameter", "=", "scale_parameter", ",", "\n", "relative_step", "=", "relative_step", ",", "\n", "warmup_init", "=", "warmup_init", ",", "\n", ")", "\n", "super", "(", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.optimizers.Adafactor._get_lr": [[385, 395], ["min", "max", "math.sqrt"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_lr", "(", "param_group", ",", "param_state", ")", ":", "\n", "        ", "rel_step_sz", "=", "param_group", "[", "\"lr\"", "]", "\n", "if", "param_group", "[", "\"relative_step\"", "]", ":", "\n", "            ", "min_step", "=", "1e-6", "*", "param_state", "[", "\"step\"", "]", "if", "param_group", "[", "\"warmup_init\"", "]", "else", "1e-2", "\n", "rel_step_sz", "=", "min", "(", "min_step", ",", "1.0", "/", "math", ".", "sqrt", "(", "param_state", "[", "\"step\"", "]", ")", ")", "\n", "", "param_scale", "=", "1.0", "\n", "if", "param_group", "[", "\"scale_parameter\"", "]", ":", "\n", "            ", "param_scale", "=", "max", "(", "param_group", "[", "\"eps\"", "]", "[", "1", "]", ",", "param_state", "[", "\"RMS\"", "]", ")", "\n", "", "return", "param_scale", "*", "rel_step_sz", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.optimizers.Adafactor._get_options": [[396, 401], ["len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_options", "(", "param_group", ",", "param_shape", ")", ":", "\n", "        ", "factored", "=", "len", "(", "param_shape", ")", ">=", "2", "\n", "use_first_moment", "=", "param_group", "[", "\"beta1\"", "]", "is", "not", "None", "\n", "return", "factored", ",", "use_first_moment", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.optimizers.Adafactor._rms": [[402, 405], ["tensor.norm", "tensor.numel"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_rms", "(", "tensor", ")", ":", "\n", "        ", "return", "tensor", ".", "norm", "(", "2", ")", "/", "(", "tensor", ".", "numel", "(", ")", "**", "0.5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.optimizers.Adafactor._approx_sq_grad": [[406, 411], ["exp_avg_sq_col.rsqrt", "torch.mm", "r_factor.unsqueeze", "exp_avg_sq_col.rsqrt.unsqueeze", "exp_avg_sq_row.mean"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_approx_sq_grad", "(", "exp_avg_sq_row", ",", "exp_avg_sq_col", ")", ":", "\n", "        ", "r_factor", "=", "(", "exp_avg_sq_row", "/", "exp_avg_sq_row", ".", "mean", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", ")", ".", "rsqrt_", "(", ")", "\n", "c_factor", "=", "exp_avg_sq_col", ".", "rsqrt", "(", ")", "\n", "return", "torch", ".", "mm", "(", "r_factor", ".", "unsqueeze", "(", "-", "1", ")", ",", "c_factor", ".", "unsqueeze", "(", "0", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.optimizers.Adafactor.step": [[412, 503], ["closure", "optimizers.Adafactor._get_options", "optimizers.Adafactor._rms", "optimizers.Adafactor._get_lr", "exp_avg_sq.rsqrt().mul_.div_", "exp_avg_sq.rsqrt().mul_.mul_", "p_data_fp32.float.float.add_", "grad.float.float.float", "RuntimeError", "len", "p_data_fp32.float.float.float", "math.pow", "exp_avg_sq_row.mul_().add_", "exp_avg_sq_col.mul_().add_", "optimizers.Adafactor._approx_sq_grad", "exp_avg_sq.rsqrt().mul_.mul_", "exp_avg_sq.mul_().add_", "exp_avg_sq.rsqrt().mul_", "exp_avg.mul_().add_", "p_data_fp32.float.float.add_", "p.data.copy_", "torch.zeros_like", "torch.zeros().to", "torch.zeros().to", "torch.zeros_like", "state[].to", "state[].to", "state[].to", "state[].to", "exp_avg_sq.rsqrt().mul_.mean", "exp_avg_sq.rsqrt().mul_.mean", "exp_avg_sq_row.mul_", "exp_avg_sq_col.mul_", "exp_avg_sq.mul_", "exp_avg_sq.rsqrt", "exp_avg.mul_", "torch.zeros", "torch.zeros", "optimizers.Adafactor._rms"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.optimizers.Adafactor._get_options", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.optimizers.Adafactor._rms", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.optimizers.Adafactor._get_lr", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.optimizers.Adafactor._approx_sq_grad", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.optimizers.Adafactor._rms"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Performs a single optimization step\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "\"params\"", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "dtype", "in", "{", "torch", ".", "float16", ",", "torch", ".", "bfloat16", "}", ":", "\n", "                    ", "grad", "=", "grad", ".", "float", "(", ")", "\n", "", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\"Adafactor does not support sparse gradients.\"", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "grad_shape", "=", "grad", ".", "shape", "\n", "\n", "factored", ",", "use_first_moment", "=", "self", ".", "_get_options", "(", "group", ",", "grad_shape", ")", "\n", "# State Initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "\"step\"", "]", "=", "0", "\n", "\n", "if", "use_first_moment", ":", "\n", "# Exponential moving average of gradient values", "\n", "                        ", "state", "[", "\"exp_avg\"", "]", "=", "torch", ".", "zeros_like", "(", "grad", ")", "\n", "", "if", "factored", ":", "\n", "                        ", "state", "[", "\"exp_avg_sq_row\"", "]", "=", "torch", ".", "zeros", "(", "grad_shape", "[", ":", "-", "1", "]", ")", ".", "to", "(", "grad", ")", "\n", "state", "[", "\"exp_avg_sq_col\"", "]", "=", "torch", ".", "zeros", "(", "grad_shape", "[", ":", "-", "2", "]", "+", "grad_shape", "[", "-", "1", ":", "]", ")", ".", "to", "(", "grad", ")", "\n", "", "else", ":", "\n", "                        ", "state", "[", "\"exp_avg_sq\"", "]", "=", "torch", ".", "zeros_like", "(", "grad", ")", "\n", "\n", "", "state", "[", "\"RMS\"", "]", "=", "0", "\n", "", "else", ":", "\n", "                    ", "if", "use_first_moment", ":", "\n", "                        ", "state", "[", "\"exp_avg\"", "]", "=", "state", "[", "\"exp_avg\"", "]", ".", "to", "(", "grad", ")", "\n", "", "if", "factored", ":", "\n", "                        ", "state", "[", "\"exp_avg_sq_row\"", "]", "=", "state", "[", "\"exp_avg_sq_row\"", "]", ".", "to", "(", "grad", ")", "\n", "state", "[", "\"exp_avg_sq_col\"", "]", "=", "state", "[", "\"exp_avg_sq_col\"", "]", ".", "to", "(", "grad", ")", "\n", "", "else", ":", "\n", "                        ", "state", "[", "\"exp_avg_sq\"", "]", "=", "state", "[", "\"exp_avg_sq\"", "]", ".", "to", "(", "grad", ")", "\n", "\n", "", "", "p_data_fp32", "=", "p", ".", "data", "\n", "if", "p", ".", "data", ".", "dtype", "in", "{", "torch", ".", "float16", ",", "torch", ".", "bfloat16", "}", ":", "\n", "                    ", "p_data_fp32", "=", "p_data_fp32", ".", "float", "(", ")", "\n", "\n", "", "state", "[", "\"step\"", "]", "+=", "1", "\n", "state", "[", "\"RMS\"", "]", "=", "self", ".", "_rms", "(", "p_data_fp32", ")", "\n", "group", "[", "\"lr\"", "]", "=", "self", ".", "_get_lr", "(", "group", ",", "state", ")", "\n", "\n", "beta2t", "=", "1.0", "-", "math", ".", "pow", "(", "state", "[", "\"step\"", "]", ",", "group", "[", "\"decay_rate\"", "]", ")", "\n", "update", "=", "(", "grad", "**", "2", ")", "+", "group", "[", "\"eps\"", "]", "[", "0", "]", "\n", "if", "factored", ":", "\n", "                    ", "exp_avg_sq_row", "=", "state", "[", "\"exp_avg_sq_row\"", "]", "\n", "exp_avg_sq_col", "=", "state", "[", "\"exp_avg_sq_col\"", "]", "\n", "\n", "exp_avg_sq_row", ".", "mul_", "(", "beta2t", ")", ".", "add_", "(", "1.0", "-", "beta2t", ",", "update", ".", "mean", "(", "dim", "=", "-", "1", ")", ")", "\n", "exp_avg_sq_col", ".", "mul_", "(", "beta2t", ")", ".", "add_", "(", "1.0", "-", "beta2t", ",", "update", ".", "mean", "(", "dim", "=", "-", "2", ")", ")", "\n", "\n", "# Approximation of exponential moving average of square of gradient", "\n", "update", "=", "self", ".", "_approx_sq_grad", "(", "exp_avg_sq_row", ",", "exp_avg_sq_col", ")", "\n", "update", ".", "mul_", "(", "grad", ")", "\n", "", "else", ":", "\n", "                    ", "exp_avg_sq", "=", "state", "[", "\"exp_avg_sq\"", "]", "\n", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2t", ")", ".", "add_", "(", "1.0", "-", "beta2t", ",", "update", ")", "\n", "update", "=", "exp_avg_sq", ".", "rsqrt", "(", ")", ".", "mul_", "(", "grad", ")", "\n", "\n", "", "update", ".", "div_", "(", "(", "self", ".", "_rms", "(", "update", ")", "/", "group", "[", "\"clip_threshold\"", "]", ")", ".", "clamp_", "(", "min", "=", "1.0", ")", ")", "\n", "update", ".", "mul_", "(", "group", "[", "\"lr\"", "]", ")", "\n", "\n", "if", "use_first_moment", ":", "\n", "                    ", "exp_avg", "=", "state", "[", "\"exp_avg\"", "]", "\n", "exp_avg", ".", "mul_", "(", "group", "[", "\"beta1\"", "]", ")", ".", "add_", "(", "1", "-", "group", "[", "\"beta1\"", "]", ",", "update", ")", "\n", "update", "=", "exp_avg", "\n", "\n", "", "if", "group", "[", "\"weight_decay\"", "]", "!=", "0", ":", "\n", "                    ", "p_data_fp32", ".", "add_", "(", "-", "group", "[", "\"weight_decay\"", "]", "*", "group", "[", "\"lr\"", "]", ",", "p_data_fp32", ")", "\n", "\n", "", "p_data_fp32", ".", "add_", "(", "-", "update", ")", "\n", "\n", "if", "p", ".", "data", ".", "dtype", "in", "{", "torch", ".", "float16", ",", "torch", ".", "bfloat16", "}", ":", "\n", "                    ", "p", ".", "data", ".", "copy_", "(", "p_data_fp32", ")", "\n", "\n", "", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.optimizers.get_constant_schedule": [[25, 37], ["torch.optim.lr_scheduler.LambdaLR"], "function", ["None"], ["def", "get_constant_schedule", "(", "optimizer", ":", "Optimizer", ",", "last_epoch", ":", "int", "=", "-", "1", ")", ":", "\n", "    ", "\"\"\"\n    Create a schedule with a constant learning rate, using the learning rate set in optimizer.\n    Args:\n        optimizer (:class:`~torch.optim.Optimizer`):\n            The optimizer for which to schedule the learning rate.\n        last_epoch (:obj:`int`, `optional`, defaults to -1):\n            The index of the last epoch when resuming training.\n    Return:\n        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"", "\n", "return", "LambdaLR", "(", "optimizer", ",", "lambda", "_", ":", "1", ",", "last_epoch", "=", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.optimizers.get_constant_schedule_with_warmup": [[39, 60], ["torch.optim.lr_scheduler.LambdaLR", "float", "float", "max"], "function", ["None"], ["", "def", "get_constant_schedule_with_warmup", "(", "optimizer", ":", "Optimizer", ",", "num_warmup_steps", ":", "int", ",", "last_epoch", ":", "int", "=", "-", "1", ")", ":", "\n", "    ", "\"\"\"\n    Create a schedule with a constant learning rate preceded by a warmup period during which the learning rate\n    increases linearly between 0 and the initial lr set in the optimizer.\n    Args:\n        optimizer (:class:`~torch.optim.Optimizer`):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (:obj:`int`):\n            The number of steps for the warmup phase.\n        last_epoch (:obj:`int`, `optional`, defaults to -1):\n            The index of the last epoch when resuming training.\n    Return:\n        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"", "\n", "\n", "def", "lr_lambda", "(", "current_step", ":", "int", ")", ":", "\n", "        ", "if", "current_step", "<", "num_warmup_steps", ":", "\n", "            ", "return", "float", "(", "current_step", ")", "/", "float", "(", "max", "(", "1.0", ",", "num_warmup_steps", ")", ")", "\n", "", "return", "1.0", "\n", "\n", "", "return", "LambdaLR", "(", "optimizer", ",", "lr_lambda", ",", "last_epoch", "=", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.optimizers.get_linear_schedule_with_warmup": [[62, 87], ["torch.optim.lr_scheduler.LambdaLR", "max", "float", "float", "float", "float", "max", "max"], "function", ["None"], ["", "def", "get_linear_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", ",", "num_training_steps", ",", "last_epoch", "=", "-", "1", ")", ":", "\n", "    ", "\"\"\"\n    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after\n    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.\n    Args:\n        optimizer (:class:`~torch.optim.Optimizer`):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (:obj:`int`):\n            The number of steps for the warmup phase.\n        num_training_steps (:obj:`int`):\n            The total number of training steps.\n        last_epoch (:obj:`int`, `optional`, defaults to -1):\n            The index of the last epoch when resuming training.\n    Return:\n        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"", "\n", "\n", "def", "lr_lambda", "(", "current_step", ":", "int", ")", ":", "\n", "        ", "if", "current_step", "<", "num_warmup_steps", ":", "\n", "            ", "return", "float", "(", "current_step", ")", "/", "float", "(", "max", "(", "1", ",", "num_warmup_steps", ")", ")", "\n", "", "return", "max", "(", "\n", "0.0", ",", "float", "(", "num_training_steps", "-", "current_step", ")", "/", "float", "(", "max", "(", "1", ",", "num_training_steps", "-", "num_warmup_steps", ")", ")", "\n", ")", "\n", "\n", "", "return", "LambdaLR", "(", "optimizer", ",", "lr_lambda", ",", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.optimizers.get_cosine_schedule_with_warmup": [[89, 119], ["torch.optim.lr_scheduler.LambdaLR", "max", "float", "float", "float", "float", "max", "max", "math.cos", "float"], "function", ["None"], ["", "def", "get_cosine_schedule_with_warmup", "(", "\n", "optimizer", ":", "Optimizer", ",", "num_warmup_steps", ":", "int", ",", "num_training_steps", ":", "int", ",", "num_cycles", ":", "float", "=", "0.5", ",", "last_epoch", ":", "int", "=", "-", "1", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Create a schedule with a learning rate that decreases following the values of the cosine function between the\n    initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n    initial lr set in the optimizer.\n    Args:\n        optimizer (:class:`~torch.optim.Optimizer`):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (:obj:`int`):\n            The number of steps for the warmup phase.\n        num_training_steps (:obj:`int`):\n            The total number of training steps.\n        num_cycles (:obj:`float`, `optional`, defaults to 0.5):\n            The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0\n            following a half-cosine).\n        last_epoch (:obj:`int`, `optional`, defaults to -1):\n            The index of the last epoch when resuming training.\n    Return:\n        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"", "\n", "\n", "def", "lr_lambda", "(", "current_step", ")", ":", "\n", "        ", "if", "current_step", "<", "num_warmup_steps", ":", "\n", "            ", "return", "float", "(", "current_step", ")", "/", "float", "(", "max", "(", "1", ",", "num_warmup_steps", ")", ")", "\n", "", "progress", "=", "float", "(", "current_step", "-", "num_warmup_steps", ")", "/", "float", "(", "max", "(", "1", ",", "num_training_steps", "-", "num_warmup_steps", ")", ")", "\n", "return", "max", "(", "0.0", ",", "0.5", "*", "(", "1.0", "+", "math", ".", "cos", "(", "math", ".", "pi", "*", "float", "(", "num_cycles", ")", "*", "2.0", "*", "progress", ")", ")", ")", "\n", "\n", "", "return", "LambdaLR", "(", "optimizer", ",", "lr_lambda", ",", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.optimizers.get_cosine_with_hard_restarts_schedule_with_warmup": [[121, 152], ["torch.optim.lr_scheduler.LambdaLR", "max", "float", "float", "float", "float", "max", "max", "math.cos", "float"], "function", ["None"], ["", "def", "get_cosine_with_hard_restarts_schedule_with_warmup", "(", "\n", "optimizer", ":", "Optimizer", ",", "num_warmup_steps", ":", "int", ",", "num_training_steps", ":", "int", ",", "num_cycles", ":", "int", "=", "1", ",", "last_epoch", ":", "int", "=", "-", "1", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Create a schedule with a learning rate that decreases following the values of the cosine function between the\n    initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases\n    linearly between 0 and the initial lr set in the optimizer.\n    Args:\n        optimizer (:class:`~torch.optim.Optimizer`):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (:obj:`int`):\n            The number of steps for the warmup phase.\n        num_training_steps (:obj:`int`):\n            The total number of training steps.\n        num_cycles (:obj:`int`, `optional`, defaults to 1):\n            The number of hard restarts to use.\n        last_epoch (:obj:`int`, `optional`, defaults to -1):\n            The index of the last epoch when resuming training.\n    Return:\n        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"", "\n", "\n", "def", "lr_lambda", "(", "current_step", ")", ":", "\n", "        ", "if", "current_step", "<", "num_warmup_steps", ":", "\n", "            ", "return", "float", "(", "current_step", ")", "/", "float", "(", "max", "(", "1", ",", "num_warmup_steps", ")", ")", "\n", "", "progress", "=", "float", "(", "current_step", "-", "num_warmup_steps", ")", "/", "float", "(", "max", "(", "1", ",", "num_training_steps", "-", "num_warmup_steps", ")", ")", "\n", "if", "progress", ">=", "1.0", ":", "\n", "            ", "return", "0.0", "\n", "", "return", "max", "(", "0.0", ",", "0.5", "*", "(", "1.0", "+", "math", ".", "cos", "(", "math", ".", "pi", "*", "(", "(", "float", "(", "num_cycles", ")", "*", "progress", ")", "%", "1.0", ")", ")", ")", ")", "\n", "\n", "", "return", "LambdaLR", "(", "optimizer", ",", "lr_lambda", ",", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.optimizers.get_polynomial_decay_schedule_with_warmup": [[154, 197], ["torch.optim.lr_scheduler.LambdaLR", "float", "float", "max"], "function", ["None"], ["", "def", "get_polynomial_decay_schedule_with_warmup", "(", "\n", "optimizer", ",", "num_warmup_steps", ",", "num_training_steps", ",", "lr_end", "=", "1e-7", ",", "power", "=", "1.0", ",", "last_epoch", "=", "-", "1", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Create a schedule with a learning rate that decreases as a polynomial decay from the initial lr set in the\n    optimizer to end lr defined by `lr_end`, after a warmup period during which it increases linearly from 0 to the\n    initial lr set in the optimizer.\n    Args:\n        optimizer (:class:`~torch.optim.Optimizer`):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (:obj:`int`):\n            The number of steps for the warmup phase.\n        num_training_steps (:obj:`int`):\n            The total number of training steps.\n        lr_end (:obj:`float`, `optional`, defaults to 1e-7):\n            The end LR.\n        power (:obj:`float`, `optional`, defaults to 1.0):\n            Power factor.\n        last_epoch (:obj:`int`, `optional`, defaults to -1):\n            The index of the last epoch when resuming training.\n    Note: `power` defaults to 1.0 as in the fairseq implementation, which in turn is based on the original BERT\n    implementation at\n    https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37\n    Return:\n        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"", "\n", "\n", "lr_init", "=", "optimizer", ".", "defaults", "[", "\"lr\"", "]", "\n", "assert", "lr_init", ">", "lr_end", ",", "f\"lr_end ({lr_end}) must be be smaller than initial lr ({lr_init})\"", "\n", "\n", "def", "lr_lambda", "(", "current_step", ":", "int", ")", ":", "\n", "        ", "if", "current_step", "<", "num_warmup_steps", ":", "\n", "            ", "return", "float", "(", "current_step", ")", "/", "float", "(", "max", "(", "1", ",", "num_warmup_steps", ")", ")", "\n", "", "elif", "current_step", ">", "num_training_steps", ":", "\n", "            ", "return", "lr_end", "/", "lr_init", "# as LambdaLR multiplies by lr_init", "\n", "", "else", ":", "\n", "            ", "lr_range", "=", "lr_init", "-", "lr_end", "\n", "decay_steps", "=", "num_training_steps", "-", "num_warmup_steps", "\n", "pct_remaining", "=", "1", "-", "(", "current_step", "-", "num_warmup_steps", ")", "/", "decay_steps", "\n", "decay", "=", "lr_range", "*", "pct_remaining", "**", "power", "+", "lr_end", "\n", "return", "decay", "/", "lr_init", "# as LambdaLR multiplies by lr_init", "\n", "\n", "", "", "return", "LambdaLR", "(", "optimizer", ",", "lr_lambda", ",", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.__init__": [[12, 18], ["os.path.abspath", "os.path.join", "os.path.dirname"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "w2i", "=", "{", "}", "\n", "self", ".", "i2w", "=", "[", "]", "\n", "self", ".", "w2c", "=", "{", "}", "\n", "self", ".", "reserved_vocab_path", "=", "os", ".", "path", ".", "abspath", "(", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "__file__", ")", ",", "\"../../models/reserved_vocab.txt\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.load": [[19, 27], ["open", "enumerate", "print", "vocab.Vocab.i2w.append", "len", "line.strip", "line.strip", "line.strip().split", "line.strip"], "methods", ["None"], ["", "def", "load", "(", "self", ",", "vocab_path", ",", "is_quiet", "=", "False", ")", ":", "\n", "        ", "with", "open", "(", "vocab_path", ",", "mode", "=", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "reader", ":", "\n", "            ", "for", "index", ",", "line", "in", "enumerate", "(", "reader", ")", ":", "\n", "                ", "w", "=", "line", ".", "strip", "(", "\"\\n\"", ")", ".", "split", "(", ")", "[", "0", "]", "if", "line", ".", "strip", "(", ")", "else", "line", ".", "strip", "(", "\"\\n\"", ")", "\n", "self", ".", "w2i", "[", "w", "]", "=", "index", "\n", "self", ".", "i2w", ".", "append", "(", "w", ")", "\n", "", "", "if", "not", "is_quiet", ":", "\n", "            ", "print", "(", "\"Vocabulary size: \"", ",", "len", "(", "self", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.save": [[28, 34], ["print", "print", "len", "open", "f.write"], "methods", ["None"], ["", "", "def", "save", "(", "self", ",", "save_path", ")", ":", "\n", "        ", "print", "(", "\"Vocabulary size: \"", ",", "len", "(", "self", ")", ")", "\n", "with", "open", "(", "save_path", ",", "mode", "=", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "for", "w", "in", "self", ".", "i2w", ":", "\n", "                ", "f", ".", "write", "(", "w", "+", "\"\\n\"", ")", "\n", "", "", "print", "(", "\"Vocabulary saving done.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get": [[35, 37], ["None"], "methods", ["None"], ["", "def", "get", "(", "self", ",", "w", ")", ":", "\n", "        ", "return", "self", ".", "w2i", "[", "w", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.__len__": [[38, 40], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "i2w", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.worker": [[41, 64], ["open", "f.readline", "f.readline", "tokenizer.tokenize", "i2w.append", "len"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.tokenizers.WordpieceTokenizer.tokenize"], ["", "def", "worker", "(", "self", ",", "corpus_path", ",", "tokenizer", ",", "start", ",", "end", ")", ":", "\n", "        ", "\"\"\" \n        Worker that creates vocabulary from corpus[start:end].\n        \"\"\"", "\n", "w2i", ",", "i2w", ",", "w2c", "=", "{", "}", ",", "[", "]", ",", "{", "}", "\n", "pos", "=", "0", "\n", "with", "open", "(", "corpus_path", ",", "mode", "=", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "while", "pos", "<", "start", ":", "\n", "                ", "f", ".", "readline", "(", ")", "\n", "pos", "+=", "1", "\n", "", "while", "True", ":", "\n", "                ", "line", "=", "f", ".", "readline", "(", ")", "\n", "pos", "+=", "1", "\n", "\n", "tokens", "=", "tokenizer", ".", "tokenize", "(", "line", ",", "use_vocab", "=", "False", ")", "\n", "for", "t", "in", "tokens", ":", "\n", "                    ", "if", "t", "not", "in", "w2i", ":", "\n", "                        ", "w2i", "[", "t", "]", ",", "w2c", "[", "t", "]", "=", "len", "(", "i2w", ")", ",", "1", "\n", "i2w", ".", "append", "(", "t", ")", "\n", "", "else", ":", "\n", "                        ", "w2c", "[", "t", "]", "+=", "1", "\n", "", "", "if", "pos", ">=", "end", "-", "1", ":", "\n", "                    ", "return", "(", "w2i", ",", "i2w", ",", "w2c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.union": [[65, 78], ["v_p.get", "i2w.append", "len"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.get"], ["", "", "", "", "def", "union", "(", "self", ",", "vocab_list", ")", ":", "\n", "        ", "\"\"\" Union vocab in all workers. \"\"\"", "\n", "w2i", ",", "i2w", ",", "w2c", "=", "{", "}", ",", "[", "]", ",", "{", "}", "\n", "index", "=", "0", "\n", "for", "v_p", "in", "vocab_list", ":", "\n", "            ", "w2i_p", ",", "i2w_p", ",", "w2c_p", "=", "v_p", ".", "get", "(", ")", "\n", "for", "w", "in", "i2w_p", ":", "\n", "                ", "if", "w", "not", "in", "w2i", ":", "\n", "                    ", "w2i", "[", "w", "]", ",", "w2c", "[", "w", "]", "=", "len", "(", "i2w", ")", ",", "w2c_p", "[", "w", "]", "\n", "i2w", ".", "append", "(", "w", ")", "\n", "", "else", ":", "\n", "                    ", "w2c", "[", "w", "]", "+=", "w2c_p", "[", "w", "]", "\n", "", "", "", "return", "(", "w2i", ",", "i2w", ",", "w2c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.build": [[79, 111], ["print", "uer.utils.misc.count_lines", "multiprocessing.Pool", "range", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "vocab.Vocab.union", "sorted", "enumerate", "vocab_list.append", "w2c.items", "open", "multiprocessing.Pool.apply_async", "vocab.Vocab.i2w.append", "line.strip().split", "len", "line.strip"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.misc.count_lines", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.vocab.Vocab.union"], ["", "def", "build", "(", "self", ",", "corpus_path", ",", "tokenizer", ",", "workers_num", "=", "1", ",", "min_count", "=", "1", ")", ":", "\n", "        ", "\"\"\" Build vocabulary from the given corpus. \"\"\"", "\n", "print", "(", "\"Start %d workers for building vocabulary...\"", "%", "workers_num", ")", "\n", "lines_num", "=", "count_lines", "(", "corpus_path", ")", "\n", "pool", "=", "Pool", "(", "workers_num", ")", "\n", "vocab_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "workers_num", ")", ":", "\n", "            ", "start", "=", "i", "*", "lines_num", "//", "workers_num", "\n", "end", "=", "(", "i", "+", "1", ")", "*", "lines_num", "//", "workers_num", "\n", "vocab_list", ".", "append", "(", "(", "pool", ".", "apply_async", "(", "func", "=", "self", ".", "worker", ",", "args", "=", "[", "corpus_path", ",", "tokenizer", ",", "start", ",", "end", "]", ")", ")", ")", "\n", "", "pool", ".", "close", "(", ")", "\n", "pool", ".", "join", "(", ")", "\n", "\n", "# Union vocab in all workers.", "\n", "w2i", ",", "i2w", ",", "w2c", "=", "self", ".", "union", "(", "vocab_list", ")", "\n", "# Sort w2c according to word count.", "\n", "sorted_w2c", "=", "sorted", "(", "w2c", ".", "items", "(", ")", ",", "key", "=", "lambda", "item", ":", "item", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "# Add special symbols and remove low frequency words.", "\n", "with", "open", "(", "self", ".", "reserved_vocab_path", ",", "mode", "=", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "reader", ":", "\n", "            ", "self", ".", "i2w", "=", "[", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "[", "0", "]", "for", "line", "in", "reader", "]", "\n", "\n", "", "for", "i", ",", "w", "in", "enumerate", "(", "self", ".", "i2w", ")", ":", "\n", "            ", "self", ".", "w2i", "[", "w", "]", "=", "i", "\n", "self", ".", "w2c", "[", "w", "]", "=", "-", "1", "\n", "\n", "", "for", "w", ",", "c", "in", "sorted_w2c", ":", "\n", "            ", "if", "c", "<", "min_count", ":", "\n", "                ", "break", "\n", "", "if", "w", "not", "in", "self", ".", "w2i", ":", "\n", "                ", "self", ".", "w2i", "[", "w", "]", ",", "self", ".", "w2c", "[", "w", "]", "=", "len", "(", "self", ".", "i2w", ")", ",", "c", "\n", "self", ".", "i2w", ".", "append", "(", "w", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.relative_position_embedding.RelativePositionEmbedding.__init__": [[11, 17], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "heads_num", ",", "bidirectional", "=", "True", ",", "num_buckets", "=", "32", ",", "max_distance", "=", "128", ")", ":", "\n", "        ", "super", "(", "RelativePositionEmbedding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_buckets", "=", "num_buckets", "\n", "self", ".", "bidirectional", "=", "bidirectional", "\n", "self", ".", "max_distance", "=", "max_distance", "\n", "self", ".", "relative_attention_bias", "=", "nn", ".", "Embedding", "(", "self", ".", "num_buckets", ",", "heads_num", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.relative_position_embedding.RelativePositionEmbedding.forward": [[19, 44], ["relative_position_embedding.RelativePositionEmbedding.relative_position_bucket", "relative_position_bucket.to.to.to", "relative_position_embedding.RelativePositionEmbedding.relative_attention_bias", "values.permute().unsqueeze.permute().unsqueeze.permute().unsqueeze", "encoder_hidden.size", "decoder_hidden.size", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "values.permute().unsqueeze.permute().unsqueeze.permute"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.relative_position_embedding.RelativePositionEmbedding.relative_position_bucket"], ["", "def", "forward", "(", "self", ",", "encoder_hidden", ",", "decoder_hidden", ")", ":", "\n", "        ", "\"\"\"\n        Compute binned relative position bias\n        Args:\n            encoder_hidden: [batch_size x seq_length x emb_size]\n            decoder_hidden: [batch_size x seq_length x emb_size]\n        Returns:\n            position_bias: [1 x heads_num x seq_length x seq_length]\n        \"\"\"", "\n", "query_length", "=", "encoder_hidden", ".", "size", "(", ")", "[", "1", "]", "\n", "key_length", "=", "decoder_hidden", ".", "size", "(", ")", "[", "1", "]", "\n", "\n", "context_position", "=", "torch", ".", "arange", "(", "query_length", ",", "dtype", "=", "torch", ".", "long", ")", "[", ":", ",", "None", "]", "\n", "memory_position", "=", "torch", ".", "arange", "(", "key_length", ",", "dtype", "=", "torch", ".", "long", ")", "[", "None", ",", ":", "]", "\n", "relative_position", "=", "memory_position", "-", "context_position", "# shape (query_length, key_length)", "\n", "relative_position_bucket", "=", "self", ".", "relative_position_bucket", "(", "\n", "relative_position", ",", "# shape (query_length, key_length)", "\n", "bidirectional", "=", "self", ".", "bidirectional", ",", "\n", "num_buckets", "=", "self", ".", "num_buckets", ",", "\n", "max_distance", "=", "self", ".", "max_distance", "\n", ")", "\n", "relative_position_bucket", "=", "relative_position_bucket", ".", "to", "(", "self", ".", "relative_attention_bias", ".", "weight", ".", "device", ")", "\n", "values", "=", "self", ".", "relative_attention_bias", "(", "relative_position_bucket", ")", "# shape (query_length, key_length, num_heads)", "\n", "values", "=", "values", ".", "permute", "(", "[", "2", ",", "0", ",", "1", "]", ")", ".", "unsqueeze", "(", "0", ")", "# shape (1, num_heads, query_length, key_length)", "\n", "return", "values", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.relative_position_embedding.RelativePositionEmbedding.relative_position_bucket": [[46, 89], ["torch.min", "torch.min", "torch.min", "torch.min", "torch.where", "torch.where", "torch.where", "torch.where", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.full_like", "torch.full_like", "torch.full_like", "torch.full_like", "torch.min", "torch.min", "torch.min", "torch.min", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.log", "torch.log", "torch.log", "torch.log", "math.log", "torch.abs.float", "torch.abs.float"], "methods", ["None"], ["", "def", "relative_position_bucket", "(", "self", ",", "relative_position", ",", "bidirectional", ",", "num_buckets", ",", "max_distance", ")", ":", "\n", "        ", "\"\"\"\n        Adapted from Mesh Tensorflow:\n        https://github.com/tensorflow/mesh/blob/0cb87fe07da627bf0b7e60475d59f95ed6b5be3d/mesh_tensorflow/transformer/transformer_layers.py#L593\n        Translate relative position to a bucket number for relative attention. The relative position is defined as\n        memory_position - query_position, i.e. the distance in tokens from the attending position to the attended-to\n        position. If bidirectional=False, then positive relative positions are invalid. We use smaller buckets for\n        small absolute relative_position and larger buckets for larger absolute relative_positions. All relative\n        positions >=max_distance map to the same bucket. All relative positions <=-max_distance map to the same bucket.\n        This should allow for more graceful generalization to longer sequences than the model has been trained on\n        Args:\n            relative_position: an int32 Tensor\n            bidirectional: a boolean - whether the attention is bidirectional\n            num_buckets: an integer\n            max_distance: an integer\n        Returns:\n            a Tensor with the same shape as relative_position, containing int32 values in the range [0, num_buckets)\n        \"\"\"", "\n", "relative_buckets", "=", "0", "\n", "if", "bidirectional", ":", "\n", "            ", "num_buckets", "//=", "2", "\n", "relative_buckets", "+=", "(", "relative_position", ">", "0", ")", ".", "to", "(", "torch", ".", "long", ")", "*", "num_buckets", "\n", "relative_position", "=", "torch", ".", "abs", "(", "relative_position", ")", "\n", "", "else", ":", "\n", "            ", "relative_position", "=", "-", "torch", ".", "min", "(", "relative_position", ",", "torch", ".", "zeros_like", "(", "relative_position", ")", ")", "\n", "# now relative_position is in the range [0, inf)", "\n", "\n", "# half of the buckets are for exact increments in positions", "\n", "", "max_exact", "=", "num_buckets", "//", "2", "\n", "is_small", "=", "relative_position", "<", "max_exact", "\n", "\n", "# The other half of the buckets are for logarithmically bigger bins in positions up to max_distance", "\n", "relative_postion_if_large", "=", "max_exact", "+", "(", "\n", "torch", ".", "log", "(", "relative_position", ".", "float", "(", ")", "/", "max_exact", ")", "\n", "/", "math", ".", "log", "(", "max_distance", "/", "max_exact", ")", "\n", "*", "(", "num_buckets", "-", "max_exact", ")", "\n", ")", ".", "to", "(", "torch", ".", "long", ")", "\n", "relative_postion_if_large", "=", "torch", ".", "min", "(", "\n", "relative_postion_if_large", ",", "torch", ".", "full_like", "(", "relative_postion_if_large", ",", "num_buckets", "-", "1", ")", "\n", ")", "\n", "\n", "relative_buckets", "+=", "torch", ".", "where", "(", "is_small", ",", "relative_position", ",", "relative_postion_if_large", ")", "\n", "return", "relative_buckets", "\n", "", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.multi_headed_attn.MultiHeadedAttention.__init__": [[12, 26], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "range"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "hidden_size", ",", "heads_num", ",", "attention_head_size", ",", "dropout", ",", "has_bias", "=", "True", ",", "with_scale", "=", "True", ")", ":", "\n", "        ", "super", "(", "MultiHeadedAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "heads_num", "=", "heads_num", "\n", "\n", "self", ".", "per_head_size", "=", "attention_head_size", "\n", "self", ".", "with_scale", "=", "with_scale", "\n", "self", ".", "inner_hidden_size", "=", "heads_num", "*", "attention_head_size", "\n", "\n", "self", ".", "linear_layers", "=", "nn", ".", "ModuleList", "(", "\n", "[", "nn", ".", "Linear", "(", "hidden_size", ",", "self", ".", "inner_hidden_size", ",", "bias", "=", "has_bias", ")", "for", "_", "in", "range", "(", "3", ")", "]", "\n", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "final_linear", "=", "nn", ".", "Linear", "(", "self", ".", "inner_hidden_size", ",", "hidden_size", ",", "bias", "=", "has_bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.multi_headed_attn.MultiHeadedAttention.forward": [[27, 72], ["query.size", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "multi_headed_attn.MultiHeadedAttention.dropout", "multi_headed_attn.MultiHeadedAttention.forward.unshape"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "key", ",", "value", ",", "query", ",", "mask", ",", "position_bias", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            key: [batch_size x seq_length x hidden_size]\n            value: [batch_size x seq_length x hidden_size]\n            query: [batch_size x seq_length x hidden_size]\n            mask: [batch_size x 1 x seq_length x seq_length]\n            position_bias: [1 x heads_num x seq_length x seq_length]\n        Returns:\n            output: [batch_size x seq_length x hidden_size]\n        \"\"\"", "\n", "batch_size", ",", "seq_length", ",", "_", "=", "query", ".", "size", "(", ")", "\n", "heads_num", "=", "self", ".", "heads_num", "\n", "per_head_size", "=", "self", ".", "per_head_size", "\n", "\n", "def", "shape", "(", "x", ")", ":", "\n", "            ", "return", "x", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "seq_length", ",", "heads_num", ",", "per_head_size", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "", "def", "unshape", "(", "x", ")", ":", "\n", "            ", "return", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "seq_length", ",", "self", ".", "inner_hidden_size", ")", "\n", "\n", "\n", "", "query", ",", "key", ",", "value", "=", "[", "l", "(", "x", ")", ".", "view", "(", "batch_size", ",", "-", "1", ",", "heads_num", ",", "per_head_size", ")", ".", "transpose", "(", "1", ",", "2", ")", "for", "l", ",", "x", "in", "zip", "(", "self", ".", "linear_layers", ",", "(", "query", ",", "key", ",", "value", ")", ")", "\n", "]", "\n", "\n", "scores", "=", "torch", ".", "matmul", "(", "query", ",", "key", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "\n", "if", "position_bias", "is", "not", "None", ":", "\n", "            ", "scores", "=", "scores", "+", "position_bias", "\n", "", "if", "self", ".", "with_scale", ":", "\n", "            ", "scores", "=", "scores", "/", "math", ".", "sqrt", "(", "float", "(", "per_head_size", ")", ")", "\n", "", "scores", "=", "scores", "+", "mask", "\n", "probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "scores", ")", "\n", "probs", "=", "self", ".", "dropout", "(", "probs", ")", "\n", "output", "=", "unshape", "(", "torch", ".", "matmul", "(", "probs", ",", "value", ")", ")", "\n", "output", "=", "self", ".", "final_linear", "(", "output", ")", "\n", "return", "output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.position_ffn.PositionwiseFeedForward.__init__": [[6, 11], ["torch.Module.__init__", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "hidden_size", ",", "feedforward_size", ",", "hidden_act", ",", "has_bias", "=", "True", ")", ":", "\n", "        ", "super", "(", "PositionwiseFeedForward", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "linear_1", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "feedforward_size", ",", "bias", "=", "has_bias", ")", "\n", "self", ".", "linear_2", "=", "nn", ".", "Linear", "(", "feedforward_size", ",", "hidden_size", ",", "bias", "=", "has_bias", ")", "\n", "self", ".", "act", "=", "str2act", "[", "hidden_act", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.position_ffn.PositionwiseFeedForward.forward": [[12, 16], ["position_ffn.PositionwiseFeedForward.act", "position_ffn.PositionwiseFeedForward.linear_2", "position_ffn.PositionwiseFeedForward.linear_1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "inter", "=", "self", ".", "act", "(", "self", ".", "linear_1", "(", "x", ")", ")", "\n", "output", "=", "self", ".", "linear_2", "(", "inter", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.position_ffn.GatedFeedForward.__init__": [[22, 28], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "hidden_size", ",", "feedforward_size", ",", "hidden_act", ",", "has_bias", "=", "True", ")", ":", "\n", "        ", "super", "(", "GatedFeedForward", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "linear_gate", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "feedforward_size", ",", "bias", "=", "has_bias", ")", "\n", "self", ".", "linear_1", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "feedforward_size", ",", "bias", "=", "has_bias", ")", "\n", "self", ".", "linear_2", "=", "nn", ".", "Linear", "(", "feedforward_size", ",", "hidden_size", ",", "bias", "=", "has_bias", ")", "\n", "self", ".", "act", "=", "str2act", "[", "hidden_act", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.position_ffn.GatedFeedForward.forward": [[29, 36], ["position_ffn.GatedFeedForward.act", "position_ffn.GatedFeedForward.linear_1", "position_ffn.GatedFeedForward.linear_2", "position_ffn.GatedFeedForward.linear_gate"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "gate", "=", "self", ".", "act", "(", "self", ".", "linear_gate", "(", "x", ")", ")", "\n", "inter_linear", "=", "self", ".", "linear_1", "(", "x", ")", "\n", "inter", "=", "gate", "*", "inter_linear", "\n", "output", "=", "self", ".", "linear_2", "(", "inter", ")", "\n", "\n", "return", "output", "", "", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.layer_norm.LayerNorm.__init__": [[6, 11], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ",", "eps", "=", "1e-6", ")", ":", "\n", "        ", "super", "(", "LayerNorm", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "eps", "=", "eps", "\n", "self", ".", "gamma", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "hidden_size", ")", ")", "\n", "self", ".", "beta", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "hidden_size", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.layer_norm.LayerNorm.forward": [[12, 18], ["x.mean", "x.std"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "mean", "=", "x", ".", "mean", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "std", "=", "x", ".", "std", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "hidden_states", "=", "self", ".", "gamma", "*", "(", "x", "-", "mean", ")", "/", "(", "std", "+", "self", ".", "eps", ")", "\n", "\n", "return", "hidden_states", "+", "self", ".", "beta", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.layer_norm.T5LayerNorm.__init__": [[21, 28], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ",", "eps", "=", "1e-6", ")", ":", "\n", "        ", "\"\"\"\n        Construct a layernorm module in the T5 style No bias and no subtraction of mean.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "hidden_size", ")", ")", "\n", "self", ".", "variance_epsilon", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.layer_norm.T5LayerNorm.forward": [[29, 35], ["hidden_states.to().pow().mean", "torch.rsqrt", "torch.rsqrt", "torch.rsqrt", "torch.rsqrt", "hidden_states.to().pow", "hidden_states.to"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# layer norm should always be calculated in float32", "\n", "        ", "variance", "=", "hidden_states", ".", "to", "(", "torch", ".", "float32", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "hidden_states", "=", "hidden_states", "*", "torch", ".", "rsqrt", "(", "variance", "+", "self", ".", "variance_epsilon", ")", "\n", "\n", "return", "self", ".", "weight", "*", "hidden_states", "", "", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.embeddings.WordEmbedding.__init__": [[11, 18], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.Embedding", "torch.Embedding", "uer.layers.layer_norm.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "vocab_size", ")", ":", "\n", "        ", "super", "(", "WordEmbedding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "remove_embedding_layernorm", "=", "args", ".", "remove_embedding_layernorm", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "args", ".", "dropout", ")", "\n", "self", ".", "word_embedding", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "args", ".", "emb_size", ")", "\n", "if", "not", "self", ".", "remove_embedding_layernorm", ":", "\n", "            ", "self", ".", "layer_norm", "=", "LayerNorm", "(", "args", ".", "emb_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.embeddings.WordEmbedding.forward": [[19, 25], ["embeddings.WordEmbedding.word_embedding", "embeddings.WordEmbedding.dropout", "embeddings.WordEmbedding.layer_norm"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "src", ",", "_", ")", ":", "\n", "        ", "emb", "=", "self", ".", "word_embedding", "(", "src", ")", "\n", "if", "not", "self", ".", "remove_embedding_layernorm", ":", "\n", "            ", "emb", "=", "self", ".", "layer_norm", "(", "emb", ")", "\n", "", "emb", "=", "self", ".", "dropout", "(", "emb", ")", "\n", "return", "emb", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.embeddings.WordPosEmbedding.__init__": [[33, 42], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "uer.layers.layer_norm.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "vocab_size", ")", ":", "\n", "        ", "super", "(", "WordPosEmbedding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "remove_embedding_layernorm", "=", "args", ".", "remove_embedding_layernorm", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "args", ".", "dropout", ")", "\n", "self", ".", "max_seq_length", "=", "args", ".", "max_seq_length", "\n", "self", ".", "word_embedding", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "args", ".", "emb_size", ")", "\n", "self", ".", "position_embedding", "=", "nn", ".", "Embedding", "(", "self", ".", "max_seq_length", ",", "args", ".", "emb_size", ")", "\n", "if", "not", "self", ".", "remove_embedding_layernorm", ":", "\n", "            ", "self", ".", "layer_norm", "=", "LayerNorm", "(", "args", ".", "emb_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.embeddings.WordPosEmbedding.forward": [[43, 56], ["embeddings.WordPosEmbedding.word_embedding", "embeddings.WordPosEmbedding.position_embedding", "embeddings.WordPosEmbedding.dropout", "torch.arange().unsqueeze().repeat", "torch.arange().unsqueeze().repeat", "torch.arange().unsqueeze().repeat", "torch.arange().unsqueeze().repeat", "embeddings.WordPosEmbedding.layer_norm", "embeddings.WordPosEmbedding.size", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "embeddings.WordPosEmbedding.size"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "src", ",", "_", ")", ":", "\n", "        ", "word_emb", "=", "self", ".", "word_embedding", "(", "src", ")", "\n", "pos_emb", "=", "self", ".", "position_embedding", "(", "\n", "torch", ".", "arange", "(", "0", ",", "word_emb", ".", "size", "(", "1", ")", ",", "device", "=", "word_emb", ".", "device", ",", "dtype", "=", "torch", ".", "long", ")", "\n", ".", "unsqueeze", "(", "0", ")", "\n", ".", "repeat", "(", "word_emb", ".", "size", "(", "0", ")", ",", "1", ")", "\n", ")", "\n", "\n", "emb", "=", "word_emb", "+", "pos_emb", "\n", "if", "not", "self", ".", "remove_embedding_layernorm", ":", "\n", "            ", "emb", "=", "self", ".", "layer_norm", "(", "emb", ")", "\n", "", "emb", "=", "self", ".", "dropout", "(", "emb", ")", "\n", "return", "emb", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.embeddings.WordPosSegEmbedding.__init__": [[63, 73], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "uer.layers.layer_norm.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "vocab_size", ")", ":", "\n", "        ", "super", "(", "WordPosSegEmbedding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "remove_embedding_layernorm", "=", "args", ".", "remove_embedding_layernorm", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "args", ".", "dropout", ")", "\n", "self", ".", "max_seq_length", "=", "args", ".", "max_seq_length", "\n", "self", ".", "word_embedding", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "args", ".", "emb_size", ")", "\n", "self", ".", "position_embedding", "=", "nn", ".", "Embedding", "(", "self", ".", "max_seq_length", ",", "args", ".", "emb_size", ")", "\n", "self", ".", "segment_embedding", "=", "nn", ".", "Embedding", "(", "3", ",", "args", ".", "emb_size", ")", "\n", "if", "not", "self", ".", "remove_embedding_layernorm", ":", "\n", "            ", "self", ".", "layer_norm", "=", "LayerNorm", "(", "args", ".", "emb_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.embeddings.WordPosSegEmbedding.forward": [[74, 88], ["embeddings.WordPosSegEmbedding.word_embedding", "embeddings.WordPosSegEmbedding.position_embedding", "embeddings.WordPosSegEmbedding.segment_embedding", "embeddings.WordPosSegEmbedding.dropout", "torch.arange().unsqueeze().repeat", "torch.arange().unsqueeze().repeat", "torch.arange().unsqueeze().repeat", "torch.arange().unsqueeze().repeat", "embeddings.WordPosSegEmbedding.layer_norm", "embeddings.WordPosSegEmbedding.size", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "embeddings.WordPosSegEmbedding.size"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "src", ",", "seg", ")", ":", "\n", "        ", "word_emb", "=", "self", ".", "word_embedding", "(", "src", ")", "\n", "pos_emb", "=", "self", ".", "position_embedding", "(", "\n", "torch", ".", "arange", "(", "0", ",", "word_emb", ".", "size", "(", "1", ")", ",", "device", "=", "word_emb", ".", "device", ",", "dtype", "=", "torch", ".", "long", ")", "\n", ".", "unsqueeze", "(", "0", ")", "\n", ".", "repeat", "(", "word_emb", ".", "size", "(", "0", ")", ",", "1", ")", "\n", ")", "\n", "seg_emb", "=", "self", ".", "segment_embedding", "(", "seg", ")", "\n", "\n", "emb", "=", "word_emb", "+", "pos_emb", "+", "seg_emb", "\n", "if", "not", "self", ".", "remove_embedding_layernorm", ":", "\n", "            ", "emb", "=", "self", ".", "layer_norm", "(", "emb", ")", "\n", "", "emb", "=", "self", ".", "dropout", "(", "emb", ")", "\n", "return", "emb", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.embeddings.WordSinusoidalposEmbedding.__init__": [[99, 121], ["torch.Module.__init__", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "pe.unsqueeze.unsqueeze.unsqueeze", "embeddings.WordSinusoidalposEmbedding.register_buffer", "torch.Embedding", "torch.Embedding", "torch.Dropout", "torch.Dropout", "ValueError", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange().unsqueeze.float", "torch.arange().unsqueeze.float", "torch.arange().unsqueeze.float", "torch.arange().unsqueeze.float", "math.log"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "vocab_size", ")", ":", "\n", "        ", "super", "(", "WordSinusoidalposEmbedding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "args", ".", "emb_size", "%", "2", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Cannot use sin/cos positional encoding with \"", "\n", "\"odd dim (got dim={:d})\"", ".", "format", "(", "args", ".", "emb_size", ")", ")", "\n", "", "self", ".", "max_seq_length", "=", "args", ".", "max_seq_length", "\n", "pe", "=", "torch", ".", "zeros", "(", "self", ".", "max_seq_length", ",", "args", ".", "emb_size", ")", "\n", "position", "=", "torch", ".", "arange", "(", "0", ",", "self", ".", "max_seq_length", ")", ".", "unsqueeze", "(", "1", ")", "\n", "div_term", "=", "torch", ".", "exp", "(", "\n", "(", "\n", "torch", ".", "arange", "(", "0", ",", "args", ".", "emb_size", ",", "2", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "*", "-", "(", "math", ".", "log", "(", "10000.0", ")", "/", "args", ".", "emb_size", ")", "\n", ")", "\n", ")", "\n", "pe", "[", ":", ",", "0", ":", ":", "2", "]", "=", "torch", ".", "sin", "(", "position", ".", "float", "(", ")", "*", "div_term", ")", "\n", "pe", "[", ":", ",", "1", ":", ":", "2", "]", "=", "torch", ".", "cos", "(", "position", ".", "float", "(", ")", "*", "div_term", ")", "\n", "pe", "=", "pe", ".", "unsqueeze", "(", "1", ")", "\n", "self", ".", "register_buffer", "(", "\"pe\"", ",", "pe", ")", "\n", "\n", "self", ".", "word_embedding", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "args", ".", "emb_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "args", ".", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.embeddings.WordSinusoidalposEmbedding.forward": [[122, 135], ["embeddings.WordSinusoidalposEmbedding.word_embedding", "embeddings.WordSinusoidalposEmbedding.dropout", "math.sqrt", "embeddings.WordSinusoidalposEmbedding.pe[].transpose", "embeddings.WordSinusoidalposEmbedding.size", "embeddings.WordSinusoidalposEmbedding.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src", ",", "_", ")", ":", "\n", "        ", "\"\"\"Embed inputs.\n        Args:\n            emb (FloatTensor): Sequence of word vectors\n                ``(batch_size, seq_len, self.dim)``\n            step (int or NoneType): If stepwise (``seq_len = 1``), use\n                the encoding for this position.\n        \"\"\"", "\n", "word_emb", "=", "self", ".", "word_embedding", "(", "src", ")", "\n", "emb", "=", "word_emb", "*", "math", ".", "sqrt", "(", "word_emb", ".", "size", "(", "-", "1", ")", ")", "\n", "emb", "=", "emb", "+", "self", ".", "pe", "[", ":", "emb", ".", "size", "(", "1", ")", "]", ".", "transpose", "(", "0", ",", "1", ")", "\n", "emb", "=", "self", ".", "dropout", "(", "emb", ")", "\n", "return", "emb", "\n", "", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.transformer.TransformerLayer.__init__": [[13, 49], ["torch.Module.__init__", "hasattr", "bool", "bool", "uer.layers.multi_headed_attn.MultiHeadedAttention", "torch.Dropout", "torch.Dropout", "uer.layers.position_ffn.GatedFeedForward", "uer.layers.position_ffn.PositionwiseFeedForward", "uer.layers.layer_norm.T5LayerNorm", "uer.layers.layer_norm.T5LayerNorm", "uer.layers.layer_norm.LayerNorm", "uer.layers.layer_norm.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "TransformerLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "layernorm_positioning", "=", "args", ".", "layernorm_positioning", "\n", "\n", "if", "hasattr", "(", "args", ",", "\"attention_head_size\"", ")", ":", "\n", "            ", "attention_head_size", "=", "args", ".", "attention_head_size", "\n", "", "else", ":", "\n", "            ", "attention_head_size", "=", "args", ".", "hidden_size", "//", "args", ".", "heads_num", "\n", "\n", "", "has_bias", "=", "bool", "(", "1", "-", "args", ".", "remove_transformer_bias", ")", "\n", "with_scale", "=", "bool", "(", "1", "-", "args", ".", "remove_attention_scale", ")", "\n", "\n", "# Multi-headed self-attention.", "\n", "self", ".", "self_attn", "=", "MultiHeadedAttention", "(", "\n", "args", ".", "hidden_size", ",", "args", ".", "heads_num", ",", "attention_head_size", ",", "args", ".", "dropout", ",", "has_bias", "=", "has_bias", ",", "with_scale", "=", "with_scale", "\n", ")", "\n", "self", ".", "dropout_1", "=", "nn", ".", "Dropout", "(", "args", ".", "dropout", ")", "\n", "\n", "# Feed forward layer.", "\n", "if", "args", ".", "feed_forward", "==", "\"gated\"", ":", "\n", "            ", "self", ".", "feed_forward", "=", "GatedFeedForward", "(", "\n", "args", ".", "hidden_size", ",", "args", ".", "feedforward_size", ",", "args", ".", "hidden_act", ",", "has_bias", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "feed_forward", "=", "PositionwiseFeedForward", "(", "\n", "args", ".", "hidden_size", ",", "args", ".", "feedforward_size", ",", "args", ".", "hidden_act", ",", "has_bias", "\n", ")", "\n", "", "self", ".", "dropout_2", "=", "nn", ".", "Dropout", "(", "args", ".", "dropout", ")", "\n", "\n", "if", "args", ".", "layernorm", "==", "\"t5\"", ":", "\n", "            ", "self", ".", "layer_norm_1", "=", "T5LayerNorm", "(", "args", ".", "hidden_size", ")", "\n", "self", ".", "layer_norm_2", "=", "T5LayerNorm", "(", "args", ".", "hidden_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "layer_norm_1", "=", "LayerNorm", "(", "args", ".", "hidden_size", ")", "\n", "self", ".", "layer_norm_2", "=", "LayerNorm", "(", "args", ".", "hidden_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.transformer.TransformerLayer.forward": [[52, 74], ["transformer.TransformerLayer.dropout_1", "transformer.TransformerLayer.layer_norm_1", "transformer.TransformerLayer.dropout_2", "transformer.TransformerLayer.layer_norm_2", "transformer.TransformerLayer.layer_norm_1", "transformer.TransformerLayer.dropout_1", "transformer.TransformerLayer.layer_norm_2", "transformer.TransformerLayer.self_attn", "transformer.TransformerLayer.feed_forward", "transformer.TransformerLayer.self_attn", "transformer.TransformerLayer.dropout_2", "transformer.TransformerLayer.feed_forward"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden", ",", "mask", ",", "position_bias", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            hidden: [batch_size x seq_length x emb_size]\n            mask: [batch_size x 1 x seq_length x seq_length]\n            position_bias: [1 x heads_num x seq_length x seq_length]\n        Returns:\n            output: [batch_size x seq_length x hidden_size]\n        \"\"\"", "\n", "\n", "if", "self", ".", "layernorm_positioning", "==", "\"post\"", ":", "\n", "            ", "inter", "=", "self", ".", "dropout_1", "(", "self", ".", "self_attn", "(", "hidden", ",", "hidden", ",", "hidden", ",", "mask", ",", "position_bias", ")", ")", "\n", "inter", "=", "self", ".", "layer_norm_1", "(", "inter", "+", "hidden", ")", "\n", "output", "=", "self", ".", "dropout_2", "(", "self", ".", "feed_forward", "(", "inter", ")", ")", "\n", "output", "=", "self", ".", "layer_norm_2", "(", "output", "+", "inter", ")", "\n", "", "else", ":", "\n", "            ", "inter", "=", "self", ".", "layer_norm_1", "(", "hidden", ")", "\n", "inter", "=", "self", ".", "dropout_1", "(", "self", ".", "self_attn", "(", "inter", ",", "inter", ",", "inter", ",", "mask", ",", "position_bias", ")", ")", "\n", "hidden", "=", "hidden", "+", "inter", "\n", "output", "=", "self", ".", "layer_norm_2", "(", "hidden", ")", "\n", "output", "=", "self", ".", "dropout_2", "(", "self", ".", "feed_forward", "(", "output", ")", ")", "+", "hidden", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.transformer.TransformerDecoderLayer.__init__": [[77, 122], ["torch.Module.__init__", "hasattr", "bool", "bool", "uer.layers.multi_headed_attn.MultiHeadedAttention", "torch.Dropout", "uer.layers.multi_headed_attn.MultiHeadedAttention", "torch.Dropout", "torch.Dropout", "uer.layers.position_ffn.GatedFeedForward", "uer.layers.position_ffn.PositionwiseFeedForward", "uer.layers.layer_norm.T5LayerNorm", "uer.layers.layer_norm.T5LayerNorm", "uer.layers.layer_norm.T5LayerNorm", "uer.layers.layer_norm.LayerNorm", "uer.layers.layer_norm.LayerNorm", "uer.layers.layer_norm.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "TransformerDecoderLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "layernorm_positioning", "=", "args", ".", "layernorm_positioning", "\n", "\n", "if", "hasattr", "(", "args", ",", "\"attention_head_size\"", ")", ":", "\n", "            ", "attention_head_size", "=", "args", ".", "attention_head_size", "\n", "", "else", ":", "\n", "            ", "attention_head_size", "=", "args", ".", "hidden_size", "//", "args", ".", "heads_num", "\n", "\n", "", "has_bias", "=", "bool", "(", "1", "-", "args", ".", "remove_transformer_bias", ")", "\n", "with_scale", "=", "bool", "(", "1", "-", "args", ".", "remove_attention_scale", ")", "\n", "\n", "# Multi-headed self-attention.", "\n", "self", ".", "self_attn", "=", "MultiHeadedAttention", "(", "\n", "args", ".", "hidden_size", ",", "args", ".", "heads_num", ",", "attention_head_size", ",", "args", ".", "dropout", ",", "has_bias", "=", "has_bias", ",", "with_scale", "=", "with_scale", "\n", ")", "\n", "self", ".", "dropout_1", "=", "nn", ".", "Dropout", "(", "args", ".", "dropout", ")", "\n", "\n", "# Multi-headed context-attention.", "\n", "self", ".", "context_attn", "=", "MultiHeadedAttention", "(", "\n", "args", ".", "hidden_size", ",", "args", ".", "heads_num", ",", "attention_head_size", ",", "args", ".", "dropout", ",", "has_bias", "=", "has_bias", ",", "with_scale", "=", "with_scale", "\n", ")", "\n", "self", ".", "dropout_2", "=", "nn", ".", "Dropout", "(", "args", ".", "dropout", ")", "\n", "\n", "# Feed forward layer.", "\n", "if", "args", ".", "feed_forward", "==", "\"gated\"", ":", "\n", "            ", "self", ".", "feed_forward", "=", "GatedFeedForward", "(", "\n", "args", ".", "hidden_size", ",", "args", ".", "feedforward_size", ",", "args", ".", "hidden_act", ",", "has_bias", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "feed_forward", "=", "PositionwiseFeedForward", "(", "\n", "args", ".", "hidden_size", ",", "args", ".", "feedforward_size", ",", "args", ".", "hidden_act", ",", "has_bias", "\n", ")", "\n", "", "self", ".", "dropout_3", "=", "nn", ".", "Dropout", "(", "args", ".", "dropout", ")", "\n", "\n", "# Layer Normalization", "\n", "if", "args", ".", "layernorm", "==", "\"t5\"", ":", "\n", "            ", "self", ".", "layer_norm_1", "=", "T5LayerNorm", "(", "args", ".", "hidden_size", ")", "\n", "self", ".", "layer_norm_2", "=", "T5LayerNorm", "(", "args", ".", "hidden_size", ")", "\n", "self", ".", "layer_norm_3", "=", "T5LayerNorm", "(", "args", ".", "hidden_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "layer_norm_1", "=", "LayerNorm", "(", "args", ".", "hidden_size", ")", "\n", "self", ".", "layer_norm_2", "=", "LayerNorm", "(", "args", ".", "hidden_size", ")", "\n", "self", ".", "layer_norm_3", "=", "LayerNorm", "(", "args", ".", "hidden_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.transformer.TransformerDecoderLayer.forward": [[125, 155], ["transformer.TransformerDecoderLayer.dropout_1", "transformer.TransformerDecoderLayer.layer_norm_1", "transformer.TransformerDecoderLayer.dropout_2", "transformer.TransformerDecoderLayer.layer_norm_2", "transformer.TransformerDecoderLayer.dropout_3", "transformer.TransformerDecoderLayer.layer_norm_3", "transformer.TransformerDecoderLayer.layer_norm_1", "transformer.TransformerDecoderLayer.dropout_1", "transformer.TransformerDecoderLayer.layer_norm_2", "transformer.TransformerDecoderLayer.dropout_2", "transformer.TransformerDecoderLayer.layer_norm_3", "transformer.TransformerDecoderLayer.self_attn", "transformer.TransformerDecoderLayer.context_attn", "transformer.TransformerDecoderLayer.feed_forward", "transformer.TransformerDecoderLayer.self_attn", "transformer.TransformerDecoderLayer.context_attn", "transformer.TransformerDecoderLayer.dropout_3", "transformer.TransformerDecoderLayer.feed_forward"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden", ",", "encoder_hidden", ",", "mask_decoder", ",", "mask_encoder", ",", "self_position_bias", "=", "None", ",", "context_position_bias", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            hidden: [batch_size x seq_length x emb_size]\n            encoder_hidden: [batch_size x seq_length x emb_size]\n            mask_encoder: [batch_size x 1 x seq_length x seq_length]\n            mask_decoder: [batch_size x 1 x seq_length x seq_length]\n            self_position_bias: [1 x heads_num x seq_length x seq_length]\n            context_position_bias: [1 x heads_num x seq_length x seq_length]\n        Returns:\n            output: [batch_size x seq_length x hidden_size]\n        \"\"\"", "\n", "\n", "if", "self", ".", "layernorm_positioning", "==", "\"post\"", ":", "\n", "            ", "query", "=", "self", ".", "dropout_1", "(", "self", ".", "self_attn", "(", "hidden", ",", "hidden", ",", "hidden", ",", "mask_decoder", ",", "self_position_bias", ")", ")", "\n", "query_norm", "=", "self", ".", "layer_norm_1", "(", "query", "+", "hidden", ")", "\n", "mid", "=", "self", ".", "dropout_2", "(", "self", ".", "context_attn", "(", "encoder_hidden", ",", "encoder_hidden", ",", "query_norm", ",", "mask_encoder", ",", "context_position_bias", ")", ")", "\n", "mid_norm", "=", "self", ".", "layer_norm_2", "(", "mid", "+", "query_norm", ")", "\n", "output", "=", "self", ".", "dropout_3", "(", "self", ".", "feed_forward", "(", "mid_norm", ")", ")", "\n", "output", "=", "self", ".", "layer_norm_3", "(", "output", "+", "mid_norm", ")", "\n", "", "else", ":", "\n", "            ", "hidden_norm", "=", "self", ".", "layer_norm_1", "(", "hidden", ")", "\n", "query", "=", "self", ".", "dropout_1", "(", "self", ".", "self_attn", "(", "hidden_norm", ",", "hidden_norm", ",", "hidden_norm", ",", "mask_decoder", ",", "self_position_bias", ")", ")", "\n", "query", "=", "query", "+", "hidden", "\n", "query_norm", "=", "self", ".", "layer_norm_2", "(", "query", ")", "\n", "mid", "=", "self", ".", "dropout_2", "(", "self", ".", "context_attn", "(", "encoder_hidden", ",", "encoder_hidden", ",", "query_norm", ",", "mask_encoder", ",", "context_position_bias", ")", ")", "\n", "mid", "=", "mid", "+", "query", "\n", "mid_norm", "=", "self", ".", "layer_norm_3", "(", "mid", ")", "\n", "output", "=", "self", ".", "dropout_3", "(", "self", ".", "feed_forward", "(", "mid_norm", ")", ")", "+", "mid", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.synthesizer.DenseAttention.__init__": [[13, 29], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Softmax", "torch.Softmax", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "seq_length", ",", "hidden_size", ",", "dropout", ")", ":", "\n", "        ", "super", "(", "DenseAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Function F() in Dense format", "\n", "# Note: \u6839\u636e\u8bba\u6587\u65e0\u6cd5\u77e5\u9053\u662f\u54ea\u4e2a\u4e00\u4e2aLinear layer\u5c06hidden_size\u8f6c\u6362\u6210seq_length\uff0c\u5148\u968f\u4fbf\u8bd5\u4e00\u8bd5", "\n", "self", ".", "F_linear_1", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "seq_length", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "F_linear_2", "=", "nn", ".", "Linear", "(", "seq_length", ",", "seq_length", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n", "# Function G() in Dense format", "\n", "self", ".", "G_linear", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ")", "\n", "\n", "# Final linear", "\n", "self", ".", "final_linear", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.synthesizer.DenseAttention.forward": [[30, 53], ["hidden.size", "synthesizer.DenseAttention.relu", "synthesizer.DenseAttention.F_linear_2", "synthesizer.DenseAttention.softmax", "synthesizer.DenseAttention.dropout", "synthesizer.DenseAttention.G_linear", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "synthesizer.DenseAttention.final_linear", "synthesizer.DenseAttention.F_linear_1", "mask.squeeze"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.act_fun.relu"], ["", "def", "forward", "(", "self", ",", "hidden", ",", "mask", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            hidden: [batch_size x seq_length x hidden_size]\n            mask: [batch_size x 1 x seq_length x seq_length]\n\n        Returns:\n            output: [batch_size x seq_length x hidden_size]\n        \"\"\"", "\n", "batch_size", ",", "seq_length", ",", "hidden_size", "=", "hidden", ".", "size", "(", ")", "\n", "\n", "scores", "=", "self", ".", "relu", "(", "self", ".", "F_linear_1", "(", "hidden", ")", ")", "\n", "scores", "=", "self", ".", "F_linear_2", "(", "scores", ")", "\n", "scores", "=", "scores", "+", "mask", ".", "squeeze", "(", "1", ")", "\n", "probs", "=", "self", ".", "softmax", "(", "scores", ")", "\n", "probs", "=", "self", ".", "dropout", "(", "probs", ")", "\n", "\n", "values", "=", "self", ".", "G_linear", "(", "hidden", ")", "\n", "\n", "output", "=", "torch", ".", "matmul", "(", "probs", ",", "values", ")", "\n", "output", "=", "self", ".", "final_linear", "(", "output", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.synthesizer.RandomAttention.__init__": [[57, 69], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Softmax", "torch.Softmax", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "seq_length", ",", "hidden_size", ",", "dropout", ")", ":", "\n", "        ", "super", "(", "RandomAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "R", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "seq_length", ",", "seq_length", ",", "dtype", "=", "torch", ".", "float32", ")", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n", "# Function G() in Dense format", "\n", "self", ".", "G_linear", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ")", "\n", "\n", "# Final linear", "\n", "self", ".", "final_linear", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.synthesizer.RandomAttention.forward": [[70, 91], ["hidden.size", "synthesizer.RandomAttention.softmax", "synthesizer.RandomAttention.dropout", "synthesizer.RandomAttention.G_linear", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "synthesizer.RandomAttention.final_linear", "mask.squeeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden", ",", "mask", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            hidden: [batch_size x seq_length x hidden_size]\n            mask: [batch_size x 1 x seq_length x seq_length]\n\n        Returns:\n            output: [batch_size x seq_length x hidden_size]\n        \"\"\"", "\n", "batch_size", ",", "seq_length", ",", "hidden_size", "=", "hidden", ".", "size", "(", ")", "\n", "\n", "scores", "=", "self", ".", "R", "+", "mask", ".", "squeeze", "(", "1", ")", "\n", "probs", "=", "self", ".", "softmax", "(", "scores", ")", "\n", "probs", "=", "self", ".", "dropout", "(", "probs", ")", "\n", "\n", "values", "=", "self", ".", "G_linear", "(", "hidden", ")", "\n", "\n", "output", "=", "torch", ".", "matmul", "(", "probs", ",", "values", ")", "\n", "output", "=", "self", ".", "final_linear", "(", "output", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.synthesizer.ISynthesizer.__init__": [[98, 113], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "uer.layers.layer_norm.LayerNorm", "uer.layers.position_ffn.PositionwiseFeedForward", "torch.Dropout", "torch.Dropout", "uer.layers.layer_norm.LayerNorm", "Exception"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "ISynthesizer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "att", "=", "None", "\n", "self", ".", "dropout_1", "=", "nn", ".", "Dropout", "(", "args", ".", "dropout", ")", "\n", "self", ".", "layer_norm_1", "=", "LayerNorm", "(", "args", ".", "hidden_size", ")", "\n", "# Feed forward layer.", "\n", "self", ".", "feed_forward", "=", "PositionwiseFeedForward", "(", "\n", "args", ".", "hidden_size", ",", "args", ".", "feedforward_size", "\n", ")", "\n", "self", ".", "dropout_2", "=", "nn", ".", "Dropout", "(", "args", ".", "dropout", ")", "\n", "self", ".", "layer_norm_2", "=", "LayerNorm", "(", "args", ".", "hidden_size", ")", "\n", "\n", "if", "self", ".", "__class__", ".", "__name__", "==", "'ISynthesizer'", ":", "\n", "            ", "raise", "Exception", "(", "\"ISynthesizer cannot be instantiated.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.synthesizer.ISynthesizer.forward": [[114, 128], ["synthesizer.ISynthesizer.dropout_1", "synthesizer.ISynthesizer.layer_norm_1", "synthesizer.ISynthesizer.dropout_2", "synthesizer.ISynthesizer.layer_norm_2", "synthesizer.ISynthesizer.att", "synthesizer.ISynthesizer.feed_forward"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden", ",", "mask", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            hidden: [batch_size x seq_length x emb_size]\n            mask: [batch_size x 1 x seq_length x seq_length]\n\n        Returns:\n            output: [batch_size x seq_length x hidden_size]\n        \"\"\"", "\n", "inter", "=", "self", ".", "dropout_1", "(", "self", ".", "att", "(", "hidden", ",", "mask", ")", ")", "\n", "inter", "=", "self", ".", "layer_norm_1", "(", "inter", "+", "hidden", ")", "\n", "output", "=", "self", ".", "dropout_2", "(", "self", ".", "feed_forward", "(", "inter", ")", ")", "\n", "output", "=", "self", ".", "layer_norm_2", "(", "output", "+", "inter", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.synthesizer.DenseSynthesizer.__init__": [[136, 142], ["synthesizer.ISynthesizer.__init__", "synthesizer.DenseAttention"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "DenseSynthesizer", ",", "self", ")", ".", "__init__", "(", "args", ")", "\n", "\n", "# dense attention", "\n", "self", ".", "att", "=", "DenseAttention", "(", "\n", "args", ".", "seq_length", ",", "args", ".", "hidden_size", ",", "args", ".", "dropout", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.layers.synthesizer.RandomSynthesizer.__init__": [[151, 157], ["synthesizer.ISynthesizer.__init__", "synthesizer.RandomAttention"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "RandomSynthesizer", ",", "self", ")", ".", "__init__", "(", "args", ")", "\n", "\n", "# dense attention", "\n", "self", ".", "att", "=", "RandomAttention", "(", "\n", "args", ".", "seq_length", ",", "args", ".", "hidden_size", ",", "args", ".", "dropout", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.decoders.transformer_decoder.TransformerDecoder.__init__": [[13, 39], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "bool", "uer.layers.relative_position_embedding.RelativePositionEmbedding", "uer.layers.transformer.TransformerDecoderLayer", "uer.layers.layer_norm.T5LayerNorm", "uer.layers.layer_norm.LayerNorm", "uer.layers.relative_position_embedding.RelativePositionEmbedding", "range"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "TransformerDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layers_num", "=", "args", ".", "layers_num", "\n", "self", ".", "layernorm_positioning", "=", "args", ".", "layernorm_positioning", "\n", "self", ".", "relative_position_embedding", "=", "args", ".", "relative_position_embedding", "\n", "self", ".", "share_relative_position_embedding", "=", "args", ".", "share_relative_position_embedding", "\n", "self", ".", "transformer_decoder", "=", "nn", ".", "ModuleList", "(", "\n", "[", "TransformerDecoderLayer", "(", "args", ")", "for", "_", "in", "range", "(", "self", ".", "layers_num", ")", "]", "\n", ")", "\n", "\n", "has_bias", "=", "bool", "(", "1", "-", "args", ".", "remove_transformer_bias", ")", "\n", "\n", "if", "self", ".", "layernorm_positioning", "==", "\"pre\"", ":", "\n", "            ", "if", "args", ".", "layernorm", "==", "\"t5\"", ":", "\n", "                ", "self", ".", "layer_norm", "=", "T5LayerNorm", "(", "args", ".", "hidden_size", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "layer_norm", "=", "LayerNorm", "(", "args", ".", "hidden_size", ")", "\n", "\n", "", "", "if", "self", ".", "relative_position_embedding", ":", "\n", "            ", "self", ".", "self_pos_emb", "=", "RelativePositionEmbedding", "(", "bidirectional", "=", "False", ",", "heads_num", "=", "args", ".", "heads_num", ",", "\n", "num_buckets", "=", "args", ".", "relative_attention_buckets_num", ")", "\n", "if", "self", ".", "share_relative_position_embedding", ":", "\n", "                ", "self", ".", "context_pos_emb", "=", "self", ".", "self_pos_emb", "\n", "", "else", ":", "\n", "                ", "self", ".", "context_pos_emb", "=", "RelativePositionEmbedding", "(", "bidirectional", "=", "False", ",", "heads_num", "=", "args", ".", "heads_num", ",", "\n", "num_buckets", "=", "args", ".", "relative_attention_buckets_num", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.decoders.transformer_decoder.TransformerDecoder.forward": [[41, 80], ["memory_bank.size", "emb.size", "mask_encoder.float.float.float", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "mask_decoder.repeat.repeat.repeat", "range", "transformer_decoder.TransformerDecoder.self_pos_emb", "transformer_decoder.TransformerDecoder.context_pos_emb", "transformer_decoder.TransformerDecoder.layer_norm"], "methods", ["None"], ["", "", "", "def", "forward", "(", "self", ",", "memory_bank", ",", "emb", ",", "additional_info", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            memory_bank: [batch_size x seq_length x emb_size]\n            emb: [batch_size x seq_length x emb_size]\n        Returns:\n            hidden: [batch_size x seq_length x hidden_size]\n        \"\"\"", "\n", "_", ",", "src_seq_length", ",", "_", "=", "memory_bank", ".", "size", "(", ")", "\n", "batch_size", ",", "tgt_seq_length", ",", "_", "=", "emb", ".", "size", "(", ")", "\n", "\n", "mask_encoder", "=", "(", "additional_info", "[", "0", "]", ">", "0", ")", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "tgt_seq_length", ",", "1", ")", ".", "unsqueeze", "(", "1", ")", "\n", "mask_encoder", "=", "mask_encoder", ".", "float", "(", ")", "\n", "mask_encoder", "=", "(", "1.0", "-", "mask_encoder", ")", "*", "-", "10000.0", "\n", "\n", "mask_decoder", "=", "torch", ".", "ones", "(", "tgt_seq_length", ",", "tgt_seq_length", ",", "device", "=", "emb", ".", "device", ")", "\n", "mask_decoder", "=", "torch", ".", "tril", "(", "mask_decoder", ")", "\n", "mask_decoder", "=", "(", "1.0", "-", "mask_decoder", ")", "*", "-", "10000", "\n", "mask_decoder", "=", "mask_decoder", ".", "repeat", "(", "batch_size", ",", "1", ",", "1", ",", "1", ")", "\n", "\n", "hidden", "=", "emb", "\n", "\n", "if", "self", ".", "relative_position_embedding", ":", "\n", "            ", "self_position_bias", "=", "self", ".", "self_pos_emb", "(", "hidden", ",", "hidden", ")", "\n", "context_position_bias", "=", "self", ".", "context_pos_emb", "(", "hidden", ",", "memory_bank", ")", "\n", "", "else", ":", "\n", "            ", "self_position_bias", "=", "None", "\n", "context_position_bias", "=", "None", "\n", "\n", "", "for", "i", "in", "range", "(", "self", ".", "layers_num", ")", ":", "\n", "            ", "hidden", "=", "self", ".", "transformer_decoder", "[", "i", "]", "(", "hidden", ",", "memory_bank", ",", "mask_decoder", ",", "mask_encoder", ",", "self_position_bias", ",", "context_position_bias", ")", "\n", "\n", "", "if", "self", ".", "layernorm_positioning", "==", "\"pre\"", ":", "\n", "            ", "return", "self", ".", "layer_norm", "(", "hidden", ")", "\n", "", "else", ":", "\n", "            ", "return", "hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.models.model.Model.__init__": [[12, 25], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "embedding", ",", "encoder", ",", "target", ")", ":", "\n", "        ", "super", "(", "Model", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embedding", "=", "embedding", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "target", "=", "target", "\n", "\n", "if", "args", ".", "target", "in", "[", "'bert'", ",", "'mlm'", "]", "and", "args", ".", "tie_weights", ":", "\n", "            ", "self", ".", "target", ".", "mlm_linear_2", ".", "weight", "=", "self", ".", "embedding", ".", "word_embedding", ".", "weight", "\n", "", "elif", "args", ".", "target", "in", "[", "'lm'", ",", "'t5'", "]", "and", "args", ".", "tie_weights", ":", "\n", "            ", "self", ".", "target", ".", "output_layer", ".", "weight", "=", "self", ".", "embedding", ".", "word_embedding", ".", "weight", "\n", "\n", "", "if", "args", ".", "target", "==", "'t5'", "and", "args", ".", "share_embedding", ":", "\n", "            ", "self", ".", "target", ".", "embedding", ".", "word_embedding", ".", "weight", "=", "self", ".", "embedding", ".", "word_embedding", ".", "weight", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.models.model.Model.forward": [[26, 31], ["model.Model.embedding", "model.Model.encoder", "model.Model.target"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "src", ",", "tgt", ",", "seg", ")", ":", "\n", "        ", "emb", "=", "self", ".", "embedding", "(", "src", ",", "seg", ")", "\n", "output", "=", "self", ".", "encoder", "(", "emb", ",", "seg", ")", "\n", "loss_info", "=", "self", ".", "target", "(", "output", ",", "tgt", ")", "\n", "return", "loss_info", "\n", "", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.bilm_target.BilmTarget.__init__": [[8, 11], ["LmTarget.__init__"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "vocab_size", ")", ":", "\n", "        ", "args", ".", "hidden_size", "=", "args", ".", "hidden_size", "//", "2", "\n", "super", "(", "BilmTarget", ",", "self", ")", ".", "__init__", "(", "args", ",", "vocab_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.bilm_target.BilmTarget.forward": [[12, 34], ["bilm_target.BilmTarget.lm", "bilm_target.BilmTarget.lm", "type"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.lm_target.LmTarget.lm", "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.lm_target.LmTarget.lm"], ["", "def", "forward", "(", "self", ",", "memory_bank", ",", "tgt", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            memory_bank: [batch_size x seq_length x hidden_size]\n            tgt: [batch_size x seq_length]\n\n        Returns:\n            loss: Language modeling loss.\n            correct: Number of words that are predicted correctly.\n            denominator: Number of predicted words.\n        \"\"\"", "\n", "\n", "assert", "type", "(", "tgt", ")", "==", "tuple", "\n", "tgt_forward", ",", "tgt_backward", "=", "tgt", "[", "0", "]", ",", "tgt", "[", "1", "]", "\n", "# Forward.", "\n", "loss_forward", ",", "correct_forward", ",", "denominator_forward", "=", "self", ".", "lm", "(", "memory_bank", "[", ":", ",", ":", ",", ":", "self", ".", "hidden_size", "]", ",", "tgt_forward", ")", "\n", "# Backward.", "\n", "loss_backward", ",", "correct_backward", ",", "denominator_backward", "=", "self", ".", "lm", "(", "memory_bank", "[", ":", ",", ":", ",", "self", ".", "hidden_size", ":", "]", ",", "tgt_backward", ")", "\n", "\n", "return", "loss_forward", ",", "loss_backward", ",", "correct_forward", ",", "correct_backward", ",", "denominator_backward", "\n", "", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.packet_reording_target.NspTarget.__init__": [[12, 20], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.LogSoftmax", "torch.LogSoftmax", "torch.NLLLoss", "torch.NLLLoss"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "vocab_size", ")", ":", "\n", "        ", "super", "(", "NspTarget", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "hidden_size", "=", "args", ".", "hidden_size", "\n", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "args", ".", "hidden_size", ",", "args", ".", "labels_num", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "-", "1", ")", "\n", "self", ".", "criterion", "=", "nn", ".", "NLLLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.packet_reording_target.NspTarget.forward": [[22, 38], ["packet_reording_target.NspTarget.linear", "packet_reording_target.NspTarget.criterion", "packet_reording_target.NspTarget.softmax().argmax().eq().sum", "packet_reording_target.NspTarget.softmax", "packet_reording_target.NspTarget.softmax().argmax().eq", "packet_reording_target.NspTarget.softmax().argmax", "packet_reording_target.NspTarget.softmax"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.act_fun.linear"], ["", "def", "forward", "(", "self", ",", "memory_bank", ",", "tgt", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            memory_bank: [batch_size x seq_length x hidden_size]\n            tgt: [batch_size]\n\n        Returns:\n            loss: Next sentence prediction loss.\n            correct: Number of sentences that are predicted correctly.\n        \"\"\"", "\n", "\n", "output", "=", "self", ".", "linear", "(", "memory_bank", "[", ":", ",", "0", ",", ":", "]", ")", "\n", "loss", "=", "self", ".", "criterion", "(", "self", ".", "softmax", "(", "output", ")", ",", "tgt", ")", "\n", "correct", "=", "self", ".", "softmax", "(", "output", ")", ".", "argmax", "(", "dim", "=", "-", "1", ")", ".", "eq", "(", "tgt", ")", ".", "sum", "(", ")", "\n", "\n", "return", "loss", ",", "correct", "\n", "", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.nsp_target.NspTarget.__init__": [[12, 20], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.LogSoftmax", "torch.LogSoftmax", "torch.NLLLoss", "torch.NLLLoss"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "vocab_size", ")", ":", "\n", "        ", "super", "(", "NspTarget", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "hidden_size", "=", "args", ".", "hidden_size", "\n", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "args", ".", "hidden_size", ",", "args", ".", "labels_num", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "-", "1", ")", "\n", "self", ".", "criterion", "=", "nn", ".", "NLLLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.nsp_target.NspTarget.forward": [[22, 38], ["nsp_target.NspTarget.linear", "nsp_target.NspTarget.criterion", "nsp_target.NspTarget.softmax().argmax().eq().sum", "nsp_target.NspTarget.softmax", "nsp_target.NspTarget.softmax().argmax().eq", "nsp_target.NspTarget.softmax().argmax", "nsp_target.NspTarget.softmax"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.act_fun.linear"], ["", "def", "forward", "(", "self", ",", "memory_bank", ",", "tgt", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            memory_bank: [batch_size x seq_length x hidden_size]\n            tgt: [batch_size]\n\n        Returns:\n            loss: Next sentence prediction loss.\n            correct: Number of sentences that are predicted correctly.\n        \"\"\"", "\n", "\n", "output", "=", "self", ".", "linear", "(", "memory_bank", "[", ":", ",", "0", ",", ":", "]", ")", "\n", "loss", "=", "self", ".", "criterion", "(", "self", ".", "softmax", "(", "output", ")", ",", "tgt", ")", "\n", "correct", "=", "self", ".", "softmax", "(", "output", ")", ".", "argmax", "(", "dim", "=", "-", "1", ")", ".", "eq", "(", "tgt", ")", ".", "sum", "(", ")", "\n", "\n", "return", "loss", ",", "correct", "\n", "", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.albert_target.AlbertTarget.__init__": [[12, 19], ["MlmTarget.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "vocab_size", ")", ":", "\n", "        ", "super", "(", "AlbertTarget", ",", "self", ")", ".", "__init__", "(", "args", ",", "vocab_size", ")", "\n", "\n", "self", ".", "factorized_embedding_parameterization", "=", "True", "\n", "# SOP.", "\n", "self", ".", "sop_linear_1", "=", "nn", ".", "Linear", "(", "args", ".", "hidden_size", ",", "args", ".", "hidden_size", ")", "\n", "self", ".", "sop_linear_2", "=", "nn", ".", "Linear", "(", "args", ".", "hidden_size", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.albert_target.AlbertTarget.forward": [[20, 47], ["albert_target.AlbertTarget.mlm", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "albert_target.AlbertTarget.sop_linear_2", "albert_target.AlbertTarget.criterion", "albert_target.AlbertTarget.softmax().argmax().eq().sum", "type", "albert_target.AlbertTarget.sop_linear_1", "albert_target.AlbertTarget.softmax", "albert_target.AlbertTarget.softmax().argmax().eq", "albert_target.AlbertTarget.softmax().argmax", "albert_target.AlbertTarget.softmax"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.mlm_target.MlmTarget.mlm"], ["", "def", "forward", "(", "self", ",", "memory_bank", ",", "tgt", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            memory_bank: [batch_size x seq_length x hidden_size]\n            tgt: tuple with tgt_mlm [batch_size x seq_length] and tgt_sop [batch_size]\n\n        Returns:\n            loss_mlm: Masked language model loss.\n            loss_sop: Sentence order prediction loss.\n            correct_mlm: Number of words that are predicted correctly.\n            correct_sop: Number of sentences that are predicted correctly.\n            denominator: Number of masked words.\n        \"\"\"", "\n", "\n", "# Masked language model (MLM).", "\n", "assert", "type", "(", "tgt", ")", "==", "tuple", "\n", "tgt_mlm", ",", "tgt_sop", "=", "tgt", "[", "0", "]", ",", "tgt", "[", "1", "]", "\n", "\n", "loss_mlm", ",", "correct_mlm", ",", "denominator", "=", "self", ".", "mlm", "(", "memory_bank", ",", "tgt_mlm", ")", "\n", "\n", "# Sentence order prediction (SOP).", "\n", "output_sop", "=", "torch", ".", "tanh", "(", "self", ".", "sop_linear_1", "(", "memory_bank", "[", ":", ",", "0", ",", ":", "]", ")", ")", "\n", "output_sop", "=", "self", ".", "sop_linear_2", "(", "output_sop", ")", "\n", "loss_sop", "=", "self", ".", "criterion", "(", "self", ".", "softmax", "(", "output_sop", ")", ",", "tgt_sop", ")", "\n", "correct_sop", "=", "self", ".", "softmax", "(", "output_sop", ")", ".", "argmax", "(", "dim", "=", "-", "1", ")", ".", "eq", "(", "tgt_sop", ")", ".", "sum", "(", ")", "\n", "\n", "return", "loss_mlm", ",", "loss_sop", ",", "correct_mlm", ",", "correct_sop", ",", "denominator", "\n", "", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.t5_target.T5Target.__init__": [[10, 16], ["LmTarget.__init__"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "vocab_size", ")", ":", "\n", "        ", "super", "(", "T5Target", ",", "self", ")", ".", "__init__", "(", "args", ",", "vocab_size", ")", "\n", "\n", "self", ".", "embedding", "=", "str2embedding", "[", "args", ".", "tgt_embedding", "]", "(", "args", ",", "vocab_size", ")", "\n", "\n", "self", ".", "decoder", "=", "str2decoder", "[", "args", ".", "decoder", "]", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.t5_target.T5Target.forward": [[17, 38], ["t5_target.T5Target.embedding", "t5_target.T5Target.decoder", "t5_target.T5Target.lm"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.lm_target.LmTarget.lm"], ["", "def", "forward", "(", "self", ",", "memory_bank", ",", "tgt", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            memory_bank: [batch_size x seq_length x hidden_size]\n            tgt: [batch_size x seq_length]\n\n        Returns:\n            loss: Language modeling loss.\n            correct: Number of words that are predicted correctly.\n            denominator: Number of predicted words.\n        \"\"\"", "\n", "tgt_in", ",", "tgt_out", ",", "src", "=", "tgt", "\n", "\n", "emb", "=", "self", ".", "embedding", "(", "tgt_in", ",", "None", ")", "\n", "\n", "hidden", "=", "self", ".", "decoder", "(", "memory_bank", ",", "emb", ",", "(", "src", ",", ")", ")", "\n", "\n", "# Language modeling (LM) with full softmax prediction.", "\n", "loss", ",", "correct", ",", "denominator", "=", "self", ".", "lm", "(", "hidden", ",", "tgt_out", ")", "\n", "\n", "return", "loss", ",", "correct", ",", "denominator", "\n", "", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.lm_target.LmTarget.__init__": [[9, 18], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.LogSoftmax", "torch.LogSoftmax", "torch.NLLLoss", "torch.NLLLoss"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "vocab_size", ")", ":", "\n", "        ", "super", "(", "LmTarget", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "hidden_size", "=", "args", ".", "hidden_size", "\n", "\n", "self", ".", "output_layer", "=", "nn", ".", "Linear", "(", "self", ".", "hidden_size", ",", "self", ".", "vocab_size", ",", "bias", "=", "args", ".", "has_lmtarget_bias", ")", "\n", "\n", "self", ".", "softmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "-", "1", ")", "\n", "self", ".", "criterion", "=", "nn", ".", "NLLLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.lm_target.LmTarget.lm": [[19, 36], ["tgt_lm.contiguous().view.contiguous().view.contiguous().view", "memory_bank.contiguous().view.contiguous().view.contiguous().view", "lm_target.LmTarget.output_layer", "lm_target.LmTarget.softmax", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "lm_target.LmTarget.criterion", "lm_target.LmTarget.size", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "tgt_lm.contiguous().view.contiguous().view.contiguous", "memory_bank.contiguous().view.contiguous().view.contiguous", "lm_target.LmTarget.size", "lm_target.LmTarget.argmax().eq().float", "lm_target.LmTarget.argmax().eq", "lm_target.LmTarget.argmax"], "methods", ["None"], ["", "def", "lm", "(", "self", ",", "memory_bank", ",", "tgt_lm", ")", ":", "\n", "# Language modeling (LM) with full softmax prediction.", "\n", "\n", "        ", "tgt_lm", "=", "tgt_lm", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "memory_bank", "=", "memory_bank", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "hidden_size", ")", "\n", "memory_bank", "=", "memory_bank", "[", "tgt_lm", ">", "0", ",", ":", "]", "\n", "tgt_lm", "=", "tgt_lm", "[", "tgt_lm", ">", "0", "]", "\n", "output", "=", "self", ".", "output_layer", "(", "memory_bank", ")", "\n", "output", "=", "self", ".", "softmax", "(", "output", ")", "\n", "denominator", "=", "torch", ".", "tensor", "(", "output", ".", "size", "(", "0", ")", "+", "1e-6", ")", "\n", "if", "output", ".", "size", "(", "0", ")", "==", "0", ":", "\n", "            ", "correct", "=", "torch", ".", "tensor", "(", "0.0", ")", "\n", "", "else", ":", "\n", "            ", "correct", "=", "torch", ".", "sum", "(", "(", "output", ".", "argmax", "(", "dim", "=", "-", "1", ")", ".", "eq", "(", "tgt_lm", ")", ")", ".", "float", "(", ")", ")", "\n", "\n", "", "loss", "=", "self", ".", "criterion", "(", "output", ",", "tgt_lm", ")", "\n", "return", "loss", ",", "correct", ",", "denominator", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.lm_target.LmTarget.forward": [[37, 52], ["lm_target.LmTarget.lm"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.lm_target.LmTarget.lm"], ["", "def", "forward", "(", "self", ",", "memory_bank", ",", "tgt", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            memory_bank: [batch_size x seq_length x hidden_size]\n            tgt: [batch_size x seq_length]\n\n        Returns:\n            loss: Language modeling loss.\n            correct: Number of words that are predicted correctly.\n            denominator: Number of predicted words.\n        \"\"\"", "\n", "# Language modeling (LM) with full softmax prediction.", "\n", "loss", ",", "correct", ",", "denominator", "=", "self", ".", "lm", "(", "memory_bank", ",", "tgt", ")", "\n", "\n", "return", "loss", ",", "correct", ",", "denominator", "\n", "", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.bert_target.BertTarget.__init__": [[12, 17], ["MlmTarget.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "vocab_size", ")", ":", "\n", "        ", "super", "(", "BertTarget", ",", "self", ")", ".", "__init__", "(", "args", ",", "vocab_size", ")", "\n", "# NSP.", "\n", "self", ".", "nsp_linear_1", "=", "nn", ".", "Linear", "(", "args", ".", "hidden_size", ",", "args", ".", "hidden_size", ")", "\n", "self", ".", "nsp_linear_2", "=", "nn", ".", "Linear", "(", "args", ".", "hidden_size", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.bert_target.BertTarget.forward": [[18, 44], ["bert_target.BertTarget.mlm", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "bert_target.BertTarget.nsp_linear_2", "bert_target.BertTarget.criterion", "bert_target.BertTarget.softmax().argmax().eq().sum", "type", "bert_target.BertTarget.nsp_linear_1", "bert_target.BertTarget.softmax", "bert_target.BertTarget.softmax().argmax().eq", "bert_target.BertTarget.softmax().argmax", "bert_target.BertTarget.softmax"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.mlm_target.MlmTarget.mlm"], ["", "def", "forward", "(", "self", ",", "memory_bank", ",", "tgt", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            memory_bank: [batch_size x seq_length x hidden_size]\n            tgt: tuple with tgt_mlm [batch_size x seq_length] and tgt_nsp [batch_size]\n\n        Returns:\n            loss_mlm: Masked language model loss.\n            loss_nsp: Next sentence prediction loss.\n            correct_mlm: Number of words that are predicted correctly.\n            correct_nsp: Number of sentences that are predicted correctly.\n            denominator: Number of masked words.\n        \"\"\"", "\n", "\n", "# Masked language model (MLM).", "\n", "assert", "type", "(", "tgt", ")", "==", "tuple", "\n", "tgt_mlm", ",", "tgt_nsp", "=", "tgt", "[", "0", "]", ",", "tgt", "[", "1", "]", "\n", "loss_mlm", ",", "correct_mlm", ",", "denominator", "=", "self", ".", "mlm", "(", "memory_bank", ",", "tgt_mlm", ")", "\n", "\n", "# Next sentence prediction (NSP).", "\n", "output_nsp", "=", "torch", ".", "tanh", "(", "self", ".", "nsp_linear_1", "(", "memory_bank", "[", ":", ",", "0", ",", ":", "]", ")", ")", "\n", "output_nsp", "=", "self", ".", "nsp_linear_2", "(", "output_nsp", ")", "\n", "loss_nsp", "=", "self", ".", "criterion", "(", "self", ".", "softmax", "(", "output_nsp", ")", ",", "tgt_nsp", ")", "\n", "correct_nsp", "=", "self", ".", "softmax", "(", "output_nsp", ")", ".", "argmax", "(", "dim", "=", "-", "1", ")", ".", "eq", "(", "tgt_nsp", ")", ".", "sum", "(", ")", "\n", "\n", "return", "loss_mlm", ",", "loss_nsp", ",", "correct_mlm", ",", "correct_nsp", ",", "denominator", "\n", "", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.seq2seq_target.Seq2seqTarget.__init__": [[10, 16], ["LmTarget.__init__", "len", "len"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "vocab_size", ")", ":", "\n", "        ", "super", "(", "Seq2seqTarget", ",", "self", ")", ".", "__init__", "(", "args", ",", "len", "(", "args", ".", "tgt_vocab", ")", ")", "\n", "\n", "self", ".", "embedding", "=", "str2embedding", "[", "args", ".", "tgt_embedding", "]", "(", "args", ",", "len", "(", "args", ".", "tgt_vocab", ")", ")", "\n", "\n", "self", ".", "decoder", "=", "str2decoder", "[", "args", ".", "decoder", "]", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.seq2seq_target.Seq2seqTarget.forward": [[17, 38], ["seq2seq_target.Seq2seqTarget.embedding", "seq2seq_target.Seq2seqTarget.decoder", "seq2seq_target.Seq2seqTarget.lm"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.lm_target.LmTarget.lm"], ["", "def", "forward", "(", "self", ",", "memory_bank", ",", "tgt", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            memory_bank: [batch_size x seq_length x hidden_size]\n            tgt: [batch_size x seq_length]\n\n        Returns:\n            loss: Language modeling loss.\n            correct: Number of words that are predicted correctly.\n            denominator: Number of predicted words.\n        \"\"\"", "\n", "tgt_in", ",", "tgt_out", ",", "src", "=", "tgt", "\n", "\n", "emb", "=", "self", ".", "embedding", "(", "tgt_in", ",", "None", ")", "\n", "\n", "hidden", "=", "self", ".", "decoder", "(", "memory_bank", ",", "emb", ",", "(", "src", ",", ")", ")", "\n", "\n", "# Language modeling (LM) with full softmax prediction.", "\n", "loss", ",", "correct", ",", "denominator", "=", "self", ".", "lm", "(", "hidden", ",", "tgt_out", ")", "\n", "\n", "return", "loss", ",", "correct", ",", "denominator", "\n", "", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.packet_distance_target.NspTarget.__init__": [[12, 20], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.LogSoftmax", "torch.LogSoftmax", "torch.NLLLoss", "torch.NLLLoss"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "vocab_size", ")", ":", "\n", "        ", "super", "(", "NspTarget", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "hidden_size", "=", "args", ".", "hidden_size", "\n", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "args", ".", "hidden_size", ",", "args", ".", "labels_num", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "-", "1", ")", "\n", "self", ".", "criterion", "=", "nn", ".", "NLLLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.packet_distance_target.NspTarget.forward": [[22, 38], ["packet_distance_target.NspTarget.linear", "packet_distance_target.NspTarget.criterion", "packet_distance_target.NspTarget.softmax().argmax().eq().sum", "packet_distance_target.NspTarget.softmax", "packet_distance_target.NspTarget.softmax().argmax().eq", "packet_distance_target.NspTarget.softmax().argmax", "packet_distance_target.NspTarget.softmax"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.act_fun.linear"], ["", "def", "forward", "(", "self", ",", "memory_bank", ",", "tgt", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            memory_bank: [batch_size x seq_length x hidden_size]\n            tgt: [batch_size]\n\n        Returns:\n            loss: Next sentence prediction loss.\n            correct: Number of sentences that are predicted correctly.\n        \"\"\"", "\n", "\n", "output", "=", "self", ".", "linear", "(", "memory_bank", "[", ":", ",", "0", ",", ":", "]", ")", "\n", "loss", "=", "self", ".", "criterion", "(", "self", ".", "softmax", "(", "output", ")", ",", "tgt", ")", "\n", "correct", "=", "self", ".", "softmax", "(", "output", ")", ".", "argmax", "(", "dim", "=", "-", "1", ")", ".", "eq", "(", "tgt", ")", ".", "sum", "(", ")", "\n", "\n", "return", "loss", ",", "correct", "\n", "", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.mlm_target.MlmTarget.__init__": [[13, 33], ["torch.Module.__init__", "torch.LogSoftmax", "torch.LogSoftmax", "torch.NLLLoss", "torch.NLLLoss", "torch.Linear", "torch.Linear", "uer.layers.layer_norm.LayerNorm", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "uer.layers.layer_norm.LayerNorm", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "vocab_size", ")", ":", "\n", "        ", "super", "(", "MlmTarget", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "hidden_size", "=", "args", ".", "hidden_size", "\n", "self", ".", "emb_size", "=", "args", ".", "emb_size", "\n", "self", ".", "factorized_embedding_parameterization", "=", "args", ".", "factorized_embedding_parameterization", "\n", "self", ".", "act", "=", "str2act", "[", "args", ".", "hidden_act", "]", "\n", "\n", "if", "self", ".", "factorized_embedding_parameterization", ":", "\n", "            ", "self", ".", "mlm_linear_1", "=", "nn", ".", "Linear", "(", "args", ".", "hidden_size", ",", "args", ".", "emb_size", ")", "\n", "self", ".", "layer_norm", "=", "LayerNorm", "(", "args", ".", "emb_size", ")", "\n", "self", ".", "mlm_linear_2", "=", "nn", ".", "Linear", "(", "args", ".", "emb_size", ",", "self", ".", "vocab_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "mlm_linear_1", "=", "nn", ".", "Linear", "(", "args", ".", "hidden_size", ",", "args", ".", "hidden_size", ")", "\n", "self", ".", "layer_norm", "=", "LayerNorm", "(", "args", ".", "hidden_size", ")", "\n", "self", ".", "mlm_linear_2", "=", "nn", ".", "Linear", "(", "args", ".", "hidden_size", ",", "self", ".", "vocab_size", ")", "\n", "\n", "", "self", ".", "softmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "self", ".", "criterion", "=", "nn", ".", "NLLLoss", "(", ")", "\n", "#self.criterion = nn.CrossEntropyLoss()", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.mlm_target.MlmTarget.mlm": [[35, 55], ["mlm_target.MlmTarget.act", "mlm_target.MlmTarget.layer_norm", "tgt_mlm.contiguous().view.contiguous().view.contiguous().view", "mlm_target.MlmTarget.mlm_linear_2", "mlm_target.MlmTarget.softmax", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "mlm_target.MlmTarget.criterion", "mlm_target.MlmTarget.mlm_linear_1", "output_mlm.contiguous().view.contiguous().view.contiguous().view", "output_mlm.contiguous().view.contiguous().view.contiguous().view", "output_mlm.contiguous().view.contiguous().view.size", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "tgt_mlm.contiguous().view.contiguous().view.contiguous", "output_mlm.contiguous().view.contiguous().view.size", "output_mlm.contiguous().view.contiguous().view.argmax().eq().float", "output_mlm.contiguous().view.contiguous().view.contiguous", "output_mlm.contiguous().view.contiguous().view.contiguous", "output_mlm.contiguous().view.contiguous().view.argmax().eq", "output_mlm.contiguous().view.contiguous().view.argmax"], "methods", ["None"], ["", "def", "mlm", "(", "self", ",", "memory_bank", ",", "tgt_mlm", ")", ":", "\n", "# Masked language modeling (MLM) with full softmax prediction.", "\n", "        ", "output_mlm", "=", "self", ".", "act", "(", "self", ".", "mlm_linear_1", "(", "memory_bank", ")", ")", "\n", "output_mlm", "=", "self", ".", "layer_norm", "(", "output_mlm", ")", "\n", "if", "self", ".", "factorized_embedding_parameterization", ":", "\n", "            ", "output_mlm", "=", "output_mlm", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "emb_size", ")", "\n", "", "else", ":", "\n", "            ", "output_mlm", "=", "output_mlm", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "hidden_size", ")", "\n", "", "tgt_mlm", "=", "tgt_mlm", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "output_mlm", "=", "output_mlm", "[", "tgt_mlm", ">", "0", ",", ":", "]", "\n", "tgt_mlm", "=", "tgt_mlm", "[", "tgt_mlm", ">", "0", "]", "\n", "output_mlm", "=", "self", ".", "mlm_linear_2", "(", "output_mlm", ")", "\n", "output_mlm", "=", "self", ".", "softmax", "(", "output_mlm", ")", "\n", "denominator", "=", "torch", ".", "tensor", "(", "output_mlm", ".", "size", "(", "0", ")", "+", "1e-6", ")", "\n", "if", "output_mlm", ".", "size", "(", "0", ")", "==", "0", ":", "\n", "            ", "correct_mlm", "=", "torch", ".", "tensor", "(", "0.0", ")", "\n", "", "else", ":", "\n", "            ", "correct_mlm", "=", "torch", ".", "sum", "(", "(", "output_mlm", ".", "argmax", "(", "dim", "=", "-", "1", ")", ".", "eq", "(", "tgt_mlm", ")", ")", ".", "float", "(", ")", ")", "\n", "", "loss_mlm", "=", "self", ".", "criterion", "(", "output_mlm", ",", "tgt_mlm", ")", "\n", "return", "loss_mlm", ",", "correct_mlm", ",", "denominator", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.mlm_target.MlmTarget.forward": [[56, 72], ["mlm_target.MlmTarget.mlm"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.mlm_target.MlmTarget.mlm"], ["", "def", "forward", "(", "self", ",", "memory_bank", ",", "tgt", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            memory_bank: [batch_size x seq_length x hidden_size]\n            tgt: [batch_size x seq_length]\n\n        Returns:\n            loss: Masked language modeling loss.\n            correct: Number of words that are predicted correctly.\n            denominator: Number of masked words.\n        \"\"\"", "\n", "\n", "# Masked language model (MLM).", "\n", "loss", ",", "correct", ",", "denominator", "=", "self", ".", "mlm", "(", "memory_bank", ",", "tgt", ")", "\n", "\n", "return", "loss", ",", "correct", ",", "denominator", "\n", "", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.cls_target.ClsTarget.__init__": [[12, 22], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.LogSoftmax", "torch.LogSoftmax", "torch.NLLLoss", "torch.NLLLoss"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "vocab_size", ")", ":", "\n", "        ", "super", "(", "ClsTarget", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "hidden_size", "=", "args", ".", "hidden_size", "\n", "\n", "self", ".", "pooling", "=", "args", ".", "pooling", "\n", "self", ".", "linear_1", "=", "nn", ".", "Linear", "(", "args", ".", "hidden_size", ",", "args", ".", "hidden_size", ")", "\n", "self", ".", "linear_2", "=", "nn", ".", "Linear", "(", "args", ".", "hidden_size", ",", "args", ".", "labels_num", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "-", "1", ")", "\n", "self", ".", "criterion", "=", "nn", ".", "NLLLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.targets.cls_target.ClsTarget.forward": [[24, 50], ["torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "cls_target.ClsTarget.linear_2", "cls_target.ClsTarget.criterion", "cls_target.ClsTarget.softmax().argmax().eq().sum", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "cls_target.ClsTarget.linear_1", "cls_target.ClsTarget.softmax", "cls_target.ClsTarget.softmax().argmax().eq", "torch.max", "torch.max", "torch.max", "torch.max", "cls_target.ClsTarget.softmax().argmax", "cls_target.ClsTarget.softmax"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "memory_bank", ",", "tgt", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            memory_bank: [batch_size x seq_length x hidden_size]\n            tgt: [batch_size]\n\n        Returns:\n            loss: Classification loss.\n            correct: Number of sentences that are predicted correctly.\n        \"\"\"", "\n", "\n", "if", "self", ".", "pooling", "==", "\"mean\"", ":", "\n", "            ", "output", "=", "torch", ".", "mean", "(", "memory_bank", ",", "dim", "=", "1", ")", "\n", "", "elif", "self", ".", "pooling", "==", "\"max\"", ":", "\n", "            ", "output", "=", "torch", ".", "max", "(", "memory_bank", ",", "dim", "=", "1", ")", "[", "0", "]", "\n", "", "elif", "self", ".", "pooling", "==", "\"last\"", ":", "\n", "            ", "output", "=", "memory_bank", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "", "else", ":", "\n", "            ", "output", "=", "memory_bank", "[", ":", ",", "0", ",", ":", "]", "\n", "", "output", "=", "torch", ".", "tanh", "(", "self", ".", "linear_1", "(", "output", ")", ")", "\n", "logits", "=", "self", ".", "linear_2", "(", "output", ")", "\n", "\n", "loss", "=", "self", ".", "criterion", "(", "self", ".", "softmax", "(", "logits", ")", ",", "tgt", ")", "\n", "correct", "=", "self", ".", "softmax", "(", "logits", ")", ".", "argmax", "(", "dim", "=", "-", "1", ")", ".", "eq", "(", "tgt", ")", ".", "sum", "(", ")", "\n", "\n", "return", "loss", ",", "correct", "\n", "", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.cnn_encoder.GatedcnnEncoder.__init__": [[6, 40], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "range", "range", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "range", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "range"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "GatedcnnEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layers_num", "=", "args", ".", "layers_num", "\n", "self", ".", "kernel_size", "=", "args", ".", "kernel_size", "\n", "self", ".", "block_size", "=", "args", ".", "block_size", "\n", "self", ".", "emb_size", "=", "args", ".", "emb_size", "\n", "self", ".", "hidden_size", "=", "args", ".", "hidden_size", "\n", "\n", "self", ".", "conv_1", "=", "nn", ".", "Conv2d", "(", "1", ",", "args", ".", "hidden_size", ",", "(", "args", ".", "kernel_size", ",", "args", ".", "emb_size", ")", ")", "\n", "self", ".", "gate_1", "=", "nn", ".", "Conv2d", "(", "1", ",", "args", ".", "hidden_size", ",", "(", "args", ".", "kernel_size", ",", "args", ".", "emb_size", ")", ")", "\n", "\n", "self", ".", "conv_b1", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "1", ",", "args", ".", "hidden_size", ",", "1", ",", "1", ")", ")", "\n", "self", ".", "gate_b1", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "1", ",", "args", ".", "hidden_size", ",", "1", ",", "1", ")", ")", "\n", "\n", "self", ".", "conv", "=", "nn", ".", "ModuleList", "(", "\n", "[", "\n", "nn", ".", "Conv2d", "(", "args", ".", "hidden_size", ",", "args", ".", "hidden_size", ",", "(", "args", ".", "kernel_size", ",", "1", ")", ")", "\n", "for", "_", "in", "range", "(", "args", ".", "layers_num", "-", "1", ")", "\n", "]", "\n", ")", "\n", "self", ".", "gate", "=", "nn", ".", "ModuleList", "(", "\n", "[", "\n", "nn", ".", "Conv2d", "(", "args", ".", "hidden_size", ",", "args", ".", "hidden_size", ",", "(", "args", ".", "kernel_size", ",", "1", ")", ")", "\n", "for", "_", "in", "range", "(", "args", ".", "layers_num", "-", "1", ")", "\n", "]", "\n", ")", "\n", "\n", "self", ".", "conv_b", "=", "nn", ".", "ParameterList", "(", "\n", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "1", ",", "args", ".", "hidden_size", ",", "1", ",", "1", ")", ")", "\n", "for", "_", "in", "range", "(", "args", ".", "layers_num", "-", "1", ")", "\n", ")", "\n", "self", ".", "gate_b", "=", "nn", ".", "ParameterList", "(", "\n", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "1", ",", "args", ".", "hidden_size", ",", "1", ",", "1", ")", ")", "\n", "for", "_", "in", "range", "(", "args", ".", "layers_num", "-", "1", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.cnn_encoder.GatedcnnEncoder.forward": [[42, 73], ["torch.cat().unsqueeze.size", "torch.cat().unsqueeze.size", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "cnn_encoder.GatedcnnEncoder.conv_1", "cnn_encoder.GatedcnnEncoder.conv_b1.repeat", "cnn_encoder.GatedcnnEncoder.gate_1", "cnn_encoder.GatedcnnEncoder.gate_b1.repeat", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "enumerate", "torch.cat.transpose().contiguous().view", "torch.cat.transpose().contiguous().view", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "zip", "cnn_encoder.GatedcnnEncoder.conv_b[].repeat", "cnn_encoder.GatedcnnEncoder.gate_b[].repeat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "conv_i", "gate_i", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.cat.transpose().contiguous", "torch.cat.transpose().contiguous", "torch.cat.transpose", "torch.cat.transpose"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "emb", ",", "seg", ")", ":", "\n", "        ", "batch_size", ",", "seq_length", ",", "_", "=", "emb", ".", "size", "(", ")", "\n", "\n", "padding", "=", "torch", ".", "zeros", "(", "[", "batch_size", ",", "self", ".", "kernel_size", "-", "1", ",", "self", ".", "emb_size", "]", ")", ".", "to", "(", "emb", ".", "device", ")", "\n", "emb", "=", "torch", ".", "cat", "(", "[", "padding", ",", "emb", "]", ",", "dim", "=", "1", ")", ".", "unsqueeze", "(", "1", ")", "# batch_size, 1, seq_length+width-1, emb_size", "\n", "\n", "hidden", "=", "self", ".", "conv_1", "(", "emb", ")", "\n", "hidden", "+=", "self", ".", "conv_b1", ".", "repeat", "(", "1", ",", "1", ",", "seq_length", ",", "1", ")", "\n", "gate", "=", "self", ".", "gate_1", "(", "emb", ")", "\n", "gate", "+=", "self", ".", "gate_b1", ".", "repeat", "(", "1", ",", "1", ",", "seq_length", ",", "1", ")", "\n", "hidden", "=", "hidden", "*", "torch", ".", "sigmoid", "(", "gate", ")", "\n", "\n", "res_input", "=", "hidden", "\n", "\n", "padding", "=", "torch", ".", "zeros", "(", "[", "batch_size", ",", "self", ".", "hidden_size", ",", "self", ".", "kernel_size", "-", "1", ",", "1", "]", ")", ".", "to", "(", "emb", ".", "device", ")", "\n", "hidden", "=", "torch", ".", "cat", "(", "[", "padding", ",", "hidden", "]", ",", "dim", "=", "2", ")", "\n", "\n", "for", "i", ",", "(", "conv_i", ",", "gate_i", ")", "in", "enumerate", "(", "zip", "(", "self", ".", "conv", ",", "self", ".", "gate", ")", ")", ":", "\n", "            ", "hidden", ",", "gate", "=", "conv_i", "(", "hidden", ")", ",", "gate_i", "(", "hidden", ")", "\n", "hidden", "+=", "self", ".", "conv_b", "[", "i", "]", ".", "repeat", "(", "1", ",", "1", ",", "seq_length", ",", "1", ")", "\n", "gate", "+=", "self", ".", "gate_b", "[", "i", "]", ".", "repeat", "(", "1", ",", "1", ",", "seq_length", ",", "1", ")", "\n", "hidden", "=", "hidden", "*", "torch", ".", "sigmoid", "(", "gate", ")", "\n", "if", "(", "i", "+", "1", ")", "%", "self", ".", "block_size", "==", "0", ":", "\n", "                ", "hidden", "=", "hidden", "+", "res_input", "\n", "res_input", "=", "hidden", "\n", "", "hidden", "=", "torch", ".", "cat", "(", "[", "padding", ",", "hidden", "]", ",", "dim", "=", "2", ")", "\n", "\n", "", "hidden", "=", "hidden", "[", ":", ",", ":", ",", "self", ".", "kernel_size", "-", "1", ":", ",", ":", "]", "\n", "output", "=", "hidden", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "seq_length", ",", "self", ".", "hidden_size", ")", "\n", "\n", "return", "output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.transformer_encoder.TransformerEncoder.__init__": [[11, 40], ["torch.Module.__init__", "bool", "torch.Linear", "torch.Linear", "uer.layers.transformer.TransformerLayer", "torch.ModuleList", "torch.ModuleList", "uer.layers.relative_position_embedding.RelativePositionEmbedding", "uer.layers.layer_norm.T5LayerNorm", "uer.layers.layer_norm.LayerNorm", "uer.layers.transformer.TransformerLayer", "range"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "TransformerEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "mask", "=", "args", ".", "mask", "\n", "self", ".", "layers_num", "=", "args", ".", "layers_num", "\n", "self", ".", "parameter_sharing", "=", "args", ".", "parameter_sharing", "\n", "self", ".", "factorized_embedding_parameterization", "=", "args", ".", "factorized_embedding_parameterization", "\n", "self", ".", "layernorm_positioning", "=", "args", ".", "layernorm_positioning", "\n", "self", ".", "relative_position_embedding", "=", "args", ".", "relative_position_embedding", "\n", "\n", "has_bias", "=", "bool", "(", "1", "-", "args", ".", "remove_transformer_bias", ")", "\n", "\n", "if", "self", ".", "factorized_embedding_parameterization", ":", "\n", "            ", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "args", ".", "emb_size", ",", "args", ".", "hidden_size", ")", "\n", "\n", "", "if", "self", ".", "parameter_sharing", ":", "\n", "            ", "self", ".", "transformer", "=", "TransformerLayer", "(", "args", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "transformer", "=", "nn", ".", "ModuleList", "(", "\n", "[", "TransformerLayer", "(", "args", ")", "for", "_", "in", "range", "(", "self", ".", "layers_num", ")", "]", "\n", ")", "\n", "", "if", "self", ".", "layernorm_positioning", "==", "\"pre\"", ":", "\n", "            ", "if", "args", ".", "layernorm", "==", "\"t5\"", ":", "\n", "                ", "self", ".", "layer_norm", "=", "T5LayerNorm", "(", "args", ".", "hidden_size", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "layer_norm", "=", "LayerNorm", "(", "args", ".", "hidden_size", ")", "\n", "\n", "", "", "if", "self", ".", "relative_position_embedding", ":", "\n", "            ", "self", ".", "relative_pos_emb", "=", "RelativePositionEmbedding", "(", "bidirectional", "=", "True", ",", "heads_num", "=", "args", ".", "heads_num", ",", "\n", "num_buckets", "=", "args", ".", "relative_attention_buckets_num", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.transformer_encoder.TransformerEncoder.forward": [[42, 103], ["transformer_encoder.TransformerEncoder.size", "range", "transformer_encoder.TransformerEncoder.linear", "mask.repeat.repeat.float", "transformer_encoder.TransformerEncoder.relative_pos_emb", "transformer_encoder.TransformerEncoder.layer_norm", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "mask.repeat.repeat.repeat", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "mask_tril.repeat.repeat.repeat", "transformer_encoder.TransformerEncoder.transformer"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.act_fun.linear"], ["", "", "def", "forward", "(", "self", ",", "emb", ",", "seg", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            emb: [batch_size x seq_length x emb_size]\n            seg: [batch_size x seq_length]\n        Returns:\n            hidden: [batch_size x seq_length x hidden_size]\n        \"\"\"", "\n", "if", "self", ".", "factorized_embedding_parameterization", ":", "\n", "            ", "emb", "=", "self", ".", "linear", "(", "emb", ")", "\n", "\n", "", "batch_size", ",", "seq_length", ",", "_", "=", "emb", ".", "size", "(", ")", "\n", "# Generate mask according to segment indicators.", "\n", "# mask: [batch_size x 1 x seq_length x seq_length]", "\n", "if", "self", ".", "mask", "==", "\"fully_visible\"", ":", "\n", "            ", "mask", "=", "(", "seg", ">", "0", ")", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "seq_length", ",", "1", ")", ".", "unsqueeze", "(", "1", ")", "\n", "mask", "=", "mask", ".", "float", "(", ")", "\n", "mask", "=", "(", "1.0", "-", "mask", ")", "*", "-", "10000.0", "\n", "", "elif", "self", ".", "mask", "==", "\"causal\"", ":", "\n", "            ", "mask", "=", "torch", ".", "ones", "(", "seq_length", ",", "seq_length", ",", "device", "=", "emb", ".", "device", ")", "\n", "mask", "=", "torch", ".", "tril", "(", "mask", ")", "\n", "mask", "=", "(", "1.0", "-", "mask", ")", "*", "-", "10000", "\n", "mask", "=", "mask", ".", "repeat", "(", "batch_size", ",", "1", ",", "1", ",", "1", ")", "\n", "", "else", ":", "\n", "            ", "mask_a", "=", "(", "seg", "==", "1", ")", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "seq_length", ",", "1", ")", ".", "unsqueeze", "(", "1", ")", ".", "float", "(", ")", "\n", "\n", "mask_b", "=", "(", "seg", ">", "0", ")", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "seq_length", ",", "1", ")", ".", "unsqueeze", "(", "1", ")", ".", "float", "(", ")", "\n", "\n", "mask_tril", "=", "torch", ".", "ones", "(", "seq_length", ",", "seq_length", ",", "device", "=", "emb", ".", "device", ")", "\n", "mask_tril", "=", "torch", ".", "tril", "(", "mask_tril", ")", "\n", "mask_tril", "=", "mask_tril", ".", "repeat", "(", "batch_size", ",", "1", ",", "1", ",", "1", ")", "\n", "\n", "mask", "=", "(", "mask_a", "+", "mask_b", "+", "mask_tril", ">=", "2", ")", ".", "float", "(", ")", "\n", "mask", "=", "(", "1.0", "-", "mask", ")", "*", "-", "10000.0", "\n", "\n", "", "hidden", "=", "emb", "\n", "\n", "if", "self", ".", "relative_position_embedding", ":", "\n", "            ", "position_bias", "=", "self", ".", "relative_pos_emb", "(", "hidden", ",", "hidden", ")", "\n", "", "else", ":", "\n", "            ", "position_bias", "=", "None", "\n", "\n", "", "for", "i", "in", "range", "(", "self", ".", "layers_num", ")", ":", "\n", "            ", "if", "self", ".", "parameter_sharing", ":", "\n", "                ", "hidden", "=", "self", ".", "transformer", "(", "hidden", ",", "mask", ",", "position_bias", "=", "position_bias", ")", "\n", "", "else", ":", "\n", "                ", "hidden", "=", "self", ".", "transformer", "[", "i", "]", "(", "hidden", ",", "mask", ",", "position_bias", "=", "position_bias", ")", "\n", "\n", "", "", "if", "self", ".", "layernorm_positioning", "==", "\"pre\"", ":", "\n", "            ", "return", "self", ".", "layer_norm", "(", "hidden", ")", "\n", "", "else", ":", "\n", "            ", "return", "hidden", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.RnnEncoder.__init__": [[7, 26], ["torch.Module.__init__", "torch.RNN", "torch.RNN", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "RnnEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "bidirectional", "=", "args", ".", "bidirectional", "\n", "if", "self", ".", "bidirectional", ":", "\n", "            ", "assert", "args", ".", "hidden_size", "%", "2", "==", "0", "\n", "self", ".", "hidden_size", "=", "args", ".", "hidden_size", "//", "2", "\n", "", "else", ":", "\n", "            ", "self", ".", "hidden_size", "=", "args", ".", "hidden_size", "\n", "", "self", ".", "layers_num", "=", "args", ".", "layers_num", "\n", "\n", "self", ".", "rnn", "=", "nn", ".", "RNN", "(", "input_size", "=", "args", ".", "emb_size", ",", "\n", "hidden_size", "=", "self", ".", "hidden_size", ",", "\n", "num_layers", "=", "args", ".", "layers_num", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "batch_first", "=", "True", ",", "\n", "bidirectional", "=", "self", ".", "bidirectional", ")", "\n", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "args", ".", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.RnnEncoder.forward": [[27, 32], ["rnn_encoder.RnnEncoder.init_hidden", "rnn_encoder.RnnEncoder.rnn", "rnn_encoder.RnnEncoder.drop", "emb.size"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BilstmEncoder.init_hidden"], ["", "def", "forward", "(", "self", ",", "emb", ",", "_", ")", ":", "\n", "        ", "hidden", "=", "self", ".", "init_hidden", "(", "emb", ".", "size", "(", "0", ")", ",", "emb", ".", "device", ")", "\n", "output", ",", "hidden", "=", "self", ".", "rnn", "(", "emb", ",", "hidden", ")", "\n", "output", "=", "self", ".", "drop", "(", "output", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.RnnEncoder.init_hidden": [[33, 38], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "batch_size", ",", "device", ")", ":", "\n", "        ", "if", "self", ".", "bidirectional", ":", "\n", "            ", "return", "torch", ".", "zeros", "(", "self", ".", "layers_num", "*", "2", ",", "batch_size", ",", "self", ".", "hidden_size", ",", "device", "=", "device", ")", "\n", "", "else", ":", "\n", "            ", "return", "torch", ".", "zeros", "(", "self", ".", "layers_num", ",", "batch_size", ",", "self", ".", "hidden_size", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.LstmEncoder.__init__": [[41, 50], ["rnn_encoder.RnnEncoder.__init__", "torch.LSTM", "torch.LSTM"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "LstmEncoder", ",", "self", ")", ".", "__init__", "(", "args", ")", "\n", "\n", "self", ".", "rnn", "=", "nn", ".", "LSTM", "(", "input_size", "=", "args", ".", "emb_size", ",", "\n", "hidden_size", "=", "self", ".", "hidden_size", ",", "\n", "num_layers", "=", "args", ".", "layers_num", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "batch_first", "=", "True", ",", "\n", "bidirectional", "=", "self", ".", "bidirectional", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.LstmEncoder.init_hidden": [[51, 58], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "batch_size", ",", "device", ")", ":", "\n", "        ", "if", "self", ".", "bidirectional", ":", "\n", "            ", "return", "(", "torch", ".", "zeros", "(", "self", ".", "layers_num", "*", "2", ",", "batch_size", ",", "self", ".", "hidden_size", ",", "device", "=", "device", ")", ",", "\n", "torch", ".", "zeros", "(", "self", ".", "layers_num", "*", "2", ",", "batch_size", ",", "self", ".", "hidden_size", ",", "device", "=", "device", ")", ")", "\n", "", "else", ":", "\n", "            ", "return", "(", "torch", ".", "zeros", "(", "self", ".", "layers_num", ",", "batch_size", ",", "self", ".", "hidden_size", ",", "device", "=", "device", ")", ",", "\n", "torch", ".", "zeros", "(", "self", ".", "layers_num", ",", "batch_size", ",", "self", ".", "hidden_size", ",", "device", "=", "device", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.GruEncoder.__init__": [[61, 70], ["rnn_encoder.RnnEncoder.__init__", "torch.GRU", "torch.GRU"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "GruEncoder", ",", "self", ")", ".", "__init__", "(", "args", ")", "\n", "\n", "self", ".", "rnn", "=", "nn", ".", "GRU", "(", "input_size", "=", "args", ".", "emb_size", ",", "\n", "hidden_size", "=", "self", ".", "hidden_size", ",", "\n", "num_layers", "=", "args", ".", "layers_num", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "batch_first", "=", "True", ",", "\n", "bidirectional", "=", "self", ".", "bidirectional", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BirnnEncoder.__init__": [[73, 93], ["torch.Module.__init__", "torch.RNN", "torch.RNN", "torch.RNN", "torch.RNN", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "BirnnEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "assert", "args", ".", "hidden_size", "%", "2", "==", "0", "\n", "self", ".", "hidden_size", "=", "args", ".", "hidden_size", "//", "2", "\n", "self", ".", "layers_num", "=", "args", ".", "layers_num", "\n", "\n", "self", ".", "rnn_forward", "=", "nn", ".", "RNN", "(", "input_size", "=", "args", ".", "emb_size", ",", "\n", "hidden_size", "=", "self", ".", "hidden_size", ",", "\n", "num_layers", "=", "args", ".", "layers_num", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "batch_first", "=", "True", ")", "\n", "\n", "self", ".", "rnn_backward", "=", "nn", ".", "RNN", "(", "input_size", "=", "args", ".", "emb_size", ",", "\n", "hidden_size", "=", "self", ".", "hidden_size", ",", "\n", "num_layers", "=", "args", ".", "layers_num", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "batch_first", "=", "True", ")", "\n", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "args", ".", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BirnnEncoder.forward": [[94, 109], ["rnn_encoder.BirnnEncoder.init_hidden", "rnn_encoder.BirnnEncoder.rnn_forward", "rnn_encoder.BirnnEncoder.drop", "flip", "rnn_encoder.BirnnEncoder.init_hidden", "rnn_encoder.BirnnEncoder.rnn_backward", "rnn_encoder.BirnnEncoder.drop", "flip", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "emb_forward.size", "flip.size"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BilstmEncoder.init_hidden", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.misc.flip", "home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BilstmEncoder.init_hidden", "home.repos.pwc.inspect_result.linwhitehat_et-bert.utils.misc.flip"], ["", "def", "forward", "(", "self", ",", "emb", ",", "_", ")", ":", "\n", "# Forward.", "\n", "        ", "emb_forward", "=", "emb", "\n", "hidden_forward", "=", "self", ".", "init_hidden", "(", "emb_forward", ".", "size", "(", "0", ")", ",", "emb_forward", ".", "device", ")", "\n", "output_forward", ",", "hidden_forward", "=", "self", ".", "rnn_forward", "(", "emb_forward", ",", "hidden_forward", ")", "\n", "output_forward", "=", "self", ".", "drop", "(", "output_forward", ")", "\n", "\n", "# Backward.", "\n", "emb_backward", "=", "flip", "(", "emb", ",", "1", ")", "\n", "hidden_backward", "=", "self", ".", "init_hidden", "(", "emb_backward", ".", "size", "(", "0", ")", ",", "emb_backward", ".", "device", ")", "\n", "output_backward", ",", "hidden_backward", "=", "self", ".", "rnn_backward", "(", "emb_backward", ",", "hidden_backward", ")", "\n", "output_backward", "=", "self", ".", "drop", "(", "output_backward", ")", "\n", "output_backward", "=", "flip", "(", "output_backward", ",", "1", ")", "\n", "\n", "return", "torch", ".", "cat", "(", "[", "output_forward", ",", "output_backward", "]", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BirnnEncoder.init_hidden": [[110, 112], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "batch_size", ",", "device", ")", ":", "\n", "        ", "return", "torch", ".", "zeros", "(", "self", ".", "layers_num", ",", "batch_size", ",", "self", ".", "hidden_size", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BilstmEncoder.__init__": [[115, 129], ["rnn_encoder.BirnnEncoder.__init__", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "BilstmEncoder", ",", "self", ")", ".", "__init__", "(", "args", ")", "\n", "\n", "self", ".", "rnn_forward", "=", "nn", ".", "LSTM", "(", "input_size", "=", "args", ".", "emb_size", ",", "\n", "hidden_size", "=", "self", ".", "hidden_size", ",", "\n", "num_layers", "=", "args", ".", "layers_num", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "batch_first", "=", "True", ")", "\n", "\n", "self", ".", "rnn_backward", "=", "nn", ".", "LSTM", "(", "input_size", "=", "args", ".", "emb_size", ",", "\n", "hidden_size", "=", "self", ".", "hidden_size", ",", "\n", "num_layers", "=", "args", ".", "layers_num", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "batch_first", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BilstmEncoder.init_hidden": [[130, 133], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "batch_size", ",", "device", ")", ":", "\n", "        ", "return", "(", "torch", ".", "zeros", "(", "self", ".", "layers_num", ",", "batch_size", ",", "self", ".", "hidden_size", ",", "device", "=", "device", ")", ",", "\n", "torch", ".", "zeros", "(", "self", ".", "layers_num", ",", "batch_size", ",", "self", ".", "hidden_size", ",", "device", "=", "device", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__": [[136, 150], ["rnn_encoder.BirnnEncoder.__init__", "torch.GRU", "torch.GRU", "torch.GRU", "torch.GRU"], "methods", ["home.repos.pwc.inspect_result.linwhitehat_et-bert.encoders.rnn_encoder.BigruEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "BigruEncoder", ",", "self", ")", ".", "__init__", "(", "args", ")", "\n", "\n", "self", ".", "rnn_forward", "=", "nn", ".", "GRU", "(", "input_size", "=", "args", ".", "emb_size", ",", "\n", "hidden_size", "=", "self", ".", "hidden_size", ",", "\n", "num_layers", "=", "args", ".", "layers_num", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "batch_first", "=", "True", ")", "\n", "\n", "self", ".", "rnn_backward", "=", "nn", ".", "GRU", "(", "input_size", "=", "args", ".", "emb_size", ",", "\n", "hidden_size", "=", "self", ".", "hidden_size", ",", "\n", "num_layers", "=", "args", ".", "layers_num", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "batch_first", "=", "True", ")", "\n", "", "", ""]]}