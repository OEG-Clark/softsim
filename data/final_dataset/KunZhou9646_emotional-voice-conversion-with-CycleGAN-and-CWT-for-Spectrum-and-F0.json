{"home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_f0.gated_linear_layer": [[3, 8], ["tensorflow.multiply", "tensorflow.sigmoid"], "function", ["None"], ["def", "gated_linear_layer", "(", "inputs", ",", "gates", ",", "name", "=", "None", ")", ":", "\n", "\n", "    ", "activation", "=", "tf", ".", "multiply", "(", "x", "=", "inputs", ",", "y", "=", "tf", ".", "sigmoid", "(", "gates", ")", ",", "name", "=", "name", ")", "\n", "\n", "return", "activation", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_f0.instance_norm_layer": [[9, 21], ["tensorflow.contrib.layers.instance_norm"], "function", ["None"], ["", "def", "instance_norm_layer", "(", "\n", "inputs", ",", "\n", "epsilon", "=", "1e-06", ",", "\n", "activation_fn", "=", "None", ",", "\n", "name", "=", "None", ")", ":", "\n", "\n", "    ", "instance_norm_layer", "=", "tf", ".", "contrib", ".", "layers", ".", "instance_norm", "(", "\n", "inputs", "=", "inputs", ",", "\n", "epsilon", "=", "epsilon", ",", "\n", "activation_fn", "=", "activation_fn", ")", "\n", "\n", "return", "instance_norm_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_f0.conv1d_layer": [[22, 43], ["tensorflow.layers.conv1d"], "function", ["None"], ["", "def", "conv1d_layer", "(", "\n", "inputs", ",", "\n", "filters", ",", "\n", "kernel_size", ",", "\n", "strides", "=", "1", ",", "\n", "padding", "=", "'same'", ",", "\n", "activation", "=", "None", ",", "\n", "kernel_initializer", "=", "None", ",", "\n", "name", "=", "None", ")", ":", "\n", "\n", "    ", "conv_layer", "=", "tf", ".", "layers", ".", "conv1d", "(", "\n", "inputs", "=", "inputs", ",", "\n", "filters", "=", "filters", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "strides", "=", "strides", ",", "\n", "padding", "=", "padding", ",", "\n", "activation", "=", "activation", ",", "\n", "kernel_initializer", "=", "kernel_initializer", ",", "\n", "name", "=", "name", ")", "\n", "\n", "return", "conv_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_f0.conv2d_layer": [[44, 65], ["tensorflow.layers.conv2d"], "function", ["None"], ["", "def", "conv2d_layer", "(", "\n", "inputs", ",", "\n", "filters", ",", "\n", "kernel_size", ",", "\n", "strides", ",", "\n", "padding", "=", "'same'", ",", "\n", "activation", "=", "None", ",", "\n", "kernel_initializer", "=", "None", ",", "\n", "name", "=", "None", ")", ":", "\n", "\n", "    ", "conv_layer", "=", "tf", ".", "layers", ".", "conv2d", "(", "\n", "inputs", "=", "inputs", ",", "\n", "filters", "=", "filters", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "strides", "=", "strides", ",", "\n", "padding", "=", "padding", ",", "\n", "activation", "=", "activation", ",", "\n", "kernel_initializer", "=", "kernel_initializer", ",", "\n", "name", "=", "name", ")", "\n", "\n", "return", "conv_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_f0.residual1d_block": [[66, 84], ["module_f0.conv1d_layer", "module_f0.instance_norm_layer", "module_f0.conv1d_layer", "module_f0.instance_norm_layer", "module_f0.gated_linear_layer", "module_f0.conv1d_layer", "module_f0.instance_norm_layer"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.gated_linear_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer"], ["", "def", "residual1d_block", "(", "\n", "inputs", ",", "\n", "filters", "=", "1024", ",", "\n", "kernel_size", "=", "3", ",", "\n", "strides", "=", "1", ",", "\n", "name_prefix", "=", "'residule_block_'", ")", ":", "\n", "\n", "    ", "h1", "=", "conv1d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "filters", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_conv'", ")", "\n", "h1_norm", "=", "instance_norm_layer", "(", "inputs", "=", "h1", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_norm'", ")", "\n", "h1_gates", "=", "conv1d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "filters", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_gates'", ")", "\n", "h1_norm_gates", "=", "instance_norm_layer", "(", "inputs", "=", "h1_gates", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_norm_gates'", ")", "\n", "h1_glu", "=", "gated_linear_layer", "(", "inputs", "=", "h1_norm", ",", "gates", "=", "h1_norm_gates", ",", "name", "=", "name_prefix", "+", "'h1_glu'", ")", "\n", "h2", "=", "conv1d_layer", "(", "inputs", "=", "h1_glu", ",", "filters", "=", "filters", "//", "2", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h2_conv'", ")", "\n", "h2_norm", "=", "instance_norm_layer", "(", "inputs", "=", "h2", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h2_norm'", ")", "\n", "\n", "h3", "=", "inputs", "+", "h2_norm", "\n", "\n", "return", "h3", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_f0.downsample1d_block": [[85, 99], ["module_f0.conv1d_layer", "module_f0.instance_norm_layer", "module_f0.conv1d_layer", "module_f0.instance_norm_layer", "module_f0.gated_linear_layer"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.gated_linear_layer"], ["", "def", "downsample1d_block", "(", "\n", "inputs", ",", "\n", "filters", ",", "\n", "kernel_size", ",", "\n", "strides", ",", "\n", "name_prefix", "=", "'downsample1d_block_'", ")", ":", "\n", "\n", "    ", "h1", "=", "conv1d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "filters", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_conv'", ")", "\n", "h1_norm", "=", "instance_norm_layer", "(", "inputs", "=", "h1", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_norm'", ")", "\n", "h1_gates", "=", "conv1d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "filters", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_gates'", ")", "\n", "h1_norm_gates", "=", "instance_norm_layer", "(", "inputs", "=", "h1_gates", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_norm_gates'", ")", "\n", "h1_glu", "=", "gated_linear_layer", "(", "inputs", "=", "h1_norm", ",", "gates", "=", "h1_norm_gates", ",", "name", "=", "name_prefix", "+", "'h1_glu'", ")", "\n", "\n", "return", "h1_glu", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_f0.downsample2d_block": [[100, 114], ["module_f0.conv2d_layer", "module_f0.instance_norm_layer", "module_f0.conv2d_layer", "module_f0.instance_norm_layer", "module_f0.gated_linear_layer"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv2d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv2d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.gated_linear_layer"], ["", "def", "downsample2d_block", "(", "\n", "inputs", ",", "\n", "filters", ",", "\n", "kernel_size", ",", "\n", "strides", ",", "\n", "name_prefix", "=", "'downsample2d_block_'", ")", ":", "\n", "\n", "    ", "h1", "=", "conv2d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "filters", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_conv'", ")", "\n", "h1_norm", "=", "instance_norm_layer", "(", "inputs", "=", "h1", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_norm'", ")", "\n", "h1_gates", "=", "conv2d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "filters", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_gates'", ")", "\n", "h1_norm_gates", "=", "instance_norm_layer", "(", "inputs", "=", "h1_gates", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_norm_gates'", ")", "\n", "h1_glu", "=", "gated_linear_layer", "(", "inputs", "=", "h1_norm", ",", "gates", "=", "h1_norm_gates", ",", "name", "=", "name_prefix", "+", "'h1_glu'", ")", "\n", "\n", "return", "h1_glu", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_f0.upsample1d_block": [[115, 134], ["module_f0.conv1d_layer", "module_f0.pixel_shuffler", "module_f0.instance_norm_layer", "module_f0.conv1d_layer", "module_f0.pixel_shuffler", "module_f0.instance_norm_layer", "module_f0.gated_linear_layer"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.pixel_shuffler", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.pixel_shuffler", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.gated_linear_layer"], ["", "def", "upsample1d_block", "(", "\n", "inputs", ",", "\n", "filters", ",", "\n", "kernel_size", ",", "\n", "strides", ",", "\n", "shuffle_size", "=", "2", ",", "\n", "name_prefix", "=", "'upsample1d_block_'", ")", ":", "\n", "\n", "    ", "h1", "=", "conv1d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "filters", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_conv'", ")", "\n", "h1_shuffle", "=", "pixel_shuffler", "(", "inputs", "=", "h1", ",", "shuffle_size", "=", "shuffle_size", ",", "name", "=", "name_prefix", "+", "'h1_shuffle'", ")", "\n", "h1_norm", "=", "instance_norm_layer", "(", "inputs", "=", "h1_shuffle", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_norm'", ")", "\n", "\n", "h1_gates", "=", "conv1d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "filters", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_gates'", ")", "\n", "h1_shuffle_gates", "=", "pixel_shuffler", "(", "inputs", "=", "h1_gates", ",", "shuffle_size", "=", "shuffle_size", ",", "name", "=", "name_prefix", "+", "'h1_shuffle_gates'", ")", "\n", "h1_norm_gates", "=", "instance_norm_layer", "(", "inputs", "=", "h1_shuffle_gates", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_norm_gates'", ")", "\n", "\n", "h1_glu", "=", "gated_linear_layer", "(", "inputs", "=", "h1_norm", ",", "gates", "=", "h1_norm_gates", ",", "name", "=", "name_prefix", "+", "'h1_glu'", ")", "\n", "\n", "return", "h1_glu", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_f0.pixel_shuffler": [[135, 147], ["tensorflow.reshape", "tensorflow.shape", "tensorflow.shape", "inputs.get_shape().as_list", "inputs.get_shape"], "function", ["None"], ["", "def", "pixel_shuffler", "(", "inputs", ",", "shuffle_size", "=", "2", ",", "name", "=", "None", ")", ":", "\n", "\n", "    ", "n", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "0", "]", "\n", "w", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "1", "]", "\n", "c", "=", "inputs", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "2", "]", "\n", "\n", "oc", "=", "c", "//", "shuffle_size", "\n", "ow", "=", "w", "*", "shuffle_size", "\n", "\n", "outputs", "=", "tf", ".", "reshape", "(", "tensor", "=", "inputs", ",", "shape", "=", "[", "n", ",", "ow", ",", "oc", "]", ",", "name", "=", "name", ")", "\n", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_f0.generator_gatedcnn": [[148, 186], ["tensorflow.transpose", "tensorflow.variable_scope", "module_f0.conv1d_layer", "module_f0.conv1d_layer", "module_f0.gated_linear_layer", "module_f0.downsample1d_block", "module_f0.downsample1d_block", "module_f0.residual1d_block", "module_f0.residual1d_block", "module_f0.residual1d_block", "module_f0.residual1d_block", "module_f0.residual1d_block", "module_f0.residual1d_block", "module_f0.upsample1d_block", "module_f0.upsample1d_block", "module_f0.conv1d_layer", "tensorflow.transpose", "scope.reuse_variables"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.gated_linear_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.downsample1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.downsample1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.residual1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.residual1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.residual1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.residual1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.residual1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.residual1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.upsample1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.upsample1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer"], ["", "def", "generator_gatedcnn", "(", "inputs", ",", "reuse", "=", "False", ",", "scope_name", "=", "'generator_gatedcnn'", ")", ":", "\n", "\n", "# inputs has shape [batch_size, num_features, time]", "\n", "# we need to convert it to [batch_size, time, num_features] for 1D convolution", "\n", "    ", "inputs", "=", "tf", ".", "transpose", "(", "inputs", ",", "perm", "=", "[", "0", ",", "2", ",", "1", "]", ",", "name", "=", "'input_transpose'", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "scope_name", ")", "as", "scope", ":", "\n", "# Discriminator would be reused in CycleGAN", "\n", "        ", "if", "reuse", ":", "\n", "            ", "scope", ".", "reuse_variables", "(", ")", "\n", "", "else", ":", "\n", "            ", "assert", "scope", ".", "reuse", "is", "False", "\n", "\n", "", "h1", "=", "conv1d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "128", ",", "kernel_size", "=", "15", ",", "strides", "=", "1", ",", "activation", "=", "None", ",", "name", "=", "'h1_conv'", ")", "\n", "h1_gates", "=", "conv1d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "128", ",", "kernel_size", "=", "15", ",", "strides", "=", "1", ",", "activation", "=", "None", ",", "name", "=", "'h1_conv_gates'", ")", "\n", "h1_glu", "=", "gated_linear_layer", "(", "inputs", "=", "h1", ",", "gates", "=", "h1_gates", ",", "name", "=", "'h1_glu'", ")", "\n", "\n", "# Downsample", "\n", "d1", "=", "downsample1d_block", "(", "inputs", "=", "h1_glu", ",", "filters", "=", "256", ",", "kernel_size", "=", "5", ",", "strides", "=", "2", ",", "name_prefix", "=", "'downsample1d_block1_'", ")", "\n", "d2", "=", "downsample1d_block", "(", "inputs", "=", "d1", ",", "filters", "=", "512", ",", "kernel_size", "=", "5", ",", "strides", "=", "2", ",", "name_prefix", "=", "'downsample1d_block2_'", ")", "\n", "\n", "# Residual blocks", "\n", "r1", "=", "residual1d_block", "(", "inputs", "=", "d2", ",", "filters", "=", "1024", ",", "kernel_size", "=", "3", ",", "strides", "=", "1", ",", "name_prefix", "=", "'residual1d_block1_'", ")", "\n", "r2", "=", "residual1d_block", "(", "inputs", "=", "r1", ",", "filters", "=", "1024", ",", "kernel_size", "=", "3", ",", "strides", "=", "1", ",", "name_prefix", "=", "'residual1d_block2_'", ")", "\n", "r3", "=", "residual1d_block", "(", "inputs", "=", "r2", ",", "filters", "=", "1024", ",", "kernel_size", "=", "3", ",", "strides", "=", "1", ",", "name_prefix", "=", "'residual1d_block3_'", ")", "\n", "r4", "=", "residual1d_block", "(", "inputs", "=", "r3", ",", "filters", "=", "1024", ",", "kernel_size", "=", "3", ",", "strides", "=", "1", ",", "name_prefix", "=", "'residual1d_block4_'", ")", "\n", "r5", "=", "residual1d_block", "(", "inputs", "=", "r4", ",", "filters", "=", "1024", ",", "kernel_size", "=", "3", ",", "strides", "=", "1", ",", "name_prefix", "=", "'residual1d_block5_'", ")", "\n", "r6", "=", "residual1d_block", "(", "inputs", "=", "r5", ",", "filters", "=", "1024", ",", "kernel_size", "=", "3", ",", "strides", "=", "1", ",", "name_prefix", "=", "'residual1d_block6_'", ")", "\n", "\n", "# Upsample", "\n", "u1", "=", "upsample1d_block", "(", "inputs", "=", "r6", ",", "filters", "=", "1024", ",", "kernel_size", "=", "5", ",", "strides", "=", "1", ",", "shuffle_size", "=", "2", ",", "name_prefix", "=", "'upsample1d_block1_'", ")", "\n", "u2", "=", "upsample1d_block", "(", "inputs", "=", "u1", ",", "filters", "=", "512", ",", "kernel_size", "=", "5", ",", "strides", "=", "1", ",", "shuffle_size", "=", "2", ",", "name_prefix", "=", "'upsample1d_block2_'", ")", "\n", "\n", "# Output", "\n", "o1", "=", "conv1d_layer", "(", "inputs", "=", "u2", ",", "filters", "=", "10", ",", "kernel_size", "=", "15", ",", "strides", "=", "1", ",", "activation", "=", "None", ",", "name", "=", "'o1_conv'", ")", "\n", "o2", "=", "tf", ".", "transpose", "(", "o1", ",", "perm", "=", "[", "0", ",", "2", ",", "1", "]", ",", "name", "=", "'output_transpose'", ")", "\n", "\n", "", "return", "o2", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_f0.discriminator": [[188, 214], ["tensorflow.expand_dims", "tensorflow.variable_scope", "module_f0.conv2d_layer", "module_f0.conv2d_layer", "module_f0.gated_linear_layer", "module_f0.downsample2d_block", "module_f0.downsample2d_block", "module_f0.downsample2d_block", "tensorflow.layers.dense", "scope.reuse_variables"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv2d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv2d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.gated_linear_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.downsample2d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.downsample2d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.downsample2d_block"], ["", "def", "discriminator", "(", "inputs", ",", "reuse", "=", "False", ",", "scope_name", "=", "'discriminator'", ")", ":", "\n", "\n", "# inputs has shape [batch_size, num_features, time]", "\n", "# we need to add channel for 2D convolution [batch_size, num_features, time, 1]", "\n", "    ", "inputs", "=", "tf", ".", "expand_dims", "(", "inputs", ",", "-", "1", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "scope_name", ")", "as", "scope", ":", "\n", "# Discriminator would be reused in CycleGAN", "\n", "        ", "if", "reuse", ":", "\n", "            ", "scope", ".", "reuse_variables", "(", ")", "\n", "", "else", ":", "\n", "            ", "assert", "scope", ".", "reuse", "is", "False", "\n", "\n", "", "h1", "=", "conv2d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "128", ",", "kernel_size", "=", "[", "3", ",", "3", "]", ",", "strides", "=", "[", "1", ",", "2", "]", ",", "activation", "=", "None", ",", "name", "=", "'h1_conv'", ")", "\n", "h1_gates", "=", "conv2d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "128", ",", "kernel_size", "=", "[", "3", ",", "3", "]", ",", "strides", "=", "[", "1", ",", "2", "]", ",", "activation", "=", "None", ",", "name", "=", "'h1_conv_gates'", ")", "\n", "h1_glu", "=", "gated_linear_layer", "(", "inputs", "=", "h1", ",", "gates", "=", "h1_gates", ",", "name", "=", "'h1_glu'", ")", "\n", "\n", "# Downsample", "\n", "d1", "=", "downsample2d_block", "(", "inputs", "=", "h1_glu", ",", "filters", "=", "256", ",", "kernel_size", "=", "[", "3", ",", "3", "]", ",", "strides", "=", "[", "2", ",", "2", "]", ",", "name_prefix", "=", "'downsample2d_block1_'", ")", "\n", "d2", "=", "downsample2d_block", "(", "inputs", "=", "d1", ",", "filters", "=", "512", ",", "kernel_size", "=", "[", "3", ",", "3", "]", ",", "strides", "=", "[", "2", ",", "2", "]", ",", "name_prefix", "=", "'downsample2d_block2_'", ")", "\n", "d3", "=", "downsample2d_block", "(", "inputs", "=", "d2", ",", "filters", "=", "1024", ",", "kernel_size", "=", "[", "6", ",", "3", "]", ",", "strides", "=", "[", "1", ",", "2", "]", ",", "name_prefix", "=", "'downsample2d_block3_'", ")", "\n", "\n", "# Output", "\n", "o1", "=", "tf", ".", "layers", ".", "dense", "(", "inputs", "=", "d3", ",", "units", "=", "1", ",", "activation", "=", "tf", ".", "nn", ".", "sigmoid", ")", "\n", "\n", "return", "o1", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.convert_separate.conversion": [[13, 119], ["model_f0.CycleGAN", "model_f0.CycleGAN.load", "model_mceps.CycleGAN", "model_mceps.CycleGAN.load", "numpy.load", "numpy.load", "os.listdir", "os.path.join", "os.path.join", "os.path.exists", "os.makedirs", "os.path.join", "librosa.load", "preprocess.wav_padding", "preprocess.world_decompose", "preprocess.world_encode_spectral_envelop", "utils.get_cont_lf0", "numpy.ascontiguousarray", "preprocess.world_decode_spectral_envelop", "preprocess.world_speech_synthesis", "librosa.output.write_wav", "os.path.join", "os.path.join", "utils.get_lf0_cwt", "utils.norm_scale", "utils.denormalize", "utils.inverse_cwt", "numpy.ascontiguousarray", "utils.get_lf0_cwt", "numpy.vstack", "utils.inverse_cwt", "sklearn.preprocessing.scale", "numpy.ascontiguousarray", "os.path.join", "model_mceps.CycleGAN.test", "model_f0.CycleGAN.test", "numpy.squeeze", "numpy.exp", "model_f0.CycleGAN.test", "numpy.squeeze", "numpy.exp", "os.path.basename", "numpy.array", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.load", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.load", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.load", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.load", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.load", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.wav_padding", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_decompose", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_encode_spectral_envelop", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.get_cont_lf0", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_decode_spectral_envelop", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_speech_synthesis", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.get_lf0_cwt", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.norm_scale", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.denormalize", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.inverse_cwt", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.get_lf0_cwt", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.inverse_cwt", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.test", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.test", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.test"], ["def", "conversion", "(", "model_f0_dir", ",", "model_f0_name", ",", "model_mceps_dir", ",", "model_mceps_name", ",", "data_dir", ",", "conversion_direction", ",", "output_dir", ")", ":", "\n", "\n", "    ", "num_mceps", "=", "24", "\n", "num_features", "=", "34", "\n", "sampling_rate", "=", "16000", "\n", "frame_period", "=", "5.0", "\n", "\n", "#    model = CycleGAN(num_features = num_features, mode = 'test')", "\n", "#    model.load(filepath = os.path.join(model_dir, model_name))", "\n", "\n", "#  import F0 model:", "\n", "model_f0", "=", "CycleGAN_f0", "(", "num_features", "=", "10", ",", "mode", "=", "'test'", ")", "\n", "model_f0", ".", "load", "(", "filepath", "=", "os", ".", "path", ".", "join", "(", "model_f0_dir", ",", "model_f0_name", ")", ")", "\n", "#  import mceps model:", "\n", "model_mceps", "=", "CycleGAN_mceps", "(", "num_features", "=", "24", ",", "mode", "=", "'test'", ")", "\n", "model_mceps", ".", "load", "(", "filepath", "=", "os", ".", "path", ".", "join", "(", "model_mceps_dir", ",", "model_mceps_name", ")", ")", "\n", "\n", "mcep_normalization_params", "=", "np", ".", "load", "(", "os", ".", "path", ".", "join", "(", "model_mceps_dir", ",", "'mcep_normalization.npz'", ")", ")", "\n", "mcep_mean_A", "=", "mcep_normalization_params", "[", "'mean_A'", "]", "\n", "mcep_std_A", "=", "mcep_normalization_params", "[", "'std_A'", "]", "\n", "mcep_mean_B", "=", "mcep_normalization_params", "[", "'mean_B'", "]", "\n", "mcep_std_B", "=", "mcep_normalization_params", "[", "'std_B'", "]", "\n", "\n", "logf0s_normalization_params", "=", "np", ".", "load", "(", "os", ".", "path", ".", "join", "(", "model_f0_dir", ",", "'logf0s_normalization.npz'", ")", ")", "\n", "logf0s_mean_A", "=", "logf0s_normalization_params", "[", "'mean_A'", "]", "\n", "logf0s_std_A", "=", "logf0s_normalization_params", "[", "'std_A'", "]", "\n", "logf0s_mean_B", "=", "logf0s_normalization_params", "[", "'mean_B'", "]", "\n", "logf0s_std_B", "=", "logf0s_normalization_params", "[", "'std_B'", "]", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "output_dir", ")", "\n", "\n", "", "for", "file", "in", "os", ".", "listdir", "(", "data_dir", ")", ":", "\n", "\n", "        ", "filepath", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "file", ")", "\n", "wav", ",", "_", "=", "librosa", ".", "load", "(", "filepath", ",", "sr", "=", "sampling_rate", ",", "mono", "=", "True", ")", "\n", "wav", "=", "wav_padding", "(", "wav", "=", "wav", ",", "sr", "=", "sampling_rate", ",", "frame_period", "=", "frame_period", ",", "multiple", "=", "4", ")", "\n", "f0", ",", "timeaxis", ",", "sp", ",", "ap", "=", "world_decompose", "(", "wav", "=", "wav", ",", "fs", "=", "sampling_rate", ",", "frame_period", "=", "frame_period", ")", "\n", "coded_sp", "=", "world_encode_spectral_envelop", "(", "sp", "=", "sp", ",", "fs", "=", "sampling_rate", ",", "dim", "=", "num_mceps", ")", "\n", "coded_sp_transposed", "=", "coded_sp", ".", "T", "\n", "# np.save('./f0',f0)", "\n", "uv", ",", "cont_lf0_lpf", "=", "get_cont_lf0", "(", "f0", ")", "\n", "\n", "if", "conversion_direction", "==", "'A2B'", ":", "\n", "#f0_converted = pitch_conversion(f0 = f0, mean_log_src = logf0s_mean_A, std_log_src = logf0s_std_A, mean_log_target = logf0s_mean_B, std_log_target = logf0s_std_B)", "\n", "#f0_converted = f0", "\n", "\n", "            ", "cont_lf0_lpf_norm", "=", "(", "cont_lf0_lpf", "-", "logf0s_mean_A", ")", "/", "logf0s_std_A", "\n", "Wavelet_lf0", ",", "scales", "=", "get_lf0_cwt", "(", "cont_lf0_lpf_norm", ")", "#[470,10]", "\n", "#np.save('./Wavelet_lf0',Wavelet_lf0)", "\n", "Wavelet_lf0_norm", ",", "mean", ",", "std", "=", "norm_scale", "(", "Wavelet_lf0", ")", "#[470,10],[1,10],[1,10]", "\n", "lf0_cwt_norm", "=", "Wavelet_lf0_norm", ".", "T", "#[10,470]", "\n", "\n", "coded_sp_norm", "=", "(", "coded_sp_transposed", "-", "mcep_mean_A", ")", "/", "mcep_std_A", "#[24,470]", "\n", "\n", "#    feats_norm = np.vstack((coded_sp_norm,lf0_cwt_norm))#[34,470]", "\n", "#    feats_converted_norm = model.test(inputs = np.array([feats_norm]), direction = conversion_direction)[0]", "\n", "\n", "# test mceps", "\n", "coded_sp_converted_norm", "=", "model_mceps", ".", "test", "(", "inputs", "=", "np", ".", "array", "(", "[", "coded_sp_norm", "]", ")", ",", "direction", "=", "conversion_direction", ")", "[", "0", "]", "\n", "# test f0:", "\n", "lf0", "=", "model_f0", ".", "test", "(", "inputs", "=", "np", ".", "array", "(", "[", "lf0_cwt_norm", "]", ")", ",", "direction", "=", "conversion_direction", ")", "[", "0", "]", "\n", "#coded_sp_converted_norm = model.test(inputs = np.array([feats_norm]), direction = conversion_direction)[0]", "\n", "\n", "#coded_sp_converted_norm = feats_converted_norm[:24]", "\n", "coded_sp_converted", "=", "coded_sp_converted_norm", "*", "mcep_std_B", "+", "mcep_mean_B", "#mceps", "\n", "\n", "#lf0 = feats_converted_norm[24:].T #[470,10]", "\n", "\n", "lf0_cwt_denormalize", "=", "denormalize", "(", "lf0", ".", "T", ",", "mean", ",", "std", ")", "#[470,10]", "\n", "#np.save('./lf0_denorm',lf0_cwt_denormalize)", "\n", "lf0_rec", "=", "inverse_cwt", "(", "lf0_cwt_denormalize", ",", "scales", ")", "#[470,1]", "\n", "#lf0_rec_norm = preprocessing.scale(lf0_rec)", "\n", "lf0_converted", "=", "lf0_rec", "*", "logf0s_std_B", "+", "logf0s_mean_B", "\n", "f0_converted", "=", "np", ".", "squeeze", "(", "uv", ")", "*", "np", ".", "exp", "(", "lf0_converted", ")", "\n", "f0_converted", "=", "np", ".", "ascontiguousarray", "(", "f0_converted", ")", "\n", "#np.save('./f0_converted',f0_converted)", "\n", "\n", "", "else", ":", "\n", "#f0_converted = pitch_conversion(f0 = f0, mean_log_src = logf0s_mean_B, std_log_src = logf0s_std_B, mean_log_target = logf0s_mean_A, std_log_target = logf0s_std_A)", "\n", "#f0_converted = f0", "\n", "            ", "cont_lf0_lpf_norm", "=", "(", "cont_lf0_lpf", "-", "logf0s_mean_B", ")", "/", "logf0s_std_B", "\n", "Wavelet_lf0", ",", "scales", "=", "get_lf0_cwt", "(", "cont_lf0_lpf_norm", ")", "\n", "lf0_cwt_norm", "=", "Wavelet_lf0", ".", "T", "#[10,470]", "\n", "coded_sp_norm", "=", "(", "coded_sp_transposed", "-", "mcep_mean_B", ")", "/", "mcep_std_B", "\n", "feats_norm", "=", "np", ".", "vstack", "(", "(", "coded_sp_norm", ",", "lf0_cwt_norm", ")", ")", "#[34,470]", "\n", "feats_converted_norm", "=", "model_f0", ".", "test", "(", "inputs", "=", "np", ".", "array", "(", "[", "feats_norm", "]", ")", ",", "direction", "=", "conversion_direction", ")", "[", "0", "]", "\n", "\n", "#coded_sp_converted_norm = model.test(inputs = np.array([feats_norm]), direction = conversion_direction)[0]", "\n", "coded_sp_converted_norm", "=", "feats_converted_norm", "[", ":", "24", "]", "\n", "coded_sp_converted", "=", "coded_sp_converted_norm", "*", "mcep_std_A", "+", "mcep_mean_A", "\n", "lf0_rec", "=", "inverse_cwt", "(", "feats_norm", "[", "24", ":", "]", ".", "T", ",", "scales", ")", "#[470,10]", "\n", "lf0_rec_norm", "=", "preprocessing", ".", "scale", "(", "lf0_rec", ")", "\n", "lf0_converted", "=", "lf0_rec_norm", "*", "logf0s_std_A", "+", "logf0s_mean_A", "\n", "f0_converted", "=", "np", ".", "squeeze", "(", "uv", ")", "*", "np", ".", "exp", "(", "lf0_converted", ")", "\n", "f0_converted", "=", "np", ".", "ascontiguousarray", "(", "f0_converted", ")", "\n", "\n", "#coded_sp_norm = (coded_sp_transposed - mcep_mean_B) / mcep_std_B", "\n", "#coded_sp_converted_norm = model.test(inputs = np.array([coded_sp_norm]), direction = conversion_direction)[0]", "\n", "#coded_sp_converted = coded_sp_converted_norm * mcep_std_A + mcep_mean_A", "\n", "\n", "", "coded_sp_converted", "=", "coded_sp_converted", ".", "T", "#[470,24]", "\n", "coded_sp_converted", "=", "np", ".", "ascontiguousarray", "(", "coded_sp_converted", ")", "\n", "decoded_sp_converted", "=", "world_decode_spectral_envelop", "(", "coded_sp", "=", "coded_sp_converted", ",", "fs", "=", "sampling_rate", ")", "\n", "wav_transformed", "=", "world_speech_synthesis", "(", "f0", "=", "f0_converted", ",", "decoded_sp", "=", "decoded_sp_converted", ",", "ap", "=", "ap", ",", "fs", "=", "sampling_rate", ",", "frame_period", "=", "frame_period", ")", "\n", "librosa", ".", "output", ".", "write_wav", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "os", ".", "path", ".", "basename", "(", "file", ")", ")", ",", "wav_transformed", ",", "sampling_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module.gated_linear_layer": [[3, 8], ["tensorflow.multiply", "tensorflow.sigmoid"], "function", ["None"], ["def", "gated_linear_layer", "(", "inputs", ",", "gates", ",", "name", "=", "None", ")", ":", "\n", "\n", "    ", "activation", "=", "tf", ".", "multiply", "(", "x", "=", "inputs", ",", "y", "=", "tf", ".", "sigmoid", "(", "gates", ")", ",", "name", "=", "name", ")", "\n", "\n", "return", "activation", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module.instance_norm_layer": [[9, 21], ["tensorflow.contrib.layers.instance_norm"], "function", ["None"], ["", "def", "instance_norm_layer", "(", "\n", "inputs", ",", "\n", "epsilon", "=", "1e-06", ",", "\n", "activation_fn", "=", "None", ",", "\n", "name", "=", "None", ")", ":", "\n", "\n", "    ", "instance_norm_layer", "=", "tf", ".", "contrib", ".", "layers", ".", "instance_norm", "(", "\n", "inputs", "=", "inputs", ",", "\n", "epsilon", "=", "epsilon", ",", "\n", "activation_fn", "=", "activation_fn", ")", "\n", "\n", "return", "instance_norm_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module.conv1d_layer": [[22, 43], ["tensorflow.layers.conv1d"], "function", ["None"], ["", "def", "conv1d_layer", "(", "\n", "inputs", ",", "\n", "filters", ",", "\n", "kernel_size", ",", "\n", "strides", "=", "1", ",", "\n", "padding", "=", "'same'", ",", "\n", "activation", "=", "None", ",", "\n", "kernel_initializer", "=", "None", ",", "\n", "name", "=", "None", ")", ":", "\n", "\n", "    ", "conv_layer", "=", "tf", ".", "layers", ".", "conv1d", "(", "\n", "inputs", "=", "inputs", ",", "\n", "filters", "=", "filters", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "strides", "=", "strides", ",", "\n", "padding", "=", "padding", ",", "\n", "activation", "=", "activation", ",", "\n", "kernel_initializer", "=", "kernel_initializer", ",", "\n", "name", "=", "name", ")", "\n", "\n", "return", "conv_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module.conv2d_layer": [[44, 65], ["tensorflow.layers.conv2d"], "function", ["None"], ["", "def", "conv2d_layer", "(", "\n", "inputs", ",", "\n", "filters", ",", "\n", "kernel_size", ",", "\n", "strides", ",", "\n", "padding", "=", "'same'", ",", "\n", "activation", "=", "None", ",", "\n", "kernel_initializer", "=", "None", ",", "\n", "name", "=", "None", ")", ":", "\n", "\n", "    ", "conv_layer", "=", "tf", ".", "layers", ".", "conv2d", "(", "\n", "inputs", "=", "inputs", ",", "\n", "filters", "=", "filters", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "strides", "=", "strides", ",", "\n", "padding", "=", "padding", ",", "\n", "activation", "=", "activation", ",", "\n", "kernel_initializer", "=", "kernel_initializer", ",", "\n", "name", "=", "name", ")", "\n", "\n", "return", "conv_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module.residual1d_block": [[66, 84], ["module.conv1d_layer", "module.instance_norm_layer", "module.conv1d_layer", "module.instance_norm_layer", "module.gated_linear_layer", "module.conv1d_layer", "module.instance_norm_layer"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.gated_linear_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer"], ["", "def", "residual1d_block", "(", "\n", "inputs", ",", "\n", "filters", "=", "1024", ",", "\n", "kernel_size", "=", "3", ",", "\n", "strides", "=", "1", ",", "\n", "name_prefix", "=", "'residule_block_'", ")", ":", "\n", "\n", "    ", "h1", "=", "conv1d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "filters", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_conv'", ")", "\n", "h1_norm", "=", "instance_norm_layer", "(", "inputs", "=", "h1", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_norm'", ")", "\n", "h1_gates", "=", "conv1d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "filters", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_gates'", ")", "\n", "h1_norm_gates", "=", "instance_norm_layer", "(", "inputs", "=", "h1_gates", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_norm_gates'", ")", "\n", "h1_glu", "=", "gated_linear_layer", "(", "inputs", "=", "h1_norm", ",", "gates", "=", "h1_norm_gates", ",", "name", "=", "name_prefix", "+", "'h1_glu'", ")", "\n", "h2", "=", "conv1d_layer", "(", "inputs", "=", "h1_glu", ",", "filters", "=", "filters", "//", "2", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h2_conv'", ")", "\n", "h2_norm", "=", "instance_norm_layer", "(", "inputs", "=", "h2", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h2_norm'", ")", "\n", "\n", "h3", "=", "inputs", "+", "h2_norm", "\n", "\n", "return", "h3", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module.downsample1d_block": [[85, 99], ["module.conv1d_layer", "module.instance_norm_layer", "module.conv1d_layer", "module.instance_norm_layer", "module.gated_linear_layer"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.gated_linear_layer"], ["", "def", "downsample1d_block", "(", "\n", "inputs", ",", "\n", "filters", ",", "\n", "kernel_size", ",", "\n", "strides", ",", "\n", "name_prefix", "=", "'downsample1d_block_'", ")", ":", "\n", "\n", "    ", "h1", "=", "conv1d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "filters", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_conv'", ")", "\n", "h1_norm", "=", "instance_norm_layer", "(", "inputs", "=", "h1", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_norm'", ")", "\n", "h1_gates", "=", "conv1d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "filters", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_gates'", ")", "\n", "h1_norm_gates", "=", "instance_norm_layer", "(", "inputs", "=", "h1_gates", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_norm_gates'", ")", "\n", "h1_glu", "=", "gated_linear_layer", "(", "inputs", "=", "h1_norm", ",", "gates", "=", "h1_norm_gates", ",", "name", "=", "name_prefix", "+", "'h1_glu'", ")", "\n", "\n", "return", "h1_glu", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module.downsample2d_block": [[100, 114], ["module.conv2d_layer", "module.instance_norm_layer", "module.conv2d_layer", "module.instance_norm_layer", "module.gated_linear_layer"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv2d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv2d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.gated_linear_layer"], ["", "def", "downsample2d_block", "(", "\n", "inputs", ",", "\n", "filters", ",", "\n", "kernel_size", ",", "\n", "strides", ",", "\n", "name_prefix", "=", "'downsample2d_block_'", ")", ":", "\n", "\n", "    ", "h1", "=", "conv2d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "filters", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_conv'", ")", "\n", "h1_norm", "=", "instance_norm_layer", "(", "inputs", "=", "h1", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_norm'", ")", "\n", "h1_gates", "=", "conv2d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "filters", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_gates'", ")", "\n", "h1_norm_gates", "=", "instance_norm_layer", "(", "inputs", "=", "h1_gates", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_norm_gates'", ")", "\n", "h1_glu", "=", "gated_linear_layer", "(", "inputs", "=", "h1_norm", ",", "gates", "=", "h1_norm_gates", ",", "name", "=", "name_prefix", "+", "'h1_glu'", ")", "\n", "\n", "return", "h1_glu", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module.upsample1d_block": [[115, 134], ["module.conv1d_layer", "module.pixel_shuffler", "module.instance_norm_layer", "module.conv1d_layer", "module.pixel_shuffler", "module.instance_norm_layer", "module.gated_linear_layer"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.pixel_shuffler", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.pixel_shuffler", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.gated_linear_layer"], ["", "def", "upsample1d_block", "(", "\n", "inputs", ",", "\n", "filters", ",", "\n", "kernel_size", ",", "\n", "strides", ",", "\n", "shuffle_size", "=", "2", ",", "\n", "name_prefix", "=", "'upsample1d_block_'", ")", ":", "\n", "\n", "    ", "h1", "=", "conv1d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "filters", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_conv'", ")", "\n", "h1_shuffle", "=", "pixel_shuffler", "(", "inputs", "=", "h1", ",", "shuffle_size", "=", "shuffle_size", ",", "name", "=", "name_prefix", "+", "'h1_shuffle'", ")", "\n", "h1_norm", "=", "instance_norm_layer", "(", "inputs", "=", "h1_shuffle", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_norm'", ")", "\n", "\n", "h1_gates", "=", "conv1d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "filters", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_gates'", ")", "\n", "h1_shuffle_gates", "=", "pixel_shuffler", "(", "inputs", "=", "h1_gates", ",", "shuffle_size", "=", "shuffle_size", ",", "name", "=", "name_prefix", "+", "'h1_shuffle_gates'", ")", "\n", "h1_norm_gates", "=", "instance_norm_layer", "(", "inputs", "=", "h1_shuffle_gates", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_norm_gates'", ")", "\n", "\n", "h1_glu", "=", "gated_linear_layer", "(", "inputs", "=", "h1_norm", ",", "gates", "=", "h1_norm_gates", ",", "name", "=", "name_prefix", "+", "'h1_glu'", ")", "\n", "\n", "return", "h1_glu", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module.pixel_shuffler": [[135, 147], ["tensorflow.reshape", "tensorflow.shape", "tensorflow.shape", "inputs.get_shape().as_list", "inputs.get_shape"], "function", ["None"], ["", "def", "pixel_shuffler", "(", "inputs", ",", "shuffle_size", "=", "2", ",", "name", "=", "None", ")", ":", "\n", "\n", "    ", "n", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "0", "]", "\n", "w", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "1", "]", "\n", "c", "=", "inputs", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "2", "]", "\n", "\n", "oc", "=", "c", "//", "shuffle_size", "\n", "ow", "=", "w", "*", "shuffle_size", "\n", "\n", "outputs", "=", "tf", ".", "reshape", "(", "tensor", "=", "inputs", ",", "shape", "=", "[", "n", ",", "ow", ",", "oc", "]", ",", "name", "=", "name", ")", "\n", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module.generator_gatedcnn": [[148, 186], ["tensorflow.transpose", "tensorflow.variable_scope", "module.conv1d_layer", "module.conv1d_layer", "module.gated_linear_layer", "module.downsample1d_block", "module.downsample1d_block", "module.residual1d_block", "module.residual1d_block", "module.residual1d_block", "module.residual1d_block", "module.residual1d_block", "module.residual1d_block", "module.upsample1d_block", "module.upsample1d_block", "module.conv1d_layer", "tensorflow.transpose", "scope.reuse_variables"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.gated_linear_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.downsample1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.downsample1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.residual1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.residual1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.residual1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.residual1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.residual1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.residual1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.upsample1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.upsample1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer"], ["", "def", "generator_gatedcnn", "(", "inputs", ",", "reuse", "=", "False", ",", "scope_name", "=", "'generator_gatedcnn'", ")", ":", "\n", "\n", "# inputs has shape [batch_size, num_features, time]", "\n", "# we need to convert it to [batch_size, time, num_features] for 1D convolution", "\n", "    ", "inputs", "=", "tf", ".", "transpose", "(", "inputs", ",", "perm", "=", "[", "0", ",", "2", ",", "1", "]", ",", "name", "=", "'input_transpose'", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "scope_name", ")", "as", "scope", ":", "\n", "# Discriminator would be reused in CycleGAN", "\n", "        ", "if", "reuse", ":", "\n", "            ", "scope", ".", "reuse_variables", "(", ")", "\n", "", "else", ":", "\n", "            ", "assert", "scope", ".", "reuse", "is", "False", "\n", "\n", "", "h1", "=", "conv1d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "128", ",", "kernel_size", "=", "15", ",", "strides", "=", "1", ",", "activation", "=", "None", ",", "name", "=", "'h1_conv'", ")", "\n", "h1_gates", "=", "conv1d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "128", ",", "kernel_size", "=", "15", ",", "strides", "=", "1", ",", "activation", "=", "None", ",", "name", "=", "'h1_conv_gates'", ")", "\n", "h1_glu", "=", "gated_linear_layer", "(", "inputs", "=", "h1", ",", "gates", "=", "h1_gates", ",", "name", "=", "'h1_glu'", ")", "\n", "\n", "# Downsample", "\n", "d1", "=", "downsample1d_block", "(", "inputs", "=", "h1_glu", ",", "filters", "=", "256", ",", "kernel_size", "=", "5", ",", "strides", "=", "2", ",", "name_prefix", "=", "'downsample1d_block1_'", ")", "\n", "d2", "=", "downsample1d_block", "(", "inputs", "=", "d1", ",", "filters", "=", "512", ",", "kernel_size", "=", "5", ",", "strides", "=", "2", ",", "name_prefix", "=", "'downsample1d_block2_'", ")", "\n", "\n", "# Residual blocks", "\n", "r1", "=", "residual1d_block", "(", "inputs", "=", "d2", ",", "filters", "=", "1024", ",", "kernel_size", "=", "3", ",", "strides", "=", "1", ",", "name_prefix", "=", "'residual1d_block1_'", ")", "\n", "r2", "=", "residual1d_block", "(", "inputs", "=", "r1", ",", "filters", "=", "1024", ",", "kernel_size", "=", "3", ",", "strides", "=", "1", ",", "name_prefix", "=", "'residual1d_block2_'", ")", "\n", "r3", "=", "residual1d_block", "(", "inputs", "=", "r2", ",", "filters", "=", "1024", ",", "kernel_size", "=", "3", ",", "strides", "=", "1", ",", "name_prefix", "=", "'residual1d_block3_'", ")", "\n", "r4", "=", "residual1d_block", "(", "inputs", "=", "r3", ",", "filters", "=", "1024", ",", "kernel_size", "=", "3", ",", "strides", "=", "1", ",", "name_prefix", "=", "'residual1d_block4_'", ")", "\n", "r5", "=", "residual1d_block", "(", "inputs", "=", "r4", ",", "filters", "=", "1024", ",", "kernel_size", "=", "3", ",", "strides", "=", "1", ",", "name_prefix", "=", "'residual1d_block5_'", ")", "\n", "r6", "=", "residual1d_block", "(", "inputs", "=", "r5", ",", "filters", "=", "1024", ",", "kernel_size", "=", "3", ",", "strides", "=", "1", ",", "name_prefix", "=", "'residual1d_block6_'", ")", "\n", "\n", "# Upsample", "\n", "u1", "=", "upsample1d_block", "(", "inputs", "=", "r6", ",", "filters", "=", "1024", ",", "kernel_size", "=", "5", ",", "strides", "=", "1", ",", "shuffle_size", "=", "2", ",", "name_prefix", "=", "'upsample1d_block1_'", ")", "\n", "u2", "=", "upsample1d_block", "(", "inputs", "=", "u1", ",", "filters", "=", "512", ",", "kernel_size", "=", "5", ",", "strides", "=", "1", ",", "shuffle_size", "=", "2", ",", "name_prefix", "=", "'upsample1d_block2_'", ")", "\n", "\n", "# Output", "\n", "o1", "=", "conv1d_layer", "(", "inputs", "=", "u2", ",", "filters", "=", "24", ",", "kernel_size", "=", "15", ",", "strides", "=", "1", ",", "activation", "=", "None", ",", "name", "=", "'o1_conv'", ")", "\n", "o2", "=", "tf", ".", "transpose", "(", "o1", ",", "perm", "=", "[", "0", ",", "2", ",", "1", "]", ",", "name", "=", "'output_transpose'", ")", "\n", "\n", "", "return", "o2", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module.discriminator": [[188, 214], ["tensorflow.expand_dims", "tensorflow.variable_scope", "module.conv2d_layer", "module.conv2d_layer", "module.gated_linear_layer", "module.downsample2d_block", "module.downsample2d_block", "module.downsample2d_block", "tensorflow.layers.dense", "scope.reuse_variables"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv2d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv2d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.gated_linear_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.downsample2d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.downsample2d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.downsample2d_block"], ["", "def", "discriminator", "(", "inputs", ",", "reuse", "=", "False", ",", "scope_name", "=", "'discriminator'", ")", ":", "\n", "\n", "# inputs has shape [batch_size, num_features, time]", "\n", "# we need to add channel for 2D convolution [batch_size, num_features, time, 1]", "\n", "    ", "inputs", "=", "tf", ".", "expand_dims", "(", "inputs", ",", "-", "1", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "scope_name", ")", "as", "scope", ":", "\n", "# Discriminator would be reused in CycleGAN", "\n", "        ", "if", "reuse", ":", "\n", "            ", "scope", ".", "reuse_variables", "(", ")", "\n", "", "else", ":", "\n", "            ", "assert", "scope", ".", "reuse", "is", "False", "\n", "\n", "", "h1", "=", "conv2d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "128", ",", "kernel_size", "=", "[", "3", ",", "3", "]", ",", "strides", "=", "[", "1", ",", "2", "]", ",", "activation", "=", "None", ",", "name", "=", "'h1_conv'", ")", "\n", "h1_gates", "=", "conv2d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "128", ",", "kernel_size", "=", "[", "3", ",", "3", "]", ",", "strides", "=", "[", "1", ",", "2", "]", ",", "activation", "=", "None", ",", "name", "=", "'h1_conv_gates'", ")", "\n", "h1_glu", "=", "gated_linear_layer", "(", "inputs", "=", "h1", ",", "gates", "=", "h1_gates", ",", "name", "=", "'h1_glu'", ")", "\n", "\n", "# Downsample", "\n", "d1", "=", "downsample2d_block", "(", "inputs", "=", "h1_glu", ",", "filters", "=", "256", ",", "kernel_size", "=", "[", "3", ",", "3", "]", ",", "strides", "=", "[", "2", ",", "2", "]", ",", "name_prefix", "=", "'downsample2d_block1_'", ")", "\n", "d2", "=", "downsample2d_block", "(", "inputs", "=", "d1", ",", "filters", "=", "512", ",", "kernel_size", "=", "[", "3", ",", "3", "]", ",", "strides", "=", "[", "2", ",", "2", "]", ",", "name_prefix", "=", "'downsample2d_block2_'", ")", "\n", "d3", "=", "downsample2d_block", "(", "inputs", "=", "d2", ",", "filters", "=", "1024", ",", "kernel_size", "=", "[", "6", ",", "3", "]", ",", "strides", "=", "[", "1", ",", "2", "]", ",", "name_prefix", "=", "'downsample2d_block3_'", ")", "\n", "\n", "# Output", "\n", "o1", "=", "tf", ".", "layers", ".", "dense", "(", "inputs", "=", "d3", ",", "units", "=", "1", ",", "activation", "=", "tf", ".", "nn", ".", "sigmoid", ")", "\n", "\n", "return", "o1", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.train_f0.train": [[12, 146], ["numpy.random.seed", "print", "time.time", "preprocess.load_wavs", "preprocess.load_wavs", "preprocess.world_encode_data", "preprocess.world_encode_data", "preprocess.logf0_statistics", "preprocess.logf0_statistics", "utils.get_lf0_cwt_norm", "utils.get_lf0_cwt_norm", "print", "print", "print", "print", "preprocess.transpose_in_list", "preprocess.transpose_in_list", "preprocess.transpose_in_list", "preprocess.transpose_in_list", "preprocess.coded_sps_normalization_fit_transoform", "preprocess.coded_sps_normalization_fit_transoform", "numpy.savez", "numpy.savez", "time.time", "print", "print", "model_f0.CycleGAN", "range", "os.path.exists", "os.makedirs", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "print", "time.time", "list", "list", "preprocess.sample_train_data", "range", "model_f0.CycleGAN.save", "time.time", "print", "os.path.exists", "os.makedirs", "os.path.exists", "os.makedirs", "list.append", "list.append", "model_f0.CycleGAN.train", "max", "max", "print"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.load_wavs", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.load_wavs", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_encode_data", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_encode_data", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.logf0_statistics", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.logf0_statistics", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.get_lf0_cwt_norm", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.get_lf0_cwt_norm", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.transpose_in_list", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.transpose_in_list", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.transpose_in_list", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.transpose_in_list", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.coded_sps_normalization_fit_transoform", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.coded_sps_normalization_fit_transoform", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.sample_train_data", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.save", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.train"], ["def", "train", "(", "train_A_dir", ",", "train_B_dir", ",", "model_dir", ",", "model_name", ",", "random_seed", ",", "validation_A_dir", ",", "validation_B_dir", ",", "output_dir", ",", "tensorboard_log_dir", ")", ":", "\n", "# 10-scale F0 + 24 MCEPs:", "\n", "    ", "np", ".", "random", ".", "seed", "(", "random_seed", ")", "\n", "num_epochs", "=", "5000", "\n", "mini_batch_size", "=", "1", "# mini_batch_size = 1 is better", "\n", "generator_learning_rate", "=", "0.0002", "\n", "generator_learning_rate_decay", "=", "generator_learning_rate", "/", "200000", "\n", "discriminator_learning_rate", "=", "0.0001", "\n", "discriminator_learning_rate_decay", "=", "discriminator_learning_rate", "/", "200000", "\n", "sampling_rate", "=", "16000", "\n", "num_mcep", "=", "24", "\n", "num_scale", "=", "10", "\n", "frame_period", "=", "5.0", "\n", "n_frames", "=", "128", "\n", "lambda_cycle", "=", "10", "\n", "lambda_identity", "=", "5", "\n", "\n", "print", "(", "'Preprocessing Data...'", ")", "\n", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "wavs_A", "=", "load_wavs", "(", "wav_dir", "=", "train_A_dir", ",", "sr", "=", "sampling_rate", ")", "\n", "wavs_B", "=", "load_wavs", "(", "wav_dir", "=", "train_B_dir", ",", "sr", "=", "sampling_rate", ")", "\n", "\n", "f0s_A", ",", "timeaxes_A", ",", "sps_A", ",", "aps_A", ",", "coded_sps_A", "=", "world_encode_data", "(", "wavs", "=", "wavs_A", ",", "fs", "=", "sampling_rate", ",", "frame_period", "=", "frame_period", ",", "coded_dim", "=", "num_mcep", ")", "\n", "f0s_B", ",", "timeaxes_B", ",", "sps_B", ",", "aps_B", ",", "coded_sps_B", "=", "world_encode_data", "(", "wavs", "=", "wavs_B", ",", "fs", "=", "sampling_rate", ",", "frame_period", "=", "frame_period", ",", "coded_dim", "=", "num_mcep", ")", "\n", "\n", "log_f0s_mean_A", ",", "log_f0s_std_A", "=", "logf0_statistics", "(", "f0s_A", ")", "\n", "log_f0s_mean_B", ",", "log_f0s_std_B", "=", "logf0_statistics", "(", "f0s_B", ")", "\n", "#############################", "\n", "#get lf0 cwt:", "\n", "lf0_cwt_norm_A", ",", "scales_A", ",", "means_A", ",", "stds_A", "=", "get_lf0_cwt_norm", "(", "f0s_A", ",", "mean", "=", "log_f0s_mean_A", ",", "std", "=", "log_f0s_std_A", ")", "\n", "lf0_cwt_norm_B", ",", "scales_B", ",", "means_B", ",", "stds_B", "=", "get_lf0_cwt_norm", "(", "f0s_B", ",", "mean", "=", "log_f0s_mean_B", ",", "std", "=", "log_f0s_std_B", ")", "\n", "\n", "\n", "print", "(", "'Log Pitch A'", ")", "\n", "print", "(", "'Mean: %f, Std: %f'", "%", "(", "log_f0s_mean_A", ",", "log_f0s_std_A", ")", ")", "\n", "print", "(", "'Log Pitch B'", ")", "\n", "print", "(", "'Mean: %f, Std: %f'", "%", "(", "log_f0s_mean_B", ",", "log_f0s_std_B", ")", ")", "\n", "\n", "lf0_cwt_norm_A_transposed", "=", "transpose_in_list", "(", "lst", "=", "lf0_cwt_norm_A", ")", "\n", "lf0_cwt_norm_B_transposed", "=", "transpose_in_list", "(", "lst", "=", "lf0_cwt_norm_B", ")", "\n", "\n", "coded_sps_A_transposed", "=", "transpose_in_list", "(", "lst", "=", "coded_sps_A", ")", "\n", "coded_sps_B_transposed", "=", "transpose_in_list", "(", "lst", "=", "coded_sps_B", ")", "\n", "\n", "coded_sps_A_norm", ",", "coded_sps_A_mean", ",", "coded_sps_A_std", "=", "coded_sps_normalization_fit_transoform", "(", "coded_sps", "=", "coded_sps_A_transposed", ")", "\n", "coded_sps_B_norm", ",", "coded_sps_B_mean", ",", "coded_sps_B_std", "=", "coded_sps_normalization_fit_transoform", "(", "coded_sps", "=", "coded_sps_B_transposed", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "model_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "model_dir", ")", "\n", "", "np", ".", "savez", "(", "os", ".", "path", ".", "join", "(", "model_dir", ",", "'logf0s_normalization.npz'", ")", ",", "mean_A", "=", "log_f0s_mean_A", ",", "std_A", "=", "log_f0s_std_A", ",", "mean_B", "=", "log_f0s_mean_B", ",", "std_B", "=", "log_f0s_std_B", ")", "\n", "np", ".", "savez", "(", "os", ".", "path", ".", "join", "(", "model_dir", ",", "'mcep_normalization.npz'", ")", ",", "mean_A", "=", "coded_sps_A_mean", ",", "std_A", "=", "coded_sps_A_std", ",", "mean_B", "=", "coded_sps_B_mean", ",", "std_B", "=", "coded_sps_B_std", ")", "\n", "\n", "if", "validation_A_dir", "is", "not", "None", ":", "\n", "        ", "validation_A_output_dir", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'converted_A'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "validation_A_output_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "validation_A_output_dir", ")", "\n", "\n", "", "", "if", "validation_B_dir", "is", "not", "None", ":", "\n", "        ", "validation_B_output_dir", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'converted_B'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "validation_B_output_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "validation_B_output_dir", ")", "\n", "\n", "", "", "end_time", "=", "time", ".", "time", "(", ")", "\n", "time_elapsed", "=", "end_time", "-", "start_time", "\n", "\n", "print", "(", "'Preprocessing Done.'", ")", "\n", "\n", "print", "(", "'Time Elapsed for Data Preprocessing: %02d:%02d:%02d'", "%", "(", "time_elapsed", "//", "3600", ",", "(", "time_elapsed", "%", "3600", "//", "60", ")", ",", "(", "time_elapsed", "%", "60", "//", "1", ")", ")", ")", "\n", "\n", "num_feats", "=", "10", "#34", "\n", "model", "=", "CycleGAN", "(", "num_features", "=", "num_feats", ")", "\n", "#model.load('./model/neutral2anger_f0/neutral2anger_f0.ckpt')", "\n", "#print('model restored')", "\n", "\n", "for", "epoch", "in", "range", "(", "num_epochs", ")", ":", "\n", "        ", "print", "(", "'Epoch: %d'", "%", "epoch", ")", "\n", "'''\n        if epoch > 60:\n            lambda_identity = 0\n        if epoch > 1250:\n            generator_learning_rate = max(0, generator_learning_rate - 0.0000002)\n            discriminator_learning_rate = max(0, discriminator_learning_rate - 0.0000001)\n        '''", "\n", "\n", "start_time_epoch", "=", "time", ".", "time", "(", ")", "\n", "###zk", "\n", "data_As", "=", "list", "(", ")", "\n", "data_Bs", "=", "list", "(", ")", "\n", "for", "lf0_a", "in", "lf0_cwt_norm_A_transposed", ":", "\n", "            ", "data_A", "=", "lf0_a", "\n", "data_As", ".", "append", "(", "data_A", ")", "\n", "\n", "", "for", "lf0_b", "in", "lf0_cwt_norm_B_transposed", ":", "\n", "            ", "data_B", "=", "lf0_b", "\n", "data_Bs", ".", "append", "(", "data_B", ")", "\n", "#        for sp_a, lf0_a in zip(coded_sps_A_norm,lf0_cwt_norm_A_transposed):", "\n", "#            data_A = np.vstack((sp_a,lf0_a))", "\n", "#            data_As.append(data_A)", "\n", "#        for sp_b, lf0_b in zip(coded_sps_B_norm,lf0_cwt_norm_B_transposed):", "\n", "#            data_B = np.vstack((sp_b,lf0_b))", "\n", "#            data_Bs.append(data_B)", "\n", "\n", "#dataset_A, dataset_B = sample_train_data(dataset_A = coded_sps_A_norm, dataset_B = coded_sps_B_norm, n_frames = n_frames)", "\n", "", "dataset_A", ",", "dataset_B", "=", "sample_train_data", "(", "dataset_A", "=", "data_As", ",", "dataset_B", "=", "data_Bs", ",", "n_frames", "=", "n_frames", ")", "\n", "\n", "n_samples", "=", "dataset_A", ".", "shape", "[", "0", "]", "\n", "\n", "for", "i", "in", "range", "(", "n_samples", "//", "mini_batch_size", ")", ":", "\n", "\n", "            ", "num_iterations", "=", "n_samples", "//", "mini_batch_size", "*", "epoch", "+", "i", "\n", "\n", "if", "num_iterations", ">", "10000", ":", "\n", "                ", "lambda_identity", "=", "0", "\n", "", "if", "num_iterations", ">", "200000", ":", "\n", "                ", "generator_learning_rate", "=", "max", "(", "0", ",", "generator_learning_rate", "-", "generator_learning_rate_decay", ")", "\n", "discriminator_learning_rate", "=", "max", "(", "0", ",", "discriminator_learning_rate", "-", "discriminator_learning_rate_decay", ")", "\n", "\n", "", "start", "=", "i", "*", "mini_batch_size", "\n", "end", "=", "(", "i", "+", "1", ")", "*", "mini_batch_size", "\n", "\n", "generator_loss", ",", "discriminator_loss", "=", "model", ".", "train", "(", "input_A", "=", "dataset_A", "[", "start", ":", "end", "]", ",", "input_B", "=", "dataset_B", "[", "start", ":", "end", "]", ",", "lambda_cycle", "=", "lambda_cycle", ",", "lambda_identity", "=", "lambda_identity", ",", "generator_learning_rate", "=", "generator_learning_rate", ",", "discriminator_learning_rate", "=", "discriminator_learning_rate", ")", "\n", "\n", "if", "i", "%", "50", "==", "0", ":", "\n", "#print('Iteration: %d, Generator Loss : %f, Discriminator Loss : %f' % (num_iterations, generator_loss, discriminator_loss))", "\n", "                ", "print", "(", "'Iteration: {:07d}, Generator Learning Rate: {:.7f}, Discriminator Learning Rate: {:.7f}, Generator Loss : {:.3f}, Discriminator Loss : {:.3f}'", ".", "format", "(", "num_iterations", ",", "generator_learning_rate", ",", "discriminator_learning_rate", ",", "generator_loss", ",", "discriminator_loss", ")", ")", "\n", "\n", "", "", "model", ".", "save", "(", "directory", "=", "model_dir", ",", "filename", "=", "model_name", ")", "\n", "\n", "end_time_epoch", "=", "time", ".", "time", "(", ")", "\n", "time_elapsed_epoch", "=", "end_time_epoch", "-", "start_time_epoch", "\n", "\n", "print", "(", "'Time Elapsed for This Epoch: %02d:%02d:%02d'", "%", "(", "time_elapsed_epoch", "//", "3600", ",", "(", "time_elapsed_epoch", "%", "3600", "//", "60", ")", ",", "(", "time_elapsed_epoch", "%", "60", "//", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.train.train": [[11, 159], ["numpy.random.seed", "print", "time.time", "preprocess.load_wavs", "preprocess.load_wavs", "preprocess.world_encode_data", "preprocess.world_encode_data", "preprocess.logf0_statistics", "preprocess.logf0_statistics", "print", "print", "print", "print", "preprocess.transpose_in_list", "preprocess.transpose_in_list", "preprocess.coded_sps_normalization_fit_transoform", "print", "preprocess.coded_sps_normalization_fit_transoform", "numpy.savez", "numpy.savez", "time.time", "print", "print", "model.CycleGAN", "range", "os.path.exists", "os.makedirs", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "print", "time.time", "preprocess.sample_train_data", "range", "model.CycleGAN.save", "time.time", "print", "os.path.exists", "os.makedirs", "os.path.exists", "os.makedirs", "model.CycleGAN.train", "max", "max", "print", "print", "os.listdir", "print", "os.listdir", "os.path.join", "librosa.load", "preprocess.wav_padding", "preprocess.world_decompose", "preprocess.pitch_conversion", "preprocess.world_encode_spectral_envelop", "numpy.ascontiguousarray", "preprocess.world_decode_spectral_envelop", "preprocess.world_speech_synthesis", "librosa.output.write_wav", "os.path.join", "librosa.load", "preprocess.wav_padding", "preprocess.world_decompose", "preprocess.pitch_conversion", "preprocess.world_encode_spectral_envelop", "numpy.ascontiguousarray", "preprocess.world_decode_spectral_envelop", "preprocess.world_speech_synthesis", "librosa.output.write_wav", "model.CycleGAN.test", "os.path.join", "model.CycleGAN.test", "os.path.join", "os.path.basename", "os.path.basename", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.load_wavs", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.load_wavs", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_encode_data", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_encode_data", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.logf0_statistics", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.logf0_statistics", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.transpose_in_list", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.transpose_in_list", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.coded_sps_normalization_fit_transoform", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.coded_sps_normalization_fit_transoform", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.sample_train_data", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.save", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.train", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.load", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.wav_padding", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_decompose", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.pitch_conversion", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_encode_spectral_envelop", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_decode_spectral_envelop", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_speech_synthesis", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.load", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.wav_padding", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_decompose", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.pitch_conversion", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_encode_spectral_envelop", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_decode_spectral_envelop", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_speech_synthesis", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.test", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.test"], ["def", "train", "(", "train_A_dir", ",", "train_B_dir", ",", "model_dir", ",", "model_name", ",", "random_seed", ",", "validation_A_dir", ",", "validation_B_dir", ",", "output_dir", ",", "tensorboard_log_dir", ")", ":", "\n", "\n", "    ", "np", ".", "random", ".", "seed", "(", "random_seed", ")", "\n", "\n", "num_epochs", "=", "5000", "\n", "mini_batch_size", "=", "1", "# mini_batch_size = 1 is better", "\n", "generator_learning_rate", "=", "0.0002", "\n", "generator_learning_rate_decay", "=", "generator_learning_rate", "/", "200000", "\n", "discriminator_learning_rate", "=", "0.0001", "\n", "discriminator_learning_rate_decay", "=", "discriminator_learning_rate", "/", "200000", "\n", "sampling_rate", "=", "16000", "\n", "num_mcep", "=", "24", "\n", "frame_period", "=", "5.0", "\n", "n_frames", "=", "128", "\n", "lambda_cycle", "=", "10", "\n", "lambda_identity", "=", "5", "\n", "\n", "print", "(", "'Preprocessing Data...'", ")", "\n", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "wavs_A", "=", "load_wavs", "(", "wav_dir", "=", "train_A_dir", ",", "sr", "=", "sampling_rate", ")", "\n", "wavs_B", "=", "load_wavs", "(", "wav_dir", "=", "train_B_dir", ",", "sr", "=", "sampling_rate", ")", "\n", "\n", "f0s_A", ",", "timeaxes_A", ",", "sps_A", ",", "aps_A", ",", "coded_sps_A", "=", "world_encode_data", "(", "wavs", "=", "wavs_A", ",", "fs", "=", "sampling_rate", ",", "frame_period", "=", "frame_period", ",", "coded_dim", "=", "num_mcep", ")", "\n", "f0s_B", ",", "timeaxes_B", ",", "sps_B", ",", "aps_B", ",", "coded_sps_B", "=", "world_encode_data", "(", "wavs", "=", "wavs_B", ",", "fs", "=", "sampling_rate", ",", "frame_period", "=", "frame_period", ",", "coded_dim", "=", "num_mcep", ")", "\n", "\n", "log_f0s_mean_A", ",", "log_f0s_std_A", "=", "logf0_statistics", "(", "f0s_A", ")", "\n", "log_f0s_mean_B", ",", "log_f0s_std_B", "=", "logf0_statistics", "(", "f0s_B", ")", "\n", "\n", "print", "(", "'Log Pitch A'", ")", "\n", "print", "(", "'Mean: %f, Std: %f'", "%", "(", "log_f0s_mean_A", ",", "log_f0s_std_A", ")", ")", "\n", "print", "(", "'Log Pitch B'", ")", "\n", "print", "(", "'Mean: %f, Std: %f'", "%", "(", "log_f0s_mean_B", ",", "log_f0s_std_B", ")", ")", "\n", "\n", "\n", "coded_sps_A_transposed", "=", "transpose_in_list", "(", "lst", "=", "coded_sps_A", ")", "\n", "coded_sps_B_transposed", "=", "transpose_in_list", "(", "lst", "=", "coded_sps_B", ")", "\n", "\n", "coded_sps_A_norm", ",", "coded_sps_A_mean", ",", "coded_sps_A_std", "=", "coded_sps_normalization_fit_transoform", "(", "coded_sps", "=", "coded_sps_A_transposed", ")", "\n", "print", "(", "\"Input data fixed.\"", ")", "\n", "coded_sps_B_norm", ",", "coded_sps_B_mean", ",", "coded_sps_B_std", "=", "coded_sps_normalization_fit_transoform", "(", "coded_sps", "=", "coded_sps_B_transposed", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "model_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "model_dir", ")", "\n", "", "np", ".", "savez", "(", "os", ".", "path", ".", "join", "(", "model_dir", ",", "'logf0s_normalization.npz'", ")", ",", "mean_A", "=", "log_f0s_mean_A", ",", "std_A", "=", "log_f0s_std_A", ",", "mean_B", "=", "log_f0s_mean_B", ",", "std_B", "=", "log_f0s_std_B", ")", "\n", "np", ".", "savez", "(", "os", ".", "path", ".", "join", "(", "model_dir", ",", "'mcep_normalization.npz'", ")", ",", "mean_A", "=", "coded_sps_A_mean", ",", "std_A", "=", "coded_sps_A_std", ",", "mean_B", "=", "coded_sps_B_mean", ",", "std_B", "=", "coded_sps_B_std", ")", "\n", "\n", "if", "validation_A_dir", "is", "not", "None", ":", "\n", "        ", "validation_A_output_dir", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'converted_A'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "validation_A_output_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "validation_A_output_dir", ")", "\n", "\n", "", "", "if", "validation_B_dir", "is", "not", "None", ":", "\n", "        ", "validation_B_output_dir", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'converted_B'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "validation_B_output_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "validation_B_output_dir", ")", "\n", "\n", "", "", "end_time", "=", "time", ".", "time", "(", ")", "\n", "time_elapsed", "=", "end_time", "-", "start_time", "\n", "\n", "print", "(", "'Preprocessing Done.'", ")", "\n", "\n", "print", "(", "'Time Elapsed for Data Preprocessing: %02d:%02d:%02d'", "%", "(", "time_elapsed", "//", "3600", ",", "(", "time_elapsed", "%", "3600", "//", "60", ")", ",", "(", "time_elapsed", "%", "60", "//", "1", ")", ")", ")", "\n", "\n", "model", "=", "CycleGAN", "(", "num_features", "=", "num_mcep", ")", "\n", "\n", "for", "epoch", "in", "range", "(", "num_epochs", ")", ":", "\n", "        ", "print", "(", "'Epoch: %d'", "%", "epoch", ")", "\n", "'''\n        if epoch > 60:\n            lambda_identity = 0\n        if epoch > 1250:\n            generator_learning_rate = max(0, generator_learning_rate - 0.0000002)\n            discriminator_learning_rate = max(0, discriminator_learning_rate - 0.0000001)\n        '''", "\n", "\n", "start_time_epoch", "=", "time", ".", "time", "(", ")", "\n", "\n", "dataset_A", ",", "dataset_B", "=", "sample_train_data", "(", "dataset_A", "=", "coded_sps_A_norm", ",", "dataset_B", "=", "coded_sps_B_norm", ",", "n_frames", "=", "n_frames", ")", "\n", "\n", "n_samples", "=", "dataset_A", ".", "shape", "[", "0", "]", "\n", "\n", "for", "i", "in", "range", "(", "n_samples", "//", "mini_batch_size", ")", ":", "\n", "\n", "            ", "num_iterations", "=", "n_samples", "//", "mini_batch_size", "*", "epoch", "+", "i", "\n", "\n", "if", "num_iterations", ">", "10000", ":", "\n", "                ", "lambda_identity", "=", "0", "\n", "", "if", "num_iterations", ">", "200000", ":", "\n", "                ", "generator_learning_rate", "=", "max", "(", "0", ",", "generator_learning_rate", "-", "generator_learning_rate_decay", ")", "\n", "discriminator_learning_rate", "=", "max", "(", "0", ",", "discriminator_learning_rate", "-", "discriminator_learning_rate_decay", ")", "\n", "\n", "", "start", "=", "i", "*", "mini_batch_size", "\n", "end", "=", "(", "i", "+", "1", ")", "*", "mini_batch_size", "\n", "\n", "generator_loss", ",", "discriminator_loss", "=", "model", ".", "train", "(", "input_A", "=", "dataset_A", "[", "start", ":", "end", "]", ",", "input_B", "=", "dataset_B", "[", "start", ":", "end", "]", ",", "lambda_cycle", "=", "lambda_cycle", ",", "lambda_identity", "=", "lambda_identity", ",", "generator_learning_rate", "=", "generator_learning_rate", ",", "discriminator_learning_rate", "=", "discriminator_learning_rate", ")", "\n", "\n", "if", "i", "%", "50", "==", "0", ":", "\n", "#print('Iteration: %d, Generator Loss : %f, Discriminator Loss : %f' % (num_iterations, generator_loss, discriminator_loss))", "\n", "                ", "print", "(", "'Iteration: {:07d}, Generator Learning Rate: {:.7f}, Discriminator Learning Rate: {:.7f}, Generator Loss : {:.3f}, Discriminator Loss : {:.3f}'", ".", "format", "(", "num_iterations", ",", "generator_learning_rate", ",", "discriminator_learning_rate", ",", "generator_loss", ",", "discriminator_loss", ")", ")", "\n", "\n", "", "", "model", ".", "save", "(", "directory", "=", "model_dir", ",", "filename", "=", "model_name", ")", "\n", "\n", "end_time_epoch", "=", "time", ".", "time", "(", ")", "\n", "time_elapsed_epoch", "=", "end_time_epoch", "-", "start_time_epoch", "\n", "\n", "print", "(", "'Time Elapsed for This Epoch: %02d:%02d:%02d'", "%", "(", "time_elapsed_epoch", "//", "3600", ",", "(", "time_elapsed_epoch", "%", "3600", "//", "60", ")", ",", "(", "time_elapsed_epoch", "%", "60", "//", "1", ")", ")", ")", "\n", "\n", "if", "validation_A_dir", "is", "not", "None", ":", "\n", "            ", "if", "epoch", "%", "50", "==", "0", ":", "\n", "                ", "print", "(", "'Generating Validation Data B from A...'", ")", "\n", "for", "file", "in", "os", ".", "listdir", "(", "validation_A_dir", ")", ":", "\n", "                    ", "filepath", "=", "os", ".", "path", ".", "join", "(", "validation_A_dir", ",", "file", ")", "\n", "wav", ",", "_", "=", "librosa", ".", "load", "(", "filepath", ",", "sr", "=", "sampling_rate", ",", "mono", "=", "True", ")", "\n", "wav", "=", "wav_padding", "(", "wav", "=", "wav", ",", "sr", "=", "sampling_rate", ",", "frame_period", "=", "frame_period", ",", "multiple", "=", "4", ")", "\n", "f0", ",", "timeaxis", ",", "sp", ",", "ap", "=", "world_decompose", "(", "wav", "=", "wav", ",", "fs", "=", "sampling_rate", ",", "frame_period", "=", "frame_period", ")", "\n", "f0_converted", "=", "pitch_conversion", "(", "f0", "=", "f0", ",", "mean_log_src", "=", "log_f0s_mean_A", ",", "std_log_src", "=", "log_f0s_std_A", ",", "mean_log_target", "=", "log_f0s_mean_B", ",", "std_log_target", "=", "log_f0s_std_B", ")", "\n", "coded_sp", "=", "world_encode_spectral_envelop", "(", "sp", "=", "sp", ",", "fs", "=", "sampling_rate", ",", "dim", "=", "num_mcep", ")", "\n", "coded_sp_transposed", "=", "coded_sp", ".", "T", "\n", "coded_sp_norm", "=", "(", "coded_sp_transposed", "-", "coded_sps_A_mean", ")", "/", "coded_sps_A_std", "\n", "coded_sp_converted_norm", "=", "model", ".", "test", "(", "inputs", "=", "np", ".", "array", "(", "[", "coded_sp_norm", "]", ")", ",", "direction", "=", "'A2B'", ")", "[", "0", "]", "\n", "coded_sp_converted", "=", "coded_sp_converted_norm", "*", "coded_sps_B_std", "+", "coded_sps_B_mean", "\n", "coded_sp_converted", "=", "coded_sp_converted", ".", "T", "\n", "coded_sp_converted", "=", "np", ".", "ascontiguousarray", "(", "coded_sp_converted", ")", "\n", "decoded_sp_converted", "=", "world_decode_spectral_envelop", "(", "coded_sp", "=", "coded_sp_converted", ",", "fs", "=", "sampling_rate", ")", "\n", "wav_transformed", "=", "world_speech_synthesis", "(", "f0", "=", "f0_converted", ",", "decoded_sp", "=", "decoded_sp_converted", ",", "ap", "=", "ap", ",", "fs", "=", "sampling_rate", ",", "frame_period", "=", "frame_period", ")", "\n", "librosa", ".", "output", ".", "write_wav", "(", "os", ".", "path", ".", "join", "(", "validation_A_output_dir", ",", "os", ".", "path", ".", "basename", "(", "file", ")", ")", ",", "wav_transformed", ",", "sampling_rate", ")", "\n", "\n", "", "", "", "if", "validation_B_dir", "is", "not", "None", ":", "\n", "            ", "if", "epoch", "%", "50", "==", "0", ":", "\n", "                ", "print", "(", "'Generating Validation Data A from B...'", ")", "\n", "for", "file", "in", "os", ".", "listdir", "(", "validation_B_dir", ")", ":", "\n", "                    ", "filepath", "=", "os", ".", "path", ".", "join", "(", "validation_B_dir", ",", "file", ")", "\n", "wav", ",", "_", "=", "librosa", ".", "load", "(", "filepath", ",", "sr", "=", "sampling_rate", ",", "mono", "=", "True", ")", "\n", "wav", "=", "wav_padding", "(", "wav", "=", "wav", ",", "sr", "=", "sampling_rate", ",", "frame_period", "=", "frame_period", ",", "multiple", "=", "4", ")", "\n", "f0", ",", "timeaxis", ",", "sp", ",", "ap", "=", "world_decompose", "(", "wav", "=", "wav", ",", "fs", "=", "sampling_rate", ",", "frame_period", "=", "frame_period", ")", "\n", "f0_converted", "=", "pitch_conversion", "(", "f0", "=", "f0", ",", "mean_log_src", "=", "log_f0s_mean_B", ",", "std_log_src", "=", "log_f0s_std_B", ",", "mean_log_target", "=", "log_f0s_mean_A", ",", "std_log_target", "=", "log_f0s_std_A", ")", "\n", "coded_sp", "=", "world_encode_spectral_envelop", "(", "sp", "=", "sp", ",", "fs", "=", "sampling_rate", ",", "dim", "=", "num_mcep", ")", "\n", "coded_sp_transposed", "=", "coded_sp", ".", "T", "\n", "coded_sp_norm", "=", "(", "coded_sp_transposed", "-", "coded_sps_B_mean", ")", "/", "coded_sps_B_std", "\n", "coded_sp_converted_norm", "=", "model", ".", "test", "(", "inputs", "=", "np", ".", "array", "(", "[", "coded_sp_norm", "]", ")", ",", "direction", "=", "'B2A'", ")", "[", "0", "]", "\n", "coded_sp_converted", "=", "coded_sp_converted_norm", "*", "coded_sps_A_std", "+", "coded_sps_A_mean", "\n", "coded_sp_converted", "=", "coded_sp_converted", ".", "T", "\n", "coded_sp_converted", "=", "np", ".", "ascontiguousarray", "(", "coded_sp_converted", ")", "\n", "decoded_sp_converted", "=", "world_decode_spectral_envelop", "(", "coded_sp", "=", "coded_sp_converted", ",", "fs", "=", "sampling_rate", ")", "\n", "wav_transformed", "=", "world_speech_synthesis", "(", "f0", "=", "f0_converted", ",", "decoded_sp", "=", "decoded_sp_converted", ",", "ap", "=", "ap", ",", "fs", "=", "sampling_rate", ",", "frame_period", "=", "frame_period", ")", "\n", "librosa", ".", "output", ".", "write_wav", "(", "os", ".", "path", ".", "join", "(", "validation_B_output_dir", ",", "os", ".", "path", ".", "basename", "(", "file", ")", ")", ",", "wav_transformed", ",", "sampling_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_mceps.CycleGAN.__init__": [[9, 31], ["model_mceps.CycleGAN.build_model", "model_mceps.CycleGAN.optimizer_initializer", "tensorflow.train.Saver", "tensorflow.Session", "model_mceps.CycleGAN.sess.run", "tensorflow.global_variables_initializer", "datetime.datetime.datetime.now", "os.path.join", "tensorflow.summary.FileWriter", "model_mceps.CycleGAN.summary", "datetime.datetime.now.strftime", "tensorflow.get_default_graph"], "methods", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.build_model", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.optimizer_initializer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.summary"], ["    ", "def", "__init__", "(", "self", ",", "num_features", ",", "discriminator", "=", "discriminator", ",", "generator", "=", "generator_gatedcnn", ",", "mode", "=", "'train'", ",", "log_dir", "=", "'./log'", ")", ":", "\n", "\n", "        ", "self", ".", "num_features", "=", "num_features", "\n", "self", ".", "input_shape", "=", "[", "None", ",", "num_features", ",", "None", "]", "# [batch_size, num_features, num_frames]", "\n", "\n", "self", ".", "discriminator", "=", "discriminator", "\n", "self", ".", "generator", "=", "generator", "\n", "self", ".", "mode", "=", "mode", "\n", "\n", "self", ".", "build_model", "(", ")", "\n", "self", ".", "optimizer_initializer", "(", ")", "\n", "\n", "self", ".", "saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "\n", "self", ".", "sess", "=", "tf", ".", "Session", "(", ")", "\n", "self", ".", "sess", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "\n", "if", "self", ".", "mode", "==", "'train'", ":", "\n", "            ", "self", ".", "train_step", "=", "0", "\n", "now", "=", "datetime", ".", "now", "(", ")", "\n", "self", ".", "log_dir", "=", "os", ".", "path", ".", "join", "(", "log_dir", ",", "now", ".", "strftime", "(", "'%Y%m%d-%H%M%S'", ")", ")", "\n", "self", ".", "writer", "=", "tf", ".", "summary", ".", "FileWriter", "(", "self", ".", "log_dir", ",", "tf", ".", "get_default_graph", "(", ")", ")", "\n", "self", ".", "generator_summaries", ",", "self", ".", "discriminator_summaries", "=", "self", ".", "summary", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_mceps.CycleGAN.build_model": [[32, 101], ["tensorflow.reset_default_graph", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "model_mceps.CycleGAN.generator", "model_mceps.CycleGAN.generator", "model_mceps.CycleGAN.generator", "model_mceps.CycleGAN.generator", "model_mceps.CycleGAN.generator", "model_mceps.CycleGAN.generator", "model_mceps.CycleGAN.discriminator", "model_mceps.CycleGAN.discriminator", "tensorflow.placeholder", "tensorflow.placeholder", "utils.l2_loss", "utils.l2_loss", "model_mceps.CycleGAN.discriminator", "model_mceps.CycleGAN.discriminator", "model_mceps.CycleGAN.discriminator", "model_mceps.CycleGAN.discriminator", "utils.l2_loss", "utils.l2_loss", "utils.l2_loss", "utils.l2_loss", "tensorflow.trainable_variables", "model_mceps.CycleGAN.generator", "model_mceps.CycleGAN.generator", "utils.l1_loss", "utils.l1_loss", "utils.l1_loss", "utils.l1_loss", "tensorflow.ones_like", "tensorflow.ones_like", "tensorflow.ones_like", "tensorflow.zeros_like", "tensorflow.ones_like", "tensorflow.zeros_like"], "methods", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.discriminator", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.discriminator", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l2_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l2_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.discriminator", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.discriminator", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.discriminator", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.discriminator", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l2_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l2_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l2_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l2_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l1_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l1_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l1_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l1_loss"], ["", "", "def", "build_model", "(", "self", ")", ":", "\n", "        ", "tf", ".", "reset_default_graph", "(", ")", "\n", "# Placeholders for real training samples", "\n", "self", ".", "input_A_real", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "self", ".", "input_shape", ",", "name", "=", "'input_A_real'", ")", "\n", "self", ".", "input_B_real", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "self", ".", "input_shape", ",", "name", "=", "'input_B_real'", ")", "\n", "# Placeholders for fake generated samples", "\n", "self", ".", "input_A_fake", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "self", ".", "input_shape", ",", "name", "=", "'input_A_fake'", ")", "\n", "self", ".", "input_B_fake", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "self", ".", "input_shape", ",", "name", "=", "'input_B_fake'", ")", "\n", "# Placeholder for test samples", "\n", "self", ".", "input_A_test", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "self", ".", "input_shape", ",", "name", "=", "'input_A_test'", ")", "\n", "self", ".", "input_B_test", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "self", ".", "input_shape", ",", "name", "=", "'input_B_test'", ")", "\n", "\n", "self", ".", "generation_B", "=", "self", ".", "generator", "(", "inputs", "=", "self", ".", "input_A_real", ",", "reuse", "=", "False", ",", "scope_name", "=", "'generator_A2B'", ")", "\n", "self", ".", "cycle_A", "=", "self", ".", "generator", "(", "inputs", "=", "self", ".", "generation_B", ",", "reuse", "=", "False", ",", "scope_name", "=", "'generator_B2A'", ")", "\n", "\n", "self", ".", "generation_A", "=", "self", ".", "generator", "(", "inputs", "=", "self", ".", "input_B_real", ",", "reuse", "=", "True", ",", "scope_name", "=", "'generator_B2A'", ")", "\n", "self", ".", "cycle_B", "=", "self", ".", "generator", "(", "inputs", "=", "self", ".", "generation_A", ",", "reuse", "=", "True", ",", "scope_name", "=", "'generator_A2B'", ")", "\n", "\n", "self", ".", "generation_A_identity", "=", "self", ".", "generator", "(", "inputs", "=", "self", ".", "input_A_real", ",", "reuse", "=", "True", ",", "scope_name", "=", "'generator_B2A'", ")", "\n", "self", ".", "generation_B_identity", "=", "self", ".", "generator", "(", "inputs", "=", "self", ".", "input_B_real", ",", "reuse", "=", "True", ",", "scope_name", "=", "'generator_A2B'", ")", "\n", "\n", "self", ".", "discrimination_A_fake", "=", "self", ".", "discriminator", "(", "inputs", "=", "self", ".", "generation_A", ",", "reuse", "=", "False", ",", "scope_name", "=", "'discriminator_A'", ")", "\n", "self", ".", "discrimination_B_fake", "=", "self", ".", "discriminator", "(", "inputs", "=", "self", ".", "generation_B", ",", "reuse", "=", "False", ",", "scope_name", "=", "'discriminator_B'", ")", "\n", "\n", "# Cycle loss", "\n", "self", ".", "cycle_loss", "=", "l1_loss", "(", "y", "=", "self", ".", "input_A_real", ",", "y_hat", "=", "self", ".", "cycle_A", ")", "+", "l1_loss", "(", "y", "=", "self", ".", "input_B_real", ",", "y_hat", "=", "self", ".", "cycle_B", ")", "\n", "\n", "# Identity loss", "\n", "self", ".", "identity_loss", "=", "l1_loss", "(", "y", "=", "self", ".", "input_A_real", ",", "y_hat", "=", "self", ".", "generation_A_identity", ")", "+", "l1_loss", "(", "y", "=", "self", ".", "input_B_real", ",", "y_hat", "=", "self", ".", "generation_B_identity", ")", "\n", "\n", "# Place holder for lambda_cycle and lambda_identity", "\n", "self", ".", "lambda_cycle", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "None", ",", "name", "=", "'lambda_cycle'", ")", "\n", "self", ".", "lambda_identity", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "None", ",", "name", "=", "'lambda_identity'", ")", "\n", "\n", "# Generator loss", "\n", "# Generator wants to fool discriminator", "\n", "self", ".", "generator_loss_A2B", "=", "l2_loss", "(", "y", "=", "tf", ".", "ones_like", "(", "self", ".", "discrimination_B_fake", ")", ",", "y_hat", "=", "self", ".", "discrimination_B_fake", ")", "\n", "self", ".", "generator_loss_B2A", "=", "l2_loss", "(", "y", "=", "tf", ".", "ones_like", "(", "self", ".", "discrimination_A_fake", ")", ",", "y_hat", "=", "self", ".", "discrimination_A_fake", ")", "\n", "\n", "# Merge the two generators and the cycle loss", "\n", "self", ".", "generator_loss", "=", "self", ".", "generator_loss_A2B", "+", "self", ".", "generator_loss_B2A", "+", "self", ".", "lambda_cycle", "*", "self", ".", "cycle_loss", "+", "self", ".", "lambda_identity", "*", "self", ".", "identity_loss", "\n", "\n", "# Discriminator loss", "\n", "self", ".", "discrimination_input_A_real", "=", "self", ".", "discriminator", "(", "inputs", "=", "self", ".", "input_A_real", ",", "reuse", "=", "True", ",", "scope_name", "=", "'discriminator_A'", ")", "\n", "self", ".", "discrimination_input_B_real", "=", "self", ".", "discriminator", "(", "inputs", "=", "self", ".", "input_B_real", ",", "reuse", "=", "True", ",", "scope_name", "=", "'discriminator_B'", ")", "\n", "self", ".", "discrimination_input_A_fake", "=", "self", ".", "discriminator", "(", "inputs", "=", "self", ".", "input_A_fake", ",", "reuse", "=", "True", ",", "scope_name", "=", "'discriminator_A'", ")", "\n", "self", ".", "discrimination_input_B_fake", "=", "self", ".", "discriminator", "(", "inputs", "=", "self", ".", "input_B_fake", ",", "reuse", "=", "True", ",", "scope_name", "=", "'discriminator_B'", ")", "\n", "\n", "# Discriminator wants to classify real and fake correctly", "\n", "self", ".", "discriminator_loss_input_A_real", "=", "l2_loss", "(", "y", "=", "tf", ".", "ones_like", "(", "self", ".", "discrimination_input_A_real", ")", ",", "y_hat", "=", "self", ".", "discrimination_input_A_real", ")", "\n", "self", ".", "discriminator_loss_input_A_fake", "=", "l2_loss", "(", "y", "=", "tf", ".", "zeros_like", "(", "self", ".", "discrimination_input_A_fake", ")", ",", "y_hat", "=", "self", ".", "discrimination_input_A_fake", ")", "\n", "self", ".", "discriminator_loss_A", "=", "(", "self", ".", "discriminator_loss_input_A_real", "+", "self", ".", "discriminator_loss_input_A_fake", ")", "/", "2", "\n", "\n", "self", ".", "discriminator_loss_input_B_real", "=", "l2_loss", "(", "y", "=", "tf", ".", "ones_like", "(", "self", ".", "discrimination_input_B_real", ")", ",", "y_hat", "=", "self", ".", "discrimination_input_B_real", ")", "\n", "self", ".", "discriminator_loss_input_B_fake", "=", "l2_loss", "(", "y", "=", "tf", ".", "zeros_like", "(", "self", ".", "discrimination_input_B_fake", ")", ",", "y_hat", "=", "self", ".", "discrimination_input_B_fake", ")", "\n", "self", ".", "discriminator_loss_B", "=", "(", "self", ".", "discriminator_loss_input_B_real", "+", "self", ".", "discriminator_loss_input_B_fake", ")", "/", "2", "\n", "\n", "# Merge the two discriminators into one", "\n", "self", ".", "discriminator_loss", "=", "self", ".", "discriminator_loss_A", "+", "self", ".", "discriminator_loss_B", "\n", "\n", "# Categorize variables because we have to optimize the two sets of the variables separately", "\n", "trainable_variables", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "self", ".", "discriminator_vars", "=", "[", "var", "for", "var", "in", "trainable_variables", "if", "'discriminator'", "in", "var", ".", "name", "]", "\n", "self", ".", "generator_vars", "=", "[", "var", "for", "var", "in", "trainable_variables", "if", "'generator'", "in", "var", ".", "name", "]", "\n", "#for var in t_vars: print(var.name)", "\n", "\n", "# Reserved for test", "\n", "self", ".", "generation_B_test", "=", "self", ".", "generator", "(", "inputs", "=", "self", ".", "input_A_test", ",", "reuse", "=", "True", ",", "scope_name", "=", "'generator_A2B'", ")", "\n", "self", ".", "generation_A_test", "=", "self", ".", "generator", "(", "inputs", "=", "self", ".", "input_B_test", ",", "reuse", "=", "True", ",", "scope_name", "=", "'generator_B2A'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_mceps.CycleGAN.optimizer_initializer": [[103, 109], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.train.AdamOptimizer().minimize", "tensorflow.train.AdamOptimizer().minimize", "tensorflow.train.AdamOptimizer", "tensorflow.train.AdamOptimizer"], "methods", ["None"], ["", "def", "optimizer_initializer", "(", "self", ")", ":", "\n", "\n", "        ", "self", ".", "generator_learning_rate", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "None", ",", "name", "=", "'generator_learning_rate'", ")", "\n", "self", ".", "discriminator_learning_rate", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "None", ",", "name", "=", "'discriminator_learning_rate'", ")", "\n", "self", ".", "discriminator_optimizer", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "self", ".", "discriminator_learning_rate", ",", "beta1", "=", "0.5", ")", ".", "minimize", "(", "self", ".", "discriminator_loss", ",", "var_list", "=", "self", ".", "discriminator_vars", ")", "\n", "self", ".", "generator_optimizer", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "self", ".", "generator_learning_rate", ",", "beta1", "=", "0.5", ")", ".", "minimize", "(", "self", ".", "generator_loss", ",", "var_list", "=", "self", ".", "generator_vars", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_mceps.CycleGAN.train": [[110, 126], ["model_mceps.CycleGAN.sess.run", "model_mceps.CycleGAN.writer.add_summary", "model_mceps.CycleGAN.sess.run", "model_mceps.CycleGAN.writer.add_summary"], "methods", ["None"], ["", "def", "train", "(", "self", ",", "input_A", ",", "input_B", ",", "lambda_cycle", ",", "lambda_identity", ",", "generator_learning_rate", ",", "discriminator_learning_rate", ")", ":", "\n", "\n", "        ", "generation_A", ",", "generation_B", ",", "generator_loss", ",", "_", ",", "generator_summaries", "=", "self", ".", "sess", ".", "run", "(", "\n", "[", "self", ".", "generation_A", ",", "self", ".", "generation_B", ",", "self", ".", "generator_loss", ",", "self", ".", "generator_optimizer", ",", "self", ".", "generator_summaries", "]", ",", "feed_dict", "=", "{", "self", ".", "lambda_cycle", ":", "lambda_cycle", ",", "self", ".", "lambda_identity", ":", "lambda_identity", ",", "self", ".", "input_A_real", ":", "input_A", ",", "self", ".", "input_B_real", ":", "input_B", ",", "self", ".", "generator_learning_rate", ":", "generator_learning_rate", "}", ")", "\n", "\n", "self", ".", "writer", ".", "add_summary", "(", "generator_summaries", ",", "self", ".", "train_step", ")", "\n", "\n", "discriminator_loss", ",", "_", ",", "discriminator_summaries", "=", "self", ".", "sess", ".", "run", "(", "[", "self", ".", "discriminator_loss", ",", "self", ".", "discriminator_optimizer", ",", "self", ".", "discriminator_summaries", "]", ",", "feed_dict", "=", "{", "self", ".", "input_A_real", ":", "input_A", ",", "self", ".", "input_B_real", ":", "input_B", ",", "self", ".", "discriminator_learning_rate", ":", "discriminator_learning_rate", ",", "self", ".", "input_A_fake", ":", "generation_A", ",", "self", ".", "input_B_fake", ":", "generation_B", "}", ")", "\n", "\n", "self", ".", "writer", ".", "add_summary", "(", "discriminator_summaries", ",", "self", ".", "train_step", ")", "\n", "\n", "self", ".", "train_step", "+=", "1", "\n", "\n", "return", "generator_loss", ",", "discriminator_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_mceps.CycleGAN.test": [[128, 138], ["model_mceps.CycleGAN.sess.run", "model_mceps.CycleGAN.sess.run", "Exception"], "methods", ["None"], ["", "def", "test", "(", "self", ",", "inputs", ",", "direction", ")", ":", "\n", "\n", "        ", "if", "direction", "==", "'A2B'", ":", "\n", "            ", "generation", "=", "self", ".", "sess", ".", "run", "(", "self", ".", "generation_B_test", ",", "feed_dict", "=", "{", "self", ".", "input_A_test", ":", "inputs", "}", ")", "\n", "", "elif", "direction", "==", "'B2A'", ":", "\n", "            ", "generation", "=", "self", ".", "sess", ".", "run", "(", "self", ".", "generation_A_test", ",", "feed_dict", "=", "{", "self", ".", "input_B_test", ":", "inputs", "}", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "'Conversion direction must be specified.'", ")", "\n", "\n", "", "return", "generation", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_mceps.CycleGAN.save": [[140, 147], ["model_mceps.CycleGAN.saver.save", "os.path.join", "os.path.exists", "os.makedirs", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.save"], ["", "def", "save", "(", "self", ",", "directory", ",", "filename", ")", ":", "\n", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "directory", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "directory", ")", "\n", "", "self", ".", "saver", ".", "save", "(", "self", ".", "sess", ",", "os", ".", "path", ".", "join", "(", "directory", ",", "filename", ")", ")", "\n", "\n", "return", "os", ".", "path", ".", "join", "(", "directory", ",", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_mceps.CycleGAN.load": [[148, 151], ["model_mceps.CycleGAN.saver.restore"], "methods", ["None"], ["", "def", "load", "(", "self", ",", "filepath", ")", ":", "\n", "\n", "        ", "self", ".", "saver", ".", "restore", "(", "self", ".", "sess", ",", "filepath", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_mceps.CycleGAN.summary": [[153, 170], ["tensorflow.name_scope", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.merge", "tensorflow.name_scope", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.merge"], "methods", ["None"], ["", "def", "summary", "(", "self", ")", ":", "\n", "\n", "        ", "with", "tf", ".", "name_scope", "(", "'generator_summaries'", ")", ":", "\n", "            ", "cycle_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "'cycle_loss'", ",", "self", ".", "cycle_loss", ")", "\n", "identity_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "'identity_loss'", ",", "self", ".", "identity_loss", ")", "\n", "generator_loss_A2B_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "'generator_loss_A2B'", ",", "self", ".", "generator_loss_A2B", ")", "\n", "generator_loss_B2A_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "'generator_loss_B2A'", ",", "self", ".", "generator_loss_B2A", ")", "\n", "generator_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "'generator_loss'", ",", "self", ".", "generator_loss", ")", "\n", "generator_summaries", "=", "tf", ".", "summary", ".", "merge", "(", "[", "cycle_loss_summary", ",", "identity_loss_summary", ",", "generator_loss_A2B_summary", ",", "generator_loss_B2A_summary", ",", "generator_loss_summary", "]", ")", "\n", "\n", "", "with", "tf", ".", "name_scope", "(", "'discriminator_summaries'", ")", ":", "\n", "            ", "discriminator_loss_A_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "'discriminator_loss_A'", ",", "self", ".", "discriminator_loss_A", ")", "\n", "discriminator_loss_B_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "'discriminator_loss_B'", ",", "self", ".", "discriminator_loss_B", ")", "\n", "discriminator_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "'discriminator_loss'", ",", "self", ".", "discriminator_loss", ")", "\n", "discriminator_summaries", "=", "tf", ".", "summary", ".", "merge", "(", "[", "discriminator_loss_A_summary", ",", "discriminator_loss_B_summary", ",", "discriminator_loss_summary", "]", ")", "\n", "\n", "", "return", "generator_summaries", ",", "discriminator_summaries", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.load_wavs": [[6, 16], ["list", "os.listdir", "os.path.join", "librosa.load", "list.append"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.load"], ["def", "load_wavs", "(", "wav_dir", ",", "sr", ")", ":", "\n", "\n", "    ", "wavs", "=", "list", "(", ")", "\n", "for", "file", "in", "os", ".", "listdir", "(", "wav_dir", ")", ":", "\n", "        ", "file_path", "=", "os", ".", "path", ".", "join", "(", "wav_dir", ",", "file", ")", "\n", "wav", ",", "_", "=", "librosa", ".", "load", "(", "file_path", ",", "sr", "=", "sr", ",", "mono", "=", "True", ")", "\n", "#wav = wav.astype(np.float64)", "\n", "wavs", ".", "append", "(", "wav", ")", "\n", "\n", "", "return", "wavs", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_decompose": [[17, 26], ["wav.astype.astype", "pyworld.harvest", "pyworld.cheaptrick", "pyworld.d4c"], "function", ["None"], ["", "def", "world_decompose", "(", "wav", ",", "fs", ",", "frame_period", "=", "5.0", ")", ":", "\n", "\n", "# Decompose speech signal into f0, spectral envelope and aperiodicity using WORLD", "\n", "    ", "wav", "=", "wav", ".", "astype", "(", "np", ".", "float64", ")", "\n", "f0", ",", "timeaxis", "=", "pyworld", ".", "harvest", "(", "wav", ",", "fs", ",", "frame_period", "=", "frame_period", ",", "f0_floor", "=", "71.0", ",", "f0_ceil", "=", "800.0", ")", "\n", "sp", "=", "pyworld", ".", "cheaptrick", "(", "wav", ",", "f0", ",", "timeaxis", ",", "fs", ")", "\n", "ap", "=", "pyworld", ".", "d4c", "(", "wav", ",", "f0", ",", "timeaxis", ",", "fs", ")", "\n", "\n", "return", "f0", ",", "timeaxis", ",", "sp", ",", "ap", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_encode_spectral_envelop": [[27, 35], ["pyworld.code_spectral_envelope"], "function", ["None"], ["", "def", "world_encode_spectral_envelop", "(", "sp", ",", "fs", ",", "dim", "=", "24", ")", ":", "\n", "\n", "# Get Mel-cepstral coefficients (MCEPs)", "\n", "\n", "#sp = sp.astype(np.float64)", "\n", "    ", "coded_sp", "=", "pyworld", ".", "code_spectral_envelope", "(", "sp", ",", "fs", ",", "dim", ")", "\n", "\n", "return", "coded_sp", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_decode_spectral_envelop": [[36, 44], ["pyworld.get_cheaptrick_fft_size", "pyworld.decode_spectral_envelope"], "function", ["None"], ["", "def", "world_decode_spectral_envelop", "(", "coded_sp", ",", "fs", ")", ":", "\n", "\n", "    ", "fftlen", "=", "pyworld", ".", "get_cheaptrick_fft_size", "(", "fs", ")", "\n", "#coded_sp = coded_sp.astype(np.float32)", "\n", "#coded_sp = np.ascontiguousarray(coded_sp)", "\n", "decoded_sp", "=", "pyworld", ".", "decode_spectral_envelope", "(", "coded_sp", ",", "fs", ",", "fftlen", ")", "\n", "\n", "return", "decoded_sp", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_encode_data": [[46, 64], ["list", "list", "list", "list", "list", "preprocess.world_decompose", "preprocess.world_encode_spectral_envelop", "list.append", "list.append", "list.append", "list.append", "list.append"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_decompose", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_encode_spectral_envelop"], ["", "def", "world_encode_data", "(", "wavs", ",", "fs", ",", "frame_period", "=", "5.0", ",", "coded_dim", "=", "24", ")", ":", "\n", "\n", "    ", "f0s", "=", "list", "(", ")", "\n", "timeaxes", "=", "list", "(", ")", "\n", "sps", "=", "list", "(", ")", "\n", "aps", "=", "list", "(", ")", "\n", "coded_sps", "=", "list", "(", ")", "\n", "\n", "for", "wav", "in", "wavs", ":", "\n", "        ", "f0", ",", "timeaxis", ",", "sp", ",", "ap", "=", "world_decompose", "(", "wav", "=", "wav", ",", "fs", "=", "fs", ",", "frame_period", "=", "frame_period", ")", "\n", "coded_sp", "=", "world_encode_spectral_envelop", "(", "sp", "=", "sp", ",", "fs", "=", "fs", ",", "dim", "=", "coded_dim", ")", "\n", "f0s", ".", "append", "(", "f0", ")", "\n", "timeaxes", ".", "append", "(", "timeaxis", ")", "\n", "sps", ".", "append", "(", "sp", ")", "\n", "aps", ".", "append", "(", "ap", ")", "\n", "coded_sps", ".", "append", "(", "coded_sp", ")", "\n", "\n", "", "return", "f0s", ",", "timeaxes", ",", "sps", ",", "aps", ",", "coded_sps", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.transpose_in_list": [[66, 72], ["list", "list.append"], "function", ["None"], ["", "def", "transpose_in_list", "(", "lst", ")", ":", "\n", "\n", "    ", "transposed_lst", "=", "list", "(", ")", "\n", "for", "array", "in", "lst", ":", "\n", "        ", "transposed_lst", ".", "append", "(", "array", ".", "T", ")", "\n", "", "return", "transposed_lst", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_decode_data": [[74, 83], ["list", "preprocess.world_decode_spectral_envelop", "list.append"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_decode_spectral_envelop"], ["", "def", "world_decode_data", "(", "coded_sps", ",", "fs", ")", ":", "\n", "\n", "    ", "decoded_sps", "=", "list", "(", ")", "\n", "\n", "for", "coded_sp", "in", "coded_sps", ":", "\n", "        ", "decoded_sp", "=", "world_decode_spectral_envelop", "(", "coded_sp", ",", "fs", ")", "\n", "decoded_sps", ".", "append", "(", "decoded_sp", ")", "\n", "\n", "", "return", "decoded_sps", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_speech_synthesis": [[85, 93], ["pyworld.synthesize", "wav.astype.astype"], "function", ["None"], ["", "def", "world_speech_synthesis", "(", "f0", ",", "decoded_sp", ",", "ap", ",", "fs", ",", "frame_period", ")", ":", "\n", "\n", "#decoded_sp = decoded_sp.astype(np.float64)", "\n", "    ", "wav", "=", "pyworld", ".", "synthesize", "(", "f0", ",", "decoded_sp", ",", "ap", ",", "fs", ",", "frame_period", ")", "\n", "# Librosa could not save wav if not doing so", "\n", "wav", "=", "wav", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n", "return", "wav", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_synthesis_data": [[95, 104], ["list", "zip", "preprocess.world_speech_synthesis", "list.append"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.world_speech_synthesis"], ["", "def", "world_synthesis_data", "(", "f0s", ",", "decoded_sps", ",", "aps", ",", "fs", ",", "frame_period", ")", ":", "\n", "\n", "    ", "wavs", "=", "list", "(", ")", "\n", "\n", "for", "f0", ",", "decoded_sp", ",", "ap", "in", "zip", "(", "f0s", ",", "decoded_sps", ",", "aps", ")", ":", "\n", "        ", "wav", "=", "world_speech_synthesis", "(", "f0", ",", "decoded_sp", ",", "ap", ",", "fs", ",", "frame_period", ")", "\n", "wavs", ".", "append", "(", "wav", ")", "\n", "\n", "", "return", "wavs", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.coded_sps_normalization_fit_transoform": [[106, 117], ["numpy.concatenate", "numpy.mean", "numpy.std", "list", "list.append"], "function", ["None"], ["", "def", "coded_sps_normalization_fit_transoform", "(", "coded_sps", ")", ":", "\n", "\n", "    ", "coded_sps_concatenated", "=", "np", ".", "concatenate", "(", "coded_sps", ",", "axis", "=", "1", ")", "\n", "coded_sps_mean", "=", "np", ".", "mean", "(", "coded_sps_concatenated", ",", "axis", "=", "1", ",", "keepdims", "=", "True", ")", "\n", "coded_sps_std", "=", "np", ".", "std", "(", "coded_sps_concatenated", ",", "axis", "=", "1", ",", "keepdims", "=", "True", ")", "\n", "\n", "coded_sps_normalized", "=", "list", "(", ")", "\n", "for", "coded_sp", "in", "coded_sps", ":", "\n", "        ", "coded_sps_normalized", ".", "append", "(", "(", "coded_sp", "-", "coded_sps_mean", ")", "/", "coded_sps_std", ")", "\n", "\n", "", "return", "coded_sps_normalized", ",", "coded_sps_mean", ",", "coded_sps_std", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.coded_sps_normalization_transoform": [[118, 125], ["list", "list.append"], "function", ["None"], ["", "def", "coded_sps_normalization_transoform", "(", "coded_sps", ",", "coded_sps_mean", ",", "coded_sps_std", ")", ":", "\n", "\n", "    ", "coded_sps_normalized", "=", "list", "(", ")", "\n", "for", "coded_sp", "in", "coded_sps", ":", "\n", "        ", "coded_sps_normalized", ".", "append", "(", "(", "coded_sp", "-", "coded_sps_mean", ")", "/", "coded_sps_std", ")", "\n", "\n", "", "return", "coded_sps_normalized", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.coded_sps_normalization_inverse_transoform": [[126, 133], ["list", "list.append"], "function", ["None"], ["", "def", "coded_sps_normalization_inverse_transoform", "(", "normalized_coded_sps", ",", "coded_sps_mean", ",", "coded_sps_std", ")", ":", "\n", "\n", "    ", "coded_sps", "=", "list", "(", ")", "\n", "for", "normalized_coded_sp", "in", "normalized_coded_sps", ":", "\n", "        ", "coded_sps", ".", "append", "(", "normalized_coded_sp", "*", "coded_sps_std", "+", "coded_sps_mean", ")", "\n", "\n", "", "return", "coded_sps", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.coded_sp_padding": [[134, 145], ["numpy.pad", "int", "numpy.ceil"], "function", ["None"], ["", "def", "coded_sp_padding", "(", "coded_sp", ",", "multiple", "=", "4", ")", ":", "\n", "\n", "    ", "num_features", "=", "coded_sp", ".", "shape", "[", "0", "]", "\n", "num_frames", "=", "coded_sp", ".", "shape", "[", "1", "]", "\n", "num_frames_padded", "=", "int", "(", "np", ".", "ceil", "(", "num_frames", "/", "multiple", ")", ")", "*", "multiple", "\n", "num_frames_diff", "=", "num_frames_padded", "-", "num_frames", "\n", "num_pad_left", "=", "num_frames_diff", "//", "2", "\n", "num_pad_right", "=", "num_frames_diff", "-", "num_pad_left", "\n", "coded_sp_padded", "=", "np", ".", "pad", "(", "coded_sp", ",", "(", "(", "0", ",", "0", ")", ",", "(", "num_pad_left", ",", "num_pad_right", ")", ")", ",", "'constant'", ",", "constant_values", "=", "0", ")", "\n", "\n", "return", "coded_sp_padded", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.wav_padding": [[146, 157], ["len", "int", "numpy.pad", "numpy.ceil", "numpy.floor"], "function", ["None"], ["", "def", "wav_padding", "(", "wav", ",", "sr", ",", "frame_period", ",", "multiple", "=", "4", ")", ":", "\n", "\n", "    ", "assert", "wav", ".", "ndim", "==", "1", "\n", "num_frames", "=", "len", "(", "wav", ")", "\n", "num_frames_padded", "=", "int", "(", "(", "np", ".", "ceil", "(", "(", "np", ".", "floor", "(", "num_frames", "/", "(", "sr", "*", "frame_period", "/", "1000", ")", ")", "+", "1", ")", "/", "multiple", "+", "1", ")", "*", "multiple", "-", "1", ")", "*", "(", "sr", "*", "frame_period", "/", "1000", ")", ")", "\n", "num_frames_diff", "=", "num_frames_padded", "-", "num_frames", "\n", "num_pad_left", "=", "num_frames_diff", "//", "2", "\n", "num_pad_right", "=", "num_frames_diff", "-", "num_pad_left", "\n", "wav_padded", "=", "np", ".", "pad", "(", "wav", ",", "(", "num_pad_left", ",", "num_pad_right", ")", ",", "'constant'", ",", "constant_values", "=", "0", ")", "\n", "\n", "return", "wav_padded", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.logf0_statistics": [[159, 166], ["numpy.ma.log", "np.ma.log.mean", "np.ma.log.std", "numpy.concatenate"], "function", ["None"], ["", "def", "logf0_statistics", "(", "f0s", ")", ":", "\n", "\n", "    ", "log_f0s_concatenated", "=", "np", ".", "ma", ".", "log", "(", "np", ".", "concatenate", "(", "f0s", ")", ")", "\n", "log_f0s_mean", "=", "log_f0s_concatenated", ".", "mean", "(", ")", "\n", "log_f0s_std", "=", "log_f0s_concatenated", ".", "std", "(", ")", "\n", "\n", "return", "log_f0s_mean", ",", "log_f0s_std", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.pitch_conversion": [[167, 173], ["numpy.exp", "numpy.log"], "function", ["None"], ["", "def", "pitch_conversion", "(", "f0", ",", "mean_log_src", ",", "std_log_src", ",", "mean_log_target", ",", "std_log_target", ")", ":", "\n", "\n", "# Logarithm Gaussian normalization for Pitch Conversions", "\n", "    ", "f0_converted", "=", "np", ".", "exp", "(", "(", "np", ".", "log", "(", "f0", ")", "-", "mean_log_src", ")", "/", "std_log_src", "*", "std_log_target", "+", "mean_log_target", ")", "\n", "\n", "return", "f0_converted", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.wavs_to_specs": [[174, 182], ["list", "librosa.stft", "list.append"], "function", ["None"], ["", "def", "wavs_to_specs", "(", "wavs", ",", "n_fft", "=", "1024", ",", "hop_length", "=", "None", ")", ":", "\n", "\n", "    ", "stfts", "=", "list", "(", ")", "\n", "for", "wav", "in", "wavs", ":", "\n", "        ", "stft", "=", "librosa", ".", "stft", "(", "wav", ",", "n_fft", "=", "n_fft", ",", "hop_length", "=", "hop_length", ")", "\n", "stfts", ".", "append", "(", "stft", ")", "\n", "\n", "", "return", "stfts", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.wavs_to_mfccs": [[184, 192], ["list", "librosa.feature.mfcc", "list.append"], "function", ["None"], ["", "def", "wavs_to_mfccs", "(", "wavs", ",", "sr", ",", "n_fft", "=", "1024", ",", "hop_length", "=", "None", ",", "n_mels", "=", "128", ",", "n_mfcc", "=", "24", ")", ":", "\n", "\n", "    ", "mfccs", "=", "list", "(", ")", "\n", "for", "wav", "in", "wavs", ":", "\n", "        ", "mfcc", "=", "librosa", ".", "feature", ".", "mfcc", "(", "y", "=", "wav", ",", "sr", "=", "sr", ",", "n_fft", "=", "n_fft", ",", "hop_length", "=", "hop_length", ",", "n_mels", "=", "n_mels", ",", "n_mfcc", "=", "n_mfcc", ")", "\n", "mfccs", ".", "append", "(", "mfcc", ")", "\n", "\n", "", "return", "mfccs", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.mfccs_normalization": [[194, 205], ["numpy.concatenate", "numpy.mean", "numpy.std", "list", "list.append"], "function", ["None"], ["", "def", "mfccs_normalization", "(", "mfccs", ")", ":", "\n", "\n", "    ", "mfccs_concatenated", "=", "np", ".", "concatenate", "(", "mfccs", ",", "axis", "=", "1", ")", "\n", "mfccs_mean", "=", "np", ".", "mean", "(", "mfccs_concatenated", ",", "axis", "=", "1", ",", "keepdims", "=", "True", ")", "\n", "mfccs_std", "=", "np", ".", "std", "(", "mfccs_concatenated", ",", "axis", "=", "1", ",", "keepdims", "=", "True", ")", "\n", "\n", "mfccs_normalized", "=", "list", "(", ")", "\n", "for", "mfcc", "in", "mfccs", ":", "\n", "        ", "mfccs_normalized", ".", "append", "(", "(", "mfcc", "-", "mfccs_mean", ")", "/", "mfccs_std", ")", "\n", "\n", "", "return", "mfccs_normalized", ",", "mfccs_mean", ",", "mfccs_std", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.preprocess.sample_train_data": [[207, 239], ["min", "numpy.arange", "numpy.arange", "numpy.random.shuffle", "numpy.random.shuffle", "list", "list", "zip", "numpy.array", "numpy.array", "len", "len", "len", "len", "numpy.random.randint", "np.array.append", "numpy.random.randint", "np.array.append"], "function", ["None"], ["", "def", "sample_train_data", "(", "dataset_A", ",", "dataset_B", ",", "n_frames", "=", "128", ")", ":", "\n", "\n", "    ", "num_samples", "=", "min", "(", "len", "(", "dataset_A", ")", ",", "len", "(", "dataset_B", ")", ")", "\n", "train_data_A_idx", "=", "np", ".", "arange", "(", "len", "(", "dataset_A", ")", ")", "\n", "train_data_B_idx", "=", "np", ".", "arange", "(", "len", "(", "dataset_B", ")", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "train_data_A_idx", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "train_data_B_idx", ")", "\n", "train_data_A_idx_subset", "=", "train_data_A_idx", "[", ":", "num_samples", "]", "\n", "train_data_B_idx_subset", "=", "train_data_B_idx", "[", ":", "num_samples", "]", "\n", "\n", "train_data_A", "=", "list", "(", ")", "\n", "train_data_B", "=", "list", "(", ")", "\n", "\n", "for", "idx_A", ",", "idx_B", "in", "zip", "(", "train_data_A_idx_subset", ",", "train_data_B_idx_subset", ")", ":", "\n", "        ", "data_A", "=", "dataset_A", "[", "idx_A", "]", "\n", "frames_A_total", "=", "data_A", ".", "shape", "[", "1", "]", "\n", "assert", "frames_A_total", ">=", "n_frames", "\n", "start_A", "=", "np", ".", "random", ".", "randint", "(", "frames_A_total", "-", "n_frames", "+", "1", ")", "\n", "end_A", "=", "start_A", "+", "n_frames", "\n", "train_data_A", ".", "append", "(", "data_A", "[", ":", ",", "start_A", ":", "end_A", "]", ")", "\n", "\n", "data_B", "=", "dataset_B", "[", "idx_B", "]", "\n", "frames_B_total", "=", "data_B", ".", "shape", "[", "1", "]", "\n", "assert", "frames_B_total", ">=", "n_frames", "\n", "start_B", "=", "np", ".", "random", ".", "randint", "(", "frames_B_total", "-", "n_frames", "+", "1", ")", "\n", "end_B", "=", "start_B", "+", "n_frames", "\n", "train_data_B", ".", "append", "(", "data_B", "[", ":", ",", "start_B", ":", "end_B", "]", ")", "\n", "\n", "", "train_data_A", "=", "np", ".", "array", "(", "train_data_A", ")", "\n", "train_data_B", "=", "np", ".", "array", "(", "train_data_B", ")", "\n", "\n", "return", "train_data_A", ",", "train_data_B", "", "", ""]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model.CycleGAN.__init__": [[9, 31], ["model.CycleGAN.build_model", "model.CycleGAN.optimizer_initializer", "tensorflow.train.Saver", "tensorflow.Session", "model.CycleGAN.sess.run", "tensorflow.global_variables_initializer", "datetime.datetime.datetime.now", "os.path.join", "tensorflow.summary.FileWriter", "model.CycleGAN.summary", "datetime.datetime.now.strftime", "tensorflow.get_default_graph"], "methods", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.build_model", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.optimizer_initializer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.summary"], ["    ", "def", "__init__", "(", "self", ",", "num_features", ",", "discriminator", "=", "discriminator", ",", "generator", "=", "generator_gatedcnn", ",", "mode", "=", "'train'", ",", "log_dir", "=", "'./log'", ")", ":", "\n", "\n", "        ", "self", ".", "num_features", "=", "num_features", "\n", "self", ".", "input_shape", "=", "[", "None", ",", "num_features", ",", "None", "]", "# [batch_size, num_features, num_frames]", "\n", "\n", "self", ".", "discriminator", "=", "discriminator", "\n", "self", ".", "generator", "=", "generator", "\n", "self", ".", "mode", "=", "mode", "\n", "\n", "self", ".", "build_model", "(", ")", "\n", "self", ".", "optimizer_initializer", "(", ")", "\n", "\n", "self", ".", "saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "\n", "self", ".", "sess", "=", "tf", ".", "Session", "(", ")", "\n", "self", ".", "sess", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "\n", "if", "self", ".", "mode", "==", "'train'", ":", "\n", "            ", "self", ".", "train_step", "=", "0", "\n", "now", "=", "datetime", ".", "now", "(", ")", "\n", "self", ".", "log_dir", "=", "os", ".", "path", ".", "join", "(", "log_dir", ",", "now", ".", "strftime", "(", "'%Y%m%d-%H%M%S'", ")", ")", "\n", "self", ".", "writer", "=", "tf", ".", "summary", ".", "FileWriter", "(", "self", ".", "log_dir", ",", "tf", ".", "get_default_graph", "(", ")", ")", "\n", "self", ".", "generator_summaries", ",", "self", ".", "discriminator_summaries", "=", "self", ".", "summary", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model.CycleGAN.build_model": [[32, 101], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "model.CycleGAN.generator", "model.CycleGAN.generator", "model.CycleGAN.generator", "model.CycleGAN.generator", "model.CycleGAN.generator", "model.CycleGAN.generator", "model.CycleGAN.discriminator", "model.CycleGAN.discriminator", "tensorflow.placeholder", "tensorflow.placeholder", "utils.l2_loss", "utils.l2_loss", "model.CycleGAN.discriminator", "model.CycleGAN.discriminator", "model.CycleGAN.discriminator", "model.CycleGAN.discriminator", "utils.l2_loss", "utils.l2_loss", "utils.l2_loss", "utils.l2_loss", "tensorflow.trainable_variables", "model.CycleGAN.generator", "model.CycleGAN.generator", "utils.l1_loss", "utils.l1_loss", "utils.l1_loss", "utils.l1_loss", "tensorflow.ones_like", "tensorflow.ones_like", "tensorflow.ones_like", "tensorflow.zeros_like", "tensorflow.ones_like", "tensorflow.zeros_like"], "methods", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.discriminator", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.discriminator", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l2_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l2_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.discriminator", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.discriminator", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.discriminator", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.discriminator", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l2_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l2_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l2_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l2_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l1_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l1_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l1_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l1_loss"], ["", "", "def", "build_model", "(", "self", ")", ":", "\n", "\n", "# Placeholders for real training samples", "\n", "        ", "self", ".", "input_A_real", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "self", ".", "input_shape", ",", "name", "=", "'input_A_real'", ")", "\n", "self", ".", "input_B_real", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "self", ".", "input_shape", ",", "name", "=", "'input_B_real'", ")", "\n", "# Placeholders for fake generated samples", "\n", "self", ".", "input_A_fake", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "self", ".", "input_shape", ",", "name", "=", "'input_A_fake'", ")", "\n", "self", ".", "input_B_fake", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "self", ".", "input_shape", ",", "name", "=", "'input_B_fake'", ")", "\n", "# Placeholder for test samples", "\n", "self", ".", "input_A_test", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "self", ".", "input_shape", ",", "name", "=", "'input_A_test'", ")", "\n", "self", ".", "input_B_test", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "self", ".", "input_shape", ",", "name", "=", "'input_B_test'", ")", "\n", "\n", "self", ".", "generation_B", "=", "self", ".", "generator", "(", "inputs", "=", "self", ".", "input_A_real", ",", "reuse", "=", "False", ",", "scope_name", "=", "'generator_A2B'", ")", "\n", "self", ".", "cycle_A", "=", "self", ".", "generator", "(", "inputs", "=", "self", ".", "generation_B", ",", "reuse", "=", "False", ",", "scope_name", "=", "'generator_B2A'", ")", "\n", "\n", "self", ".", "generation_A", "=", "self", ".", "generator", "(", "inputs", "=", "self", ".", "input_B_real", ",", "reuse", "=", "True", ",", "scope_name", "=", "'generator_B2A'", ")", "\n", "self", ".", "cycle_B", "=", "self", ".", "generator", "(", "inputs", "=", "self", ".", "generation_A", ",", "reuse", "=", "True", ",", "scope_name", "=", "'generator_A2B'", ")", "\n", "\n", "self", ".", "generation_A_identity", "=", "self", ".", "generator", "(", "inputs", "=", "self", ".", "input_A_real", ",", "reuse", "=", "True", ",", "scope_name", "=", "'generator_B2A'", ")", "\n", "self", ".", "generation_B_identity", "=", "self", ".", "generator", "(", "inputs", "=", "self", ".", "input_B_real", ",", "reuse", "=", "True", ",", "scope_name", "=", "'generator_A2B'", ")", "\n", "\n", "self", ".", "discrimination_A_fake", "=", "self", ".", "discriminator", "(", "inputs", "=", "self", ".", "generation_A", ",", "reuse", "=", "False", ",", "scope_name", "=", "'discriminator_A'", ")", "\n", "self", ".", "discrimination_B_fake", "=", "self", ".", "discriminator", "(", "inputs", "=", "self", ".", "generation_B", ",", "reuse", "=", "False", ",", "scope_name", "=", "'discriminator_B'", ")", "\n", "\n", "# Cycle loss", "\n", "self", ".", "cycle_loss", "=", "l1_loss", "(", "y", "=", "self", ".", "input_A_real", ",", "y_hat", "=", "self", ".", "cycle_A", ")", "+", "l1_loss", "(", "y", "=", "self", ".", "input_B_real", ",", "y_hat", "=", "self", ".", "cycle_B", ")", "\n", "\n", "# Identity loss", "\n", "self", ".", "identity_loss", "=", "l1_loss", "(", "y", "=", "self", ".", "input_A_real", ",", "y_hat", "=", "self", ".", "generation_A_identity", ")", "+", "l1_loss", "(", "y", "=", "self", ".", "input_B_real", ",", "y_hat", "=", "self", ".", "generation_B_identity", ")", "\n", "\n", "# Place holder for lambda_cycle and lambda_identity", "\n", "self", ".", "lambda_cycle", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "None", ",", "name", "=", "'lambda_cycle'", ")", "\n", "self", ".", "lambda_identity", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "None", ",", "name", "=", "'lambda_identity'", ")", "\n", "\n", "# Generator loss", "\n", "# Generator wants to fool discriminator", "\n", "self", ".", "generator_loss_A2B", "=", "l2_loss", "(", "y", "=", "tf", ".", "ones_like", "(", "self", ".", "discrimination_B_fake", ")", ",", "y_hat", "=", "self", ".", "discrimination_B_fake", ")", "\n", "self", ".", "generator_loss_B2A", "=", "l2_loss", "(", "y", "=", "tf", ".", "ones_like", "(", "self", ".", "discrimination_A_fake", ")", ",", "y_hat", "=", "self", ".", "discrimination_A_fake", ")", "\n", "\n", "# Merge the two generators and the cycle loss", "\n", "self", ".", "generator_loss", "=", "self", ".", "generator_loss_A2B", "+", "self", ".", "generator_loss_B2A", "+", "self", ".", "lambda_cycle", "*", "self", ".", "cycle_loss", "+", "self", ".", "lambda_identity", "*", "self", ".", "identity_loss", "\n", "\n", "# Discriminator loss", "\n", "self", ".", "discrimination_input_A_real", "=", "self", ".", "discriminator", "(", "inputs", "=", "self", ".", "input_A_real", ",", "reuse", "=", "True", ",", "scope_name", "=", "'discriminator_A'", ")", "\n", "self", ".", "discrimination_input_B_real", "=", "self", ".", "discriminator", "(", "inputs", "=", "self", ".", "input_B_real", ",", "reuse", "=", "True", ",", "scope_name", "=", "'discriminator_B'", ")", "\n", "self", ".", "discrimination_input_A_fake", "=", "self", ".", "discriminator", "(", "inputs", "=", "self", ".", "input_A_fake", ",", "reuse", "=", "True", ",", "scope_name", "=", "'discriminator_A'", ")", "\n", "self", ".", "discrimination_input_B_fake", "=", "self", ".", "discriminator", "(", "inputs", "=", "self", ".", "input_B_fake", ",", "reuse", "=", "True", ",", "scope_name", "=", "'discriminator_B'", ")", "\n", "\n", "# Discriminator wants to classify real and fake correctly", "\n", "self", ".", "discriminator_loss_input_A_real", "=", "l2_loss", "(", "y", "=", "tf", ".", "ones_like", "(", "self", ".", "discrimination_input_A_real", ")", ",", "y_hat", "=", "self", ".", "discrimination_input_A_real", ")", "\n", "self", ".", "discriminator_loss_input_A_fake", "=", "l2_loss", "(", "y", "=", "tf", ".", "zeros_like", "(", "self", ".", "discrimination_input_A_fake", ")", ",", "y_hat", "=", "self", ".", "discrimination_input_A_fake", ")", "\n", "self", ".", "discriminator_loss_A", "=", "(", "self", ".", "discriminator_loss_input_A_real", "+", "self", ".", "discriminator_loss_input_A_fake", ")", "/", "2", "\n", "\n", "self", ".", "discriminator_loss_input_B_real", "=", "l2_loss", "(", "y", "=", "tf", ".", "ones_like", "(", "self", ".", "discrimination_input_B_real", ")", ",", "y_hat", "=", "self", ".", "discrimination_input_B_real", ")", "\n", "self", ".", "discriminator_loss_input_B_fake", "=", "l2_loss", "(", "y", "=", "tf", ".", "zeros_like", "(", "self", ".", "discrimination_input_B_fake", ")", ",", "y_hat", "=", "self", ".", "discrimination_input_B_fake", ")", "\n", "self", ".", "discriminator_loss_B", "=", "(", "self", ".", "discriminator_loss_input_B_real", "+", "self", ".", "discriminator_loss_input_B_fake", ")", "/", "2", "\n", "\n", "# Merge the two discriminators into one", "\n", "self", ".", "discriminator_loss", "=", "self", ".", "discriminator_loss_A", "+", "self", ".", "discriminator_loss_B", "\n", "\n", "# Categorize variables because we have to optimize the two sets of the variables separately", "\n", "trainable_variables", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "self", ".", "discriminator_vars", "=", "[", "var", "for", "var", "in", "trainable_variables", "if", "'discriminator'", "in", "var", ".", "name", "]", "\n", "self", ".", "generator_vars", "=", "[", "var", "for", "var", "in", "trainable_variables", "if", "'generator'", "in", "var", ".", "name", "]", "\n", "#for var in t_vars: print(var.name)", "\n", "\n", "# Reserved for test", "\n", "self", ".", "generation_B_test", "=", "self", ".", "generator", "(", "inputs", "=", "self", ".", "input_A_test", ",", "reuse", "=", "True", ",", "scope_name", "=", "'generator_A2B'", ")", "\n", "self", ".", "generation_A_test", "=", "self", ".", "generator", "(", "inputs", "=", "self", ".", "input_B_test", ",", "reuse", "=", "True", ",", "scope_name", "=", "'generator_B2A'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model.CycleGAN.optimizer_initializer": [[103, 109], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.train.AdamOptimizer().minimize", "tensorflow.train.AdamOptimizer().minimize", "tensorflow.train.AdamOptimizer", "tensorflow.train.AdamOptimizer"], "methods", ["None"], ["", "def", "optimizer_initializer", "(", "self", ")", ":", "\n", "\n", "        ", "self", ".", "generator_learning_rate", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "None", ",", "name", "=", "'generator_learning_rate'", ")", "\n", "self", ".", "discriminator_learning_rate", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "None", ",", "name", "=", "'discriminator_learning_rate'", ")", "\n", "self", ".", "discriminator_optimizer", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "self", ".", "discriminator_learning_rate", ",", "beta1", "=", "0.5", ")", ".", "minimize", "(", "self", ".", "discriminator_loss", ",", "var_list", "=", "self", ".", "discriminator_vars", ")", "\n", "self", ".", "generator_optimizer", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "self", ".", "generator_learning_rate", ",", "beta1", "=", "0.5", ")", ".", "minimize", "(", "self", ".", "generator_loss", ",", "var_list", "=", "self", ".", "generator_vars", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model.CycleGAN.train": [[110, 126], ["model.CycleGAN.sess.run", "model.CycleGAN.writer.add_summary", "model.CycleGAN.sess.run", "model.CycleGAN.writer.add_summary"], "methods", ["None"], ["", "def", "train", "(", "self", ",", "input_A", ",", "input_B", ",", "lambda_cycle", ",", "lambda_identity", ",", "generator_learning_rate", ",", "discriminator_learning_rate", ")", ":", "\n", "\n", "        ", "generation_A", ",", "generation_B", ",", "generator_loss", ",", "_", ",", "generator_summaries", "=", "self", ".", "sess", ".", "run", "(", "\n", "[", "self", ".", "generation_A", ",", "self", ".", "generation_B", ",", "self", ".", "generator_loss", ",", "self", ".", "generator_optimizer", ",", "self", ".", "generator_summaries", "]", ",", "feed_dict", "=", "{", "self", ".", "lambda_cycle", ":", "lambda_cycle", ",", "self", ".", "lambda_identity", ":", "lambda_identity", ",", "self", ".", "input_A_real", ":", "input_A", ",", "self", ".", "input_B_real", ":", "input_B", ",", "self", ".", "generator_learning_rate", ":", "generator_learning_rate", "}", ")", "\n", "\n", "self", ".", "writer", ".", "add_summary", "(", "generator_summaries", ",", "self", ".", "train_step", ")", "\n", "\n", "discriminator_loss", ",", "_", ",", "discriminator_summaries", "=", "self", ".", "sess", ".", "run", "(", "[", "self", ".", "discriminator_loss", ",", "self", ".", "discriminator_optimizer", ",", "self", ".", "discriminator_summaries", "]", ",", "feed_dict", "=", "{", "self", ".", "input_A_real", ":", "input_A", ",", "self", ".", "input_B_real", ":", "input_B", ",", "self", ".", "discriminator_learning_rate", ":", "discriminator_learning_rate", ",", "self", ".", "input_A_fake", ":", "generation_A", ",", "self", ".", "input_B_fake", ":", "generation_B", "}", ")", "\n", "\n", "self", ".", "writer", ".", "add_summary", "(", "discriminator_summaries", ",", "self", ".", "train_step", ")", "\n", "\n", "self", ".", "train_step", "+=", "1", "\n", "\n", "return", "generator_loss", ",", "discriminator_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model.CycleGAN.test": [[128, 138], ["model.CycleGAN.sess.run", "model.CycleGAN.sess.run", "Exception"], "methods", ["None"], ["", "def", "test", "(", "self", ",", "inputs", ",", "direction", ")", ":", "\n", "\n", "        ", "if", "direction", "==", "'A2B'", ":", "\n", "            ", "generation", "=", "self", ".", "sess", ".", "run", "(", "self", ".", "generation_B_test", ",", "feed_dict", "=", "{", "self", ".", "input_A_test", ":", "inputs", "}", ")", "\n", "", "elif", "direction", "==", "'B2A'", ":", "\n", "            ", "generation", "=", "self", ".", "sess", ".", "run", "(", "self", ".", "generation_A_test", ",", "feed_dict", "=", "{", "self", ".", "input_B_test", ":", "inputs", "}", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "'Conversion direction must be specified.'", ")", "\n", "\n", "", "return", "generation", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model.CycleGAN.save": [[140, 147], ["model.CycleGAN.saver.save", "os.path.join", "os.path.exists", "os.makedirs", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.save"], ["", "def", "save", "(", "self", ",", "directory", ",", "filename", ")", ":", "\n", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "directory", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "directory", ")", "\n", "", "self", ".", "saver", ".", "save", "(", "self", ".", "sess", ",", "os", ".", "path", ".", "join", "(", "directory", ",", "filename", ")", ")", "\n", "\n", "return", "os", ".", "path", ".", "join", "(", "directory", ",", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model.CycleGAN.load": [[148, 151], ["model.CycleGAN.saver.restore"], "methods", ["None"], ["", "def", "load", "(", "self", ",", "filepath", ")", ":", "\n", "\n", "        ", "self", ".", "saver", ".", "restore", "(", "self", ".", "sess", ",", "filepath", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model.CycleGAN.summary": [[153, 170], ["tensorflow.name_scope", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.merge", "tensorflow.name_scope", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.merge"], "methods", ["None"], ["", "def", "summary", "(", "self", ")", ":", "\n", "\n", "        ", "with", "tf", ".", "name_scope", "(", "'generator_summaries'", ")", ":", "\n", "            ", "cycle_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "'cycle_loss'", ",", "self", ".", "cycle_loss", ")", "\n", "identity_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "'identity_loss'", ",", "self", ".", "identity_loss", ")", "\n", "generator_loss_A2B_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "'generator_loss_A2B'", ",", "self", ".", "generator_loss_A2B", ")", "\n", "generator_loss_B2A_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "'generator_loss_B2A'", ",", "self", ".", "generator_loss_B2A", ")", "\n", "generator_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "'generator_loss'", ",", "self", ".", "generator_loss", ")", "\n", "generator_summaries", "=", "tf", ".", "summary", ".", "merge", "(", "[", "cycle_loss_summary", ",", "identity_loss_summary", ",", "generator_loss_A2B_summary", ",", "generator_loss_B2A_summary", ",", "generator_loss_summary", "]", ")", "\n", "\n", "", "with", "tf", ".", "name_scope", "(", "'discriminator_summaries'", ")", ":", "\n", "            ", "discriminator_loss_A_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "'discriminator_loss_A'", ",", "self", ".", "discriminator_loss_A", ")", "\n", "discriminator_loss_B_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "'discriminator_loss_B'", ",", "self", ".", "discriminator_loss_B", ")", "\n", "discriminator_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "'discriminator_loss'", ",", "self", ".", "discriminator_loss", ")", "\n", "discriminator_summaries", "=", "tf", ".", "summary", ".", "merge", "(", "[", "discriminator_loss_A_summary", ",", "discriminator_loss_B_summary", ",", "discriminator_loss_summary", "]", ")", "\n", "\n", "", "return", "generator_summaries", ",", "discriminator_summaries", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.__init__": [[9, 31], ["model_f0.CycleGAN.build_model", "model_f0.CycleGAN.optimizer_initializer", "tensorflow.train.Saver", "tensorflow.Session", "model_f0.CycleGAN.sess.run", "tensorflow.global_variables_initializer", "datetime.datetime.datetime.now", "os.path.join", "tensorflow.summary.FileWriter", "model_f0.CycleGAN.summary", "datetime.datetime.now.strftime", "tensorflow.get_default_graph"], "methods", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.build_model", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.optimizer_initializer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.summary"], ["    ", "def", "__init__", "(", "self", ",", "num_features", ",", "discriminator", "=", "discriminator", ",", "generator", "=", "generator_gatedcnn", ",", "mode", "=", "'train'", ",", "log_dir", "=", "'./log'", ")", ":", "\n", "\n", "        ", "self", ".", "num_features", "=", "num_features", "\n", "self", ".", "input_shape", "=", "[", "None", ",", "num_features", ",", "None", "]", "# [batch_size, num_features, num_frames]", "\n", "\n", "self", ".", "discriminator", "=", "discriminator", "\n", "self", ".", "generator", "=", "generator", "\n", "self", ".", "mode", "=", "mode", "\n", "\n", "self", ".", "build_model", "(", ")", "\n", "self", ".", "optimizer_initializer", "(", ")", "\n", "\n", "self", ".", "saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "\n", "self", ".", "sess", "=", "tf", ".", "Session", "(", ")", "\n", "self", ".", "sess", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "\n", "if", "self", ".", "mode", "==", "'train'", ":", "\n", "            ", "self", ".", "train_step", "=", "0", "\n", "now", "=", "datetime", ".", "now", "(", ")", "\n", "self", ".", "log_dir", "=", "os", ".", "path", ".", "join", "(", "log_dir", ",", "now", ".", "strftime", "(", "'%Y%m%d-%H%M%S'", ")", ")", "\n", "self", ".", "writer", "=", "tf", ".", "summary", ".", "FileWriter", "(", "self", ".", "log_dir", ",", "tf", ".", "get_default_graph", "(", ")", ")", "\n", "self", ".", "generator_summaries", ",", "self", ".", "discriminator_summaries", "=", "self", ".", "summary", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.build_model": [[32, 101], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "model_f0.CycleGAN.generator", "model_f0.CycleGAN.generator", "model_f0.CycleGAN.generator", "model_f0.CycleGAN.generator", "model_f0.CycleGAN.generator", "model_f0.CycleGAN.generator", "model_f0.CycleGAN.discriminator", "model_f0.CycleGAN.discriminator", "tensorflow.placeholder", "tensorflow.placeholder", "utils.l2_loss", "utils.l2_loss", "model_f0.CycleGAN.discriminator", "model_f0.CycleGAN.discriminator", "model_f0.CycleGAN.discriminator", "model_f0.CycleGAN.discriminator", "utils.l2_loss", "utils.l2_loss", "utils.l2_loss", "utils.l2_loss", "tensorflow.trainable_variables", "model_f0.CycleGAN.generator", "model_f0.CycleGAN.generator", "utils.l1_loss", "utils.l1_loss", "utils.l1_loss", "utils.l1_loss", "tensorflow.ones_like", "tensorflow.ones_like", "tensorflow.ones_like", "tensorflow.zeros_like", "tensorflow.ones_like", "tensorflow.zeros_like"], "methods", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.discriminator", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.discriminator", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l2_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l2_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.discriminator", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.discriminator", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.discriminator", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.discriminator", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l2_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l2_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l2_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l2_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l1_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l1_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l1_loss", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l1_loss"], ["", "", "def", "build_model", "(", "self", ")", ":", "\n", "\n", "# Placeholders for real training samples", "\n", "        ", "self", ".", "input_A_real", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "self", ".", "input_shape", ",", "name", "=", "'input_A_real'", ")", "\n", "self", ".", "input_B_real", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "self", ".", "input_shape", ",", "name", "=", "'input_B_real'", ")", "\n", "# Placeholders for fake generated samples", "\n", "self", ".", "input_A_fake", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "self", ".", "input_shape", ",", "name", "=", "'input_A_fake'", ")", "\n", "self", ".", "input_B_fake", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "self", ".", "input_shape", ",", "name", "=", "'input_B_fake'", ")", "\n", "# Placeholder for test samples", "\n", "self", ".", "input_A_test", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "self", ".", "input_shape", ",", "name", "=", "'input_A_test'", ")", "\n", "self", ".", "input_B_test", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "self", ".", "input_shape", ",", "name", "=", "'input_B_test'", ")", "\n", "\n", "self", ".", "generation_B", "=", "self", ".", "generator", "(", "inputs", "=", "self", ".", "input_A_real", ",", "reuse", "=", "False", ",", "scope_name", "=", "'generator_A2B'", ")", "\n", "self", ".", "cycle_A", "=", "self", ".", "generator", "(", "inputs", "=", "self", ".", "generation_B", ",", "reuse", "=", "False", ",", "scope_name", "=", "'generator_B2A'", ")", "\n", "\n", "self", ".", "generation_A", "=", "self", ".", "generator", "(", "inputs", "=", "self", ".", "input_B_real", ",", "reuse", "=", "True", ",", "scope_name", "=", "'generator_B2A'", ")", "\n", "self", ".", "cycle_B", "=", "self", ".", "generator", "(", "inputs", "=", "self", ".", "generation_A", ",", "reuse", "=", "True", ",", "scope_name", "=", "'generator_A2B'", ")", "\n", "\n", "self", ".", "generation_A_identity", "=", "self", ".", "generator", "(", "inputs", "=", "self", ".", "input_A_real", ",", "reuse", "=", "True", ",", "scope_name", "=", "'generator_B2A'", ")", "\n", "self", ".", "generation_B_identity", "=", "self", ".", "generator", "(", "inputs", "=", "self", ".", "input_B_real", ",", "reuse", "=", "True", ",", "scope_name", "=", "'generator_A2B'", ")", "\n", "\n", "self", ".", "discrimination_A_fake", "=", "self", ".", "discriminator", "(", "inputs", "=", "self", ".", "generation_A", ",", "reuse", "=", "False", ",", "scope_name", "=", "'discriminator_A'", ")", "\n", "self", ".", "discrimination_B_fake", "=", "self", ".", "discriminator", "(", "inputs", "=", "self", ".", "generation_B", ",", "reuse", "=", "False", ",", "scope_name", "=", "'discriminator_B'", ")", "\n", "\n", "# Cycle loss", "\n", "self", ".", "cycle_loss", "=", "l1_loss", "(", "y", "=", "self", ".", "input_A_real", ",", "y_hat", "=", "self", ".", "cycle_A", ")", "+", "l1_loss", "(", "y", "=", "self", ".", "input_B_real", ",", "y_hat", "=", "self", ".", "cycle_B", ")", "\n", "\n", "# Identity loss", "\n", "self", ".", "identity_loss", "=", "l1_loss", "(", "y", "=", "self", ".", "input_A_real", ",", "y_hat", "=", "self", ".", "generation_A_identity", ")", "+", "l1_loss", "(", "y", "=", "self", ".", "input_B_real", ",", "y_hat", "=", "self", ".", "generation_B_identity", ")", "\n", "\n", "# Place holder for lambda_cycle and lambda_identity", "\n", "self", ".", "lambda_cycle", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "None", ",", "name", "=", "'lambda_cycle'", ")", "\n", "self", ".", "lambda_identity", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "None", ",", "name", "=", "'lambda_identity'", ")", "\n", "\n", "# Generator loss", "\n", "# Generator wants to fool discriminator", "\n", "self", ".", "generator_loss_A2B", "=", "l2_loss", "(", "y", "=", "tf", ".", "ones_like", "(", "self", ".", "discrimination_B_fake", ")", ",", "y_hat", "=", "self", ".", "discrimination_B_fake", ")", "\n", "self", ".", "generator_loss_B2A", "=", "l2_loss", "(", "y", "=", "tf", ".", "ones_like", "(", "self", ".", "discrimination_A_fake", ")", ",", "y_hat", "=", "self", ".", "discrimination_A_fake", ")", "\n", "\n", "# Merge the two generators and the cycle loss", "\n", "self", ".", "generator_loss", "=", "self", ".", "generator_loss_A2B", "+", "self", ".", "generator_loss_B2A", "+", "self", ".", "lambda_cycle", "*", "self", ".", "cycle_loss", "+", "self", ".", "lambda_identity", "*", "self", ".", "identity_loss", "\n", "\n", "# Discriminator loss", "\n", "self", ".", "discrimination_input_A_real", "=", "self", ".", "discriminator", "(", "inputs", "=", "self", ".", "input_A_real", ",", "reuse", "=", "True", ",", "scope_name", "=", "'discriminator_A'", ")", "\n", "self", ".", "discrimination_input_B_real", "=", "self", ".", "discriminator", "(", "inputs", "=", "self", ".", "input_B_real", ",", "reuse", "=", "True", ",", "scope_name", "=", "'discriminator_B'", ")", "\n", "self", ".", "discrimination_input_A_fake", "=", "self", ".", "discriminator", "(", "inputs", "=", "self", ".", "input_A_fake", ",", "reuse", "=", "True", ",", "scope_name", "=", "'discriminator_A'", ")", "\n", "self", ".", "discrimination_input_B_fake", "=", "self", ".", "discriminator", "(", "inputs", "=", "self", ".", "input_B_fake", ",", "reuse", "=", "True", ",", "scope_name", "=", "'discriminator_B'", ")", "\n", "\n", "# Discriminator wants to classify real and fake correctly", "\n", "self", ".", "discriminator_loss_input_A_real", "=", "l2_loss", "(", "y", "=", "tf", ".", "ones_like", "(", "self", ".", "discrimination_input_A_real", ")", ",", "y_hat", "=", "self", ".", "discrimination_input_A_real", ")", "\n", "self", ".", "discriminator_loss_input_A_fake", "=", "l2_loss", "(", "y", "=", "tf", ".", "zeros_like", "(", "self", ".", "discrimination_input_A_fake", ")", ",", "y_hat", "=", "self", ".", "discrimination_input_A_fake", ")", "\n", "self", ".", "discriminator_loss_A", "=", "(", "self", ".", "discriminator_loss_input_A_real", "+", "self", ".", "discriminator_loss_input_A_fake", ")", "/", "2", "\n", "\n", "self", ".", "discriminator_loss_input_B_real", "=", "l2_loss", "(", "y", "=", "tf", ".", "ones_like", "(", "self", ".", "discrimination_input_B_real", ")", ",", "y_hat", "=", "self", ".", "discrimination_input_B_real", ")", "\n", "self", ".", "discriminator_loss_input_B_fake", "=", "l2_loss", "(", "y", "=", "tf", ".", "zeros_like", "(", "self", ".", "discrimination_input_B_fake", ")", ",", "y_hat", "=", "self", ".", "discrimination_input_B_fake", ")", "\n", "self", ".", "discriminator_loss_B", "=", "(", "self", ".", "discriminator_loss_input_B_real", "+", "self", ".", "discriminator_loss_input_B_fake", ")", "/", "2", "\n", "\n", "# Merge the two discriminators into one", "\n", "self", ".", "discriminator_loss", "=", "self", ".", "discriminator_loss_A", "+", "self", ".", "discriminator_loss_B", "\n", "\n", "# Categorize variables because we have to optimize the two sets of the variables separately", "\n", "trainable_variables", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "self", ".", "discriminator_vars", "=", "[", "var", "for", "var", "in", "trainable_variables", "if", "'discriminator'", "in", "var", ".", "name", "]", "\n", "self", ".", "generator_vars", "=", "[", "var", "for", "var", "in", "trainable_variables", "if", "'generator'", "in", "var", ".", "name", "]", "\n", "#for var in t_vars: print(var.name)", "\n", "\n", "# Reserved for test", "\n", "self", ".", "generation_B_test", "=", "self", ".", "generator", "(", "inputs", "=", "self", ".", "input_A_test", ",", "reuse", "=", "True", ",", "scope_name", "=", "'generator_A2B'", ")", "\n", "self", ".", "generation_A_test", "=", "self", ".", "generator", "(", "inputs", "=", "self", ".", "input_B_test", ",", "reuse", "=", "True", ",", "scope_name", "=", "'generator_B2A'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.optimizer_initializer": [[103, 109], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.train.AdamOptimizer().minimize", "tensorflow.train.AdamOptimizer().minimize", "tensorflow.train.AdamOptimizer", "tensorflow.train.AdamOptimizer"], "methods", ["None"], ["", "def", "optimizer_initializer", "(", "self", ")", ":", "\n", "\n", "        ", "self", ".", "generator_learning_rate", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "None", ",", "name", "=", "'generator_learning_rate'", ")", "\n", "self", ".", "discriminator_learning_rate", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "None", ",", "name", "=", "'discriminator_learning_rate'", ")", "\n", "self", ".", "discriminator_optimizer", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "self", ".", "discriminator_learning_rate", ",", "beta1", "=", "0.5", ")", ".", "minimize", "(", "self", ".", "discriminator_loss", ",", "var_list", "=", "self", ".", "discriminator_vars", ")", "\n", "self", ".", "generator_optimizer", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "self", ".", "generator_learning_rate", ",", "beta1", "=", "0.5", ")", ".", "minimize", "(", "self", ".", "generator_loss", ",", "var_list", "=", "self", ".", "generator_vars", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.train": [[110, 126], ["model_f0.CycleGAN.sess.run", "model_f0.CycleGAN.writer.add_summary", "model_f0.CycleGAN.sess.run", "model_f0.CycleGAN.writer.add_summary"], "methods", ["None"], ["", "def", "train", "(", "self", ",", "input_A", ",", "input_B", ",", "lambda_cycle", ",", "lambda_identity", ",", "generator_learning_rate", ",", "discriminator_learning_rate", ")", ":", "\n", "\n", "        ", "generation_A", ",", "generation_B", ",", "generator_loss", ",", "_", ",", "generator_summaries", "=", "self", ".", "sess", ".", "run", "(", "\n", "[", "self", ".", "generation_A", ",", "self", ".", "generation_B", ",", "self", ".", "generator_loss", ",", "self", ".", "generator_optimizer", ",", "self", ".", "generator_summaries", "]", ",", "feed_dict", "=", "{", "self", ".", "lambda_cycle", ":", "lambda_cycle", ",", "self", ".", "lambda_identity", ":", "lambda_identity", ",", "self", ".", "input_A_real", ":", "input_A", ",", "self", ".", "input_B_real", ":", "input_B", ",", "self", ".", "generator_learning_rate", ":", "generator_learning_rate", "}", ")", "\n", "\n", "self", ".", "writer", ".", "add_summary", "(", "generator_summaries", ",", "self", ".", "train_step", ")", "\n", "\n", "discriminator_loss", ",", "_", ",", "discriminator_summaries", "=", "self", ".", "sess", ".", "run", "(", "[", "self", ".", "discriminator_loss", ",", "self", ".", "discriminator_optimizer", ",", "self", ".", "discriminator_summaries", "]", ",", "feed_dict", "=", "{", "self", ".", "input_A_real", ":", "input_A", ",", "self", ".", "input_B_real", ":", "input_B", ",", "self", ".", "discriminator_learning_rate", ":", "discriminator_learning_rate", ",", "self", ".", "input_A_fake", ":", "generation_A", ",", "self", ".", "input_B_fake", ":", "generation_B", "}", ")", "\n", "\n", "self", ".", "writer", ".", "add_summary", "(", "discriminator_summaries", ",", "self", ".", "train_step", ")", "\n", "\n", "self", ".", "train_step", "+=", "1", "\n", "\n", "return", "generator_loss", ",", "discriminator_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.test": [[128, 138], ["model_f0.CycleGAN.sess.run", "model_f0.CycleGAN.sess.run", "Exception"], "methods", ["None"], ["", "def", "test", "(", "self", ",", "inputs", ",", "direction", ")", ":", "\n", "\n", "        ", "if", "direction", "==", "'A2B'", ":", "\n", "            ", "generation", "=", "self", ".", "sess", ".", "run", "(", "self", ".", "generation_B_test", ",", "feed_dict", "=", "{", "self", ".", "input_A_test", ":", "inputs", "}", ")", "\n", "", "elif", "direction", "==", "'B2A'", ":", "\n", "            ", "generation", "=", "self", ".", "sess", ".", "run", "(", "self", ".", "generation_A_test", ",", "feed_dict", "=", "{", "self", ".", "input_B_test", ":", "inputs", "}", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "'Conversion direction must be specified.'", ")", "\n", "\n", "", "return", "generation", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.save": [[140, 147], ["model_f0.CycleGAN.saver.save", "os.path.join", "os.path.exists", "os.makedirs", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.save"], ["", "def", "save", "(", "self", ",", "directory", ",", "filename", ")", ":", "\n", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "directory", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "directory", ")", "\n", "", "self", ".", "saver", ".", "save", "(", "self", ".", "sess", ",", "os", ".", "path", ".", "join", "(", "directory", ",", "filename", ")", ")", "\n", "\n", "return", "os", ".", "path", ".", "join", "(", "directory", ",", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.load": [[148, 151], ["model_f0.CycleGAN.saver.restore"], "methods", ["None"], ["", "def", "load", "(", "self", ",", "filepath", ")", ":", "\n", "\n", "        ", "self", ".", "saver", ".", "restore", "(", "self", ".", "sess", ",", "filepath", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.model_f0.CycleGAN.summary": [[153, 170], ["tensorflow.name_scope", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.merge", "tensorflow.name_scope", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.merge"], "methods", ["None"], ["", "def", "summary", "(", "self", ")", ":", "\n", "\n", "        ", "with", "tf", ".", "name_scope", "(", "'generator_summaries'", ")", ":", "\n", "            ", "cycle_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "'cycle_loss'", ",", "self", ".", "cycle_loss", ")", "\n", "identity_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "'identity_loss'", ",", "self", ".", "identity_loss", ")", "\n", "generator_loss_A2B_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "'generator_loss_A2B'", ",", "self", ".", "generator_loss_A2B", ")", "\n", "generator_loss_B2A_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "'generator_loss_B2A'", ",", "self", ".", "generator_loss_B2A", ")", "\n", "generator_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "'generator_loss'", ",", "self", ".", "generator_loss", ")", "\n", "generator_summaries", "=", "tf", ".", "summary", ".", "merge", "(", "[", "cycle_loss_summary", ",", "identity_loss_summary", ",", "generator_loss_A2B_summary", ",", "generator_loss_B2A_summary", ",", "generator_loss_summary", "]", ")", "\n", "\n", "", "with", "tf", ".", "name_scope", "(", "'discriminator_summaries'", ")", ":", "\n", "            ", "discriminator_loss_A_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "'discriminator_loss_A'", ",", "self", ".", "discriminator_loss_A", ")", "\n", "discriminator_loss_B_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "'discriminator_loss_B'", ",", "self", ".", "discriminator_loss_B", ")", "\n", "discriminator_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "'discriminator_loss'", ",", "self", ".", "discriminator_loss", ")", "\n", "discriminator_summaries", "=", "tf", ".", "summary", ".", "merge", "(", "[", "discriminator_loss_A_summary", ",", "discriminator_loss_B_summary", ",", "discriminator_loss_summary", "]", ")", "\n", "\n", "", "return", "generator_summaries", ",", "discriminator_summaries", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.gated_linear_layer": [[3, 8], ["tensorflow.multiply", "tensorflow.sigmoid"], "function", ["None"], ["def", "gated_linear_layer", "(", "inputs", ",", "gates", ",", "name", "=", "None", ")", ":", "\n", "\n", "    ", "activation", "=", "tf", ".", "multiply", "(", "x", "=", "inputs", ",", "y", "=", "tf", ".", "sigmoid", "(", "gates", ")", ",", "name", "=", "name", ")", "\n", "\n", "return", "activation", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer": [[9, 21], ["tensorflow.contrib.layers.instance_norm"], "function", ["None"], ["", "def", "instance_norm_layer", "(", "\n", "inputs", ",", "\n", "epsilon", "=", "1e-06", ",", "\n", "activation_fn", "=", "None", ",", "\n", "name", "=", "None", ")", ":", "\n", "\n", "    ", "instance_norm_layer", "=", "tf", ".", "contrib", ".", "layers", ".", "instance_norm", "(", "\n", "inputs", "=", "inputs", ",", "\n", "epsilon", "=", "epsilon", ",", "\n", "activation_fn", "=", "activation_fn", ")", "\n", "\n", "return", "instance_norm_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer": [[22, 43], ["tensorflow.layers.conv1d"], "function", ["None"], ["", "def", "conv1d_layer", "(", "\n", "inputs", ",", "\n", "filters", ",", "\n", "kernel_size", ",", "\n", "strides", "=", "1", ",", "\n", "padding", "=", "'same'", ",", "\n", "activation", "=", "None", ",", "\n", "kernel_initializer", "=", "None", ",", "\n", "name", "=", "None", ")", ":", "\n", "\n", "    ", "conv_layer", "=", "tf", ".", "layers", ".", "conv1d", "(", "\n", "inputs", "=", "inputs", ",", "\n", "filters", "=", "filters", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "strides", "=", "strides", ",", "\n", "padding", "=", "padding", ",", "\n", "activation", "=", "activation", ",", "\n", "kernel_initializer", "=", "kernel_initializer", ",", "\n", "name", "=", "name", ")", "\n", "\n", "return", "conv_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv2d_layer": [[44, 65], ["tensorflow.layers.conv2d"], "function", ["None"], ["", "def", "conv2d_layer", "(", "\n", "inputs", ",", "\n", "filters", ",", "\n", "kernel_size", ",", "\n", "strides", ",", "\n", "padding", "=", "'same'", ",", "\n", "activation", "=", "None", ",", "\n", "kernel_initializer", "=", "None", ",", "\n", "name", "=", "None", ")", ":", "\n", "\n", "    ", "conv_layer", "=", "tf", ".", "layers", ".", "conv2d", "(", "\n", "inputs", "=", "inputs", ",", "\n", "filters", "=", "filters", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "strides", "=", "strides", ",", "\n", "padding", "=", "padding", ",", "\n", "activation", "=", "activation", ",", "\n", "kernel_initializer", "=", "kernel_initializer", ",", "\n", "name", "=", "name", ")", "\n", "\n", "return", "conv_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.residual1d_block": [[66, 84], ["module_mceps.conv1d_layer", "module_mceps.instance_norm_layer", "module_mceps.conv1d_layer", "module_mceps.instance_norm_layer", "module_mceps.gated_linear_layer", "module_mceps.conv1d_layer", "module_mceps.instance_norm_layer"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.gated_linear_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer"], ["", "def", "residual1d_block", "(", "\n", "inputs", ",", "\n", "filters", "=", "1024", ",", "\n", "kernel_size", "=", "3", ",", "\n", "strides", "=", "1", ",", "\n", "name_prefix", "=", "'residule_block_'", ")", ":", "\n", "\n", "    ", "h1", "=", "conv1d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "filters", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_conv'", ")", "\n", "h1_norm", "=", "instance_norm_layer", "(", "inputs", "=", "h1", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_norm'", ")", "\n", "h1_gates", "=", "conv1d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "filters", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_gates'", ")", "\n", "h1_norm_gates", "=", "instance_norm_layer", "(", "inputs", "=", "h1_gates", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_norm_gates'", ")", "\n", "h1_glu", "=", "gated_linear_layer", "(", "inputs", "=", "h1_norm", ",", "gates", "=", "h1_norm_gates", ",", "name", "=", "name_prefix", "+", "'h1_glu'", ")", "\n", "h2", "=", "conv1d_layer", "(", "inputs", "=", "h1_glu", ",", "filters", "=", "filters", "//", "2", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h2_conv'", ")", "\n", "h2_norm", "=", "instance_norm_layer", "(", "inputs", "=", "h2", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h2_norm'", ")", "\n", "\n", "h3", "=", "inputs", "+", "h2_norm", "\n", "\n", "return", "h3", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.downsample1d_block": [[85, 99], ["module_mceps.conv1d_layer", "module_mceps.instance_norm_layer", "module_mceps.conv1d_layer", "module_mceps.instance_norm_layer", "module_mceps.gated_linear_layer"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.gated_linear_layer"], ["", "def", "downsample1d_block", "(", "\n", "inputs", ",", "\n", "filters", ",", "\n", "kernel_size", ",", "\n", "strides", ",", "\n", "name_prefix", "=", "'downsample1d_block_'", ")", ":", "\n", "\n", "    ", "h1", "=", "conv1d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "filters", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_conv'", ")", "\n", "h1_norm", "=", "instance_norm_layer", "(", "inputs", "=", "h1", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_norm'", ")", "\n", "h1_gates", "=", "conv1d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "filters", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_gates'", ")", "\n", "h1_norm_gates", "=", "instance_norm_layer", "(", "inputs", "=", "h1_gates", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_norm_gates'", ")", "\n", "h1_glu", "=", "gated_linear_layer", "(", "inputs", "=", "h1_norm", ",", "gates", "=", "h1_norm_gates", ",", "name", "=", "name_prefix", "+", "'h1_glu'", ")", "\n", "\n", "return", "h1_glu", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.downsample2d_block": [[100, 114], ["module_mceps.conv2d_layer", "module_mceps.instance_norm_layer", "module_mceps.conv2d_layer", "module_mceps.instance_norm_layer", "module_mceps.gated_linear_layer"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv2d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv2d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.gated_linear_layer"], ["", "def", "downsample2d_block", "(", "\n", "inputs", ",", "\n", "filters", ",", "\n", "kernel_size", ",", "\n", "strides", ",", "\n", "name_prefix", "=", "'downsample2d_block_'", ")", ":", "\n", "\n", "    ", "h1", "=", "conv2d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "filters", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_conv'", ")", "\n", "h1_norm", "=", "instance_norm_layer", "(", "inputs", "=", "h1", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_norm'", ")", "\n", "h1_gates", "=", "conv2d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "filters", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_gates'", ")", "\n", "h1_norm_gates", "=", "instance_norm_layer", "(", "inputs", "=", "h1_gates", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_norm_gates'", ")", "\n", "h1_glu", "=", "gated_linear_layer", "(", "inputs", "=", "h1_norm", ",", "gates", "=", "h1_norm_gates", ",", "name", "=", "name_prefix", "+", "'h1_glu'", ")", "\n", "\n", "return", "h1_glu", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.upsample1d_block": [[115, 134], ["module_mceps.conv1d_layer", "module_mceps.pixel_shuffler", "module_mceps.instance_norm_layer", "module_mceps.conv1d_layer", "module_mceps.pixel_shuffler", "module_mceps.instance_norm_layer", "module_mceps.gated_linear_layer"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.pixel_shuffler", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.pixel_shuffler", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.instance_norm_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.gated_linear_layer"], ["", "def", "upsample1d_block", "(", "\n", "inputs", ",", "\n", "filters", ",", "\n", "kernel_size", ",", "\n", "strides", ",", "\n", "shuffle_size", "=", "2", ",", "\n", "name_prefix", "=", "'upsample1d_block_'", ")", ":", "\n", "\n", "    ", "h1", "=", "conv1d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "filters", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_conv'", ")", "\n", "h1_shuffle", "=", "pixel_shuffler", "(", "inputs", "=", "h1", ",", "shuffle_size", "=", "shuffle_size", ",", "name", "=", "name_prefix", "+", "'h1_shuffle'", ")", "\n", "h1_norm", "=", "instance_norm_layer", "(", "inputs", "=", "h1_shuffle", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_norm'", ")", "\n", "\n", "h1_gates", "=", "conv1d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "filters", ",", "kernel_size", "=", "kernel_size", ",", "strides", "=", "strides", ",", "activation", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_gates'", ")", "\n", "h1_shuffle_gates", "=", "pixel_shuffler", "(", "inputs", "=", "h1_gates", ",", "shuffle_size", "=", "shuffle_size", ",", "name", "=", "name_prefix", "+", "'h1_shuffle_gates'", ")", "\n", "h1_norm_gates", "=", "instance_norm_layer", "(", "inputs", "=", "h1_shuffle_gates", ",", "activation_fn", "=", "None", ",", "name", "=", "name_prefix", "+", "'h1_norm_gates'", ")", "\n", "\n", "h1_glu", "=", "gated_linear_layer", "(", "inputs", "=", "h1_norm", ",", "gates", "=", "h1_norm_gates", ",", "name", "=", "name_prefix", "+", "'h1_glu'", ")", "\n", "\n", "return", "h1_glu", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.pixel_shuffler": [[135, 147], ["tensorflow.reshape", "tensorflow.shape", "tensorflow.shape", "inputs.get_shape().as_list", "inputs.get_shape"], "function", ["None"], ["", "def", "pixel_shuffler", "(", "inputs", ",", "shuffle_size", "=", "2", ",", "name", "=", "None", ")", ":", "\n", "\n", "    ", "n", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "0", "]", "\n", "w", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "1", "]", "\n", "c", "=", "inputs", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "2", "]", "\n", "\n", "oc", "=", "c", "//", "shuffle_size", "\n", "ow", "=", "w", "*", "shuffle_size", "\n", "\n", "outputs", "=", "tf", ".", "reshape", "(", "tensor", "=", "inputs", ",", "shape", "=", "[", "n", ",", "ow", ",", "oc", "]", ",", "name", "=", "name", ")", "\n", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.generator_gatedcnn": [[148, 186], ["tensorflow.transpose", "tensorflow.variable_scope", "module_mceps.conv1d_layer", "module_mceps.conv1d_layer", "module_mceps.gated_linear_layer", "module_mceps.downsample1d_block", "module_mceps.downsample1d_block", "module_mceps.residual1d_block", "module_mceps.residual1d_block", "module_mceps.residual1d_block", "module_mceps.residual1d_block", "module_mceps.residual1d_block", "module_mceps.residual1d_block", "module_mceps.upsample1d_block", "module_mceps.upsample1d_block", "module_mceps.conv1d_layer", "tensorflow.transpose", "scope.reuse_variables"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.gated_linear_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.downsample1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.downsample1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.residual1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.residual1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.residual1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.residual1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.residual1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.residual1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.upsample1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.upsample1d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv1d_layer"], ["", "def", "generator_gatedcnn", "(", "inputs", ",", "reuse", "=", "False", ",", "scope_name", "=", "'generator_gatedcnn'", ")", ":", "\n", "\n", "# inputs has shape [batch_size, num_features, time]", "\n", "# we need to convert it to [batch_size, time, num_features] for 1D convolution", "\n", "    ", "inputs", "=", "tf", ".", "transpose", "(", "inputs", ",", "perm", "=", "[", "0", ",", "2", ",", "1", "]", ",", "name", "=", "'input_transpose'", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "scope_name", ")", "as", "scope", ":", "\n", "# Discriminator would be reused in CycleGAN", "\n", "        ", "if", "reuse", ":", "\n", "            ", "scope", ".", "reuse_variables", "(", ")", "\n", "", "else", ":", "\n", "            ", "assert", "scope", ".", "reuse", "is", "False", "\n", "\n", "", "h1", "=", "conv1d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "128", ",", "kernel_size", "=", "15", ",", "strides", "=", "1", ",", "activation", "=", "None", ",", "name", "=", "'h1_conv'", ")", "\n", "h1_gates", "=", "conv1d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "128", ",", "kernel_size", "=", "15", ",", "strides", "=", "1", ",", "activation", "=", "None", ",", "name", "=", "'h1_conv_gates'", ")", "\n", "h1_glu", "=", "gated_linear_layer", "(", "inputs", "=", "h1", ",", "gates", "=", "h1_gates", ",", "name", "=", "'h1_glu'", ")", "\n", "\n", "# Downsample", "\n", "d1", "=", "downsample1d_block", "(", "inputs", "=", "h1_glu", ",", "filters", "=", "256", ",", "kernel_size", "=", "5", ",", "strides", "=", "2", ",", "name_prefix", "=", "'downsample1d_block1_'", ")", "\n", "d2", "=", "downsample1d_block", "(", "inputs", "=", "d1", ",", "filters", "=", "512", ",", "kernel_size", "=", "5", ",", "strides", "=", "2", ",", "name_prefix", "=", "'downsample1d_block2_'", ")", "\n", "\n", "# Residual blocks", "\n", "r1", "=", "residual1d_block", "(", "inputs", "=", "d2", ",", "filters", "=", "1024", ",", "kernel_size", "=", "3", ",", "strides", "=", "1", ",", "name_prefix", "=", "'residual1d_block1_'", ")", "\n", "r2", "=", "residual1d_block", "(", "inputs", "=", "r1", ",", "filters", "=", "1024", ",", "kernel_size", "=", "3", ",", "strides", "=", "1", ",", "name_prefix", "=", "'residual1d_block2_'", ")", "\n", "r3", "=", "residual1d_block", "(", "inputs", "=", "r2", ",", "filters", "=", "1024", ",", "kernel_size", "=", "3", ",", "strides", "=", "1", ",", "name_prefix", "=", "'residual1d_block3_'", ")", "\n", "r4", "=", "residual1d_block", "(", "inputs", "=", "r3", ",", "filters", "=", "1024", ",", "kernel_size", "=", "3", ",", "strides", "=", "1", ",", "name_prefix", "=", "'residual1d_block4_'", ")", "\n", "r5", "=", "residual1d_block", "(", "inputs", "=", "r4", ",", "filters", "=", "1024", ",", "kernel_size", "=", "3", ",", "strides", "=", "1", ",", "name_prefix", "=", "'residual1d_block5_'", ")", "\n", "r6", "=", "residual1d_block", "(", "inputs", "=", "r5", ",", "filters", "=", "1024", ",", "kernel_size", "=", "3", ",", "strides", "=", "1", ",", "name_prefix", "=", "'residual1d_block6_'", ")", "\n", "\n", "# Upsample", "\n", "u1", "=", "upsample1d_block", "(", "inputs", "=", "r6", ",", "filters", "=", "1024", ",", "kernel_size", "=", "5", ",", "strides", "=", "1", ",", "shuffle_size", "=", "2", ",", "name_prefix", "=", "'upsample1d_block1_'", ")", "\n", "u2", "=", "upsample1d_block", "(", "inputs", "=", "u1", ",", "filters", "=", "512", ",", "kernel_size", "=", "5", ",", "strides", "=", "1", ",", "shuffle_size", "=", "2", ",", "name_prefix", "=", "'upsample1d_block2_'", ")", "\n", "\n", "# Output", "\n", "o1", "=", "conv1d_layer", "(", "inputs", "=", "u2", ",", "filters", "=", "24", ",", "kernel_size", "=", "15", ",", "strides", "=", "1", ",", "activation", "=", "None", ",", "name", "=", "'o1_conv'", ")", "\n", "o2", "=", "tf", ".", "transpose", "(", "o1", ",", "perm", "=", "[", "0", ",", "2", ",", "1", "]", ",", "name", "=", "'output_transpose'", ")", "\n", "\n", "", "return", "o2", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.discriminator": [[188, 214], ["tensorflow.expand_dims", "tensorflow.variable_scope", "module_mceps.conv2d_layer", "module_mceps.conv2d_layer", "module_mceps.gated_linear_layer", "module_mceps.downsample2d_block", "module_mceps.downsample2d_block", "module_mceps.downsample2d_block", "tensorflow.layers.dense", "scope.reuse_variables"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv2d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.conv2d_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.gated_linear_layer", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.downsample2d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.downsample2d_block", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.module_mceps.downsample2d_block"], ["", "def", "discriminator", "(", "inputs", ",", "reuse", "=", "False", ",", "scope_name", "=", "'discriminator'", ")", ":", "\n", "\n", "# inputs has shape [batch_size, num_features, time]", "\n", "# we need to add channel for 2D convolution [batch_size, num_features, time, 1]", "\n", "    ", "inputs", "=", "tf", ".", "expand_dims", "(", "inputs", ",", "-", "1", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "scope_name", ")", "as", "scope", ":", "\n", "# Discriminator would be reused in CycleGAN", "\n", "        ", "if", "reuse", ":", "\n", "            ", "scope", ".", "reuse_variables", "(", ")", "\n", "", "else", ":", "\n", "            ", "assert", "scope", ".", "reuse", "is", "False", "\n", "\n", "", "h1", "=", "conv2d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "128", ",", "kernel_size", "=", "[", "3", ",", "3", "]", ",", "strides", "=", "[", "1", ",", "2", "]", ",", "activation", "=", "None", ",", "name", "=", "'h1_conv'", ")", "\n", "h1_gates", "=", "conv2d_layer", "(", "inputs", "=", "inputs", ",", "filters", "=", "128", ",", "kernel_size", "=", "[", "3", ",", "3", "]", ",", "strides", "=", "[", "1", ",", "2", "]", ",", "activation", "=", "None", ",", "name", "=", "'h1_conv_gates'", ")", "\n", "h1_glu", "=", "gated_linear_layer", "(", "inputs", "=", "h1", ",", "gates", "=", "h1_gates", ",", "name", "=", "'h1_glu'", ")", "\n", "\n", "# Downsample", "\n", "d1", "=", "downsample2d_block", "(", "inputs", "=", "h1_glu", ",", "filters", "=", "256", ",", "kernel_size", "=", "[", "3", ",", "3", "]", ",", "strides", "=", "[", "2", ",", "2", "]", ",", "name_prefix", "=", "'downsample2d_block1_'", ")", "\n", "d2", "=", "downsample2d_block", "(", "inputs", "=", "d1", ",", "filters", "=", "512", ",", "kernel_size", "=", "[", "3", ",", "3", "]", ",", "strides", "=", "[", "2", ",", "2", "]", ",", "name_prefix", "=", "'downsample2d_block2_'", ")", "\n", "d3", "=", "downsample2d_block", "(", "inputs", "=", "d2", ",", "filters", "=", "1024", ",", "kernel_size", "=", "[", "6", ",", "3", "]", ",", "strides", "=", "[", "1", ",", "2", "]", ",", "name_prefix", "=", "'downsample2d_block3_'", ")", "\n", "\n", "# Output", "\n", "o1", "=", "tf", ".", "layers", ".", "dense", "(", "inputs", "=", "d3", ",", "units", "=", "1", ",", "activation", "=", "tf", ".", "nn", ".", "sigmoid", ")", "\n", "\n", "return", "o1", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l1_loss": [[15, 18], ["tensorflow.reduce_mean", "tensorflow.abs"], "function", ["None"], ["def", "l1_loss", "(", "y", ",", "y_hat", ")", ":", "\n", "\n", "    ", "return", "tf", ".", "reduce_mean", "(", "tf", ".", "abs", "(", "y", "-", "y_hat", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.l2_loss": [[19, 22], ["tensorflow.reduce_mean", "tensorflow.square"], "function", ["None"], ["", "def", "l2_loss", "(", "y", ",", "y_hat", ")", ":", "\n", "\n", "    ", "return", "tf", ".", "reduce_mean", "(", "tf", ".", "square", "(", "y", "-", "y_hat", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.cross_entropy_loss": [[23, 25], ["tensorflow.reduce_mean", "tensorflow.nn.sigmoid_cross_entropy_with_logits"], "function", ["None"], ["", "def", "cross_entropy_loss", "(", "logits", ",", "labels", ")", ":", "\n", "    ", "return", "tf", ".", "reduce_mean", "(", "tf", ".", "nn", ".", "sigmoid_cross_entropy_with_logits", "(", "logits", "=", "logits", ",", "labels", "=", "labels", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.convert_continuos_f0": [[27, 60], ["numpy.float32", "scipy.interpolate.interp1d", "scipy.interpolate.interp1d.", "logging.warn", "numpy.where", "numpy.arange", "numpy.where", "numpy.where"], "function", ["None"], ["", "def", "convert_continuos_f0", "(", "f0", ")", ":", "\n", "    ", "\"\"\"CONVERT F0 TO CONTINUOUS F0\n\n    Args:\n        f0 (ndarray): original f0 sequence with the shape (T)\n\n    Return:\n        (ndarray): continuous f0 with the shape (T)\n    \"\"\"", "\n", "# get uv information as binary", "\n", "uv", "=", "np", ".", "float32", "(", "f0", "!=", "0", ")", "\n", "\n", "# get start and end of f0", "\n", "if", "(", "f0", "==", "0", ")", ".", "all", "(", ")", ":", "\n", "        ", "logging", ".", "warn", "(", "\"all of the f0 values are 0.\"", ")", "\n", "return", "uv", ",", "f0", "\n", "", "start_f0", "=", "f0", "[", "f0", "!=", "0", "]", "[", "0", "]", "\n", "end_f0", "=", "f0", "[", "f0", "!=", "0", "]", "[", "-", "1", "]", "\n", "\n", "# padding start and end of f0 sequence", "\n", "start_idx", "=", "np", ".", "where", "(", "f0", "==", "start_f0", ")", "[", "0", "]", "[", "0", "]", "\n", "end_idx", "=", "np", ".", "where", "(", "f0", "==", "end_f0", ")", "[", "0", "]", "[", "-", "1", "]", "\n", "f0", "[", ":", "start_idx", "]", "=", "start_f0", "\n", "f0", "[", "end_idx", ":", "]", "=", "end_f0", "\n", "\n", "# get non-zero frame index", "\n", "nz_frames", "=", "np", ".", "where", "(", "f0", "!=", "0", ")", "[", "0", "]", "\n", "\n", "# perform linear interpolation", "\n", "f", "=", "interp1d", "(", "nz_frames", ",", "f0", "[", "nz_frames", "]", ")", "\n", "cont_f0", "=", "f", "(", "np", ".", "arange", "(", "0", ",", "f0", ".", "shape", "[", "0", "]", ")", ")", "\n", "\n", "return", "uv", ",", "cont_f0", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.get_cont_lf0": [[62, 67], ["utils.convert_continuos_f0", "numpy.log"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.convert_continuos_f0"], ["", "def", "get_cont_lf0", "(", "f0", ",", "frame_period", "=", "5.0", ")", ":", "\n", "    ", "uv", ",", "cont_f0_lpf", "=", "convert_continuos_f0", "(", "f0", ")", "\n", "#cont_f0_lpf = low_pass_filter(cont_f0_lpf, int(1.0 / (frame_period * 0.001)), cutoff=20)", "\n", "cont_lf0_lpf", "=", "np", ".", "log", "(", "cont_f0_lpf", ")", "\n", "return", "uv", ",", "cont_lf0_lpf", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.get_lf0_cwt": [[74, 90], ["pycwt.MexicanHat", "pycwt.cwt", "numpy.squeeze", "numpy.real"], "function", ["None"], ["", "def", "get_lf0_cwt", "(", "lf0", ")", ":", "\n", "    ", "mother", "=", "wavelet", ".", "MexicanHat", "(", ")", "\n", "#dt = 0.005", "\n", "dt", "=", "0.005", "\n", "dj", "=", "1", "\n", "s0", "=", "dt", "*", "2", "\n", "J", "=", "9", "\n", "#C_delta = 3.541", "\n", "#Wavelet_lf0, scales, _, _, _, _ = wavelet.cwt(np.squeeze(lf0), dt, dj, s0, J, mother)", "\n", "Wavelet_lf0", ",", "scales", ",", "freqs", ",", "coi", ",", "fft", ",", "fftfreqs", "=", "wavelet", ".", "cwt", "(", "np", ".", "squeeze", "(", "lf0", ")", ",", "dt", ",", "dj", ",", "s0", ",", "J", ",", "mother", ")", "\n", "#Wavelet_le, scales, _, _, _, _ = wavelet.cwt(np.squeeze(le), dt, dj, s0, J, mother)", "\n", "Wavelet_lf0", "=", "np", ".", "real", "(", "Wavelet_lf0", ")", ".", "T", "\n", "#Wavelet_le = np.real(Wavelet_le).T   # (T, D=10)", "\n", "#0lf0_le_cwt = np.concatenate((Wavelet_lf0, Wavelet_le), -1)", "\n", "#  iwave = wavelet.icwt(np.squeeze(lf0), scales, dt, dj, mother) * std", "\n", "return", "Wavelet_lf0", ",", "scales", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.inverse_cwt": [[92, 99], ["numpy.zeros", "range", "numpy.sum", "sklearn.preprocessing.scale", "len", "len"], "function", ["None"], ["", "def", "inverse_cwt", "(", "Wavelet_lf0", ",", "scales", ")", ":", "\n", "    ", "lf0_rec", "=", "np", ".", "zeros", "(", "[", "Wavelet_lf0", ".", "shape", "[", "0", "]", ",", "len", "(", "scales", ")", "]", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "scales", ")", ")", ":", "\n", "        ", "lf0_rec", "[", ":", ",", "i", "]", "=", "Wavelet_lf0", "[", ":", ",", "i", "]", "*", "(", "(", "i", "+", "1", "+", "2.5", ")", "**", "(", "-", "2.5", ")", ")", "\n", "", "lf0_rec_sum", "=", "np", ".", "sum", "(", "lf0_rec", ",", "axis", "=", "1", ")", "\n", "lf0_rec_sum", "=", "preprocessing", ".", "scale", "(", "lf0_rec_sum", ")", "\n", "return", "lf0_rec_sum", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.low_pass_filter": [[101, 122], ["scipy.signal.firwin", "numpy.pad", "scipy.signal.lfilter"], "function", ["None"], ["", "def", "low_pass_filter", "(", "x", ",", "fs", ",", "cutoff", "=", "70", ",", "padding", "=", "True", ")", ":", "\n", "    ", "\"\"\"FUNCTION TO APPLY LOW PASS FILTER\n\n    Args:\n        x (ndarray): Waveform sequence\n        fs (int): Sampling frequency\n        cutoff (float): Cutoff frequency of low pass filter\n\n    Return:\n        (ndarray): Low pass filtered waveform sequence\n    \"\"\"", "\n", "nyquist", "=", "fs", "//", "2", "\n", "norm_cutoff", "=", "cutoff", "/", "nyquist", "\n", "\n", "# low cut filter", "\n", "numtaps", "=", "255", "\n", "fil", "=", "firwin", "(", "numtaps", ",", "norm_cutoff", ")", "\n", "x_pad", "=", "np", ".", "pad", "(", "x", ",", "(", "numtaps", ",", "numtaps", ")", ",", "'edge'", ")", "\n", "lpf_x", "=", "lfilter", "(", "fil", ",", "1", ",", "x_pad", ")", "\n", "lpf_x", "=", "lpf_x", "[", "numtaps", "+", "numtaps", "//", "2", ":", "-", "numtaps", "//", "2", "]", "\n", "return", "lpf_x", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.norm_scale": [[123, 132], ["numpy.zeros", "numpy.zeros", "numpy.zeros", "range", "Wavelet_lf0[].mean", "Wavelet_lf0[].std"], "function", ["None"], ["", "def", "norm_scale", "(", "Wavelet_lf0", ")", ":", "\n", "    ", "Wavelet_lf0_norm", "=", "np", ".", "zeros", "(", "(", "Wavelet_lf0", ".", "shape", "[", "0", "]", ",", "Wavelet_lf0", ".", "shape", "[", "1", "]", ")", ")", "\n", "mean", "=", "np", ".", "zeros", "(", "(", "1", ",", "Wavelet_lf0", ".", "shape", "[", "1", "]", ")", ")", "#[1,10]", "\n", "std", "=", "np", ".", "zeros", "(", "(", "1", ",", "Wavelet_lf0", ".", "shape", "[", "1", "]", ")", ")", "\n", "for", "scale", "in", "range", "(", "Wavelet_lf0", ".", "shape", "[", "1", "]", ")", ":", "\n", "        ", "mean", "[", ":", ",", "scale", "]", "=", "Wavelet_lf0", "[", ":", ",", "scale", "]", ".", "mean", "(", ")", "\n", "std", "[", ":", ",", "scale", "]", "=", "Wavelet_lf0", "[", ":", ",", "scale", "]", ".", "std", "(", ")", "\n", "Wavelet_lf0_norm", "[", ":", ",", "scale", "]", "=", "(", "Wavelet_lf0", "[", ":", ",", "scale", "]", "-", "mean", "[", ":", ",", "scale", "]", ")", "/", "std", "[", ":", ",", "scale", "]", "\n", "", "return", "Wavelet_lf0_norm", ",", "mean", ",", "std", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.get_lf0_cwt_norm": [[133, 162], ["list", "list", "list", "list", "list", "list", "list", "list", "utils.get_cont_lf0", "utils.get_lf0_cwt", "utils.norm_scale", "list.append", "list.append", "list.append", "list.append", "list.append", "list.append", "list.append", "list.append"], "function", ["home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.get_cont_lf0", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.get_lf0_cwt", "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.norm_scale"], ["", "def", "get_lf0_cwt_norm", "(", "f0s", ",", "mean", ",", "std", ")", ":", "\n", "\n", "    ", "uvs", "=", "list", "(", ")", "\n", "cont_lf0_lpfs", "=", "list", "(", ")", "\n", "cont_lf0_lpf_norms", "=", "list", "(", ")", "\n", "Wavelet_lf0s", "=", "list", "(", ")", "\n", "Wavelet_lf0s_norm", "=", "list", "(", ")", "\n", "scaless", "=", "list", "(", ")", "\n", "\n", "means", "=", "list", "(", ")", "\n", "stds", "=", "list", "(", ")", "\n", "for", "f0", "in", "f0s", ":", "\n", "\n", "        ", "uv", ",", "cont_lf0_lpf", "=", "get_cont_lf0", "(", "f0", ")", "\n", "cont_lf0_lpf_norm", "=", "(", "cont_lf0_lpf", "-", "mean", ")", "/", "std", "\n", "\n", "Wavelet_lf0", ",", "scales", "=", "get_lf0_cwt", "(", "cont_lf0_lpf_norm", ")", "#[560,10]", "\n", "Wavelet_lf0_norm", ",", "mean_scale", ",", "std_scale", "=", "norm_scale", "(", "Wavelet_lf0", ")", "#[560,10],[1,10],[1,10]", "\n", "\n", "Wavelet_lf0s_norm", ".", "append", "(", "Wavelet_lf0_norm", ")", "\n", "uvs", ".", "append", "(", "uv", ")", "\n", "cont_lf0_lpfs", ".", "append", "(", "cont_lf0_lpf", ")", "\n", "cont_lf0_lpf_norms", ".", "append", "(", "cont_lf0_lpf_norm", ")", "\n", "Wavelet_lf0s", ".", "append", "(", "Wavelet_lf0", ")", "\n", "scaless", ".", "append", "(", "scales", ")", "\n", "means", ".", "append", "(", "mean_scale", ")", "\n", "stds", ".", "append", "(", "std_scale", ")", "\n", "\n", "", "return", "Wavelet_lf0s_norm", ",", "scaless", ",", "means", ",", "stds", "\n", "\n"]], "home.repos.pwc.inspect_result.KunZhou9646_emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0.Parallel-data-free emotional voice conversion with CycleGAN and CWT.utils.denormalize": [[164, 169], ["numpy.zeros", "range"], "function", ["None"], ["", "def", "denormalize", "(", "Wavelet_lf0_norm", ",", "mean", ",", "std", ")", ":", "\n", "    ", "Wavelet_lf0_denorm", "=", "np", ".", "zeros", "(", "(", "Wavelet_lf0_norm", ".", "shape", "[", "0", "]", ",", "Wavelet_lf0_norm", ".", "shape", "[", "1", "]", ")", ")", "\n", "for", "scale", "in", "range", "(", "Wavelet_lf0_norm", ".", "shape", "[", "1", "]", ")", ":", "\n", "        ", "Wavelet_lf0_denorm", "[", ":", ",", "scale", "]", "=", "Wavelet_lf0_norm", "[", ":", ",", "scale", "]", "*", "std", "[", ":", ",", "scale", "]", "+", "mean", "[", ":", ",", "scale", "]", "\n", "", "return", "Wavelet_lf0_denorm", "\n", "\n"]]}