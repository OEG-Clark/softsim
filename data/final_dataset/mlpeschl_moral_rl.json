{"home.repos.pwc.inspect_result.mlpeschl_moral_rl.utils.evaluate_ppo.evaluate_ppo": [[8, 42], ["GymWrapper", "GymWrapper.reset", "torch.tensor().float().to", "range", "numpy.array", "np.array.mean", "np.array.std", "ppo.act", "GymWrapper.step", "np.array().sum.append", "env.reset.copy", "torch.tensor().float().to", "list", "list", "torch.tensor().float", "GymWrapper.reset", "numpy.array().sum", "np.array.append", "torch.tensor().float", "torch.tensor", "numpy.array", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.VecEnv.reset", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.ppo.PPO.act", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.VecEnv.step", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.VecEnv.reset"], ["def", "evaluate_ppo", "(", "ppo", ",", "config", ",", "n_eval", "=", "1000", ")", ":", "\n", "    ", "\"\"\"\n    :param ppo: Trained policy\n    :param config: Environment config\n    :param n_eval: Number of evaluation steps\n    :return: mean, std of rewards\n    \"\"\"", "\n", "env", "=", "GymWrapper", "(", "config", ".", "env_id", ")", "\n", "states", "=", "env", ".", "reset", "(", ")", "\n", "states_tensor", "=", "torch", ".", "tensor", "(", "states", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "\n", "obj_logs", "=", "[", "]", "\n", "obj_returns", "=", "[", "]", "\n", "\n", "for", "t", "in", "range", "(", "n_eval", ")", ":", "\n", "        ", "actions", ",", "log_probs", "=", "ppo", ".", "act", "(", "states_tensor", ")", "\n", "next_states", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "actions", ")", "\n", "obj_logs", ".", "append", "(", "reward", ")", "\n", "\n", "if", "done", ":", "\n", "            ", "next_states", "=", "env", ".", "reset", "(", ")", "\n", "obj_logs", "=", "np", ".", "array", "(", "obj_logs", ")", ".", "sum", "(", "axis", "=", "0", ")", "\n", "obj_returns", ".", "append", "(", "obj_logs", ")", "\n", "obj_logs", "=", "[", "]", "\n", "\n", "# Prepare state input for next time step", "\n", "", "states", "=", "next_states", ".", "copy", "(", ")", "\n", "states_tensor", "=", "torch", ".", "tensor", "(", "states", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "\n", "", "obj_returns", "=", "np", ".", "array", "(", "obj_returns", ")", "\n", "obj_means", "=", "obj_returns", ".", "mean", "(", "axis", "=", "0", ")", "\n", "obj_std", "=", "obj_returns", ".", "std", "(", "axis", "=", "0", ")", "\n", "\n", "return", "list", "(", "obj_means", ")", ",", "list", "(", "obj_std", ")", "", "", ""]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.PreferenceLearner.__init__": [[7, 14], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "n_iter", ",", "warmup", ",", "d", ")", ":", "\n", "        ", "self", ".", "n_iter", "=", "n_iter", "\n", "self", ".", "warmup", "=", "warmup", "\n", "self", ".", "d", "=", "d", "\n", "self", ".", "accept_rates", "=", "None", "\n", "self", ".", "deltas", "=", "[", "]", "\n", "self", ".", "prefs", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.PreferenceLearner.log_preference": [[15, 18], ["active_learning.PreferenceLearner.deltas.append", "active_learning.PreferenceLearner.prefs.append"], "methods", ["None"], ["", "def", "log_preference", "(", "self", ",", "delta", ",", "pref", ")", ":", "\n", "        ", "self", ".", "deltas", ".", "append", "(", "delta", ")", "\n", "self", ".", "prefs", ".", "append", "(", "pref", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.PreferenceLearner.w_prior": [[19, 24], ["numpy.all", "numpy.linalg.norm", "numpy.array", "math.gamma"], "methods", ["None"], ["", "def", "w_prior", "(", "self", ",", "w", ")", ":", "\n", "        ", "if", "np", ".", "linalg", ".", "norm", "(", "w", ")", "<=", "1", "and", "np", ".", "all", "(", "np", ".", "array", "(", "w", ")", ">=", "0", ")", ":", "\n", "            ", "return", "(", "2", "**", "self", ".", "d", ")", "/", "(", "math", ".", "pi", "**", "(", "self", ".", "d", "/", "2", ")", "/", "math", ".", "gamma", "(", "self", ".", "d", "/", "2", "+", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.PreferenceLearner.sample_w_prior": [[25, 31], ["numpy.random.rand", "numpy.array", "w_out.append", "list", "numpy.linalg.norm"], "methods", ["None"], ["", "", "def", "sample_w_prior", "(", "self", ",", "n", ")", ":", "\n", "        ", "sample", "=", "np", ".", "random", ".", "rand", "(", "n", ",", "self", ".", "d", ")", "\n", "w_out", "=", "[", "]", "\n", "for", "w", "in", "sample", ":", "\n", "            ", "w_out", ".", "append", "(", "list", "(", "w", "/", "np", ".", "linalg", ".", "norm", "(", "w", ")", ")", ")", "\n", "", "return", "np", ".", "array", "(", "w_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.PreferenceLearner.f_loglik": [[32, 35], ["numpy.log", "numpy.minimum", "numpy.exp", "numpy.dot"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "f_loglik", "(", "w", ",", "delta", ",", "pref", ")", ":", "\n", "        ", "return", "np", ".", "log", "(", "np", ".", "minimum", "(", "1", ",", "np", ".", "exp", "(", "pref", "*", "np", ".", "dot", "(", "w", ",", "delta", ")", ")", "+", "1e-5", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.PreferenceLearner.vanilla_loglik": [[36, 39], ["numpy.log", "numpy.exp", "numpy.dot"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "vanilla_loglik", "(", "w", ",", "delta", ",", "pref", ")", ":", "\n", "        ", "return", "np", ".", "log", "(", "1", "/", "(", "1", "+", "np", ".", "exp", "(", "-", "pref", "*", "np", ".", "dot", "(", "w", ",", "delta", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.PreferenceLearner.propose_w_prob": [[40, 44], ["scipy.multivariate_normal().pdf", "scipy.multivariate_normal"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "propose_w_prob", "(", "w1", ",", "w2", ")", ":", "\n", "        ", "q", "=", "st", ".", "multivariate_normal", "(", "mean", "=", "w1", ",", "cov", "=", "1", ")", ".", "pdf", "(", "w2", ")", "\n", "return", "q", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.PreferenceLearner.propose_w": [[45, 49], ["scipy.multivariate_normal().rvs", "scipy.multivariate_normal"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "propose_w", "(", "w_curr", ")", ":", "\n", "        ", "w_new", "=", "st", ".", "multivariate_normal", "(", "mean", "=", "w_curr", ",", "cov", "=", "1", ")", ".", "rvs", "(", ")", "\n", "return", "w_new", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.PreferenceLearner.posterior_log_prob": [[50, 58], ["range", "numpy.sum", "numpy.log", "len", "f_logliks.append", "active_learning.PreferenceLearner.f_loglik", "active_learning.PreferenceLearner.w_prior"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.PreferenceLearner.f_loglik", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.PreferenceLearner.w_prior"], ["", "def", "posterior_log_prob", "(", "self", ",", "deltas", ",", "prefs", ",", "w", ")", ":", "\n", "        ", "f_logliks", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "prefs", ")", ")", ":", "\n", "            ", "f_logliks", ".", "append", "(", "self", ".", "f_loglik", "(", "w", ",", "deltas", "[", "i", "]", ",", "prefs", "[", "i", "]", ")", ")", "\n", "", "loglik", "=", "np", ".", "sum", "(", "f_logliks", ")", "\n", "log_prior", "=", "np", ".", "log", "(", "self", ".", "w_prior", "(", "w", ")", "+", "1e-5", ")", "\n", "\n", "return", "loglik", "+", "log_prior", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.PreferenceLearner.mcmc_vanilla": [[59, 93], ["range", "active_learning.PreferenceLearner.propose_w", "active_learning.PreferenceLearner.posterior_log_prob", "active_learning.PreferenceLearner.posterior_log_prob", "min", "accept_rates.append", "numpy.array", "numpy.array", "scipy.uniform().rvs", "w_arr.append", "w_arr.append", "range", "active_learning.PreferenceLearner.propose_w_prob", "active_learning.PreferenceLearner.propose_w_prob", "numpy.exp", "scipy.uniform"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.PreferenceLearner.propose_w", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.PreferenceLearner.posterior_log_prob", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.PreferenceLearner.posterior_log_prob", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.PreferenceLearner.propose_w_prob", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.PreferenceLearner.propose_w_prob"], ["", "def", "mcmc_vanilla", "(", "self", ",", "w_init", "=", "'mode'", ")", ":", "\n", "        ", "if", "w_init", "==", "'mode'", ":", "\n", "            ", "w_init", "=", "[", "0", "for", "i", "in", "range", "(", "self", ".", "d", ")", "]", "\n", "\n", "", "w_arr", "=", "[", "]", "\n", "w_curr", "=", "w_init", "\n", "accept_rates", "=", "[", "]", "\n", "accept_cum", "=", "0", "\n", "\n", "for", "i", "in", "range", "(", "1", ",", "self", ".", "warmup", "+", "self", ".", "n_iter", "+", "1", ")", ":", "\n", "            ", "w_new", "=", "self", ".", "propose_w", "(", "w_curr", ")", "\n", "\n", "prob_curr", "=", "self", ".", "posterior_log_prob", "(", "self", ".", "deltas", ",", "self", ".", "prefs", ",", "w_curr", ")", "\n", "prob_new", "=", "self", ".", "posterior_log_prob", "(", "self", ".", "deltas", ",", "self", ".", "prefs", ",", "w_new", ")", "\n", "\n", "if", "prob_new", ">", "prob_curr", ":", "\n", "                ", "acceptance_ratio", "=", "1", "\n", "", "else", ":", "\n", "                ", "qr", "=", "self", ".", "propose_w_prob", "(", "w_curr", ",", "w_new", ")", "/", "self", ".", "propose_w_prob", "(", "w_new", ",", "w_curr", ")", "\n", "acceptance_ratio", "=", "np", ".", "exp", "(", "prob_new", "-", "prob_curr", ")", "*", "qr", "\n", "", "acceptance_prob", "=", "min", "(", "1", ",", "acceptance_ratio", ")", "\n", "\n", "if", "acceptance_prob", ">", "st", ".", "uniform", "(", "0", ",", "1", ")", ".", "rvs", "(", ")", ":", "\n", "                ", "w_curr", "=", "w_new", "\n", "accept_cum", "=", "accept_cum", "+", "1", "\n", "w_arr", ".", "append", "(", "w_new", ")", "\n", "", "else", ":", "\n", "                ", "w_arr", ".", "append", "(", "w_curr", ")", "\n", "\n", "", "accept_rates", ".", "append", "(", "accept_cum", "/", "i", ")", "\n", "\n", "", "self", ".", "accept_rates", "=", "np", ".", "array", "(", "accept_rates", ")", "[", "self", ".", "warmup", ":", "]", "\n", "\n", "return", "np", ".", "array", "(", "w_arr", ")", "[", "self", ".", "warmup", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.VolumeBuffer.__init__": [[96, 104], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "auto_pref", "=", "True", ")", ":", "\n", "        ", "self", ".", "auto_pref", "=", "auto_pref", "\n", "self", ".", "best_volume", "=", "-", "np", ".", "inf", "\n", "self", ".", "best_delta", "=", "None", "\n", "self", ".", "best_observed_returns", "=", "None", "\n", "self", ".", "best_returns", "=", "None", "\n", "self", ".", "observed_logs", "=", "[", "]", "\n", "self", ".", "objective_logs", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.VolumeBuffer.log_statistics": [[105, 107], ["active_learning.VolumeBuffer.objective_logs.append"], "methods", ["None"], ["", "def", "log_statistics", "(", "self", ",", "statistics", ")", ":", "\n", "        ", "self", ".", "objective_logs", ".", "append", "(", "statistics", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.VolumeBuffer.log_rewards": [[108, 110], ["active_learning.VolumeBuffer.observed_logs.append"], "methods", ["None"], ["", "def", "log_rewards", "(", "self", ",", "rewards", ")", ":", "\n", "        ", "self", ".", "observed_logs", ".", "append", "(", "rewards", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.VolumeBuffer.volume_removal": [[111, 120], ["min", "active_learning.PreferenceLearner.f_loglik", "active_learning.PreferenceLearner.f_loglik", "len", "len"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.PreferenceLearner.f_loglik", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.PreferenceLearner.f_loglik"], ["", "@", "staticmethod", "\n", "def", "volume_removal", "(", "w_posterior", ",", "delta", ")", ":", "\n", "        ", "expected_volume_a", "=", "0", "\n", "expected_volume_b", "=", "0", "\n", "for", "w", "in", "w_posterior", ":", "\n", "            ", "expected_volume_a", "+=", "(", "1", "-", "PreferenceLearner", ".", "f_loglik", "(", "w", ",", "delta", ",", "1", ")", ")", "\n", "expected_volume_b", "+=", "(", "1", "-", "PreferenceLearner", ".", "f_loglik", "(", "w", ",", "delta", ",", "-", "1", ")", ")", "\n", "\n", "", "return", "min", "(", "expected_volume_a", "/", "len", "(", "w_posterior", ")", ",", "expected_volume_b", "/", "len", "(", "w_posterior", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.VolumeBuffer.sample_return_pair": [[121, 152], ["numpy.array().sum", "numpy.random.choice", "numpy.arange", "numpy.array().sum", "numpy.array", "len", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array"], "methods", ["None"], ["", "def", "sample_return_pair", "(", "self", ")", ":", "\n", "        ", "observed_logs_returns", "=", "np", ".", "array", "(", "self", ".", "observed_logs", ")", ".", "sum", "(", "axis", "=", "0", ")", "\n", "rand_idx", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "len", "(", "observed_logs_returns", ")", ")", ",", "2", ",", "replace", "=", "False", ")", "\n", "\n", "# v2-Environment comparison", "\n", "#new_returns_a = observed_logs_returns[rand_idx[0]]", "\n", "#new_returns_b = observed_logs_returns[rand_idx[1]]", "\n", "\n", "# v3-Environment comparison (vase agnostic)", "\n", "new_returns_a", "=", "observed_logs_returns", "[", "rand_idx", "[", "0", "]", ",", "0", ":", "3", "]", "\n", "new_returns_b", "=", "observed_logs_returns", "[", "rand_idx", "[", "1", "]", ",", "0", ":", "3", "]", "\n", "\n", "# Reset observed logs", "\n", "self", ".", "observed_logs", "=", "[", "]", "\n", "\n", "# Also return ground truth logs for automatic preferences", "\n", "if", "self", ".", "auto_pref", ":", "\n", "            ", "objective_logs_returns", "=", "np", ".", "array", "(", "self", ".", "objective_logs", ")", ".", "sum", "(", "axis", "=", "0", ")", "\n", "\n", "# v2-Environment comparison", "\n", "#logs_a = objective_logs_returns[rand_idx[0]]", "\n", "#logs_b = objective_logs_returns[rand_idx[1]]", "\n", "\n", "# v3-Environment comparison (vase agnostic)", "\n", "logs_a", "=", "objective_logs_returns", "[", "rand_idx", "[", "0", "]", ",", "0", ":", "3", "]", "\n", "logs_b", "=", "objective_logs_returns", "[", "rand_idx", "[", "1", "]", ",", "0", ":", "3", "]", "\n", "\n", "self", ".", "objective_logs", "=", "[", "]", "\n", "return", "np", ".", "array", "(", "new_returns_a", ")", ",", "np", ".", "array", "(", "new_returns_b", ")", ",", "logs_a", ",", "logs_b", "\n", "", "else", ":", "\n", "            ", "return", "np", ".", "array", "(", "new_returns_a", ")", ",", "np", ".", "array", "(", "new_returns_b", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.VolumeBuffer.compare_delta": [[153, 161], ["active_learning.VolumeBuffer.volume_removal"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.VolumeBuffer.volume_removal"], ["", "", "def", "compare_delta", "(", "self", ",", "w_posterior", ",", "new_returns_a", ",", "new_returns_b", ",", "logs_a", "=", "None", ",", "logs_b", "=", "None", ",", "random", "=", "False", ")", ":", "\n", "        ", "delta", "=", "new_returns_a", "-", "new_returns_b", "\n", "volume_delta", "=", "self", ".", "volume_removal", "(", "w_posterior", ",", "delta", ")", "\n", "if", "volume_delta", ">", "self", ".", "best_volume", "or", "random", ":", "\n", "            ", "self", ".", "best_volume", "=", "volume_delta", "\n", "self", ".", "best_delta", "=", "delta", "\n", "self", ".", "best_observed_returns", "=", "(", "new_returns_a", ",", "new_returns_b", ")", "\n", "self", ".", "best_returns", "=", "(", "logs_a", ",", "logs_b", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.active_learning.VolumeBuffer.reset": [[162, 167], ["None"], "methods", ["None"], ["", "", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "best_volume", "=", "-", "np", ".", "inf", "\n", "self", ".", "best_delta", "=", "None", "\n", "self", ".", "best_returns", "=", "None", "\n", "self", ".", "best_observed_returns", "=", "(", "None", ",", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.preference_giver.TargetGiverv3.__init__": [[7, 9], ["numpy.array"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "target", ")", ":", "\n", "        ", "self", ".", "target", "=", "np", ".", "array", "(", "target", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.preference_giver.TargetGiverv3.query_pair": [[10, 20], ["None"], "methods", ["None"], ["", "def", "query_pair", "(", "self", ",", "ret_a", ",", "ret_b", ")", ":", "\n", "        ", "dist_a", "=", "(", "(", "ret_a", "-", "self", ".", "target", ")", "**", "2", ")", ".", "sum", "(", ")", "\n", "dist_b", "=", "(", "(", "ret_b", "-", "self", ".", "target", ")", "**", "2", ")", ".", "sum", "(", ")", "\n", "\n", "if", "dist_a", "<", "dist_b", ":", "\n", "            ", "return", "[", "1", ",", "0", "]", "\n", "", "elif", "dist_b", "<", "dist_a", ":", "\n", "            ", "return", "[", "0", ",", "1", "]", "\n", "", "else", ":", "\n", "            ", "return", "[", "0.5", ",", "0.5", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.preference_giver.PreferenceGiverv3.__init__": [[23, 33], ["len", "sum", "preference_giver.PreferenceGiverv3.ratio_normalized.append"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "ratio", ",", "pbrl", "=", "False", ")", ":", "\n", "        ", "self", ".", "ratio", "=", "ratio", "\n", "self", ".", "d", "=", "len", "(", "ratio", ")", "\n", "self", ".", "ratio_normalized", "=", "[", "]", "\n", "self", ".", "pbrl", "=", "pbrl", "\n", "\n", "ratio_sum", "=", "sum", "(", "ratio", ")", "\n", "\n", "for", "elem", "in", "ratio", ":", "\n", "            ", "self", ".", "ratio_normalized", ".", "append", "(", "elem", "/", "ratio_sum", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.preference_giver.PreferenceGiverv3.query_pair": [[34, 85], ["range", "sum", "sum", "range", "scipy.entropy", "scipy.entropy", "ret_a.copy", "ret_b.copy", "ret_a_normalized.append", "ret_b_normalized.append", "print", "print", "ret_a.copy", "ret_b.copy", "numpy.isclose", "numpy.random.rand"], "methods", ["None"], ["", "", "def", "query_pair", "(", "self", ",", "ret_a", ",", "ret_b", ")", ":", "\n", "\n", "        ", "if", "self", ".", "pbrl", ":", "\n", "            ", "ret_a_copy", "=", "ret_a", ".", "copy", "(", ")", "[", ":", "-", "1", "]", "\n", "ret_b_copy", "=", "ret_b", ".", "copy", "(", ")", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "ret_a_copy", "=", "ret_a", ".", "copy", "(", ")", "\n", "ret_b_copy", "=", "ret_b", ".", "copy", "(", ")", "\n", "\n", "", "ret_a_normalized", "=", "[", "]", "\n", "ret_b_normalized", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "d", ")", ":", "\n", "# To avoid numerical instabilities in KL", "\n", "            ", "ret_a_copy", "[", "i", "]", "+=", "1e-5", "\n", "ret_b_copy", "[", "i", "]", "+=", "1e-5", "\n", "\n", "", "ret_a_sum", "=", "sum", "(", "ret_a_copy", ")", "\n", "ret_b_sum", "=", "sum", "(", "ret_b_copy", ")", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "d", ")", ":", "\n", "            ", "ret_a_normalized", ".", "append", "(", "ret_a_copy", "[", "i", "]", "/", "ret_a_sum", ")", "\n", "ret_b_normalized", ".", "append", "(", "ret_b_copy", "[", "i", "]", "/", "ret_b_sum", ")", "\n", "\n", "", "kl_a", "=", "st", ".", "entropy", "(", "ret_a_normalized", ",", "self", ".", "ratio_normalized", ")", "\n", "kl_b", "=", "st", ".", "entropy", "(", "ret_b_normalized", ",", "self", ".", "ratio_normalized", ")", "\n", "\n", "if", "self", ".", "pbrl", ":", "\n", "            ", "print", "(", "kl_a", ")", "\n", "print", "(", "kl_b", ")", "\n", "\n", "if", "ret_a", "[", "-", "1", "]", "<", "ret_b", "[", "-", "1", "]", ":", "\n", "                ", "return", "[", "0", ",", "1", "]", "\n", "", "elif", "ret_a", "[", "-", "1", "]", ">", "ret_b", "[", "-", "1", "]", ":", "\n", "                ", "return", "[", "1", ",", "0", "]", "\n", "", "else", ":", "\n", "                ", "if", "np", ".", "isclose", "(", "kl_a", ",", "kl_b", ",", "rtol", "=", "1e-5", ")", ":", "\n", "                    ", "preference", "=", "[", "0.5", ",", "0.5", "]", "\n", "", "elif", "kl_a", "<", "kl_b", ":", "\n", "                    ", "preference", "=", "[", "1", ",", "0", "]", "\n", "", "else", ":", "\n", "                    ", "preference", "=", "[", "0", ",", "1", "]", "\n", "", "return", "preference", "\n", "", "", "else", ":", "\n", "            ", "if", "kl_a", "<", "kl_b", ":", "\n", "                ", "preference", "=", "1", "\n", "", "elif", "kl_b", "<", "kl_a", ":", "\n", "                ", "preference", "=", "-", "1", "\n", "", "else", ":", "\n", "                ", "preference", "=", "1", "if", "np", ".", "random", ".", "rand", "(", ")", "<", "0.5", "else", "-", "1", "\n", "", "return", "preference", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.preference_giver.MaliciousPreferenceGiverv3.__init__": [[88, 90], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "bad_idx", ")", ":", "\n", "        ", "self", ".", "bad_idx", "=", "bad_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.preference_giver.MaliciousPreferenceGiverv3.query_pair": [[91, 104], ["numpy.random.rand"], "methods", ["None"], ["", "def", "query_pair", "(", "self", ",", "ret_a", ",", "ret_b", ")", ":", "\n", "# Assumes negative reward for bad_idx component", "\n", "        ", "damage_a", "=", "-", "ret_a", "[", "self", ".", "bad_idx", "]", "\n", "damage_b", "=", "-", "ret_b", "[", "self", ".", "bad_idx", "]", "\n", "\n", "if", "damage_a", ">", "damage_b", ":", "\n", "            ", "preference", "=", "1", "\n", "", "elif", "damage_b", ">", "damage_a", ":", "\n", "            ", "preference", "=", "-", "1", "\n", "", "else", ":", "\n", "            ", "preference", "=", "1", "if", "np", ".", "random", ".", "rand", "(", ")", "<", "0.5", "else", "-", "1", "\n", "\n", "", "return", "preference", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.preference_giver.PbRLPreferenceGiverv2.__init__": [[107, 109], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.preference_giver.PbRLPreferenceGiverv2.query_pair": [[110, 137], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "query_pair", "(", "ret_a", ",", "ret_b", ",", "primary", "=", "False", ")", ":", "\n", "        ", "ppl_saved_a", "=", "ret_a", "[", "1", "]", "\n", "goal_time_a", "=", "ret_a", "[", "0", "]", "\n", "ppl_saved_b", "=", "ret_b", "[", "1", "]", "\n", "goal_time_b", "=", "ret_b", "[", "0", "]", "\n", "\n", "if", "primary", ":", "\n", "            ", "if", "goal_time_a", ">", "goal_time_b", ":", "\n", "                ", "preference", "=", "[", "1", ",", "0", "]", "\n", "", "elif", "goal_time_b", ">", "goal_time_a", ":", "\n", "                ", "preference", "=", "[", "0", ",", "1", "]", "\n", "", "else", ":", "\n", "                ", "preference", "=", "[", "0.5", ",", "0.5", "]", "\n", "", "", "else", ":", "\n", "            ", "if", "ppl_saved_a", ">", "ppl_saved_b", ":", "\n", "                ", "preference", "=", "[", "1", ",", "0", "]", "\n", "", "elif", "ppl_saved_b", ">", "ppl_saved_a", ":", "\n", "                ", "preference", "=", "[", "0", ",", "1", "]", "\n", "", "elif", "goal_time_a", ">", "goal_time_b", ":", "\n", "                ", "preference", "=", "[", "1", ",", "0", "]", "\n", "", "elif", "goal_time_b", ">", "goal_time_a", ":", "\n", "                ", "preference", "=", "[", "0", ",", "1", "]", "\n", "", "else", ":", "\n", "                ", "preference", "=", "[", "0.5", ",", "0.5", "]", "\n", "\n", "", "", "return", "preference", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.preference_giver.PbRLSoftPreferenceGiverv2.__init__": [[142, 144], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "threshold", ")", ":", "\n", "        ", "self", ".", "threshold", "=", "threshold", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.preference_giver.PbRLSoftPreferenceGiverv2.query_pair": [[145, 162], ["preference_giver.PbRLPreferenceGiverv2.query_pair"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.preference_giver.PbRLSoftPreferenceGiverv2.query_pair"], ["", "def", "query_pair", "(", "self", ",", "ret_a", ",", "ret_b", ")", ":", "\n", "        ", "ppl_saved_a", "=", "ret_a", "[", "1", "]", "\n", "goal_time_a", "=", "ret_a", "[", "0", "]", "\n", "ppl_saved_b", "=", "ret_b", "[", "1", "]", "\n", "goal_time_b", "=", "ret_b", "[", "0", "]", "\n", "\n", "if", "ppl_saved_a", "<", "self", ".", "threshold", "and", "ppl_saved_b", "<", "self", ".", "threshold", ":", "\n", "            ", "preference", "=", "PbRLPreferenceGiverv2", ".", "query_pair", "(", "ret_a", ",", "ret_b", ")", "\n", "", "else", ":", "\n", "            ", "if", "goal_time_a", ">", "goal_time_b", ":", "\n", "                ", "preference", "=", "[", "1", ",", "0", "]", "\n", "", "elif", "goal_time_b", ">", "goal_time_a", ":", "\n", "                ", "preference", "=", "[", "0", ",", "1", "]", "\n", "", "else", ":", "\n", "                ", "preference", "=", "[", "0.5", ",", "0.5", "]", "\n", "\n", "", "", "return", "preference", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.airl.DiscriminatorMLP.__init__": [[12, 32], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceBuffer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "state_shape", ",", "in_channels", "=", "6", ")", ":", "\n", "        ", "super", "(", "DiscriminatorMLP", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "state_shape", "=", "state_shape", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "\n", "# Layers", "\n", "# self.action_embedding = nn.Linear(n_actions, state_shape[0]*state_shape[1])", "\n", "self", ".", "reward_l1", "=", "nn", ".", "Linear", "(", "self", ".", "in_channels", "*", "self", ".", "state_shape", "[", "0", "]", "*", "self", ".", "state_shape", "[", "1", "]", ",", "256", ")", "\n", "self", ".", "reward_l2", "=", "nn", ".", "Linear", "(", "256", ",", "512", ")", "\n", "self", ".", "reward_l3", "=", "nn", ".", "Linear", "(", "512", ",", "256", ")", "\n", "self", ".", "reward_out", "=", "nn", ".", "Linear", "(", "256", ",", "1", ")", "\n", "\n", "self", ".", "value_l1", "=", "nn", ".", "Linear", "(", "self", ".", "in_channels", "*", "self", ".", "state_shape", "[", "0", "]", "*", "self", ".", "state_shape", "[", "1", "]", ",", "256", ")", "\n", "self", ".", "value_l2", "=", "nn", ".", "Linear", "(", "256", ",", "512", ")", "\n", "self", ".", "value_l3", "=", "nn", ".", "Linear", "(", "512", ",", "256", ")", "\n", "self", ".", "value_out", "=", "nn", ".", "Linear", "(", "256", ",", "1", ")", "\n", "\n", "# Activation", "\n", "self", ".", "relu", "=", "nn", ".", "LeakyReLU", "(", "0.01", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.airl.DiscriminatorMLP.g": [[33, 43], ["state.view.view.view", "airl.DiscriminatorMLP.relu", "airl.DiscriminatorMLP.relu", "airl.DiscriminatorMLP.relu", "airl.DiscriminatorMLP.view", "airl.DiscriminatorMLP.reward_out", "airl.DiscriminatorMLP.reward_l1", "airl.DiscriminatorMLP.reward_l2", "airl.DiscriminatorMLP.reward_l3"], "methods", ["None"], ["", "def", "g", "(", "self", ",", "state", ")", ":", "\n", "        ", "state", "=", "state", ".", "view", "(", "state", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "reward_l1", "(", "state", ")", ")", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "reward_l2", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "reward_l3", "(", "x", ")", ")", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "x", "=", "self", ".", "reward_out", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.airl.DiscriminatorMLP.h": [[44, 54], ["state.view.view.view", "airl.DiscriminatorMLP.relu", "airl.DiscriminatorMLP.relu", "airl.DiscriminatorMLP.relu", "airl.DiscriminatorMLP.view", "airl.DiscriminatorMLP.value_out", "airl.DiscriminatorMLP.value_l1", "airl.DiscriminatorMLP.value_l2", "airl.DiscriminatorMLP.value_l3"], "methods", ["None"], ["", "def", "h", "(", "self", ",", "state", ")", ":", "\n", "        ", "state", "=", "state", ".", "view", "(", "state", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "value_l1", "(", "state", ")", ")", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "value_l2", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "value_l3", "(", "x", ")", ")", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "x", "=", "self", ".", "value_out", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.airl.DiscriminatorMLP.forward": [[55, 63], ["airl.DiscriminatorMLP.g", "airl.DiscriminatorMLP.h", "airl.DiscriminatorMLP.h"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.airl.Discriminator.g", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.airl.Discriminator.h", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.airl.Discriminator.h"], ["", "def", "forward", "(", "self", ",", "state", ",", "next_state", ",", "gamma", ")", ":", "\n", "        ", "reward", "=", "self", ".", "g", "(", "state", ")", "\n", "value_state", "=", "self", ".", "h", "(", "state", ")", "\n", "value_next_state", "=", "self", ".", "h", "(", "next_state", ")", "\n", "\n", "advantage", "=", "reward", "+", "gamma", "*", "value_next_state", "-", "value_state", "\n", "\n", "return", "advantage", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.airl.DiscriminatorMLP.discriminate": [[64, 72], ["airl.DiscriminatorMLP.forward", "advantage.squeeze.squeeze.squeeze", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "print"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.forward"], ["", "def", "discriminate", "(", "self", ",", "state", ",", "next_state", ",", "gamma", ",", "action_probability", ")", ":", "\n", "        ", "advantage", "=", "self", ".", "forward", "(", "state", ",", "next_state", ",", "gamma", ")", "\n", "advantage", "=", "advantage", ".", "squeeze", "(", "1", ")", "\n", "exp_advantage", "=", "torch", ".", "exp", "(", "advantage", ")", "\n", "#print((exp_advantage/(exp_advantage + action_probability + 1e-5)).shape)", "\n", "\n", "print", "(", "exp_advantage", "/", "(", "exp_advantage", "+", "action_probability", ")", ")", "\n", "return", "exp_advantage", "/", "(", "exp_advantage", "+", "action_probability", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.airl.DiscriminatorMLP.predict_reward": [[73, 78], ["airl.DiscriminatorMLP.forward", "advantage.squeeze.squeeze.squeeze", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.forward"], ["", "def", "predict_reward", "(", "self", ",", "state", ",", "next_state", ",", "gamma", ",", "action_probability", ")", ":", "\n", "        ", "advantage", "=", "self", ".", "forward", "(", "state", ",", "next_state", ",", "gamma", ")", "\n", "advantage", "=", "advantage", ".", "squeeze", "(", "1", ")", "\n", "\n", "return", "advantage", "-", "torch", ".", "log", "(", "action_probability", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.airl.Discriminator.__init__": [[81, 110], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Linear", "torch.Linear", "torch.Linear", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceBuffer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "state_shape", ",", "in_channels", "=", "6", ",", "latent_dim", "=", "None", ")", ":", "\n", "        ", "super", "(", "Discriminator", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "state_shape", "=", "state_shape", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "eval", "=", "False", "\n", "self", ".", "utopia_point", "=", "None", "\n", "\n", "# Latent conditioning", "\n", "if", "latent_dim", "is", "not", "None", ":", "\n", "            ", "self", ".", "latent_dim", "=", "latent_dim", "\n", "self", ".", "latent_embedding_value", "=", "nn", ".", "Linear", "(", "latent_dim", ",", "state_shape", "[", "0", "]", "*", "state_shape", "[", "1", "]", ")", "\n", "self", ".", "latent_embedding_reward", "=", "nn", ".", "Linear", "(", "latent_dim", ",", "state_shape", "[", "0", "]", "*", "state_shape", "[", "1", "]", ")", "\n", "self", ".", "in_channels", "=", "in_channels", "+", "1", "\n", "\n", "# Layers", "\n", "# self.action_embedding = nn.Linear(n_actions, state_shape[0]*state_shape[1])", "\n", "", "self", ".", "reward_conv1", "=", "nn", ".", "Conv2d", "(", "in_channels", "=", "self", ".", "in_channels", ",", "out_channels", "=", "32", ",", "kernel_size", "=", "2", ")", "\n", "self", ".", "reward_conv2", "=", "nn", ".", "Conv2d", "(", "in_channels", "=", "32", ",", "out_channels", "=", "32", ",", "kernel_size", "=", "2", ")", "\n", "self", ".", "reward_conv3", "=", "nn", ".", "Conv2d", "(", "in_channels", "=", "32", ",", "out_channels", "=", "16", ",", "kernel_size", "=", "2", ")", "\n", "self", ".", "reward_out", "=", "nn", ".", "Linear", "(", "16", "*", "(", "state_shape", "[", "0", "]", "-", "3", ")", "*", "(", "state_shape", "[", "1", "]", "-", "3", ")", ",", "1", ")", "\n", "\n", "self", ".", "value_conv1", "=", "nn", ".", "Conv2d", "(", "in_channels", "=", "self", ".", "in_channels", ",", "out_channels", "=", "32", ",", "kernel_size", "=", "2", ")", "\n", "self", ".", "value_conv2", "=", "nn", ".", "Conv2d", "(", "in_channels", "=", "32", ",", "out_channels", "=", "32", ",", "kernel_size", "=", "2", ")", "\n", "self", ".", "value_conv3", "=", "nn", ".", "Conv2d", "(", "in_channels", "=", "32", ",", "out_channels", "=", "16", ",", "kernel_size", "=", "2", ")", "\n", "self", ".", "value_out", "=", "nn", ".", "Linear", "(", "16", "*", "(", "state_shape", "[", "0", "]", "-", "3", ")", "*", "(", "state_shape", "[", "1", "]", "-", "3", ")", ",", "1", ")", "\n", "\n", "# Activation", "\n", "self", ".", "relu", "=", "nn", ".", "LeakyReLU", "(", "0.01", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.airl.Discriminator.set_eval": [[111, 113], ["None"], "methods", ["None"], ["", "def", "set_eval", "(", "self", ")", ":", "\n", "        ", "self", ".", "eval", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.airl.Discriminator.g": [[114, 133], ["torch.cat.view", "torch.cat.view", "torch.cat.view", "airl.Discriminator.relu", "airl.Discriminator.relu", "airl.Discriminator.relu", "airl.Discriminator.view", "airl.Discriminator.reward_out", "torch.one_hot().float().to", "torch.one_hot().float().to", "torch.one_hot().float().to", "airl.Discriminator.latent_embedding_reward", "latent.repeat_interleave.repeat_interleave.view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "airl.Discriminator.reward_conv1", "airl.Discriminator.reward_conv2", "airl.Discriminator.reward_conv3", "latent.repeat_interleave.repeat_interleave.repeat_interleave", "torch.one_hot().float", "torch.one_hot().float", "torch.one_hot().float", "torch.one_hot", "torch.one_hot", "torch.one_hot", "latent.repeat_interleave.repeat_interleave.long"], "methods", ["None"], ["", "def", "g", "(", "self", ",", "state", ",", "latent", "=", "None", ")", ":", "\n", "        ", "state", "=", "state", ".", "view", "(", "-", "1", ",", "self", ".", "in_channels", ",", "self", ".", "state_shape", "[", "0", "]", ",", "self", ".", "state_shape", "[", "1", "]", ")", "\n", "\n", "if", "latent", "is", "not", "None", ":", "\n", "            ", "latent", "=", "F", ".", "one_hot", "(", "latent", ".", "long", "(", ")", ",", "self", ".", "latent_dim", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "latent", "=", "self", ".", "latent_embedding_reward", "(", "latent", ")", "\n", "latent", "=", "latent", ".", "view", "(", "-", "1", ",", "1", ",", "self", ".", "state_shape", "[", "0", "]", ",", "self", ".", "state_shape", "[", "1", "]", ")", "\n", "if", "latent", ".", "shape", "[", "0", "]", "==", "1", ":", "\n", "                ", "latent", "=", "latent", ".", "repeat_interleave", "(", "repeats", "=", "state", ".", "shape", "[", "0", "]", ",", "dim", "=", "0", ")", "\n", "\n", "", "state", "=", "torch", ".", "cat", "(", "[", "state", ",", "latent", "]", ",", "dim", "=", "1", ")", "\n", "# Conv + Linear", "\n", "", "x", "=", "self", ".", "relu", "(", "self", ".", "reward_conv1", "(", "state", ")", ")", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "reward_conv2", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "reward_conv3", "(", "x", ")", ")", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "x", "=", "self", ".", "reward_out", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.airl.Discriminator.h": [[134, 153], ["torch.cat.view", "torch.cat.view", "torch.cat.view", "airl.Discriminator.relu", "airl.Discriminator.relu", "airl.Discriminator.relu", "airl.Discriminator.view", "airl.Discriminator.value_out", "torch.one_hot().float().to", "torch.one_hot().float().to", "torch.one_hot().float().to", "airl.Discriminator.latent_embedding_value", "latent.repeat_interleave.repeat_interleave.view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "airl.Discriminator.value_conv1", "airl.Discriminator.value_conv2", "airl.Discriminator.value_conv3", "latent.repeat_interleave.repeat_interleave.repeat_interleave", "torch.one_hot().float", "torch.one_hot().float", "torch.one_hot().float", "torch.one_hot", "torch.one_hot", "torch.one_hot", "latent.repeat_interleave.repeat_interleave.long"], "methods", ["None"], ["", "def", "h", "(", "self", ",", "state", ",", "latent", "=", "None", ")", ":", "\n", "        ", "state", "=", "state", ".", "view", "(", "-", "1", ",", "self", ".", "in_channels", ",", "self", ".", "state_shape", "[", "0", "]", ",", "self", ".", "state_shape", "[", "1", "]", ")", "\n", "\n", "if", "latent", "is", "not", "None", ":", "\n", "            ", "latent", "=", "F", ".", "one_hot", "(", "latent", ".", "long", "(", ")", ",", "self", ".", "latent_dim", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "latent", "=", "self", ".", "latent_embedding_value", "(", "latent", ")", "\n", "latent", "=", "latent", ".", "view", "(", "-", "1", ",", "1", ",", "self", ".", "state_shape", "[", "0", "]", ",", "self", ".", "state_shape", "[", "1", "]", ")", "\n", "if", "latent", ".", "shape", "[", "0", "]", "==", "1", ":", "\n", "                ", "latent", "=", "latent", ".", "repeat_interleave", "(", "repeats", "=", "state", ".", "shape", "[", "0", "]", ",", "dim", "=", "0", ")", "\n", "\n", "", "state", "=", "torch", ".", "cat", "(", "[", "state", ",", "latent", "]", ",", "dim", "=", "1", ")", "\n", "# Conv + Linear", "\n", "", "x", "=", "self", ".", "relu", "(", "self", ".", "value_conv1", "(", "state", ")", ")", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "value_conv2", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "value_conv3", "(", "x", ")", ")", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "x", "=", "self", ".", "value_out", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.airl.Discriminator.forward": [[154, 165], ["airl.Discriminator.g", "airl.Discriminator.h", "airl.Discriminator.h", "numpy.abs"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.airl.Discriminator.g", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.airl.Discriminator.h", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.airl.Discriminator.h"], ["", "def", "forward", "(", "self", ",", "state", ",", "next_state", ",", "gamma", ",", "latent", "=", "None", ")", ":", "\n", "        ", "reward", "=", "self", ".", "g", "(", "state", ",", "latent", ")", "\n", "value_state", "=", "self", ".", "h", "(", "state", ",", "latent", ")", "\n", "value_next_state", "=", "self", ".", "h", "(", "next_state", ",", "latent", ")", "\n", "\n", "advantage", "=", "reward", "+", "gamma", "*", "value_next_state", "-", "value_state", "\n", "\n", "if", "self", ".", "eval", ":", "\n", "            ", "return", "advantage", "/", "np", ".", "abs", "(", "self", ".", "utopia_point", ")", "\n", "", "else", ":", "\n", "            ", "return", "advantage", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.airl.Discriminator.discriminate": [[166, 177], ["airl.Discriminator.squeeze", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "print", "airl.Discriminator.forward", "airl.Discriminator.forward"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.forward", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.forward"], ["", "", "def", "discriminate", "(", "self", ",", "state", ",", "next_state", ",", "gamma", ",", "action_probability", ",", "latent", "=", "None", ")", ":", "\n", "        ", "if", "latent", "is", "not", "None", ":", "\n", "            ", "advantage", "=", "self", ".", "forward", "(", "state", ",", "next_state", ",", "gamma", ",", "latent", ")", "\n", "", "else", ":", "\n", "            ", "advantage", "=", "self", ".", "forward", "(", "state", ",", "next_state", ",", "gamma", ")", "\n", "", "advantage", "=", "advantage", ".", "squeeze", "(", "1", ")", "\n", "exp_advantage", "=", "torch", ".", "exp", "(", "advantage", ")", "\n", "#print((exp_advantage/(exp_advantage + action_probability + 1e-5)).shape)", "\n", "\n", "print", "(", "exp_advantage", "/", "(", "exp_advantage", "+", "action_probability", ")", ")", "\n", "return", "exp_advantage", "/", "(", "exp_advantage", "+", "action_probability", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.airl.Discriminator.predict_reward": [[178, 187], ["airl.Discriminator.squeeze", "airl.Discriminator.forward", "airl.Discriminator.forward", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.forward", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.forward"], ["", "def", "predict_reward", "(", "self", ",", "state", ",", "next_state", ",", "gamma", ",", "action_probability", ",", "latent", "=", "None", ")", ":", "\n", "        ", "if", "latent", "is", "not", "None", ":", "\n", "            ", "advantage", "=", "self", ".", "forward", "(", "state", ",", "next_state", ",", "gamma", ",", "latent", ")", "\n", "", "else", ":", "\n", "            ", "advantage", "=", "self", ".", "forward", "(", "state", ",", "next_state", ",", "gamma", ")", "\n", "\n", "", "advantage", "=", "advantage", ".", "squeeze", "(", "1", ")", "\n", "\n", "return", "advantage", "-", "torch", ".", "log", "(", "action_probability", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.airl.Discriminator.estimate_utopia": [[188, 225], ["GymWrapper", "GymWrapper.reset", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "range", "imitation_policy.act", "GymWrapper.step", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "airl.Discriminator.forward().item", "GymWrapper.reset.copy", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "sum", "len", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "GymWrapper.reset", "estimated_returns.append", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "airl.Discriminator.forward", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.VecEnv.reset", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.ppo.PPO.act", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.VecEnv.step", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.VecEnv.reset", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.forward"], ["", "def", "estimate_utopia", "(", "self", ",", "imitation_policy", ",", "config", ",", "steps", "=", "10000", ")", ":", "\n", "        ", "env", "=", "GymWrapper", "(", "config", ".", "env_id", ")", "\n", "states", "=", "env", ".", "reset", "(", ")", "\n", "states_tensor", "=", "torch", ".", "tensor", "(", "states", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "\n", "# Fetch Shapes", "\n", "n_actions", "=", "env", ".", "action_space", ".", "n", "\n", "obs_shape", "=", "env", ".", "observation_space", ".", "shape", "\n", "state_shape", "=", "obs_shape", "[", ":", "-", "1", "]", "\n", "in_channels", "=", "obs_shape", "[", "-", "1", "]", "\n", "\n", "# Init returns", "\n", "estimated_returns", "=", "[", "]", "\n", "running_returns", "=", "0", "\n", "\n", "for", "t", "in", "range", "(", "steps", ")", ":", "\n", "            ", "actions", ",", "log_probs", "=", "imitation_policy", ".", "act", "(", "states_tensor", ")", "\n", "next_states", ",", "rewards", ",", "done", ",", "info", "=", "env", ".", "step", "(", "actions", ")", "\n", "\n", "airl_state", "=", "torch", ".", "tensor", "(", "states", ")", ".", "to", "(", "device", ")", ".", "float", "(", ")", "\n", "airl_next_state", "=", "torch", ".", "tensor", "(", "next_states", ")", ".", "to", "(", "device", ")", ".", "float", "(", ")", "\n", "airl_rewards", "=", "self", ".", "forward", "(", "airl_state", ",", "airl_next_state", ",", "config", ".", "gamma", ")", ".", "item", "(", ")", "\n", "if", "done", ":", "\n", "                ", "airl_rewards", "=", "0", "\n", "next_states", "=", "env", ".", "reset", "(", ")", "\n", "", "running_returns", "+=", "airl_rewards", "\n", "\n", "if", "done", ":", "\n", "                ", "estimated_returns", ".", "append", "(", "running_returns", ")", "\n", "running_returns", "=", "0", "\n", "\n", "", "states", "=", "next_states", ".", "copy", "(", ")", "\n", "states_tensor", "=", "torch", ".", "tensor", "(", "states", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "\n", "", "self", ".", "utopia_point", "=", "sum", "(", "estimated_returns", ")", "/", "len", "(", "estimated_returns", ")", "\n", "\n", "return", "self", ".", "utopia_point", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.airl.training_sampler": [[227, 273], ["range", "numpy.random.randint", "numpy.random.randint", "states.append", "next_states.append", "action_probabilities.append", "labels.append", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().long().to", "torch.tensor().long().to", "torch.tensor().long().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "len", "ppo.forward", "action_probability.squeeze.squeeze", "latents.append", "ppo.forward", "action_probability.squeeze.squeeze", "action_probability[].item", "len", "latent_posterior.sample_prior", "torch.tensor().to.to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().to.cpu().item", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().to.cpu", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.forward", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.forward"], ["", "", "def", "training_sampler", "(", "expert_trajectories", ",", "policy_trajectories", ",", "ppo", ",", "batch_size", ",", "latent_posterior", "=", "None", ")", ":", "\n", "    ", "states", "=", "[", "]", "\n", "action_probabilities", "=", "[", "]", "\n", "next_states", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "latents", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "# 1 if (s,a,s') comes from expert, 0 otherwise", "\n", "# expert_boolean = np.random.randint(2)", "\n", "        ", "expert_boolean", "=", "1", "if", "i", "<", "batch_size", "/", "2", "else", "0", "\n", "if", "expert_boolean", "==", "1", ":", "\n", "            ", "selected_trajectories", "=", "expert_trajectories", "\n", "", "else", ":", "\n", "            ", "selected_trajectories", "=", "policy_trajectories", "\n", "\n", "", "random_tau_idx", "=", "np", ".", "random", ".", "randint", "(", "len", "(", "selected_trajectories", ")", ")", "\n", "random_tau", "=", "selected_trajectories", "[", "random_tau_idx", "]", "[", "'states'", "]", "\n", "random_state_idx", "=", "np", ".", "random", ".", "randint", "(", "len", "(", "random_tau", ")", "-", "1", ")", "\n", "state", "=", "random_tau", "[", "random_state_idx", "]", "\n", "next_state", "=", "random_tau", "[", "random_state_idx", "+", "1", "]", "\n", "\n", "# Sample random latent to condition ppo on for expert samples", "\n", "if", "latent_posterior", "is", "not", "None", ":", "\n", "            ", "if", "expert_boolean", "==", "1", ":", "\n", "                ", "latent", "=", "latent_posterior", ".", "sample_prior", "(", ")", "\n", "latent", "=", "latent", ".", "to", "(", "device", ")", "\n", "", "else", ":", "\n", "                ", "latent", "=", "torch", ".", "tensor", "(", "selected_trajectories", "[", "random_tau_idx", "]", "[", "'latents'", "]", ")", ".", "to", "(", "device", ")", "\n", "\n", "", "action_probability", ",", "_", "=", "ppo", ".", "forward", "(", "torch", ".", "tensor", "(", "state", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", ",", "latent", ")", "\n", "action_probability", "=", "action_probability", ".", "squeeze", "(", "0", ")", "\n", "latents", ".", "append", "(", "latent", ".", "cpu", "(", ")", ".", "item", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "action_probability", ",", "_", "=", "ppo", ".", "forward", "(", "torch", ".", "tensor", "(", "state", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", ")", "\n", "action_probability", "=", "action_probability", ".", "squeeze", "(", "0", ")", "\n", "# Get the action that was actually selected in the trajectory", "\n", "", "selected_action", "=", "selected_trajectories", "[", "random_tau_idx", "]", "[", "'actions'", "]", "[", "random_state_idx", "]", "\n", "\n", "states", ".", "append", "(", "state", ")", "\n", "next_states", ".", "append", "(", "next_state", ")", "\n", "action_probabilities", ".", "append", "(", "action_probability", "[", "selected_action", "]", ".", "item", "(", ")", ")", "\n", "labels", ".", "append", "(", "expert_boolean", ")", "\n", "\n", "", "return", "torch", ".", "tensor", "(", "states", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", ",", "torch", ".", "tensor", "(", "next_states", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", ",", "torch", ".", "tensor", "(", "action_probabilities", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", ",", "torch", ".", "tensor", "(", "labels", ")", ".", "long", "(", ")", ".", "to", "(", "device", ")", ",", "torch", ".", "tensor", "(", "latents", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.airl.update_discriminator": [[275, 299], ["torch.CrossEntropyLoss", "airl.training_sampler", "torch.cat", "torch.cat", "torch.cat", "nn.CrossEntropyLoss.", "torch.argmax", "torch.argmax", "torch.argmax", "optimizer.zero_grad", "criterion.backward", "optimizer.step", "len", "discriminator.forward", "discriminator.forward", "criterion.item", "torch.mean().item", "torch.mean().item", "torch.mean().item", "torch.mean().item", "torch.mean().item", "torch.mean().item", "torch.log().unsqueeze", "torch.log().unsqueeze", "torch.log().unsqueeze", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.log", "torch.log", "torch.log"], "function", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.airl.training_sampler", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.VecEnv.step", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.forward", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.forward"], ["", "def", "update_discriminator", "(", "discriminator", ",", "optimizer", ",", "gamma", ",", "expert_trajectories", ",", "policy_trajectories", ",", "ppo", ",", "batch_size", ",", "\n", "latent_posterior", "=", "None", ")", ":", "\n", "    ", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "states", ",", "next_states", ",", "action_probabilities", ",", "labels", ",", "latents", "=", "training_sampler", "(", "expert_trajectories", ",", "policy_trajectories", ",", "ppo", ",", "batch_size", ",", "latent_posterior", ")", "\n", "if", "len", "(", "latents", ")", ">", "0", ":", "\n", "        ", "advantages", "=", "discriminator", ".", "forward", "(", "states", ",", "next_states", ",", "gamma", ",", "latents", ")", "\n", "", "else", ":", "\n", "        ", "advantages", "=", "discriminator", ".", "forward", "(", "states", ",", "next_states", ",", "gamma", ")", "\n", "# Cat advantages and log_probs to (batch_size, 2)", "\n", "", "class_predictions", "=", "torch", ".", "cat", "(", "[", "torch", ".", "log", "(", "action_probabilities", ")", ".", "unsqueeze", "(", "1", ")", ",", "advantages", "]", ",", "dim", "=", "1", ")", "\n", "# Compute Loss function", "\n", "loss", "=", "criterion", "(", "class_predictions", ",", "labels", ")", "\n", "# Compute Accuracies", "\n", "label_predictions", "=", "torch", ".", "argmax", "(", "class_predictions", ",", "dim", "=", "1", ")", "\n", "predicted_fake", "=", "(", "label_predictions", "[", "labels", "==", "0", "]", "==", "0", ")", ".", "float", "(", ")", "\n", "predicted_expert", "=", "(", "label_predictions", "[", "labels", "==", "1", "]", "==", "1", ")", ".", "float", "(", ")", "\n", "\n", "# print(loss.item())", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "return", "loss", ".", "item", "(", ")", ",", "torch", ".", "mean", "(", "predicted_fake", ")", ".", "item", "(", ")", ",", "torch", ".", "mean", "(", "predicted_expert", ")", ".", "item", "(", ")", "", "", ""]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.ppo.PPO.__init__": [[12, 28], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Softmax", "torch.Softmax", "torch.Softmax"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceBuffer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "state_shape", ",", "in_channels", "=", "6", ",", "n_actions", "=", "9", ")", ":", "\n", "        ", "super", "(", "PPO", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# General Parameters", "\n", "self", ".", "state_shape", "=", "state_shape", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "\n", "# Network Layers", "\n", "self", ".", "l1", "=", "nn", ".", "Conv2d", "(", "in_channels", "=", "self", ".", "in_channels", ",", "out_channels", "=", "64", ",", "kernel_size", "=", "2", ")", "\n", "self", ".", "l2", "=", "nn", ".", "Conv2d", "(", "in_channels", "=", "64", ",", "out_channels", "=", "256", ",", "kernel_size", "=", "2", ")", "\n", "self", ".", "actor_l3", "=", "nn", ".", "Conv2d", "(", "in_channels", "=", "256", ",", "out_channels", "=", "32", ",", "kernel_size", "=", "2", ")", "\n", "self", ".", "critic_l3", "=", "nn", ".", "Conv2d", "(", "in_channels", "=", "256", ",", "out_channels", "=", "32", ",", "kernel_size", "=", "2", ")", "\n", "self", ".", "actor_out", "=", "nn", ".", "Linear", "(", "32", "*", "(", "state_shape", "[", "0", "]", "-", "3", ")", "*", "(", "state_shape", "[", "1", "]", "-", "3", ")", ",", "n_actions", ")", "\n", "self", ".", "critic_out", "=", "nn", ".", "Linear", "(", "32", "*", "(", "state_shape", "[", "0", "]", "-", "3", ")", "*", "(", "state_shape", "[", "1", "]", "-", "3", ")", ",", "1", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.ppo.PPO.forward": [[29, 41], ["ppo.PPO.view", "ppo.PPO.relu", "ppo.PPO.relu", "ppo.PPO.relu", "ppo.PPO.view", "ppo.PPO.relu", "ppo.PPO.view", "ppo.PPO.softmax", "ppo.PPO.critic_out", "ppo.PPO.l1", "ppo.PPO.l2", "ppo.PPO.actor_l3", "ppo.PPO.critic_l3", "ppo.PPO.actor_out"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", ".", "view", "(", "-", "1", ",", "self", ".", "in_channels", ",", "self", ".", "state_shape", "[", "0", "]", ",", "self", ".", "state_shape", "[", "1", "]", ")", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "l1", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "l2", "(", "x", ")", ")", "\n", "x_actor", "=", "self", ".", "relu", "(", "self", ".", "actor_l3", "(", "x", ")", ")", "\n", "x_actor", "=", "x_actor", ".", "view", "(", "x_actor", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "x_critic", "=", "self", ".", "relu", "(", "self", ".", "critic_l3", "(", "x", ")", ")", "\n", "x_critic", "=", "x_critic", ".", "view", "(", "x_critic", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "x_actor", "=", "self", ".", "softmax", "(", "self", ".", "actor_out", "(", "x_actor", ")", ")", "\n", "x_critic", "=", "self", ".", "critic_out", "(", "x_critic", ")", "\n", "\n", "return", "x_actor", ",", "x_critic", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.ppo.PPO.act": [[42, 47], ["ppo.PPO.forward", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical.sample", "torch.distributions.Categorical.sample", "torch.distributions.Categorical.sample", "torch.distributions.Categorical.sample.detach().cpu().numpy", "torch.distributions.Categorical.log_prob().detach().cpu().numpy", "torch.distributions.Categorical.log_prob().detach().cpu().numpy", "torch.distributions.Categorical.log_prob().detach().cpu().numpy", "torch.distributions.Categorical.sample.detach().cpu", "torch.distributions.Categorical.log_prob().detach().cpu", "torch.distributions.Categorical.log_prob().detach().cpu", "torch.distributions.Categorical.log_prob().detach().cpu", "torch.distributions.Categorical.sample.detach", "torch.distributions.Categorical.log_prob().detach", "torch.distributions.Categorical.log_prob().detach", "torch.distributions.Categorical.log_prob().detach", "torch.distributions.Categorical.log_prob", "torch.distributions.Categorical.log_prob", "torch.distributions.Categorical.log_prob"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.forward"], ["", "def", "act", "(", "self", ",", "state", ")", ":", "\n", "        ", "action_probabilities", ",", "_", "=", "self", ".", "forward", "(", "state", ")", "\n", "m", "=", "Categorical", "(", "action_probabilities", ")", "\n", "action", "=", "m", ".", "sample", "(", ")", "\n", "return", "action", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "m", ".", "log_prob", "(", "action", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.ppo.PPO.evaluate_trajectory": [[48, 57], ["torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "ppo.PPO.forward", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical.entropy().mean", "torch.distributions.Categorical.entropy().mean", "torch.distributions.Categorical.entropy().mean", "torch.distributions.Categorical.log_prob", "torch.distributions.Categorical.log_prob", "torch.distributions.Categorical.log_prob", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.distributions.Categorical.entropy", "torch.distributions.Categorical.entropy", "torch.distributions.Categorical.entropy", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.forward"], ["", "def", "evaluate_trajectory", "(", "self", ",", "tau", ")", ":", "\n", "        ", "trajectory_states", "=", "torch", ".", "tensor", "(", "tau", "[", "'states'", "]", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "trajectory_actions", "=", "torch", ".", "tensor", "(", "tau", "[", "'actions'", "]", ")", ".", "to", "(", "device", ")", "\n", "action_probabilities", ",", "critic_values", "=", "self", ".", "forward", "(", "trajectory_states", ")", "\n", "dist", "=", "Categorical", "(", "action_probabilities", ")", "\n", "action_entropy", "=", "dist", ".", "entropy", "(", ")", ".", "mean", "(", ")", "\n", "action_log_probabilities", "=", "dist", ".", "log_prob", "(", "trajectory_actions", ")", "\n", "\n", "return", "action_log_probabilities", ",", "torch", ".", "squeeze", "(", "critic_values", ")", ",", "action_entropy", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.ppo.TrajectoryDataset.__init__": [[60, 66], ["range"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "batch_size", ",", "n_workers", ")", ":", "\n", "        ", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "n_workers", "=", "n_workers", "\n", "self", ".", "trajectories", "=", "[", "]", "\n", "self", ".", "buffer", "=", "[", "{", "'states'", ":", "[", "]", ",", "'actions'", ":", "[", "]", ",", "'rewards'", ":", "[", "]", ",", "'log_probs'", ":", "[", "]", ",", "'latents'", ":", "None", ",", "'logs'", ":", "[", "]", "}", "\n", "for", "i", "in", "range", "(", "n_workers", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.ppo.TrajectoryDataset.reset_buffer": [[67, 69], ["None"], "methods", ["None"], ["", "def", "reset_buffer", "(", "self", ",", "i", ")", ":", "\n", "        ", "self", ".", "buffer", "[", "i", "]", "=", "{", "'states'", ":", "[", "]", ",", "'actions'", ":", "[", "]", ",", "'rewards'", ":", "[", "]", ",", "'log_probs'", ":", "[", "]", ",", "'latents'", ":", "None", ",", "'logs'", ":", "[", "]", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.ppo.TrajectoryDataset.reset_trajectories": [[70, 72], ["None"], "methods", ["None"], ["", "def", "reset_trajectories", "(", "self", ")", ":", "\n", "        ", "self", ".", "trajectories", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.ppo.TrajectoryDataset.write_tuple": [[73, 92], ["range", "[].append", "[].append", "[].append", "[].append", "len", "[].append", "ppo.TrajectoryDataset.trajectories.append", "ppo.TrajectoryDataset.reset_buffer", "ppo.TrajectoryDataset.buffer[].copy"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.ppo.TrajectoryDataset.reset_buffer"], ["", "def", "write_tuple", "(", "self", ",", "states", ",", "actions", ",", "rewards", ",", "done", ",", "log_probs", ",", "logs", "=", "None", ")", ":", "\n", "# Takes states of shape (n_workers, state_shape[0], state_shape[1])", "\n", "        ", "for", "i", "in", "range", "(", "self", ".", "n_workers", ")", ":", "\n", "            ", "self", ".", "buffer", "[", "i", "]", "[", "'states'", "]", ".", "append", "(", "states", "[", "i", "]", ")", "\n", "self", ".", "buffer", "[", "i", "]", "[", "'actions'", "]", ".", "append", "(", "actions", "[", "i", "]", ")", "\n", "self", ".", "buffer", "[", "i", "]", "[", "'rewards'", "]", ".", "append", "(", "rewards", "[", "i", "]", ")", "\n", "self", ".", "buffer", "[", "i", "]", "[", "'log_probs'", "]", ".", "append", "(", "log_probs", "[", "i", "]", ")", "\n", "\n", "if", "logs", "is", "not", "None", ":", "\n", "                ", "self", ".", "buffer", "[", "i", "]", "[", "'logs'", "]", ".", "append", "(", "logs", "[", "i", "]", ")", "\n", "\n", "", "if", "done", "[", "i", "]", ":", "\n", "                ", "self", ".", "trajectories", ".", "append", "(", "self", ".", "buffer", "[", "i", "]", ".", "copy", "(", ")", ")", "\n", "self", ".", "reset_buffer", "(", "i", ")", "\n", "\n", "", "", "if", "len", "(", "self", ".", "trajectories", ")", ">=", "self", ".", "batch_size", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.ppo.TrajectoryDataset.log_returns": [[93, 99], ["enumerate", "sum", "range", "len"], "methods", ["None"], ["", "", "def", "log_returns", "(", "self", ")", ":", "\n", "# Calculates (undiscounted) returns in self.trajectories", "\n", "        ", "returns", "=", "[", "0", "for", "i", "in", "range", "(", "len", "(", "self", ".", "trajectories", ")", ")", "]", "\n", "for", "i", ",", "tau", "in", "enumerate", "(", "self", ".", "trajectories", ")", ":", "\n", "            ", "returns", "[", "i", "]", "=", "sum", "(", "tau", "[", "'rewards'", "]", ")", "\n", "", "return", "returns", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.ppo.TrajectoryDataset.log_objectives": [[100, 107], ["enumerate", "numpy.array", "objective_logs.append", "list", "numpy.array().sum", "numpy.array"], "methods", ["None"], ["", "def", "log_objectives", "(", "self", ")", ":", "\n", "# Calculates achieved objectives objectives in self.trajectories", "\n", "        ", "objective_logs", "=", "[", "]", "\n", "for", "i", ",", "tau", "in", "enumerate", "(", "self", ".", "trajectories", ")", ":", "\n", "            ", "objective_logs", ".", "append", "(", "list", "(", "np", ".", "array", "(", "tau", "[", "'logs'", "]", ")", ".", "sum", "(", "axis", "=", "0", ")", ")", ")", "\n", "\n", "", "return", "np", ".", "array", "(", "objective_logs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.ppo.g_clip": [[108, 110], ["torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor", "torch.tensor", "torch.tensor"], "function", ["None"], ["", "", "def", "g_clip", "(", "epsilon", ",", "A", ")", ":", "\n", "    ", "return", "torch", ".", "tensor", "(", "[", "1", "+", "epsilon", "if", "i", "else", "1", "-", "epsilon", "for", "i", "in", "A", ">=", "0", "]", ")", ".", "to", "(", "device", ")", "*", "A", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.ppo.update_policy": [[112, 135], ["range", "enumerate", "optimizer.zero_grad", "overall_loss.backward", "optimizer.step", "numpy.array", "ppo.evaluate_trajectory", "torch.exp", "torch.exp", "torch.exp", "torch.mean", "torch.mean", "torch.mean", "returns.insert", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "critic_values.detach().to", "torch.min", "torch.min", "torch.min", "torch.mean", "torch.mean", "torch.mean", "np.array.mean", "np.array.std", "torch.tensor().detach().to", "torch.tensor().detach().to", "torch.tensor().detach().to", "ppo.g_clip", "torch.tensor", "torch.tensor", "torch.tensor", "critic_values.detach", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().detach", "torch.tensor().detach", "torch.tensor().detach", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.VecEnv.step", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.evaluate_trajectory", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.moral.ppo.g_clip"], ["", "def", "update_policy", "(", "ppo", ",", "dataset", ",", "optimizer", ",", "gamma", ",", "epsilon", ",", "n_epochs", ",", "entropy_reg", ")", ":", "\n", "    ", "for", "epoch", "in", "range", "(", "n_epochs", ")", ":", "\n", "        ", "batch_loss", "=", "0", "\n", "value_loss", "=", "0", "\n", "for", "i", ",", "tau", "in", "enumerate", "(", "dataset", ".", "trajectories", ")", ":", "\n", "            ", "reward_togo", "=", "0", "\n", "returns", "=", "[", "]", "\n", "normalized_reward", "=", "np", ".", "array", "(", "tau", "[", "'rewards'", "]", ")", "\n", "normalized_reward", "=", "(", "normalized_reward", "-", "normalized_reward", ".", "mean", "(", ")", ")", "/", "(", "normalized_reward", ".", "std", "(", ")", "+", "1e-5", ")", "\n", "for", "r", "in", "normalized_reward", "[", ":", ":", "-", "1", "]", ":", "\n", "# Compute rewards-to-go and advantage estimates", "\n", "                ", "reward_togo", "=", "r", "+", "gamma", "*", "reward_togo", "\n", "returns", ".", "insert", "(", "0", ",", "reward_togo", ")", "\n", "", "action_log_probabilities", ",", "critic_values", ",", "action_entropy", "=", "ppo", ".", "evaluate_trajectory", "(", "tau", ")", "\n", "advantages", "=", "torch", ".", "tensor", "(", "returns", ")", ".", "to", "(", "device", ")", "-", "critic_values", ".", "detach", "(", ")", ".", "to", "(", "device", ")", "\n", "likelihood_ratios", "=", "torch", ".", "exp", "(", "action_log_probabilities", "-", "torch", ".", "tensor", "(", "tau", "[", "'log_probs'", "]", ")", ".", "detach", "(", ")", ".", "to", "(", "device", ")", ")", "\n", "clipped_losses", "=", "-", "torch", ".", "min", "(", "likelihood_ratios", "*", "advantages", ",", "g_clip", "(", "epsilon", ",", "advantages", ")", ")", "\n", "batch_loss", "+=", "torch", ".", "mean", "(", "clipped_losses", ")", "-", "entropy_reg", "*", "action_entropy", "\n", "value_loss", "+=", "torch", ".", "mean", "(", "(", "torch", ".", "tensor", "(", "returns", ")", ".", "to", "(", "device", ")", "-", "critic_values", ")", "**", "2", ")", "\n", "", "overall_loss", "=", "(", "batch_loss", "+", "value_loss", ")", "/", "dataset", ".", "batch_size", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "overall_loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v3.MailDrape.__init__": [[111, 114], ["pycolab.things.Drape.__init__", "randomized_v3.MailDrape.curtain.fill"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceBuffer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "curtain", ",", "character", ")", ":", "\n", "        ", "super", "(", "MailDrape", ",", "self", ")", ".", "__init__", "(", "curtain", ",", "character", ")", "\n", "self", ".", "curtain", ".", "fill", "(", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v3.MailDrape.update": [[115, 148], ["numpy.random.choice", "range", "randomized_v3.scalar_to_idx", "the_plot.add_reward", "the_plot.add_reward", "the_plot.add_reward", "the_plot.add_reward", "randomized_v3.scalar_to_idx", "randomized_v3.scalar_to_idx", "randomized_v3.scalar_to_idx", "randomized_v3.scalar_to_idx", "numpy.array", "numpy.array", "numpy.array", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.scalar_to_idx", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.scalar_to_idx", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.scalar_to_idx", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.scalar_to_idx", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.scalar_to_idx"], ["", "def", "update", "(", "self", ",", "actions", ",", "board", ",", "layers", ",", "backdrop", ",", "things", ",", "the_plot", ")", ":", "\n", "        ", "del", "backdrop", "\n", "\n", "if", "the_plot", ".", "frame", "==", "0", ":", "\n", "# Random initialization of player, fire and citizen", "\n", "            ", "random_positions", "=", "np", ".", "random", ".", "choice", "(", "14", "*", "14", ",", "size", "=", "N_MAIL", "+", "N_CITIZEN", "+", "N_STREET", "+", "N_VASE", "+", "1", ",", "replace", "=", "False", ")", "\n", "for", "i", "in", "range", "(", "N_MAIL", ")", ":", "\n", "                ", "tmp_idx", "=", "scalar_to_idx", "(", "random_positions", "[", "i", "]", ")", "\n", "self", ".", "curtain", "[", "tmp_idx", "]", "=", "True", "\n", "", "the_plot", "[", "'P_pos'", "]", "=", "scalar_to_idx", "(", "random_positions", "[", "-", "1", "]", ")", "\n", "the_plot", "[", "'C_pos'", "]", "=", "[", "scalar_to_idx", "(", "i", ")", "for", "i", "in", "random_positions", "[", "N_MAIL", ":", "N_MAIL", "+", "N_CITIZEN", "]", "]", "\n", "the_plot", "[", "'S_pos'", "]", "=", "[", "scalar_to_idx", "(", "i", ")", "for", "i", "in", "random_positions", "[", "N_MAIL", "+", "N_CITIZEN", "\n", ":", "N_MAIL", "+", "N_CITIZEN", "+", "N_STREET", "]", "]", "\n", "the_plot", "[", "'V_pos'", "]", "=", "[", "scalar_to_idx", "(", "i", ")", "for", "i", "in", "random_positions", "[", "N_MAIL", "+", "N_CITIZEN", "+", "N_STREET", "\n", ":", "N_MAIL", "+", "N_CITIZEN", "+", "N_STREET", "+", "N_VASE", "]", "]", "\n", "\n", "", "player_pattern_position", "=", "things", "[", "'P'", "]", ".", "position", "\n", "player_row", "=", "player_pattern_position", ".", "row", "\n", "player_col", "=", "player_pattern_position", ".", "col", "\n", "\n", "# Check for 'pick up' action:", "\n", "if", "actions", "==", "5", "and", "self", ".", "curtain", "[", "(", "player_row", "-", "1", ",", "player_col", ")", "]", ":", "# grab upward?", "\n", "            ", "self", ".", "curtain", "[", "(", "player_row", "-", "1", ",", "player_col", ")", "]", "=", "False", "\n", "the_plot", ".", "add_reward", "(", "np", ".", "array", "(", "[", "1.", ",", "0.", ",", "0.", ",", "0.", "]", ")", ")", "\n", "", "if", "actions", "==", "6", "and", "self", ".", "curtain", "[", "(", "player_row", "+", "1", ",", "player_col", ")", "]", ":", "# grab downward?", "\n", "            ", "self", ".", "curtain", "[", "(", "player_row", "+", "1", ",", "player_col", ")", "]", "=", "False", "\n", "the_plot", ".", "add_reward", "(", "np", ".", "array", "(", "[", "1.", ",", "0.", ",", "0.", ",", "0.", "]", ")", ")", "\n", "", "if", "actions", "==", "7", "and", "self", ".", "curtain", "[", "(", "player_row", ",", "player_col", "-", "1", ")", "]", ":", "# grab leftward?", "\n", "            ", "self", ".", "curtain", "[", "(", "player_row", ",", "player_col", "-", "1", ")", "]", "=", "False", "\n", "the_plot", ".", "add_reward", "(", "np", ".", "array", "(", "[", "1.", ",", "0.", ",", "0.", ",", "0.", "]", ")", ")", "\n", "", "if", "actions", "==", "8", "and", "self", ".", "curtain", "[", "(", "player_row", ",", "player_col", "+", "1", ")", "]", ":", "# grab rightward?", "\n", "            ", "self", ".", "curtain", "[", "(", "player_row", ",", "player_col", "+", "1", ")", "]", "=", "False", "\n", "the_plot", ".", "add_reward", "(", "np", ".", "array", "(", "[", "1.", ",", "0.", ",", "0.", ",", "0.", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v3.CitizenDrape.__init__": [[154, 157], ["pycolab.things.Drape.__init__", "randomized_v3.CitizenDrape.curtain.fill"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceBuffer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "curtain", ",", "character", ")", ":", "\n", "        ", "super", "(", "CitizenDrape", ",", "self", ")", ".", "__init__", "(", "curtain", ",", "character", ")", "\n", "self", ".", "curtain", ".", "fill", "(", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v3.CitizenDrape.update": [[158, 184], ["the_plot.add_reward", "the_plot.add_reward", "the_plot.add_reward", "the_plot.add_reward", "numpy.array", "numpy.array", "numpy.array", "numpy.array"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "actions", ",", "board", ",", "layers", ",", "backdrop", ",", "things", ",", "the_plot", ")", ":", "\n", "        ", "del", "backdrop", "\n", "\n", "if", "the_plot", ".", "frame", "==", "0", ":", "\n", "# Random initialization of player and fire", "\n", "            ", "citizen_positions", "=", "the_plot", "[", "'C_pos'", "]", "\n", "for", "pos", "in", "citizen_positions", ":", "\n", "                ", "self", ".", "curtain", "[", "pos", "]", "=", "True", "\n", "\n", "", "", "player_pattern_position", "=", "things", "[", "'P'", "]", ".", "position", "\n", "player_row", "=", "player_pattern_position", ".", "row", "\n", "player_col", "=", "player_pattern_position", ".", "col", "\n", "\n", "# Check for 'pick up' action:", "\n", "if", "actions", "==", "5", "and", "self", ".", "curtain", "[", "(", "player_row", "-", "1", ",", "player_col", ")", "]", ":", "# grab upward?", "\n", "            ", "self", ".", "curtain", "[", "(", "player_row", "-", "1", ",", "player_col", ")", "]", "=", "False", "\n", "the_plot", ".", "add_reward", "(", "np", ".", "array", "(", "[", "0.", ",", "1.", ",", "0.", ",", "0.", "]", ")", ")", "\n", "", "if", "actions", "==", "6", "and", "self", ".", "curtain", "[", "(", "player_row", "+", "1", ",", "player_col", ")", "]", ":", "# grab downward?", "\n", "            ", "self", ".", "curtain", "[", "(", "player_row", "+", "1", ",", "player_col", ")", "]", "=", "False", "\n", "the_plot", ".", "add_reward", "(", "np", ".", "array", "(", "[", "0.", ",", "1.", ",", "0.", ",", "0.", "]", ")", ")", "\n", "", "if", "actions", "==", "7", "and", "self", ".", "curtain", "[", "(", "player_row", ",", "player_col", "-", "1", ")", "]", ":", "# grab leftward?", "\n", "            ", "self", ".", "curtain", "[", "(", "player_row", ",", "player_col", "-", "1", ")", "]", "=", "False", "\n", "the_plot", ".", "add_reward", "(", "np", ".", "array", "(", "[", "0.", ",", "1.", ",", "0.", ",", "0.", "]", ")", ")", "\n", "", "if", "actions", "==", "8", "and", "self", ".", "curtain", "[", "(", "player_row", ",", "player_col", "+", "1", ")", "]", ":", "# grab rightward?", "\n", "            ", "self", ".", "curtain", "[", "(", "player_row", ",", "player_col", "+", "1", ")", "]", "=", "False", "\n", "the_plot", ".", "add_reward", "(", "np", ".", "array", "(", "[", "0.", ",", "1.", ",", "0.", ",", "0.", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v3.StreetDrape.__init__": [[186, 189], ["pycolab.things.Drape.__init__", "randomized_v3.StreetDrape.curtain.fill"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceBuffer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "curtain", ",", "character", ")", ":", "\n", "        ", "super", "(", "StreetDrape", ",", "self", ")", ".", "__init__", "(", "curtain", ",", "character", ")", "\n", "self", ".", "curtain", ".", "fill", "(", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v3.StreetDrape.update": [[190, 216], ["the_plot.add_reward", "the_plot.add_reward", "the_plot.add_reward", "the_plot.add_reward", "numpy.array", "numpy.array", "numpy.array", "numpy.array"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "actions", ",", "board", ",", "layers", ",", "backdrop", ",", "things", ",", "the_plot", ")", ":", "\n", "        ", "del", "backdrop", "\n", "\n", "if", "the_plot", ".", "frame", "==", "0", ":", "\n", "# Random initialization of player and fire", "\n", "            ", "citizen_positions", "=", "the_plot", "[", "'S_pos'", "]", "\n", "for", "pos", "in", "citizen_positions", ":", "\n", "                ", "self", ".", "curtain", "[", "pos", "]", "=", "True", "\n", "\n", "", "", "player_pattern_position", "=", "things", "[", "'P'", "]", ".", "position", "\n", "player_row", "=", "player_pattern_position", ".", "row", "\n", "player_col", "=", "player_pattern_position", ".", "col", "\n", "\n", "# Check for 'pick up' action:", "\n", "if", "actions", "==", "5", "and", "self", ".", "curtain", "[", "(", "player_row", "-", "1", ",", "player_col", ")", "]", ":", "# grab upward?", "\n", "            ", "self", ".", "curtain", "[", "(", "player_row", "-", "1", ",", "player_col", ")", "]", "=", "False", "\n", "the_plot", ".", "add_reward", "(", "np", ".", "array", "(", "[", "0.", ",", "0.", ",", "1.", ",", "0.", "]", ")", ")", "\n", "", "if", "actions", "==", "6", "and", "self", ".", "curtain", "[", "(", "player_row", "+", "1", ",", "player_col", ")", "]", ":", "# grab downward?", "\n", "            ", "self", ".", "curtain", "[", "(", "player_row", "+", "1", ",", "player_col", ")", "]", "=", "False", "\n", "the_plot", ".", "add_reward", "(", "np", ".", "array", "(", "[", "0.", ",", "0.", ",", "1.", ",", "0.", "]", ")", ")", "\n", "", "if", "actions", "==", "7", "and", "self", ".", "curtain", "[", "(", "player_row", ",", "player_col", "-", "1", ")", "]", ":", "# grab leftward?", "\n", "            ", "self", ".", "curtain", "[", "(", "player_row", ",", "player_col", "-", "1", ")", "]", "=", "False", "\n", "the_plot", ".", "add_reward", "(", "np", ".", "array", "(", "[", "0.", ",", "0.", ",", "1.", ",", "0.", "]", ")", ")", "\n", "", "if", "actions", "==", "8", "and", "self", ".", "curtain", "[", "(", "player_row", ",", "player_col", "+", "1", ")", "]", ":", "# grab rightward?", "\n", "            ", "self", ".", "curtain", "[", "(", "player_row", ",", "player_col", "+", "1", ")", "]", "=", "False", "\n", "the_plot", ".", "add_reward", "(", "np", ".", "array", "(", "[", "0.", ",", "0.", ",", "1.", ",", "0.", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v3.VaseDrape.__init__": [[218, 221], ["pycolab.things.Drape.__init__", "randomized_v3.VaseDrape.curtain.fill"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceBuffer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "curtain", ",", "character", ")", ":", "\n", "        ", "super", "(", "VaseDrape", ",", "self", ")", ".", "__init__", "(", "curtain", ",", "character", ")", "\n", "self", ".", "curtain", ".", "fill", "(", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v3.VaseDrape.update": [[222, 238], ["the_plot.add_reward", "numpy.array"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "actions", ",", "board", ",", "layers", ",", "backdrop", ",", "things", ",", "the_plot", ")", ":", "\n", "        ", "del", "backdrop", "\n", "\n", "if", "the_plot", ".", "frame", "==", "0", ":", "\n", "# Random initialization of player and fire", "\n", "            ", "citizen_positions", "=", "the_plot", "[", "'V_pos'", "]", "\n", "for", "pos", "in", "citizen_positions", ":", "\n", "                ", "self", ".", "curtain", "[", "pos", "]", "=", "True", "\n", "\n", "", "", "player_pattern_position", "=", "things", "[", "'P'", "]", ".", "position", "\n", "player_row", "=", "player_pattern_position", ".", "row", "\n", "player_col", "=", "player_pattern_position", ".", "col", "\n", "\n", "if", "self", ".", "curtain", "[", "(", "player_row", ",", "player_col", ")", "]", ":", "\n", "            ", "self", ".", "curtain", "[", "(", "player_row", ",", "player_col", ")", "]", "=", "False", "\n", "the_plot", ".", "add_reward", "(", "np", ".", "array", "(", "[", "0.", ",", "0.", ",", "0.", ",", "-", "1.", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v3.HouseDrape.__init__": [[241, 244], ["pycolab.things.Drape.__init__", "randomized_v3.HouseDrape.curtain.fill"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceBuffer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "curtain", ",", "character", ")", ":", "\n", "        ", "super", "(", "HouseDrape", ",", "self", ")", ".", "__init__", "(", "curtain", ",", "character", ")", "\n", "self", ".", "curtain", ".", "fill", "(", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v3.HouseDrape.update": [[245, 253], ["None"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "actions", ",", "board", ",", "layers", ",", "backdrop", ",", "things", ",", "the_plot", ")", ":", "\n", "        ", "del", "backdrop", "\n", "\n", "if", "the_plot", ".", "frame", "==", "0", ":", "\n", "# Random initialization of player and fire", "\n", "            ", "citizen_positions", "=", "the_plot", "[", "'H_pos'", "]", "\n", "for", "pos", "in", "citizen_positions", ":", "\n", "                ", "self", ".", "curtain", "[", "pos", "]", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v3.PlayerSprite.__init__": [[257, 261], ["pycolab.prefab_parts.sprites.MazeWalker.__init__"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceBuffer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "corner", ",", "position", ",", "character", ")", ":", "\n", "        ", "\"\"\"Constructor: simply supplies characters that players can't traverse.\"\"\"", "\n", "super", "(", "PlayerSprite", ",", "self", ")", ".", "__init__", "(", "\n", "corner", ",", "position", ",", "character", ",", "impassable", "=", "'#H.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v3.PlayerSprite.update": [[262, 276], ["randomized_v3.PlayerSprite._teleport", "randomized_v3.PlayerSprite._north", "randomized_v3.PlayerSprite._south", "randomized_v3.PlayerSprite._west", "randomized_v3.PlayerSprite._east"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "actions", ",", "board", ",", "layers", ",", "backdrop", ",", "things", ",", "the_plot", ")", ":", "\n", "        ", "del", "backdrop", ",", "things", ",", "layers", "# Unused.", "\n", "\n", "if", "the_plot", ".", "frame", "==", "0", ":", "\n", "            ", "self", ".", "_teleport", "(", "the_plot", "[", "'P_pos'", "]", ")", "\n", "\n", "", "if", "actions", "==", "0", ":", "# go upward?", "\n", "            ", "self", ".", "_north", "(", "board", ",", "the_plot", ")", "\n", "", "elif", "actions", "==", "1", ":", "# go downward?", "\n", "            ", "self", ".", "_south", "(", "board", ",", "the_plot", ")", "\n", "", "elif", "actions", "==", "2", ":", "# go leftward?", "\n", "            ", "self", ".", "_west", "(", "board", ",", "the_plot", ")", "\n", "", "elif", "actions", "==", "3", ":", "# go rightward?", "\n", "            ", "self", ".", "_east", "(", "board", ",", "the_plot", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v3.JudgeDrape.__init__": [[279, 283], ["pycolab.things.Drape.__init__"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceBuffer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "curtain", ",", "character", ")", ":", "\n", "        ", "super", "(", "JudgeDrape", ",", "self", ")", ".", "__init__", "(", "curtain", ",", "character", ")", "\n", "self", ".", "_step_counter", "=", "0", "\n", "self", ".", "_max_steps", "=", "MAX_STEPS", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v3.JudgeDrape.update": [[285, 296], ["randomized_v3.JudgeDrape.curtain.fill", "the_plot.add_reward", "numpy.array", "the_plot.terminate_episode"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "actions", ",", "board", ",", "layers", ",", "backdrop", ",", "things", ",", "the_plot", ")", ":", "\n", "# Clear our curtain and mark the locations of all the boxes True.", "\n", "        ", "self", ".", "curtain", ".", "fill", "(", "False", ")", "\n", "the_plot", ".", "add_reward", "(", "np", ".", "array", "(", "[", "0.", ",", "0.", ",", "0.", ",", "0.", "]", ")", ")", "\n", "#the_plot.add_reward(-0.1)", "\n", "self", ".", "_step_counter", "+=", "1", "\n", "\n", "# See if we should quit: it happens if the user solves the puzzle or if", "\n", "# they give up and execute the 'quit' action.", "\n", "if", "(", "actions", "==", "9", ")", "or", "(", "self", ".", "_step_counter", "==", "self", ".", "_max_steps", ")", ":", "\n", "            ", "the_plot", ".", "terminate_episode", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v3.make_game": [[77, 103], ["pycolab.ascii_art.ascii_art_to_game"], "function", ["None"], ["def", "make_game", "(", "seed", "=", "None", ",", "demo", "=", "False", ")", ":", "\n", "    ", "delivery_art", "=", "DELIVERY_ART", "\n", "what_lies_beneath", "=", "BACKGROUND_ART", "\n", "sprites", "=", "{", "'P'", ":", "PlayerSprite", "}", "\n", "\n", "if", "demo", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "", "else", ":", "\n", "        ", "drapes", "=", "{", "'X'", ":", "JudgeDrape", "}", "\n", "\n", "", "drapes", "[", "'F'", "]", "=", "MailDrape", "\n", "drapes", "[", "'C'", "]", "=", "CitizenDrape", "\n", "drapes", "[", "'S'", "]", "=", "StreetDrape", "\n", "drapes", "[", "'V'", "]", "=", "VaseDrape", "\n", "\n", "update_schedule", "=", "[", "[", "'F'", "]", ",", "\n", "[", "'C'", "]", ",", "\n", "[", "'S'", "]", ",", "\n", "[", "'V'", "]", ",", "\n", "[", "'X'", "]", ",", "\n", "[", "'P'", "]", "]", "\n", "\n", "\n", "return", "ascii_art", ".", "ascii_art_to_game", "(", "\n", "delivery_art", ",", "what_lies_beneath", ",", "sprites", ",", "drapes", ",", "\n", "update_schedule", "=", "update_schedule", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v3.scalar_to_idx": [[104, 108], ["int", "numpy.floor"], "function", ["None"], ["", "def", "scalar_to_idx", "(", "x", ")", ":", "\n", "    ", "row", "=", "x", "%", "14", "\n", "col", "=", "int", "(", "np", ".", "floor", "(", "x", "/", "14", ")", ")", "\n", "return", "(", "row", "+", "1", ",", "col", "+", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v3.main": [[298, 317], ["randomized_v3.make_game", "pycolab.human_ui.CursesUi", "human_ui.CursesUi.play"], "function", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.make_game"], ["", "", "", "def", "main", "(", "demo", ")", ":", "\n", "# Builds an interactive game session.", "\n", "    ", "game", "=", "make_game", "(", "demo", "=", "demo", ")", "\n", "\n", "\n", "ui", "=", "human_ui", ".", "CursesUi", "(", "\n", "keys_to_actions", "=", "{", "curses", ".", "KEY_UP", ":", "0", ",", "curses", ".", "KEY_DOWN", ":", "1", ",", "\n", "curses", ".", "KEY_LEFT", ":", "2", ",", "curses", ".", "KEY_RIGHT", ":", "3", ",", "\n", "'w'", ":", "5", ",", "\n", "'s'", ":", "6", ",", "\n", "'a'", ":", "7", ",", "\n", "'d'", ":", "8", ",", "\n", "-", "1", ":", "4", ",", "\n", "'q'", ":", "9", ",", "'Q'", ":", "9", "}", ",", "\n", "delay", "=", "1000", ",", "\n", "colour_fg", "=", "DELIVERY_FG_COLOURS", ")", "\n", "\n", "# Let the game begin!", "\n", "ui", ".", "play", "(", "game", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.GymWrapper.__init__": [[17, 45], ["gym.spaces.Discrete", "gym.spaces.Box", "pycolab.rendering.ObservationToFeatureArray", "gym_wrapper.GymWrapper.seed", "gym_wrapper.GymWrapper.reset", "len"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.GymWrapper.seed", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.VecEnv.reset"], ["def", "__init__", "(", "self", ",", "env_id", ")", ":", "\n", "        ", "self", ".", "env_id", "=", "env_id", "\n", "\n", "if", "env_id", "==", "'randomized_v2'", ":", "\n", "            ", "self", ".", "layers", "=", "(", "'#'", ",", "'P'", ",", "'C'", ",", "'H'", ",", "'G'", ")", "\n", "self", ".", "width", "=", "8", "\n", "self", ".", "height", "=", "8", "\n", "self", ".", "num_actions", "=", "9", "\n", "", "elif", "env_id", "==", "'randomized_v3'", ":", "\n", "            ", "self", ".", "layers", "=", "(", "'#'", ",", "'P'", ",", "'F'", ",", "'C'", ",", "'S'", ",", "'V'", ")", "\n", "self", ".", "width", "=", "16", "\n", "self", ".", "height", "=", "16", "\n", "self", ".", "num_actions", "=", "9", "\n", "\n", "", "self", ".", "game", "=", "None", "\n", "self", ".", "np_random", "=", "None", "\n", "\n", "self", ".", "action_space", "=", "spaces", ".", "Discrete", "(", "self", ".", "num_actions", ")", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "\n", "low", "=", "0", ",", "high", "=", "1", ",", "\n", "shape", "=", "(", "self", ".", "width", ",", "self", ".", "height", ",", "len", "(", "self", ".", "layers", ")", ")", ",", "\n", "dtype", "=", "np", ".", "int32", "\n", ")", "\n", "\n", "self", ".", "renderer", "=", "rendering", ".", "ObservationToFeatureArray", "(", "self", ".", "layers", ")", "\n", "\n", "self", ".", "seed", "(", ")", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.GymWrapper.seed": [[46, 49], ["gym.utils.seeding.np_random"], "methods", ["None"], ["", "def", "seed", "(", "self", ",", "seed", "=", "None", ")", ":", "\n", "        ", "self", ".", "np_random", ",", "seed", "=", "seeding", ".", "np_random", "(", "seed", ")", "\n", "return", "[", "seed", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.GymWrapper._obs_to_np_array": [[50, 52], ["copy.copy", "gym_wrapper.GymWrapper.renderer"], "methods", ["None"], ["", "def", "_obs_to_np_array", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "copy", ".", "copy", "(", "self", ".", "renderer", "(", "obs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.GymWrapper.reset": [[53, 61], ["gym_wrapper.GymWrapper.game.its_showtime", "gym_wrapper.GymWrapper._obs_to_np_array", "envs.randomized_v2.make_game", "envs.randomized_v2.make_game", "envs.randomized_v2.make_game", "envs.randomized_v2.make_game", "envs.randomized_v3.make_game", "envs.randomized_v3.make_game", "envs.randomized_v3.make_game", "envs.randomized_v3.make_game"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.GymWrapper._obs_to_np_array", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.make_game", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.make_game", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.make_game", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.make_game", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.make_game", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.make_game", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.make_game", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.make_game"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "env_id", "==", "'randomized_v2'", ":", "\n", "            ", "self", ".", "game", "=", "envs", ".", "randomized_v2", ".", "make_game", "(", ")", "\n", "", "elif", "self", ".", "env_id", "==", "'randomized_v3'", ":", "\n", "            ", "self", ".", "game", "=", "envs", ".", "randomized_v3", ".", "make_game", "(", ")", "\n", "\n", "", "obs", ",", "_", ",", "_", "=", "self", ".", "game", ".", "its_showtime", "(", ")", "\n", "return", "self", ".", "_obs_to_np_array", "(", "obs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.GymWrapper.step": [[62, 65], ["gym_wrapper.GymWrapper.game.play", "gym_wrapper.GymWrapper._obs_to_np_array"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.GymWrapper._obs_to_np_array"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "obs", ",", "reward", ",", "_", "=", "self", ".", "game", ".", "play", "(", "action", ")", "\n", "return", "self", ".", "_obs_to_np_array", "(", "obs", ")", ",", "reward", ",", "self", ".", "game", ".", "game_over", ",", "self", ".", "game", ".", "the_plot", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.VecEnv.__init__": [[85, 91], ["gym_wrapper.make_env", "range", "int", "str().replace", "str", "time.time"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.make_env"], ["    ", "def", "__init__", "(", "self", ",", "env_id", ",", "n_envs", ")", ":", "\n", "        ", "self", ".", "env_list", "=", "[", "make_env", "(", "env_id", ",", "i", ",", "(", "int", "(", "str", "(", "time", ".", "time", "(", ")", ")", ".", "replace", "(", "'.'", ",", "''", ")", "[", "-", "8", ":", "]", ")", "+", "i", ")", ")", "(", ")", "for", "i", "in", "range", "(", "n_envs", ")", "]", "\n", "self", ".", "n_envs", "=", "n_envs", "\n", "self", ".", "env_id", "=", "env_id", "\n", "self", ".", "action_space", "=", "self", ".", "env_list", "[", "0", "]", ".", "action_space", "\n", "self", ".", "observation_space", "=", "self", ".", "env_list", "[", "0", "]", ".", "observation_space", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.VecEnv.reset": [[92, 98], ["range", "numpy.stack", "obs_list.append", "gym_wrapper.VecEnv.env_list[].reset"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.VecEnv.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "obs_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "n_envs", ")", ":", "\n", "            ", "obs_list", ".", "append", "(", "self", ".", "env_list", "[", "i", "]", ".", "reset", "(", ")", ")", "\n", "\n", "", "return", "np", ".", "stack", "(", "obs_list", ",", "axis", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.VecEnv.step": [[99, 116], ["range", "gym_wrapper.VecEnv.env_list[].step", "obs_list.append", "rew_list.append", "done_list.append", "info_list.append", "numpy.stack", "gym_wrapper.VecEnv.env_list[].reset"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.VecEnv.step", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.VecEnv.reset"], ["", "def", "step", "(", "self", ",", "actions", ")", ":", "\n", "        ", "obs_list", "=", "[", "]", "\n", "rew_list", "=", "[", "]", "\n", "done_list", "=", "[", "]", "\n", "info_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "n_envs", ")", ":", "\n", "            ", "obs_i", ",", "rew_i", ",", "done_i", ",", "info_i", "=", "self", ".", "env_list", "[", "i", "]", ".", "step", "(", "actions", "[", "i", "]", ")", "\n", "\n", "if", "done_i", ":", "\n", "                ", "obs_i", "=", "self", ".", "env_list", "[", "i", "]", ".", "reset", "(", ")", "\n", "\n", "", "obs_list", ".", "append", "(", "obs_i", ")", "\n", "rew_list", ".", "append", "(", "rew_i", ")", "\n", "done_list", ".", "append", "(", "done_i", ")", "\n", "info_list", ".", "append", "(", "info_i", ")", "\n", "\n", "", "return", "np", ".", "stack", "(", "obs_list", ",", "axis", "=", "0", ")", ",", "rew_list", ",", "done_list", ",", "info_list", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.make_env": [[66, 83], ["stable_baselines3.common.utils.set_random_seed", "gym_wrapper.GymWrapper", "gym_wrapper.GymWrapper.seed"], "function", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.GymWrapper.seed"], ["", "", "def", "make_env", "(", "env_id", ":", "str", ",", "rank", ":", "int", ",", "seed", ":", "int", "=", "0", ")", "->", "Callable", ":", "\n", "    ", "\"\"\"\n    Utility function for multiprocessed env.\n\n    :param env_id: (str) the environment ID\n    :param seed: (int) the initial seed for RNG\n    :param rank: (int) index of the subprocess\n    :return: (Callable)\n    \"\"\"", "\n", "\n", "def", "_init", "(", ")", "->", "gym", ".", "Env", ":", "\n", "        ", "env", "=", "GymWrapper", "(", "env_id", ")", "\n", "env", ".", "seed", "(", "seed", "+", "rank", ")", "\n", "return", "env", "\n", "\n", "", "set_random_seed", "(", "seed", ")", "\n", "return", "_init", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.GoalDrape.__init__": [[92, 94], ["pycolab.things.Drape.__init__"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceBuffer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "curtain", ",", "character", ")", ":", "\n", "        ", "super", "(", "GoalDrape", ",", "self", ")", ".", "__init__", "(", "curtain", ",", "character", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.GoalDrape.update": [[95, 104], ["the_plot.add_reward", "numpy.array"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "actions", ",", "board", ",", "layers", ",", "backdrop", ",", "things", ",", "the_plot", ")", ":", "\n", "        ", "del", "backdrop", "# Unused.", "\n", "\n", "player_pattern_position", "=", "things", "[", "'P'", "]", ".", "position", "\n", "player_row", "=", "player_pattern_position", ".", "row", "\n", "player_col", "=", "player_pattern_position", ".", "col", "\n", "\n", "if", "self", ".", "curtain", "[", "(", "player_row", ",", "player_col", ")", "]", ":", "\n", "            ", "the_plot", ".", "add_reward", "(", "np", ".", "array", "(", "[", "0.1", ",", "0.", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.CitizenDrape.__init__": [[107, 110], ["pycolab.things.Drape.__init__", "randomized_v2.CitizenDrape.curtain.fill"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceBuffer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "curtain", ",", "character", ")", ":", "\n", "        ", "super", "(", "CitizenDrape", ",", "self", ")", ".", "__init__", "(", "curtain", ",", "character", ")", "\n", "self", ".", "curtain", ".", "fill", "(", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.CitizenDrape.update": [[111, 140], ["numpy.random.choice", "range", "randomized_v2.scalar_to_idx", "the_plot.add_reward", "the_plot.add_reward", "the_plot.add_reward", "the_plot.add_reward", "randomized_v2.scalar_to_idx", "randomized_v2.scalar_to_idx", "numpy.array", "numpy.array", "numpy.array", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.scalar_to_idx", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.scalar_to_idx", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.scalar_to_idx"], ["", "def", "update", "(", "self", ",", "actions", ",", "board", ",", "layers", ",", "backdrop", ",", "things", ",", "the_plot", ")", ":", "\n", "        ", "del", "backdrop", "\n", "\n", "if", "the_plot", ".", "frame", "==", "0", ":", "\n", "# Random initialization of player, fire and citizen", "\n", "            ", "random_positions", "=", "np", ".", "random", ".", "choice", "(", "6", "*", "6", ",", "size", "=", "N_CITIZEN", "+", "N_HOUSE", "+", "1", ",", "replace", "=", "False", ")", "\n", "for", "i", "in", "range", "(", "N_CITIZEN", ")", ":", "\n", "                ", "tmp_idx", "=", "scalar_to_idx", "(", "random_positions", "[", "i", "]", ")", "\n", "self", ".", "curtain", "[", "tmp_idx", "]", "=", "True", "\n", "", "the_plot", "[", "'P_pos'", "]", "=", "scalar_to_idx", "(", "random_positions", "[", "-", "1", "]", ")", "\n", "the_plot", "[", "'H_pos'", "]", "=", "[", "scalar_to_idx", "(", "i", ")", "for", "i", "in", "random_positions", "[", "N_CITIZEN", ":", "N_CITIZEN", "+", "N_HOUSE", "]", "]", "\n", "\n", "", "player_pattern_position", "=", "things", "[", "'P'", "]", ".", "position", "\n", "player_row", "=", "player_pattern_position", ".", "row", "\n", "player_col", "=", "player_pattern_position", ".", "col", "\n", "\n", "# Check for 'pick up' action:", "\n", "if", "actions", "==", "5", "and", "self", ".", "curtain", "[", "(", "player_row", "-", "1", ",", "player_col", ")", "]", ":", "# grab upward?", "\n", "            ", "self", ".", "curtain", "[", "(", "player_row", "-", "1", ",", "player_col", ")", "]", "=", "False", "\n", "the_plot", ".", "add_reward", "(", "np", ".", "array", "(", "[", "0.", ",", "1.", "]", ")", ")", "\n", "", "if", "actions", "==", "6", "and", "self", ".", "curtain", "[", "(", "player_row", "+", "1", ",", "player_col", ")", "]", ":", "# grab downward?", "\n", "            ", "self", ".", "curtain", "[", "(", "player_row", "+", "1", ",", "player_col", ")", "]", "=", "False", "\n", "the_plot", ".", "add_reward", "(", "np", ".", "array", "(", "[", "0.", ",", "1.", "]", ")", ")", "\n", "", "if", "actions", "==", "7", "and", "self", ".", "curtain", "[", "(", "player_row", ",", "player_col", "-", "1", ")", "]", ":", "# grab leftward?", "\n", "            ", "self", ".", "curtain", "[", "(", "player_row", ",", "player_col", "-", "1", ")", "]", "=", "False", "\n", "the_plot", ".", "add_reward", "(", "np", ".", "array", "(", "[", "0.", ",", "1.", "]", ")", ")", "\n", "", "if", "actions", "==", "8", "and", "self", ".", "curtain", "[", "(", "player_row", ",", "player_col", "+", "1", ")", "]", ":", "# grab rightward?", "\n", "            ", "self", ".", "curtain", "[", "(", "player_row", ",", "player_col", "+", "1", ")", "]", "=", "False", "\n", "the_plot", ".", "add_reward", "(", "np", ".", "array", "(", "[", "0.", ",", "1.", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.HouseDrape.__init__": [[147, 150], ["pycolab.things.Drape.__init__", "randomized_v2.HouseDrape.curtain.fill"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceBuffer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "curtain", ",", "character", ")", ":", "\n", "        ", "super", "(", "HouseDrape", ",", "self", ")", ".", "__init__", "(", "curtain", ",", "character", ")", "\n", "self", ".", "curtain", ".", "fill", "(", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.HouseDrape.update": [[151, 159], ["None"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "actions", ",", "board", ",", "layers", ",", "backdrop", ",", "things", ",", "the_plot", ")", ":", "\n", "        ", "del", "backdrop", "\n", "\n", "if", "the_plot", ".", "frame", "==", "0", ":", "\n", "# Random initialization of player and fire", "\n", "            ", "citizen_positions", "=", "the_plot", "[", "'H_pos'", "]", "\n", "for", "pos", "in", "citizen_positions", ":", "\n", "                ", "self", ".", "curtain", "[", "pos", "]", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.PlayerSprite.__init__": [[163, 167], ["pycolab.prefab_parts.sprites.MazeWalker.__init__"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceBuffer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "corner", ",", "position", ",", "character", ")", ":", "\n", "        ", "\"\"\"Constructor: simply supplies characters that players can't traverse.\"\"\"", "\n", "super", "(", "PlayerSprite", ",", "self", ")", ".", "__init__", "(", "\n", "corner", ",", "position", ",", "character", ",", "impassable", "=", "'#H.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.PlayerSprite.update": [[168, 182], ["randomized_v2.PlayerSprite._teleport", "randomized_v2.PlayerSprite._north", "randomized_v2.PlayerSprite._south", "randomized_v2.PlayerSprite._west", "randomized_v2.PlayerSprite._east"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "actions", ",", "board", ",", "layers", ",", "backdrop", ",", "things", ",", "the_plot", ")", ":", "\n", "        ", "del", "backdrop", ",", "things", ",", "layers", "# Unused.", "\n", "\n", "if", "the_plot", ".", "frame", "==", "0", ":", "\n", "            ", "self", ".", "_teleport", "(", "the_plot", "[", "'P_pos'", "]", ")", "\n", "\n", "", "if", "actions", "==", "0", ":", "# go upward?", "\n", "            ", "self", ".", "_north", "(", "board", ",", "the_plot", ")", "\n", "", "elif", "actions", "==", "1", ":", "# go downward?", "\n", "            ", "self", ".", "_south", "(", "board", ",", "the_plot", ")", "\n", "", "elif", "actions", "==", "2", ":", "# go leftward?", "\n", "            ", "self", ".", "_west", "(", "board", ",", "the_plot", ")", "\n", "", "elif", "actions", "==", "3", ":", "# go rightward?", "\n", "            ", "self", ".", "_east", "(", "board", ",", "the_plot", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.JudgeDrape.__init__": [[185, 189], ["pycolab.things.Drape.__init__"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceBuffer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "curtain", ",", "character", ")", ":", "\n", "        ", "super", "(", "JudgeDrape", ",", "self", ")", ".", "__init__", "(", "curtain", ",", "character", ")", "\n", "self", ".", "_step_counter", "=", "0", "\n", "self", ".", "_max_steps", "=", "MAX_STEPS", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.JudgeDrape.update": [[190, 201], ["randomized_v2.JudgeDrape.curtain.fill", "the_plot.add_reward", "numpy.array", "the_plot.terminate_episode"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "actions", ",", "board", ",", "layers", ",", "backdrop", ",", "things", ",", "the_plot", ")", ":", "\n", "# Clear our curtain and mark the locations of all the boxes True.", "\n", "        ", "self", ".", "curtain", ".", "fill", "(", "False", ")", "\n", "the_plot", ".", "add_reward", "(", "np", ".", "array", "(", "[", "0.", ",", "0.", "]", ")", ")", "\n", "#the_plot.add_reward(-0.1)", "\n", "self", ".", "_step_counter", "+=", "1", "\n", "\n", "# See if we should quit: it happens if the user solves the puzzle or if", "\n", "# they give up and execute the 'quit' action.", "\n", "if", "(", "actions", "==", "9", ")", "or", "(", "self", ".", "_step_counter", "==", "self", ".", "_max_steps", ")", ":", "\n", "            ", "the_plot", ".", "terminate_episode", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.make_game": [[59, 83], ["pycolab.ascii_art.ascii_art_to_game"], "function", ["None"], ["def", "make_game", "(", "seed", "=", "None", ",", "demo", "=", "False", ")", ":", "\n", "    ", "warehouse_art", "=", "WAREHOUSE_ART", "\n", "what_lies_beneath", "=", "BACKGROUND_ART", "\n", "sprites", "=", "{", "'P'", ":", "PlayerSprite", "}", "\n", "\n", "if", "demo", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "", "else", ":", "\n", "        ", "drapes", "=", "{", "'X'", ":", "JudgeDrape", "}", "\n", "\n", "", "drapes", "[", "'C'", "]", "=", "CitizenDrape", "\n", "drapes", "[", "'H'", "]", "=", "HouseDrape", "\n", "drapes", "[", "'G'", "]", "=", "GoalDrape", "\n", "\n", "update_schedule", "=", "[", "[", "'C'", "]", ",", "\n", "[", "'G'", "]", ",", "\n", "[", "'H'", "]", ",", "\n", "[", "'X'", "]", ",", "\n", "[", "'P'", "]", "]", "\n", "\n", "\n", "return", "ascii_art", ".", "ascii_art_to_game", "(", "\n", "warehouse_art", ",", "what_lies_beneath", ",", "sprites", ",", "drapes", ",", "\n", "update_schedule", "=", "update_schedule", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.scalar_to_idx": [[85, 89], ["int", "numpy.floor"], "function", ["None"], ["", "def", "scalar_to_idx", "(", "x", ")", ":", "\n", "    ", "row", "=", "x", "%", "6", "\n", "col", "=", "int", "(", "np", ".", "floor", "(", "x", "/", "6", ")", ")", "\n", "return", "(", "row", "+", "1", ",", "col", "+", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.main": [[203, 222], ["randomized_v2.make_game", "pycolab.human_ui.CursesUi", "human_ui.CursesUi.play"], "function", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.randomized_v2.make_game"], ["", "", "", "def", "main", "(", "demo", ")", ":", "\n", "# Builds an interactive game session.", "\n", "    ", "game", "=", "make_game", "(", "demo", "=", "demo", ")", "\n", "\n", "\n", "ui", "=", "human_ui", ".", "CursesUi", "(", "\n", "keys_to_actions", "=", "{", "curses", ".", "KEY_UP", ":", "0", ",", "curses", ".", "KEY_DOWN", ":", "1", ",", "\n", "curses", ".", "KEY_LEFT", ":", "2", ",", "curses", ".", "KEY_RIGHT", ":", "3", ",", "\n", "'w'", ":", "5", ",", "\n", "'s'", ":", "6", ",", "\n", "'a'", ":", "7", ",", "\n", "'d'", ":", "8", ",", "\n", "-", "1", ":", "4", ",", "\n", "'q'", ":", "9", ",", "'Q'", ":", "9", "}", ",", "\n", "delay", "=", "1000", ",", "\n", "colour_fg", "=", "WAREHOUSE_FG_COLOURS", ")", "\n", "\n", "# Let the game begin!", "\n", "ui", ".", "play", "(", "game", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelConv.__init__": [[11, 30], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Linear", "torch.Linear", "torch.Linear", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceBuffer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "state_shape", ",", "in_channels", "=", "6", ",", "n_actions", "=", "9", ")", ":", "\n", "        ", "super", "(", "PreferenceModelConv", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# General Parameters", "\n", "self", ".", "state_shape", "=", "state_shape", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "\n", "self", ".", "action_embedding", "=", "nn", ".", "Linear", "(", "n_actions", ",", "in_channels", "*", "state_shape", "[", "0", "]", "*", "state_shape", "[", "1", "]", ")", "\n", "self", ".", "reward_conv1", "=", "nn", ".", "Conv2d", "(", "in_channels", "=", "self", ".", "in_channels", "*", "2", ",", "out_channels", "=", "128", ",", "kernel_size", "=", "2", ")", "\n", "self", ".", "reward_conv2", "=", "nn", ".", "Conv2d", "(", "in_channels", "=", "128", ",", "out_channels", "=", "64", ",", "kernel_size", "=", "2", ")", "\n", "self", ".", "reward_conv3", "=", "nn", ".", "Conv2d", "(", "in_channels", "=", "64", ",", "out_channels", "=", "32", ",", "kernel_size", "=", "2", ")", "\n", "self", ".", "reward_out", "=", "nn", ".", "Linear", "(", "32", "*", "(", "state_shape", "[", "0", "]", "-", "3", ")", "*", "(", "state_shape", "[", "1", "]", "-", "3", ")", ",", "1", ")", "\n", "\n", "# Dropout", "\n", "#self.dropout = nn.Dropout(p=0)", "\n", "\n", "# Activation", "\n", "self", ".", "relu", "=", "nn", ".", "LeakyReLU", "(", "0.01", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelConv.forward": [[31, 45], ["torch.one_hot().float().to", "torch.one_hot().float().to", "torch.one_hot().float().to", "preference_model.PreferenceModelConv.relu().view", "state.view.view.view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "preference_model.PreferenceModelConv.relu", "preference_model.PreferenceModelConv.relu", "preference_model.PreferenceModelConv.relu", "preference_model.PreferenceModelConv.view", "preference_model.PreferenceModelConv.reward_out", "preference_model.PreferenceModelConv.reward_conv1", "preference_model.PreferenceModelConv.reward_conv2", "preference_model.PreferenceModelConv.reward_conv3", "torch.one_hot().float", "torch.one_hot().float", "torch.one_hot().float", "preference_model.PreferenceModelConv.relu", "preference_model.PreferenceModelConv.action_embedding", "torch.one_hot", "torch.one_hot", "torch.one_hot", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "state", ",", "action", ")", ":", "\n", "        ", "action_embedded", "=", "F", ".", "one_hot", "(", "torch", ".", "tensor", "(", "action", ")", ".", "long", "(", ")", ",", "self", ".", "n_actions", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "action_embedded", "=", "self", ".", "relu", "(", "self", ".", "action_embedding", "(", "action_embedded", ")", ")", ".", "view", "(", "-", "1", ",", "self", ".", "in_channels", ",", "\n", "self", ".", "state_shape", "[", "0", "]", ",", "\n", "self", ".", "state_shape", "[", "1", "]", ")", "\n", "state", "=", "state", ".", "view", "(", "-", "1", ",", "self", ".", "in_channels", ",", "self", ".", "state_shape", "[", "0", "]", ",", "self", ".", "state_shape", "[", "1", "]", ")", "\n", "x", "=", "torch", ".", "cat", "(", "[", "state", ",", "action_embedded", "]", ",", "dim", "=", "1", ")", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "reward_conv1", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "reward_conv2", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "reward_conv3", "(", "x", ")", ")", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "x", "=", "self", ".", "reward_out", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelConv.evaluate_trajectory": [[46, 52], ["torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "preference_model.PreferenceModelConv.forward", "preference_model.PreferenceModelConv.sum().squeeze", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "preference_model.PreferenceModelConv.sum", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.forward"], ["", "def", "evaluate_trajectory", "(", "self", ",", "tau", ")", ":", "\n", "        ", "trajectory_states", "=", "torch", ".", "tensor", "(", "tau", "[", "'states'", "]", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "trajectory_actions", "=", "torch", ".", "tensor", "(", "tau", "[", "'actions'", "]", ")", ".", "to", "(", "device", ")", "\n", "predicted_rewards", "=", "self", ".", "forward", "(", "trajectory_states", ",", "trajectory_actions", ")", "\n", "\n", "return", "predicted_rewards", ".", "sum", "(", "dim", "=", "0", ")", ".", "squeeze", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelConv.compare_trajectory": [[53, 60], ["preference_model.PreferenceModelConv.evaluate_trajectory", "preference_model.PreferenceModelConv.evaluate_trajectory", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.evaluate_trajectory", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.evaluate_trajectory"], ["", "def", "compare_trajectory", "(", "self", ",", "tau_1", ",", "tau_2", ")", ":", "\n", "# Returns log P(tau_1 > tau_2) using the Bradley-Terry model", "\n", "\n", "        ", "returns_1", "=", "self", ".", "evaluate_trajectory", "(", "tau_1", ")", "\n", "returns_2", "=", "self", ".", "evaluate_trajectory", "(", "tau_2", ")", "\n", "\n", "return", "returns_1", "-", "torch", ".", "log", "(", "torch", ".", "exp", "(", "returns_1", ")", "+", "torch", ".", "exp", "(", "returns_2", ")", "+", "1e-6", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.__init__": [[63, 83], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceBuffer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "state_shape", ",", "in_channels", "=", "6", ",", "n_actions", "=", "9", ")", ":", "\n", "        ", "super", "(", "PreferenceModelMLP", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# General Parameters", "\n", "self", ".", "state_shape", "=", "state_shape", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "\n", "# Layers", "\n", "self", ".", "action_embedding", "=", "nn", ".", "Linear", "(", "n_actions", ",", "512", ")", "\n", "self", ".", "reward_l1", "=", "nn", ".", "Linear", "(", "self", ".", "in_channels", "*", "self", ".", "state_shape", "[", "0", "]", "*", "self", ".", "state_shape", "[", "1", "]", ",", "512", ")", "\n", "self", ".", "reward_l2", "=", "nn", ".", "Linear", "(", "1024", ",", "2056", ")", "\n", "self", ".", "reward_l3", "=", "nn", ".", "Linear", "(", "2056", ",", "1024", ")", "\n", "self", ".", "reward_out", "=", "nn", ".", "Linear", "(", "1024", ",", "1", ")", "\n", "\n", "# Dropout", "\n", "#self.dropout = nn.Dropout(p=0)", "\n", "\n", "# Activation", "\n", "self", ".", "relu", "=", "nn", ".", "LeakyReLU", "(", "0.01", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.forward": [[85, 100], ["torch.one_hot().float().to", "torch.one_hot().float().to", "torch.one_hot().float().to", "preference_model.PreferenceModelMLP.relu", "state.view.view.view", "preference_model.PreferenceModelMLP.relu", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "preference_model.PreferenceModelMLP.relu", "preference_model.PreferenceModelMLP.relu", "preference_model.PreferenceModelMLP.reward_out", "preference_model.PreferenceModelMLP.action_embedding", "preference_model.PreferenceModelMLP.reward_l1", "preference_model.PreferenceModelMLP.reward_l2", "preference_model.PreferenceModelMLP.reward_l3", "torch.one_hot().float", "torch.one_hot().float", "torch.one_hot().float", "torch.one_hot", "torch.one_hot", "torch.one_hot", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "state", ",", "action", ")", ":", "\n", "        ", "action_embedded", "=", "F", ".", "one_hot", "(", "torch", ".", "tensor", "(", "action", ")", ".", "long", "(", ")", ",", "self", ".", "n_actions", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "action_embedded", "=", "self", ".", "relu", "(", "self", ".", "action_embedding", "(", "action_embedded", ")", ")", "\n", "\n", "state", "=", "state", ".", "view", "(", "state", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "state_embedded", "=", "self", ".", "relu", "(", "self", ".", "reward_l1", "(", "state", ")", ")", "\n", "\n", "x", "=", "torch", ".", "cat", "(", "[", "state_embedded", ",", "action_embedded", "]", ",", "dim", "=", "-", "1", ")", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "reward_l2", "(", "x", ")", ")", "\n", "#x = self.dropout(x)", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "reward_l3", "(", "x", ")", ")", "\n", "#x = self.dropout(x)", "\n", "x", "=", "self", ".", "reward_out", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.evaluate_trajectory": [[101, 107], ["torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().float().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "preference_model.PreferenceModelMLP.forward", "preference_model.PreferenceModelMLP.sum().squeeze", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "preference_model.PreferenceModelMLP.sum", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.forward"], ["", "def", "evaluate_trajectory", "(", "self", ",", "tau", ")", ":", "\n", "        ", "trajectory_states", "=", "torch", ".", "tensor", "(", "tau", "[", "'states'", "]", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "trajectory_actions", "=", "torch", ".", "tensor", "(", "tau", "[", "'actions'", "]", ")", ".", "to", "(", "device", ")", "\n", "predicted_rewards", "=", "self", ".", "forward", "(", "trajectory_states", ",", "trajectory_actions", ")", "\n", "\n", "return", "predicted_rewards", ".", "sum", "(", "dim", "=", "0", ")", ".", "squeeze", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.compare_trajectory": [[108, 115], ["preference_model.PreferenceModelMLP.evaluate_trajectory", "preference_model.PreferenceModelMLP.evaluate_trajectory", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp"], "methods", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.evaluate_trajectory", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.evaluate_trajectory"], ["", "def", "compare_trajectory", "(", "self", ",", "tau_1", ",", "tau_2", ")", ":", "\n", "# Returns log P(tau_1 > tau_2) using the Bradley-Terry model", "\n", "\n", "        ", "returns_1", "=", "self", ".", "evaluate_trajectory", "(", "tau_1", ")", "\n", "returns_2", "=", "self", ".", "evaluate_trajectory", "(", "tau_2", ")", "\n", "\n", "return", "returns_1", "-", "torch", ".", "log", "(", "torch", ".", "exp", "(", "returns_1", ")", "+", "torch", ".", "exp", "(", "returns_2", ")", "+", "1e-6", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceBuffer.__init__": [[117, 119], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "storage", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceBuffer.add_preference": [[120, 122], ["preference_model.PreferenceBuffer.storage.append"], "methods", ["None"], ["", "def", "add_preference", "(", "self", ",", "tau_1", ",", "tau_2", ",", "mu", ")", ":", "\n", "        ", "self", ".", "storage", ".", "append", "(", "(", "tau_1", ",", "tau_2", ",", "mu", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.update_preference_model": [[124, 143], ["torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "range", "preference_optimizer.zero_grad", "torch.tensor().to.backward", "preference_optimizer.step", "torch.tensor().to.item", "numpy.random.randint", "preference_model.compare_trajectory", "preference_model.compare_trajectory", "torch.tensor", "torch.tensor", "torch.tensor", "len"], "function", ["home.repos.pwc.inspect_result.mlpeschl_moral_rl.envs.gym_wrapper.VecEnv.step", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.compare_trajectory", "home.repos.pwc.inspect_result.mlpeschl_moral_rl.drlhp.preference_model.PreferenceModelMLP.compare_trajectory"], ["", "", "def", "update_preference_model", "(", "preference_model", ",", "preference_buffer", ",", "preference_optimizer", ",", "batch_size", ")", ":", "\n", "    ", "overall_loss", "=", "torch", ".", "tensor", "(", "0.", ")", ".", "to", "(", "device", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "\n", "# Sample random preference", "\n", "        ", "rand_idx", "=", "np", ".", "random", ".", "randint", "(", "len", "(", "preference_buffer", ".", "storage", ")", ")", "\n", "rand_tau_1", ",", "rand_tau_2", ",", "rand_mu", "=", "preference_buffer", ".", "storage", "[", "rand_idx", "]", "\n", "\n", "# Add to Loss", "\n", "superior_log_prob", "=", "preference_model", ".", "compare_trajectory", "(", "rand_tau_1", ",", "rand_tau_2", ")", "\n", "inferior_log_prob", "=", "preference_model", ".", "compare_trajectory", "(", "rand_tau_2", ",", "rand_tau_1", ")", "\n", "overall_loss", "-=", "(", "rand_mu", "[", "0", "]", "*", "superior_log_prob", "+", "rand_mu", "[", "1", "]", "*", "inferior_log_prob", ")", "\n", "\n", "", "overall_loss", "=", "overall_loss", "/", "batch_size", "\n", "preference_optimizer", ".", "zero_grad", "(", ")", "\n", "overall_loss", ".", "backward", "(", ")", "\n", "preference_optimizer", ".", "step", "(", ")", "\n", "\n", "return", "overall_loss", ".", "item", "(", ")", "\n", "", ""]]}