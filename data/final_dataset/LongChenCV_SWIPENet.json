{"home.repos.pwc.inspect_result.LongChenCV_SWIPENet.None.ssd512_evaluation.intersection_area_": [[102, 152], ["numpy.maximum", "numpy.minimum", "numpy.maximum", "numpy.tile", "numpy.tile", "numpy.tile", "numpy.tile", "numpy.maximum", "numpy.minimum", "numpy.maximum", "numpy.expand_dims", "numpy.expand_dims", "numpy.expand_dims", "numpy.expand_dims"], "function", ["None"], ["def", "intersection_area_", "(", "boxes1", ",", "boxes2", ",", "coords", "=", "'corners'", ",", "mode", "=", "'outer_product'", ",", "border_pixels", "=", "'half'", ")", ":", "\n", "    ", "'''\n    The same as 'intersection_area()' but for internal use, i.e. without all the safety checks.\n    '''", "\n", "m", "=", "boxes1", ".", "shape", "[", "0", "]", "# The number of boxes in `boxes1`", "\n", "n", "=", "boxes2", ".", "shape", "[", "0", "]", "# The number of boxes in `boxes2`", "\n", "# Set the correct coordinate indices for the respective formats.", "\n", "if", "coords", "==", "'corners'", ":", "\n", "        ", "xmin", "=", "0", "\n", "ymin", "=", "1", "\n", "xmax", "=", "2", "\n", "ymax", "=", "3", "\n", "", "elif", "coords", "==", "'minmax'", ":", "\n", "        ", "xmin", "=", "0", "\n", "xmax", "=", "1", "\n", "ymin", "=", "2", "\n", "ymax", "=", "3", "\n", "\n", "", "if", "border_pixels", "==", "'half'", ":", "\n", "        ", "d", "=", "0", "\n", "", "elif", "border_pixels", "==", "'include'", ":", "\n", "        ", "d", "=", "1", "# If border pixels are supposed to belong to the bounding boxes, we have to add one pixel to any difference `xmax - xmin` or `ymax - ymin`.", "\n", "", "elif", "border_pixels", "==", "'exclude'", ":", "\n", "        ", "d", "=", "-", "1", "# If border pixels are not supposed to belong to the bounding boxes, we have to subtract one pixel from any difference `xmax - xmin` or `ymax - ymin`.", "\n", "\n", "# Compute the intersection areas.", "\n", "", "if", "mode", "==", "'outer_product'", ":", "\n", "# For all possible box combinations, get the greater xmin and ymin values.", "\n", "# This is a tensor of shape (m,n,2).", "\n", "        ", "min_xy", "=", "np", ".", "maximum", "(", "np", ".", "tile", "(", "np", ".", "expand_dims", "(", "boxes1", "[", ":", ",", "[", "xmin", ",", "ymin", "]", "]", ",", "axis", "=", "1", ")", ",", "reps", "=", "(", "1", ",", "n", ",", "1", ")", ")", ",", "\n", "np", ".", "tile", "(", "np", ".", "expand_dims", "(", "boxes2", "[", ":", ",", "[", "xmin", ",", "ymin", "]", "]", ",", "axis", "=", "0", ")", ",", "reps", "=", "(", "m", ",", "1", ",", "1", ")", ")", ")", "\n", "# For all possible box combinations, get the smaller xmax and ymax values.", "\n", "# This is a tensor of shape (m,n,2).", "\n", "max_xy", "=", "np", ".", "minimum", "(", "np", ".", "tile", "(", "np", ".", "expand_dims", "(", "boxes1", "[", ":", ",", "[", "xmax", ",", "ymax", "]", "]", ",", "axis", "=", "1", ")", ",", "reps", "=", "(", "1", ",", "n", ",", "1", ")", ")", ",", "\n", "np", ".", "tile", "(", "np", ".", "expand_dims", "(", "boxes2", "[", ":", ",", "[", "xmax", ",", "ymax", "]", "]", ",", "axis", "=", "0", ")", ",", "reps", "=", "(", "m", ",", "1", ",", "1", ")", ")", ")", "\n", "\n", "# Compute the side lengths of the intersection rectangles.", "\n", "side_lengths", "=", "np", ".", "maximum", "(", "0", ",", "max_xy", "-", "min_xy", "+", "d", ")", "\n", "\n", "return", "side_lengths", "[", ":", ",", ":", ",", "0", "]", "*", "side_lengths", "[", ":", ",", ":", ",", "1", "]", "\n", "\n", "", "elif", "mode", "==", "'element-wise'", ":", "\n", "\n", "        ", "min_xy", "=", "np", ".", "maximum", "(", "boxes1", "[", ":", ",", "[", "xmin", ",", "ymin", "]", "]", ",", "boxes2", "[", ":", ",", "[", "xmin", ",", "ymin", "]", "]", ")", "\n", "max_xy", "=", "np", ".", "minimum", "(", "boxes1", "[", ":", ",", "[", "xmax", ",", "ymax", "]", "]", ",", "boxes2", "[", ":", ",", "[", "xmax", ",", "ymax", "]", "]", ")", "\n", "\n", "# Compute the side lengths of the intersection rectangles.", "\n", "side_lengths", "=", "np", ".", "maximum", "(", "0", ",", "max_xy", "-", "min_xy", "+", "d", ")", "\n", "\n", "return", "side_lengths", "[", ":", ",", "0", "]", "*", "side_lengths", "[", ":", ",", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.None.ssd512_evaluation.iou": [[153, 213], ["ssd512_evaluation.intersection_area_", "ValueError", "ValueError", "numpy.expand_dims", "numpy.expand_dims", "ValueError", "numpy.tile", "numpy.tile", "numpy.expand_dims", "numpy.expand_dims"], "function", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.intersection_area_"], ["", "", "def", "iou", "(", "boxes1", ",", "boxes2", ",", "coords", "=", "'centroids'", ",", "mode", "=", "'outer_product'", ",", "border_pixels", "=", "'half'", ")", ":", "\n", "\n", "# Make sure the boxes have the right shapes.", "\n", "    ", "if", "boxes1", ".", "ndim", ">", "2", ":", "raise", "ValueError", "(", "\n", "\"boxes1 must have rank either 1 or 2, but has rank {}.\"", ".", "format", "(", "boxes1", ".", "ndim", ")", ")", "\n", "if", "boxes2", ".", "ndim", ">", "2", ":", "raise", "ValueError", "(", "\n", "\"boxes2 must have rank either 1 or 2, but has rank {}.\"", ".", "format", "(", "boxes2", ".", "ndim", ")", ")", "\n", "\n", "if", "boxes1", ".", "ndim", "==", "1", ":", "boxes1", "=", "np", ".", "expand_dims", "(", "boxes1", ",", "axis", "=", "0", ")", "\n", "if", "boxes2", ".", "ndim", "==", "1", ":", "boxes2", "=", "np", ".", "expand_dims", "(", "boxes2", ",", "axis", "=", "0", ")", "\n", "\n", "if", "not", "(", "boxes1", ".", "shape", "[", "1", "]", "==", "boxes2", ".", "shape", "[", "1", "]", "==", "4", ")", ":", "raise", "ValueError", "(", "\n", "\"All boxes must consist of 4 coordinates, but the boxes in `boxes1` and `boxes2` have {} and {} coordinates, respectively.\"", ".", "format", "(", "\n", "boxes1", ".", "shape", "[", "1", "]", ",", "boxes2", ".", "shape", "[", "1", "]", ")", ")", "\n", "\n", "# Compute the interesection areas.", "\n", "\n", "intersection_areas", "=", "intersection_area_", "(", "boxes1", ",", "boxes2", ",", "coords", "=", "coords", ",", "mode", "=", "mode", ")", "\n", "\n", "m", "=", "boxes1", ".", "shape", "[", "0", "]", "# The number of boxes in `boxes1`", "\n", "n", "=", "boxes2", ".", "shape", "[", "0", "]", "# The number of boxes in `boxes2`", "\n", "\n", "# Compute the union areas.", "\n", "\n", "# Set the correct coordinate indices for the respective formats.", "\n", "if", "coords", "==", "'corners'", ":", "\n", "        ", "xmin", "=", "0", "\n", "ymin", "=", "1", "\n", "xmax", "=", "2", "\n", "ymax", "=", "3", "\n", "", "elif", "coords", "==", "'minmax'", ":", "\n", "        ", "xmin", "=", "0", "\n", "xmax", "=", "1", "\n", "ymin", "=", "2", "\n", "ymax", "=", "3", "\n", "\n", "", "if", "border_pixels", "==", "'half'", ":", "\n", "        ", "d", "=", "0", "\n", "", "elif", "border_pixels", "==", "'include'", ":", "\n", "        ", "d", "=", "1", "# If border pixels are supposed to belong to the bounding boxes, we have to add one pixel to any difference `xmax - xmin` or `ymax - ymin`.", "\n", "", "elif", "border_pixels", "==", "'exclude'", ":", "\n", "        ", "d", "=", "-", "1", "# If border pixels are not supposed to belong to the bounding boxes, we have to subtract one pixel from any difference `xmax - xmin` or `ymax - ymin`.", "\n", "\n", "", "if", "mode", "==", "'outer_product'", ":", "\n", "\n", "        ", "boxes1_areas", "=", "np", ".", "tile", "(", "\n", "np", ".", "expand_dims", "(", "(", "boxes1", "[", ":", ",", "xmax", "]", "-", "boxes1", "[", ":", ",", "xmin", "]", "+", "d", ")", "*", "(", "boxes1", "[", ":", ",", "ymax", "]", "-", "boxes1", "[", ":", ",", "ymin", "]", "+", "d", ")", ",", "\n", "axis", "=", "1", ")", ",", "reps", "=", "(", "1", ",", "n", ")", ")", "\n", "boxes2_areas", "=", "np", ".", "tile", "(", "\n", "np", ".", "expand_dims", "(", "(", "boxes2", "[", ":", ",", "xmax", "]", "-", "boxes2", "[", ":", ",", "xmin", "]", "+", "d", ")", "*", "(", "boxes2", "[", ":", ",", "ymax", "]", "-", "boxes2", "[", ":", ",", "ymin", "]", "+", "d", ")", ",", "\n", "axis", "=", "0", ")", ",", "reps", "=", "(", "m", ",", "1", ")", ")", "\n", "\n", "", "elif", "mode", "==", "'element-wise'", ":", "\n", "\n", "        ", "boxes1_areas", "=", "(", "boxes1", "[", ":", ",", "xmax", "]", "-", "boxes1", "[", ":", ",", "xmin", "]", "+", "d", ")", "*", "(", "boxes1", "[", ":", ",", "ymax", "]", "-", "boxes1", "[", ":", ",", "ymin", "]", "+", "d", ")", "\n", "boxes2_areas", "=", "(", "boxes2", "[", ":", ",", "xmax", "]", "-", "boxes2", "[", ":", ",", "xmin", "]", "+", "d", ")", "*", "(", "boxes2", "[", ":", ",", "ymax", "]", "-", "boxes2", "[", ":", ",", "ymin", "]", "+", "d", ")", "\n", "\n", "", "union_areas", "=", "boxes1_areas", "+", "boxes2_areas", "-", "intersection_areas", "\n", "\n", "return", "intersection_areas", "/", "union_areas", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.None.ssd512_evaluation.nms": [[214, 250], ["numpy.argsort", "pick.append", "range", "numpy.delete", "max", "max", "min", "min", "suppress.append"], "function", ["None"], ["", "def", "nms", "(", "boxes", ",", "overlap", ")", ":", "\n", "    ", "if", "not", "boxes", ".", "shape", "[", "0", "]", ":", "\n", "        ", "pick", "=", "[", "]", "\n", "", "else", ":", "\n", "        ", "trial", "=", "boxes", "\n", "x1", "=", "trial", "[", ":", ",", "0", "]", "\n", "y1", "=", "trial", "[", ":", ",", "1", "]", "\n", "x2", "=", "trial", "[", ":", ",", "2", "]", "\n", "y2", "=", "trial", "[", ":", ",", "3", "]", "\n", "score", "=", "trial", "[", ":", ",", "4", "]", "\n", "area", "=", "(", "x2", "-", "x1", "+", "1", ")", "*", "(", "y2", "-", "y1", "+", "1", ")", "\n", "\n", "I", "=", "np", ".", "argsort", "(", "score", ")", "\n", "pick", "=", "[", "]", "\n", "count", "=", "1", "\n", "while", "(", "I", ".", "size", "!=", "0", ")", ":", "\n", "# print \"Iteration:\",count", "\n", "            ", "last", "=", "I", ".", "size", "\n", "i", "=", "I", "[", "last", "-", "1", "]", "\n", "pick", ".", "append", "(", "i", ")", "\n", "suppress", "=", "[", "last", "-", "1", "]", "\n", "for", "pos", "in", "range", "(", "last", "-", "1", ")", ":", "\n", "                ", "j", "=", "I", "[", "pos", "]", "\n", "xx1", "=", "max", "(", "x1", "[", "i", "]", ",", "x1", "[", "j", "]", ")", "\n", "yy1", "=", "max", "(", "y1", "[", "i", "]", ",", "y1", "[", "j", "]", ")", "\n", "xx2", "=", "min", "(", "x2", "[", "i", "]", ",", "x2", "[", "j", "]", ")", "\n", "yy2", "=", "min", "(", "y2", "[", "i", "]", ",", "y2", "[", "j", "]", ")", "\n", "w", "=", "xx2", "-", "xx1", "+", "1", "\n", "h", "=", "yy2", "-", "yy1", "+", "1", "\n", "if", "(", "w", ">", "0", "and", "h", ">", "0", ")", ":", "\n", "                    ", "o", "=", "w", "*", "h", "/", "area", "[", "j", "]", "\n", "if", "(", "o", ">", "overlap", ")", ":", "\n", "                        ", "suppress", ".", "append", "(", "pos", ")", "\n", "", "", "", "I", "=", "np", ".", "delete", "(", "I", ",", "suppress", ")", "\n", "count", "=", "count", "+", "1", "\n", "", "", "return", "pick", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.None.ssd512_training.lr_schedule": [[143, 150], ["None"], "function", ["None"], ["def", "lr_schedule", "(", "epoch", ")", ":", "\n", "    ", "if", "epoch", "<", "80", ":", "\n", "        ", "return", "0.001", "\n", "", "elif", "epoch", "<", "120", ":", "\n", "        ", "return", "0.0001", "\n", "", "else", ":", "\n", "        ", "return", "0.0001", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.convert_coordinates": [[24, 88], ["numpy.copy().astype", "numpy.copy", "ValueError"], "function", ["None"], ["def", "convert_coordinates", "(", "tensor", ",", "start_index", ",", "conversion", ",", "border_pixels", "=", "'half'", ")", ":", "\n", "    ", "'''\n    Convert coordinates for axis-aligned 2D boxes between two coordinate formats.\n\n    Creates a copy of `tensor`, i.e. does not operate in place. Currently there are\n    three supported coordinate formats that can be converted from and to each other:\n        1) (xmin, xmax, ymin, ymax) - the 'minmax' format\n        2) (xmin, ymin, xmax, ymax) - the 'corners' format\n        2) (cx, cy, w, h) - the 'centroids' format\n\n    Arguments:\n        tensor (array): A Numpy nD array containing the four consecutive coordinates\n            to be converted somewhere in the last axis.\n        start_index (int): The index of the first coordinate in the last axis of `tensor`.\n        conversion (str, optional): The conversion direction. Can be 'minmax2centroids',\n            'centroids2minmax', 'corners2centroids', 'centroids2corners', 'minmax2corners',\n            or 'corners2minmax'.\n        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n            to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n            If 'half', then one of each of the two horizontal and vertical borders belong\n            to the boxex, but not the other.\n\n    Returns:\n        A Numpy nD array, a copy of the input tensor with the converted coordinates\n        in place of the original coordinates and the unaltered elements of the original\n        tensor elsewhere.\n    '''", "\n", "if", "border_pixels", "==", "'half'", ":", "\n", "        ", "d", "=", "0", "\n", "", "elif", "border_pixels", "==", "'include'", ":", "\n", "        ", "d", "=", "1", "\n", "", "elif", "border_pixels", "==", "'exclude'", ":", "\n", "        ", "d", "=", "-", "1", "\n", "\n", "", "ind", "=", "start_index", "\n", "tensor1", "=", "np", ".", "copy", "(", "tensor", ")", ".", "astype", "(", "np", ".", "float", ")", "\n", "if", "conversion", "==", "'minmax2centroids'", ":", "\n", "        ", "tensor1", "[", "...", ",", "ind", "]", "=", "(", "tensor", "[", "...", ",", "ind", "]", "+", "tensor", "[", "...", ",", "ind", "+", "1", "]", ")", "/", "2.0", "# Set cx", "\n", "tensor1", "[", "...", ",", "ind", "+", "1", "]", "=", "(", "tensor", "[", "...", ",", "ind", "+", "2", "]", "+", "tensor", "[", "...", ",", "ind", "+", "3", "]", ")", "/", "2.0", "# Set cy", "\n", "tensor1", "[", "...", ",", "ind", "+", "2", "]", "=", "tensor", "[", "...", ",", "ind", "+", "1", "]", "-", "tensor", "[", "...", ",", "ind", "]", "+", "d", "# Set w", "\n", "tensor1", "[", "...", ",", "ind", "+", "3", "]", "=", "tensor", "[", "...", ",", "ind", "+", "3", "]", "-", "tensor", "[", "...", ",", "ind", "+", "2", "]", "+", "d", "# Set h", "\n", "", "elif", "conversion", "==", "'centroids2minmax'", ":", "\n", "        ", "tensor1", "[", "...", ",", "ind", "]", "=", "tensor", "[", "...", ",", "ind", "]", "-", "tensor", "[", "...", ",", "ind", "+", "2", "]", "/", "2.0", "# Set xmin", "\n", "tensor1", "[", "...", ",", "ind", "+", "1", "]", "=", "tensor", "[", "...", ",", "ind", "]", "+", "tensor", "[", "...", ",", "ind", "+", "2", "]", "/", "2.0", "# Set xmax", "\n", "tensor1", "[", "...", ",", "ind", "+", "2", "]", "=", "tensor", "[", "...", ",", "ind", "+", "1", "]", "-", "tensor", "[", "...", ",", "ind", "+", "3", "]", "/", "2.0", "# Set ymin", "\n", "tensor1", "[", "...", ",", "ind", "+", "3", "]", "=", "tensor", "[", "...", ",", "ind", "+", "1", "]", "+", "tensor", "[", "...", ",", "ind", "+", "3", "]", "/", "2.0", "# Set ymax", "\n", "", "elif", "conversion", "==", "'corners2centroids'", ":", "\n", "        ", "tensor1", "[", "...", ",", "ind", "]", "=", "(", "tensor", "[", "...", ",", "ind", "]", "+", "tensor", "[", "...", ",", "ind", "+", "2", "]", ")", "/", "2.0", "# Set cx", "\n", "tensor1", "[", "...", ",", "ind", "+", "1", "]", "=", "(", "tensor", "[", "...", ",", "ind", "+", "1", "]", "+", "tensor", "[", "...", ",", "ind", "+", "3", "]", ")", "/", "2.0", "# Set cy", "\n", "tensor1", "[", "...", ",", "ind", "+", "2", "]", "=", "tensor", "[", "...", ",", "ind", "+", "2", "]", "-", "tensor", "[", "...", ",", "ind", "]", "+", "d", "# Set w", "\n", "tensor1", "[", "...", ",", "ind", "+", "3", "]", "=", "tensor", "[", "...", ",", "ind", "+", "3", "]", "-", "tensor", "[", "...", ",", "ind", "+", "1", "]", "+", "d", "# Set h", "\n", "", "elif", "conversion", "==", "'centroids2corners'", ":", "\n", "        ", "tensor1", "[", "...", ",", "ind", "]", "=", "tensor", "[", "...", ",", "ind", "]", "-", "tensor", "[", "...", ",", "ind", "+", "2", "]", "/", "2.0", "# Set xmin", "\n", "tensor1", "[", "...", ",", "ind", "+", "1", "]", "=", "tensor", "[", "...", ",", "ind", "+", "1", "]", "-", "tensor", "[", "...", ",", "ind", "+", "3", "]", "/", "2.0", "# Set ymin", "\n", "tensor1", "[", "...", ",", "ind", "+", "2", "]", "=", "tensor", "[", "...", ",", "ind", "]", "+", "tensor", "[", "...", ",", "ind", "+", "2", "]", "/", "2.0", "# Set xmax", "\n", "tensor1", "[", "...", ",", "ind", "+", "3", "]", "=", "tensor", "[", "...", ",", "ind", "+", "1", "]", "+", "tensor", "[", "...", ",", "ind", "+", "3", "]", "/", "2.0", "# Set ymax", "\n", "", "elif", "(", "conversion", "==", "'minmax2corners'", ")", "or", "(", "conversion", "==", "'corners2minmax'", ")", ":", "\n", "        ", "tensor1", "[", "...", ",", "ind", "+", "1", "]", "=", "tensor", "[", "...", ",", "ind", "+", "2", "]", "\n", "tensor1", "[", "...", ",", "ind", "+", "2", "]", "=", "tensor", "[", "...", ",", "ind", "+", "1", "]", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unexpected conversion value. Supported values are 'minmax2centroids', 'centroids2minmax', 'corners2centroids', 'centroids2corners', 'minmax2corners', and 'corners2minmax'.\"", ")", "\n", "\n", "", "return", "tensor1", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.convert_coordinates2": [[89, 118], ["numpy.copy().astype", "numpy.array", "numpy.dot", "numpy.copy", "numpy.array", "numpy.dot", "ValueError"], "function", ["None"], ["", "def", "convert_coordinates2", "(", "tensor", ",", "start_index", ",", "conversion", ")", ":", "\n", "    ", "'''\n    A matrix multiplication implementation of `convert_coordinates()`.\n    Supports only conversion between the 'centroids' and 'minmax' formats.\n\n    This function is marginally slower on average than `convert_coordinates()`,\n    probably because it involves more (unnecessary) arithmetic operations (unnecessary\n    because the two matrices are sparse).\n\n    For details please refer to the documentation of `convert_coordinates()`.\n    '''", "\n", "ind", "=", "start_index", "\n", "tensor1", "=", "np", ".", "copy", "(", "tensor", ")", ".", "astype", "(", "np", ".", "float", ")", "\n", "if", "conversion", "==", "'minmax2centroids'", ":", "\n", "        ", "M", "=", "np", ".", "array", "(", "[", "[", "0.5", ",", "0.", ",", "-", "1.", ",", "0.", "]", ",", "\n", "[", "0.5", ",", "0.", ",", "1.", ",", "0.", "]", ",", "\n", "[", "0.", ",", "0.5", ",", "0.", ",", "-", "1.", "]", ",", "\n", "[", "0.", ",", "0.5", ",", "0.", ",", "1.", "]", "]", ")", "\n", "tensor1", "[", "...", ",", "ind", ":", "ind", "+", "4", "]", "=", "np", ".", "dot", "(", "tensor1", "[", "...", ",", "ind", ":", "ind", "+", "4", "]", ",", "M", ")", "\n", "", "elif", "conversion", "==", "'centroids2minmax'", ":", "\n", "        ", "M", "=", "np", ".", "array", "(", "[", "[", "1.", ",", "1.", ",", "0.", ",", "0.", "]", ",", "\n", "[", "0.", ",", "0.", ",", "1.", ",", "1.", "]", ",", "\n", "[", "-", "0.5", ",", "0.5", ",", "0.", ",", "0.", "]", ",", "\n", "[", "0.", ",", "0.", ",", "-", "0.5", ",", "0.5", "]", "]", ")", "# The multiplicative inverse of the matrix above", "\n", "tensor1", "[", "...", ",", "ind", ":", "ind", "+", "4", "]", "=", "np", ".", "dot", "(", "tensor1", "[", "...", ",", "ind", ":", "ind", "+", "4", "]", ",", "M", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unexpected conversion value. Supported values are 'minmax2centroids' and 'centroids2minmax'.\"", ")", "\n", "\n", "", "return", "tensor1", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.intersection_area": [[119, 225], ["ValueError", "ValueError", "numpy.expand_dims", "numpy.expand_dims", "ValueError", "ValueError", "bounding_box_utils.convert_coordinates", "bounding_box_utils.convert_coordinates", "numpy.maximum", "numpy.minimum", "numpy.maximum", "format", "ValueError", "numpy.tile", "numpy.tile", "numpy.tile", "numpy.tile", "numpy.maximum", "numpy.minimum", "numpy.maximum", "numpy.expand_dims", "numpy.expand_dims", "numpy.expand_dims", "numpy.expand_dims"], "function", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.convert_coordinates", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.convert_coordinates"], ["", "def", "intersection_area", "(", "boxes1", ",", "boxes2", ",", "coords", "=", "'centroids'", ",", "mode", "=", "'outer_product'", ",", "border_pixels", "=", "'half'", ")", ":", "\n", "    ", "'''\n    Computes the intersection areas of two sets of axis-aligned 2D rectangular boxes.\n\n    Let `boxes1` and `boxes2` contain `m` and `n` boxes, respectively.\n\n    In 'outer_product' mode, returns an `(m,n)` matrix with the intersection areas for all possible\n    combinations of the boxes in `boxes1` and `boxes2`.\n\n    In 'element-wise' mode, `m` and `n` must be broadcast-compatible. Refer to the explanation\n    of the `mode` argument for details.\n\n    Arguments:\n        boxes1 (array): Either a 1D Numpy array of shape `(4, )` containing the coordinates for one box in the\n            format specified by `coords` or a 2D Numpy array of shape `(m, 4)` containing the coordinates for `m` boxes.\n            If `mode` is set to 'element_wise', the shape must be broadcast-compatible with `boxes2`.\n        boxes2 (array): Either a 1D Numpy array of shape `(4, )` containing the coordinates for one box in the\n            format specified by `coords` or a 2D Numpy array of shape `(n, 4)` containing the coordinates for `n` boxes.\n            If `mode` is set to 'element_wise', the shape must be broadcast-compatible with `boxes1`.\n        coords (str, optional): The coordinate format in the input arrays. Can be either 'centroids' for the format\n            `(cx, cy, w, h)`, 'minmax' for the format `(xmin, xmax, ymin, ymax)`, or 'corners' for the format\n            `(xmin, ymin, xmax, ymax)`.\n        mode (str, optional): Can be one of 'outer_product' and 'element-wise'. In 'outer_product' mode, returns an\n            `(m,n)` matrix with the intersection areas for all possible combinations of the `m` boxes in `boxes1` with the\n            `n` boxes in `boxes2`. In 'element-wise' mode, returns a 1D array and the shapes of `boxes1` and `boxes2`\n            must be boadcast-compatible. If both `boxes1` and `boxes2` have `m` boxes, then this returns an array of\n            length `m` where the i-th position contains the intersection area of `boxes1[i]` with `boxes2[i]`.\n        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n            to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n            If 'half', then one of each of the two horizontal and vertical borders belong\n            to the boxex, but not the other.\n\n    Returns:\n        A 1D or 2D Numpy array (refer to the `mode` argument for details) of dtype float containing values with\n        the intersection areas of the boxes in `boxes1` and `boxes2`.\n    '''", "\n", "\n", "# Make sure the boxes have the right shapes.", "\n", "if", "boxes1", ".", "ndim", ">", "2", ":", "raise", "ValueError", "(", "\"boxes1 must have rank either 1 or 2, but has rank {}.\"", ".", "format", "(", "boxes1", ".", "ndim", ")", ")", "\n", "if", "boxes2", ".", "ndim", ">", "2", ":", "raise", "ValueError", "(", "\"boxes2 must have rank either 1 or 2, but has rank {}.\"", ".", "format", "(", "boxes2", ".", "ndim", ")", ")", "\n", "\n", "if", "boxes1", ".", "ndim", "==", "1", ":", "boxes1", "=", "np", ".", "expand_dims", "(", "boxes1", ",", "axis", "=", "0", ")", "\n", "if", "boxes2", ".", "ndim", "==", "1", ":", "boxes2", "=", "np", ".", "expand_dims", "(", "boxes2", ",", "axis", "=", "0", ")", "\n", "\n", "if", "not", "(", "boxes1", ".", "shape", "[", "1", "]", "==", "boxes2", ".", "shape", "[", "1", "]", "==", "4", ")", ":", "raise", "ValueError", "(", "\"All boxes must consist of 4 coordinates, but the boxes in `boxes1` and `boxes2` have {} and {} coordinates, respectively.\"", ".", "format", "(", "boxes1", ".", "shape", "[", "1", "]", ",", "boxes2", ".", "shape", "[", "1", "]", ")", ")", "\n", "if", "not", "mode", "in", "{", "'outer_product'", ",", "'element-wise'", "}", ":", "raise", "ValueError", "(", "\"`mode` must be one of 'outer_product' and 'element-wise', but got '{}'.\"", ",", "format", "(", "mode", ")", ")", "\n", "\n", "# Convert the coordinates if necessary.", "\n", "if", "coords", "==", "'centroids'", ":", "\n", "        ", "boxes1", "=", "convert_coordinates", "(", "boxes1", ",", "start_index", "=", "0", ",", "conversion", "=", "'centroids2corners'", ")", "\n", "boxes2", "=", "convert_coordinates", "(", "boxes2", ",", "start_index", "=", "0", ",", "conversion", "=", "'centroids2corners'", ")", "\n", "coords", "=", "'corners'", "\n", "", "elif", "not", "(", "coords", "in", "{", "'minmax'", ",", "'corners'", "}", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unexpected value for `coords`. Supported values are 'minmax', 'corners' and 'centroids'.\"", ")", "\n", "\n", "", "m", "=", "boxes1", ".", "shape", "[", "0", "]", "# The number of boxes in `boxes1`", "\n", "n", "=", "boxes2", ".", "shape", "[", "0", "]", "# The number of boxes in `boxes2`", "\n", "\n", "# Set the correct coordinate indices for the respective formats.", "\n", "if", "coords", "==", "'corners'", ":", "\n", "        ", "xmin", "=", "0", "\n", "ymin", "=", "1", "\n", "xmax", "=", "2", "\n", "ymax", "=", "3", "\n", "", "elif", "coords", "==", "'minmax'", ":", "\n", "        ", "xmin", "=", "0", "\n", "xmax", "=", "1", "\n", "ymin", "=", "2", "\n", "ymax", "=", "3", "\n", "\n", "", "if", "border_pixels", "==", "'half'", ":", "\n", "        ", "d", "=", "0", "\n", "", "elif", "border_pixels", "==", "'include'", ":", "\n", "        ", "d", "=", "1", "# If border pixels are supposed to belong to the bounding boxes, we have to add one pixel to any difference `xmax - xmin` or `ymax - ymin`.", "\n", "", "elif", "border_pixels", "==", "'exclude'", ":", "\n", "        ", "d", "=", "-", "1", "# If border pixels are not supposed to belong to the bounding boxes, we have to subtract one pixel from any difference `xmax - xmin` or `ymax - ymin`.", "\n", "\n", "# Compute the intersection areas.", "\n", "\n", "", "if", "mode", "==", "'outer_product'", ":", "\n", "\n", "# For all possible box combinations, get the greater xmin and ymin values.", "\n", "# This is a tensor of shape (m,n,2).", "\n", "        ", "min_xy", "=", "np", ".", "maximum", "(", "np", ".", "tile", "(", "np", ".", "expand_dims", "(", "boxes1", "[", ":", ",", "[", "xmin", ",", "ymin", "]", "]", ",", "axis", "=", "1", ")", ",", "reps", "=", "(", "1", ",", "n", ",", "1", ")", ")", ",", "\n", "np", ".", "tile", "(", "np", ".", "expand_dims", "(", "boxes2", "[", ":", ",", "[", "xmin", ",", "ymin", "]", "]", ",", "axis", "=", "0", ")", ",", "reps", "=", "(", "m", ",", "1", ",", "1", ")", ")", ")", "\n", "\n", "# For all possible box combinations, get the smaller xmax and ymax values.", "\n", "# This is a tensor of shape (m,n,2).", "\n", "max_xy", "=", "np", ".", "minimum", "(", "np", ".", "tile", "(", "np", ".", "expand_dims", "(", "boxes1", "[", ":", ",", "[", "xmax", ",", "ymax", "]", "]", ",", "axis", "=", "1", ")", ",", "reps", "=", "(", "1", ",", "n", ",", "1", ")", ")", ",", "\n", "np", ".", "tile", "(", "np", ".", "expand_dims", "(", "boxes2", "[", ":", ",", "[", "xmax", ",", "ymax", "]", "]", ",", "axis", "=", "0", ")", ",", "reps", "=", "(", "m", ",", "1", ",", "1", ")", ")", ")", "\n", "\n", "# Compute the side lengths of the intersection rectangles.", "\n", "side_lengths", "=", "np", ".", "maximum", "(", "0", ",", "max_xy", "-", "min_xy", "+", "d", ")", "\n", "\n", "return", "side_lengths", "[", ":", ",", ":", ",", "0", "]", "*", "side_lengths", "[", ":", ",", ":", ",", "1", "]", "\n", "\n", "", "elif", "mode", "==", "'element-wise'", ":", "\n", "\n", "        ", "min_xy", "=", "np", ".", "maximum", "(", "boxes1", "[", ":", ",", "[", "xmin", ",", "ymin", "]", "]", ",", "boxes2", "[", ":", ",", "[", "xmin", ",", "ymin", "]", "]", ")", "\n", "max_xy", "=", "np", ".", "minimum", "(", "boxes1", "[", ":", ",", "[", "xmax", ",", "ymax", "]", "]", ",", "boxes2", "[", ":", ",", "[", "xmax", ",", "ymax", "]", "]", ")", "\n", "\n", "# Compute the side lengths of the intersection rectangles.", "\n", "side_lengths", "=", "np", ".", "maximum", "(", "0", ",", "max_xy", "-", "min_xy", "+", "d", ")", "\n", "\n", "return", "side_lengths", "[", ":", ",", "0", "]", "*", "side_lengths", "[", ":", ",", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.intersection_area_": [[226, 281], ["numpy.maximum", "numpy.minimum", "numpy.maximum", "numpy.tile", "numpy.tile", "numpy.tile", "numpy.tile", "numpy.maximum", "numpy.minimum", "numpy.maximum", "numpy.expand_dims", "numpy.expand_dims", "numpy.expand_dims", "numpy.expand_dims"], "function", ["None"], ["", "", "def", "intersection_area_", "(", "boxes1", ",", "boxes2", ",", "coords", "=", "'corners'", ",", "mode", "=", "'outer_product'", ",", "border_pixels", "=", "'half'", ")", ":", "\n", "    ", "'''\n    The same as 'intersection_area()' but for internal use, i.e. without all the safety checks.\n    '''", "\n", "\n", "m", "=", "boxes1", ".", "shape", "[", "0", "]", "# The number of boxes in `boxes1`", "\n", "n", "=", "boxes2", ".", "shape", "[", "0", "]", "# The number of boxes in `boxes2`", "\n", "\n", "# Set the correct coordinate indices for the respective formats.", "\n", "if", "coords", "==", "'corners'", ":", "\n", "        ", "xmin", "=", "0", "\n", "ymin", "=", "1", "\n", "xmax", "=", "2", "\n", "ymax", "=", "3", "\n", "", "elif", "coords", "==", "'minmax'", ":", "\n", "        ", "xmin", "=", "0", "\n", "xmax", "=", "1", "\n", "ymin", "=", "2", "\n", "ymax", "=", "3", "\n", "\n", "", "if", "border_pixels", "==", "'half'", ":", "\n", "        ", "d", "=", "0", "\n", "", "elif", "border_pixels", "==", "'include'", ":", "\n", "        ", "d", "=", "1", "# If border pixels are supposed to belong to the bounding boxes, we have to add one pixel to any difference `xmax - xmin` or `ymax - ymin`.", "\n", "", "elif", "border_pixels", "==", "'exclude'", ":", "\n", "        ", "d", "=", "-", "1", "# If border pixels are not supposed to belong to the bounding boxes, we have to subtract one pixel from any difference `xmax - xmin` or `ymax - ymin`.", "\n", "\n", "# Compute the intersection areas.", "\n", "\n", "", "if", "mode", "==", "'outer_product'", ":", "\n", "\n", "# For all possible box combinations, get the greater xmin and ymin values.", "\n", "# This is a tensor of shape (m,n,2).", "\n", "        ", "min_xy", "=", "np", ".", "maximum", "(", "np", ".", "tile", "(", "np", ".", "expand_dims", "(", "boxes1", "[", ":", ",", "[", "xmin", ",", "ymin", "]", "]", ",", "axis", "=", "1", ")", ",", "reps", "=", "(", "1", ",", "n", ",", "1", ")", ")", ",", "\n", "np", ".", "tile", "(", "np", ".", "expand_dims", "(", "boxes2", "[", ":", ",", "[", "xmin", ",", "ymin", "]", "]", ",", "axis", "=", "0", ")", ",", "reps", "=", "(", "m", ",", "1", ",", "1", ")", ")", ")", "\n", "\n", "# For all possible box combinations, get the smaller xmax and ymax values.", "\n", "# This is a tensor of shape (m,n,2).", "\n", "max_xy", "=", "np", ".", "minimum", "(", "np", ".", "tile", "(", "np", ".", "expand_dims", "(", "boxes1", "[", ":", ",", "[", "xmax", ",", "ymax", "]", "]", ",", "axis", "=", "1", ")", ",", "reps", "=", "(", "1", ",", "n", ",", "1", ")", ")", ",", "\n", "np", ".", "tile", "(", "np", ".", "expand_dims", "(", "boxes2", "[", ":", ",", "[", "xmax", ",", "ymax", "]", "]", ",", "axis", "=", "0", ")", ",", "reps", "=", "(", "m", ",", "1", ",", "1", ")", ")", ")", "\n", "\n", "# Compute the side lengths of the intersection rectangles.", "\n", "side_lengths", "=", "np", ".", "maximum", "(", "0", ",", "max_xy", "-", "min_xy", "+", "d", ")", "\n", "\n", "return", "side_lengths", "[", ":", ",", ":", ",", "0", "]", "*", "side_lengths", "[", ":", ",", ":", ",", "1", "]", "\n", "\n", "", "elif", "mode", "==", "'element-wise'", ":", "\n", "\n", "        ", "min_xy", "=", "np", ".", "maximum", "(", "boxes1", "[", ":", ",", "[", "xmin", ",", "ymin", "]", "]", ",", "boxes2", "[", ":", ",", "[", "xmin", ",", "ymin", "]", "]", ")", "\n", "max_xy", "=", "np", ".", "minimum", "(", "boxes1", "[", ":", ",", "[", "xmax", ",", "ymax", "]", "]", ",", "boxes2", "[", ":", ",", "[", "xmax", ",", "ymax", "]", "]", ")", "\n", "\n", "# Compute the side lengths of the intersection rectangles.", "\n", "side_lengths", "=", "np", ".", "maximum", "(", "0", ",", "max_xy", "-", "min_xy", "+", "d", ")", "\n", "\n", "return", "side_lengths", "[", ":", ",", "0", "]", "*", "side_lengths", "[", ":", ",", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.iou": [[283, 384], ["bounding_box_utils.intersection_area_", "ValueError", "ValueError", "numpy.expand_dims", "numpy.expand_dims", "ValueError", "ValueError", "bounding_box_utils.convert_coordinates", "bounding_box_utils.convert_coordinates", "numpy.tile", "numpy.tile", "ValueError", "numpy.expand_dims", "numpy.expand_dims"], "function", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.intersection_area_", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.convert_coordinates", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.convert_coordinates"], ["", "", "def", "iou", "(", "boxes1", ",", "boxes2", ",", "coords", "=", "'centroids'", ",", "mode", "=", "'outer_product'", ",", "border_pixels", "=", "'half'", ")", ":", "\n", "    ", "'''\n    Computes the intersection-over-union similarity (also known as Jaccard similarity)\n    of two sets of axis-aligned 2D rectangular boxes.\n\n    Let `boxes1` and `boxes2` contain `m` and `n` boxes, respectively.\n\n    In 'outer_product' mode, returns an `(m,n)` matrix with the IoUs for all possible\n    combinations of the boxes in `boxes1` and `boxes2`.\n\n    In 'element-wise' mode, `m` and `n` must be broadcast-compatible. Refer to the explanation\n    of the `mode` argument for details.\n\n    Arguments:\n        boxes1 (array): Either a 1D Numpy array of shape `(4, )` containing the coordinates for one box in the\n            format specified by `coords` or a 2D Numpy array of shape `(m, 4)` containing the coordinates for `m` boxes.\n            If `mode` is set to 'element_wise', the shape must be broadcast-compatible with `boxes2`.\n        boxes2 (array): Either a 1D Numpy array of shape `(4, )` containing the coordinates for one box in the\n            format specified by `coords` or a 2D Numpy array of shape `(n, 4)` containing the coordinates for `n` boxes.\n            If `mode` is set to 'element_wise', the shape must be broadcast-compatible with `boxes1`.\n        coords (str, optional): The coordinate format in the input arrays. Can be either 'centroids' for the format\n            `(cx, cy, w, h)`, 'minmax' for the format `(xmin, xmax, ymin, ymax)`, or 'corners' for the format\n            `(xmin, ymin, xmax, ymax)`.\n        mode (str, optional): Can be one of 'outer_product' and 'element-wise'. In 'outer_product' mode, returns an\n            `(m,n)` matrix with the IoU overlaps for all possible combinations of the `m` boxes in `boxes1` with the\n            `n` boxes in `boxes2`. In 'element-wise' mode, returns a 1D array and the shapes of `boxes1` and `boxes2`\n            must be boadcast-compatible. If both `boxes1` and `boxes2` have `m` boxes, then this returns an array of\n            length `m` where the i-th position contains the IoU overlap of `boxes1[i]` with `boxes2[i]`.\n        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n            to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n            If 'half', then one of each of the two horizontal and vertical borders belong\n            to the boxex, but not the other.\n\n    Returns:\n        A 1D or 2D Numpy array (refer to the `mode` argument for details) of dtype float containing values in [0,1],\n        the Jaccard similarity of the boxes in `boxes1` and `boxes2`. 0 means there is no overlap between two given\n        boxes, 1 means their coordinates are identical.\n    '''", "\n", "\n", "# Make sure the boxes have the right shapes.", "\n", "if", "boxes1", ".", "ndim", ">", "2", ":", "raise", "ValueError", "(", "\"boxes1 must have rank either 1 or 2, but has rank {}.\"", ".", "format", "(", "boxes1", ".", "ndim", ")", ")", "\n", "if", "boxes2", ".", "ndim", ">", "2", ":", "raise", "ValueError", "(", "\"boxes2 must have rank either 1 or 2, but has rank {}.\"", ".", "format", "(", "boxes2", ".", "ndim", ")", ")", "\n", "\n", "if", "boxes1", ".", "ndim", "==", "1", ":", "boxes1", "=", "np", ".", "expand_dims", "(", "boxes1", ",", "axis", "=", "0", ")", "\n", "if", "boxes2", ".", "ndim", "==", "1", ":", "boxes2", "=", "np", ".", "expand_dims", "(", "boxes2", ",", "axis", "=", "0", ")", "\n", "\n", "if", "not", "(", "boxes1", ".", "shape", "[", "1", "]", "==", "boxes2", ".", "shape", "[", "1", "]", "==", "4", ")", ":", "raise", "ValueError", "(", "\"All boxes must consist of 4 coordinates, but the boxes in `boxes1` and `boxes2` have {} and {} coordinates, respectively.\"", ".", "format", "(", "boxes1", ".", "shape", "[", "1", "]", ",", "boxes2", ".", "shape", "[", "1", "]", ")", ")", "\n", "if", "not", "mode", "in", "{", "'outer_product'", ",", "'element-wise'", "}", ":", "raise", "ValueError", "(", "\"`mode` must be one of 'outer_product' and 'element-wise', but got '{}'.\"", ".", "format", "(", "mode", ")", ")", "\n", "\n", "# Convert the coordinates if necessary.", "\n", "if", "coords", "==", "'centroids'", ":", "\n", "        ", "boxes1", "=", "convert_coordinates", "(", "boxes1", ",", "start_index", "=", "0", ",", "conversion", "=", "'centroids2corners'", ")", "\n", "boxes2", "=", "convert_coordinates", "(", "boxes2", ",", "start_index", "=", "0", ",", "conversion", "=", "'centroids2corners'", ")", "\n", "coords", "=", "'corners'", "\n", "", "elif", "not", "(", "coords", "in", "{", "'minmax'", ",", "'corners'", "}", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unexpected value for `coords`. Supported values are 'minmax', 'corners' and 'centroids'.\"", ")", "\n", "\n", "# Compute the IoU.", "\n", "\n", "# Compute the interesection areas.", "\n", "\n", "", "intersection_areas", "=", "intersection_area_", "(", "boxes1", ",", "boxes2", ",", "coords", "=", "coords", ",", "mode", "=", "mode", ")", "\n", "\n", "m", "=", "boxes1", ".", "shape", "[", "0", "]", "# The number of boxes in `boxes1`", "\n", "n", "=", "boxes2", ".", "shape", "[", "0", "]", "# The number of boxes in `boxes2`", "\n", "\n", "# Compute the union areas.", "\n", "\n", "# Set the correct coordinate indices for the respective formats.", "\n", "if", "coords", "==", "'corners'", ":", "\n", "        ", "xmin", "=", "0", "\n", "ymin", "=", "1", "\n", "xmax", "=", "2", "\n", "ymax", "=", "3", "\n", "", "elif", "coords", "==", "'minmax'", ":", "\n", "        ", "xmin", "=", "0", "\n", "xmax", "=", "1", "\n", "ymin", "=", "2", "\n", "ymax", "=", "3", "\n", "\n", "", "if", "border_pixels", "==", "'half'", ":", "\n", "        ", "d", "=", "0", "\n", "", "elif", "border_pixels", "==", "'include'", ":", "\n", "        ", "d", "=", "1", "# If border pixels are supposed to belong to the bounding boxes, we have to add one pixel to any difference `xmax - xmin` or `ymax - ymin`.", "\n", "", "elif", "border_pixels", "==", "'exclude'", ":", "\n", "        ", "d", "=", "-", "1", "# If border pixels are not supposed to belong to the bounding boxes, we have to subtract one pixel from any difference `xmax - xmin` or `ymax - ymin`.", "\n", "\n", "", "if", "mode", "==", "'outer_product'", ":", "\n", "\n", "        ", "boxes1_areas", "=", "np", ".", "tile", "(", "np", ".", "expand_dims", "(", "(", "boxes1", "[", ":", ",", "xmax", "]", "-", "boxes1", "[", ":", ",", "xmin", "]", "+", "d", ")", "*", "(", "boxes1", "[", ":", ",", "ymax", "]", "-", "boxes1", "[", ":", ",", "ymin", "]", "+", "d", ")", ",", "axis", "=", "1", ")", ",", "reps", "=", "(", "1", ",", "n", ")", ")", "\n", "boxes2_areas", "=", "np", ".", "tile", "(", "np", ".", "expand_dims", "(", "(", "boxes2", "[", ":", ",", "xmax", "]", "-", "boxes2", "[", ":", ",", "xmin", "]", "+", "d", ")", "*", "(", "boxes2", "[", ":", ",", "ymax", "]", "-", "boxes2", "[", ":", ",", "ymin", "]", "+", "d", ")", ",", "axis", "=", "0", ")", ",", "reps", "=", "(", "m", ",", "1", ")", ")", "\n", "\n", "", "elif", "mode", "==", "'element-wise'", ":", "\n", "\n", "        ", "boxes1_areas", "=", "(", "boxes1", "[", ":", ",", "xmax", "]", "-", "boxes1", "[", ":", ",", "xmin", "]", "+", "d", ")", "*", "(", "boxes1", "[", ":", ",", "ymax", "]", "-", "boxes1", "[", ":", ",", "ymin", "]", "+", "d", ")", "\n", "boxes2_areas", "=", "(", "boxes2", "[", ":", ",", "xmax", "]", "-", "boxes2", "[", ":", ",", "xmin", "]", "+", "d", ")", "*", "(", "boxes2", "[", ":", ",", "ymax", "]", "-", "boxes2", "[", ":", ",", "ymin", "]", "+", "d", ")", "\n", "\n", "", "union_areas", "=", "boxes1_areas", "+", "boxes2_areas", "-", "intersection_areas", "\n", "\n", "return", "intersection_areas", "/", "union_areas", "\n", "", ""]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.models.keras_ssd512.ssd_512": [[33, 290], ["numpy.array", "numpy.any", "keras.layers.Input", "keras.layers.concatenate", "keras.layers.concatenate", "keras.layers.concatenate", "keras.layers.concatenate", "ValueError", "ValueError", "numpy.linspace", "len", "ValueError", "ValueError", "ValueError", "ValueError", "keras.layers.Lambda", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.MaxPooling2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.MaxPooling2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.MaxPooling2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.MaxPooling2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.MaxPooling2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras_layers.keras_layer_AnchorBoxes.AnchorBoxes", "keras_layers.keras_layer_AnchorBoxes.AnchorBoxes", "keras_layers.keras_layer_AnchorBoxes.AnchorBoxes", "keras.layers.Reshape", "keras.layers.Reshape", "keras.layers.Reshape", "keras.layers.Reshape", "keras.layers.Reshape", "keras.layers.Reshape", "keras.layers.Reshape", "keras.layers.Reshape", "keras.layers.Reshape", "keras.layers.Concatenate", "keras.layers.Concatenate", "keras.layers.Concatenate", "keras.layers.Activation", "keras.layers.Concatenate", "keras.models.Model", "numpy.array", "len", "ValueError", "len", "ValueError", "len", "len", "len", "numpy.array", "numpy.array", "len", "keras.stack", "keras.layers.Lambda", "keras.layers.Lambda", "keras.layers.Lambda", "keras.models.Model", "len", "len.append", "len.append", "len", "len", "keras.stack", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras_layers.keras_layer_DecodeDetections.DecodeDetections", "keras.models.Model", "ValueError", "len", "len", "len", "keras_layers.keras_layer_DecodeDetectionsFast.DecodeDetectionsFast", "len"], "function", ["None"], ["def", "ssd_512", "(", "image_size", ",", "\n", "n_classes", ",", "\n", "mode", "=", "'training'", ",", "\n", "l2_regularization", "=", "0.0005", ",", "\n", "min_scale", "=", "None", ",", "\n", "max_scale", "=", "None", ",", "\n", "scales", "=", "None", ",", "\n", "aspect_ratios_global", "=", "None", ",", "\n", "aspect_ratios_per_layer", "=", "[", "[", "1.0", ",", "2.0", ",", "0.5", ",", "3.0", ",", "1.0", "/", "3.0", "]", ",", "\n", "[", "1.0", ",", "2.0", ",", "0.5", ",", "3.0", ",", "1.0", "/", "3.0", "]", ",", "\n", "[", "1.0", ",", "2.0", ",", "0.5", ",", "3.0", ",", "1.0", "/", "3.0", "]", "]", ",", "\n", "two_boxes_for_ar1", "=", "True", ",", "\n", "steps", "=", "[", "8", ",", "4", ",", "2", "]", ",", "\n", "offsets", "=", "None", ",", "\n", "clip_boxes", "=", "False", ",", "\n", "variances", "=", "[", "0.1", ",", "0.1", ",", "0.2", ",", "0.2", "]", ",", "\n", "coords", "=", "'centroids'", ",", "\n", "normalize_coords", "=", "True", ",", "\n", "subtract_mean", "=", "[", "123", ",", "117", ",", "104", "]", ",", "\n", "divide_by_stddev", "=", "None", ",", "\n", "swap_channels", "=", "[", "2", ",", "1", ",", "0", "]", ",", "\n", "confidence_thresh", "=", "0.01", ",", "\n", "iou_threshold", "=", "0.45", ",", "\n", "top_k", "=", "200", ",", "\n", "nms_max_output_size", "=", "400", ",", "\n", "return_predictor_sizes", "=", "False", ")", ":", "\n", "\n", "\n", "    ", "n_predictor_layers", "=", "3", "# The number of predictor conv layers in the network is 7 for the original SSD512", "\n", "n_classes", "+=", "1", "# Account for the background class.", "\n", "l2_reg", "=", "l2_regularization", "# Make the internal name shorter.", "\n", "img_height", ",", "img_width", ",", "img_channels", "=", "image_size", "[", "0", "]", ",", "image_size", "[", "1", "]", ",", "image_size", "[", "2", "]", "\n", "\n", "if", "aspect_ratios_global", "is", "None", "and", "aspect_ratios_per_layer", "is", "None", ":", "\n", "        ", "raise", "ValueError", "(", "\"`aspect_ratios_global` and `aspect_ratios_per_layer` cannot both be None. At least one needs to be specified.\"", ")", "\n", "", "if", "aspect_ratios_per_layer", ":", "\n", "        ", "if", "len", "(", "aspect_ratios_per_layer", ")", "!=", "n_predictor_layers", ":", "\n", "            ", "raise", "ValueError", "(", "\"It must be either aspect_ratios_per_layer is None or len(aspect_ratios_per_layer) == {}, but len(aspect_ratios_per_layer) == {}.\"", ".", "format", "(", "n_predictor_layers", ",", "len", "(", "aspect_ratios_per_layer", ")", ")", ")", "\n", "\n", "", "", "if", "(", "min_scale", "is", "None", "or", "max_scale", "is", "None", ")", "and", "scales", "is", "None", ":", "\n", "        ", "raise", "ValueError", "(", "\"Either `min_scale` and `max_scale` or `scales` need to be specified.\"", ")", "\n", "", "if", "scales", ":", "\n", "        ", "if", "len", "(", "scales", ")", "!=", "n_predictor_layers", "+", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"It must be either scales is None or len(scales) == {}, but len(scales) == {}.\"", ".", "format", "(", "n_predictor_layers", "+", "1", ",", "len", "(", "scales", ")", ")", ")", "\n", "", "", "else", ":", "# If no explicit list of scaling factors was passed, compute the list of scaling factors from `min_scale` and `max_scale`", "\n", "        ", "scales", "=", "np", ".", "linspace", "(", "min_scale", ",", "max_scale", ",", "n_predictor_layers", "+", "1", ")", "\n", "\n", "", "if", "len", "(", "variances", ")", "!=", "4", ":", "\n", "        ", "raise", "ValueError", "(", "\"4 variance values must be pased, but {} values were received.\"", ".", "format", "(", "len", "(", "variances", ")", ")", ")", "\n", "", "variances", "=", "np", ".", "array", "(", "variances", ")", "\n", "if", "np", ".", "any", "(", "variances", "<=", "0", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"All variances must be >0, but the variances given are {}\"", ".", "format", "(", "variances", ")", ")", "\n", "\n", "", "if", "(", "not", "(", "steps", "is", "None", ")", ")", "and", "(", "len", "(", "steps", ")", "!=", "n_predictor_layers", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"You must provide at least one step value per predictor layer.\"", ")", "\n", "\n", "", "if", "(", "not", "(", "offsets", "is", "None", ")", ")", "and", "(", "len", "(", "offsets", ")", "!=", "n_predictor_layers", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"You must provide at least one offset value per predictor layer.\"", ")", "\n", "\n", "# Set the aspect ratios for each predictor layer. These are only needed for the anchor box layers.", "\n", "", "if", "aspect_ratios_per_layer", ":", "\n", "        ", "aspect_ratios", "=", "aspect_ratios_per_layer", "\n", "", "else", ":", "\n", "        ", "aspect_ratios", "=", "[", "aspect_ratios_global", "]", "*", "n_predictor_layers", "\n", "\n", "# Compute the number of boxes to be predicted per cell for each predictor layer.", "\n", "# We need this so that we know how many channels the predictor layers need to have.", "\n", "", "if", "aspect_ratios_per_layer", ":", "\n", "        ", "n_boxes", "=", "[", "]", "\n", "for", "ar", "in", "aspect_ratios_per_layer", ":", "\n", "            ", "if", "(", "1", "in", "ar", ")", "&", "two_boxes_for_ar1", ":", "\n", "                ", "n_boxes", ".", "append", "(", "len", "(", "ar", ")", "+", "1", ")", "# +1 for the second box for aspect ratio 1", "\n", "", "else", ":", "\n", "                ", "n_boxes", ".", "append", "(", "len", "(", "ar", ")", ")", "\n", "", "", "", "else", ":", "# If only a global aspect ratio list was passed, then the number of boxes is the same for each predictor layer", "\n", "        ", "if", "(", "1", "in", "aspect_ratios_global", ")", "&", "two_boxes_for_ar1", ":", "\n", "            ", "n_boxes", "=", "len", "(", "aspect_ratios_global", ")", "+", "1", "\n", "", "else", ":", "\n", "            ", "n_boxes", "=", "len", "(", "aspect_ratios_global", ")", "\n", "", "n_boxes", "=", "[", "n_boxes", "]", "*", "n_predictor_layers", "\n", "\n", "", "if", "steps", "is", "None", ":", "\n", "        ", "steps", "=", "[", "None", "]", "*", "n_predictor_layers", "\n", "", "if", "offsets", "is", "None", ":", "\n", "        ", "offsets", "=", "[", "None", "]", "*", "n_predictor_layers", "\n", "\n", "", "def", "identity_layer", "(", "tensor", ")", ":", "\n", "        ", "return", "tensor", "\n", "\n", "", "def", "input_mean_normalization", "(", "tensor", ")", ":", "\n", "        ", "return", "tensor", "-", "np", ".", "array", "(", "subtract_mean", ")", "\n", "\n", "", "def", "input_stddev_normalization", "(", "tensor", ")", ":", "\n", "        ", "return", "tensor", "/", "np", ".", "array", "(", "divide_by_stddev", ")", "\n", "\n", "", "def", "input_channel_swap", "(", "tensor", ")", ":", "\n", "        ", "if", "len", "(", "swap_channels", ")", "==", "3", ":", "\n", "            ", "return", "K", ".", "stack", "(", "[", "tensor", "[", "...", ",", "swap_channels", "[", "0", "]", "]", ",", "tensor", "[", "...", ",", "swap_channels", "[", "1", "]", "]", ",", "tensor", "[", "...", ",", "swap_channels", "[", "2", "]", "]", "]", ",", "axis", "=", "-", "1", ")", "\n", "", "elif", "len", "(", "swap_channels", ")", "==", "4", ":", "\n", "            ", "return", "K", ".", "stack", "(", "[", "tensor", "[", "...", ",", "swap_channels", "[", "0", "]", "]", ",", "tensor", "[", "...", ",", "swap_channels", "[", "1", "]", "]", ",", "tensor", "[", "...", ",", "swap_channels", "[", "2", "]", "]", ",", "tensor", "[", "...", ",", "swap_channels", "[", "3", "]", "]", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "", "", "x", "=", "Input", "(", "shape", "=", "(", "img_height", ",", "img_width", ",", "img_channels", ")", ")", "\n", "\n", "# The following identity layer is only needed so that the subsequent lambda layers can be optional.", "\n", "x1", "=", "Lambda", "(", "identity_layer", ",", "output_shape", "=", "(", "img_height", ",", "img_width", ",", "img_channels", ")", ",", "name", "=", "'identity_layer'", ")", "(", "x", ")", "\n", "if", "not", "(", "subtract_mean", "is", "None", ")", ":", "\n", "        ", "x1", "=", "Lambda", "(", "input_mean_normalization", ",", "output_shape", "=", "(", "img_height", ",", "img_width", ",", "img_channels", ")", ",", "name", "=", "'input_mean_normalization'", ")", "(", "x1", ")", "\n", "", "if", "not", "(", "divide_by_stddev", "is", "None", ")", ":", "\n", "        ", "x1", "=", "Lambda", "(", "input_stddev_normalization", ",", "output_shape", "=", "(", "img_height", ",", "img_width", ",", "img_channels", ")", ",", "name", "=", "'input_stddev_normalization'", ")", "(", "x1", ")", "\n", "", "if", "swap_channels", ":", "\n", "        ", "x1", "=", "Lambda", "(", "input_channel_swap", ",", "output_shape", "=", "(", "img_height", ",", "img_width", ",", "img_channels", ")", ",", "name", "=", "'input_channel_swap'", ")", "(", "x1", ")", "\n", "# 512  step=1", "\n", "", "conv1_1", "=", "Conv2D", "(", "64", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv1_1'", ")", "(", "x1", ")", "\n", "conv1_2", "=", "Conv2D", "(", "64", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv1_2'", ")", "(", "conv1_1", ")", "\n", "pool1", "=", "MaxPooling2D", "(", "pool_size", "=", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "padding", "=", "'same'", ",", "name", "=", "'pool1'", ")", "(", "conv1_2", ")", "\n", "# 256  step=2", "\n", "conv2_1", "=", "Conv2D", "(", "128", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv2_1'", ")", "(", "pool1", ")", "\n", "conv2_2", "=", "Conv2D", "(", "128", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv2_2'", ")", "(", "conv2_1", ")", "\n", "pool2", "=", "MaxPooling2D", "(", "pool_size", "=", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "padding", "=", "'same'", ",", "name", "=", "'pool2'", ")", "(", "conv2_2", ")", "\n", "# 128  step=4", "\n", "conv3_1", "=", "Conv2D", "(", "256", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv3_1'", ")", "(", "pool2", ")", "\n", "conv3_2", "=", "Conv2D", "(", "256", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv3_2'", ")", "(", "conv3_1", ")", "\n", "conv3_3", "=", "Conv2D", "(", "256", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv3_3'", ")", "(", "conv3_2", ")", "\n", "pool3", "=", "MaxPooling2D", "(", "pool_size", "=", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "padding", "=", "'same'", ",", "name", "=", "'pool3'", ")", "(", "conv3_3", ")", "\n", "# 64  step=8", "\n", "conv4_1", "=", "Conv2D", "(", "512", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv4_1'", ")", "(", "pool3", ")", "\n", "conv4_2", "=", "Conv2D", "(", "512", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv4_2'", ")", "(", "conv4_1", ")", "\n", "conv4_3", "=", "Conv2D", "(", "512", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv4_3'", ")", "(", "conv4_2", ")", "\n", "pool4", "=", "MaxPooling2D", "(", "pool_size", "=", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "padding", "=", "'same'", ",", "name", "=", "'pool4'", ")", "(", "conv4_3", ")", "\n", "# 32  step=16", "\n", "conv5_1", "=", "Conv2D", "(", "512", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv5_1'", ")", "(", "pool4", ")", "\n", "conv5_2", "=", "Conv2D", "(", "512", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv5_2'", ")", "(", "conv5_1", ")", "\n", "conv5_3", "=", "Conv2D", "(", "512", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv5_3'", ")", "(", "conv5_2", ")", "\n", "# 16  step=32", "\n", "pool5", "=", "MaxPooling2D", "(", "pool_size", "=", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "padding", "=", "'same'", ",", "name", "=", "'pool5'", ")", "(", "conv5_3", ")", "\n", "# 16  step=32", "\n", "dilateconv6_1", "=", "Conv2D", "(", "512", ",", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ")", ",", "dilation_rate", "=", "2", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'dilateconv6_1'", ")", "(", "pool5", ")", "\n", "dilateconv6_2", "=", "Conv2D", "(", "512", ",", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ")", ",", "dilation_rate", "=", "2", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'dilateconv6_2'", ")", "(", "dilateconv6_1", ")", "\n", "dilateconv6_3", "=", "Conv2D", "(", "512", ",", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ")", ",", "dilation_rate", "=", "2", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'dilateconv6_3'", ")", "(", "dilateconv6_2", ")", "\n", "dilateconv6_4", "=", "Conv2D", "(", "512", ",", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ")", ",", "dilation_rate", "=", "2", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'dilateconv6_4'", ")", "(", "dilateconv6_3", ")", "\n", "conv7_add", "=", "concatenate", "(", "[", "pool5", ",", "dilateconv6_4", "]", ",", "axis", "=", "3", ")", "\n", "# 32  step=16", "\n", "deconv1_1", "=", "Conv2DTranspose", "(", "512", ",", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'deconv1_1'", ")", "(", "conv7_add", ")", "\n", "deconv1_2", "=", "Conv2DTranspose", "(", "512", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'deconv1_2'", ")", "(", "deconv1_1", ")", "\n", "deconv1", "=", "concatenate", "(", "[", "deconv1_2", ",", "conv5_3", "]", ",", "axis", "=", "3", ")", "\n", "# 64  step=8", "\n", "deconv2_1", "=", "Conv2DTranspose", "(", "512", ",", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'deconv2_1'", ")", "(", "deconv1", ")", "\n", "deconv2_2", "=", "Conv2DTranspose", "(", "512", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'deconv2_2'", ")", "(", "deconv2_1", ")", "\n", "deconv2", "=", "concatenate", "(", "[", "deconv2_2", ",", "conv4_3", "]", ",", "axis", "=", "3", ")", "\n", "# 128  step=4", "\n", "deconv3_1", "=", "Conv2DTranspose", "(", "512", ",", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'deconv3_1'", ")", "(", "deconv2", ")", "\n", "deconv3_2", "=", "Conv2DTranspose", "(", "256", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'deconv3_2'", ")", "(", "deconv3_1", ")", "\n", "deconv3", "=", "concatenate", "(", "[", "deconv3_2", ",", "conv3_3", "]", ",", "axis", "=", "3", ")", "\n", "\n", "# Feed conv4_3 into the L2 normalization layer", "\n", "# conv4_3_norm = L2Normalization(gamma_init=20, name='conv4_3_norm')(conv4_3)", "\n", "# We precidt `n_classes` confidence values for each box, hence the confidence predictors have depth `n_boxes * n_classes`", "\n", "# Output shape of the confidence layers: `(batch, height, width, n_boxes * n_classes)`", "\n", "deconv1_mbox_conf", "=", "Conv2D", "(", "n_boxes", "[", "0", "]", "*", "n_classes", ",", "(", "3", ",", "3", ")", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'deconv1_mbox_conf'", ")", "(", "deconv1", ")", "\n", "deconv2_mbox_conf", "=", "Conv2D", "(", "n_boxes", "[", "1", "]", "*", "n_classes", ",", "(", "3", ",", "3", ")", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'deconv2_mbox_conf'", ")", "(", "deconv2", ")", "\n", "deconv3_mbox_conf", "=", "Conv2D", "(", "n_boxes", "[", "2", "]", "*", "n_classes", ",", "(", "3", ",", "3", ")", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'deconv3_mbox_conf'", ")", "(", "deconv3", ")", "\n", "\n", "# We predict 4 box coordinates for each box, hence the localization predictors have depth `n_boxes * 4`", "\n", "# Output shape of the localization layers: `(batch, height, width, n_boxes * 4)`", "\n", "deconv1_mbox_loc", "=", "Conv2D", "(", "n_boxes", "[", "0", "]", "*", "4", ",", "(", "3", ",", "3", ")", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'deconv1_mbox_loc'", ")", "(", "deconv1", ")", "\n", "deconv2_mbox_loc", "=", "Conv2D", "(", "n_boxes", "[", "1", "]", "*", "4", ",", "(", "3", ",", "3", ")", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'deconv2_mbox_loc'", ")", "(", "deconv2", ")", "\n", "deconv3_mbox_loc", "=", "Conv2D", "(", "n_boxes", "[", "2", "]", "*", "4", ",", "(", "3", ",", "3", ")", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'deconv3_mbox_loc'", ")", "(", "deconv3", ")", "\n", "\n", "# Generate the anchor boxes (called \"priors\" in the original Caffe/C++ implementation, so I'll keep their layer names)", "\n", "# Output shape of anchors: `(batch, height, width, n_boxes, 8)`", "\n", "deconv1_mbox_priorbox", "=", "AnchorBoxes", "(", "img_height", ",", "img_width", ",", "this_scale", "=", "scales", "[", "0", "]", ",", "next_scale", "=", "scales", "[", "1", "]", ",", "aspect_ratios", "=", "aspect_ratios", "[", "0", "]", ",", "\n", "two_boxes_for_ar1", "=", "two_boxes_for_ar1", ",", "this_steps", "=", "steps", "[", "0", "]", ",", "this_offsets", "=", "offsets", "[", "0", "]", ",", "clip_boxes", "=", "clip_boxes", ",", "\n", "variances", "=", "variances", ",", "coords", "=", "coords", ",", "normalize_coords", "=", "normalize_coords", ",", "name", "=", "'deconv1_mbox_priorbox'", ")", "(", "deconv1_mbox_loc", ")", "\n", "deconv2_mbox_priorbox", "=", "AnchorBoxes", "(", "img_height", ",", "img_width", ",", "this_scale", "=", "scales", "[", "1", "]", ",", "next_scale", "=", "scales", "[", "2", "]", ",", "aspect_ratios", "=", "aspect_ratios", "[", "1", "]", ",", "\n", "two_boxes_for_ar1", "=", "two_boxes_for_ar1", ",", "this_steps", "=", "steps", "[", "1", "]", ",", "this_offsets", "=", "offsets", "[", "1", "]", ",", "clip_boxes", "=", "clip_boxes", ",", "\n", "variances", "=", "variances", ",", "coords", "=", "coords", ",", "normalize_coords", "=", "normalize_coords", ",", "name", "=", "'deconv2_mbox_priorbox'", ")", "(", "deconv2_mbox_loc", ")", "\n", "deconv3_mbox_priorbox", "=", "AnchorBoxes", "(", "img_height", ",", "img_width", ",", "this_scale", "=", "scales", "[", "2", "]", ",", "next_scale", "=", "scales", "[", "3", "]", ",", "aspect_ratios", "=", "aspect_ratios", "[", "2", "]", ",", "\n", "two_boxes_for_ar1", "=", "two_boxes_for_ar1", ",", "this_steps", "=", "steps", "[", "2", "]", ",", "this_offsets", "=", "offsets", "[", "2", "]", ",", "clip_boxes", "=", "clip_boxes", ",", "\n", "variances", "=", "variances", ",", "coords", "=", "coords", ",", "normalize_coords", "=", "normalize_coords", ",", "name", "=", "'deconv3_mbox_priorbox'", ")", "(", "deconv3_mbox_loc", ")", "\n", "\n", "# Reshape the class predictions, yielding 3D tensors of shape `(batch, height * width * n_boxes, n_classes)`", "\n", "# We want the classes isolated in the last axis to perform softmax on them", "\n", "deconv1_mbox_conf_reshape", "=", "Reshape", "(", "(", "-", "1", ",", "n_classes", ")", ",", "name", "=", "'deconv1_mbox_conf_reshape'", ")", "(", "deconv1_mbox_conf", ")", "\n", "deconv2_mbox_conf_reshape", "=", "Reshape", "(", "(", "-", "1", ",", "n_classes", ")", ",", "name", "=", "'deconv2_mbox_conf_reshape'", ")", "(", "deconv2_mbox_conf", ")", "\n", "deconv3_mbox_conf_reshape", "=", "Reshape", "(", "(", "-", "1", ",", "n_classes", ")", ",", "name", "=", "'deconv3_mbox_conf_reshape'", ")", "(", "deconv3_mbox_conf", ")", "\n", "\n", "# Reshape the box predictions, yielding 3D tensors of shape `(batch, height * width * n_boxes, 4)`", "\n", "# We want the four box coordinates isolated in the last axis to compute the smooth L1 loss", "\n", "deconv1_mbox_loc_reshape", "=", "Reshape", "(", "(", "-", "1", ",", "4", ")", ",", "name", "=", "'deconv1_mbox_loc_reshape'", ")", "(", "deconv1_mbox_loc", ")", "\n", "deconv2_mbox_loc_reshape", "=", "Reshape", "(", "(", "-", "1", ",", "4", ")", ",", "name", "=", "'deconv2_mbox_loc_reshape'", ")", "(", "deconv2_mbox_loc", ")", "\n", "deconv3_mbox_loc_reshape", "=", "Reshape", "(", "(", "-", "1", ",", "4", ")", ",", "name", "=", "'deconv3_mbox_loc_reshape'", ")", "(", "deconv3_mbox_loc", ")", "\n", "\n", "# Reshape the anchor box tensors, yielding 3D tensors of shape `(batch, height * width * n_boxes, 8)`", "\n", "deconv1_mbox_priorbox_reshape", "=", "Reshape", "(", "(", "-", "1", ",", "8", ")", ",", "name", "=", "'deconv1_mbox_priorbox_reshape'", ")", "(", "deconv1_mbox_priorbox", ")", "\n", "deconv2_mbox_priorbox_reshape", "=", "Reshape", "(", "(", "-", "1", ",", "8", ")", ",", "name", "=", "'deconv2_mbox_priorbox_reshape'", ")", "(", "deconv2_mbox_priorbox", ")", "\n", "deconv3_mbox_priorbox_reshape", "=", "Reshape", "(", "(", "-", "1", ",", "8", ")", ",", "name", "=", "'deconv3_mbox_priorbox_reshape'", ")", "(", "deconv3_mbox_priorbox", ")", "\n", "\n", "# Concatenate the predictions from the different layers", "\n", "# Axis 0 (batch) and axis 2 (n_classes or 4, respectively) are identical for all layer predictions,", "\n", "# so we want to concatenate along axis 1, the number of boxes per layer", "\n", "# Output shape of `mbox_conf`: (batch, n_boxes_total, n_classes)", "\n", "mbox_conf", "=", "Concatenate", "(", "axis", "=", "1", ",", "name", "=", "'mbox_conf'", ")", "(", "[", "deconv1_mbox_conf_reshape", ",", "\n", "deconv2_mbox_conf_reshape", ",", "\n", "deconv3_mbox_conf_reshape", "]", ")", "\n", "\n", "# Output shape of `mbox_loc`: (batch, n_boxes_total, 4)", "\n", "mbox_loc", "=", "Concatenate", "(", "axis", "=", "1", ",", "name", "=", "'mbox_loc'", ")", "(", "[", "deconv1_mbox_loc_reshape", ",", "\n", "deconv2_mbox_loc_reshape", ",", "\n", "deconv3_mbox_loc_reshape", "]", ")", "\n", "\n", "# Output shape of `mbox_priorbox`: (batch, n_boxes_total, 8)", "\n", "mbox_priorbox", "=", "Concatenate", "(", "axis", "=", "1", ",", "name", "=", "'mbox_priorbox'", ")", "(", "[", "deconv1_mbox_priorbox_reshape", ",", "\n", "deconv2_mbox_priorbox_reshape", ",", "\n", "deconv3_mbox_priorbox_reshape", "]", ")", "\n", "\n", "# The box coordinate predictions will go into the loss function just the way they are,", "\n", "# but for the class predictions, we'll apply a softmax activation layer first", "\n", "mbox_conf_softmax", "=", "Activation", "(", "'softmax'", ",", "name", "=", "'mbox_conf_softmax'", ")", "(", "mbox_conf", ")", "\n", "\n", "# Concatenate the class and box predictions and the anchors to one large predictions vector", "\n", "# Output shape of `predictions`: (batch, n_boxes_total, n_classes + 4 + 8)", "\n", "predictions", "=", "Concatenate", "(", "axis", "=", "2", ",", "name", "=", "'predictions'", ")", "(", "[", "mbox_conf_softmax", ",", "mbox_loc", ",", "mbox_priorbox", "]", ")", "\n", "\n", "if", "mode", "==", "'training'", ":", "\n", "        ", "model", "=", "Model", "(", "inputs", "=", "x", ",", "outputs", "=", "predictions", ")", "\n", "", "elif", "mode", "==", "'inference'", ":", "\n", "        ", "decoded_predictions", "=", "DecodeDetections", "(", "confidence_thresh", "=", "confidence_thresh", ",", "\n", "iou_threshold", "=", "iou_threshold", ",", "\n", "top_k", "=", "top_k", ",", "\n", "nms_max_output_size", "=", "nms_max_output_size", ",", "\n", "coords", "=", "coords", ",", "\n", "normalize_coords", "=", "normalize_coords", ",", "\n", "img_height", "=", "img_height", ",", "\n", "img_width", "=", "img_width", ",", "\n", "name", "=", "'decoded_predictions'", ")", "(", "predictions", ")", "\n", "model", "=", "Model", "(", "inputs", "=", "x", ",", "outputs", "=", "decoded_predictions", ")", "\n", "", "elif", "mode", "==", "'inference_fast'", ":", "\n", "        ", "decoded_predictions", "=", "DecodeDetectionsFast", "(", "confidence_thresh", "=", "confidence_thresh", ",", "\n", "iou_threshold", "=", "iou_threshold", ",", "\n", "top_k", "=", "top_k", ",", "\n", "nms_max_output_size", "=", "nms_max_output_size", ",", "\n", "coords", "=", "coords", ",", "\n", "normalize_coords", "=", "normalize_coords", ",", "\n", "img_height", "=", "img_height", ",", "\n", "img_width", "=", "img_width", ",", "\n", "name", "=", "'decoded_predictions'", ")", "(", "predictions", ")", "\n", "model", "=", "Model", "(", "inputs", "=", "x", ",", "outputs", "=", "decoded_predictions", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"`mode` must be one of 'training', 'inference' or 'inference_fast', but received '{}'.\"", ".", "format", "(", "mode", ")", ")", "\n", "\n", "", "if", "return_predictor_sizes", ":", "\n", "        ", "predictor_sizes", "=", "np", ".", "array", "(", "[", "deconv1_mbox_conf", ".", "_keras_shape", "[", "1", ":", "3", "]", ",", "\n", "deconv2_mbox_conf", ".", "_keras_shape", "[", "1", ":", "3", "]", ",", "\n", "deconv3_mbox_conf", ".", "_keras_shape", "[", "1", ":", "3", "]", "]", ")", "\n", "return", "model", ",", "predictor_sizes", "\n", "", "else", ":", "\n", "        ", "return", "model", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.models.keras_ssd512_skip.ssd_512": [[33, 449], ["numpy.array", "numpy.any", "keras.layers.Input", "keras.layers.concatenate", "keras.layers.concatenate", "keras.layers.concatenate", "keras.layers.concatenate", "ValueError", "ValueError", "numpy.linspace", "len", "ValueError", "ValueError", "ValueError", "ValueError", "keras.layers.Lambda", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.MaxPooling2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.MaxPooling2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.MaxPooling2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.MaxPooling2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.MaxPooling2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras_layers.keras_layer_AnchorBoxes.AnchorBoxes", "keras_layers.keras_layer_AnchorBoxes.AnchorBoxes", "keras_layers.keras_layer_AnchorBoxes.AnchorBoxes", "keras_layers.keras_layer_AnchorBoxes.AnchorBoxes", "keras_layers.keras_layer_AnchorBoxes.AnchorBoxes", "keras.layers.Reshape", "keras.layers.Reshape", "keras.layers.Reshape", "keras.layers.Reshape", "keras.layers.Reshape", "keras.layers.Reshape", "keras.layers.Reshape", "keras.layers.Reshape", "keras.layers.Reshape", "keras.layers.Reshape", "keras.layers.Reshape", "keras.layers.Reshape", "keras.layers.Reshape", "keras.layers.Reshape", "keras.layers.Reshape", "keras.layers.Concatenate", "keras.layers.Concatenate", "keras.layers.Concatenate", "keras.layers.Activation", "keras.layers.Concatenate", "keras.models.Model", "numpy.array", "len", "ValueError", "len", "ValueError", "len", "len", "len", "numpy.array", "numpy.array", "len", "keras.stack", "keras.layers.Lambda", "keras.layers.Lambda", "keras.layers.Lambda", "keras.models.Model", "len", "len.append", "len.append", "len", "len", "keras.stack", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras_layers.keras_layer_DecodeDetections.DecodeDetections", "keras.models.Model", "ValueError", "len", "len", "len", "keras_layers.keras_layer_DecodeDetectionsFast.DecodeDetectionsFast", "len"], "function", ["None"], ["def", "ssd_512", "(", "image_size", ",", "\n", "n_classes", ",", "\n", "mode", "=", "'training'", ",", "\n", "l2_regularization", "=", "0.0005", ",", "\n", "min_scale", "=", "None", ",", "\n", "max_scale", "=", "None", ",", "\n", "scales", "=", "None", ",", "\n", "aspect_ratios_global", "=", "None", ",", "\n", "aspect_ratios_per_layer", "=", "[", "[", "1.0", ",", "2.0", ",", "0.5", "]", ",", "\n", "[", "1.0", ",", "2.0", ",", "0.5", ",", "3.0", ",", "1.0", "/", "3.0", "]", ",", "\n", "[", "1.0", ",", "2.0", ",", "0.5", ",", "3.0", ",", "1.0", "/", "3.0", "]", ",", "\n", "[", "1.0", ",", "2.0", ",", "0.5", ",", "3.0", ",", "1.0", "/", "3.0", "]", ",", "\n", "[", "1.0", ",", "2.0", ",", "0.5", ",", "3.0", ",", "1.0", "/", "3.0", "]", ",", "\n", "[", "1.0", ",", "2.0", ",", "0.5", "]", ",", "\n", "[", "1.0", ",", "2.0", ",", "0.5", "]", "]", ",", "\n", "two_boxes_for_ar1", "=", "True", ",", "\n", "steps", "=", "[", "8", ",", "16", ",", "32", ",", "64", ",", "128", ",", "256", ",", "512", "]", ",", "\n", "offsets", "=", "None", ",", "\n", "clip_boxes", "=", "False", ",", "\n", "variances", "=", "[", "0.1", ",", "0.1", ",", "0.2", ",", "0.2", "]", ",", "\n", "coords", "=", "'centroids'", ",", "\n", "normalize_coords", "=", "True", ",", "\n", "subtract_mean", "=", "[", "123", ",", "117", ",", "104", "]", ",", "\n", "divide_by_stddev", "=", "None", ",", "\n", "swap_channels", "=", "[", "2", ",", "1", ",", "0", "]", ",", "\n", "confidence_thresh", "=", "0.01", ",", "\n", "iou_threshold", "=", "0.45", ",", "\n", "top_k", "=", "200", ",", "\n", "nms_max_output_size", "=", "400", ",", "\n", "return_predictor_sizes", "=", "False", ")", ":", "\n", "    ", "'''\n    Build a Keras model with SSD512 architecture, see references.\n\n    The base network is a reduced atrous VGG-16, extended by the SSD architecture,\n    as described in the paper.\n\n    Most of the arguments that this function takes are only needed for the anchor\n    box layers. In case you're training the network, the parameters passed here must\n    be the same as the ones used to set up `SSDBoxEncoder`. In case you're loading\n    trained weights, the parameters passed here must be the same as the ones used\n    to produce the trained weights.\n\n    Some of these arguments are explained in more detail in the documentation of the\n    `SSDBoxEncoder` class.\n\n    Note: Requires Keras v2.0 or later. Currently works only with the\n    TensorFlow backend (v1.0 or later).\n\n    Arguments:\n        image_size (tuple): The input image size in the format `(height, width, channels)`.\n        n_classes (int): The number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO.\n        mode (str, optional): One of 'training', 'inference' and 'inference_fast'. In 'training' mode,\n            the model outputs the raw prediction tensor, while in 'inference' and 'inference_fast' modes,\n            the raw predictions are decoded into absolute coordinates and filtered via confidence thresholding,\n            non-maximum suppression, and top-k filtering. The difference between latter two modes is that\n            'inference' follows the exact procedure of the original Caffe implementation, while\n            'inference_fast' uses a faster prediction decoding procedure.\n        l2_regularization (float, optional): The L2-regularization rate. Applies to all convolutional layers.\n            Set to zero to deactivate L2-regularization.\n        min_scale (float, optional): The smallest scaling factor for the size of the anchor boxes as a fraction\n            of the shorter side of the input images.\n        max_scale (float, optional): The largest scaling factor for the size of the anchor boxes as a fraction\n            of the shorter side of the input images. All scaling factors between the smallest and the\n            largest will be linearly interpolated. Note that the second to last of the linearly interpolated\n            scaling factors will actually be the scaling factor for the last predictor layer, while the last\n            scaling factor is used for the second box for aspect ratio 1 in the last predictor layer\n            if `two_boxes_for_ar1` is `True`.\n        scales (list, optional): A list of floats containing scaling factors per convolutional predictor layer.\n            This list must be one element longer than the number of predictor layers. The first `k` elements are the\n            scaling factors for the `k` predictor layers, while the last element is used for the second box\n            for aspect ratio 1 in the last predictor layer if `two_boxes_for_ar1` is `True`. This additional\n            last scaling factor must be passed either way, even if it is not being used.\n            If a list is passed, this argument overrides `min_scale` and `max_scale`. All scaling factors\n            must be greater than zero.\n        aspect_ratios_global (list, optional): The list of aspect ratios for which anchor boxes are to be\n            generated. This list is valid for all prediction layers.\n        aspect_ratios_per_layer (list, optional): A list containing one aspect ratio list for each prediction layer.\n            This allows you to set the aspect ratios for each predictor layer individually, which is the case for the\n            original SSD512 implementation. If a list is passed, it overrides `aspect_ratios_global`.\n        two_boxes_for_ar1 (bool, optional): Only relevant for aspect ratio lists that contain 1. Will be ignored otherwise.\n            If `True`, two anchor boxes will be generated for aspect ratio 1. The first will be generated\n            using the scaling factor for the respective layer, the second one will be generated using\n            geometric mean of said scaling factor and next bigger scaling factor.\n        steps (list, optional): `None` or a list with as many elements as there are predictor layers. The elements can be\n            either ints/floats or tuples of two ints/floats. These numbers represent for each predictor layer how many\n            pixels apart the anchor box center points should be vertically and horizontally along the spatial grid over\n            the image. If the list contains ints/floats, then that value will be used for both spatial dimensions.\n            If the list contains tuples of two ints/floats, then they represent `(step_height, step_width)`.\n            If no steps are provided, then they will be computed such that the anchor box center points will form an\n            equidistant grid within the image dimensions.\n        offsets (list, optional): `None` or a list with as many elements as there are predictor layers. The elements can be\n            either floats or tuples of two floats. These numbers represent for each predictor layer how many\n            pixels from the top and left boarders of the image the top-most and left-most anchor box center points should be\n            as a fraction of `steps`. The last bit is important: The offsets are not absolute pixel values, but fractions\n            of the step size specified in the `steps` argument. If the list contains floats, then that value will\n            be used for both spatial dimensions. If the list contains tuples of two floats, then they represent\n            `(vertical_offset, horizontal_offset)`. If no offsets are provided, then they will default to 0.5 of the step size.\n        clip_boxes (bool, optional): If `True`, clips the anchor box coordinates to stay within image boundaries.\n        variances (list, optional): A list of 4 floats >0. The anchor box offset for each coordinate will be divided by\n            its respective variance value.\n        coords (str, optional): The box coordinate format to be used internally by the model (i.e. this is not the input format\n            of the ground truth labels). Can be either 'centroids' for the format `(cx, cy, w, h)` (box center coordinates, width,\n            and height), 'minmax' for the format `(xmin, xmax, ymin, ymax)`, or 'corners' for the format `(xmin, ymin, xmax, ymax)`.\n        normalize_coords (bool, optional): Set to `True` if the model is supposed to use relative instead of absolute coordinates,\n            i.e. if the model predicts box coordinates within [0,1] instead of absolute coordinates.\n        subtract_mean (array-like, optional): `None` or an array-like object of integers or floating point values\n            of any shape that is broadcast-compatible with the image shape. The elements of this array will be\n            subtracted from the image pixel intensity values. For example, pass a list of three integers\n            to perform per-channel mean normalization for color images.\n        divide_by_stddev (array-like, optional): `None` or an array-like object of non-zero integers or\n            floating point values of any shape that is broadcast-compatible with the image shape. The image pixel\n            intensity values will be divided by the elements of this array. For example, pass a list\n            of three integers to perform per-channel standard deviation normalization for color images.\n        swap_channels (list, optional): Either `False` or a list of integers representing the desired order in which the input\n            image channels should be swapped.\n        confidence_thresh (float, optional): A float in [0,1), the minimum classification confidence in a specific\n            positive class in order to be considered for the non-maximum suppression stage for the respective class.\n            A lower value will result in a larger part of the selection process being done by the non-maximum suppression\n            stage, while a larger value will result in a larger part of the selection process happening in the confidence\n            thresholding stage.\n        iou_threshold (float, optional): A float in [0,1]. All boxes that have a Jaccard similarity of greater than `iou_threshold`\n            with a locally maximal box will be removed from the set of predictions for a given class, where 'maximal' refers\n            to the box's confidence score.\n        top_k (int, optional): The number of highest scoring predictions to be kept for each batch item after the\n            non-maximum suppression stage.\n        nms_max_output_size (int, optional): The maximal number of predictions that will be left over after the NMS stage.\n        return_predictor_sizes (bool, optional): If `True`, this function not only returns the model, but also\n            a list containing the spatial dimensions of the predictor layers. This isn't strictly necessary since\n            you can always get their sizes easily via the Keras API, but it's convenient and less error-prone\n            to get them this way. They are only relevant for training anyway (SSDBoxEncoder needs to know the\n            spatial dimensions of the predictor layers), for inference you don't need them.\n\n    Returns:\n        model: The Keras SSD512 model.\n        predictor_sizes (optional): A Numpy array containing the `(height, width)` portion\n            of the output tensor shape for each convolutional predictor layer. During\n            training, the generator function needs this in order to transform\n            the ground truth labels into tensors of identical structure as the\n            output tensors of the model, which is in turn needed for the cost\n            function.\n\n    References:\n        https://arxiv.org/abs/1512.02325v5\n    '''", "\n", "\n", "n_predictor_layers", "=", "5", "# The number of predictor conv layers in the network is 7 for the original SSD512", "\n", "n_classes", "+=", "1", "# Account for the background class.", "\n", "l2_reg", "=", "l2_regularization", "# Make the internal name shorter.", "\n", "img_height", ",", "img_width", ",", "img_channels", "=", "image_size", "[", "0", "]", ",", "image_size", "[", "1", "]", ",", "image_size", "[", "2", "]", "\n", "\n", "############################################################################", "\n", "# Get a few exceptions out of the way.", "\n", "############################################################################", "\n", "############################################################################", "\n", "if", "aspect_ratios_global", "is", "None", "and", "aspect_ratios_per_layer", "is", "None", ":", "\n", "        ", "raise", "ValueError", "(", "\"`aspect_ratios_global` and `aspect_ratios_per_layer` cannot both be None. At least one needs to be specified.\"", ")", "\n", "", "if", "aspect_ratios_per_layer", ":", "\n", "        ", "if", "len", "(", "aspect_ratios_per_layer", ")", "!=", "n_predictor_layers", ":", "\n", "            ", "raise", "ValueError", "(", "\"It must be either aspect_ratios_per_layer is None or len(aspect_ratios_per_layer) == {}, but len(aspect_ratios_per_layer) == {}.\"", ".", "format", "(", "n_predictor_layers", ",", "len", "(", "aspect_ratios_per_layer", ")", ")", ")", "\n", "\n", "", "", "if", "(", "min_scale", "is", "None", "or", "max_scale", "is", "None", ")", "and", "scales", "is", "None", ":", "\n", "        ", "raise", "ValueError", "(", "\"Either `min_scale` and `max_scale` or `scales` need to be specified.\"", ")", "\n", "", "if", "scales", ":", "\n", "        ", "if", "len", "(", "scales", ")", "!=", "n_predictor_layers", "+", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"It must be either scales is None or len(scales) == {}, but len(scales) == {}.\"", ".", "format", "(", "n_predictor_layers", "+", "1", ",", "len", "(", "scales", ")", ")", ")", "\n", "", "", "else", ":", "# If no explicit list of scaling factors was passed, compute the list of scaling factors from `min_scale` and `max_scale`", "\n", "        ", "scales", "=", "np", ".", "linspace", "(", "min_scale", ",", "max_scale", ",", "n_predictor_layers", "+", "1", ")", "\n", "\n", "", "if", "len", "(", "variances", ")", "!=", "4", ":", "\n", "        ", "raise", "ValueError", "(", "\"4 variance values must be pased, but {} values were received.\"", ".", "format", "(", "len", "(", "variances", ")", ")", ")", "\n", "", "variances", "=", "np", ".", "array", "(", "variances", ")", "\n", "if", "np", ".", "any", "(", "variances", "<=", "0", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"All variances must be >0, but the variances given are {}\"", ".", "format", "(", "variances", ")", ")", "\n", "\n", "", "if", "(", "not", "(", "steps", "is", "None", ")", ")", "and", "(", "len", "(", "steps", ")", "!=", "n_predictor_layers", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"You must provide at least one step value per predictor layer.\"", ")", "\n", "\n", "", "if", "(", "not", "(", "offsets", "is", "None", ")", ")", "and", "(", "len", "(", "offsets", ")", "!=", "n_predictor_layers", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"You must provide at least one offset value per predictor layer.\"", ")", "\n", "\n", "############################################################################", "\n", "# Compute the anchor box parameters.", "\n", "############################################################################", "\n", "\n", "# Set the aspect ratios for each predictor layer. These are only needed for the anchor box layers.", "\n", "", "if", "aspect_ratios_per_layer", ":", "\n", "        ", "aspect_ratios", "=", "aspect_ratios_per_layer", "\n", "", "else", ":", "\n", "        ", "aspect_ratios", "=", "[", "aspect_ratios_global", "]", "*", "n_predictor_layers", "\n", "\n", "# Compute the number of boxes to be predicted per cell for each predictor layer.", "\n", "# We need this so that we know how many channels the predictor layers need to have.", "\n", "", "if", "aspect_ratios_per_layer", ":", "\n", "        ", "n_boxes", "=", "[", "]", "\n", "for", "ar", "in", "aspect_ratios_per_layer", ":", "\n", "            ", "if", "(", "1", "in", "ar", ")", "&", "two_boxes_for_ar1", ":", "\n", "                ", "n_boxes", ".", "append", "(", "len", "(", "ar", ")", "+", "1", ")", "# +1 for the second box for aspect ratio 1", "\n", "", "else", ":", "\n", "                ", "n_boxes", ".", "append", "(", "len", "(", "ar", ")", ")", "\n", "", "", "", "else", ":", "# If only a global aspect ratio list was passed, then the number of boxes is the same for each predictor layer", "\n", "        ", "if", "(", "1", "in", "aspect_ratios_global", ")", "&", "two_boxes_for_ar1", ":", "\n", "            ", "n_boxes", "=", "len", "(", "aspect_ratios_global", ")", "+", "1", "\n", "", "else", ":", "\n", "            ", "n_boxes", "=", "len", "(", "aspect_ratios_global", ")", "\n", "", "n_boxes", "=", "[", "n_boxes", "]", "*", "n_predictor_layers", "\n", "\n", "", "if", "steps", "is", "None", ":", "\n", "        ", "steps", "=", "[", "None", "]", "*", "n_predictor_layers", "\n", "", "if", "offsets", "is", "None", ":", "\n", "        ", "offsets", "=", "[", "None", "]", "*", "n_predictor_layers", "\n", "\n", "############################################################################", "\n", "# Define functions for the Lambda layers below.", "\n", "############################################################################", "\n", "\n", "", "def", "identity_layer", "(", "tensor", ")", ":", "\n", "        ", "return", "tensor", "\n", "\n", "", "def", "input_mean_normalization", "(", "tensor", ")", ":", "\n", "        ", "return", "tensor", "-", "np", ".", "array", "(", "subtract_mean", ")", "\n", "\n", "", "def", "input_stddev_normalization", "(", "tensor", ")", ":", "\n", "        ", "return", "tensor", "/", "np", ".", "array", "(", "divide_by_stddev", ")", "\n", "\n", "", "def", "input_channel_swap", "(", "tensor", ")", ":", "\n", "        ", "if", "len", "(", "swap_channels", ")", "==", "3", ":", "\n", "            ", "return", "K", ".", "stack", "(", "[", "tensor", "[", "...", ",", "swap_channels", "[", "0", "]", "]", ",", "tensor", "[", "...", ",", "swap_channels", "[", "1", "]", "]", ",", "tensor", "[", "...", ",", "swap_channels", "[", "2", "]", "]", "]", ",", "axis", "=", "-", "1", ")", "\n", "", "elif", "len", "(", "swap_channels", ")", "==", "4", ":", "\n", "            ", "return", "K", ".", "stack", "(", "[", "tensor", "[", "...", ",", "swap_channels", "[", "0", "]", "]", ",", "tensor", "[", "...", ",", "swap_channels", "[", "1", "]", "]", ",", "tensor", "[", "...", ",", "swap_channels", "[", "2", "]", "]", ",", "tensor", "[", "...", ",", "swap_channels", "[", "3", "]", "]", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "############################################################################", "\n", "# Build the network.", "\n", "############################################################################", "\n", "\n", "", "", "x", "=", "Input", "(", "shape", "=", "(", "img_height", ",", "img_width", ",", "img_channels", ")", ")", "\n", "\n", "# The following identity layer is only needed so that the subsequent lambda layers can be optional.", "\n", "x1", "=", "Lambda", "(", "identity_layer", ",", "output_shape", "=", "(", "img_height", ",", "img_width", ",", "img_channels", ")", ",", "name", "=", "'identity_layer'", ")", "(", "x", ")", "\n", "if", "not", "(", "subtract_mean", "is", "None", ")", ":", "\n", "        ", "x1", "=", "Lambda", "(", "input_mean_normalization", ",", "output_shape", "=", "(", "img_height", ",", "img_width", ",", "img_channels", ")", ",", "name", "=", "'input_mean_normalization'", ")", "(", "x1", ")", "\n", "", "if", "not", "(", "divide_by_stddev", "is", "None", ")", ":", "\n", "        ", "x1", "=", "Lambda", "(", "input_stddev_normalization", ",", "output_shape", "=", "(", "img_height", ",", "img_width", ",", "img_channels", ")", ",", "name", "=", "'input_stddev_normalization'", ")", "(", "x1", ")", "\n", "", "if", "swap_channels", ":", "\n", "        ", "x1", "=", "Lambda", "(", "input_channel_swap", ",", "output_shape", "=", "(", "img_height", ",", "img_width", ",", "img_channels", ")", ",", "name", "=", "'input_channel_swap'", ")", "(", "x1", ")", "\n", "\n", "", "conv1_1", "=", "Conv2D", "(", "64", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv1_1'", ")", "(", "x1", ")", "\n", "conv1_2", "=", "Conv2D", "(", "64", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv1_2'", ")", "(", "conv1_1", ")", "\n", "pool1", "=", "MaxPooling2D", "(", "pool_size", "=", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "padding", "=", "'same'", ",", "name", "=", "'pool1'", ")", "(", "conv1_2", ")", "\n", "\n", "conv2_1", "=", "Conv2D", "(", "128", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv2_1'", ")", "(", "pool1", ")", "\n", "conv2_2", "=", "Conv2D", "(", "128", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv2_2'", ")", "(", "conv2_1", ")", "\n", "pool2", "=", "MaxPooling2D", "(", "pool_size", "=", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "padding", "=", "'same'", ",", "name", "=", "'pool2'", ")", "(", "conv2_2", ")", "\n", "\n", "conv3_1", "=", "Conv2D", "(", "256", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv3_1'", ")", "(", "pool2", ")", "\n", "conv3_2", "=", "Conv2D", "(", "256", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv3_2'", ")", "(", "conv3_1", ")", "\n", "conv3_3", "=", "Conv2D", "(", "256", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv3_3'", ")", "(", "conv3_2", ")", "\n", "pool3", "=", "MaxPooling2D", "(", "pool_size", "=", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "padding", "=", "'same'", ",", "name", "=", "'pool3'", ")", "(", "conv3_3", ")", "\n", "\n", "conv4_1", "=", "Conv2D", "(", "512", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv4_1'", ")", "(", "pool3", ")", "\n", "conv4_2", "=", "Conv2D", "(", "512", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv4_2'", ")", "(", "conv4_1", ")", "\n", "conv4_3", "=", "Conv2D", "(", "512", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv4_3'", ")", "(", "conv4_2", ")", "\n", "pool4", "=", "MaxPooling2D", "(", "pool_size", "=", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "padding", "=", "'same'", ",", "name", "=", "'pool4'", ")", "(", "conv4_3", ")", "\n", "\n", "conv5_1", "=", "Conv2D", "(", "512", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv5_1'", ")", "(", "pool4", ")", "\n", "conv5_2", "=", "Conv2D", "(", "512", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv5_2'", ")", "(", "conv5_1", ")", "\n", "conv5_3", "=", "Conv2D", "(", "512", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv5_3'", ")", "(", "conv5_2", ")", "\n", "\n", "newpool5", "=", "MaxPooling2D", "(", "pool_size", "=", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "padding", "=", "'same'", ",", "name", "=", "'newpool5'", ")", "(", "conv5_3", ")", "# 16  step=32", "\n", "conv6_1_add", "=", "Conv2D", "(", "512", ",", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv6_1_add'", ")", "(", "newpool5", ")", "\n", "conv6_2_add", "=", "Conv2D", "(", "512", ",", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv6_2_add'", ")", "(", "conv6_1_add", ")", "\n", "\n", "conv7_add", "=", "Conv2D", "(", "512", ",", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv7_add'", ")", "(", "conv6_2_add", ")", "\n", "\n", "conv7_up", "=", "Conv2DTranspose", "(", "512", ",", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv7_up'", ")", "(", "conv7_add", ")", "\n", "deconv6_1", "=", "concatenate", "(", "[", "conv7_up", ",", "conv6_2_add", "]", ",", "axis", "=", "3", ")", "\n", "deconv6_2", "=", "Conv2DTranspose", "(", "512", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'deconv6_2'", ")", "(", "deconv6_1", ")", "\n", "\n", "conv6_up", "=", "Conv2DTranspose", "(", "512", ",", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv6_up'", ")", "(", "deconv6_2", ")", "\n", "deconv5_1", "=", "concatenate", "(", "[", "conv6_up", ",", "conv5_3", "]", ",", "axis", "=", "3", ")", "\n", "deconv5_2", "=", "Conv2DTranspose", "(", "512", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'deconv5_2'", ")", "(", "deconv5_1", ")", "\n", "\n", "conv5_up", "=", "Conv2DTranspose", "(", "512", ",", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv5_up'", ")", "(", "deconv5_2", ")", "\n", "deconv4_1", "=", "concatenate", "(", "[", "conv5_up", ",", "conv4_3", "]", ",", "axis", "=", "3", ")", "\n", "deconv4_2", "=", "Conv2DTranspose", "(", "512", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'deconv4_2'", ")", "(", "deconv4_1", ")", "\n", "\n", "conv4_up", "=", "Conv2DTranspose", "(", "256", ",", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv4_up'", ")", "(", "deconv4_2", ")", "\n", "deconv3_1", "=", "concatenate", "(", "[", "conv4_up", ",", "conv3_3", "]", ",", "axis", "=", "3", ")", "\n", "deconv3_2", "=", "Conv2DTranspose", "(", "512", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "'relu'", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'deconv3_2'", ")", "(", "deconv3_1", ")", "\n", "\n", "# Feed conv4_3 into the L2 normalization layer", "\n", "# conv4_3_norm = L2Normalization(gamma_init=20, name='conv4_3_norm')(conv4_3)", "\n", "# We precidt `n_classes` confidence values for each box, hence the confidence predictors have depth `n_boxes * n_classes`", "\n", "# Output shape of the confidence layers: `(batch, height, width, n_boxes * n_classes)`", "\n", "deconv3_2_mbox_conf", "=", "Conv2D", "(", "n_boxes", "[", "0", "]", "*", "n_classes", ",", "(", "3", ",", "3", ")", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'deconv3_2_mbox_conf'", ")", "(", "deconv3_2", ")", "\n", "deconv4_2_mbox_conf", "=", "Conv2D", "(", "n_boxes", "[", "1", "]", "*", "n_classes", ",", "(", "3", ",", "3", ")", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'deconv4_2_mbox_conf'", ")", "(", "deconv4_2", ")", "\n", "deconv5_2_mbox_conf", "=", "Conv2D", "(", "n_boxes", "[", "2", "]", "*", "n_classes", ",", "(", "3", ",", "3", ")", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'deconv5_2_mbox_conf'", ")", "(", "deconv5_2", ")", "\n", "deconv6_2_mbox_conf", "=", "Conv2D", "(", "n_boxes", "[", "3", "]", "*", "n_classes", ",", "(", "3", ",", "3", ")", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'deconv6_2_mbox_conf'", ")", "(", "deconv6_2", ")", "\n", "conv7_add_mbox_conf", "=", "Conv2D", "(", "n_boxes", "[", "4", "]", "*", "n_classes", ",", "(", "3", ",", "3", ")", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv7_add_mbox_conf'", ")", "(", "conv7_add", ")", "\n", "\n", "# We predict 4 box coordinates for each box, hence the localization predictors have depth `n_boxes * 4`", "\n", "# Output shape of the localization layers: `(batch, height, width, n_boxes * 4)`", "\n", "deconv3_2_mbox_loc", "=", "Conv2D", "(", "n_boxes", "[", "0", "]", "*", "4", ",", "(", "3", ",", "3", ")", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'deconv3_2_mbox_loc'", ")", "(", "deconv3_2", ")", "\n", "deconv4_2_mbox_loc", "=", "Conv2D", "(", "n_boxes", "[", "1", "]", "*", "4", ",", "(", "3", ",", "3", ")", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'deconv4_2_mbox_loc'", ")", "(", "deconv4_2", ")", "\n", "deconv5_2_mbox_loc", "=", "Conv2D", "(", "n_boxes", "[", "2", "]", "*", "4", ",", "(", "3", ",", "3", ")", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'deconv5_2_mbox_loc'", ")", "(", "deconv5_2", ")", "\n", "deconv6_2_mbox_loc", "=", "Conv2D", "(", "n_boxes", "[", "3", "]", "*", "4", ",", "(", "3", ",", "3", ")", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'deconv6_2_mbox_loc'", ")", "(", "deconv6_2", ")", "\n", "conv7_add_mbox_loc", "=", "Conv2D", "(", "n_boxes", "[", "4", "]", "*", "4", ",", "(", "3", ",", "3", ")", ",", "padding", "=", "'same'", ",", "kernel_initializer", "=", "'he_normal'", ",", "kernel_regularizer", "=", "l2", "(", "l2_reg", ")", ",", "name", "=", "'conv7_add_mbox_loc'", ")", "(", "conv7_add", ")", "\n", "\n", "# Generate the anchor boxes (called \"priors\" in the original Caffe/C++ implementation, so I'll keep their layer names)", "\n", "# Output shape of anchors: `(batch, height, width, n_boxes, 8)`", "\n", "deconv3_2_mbox_priorbox", "=", "AnchorBoxes", "(", "img_height", ",", "img_width", ",", "this_scale", "=", "scales", "[", "0", "]", ",", "next_scale", "=", "scales", "[", "1", "]", ",", "aspect_ratios", "=", "aspect_ratios", "[", "0", "]", ",", "\n", "two_boxes_for_ar1", "=", "two_boxes_for_ar1", ",", "this_steps", "=", "steps", "[", "0", "]", ",", "this_offsets", "=", "offsets", "[", "0", "]", ",", "clip_boxes", "=", "clip_boxes", ",", "\n", "variances", "=", "variances", ",", "coords", "=", "coords", ",", "normalize_coords", "=", "normalize_coords", ",", "name", "=", "'deconv3_2_mbox_priorbox'", ")", "(", "deconv3_2_mbox_loc", ")", "\n", "deconv4_2_mbox_priorbox", "=", "AnchorBoxes", "(", "img_height", ",", "img_width", ",", "this_scale", "=", "scales", "[", "1", "]", ",", "next_scale", "=", "scales", "[", "2", "]", ",", "aspect_ratios", "=", "aspect_ratios", "[", "1", "]", ",", "\n", "two_boxes_for_ar1", "=", "two_boxes_for_ar1", ",", "this_steps", "=", "steps", "[", "1", "]", ",", "this_offsets", "=", "offsets", "[", "1", "]", ",", "clip_boxes", "=", "clip_boxes", ",", "\n", "variances", "=", "variances", ",", "coords", "=", "coords", ",", "normalize_coords", "=", "normalize_coords", ",", "name", "=", "'deconv4_2_mbox_priorbox'", ")", "(", "deconv4_2_mbox_loc", ")", "\n", "deconv5_2_mbox_priorbox", "=", "AnchorBoxes", "(", "img_height", ",", "img_width", ",", "this_scale", "=", "scales", "[", "2", "]", ",", "next_scale", "=", "scales", "[", "3", "]", ",", "aspect_ratios", "=", "aspect_ratios", "[", "2", "]", ",", "\n", "two_boxes_for_ar1", "=", "two_boxes_for_ar1", ",", "this_steps", "=", "steps", "[", "2", "]", ",", "this_offsets", "=", "offsets", "[", "2", "]", ",", "clip_boxes", "=", "clip_boxes", ",", "\n", "variances", "=", "variances", ",", "coords", "=", "coords", ",", "normalize_coords", "=", "normalize_coords", ",", "name", "=", "'deconv5_2_mbox_priorbox'", ")", "(", "deconv5_2_mbox_loc", ")", "\n", "deconv6_2_mbox_priorbox", "=", "AnchorBoxes", "(", "img_height", ",", "img_width", ",", "this_scale", "=", "scales", "[", "3", "]", ",", "next_scale", "=", "scales", "[", "4", "]", ",", "aspect_ratios", "=", "aspect_ratios", "[", "3", "]", ",", "\n", "two_boxes_for_ar1", "=", "two_boxes_for_ar1", ",", "this_steps", "=", "steps", "[", "3", "]", ",", "this_offsets", "=", "offsets", "[", "3", "]", ",", "clip_boxes", "=", "clip_boxes", ",", "\n", "variances", "=", "variances", ",", "coords", "=", "coords", ",", "normalize_coords", "=", "normalize_coords", ",", "name", "=", "'deconv6_2_mbox_priorbox'", ")", "(", "deconv6_2_mbox_loc", ")", "\n", "conv7_add_mbox_priorbox", "=", "AnchorBoxes", "(", "img_height", ",", "img_width", ",", "this_scale", "=", "scales", "[", "4", "]", ",", "next_scale", "=", "scales", "[", "5", "]", ",", "aspect_ratios", "=", "aspect_ratios", "[", "4", "]", ",", "\n", "two_boxes_for_ar1", "=", "two_boxes_for_ar1", ",", "this_steps", "=", "steps", "[", "4", "]", ",", "this_offsets", "=", "offsets", "[", "4", "]", ",", "clip_boxes", "=", "clip_boxes", ",", "\n", "variances", "=", "variances", ",", "coords", "=", "coords", ",", "normalize_coords", "=", "normalize_coords", ",", "name", "=", "'conv7_add_mbox_priorbox'", ")", "(", "conv7_add_mbox_loc", ")", "\n", "\n", "# Reshape the class predictions, yielding 3D tensors of shape `(batch, height * width * n_boxes, n_classes)`", "\n", "# We want the classes isolated in the last axis to perform softmax on them", "\n", "deconv3_2_mbox_conf_reshape", "=", "Reshape", "(", "(", "-", "1", ",", "n_classes", ")", ",", "name", "=", "'deconv3_2_mbox_conf_reshape'", ")", "(", "deconv3_2_mbox_conf", ")", "\n", "deconv4_2_mbox_conf_reshape", "=", "Reshape", "(", "(", "-", "1", ",", "n_classes", ")", ",", "name", "=", "'deconv4_2_mbox_conf_reshape'", ")", "(", "deconv4_2_mbox_conf", ")", "\n", "deconv5_2_mbox_conf_reshape", "=", "Reshape", "(", "(", "-", "1", ",", "n_classes", ")", ",", "name", "=", "'deconv5_2_mbox_conf_reshape'", ")", "(", "deconv5_2_mbox_conf", ")", "\n", "deconv6_2_mbox_conf_reshape", "=", "Reshape", "(", "(", "-", "1", ",", "n_classes", ")", ",", "name", "=", "'deconv6_2_mbox_conf_reshape'", ")", "(", "deconv6_2_mbox_conf", ")", "\n", "conv7_add_mbox_conf_reshape", "=", "Reshape", "(", "(", "-", "1", ",", "n_classes", ")", ",", "name", "=", "'conv7_add_mbox_conf_reshape'", ")", "(", "conv7_add_mbox_conf", ")", "\n", "\n", "# Reshape the box predictions, yielding 3D tensors of shape `(batch, height * width * n_boxes, 4)`", "\n", "# We want the four box coordinates isolated in the last axis to compute the smooth L1 loss", "\n", "deconv3_2_mbox_loc_reshape", "=", "Reshape", "(", "(", "-", "1", ",", "4", ")", ",", "name", "=", "'deconv3_2_mbox_loc_reshape'", ")", "(", "deconv3_2_mbox_loc", ")", "\n", "deconv4_2_mbox_loc_reshape", "=", "Reshape", "(", "(", "-", "1", ",", "4", ")", ",", "name", "=", "'deconv4_2_mbox_loc_reshape'", ")", "(", "deconv4_2_mbox_loc", ")", "\n", "deconv5_2_mbox_loc_reshape", "=", "Reshape", "(", "(", "-", "1", ",", "4", ")", ",", "name", "=", "'deconv5_2_mbox_loc_reshape'", ")", "(", "deconv5_2_mbox_loc", ")", "\n", "deconv6_2_mbox_loc_reshape", "=", "Reshape", "(", "(", "-", "1", ",", "4", ")", ",", "name", "=", "'deconv6_2_mbox_loc_reshape'", ")", "(", "deconv6_2_mbox_loc", ")", "\n", "conv7_add_mbox_loc_reshape", "=", "Reshape", "(", "(", "-", "1", ",", "4", ")", ",", "name", "=", "'conv7_add_mbox_loc_reshape'", ")", "(", "conv7_add_mbox_loc", ")", "\n", "\n", "# Reshape the anchor box tensors, yielding 3D tensors of shape `(batch, height * width * n_boxes, 8)`", "\n", "deconv3_2_mbox_priorbox_reshape", "=", "Reshape", "(", "(", "-", "1", ",", "8", ")", ",", "name", "=", "'deconv3_2_mbox_priorbox_reshape'", ")", "(", "deconv3_2_mbox_priorbox", ")", "\n", "deconv4_2_mbox_priorbox_reshape", "=", "Reshape", "(", "(", "-", "1", ",", "8", ")", ",", "name", "=", "'deconv4_2_mbox_priorbox_reshape'", ")", "(", "deconv4_2_mbox_priorbox", ")", "\n", "deconv5_2_mbox_priorbox_reshape", "=", "Reshape", "(", "(", "-", "1", ",", "8", ")", ",", "name", "=", "'deconv5_2_mbox_priorbox_reshape'", ")", "(", "deconv5_2_mbox_priorbox", ")", "\n", "deconv6_2_mbox_priorbox_reshape", "=", "Reshape", "(", "(", "-", "1", ",", "8", ")", ",", "name", "=", "'deconv6_2_mbox_priorbox_reshape'", ")", "(", "deconv6_2_mbox_priorbox", ")", "\n", "conv7_add_mbox_priorbox_reshape", "=", "Reshape", "(", "(", "-", "1", ",", "8", ")", ",", "name", "=", "'conv7_add_mbox_priorbox_reshape'", ")", "(", "conv7_add_mbox_priorbox", ")", "\n", "\n", "# Concatenate the predictions from the different layers", "\n", "# Axis 0 (batch) and axis 2 (n_classes or 4, respectively) are identical for all layer predictions,", "\n", "# so we want to concatenate along axis 1, the number of boxes per layer", "\n", "# Output shape of `mbox_conf`: (batch, n_boxes_total, n_classes)", "\n", "mbox_conf", "=", "Concatenate", "(", "axis", "=", "1", ",", "name", "=", "'mbox_conf'", ")", "(", "[", "deconv3_2_mbox_conf_reshape", ",", "\n", "deconv4_2_mbox_conf_reshape", ",", "\n", "deconv5_2_mbox_conf_reshape", ",", "\n", "deconv6_2_mbox_conf_reshape", ",", "\n", "conv7_add_mbox_conf_reshape", "]", ")", "\n", "\n", "# Output shape of `mbox_loc`: (batch, n_boxes_total, 4)", "\n", "mbox_loc", "=", "Concatenate", "(", "axis", "=", "1", ",", "name", "=", "'mbox_loc'", ")", "(", "[", "deconv3_2_mbox_loc_reshape", ",", "\n", "deconv4_2_mbox_loc_reshape", ",", "\n", "deconv5_2_mbox_loc_reshape", ",", "\n", "deconv6_2_mbox_loc_reshape", ",", "\n", "conv7_add_mbox_loc_reshape", "]", ")", "\n", "\n", "# Output shape of `mbox_priorbox`: (batch, n_boxes_total, 8)", "\n", "mbox_priorbox", "=", "Concatenate", "(", "axis", "=", "1", ",", "name", "=", "'mbox_priorbox'", ")", "(", "[", "deconv3_2_mbox_priorbox_reshape", ",", "\n", "deconv4_2_mbox_priorbox_reshape", ",", "\n", "deconv5_2_mbox_priorbox_reshape", ",", "\n", "deconv6_2_mbox_priorbox_reshape", ",", "\n", "conv7_add_mbox_priorbox_reshape", "]", ")", "\n", "\n", "# The box coordinate predictions will go into the loss function just the way they are,", "\n", "# but for the class predictions, we'll apply a softmax activation layer first", "\n", "mbox_conf_softmax", "=", "Activation", "(", "'softmax'", ",", "name", "=", "'mbox_conf_softmax'", ")", "(", "mbox_conf", ")", "\n", "\n", "# Concatenate the class and box predictions and the anchors to one large predictions vector", "\n", "# Output shape of `predictions`: (batch, n_boxes_total, n_classes + 4 + 8)", "\n", "predictions", "=", "Concatenate", "(", "axis", "=", "2", ",", "name", "=", "'predictions'", ")", "(", "[", "mbox_conf_softmax", ",", "mbox_loc", ",", "mbox_priorbox", "]", ")", "\n", "\n", "if", "mode", "==", "'training'", ":", "\n", "        ", "model", "=", "Model", "(", "inputs", "=", "x", ",", "outputs", "=", "predictions", ")", "\n", "", "elif", "mode", "==", "'inference'", ":", "\n", "        ", "decoded_predictions", "=", "DecodeDetections", "(", "confidence_thresh", "=", "confidence_thresh", ",", "\n", "iou_threshold", "=", "iou_threshold", ",", "\n", "top_k", "=", "top_k", ",", "\n", "nms_max_output_size", "=", "nms_max_output_size", ",", "\n", "coords", "=", "coords", ",", "\n", "normalize_coords", "=", "normalize_coords", ",", "\n", "img_height", "=", "img_height", ",", "\n", "img_width", "=", "img_width", ",", "\n", "name", "=", "'decoded_predictions'", ")", "(", "predictions", ")", "\n", "model", "=", "Model", "(", "inputs", "=", "x", ",", "outputs", "=", "decoded_predictions", ")", "\n", "", "elif", "mode", "==", "'inference_fast'", ":", "\n", "        ", "decoded_predictions", "=", "DecodeDetectionsFast", "(", "confidence_thresh", "=", "confidence_thresh", ",", "\n", "iou_threshold", "=", "iou_threshold", ",", "\n", "top_k", "=", "top_k", ",", "\n", "nms_max_output_size", "=", "nms_max_output_size", ",", "\n", "coords", "=", "coords", ",", "\n", "normalize_coords", "=", "normalize_coords", ",", "\n", "img_height", "=", "img_height", ",", "\n", "img_width", "=", "img_width", ",", "\n", "name", "=", "'decoded_predictions'", ")", "(", "predictions", ")", "\n", "model", "=", "Model", "(", "inputs", "=", "x", ",", "outputs", "=", "decoded_predictions", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"`mode` must be one of 'training', 'inference' or 'inference_fast', but received '{}'.\"", ".", "format", "(", "mode", ")", ")", "\n", "\n", "", "if", "return_predictor_sizes", ":", "\n", "        ", "predictor_sizes", "=", "np", ".", "array", "(", "[", "deconv3_2_mbox_conf", ".", "_keras_shape", "[", "1", ":", "3", "]", ",", "\n", "deconv4_2_mbox_conf", ".", "_keras_shape", "[", "1", ":", "3", "]", ",", "\n", "deconv5_2_mbox_conf", ".", "_keras_shape", "[", "1", ":", "3", "]", ",", "\n", "deconv6_2_mbox_conf", ".", "_keras_shape", "[", "1", ":", "3", "]", ",", "\n", "conv7_add_mbox_conf", ".", "_keras_shape", "[", "1", ":", "3", "]", "]", ")", "\n", "return", "model", ",", "predictor_sizes", "\n", "", "else", ":", "\n", "        ", "return", "model", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_loss_function.keras_ssd_loss.SSDLoss.__init__": [[27, 52], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "neg_pos_ratio", "=", "3", ",", "\n", "n_neg_min", "=", "0", ",", "\n", "alpha", "=", "1.0", ")", ":", "\n", "        ", "'''\n        Arguments:\n            neg_pos_ratio (int, optional): The maximum ratio of negative (i.e. background)\n                to positive ground truth boxes to include in the loss computation.\n                There are no actual background ground truth boxes of course, but `y_true`\n                contains anchor boxes labeled with the background class. Since\n                the number of background boxes in `y_true` will usually exceed\n                the number of positive boxes by far, it is necessary to balance\n                their influence on the loss. Defaults to 3 following the paper.\n            n_neg_min (int, optional): The minimum number of negative ground truth boxes to\n                enter the loss computation *per batch*. This argument can be used to make\n                sure that the model learns from a minimum number of negatives in batches\n                in which there are very few, or even none at all, positive ground truth\n                boxes. It defaults to 0 and if used, it should be set to a value that\n                stands in reasonable proportion to the batch size used for training.\n            alpha (float, optional): A factor to weight the localization loss in the\n                computation of the total loss. Defaults to 1.0 following the paper.\n        '''", "\n", "self", ".", "neg_pos_ratio", "=", "neg_pos_ratio", "\n", "self", ".", "n_neg_min", "=", "n_neg_min", "\n", "self", ".", "alpha", "=", "alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_loss_function.keras_ssd_loss.SSDLoss.smooth_L1_loss": [[53, 76], ["tensorflow.abs", "tensorflow.where", "tensorflow.reduce_sum", "tensorflow.less"], "methods", ["None"], ["", "def", "smooth_L1_loss", "(", "self", ",", "y_true", ",", "y_pred", ")", ":", "\n", "        ", "'''\n        Compute smooth L1 loss, see references.\n\n        Arguments:\n            y_true (nD tensor): A TensorFlow tensor of any shape containing the ground truth data.\n                In this context, the expected tensor has shape `(batch_size, #boxes, 4)` and\n                contains the ground truth bounding box coordinates, where the last dimension\n                contains `(xmin, xmax, ymin, ymax)`.\n            y_pred (nD tensor): A TensorFlow tensor of identical structure to `y_true` containing\n                the predicted data, in this context the predicted bounding box coordinates.\n\n        Returns:\n            The smooth L1 loss, a nD-1 Tensorflow tensor. In this context a 2D tensor\n            of shape (batch, n_boxes_total).\n\n        References:\n            https://arxiv.org/abs/1504.08083\n        '''", "\n", "absolute_loss", "=", "tf", ".", "abs", "(", "y_true", "-", "y_pred", ")", "\n", "square_loss", "=", "0.5", "*", "(", "y_true", "-", "y_pred", ")", "**", "2", "\n", "l1_loss", "=", "tf", ".", "where", "(", "tf", ".", "less", "(", "absolute_loss", ",", "1.0", ")", ",", "square_loss", ",", "absolute_loss", "-", "0.5", ")", "\n", "return", "tf", ".", "reduce_sum", "(", "l1_loss", ",", "axis", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_loss_function.keras_ssd_loss.SSDLoss.log_loss": [[77, 97], ["tensorflow.maximum", "tensorflow.reduce_sum", "tensorflow.log"], "methods", ["None"], ["", "def", "log_loss", "(", "self", ",", "y_true", ",", "y_pred", ")", ":", "\n", "        ", "'''\n        Compute the softmax log loss.\n\n        Arguments:\n            y_true (nD tensor): A TensorFlow tensor of any shape containing the ground truth data.\n                In this context, the expected tensor has shape (batch_size, #boxes, #classes)\n                and contains the ground truth bounding box categories.\n            y_pred (nD tensor): A TensorFlow tensor of identical structure to `y_true` containing\n                the predicted data, in this context the predicted bounding box categories.\n\n        Returns:\n            The softmax log loss, a nD-1 Tensorflow tensor. In this context a 2D tensor\n            of shape (batch, n_boxes_total).\n        '''", "\n", "# Make sure that `y_pred` doesn't contain any zeros (which would break the log function)", "\n", "y_pred", "=", "tf", ".", "maximum", "(", "y_pred", ",", "1e-15", ")", "\n", "# Compute the log loss", "\n", "log_loss", "=", "-", "tf", ".", "reduce_sum", "(", "y_true", "*", "tf", ".", "log", "(", "y_pred", ")", ",", "axis", "=", "-", "1", ")", "\n", "return", "log_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_loss_function.keras_ssd_loss.SSDLoss.compute_loss": [[98, 216], ["tensorflow.constant", "tensorflow.constant", "tensorflow.constant", "sample_weight.get_shape", "tensorflow.to_float", "tensorflow.to_float", "tensorflow.to_float", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.count_nonzero", "tensorflow.minimum", "tensorflow.cond", "tensorflow.reduce_sum", "tensorflow.shape", "tensorflow.shape", "keras_ssd_loss.SSDLoss.log_loss", "keras_ssd_loss.SSDLoss.smooth_L1_loss", "tensorflow.reduce_max", "tensorflow.maximum", "tensorflow.zeros", "tensorflow.reshape", "tensorflow.nn.top_k", "tensorflow.scatter_nd", "tensorflow.to_float", "tensorflow.reduce_sum", "tensorflow.equal", "tensorflow.maximum", "tensorflow.to_float", "tensorflow.reshape", "tensorflow.constant", "tensorflow.to_int32", "tensorflow.expand_dims", "tensorflow.ones_like", "tensorflow.shape"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_loss_function.keras_ssd_loss.SSDLoss.log_loss", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_loss_function.keras_ssd_loss.SSDLoss.smooth_L1_loss"], ["", "def", "compute_loss", "(", "self", ",", "y_true", ",", "y_pred", ")", ":", "\n", "        ", "'''\n        Compute the loss of the SSD model prediction against the ground truth.\n\n        Arguments:\n            y_true (array): A Numpy array of shape `(batch_size, #boxes, #classes + 12)`,\n                where `#boxes` is the total number of boxes that the model predicts\n                per image. Be careful to make sure that the index of each given\n                box in `y_true` is the same as the index for the corresponding\n                box in `y_pred`. The last axis must have length `#classes + 12` and contain\n                `[classes one-hot encoded, 4 ground truth box coordinate offsets, 8 arbitrary entries]`\n                in this order, including the background class. The last eight entries of the\n                last axis are not used by this function and therefore their contents are\n                irrelevant, they only exist so that `y_true` has the same shape as `y_pred`,\n                where the last four entries of the last axis contain the anchor box\n                coordinates, which are needed during inference. Important: Boxes that\n                you want the cost function to ignore need to have a one-hot\n                class vector of all zeros.\n            y_pred (Keras tensor): The model prediction. The shape is identical\n                to that of `y_true`, i.e. `(batch_size, #boxes, #classes + 12)`.\n                The last axis must contain entries in the format\n                `[classes one-hot encoded, 4 predicted box coordinate offsets, 8 arbitrary entries]`.\n\n        Returns:\n            A scalar, the total multitask loss for classification and localization.\n        '''", "\n", "self", ".", "neg_pos_ratio", "=", "tf", ".", "constant", "(", "self", ".", "neg_pos_ratio", ")", "\n", "self", ".", "n_neg_min", "=", "tf", ".", "constant", "(", "self", ".", "n_neg_min", ")", "\n", "self", ".", "alpha", "=", "tf", ".", "constant", "(", "self", ".", "alpha", ")", "\n", "# The implement of Sample-Weighted loss", "\n", "sample_weight", "=", "y_true", "[", ":", ",", ":", ",", "0", "]", "\n", "aaa", "=", "sample_weight", ".", "get_shape", "(", ")", "\n", "y_true", "=", "y_true", "[", ":", ",", ":", ",", "1", ":", "]", "\n", "batch_size", "=", "tf", ".", "shape", "(", "y_pred", ")", "[", "0", "]", "# Output dtype: tf.int32", "\n", "n_boxes", "=", "tf", ".", "shape", "(", "y_pred", ")", "[", "1", "]", "# Output dtype: tf.int32, note that `n_boxes` in this context denotes the total number of boxes per image, not the number of boxes per cell.", "\n", "\n", "# 1: Compute the losses for class and box predictions for every box.", "\n", "\n", "classification_loss", "=", "tf", ".", "to_float", "(", "self", ".", "log_loss", "(", "y_true", "[", ":", ",", ":", ",", ":", "-", "12", "]", ",", "y_pred", "[", ":", ",", ":", ",", ":", "-", "12", "]", ")", ")", "# Output shape: (batch_size, n_boxes)", "\n", "classification_loss", "=", "classification_loss", "*", "sample_weight", "\n", "localization_loss", "=", "tf", ".", "to_float", "(", "self", ".", "smooth_L1_loss", "(", "y_true", "[", ":", ",", ":", ",", "-", "12", ":", "-", "8", "]", ",", "y_pred", "[", ":", ",", ":", ",", "-", "12", ":", "-", "8", "]", ")", ")", "# Output shape: (batch_size, n_boxes)", "\n", "localization_loss", "=", "localization_loss", "*", "sample_weight", "\n", "# 2: Compute the classification losses for the positive and negative targets.", "\n", "\n", "# Create masks for the positive and negative ground truth classes.", "\n", "negatives", "=", "y_true", "[", ":", ",", ":", ",", "0", "]", "# Tensor of shape (batch_size, n_boxes)", "\n", "positives", "=", "tf", ".", "to_float", "(", "tf", ".", "reduce_max", "(", "y_true", "[", ":", ",", ":", ",", "1", ":", "-", "12", "]", ",", "axis", "=", "-", "1", ")", ")", "# Tensor of shape (batch_size, n_boxes)", "\n", "\n", "# Count the number of positive boxes (classes 1 to n) in y_true across the whole batch.", "\n", "n_positive", "=", "tf", ".", "reduce_sum", "(", "positives", ")", "\n", "\n", "# Now mask all negative boxes and sum up the losses for the positive boxes PER batch item", "\n", "# (Keras loss functions must output one scalar loss value PER batch item, rather than just", "\n", "# one scalar for the entire batch, that's why we're not summing across all axes).", "\n", "pos_class_loss", "=", "tf", ".", "reduce_sum", "(", "classification_loss", "*", "positives", ",", "axis", "=", "-", "1", ")", "# Tensor of shape (batch_size,)", "\n", "\n", "# Compute the classification loss for the negative default boxes (if there are any).", "\n", "\n", "# First, compute the classification loss for all negative boxes.", "\n", "neg_class_loss_all", "=", "classification_loss", "*", "negatives", "# Tensor of shape (batch_size, n_boxes)", "\n", "n_neg_losses", "=", "tf", ".", "count_nonzero", "(", "neg_class_loss_all", ",", "dtype", "=", "tf", ".", "int32", ")", "# The number of non-zero loss entries in `neg_class_loss_all`", "\n", "# What's the point of `n_neg_losses`? For the next step, which will be to compute which negative boxes enter the classification", "\n", "# loss, we don't just want to know how many negative ground truth boxes there are, but for how many of those there actually is", "\n", "# a positive (i.e. non-zero) loss. This is necessary because `tf.nn.top-k()` in the function below will pick the top k boxes with", "\n", "# the highest losses no matter what, even if it receives a vector where all losses are zero. In the unlikely event that all negative", "\n", "# classification losses ARE actually zero though, this behavior might lead to `tf.nn.top-k()` returning the indices of positive", "\n", "# boxes, leading to an incorrect negative classification loss computation, and hence an incorrect overall loss computation.", "\n", "# We therefore need to make sure that `n_negative_keep`, which assumes the role of the `k` argument in `tf.nn.top-k()`,", "\n", "# is at most the number of negative boxes for which there is a positive classification loss.", "\n", "\n", "# Compute the number of negative examples we want to account for in the loss.", "\n", "# We'll keep at most `self.neg_pos_ratio` times the number of positives in `y_true`, but at least `self.n_neg_min` (unless `n_neg_loses` is smaller).", "\n", "n_negative_keep", "=", "tf", ".", "minimum", "(", "tf", ".", "maximum", "(", "self", ".", "neg_pos_ratio", "*", "tf", ".", "to_int32", "(", "n_positive", ")", ",", "self", ".", "n_neg_min", ")", ",", "n_neg_losses", ")", "\n", "\n", "# In the unlikely case when either (1) there are no negative ground truth boxes at all", "\n", "# or (2) the classification loss for all negative boxes is zero, return zero as the `neg_class_loss`.", "\n", "def", "f1", "(", ")", ":", "\n", "            ", "return", "tf", ".", "zeros", "(", "[", "batch_size", "]", ")", "\n", "# Otherwise compute the negative loss.", "\n", "", "def", "f2", "(", ")", ":", "\n", "# Now we'll identify the top-k (where k == `n_negative_keep`) boxes with the highest confidence loss that", "\n", "# belong to the background class in the ground truth data. Note that this doesn't necessarily mean that the model", "\n", "# predicted the wrong class for those boxes, it just means that the loss for those boxes is the highest.", "\n", "\n", "# To do this, we reshape `neg_class_loss_all` to 1D...", "\n", "            ", "neg_class_loss_all_1D", "=", "tf", ".", "reshape", "(", "neg_class_loss_all", ",", "[", "-", "1", "]", ")", "# Tensor of shape (batch_size * n_boxes,)", "\n", "# ...and then we get the indices for the `n_negative_keep` boxes with the highest loss out of those...", "\n", "values", ",", "indices", "=", "tf", ".", "nn", ".", "top_k", "(", "neg_class_loss_all_1D", ",", "\n", "k", "=", "n_negative_keep", ",", "\n", "sorted", "=", "False", ")", "# We don't need them sorted.", "\n", "# ...and with these indices we'll create a mask...", "\n", "negatives_keep", "=", "tf", ".", "scatter_nd", "(", "indices", "=", "tf", ".", "expand_dims", "(", "indices", ",", "axis", "=", "1", ")", ",", "\n", "updates", "=", "tf", ".", "ones_like", "(", "indices", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "\n", "shape", "=", "tf", ".", "shape", "(", "neg_class_loss_all_1D", ")", ")", "# Tensor of shape (batch_size * n_boxes,)", "\n", "negatives_keep", "=", "tf", ".", "to_float", "(", "tf", ".", "reshape", "(", "negatives_keep", ",", "[", "batch_size", ",", "n_boxes", "]", ")", ")", "# Tensor of shape (batch_size, n_boxes)", "\n", "# ...and use it to keep only those boxes and mask all other classification losses", "\n", "neg_class_loss", "=", "tf", ".", "reduce_sum", "(", "classification_loss", "*", "negatives_keep", ",", "axis", "=", "-", "1", ")", "# Tensor of shape (batch_size,)", "\n", "return", "neg_class_loss", "\n", "\n", "", "neg_class_loss", "=", "tf", ".", "cond", "(", "tf", ".", "equal", "(", "n_neg_losses", ",", "tf", ".", "constant", "(", "0", ")", ")", ",", "f1", ",", "f2", ")", "\n", "\n", "class_loss", "=", "pos_class_loss", "+", "neg_class_loss", "# Tensor of shape (batch_size,)", "\n", "\n", "# 3: Compute the localization loss for the positive targets.", "\n", "#    We don't compute a localization loss for negative predicted boxes (obviously: there are no ground truth boxes they would correspond to).", "\n", "\n", "loc_loss", "=", "tf", ".", "reduce_sum", "(", "localization_loss", "*", "positives", ",", "axis", "=", "-", "1", ")", "# Tensor of shape (batch_size,)", "\n", "\n", "# 4: Compute the total loss.", "\n", "\n", "total_loss", "=", "(", "class_loss", "+", "self", ".", "alpha", "*", "loc_loss", ")", "/", "tf", ".", "maximum", "(", "1.0", ",", "n_positive", ")", "# In case `n_positive == 0`", "\n", "# Keras has the annoying habit of dividing the loss by the batch size, which sucks in our case", "\n", "# because the relevant criterion to average our loss over is the number of positive boxes in the batch", "\n", "# (by which we're dividing in the line above), not the batch size. So in order to revert Keras' averaging", "\n", "# over the batch size, we'll have to multiply by it.", "\n", "total_loss", "=", "total_loss", "*", "tf", ".", "to_float", "(", "batch_size", ")", "\n", "\n", "return", "total_loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.data_augmentation_chain_variable_input_size.DataAugmentationVariableInputSize.__init__": [[39, 138], ["data_generator.object_detection_2d_image_boxes_validation_utils.BoxFilter", "data_generator.object_detection_2d_image_boxes_validation_utils.BoxFilter", "data_generator.object_detection_2d_image_boxes_validation_utils.ImageValidator", "data_generator.object_detection_2d_photometric_ops.ConvertTo3Channels", "data_generator.object_detection_2d_photometric_ops.ConvertColor", "data_generator.object_detection_2d_photometric_ops.ConvertColor", "data_generator.object_detection_2d_photometric_ops.ConvertDataType", "data_generator.object_detection_2d_photometric_ops.ConvertDataType", "data_generator.object_detection_2d_geometric_ops.Resize", "data_generator.object_detection_2d_photometric_ops.RandomBrightness", "data_generator.object_detection_2d_photometric_ops.RandomContrast", "data_generator.object_detection_2d_photometric_ops.RandomSaturation", "data_generator.object_detection_2d_photometric_ops.RandomHue", "data_generator.object_detection_2d_geometric_ops.RandomFlip", "data_generator.object_detection_2d_patch_sampling_ops.PatchCoordinateGenerator", "data_generator.object_detection_2d_patch_sampling_ops.RandomPatch"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "resize_height", ",", "\n", "resize_width", ",", "\n", "random_brightness", "=", "(", "-", "48", ",", "48", ",", "0.5", ")", ",", "\n", "random_contrast", "=", "(", "0.5", ",", "1.8", ",", "0.5", ")", ",", "\n", "random_saturation", "=", "(", "0.5", ",", "1.8", ",", "0.5", ")", ",", "\n", "random_hue", "=", "(", "18", ",", "0.5", ")", ",", "\n", "random_flip", "=", "0.5", ",", "\n", "min_scale", "=", "0.3", ",", "\n", "max_scale", "=", "2.0", ",", "\n", "min_aspect_ratio", "=", "0.5", ",", "\n", "max_aspect_ratio", "=", "2.0", ",", "\n", "n_trials_max", "=", "3", ",", "\n", "clip_boxes", "=", "True", ",", "\n", "overlap_criterion", "=", "'area'", ",", "\n", "bounds_box_filter", "=", "(", "0.3", ",", "1.0", ")", ",", "\n", "bounds_validator", "=", "(", "0.5", ",", "1.0", ")", ",", "\n", "n_boxes_min", "=", "1", ",", "\n", "background", "=", "(", "0", ",", "0", ",", "0", ")", ",", "\n", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ")", ":", "\n", "\n", "        ", "self", ".", "n_trials_max", "=", "n_trials_max", "\n", "self", ".", "clip_boxes", "=", "clip_boxes", "\n", "self", ".", "overlap_criterion", "=", "overlap_criterion", "\n", "self", ".", "bounds_box_filter", "=", "bounds_box_filter", "\n", "self", ".", "bounds_validator", "=", "bounds_validator", "\n", "self", ".", "n_boxes_min", "=", "n_boxes_min", "\n", "self", ".", "background", "=", "background", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "\n", "# Determines which boxes are kept in an image after the transformations have been applied.", "\n", "self", ".", "box_filter_patch", "=", "BoxFilter", "(", "check_overlap", "=", "True", ",", "\n", "check_min_area", "=", "False", ",", "\n", "check_degenerate", "=", "False", ",", "\n", "overlap_criterion", "=", "self", ".", "overlap_criterion", ",", "\n", "overlap_bounds", "=", "self", ".", "bounds_box_filter", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n", "self", ".", "box_filter_resize", "=", "BoxFilter", "(", "check_overlap", "=", "False", ",", "\n", "check_min_area", "=", "True", ",", "\n", "check_degenerate", "=", "True", ",", "\n", "min_area", "=", "16", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n", "# Determines whether the result of the transformations is a valid training image.", "\n", "self", ".", "image_validator", "=", "ImageValidator", "(", "overlap_criterion", "=", "self", ".", "overlap_criterion", ",", "\n", "bounds", "=", "self", ".", "bounds_validator", ",", "\n", "n_boxes_min", "=", "self", ".", "n_boxes_min", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n", "# Utility transformations", "\n", "self", ".", "convert_to_3_channels", "=", "ConvertTo3Channels", "(", ")", "# Make sure all images end up having 3 channels.", "\n", "self", ".", "convert_RGB_to_HSV", "=", "ConvertColor", "(", "current", "=", "'RGB'", ",", "to", "=", "'HSV'", ")", "\n", "self", ".", "convert_HSV_to_RGB", "=", "ConvertColor", "(", "current", "=", "'HSV'", ",", "to", "=", "'RGB'", ")", "\n", "self", ".", "convert_to_float32", "=", "ConvertDataType", "(", "to", "=", "'float32'", ")", "\n", "self", ".", "convert_to_uint8", "=", "ConvertDataType", "(", "to", "=", "'uint8'", ")", "\n", "self", ".", "resize", "=", "Resize", "(", "height", "=", "resize_height", ",", "\n", "width", "=", "resize_width", ",", "\n", "box_filter", "=", "self", ".", "box_filter_resize", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n", "# Photometric transformations", "\n", "self", ".", "random_brightness", "=", "RandomBrightness", "(", "lower", "=", "random_brightness", "[", "0", "]", ",", "upper", "=", "random_brightness", "[", "1", "]", ",", "prob", "=", "random_brightness", "[", "2", "]", ")", "\n", "self", ".", "random_contrast", "=", "RandomContrast", "(", "lower", "=", "random_contrast", "[", "0", "]", ",", "upper", "=", "random_contrast", "[", "1", "]", ",", "prob", "=", "random_contrast", "[", "2", "]", ")", "\n", "self", ".", "random_saturation", "=", "RandomSaturation", "(", "lower", "=", "random_saturation", "[", "0", "]", ",", "upper", "=", "random_saturation", "[", "1", "]", ",", "prob", "=", "random_saturation", "[", "2", "]", ")", "\n", "self", ".", "random_hue", "=", "RandomHue", "(", "max_delta", "=", "random_hue", "[", "0", "]", ",", "prob", "=", "random_hue", "[", "1", "]", ")", "\n", "\n", "# Geometric transformations", "\n", "self", ".", "random_flip", "=", "RandomFlip", "(", "dim", "=", "'horizontal'", ",", "prob", "=", "random_flip", ",", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "self", ".", "patch_coord_generator", "=", "PatchCoordinateGenerator", "(", "must_match", "=", "'w_ar'", ",", "\n", "min_scale", "=", "min_scale", ",", "\n", "max_scale", "=", "max_scale", ",", "\n", "scale_uniformly", "=", "False", ",", "\n", "min_aspect_ratio", "=", "min_aspect_ratio", ",", "\n", "max_aspect_ratio", "=", "max_aspect_ratio", ")", "\n", "self", ".", "random_patch", "=", "RandomPatch", "(", "patch_coord_generator", "=", "self", ".", "patch_coord_generator", ",", "\n", "box_filter", "=", "self", ".", "box_filter_patch", ",", "\n", "image_validator", "=", "self", ".", "image_validator", ",", "\n", "n_trials_max", "=", "self", ".", "n_trials_max", ",", "\n", "clip_boxes", "=", "self", ".", "clip_boxes", ",", "\n", "prob", "=", "1.0", ",", "\n", "can_fail", "=", "False", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n", "# Define the processing chain", "\n", "self", ".", "transformations", "=", "[", "self", ".", "convert_to_3_channels", ",", "\n", "self", ".", "convert_to_float32", ",", "\n", "self", ".", "random_brightness", ",", "\n", "self", ".", "random_contrast", ",", "\n", "self", ".", "convert_to_uint8", ",", "\n", "self", ".", "convert_RGB_to_HSV", ",", "\n", "self", ".", "convert_to_float32", ",", "\n", "self", ".", "random_saturation", ",", "\n", "self", ".", "random_hue", ",", "\n", "self", ".", "convert_to_uint8", ",", "\n", "self", ".", "convert_HSV_to_RGB", ",", "\n", "self", ".", "random_patch", ",", "\n", "self", ".", "random_flip", ",", "\n", "self", ".", "resize", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.data_augmentation_chain_variable_input_size.DataAugmentationVariableInputSize.__call__": [[139, 153], ["transform", "transform"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "\n", "        ", "self", ".", "random_patch", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "self", ".", "random_flip", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "self", ".", "resize", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "\n", "if", "not", "(", "labels", "is", "None", ")", ":", "\n", "            ", "for", "transform", "in", "self", ".", "transformations", ":", "\n", "                ", "image", ",", "labels", "=", "transform", "(", "image", ",", "labels", ")", "\n", "", "return", "image", ",", "labels", "\n", "", "else", ":", "\n", "            ", "for", "transform", "in", "self", ".", "sequence1", ":", "\n", "                ", "image", "=", "transform", "(", "image", ")", "\n", "", "return", "image", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_geometric_ops.Resize.__init__": [[32, 60], ["ValueError", "isinstance"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "height", ",", "\n", "width", ",", "\n", "interpolation_mode", "=", "cv2", ".", "INTER_LINEAR", ",", "\n", "box_filter", "=", "None", ",", "\n", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ")", ":", "\n", "        ", "'''\n        Arguments:\n            height (int): The desired height of the output images in pixels.\n            width (int): The desired width of the output images in pixels.\n            interpolation_mode (int, optional): An integer that denotes a valid\n                OpenCV interpolation mode. For example, integers 0 through 5 are\n                valid interpolation modes.\n            box_filter (BoxFilter, optional): Only relevant if ground truth bounding boxes are given.\n                A `BoxFilter` object to filter out bounding boxes that don't meet the given criteria\n                after the transformation. Refer to the `BoxFilter` documentation for details. If `None`,\n                the validity of the bounding boxes is not checked.\n            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n        '''", "\n", "if", "not", "(", "isinstance", "(", "box_filter", ",", "BoxFilter", ")", "or", "box_filter", "is", "None", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`box_filter` must be either `None` or a `BoxFilter` object.\"", ")", "\n", "", "self", ".", "out_height", "=", "height", "\n", "self", ".", "out_width", "=", "width", "\n", "self", ".", "interpolation_mode", "=", "interpolation_mode", "\n", "self", ".", "box_filter", "=", "box_filter", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_geometric_ops.Resize.__call__": [[61, 102], ["cv2.resize", "numpy.copy", "numpy.round", "numpy.round", "numpy.copy", "numpy.round", "numpy.round", "object_detection_2d_geometric_ops.Resize.box_filter"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ",", "return_inverter", "=", "False", ")", ":", "\n", "\n", "        ", "img_height", ",", "img_width", "=", "image", ".", "shape", "[", ":", "2", "]", "\n", "xmin", "=", "self", ".", "labels_format", "[", "'xmin'", "]", "\n", "ymin", "=", "self", ".", "labels_format", "[", "'ymin'", "]", "\n", "xmax", "=", "self", ".", "labels_format", "[", "'xmax'", "]", "\n", "ymax", "=", "self", ".", "labels_format", "[", "'ymax'", "]", "\n", "\n", "image", "=", "cv2", ".", "resize", "(", "image", ",", "\n", "dsize", "=", "(", "self", ".", "out_width", ",", "self", ".", "out_height", ")", ",", "\n", "interpolation", "=", "self", ".", "interpolation_mode", ")", "\n", "\n", "if", "return_inverter", ":", "\n", "            ", "def", "inverter", "(", "labels", ")", ":", "\n", "                ", "labels", "=", "np", ".", "copy", "(", "labels", ")", "\n", "labels", "[", ":", ",", "[", "ymin", ",", "ymax", "]", "]", "=", "np", ".", "round", "(", "labels", "[", ":", ",", "[", "ymin", ",", "ymax", "]", "]", "*", "(", "img_height", "/", "self", ".", "out_height", ")", ",", "decimals", "=", "0", ")", "\n", "labels", "[", ":", ",", "[", "xmin", ",", "xmax", "]", "]", "=", "np", ".", "round", "(", "labels", "[", ":", ",", "[", "xmin", ",", "xmax", "]", "]", "*", "(", "img_width", "/", "self", ".", "out_width", ")", ",", "decimals", "=", "0", ")", "\n", "# labels[:, [ymin+1, ymax+1]] = np.round(labels[:, [ymin+1, ymax+1]] * (img_height / self.out_height), decimals=0)", "\n", "# labels[:, [xmin+1, xmax+1]] = np.round(labels[:, [xmin+1, xmax+1]] * (img_width / self.out_width), decimals=0)", "\n", "return", "labels", "\n", "\n", "", "", "if", "labels", "is", "None", ":", "\n", "            ", "if", "return_inverter", ":", "\n", "                ", "return", "image", ",", "inverter", "\n", "", "else", ":", "\n", "                ", "return", "image", "\n", "", "", "else", ":", "\n", "            ", "labels", "=", "np", ".", "copy", "(", "labels", ")", "\n", "labels", "[", ":", ",", "[", "ymin", ",", "ymax", "]", "]", "=", "np", ".", "round", "(", "labels", "[", ":", ",", "[", "ymin", ",", "ymax", "]", "]", "*", "(", "self", ".", "out_height", "/", "img_height", ")", ",", "decimals", "=", "0", ")", "\n", "labels", "[", ":", ",", "[", "xmin", ",", "xmax", "]", "]", "=", "np", ".", "round", "(", "labels", "[", ":", ",", "[", "xmin", ",", "xmax", "]", "]", "*", "(", "self", ".", "out_width", "/", "img_width", ")", ",", "decimals", "=", "0", ")", "\n", "\n", "if", "not", "(", "self", ".", "box_filter", "is", "None", ")", ":", "\n", "                ", "self", ".", "box_filter", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "labels", "=", "self", ".", "box_filter", "(", "labels", "=", "labels", ",", "\n", "image_height", "=", "self", ".", "out_height", ",", "\n", "image_width", "=", "self", ".", "out_width", ")", "\n", "\n", "", "if", "return_inverter", ":", "\n", "                ", "return", "image", ",", "labels", ",", "inverter", "\n", "", "else", ":", "\n", "                ", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_geometric_ops.ResizeRandomInterp.__init__": [[109, 145], ["object_detection_2d_geometric_ops.Resize", "isinstance", "ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "height", ",", "\n", "width", ",", "\n", "interpolation_modes", "=", "[", "cv2", ".", "INTER_NEAREST", ",", "\n", "cv2", ".", "INTER_LINEAR", ",", "\n", "cv2", ".", "INTER_CUBIC", ",", "\n", "cv2", ".", "INTER_AREA", ",", "\n", "cv2", ".", "INTER_LANCZOS4", "]", ",", "\n", "box_filter", "=", "None", ",", "\n", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ")", ":", "\n", "        ", "'''\n        Arguments:\n            height (int): The desired height of the output image in pixels.\n            width (int): The desired width of the output image in pixels.\n            interpolation_modes (list/tuple, optional): A list/tuple of integers\n                that represent valid OpenCV interpolation modes. For example,\n                integers 0 through 5 are valid interpolation modes.\n            box_filter (BoxFilter, optional): Only relevant if ground truth bounding boxes are given.\n                A `BoxFilter` object to filter out bounding boxes that don't meet the given criteria\n                after the transformation. Refer to the `BoxFilter` documentation for details. If `None`,\n                the validity of the bounding boxes is not checked.\n            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n        '''", "\n", "if", "not", "(", "isinstance", "(", "interpolation_modes", ",", "(", "list", ",", "tuple", ")", ")", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`interpolation_mode` must be a list or tuple.\"", ")", "\n", "", "self", ".", "height", "=", "height", "\n", "self", ".", "width", "=", "width", "\n", "self", ".", "interpolation_modes", "=", "interpolation_modes", "\n", "self", ".", "box_filter", "=", "box_filter", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "self", ".", "resize", "=", "Resize", "(", "height", "=", "self", ".", "height", ",", "\n", "width", "=", "self", ".", "width", ",", "\n", "box_filter", "=", "self", ".", "box_filter", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_geometric_ops.ResizeRandomInterp.__call__": [[146, 150], ["numpy.random.choice", "object_detection_2d_geometric_ops.ResizeRandomInterp.resize"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ",", "return_inverter", "=", "False", ")", ":", "\n", "        ", "self", ".", "resize", ".", "interpolation_mode", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "interpolation_modes", ")", "\n", "self", ".", "resize", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "return", "self", ".", "resize", "(", "image", ",", "labels", ",", "return_inverter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_geometric_ops.Flip.__init__": [[155, 171], ["ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "dim", "=", "'horizontal'", ",", "\n", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ")", ":", "\n", "        ", "'''\n        Arguments:\n            dim (str, optional): Can be either of 'horizontal' and 'vertical'.\n                If 'horizontal', images will be flipped horizontally, i.e. along\n                the vertical axis. If 'horizontal', images will be flipped vertically,\n                i.e. along the horizontal axis.\n            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n        '''", "\n", "if", "not", "(", "dim", "in", "{", "'horizontal'", ",", "'vertical'", "}", ")", ":", "raise", "ValueError", "(", "\"`dim` can be one of 'horizontal' and 'vertical'.\"", ")", "\n", "self", ".", "dim", "=", "dim", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_geometric_ops.Flip.__call__": [[172, 197], ["numpy.copy", "numpy.copy"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ",", "return_inverter", "=", "False", ")", ":", "\n", "\n", "        ", "img_height", ",", "img_width", "=", "image", ".", "shape", "[", ":", "2", "]", "\n", "\n", "xmin", "=", "self", ".", "labels_format", "[", "'xmin'", "]", "\n", "ymin", "=", "self", ".", "labels_format", "[", "'ymin'", "]", "\n", "xmax", "=", "self", ".", "labels_format", "[", "'xmax'", "]", "\n", "ymax", "=", "self", ".", "labels_format", "[", "'ymax'", "]", "\n", "\n", "if", "self", ".", "dim", "==", "'horizontal'", ":", "\n", "            ", "image", "=", "image", "[", ":", ",", ":", ":", "-", "1", "]", "\n", "if", "labels", "is", "None", ":", "\n", "                ", "return", "image", "\n", "", "else", ":", "\n", "                ", "labels", "=", "np", ".", "copy", "(", "labels", ")", "\n", "labels", "[", ":", ",", "[", "xmin", ",", "xmax", "]", "]", "=", "img_width", "-", "labels", "[", ":", ",", "[", "xmax", ",", "xmin", "]", "]", "\n", "return", "image", ",", "labels", "\n", "", "", "else", ":", "\n", "            ", "image", "=", "image", "[", ":", ":", "-", "1", "]", "\n", "if", "labels", "is", "None", ":", "\n", "                ", "return", "image", "\n", "", "else", ":", "\n", "                ", "labels", "=", "np", ".", "copy", "(", "labels", ")", "\n", "labels", "[", ":", ",", "[", "ymin", ",", "ymax", "]", "]", "=", "img_height", "-", "labels", "[", ":", ",", "[", "ymax", ",", "ymin", "]", "]", "\n", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_geometric_ops.RandomFlip.__init__": [[203, 223], ["object_detection_2d_geometric_ops.Flip"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "dim", "=", "'horizontal'", ",", "\n", "prob", "=", "0.5", ",", "\n", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ")", ":", "\n", "        ", "'''\n        Arguments:\n            dim (str, optional): Can be either of 'horizontal' and 'vertical'.\n                If 'horizontal', images will be flipped horizontally, i.e. along\n                the vertical axis. If 'horizontal', images will be flipped vertically,\n                i.e. along the horizontal axis.\n            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n                unaltered image is returned.\n            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n        '''", "\n", "self", ".", "dim", "=", "dim", "\n", "self", ".", "prob", "=", "prob", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "self", ".", "flip", "=", "Flip", "(", "dim", "=", "self", ".", "dim", ",", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_geometric_ops.RandomFlip.__call__": [[224, 233], ["numpy.random.uniform", "object_detection_2d_geometric_ops.RandomFlip.flip"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "        ", "p", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "\n", "if", "p", ">=", "(", "1.0", "-", "self", ".", "prob", ")", ":", "\n", "            ", "self", ".", "flip", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "return", "self", ".", "flip", "(", "image", ",", "labels", ")", "\n", "", "elif", "labels", "is", "None", ":", "\n", "            ", "return", "image", "\n", "", "else", ":", "\n", "            ", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_geometric_ops.Translate.__init__": [[239, 276], ["ValueError", "isinstance"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "dy", ",", "\n", "dx", ",", "\n", "clip_boxes", "=", "True", ",", "\n", "box_filter", "=", "None", ",", "\n", "background", "=", "(", "0", ",", "0", ",", "0", ")", ",", "\n", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ")", ":", "\n", "        ", "'''\n        Arguments:\n            dy (float): The fraction of the image height by which to translate images along the\n                vertical axis. Positive values translate images downwards, negative values\n                translate images upwards.\n            dx (float): The fraction of the image width by which to translate images along the\n                horizontal axis. Positive values translate images to the right, negative values\n                translate images to the left.\n            clip_boxes (bool, optional): Only relevant if ground truth bounding boxes are given.\n                If `True`, any ground truth bounding boxes will be clipped to lie entirely within the\n                image after the translation.\n            box_filter (BoxFilter, optional): Only relevant if ground truth bounding boxes are given.\n                A `BoxFilter` object to filter out bounding boxes that don't meet the given criteria\n                after the transformation. Refer to the `BoxFilter` documentation for details. If `None`,\n                the validity of the bounding boxes is not checked.\n            background (list/tuple, optional): A 3-tuple specifying the RGB color value of the\n                background pixels of the translated images.\n            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n        '''", "\n", "\n", "if", "not", "(", "isinstance", "(", "box_filter", ",", "BoxFilter", ")", "or", "box_filter", "is", "None", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`box_filter` must be either `None` or a `BoxFilter` object.\"", ")", "\n", "", "self", ".", "dy_rel", "=", "dy", "\n", "self", ".", "dx_rel", "=", "dx", "\n", "self", ".", "clip_boxes", "=", "clip_boxes", "\n", "self", ".", "box_filter", "=", "box_filter", "\n", "self", ".", "background", "=", "background", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_geometric_ops.Translate.__call__": [[277, 319], ["int", "int", "numpy.float32", "cv2.warpAffine", "round", "round", "numpy.copy", "object_detection_2d_geometric_ops.Translate.box_filter", "numpy.clip", "numpy.clip"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "\n", "        ", "img_height", ",", "img_width", "=", "image", ".", "shape", "[", ":", "2", "]", "\n", "\n", "# Compute the translation matrix.", "\n", "dy_abs", "=", "int", "(", "round", "(", "img_height", "*", "self", ".", "dy_rel", ")", ")", "\n", "dx_abs", "=", "int", "(", "round", "(", "img_width", "*", "self", ".", "dx_rel", ")", ")", "\n", "M", "=", "np", ".", "float32", "(", "[", "[", "1", ",", "0", ",", "dx_abs", "]", ",", "\n", "[", "0", ",", "1", ",", "dy_abs", "]", "]", ")", "\n", "\n", "# Translate the image.", "\n", "image", "=", "cv2", ".", "warpAffine", "(", "image", ",", "\n", "M", "=", "M", ",", "\n", "dsize", "=", "(", "img_width", ",", "img_height", ")", ",", "\n", "borderMode", "=", "cv2", ".", "BORDER_CONSTANT", ",", "\n", "borderValue", "=", "self", ".", "background", ")", "\n", "\n", "if", "labels", "is", "None", ":", "\n", "            ", "return", "image", "\n", "", "else", ":", "\n", "            ", "xmin", "=", "self", ".", "labels_format", "[", "'xmin'", "]", "\n", "ymin", "=", "self", ".", "labels_format", "[", "'ymin'", "]", "\n", "xmax", "=", "self", ".", "labels_format", "[", "'xmax'", "]", "\n", "ymax", "=", "self", ".", "labels_format", "[", "'ymax'", "]", "\n", "\n", "labels", "=", "np", ".", "copy", "(", "labels", ")", "\n", "# Translate the box coordinates to the translated image's coordinate system.", "\n", "labels", "[", ":", ",", "[", "xmin", ",", "xmax", "]", "]", "+=", "dx_abs", "\n", "labels", "[", ":", ",", "[", "ymin", ",", "ymax", "]", "]", "+=", "dy_abs", "\n", "\n", "# Compute all valid boxes for this patch.", "\n", "if", "not", "(", "self", ".", "box_filter", "is", "None", ")", ":", "\n", "                ", "self", ".", "box_filter", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "labels", "=", "self", ".", "box_filter", "(", "labels", "=", "labels", ",", "\n", "image_height", "=", "img_height", ",", "\n", "image_width", "=", "img_width", ")", "\n", "\n", "", "if", "self", ".", "clip_boxes", ":", "\n", "                ", "labels", "[", ":", ",", "[", "ymin", ",", "ymax", "]", "]", "=", "np", ".", "clip", "(", "labels", "[", ":", ",", "[", "ymin", ",", "ymax", "]", "]", ",", "a_min", "=", "0", ",", "a_max", "=", "img_height", "-", "1", ")", "\n", "labels", "[", ":", ",", "[", "xmin", ",", "xmax", "]", "]", "=", "np", ".", "clip", "(", "labels", "[", ":", ",", "[", "xmin", ",", "xmax", "]", "]", ",", "a_min", "=", "0", ",", "a_max", "=", "img_width", "-", "1", ")", "\n", "\n", "", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_geometric_ops.RandomTranslate.__init__": [[325, 393], ["object_detection_2d_geometric_ops.Translate", "ValueError", "ValueError", "ValueError", "ValueError", "isinstance"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "dy_minmax", "=", "(", "0.03", ",", "0.3", ")", ",", "\n", "dx_minmax", "=", "(", "0.03", ",", "0.3", ")", ",", "\n", "prob", "=", "0.5", ",", "\n", "clip_boxes", "=", "True", ",", "\n", "box_filter", "=", "None", ",", "\n", "image_validator", "=", "None", ",", "\n", "n_trials_max", "=", "3", ",", "\n", "background", "=", "(", "0", ",", "0", ",", "0", ")", ",", "\n", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ")", ":", "\n", "        ", "'''\n        Arguments:\n            dy_minmax (list/tuple, optional): A 2-tuple `(min, max)` of non-negative floats that\n                determines the minimum and maximum relative translation of images along the vertical\n                axis both upward and downward. That is, images will be randomly translated by at least\n                `min` and at most `max` either upward or downward. For example, if `dy_minmax == (0.05,0.3)`,\n                an image of size `(100,100)` will be translated by at least 5 and at most 30 pixels\n                either upward or downward. The translation direction is chosen randomly.\n            dx_minmax (list/tuple, optional): A 2-tuple `(min, max)` of non-negative floats that\n                determines the minimum and maximum relative translation of images along the horizontal\n                axis both to the left and right. That is, images will be randomly translated by at least\n                `min` and at most `max` either left or right. For example, if `dx_minmax == (0.05,0.3)`,\n                an image of size `(100,100)` will be translated by at least 5 and at most 30 pixels\n                either left or right. The translation direction is chosen randomly.\n            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n                unaltered image is returned.\n            clip_boxes (bool, optional): Only relevant if ground truth bounding boxes are given.\n                If `True`, any ground truth bounding boxes will be clipped to lie entirely within the\n                image after the translation.\n            box_filter (BoxFilter, optional): Only relevant if ground truth bounding boxes are given.\n                A `BoxFilter` object to filter out bounding boxes that don't meet the given criteria\n                after the transformation. Refer to the `BoxFilter` documentation for details. If `None`,\n                the validity of the bounding boxes is not checked.\n            image_validator (ImageValidator, optional): Only relevant if ground truth bounding boxes are given.\n                An `ImageValidator` object to determine whether a translated image is valid. If `None`,\n                any outcome is valid.\n            n_trials_max (int, optional): Only relevant if ground truth bounding boxes are given.\n                Determines the maxmial number of trials to produce a valid image. If no valid image could\n                be produced in `n_trials_max` trials, returns the unaltered input image.\n            background (list/tuple, optional): A 3-tuple specifying the RGB color value of the\n                background pixels of the translated images.\n            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n        '''", "\n", "if", "dy_minmax", "[", "0", "]", ">", "dy_minmax", "[", "1", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\"It must be `dy_minmax[0] <= dy_minmax[1]`.\"", ")", "\n", "", "if", "dx_minmax", "[", "0", "]", ">", "dx_minmax", "[", "1", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\"It must be `dx_minmax[0] <= dx_minmax[1]`.\"", ")", "\n", "", "if", "dy_minmax", "[", "0", "]", "<", "0", "or", "dx_minmax", "[", "0", "]", "<", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\"It must be `dy_minmax[0] >= 0` and `dx_minmax[0] >= 0`.\"", ")", "\n", "", "if", "not", "(", "isinstance", "(", "image_validator", ",", "ImageValidator", ")", "or", "image_validator", "is", "None", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`image_validator` must be either `None` or an `ImageValidator` object.\"", ")", "\n", "", "self", ".", "dy_minmax", "=", "dy_minmax", "\n", "self", ".", "dx_minmax", "=", "dx_minmax", "\n", "self", ".", "prob", "=", "prob", "\n", "self", ".", "clip_boxes", "=", "clip_boxes", "\n", "self", ".", "box_filter", "=", "box_filter", "\n", "self", ".", "image_validator", "=", "image_validator", "\n", "self", ".", "n_trials_max", "=", "n_trials_max", "\n", "self", ".", "background", "=", "background", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "self", ".", "translate", "=", "Translate", "(", "dy", "=", "0", ",", "\n", "dx", "=", "0", ",", "\n", "clip_boxes", "=", "self", ".", "clip_boxes", ",", "\n", "box_filter", "=", "self", ".", "box_filter", ",", "\n", "background", "=", "self", ".", "background", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_geometric_ops.RandomTranslate.__call__": [[394, 449], ["numpy.random.uniform", "range", "max", "numpy.random.uniform", "numpy.random.uniform", "numpy.random.choice", "numpy.random.choice", "object_detection_2d_geometric_ops.RandomTranslate.translate", "numpy.copy", "int", "int", "object_detection_2d_geometric_ops.RandomTranslate.image_validator", "round", "round", "object_detection_2d_geometric_ops.RandomTranslate.translate"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "\n", "        ", "p", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "\n", "if", "p", ">=", "(", "1.0", "-", "self", ".", "prob", ")", ":", "\n", "\n", "            ", "img_height", ",", "img_width", "=", "image", ".", "shape", "[", ":", "2", "]", "\n", "\n", "xmin", "=", "self", ".", "labels_format", "[", "'xmin'", "]", "\n", "ymin", "=", "self", ".", "labels_format", "[", "'ymin'", "]", "\n", "xmax", "=", "self", ".", "labels_format", "[", "'xmax'", "]", "\n", "ymax", "=", "self", ".", "labels_format", "[", "'ymax'", "]", "\n", "\n", "# Override the preset labels format.", "\n", "if", "not", "self", ".", "image_validator", "is", "None", ":", "\n", "                ", "self", ".", "image_validator", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "", "self", ".", "translate", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "\n", "for", "_", "in", "range", "(", "max", "(", "1", ",", "self", ".", "n_trials_max", ")", ")", ":", "\n", "\n", "# Pick the relative amount by which to translate.", "\n", "                ", "dy_abs", "=", "np", ".", "random", ".", "uniform", "(", "self", ".", "dy_minmax", "[", "0", "]", ",", "self", ".", "dy_minmax", "[", "1", "]", ")", "\n", "dx_abs", "=", "np", ".", "random", ".", "uniform", "(", "self", ".", "dx_minmax", "[", "0", "]", ",", "self", ".", "dx_minmax", "[", "1", "]", ")", "\n", "# Pick the direction in which to translate.", "\n", "dy", "=", "np", ".", "random", ".", "choice", "(", "[", "-", "dy_abs", ",", "dy_abs", "]", ")", "\n", "dx", "=", "np", ".", "random", ".", "choice", "(", "[", "-", "dx_abs", ",", "dx_abs", "]", ")", "\n", "self", ".", "translate", ".", "dy_rel", "=", "dy", "\n", "self", ".", "translate", ".", "dx_rel", "=", "dx", "\n", "\n", "if", "(", "labels", "is", "None", ")", "or", "(", "self", ".", "image_validator", "is", "None", ")", ":", "\n", "# We either don't have any boxes or if we do, we will accept any outcome as valid.", "\n", "                    ", "return", "self", ".", "translate", "(", "image", ",", "labels", ")", "\n", "", "else", ":", "\n", "# Translate the box coordinates to the translated image's coordinate system.", "\n", "                    ", "new_labels", "=", "np", ".", "copy", "(", "labels", ")", "\n", "new_labels", "[", ":", ",", "[", "ymin", ",", "ymax", "]", "]", "+=", "int", "(", "round", "(", "img_height", "*", "dy", ")", ")", "\n", "new_labels", "[", ":", ",", "[", "xmin", ",", "xmax", "]", "]", "+=", "int", "(", "round", "(", "img_width", "*", "dx", ")", ")", "\n", "\n", "# Check if the patch is valid.", "\n", "if", "self", ".", "image_validator", "(", "labels", "=", "new_labels", ",", "\n", "image_height", "=", "img_height", ",", "\n", "image_width", "=", "img_width", ")", ":", "\n", "                        ", "return", "self", ".", "translate", "(", "image", ",", "labels", ")", "\n", "\n", "# If all attempts failed, return the unaltered input image.", "\n", "", "", "", "if", "labels", "is", "None", ":", "\n", "                ", "return", "image", "\n", "\n", "", "else", ":", "\n", "                ", "return", "image", ",", "labels", "\n", "\n", "", "", "elif", "labels", "is", "None", ":", "\n", "            ", "return", "image", "\n", "\n", "", "else", ":", "\n", "            ", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_geometric_ops.Scale.__init__": [[455, 487], ["ValueError", "ValueError", "isinstance"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "factor", ",", "\n", "clip_boxes", "=", "True", ",", "\n", "box_filter", "=", "None", ",", "\n", "background", "=", "(", "0", ",", "0", ",", "0", ")", ",", "\n", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ")", ":", "\n", "        ", "'''\n        Arguments:\n            factor (float): The fraction of the image size by which to scale images. Must be positive.\n            clip_boxes (bool, optional): Only relevant if ground truth bounding boxes are given.\n                If `True`, any ground truth bounding boxes will be clipped to lie entirely within the\n                image after the translation.\n            box_filter (BoxFilter, optional): Only relevant if ground truth bounding boxes are given.\n                A `BoxFilter` object to filter out bounding boxes that don't meet the given criteria\n                after the transformation. Refer to the `BoxFilter` documentation for details. If `None`,\n                the validity of the bounding boxes is not checked.\n            background (list/tuple, optional): A 3-tuple specifying the RGB color value of the potential\n                background pixels of the scaled images.\n            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n        '''", "\n", "\n", "if", "factor", "<=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\"It must be `factor > 0`.\"", ")", "\n", "", "if", "not", "(", "isinstance", "(", "box_filter", ",", "BoxFilter", ")", "or", "box_filter", "is", "None", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`box_filter` must be either `None` or a `BoxFilter` object.\"", ")", "\n", "", "self", ".", "factor", "=", "factor", "\n", "self", ".", "clip_boxes", "=", "clip_boxes", "\n", "self", ".", "box_filter", "=", "box_filter", "\n", "self", ".", "background", "=", "background", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_geometric_ops.Scale.__call__": [[488, 534], ["cv2.getRotationMatrix2D", "cv2.warpAffine", "numpy.copy", "numpy.array", "numpy.array", "numpy.round().astype", "numpy.round().astype", "numpy.dot", "numpy.dot", "object_detection_2d_geometric_ops.Scale.box_filter", "numpy.clip", "numpy.clip", "numpy.ones", "numpy.ones", "numpy.round", "numpy.round"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "\n", "        ", "img_height", ",", "img_width", "=", "image", ".", "shape", "[", ":", "2", "]", "\n", "\n", "# Compute the rotation matrix.", "\n", "M", "=", "cv2", ".", "getRotationMatrix2D", "(", "center", "=", "(", "img_width", "/", "2", ",", "img_height", "/", "2", ")", ",", "\n", "angle", "=", "0", ",", "\n", "scale", "=", "self", ".", "factor", ")", "\n", "\n", "# Scale the image.", "\n", "image", "=", "cv2", ".", "warpAffine", "(", "image", ",", "\n", "M", "=", "M", ",", "\n", "dsize", "=", "(", "img_width", ",", "img_height", ")", ",", "\n", "borderMode", "=", "cv2", ".", "BORDER_CONSTANT", ",", "\n", "borderValue", "=", "self", ".", "background", ")", "\n", "\n", "if", "labels", "is", "None", ":", "\n", "            ", "return", "image", "\n", "", "else", ":", "\n", "            ", "xmin", "=", "self", ".", "labels_format", "[", "'xmin'", "]", "\n", "ymin", "=", "self", ".", "labels_format", "[", "'ymin'", "]", "\n", "xmax", "=", "self", ".", "labels_format", "[", "'xmax'", "]", "\n", "ymax", "=", "self", ".", "labels_format", "[", "'ymax'", "]", "\n", "\n", "labels", "=", "np", ".", "copy", "(", "labels", ")", "\n", "# Scale the bounding boxes accordingly.", "\n", "# Transform two opposite corner points of the rectangular boxes using the rotation matrix `M`.", "\n", "toplefts", "=", "np", ".", "array", "(", "[", "labels", "[", ":", ",", "xmin", "]", ",", "labels", "[", ":", ",", "ymin", "]", ",", "np", ".", "ones", "(", "labels", ".", "shape", "[", "0", "]", ")", "]", ")", "\n", "bottomrights", "=", "np", ".", "array", "(", "[", "labels", "[", ":", ",", "xmax", "]", ",", "labels", "[", ":", ",", "ymax", "]", ",", "np", ".", "ones", "(", "labels", ".", "shape", "[", "0", "]", ")", "]", ")", "\n", "new_toplefts", "=", "(", "np", ".", "dot", "(", "M", ",", "toplefts", ")", ")", ".", "T", "\n", "new_bottomrights", "=", "(", "np", ".", "dot", "(", "M", ",", "bottomrights", ")", ")", ".", "T", "\n", "labels", "[", ":", ",", "[", "xmin", ",", "ymin", "]", "]", "=", "np", ".", "round", "(", "new_toplefts", ",", "decimals", "=", "0", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "labels", "[", ":", ",", "[", "xmax", ",", "ymax", "]", "]", "=", "np", ".", "round", "(", "new_bottomrights", ",", "decimals", "=", "0", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "\n", "# Compute all valid boxes for this patch.", "\n", "if", "not", "(", "self", ".", "box_filter", "is", "None", ")", ":", "\n", "                ", "self", ".", "box_filter", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "labels", "=", "self", ".", "box_filter", "(", "labels", "=", "labels", ",", "\n", "image_height", "=", "img_height", ",", "\n", "image_width", "=", "img_width", ")", "\n", "\n", "", "if", "self", ".", "clip_boxes", ":", "\n", "                ", "labels", "[", ":", ",", "[", "ymin", ",", "ymax", "]", "]", "=", "np", ".", "clip", "(", "labels", "[", ":", ",", "[", "ymin", ",", "ymax", "]", "]", ",", "a_min", "=", "0", ",", "a_max", "=", "img_height", "-", "1", ")", "\n", "labels", "[", ":", ",", "[", "xmin", ",", "xmax", "]", "]", "=", "np", ".", "clip", "(", "labels", "[", ":", ",", "[", "xmin", ",", "xmax", "]", "]", ",", "a_min", "=", "0", ",", "a_max", "=", "img_width", "-", "1", ")", "\n", "\n", "", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_geometric_ops.RandomScale.__init__": [[540, 596], ["object_detection_2d_geometric_ops.Scale", "ValueError", "ValueError", "isinstance"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "min_factor", "=", "0.5", ",", "\n", "max_factor", "=", "1.5", ",", "\n", "prob", "=", "0.5", ",", "\n", "clip_boxes", "=", "True", ",", "\n", "box_filter", "=", "None", ",", "\n", "image_validator", "=", "None", ",", "\n", "n_trials_max", "=", "3", ",", "\n", "background", "=", "(", "0", ",", "0", ",", "0", ")", ",", "\n", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ")", ":", "\n", "        ", "'''\n        Arguments:\n            min_factor (float, optional): The minimum fraction of the image size by which to scale images.\n                Must be positive.\n            max_factor (float, optional): The maximum fraction of the image size by which to scale images.\n                Must be positive.\n            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n                unaltered image is returned.\n            clip_boxes (bool, optional): Only relevant if ground truth bounding boxes are given.\n                If `True`, any ground truth bounding boxes will be clipped to lie entirely within the\n                image after the translation.\n            box_filter (BoxFilter, optional): Only relevant if ground truth bounding boxes are given.\n                A `BoxFilter` object to filter out bounding boxes that don't meet the given criteria\n                after the transformation. Refer to the `BoxFilter` documentation for details. If `None`,\n                the validity of the bounding boxes is not checked.\n            image_validator (ImageValidator, optional): Only relevant if ground truth bounding boxes are given.\n                An `ImageValidator` object to determine whether a scaled image is valid. If `None`,\n                any outcome is valid.\n            n_trials_max (int, optional): Only relevant if ground truth bounding boxes are given.\n                Determines the maxmial number of trials to produce a valid image. If no valid image could\n                be produced in `n_trials_max` trials, returns the unaltered input image.\n            background (list/tuple, optional): A 3-tuple specifying the RGB color value of the potential\n                background pixels of the scaled images.\n            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n        '''", "\n", "\n", "if", "not", "(", "0", "<", "min_factor", "<=", "max_factor", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"It must be `0 < min_factor <= max_factor`.\"", ")", "\n", "", "if", "not", "(", "isinstance", "(", "image_validator", ",", "ImageValidator", ")", "or", "image_validator", "is", "None", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`image_validator` must be either `None` or an `ImageValidator` object.\"", ")", "\n", "", "self", ".", "min_factor", "=", "min_factor", "\n", "self", ".", "max_factor", "=", "max_factor", "\n", "self", ".", "prob", "=", "prob", "\n", "self", ".", "clip_boxes", "=", "clip_boxes", "\n", "self", ".", "box_filter", "=", "box_filter", "\n", "self", ".", "image_validator", "=", "image_validator", "\n", "self", ".", "n_trials_max", "=", "n_trials_max", "\n", "self", ".", "background", "=", "background", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "self", ".", "scale", "=", "Scale", "(", "factor", "=", "1.0", ",", "\n", "clip_boxes", "=", "self", ".", "clip_boxes", ",", "\n", "box_filter", "=", "self", ".", "box_filter", ",", "\n", "background", "=", "self", ".", "background", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_geometric_ops.RandomScale.__call__": [[597, 659], ["numpy.random.uniform", "range", "max", "numpy.random.uniform", "object_detection_2d_geometric_ops.RandomScale.scale", "numpy.array", "numpy.array", "cv2.getRotationMatrix2D", "numpy.copy", "numpy.around().astype", "numpy.around().astype", "object_detection_2d_geometric_ops.RandomScale.image_validator", "numpy.dot", "numpy.dot", "object_detection_2d_geometric_ops.RandomScale.scale", "numpy.ones", "numpy.ones", "numpy.around", "numpy.around"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "\n", "        ", "p", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "\n", "if", "p", ">=", "(", "1.0", "-", "self", ".", "prob", ")", ":", "\n", "\n", "            ", "img_height", ",", "img_width", "=", "image", ".", "shape", "[", ":", "2", "]", "\n", "\n", "xmin", "=", "self", ".", "labels_format", "[", "'xmin'", "]", "\n", "ymin", "=", "self", ".", "labels_format", "[", "'ymin'", "]", "\n", "xmax", "=", "self", ".", "labels_format", "[", "'xmax'", "]", "\n", "ymax", "=", "self", ".", "labels_format", "[", "'ymax'", "]", "\n", "\n", "# Override the preset labels format.", "\n", "if", "not", "self", ".", "image_validator", "is", "None", ":", "\n", "                ", "self", ".", "image_validator", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "", "self", ".", "scale", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "\n", "for", "_", "in", "range", "(", "max", "(", "1", ",", "self", ".", "n_trials_max", ")", ")", ":", "\n", "\n", "# Pick a scaling factor.", "\n", "                ", "factor", "=", "np", ".", "random", ".", "uniform", "(", "self", ".", "min_factor", ",", "self", ".", "max_factor", ")", "\n", "self", ".", "scale", ".", "factor", "=", "factor", "\n", "\n", "if", "(", "labels", "is", "None", ")", "or", "(", "self", ".", "image_validator", "is", "None", ")", ":", "\n", "# We either don't have any boxes or if we do, we will accept any outcome as valid.", "\n", "                    ", "return", "self", ".", "scale", "(", "image", ",", "labels", ")", "\n", "", "else", ":", "\n", "# Scale the bounding boxes accordingly.", "\n", "# Transform two opposite corner points of the rectangular boxes using the rotation matrix `M`.", "\n", "                    ", "toplefts", "=", "np", ".", "array", "(", "[", "labels", "[", ":", ",", "xmin", "]", ",", "labels", "[", ":", ",", "ymin", "]", ",", "np", ".", "ones", "(", "labels", ".", "shape", "[", "0", "]", ")", "]", ")", "\n", "bottomrights", "=", "np", ".", "array", "(", "[", "labels", "[", ":", ",", "xmax", "]", ",", "labels", "[", ":", ",", "ymax", "]", ",", "np", ".", "ones", "(", "labels", ".", "shape", "[", "0", "]", ")", "]", ")", "\n", "\n", "# Compute the rotation matrix.", "\n", "M", "=", "cv2", ".", "getRotationMatrix2D", "(", "center", "=", "(", "img_width", "/", "2", ",", "img_height", "/", "2", ")", ",", "\n", "angle", "=", "0", ",", "\n", "scale", "=", "factor", ")", "\n", "\n", "new_toplefts", "=", "(", "np", ".", "dot", "(", "M", ",", "toplefts", ")", ")", ".", "T", "\n", "new_bottomrights", "=", "(", "np", ".", "dot", "(", "M", ",", "bottomrights", ")", ")", ".", "T", "\n", "\n", "new_labels", "=", "np", ".", "copy", "(", "labels", ")", "\n", "new_labels", "[", ":", ",", "[", "xmin", ",", "ymin", "]", "]", "=", "np", ".", "around", "(", "new_toplefts", ",", "decimals", "=", "0", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "new_labels", "[", ":", ",", "[", "xmax", ",", "ymax", "]", "]", "=", "np", ".", "around", "(", "new_bottomrights", ",", "decimals", "=", "0", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "\n", "# Check if the patch is valid.", "\n", "if", "self", ".", "image_validator", "(", "labels", "=", "new_labels", ",", "\n", "image_height", "=", "img_height", ",", "\n", "image_width", "=", "img_width", ")", ":", "\n", "                        ", "return", "self", ".", "scale", "(", "image", ",", "labels", ")", "\n", "\n", "# If all attempts failed, return the unaltered input image.", "\n", "", "", "", "if", "labels", "is", "None", ":", "\n", "                ", "return", "image", "\n", "\n", "", "else", ":", "\n", "                ", "return", "image", ",", "labels", "\n", "\n", "", "", "elif", "labels", "is", "None", ":", "\n", "            ", "return", "image", "\n", "\n", "", "else", ":", "\n", "            ", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_geometric_ops.Rotate.__init__": [[665, 681], ["ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "angle", ",", "\n", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ")", ":", "\n", "        ", "'''\n        Arguments:\n            angle (int): The angle in degrees by which to rotate the images counter-clockwise.\n                Only 90, 180, and 270 are valid values.\n            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n        '''", "\n", "\n", "if", "not", "angle", "in", "{", "90", ",", "180", ",", "270", "}", ":", "\n", "            ", "raise", "ValueError", "(", "\"`angle` must be in the set {90, 180, 270}.\"", ")", "\n", "", "self", ".", "angle", "=", "angle", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_geometric_ops.Rotate.__call__": [[682, 739], ["cv2.getRotationMatrix2D", "numpy.abs", "numpy.abs", "int", "int", "cv2.warpAffine", "numpy.copy", "numpy.array", "numpy.array", "numpy.round().astype", "numpy.round().astype", "numpy.dot", "numpy.dot", "numpy.ones", "numpy.ones", "numpy.round", "numpy.round"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "\n", "        ", "img_height", ",", "img_width", "=", "image", ".", "shape", "[", ":", "2", "]", "\n", "\n", "# Compute the rotation matrix.", "\n", "M", "=", "cv2", ".", "getRotationMatrix2D", "(", "center", "=", "(", "img_width", "/", "2", ",", "img_height", "/", "2", ")", ",", "\n", "angle", "=", "self", ".", "angle", ",", "\n", "scale", "=", "1", ")", "\n", "\n", "# Get the sine and cosine from the rotation matrix.", "\n", "cos_angle", "=", "np", ".", "abs", "(", "M", "[", "0", ",", "0", "]", ")", "\n", "sin_angle", "=", "np", ".", "abs", "(", "M", "[", "0", ",", "1", "]", ")", "\n", "\n", "# Compute the new bounding dimensions of the image.", "\n", "img_width_new", "=", "int", "(", "img_height", "*", "sin_angle", "+", "img_width", "*", "cos_angle", ")", "\n", "img_height_new", "=", "int", "(", "img_height", "*", "cos_angle", "+", "img_width", "*", "sin_angle", ")", "\n", "\n", "# Adjust the rotation matrix to take into account the translation.", "\n", "M", "[", "1", ",", "2", "]", "+=", "(", "img_height_new", "-", "img_height", ")", "/", "2", "\n", "M", "[", "0", ",", "2", "]", "+=", "(", "img_width_new", "-", "img_width", ")", "/", "2", "\n", "\n", "# Rotate the image.", "\n", "image", "=", "cv2", ".", "warpAffine", "(", "image", ",", "\n", "M", "=", "M", ",", "\n", "dsize", "=", "(", "img_width_new", ",", "img_height_new", ")", ")", "\n", "\n", "if", "labels", "is", "None", ":", "\n", "            ", "return", "image", "\n", "", "else", ":", "\n", "            ", "xmin", "=", "self", ".", "labels_format", "[", "'xmin'", "]", "\n", "ymin", "=", "self", ".", "labels_format", "[", "'ymin'", "]", "\n", "xmax", "=", "self", ".", "labels_format", "[", "'xmax'", "]", "\n", "ymax", "=", "self", ".", "labels_format", "[", "'ymax'", "]", "\n", "\n", "labels", "=", "np", ".", "copy", "(", "labels", ")", "\n", "# Rotate the bounding boxes accordingly.", "\n", "# Transform two opposite corner points of the rectangular boxes using the rotation matrix `M`.", "\n", "toplefts", "=", "np", ".", "array", "(", "[", "labels", "[", ":", ",", "xmin", "]", ",", "labels", "[", ":", ",", "ymin", "]", ",", "np", ".", "ones", "(", "labels", ".", "shape", "[", "0", "]", ")", "]", ")", "\n", "bottomrights", "=", "np", ".", "array", "(", "[", "labels", "[", ":", ",", "xmax", "]", ",", "labels", "[", ":", ",", "ymax", "]", ",", "np", ".", "ones", "(", "labels", ".", "shape", "[", "0", "]", ")", "]", ")", "\n", "new_toplefts", "=", "(", "np", ".", "dot", "(", "M", ",", "toplefts", ")", ")", ".", "T", "\n", "new_bottomrights", "=", "(", "np", ".", "dot", "(", "M", ",", "bottomrights", ")", ")", ".", "T", "\n", "labels", "[", ":", ",", "[", "xmin", ",", "ymin", "]", "]", "=", "np", ".", "round", "(", "new_toplefts", ",", "decimals", "=", "0", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "labels", "[", ":", ",", "[", "xmax", ",", "ymax", "]", "]", "=", "np", ".", "round", "(", "new_bottomrights", ",", "decimals", "=", "0", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "\n", "if", "self", ".", "angle", "==", "90", ":", "\n", "# ymin and ymax were switched by the rotation.", "\n", "                ", "labels", "[", ":", ",", "[", "ymax", ",", "ymin", "]", "]", "=", "labels", "[", ":", ",", "[", "ymin", ",", "ymax", "]", "]", "\n", "", "elif", "self", ".", "angle", "==", "180", ":", "\n", "# ymin and ymax were switched by the rotation,", "\n", "# and also xmin and xmax were switched.", "\n", "                ", "labels", "[", ":", ",", "[", "ymax", ",", "ymin", "]", "]", "=", "labels", "[", ":", ",", "[", "ymin", ",", "ymax", "]", "]", "\n", "labels", "[", ":", ",", "[", "xmax", ",", "xmin", "]", "]", "=", "labels", "[", ":", ",", "[", "xmin", ",", "xmax", "]", "]", "\n", "", "elif", "self", ".", "angle", "==", "270", ":", "\n", "# xmin and xmax were switched by the rotation.", "\n", "                ", "labels", "[", ":", ",", "[", "xmax", ",", "xmin", "]", "]", "=", "labels", "[", ":", ",", "[", "xmin", ",", "xmax", "]", "]", "\n", "\n", "", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_geometric_ops.RandomRotate.__init__": [[745, 766], ["object_detection_2d_geometric_ops.Rotate", "ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "angles", "=", "[", "90", ",", "180", ",", "270", "]", ",", "\n", "prob", "=", "0.5", ",", "\n", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ")", ":", "\n", "        ", "'''\n        Arguments:\n            angle (list): The list of angles in degrees from which one is randomly selected to rotate\n                the images counter-clockwise. Only 90, 180, and 270 are valid values.\n            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n                unaltered image is returned.\n            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n        '''", "\n", "for", "angle", "in", "angles", ":", "\n", "            ", "if", "not", "angle", "in", "{", "90", ",", "180", ",", "270", "}", ":", "\n", "                ", "raise", "ValueError", "(", "\"`angles` can only contain the values 90, 180, and 270.\"", ")", "\n", "", "", "self", ".", "angles", "=", "angles", "\n", "self", ".", "prob", "=", "prob", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "self", ".", "rotate", "=", "Rotate", "(", "angle", "=", "90", ",", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_geometric_ops.RandomRotate.__call__": [[767, 781], ["numpy.random.uniform", "random.choice", "object_detection_2d_geometric_ops.RandomRotate.rotate"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "\n", "        ", "p", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "\n", "if", "p", ">=", "(", "1.0", "-", "self", ".", "prob", ")", ":", "\n", "# Pick a rotation angle.", "\n", "            ", "self", ".", "rotate", ".", "angle", "=", "random", ".", "choice", "(", "self", ".", "angles", ")", "\n", "self", ".", "rotate", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "return", "self", ".", "rotate", "(", "image", ",", "labels", ")", "\n", "\n", "", "elif", "labels", "is", "None", ":", "\n", "            ", "return", "image", "\n", "\n", "", "else", ":", "\n", "            ", "return", "image", ",", "labels", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_image_boxes_validation_utils.BoundGenerator.__init__": [[33, 70], ["len", "ValueError", "list", "object_detection_2d_image_boxes_validation_utils.BoundGenerator.sample_space.append", "len", "len", "len", "ValueError", "ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "sample_space", "=", "(", "(", "0.1", ",", "None", ")", ",", "\n", "(", "0.3", ",", "None", ")", ",", "\n", "(", "0.5", ",", "None", ")", ",", "\n", "(", "0.7", ",", "None", ")", ",", "\n", "(", "0.9", ",", "None", ")", ",", "\n", "(", "None", ",", "None", ")", ")", ",", "\n", "weights", "=", "None", ")", ":", "\n", "        ", "'''\n        Arguments:\n            sample_space (list or tuple): A list, tuple, or array-like object of shape\n                `(n, 2)` that contains `n` samples to choose from, where each sample\n                is a 2-tuple of scalars and/or `None` values.\n            weights (list or tuple, optional): A list or tuple representing the distribution\n                over the sample space. If `None`, a uniform distribution will be assumed.\n        '''", "\n", "\n", "if", "(", "not", "(", "weights", "is", "None", ")", ")", "and", "len", "(", "weights", ")", "!=", "len", "(", "sample_space", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`weights` must either be `None` for uniform distribution or have the same length as `sample_space`.\"", ")", "\n", "\n", "", "self", ".", "sample_space", "=", "[", "]", "\n", "for", "bound_pair", "in", "sample_space", ":", "\n", "            ", "if", "len", "(", "bound_pair", ")", "!=", "2", ":", "\n", "                ", "raise", "ValueError", "(", "\"All elements of the sample space must be 2-tuples.\"", ")", "\n", "", "bound_pair", "=", "list", "(", "bound_pair", ")", "\n", "if", "bound_pair", "[", "0", "]", "is", "None", ":", "bound_pair", "[", "0", "]", "=", "0.0", "\n", "if", "bound_pair", "[", "1", "]", "is", "None", ":", "bound_pair", "[", "1", "]", "=", "1.0", "\n", "if", "bound_pair", "[", "0", "]", ">", "bound_pair", "[", "1", "]", ":", "\n", "                ", "raise", "ValueError", "(", "\"For all sample space elements, the lower bound cannot be greater than the upper bound.\"", ")", "\n", "", "self", ".", "sample_space", ".", "append", "(", "bound_pair", ")", "\n", "\n", "", "self", ".", "sample_space_size", "=", "len", "(", "self", ".", "sample_space", ")", "\n", "\n", "if", "weights", "is", "None", ":", "\n", "            ", "self", ".", "weights", "=", "[", "1.0", "/", "self", ".", "sample_space_size", "]", "*", "self", ".", "sample_space_size", "\n", "", "else", ":", "\n", "            ", "self", ".", "weights", "=", "weights", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_image_boxes_validation_utils.BoundGenerator.__call__": [[71, 78], ["numpy.random.choice"], "methods", ["None"], ["", "", "def", "__call__", "(", "self", ")", ":", "\n", "        ", "'''\n        Returns:\n            An item of the sample space, i.e. a 2-tuple of scalars.\n        '''", "\n", "i", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "sample_space_size", ",", "p", "=", "self", ".", "weights", ")", "\n", "return", "self", ".", "sample_space", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_image_boxes_validation_utils.BoxFilter.__init__": [[84, 146], ["isinstance", "ValueError", "isinstance", "ValueError", "ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "check_overlap", "=", "True", ",", "\n", "check_min_area", "=", "True", ",", "\n", "check_degenerate", "=", "True", ",", "\n", "overlap_criterion", "=", "'center_point'", ",", "\n", "overlap_bounds", "=", "(", "0.3", ",", "1.0", ")", ",", "\n", "min_area", "=", "16", ",", "\n", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ",", "\n", "border_pixels", "=", "'half'", ")", ":", "\n", "        ", "'''\n        Arguments:\n            check_overlap (bool, optional): Whether or not to enforce the overlap requirements defined by\n                `overlap_criterion` and `overlap_bounds`. Sometimes you might want to use the box filter only\n                to enforce a certain minimum area for all boxes (see next argument), in such cases you can\n                turn the overlap requirements off.\n            check_min_area (bool, optional): Whether or not to enforce the minimum area requirement defined\n                by `min_area`. If `True`, any boxes that have an area (in pixels) that is smaller than `min_area`\n                will be removed from the labels of an image. Bounding boxes below a certain area aren't useful\n                training examples. An object that takes up only, say, 5 pixels in an image is probably not\n                recognizable anymore, neither for a human, nor for an object detection model. It makes sense\n                to remove such boxes.\n            check_degenerate (bool, optional): Whether or not to check for and remove degenerate bounding boxes.\n                Degenerate bounding boxes are boxes that have `xmax <= xmin` and/or `ymax <= ymin`. In particular,\n                boxes with a width and/or height of zero are degenerate. It is obviously important to filter out\n                such boxes, so you should only set this option to `False` if you are certain that degenerate\n                boxes are not possible in your data and processing chain.\n            overlap_criterion (str, optional): Can be either of 'center_point', 'iou', or 'area'. Determines\n                which boxes are considered valid with respect to a given image. If set to 'center_point',\n                a given bounding box is considered valid if its center point lies within the image.\n                If set to 'area', a given bounding box is considered valid if the quotient of its intersection\n                area with the image and its own area is within the given `overlap_bounds`. If set to 'iou', a given\n                bounding box is considered valid if its IoU with the image is within the given `overlap_bounds`.\n            overlap_bounds (list or BoundGenerator, optional): Only relevant if `overlap_criterion` is 'area' or 'iou'.\n                Determines the lower and upper bounds for `overlap_criterion`. Can be either a 2-tuple of scalars\n                representing a lower bound and an upper bound, or a `BoundGenerator` object, which provides\n                the possibility to generate bounds randomly.\n            min_area (int, optional): Only relevant if `check_min_area` is `True`. Defines the minimum area in\n                pixels that a bounding box must have in order to be valid. Boxes with an area smaller than this\n                will be removed.\n            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n            border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n                Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n                to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n                If 'half', then one of each of the two horizontal and vertical borders belong\n                to the boxex, but not the other.\n        '''", "\n", "if", "not", "isinstance", "(", "overlap_bounds", ",", "(", "list", ",", "tuple", ",", "BoundGenerator", ")", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`overlap_bounds` must be either a 2-tuple of scalars or a `BoundGenerator` object.\"", ")", "\n", "", "if", "isinstance", "(", "overlap_bounds", ",", "(", "list", ",", "tuple", ")", ")", "and", "(", "overlap_bounds", "[", "0", "]", ">", "overlap_bounds", "[", "1", "]", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"The lower bound must not be greater than the upper bound.\"", ")", "\n", "", "if", "not", "(", "overlap_criterion", "in", "{", "'iou'", ",", "'area'", ",", "'center_point'", "}", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`overlap_criterion` must be one of 'iou', 'area', or 'center_point'.\"", ")", "\n", "", "self", ".", "overlap_criterion", "=", "overlap_criterion", "\n", "self", ".", "overlap_bounds", "=", "overlap_bounds", "\n", "self", ".", "min_area", "=", "min_area", "\n", "self", ".", "check_overlap", "=", "check_overlap", "\n", "self", ".", "check_min_area", "=", "check_min_area", "\n", "self", ".", "check_degenerate", "=", "check_degenerate", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "self", ".", "border_pixels", "=", "border_pixels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_image_boxes_validation_utils.BoxFilter.__call__": [[147, 233], ["numpy.copy", "numpy.ones", "isinstance", "object_detection_2d_image_boxes_validation_utils.BoxFilter.overlap_bounds", "numpy.array", "bounding_box_utils.bounding_box_utils.iou", "numpy.copy", "numpy.clip", "numpy.clip"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.iou"], ["", "def", "__call__", "(", "self", ",", "\n", "labels", ",", "\n", "image_height", "=", "None", ",", "\n", "image_width", "=", "None", ")", ":", "\n", "        ", "'''\n        Arguments:\n            labels (array): The labels to be filtered. This is an array with shape `(m,n)`, where\n                `m` is the number of bounding boxes and `n` is the number of elements that defines\n                each bounding box (box coordinates, class ID, etc.). The box coordinates are expected\n                to be in the image's coordinate system.\n            image_height (int): Only relevant if `check_overlap == True`. The height of the image\n                (in pixels) to compare the box coordinates to.\n            image_width (int): `check_overlap == True`. The width of the image (in pixels) to compare\n                the box coordinates to.\n\n        Returns:\n            An array containing the labels of all boxes that are valid.\n        '''", "\n", "\n", "labels", "=", "np", ".", "copy", "(", "labels", ")", "\n", "\n", "xmin", "=", "self", ".", "labels_format", "[", "'xmin'", "]", "\n", "ymin", "=", "self", ".", "labels_format", "[", "'ymin'", "]", "\n", "xmax", "=", "self", ".", "labels_format", "[", "'xmax'", "]", "\n", "ymax", "=", "self", ".", "labels_format", "[", "'ymax'", "]", "\n", "\n", "# Record the boxes that pass all checks here.", "\n", "requirements_met", "=", "np", ".", "ones", "(", "shape", "=", "labels", ".", "shape", "[", "0", "]", ",", "dtype", "=", "np", ".", "bool", ")", "\n", "\n", "if", "self", ".", "check_degenerate", ":", "\n", "\n", "            ", "non_degenerate", "=", "(", "labels", "[", ":", ",", "xmax", "]", ">", "labels", "[", ":", ",", "xmin", "]", ")", "*", "(", "labels", "[", ":", ",", "ymax", "]", ">", "labels", "[", ":", ",", "ymin", "]", ")", "\n", "requirements_met", "*=", "non_degenerate", "\n", "\n", "", "if", "self", ".", "check_min_area", ":", "\n", "\n", "            ", "min_area_met", "=", "(", "labels", "[", ":", ",", "xmax", "]", "-", "labels", "[", ":", ",", "xmin", "]", ")", "*", "(", "labels", "[", ":", ",", "ymax", "]", "-", "labels", "[", ":", ",", "ymin", "]", ")", ">=", "self", ".", "min_area", "\n", "requirements_met", "*=", "min_area_met", "\n", "\n", "", "if", "self", ".", "check_overlap", ":", "\n", "\n", "# Get the lower and upper bounds.", "\n", "            ", "if", "isinstance", "(", "self", ".", "overlap_bounds", ",", "BoundGenerator", ")", ":", "\n", "                ", "lower", ",", "upper", "=", "self", ".", "overlap_bounds", "(", ")", "\n", "", "else", ":", "\n", "                ", "lower", ",", "upper", "=", "self", ".", "overlap_bounds", "\n", "\n", "# Compute which boxes are valid.", "\n", "\n", "", "if", "self", ".", "overlap_criterion", "==", "'iou'", ":", "\n", "# Compute the patch coordinates.", "\n", "                ", "image_coords", "=", "np", ".", "array", "(", "[", "0", ",", "0", ",", "image_width", ",", "image_height", "]", ")", "\n", "# Compute the IoU between the patch and all of the ground truth boxes.", "\n", "image_boxes_iou", "=", "iou", "(", "image_coords", ",", "labels", "[", ":", ",", "[", "xmin", ",", "ymin", ",", "xmax", ",", "ymax", "]", "]", ",", "coords", "=", "'corners'", ",", "mode", "=", "'element-wise'", ",", "border_pixels", "=", "self", ".", "border_pixels", ")", "\n", "requirements_met", "*=", "(", "image_boxes_iou", ">", "lower", ")", "*", "(", "image_boxes_iou", "<=", "upper", ")", "\n", "\n", "", "elif", "self", ".", "overlap_criterion", "==", "'area'", ":", "\n", "                ", "if", "self", ".", "border_pixels", "==", "'half'", ":", "\n", "                    ", "d", "=", "0", "\n", "", "elif", "self", ".", "border_pixels", "==", "'include'", ":", "\n", "                    ", "d", "=", "1", "# If border pixels are supposed to belong to the bounding boxes, we have to add one pixel to any difference `xmax - xmin` or `ymax - ymin`.", "\n", "", "elif", "self", ".", "border_pixels", "==", "'exclude'", ":", "\n", "                    ", "d", "=", "-", "1", "# If border pixels are not supposed to belong to the bounding boxes, we have to subtract one pixel from any difference `xmax - xmin` or `ymax - ymin`.", "\n", "# Compute the areas of the boxes.", "\n", "", "box_areas", "=", "(", "labels", "[", ":", ",", "xmax", "]", "-", "labels", "[", ":", ",", "xmin", "]", "+", "d", ")", "*", "(", "labels", "[", ":", ",", "ymax", "]", "-", "labels", "[", ":", ",", "ymin", "]", "+", "d", ")", "\n", "# Compute the intersection area between the patch and all of the ground truth boxes.", "\n", "clipped_boxes", "=", "np", ".", "copy", "(", "labels", ")", "\n", "clipped_boxes", "[", ":", ",", "[", "ymin", ",", "ymax", "]", "]", "=", "np", ".", "clip", "(", "labels", "[", ":", ",", "[", "ymin", ",", "ymax", "]", "]", ",", "a_min", "=", "0", ",", "a_max", "=", "image_height", "-", "1", ")", "\n", "clipped_boxes", "[", ":", ",", "[", "xmin", ",", "xmax", "]", "]", "=", "np", ".", "clip", "(", "labels", "[", ":", ",", "[", "xmin", ",", "xmax", "]", "]", ",", "a_min", "=", "0", ",", "a_max", "=", "image_width", "-", "1", ")", "\n", "intersection_areas", "=", "(", "clipped_boxes", "[", ":", ",", "xmax", "]", "-", "clipped_boxes", "[", ":", ",", "xmin", "]", "+", "d", ")", "*", "(", "clipped_boxes", "[", ":", ",", "ymax", "]", "-", "clipped_boxes", "[", ":", ",", "ymin", "]", "+", "d", ")", "# +1 because the border pixels belong to the box areas.", "\n", "# Check which boxes meet the overlap requirements.", "\n", "if", "lower", "==", "0.0", ":", "\n", "                    ", "mask_lower", "=", "intersection_areas", ">", "lower", "*", "box_areas", "# If `self.lower == 0`, we want to make sure that boxes with area 0 don't count, hence the \">\" sign instead of the \">=\" sign.", "\n", "", "else", ":", "\n", "                    ", "mask_lower", "=", "intersection_areas", ">=", "lower", "*", "box_areas", "# Especially for the case `self.lower == 1` we want the \">=\" sign, otherwise no boxes would count at all.", "\n", "", "mask_upper", "=", "intersection_areas", "<=", "upper", "*", "box_areas", "\n", "requirements_met", "*=", "mask_lower", "*", "mask_upper", "\n", "\n", "", "elif", "self", ".", "overlap_criterion", "==", "'center_point'", ":", "\n", "# Compute the center points of the boxes.", "\n", "                ", "cy", "=", "(", "labels", "[", ":", ",", "ymin", "]", "+", "labels", "[", ":", ",", "ymax", "]", ")", "/", "2", "\n", "cx", "=", "(", "labels", "[", ":", ",", "xmin", "]", "+", "labels", "[", ":", ",", "xmax", "]", ")", "/", "2", "\n", "# Check which of the boxes have center points within the cropped patch remove those that don't.", "\n", "requirements_met", "*=", "(", "cy", ">=", "0.0", ")", "*", "(", "cy", "<=", "image_height", "-", "1", ")", "*", "(", "cx", ">=", "0.0", ")", "*", "(", "cx", "<=", "image_width", "-", "1", ")", "\n", "\n", "", "", "return", "labels", "[", "requirements_met", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_image_boxes_validation_utils.ImageValidator.__init__": [[240, 285], ["object_detection_2d_image_boxes_validation_utils.BoxFilter", "ValueError", "isinstance"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "overlap_criterion", "=", "'center_point'", ",", "\n", "bounds", "=", "(", "0.3", ",", "1.0", ")", ",", "\n", "n_boxes_min", "=", "1", ",", "\n", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ",", "\n", "border_pixels", "=", "'half'", ")", ":", "\n", "        ", "'''\n        Arguments:\n            overlap_criterion (str, optional): Can be either of 'center_point', 'iou', or 'area'. Determines\n                which boxes are considered valid with respect to a given image. If set to 'center_point',\n                a given bounding box is considered valid if its center point lies within the image.\n                If set to 'area', a given bounding box is considered valid if the quotient of its intersection\n                area with the image and its own area is within `lower` and `upper`. If set to 'iou', a given\n                bounding box is considered valid if its IoU with the image is within `lower` and `upper`.\n            bounds (list or BoundGenerator, optional): Only relevant if `overlap_criterion` is 'area' or 'iou'.\n                Determines the lower and upper bounds for `overlap_criterion`. Can be either a 2-tuple of scalars\n                representing a lower bound and an upper bound, or a `BoundGenerator` object, which provides\n                the possibility to generate bounds randomly.\n            n_boxes_min (int or str, optional): Either a non-negative integer or the string 'all'.\n                Determines the minimum number of boxes that must meet the `overlap_criterion` with respect to\n                an image of the given height and width in order for the image to be a valid image.\n                If set to 'all', an image is considered valid if all given boxes meet the `overlap_criterion`.\n            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n            border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n                Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n                to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n                If 'half', then one of each of the two horizontal and vertical borders belong\n                to the boxex, but not the other.\n        '''", "\n", "if", "not", "(", "(", "isinstance", "(", "n_boxes_min", ",", "int", ")", "and", "n_boxes_min", ">", "0", ")", "or", "n_boxes_min", "==", "'all'", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`n_boxes_min` must be a positive integer or 'all'.\"", ")", "\n", "", "self", ".", "overlap_criterion", "=", "overlap_criterion", "\n", "self", ".", "bounds", "=", "bounds", "\n", "self", ".", "n_boxes_min", "=", "n_boxes_min", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "self", ".", "border_pixels", "=", "border_pixels", "\n", "self", ".", "box_filter", "=", "BoxFilter", "(", "check_overlap", "=", "True", ",", "\n", "check_min_area", "=", "False", ",", "\n", "check_degenerate", "=", "False", ",", "\n", "overlap_criterion", "=", "self", ".", "overlap_criterion", ",", "\n", "overlap_bounds", "=", "self", ".", "bounds", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ",", "\n", "border_pixels", "=", "self", ".", "border_pixels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_image_boxes_validation_utils.ImageValidator.__call__": [[286, 323], ["object_detection_2d_image_boxes_validation_utils.ImageValidator.box_filter", "isinstance", "len", "len", "len"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "\n", "labels", ",", "\n", "image_height", ",", "\n", "image_width", ")", ":", "\n", "        ", "'''\n        Arguments:\n            labels (array): The labels to be tested. The box coordinates are expected\n                to be in the image's coordinate system.\n            image_height (int): The height of the image to compare the box coordinates to.\n            image_width (int): The width of the image to compare the box coordinates to.\n\n        Returns:\n            A boolean indicating whether an imgae of the given height and width is\n            valid with respect to the given bounding boxes.\n        '''", "\n", "\n", "self", ".", "box_filter", ".", "overlap_bounds", "=", "self", ".", "bounds", "\n", "self", ".", "box_filter", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "\n", "# Get all boxes that meet the overlap requirements.", "\n", "valid_labels", "=", "self", ".", "box_filter", "(", "labels", "=", "labels", ",", "\n", "image_height", "=", "image_height", ",", "\n", "image_width", "=", "image_width", ")", "\n", "\n", "# Check whether enough boxes meet the requirements.", "\n", "if", "isinstance", "(", "self", ".", "n_boxes_min", ",", "int", ")", ":", "\n", "# The image is valid if at least `self.n_boxes_min` ground truth boxes meet the requirements.", "\n", "            ", "if", "len", "(", "valid_labels", ")", ">=", "self", ".", "n_boxes_min", ":", "\n", "                ", "return", "True", "\n", "", "else", ":", "\n", "                ", "return", "False", "\n", "", "", "elif", "self", ".", "n_boxes_min", "==", "'all'", ":", "\n", "# The image is valid if all ground truth boxes meet the requirements.", "\n", "            ", "if", "len", "(", "valid_labels", ")", "==", "len", "(", "labels", ")", ":", "\n", "                ", "return", "True", "\n", "", "else", ":", "\n", "                ", "return", "False", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.ConvertColor.__init__": [[28, 43], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "current", "=", "'RGB'", ",", "to", "=", "'HSV'", ",", "keep_3ch", "=", "True", ")", ":", "\n", "        ", "'''\n        Arguments:\n            current (str, optional): The current color space of the images. Can be\n                one of 'RGB' and 'HSV'.\n            to (str, optional): The target color space of the images. Can be one of\n                'RGB', 'HSV', and 'GRAY'.\n            keep_3ch (bool, optional): Only relevant if `to == GRAY`.\n                If `True`, the resulting grayscale images will have three channels.\n        '''", "\n", "if", "not", "(", "(", "current", "in", "{", "'RGB'", ",", "'HSV'", "}", ")", "and", "(", "to", "in", "{", "'RGB'", ",", "'HSV'", ",", "'GRAY'", "}", ")", ")", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "self", ".", "current", "=", "current", "\n", "self", ".", "to", "=", "to", "\n", "self", ".", "keep_3ch", "=", "keep_3ch", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.ConvertColor.__call__": [[44, 61], ["cv2.cvtColor", "cv2.cvtColor", "numpy.stack", "cv2.cvtColor", "cv2.cvtColor", "numpy.stack"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "current", "==", "'RGB'", "and", "self", ".", "to", "==", "'HSV'", ":", "\n", "            ", "image", "=", "cv2", ".", "cvtColor", "(", "image", ",", "cv2", ".", "COLOR_RGB2HSV", ")", "\n", "", "elif", "self", ".", "current", "==", "'RGB'", "and", "self", ".", "to", "==", "'GRAY'", ":", "\n", "            ", "image", "=", "cv2", ".", "cvtColor", "(", "image", ",", "cv2", ".", "COLOR_RGB2GRAY", ")", "\n", "if", "self", ".", "keep_3ch", ":", "\n", "                ", "image", "=", "np", ".", "stack", "(", "[", "image", "]", "*", "3", ",", "axis", "=", "-", "1", ")", "\n", "", "", "elif", "self", ".", "current", "==", "'HSV'", "and", "self", ".", "to", "==", "'RGB'", ":", "\n", "            ", "image", "=", "cv2", ".", "cvtColor", "(", "image", ",", "cv2", ".", "COLOR_HSV2RGB", ")", "\n", "", "elif", "self", ".", "current", "==", "'HSV'", "and", "self", ".", "to", "==", "'GRAY'", ":", "\n", "            ", "image", "=", "cv2", ".", "cvtColor", "(", "image", ",", "cv2", ".", "COLOR_HSV2GRAY", ")", "\n", "if", "self", ".", "keep_3ch", ":", "\n", "                ", "image", "=", "np", ".", "stack", "(", "[", "image", "]", "*", "3", ",", "axis", "=", "-", "1", ")", "\n", "", "", "if", "labels", "is", "None", ":", "\n", "            ", "return", "image", "\n", "", "else", ":", "\n", "            ", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.ConvertDataType.__init__": [[68, 77], ["ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "to", "=", "'uint8'", ")", ":", "\n", "        ", "'''\n        Arguments:\n            to (string, optional): To which datatype to convert the input images.\n                Can be either of 'uint8' and 'float32'.\n        '''", "\n", "if", "not", "(", "to", "==", "'uint8'", "or", "to", "==", "'float32'", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`to` can be either of 'uint8' or 'float32'.\"", ")", "\n", "", "self", ".", "to", "=", "to", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.ConvertDataType.__call__": [[78, 87], ["numpy.round().astype", "image.astype.astype.astype", "numpy.round"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "to", "==", "'uint8'", ":", "\n", "            ", "image", "=", "np", ".", "round", "(", "image", ",", "decimals", "=", "0", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "", "else", ":", "\n", "            ", "image", "=", "image", ".", "astype", "(", "np", ".", "float32", ")", "\n", "", "if", "labels", "is", "None", ":", "\n", "            ", "return", "image", "\n", "", "else", ":", "\n", "            ", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.ConvertTo3Channels.__init__": [[94, 96], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.ConvertTo3Channels.__call__": [[97, 109], ["numpy.stack", "numpy.concatenate"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "        ", "if", "image", ".", "ndim", "==", "2", ":", "\n", "            ", "image", "=", "np", ".", "stack", "(", "[", "image", "]", "*", "3", ",", "axis", "=", "-", "1", ")", "\n", "", "elif", "image", ".", "ndim", "==", "3", ":", "\n", "            ", "if", "image", ".", "shape", "[", "2", "]", "==", "1", ":", "\n", "                ", "image", "=", "np", ".", "concatenate", "(", "[", "image", "]", "*", "3", ",", "axis", "=", "-", "1", ")", "\n", "", "elif", "image", ".", "shape", "[", "2", "]", "==", "4", ":", "\n", "                ", "image", "=", "image", "[", ":", ",", ":", ",", ":", "3", "]", "\n", "", "", "if", "labels", "is", "None", ":", "\n", "            ", "return", "image", "\n", "", "else", ":", "\n", "            ", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.Hue.__init__": [[118, 127], ["ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "delta", ")", ":", "\n", "        ", "'''\n        Arguments:\n            delta (int): An integer in the closed interval `[-180, 180]` that determines the hue change, where\n                a change by integer `delta` means a change by `2 * delta` degrees. Read up on the HSV color format\n                if you need more information.\n        '''", "\n", "if", "not", "(", "-", "180", "<=", "delta", "<=", "180", ")", ":", "raise", "ValueError", "(", "\"`delta` must be in the closed interval `[-180, 180]`.\"", ")", "\n", "self", ".", "delta", "=", "delta", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.Hue.__call__": [[128, 134], ["None"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "        ", "image", "[", ":", ",", ":", ",", "0", "]", "=", "(", "image", "[", ":", ",", ":", ",", "0", "]", "+", "self", ".", "delta", ")", "%", "180.0", "\n", "if", "labels", "is", "None", ":", "\n", "            ", "return", "image", "\n", "", "else", ":", "\n", "            ", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.RandomHue.__init__": [[143, 155], ["object_detection_2d_photometric_ops.Hue", "ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "max_delta", "=", "18", ",", "prob", "=", "0.5", ")", ":", "\n", "        ", "'''\n        Arguments:\n            max_delta (int): An integer in the closed interval `[0, 180]` that determines the maximal absolute\n                hue change.\n            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n                unaltered image is returned.\n        '''", "\n", "if", "not", "(", "0", "<=", "max_delta", "<=", "180", ")", ":", "raise", "ValueError", "(", "\"`max_delta` must be in the closed interval `[0, 180]`.\"", ")", "\n", "self", ".", "max_delta", "=", "max_delta", "\n", "self", ".", "prob", "=", "prob", "\n", "self", ".", "change_hue", "=", "Hue", "(", "delta", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.RandomHue.__call__": [[156, 165], ["numpy.random.uniform", "numpy.random.uniform", "object_detection_2d_photometric_ops.RandomHue.change_hue"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "        ", "p", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "\n", "if", "p", ">=", "(", "1.0", "-", "self", ".", "prob", ")", ":", "\n", "            ", "self", ".", "change_hue", ".", "delta", "=", "np", ".", "random", ".", "uniform", "(", "-", "self", ".", "max_delta", ",", "self", ".", "max_delta", ")", "\n", "return", "self", ".", "change_hue", "(", "image", ",", "labels", ")", "\n", "", "elif", "labels", "is", "None", ":", "\n", "            ", "return", "image", "\n", "", "else", ":", "\n", "            ", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.Saturation.__init__": [[174, 183], ["ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "factor", ")", ":", "\n", "        ", "'''\n        Arguments:\n            factor (float): A float greater than zero that determines saturation change, where\n                values less than one result in less saturation and values greater than one result\n                in more saturation.\n        '''", "\n", "if", "factor", "<=", "0.0", ":", "raise", "ValueError", "(", "\"It must be `factor > 0`.\"", ")", "\n", "self", ".", "factor", "=", "factor", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.Saturation.__call__": [[184, 190], ["numpy.clip"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "        ", "image", "[", ":", ",", ":", ",", "1", "]", "=", "np", ".", "clip", "(", "image", "[", ":", ",", ":", ",", "1", "]", "*", "self", ".", "factor", ",", "0", ",", "255", ")", "\n", "if", "labels", "is", "None", ":", "\n", "            ", "return", "image", "\n", "", "else", ":", "\n", "            ", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.RandomSaturation.__init__": [[199, 214], ["object_detection_2d_photometric_ops.Saturation", "ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "lower", "=", "0.3", ",", "upper", "=", "2.0", ",", "prob", "=", "0.5", ")", ":", "\n", "        ", "'''\n        Arguments:\n            lower (float, optional): A float greater than zero, the lower bound for the random\n                saturation change.\n            upper (float, optional): A float greater than zero, the upper bound for the random\n                saturation change. Must be greater than `lower`.\n            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n                unaltered image is returned.\n        '''", "\n", "if", "lower", ">=", "upper", ":", "raise", "ValueError", "(", "\"`upper` must be greater than `lower`.\"", ")", "\n", "self", ".", "lower", "=", "lower", "\n", "self", ".", "upper", "=", "upper", "\n", "self", ".", "prob", "=", "prob", "\n", "self", ".", "change_saturation", "=", "Saturation", "(", "factor", "=", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.RandomSaturation.__call__": [[215, 224], ["numpy.random.uniform", "numpy.random.uniform", "object_detection_2d_photometric_ops.RandomSaturation.change_saturation"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "        ", "p", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "\n", "if", "p", ">=", "(", "1.0", "-", "self", ".", "prob", ")", ":", "\n", "            ", "self", ".", "change_saturation", ".", "factor", "=", "np", ".", "random", ".", "uniform", "(", "self", ".", "lower", ",", "self", ".", "upper", ")", "\n", "return", "self", ".", "change_saturation", "(", "image", ",", "labels", ")", "\n", "", "elif", "labels", "is", "None", ":", "\n", "            ", "return", "image", "\n", "", "else", ":", "\n", "            ", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.Brightness.__init__": [[233, 240], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "delta", ")", ":", "\n", "        ", "'''\n        Arguments:\n            delta (int): An integer, the amount to add to or subtract from the intensity\n                of every pixel.\n        '''", "\n", "self", ".", "delta", "=", "delta", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.Brightness.__call__": [[241, 247], ["numpy.clip"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "        ", "image", "=", "np", ".", "clip", "(", "image", "+", "self", ".", "delta", ",", "0", ",", "255", ")", "\n", "if", "labels", "is", "None", ":", "\n", "            ", "return", "image", "\n", "", "else", ":", "\n", "            ", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.RandomBrightness.__init__": [[256, 270], ["float", "float", "object_detection_2d_photometric_ops.Brightness", "ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "lower", "=", "-", "84", ",", "upper", "=", "84", ",", "prob", "=", "0.5", ")", ":", "\n", "        ", "'''\n        Arguments:\n            lower (int, optional): An integer, the lower bound for the random brightness change.\n            upper (int, optional): An integer, the upper bound for the random brightness change.\n                Must be greater than `lower`.\n            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n                unaltered image is returned.\n        '''", "\n", "if", "lower", ">=", "upper", ":", "raise", "ValueError", "(", "\"`upper` must be greater than `lower`.\"", ")", "\n", "self", ".", "lower", "=", "float", "(", "lower", ")", "\n", "self", ".", "upper", "=", "float", "(", "upper", ")", "\n", "self", ".", "prob", "=", "prob", "\n", "self", ".", "change_brightness", "=", "Brightness", "(", "delta", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.RandomBrightness.__call__": [[271, 280], ["numpy.random.uniform", "numpy.random.uniform", "object_detection_2d_photometric_ops.RandomBrightness.change_brightness"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "        ", "p", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "\n", "if", "p", ">=", "(", "1.0", "-", "self", ".", "prob", ")", ":", "\n", "            ", "self", ".", "change_brightness", ".", "delta", "=", "np", ".", "random", ".", "uniform", "(", "self", ".", "lower", ",", "self", ".", "upper", ")", "\n", "return", "self", ".", "change_brightness", "(", "image", ",", "labels", ")", "\n", "", "elif", "labels", "is", "None", ":", "\n", "            ", "return", "image", "\n", "", "else", ":", "\n", "            ", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.Contrast.__init__": [[289, 298], ["ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "factor", ")", ":", "\n", "        ", "'''\n        Arguments:\n            factor (float): A float greater than zero that determines contrast change, where\n                values less than one result in less contrast and values greater than one result\n                in more contrast.\n        '''", "\n", "if", "factor", "<=", "0.0", ":", "raise", "ValueError", "(", "\"It must be `factor > 0`.\"", ")", "\n", "self", ".", "factor", "=", "factor", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.Contrast.__call__": [[299, 305], ["numpy.clip"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "        ", "image", "=", "np", ".", "clip", "(", "127.5", "+", "self", ".", "factor", "*", "(", "image", "-", "127.5", ")", ",", "0", ",", "255", ")", "\n", "if", "labels", "is", "None", ":", "\n", "            ", "return", "image", "\n", "", "else", ":", "\n", "            ", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.RandomContrast.__init__": [[314, 329], ["object_detection_2d_photometric_ops.Contrast", "ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "lower", "=", "0.5", ",", "upper", "=", "1.5", ",", "prob", "=", "0.5", ")", ":", "\n", "        ", "'''\n        Arguments:\n            lower (float, optional): A float greater than zero, the lower bound for the random\n                contrast change.\n            upper (float, optional): A float greater than zero, the upper bound for the random\n                contrast change. Must be greater than `lower`.\n            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n                unaltered image is returned.\n        '''", "\n", "if", "lower", ">=", "upper", ":", "raise", "ValueError", "(", "\"`upper` must be greater than `lower`.\"", ")", "\n", "self", ".", "lower", "=", "lower", "\n", "self", ".", "upper", "=", "upper", "\n", "self", ".", "prob", "=", "prob", "\n", "self", ".", "change_contrast", "=", "Contrast", "(", "factor", "=", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.RandomContrast.__call__": [[330, 339], ["numpy.random.uniform", "numpy.random.uniform", "object_detection_2d_photometric_ops.RandomContrast.change_contrast"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "        ", "p", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "\n", "if", "p", ">=", "(", "1.0", "-", "self", ".", "prob", ")", ":", "\n", "            ", "self", ".", "change_contrast", ".", "factor", "=", "np", ".", "random", ".", "uniform", "(", "self", ".", "lower", ",", "self", ".", "upper", ")", "\n", "return", "self", ".", "change_contrast", "(", "image", ",", "labels", ")", "\n", "", "elif", "labels", "is", "None", ":", "\n", "            ", "return", "image", "\n", "", "else", ":", "\n", "            ", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.Gamma.__init__": [[346, 357], ["numpy.array().astype", "ValueError", "numpy.array", "numpy.arange"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "gamma", ")", ":", "\n", "        ", "'''\n        Arguments:\n            gamma (float): A float greater than zero that determines gamma change.\n        '''", "\n", "if", "gamma", "<=", "0.0", ":", "raise", "ValueError", "(", "\"It must be `gamma > 0`.\"", ")", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "gamma_inv", "=", "1.0", "/", "gamma", "\n", "# Build a lookup table mapping the pixel values [0, 255] to", "\n", "# their adjusted gamma values.", "\n", "self", ".", "table", "=", "np", ".", "array", "(", "[", "(", "(", "i", "/", "255.0", ")", "**", "self", ".", "gamma_inv", ")", "*", "255", "for", "i", "in", "np", ".", "arange", "(", "0", ",", "256", ")", "]", ")", ".", "astype", "(", "\"uint8\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.Gamma.__call__": [[358, 364], ["cv2.LUT"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "        ", "image", "=", "cv2", ".", "LUT", "(", "image", ",", "table", ")", "\n", "if", "labels", "is", "None", ":", "\n", "            ", "return", "image", "\n", "", "else", ":", "\n", "            ", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.RandomGamma.__init__": [[371, 385], ["ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "lower", "=", "0.25", ",", "upper", "=", "2.0", ",", "prob", "=", "0.5", ")", ":", "\n", "        ", "'''\n        Arguments:\n            lower (float, optional): A float greater than zero, the lower bound for the random\n                gamma change.\n            upper (float, optional): A float greater than zero, the upper bound for the random\n                gamma change. Must be greater than `lower`.\n            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n                unaltered image is returned.\n        '''", "\n", "if", "lower", ">=", "upper", ":", "raise", "ValueError", "(", "\"`upper` must be greater than `lower`.\"", ")", "\n", "self", ".", "lower", "=", "lower", "\n", "self", ".", "upper", "=", "upper", "\n", "self", ".", "prob", "=", "prob", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.RandomGamma.__call__": [[386, 396], ["numpy.random.uniform", "numpy.random.uniform", "object_detection_2d_photometric_ops.Gamma", "Gamma."], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "        ", "p", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "\n", "if", "p", ">=", "(", "1.0", "-", "self", ".", "prob", ")", ":", "\n", "            ", "gamma", "=", "np", ".", "random", ".", "uniform", "(", "self", ".", "lower", ",", "self", ".", "upper", ")", "\n", "change_gamma", "=", "Gamma", "(", "gamma", "=", "gamma", ")", "\n", "return", "change_gamma", "(", "image", ",", "labels", ")", "\n", "", "elif", "labels", "is", "None", ":", "\n", "            ", "return", "image", "\n", "", "else", ":", "\n", "            ", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.HistogramEqualization.__init__": [[403, 405], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.HistogramEqualization.__call__": [[406, 412], ["cv2.equalizeHist"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "        ", "image", "[", ":", ",", ":", ",", "2", "]", "=", "cv2", ".", "equalizeHist", "(", "image", "[", ":", ",", ":", ",", "2", "]", ")", "\n", "if", "labels", "is", "None", ":", "\n", "            ", "return", "image", "\n", "", "else", ":", "\n", "            ", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.RandomHistogramEqualization.__init__": [[420, 428], ["object_detection_2d_photometric_ops.HistogramEqualization"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "prob", "=", "0.5", ")", ":", "\n", "        ", "'''\n        Arguments:\n            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n                unaltered image is returned.\n        '''", "\n", "self", ".", "prob", "=", "prob", "\n", "self", ".", "equalize", "=", "HistogramEqualization", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.RandomHistogramEqualization.__call__": [[429, 437], ["numpy.random.uniform", "object_detection_2d_photometric_ops.RandomHistogramEqualization.equalize"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "        ", "p", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "\n", "if", "p", ">=", "(", "1.0", "-", "self", ".", "prob", ")", ":", "\n", "            ", "return", "self", ".", "equalize", "(", "image", ",", "labels", ")", "\n", "", "elif", "labels", "is", "None", ":", "\n", "            ", "return", "image", "\n", "", "else", ":", "\n", "            ", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.ChannelSwap.__init__": [[442, 449], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "order", ")", ":", "\n", "        ", "'''\n        Arguments:\n            order (tuple): A tuple of integers that defines the desired channel order\n                of the input images after the channel swap.\n        '''", "\n", "self", ".", "order", "=", "order", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.ChannelSwap.__call__": [[450, 456], ["None"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "        ", "image", "=", "image", "[", ":", ",", ":", ",", "self", ".", "order", "]", "\n", "if", "labels", "is", "None", ":", "\n", "            ", "return", "image", "\n", "", "else", ":", "\n", "            ", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.RandomChannelSwap.__init__": [[463, 475], ["object_detection_2d_photometric_ops.ChannelSwap"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "prob", "=", "0.5", ")", ":", "\n", "        ", "'''\n        Arguments:\n            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n                unaltered image is returned.\n        '''", "\n", "self", ".", "prob", "=", "prob", "\n", "# All possible permutations of the three image channels except the original order.", "\n", "self", ".", "permutations", "=", "(", "(", "0", ",", "2", ",", "1", ")", ",", "\n", "(", "1", ",", "0", ",", "2", ")", ",", "(", "1", ",", "2", ",", "0", ")", ",", "\n", "(", "2", ",", "0", ",", "1", ")", ",", "(", "2", ",", "1", ",", "0", ")", ")", "\n", "self", ".", "swap_channels", "=", "ChannelSwap", "(", "order", "=", "(", "0", ",", "1", ",", "2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_photometric_ops.RandomChannelSwap.__call__": [[476, 486], ["numpy.random.uniform", "numpy.random.randint", "object_detection_2d_photometric_ops.RandomChannelSwap.swap_channels"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "        ", "p", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "\n", "if", "p", ">=", "(", "1.0", "-", "self", ".", "prob", ")", ":", "\n", "            ", "i", "=", "np", ".", "random", ".", "randint", "(", "5", ")", "# There are 6 possible permutations.", "\n", "self", ".", "swap_channels", ".", "order", "=", "self", ".", "permutations", "[", "i", "]", "\n", "return", "self", ".", "swap_channels", "(", "image", ",", "labels", ")", "\n", "", "elif", "labels", "is", "None", ":", "\n", "            ", "return", "image", "\n", "", "else", ":", "\n", "            ", "return", "image", ",", "labels", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_patch_sampling_ops.PatchCoordinateGenerator.__init__": [[29, 116], ["ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "img_height", "=", "None", ",", "\n", "img_width", "=", "None", ",", "\n", "must_match", "=", "'h_w'", ",", "\n", "min_scale", "=", "0.3", ",", "\n", "max_scale", "=", "1.0", ",", "\n", "scale_uniformly", "=", "False", ",", "\n", "min_aspect_ratio", "=", "0.5", ",", "\n", "max_aspect_ratio", "=", "2.0", ",", "\n", "patch_ymin", "=", "None", ",", "\n", "patch_xmin", "=", "None", ",", "\n", "patch_height", "=", "None", ",", "\n", "patch_width", "=", "None", ",", "\n", "patch_aspect_ratio", "=", "None", ")", ":", "\n", "        ", "'''\n        Arguments:\n            img_height (int): The height of the image for which the patch coordinates\n                shall be generated. Doesn't have to be known upon construction.\n            img_width (int): The width of the image for which the patch coordinates\n                shall be generated. Doesn't have to be known upon construction.\n            must_match (str, optional): Can be either of 'h_w', 'h_ar', and 'w_ar'.\n                Specifies which two of the three quantities height, width, and aspect\n                ratio determine the shape of the generated patch. The respective third\n                quantity will be computed from the other two. For example,\n                if `must_match == 'h_w'`, then the patch's height and width will be\n                set to lie within [min_scale, max_scale] of the image size or to\n                `patch_height` and/or `patch_width`, if given. The patch's aspect ratio\n                is the dependent variable in this case, it will be computed from the\n                height and width. Any given values for `patch_aspect_ratio`,\n                `min_aspect_ratio`, or `max_aspect_ratio` will be ignored.\n            min_scale (float, optional): The minimum size of a dimension of the patch\n                as a fraction of the respective dimension of the image. Can be greater\n                than 1. For example, if the image width is 200 and `min_scale == 0.5`,\n                then the width of the generated patch will be at least 100. If `min_scale == 1.5`,\n                the width of the generated patch will be at least 300.\n            max_scale (float, optional): The maximum size of a dimension of the patch\n                as a fraction of the respective dimension of the image. Can be greater\n                than 1. For example, if the image width is 200 and `max_scale == 1.0`,\n                then the width of the generated patch will be at most 200. If `max_scale == 1.5`,\n                the width of the generated patch will be at most 300. Must be greater than\n                `min_scale`.\n            scale_uniformly (bool, optional): If `True` and if `must_match == 'h_w'`,\n                the patch height and width will be scaled uniformly, otherwise they will\n                be scaled independently.\n            min_aspect_ratio (float, optional): Determines the minimum aspect ratio\n                for the generated patches.\n            max_aspect_ratio (float, optional): Determines the maximum aspect ratio\n                for the generated patches.\n            patch_ymin (int, optional): `None` or the vertical coordinate of the top left\n                corner of the generated patches. If this is not `None`, the position of the\n                patches along the vertical axis is fixed. If this is `None`, then the\n                vertical position of generated patches will be chosen randomly such that\n                the overlap of a patch and the image along the vertical dimension is\n                always maximal.\n            patch_xmin (int, optional): `None` or the horizontal coordinate of the top left\n                corner of the generated patches. If this is not `None`, the position of the\n                patches along the horizontal axis is fixed. If this is `None`, then the\n                horizontal position of generated patches will be chosen randomly such that\n                the overlap of a patch and the image along the horizontal dimension is\n                always maximal.\n            patch_height (int, optional): `None` or the fixed height of the generated patches.\n            patch_width (int, optional): `None` or the fixed width of the generated patches.\n            patch_aspect_ratio (float, optional): `None` or the fixed aspect ratio of the\n                generated patches.\n        '''", "\n", "\n", "if", "not", "(", "must_match", "in", "{", "'h_w'", ",", "'h_ar'", ",", "'w_ar'", "}", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`must_match` must be either of 'h_w', 'h_ar' and 'w_ar'.\"", ")", "\n", "", "if", "min_scale", ">=", "max_scale", ":", "\n", "            ", "raise", "ValueError", "(", "\"It must be `min_scale < max_scale`.\"", ")", "\n", "", "if", "min_aspect_ratio", ">=", "max_aspect_ratio", ":", "\n", "            ", "raise", "ValueError", "(", "\"It must be `min_aspect_ratio < max_aspect_ratio`.\"", ")", "\n", "", "if", "scale_uniformly", "and", "not", "(", "(", "patch_height", "is", "None", ")", "and", "(", "patch_width", "is", "None", ")", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"If `scale_uniformly == True`, `patch_height` and `patch_width` must both be `None`.\"", ")", "\n", "", "self", ".", "img_height", "=", "img_height", "\n", "self", ".", "img_width", "=", "img_width", "\n", "self", ".", "must_match", "=", "must_match", "\n", "self", ".", "min_scale", "=", "min_scale", "\n", "self", ".", "max_scale", "=", "max_scale", "\n", "self", ".", "scale_uniformly", "=", "scale_uniformly", "\n", "self", ".", "min_aspect_ratio", "=", "min_aspect_ratio", "\n", "self", ".", "max_aspect_ratio", "=", "max_aspect_ratio", "\n", "self", ".", "patch_ymin", "=", "patch_ymin", "\n", "self", ".", "patch_xmin", "=", "patch_xmin", "\n", "self", ".", "patch_height", "=", "patch_height", "\n", "self", ".", "patch_width", "=", "patch_width", "\n", "self", ".", "patch_aspect_ratio", "=", "patch_aspect_ratio", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_patch_sampling_ops.PatchCoordinateGenerator.__call__": [[117, 198], ["numpy.random.uniform", "int", "int", "int", "numpy.random.randint", "numpy.random.randint", "numpy.random.randint", "numpy.random.randint", "int", "int", "int", "numpy.random.uniform", "int", "int", "numpy.random.uniform", "numpy.random.uniform", "numpy.random.uniform", "numpy.random.uniform", "numpy.random.uniform"], "methods", ["None"], ["", "def", "__call__", "(", "self", ")", ":", "\n", "        ", "'''\n        Returns:\n            A 4-tuple `(ymin, xmin, height, width)` that represents the coordinates\n            of the generated patch.\n        '''", "\n", "\n", "# Get the patch height and width.", "\n", "\n", "if", "self", ".", "must_match", "==", "'h_w'", ":", "# Aspect is the dependent variable.", "\n", "            ", "if", "not", "self", ".", "scale_uniformly", ":", "\n", "# Get the height.", "\n", "                ", "if", "self", ".", "patch_height", "is", "None", ":", "\n", "                    ", "patch_height", "=", "int", "(", "np", ".", "random", ".", "uniform", "(", "self", ".", "min_scale", ",", "self", ".", "max_scale", ")", "*", "self", ".", "img_height", ")", "\n", "", "else", ":", "\n", "                    ", "patch_height", "=", "self", ".", "patch_height", "\n", "# Get the width.", "\n", "", "if", "self", ".", "patch_width", "is", "None", ":", "\n", "                    ", "patch_width", "=", "int", "(", "np", ".", "random", ".", "uniform", "(", "self", ".", "min_scale", ",", "self", ".", "max_scale", ")", "*", "self", ".", "img_width", ")", "\n", "", "else", ":", "\n", "                    ", "patch_width", "=", "self", ".", "patch_width", "\n", "", "", "else", ":", "\n", "                ", "scaling_factor", "=", "np", ".", "random", ".", "uniform", "(", "self", ".", "min_scale", ",", "self", ".", "max_scale", ")", "\n", "patch_height", "=", "int", "(", "scaling_factor", "*", "self", ".", "img_height", ")", "\n", "patch_width", "=", "int", "(", "scaling_factor", "*", "self", ".", "img_width", ")", "\n", "\n", "", "", "elif", "self", ".", "must_match", "==", "'h_ar'", ":", "# Width is the dependent variable.", "\n", "# Get the height.", "\n", "            ", "if", "self", ".", "patch_height", "is", "None", ":", "\n", "                ", "patch_height", "=", "int", "(", "np", ".", "random", ".", "uniform", "(", "self", ".", "min_scale", ",", "self", ".", "max_scale", ")", "*", "self", ".", "img_height", ")", "\n", "", "else", ":", "\n", "                ", "patch_height", "=", "self", ".", "patch_height", "\n", "# Get the aspect ratio.", "\n", "", "if", "self", ".", "patch_aspect_ratio", "is", "None", ":", "\n", "                ", "patch_aspect_ratio", "=", "np", ".", "random", ".", "uniform", "(", "self", ".", "min_aspect_ratio", ",", "self", ".", "max_aspect_ratio", ")", "\n", "", "else", ":", "\n", "                ", "patch_aspect_ratio", "=", "self", ".", "patch_aspect_ratio", "\n", "# Get the width.", "\n", "", "patch_width", "=", "int", "(", "patch_height", "*", "patch_aspect_ratio", ")", "\n", "\n", "", "elif", "self", ".", "must_match", "==", "'w_ar'", ":", "# Height is the dependent variable.", "\n", "# Get the width.", "\n", "            ", "if", "self", ".", "patch_width", "is", "None", ":", "\n", "                ", "patch_width", "=", "int", "(", "np", ".", "random", ".", "uniform", "(", "self", ".", "min_scale", ",", "self", ".", "max_scale", ")", "*", "self", ".", "img_width", ")", "\n", "", "else", ":", "\n", "                ", "patch_width", "=", "self", ".", "patch_width", "\n", "# Get the aspect ratio.", "\n", "", "if", "self", ".", "patch_aspect_ratio", "is", "None", ":", "\n", "                ", "patch_aspect_ratio", "=", "np", ".", "random", ".", "uniform", "(", "self", ".", "min_aspect_ratio", ",", "self", ".", "max_aspect_ratio", ")", "\n", "", "else", ":", "\n", "                ", "patch_aspect_ratio", "=", "self", ".", "patch_aspect_ratio", "\n", "# Get the height.", "\n", "", "patch_height", "=", "int", "(", "patch_width", "/", "patch_aspect_ratio", ")", "\n", "\n", "# Get the top left corner coordinates of the patch.", "\n", "\n", "", "if", "self", ".", "patch_ymin", "is", "None", ":", "\n", "# Compute how much room we have along the vertical axis to place the patch.", "\n", "# A negative number here means that we want to sample a patch that is larger than the original image", "\n", "# in the vertical dimension, in which case the patch will be placed such that it fully contains the", "\n", "# image in the vertical dimension.", "\n", "            ", "y_range", "=", "self", ".", "img_height", "-", "patch_height", "\n", "# Select a random top left corner for the sample position from the possible positions.", "\n", "if", "y_range", ">=", "0", ":", "patch_ymin", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "y_range", "+", "1", ")", "# There are y_range + 1 possible positions for the crop in the vertical dimension.", "\n", "else", ":", "patch_ymin", "=", "np", ".", "random", ".", "randint", "(", "y_range", ",", "1", ")", "# The possible positions for the image on the background canvas in the vertical dimension.", "\n", "", "else", ":", "\n", "            ", "patch_ymin", "=", "self", ".", "patch_ymin", "\n", "\n", "", "if", "self", ".", "patch_xmin", "is", "None", ":", "\n", "# Compute how much room we have along the horizontal axis to place the patch.", "\n", "# A negative number here means that we want to sample a patch that is larger than the original image", "\n", "# in the horizontal dimension, in which case the patch will be placed such that it fully contains the", "\n", "# image in the horizontal dimension.", "\n", "            ", "x_range", "=", "self", ".", "img_width", "-", "patch_width", "\n", "# Select a random top left corner for the sample position from the possible positions.", "\n", "if", "x_range", ">=", "0", ":", "patch_xmin", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "x_range", "+", "1", ")", "# There are x_range + 1 possible positions for the crop in the horizontal dimension.", "\n", "else", ":", "patch_xmin", "=", "np", ".", "random", ".", "randint", "(", "x_range", ",", "1", ")", "# The possible positions for the image on the background canvas in the horizontal dimension.", "\n", "", "else", ":", "\n", "            ", "patch_xmin", "=", "self", ".", "patch_xmin", "\n", "\n", "", "return", "(", "patch_ymin", ",", "patch_xmin", ",", "patch_height", ",", "patch_width", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_patch_sampling_ops.CropPad.__init__": [[216, 265], ["ValueError", "isinstance"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "patch_ymin", ",", "\n", "patch_xmin", ",", "\n", "patch_height", ",", "\n", "patch_width", ",", "\n", "clip_boxes", "=", "True", ",", "\n", "box_filter", "=", "None", ",", "\n", "background", "=", "(", "0", ",", "0", ",", "0", ")", ",", "\n", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ")", ":", "\n", "        ", "'''\n        Arguments:\n            patch_ymin (int, optional): The vertical coordinate of the top left corner of the output\n                patch relative to the image coordinate system. Can be negative (i.e. lie outside the image)\n                as long as the resulting patch still overlaps with the image.\n            patch_ymin (int, optional): The horizontal coordinate of the top left corner of the output\n                patch relative to the image coordinate system. Can be negative (i.e. lie outside the image)\n                as long as the resulting patch still overlaps with the image.\n            patch_height (int): The height of the patch to be sampled from the image. Can be greater\n                than the height of the input image.\n            patch_width (int): The width of the patch to be sampled from the image. Can be greater\n                than the width of the input image.\n            clip_boxes (bool, optional): Only relevant if ground truth bounding boxes are given.\n                If `True`, any ground truth bounding boxes will be clipped to lie entirely within the\n                sampled patch.\n            box_filter (BoxFilter, optional): Only relevant if ground truth bounding boxes are given.\n                A `BoxFilter` object to filter out bounding boxes that don't meet the given criteria\n                after the transformation. Refer to the `BoxFilter` documentation for details. If `None`,\n                the validity of the bounding boxes is not checked.\n            background (list/tuple, optional): A 3-tuple specifying the RGB color value of the potential\n                background pixels of the scaled images. In the case of single-channel images,\n                the first element of `background` will be used as the background pixel value.\n            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n        '''", "\n", "#if (patch_height <= 0) or (patch_width <= 0):", "\n", "#    raise ValueError(\"Patch height and width must both be positive.\")", "\n", "#if (patch_ymin + patch_height < 0) or (patch_xmin + patch_width < 0):", "\n", "#    raise ValueError(\"A patch with the given coordinates cannot overlap with an input image.\")", "\n", "if", "not", "(", "isinstance", "(", "box_filter", ",", "BoxFilter", ")", "or", "box_filter", "is", "None", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`box_filter` must be either `None` or a `BoxFilter` object.\"", ")", "\n", "", "self", ".", "patch_height", "=", "patch_height", "\n", "self", ".", "patch_width", "=", "patch_width", "\n", "self", ".", "patch_ymin", "=", "patch_ymin", "\n", "self", ".", "patch_xmin", "=", "patch_xmin", "\n", "self", ".", "clip_boxes", "=", "clip_boxes", "\n", "self", ".", "box_filter", "=", "box_filter", "\n", "self", ".", "background", "=", "background", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_patch_sampling_ops.CropPad.__call__": [[266, 349], ["numpy.copy", "ValueError", "numpy.zeros", "min", "min", "numpy.zeros", "min", "min", "numpy.copy", "object_detection_2d_patch_sampling_ops.CropPad.box_filter", "numpy.clip", "numpy.clip", "min", "min", "min", "min"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ",", "return_inverter", "=", "False", ")", ":", "\n", "\n", "        ", "img_height", ",", "img_width", "=", "image", ".", "shape", "[", ":", "2", "]", "\n", "\n", "if", "(", "self", ".", "patch_ymin", ">", "img_height", ")", "or", "(", "self", ".", "patch_xmin", ">", "img_width", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"The given patch doesn't overlap with the input image.\"", ")", "\n", "\n", "", "labels", "=", "np", ".", "copy", "(", "labels", ")", "\n", "\n", "xmin", "=", "self", ".", "labels_format", "[", "'xmin'", "]", "\n", "ymin", "=", "self", ".", "labels_format", "[", "'ymin'", "]", "\n", "xmax", "=", "self", ".", "labels_format", "[", "'xmax'", "]", "\n", "ymax", "=", "self", ".", "labels_format", "[", "'ymax'", "]", "\n", "\n", "# Top left corner of the patch relative to the image coordinate system:", "\n", "patch_ymin", "=", "self", ".", "patch_ymin", "\n", "patch_xmin", "=", "self", ".", "patch_xmin", "\n", "\n", "# Create a canvas of the size of the patch we want to end up with.", "\n", "if", "image", ".", "ndim", "==", "3", ":", "\n", "            ", "canvas", "=", "np", ".", "zeros", "(", "shape", "=", "(", "self", ".", "patch_height", ",", "self", ".", "patch_width", ",", "3", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "canvas", "[", ":", ",", ":", "]", "=", "self", ".", "background", "\n", "", "elif", "image", ".", "ndim", "==", "2", ":", "\n", "            ", "canvas", "=", "np", ".", "zeros", "(", "shape", "=", "(", "self", ".", "patch_height", ",", "self", ".", "patch_width", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "canvas", "[", ":", ",", ":", "]", "=", "self", ".", "background", "[", "0", "]", "\n", "\n", "# Perform the crop.", "\n", "", "if", "patch_ymin", "<", "0", "and", "patch_xmin", "<", "0", ":", "# Pad the image at the top and on the left.", "\n", "            ", "image_crop_height", "=", "min", "(", "img_height", ",", "self", ".", "patch_height", "+", "patch_ymin", ")", "# The number of pixels of the image that will end up on the canvas in the vertical direction.", "\n", "image_crop_width", "=", "min", "(", "img_width", ",", "self", ".", "patch_width", "+", "patch_xmin", ")", "# The number of pixels of the image that will end up on the canvas in the horizontal direction.", "\n", "canvas", "[", "-", "patch_ymin", ":", "-", "patch_ymin", "+", "image_crop_height", ",", "-", "patch_xmin", ":", "-", "patch_xmin", "+", "image_crop_width", "]", "=", "image", "[", ":", "image_crop_height", ",", ":", "image_crop_width", "]", "\n", "\n", "", "elif", "patch_ymin", "<", "0", "and", "patch_xmin", ">=", "0", ":", "# Pad the image at the top and crop it on the left.", "\n", "            ", "image_crop_height", "=", "min", "(", "img_height", ",", "self", ".", "patch_height", "+", "patch_ymin", ")", "# The number of pixels of the image that will end up on the canvas in the vertical direction.", "\n", "image_crop_width", "=", "min", "(", "self", ".", "patch_width", ",", "img_width", "-", "patch_xmin", ")", "# The number of pixels of the image that will end up on the canvas in the horizontal direction.", "\n", "canvas", "[", "-", "patch_ymin", ":", "-", "patch_ymin", "+", "image_crop_height", ",", ":", "image_crop_width", "]", "=", "image", "[", ":", "image_crop_height", ",", "patch_xmin", ":", "patch_xmin", "+", "image_crop_width", "]", "\n", "\n", "", "elif", "patch_ymin", ">=", "0", "and", "patch_xmin", "<", "0", ":", "# Crop the image at the top and pad it on the left.", "\n", "            ", "image_crop_height", "=", "min", "(", "self", ".", "patch_height", ",", "img_height", "-", "patch_ymin", ")", "# The number of pixels of the image that will end up on the canvas in the vertical direction.", "\n", "image_crop_width", "=", "min", "(", "img_width", ",", "self", ".", "patch_width", "+", "patch_xmin", ")", "# The number of pixels of the image that will end up on the canvas in the horizontal direction.", "\n", "canvas", "[", ":", "image_crop_height", ",", "-", "patch_xmin", ":", "-", "patch_xmin", "+", "image_crop_width", "]", "=", "image", "[", "patch_ymin", ":", "patch_ymin", "+", "image_crop_height", ",", ":", "image_crop_width", "]", "\n", "\n", "", "elif", "patch_ymin", ">=", "0", "and", "patch_xmin", ">=", "0", ":", "# Crop the image at the top and on the left.", "\n", "            ", "image_crop_height", "=", "min", "(", "self", ".", "patch_height", ",", "img_height", "-", "patch_ymin", ")", "# The number of pixels of the image that will end up on the canvas in the vertical direction.", "\n", "image_crop_width", "=", "min", "(", "self", ".", "patch_width", ",", "img_width", "-", "patch_xmin", ")", "# The number of pixels of the image that will end up on the canvas in the horizontal direction.", "\n", "canvas", "[", ":", "image_crop_height", ",", ":", "image_crop_width", "]", "=", "image", "[", "patch_ymin", ":", "patch_ymin", "+", "image_crop_height", ",", "patch_xmin", ":", "patch_xmin", "+", "image_crop_width", "]", "\n", "\n", "", "image", "=", "canvas", "\n", "\n", "if", "return_inverter", ":", "\n", "            ", "def", "inverter", "(", "labels", ")", ":", "\n", "                ", "labels", "=", "np", ".", "copy", "(", "labels", ")", "\n", "labels", "[", ":", ",", "[", "ymin", "+", "1", ",", "ymax", "+", "1", "]", "]", "+=", "patch_ymin", "\n", "labels", "[", ":", ",", "[", "xmin", "+", "1", ",", "xmax", "+", "1", "]", "]", "+=", "patch_xmin", "\n", "return", "labels", "\n", "\n", "", "", "if", "not", "(", "labels", "is", "None", ")", ":", "\n", "\n", "# Translate the box coordinates to the patch's coordinate system.", "\n", "            ", "labels", "[", ":", ",", "[", "ymin", ",", "ymax", "]", "]", "-=", "patch_ymin", "\n", "labels", "[", ":", ",", "[", "xmin", ",", "xmax", "]", "]", "-=", "patch_xmin", "\n", "\n", "# Compute all valid boxes for this patch.", "\n", "if", "not", "(", "self", ".", "box_filter", "is", "None", ")", ":", "\n", "                ", "self", ".", "box_filter", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "labels", "=", "self", ".", "box_filter", "(", "labels", "=", "labels", ",", "\n", "image_height", "=", "self", ".", "patch_height", ",", "\n", "image_width", "=", "self", ".", "patch_width", ")", "\n", "\n", "", "if", "self", ".", "clip_boxes", ":", "\n", "                ", "labels", "[", ":", ",", "[", "ymin", ",", "ymax", "]", "]", "=", "np", ".", "clip", "(", "labels", "[", ":", ",", "[", "ymin", ",", "ymax", "]", "]", ",", "a_min", "=", "0", ",", "a_max", "=", "self", ".", "patch_height", "-", "1", ")", "\n", "labels", "[", ":", ",", "[", "xmin", ",", "xmax", "]", "]", "=", "np", ".", "clip", "(", "labels", "[", ":", ",", "[", "xmin", ",", "xmax", "]", "]", ",", "a_min", "=", "0", ",", "a_max", "=", "self", ".", "patch_width", "-", "1", ")", "\n", "\n", "", "if", "return_inverter", ":", "\n", "                ", "return", "image", ",", "labels", ",", "inverter", "\n", "", "else", ":", "\n", "                ", "return", "image", ",", "labels", "\n", "\n", "", "", "else", ":", "\n", "            ", "if", "return_inverter", ":", "\n", "                ", "return", "image", ",", "inverter", "\n", "", "else", ":", "\n", "                ", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_patch_sampling_ops.Crop.__init__": [[357, 379], ["object_detection_2d_patch_sampling_ops.CropPad"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "crop_top", ",", "\n", "crop_bottom", ",", "\n", "crop_left", ",", "\n", "crop_right", ",", "\n", "clip_boxes", "=", "True", ",", "\n", "box_filter", "=", "None", ",", "\n", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ")", ":", "\n", "        ", "self", ".", "crop_top", "=", "crop_top", "\n", "self", ".", "crop_bottom", "=", "crop_bottom", "\n", "self", ".", "crop_left", "=", "crop_left", "\n", "self", ".", "crop_right", "=", "crop_right", "\n", "self", ".", "clip_boxes", "=", "clip_boxes", "\n", "self", ".", "box_filter", "=", "box_filter", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "self", ".", "crop", "=", "CropPad", "(", "patch_ymin", "=", "self", ".", "crop_top", ",", "\n", "patch_xmin", "=", "self", ".", "crop_left", ",", "\n", "patch_height", "=", "None", ",", "\n", "patch_width", "=", "None", ",", "\n", "clip_boxes", "=", "self", ".", "clip_boxes", ",", "\n", "box_filter", "=", "self", ".", "box_filter", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_patch_sampling_ops.Crop.__call__": [[380, 389], ["object_detection_2d_patch_sampling_ops.Crop.crop"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ",", "return_inverter", "=", "False", ")", ":", "\n", "\n", "        ", "img_height", ",", "img_width", "=", "image", ".", "shape", "[", ":", "2", "]", "\n", "\n", "self", ".", "crop", ".", "patch_height", "=", "img_height", "-", "self", ".", "crop_top", "-", "self", ".", "crop_bottom", "\n", "self", ".", "crop", ".", "patch_width", "=", "img_width", "-", "self", ".", "crop_left", "-", "self", ".", "crop_right", "\n", "self", ".", "crop", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "\n", "return", "self", ".", "crop", "(", "image", ",", "labels", ",", "return_inverter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_patch_sampling_ops.Pad.__init__": [[397, 418], ["object_detection_2d_patch_sampling_ops.CropPad"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "pad_top", ",", "\n", "pad_bottom", ",", "\n", "pad_left", ",", "\n", "pad_right", ",", "\n", "background", "=", "(", "0", ",", "0", ",", "0", ")", ",", "\n", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ")", ":", "\n", "        ", "self", ".", "pad_top", "=", "pad_top", "\n", "self", ".", "pad_bottom", "=", "pad_bottom", "\n", "self", ".", "pad_left", "=", "pad_left", "\n", "self", ".", "pad_right", "=", "pad_right", "\n", "self", ".", "background", "=", "background", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "self", ".", "pad", "=", "CropPad", "(", "patch_ymin", "=", "-", "self", ".", "pad_top", ",", "\n", "patch_xmin", "=", "-", "self", ".", "pad_left", ",", "\n", "patch_height", "=", "None", ",", "\n", "patch_width", "=", "None", ",", "\n", "clip_boxes", "=", "False", ",", "\n", "box_filter", "=", "None", ",", "\n", "background", "=", "self", ".", "background", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_patch_sampling_ops.Pad.__call__": [[419, 428], ["object_detection_2d_patch_sampling_ops.Pad.pad"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ",", "return_inverter", "=", "False", ")", ":", "\n", "\n", "        ", "img_height", ",", "img_width", "=", "image", ".", "shape", "[", ":", "2", "]", "\n", "\n", "self", ".", "pad", ".", "patch_height", "=", "img_height", "+", "self", ".", "pad_top", "+", "self", ".", "pad_bottom", "\n", "self", ".", "pad", ".", "patch_width", "=", "img_width", "+", "self", ".", "pad_left", "+", "self", ".", "pad_right", "\n", "self", ".", "pad", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "\n", "return", "self", ".", "pad", "(", "image", ",", "labels", ",", "return_inverter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_patch_sampling_ops.RandomPatch.__init__": [[446, 505], ["object_detection_2d_patch_sampling_ops.CropPad", "isinstance", "ValueError", "ValueError", "isinstance"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "patch_coord_generator", ",", "\n", "box_filter", "=", "None", ",", "\n", "image_validator", "=", "None", ",", "\n", "n_trials_max", "=", "3", ",", "\n", "clip_boxes", "=", "True", ",", "\n", "prob", "=", "1.0", ",", "\n", "background", "=", "(", "0", ",", "0", ",", "0", ")", ",", "\n", "can_fail", "=", "False", ",", "\n", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ")", ":", "\n", "        ", "'''\n        Arguments:\n            patch_coord_generator (PatchCoordinateGenerator): A `PatchCoordinateGenerator` object\n                to generate the positions and sizes of the patches to be sampled from the input images.\n            box_filter (BoxFilter, optional): Only relevant if ground truth bounding boxes are given.\n                A `BoxFilter` object to filter out bounding boxes that don't meet the given criteria\n                after the transformation. Refer to the `BoxFilter` documentation for details. If `None`,\n                the validity of the bounding boxes is not checked.\n            image_validator (ImageValidator, optional): Only relevant if ground truth bounding boxes are given.\n                An `ImageValidator` object to determine whether a sampled patch is valid. If `None`,\n                any outcome is valid.\n            n_trials_max (int, optional): Only relevant if ground truth bounding boxes are given.\n                Determines the maxmial number of trials to sample a valid patch. If no valid patch could\n                be sampled in `n_trials_max` trials, returns one `None` in place of each regular output.\n            clip_boxes (bool, optional): Only relevant if ground truth bounding boxes are given.\n                If `True`, any ground truth bounding boxes will be clipped to lie entirely within the\n                sampled patch.\n            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n                unaltered image is returned.\n            background (list/tuple, optional): A 3-tuple specifying the RGB color value of the potential\n                background pixels of the scaled images. In the case of single-channel images,\n                the first element of `background` will be used as the background pixel value.\n            can_fail (bool, optional): If `True`, will return `None` if no valid patch could be found after\n                `n_trials_max` trials. If `False`, will return the unaltered input image in such a case.\n            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n        '''", "\n", "if", "not", "isinstance", "(", "patch_coord_generator", ",", "PatchCoordinateGenerator", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`patch_coord_generator` must be an instance of `PatchCoordinateGenerator`.\"", ")", "\n", "", "if", "not", "(", "isinstance", "(", "image_validator", ",", "ImageValidator", ")", "or", "image_validator", "is", "None", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`image_validator` must be either `None` or an `ImageValidator` object.\"", ")", "\n", "", "self", ".", "patch_coord_generator", "=", "patch_coord_generator", "\n", "self", ".", "box_filter", "=", "box_filter", "\n", "self", ".", "image_validator", "=", "image_validator", "\n", "self", ".", "n_trials_max", "=", "n_trials_max", "\n", "self", ".", "clip_boxes", "=", "clip_boxes", "\n", "self", ".", "prob", "=", "prob", "\n", "self", ".", "background", "=", "background", "\n", "self", ".", "can_fail", "=", "can_fail", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "self", ".", "sample_patch", "=", "CropPad", "(", "patch_ymin", "=", "None", ",", "\n", "patch_xmin", "=", "None", ",", "\n", "patch_height", "=", "None", ",", "\n", "patch_width", "=", "None", ",", "\n", "clip_boxes", "=", "self", ".", "clip_boxes", ",", "\n", "box_filter", "=", "self", ".", "box_filter", ",", "\n", "background", "=", "self", ".", "background", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_patch_sampling_ops.RandomPatch.__call__": [[506, 590], ["numpy.random.uniform", "range", "max", "object_detection_2d_patch_sampling_ops.RandomPatch.patch_coord_generator", "object_detection_2d_patch_sampling_ops.RandomPatch.sample_patch", "numpy.copy", "object_detection_2d_patch_sampling_ops.RandomPatch.image_validator", "object_detection_2d_patch_sampling_ops.RandomPatch.sample_patch"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ",", "return_inverter", "=", "False", ")", ":", "\n", "\n", "        ", "p", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "\n", "if", "p", ">=", "(", "1.0", "-", "self", ".", "prob", ")", ":", "\n", "\n", "            ", "img_height", ",", "img_width", "=", "image", ".", "shape", "[", ":", "2", "]", "\n", "self", ".", "patch_coord_generator", ".", "img_height", "=", "img_height", "\n", "self", ".", "patch_coord_generator", ".", "img_width", "=", "img_width", "\n", "\n", "xmin", "=", "self", ".", "labels_format", "[", "'xmin'", "]", "\n", "ymin", "=", "self", ".", "labels_format", "[", "'ymin'", "]", "\n", "xmax", "=", "self", ".", "labels_format", "[", "'xmax'", "]", "\n", "ymax", "=", "self", ".", "labels_format", "[", "'ymax'", "]", "\n", "\n", "# Override the preset labels format.", "\n", "if", "not", "self", ".", "image_validator", "is", "None", ":", "\n", "                ", "self", ".", "image_validator", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "", "self", ".", "sample_patch", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "\n", "for", "_", "in", "range", "(", "max", "(", "1", ",", "self", ".", "n_trials_max", ")", ")", ":", "\n", "\n", "# Generate patch coordinates.", "\n", "                ", "patch_ymin", ",", "patch_xmin", ",", "patch_height", ",", "patch_width", "=", "self", ".", "patch_coord_generator", "(", ")", "\n", "\n", "self", ".", "sample_patch", ".", "patch_ymin", "=", "patch_ymin", "\n", "self", ".", "sample_patch", ".", "patch_xmin", "=", "patch_xmin", "\n", "self", ".", "sample_patch", ".", "patch_height", "=", "patch_height", "\n", "self", ".", "sample_patch", ".", "patch_width", "=", "patch_width", "\n", "\n", "if", "(", "labels", "is", "None", ")", "or", "(", "self", ".", "image_validator", "is", "None", ")", ":", "\n", "# We either don't have any boxes or if we do, we will accept any outcome as valid.", "\n", "                    ", "return", "self", ".", "sample_patch", "(", "image", ",", "labels", ",", "return_inverter", ")", "\n", "", "else", ":", "\n", "# Translate the box coordinates to the patch's coordinate system.", "\n", "                    ", "new_labels", "=", "np", ".", "copy", "(", "labels", ")", "\n", "new_labels", "[", ":", ",", "[", "ymin", ",", "ymax", "]", "]", "-=", "patch_ymin", "\n", "new_labels", "[", ":", ",", "[", "xmin", ",", "xmax", "]", "]", "-=", "patch_xmin", "\n", "# Check if the patch is valid.", "\n", "if", "self", ".", "image_validator", "(", "labels", "=", "new_labels", ",", "\n", "image_height", "=", "patch_height", ",", "\n", "image_width", "=", "patch_width", ")", ":", "\n", "                        ", "return", "self", ".", "sample_patch", "(", "image", ",", "labels", ",", "return_inverter", ")", "\n", "\n", "# If we weren't able to sample a valid patch...", "\n", "", "", "", "if", "self", ".", "can_fail", ":", "\n", "# ...return `None`.", "\n", "                ", "if", "labels", "is", "None", ":", "\n", "                    ", "if", "return_inverter", ":", "\n", "                        ", "return", "None", ",", "None", "\n", "", "else", ":", "\n", "                        ", "return", "None", "\n", "", "", "else", ":", "\n", "                    ", "if", "return_inverter", ":", "\n", "                        ", "return", "None", ",", "None", ",", "None", "\n", "", "else", ":", "\n", "                        ", "return", "None", ",", "None", "\n", "", "", "", "else", ":", "\n", "# ...return the unaltered input image.", "\n", "                ", "if", "labels", "is", "None", ":", "\n", "                    ", "if", "return_inverter", ":", "\n", "                        ", "return", "image", ",", "None", "\n", "", "else", ":", "\n", "                        ", "return", "image", "\n", "", "", "else", ":", "\n", "                    ", "if", "return_inverter", ":", "\n", "                        ", "return", "image", ",", "labels", ",", "None", "\n", "", "else", ":", "\n", "                        ", "return", "image", ",", "labels", "\n", "\n", "", "", "", "", "else", ":", "\n", "            ", "if", "return_inverter", ":", "\n", "                ", "def", "inverter", "(", "labels", ")", ":", "\n", "                    ", "return", "labels", "\n", "\n", "", "", "if", "labels", "is", "None", ":", "\n", "                ", "if", "return_inverter", ":", "\n", "                    ", "return", "image", ",", "inverter", "\n", "", "else", ":", "\n", "                    ", "return", "image", "\n", "", "", "else", ":", "\n", "                ", "if", "return_inverter", ":", "\n", "                    ", "return", "image", ",", "labels", ",", "inverter", "\n", "", "else", ":", "\n", "                    ", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_patch_sampling_ops.RandomPatchInf.__init__": [[607, 672], ["object_detection_2d_patch_sampling_ops.CropPad", "isinstance", "ValueError", "ValueError", "ValueError", "isinstance", "isinstance"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "patch_coord_generator", ",", "\n", "box_filter", "=", "None", ",", "\n", "image_validator", "=", "None", ",", "\n", "bound_generator", "=", "None", ",", "\n", "n_trials_max", "=", "50", ",", "\n", "clip_boxes", "=", "True", ",", "\n", "prob", "=", "0.857", ",", "\n", "background", "=", "(", "0", ",", "0", ",", "0", ")", ",", "\n", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ")", ":", "\n", "        ", "'''\n        Arguments:\n            patch_coord_generator (PatchCoordinateGenerator): A `PatchCoordinateGenerator` object\n                to generate the positions and sizes of the patches to be sampled from the input images.\n            box_filter (BoxFilter, optional): Only relevant if ground truth bounding boxes are given.\n                A `BoxFilter` object to filter out bounding boxes that don't meet the given criteria\n                after the transformation. Refer to the `BoxFilter` documentation for details. If `None`,\n                the validity of the bounding boxes is not checked.\n            image_validator (ImageValidator, optional): Only relevant if ground truth bounding boxes are given.\n                An `ImageValidator` object to determine whether a sampled patch is valid. If `None`,\n                any outcome is valid.\n            bound_generator (BoundGenerator, optional): A `BoundGenerator` object to generate upper and\n                lower bound values for the patch validator. Every `n_trials_max` trials, a new pair of\n                upper and lower bounds will be generated until a valid patch is found or the original image\n                is returned. This bound generator overrides the bound generator of the patch validator.\n            n_trials_max (int, optional): Only relevant if ground truth bounding boxes are given.\n                The sampler will run indefinitely until either a valid patch is found or the original image\n                is returned, but this determines the maxmial number of trials to sample a valid patch for each\n                selected pair of lower and upper bounds before a new pair is picked.\n            clip_boxes (bool, optional): Only relevant if ground truth bounding boxes are given.\n                If `True`, any ground truth bounding boxes will be clipped to lie entirely within the\n                sampled patch.\n            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n                unaltered image is returned.\n            background (list/tuple, optional): A 3-tuple specifying the RGB color value of the potential\n                background pixels of the scaled images. In the case of single-channel images,\n                the first element of `background` will be used as the background pixel value.\n            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n        '''", "\n", "\n", "if", "not", "isinstance", "(", "patch_coord_generator", ",", "PatchCoordinateGenerator", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`patch_coord_generator` must be an instance of `PatchCoordinateGenerator`.\"", ")", "\n", "", "if", "not", "(", "isinstance", "(", "image_validator", ",", "ImageValidator", ")", "or", "image_validator", "is", "None", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`image_validator` must be either `None` or an `ImageValidator` object.\"", ")", "\n", "", "if", "not", "(", "isinstance", "(", "bound_generator", ",", "BoundGenerator", ")", "or", "bound_generator", "is", "None", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`bound_generator` must be either `None` or a `BoundGenerator` object.\"", ")", "\n", "", "self", ".", "patch_coord_generator", "=", "patch_coord_generator", "\n", "self", ".", "box_filter", "=", "box_filter", "\n", "self", ".", "image_validator", "=", "image_validator", "\n", "self", ".", "bound_generator", "=", "bound_generator", "\n", "self", ".", "n_trials_max", "=", "n_trials_max", "\n", "self", ".", "clip_boxes", "=", "clip_boxes", "\n", "self", ".", "prob", "=", "prob", "\n", "self", ".", "background", "=", "background", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "self", ".", "sample_patch", "=", "CropPad", "(", "patch_ymin", "=", "None", ",", "\n", "patch_xmin", "=", "None", ",", "\n", "patch_height", "=", "None", ",", "\n", "patch_width", "=", "None", ",", "\n", "clip_boxes", "=", "self", ".", "clip_boxes", ",", "\n", "box_filter", "=", "self", ".", "box_filter", ",", "\n", "background", "=", "self", ".", "background", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_patch_sampling_ops.RandomPatchInf.__call__": [[673, 743], ["numpy.random.uniform", "range", "object_detection_2d_patch_sampling_ops.RandomPatchInf.bound_generator", "max", "object_detection_2d_patch_sampling_ops.RandomPatchInf.patch_coord_generator", "object_detection_2d_patch_sampling_ops.RandomPatchInf.sample_patch", "numpy.copy", "object_detection_2d_patch_sampling_ops.RandomPatchInf.image_validator", "object_detection_2d_patch_sampling_ops.RandomPatchInf.sample_patch"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ",", "return_inverter", "=", "False", ")", ":", "\n", "\n", "        ", "img_height", ",", "img_width", "=", "image", ".", "shape", "[", ":", "2", "]", "\n", "self", ".", "patch_coord_generator", ".", "img_height", "=", "img_height", "\n", "self", ".", "patch_coord_generator", ".", "img_width", "=", "img_width", "\n", "\n", "xmin", "=", "self", ".", "labels_format", "[", "'xmin'", "]", "\n", "ymin", "=", "self", ".", "labels_format", "[", "'ymin'", "]", "\n", "xmax", "=", "self", ".", "labels_format", "[", "'xmax'", "]", "\n", "ymax", "=", "self", ".", "labels_format", "[", "'ymax'", "]", "\n", "\n", "# Override the preset labels format.", "\n", "if", "not", "self", ".", "image_validator", "is", "None", ":", "\n", "            ", "self", ".", "image_validator", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "", "self", ".", "sample_patch", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "\n", "while", "True", ":", "# Keep going until we either find a valid patch or return the original image.", "\n", "\n", "            ", "p", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "\n", "if", "p", ">=", "(", "1.0", "-", "self", ".", "prob", ")", ":", "\n", "\n", "# In case we have a bound generator, pick a lower and upper bound for the patch validator.", "\n", "                ", "if", "not", "(", "(", "self", ".", "image_validator", "is", "None", ")", "or", "(", "self", ".", "bound_generator", "is", "None", ")", ")", ":", "\n", "                    ", "self", ".", "image_validator", ".", "bounds", "=", "self", ".", "bound_generator", "(", ")", "\n", "\n", "# Use at most `self.n_trials_max` attempts to find a crop", "\n", "# that meets our requirements.", "\n", "", "for", "_", "in", "range", "(", "max", "(", "1", ",", "self", ".", "n_trials_max", ")", ")", ":", "\n", "\n", "# Generate patch coordinates.", "\n", "                    ", "patch_ymin", ",", "patch_xmin", ",", "patch_height", ",", "patch_width", "=", "self", ".", "patch_coord_generator", "(", ")", "\n", "\n", "self", ".", "sample_patch", ".", "patch_ymin", "=", "patch_ymin", "\n", "self", ".", "sample_patch", ".", "patch_xmin", "=", "patch_xmin", "\n", "self", ".", "sample_patch", ".", "patch_height", "=", "patch_height", "\n", "self", ".", "sample_patch", ".", "patch_width", "=", "patch_width", "\n", "\n", "# Check if the resulting patch meets the aspect ratio requirements.", "\n", "aspect_ratio", "=", "patch_width", "/", "patch_height", "\n", "if", "not", "(", "self", ".", "patch_coord_generator", ".", "min_aspect_ratio", "<=", "aspect_ratio", "<=", "self", ".", "patch_coord_generator", ".", "max_aspect_ratio", ")", ":", "\n", "                        ", "continue", "\n", "\n", "", "if", "(", "labels", "is", "None", ")", "or", "(", "self", ".", "image_validator", "is", "None", ")", ":", "\n", "# We either don't have any boxes or if we do, we will accept any outcome as valid.", "\n", "                        ", "return", "self", ".", "sample_patch", "(", "image", ",", "labels", ",", "return_inverter", ")", "\n", "", "else", ":", "\n", "# Translate the box coordinates to the patch's coordinate system.", "\n", "                        ", "new_labels", "=", "np", ".", "copy", "(", "labels", ")", "\n", "new_labels", "[", ":", ",", "[", "ymin", ",", "ymax", "]", "]", "-=", "patch_ymin", "\n", "new_labels", "[", ":", ",", "[", "xmin", ",", "xmax", "]", "]", "-=", "patch_xmin", "\n", "# Check if the patch contains the minimum number of boxes we require.", "\n", "if", "self", ".", "image_validator", "(", "labels", "=", "new_labels", ",", "\n", "image_height", "=", "patch_height", ",", "\n", "image_width", "=", "patch_width", ")", ":", "\n", "                            ", "return", "self", ".", "sample_patch", "(", "image", ",", "labels", ",", "return_inverter", ")", "\n", "", "", "", "", "else", ":", "\n", "                ", "if", "return_inverter", ":", "\n", "                    ", "def", "inverter", "(", "labels", ")", ":", "\n", "                        ", "return", "labels", "\n", "\n", "", "", "if", "labels", "is", "None", ":", "\n", "                    ", "if", "return_inverter", ":", "\n", "                        ", "return", "image", ",", "inverter", "\n", "", "else", ":", "\n", "                        ", "return", "image", "\n", "", "", "else", ":", "\n", "                    ", "if", "return_inverter", ":", "\n", "                        ", "return", "image", ",", "labels", ",", "inverter", "\n", "", "else", ":", "\n", "                        ", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_patch_sampling_ops.RandomMaxCropFixedAR.__init__": [[753, 795], ["object_detection_2d_patch_sampling_ops.RandomPatch", "object_detection_2d_patch_sampling_ops.PatchCoordinateGenerator"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "patch_aspect_ratio", ",", "\n", "box_filter", "=", "None", ",", "\n", "image_validator", "=", "None", ",", "\n", "n_trials_max", "=", "3", ",", "\n", "clip_boxes", "=", "True", ",", "\n", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ")", ":", "\n", "        ", "'''\n        Arguments:\n            patch_aspect_ratio (float): The fixed aspect ratio that all sampled patches will have.\n            box_filter (BoxFilter, optional): Only relevant if ground truth bounding boxes are given.\n                A `BoxFilter` object to filter out bounding boxes that don't meet the given criteria\n                after the transformation. Refer to the `BoxFilter` documentation for details. If `None`,\n                the validity of the bounding boxes is not checked.\n            image_validator (ImageValidator, optional): Only relevant if ground truth bounding boxes are given.\n                An `ImageValidator` object to determine whether a sampled patch is valid. If `None`,\n                any outcome is valid.\n            n_trials_max (int, optional): Only relevant if ground truth bounding boxes are given.\n                Determines the maxmial number of trials to sample a valid patch. If no valid patch could\n                be sampled in `n_trials_max` trials, returns `None`.\n            clip_boxes (bool, optional): Only relevant if ground truth bounding boxes are given.\n                If `True`, any ground truth bounding boxes will be clipped to lie entirely within the\n                sampled patch.\n            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n        '''", "\n", "\n", "self", ".", "patch_aspect_ratio", "=", "patch_aspect_ratio", "\n", "self", ".", "box_filter", "=", "box_filter", "\n", "self", ".", "image_validator", "=", "image_validator", "\n", "self", ".", "n_trials_max", "=", "n_trials_max", "\n", "self", ".", "clip_boxes", "=", "clip_boxes", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "self", ".", "random_patch", "=", "RandomPatch", "(", "patch_coord_generator", "=", "PatchCoordinateGenerator", "(", ")", ",", "# Just a dummy object", "\n", "box_filter", "=", "self", ".", "box_filter", ",", "\n", "image_validator", "=", "self", ".", "image_validator", ",", "\n", "n_trials_max", "=", "self", ".", "n_trials_max", ",", "\n", "clip_boxes", "=", "self", ".", "clip_boxes", ",", "\n", "prob", "=", "1.0", ",", "\n", "can_fail", "=", "False", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_patch_sampling_ops.RandomMaxCropFixedAR.__call__": [[796, 822], ["object_detection_2d_patch_sampling_ops.PatchCoordinateGenerator", "object_detection_2d_patch_sampling_ops.RandomMaxCropFixedAR.random_patch", "int", "int", "round", "round"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ",", "return_inverter", "=", "False", ")", ":", "\n", "\n", "        ", "img_height", ",", "img_width", "=", "image", ".", "shape", "[", ":", "2", "]", "\n", "\n", "# The ratio of the input image aspect ratio and patch aspect ratio determines the maximal possible crop.", "\n", "image_aspect_ratio", "=", "img_width", "/", "img_height", "\n", "\n", "if", "image_aspect_ratio", "<", "self", ".", "patch_aspect_ratio", ":", "\n", "            ", "patch_width", "=", "img_width", "\n", "patch_height", "=", "int", "(", "round", "(", "patch_width", "/", "self", ".", "patch_aspect_ratio", ")", ")", "\n", "", "else", ":", "\n", "            ", "patch_height", "=", "img_height", "\n", "patch_width", "=", "int", "(", "round", "(", "patch_height", "*", "self", ".", "patch_aspect_ratio", ")", ")", "\n", "\n", "# Now that we know the desired height and width for the patch,", "\n", "# instantiate an appropriate patch coordinate generator.", "\n", "", "patch_coord_generator", "=", "PatchCoordinateGenerator", "(", "img_height", "=", "img_height", ",", "\n", "img_width", "=", "img_width", ",", "\n", "must_match", "=", "'h_w'", ",", "\n", "patch_height", "=", "patch_height", ",", "\n", "patch_width", "=", "patch_width", ")", "\n", "\n", "# The rest of the work is done by `RandomPatch`.", "\n", "self", ".", "random_patch", ".", "patch_coord_generator", "=", "patch_coord_generator", "\n", "self", ".", "random_patch", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "return", "self", ".", "random_patch", "(", "image", ",", "labels", ",", "return_inverter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_patch_sampling_ops.RandomPadFixedAR.__init__": [[832, 858], ["object_detection_2d_patch_sampling_ops.RandomPatch", "object_detection_2d_patch_sampling_ops.PatchCoordinateGenerator"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "patch_aspect_ratio", ",", "\n", "background", "=", "(", "0", ",", "0", ",", "0", ")", ",", "\n", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ")", ":", "\n", "        ", "'''\n        Arguments:\n            patch_aspect_ratio (float): The fixed aspect ratio that all sampled patches will have.\n            background (list/tuple, optional): A 3-tuple specifying the RGB color value of the potential\n                background pixels of the scaled images. In the case of single-channel images,\n                the first element of `background` will be used as the background pixel value.\n            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n        '''", "\n", "\n", "self", ".", "patch_aspect_ratio", "=", "patch_aspect_ratio", "\n", "self", ".", "background", "=", "background", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "self", ".", "random_patch", "=", "RandomPatch", "(", "patch_coord_generator", "=", "PatchCoordinateGenerator", "(", ")", ",", "# Just a dummy object", "\n", "box_filter", "=", "None", ",", "\n", "image_validator", "=", "None", ",", "\n", "n_trials_max", "=", "1", ",", "\n", "clip_boxes", "=", "False", ",", "\n", "background", "=", "self", ".", "background", ",", "\n", "prob", "=", "1.0", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_patch_sampling_ops.RandomPadFixedAR.__call__": [[859, 882], ["object_detection_2d_patch_sampling_ops.PatchCoordinateGenerator", "object_detection_2d_patch_sampling_ops.RandomPadFixedAR.random_patch", "int", "int", "round", "round"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ",", "return_inverter", "=", "False", ")", ":", "\n", "\n", "        ", "img_height", ",", "img_width", "=", "image", ".", "shape", "[", ":", "2", "]", "\n", "\n", "if", "img_width", "<", "img_height", ":", "\n", "            ", "patch_height", "=", "img_height", "\n", "patch_width", "=", "int", "(", "round", "(", "patch_height", "*", "self", ".", "patch_aspect_ratio", ")", ")", "\n", "", "else", ":", "\n", "            ", "patch_width", "=", "img_width", "\n", "patch_height", "=", "int", "(", "round", "(", "patch_width", "/", "self", ".", "patch_aspect_ratio", ")", ")", "\n", "\n", "# Now that we know the desired height and width for the patch,", "\n", "# instantiate an appropriate patch coordinate generator.", "\n", "", "patch_coord_generator", "=", "PatchCoordinateGenerator", "(", "img_height", "=", "img_height", ",", "\n", "img_width", "=", "img_width", ",", "\n", "must_match", "=", "'h_w'", ",", "\n", "patch_height", "=", "patch_height", ",", "\n", "patch_width", "=", "patch_width", ")", "\n", "\n", "# The rest of the work is done by `RandomPatch`.", "\n", "self", ".", "random_patch", ".", "patch_coord_generator", "=", "patch_coord_generator", "\n", "self", ".", "random_patch", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "return", "self", ".", "random_patch", "(", "image", ",", "labels", ",", "return_inverter", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_misc_utils.apply_inverse_transforms": [[22, 74], ["isinstance", "range", "isinstance", "len", "np.copy.append", "numpy.copy", "range", "ValueError", "numpy.copy", "len", "inverter", "inverter"], "function", ["None"], ["def", "apply_inverse_transforms", "(", "y_pred_decoded", ",", "inverse_transforms", ")", ":", "\n", "    ", "'''\n    Takes a list or Numpy array of decoded predictions and applies a given list of\n    transforms to them. The list of inverse transforms would usually contain the\n    inverter functions that some of the image transformations that come with this\n    data generator return. This function would normally be used to transform predictions\n    that were made on a transformed image back to the original image.\n\n    Arguments:\n        y_pred_decoded (list or array): Either a list of length `batch_size` that\n            contains Numpy arrays that contain the predictions for each batch item\n            or a Numpy array. If this is a list of Numpy arrays, the arrays would\n            usually have the shape `(num_predictions, 6)`, where `num_predictions`\n            is different for each batch item. If this is a Numpy array, it would\n            usually have the shape `(batch_size, num_predictions, 6)`. The last axis\n            would usually contain the class ID, confidence score, and four bounding\n            box coordinates for each prediction.\n        inverse_predictions (list): A nested list of length `batch_size` that contains\n            for each batch item a list of functions that take one argument (one element\n            of `y_pred_decoded` if it is a list or one slice along the first axis of\n            `y_pred_decoded` if it is an array) and return an output of the same shape\n            and data type.\n\n    Returns:\n        The transformed predictions, which have the same structure as `y_pred_decoded`.\n    '''", "\n", "\n", "if", "isinstance", "(", "y_pred_decoded", ",", "list", ")", ":", "\n", "\n", "        ", "y_pred_decoded_inv", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "y_pred_decoded", ")", ")", ":", "\n", "            ", "y_pred_decoded_inv", ".", "append", "(", "np", ".", "copy", "(", "y_pred_decoded", "[", "i", "]", ")", ")", "\n", "if", "y_pred_decoded_inv", "[", "i", "]", ".", "size", ">", "0", ":", "# If there are any predictions for this batch item.", "\n", "                ", "for", "inverter", "in", "inverse_transforms", "[", "i", "]", ":", "\n", "                    ", "if", "not", "(", "inverter", "is", "None", ")", ":", "\n", "                        ", "y_pred_decoded_inv", "[", "i", "]", "=", "inverter", "(", "y_pred_decoded_inv", "[", "i", "]", ")", "\n", "\n", "", "", "", "", "", "elif", "isinstance", "(", "y_pred_decoded", ",", "np", ".", "ndarray", ")", ":", "\n", "\n", "        ", "y_pred_decoded_inv", "=", "np", ".", "copy", "(", "y_pred_decoded", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "y_pred_decoded", ")", ")", ":", "\n", "            ", "if", "y_pred_decoded_inv", "[", "i", "]", ".", "size", ">", "0", ":", "# If there are any predictions for this batch item.", "\n", "                ", "for", "inverter", "in", "inverse_transforms", "[", "i", "]", ":", "\n", "                    ", "if", "not", "(", "inverter", "is", "None", ")", ":", "\n", "                        ", "y_pred_decoded_inv", "[", "i", "]", "=", "inverter", "(", "y_pred_decoded_inv", "[", "i", "]", ")", "\n", "\n", "", "", "", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"`y_pred_decoded` must be either a list or a Numpy array.\"", ")", "\n", "\n", "", "return", "y_pred_decoded_inv", "\n", "", ""]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.data_augmentation_chain_constant_input_size.DataAugmentationConstantInputSize.__init__": [[34, 154], ["data_generator.object_detection_2d_image_boxes_validation_utils.BoxFilter", "data_generator.object_detection_2d_image_boxes_validation_utils.ImageValidator", "data_generator.object_detection_2d_photometric_ops.ConvertColor", "data_generator.object_detection_2d_photometric_ops.ConvertColor", "data_generator.object_detection_2d_photometric_ops.ConvertDataType", "data_generator.object_detection_2d_photometric_ops.ConvertDataType", "data_generator.object_detection_2d_photometric_ops.ConvertTo3Channels", "data_generator.object_detection_2d_photometric_ops.RandomBrightness", "data_generator.object_detection_2d_photometric_ops.RandomContrast", "data_generator.object_detection_2d_photometric_ops.RandomSaturation", "data_generator.object_detection_2d_photometric_ops.RandomHue", "data_generator.object_detection_2d_geometric_ops.RandomFlip", "data_generator.object_detection_2d_geometric_ops.RandomTranslate", "data_generator.object_detection_2d_geometric_ops.RandomScale", "data_generator.object_detection_2d_geometric_ops.RandomScale", "ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "random_brightness", "=", "(", "-", "48", ",", "48", ",", "0.5", ")", ",", "\n", "random_contrast", "=", "(", "0.5", ",", "1.8", ",", "0.5", ")", ",", "\n", "random_saturation", "=", "(", "0.5", ",", "1.8", ",", "0.5", ")", ",", "\n", "random_hue", "=", "(", "18", ",", "0.5", ")", ",", "\n", "random_flip", "=", "0.5", ",", "\n", "random_translate", "=", "(", "(", "0.03", ",", "0.5", ")", ",", "(", "0.03", ",", "0.5", ")", ",", "0.5", ")", ",", "\n", "random_scale", "=", "(", "0.5", ",", "2.0", ",", "0.5", ")", ",", "\n", "n_trials_max", "=", "3", ",", "\n", "clip_boxes", "=", "True", ",", "\n", "overlap_criterion", "=", "'area'", ",", "\n", "bounds_box_filter", "=", "(", "0.3", ",", "1.0", ")", ",", "\n", "bounds_validator", "=", "(", "0.5", ",", "1.0", ")", ",", "\n", "n_boxes_min", "=", "1", ",", "\n", "background", "=", "(", "0", ",", "0", ",", "0", ")", ",", "\n", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ")", ":", "\n", "\n", "        ", "if", "(", "random_scale", "[", "0", "]", ">=", "1", ")", "or", "(", "random_scale", "[", "1", "]", "<=", "1", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"This sequence of transformations only makes sense if the minimum scaling factor is <1 and the maximum scaling factor is >1.\"", ")", "\n", "\n", "", "self", ".", "n_trials_max", "=", "n_trials_max", "\n", "self", ".", "clip_boxes", "=", "clip_boxes", "\n", "self", ".", "overlap_criterion", "=", "overlap_criterion", "\n", "self", ".", "bounds_box_filter", "=", "bounds_box_filter", "\n", "self", ".", "bounds_validator", "=", "bounds_validator", "\n", "self", ".", "n_boxes_min", "=", "n_boxes_min", "\n", "self", ".", "background", "=", "background", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "\n", "# Determines which boxes are kept in an image after the transformations have been applied.", "\n", "self", ".", "box_filter", "=", "BoxFilter", "(", "check_overlap", "=", "True", ",", "\n", "check_min_area", "=", "True", ",", "\n", "check_degenerate", "=", "True", ",", "\n", "overlap_criterion", "=", "self", ".", "overlap_criterion", ",", "\n", "overlap_bounds", "=", "self", ".", "bounds_box_filter", ",", "\n", "min_area", "=", "16", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n", "# Determines whether the result of the transformations is a valid training image.", "\n", "self", ".", "image_validator", "=", "ImageValidator", "(", "overlap_criterion", "=", "self", ".", "overlap_criterion", ",", "\n", "bounds", "=", "self", ".", "bounds_validator", ",", "\n", "n_boxes_min", "=", "self", ".", "n_boxes_min", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n", "# Utility distortions", "\n", "self", ".", "convert_RGB_to_HSV", "=", "ConvertColor", "(", "current", "=", "'RGB'", ",", "to", "=", "'HSV'", ")", "\n", "self", ".", "convert_HSV_to_RGB", "=", "ConvertColor", "(", "current", "=", "'HSV'", ",", "to", "=", "'RGB'", ")", "\n", "self", ".", "convert_to_float32", "=", "ConvertDataType", "(", "to", "=", "'float32'", ")", "\n", "self", ".", "convert_to_uint8", "=", "ConvertDataType", "(", "to", "=", "'uint8'", ")", "\n", "self", ".", "convert_to_3_channels", "=", "ConvertTo3Channels", "(", ")", "# Make sure all images end up having 3 channels.", "\n", "\n", "# Photometric transformations", "\n", "self", ".", "random_brightness", "=", "RandomBrightness", "(", "lower", "=", "random_brightness", "[", "0", "]", ",", "upper", "=", "random_brightness", "[", "1", "]", ",", "prob", "=", "random_brightness", "[", "2", "]", ")", "\n", "self", ".", "random_contrast", "=", "RandomContrast", "(", "lower", "=", "random_contrast", "[", "0", "]", ",", "upper", "=", "random_contrast", "[", "1", "]", ",", "prob", "=", "random_contrast", "[", "2", "]", ")", "\n", "self", ".", "random_saturation", "=", "RandomSaturation", "(", "lower", "=", "random_saturation", "[", "0", "]", ",", "upper", "=", "random_saturation", "[", "1", "]", ",", "prob", "=", "random_saturation", "[", "2", "]", ")", "\n", "self", ".", "random_hue", "=", "RandomHue", "(", "max_delta", "=", "random_hue", "[", "0", "]", ",", "prob", "=", "random_hue", "[", "1", "]", ")", "\n", "\n", "# Geometric transformations", "\n", "self", ".", "random_flip", "=", "RandomFlip", "(", "dim", "=", "'horizontal'", ",", "prob", "=", "random_flip", ",", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "self", ".", "random_translate", "=", "RandomTranslate", "(", "dy_minmax", "=", "random_translate", "[", "0", "]", ",", "\n", "dx_minmax", "=", "random_translate", "[", "1", "]", ",", "\n", "prob", "=", "random_translate", "[", "2", "]", ",", "\n", "clip_boxes", "=", "self", ".", "clip_boxes", ",", "\n", "box_filter", "=", "self", ".", "box_filter", ",", "\n", "image_validator", "=", "self", ".", "image_validator", ",", "\n", "n_trials_max", "=", "self", ".", "n_trials_max", ",", "\n", "background", "=", "self", ".", "background", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "self", ".", "random_zoom_in", "=", "RandomScale", "(", "min_factor", "=", "1.0", ",", "\n", "max_factor", "=", "random_scale", "[", "1", "]", ",", "\n", "prob", "=", "random_scale", "[", "2", "]", ",", "\n", "clip_boxes", "=", "self", ".", "clip_boxes", ",", "\n", "box_filter", "=", "self", ".", "box_filter", ",", "\n", "image_validator", "=", "self", ".", "image_validator", ",", "\n", "n_trials_max", "=", "self", ".", "n_trials_max", ",", "\n", "background", "=", "self", ".", "background", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "self", ".", "random_zoom_out", "=", "RandomScale", "(", "min_factor", "=", "random_scale", "[", "0", "]", ",", "\n", "max_factor", "=", "1.0", ",", "\n", "prob", "=", "random_scale", "[", "2", "]", ",", "\n", "clip_boxes", "=", "self", ".", "clip_boxes", ",", "\n", "box_filter", "=", "self", ".", "box_filter", ",", "\n", "image_validator", "=", "self", ".", "image_validator", ",", "\n", "n_trials_max", "=", "self", ".", "n_trials_max", ",", "\n", "background", "=", "self", ".", "background", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n", "# If we zoom in, do translation before scaling.", "\n", "self", ".", "sequence1", "=", "[", "self", ".", "convert_to_3_channels", ",", "\n", "self", ".", "convert_to_float32", ",", "\n", "self", ".", "random_brightness", ",", "\n", "self", ".", "random_contrast", ",", "\n", "self", ".", "convert_to_uint8", ",", "\n", "self", ".", "convert_RGB_to_HSV", ",", "\n", "self", ".", "convert_to_float32", ",", "\n", "self", ".", "random_saturation", ",", "\n", "self", ".", "random_hue", ",", "\n", "self", ".", "convert_to_uint8", ",", "\n", "self", ".", "convert_HSV_to_RGB", ",", "\n", "self", ".", "random_translate", ",", "\n", "self", ".", "random_zoom_in", ",", "\n", "self", ".", "random_flip", "]", "\n", "\n", "# If we zoom out, do scaling before translation.", "\n", "self", ".", "sequence2", "=", "[", "self", ".", "convert_to_3_channels", ",", "\n", "self", ".", "convert_to_float32", ",", "\n", "self", ".", "random_brightness", ",", "\n", "self", ".", "convert_to_uint8", ",", "\n", "self", ".", "convert_RGB_to_HSV", ",", "\n", "self", ".", "convert_to_float32", ",", "\n", "self", ".", "random_saturation", ",", "\n", "self", ".", "random_hue", ",", "\n", "self", ".", "convert_to_uint8", ",", "\n", "self", ".", "convert_HSV_to_RGB", ",", "\n", "self", ".", "convert_to_float32", ",", "\n", "self", ".", "random_contrast", ",", "\n", "self", ".", "convert_to_uint8", ",", "\n", "self", ".", "random_zoom_out", ",", "\n", "self", ".", "random_translate", ",", "\n", "self", ".", "random_flip", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.data_augmentation_chain_constant_input_size.DataAugmentationConstantInputSize.__call__": [[155, 184], ["numpy.random.choice", "transform", "transform", "transform", "transform"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "\n", "        ", "self", ".", "random_translate", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "self", ".", "random_zoom_in", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "self", ".", "random_zoom_out", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "self", ".", "random_flip", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "\n", "# Choose sequence 1 with probability 0.5.", "\n", "if", "np", ".", "random", ".", "choice", "(", "2", ")", ":", "\n", "\n", "            ", "if", "not", "(", "labels", "is", "None", ")", ":", "\n", "                ", "for", "transform", "in", "self", ".", "sequence1", ":", "\n", "                    ", "image", ",", "labels", "=", "transform", "(", "image", ",", "labels", ")", "\n", "", "return", "image", ",", "labels", "\n", "", "else", ":", "\n", "                ", "for", "transform", "in", "self", ".", "sequence1", ":", "\n", "                    ", "image", "=", "transform", "(", "image", ")", "\n", "", "return", "image", "\n", "# Choose sequence 2 with probability 0.5.", "\n", "", "", "else", ":", "\n", "\n", "            ", "if", "not", "(", "labels", "is", "None", ")", ":", "\n", "                ", "for", "transform", "in", "self", ".", "sequence2", ":", "\n", "                    ", "image", ",", "labels", "=", "transform", "(", "image", ",", "labels", ")", "\n", "", "return", "image", ",", "labels", "\n", "", "else", ":", "\n", "                ", "for", "transform", "in", "self", ".", "sequence2", ":", "\n", "                    ", "image", "=", "transform", "(", "image", ")", "\n", "", "return", "image", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.data_augmentation_chain_satellite.DataAugmentationSatellite.__init__": [[37, 141], ["data_generator.object_detection_2d_image_boxes_validation_utils.BoxFilter", "data_generator.object_detection_2d_image_boxes_validation_utils.BoxFilter", "data_generator.object_detection_2d_image_boxes_validation_utils.ImageValidator", "data_generator.object_detection_2d_photometric_ops.ConvertTo3Channels", "data_generator.object_detection_2d_photometric_ops.ConvertColor", "data_generator.object_detection_2d_photometric_ops.ConvertColor", "data_generator.object_detection_2d_photometric_ops.ConvertDataType", "data_generator.object_detection_2d_photometric_ops.ConvertDataType", "data_generator.object_detection_2d_geometric_ops.Resize", "data_generator.object_detection_2d_photometric_ops.RandomBrightness", "data_generator.object_detection_2d_photometric_ops.RandomContrast", "data_generator.object_detection_2d_photometric_ops.RandomSaturation", "data_generator.object_detection_2d_photometric_ops.RandomHue", "data_generator.object_detection_2d_geometric_ops.RandomFlip", "data_generator.object_detection_2d_geometric_ops.RandomFlip", "data_generator.object_detection_2d_geometric_ops.RandomRotate", "data_generator.object_detection_2d_patch_sampling_ops.PatchCoordinateGenerator", "data_generator.object_detection_2d_patch_sampling_ops.RandomPatch"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "resize_height", ",", "\n", "resize_width", ",", "\n", "random_brightness", "=", "(", "-", "48", ",", "48", ",", "0.5", ")", ",", "\n", "random_contrast", "=", "(", "0.5", ",", "1.8", ",", "0.5", ")", ",", "\n", "random_saturation", "=", "(", "0.5", ",", "1.8", ",", "0.5", ")", ",", "\n", "random_hue", "=", "(", "18", ",", "0.5", ")", ",", "\n", "random_flip", "=", "0.5", ",", "\n", "random_rotate", "=", "(", "[", "90", ",", "180", ",", "270", "]", ",", "0.5", ")", ",", "\n", "min_scale", "=", "0.3", ",", "\n", "max_scale", "=", "2.0", ",", "\n", "min_aspect_ratio", "=", "0.8", ",", "\n", "max_aspect_ratio", "=", "1.25", ",", "\n", "n_trials_max", "=", "3", ",", "\n", "clip_boxes", "=", "True", ",", "\n", "overlap_criterion", "=", "'area'", ",", "\n", "bounds_box_filter", "=", "(", "0.3", ",", "1.0", ")", ",", "\n", "bounds_validator", "=", "(", "0.5", ",", "1.0", ")", ",", "\n", "n_boxes_min", "=", "1", ",", "\n", "background", "=", "(", "0", ",", "0", ",", "0", ")", ",", "\n", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ")", ":", "\n", "\n", "        ", "self", ".", "n_trials_max", "=", "n_trials_max", "\n", "self", ".", "clip_boxes", "=", "clip_boxes", "\n", "self", ".", "overlap_criterion", "=", "overlap_criterion", "\n", "self", ".", "bounds_box_filter", "=", "bounds_box_filter", "\n", "self", ".", "bounds_validator", "=", "bounds_validator", "\n", "self", ".", "n_boxes_min", "=", "n_boxes_min", "\n", "self", ".", "background", "=", "background", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "\n", "# Determines which boxes are kept in an image after the transformations have been applied.", "\n", "self", ".", "box_filter_patch", "=", "BoxFilter", "(", "check_overlap", "=", "True", ",", "\n", "check_min_area", "=", "False", ",", "\n", "check_degenerate", "=", "False", ",", "\n", "overlap_criterion", "=", "self", ".", "overlap_criterion", ",", "\n", "overlap_bounds", "=", "self", ".", "bounds_box_filter", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n", "self", ".", "box_filter_resize", "=", "BoxFilter", "(", "check_overlap", "=", "False", ",", "\n", "check_min_area", "=", "True", ",", "\n", "check_degenerate", "=", "True", ",", "\n", "min_area", "=", "16", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n", "# Determines whether the result of the transformations is a valid training image.", "\n", "self", ".", "image_validator", "=", "ImageValidator", "(", "overlap_criterion", "=", "self", ".", "overlap_criterion", ",", "\n", "bounds", "=", "self", ".", "bounds_validator", ",", "\n", "n_boxes_min", "=", "self", ".", "n_boxes_min", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n", "# Utility transformations", "\n", "self", ".", "convert_to_3_channels", "=", "ConvertTo3Channels", "(", ")", "# Make sure all images end up having 3 channels.", "\n", "self", ".", "convert_RGB_to_HSV", "=", "ConvertColor", "(", "current", "=", "'RGB'", ",", "to", "=", "'HSV'", ")", "\n", "self", ".", "convert_HSV_to_RGB", "=", "ConvertColor", "(", "current", "=", "'HSV'", ",", "to", "=", "'RGB'", ")", "\n", "self", ".", "convert_to_float32", "=", "ConvertDataType", "(", "to", "=", "'float32'", ")", "\n", "self", ".", "convert_to_uint8", "=", "ConvertDataType", "(", "to", "=", "'uint8'", ")", "\n", "self", ".", "resize", "=", "Resize", "(", "height", "=", "resize_height", ",", "\n", "width", "=", "resize_width", ",", "\n", "box_filter", "=", "self", ".", "box_filter_resize", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n", "# Photometric transformations", "\n", "self", ".", "random_brightness", "=", "RandomBrightness", "(", "lower", "=", "random_brightness", "[", "0", "]", ",", "upper", "=", "random_brightness", "[", "1", "]", ",", "prob", "=", "random_brightness", "[", "2", "]", ")", "\n", "self", ".", "random_contrast", "=", "RandomContrast", "(", "lower", "=", "random_contrast", "[", "0", "]", ",", "upper", "=", "random_contrast", "[", "1", "]", ",", "prob", "=", "random_contrast", "[", "2", "]", ")", "\n", "self", ".", "random_saturation", "=", "RandomSaturation", "(", "lower", "=", "random_saturation", "[", "0", "]", ",", "upper", "=", "random_saturation", "[", "1", "]", ",", "prob", "=", "random_saturation", "[", "2", "]", ")", "\n", "self", ".", "random_hue", "=", "RandomHue", "(", "max_delta", "=", "random_hue", "[", "0", "]", ",", "prob", "=", "random_hue", "[", "1", "]", ")", "\n", "\n", "# Geometric transformations", "\n", "self", ".", "random_horizontal_flip", "=", "RandomFlip", "(", "dim", "=", "'horizontal'", ",", "prob", "=", "random_flip", ",", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "self", ".", "random_vertical_flip", "=", "RandomFlip", "(", "dim", "=", "'vertical'", ",", "prob", "=", "random_flip", ",", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "self", ".", "random_rotate", "=", "RandomRotate", "(", "angles", "=", "random_rotate", "[", "0", "]", ",", "prob", "=", "random_rotate", "[", "1", "]", ",", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "self", ".", "patch_coord_generator", "=", "PatchCoordinateGenerator", "(", "must_match", "=", "'w_ar'", ",", "\n", "min_scale", "=", "min_scale", ",", "\n", "max_scale", "=", "max_scale", ",", "\n", "scale_uniformly", "=", "False", ",", "\n", "min_aspect_ratio", "=", "min_aspect_ratio", ",", "\n", "max_aspect_ratio", "=", "max_aspect_ratio", ")", "\n", "self", ".", "random_patch", "=", "RandomPatch", "(", "patch_coord_generator", "=", "self", ".", "patch_coord_generator", ",", "\n", "box_filter", "=", "self", ".", "box_filter_patch", ",", "\n", "image_validator", "=", "self", ".", "image_validator", ",", "\n", "n_trials_max", "=", "self", ".", "n_trials_max", ",", "\n", "clip_boxes", "=", "self", ".", "clip_boxes", ",", "\n", "prob", "=", "1.0", ",", "\n", "can_fail", "=", "False", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n", "# Define the processing chain.", "\n", "self", ".", "transformations", "=", "[", "self", ".", "convert_to_3_channels", ",", "\n", "self", ".", "convert_to_float32", ",", "\n", "self", ".", "random_brightness", ",", "\n", "self", ".", "random_contrast", ",", "\n", "self", ".", "convert_to_uint8", ",", "\n", "self", ".", "convert_RGB_to_HSV", ",", "\n", "self", ".", "convert_to_float32", ",", "\n", "self", ".", "random_saturation", ",", "\n", "self", ".", "random_hue", ",", "\n", "self", ".", "convert_to_uint8", ",", "\n", "self", ".", "convert_HSV_to_RGB", ",", "\n", "self", ".", "random_horizontal_flip", ",", "\n", "self", ".", "random_vertical_flip", ",", "\n", "self", ".", "random_rotate", ",", "\n", "self", ".", "random_patch", ",", "\n", "self", ".", "resize", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.data_augmentation_chain_satellite.DataAugmentationSatellite.__call__": [[142, 158], ["transform", "transform"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ")", ":", "\n", "\n", "        ", "self", ".", "random_patch", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "self", ".", "random_horizontal_flip", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "self", ".", "random_vertical_flip", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "self", ".", "random_rotate", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "self", ".", "resize", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "\n", "if", "not", "(", "labels", "is", "None", ")", ":", "\n", "            ", "for", "transform", "in", "self", ".", "transformations", ":", "\n", "                ", "image", ",", "labels", "=", "transform", "(", "image", ",", "labels", ")", "\n", "", "return", "image", ",", "labels", "\n", "", "else", ":", "\n", "            ", "for", "transform", "in", "self", ".", "sequence1", ":", "\n", "                ", "image", "=", "transform", "(", "image", ")", "\n", "", "return", "image", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_data_generator.DataGenerator.__init__": [[81, 222], ["isinstance", "len", "numpy.arange", "isinstance", "isinstance", "isinstance", "object_detection_2d_data_generator.DataGenerator.load_hdf5_dataset", "isinstance", "isinstance", "isinstance", "isinstance", "ValueError", "tqdm.tqdm.tqdm", "open", "pickle.load", "ValueError", "open", "pickle.load", "ValueError", "open", "pickle.load", "ValueError", "open", "PIL.Image.open", "object_detection_2d_data_generator.DataGenerator.images.append", "pickle.load", "numpy.array", "ValueError", "os.path.join", "line.strip"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_data_generator.DataGenerator.load_hdf5_dataset"], ["def", "__init__", "(", "self", ",", "\n", "load_images_into_memory", "=", "False", ",", "\n", "hdf5_dataset_path", "=", "None", ",", "\n", "filenames", "=", "None", ",", "\n", "filenames_type", "=", "'text'", ",", "\n", "images_dir", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "image_ids", "=", "None", ",", "\n", "eval_neutral", "=", "None", ",", "\n", "labels_output_format", "=", "(", "'class_id'", ",", "'xmin'", ",", "'ymin'", ",", "'xmax'", ",", "'ymax'", ")", ",", "\n", "verbose", "=", "True", ")", ":", "\n", "        ", "'''\n        Initializes the data generator. You can either load a dataset directly here in the constructor,\n        e.g. an HDF5 dataset, or you can use one of the parser methods to read in a dataset.\n\n        Arguments:\n            load_images_into_memory (bool, optional): If `True`, the entire dataset will be loaded into memory.\n                This enables noticeably faster data generation than loading batches of images into memory ad hoc.\n                Be sure that you have enough memory before you activate this option.\n            hdf5_dataset_path (str, optional): The full file path of an HDF5 file that contains a dataset in the\n                format that the `create_hdf5_dataset()` method produces. If you load such an HDF5 dataset, you\n                don't need to use any of the parser methods anymore, the HDF5 dataset already contains all relevant\n                data.\n            filenames (string or list, optional): `None` or either a Python list/tuple or a string representing\n                a filepath. If a list/tuple is passed, it must contain the file names (full paths) of the\n                images to be used. Note that the list/tuple must contain the paths to the images,\n                not the images themselves. If a filepath string is passed, it must point either to\n                (1) a pickled file containing a list/tuple as described above. In this case the `filenames_type`\n                argument must be set to `pickle`.\n                Or\n                (2) a text file. Each line of the text file contains the file name (basename of the file only,\n                not the full directory path) to one image and nothing else. In this case the `filenames_type`\n                argument must be set to `text` and you must pass the path to the directory that contains the\n                images in `images_dir`.\n            filenames_type (string, optional): In case a string is passed for `filenames`, this indicates what\n                type of file `filenames` is. It can be either 'pickle' for a pickled file or 'text' for a\n                plain text file.\n            images_dir (string, optional): In case a text file is passed for `filenames`, the full paths to\n                the images will be composed from `images_dir` and the names in the text file, i.e. this\n                should be the directory that contains the images to which the text file refers.\n                If `filenames_type` is not 'text', then this argument is irrelevant.\n            labels (string or list, optional): `None` or either a Python list/tuple or a string representing\n                the path to a pickled file containing a list/tuple. The list/tuple must contain Numpy arrays\n                that represent the labels of the dataset.\n            image_ids (string or list, optional): `None` or either a Python list/tuple or a string representing\n                the path to a pickled file containing a list/tuple. The list/tuple must contain the image\n                IDs of the images in the dataset.\n            eval_neutral (string or list, optional): `None` or either a Python list/tuple or a string representing\n                the path to a pickled file containing a list/tuple. The list/tuple must contain for each image\n                a list that indicates for each ground truth object in the image whether that object is supposed\n                to be treated as neutral during an evaluation.\n            labels_output_format (list, optional): A list of five strings representing the desired order of the five\n                items class ID, xmin, ymin, xmax, ymax in the generated ground truth data (if any). The expected\n                strings are 'xmin', 'ymin', 'xmax', 'ymax', 'class_id'.\n            verbose (bool, optional): If `True`, prints out the progress for some constructor operations that may\n                take a bit longer.\n        '''", "\n", "self", ".", "labels_output_format", "=", "labels_output_format", "\n", "self", ".", "labels_format", "=", "{", "'class_id'", ":", "1", ",", "\n", "'xmin'", ":", "2", ",", "\n", "'ymin'", ":", "3", ",", "\n", "'xmax'", ":", "4", ",", "\n", "'ymax'", ":", "5", "}", "\n", "# self.labels_format={'class_id': labels_output_format.index('class_id'),", "\n", "#                     'xmin': labels_output_format.index('xmin'),", "\n", "#                     'ymin': labels_output_format.index('ymin'),", "\n", "#                     'xmax': labels_output_format.index('xmax'),", "\n", "#                     'ymax': labels_output_format.index('ymax')}", "\n", "\n", "self", ".", "dataset_size", "=", "0", "# As long as we haven't loaded anything yet, the dataset size is zero.", "\n", "self", ".", "load_images_into_memory", "=", "load_images_into_memory", "\n", "self", ".", "images", "=", "None", "# The only way that this list will not stay `None` is if `load_images_into_memory == True`.", "\n", "\n", "# `self.filenames` is a list containing all file names of the image samples (full paths).", "\n", "# Note that it does not contain the actual image files themselves. This list is one of the outputs of the parser methods.", "\n", "# In case you are loading an HDF5 dataset, this list will be `None`.", "\n", "if", "not", "filenames", "is", "None", ":", "\n", "            ", "if", "isinstance", "(", "filenames", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "                ", "self", ".", "filenames", "=", "filenames", "\n", "", "elif", "isinstance", "(", "filenames", ",", "str", ")", ":", "\n", "                ", "with", "open", "(", "filenames", ",", "'rb'", ")", "as", "f", ":", "\n", "                    ", "if", "filenames_type", "==", "'pickle'", ":", "\n", "                        ", "self", ".", "filenames", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "elif", "filenames_type", "==", "'text'", ":", "\n", "                        ", "self", ".", "filenames", "=", "[", "os", ".", "path", ".", "join", "(", "images_dir", ",", "line", ".", "strip", "(", ")", ")", "for", "line", "in", "f", "]", "\n", "", "else", ":", "\n", "                        ", "raise", "ValueError", "(", "\"`filenames_type` can be either 'text' or 'pickle'.\"", ")", "\n", "", "", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"`filenames` must be either a Python list/tuple or a string representing a filepath (to a pickled or text file). The value you passed is neither of the two.\"", ")", "\n", "", "self", ".", "dataset_size", "=", "len", "(", "self", ".", "filenames", ")", "\n", "self", ".", "dataset_indices", "=", "np", ".", "arange", "(", "self", ".", "dataset_size", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "if", "load_images_into_memory", ":", "\n", "                ", "self", ".", "images", "=", "[", "]", "\n", "if", "verbose", ":", "it", "=", "tqdm", "(", "self", ".", "filenames", ",", "desc", "=", "'Loading images into memory'", ",", "file", "=", "sys", ".", "stdout", ")", "\n", "else", ":", "it", "=", "self", ".", "filenames", "\n", "for", "filename", "in", "it", ":", "\n", "                    ", "with", "Image", ".", "open", "(", "filename", ")", "as", "image", ":", "\n", "                        ", "self", ".", "images", ".", "append", "(", "np", ".", "array", "(", "image", ",", "dtype", "=", "np", ".", "uint8", ")", ")", "\n", "", "", "", "", "else", ":", "\n", "            ", "self", ".", "filenames", "=", "None", "\n", "\n", "# In case ground truth is available, `self.labels` is a list containing for each image a list (or NumPy array)", "\n", "# of ground truth bounding boxes for that image.", "\n", "", "if", "not", "labels", "is", "None", ":", "\n", "            ", "if", "isinstance", "(", "labels", ",", "str", ")", ":", "\n", "                ", "with", "open", "(", "labels", ",", "'rb'", ")", "as", "f", ":", "\n", "                    ", "self", ".", "labels", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "", "elif", "isinstance", "(", "labels", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "                ", "self", ".", "labels", "=", "labels", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"`labels` must be either a Python list/tuple or a string representing the path to a pickled file containing a list/tuple. The value you passed is neither of the two.\"", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "labels", "=", "None", "\n", "\n", "", "if", "not", "image_ids", "is", "None", ":", "\n", "            ", "if", "isinstance", "(", "image_ids", ",", "str", ")", ":", "\n", "                ", "with", "open", "(", "image_ids", ",", "'rb'", ")", "as", "f", ":", "\n", "                    ", "self", ".", "image_ids", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "", "elif", "isinstance", "(", "image_ids", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "                ", "self", ".", "image_ids", "=", "image_ids", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"`image_ids` must be either a Python list/tuple or a string representing the path to a pickled file containing a list/tuple. The value you passed is neither of the two.\"", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "image_ids", "=", "None", "\n", "\n", "", "if", "not", "eval_neutral", "is", "None", ":", "\n", "            ", "if", "isinstance", "(", "eval_neutral", ",", "str", ")", ":", "\n", "                ", "with", "open", "(", "eval_neutral", ",", "'rb'", ")", "as", "f", ":", "\n", "                    ", "self", ".", "eval_neutral", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "", "elif", "isinstance", "(", "eval_neutral", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "                ", "self", ".", "eval_neutral", "=", "eval_neutral", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"`image_ids` must be either a Python list/tuple or a string representing the path to a pickled file containing a list/tuple. The value you passed is neither of the two.\"", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "eval_neutral", "=", "None", "\n", "\n", "", "if", "not", "hdf5_dataset_path", "is", "None", ":", "\n", "            ", "self", ".", "hdf5_dataset_path", "=", "hdf5_dataset_path", "\n", "self", ".", "load_hdf5_dataset", "(", "verbose", "=", "verbose", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "hdf5_dataset", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_data_generator.DataGenerator.load_hdf5_dataset": [[223, 274], ["h5py.File", "len", "numpy.arange", "tqdm.tqdm.trange", "range", "object_detection_2d_data_generator.DataGenerator.images.append", "tqdm.tqdm.trange", "range", "object_detection_2d_data_generator.DataGenerator.labels.append", "object_detection_2d_data_generator.DataGenerator.sample_weights.append", "tqdm.tqdm.trange", "range", "object_detection_2d_data_generator.DataGenerator.image_ids.append", "tqdm.tqdm.trange", "range", "object_detection_2d_data_generator.DataGenerator.eval_neutral.append", "[].reshape", "labels[].reshape"], "methods", ["None"], ["", "", "def", "load_hdf5_dataset", "(", "self", ",", "verbose", "=", "True", ")", ":", "\n", "        ", "'''\n        Loads an HDF5 dataset that is in the format that the `create_hdf5_dataset()` method\n        produces.\n\n        Arguments:\n            verbose (bool, optional): If `True`, prints out the progress while loading\n                the dataset.\n\n        Returns:\n            None.\n        '''", "\n", "\n", "self", ".", "hdf5_dataset", "=", "h5py", ".", "File", "(", "self", ".", "hdf5_dataset_path", ",", "'r'", ")", "\n", "self", ".", "dataset_size", "=", "len", "(", "self", ".", "hdf5_dataset", "[", "'images'", "]", ")", "\n", "self", ".", "dataset_indices", "=", "np", ".", "arange", "(", "self", ".", "dataset_size", ",", "dtype", "=", "np", ".", "int32", ")", "# Instead of shuffling the HDF5 dataset or images in memory, we will shuffle this index list.", "\n", "\n", "if", "self", ".", "load_images_into_memory", ":", "\n", "            ", "self", ".", "images", "=", "[", "]", "\n", "if", "verbose", ":", "tr", "=", "trange", "(", "self", ".", "dataset_size", ",", "desc", "=", "'Loading images into memory'", ",", "file", "=", "sys", ".", "stdout", ")", "\n", "else", ":", "tr", "=", "range", "(", "self", ".", "dataset_size", ")", "\n", "for", "i", "in", "tr", ":", "\n", "                ", "self", ".", "images", ".", "append", "(", "self", ".", "hdf5_dataset", "[", "'images'", "]", "[", "i", "]", ".", "reshape", "(", "self", ".", "hdf5_dataset", "[", "'image_shapes'", "]", "[", "i", "]", ")", ")", "\n", "\n", "", "", "if", "self", ".", "hdf5_dataset", ".", "attrs", "[", "'has_labels'", "]", ":", "\n", "            ", "self", ".", "labels", "=", "[", "]", "\n", "self", ".", "sample_weights", "=", "[", "]", "\n", "sample_weights", "=", "self", ".", "hdf5_dataset", "[", "'sample_weights'", "]", "\n", "labels", "=", "self", ".", "hdf5_dataset", "[", "'labels'", "]", "\n", "label_shapes", "=", "self", ".", "hdf5_dataset", "[", "'label_shapes'", "]", "\n", "if", "verbose", ":", "tr", "=", "trange", "(", "self", ".", "dataset_size", ",", "desc", "=", "'Loading labels'", ",", "file", "=", "sys", ".", "stdout", ")", "\n", "else", ":", "tr", "=", "range", "(", "self", ".", "dataset_size", ")", "\n", "for", "i", "in", "tr", ":", "\n", "                ", "self", ".", "labels", ".", "append", "(", "labels", "[", "i", "]", ".", "reshape", "(", "label_shapes", "[", "i", "]", ")", ")", "\n", "self", ".", "sample_weights", ".", "append", "(", "sample_weights", "[", "i", "]", ")", "\n", "\n", "", "", "if", "self", ".", "hdf5_dataset", ".", "attrs", "[", "'has_image_ids'", "]", ":", "\n", "            ", "self", ".", "image_ids", "=", "[", "]", "\n", "image_ids", "=", "self", ".", "hdf5_dataset", "[", "'image_ids'", "]", "\n", "if", "verbose", ":", "tr", "=", "trange", "(", "self", ".", "dataset_size", ",", "desc", "=", "'Loading image IDs'", ",", "file", "=", "sys", ".", "stdout", ")", "\n", "else", ":", "tr", "=", "range", "(", "self", ".", "dataset_size", ")", "\n", "for", "i", "in", "tr", ":", "\n", "                ", "self", ".", "image_ids", ".", "append", "(", "image_ids", "[", "i", "]", ")", "\n", "\n", "", "", "if", "self", ".", "hdf5_dataset", ".", "attrs", "[", "'has_eval_neutral'", "]", ":", "\n", "            ", "self", ".", "eval_neutral", "=", "[", "]", "\n", "eval_neutral", "=", "self", ".", "hdf5_dataset", "[", "'eval_neutral'", "]", "\n", "if", "verbose", ":", "tr", "=", "trange", "(", "self", ".", "dataset_size", ",", "desc", "=", "'Loading evaluation-neutrality annotations'", ",", "file", "=", "sys", ".", "stdout", ")", "\n", "else", ":", "tr", "=", "range", "(", "self", ".", "dataset_size", ")", "\n", "for", "i", "in", "tr", ":", "\n", "                ", "self", ".", "eval_neutral", ".", "append", "(", "eval_neutral", "[", "i", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_data_generator.DataGenerator.parse_csv": [[275, 407], ["sorted", "enumerate", "len", "numpy.arange", "ValueError", "open", "csv.reader", "next", "[].split", "current_labels.append", "current_labels.append", "tqdm.tqdm.tqdm", "box.append", "sorted.append", "numpy.random.uniform", "object_detection_2d_data_generator.DataGenerator.labels.append", "object_detection_2d_data_generator.DataGenerator.filenames.append", "object_detection_2d_data_generator.DataGenerator.image_ids.append", "box[].split", "PIL.Image.open", "object_detection_2d_data_generator.DataGenerator.images.append", "int", "row[].strip", "box.append", "len", "numpy.random.uniform", "object_detection_2d_data_generator.DataGenerator.labels.append", "object_detection_2d_data_generator.DataGenerator.filenames.append", "object_detection_2d_data_generator.DataGenerator.image_ids.append", "object_detection_2d_data_generator.DataGenerator.labels.append", "object_detection_2d_data_generator.DataGenerator.filenames.append", "object_detection_2d_data_generator.DataGenerator.image_ids.append", "numpy.stack", "os.path.join", "len", "numpy.random.uniform", "object_detection_2d_data_generator.DataGenerator.labels.append", "object_detection_2d_data_generator.DataGenerator.filenames.append", "object_detection_2d_data_generator.DataGenerator.image_ids.append", "numpy.array", "row[].strip", "int", "object_detection_2d_data_generator.DataGenerator.labels.append", "object_detection_2d_data_generator.DataGenerator.filenames.append", "object_detection_2d_data_generator.DataGenerator.image_ids.append", "numpy.stack", "os.path.join", "numpy.stack", "os.path.join", "object_detection_2d_data_generator.DataGenerator.labels.append", "object_detection_2d_data_generator.DataGenerator.filenames.append", "object_detection_2d_data_generator.DataGenerator.image_ids.append", "numpy.stack", "os.path.join", "row[].strip", "numpy.stack", "os.path.join", "numpy.stack", "os.path.join", "object_detection_2d_data_generator.DataGenerator.input_format.index", "object_detection_2d_data_generator.DataGenerator.input_format.index", "object_detection_2d_data_generator.DataGenerator.input_format.index"], "methods", ["None"], ["", "", "", "def", "parse_csv", "(", "self", ",", "\n", "images_dir", ",", "\n", "labels_filename", ",", "\n", "input_format", ",", "\n", "include_classes", "=", "'all'", ",", "\n", "random_sample", "=", "False", ",", "\n", "ret", "=", "False", ",", "\n", "verbose", "=", "True", ")", ":", "\n", "        ", "'''\n        Arguments:\n            images_dir (str): The path to the directory that contains the images.\n            labels_filename (str): The filepath to a CSV file that contains one ground truth bounding box per line\n                and each line contains the following six items: image file name, class ID, xmin, xmax, ymin, ymax.\n                The six items do not have to be in a specific order, but they must be the first six columns of\n                each line. The order of these items in the CSV file must be specified in `input_format`.\n                The class ID is an integer greater than zero. Class ID 0 is reserved for the background class.\n                `xmin` and `xmax` are the left-most and right-most absolute horizontal coordinates of the box,\n                `ymin` and `ymax` are the top-most and bottom-most absolute vertical coordinates of the box.\n                The image name is expected to be just the name of the image file without the directory path\n                at which the image is located.\n            input_format (list): A list of six strings representing the order of the six items\n                image file name, class ID, xmin, xmax, ymin, ymax in the input CSV file. The expected strings\n                are 'image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'.\n            include_classes (list, optional): Either 'all' or a list of integers containing the class IDs that\n                are to be included in the dataset. If 'all', all ground truth boxes will be included in the dataset.\n            random_sample (float, optional): Either `False` or a float in `[0,1]`. If this is `False`, the\n                full dataset will be used by the generator. If this is a float in `[0,1]`, a randomly sampled\n                fraction of the dataset will be used, where `random_sample` is the fraction of the dataset\n                to be used. For example, if `random_sample = 0.2`, 20 precent of the dataset will be randomly selected,\n                the rest will be ommitted. The fraction refers to the number of images, not to the number\n                of boxes, i.e. each image that will be added to the dataset will always be added with all\n                of its boxes.\n            ret (bool, optional): Whether or not to return the outputs of the parser.\n            verbose (bool, optional): If `True`, prints out the progress for operations that may take a bit longer.\n\n        Returns:\n            None by default, optionally lists for whichever are available of images, image filenames, labels, and image IDs.\n        '''", "\n", "\n", "# Set class members.", "\n", "self", ".", "images_dir", "=", "images_dir", "\n", "self", ".", "labels_filename", "=", "labels_filename", "\n", "self", ".", "input_format", "=", "input_format", "\n", "self", ".", "include_classes", "=", "include_classes", "\n", "\n", "# Before we begin, make sure that we have a labels_filename and an input_format", "\n", "if", "self", ".", "labels_filename", "is", "None", "or", "self", ".", "input_format", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"`labels_filename` and/or `input_format` have not been set yet. You need to pass them as arguments.\"", ")", "\n", "\n", "# Erase data that might have been parsed before", "\n", "", "self", ".", "filenames", "=", "[", "]", "\n", "self", ".", "image_ids", "=", "[", "]", "\n", "self", ".", "labels", "=", "[", "]", "\n", "\n", "# First, just read in the CSV file lines and sort them.", "\n", "\n", "data", "=", "[", "]", "\n", "\n", "with", "open", "(", "self", ".", "labels_filename", ",", "newline", "=", "''", ")", "as", "csvfile", ":", "\n", "            ", "csvread", "=", "csv", ".", "reader", "(", "csvfile", ",", "delimiter", "=", "','", ")", "\n", "next", "(", "csvread", ")", "# Skip the header row.", "\n", "for", "row", "in", "csvread", ":", "# For every line (i.e for every bounding box) in the CSV file...", "\n", "                ", "if", "self", ".", "include_classes", "==", "'all'", "or", "int", "(", "row", "[", "self", ".", "input_format", ".", "index", "(", "'class_id'", ")", "]", ".", "strip", "(", ")", ")", "in", "self", ".", "include_classes", ":", "# If the class_id is among the classes that are to be included in the dataset...", "\n", "                    ", "box", "=", "[", "]", "# Store the box class and coordinates here", "\n", "box", ".", "append", "(", "row", "[", "self", ".", "input_format", ".", "index", "(", "'image_name'", ")", "]", ".", "strip", "(", ")", ")", "# Select the image name column in the input format and append its content to `box`", "\n", "for", "element", "in", "self", ".", "labels_output_format", ":", "# For each element in the output format (where the elements are the class ID and the four box coordinates)...", "\n", "                        ", "box", ".", "append", "(", "int", "(", "row", "[", "self", ".", "input_format", ".", "index", "(", "element", ")", "]", ".", "strip", "(", ")", ")", ")", "# ...select the respective column in the input format and append it to `box`.", "\n", "", "data", ".", "append", "(", "box", ")", "\n", "\n", "", "", "", "data", "=", "sorted", "(", "data", ")", "# The data needs to be sorted, otherwise the next step won't give the correct result", "\n", "\n", "# Now that we've made sure that the data is sorted by file names,", "\n", "# we can compile the actual samples and labels lists", "\n", "\n", "current_file", "=", "data", "[", "0", "]", "[", "0", "]", "# The current image for which we're collecting the ground truth boxes", "\n", "current_image_id", "=", "data", "[", "0", "]", "[", "0", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", "# The image ID will be the portion of the image name before the first dot.", "\n", "current_labels", "=", "[", "]", "# The list where we collect all ground truth boxes for a given image", "\n", "add_to_dataset", "=", "False", "\n", "for", "i", ",", "box", "in", "enumerate", "(", "data", ")", ":", "\n", "\n", "            ", "if", "box", "[", "0", "]", "==", "current_file", ":", "# If this box (i.e. this line of the CSV file) belongs to the current image file", "\n", "                ", "current_labels", ".", "append", "(", "box", "[", "1", ":", "]", ")", "\n", "if", "i", "==", "len", "(", "data", ")", "-", "1", ":", "# If this is the last line of the CSV file", "\n", "                    ", "if", "random_sample", ":", "# In case we're not using the full dataset, but a random sample of it.", "\n", "                        ", "p", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "\n", "if", "p", ">=", "(", "1", "-", "random_sample", ")", ":", "\n", "                            ", "self", ".", "labels", ".", "append", "(", "np", ".", "stack", "(", "current_labels", ",", "axis", "=", "0", ")", ")", "\n", "self", ".", "filenames", ".", "append", "(", "os", ".", "path", ".", "join", "(", "self", ".", "images_dir", ",", "current_file", ")", ")", "\n", "self", ".", "image_ids", ".", "append", "(", "current_image_id", ")", "\n", "", "", "else", ":", "\n", "                        ", "self", ".", "labels", ".", "append", "(", "np", ".", "stack", "(", "current_labels", ",", "axis", "=", "0", ")", ")", "\n", "self", ".", "filenames", ".", "append", "(", "os", ".", "path", ".", "join", "(", "self", ".", "images_dir", ",", "current_file", ")", ")", "\n", "self", ".", "image_ids", ".", "append", "(", "current_image_id", ")", "\n", "", "", "", "else", ":", "# If this box belongs to a new image file", "\n", "                ", "if", "random_sample", ":", "# In case we're not using the full dataset, but a random sample of it.", "\n", "                    ", "p", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "\n", "if", "p", ">=", "(", "1", "-", "random_sample", ")", ":", "\n", "                        ", "self", ".", "labels", ".", "append", "(", "np", ".", "stack", "(", "current_labels", ",", "axis", "=", "0", ")", ")", "\n", "self", ".", "filenames", ".", "append", "(", "os", ".", "path", ".", "join", "(", "self", ".", "images_dir", ",", "current_file", ")", ")", "\n", "self", ".", "image_ids", ".", "append", "(", "current_image_id", ")", "\n", "", "", "else", ":", "\n", "                    ", "self", ".", "labels", ".", "append", "(", "np", ".", "stack", "(", "current_labels", ",", "axis", "=", "0", ")", ")", "\n", "self", ".", "filenames", ".", "append", "(", "os", ".", "path", ".", "join", "(", "self", ".", "images_dir", ",", "current_file", ")", ")", "\n", "self", ".", "image_ids", ".", "append", "(", "current_image_id", ")", "\n", "", "current_labels", "=", "[", "]", "# Reset the labels list because this is a new file.", "\n", "current_file", "=", "box", "[", "0", "]", "\n", "current_image_id", "=", "box", "[", "0", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "current_labels", ".", "append", "(", "box", "[", "1", ":", "]", ")", "\n", "if", "i", "==", "len", "(", "data", ")", "-", "1", ":", "# If this is the last line of the CSV file", "\n", "                    ", "if", "random_sample", ":", "# In case we're not using the full dataset, but a random sample of it.", "\n", "                        ", "p", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "\n", "if", "p", ">=", "(", "1", "-", "random_sample", ")", ":", "\n", "                            ", "self", ".", "labels", ".", "append", "(", "np", ".", "stack", "(", "current_labels", ",", "axis", "=", "0", ")", ")", "\n", "self", ".", "filenames", ".", "append", "(", "os", ".", "path", ".", "join", "(", "self", ".", "images_dir", ",", "current_file", ")", ")", "\n", "self", ".", "image_ids", ".", "append", "(", "current_image_id", ")", "\n", "", "", "else", ":", "\n", "                        ", "self", ".", "labels", ".", "append", "(", "np", ".", "stack", "(", "current_labels", ",", "axis", "=", "0", ")", ")", "\n", "self", ".", "filenames", ".", "append", "(", "os", ".", "path", ".", "join", "(", "self", ".", "images_dir", ",", "current_file", ")", ")", "\n", "self", ".", "image_ids", ".", "append", "(", "current_image_id", ")", "\n", "\n", "", "", "", "", "self", ".", "dataset_size", "=", "len", "(", "self", ".", "filenames", ")", "\n", "self", ".", "dataset_indices", "=", "np", ".", "arange", "(", "self", ".", "dataset_size", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "if", "self", ".", "load_images_into_memory", ":", "\n", "            ", "self", ".", "images", "=", "[", "]", "\n", "if", "verbose", ":", "it", "=", "tqdm", "(", "self", ".", "filenames", ",", "desc", "=", "'Loading images into memory'", ",", "file", "=", "sys", ".", "stdout", ")", "\n", "else", ":", "it", "=", "self", ".", "filenames", "\n", "for", "filename", "in", "it", ":", "\n", "                ", "with", "Image", ".", "open", "(", "filename", ")", "as", "image", ":", "\n", "                    ", "self", ".", "images", ".", "append", "(", "np", ".", "array", "(", "image", ",", "dtype", "=", "np", ".", "uint8", ")", ")", "\n", "\n", "", "", "", "if", "ret", ":", "# In case we want to return these", "\n", "            ", "return", "self", ".", "images", ",", "self", ".", "filenames", ",", "self", ".", "labels", ",", "self", ".", "image_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_data_generator.DataGenerator.parse_xml": [[408, 565], ["zip", "len", "numpy.arange", "len", "open", "tqdm.tqdm.tqdm", "open", "object_detection_2d_data_generator.DataGenerator.filenames.append", "tqdm.tqdm.tqdm", "line.strip", "line.strip().split", "os.path.join", "BeautifulSoup.find_all", "object_detection_2d_data_generator.DataGenerator.labels.append", "object_detection_2d_data_generator.DataGenerator.eval_neutral.append", "PIL.Image.open", "object_detection_2d_data_generator.DataGenerator.images.append", "open", "BeautifulSoup", "object_detection_2d_data_generator.DataGenerator.classes.index", "int", "int", "obj.find", "int", "int", "int", "int", "boxes.append", "numpy.array", "os.path.basename", "line.strip", "os.path.join", "obj.find", "obj.find", "gtboxlist.index", "float", "box.append", "box.append", "box.append", "eval_neutr.append", "eval_neutr.append", "obj.find", "obj.find", "str", "str", "str", "str"], "methods", ["None"], ["", "", "def", "parse_xml", "(", "self", ",", "\n", "images_dirs", ",", "\n", "image_set_filenames", ",", "\n", "sample_weights_dirs", "=", "None", ",", "\n", "annotations_dirs", "=", "[", "]", ",", "\n", "classes", "=", "[", "'background'", ",", "\n", "'aeroplane'", ",", "'bicycle'", ",", "'bird'", ",", "'boat'", ",", "\n", "'bottle'", ",", "'bus'", ",", "'car'", ",", "'cat'", ",", "\n", "'chair'", ",", "'cow'", ",", "'diningtable'", ",", "'dog'", ",", "\n", "'horse'", ",", "'motorbike'", ",", "'person'", ",", "'pottedplant'", ",", "\n", "'sheep'", ",", "'sofa'", ",", "'train'", ",", "'tvmonitor'", "]", ",", "\n", "include_classes", "=", "'all'", ",", "\n", "exclude_truncated", "=", "False", ",", "\n", "exclude_difficult", "=", "False", ",", "\n", "ret", "=", "False", ",", "\n", "verbose", "=", "True", ")", ":", "\n", "        ", "'''\n        This is an XML parser for the Pascal VOC datasets. It might be applicable to other datasets with minor changes to\n        the code, but in its current form it expects the data format and XML tags of the Pascal VOC datasets.\n\n        Arguments:\n            images_dirs (list): A list of strings, where each string is the path of a directory that\n                contains images that are to be part of the dataset. This allows you to aggregate multiple datasets\n                into one (e.g. one directory that contains the images for Pascal VOC 2007, another that contains\n                the images for Pascal VOC 2012, etc.).\n            image_set_filenames (list): A list of strings, where each string is the path of the text file with the image\n                set to be loaded. Must be one file per image directory given. These text files define what images in the\n                respective image directories are to be part of the dataset and simply contains one image ID per line\n                and nothing else.\n            annotations_dirs (list, optional): A list of strings, where each string is the path of a directory that\n                contains the annotations (XML files) that belong to the images in the respective image directories given.\n                The directories must contain one XML file per image and the name of an XML file must be the image ID\n                of the image it belongs to. The content of the XML files must be in the Pascal VOC format.\n            classes (list, optional): A list containing the names of the object classes as found in the\n                `name` XML tags. Must include the class `background` as the first list item. The order of this list\n                defines the class IDs.\n            include_classes (list, optional): Either 'all' or a list of integers containing the class IDs that\n                are to be included in the dataset. If 'all', all ground truth boxes will be included in the dataset.\n            exclude_truncated (bool, optional): If `True`, excludes boxes that are labeled as 'truncated'.\n            exclude_difficult (bool, optional): If `True`, excludes boxes that are labeled as 'difficult'.\n            ret (bool, optional): Whether or not to return the outputs of the parser.\n            verbose (bool, optional): If `True`, prints out the progress for operations that may take a bit longer.\n\n        Returns:\n            None by default, optionally lists for whichever are available of images, image filenames, labels, image IDs,\n            and a list indicating which boxes are annotated with the label \"difficult\".\n        '''", "\n", "# Set class members.", "\n", "self", ".", "images_dirs", "=", "images_dirs", "\n", "self", ".", "annotations_dirs", "=", "annotations_dirs", "\n", "self", ".", "sample_weights_dirs", "=", "sample_weights_dirs", "\n", "self", ".", "image_set_filenames", "=", "image_set_filenames", "\n", "self", ".", "classes", "=", "classes", "\n", "self", ".", "include_classes", "=", "include_classes", "\n", "\n", "# Erase data that might have been parsed before.", "\n", "self", ".", "filenames", "=", "[", "]", "\n", "self", ".", "image_ids", "=", "[", "]", "\n", "self", ".", "labels", "=", "[", "]", "\n", "self", ".", "sample_weights", "=", "[", "]", "\n", "self", ".", "eval_neutral", "=", "[", "]", "\n", "if", "not", "annotations_dirs", ":", "\n", "            ", "self", ".", "labels", "=", "None", "\n", "self", ".", "eval_neutral", "=", "None", "\n", "annotations_dirs", "=", "[", "None", "]", "*", "len", "(", "images_dirs", ")", "\n", "\n", "", "for", "images_dir", ",", "image_set_filename", ",", "annotations_dir", "in", "zip", "(", "images_dirs", ",", "image_set_filenames", ",", "annotations_dirs", ")", ":", "\n", "# Read the image set file that so that we know all the IDs of all the images to be included in the dataset.", "\n", "            ", "with", "open", "(", "image_set_filename", ")", "as", "f", ":", "\n", "                ", "image_ids", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "f", "]", "# Note: These are strings, not integers.", "\n", "self", ".", "image_ids", "+=", "image_ids", "\n", "\n", "", "if", "verbose", ":", "it", "=", "tqdm", "(", "image_ids", ",", "desc", "=", "\"Processing image set '{}'\"", ".", "format", "(", "os", ".", "path", ".", "basename", "(", "image_set_filename", ")", ")", ",", "file", "=", "sys", ".", "stdout", ")", "\n", "else", ":", "it", "=", "image_ids", "\n", "with", "open", "(", "'/data/deeplearn/VOCdevkit/VOC2013/weights.txt'", ")", "as", "f", ":", "\n", "                ", "weights", "=", "[", "line", ".", "strip", "(", ")", ".", "split", "(", "','", ")", "for", "line", "in", "f", "]", "\n", "gtboxlist", "=", "[", "box", "[", "0", "]", "for", "box", "in", "weights", "]", "\n", "gtweightlist", "=", "[", "box", "[", "1", "]", "for", "box", "in", "weights", "]", "\n", "\n", "# Loop over all images in this dataset.", "\n", "", "for", "image_id", "in", "it", ":", "\n", "                ", "filename", "=", "'{}'", ".", "format", "(", "image_id", ")", "+", "'.jpg'", "\n", "self", ".", "filenames", ".", "append", "(", "os", ".", "path", ".", "join", "(", "images_dir", ",", "filename", ")", ")", "\n", "\n", "if", "not", "annotations_dirs", "is", "None", ":", "\n", "# Parse the XML file for this image.", "\n", "                    ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "annotations_dir", ",", "image_id", "+", "'.xml'", ")", ")", "as", "f", ":", "\n", "                        ", "soup", "=", "BeautifulSoup", "(", "f", ",", "'xml'", ")", "\n", "\n", "", "folder", "=", "soup", ".", "folder", ".", "text", "# In case we want to return the folder in addition to the image file name. Relevant for determining which dataset an image belongs to.", "\n", "#filename = soup.filename.text", "\n", "\n", "boxes", "=", "[", "]", "# We'll store all boxes for this image here.", "\n", "eval_neutr", "=", "[", "]", "# We'll store whether a box is annotated as \"difficult\" here.", "\n", "objects", "=", "soup", ".", "find_all", "(", "'object'", ")", "# Get a list of all objects in this image.", "\n", "\n", "# Parse the data for each object.", "\n", "for", "obj", "in", "objects", ":", "\n", "                        ", "class_name", "=", "obj", ".", "find", "(", "'name'", ",", "recursive", "=", "False", ")", ".", "text", "\n", "class_id", "=", "self", ".", "classes", ".", "index", "(", "class_name", ")", "\n", "# Check whether this class is supposed to be included in the dataset.", "\n", "if", "(", "not", "self", ".", "include_classes", "==", "'all'", ")", "and", "(", "not", "class_id", "in", "self", ".", "include_classes", ")", ":", "continue", "\n", "pose", "=", "obj", ".", "find", "(", "'pose'", ",", "recursive", "=", "False", ")", ".", "text", "\n", "truncated", "=", "int", "(", "obj", ".", "find", "(", "'truncated'", ",", "recursive", "=", "False", ")", ".", "text", ")", "\n", "if", "exclude_truncated", "and", "(", "truncated", "==", "1", ")", ":", "continue", "\n", "difficult", "=", "int", "(", "obj", ".", "find", "(", "'difficult'", ",", "recursive", "=", "False", ")", ".", "text", ")", "\n", "if", "exclude_difficult", "and", "(", "difficult", "==", "1", ")", ":", "continue", "\n", "# Get the bounding box coordinates.", "\n", "bndbox", "=", "obj", ".", "find", "(", "'bndbox'", ",", "recursive", "=", "False", ")", "\n", "xmin", "=", "int", "(", "bndbox", ".", "xmin", ".", "text", ")", "\n", "ymin", "=", "int", "(", "bndbox", ".", "ymin", ".", "text", ")", "\n", "xmax", "=", "int", "(", "bndbox", ".", "xmax", ".", "text", ")", "\n", "ymax", "=", "int", "(", "bndbox", ".", "ymax", ".", "text", ")", "\n", "item_dict", "=", "{", "'folder'", ":", "folder", ",", "\n", "'image_name'", ":", "filename", ",", "\n", "'image_id'", ":", "image_id", ",", "\n", "'class_name'", ":", "class_name", ",", "\n", "'class_id'", ":", "class_id", ",", "\n", "'pose'", ":", "pose", ",", "\n", "'truncated'", ":", "truncated", ",", "\n", "'difficult'", ":", "difficult", ",", "\n", "'xmin'", ":", "xmin", ",", "\n", "'ymin'", ":", "ymin", ",", "\n", "'xmax'", ":", "xmax", ",", "\n", "'ymax'", ":", "ymax", "}", "\n", "\n", "if", "not", "sample_weights_dirs", "is", "None", ":", "\n", "                            ", "boxstr", "=", "image_id", "+", "' '", "+", "str", "(", "xmin", ")", "+", "' '", "+", "str", "(", "ymin", ")", "+", "' '", "+", "str", "(", "xmax", ")", "+", "' '", "+", "str", "(", "ymax", ")", "\n", "index", "=", "gtboxlist", ".", "index", "(", "boxstr", ")", "\n", "weight", "=", "float", "(", "gtweightlist", "[", "index", "]", ")", "\n", "", "box", "=", "[", "]", "\n", "if", "not", "sample_weights_dirs", "is", "None", ":", "\n", "                            ", "box", ".", "append", "(", "weight", ")", "\n", "", "else", ":", "\n", "                            ", "box", ".", "append", "(", "1", ")", "\n", "", "for", "item", "in", "self", ".", "labels_output_format", ":", "\n", "                            ", "box", ".", "append", "(", "item_dict", "[", "item", "]", ")", "\n", "", "boxes", ".", "append", "(", "box", ")", "\n", "\n", "if", "difficult", ":", "eval_neutr", ".", "append", "(", "True", ")", "\n", "else", ":", "eval_neutr", ".", "append", "(", "False", ")", "\n", "\n", "", "self", ".", "labels", ".", "append", "(", "boxes", ")", "\n", "self", ".", "eval_neutral", ".", "append", "(", "eval_neutr", ")", "\n", "\n", "", "", "", "self", ".", "dataset_size", "=", "len", "(", "self", ".", "filenames", ")", "\n", "self", ".", "dataset_indices", "=", "np", ".", "arange", "(", "self", ".", "dataset_size", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "if", "self", ".", "load_images_into_memory", ":", "\n", "            ", "self", ".", "images", "=", "[", "]", "\n", "if", "verbose", ":", "it", "=", "tqdm", "(", "self", ".", "filenames", ",", "desc", "=", "'Loading images into memory'", ",", "file", "=", "sys", ".", "stdout", ")", "\n", "else", ":", "it", "=", "self", ".", "filenames", "\n", "for", "filename", "in", "it", ":", "\n", "                ", "with", "Image", ".", "open", "(", "filename", ")", "as", "image", ":", "\n", "                    ", "self", ".", "images", ".", "append", "(", "np", ".", "array", "(", "image", ",", "dtype", "=", "np", ".", "uint8", ")", ")", "\n", "\n", "", "", "", "if", "ret", ":", "\n", "            ", "return", "self", ".", "images", ",", "self", ".", "filenames", ",", "self", ".", "labels", ",", "self", ".", "image_ids", ",", "self", ".", "eval_neutral", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_data_generator.DataGenerator.parse_json": [[566, 690], ["object_detection_2d_data_generator.DataGenerator.classes_to_names.append", "enumerate", "zip", "len", "numpy.arange", "open", "json.load", "object_detection_2d_data_generator.DataGenerator.classes_to_names.append", "open", "json.load", "collections.defaultdict", "tqdm.tqdm.tqdm", "object_detection_2d_data_generator.DataGenerator.filenames.append", "object_detection_2d_data_generator.DataGenerator.image_ids.append", "tqdm.tqdm.tqdm", "image_ids_to_annotations[].append", "os.path.join", "object_detection_2d_data_generator.DataGenerator.labels.append", "PIL.Image.open", "object_detection_2d_data_generator.DataGenerator.images.append", "boxes.append", "numpy.array", "os.path.basename", "box.append"], "methods", ["None"], ["", "", "def", "parse_json", "(", "self", ",", "\n", "images_dirs", ",", "\n", "annotations_filenames", ",", "\n", "ground_truth_available", "=", "False", ",", "\n", "include_classes", "=", "'all'", ",", "\n", "ret", "=", "False", ",", "\n", "verbose", "=", "True", ")", ":", "\n", "        ", "'''\n        This is an JSON parser for the MS COCO datasets. It might be applicable to other datasets with minor changes to\n        the code, but in its current form it expects the JSON format of the MS COCO datasets.\n\n        Arguments:\n            images_dirs (list, optional): A list of strings, where each string is the path of a directory that\n                contains images that are to be part of the dataset. This allows you to aggregate multiple datasets\n                into one (e.g. one directory that contains the images for MS COCO Train 2014, another one for MS COCO\n                Val 2014, another one for MS COCO Train 2017 etc.).\n            annotations_filenames (list): A list of strings, where each string is the path of the JSON file\n                that contains the annotations for the images in the respective image directories given, i.e. one\n                JSON file per image directory that contains the annotations for all images in that directory.\n                The content of the JSON files must be in MS COCO object detection format. Note that these annotations\n                files do not necessarily need to contain ground truth information. MS COCO also provides annotations\n                files without ground truth information for the test datasets, called `image_info_[...].json`.\n            ground_truth_available (bool, optional): Set `True` if the annotations files contain ground truth information.\n            include_classes (list, optional): Either 'all' or a list of integers containing the class IDs that\n                are to be included in the dataset. If 'all', all ground truth boxes will be included in the dataset.\n            ret (bool, optional): Whether or not to return the outputs of the parser.\n            verbose (bool, optional): If `True`, prints out the progress for operations that may take a bit longer.\n\n        Returns:\n            None by default, optionally lists for whichever are available of images, image filenames, labels and image IDs.\n        '''", "\n", "self", ".", "images_dirs", "=", "images_dirs", "\n", "self", ".", "annotations_filenames", "=", "annotations_filenames", "\n", "self", ".", "include_classes", "=", "include_classes", "\n", "# Erase data that might have been parsed before.", "\n", "self", ".", "filenames", "=", "[", "]", "\n", "self", ".", "image_ids", "=", "[", "]", "\n", "self", ".", "labels", "=", "[", "]", "\n", "if", "not", "ground_truth_available", ":", "\n", "            ", "self", ".", "labels", "=", "None", "\n", "\n", "# Build the dictionaries that map between class names and class IDs.", "\n", "", "with", "open", "(", "annotations_filenames", "[", "0", "]", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "annotations", "=", "json", ".", "load", "(", "f", ")", "\n", "# Unfortunately the 80 MS COCO class IDs are not all consecutive. They go", "\n", "# from 1 to 90 and some numbers are skipped. Since the IDs that we feed", "\n", "# into a neural network must be consecutive, we'll save both the original", "\n", "# (non-consecutive) IDs as well as transformed maps.", "\n", "# We'll save both the map between the original", "\n", "", "self", ".", "cats_to_names", "=", "{", "}", "# The map between class names (values) and their original IDs (keys)", "\n", "self", ".", "classes_to_names", "=", "[", "]", "# A list of the class names with their indices representing the transformed IDs", "\n", "self", ".", "classes_to_names", ".", "append", "(", "'background'", ")", "# Need to add the background class first so that the indexing is right.", "\n", "self", ".", "cats_to_classes", "=", "{", "}", "# A dictionary that maps between the original (keys) and the transformed IDs (values)", "\n", "self", ".", "classes_to_cats", "=", "{", "}", "# A dictionary that maps between the transformed (keys) and the original IDs (values)", "\n", "for", "i", ",", "cat", "in", "enumerate", "(", "annotations", "[", "'categories'", "]", ")", ":", "\n", "            ", "self", ".", "cats_to_names", "[", "cat", "[", "'id'", "]", "]", "=", "cat", "[", "'name'", "]", "\n", "self", ".", "classes_to_names", ".", "append", "(", "cat", "[", "'name'", "]", ")", "\n", "self", ".", "cats_to_classes", "[", "cat", "[", "'id'", "]", "]", "=", "i", "+", "1", "\n", "self", ".", "classes_to_cats", "[", "i", "+", "1", "]", "=", "cat", "[", "'id'", "]", "\n", "\n", "# Iterate over all datasets.", "\n", "", "for", "images_dir", ",", "annotations_filename", "in", "zip", "(", "self", ".", "images_dirs", ",", "self", ".", "annotations_filenames", ")", ":", "\n", "# Load the JSON file.", "\n", "            ", "with", "open", "(", "annotations_filename", ",", "'r'", ")", "as", "f", ":", "\n", "                ", "annotations", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "if", "ground_truth_available", ":", "\n", "# Create the annotations map, a dictionary whose keys are the image IDs", "\n", "# and whose values are the annotations for the respective image ID.", "\n", "                ", "image_ids_to_annotations", "=", "defaultdict", "(", "list", ")", "\n", "for", "annotation", "in", "annotations", "[", "'annotations'", "]", ":", "\n", "                    ", "image_ids_to_annotations", "[", "annotation", "[", "'image_id'", "]", "]", ".", "append", "(", "annotation", ")", "\n", "\n", "", "", "if", "verbose", ":", "it", "=", "tqdm", "(", "annotations", "[", "'images'", "]", ",", "desc", "=", "\"Processing '{}'\"", ".", "format", "(", "os", ".", "path", ".", "basename", "(", "annotations_filename", ")", ")", ",", "file", "=", "sys", ".", "stdout", ")", "\n", "else", ":", "it", "=", "annotations", "[", "'images'", "]", "\n", "\n", "# Loop over all images in this dataset.", "\n", "for", "img", "in", "it", ":", "\n", "\n", "                ", "self", ".", "filenames", ".", "append", "(", "os", ".", "path", ".", "join", "(", "images_dir", ",", "img", "[", "'file_name'", "]", ")", ")", "\n", "self", ".", "image_ids", ".", "append", "(", "img", "[", "'id'", "]", ")", "\n", "\n", "if", "ground_truth_available", ":", "\n", "# Get all annotations for this image.", "\n", "                    ", "annotations", "=", "image_ids_to_annotations", "[", "img", "[", "'id'", "]", "]", "\n", "boxes", "=", "[", "]", "\n", "for", "annotation", "in", "annotations", ":", "\n", "                        ", "cat_id", "=", "annotation", "[", "'category_id'", "]", "\n", "# Check if this class is supposed to be included in the dataset.", "\n", "if", "(", "not", "self", ".", "include_classes", "==", "'all'", ")", "and", "(", "not", "cat_id", "in", "self", ".", "include_classes", ")", ":", "continue", "\n", "# Transform the original class ID to fit in the sequence of consecutive IDs.", "\n", "class_id", "=", "self", ".", "cats_to_classes", "[", "cat_id", "]", "\n", "xmin", "=", "annotation", "[", "'bbox'", "]", "[", "0", "]", "\n", "ymin", "=", "annotation", "[", "'bbox'", "]", "[", "1", "]", "\n", "width", "=", "annotation", "[", "'bbox'", "]", "[", "2", "]", "\n", "height", "=", "annotation", "[", "'bbox'", "]", "[", "3", "]", "\n", "# Compute `xmax` and `ymax`.", "\n", "xmax", "=", "xmin", "+", "width", "\n", "ymax", "=", "ymin", "+", "height", "\n", "item_dict", "=", "{", "'image_name'", ":", "img", "[", "'file_name'", "]", ",", "\n", "'image_id'", ":", "img", "[", "'id'", "]", ",", "\n", "'class_id'", ":", "class_id", ",", "\n", "'xmin'", ":", "xmin", ",", "\n", "'ymin'", ":", "ymin", ",", "\n", "'xmax'", ":", "xmax", ",", "\n", "'ymax'", ":", "ymax", "}", "\n", "box", "=", "[", "]", "\n", "for", "item", "in", "self", ".", "labels_output_format", ":", "\n", "                            ", "box", ".", "append", "(", "item_dict", "[", "item", "]", ")", "\n", "", "boxes", ".", "append", "(", "box", ")", "\n", "", "self", ".", "labels", ".", "append", "(", "boxes", ")", "\n", "\n", "", "", "", "self", ".", "dataset_size", "=", "len", "(", "self", ".", "filenames", ")", "\n", "self", ".", "dataset_indices", "=", "np", ".", "arange", "(", "self", ".", "dataset_size", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "if", "self", ".", "load_images_into_memory", ":", "\n", "            ", "self", ".", "images", "=", "[", "]", "\n", "if", "verbose", ":", "it", "=", "tqdm", "(", "self", ".", "filenames", ",", "desc", "=", "'Loading images into memory'", ",", "file", "=", "sys", ".", "stdout", ")", "\n", "else", ":", "it", "=", "self", ".", "filenames", "\n", "for", "filename", "in", "it", ":", "\n", "                ", "with", "Image", ".", "open", "(", "filename", ")", "as", "image", ":", "\n", "                    ", "self", ".", "images", ".", "append", "(", "np", ".", "array", "(", "image", ",", "dtype", "=", "np", ".", "uint8", ")", ")", "\n", "\n", "", "", "", "if", "ret", ":", "\n", "            ", "return", "self", ".", "images", ",", "self", ".", "filenames", ",", "self", ".", "labels", ",", "self", ".", "image_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_data_generator.DataGenerator.create_hdf5_dataset": [[691, 853], ["len", "h5py.File", "h5py.File.attrs.create", "h5py.File.attrs.create", "h5py.File.attrs.create", "h5py.File.create_dataset", "h5py.File.create_dataset", "h5py.File.close", "h5py.File", "len", "numpy.arange", "h5py.File.attrs.create", "h5py.File.attrs.create", "h5py.File.create_dataset", "h5py.File.create_dataset", "h5py.File.attrs.modify", "h5py.File.create_dataset", "h5py.File.attrs.modify", "h5py.File.create_dataset", "h5py.File.attrs.modify", "tqdm.tqdm.trange", "range", "h5py.special_dtype", "PIL.Image.open", "numpy.asarray", "numpy.concatenate.reshape", "numpy.asarray", "numpy.asarray.reshape", "h5py.special_dtype", "h5py.special_dtype", "h5py.special_dtype", "numpy.stack", "cv2.resize", "numpy.concatenate"], "methods", ["None"], ["", "", "def", "create_hdf5_dataset", "(", "self", ",", "\n", "file_path", "=", "'dataset.h5'", ",", "\n", "resize", "=", "False", ",", "\n", "variable_image_size", "=", "True", ",", "\n", "verbose", "=", "True", ")", ":", "\n", "        ", "'''\n        Converts the currently loaded dataset into a HDF5 file. This HDF5 file contains all\n        images as uncompressed arrays in a contiguous block of memory, which allows for them\n        to be loaded faster. Such an uncompressed dataset, however, may take up considerably\n        more space on your hard drive than the sum of the source images in a compressed format\n        such as JPG or PNG.\n\n        It is recommended that you always convert the dataset into an HDF5 dataset if you\n        have enugh hard drive space since loading from an HDF5 dataset accelerates the data\n        generation noticeably.\n\n        Note that you must load a dataset (e.g. via one of the parser methods) before creating\n        an HDF5 dataset from it.\n\n        The created HDF5 dataset will remain open upon its creation so that it can be used right\n        away.\n\n        Arguments:\n            file_path (str, optional): The full file path under which to store the HDF5 dataset.\n                You can load this output file via the `DataGenerator` constructor in the future.\n            resize (tuple, optional): `False` or a 2-tuple `(height, width)` that represents the\n                target size for the images. All images in the dataset will be resized to this\n                target size before they will be written to the HDF5 file. If `False`, no resizing\n                will be performed.\n            variable_image_size (bool, optional): The only purpose of this argument is that its\n                value will be stored in the HDF5 dataset in order to be able to quickly find out\n                whether the images in the dataset all have the same size or not.\n            verbose (bool, optional): Whether or not prit out the progress of the dataset creation.\n\n        Returns:\n            None.\n        '''", "\n", "\n", "self", ".", "hdf5_dataset_path", "=", "file_path", "\n", "\n", "dataset_size", "=", "len", "(", "self", ".", "filenames", ")", "\n", "\n", "# Create the HDF5 file.", "\n", "hdf5_dataset", "=", "h5py", ".", "File", "(", "file_path", ",", "'w'", ")", "\n", "\n", "# Create a few attributes that tell us what this dataset contains.", "\n", "# The dataset will obviously always contain images, but maybe it will", "\n", "# also contain labels, image IDs, etc.", "\n", "hdf5_dataset", ".", "attrs", ".", "create", "(", "name", "=", "'has_labels'", ",", "data", "=", "False", ",", "shape", "=", "None", ",", "dtype", "=", "np", ".", "bool_", ")", "\n", "hdf5_dataset", ".", "attrs", ".", "create", "(", "name", "=", "'has_image_ids'", ",", "data", "=", "False", ",", "shape", "=", "None", ",", "dtype", "=", "np", ".", "bool_", ")", "\n", "hdf5_dataset", ".", "attrs", ".", "create", "(", "name", "=", "'has_eval_neutral'", ",", "data", "=", "False", ",", "shape", "=", "None", ",", "dtype", "=", "np", ".", "bool_", ")", "\n", "# It's useful to be able to quickly check whether the images in a dataset all", "\n", "# have the same size or not, so add a boolean attribute for that.", "\n", "if", "variable_image_size", "and", "not", "resize", ":", "\n", "            ", "hdf5_dataset", ".", "attrs", ".", "create", "(", "name", "=", "'variable_image_size'", ",", "data", "=", "True", ",", "shape", "=", "None", ",", "dtype", "=", "np", ".", "bool_", ")", "\n", "", "else", ":", "\n", "            ", "hdf5_dataset", ".", "attrs", ".", "create", "(", "name", "=", "'variable_image_size'", ",", "data", "=", "False", ",", "shape", "=", "None", ",", "dtype", "=", "np", ".", "bool_", ")", "\n", "\n", "# Create the dataset in which the images will be stored as flattened arrays.", "\n", "# This allows us, among other things, to store images of variable size.", "\n", "", "hdf5_images", "=", "hdf5_dataset", ".", "create_dataset", "(", "name", "=", "'images'", ",", "\n", "shape", "=", "(", "dataset_size", ",", ")", ",", "\n", "maxshape", "=", "(", "None", ")", ",", "\n", "dtype", "=", "h5py", ".", "special_dtype", "(", "vlen", "=", "np", ".", "uint8", ")", ")", "\n", "\n", "# Create the dataset that will hold the image heights, widths and channels that", "\n", "# we need in order to reconstruct the images from the flattened arrays later.", "\n", "hdf5_image_shapes", "=", "hdf5_dataset", ".", "create_dataset", "(", "name", "=", "'image_shapes'", ",", "\n", "shape", "=", "(", "dataset_size", ",", "3", ")", ",", "\n", "maxshape", "=", "(", "None", ",", "3", ")", ",", "\n", "dtype", "=", "np", ".", "int32", ")", "\n", "\n", "if", "not", "(", "self", ".", "labels", "is", "None", ")", ":", "\n", "\n", "# Create the dataset in which the labels will be stored as flattened arrays.", "\n", "            ", "hdf5_labels", "=", "hdf5_dataset", ".", "create_dataset", "(", "name", "=", "'labels'", ",", "\n", "shape", "=", "(", "dataset_size", ",", ")", ",", "\n", "maxshape", "=", "(", "None", ")", ",", "\n", "dtype", "=", "h5py", ".", "special_dtype", "(", "vlen", "=", "np", ".", "int32", ")", ")", "\n", "\n", "# Create the dataset that will hold the dimensions of the labels arrays for", "\n", "# each image so that we can restore the labels from the flattened arrays later.", "\n", "hdf5_label_shapes", "=", "hdf5_dataset", ".", "create_dataset", "(", "name", "=", "'label_shapes'", ",", "\n", "shape", "=", "(", "dataset_size", ",", "2", ")", ",", "\n", "maxshape", "=", "(", "None", ",", "2", ")", ",", "\n", "dtype", "=", "np", ".", "int32", ")", "\n", "\n", "hdf5_dataset", ".", "attrs", ".", "modify", "(", "name", "=", "'has_labels'", ",", "value", "=", "True", ")", "\n", "\n", "", "if", "not", "(", "self", ".", "image_ids", "is", "None", ")", ":", "\n", "\n", "            ", "hdf5_image_ids", "=", "hdf5_dataset", ".", "create_dataset", "(", "name", "=", "'image_ids'", ",", "\n", "shape", "=", "(", "dataset_size", ",", ")", ",", "\n", "maxshape", "=", "(", "None", ")", ",", "\n", "dtype", "=", "h5py", ".", "special_dtype", "(", "vlen", "=", "str", ")", ")", "\n", "\n", "hdf5_dataset", ".", "attrs", ".", "modify", "(", "name", "=", "'has_image_ids'", ",", "value", "=", "True", ")", "\n", "\n", "", "if", "not", "(", "self", ".", "eval_neutral", "is", "None", ")", ":", "\n", "\n", "# Create the dataset in which the labels will be stored as flattened arrays.", "\n", "            ", "hdf5_eval_neutral", "=", "hdf5_dataset", ".", "create_dataset", "(", "name", "=", "'eval_neutral'", ",", "\n", "shape", "=", "(", "dataset_size", ",", ")", ",", "\n", "maxshape", "=", "(", "None", ")", ",", "\n", "dtype", "=", "h5py", ".", "special_dtype", "(", "vlen", "=", "np", ".", "bool_", ")", ")", "\n", "\n", "hdf5_dataset", ".", "attrs", ".", "modify", "(", "name", "=", "'has_eval_neutral'", ",", "value", "=", "True", ")", "\n", "\n", "", "if", "verbose", ":", "\n", "            ", "tr", "=", "trange", "(", "dataset_size", ",", "desc", "=", "'Creating HDF5 dataset'", ",", "file", "=", "sys", ".", "stdout", ")", "\n", "", "else", ":", "\n", "            ", "tr", "=", "range", "(", "dataset_size", ")", "\n", "\n", "# Iterate over all images in the dataset.", "\n", "", "for", "i", "in", "tr", ":", "\n", "\n", "# Store the image.", "\n", "            ", "with", "Image", ".", "open", "(", "self", ".", "filenames", "[", "i", "]", ")", "as", "image", ":", "\n", "\n", "                ", "image", "=", "np", ".", "asarray", "(", "image", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "\n", "# Make sure all images end up having three channels.", "\n", "if", "image", ".", "ndim", "==", "2", ":", "\n", "                    ", "image", "=", "np", ".", "stack", "(", "[", "image", "]", "*", "3", ",", "axis", "=", "-", "1", ")", "\n", "", "elif", "image", ".", "ndim", "==", "3", ":", "\n", "                    ", "if", "image", ".", "shape", "[", "2", "]", "==", "1", ":", "\n", "                        ", "image", "=", "np", ".", "concatenate", "(", "[", "image", "]", "*", "3", ",", "axis", "=", "-", "1", ")", "\n", "", "elif", "image", ".", "shape", "[", "2", "]", "==", "4", ":", "\n", "                        ", "image", "=", "image", "[", ":", ",", ":", ",", ":", "3", "]", "\n", "\n", "", "", "if", "resize", ":", "\n", "                    ", "image", "=", "cv2", ".", "resize", "(", "image", ",", "dsize", "=", "(", "resize", "[", "1", "]", ",", "resize", "[", "0", "]", ")", ")", "\n", "\n", "# Flatten the image array and write it to the images dataset.", "\n", "", "hdf5_images", "[", "i", "]", "=", "image", ".", "reshape", "(", "-", "1", ")", "\n", "# Write the image's shape to the image shapes dataset.", "\n", "hdf5_image_shapes", "[", "i", "]", "=", "image", ".", "shape", "\n", "\n", "# Store the ground truth if we have any.", "\n", "", "if", "not", "(", "self", ".", "labels", "is", "None", ")", ":", "\n", "\n", "                ", "labels", "=", "np", ".", "asarray", "(", "self", ".", "labels", "[", "i", "]", ")", "\n", "# Flatten the labels array and write it to the labels dataset.", "\n", "hdf5_labels", "[", "i", "]", "=", "labels", ".", "reshape", "(", "-", "1", ")", "\n", "# Write the labels' shape to the label shapes dataset.", "\n", "hdf5_label_shapes", "[", "i", "]", "=", "labels", ".", "shape", "\n", "\n", "# Store the image ID if we have one.", "\n", "", "if", "not", "(", "self", ".", "image_ids", "is", "None", ")", ":", "\n", "\n", "                ", "hdf5_image_ids", "[", "i", "]", "=", "self", ".", "image_ids", "[", "i", "]", "\n", "\n", "# Store the evaluation-neutrality annotations if we have any.", "\n", "", "if", "not", "(", "self", ".", "eval_neutral", "is", "None", ")", ":", "\n", "\n", "                ", "hdf5_eval_neutral", "[", "i", "]", "=", "self", ".", "eval_neutral", "[", "i", "]", "\n", "\n", "", "", "hdf5_dataset", ".", "close", "(", ")", "\n", "self", ".", "hdf5_dataset", "=", "h5py", ".", "File", "(", "file_path", ",", "'r'", ")", "\n", "self", ".", "hdf5_dataset_path", "=", "file_path", "\n", "self", ".", "dataset_size", "=", "len", "(", "self", ".", "hdf5_dataset", "[", "'images'", "]", ")", "\n", "self", ".", "dataset_indices", "=", "np", ".", "arange", "(", "self", ".", "dataset_size", ",", "dtype", "=", "np", ".", "int32", ")", "# Instead of shuffling the HDF5 dataset, we will shuffle this index list.", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_data_generator.DataGenerator.generate": [[854, 1206], ["object_detection_2d_data_generator.DatasetError", "any", "sklearn.utils.shuffle", "range", "data_generator.object_detection_2d_image_boxes_validation_utils.BoxFilter", "range", "numpy.array", "warnings.warn", "any", "objects_to_shuffle.append", "objects_to_shuffle.append", "objects_to_shuffle.append", "objects_to_shuffle.append", "len", "copy.deepcopy", "copy.deepcopy", "copy.deepcopy", "len", "sorted", "object_detection_2d_data_generator.DegenerateBatchError", "ret.append", "ret.append", "ret.append", "ret.append", "ret.append", "ret.append", "ret.append", "ret.append", "ret.append", "ret.append", "warnings.warn", "isinstance", "sklearn.utils.shuffle", "range", "numpy.array.append", "numpy.array", "batch_inverse_transforms.append", "numpy.array.pop", "batch_filenames.pop", "isinstance", "label_encoder", "label_encoder", "warnings.warn", "objects_to_shuffle.append", "objects_to_shuffle.append", "objects_to_shuffle.append", "objects_to_shuffle.append", "len", "numpy.array.append", "batch_items_to_remove.append", "batch_inverse_transforms.append", "numpy.any", "numpy.any", "batch_inverse_transforms.pop", "copy.deepcopy.pop", "batch_image_ids.pop", "batch_eval_neutral.pop", "copy.deepcopy.pop", "copy.deepcopy.pop", "[].reshape", "PIL.Image.open", "numpy.array.append", "warnings.warn", "numpy.array", "transform", "inverse_transforms.append", "transform", "batch_items_to_remove.append", "batch_inverse_transforms.append", "transform", "inverse_transforms.append", "transform", "data_generator.object_detection_2d_image_boxes_validation_utils.BoxFilter.", "batch_items_to_remove.append", "inspect.signature", "inspect.signature"], "methods", ["None"], ["", "def", "generate", "(", "self", ",", "\n", "batch_size", "=", "32", ",", "\n", "shuffle", "=", "True", ",", "\n", "transformations", "=", "[", "]", ",", "\n", "label_encoder", "=", "None", ",", "\n", "returns", "=", "{", "'processed_images'", ",", "'encoded_labels'", "}", ",", "\n", "keep_images_without_gt", "=", "False", ",", "\n", "degenerate_box_handling", "=", "'remove'", ")", ":", "\n", "        ", "'''\n        Generates batches of samples and (optionally) corresponding labels indefinitely.\n\n        Can shuffle the samples consistently after each complete pass.\n\n        Optionally takes a list of arbitrary image transformations to apply to the\n        samples ad hoc.\n\n        Arguments:\n            batch_size (int, optional): The size of the batches to be generated.\n            shuffle (bool, optional): Whether or not to shuffle the dataset before each pass.\n                This option should always be `True` during training, but it can be useful to turn shuffling off\n                for debugging or if you're using the generator for prediction.\n            transformations (list, optional): A list of transformations that will be applied to the images and labels\n                in the given order. Each transformation is a callable that takes as input an image (as a Numpy array)\n                and optionally labels (also as a Numpy array) and returns an image and optionally labels in the same\n                format.\n            label_encoder (callable, optional): Only relevant if labels are given. A callable that takes as input the\n                labels of a batch (as a list of Numpy arrays) and returns some structure that represents those labels.\n                The general use case for this is to convert labels from their input format to a format that a given object\n                detection model needs as its training targets.\n            returns (set, optional): A set of strings that determines what outputs the generator yields. The generator's output\n                is always a tuple that contains the outputs specified in this set and only those. If an output is not available,\n                it will be `None`. The output tuple can contain the following outputs according to the specified keyword strings:\n                * 'processed_images': An array containing the processed images. Will always be in the outputs, so it doesn't\n                    matter whether or not you include this keyword in the set.\n                * 'encoded_labels': The encoded labels tensor. Will always be in the outputs if a label encoder is given,\n                    so it doesn't matter whether or not you include this keyword in the set if you pass a label encoder.\n                * 'matched_anchors': Only available if `labels_encoder` is an `SSDInputEncoder` object. The same as 'encoded_labels',\n                    but containing anchor box coordinates for all matched anchor boxes instead of ground truth coordinates.\n                    This can be useful to visualize what anchor boxes are being matched to each ground truth box. Only available\n                    in training mode.\n                * 'processed_labels': The processed, but not yet encoded labels. This is a list that contains for each\n                    batch image a Numpy array with all ground truth boxes for that image. Only available if ground truth is available.\n                * 'filenames': A list containing the file names (full paths) of the images in the batch.\n                * 'image_ids': A list containing the integer IDs of the images in the batch. Only available if there\n                    are image IDs available.\n                * 'evaluation-neutral': A nested list of lists of booleans. Each list contains `True` or `False` for every ground truth\n                    bounding box of the respective image depending on whether that bounding box is supposed to be evaluation-neutral (`True`)\n                    or not (`False`). May return `None` if there exists no such concept for a given dataset. An example for\n                    evaluation-neutrality are the ground truth boxes annotated as \"difficult\" in the Pascal VOC datasets, which are\n                    usually treated to be neutral in a model evaluation.\n                * 'inverse_transform': A nested list that contains a list of \"inverter\" functions for each item in the batch.\n                    These inverter functions take (predicted) labels for an image as input and apply the inverse of the transformations\n                    that were applied to the original image to them. This makes it possible to let the model make predictions on a\n                    transformed image and then convert these predictions back to the original image. This is mostly relevant for\n                    evaluation: If you want to evaluate your model on a dataset with varying image sizes, then you are forced to\n                    transform the images somehow (e.g. by resizing or cropping) to make them all the same size. Your model will then\n                    predict boxes for those transformed images, but for the evaluation you will need predictions with respect to the\n                    original images, not with respect to the transformed images. This means you will have to transform the predicted\n                    box coordinates back to the original image sizes. Note that for each image, the inverter functions for that\n                    image need to be applied in the order in which they are given in the respective list for that image.\n                * 'original_images': A list containing the original images in the batch before any processing.\n                * 'original_labels': A list containing the original ground truth boxes for the images in this batch before any\n                    processing. Only available if ground truth is available.\n                The order of the outputs in the tuple is the order of the list above. If `returns` contains a keyword for an\n                output that is unavailable, that output omitted in the yielded tuples and a warning will be raised.\n            keep_images_without_gt (bool, optional): If `False`, images for which there aren't any ground truth boxes before\n                any transformations have been applied will be removed from the batch. If `True`, such images will be kept\n                in the batch.\n            degenerate_box_handling (str, optional): How to handle degenerate boxes, which are boxes that have `xmax <= xmin` and/or\n                `ymax <= ymin`. Degenerate boxes can sometimes be in the dataset, or non-degenerate boxes can become degenerate\n                after they were processed by transformations. Note that the generator checks for degenerate boxes after all\n                transformations have been applied (if any), but before the labels were passed to the `label_encoder` (if one was given).\n                Can be one of 'warn' or 'remove'. If 'warn', the generator will merely print a warning to let you know that there\n                are degenerate boxes in a batch. If 'remove', the generator will remove degenerate boxes from the batch silently.\n\n        Yields:\n            The next batch as a tuple of items as defined by the `returns` argument.\n        '''", "\n", "\n", "if", "self", ".", "dataset_size", "==", "0", ":", "\n", "            ", "raise", "DatasetError", "(", "\"Cannot generate batches because you did not load a dataset.\"", ")", "\n", "\n", "#############################################################################################", "\n", "# Warn if any of the set returns aren't possible.", "\n", "#############################################################################################", "\n", "\n", "", "if", "self", ".", "labels", "is", "None", ":", "\n", "            ", "if", "any", "(", "[", "ret", "in", "returns", "for", "ret", "in", "[", "'original_labels'", ",", "'processed_labels'", ",", "'encoded_labels'", ",", "'matched_anchors'", ",", "'evaluation-neutral'", "]", "]", ")", ":", "\n", "                ", "warnings", ".", "warn", "(", "\"Since no labels were given, none of 'original_labels', 'processed_labels', 'evaluation-neutral', 'encoded_labels', and 'matched_anchors' \"", "+", "\n", "\"are possible returns, but you set `returns = {}`. The impossible returns will be `None`.\"", ".", "format", "(", "returns", ")", ")", "\n", "", "", "elif", "label_encoder", "is", "None", ":", "\n", "            ", "if", "any", "(", "[", "ret", "in", "returns", "for", "ret", "in", "[", "'encoded_labels'", ",", "'matched_anchors'", "]", "]", ")", ":", "\n", "                ", "warnings", ".", "warn", "(", "\"Since no label encoder was given, 'encoded_labels' and 'matched_anchors' aren't possible returns, \"", "+", "\n", "\"but you set `returns = {}`. The impossible returns will be `None`.\"", ".", "format", "(", "returns", ")", ")", "\n", "", "", "elif", "not", "isinstance", "(", "label_encoder", ",", "SSDInputEncoder", ")", ":", "\n", "            ", "if", "'matched_anchors'", "in", "returns", ":", "\n", "                ", "warnings", ".", "warn", "(", "\"`label_encoder` is not an `SSDInputEncoder` object, therefore 'matched_anchors' is not a possible return, \"", "+", "\n", "\"but you set `returns = {}`. The impossible returns will be `None`.\"", ".", "format", "(", "returns", ")", ")", "\n", "\n", "#############################################################################################", "\n", "# Do a few preparatory things like maybe shuffling the dataset initially.", "\n", "#############################################################################################", "\n", "\n", "", "", "if", "shuffle", ":", "\n", "            ", "objects_to_shuffle", "=", "[", "self", ".", "dataset_indices", "]", "\n", "if", "not", "(", "self", ".", "filenames", "is", "None", ")", ":", "\n", "                ", "objects_to_shuffle", ".", "append", "(", "self", ".", "filenames", ")", "\n", "", "if", "not", "(", "self", ".", "labels", "is", "None", ")", ":", "\n", "                ", "objects_to_shuffle", ".", "append", "(", "self", ".", "labels", ")", "\n", "", "if", "not", "(", "self", ".", "image_ids", "is", "None", ")", ":", "\n", "                ", "objects_to_shuffle", ".", "append", "(", "self", ".", "image_ids", ")", "\n", "", "if", "not", "(", "self", ".", "eval_neutral", "is", "None", ")", ":", "\n", "                ", "objects_to_shuffle", ".", "append", "(", "self", ".", "eval_neutral", ")", "\n", "", "shuffled_objects", "=", "sklearn", ".", "utils", ".", "shuffle", "(", "*", "objects_to_shuffle", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "objects_to_shuffle", ")", ")", ":", "\n", "                ", "objects_to_shuffle", "[", "i", "]", "[", ":", "]", "=", "shuffled_objects", "[", "i", "]", "\n", "\n", "", "", "if", "degenerate_box_handling", "==", "'remove'", ":", "\n", "            ", "box_filter", "=", "BoxFilter", "(", "check_overlap", "=", "False", ",", "\n", "check_min_area", "=", "False", ",", "\n", "check_degenerate", "=", "True", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n", "# Override the labels formats of all the transformations to make sure they are set correctly.", "\n", "", "if", "not", "(", "self", ".", "labels", "is", "None", ")", ":", "\n", "            ", "for", "transform", "in", "transformations", ":", "\n", "                ", "transform", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "\n", "#############################################################################################", "\n", "# Generate mini batches.", "\n", "#############################################################################################", "\n", "\n", "", "", "current", "=", "0", "\n", "\n", "while", "True", ":", "\n", "\n", "            ", "batch_X", ",", "batch_y", "=", "[", "]", ",", "[", "]", "\n", "\n", "if", "current", ">=", "self", ".", "dataset_size", ":", "\n", "                ", "current", "=", "0", "\n", "\n", "#########################################################################################", "\n", "# Maybe shuffle the dataset if a full pass over the dataset has finished.", "\n", "#########################################################################################", "\n", "\n", "if", "shuffle", ":", "\n", "                    ", "objects_to_shuffle", "=", "[", "self", ".", "dataset_indices", "]", "\n", "if", "not", "(", "self", ".", "filenames", "is", "None", ")", ":", "\n", "                        ", "objects_to_shuffle", ".", "append", "(", "self", ".", "filenames", ")", "\n", "", "if", "not", "(", "self", ".", "labels", "is", "None", ")", ":", "\n", "                        ", "objects_to_shuffle", ".", "append", "(", "self", ".", "labels", ")", "\n", "", "if", "not", "(", "self", ".", "image_ids", "is", "None", ")", ":", "\n", "                        ", "objects_to_shuffle", ".", "append", "(", "self", ".", "image_ids", ")", "\n", "", "if", "not", "(", "self", ".", "eval_neutral", "is", "None", ")", ":", "\n", "                        ", "objects_to_shuffle", ".", "append", "(", "self", ".", "eval_neutral", ")", "\n", "", "shuffled_objects", "=", "sklearn", ".", "utils", ".", "shuffle", "(", "*", "objects_to_shuffle", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "objects_to_shuffle", ")", ")", ":", "\n", "                        ", "objects_to_shuffle", "[", "i", "]", "[", ":", "]", "=", "shuffled_objects", "[", "i", "]", "\n", "\n", "#########################################################################################", "\n", "# Get the images, (maybe) image IDs, (maybe) labels, etc. for this batch.", "\n", "#########################################################################################", "\n", "\n", "# We prioritize our options in the following order:", "\n", "# 1) If we have the images already loaded in memory, get them from there.", "\n", "# 2) Else, if we have an HDF5 dataset, get the images from there.", "\n", "# 3) Else, if we have neither of the above, we'll have to load the individual image", "\n", "#    files from disk.", "\n", "", "", "", "batch_indices", "=", "self", ".", "dataset_indices", "[", "current", ":", "current", "+", "batch_size", "]", "\n", "if", "not", "(", "self", ".", "images", "is", "None", ")", ":", "\n", "                ", "for", "i", "in", "batch_indices", ":", "\n", "                    ", "batch_X", ".", "append", "(", "self", ".", "images", "[", "i", "]", ")", "\n", "", "if", "not", "(", "self", ".", "filenames", "is", "None", ")", ":", "\n", "                    ", "batch_filenames", "=", "self", ".", "filenames", "[", "current", ":", "current", "+", "batch_size", "]", "\n", "", "else", ":", "\n", "                    ", "batch_filenames", "=", "None", "\n", "", "", "elif", "not", "(", "self", ".", "hdf5_dataset", "is", "None", ")", ":", "\n", "                ", "for", "i", "in", "batch_indices", ":", "\n", "                    ", "batch_X", ".", "append", "(", "self", ".", "hdf5_dataset", "[", "'images'", "]", "[", "i", "]", ".", "reshape", "(", "self", ".", "hdf5_dataset", "[", "'image_shapes'", "]", "[", "i", "]", ")", ")", "\n", "", "if", "not", "(", "self", ".", "filenames", "is", "None", ")", ":", "\n", "                    ", "batch_filenames", "=", "self", ".", "filenames", "[", "current", ":", "current", "+", "batch_size", "]", "\n", "", "else", ":", "\n", "                    ", "batch_filenames", "=", "None", "\n", "", "", "else", ":", "\n", "                ", "batch_filenames", "=", "self", ".", "filenames", "[", "current", ":", "current", "+", "batch_size", "]", "\n", "for", "filename", "in", "batch_filenames", ":", "\n", "                    ", "with", "Image", ".", "open", "(", "filename", ")", "as", "image", ":", "\n", "                        ", "batch_X", ".", "append", "(", "np", ".", "array", "(", "image", ",", "dtype", "=", "np", ".", "uint8", ")", ")", "\n", "\n", "# Get the labels for this batch (if there are any).", "\n", "", "", "", "if", "not", "(", "self", ".", "labels", "is", "None", ")", ":", "\n", "                ", "batch_y", "=", "deepcopy", "(", "self", ".", "labels", "[", "current", ":", "current", "+", "batch_size", "]", ")", "\n", "", "else", ":", "\n", "                ", "batch_y", "=", "None", "\n", "\n", "", "if", "not", "(", "self", ".", "eval_neutral", "is", "None", ")", ":", "\n", "                ", "batch_eval_neutral", "=", "self", ".", "eval_neutral", "[", "current", ":", "current", "+", "batch_size", "]", "\n", "", "else", ":", "\n", "                ", "batch_eval_neutral", "=", "None", "\n", "\n", "# Get the image IDs for this batch (if there are any).", "\n", "", "if", "not", "(", "self", ".", "image_ids", "is", "None", ")", ":", "\n", "                ", "batch_image_ids", "=", "self", ".", "image_ids", "[", "current", ":", "current", "+", "batch_size", "]", "\n", "", "else", ":", "\n", "                ", "batch_image_ids", "=", "None", "\n", "\n", "", "if", "'original_images'", "in", "returns", ":", "\n", "                ", "batch_original_images", "=", "deepcopy", "(", "batch_X", ")", "# The original, unaltered images", "\n", "", "if", "'original_labels'", "in", "returns", ":", "\n", "                ", "batch_original_labels", "=", "deepcopy", "(", "batch_y", ")", "# The original, unaltered labels", "\n", "\n", "", "current", "+=", "batch_size", "\n", "\n", "#########################################################################################", "\n", "# Maybe perform image transformations.", "\n", "#########################################################################################", "\n", "\n", "batch_items_to_remove", "=", "[", "]", "# In case we need to remove any images from the batch, store their indices in this list.", "\n", "batch_inverse_transforms", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "batch_X", ")", ")", ":", "\n", "\n", "                ", "if", "not", "(", "self", ".", "labels", "is", "None", ")", ":", "\n", "# Convert the labels for this image to an array (in case they aren't already).", "\n", "                    ", "batch_y", "[", "i", "]", "=", "np", ".", "array", "(", "batch_y", "[", "i", "]", ")", "\n", "# If this image has no ground truth boxes, maybe we don't want to keep it in the batch.", "\n", "if", "(", "batch_y", "[", "i", "]", ".", "size", "==", "0", ")", "and", "not", "keep_images_without_gt", ":", "\n", "                        ", "batch_items_to_remove", ".", "append", "(", "i", ")", "\n", "batch_inverse_transforms", ".", "append", "(", "[", "]", ")", "\n", "continue", "\n", "\n", "# Apply any image transformations we may have received.", "\n", "", "", "if", "transformations", ":", "\n", "\n", "                    ", "inverse_transforms", "=", "[", "]", "\n", "\n", "for", "transform", "in", "transformations", ":", "\n", "\n", "                        ", "if", "not", "(", "self", ".", "labels", "is", "None", ")", ":", "\n", "\n", "                            ", "if", "(", "'inverse_transform'", "in", "returns", ")", "and", "(", "'return_inverter'", "in", "inspect", ".", "signature", "(", "transform", ")", ".", "parameters", ")", ":", "\n", "                                ", "batch_X", "[", "i", "]", ",", "batch_y", "[", "i", "]", ",", "inverse_transform", "=", "transform", "(", "batch_X", "[", "i", "]", ",", "batch_y", "[", "i", "]", ",", "return_inverter", "=", "True", ")", "\n", "inverse_transforms", ".", "append", "(", "inverse_transform", ")", "\n", "", "else", ":", "\n", "                                ", "batch_X", "[", "i", "]", ",", "batch_y", "[", "i", "]", "=", "transform", "(", "batch_X", "[", "i", "]", ",", "batch_y", "[", "i", "]", ")", "\n", "\n", "", "if", "batch_X", "[", "i", "]", "is", "None", ":", "# In case the transform failed to produce an output image, which is possible for some random transforms.", "\n", "                                ", "batch_items_to_remove", ".", "append", "(", "i", ")", "\n", "batch_inverse_transforms", ".", "append", "(", "[", "]", ")", "\n", "continue", "\n", "\n", "", "", "else", ":", "\n", "\n", "                            ", "if", "(", "'inverse_transform'", "in", "returns", ")", "and", "(", "'return_inverter'", "in", "inspect", ".", "signature", "(", "transform", ")", ".", "parameters", ")", ":", "\n", "                                ", "batch_X", "[", "i", "]", ",", "inverse_transform", "=", "transform", "(", "batch_X", "[", "i", "]", ",", "return_inverter", "=", "True", ")", "\n", "inverse_transforms", ".", "append", "(", "inverse_transform", ")", "\n", "", "else", ":", "\n", "                                ", "batch_X", "[", "i", "]", "=", "transform", "(", "batch_X", "[", "i", "]", ")", "\n", "\n", "", "", "", "batch_inverse_transforms", ".", "append", "(", "inverse_transforms", "[", ":", ":", "-", "1", "]", ")", "\n", "\n", "#########################################################################################", "\n", "# Check for degenerate boxes in this batch item.", "\n", "#########################################################################################", "\n", "\n", "", "if", "not", "(", "self", ".", "labels", "is", "None", ")", ":", "\n", "\n", "                    ", "xmin", "=", "self", ".", "labels_format", "[", "'xmin'", "]", "\n", "ymin", "=", "self", ".", "labels_format", "[", "'ymin'", "]", "\n", "xmax", "=", "self", ".", "labels_format", "[", "'xmax'", "]", "\n", "ymax", "=", "self", ".", "labels_format", "[", "'ymax'", "]", "\n", "\n", "if", "np", ".", "any", "(", "batch_y", "[", "i", "]", "[", ":", ",", "xmax", "]", "-", "batch_y", "[", "i", "]", "[", ":", ",", "xmin", "]", "<=", "0", ")", "or", "np", ".", "any", "(", "batch_y", "[", "i", "]", "[", ":", ",", "ymax", "]", "-", "batch_y", "[", "i", "]", "[", ":", ",", "ymin", "]", "<=", "0", ")", ":", "\n", "                        ", "if", "degenerate_box_handling", "==", "'warn'", ":", "\n", "                            ", "warnings", ".", "warn", "(", "\"Detected degenerate ground truth bounding boxes for batch item {} with bounding boxes {}, \"", ".", "format", "(", "i", ",", "batch_y", "[", "i", "]", ")", "+", "\n", "\"i.e. bounding boxes where xmax <= xmin and/or ymax <= ymin. \"", "+", "\n", "\"This could mean that your dataset contains degenerate ground truth boxes, or that any image transformations you may apply might \"", "+", "\n", "\"result in degenerate ground truth boxes, or that you are parsing the ground truth in the wrong coordinate format.\"", "+", "\n", "\"Degenerate ground truth bounding boxes may lead to NaN errors during the training.\"", ")", "\n", "", "elif", "degenerate_box_handling", "==", "'remove'", ":", "\n", "                            ", "batch_y", "[", "i", "]", "=", "box_filter", "(", "batch_y", "[", "i", "]", ")", "\n", "if", "(", "batch_y", "[", "i", "]", ".", "size", "==", "0", ")", "and", "not", "keep_images_without_gt", ":", "\n", "                                ", "batch_items_to_remove", ".", "append", "(", "i", ")", "\n", "\n", "#########################################################################################", "\n", "# Remove any items we might not want to keep from the batch.", "\n", "#########################################################################################", "\n", "\n", "", "", "", "", "", "if", "batch_items_to_remove", ":", "\n", "                ", "for", "j", "in", "sorted", "(", "batch_items_to_remove", ",", "reverse", "=", "True", ")", ":", "\n", "# This isn't efficient, but it hopefully shouldn't need to be done often anyway.", "\n", "                    ", "batch_X", ".", "pop", "(", "j", ")", "\n", "batch_filenames", ".", "pop", "(", "j", ")", "\n", "if", "batch_inverse_transforms", ":", "batch_inverse_transforms", ".", "pop", "(", "j", ")", "\n", "if", "not", "(", "self", ".", "labels", "is", "None", ")", ":", "batch_y", ".", "pop", "(", "j", ")", "\n", "if", "not", "(", "self", ".", "image_ids", "is", "None", ")", ":", "batch_image_ids", ".", "pop", "(", "j", ")", "\n", "if", "not", "(", "self", ".", "eval_neutral", "is", "None", ")", ":", "batch_eval_neutral", ".", "pop", "(", "j", ")", "\n", "if", "'original_images'", "in", "returns", ":", "batch_original_images", ".", "pop", "(", "j", ")", "\n", "if", "'original_labels'", "in", "returns", "and", "not", "(", "self", ".", "labels", "is", "None", ")", ":", "batch_original_labels", ".", "pop", "(", "j", ")", "\n", "\n", "#########################################################################################", "\n", "\n", "# CAUTION: Converting `batch_X` into an array will result in an empty batch if the images have varying sizes", "\n", "#          or varying numbers of channels. At this point, all images must have the same size and the same", "\n", "#          number of channels.", "\n", "", "", "batch_X", "=", "np", ".", "array", "(", "batch_X", ")", "\n", "if", "(", "batch_X", ".", "size", "==", "0", ")", ":", "\n", "                ", "raise", "DegenerateBatchError", "(", "\"You produced an empty batch. This might be because the images in the batch vary \"", "+", "\n", "\"in their size and/or number of channels. Note that after all transformations \"", "+", "\n", "\"(if any were given) have been applied to all images in the batch, all images \"", "+", "\n", "\"must be homogenous in size along all axes.\"", ")", "\n", "\n", "#########################################################################################", "\n", "# If we have a label encoder, encode our labels.", "\n", "#########################################################################################", "\n", "\n", "# batch_y_sample_weights = []", "\n", "# batch_y_new = []", "\n", "# for i in range(len(batch_y)):", "\n", "#     sep=batch_y[i]", "\n", "#     weights = sep[:, 5]", "\n", "#     batchy = sep[:, :5]", "\n", "#     batch_y_sample_weights.append(weights)", "\n", "#     batch_y_new.append(batchy)", "\n", "", "if", "not", "(", "label_encoder", "is", "None", "or", "self", ".", "labels", "is", "None", ")", ":", "\n", "\n", "                ", "if", "(", "'matched_anchors'", "in", "returns", ")", "and", "isinstance", "(", "label_encoder", ",", "SSDInputEncoder", ")", ":", "\n", "                    ", "batch_y_encoded", ",", "batch_matched_anchors", "=", "label_encoder", "(", "batch_y", ",", "diagnostics", "=", "True", ")", "\n", "", "else", ":", "\n", "                    ", "batch_y_encoded", "=", "label_encoder", "(", "batch_y", ",", "diagnostics", "=", "False", ")", "\n", "batch_matched_anchors", "=", "None", "\n", "", "", "else", ":", "\n", "                ", "batch_y_encoded", "=", "None", "\n", "batch_matched_anchors", "=", "None", "\n", "\n", "#########################################################################################", "\n", "# Compose the output.", "\n", "#########################################################################################", "\n", "\n", "", "ret", "=", "[", "]", "\n", "if", "'processed_images'", "in", "returns", ":", "ret", ".", "append", "(", "batch_X", ")", "\n", "if", "'encoded_labels'", "in", "returns", ":", "ret", ".", "append", "(", "batch_y_encoded", ")", "\n", "if", "'matched_anchors'", "in", "returns", ":", "ret", ".", "append", "(", "batch_matched_anchors", ")", "\n", "if", "'processed_labels'", "in", "returns", ":", "ret", ".", "append", "(", "batch_y", ")", "\n", "if", "'filenames'", "in", "returns", ":", "ret", ".", "append", "(", "batch_filenames", ")", "\n", "if", "'image_ids'", "in", "returns", ":", "ret", ".", "append", "(", "batch_image_ids", ")", "\n", "if", "'evaluation-neutral'", "in", "returns", ":", "ret", ".", "append", "(", "batch_eval_neutral", ")", "\n", "if", "'inverse_transform'", "in", "returns", ":", "ret", ".", "append", "(", "batch_inverse_transforms", ")", "\n", "if", "'original_images'", "in", "returns", ":", "ret", ".", "append", "(", "batch_original_images", ")", "\n", "if", "'original_labels'", "in", "returns", ":", "ret", ".", "append", "(", "batch_original_labels", ")", "\n", "\n", "yield", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_data_generator.DataGenerator.save_dataset": [[1207, 1237], ["open", "pickle.dump", "open", "pickle.dump", "open", "pickle.dump", "open", "pickle.dump"], "methods", ["None"], ["", "", "def", "save_dataset", "(", "self", ",", "\n", "filenames_path", "=", "'filenames.pkl'", ",", "\n", "labels_path", "=", "None", ",", "\n", "image_ids_path", "=", "None", ",", "\n", "eval_neutral_path", "=", "None", ")", ":", "\n", "        ", "'''\n        Writes the current `filenames`, `labels`, and `image_ids` lists to the specified files.\n        This is particularly useful for large datasets with annotations that are\n        parsed from XML files, which can take quite long. If you'll be using the\n        same dataset repeatedly, you don't want to have to parse the XML label\n        files every time.\n\n        Arguments:\n            filenames_path (str): The path under which to save the filenames pickle.\n            labels_path (str): The path under which to save the labels pickle.\n            image_ids_path (str, optional): The path under which to save the image IDs pickle.\n            eval_neutral_path (str, optional): The path under which to save the pickle for\n                the evaluation-neutrality annotations.\n        '''", "\n", "with", "open", "(", "filenames_path", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "self", ".", "filenames", ",", "f", ")", "\n", "", "if", "not", "labels_path", "is", "None", ":", "\n", "            ", "with", "open", "(", "labels_path", ",", "'wb'", ")", "as", "f", ":", "\n", "                ", "pickle", ".", "dump", "(", "self", ".", "labels", ",", "f", ")", "\n", "", "", "if", "not", "image_ids_path", "is", "None", ":", "\n", "            ", "with", "open", "(", "image_ids_path", ",", "'wb'", ")", "as", "f", ":", "\n", "                ", "pickle", ".", "dump", "(", "self", ".", "image_ids", ",", "f", ")", "\n", "", "", "if", "not", "eval_neutral_path", "is", "None", ":", "\n", "            ", "with", "open", "(", "eval_neutral_path", ",", "'wb'", ")", "as", "f", ":", "\n", "                ", "pickle", ".", "dump", "(", "self", ".", "eval_neutral", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_data_generator.DataGenerator.get_dataset": [[1238, 1245], ["None"], "methods", ["None"], ["", "", "", "def", "get_dataset", "(", "self", ")", ":", "\n", "        ", "'''\n        Returns:\n            4-tuple containing lists and/or `None` for the filenames, labels, image IDs,\n            and evaluation-neutrality annotations.\n        '''", "\n", "return", "self", ".", "filenames", ",", "self", ".", "labels", ",", "self", ".", "image_ids", ",", "self", ".", "eval_neutral", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_data_generator.DataGenerator.get_dataset_size": [[1246, 1252], ["None"], "methods", ["None"], ["", "def", "get_dataset_size", "(", "self", ")", ":", "\n", "        ", "'''\n        Returns:\n            The number of images in the dataset.\n        '''", "\n", "return", "self", ".", "dataset_size", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.data_augmentation_chain_original_ssd.SSDRandomCrop.__init__": [[37, 98], ["data_generator.object_detection_2d_image_boxes_validation_utils.BoundGenerator", "data_generator.object_detection_2d_patch_sampling_ops.PatchCoordinateGenerator", "data_generator.object_detection_2d_image_boxes_validation_utils.BoxFilter", "data_generator.object_detection_2d_image_boxes_validation_utils.ImageValidator", "data_generator.object_detection_2d_patch_sampling_ops.RandomPatchInf"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ")", ":", "\n", "        ", "'''\n        Arguments:\n            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n        '''", "\n", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "\n", "# This randomly samples one of the lower IoU bounds defined", "\n", "# by the `sample_space` every time it is called.", "\n", "self", ".", "bound_generator", "=", "BoundGenerator", "(", "sample_space", "=", "(", "(", "None", ",", "None", ")", ",", "\n", "(", "0.1", ",", "None", ")", ",", "\n", "(", "0.3", ",", "None", ")", ",", "\n", "(", "0.5", ",", "None", ")", ",", "\n", "(", "0.7", ",", "None", ")", ",", "\n", "(", "0.9", ",", "None", ")", ")", ",", "\n", "weights", "=", "None", ")", "\n", "\n", "# Produces coordinates for candidate patches such that the height", "\n", "# and width of the patches are between 0.3 and 1.0 of the height", "\n", "# and width of the respective image and the aspect ratio of the", "\n", "# patches is between 0.5 and 2.0.", "\n", "self", ".", "patch_coord_generator", "=", "PatchCoordinateGenerator", "(", "must_match", "=", "'h_w'", ",", "\n", "min_scale", "=", "0.3", ",", "\n", "max_scale", "=", "1.0", ",", "\n", "scale_uniformly", "=", "False", ",", "\n", "min_aspect_ratio", "=", "0.5", ",", "\n", "max_aspect_ratio", "=", "2.0", ")", "\n", "\n", "# Filters out boxes whose center point does not lie within the", "\n", "# chosen patches.", "\n", "self", ".", "box_filter", "=", "BoxFilter", "(", "check_overlap", "=", "True", ",", "\n", "check_min_area", "=", "False", ",", "\n", "check_degenerate", "=", "False", ",", "\n", "overlap_criterion", "=", "'center_point'", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n", "# Determines whether a given patch is considered a valid patch.", "\n", "# Defines a patch to be valid if at least one ground truth bounding box", "\n", "# (n_boxes_min == 1) has an IoU overlap with the patch that", "\n", "# meets the requirements defined by `bound_generator`.", "\n", "self", ".", "image_validator", "=", "ImageValidator", "(", "overlap_criterion", "=", "'iou'", ",", "\n", "n_boxes_min", "=", "1", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ",", "\n", "border_pixels", "=", "'half'", ")", "\n", "\n", "# Performs crops according to the parameters set in the objects above.", "\n", "# Runs until either a valid patch is found or the original input image", "\n", "# is returned unaltered. Runs a maximum of 50 trials to find a valid", "\n", "# patch for each new sampled IoU threshold. Every 50 trials, the original", "\n", "# image is returned as is with probability (1 - prob) = 0.143.", "\n", "self", ".", "random_crop", "=", "RandomPatchInf", "(", "patch_coord_generator", "=", "self", ".", "patch_coord_generator", ",", "\n", "box_filter", "=", "self", ".", "box_filter", ",", "\n", "image_validator", "=", "self", ".", "image_validator", ",", "\n", "bound_generator", "=", "self", ".", "bound_generator", ",", "\n", "n_trials_max", "=", "50", ",", "\n", "clip_boxes", "=", "True", ",", "\n", "prob", "=", "0.857", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.data_augmentation_chain_original_ssd.SSDRandomCrop.__call__": [[99, 102], ["data_augmentation_chain_original_ssd.SSDRandomCrop.random_crop"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ",", "return_inverter", "=", "False", ")", ":", "\n", "        ", "self", ".", "random_crop", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "return", "self", ".", "random_crop", "(", "image", ",", "labels", ",", "return_inverter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.data_augmentation_chain_original_ssd.SSDExpand.__init__": [[111, 141], ["data_generator.object_detection_2d_patch_sampling_ops.PatchCoordinateGenerator", "data_generator.object_detection_2d_patch_sampling_ops.RandomPatch"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "background", "=", "(", "123", ",", "117", ",", "104", ")", ",", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ")", ":", "\n", "        ", "'''\n        Arguments:\n            background (list/tuple, optional): A 3-tuple specifying the RGB color value of the\n                background pixels of the translated images.\n            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n        '''", "\n", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "\n", "# Generate coordinates for patches that are between 1.0 and 4.0 times", "\n", "# the size of the input image in both spatial dimensions.", "\n", "self", ".", "patch_coord_generator", "=", "PatchCoordinateGenerator", "(", "must_match", "=", "'h_w'", ",", "\n", "min_scale", "=", "1.0", ",", "\n", "max_scale", "=", "4.0", ",", "\n", "scale_uniformly", "=", "True", ")", "\n", "\n", "# With probability 0.5, place the input image randomly on a canvas filled with", "\n", "# mean color values according to the parameters set above. With probability 0.5,", "\n", "# return the input image unaltered.", "\n", "self", ".", "expand", "=", "RandomPatch", "(", "patch_coord_generator", "=", "self", ".", "patch_coord_generator", ",", "\n", "box_filter", "=", "None", ",", "\n", "image_validator", "=", "None", ",", "\n", "n_trials_max", "=", "1", ",", "\n", "clip_boxes", "=", "False", ",", "\n", "prob", "=", "0.5", ",", "\n", "background", "=", "background", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.data_augmentation_chain_original_ssd.SSDExpand.__call__": [[142, 145], ["data_augmentation_chain_original_ssd.SSDExpand.expand"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", "=", "None", ",", "return_inverter", "=", "False", ")", ":", "\n", "        ", "self", ".", "expand", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "return", "self", ".", "expand", "(", "image", ",", "labels", ",", "return_inverter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.data_augmentation_chain_original_ssd.SSDPhotometricDistortions.__init__": [[152, 192], ["data_generator.object_detection_2d_photometric_ops.ConvertColor", "data_generator.object_detection_2d_photometric_ops.ConvertColor", "data_generator.object_detection_2d_photometric_ops.ConvertDataType", "data_generator.object_detection_2d_photometric_ops.ConvertDataType", "data_generator.object_detection_2d_photometric_ops.ConvertTo3Channels", "data_generator.object_detection_2d_photometric_ops.RandomBrightness", "data_generator.object_detection_2d_photometric_ops.RandomContrast", "data_generator.object_detection_2d_photometric_ops.RandomSaturation", "data_generator.object_detection_2d_photometric_ops.RandomHue", "data_generator.object_detection_2d_photometric_ops.RandomChannelSwap"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "\n", "        ", "self", ".", "convert_RGB_to_HSV", "=", "ConvertColor", "(", "current", "=", "'RGB'", ",", "to", "=", "'HSV'", ")", "\n", "self", ".", "convert_HSV_to_RGB", "=", "ConvertColor", "(", "current", "=", "'HSV'", ",", "to", "=", "'RGB'", ")", "\n", "self", ".", "convert_to_float32", "=", "ConvertDataType", "(", "to", "=", "'float32'", ")", "\n", "self", ".", "convert_to_uint8", "=", "ConvertDataType", "(", "to", "=", "'uint8'", ")", "\n", "self", ".", "convert_to_3_channels", "=", "ConvertTo3Channels", "(", ")", "\n", "self", ".", "random_brightness", "=", "RandomBrightness", "(", "lower", "=", "-", "32", ",", "upper", "=", "32", ",", "prob", "=", "0.5", ")", "\n", "self", ".", "random_contrast", "=", "RandomContrast", "(", "lower", "=", "0.5", ",", "upper", "=", "1.5", ",", "prob", "=", "0.5", ")", "\n", "self", ".", "random_saturation", "=", "RandomSaturation", "(", "lower", "=", "0.5", ",", "upper", "=", "1.5", ",", "prob", "=", "0.5", ")", "\n", "self", ".", "random_hue", "=", "RandomHue", "(", "max_delta", "=", "18", ",", "prob", "=", "0.5", ")", "\n", "self", ".", "random_channel_swap", "=", "RandomChannelSwap", "(", "prob", "=", "0.0", ")", "\n", "\n", "self", ".", "sequence1", "=", "[", "self", ".", "convert_to_3_channels", ",", "\n", "self", ".", "convert_to_float32", ",", "\n", "self", ".", "random_brightness", ",", "\n", "self", ".", "random_contrast", ",", "\n", "self", ".", "convert_to_uint8", ",", "\n", "self", ".", "convert_RGB_to_HSV", ",", "\n", "self", ".", "convert_to_float32", ",", "\n", "self", ".", "random_saturation", ",", "\n", "self", ".", "random_hue", ",", "\n", "self", ".", "convert_to_uint8", ",", "\n", "self", ".", "convert_HSV_to_RGB", ",", "\n", "self", ".", "random_channel_swap", "]", "\n", "\n", "self", ".", "sequence2", "=", "[", "self", ".", "convert_to_3_channels", ",", "\n", "self", ".", "convert_to_float32", ",", "\n", "self", ".", "random_brightness", ",", "\n", "self", ".", "convert_to_uint8", ",", "\n", "self", ".", "convert_RGB_to_HSV", ",", "\n", "self", ".", "convert_to_float32", ",", "\n", "self", ".", "random_saturation", ",", "\n", "self", ".", "random_hue", ",", "\n", "self", ".", "convert_to_uint8", ",", "\n", "self", ".", "convert_HSV_to_RGB", ",", "\n", "self", ".", "convert_to_float32", ",", "\n", "self", ".", "random_contrast", ",", "\n", "self", ".", "convert_to_uint8", ",", "\n", "self", ".", "random_channel_swap", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.data_augmentation_chain_original_ssd.SSDPhotometricDistortions.__call__": [[193, 207], ["numpy.random.choice", "transform", "transform"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", ")", ":", "\n", "\n", "# Choose sequence 1 with probability 0.5.", "\n", "        ", "if", "np", ".", "random", ".", "choice", "(", "2", ")", ":", "\n", "\n", "            ", "for", "transform", "in", "self", ".", "sequence1", ":", "\n", "                ", "image", ",", "labels", "=", "transform", "(", "image", ",", "labels", ")", "\n", "", "return", "image", ",", "labels", "\n", "# Choose sequence 2 with probability 0.5.", "\n", "", "else", ":", "\n", "\n", "            ", "for", "transform", "in", "self", ".", "sequence2", ":", "\n", "                ", "image", ",", "labels", "=", "transform", "(", "image", ",", "labels", ")", "\n", "", "return", "image", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.data_augmentation_chain_original_ssd.SSDDataAugmentation.__init__": [[214, 261], ["data_augmentation_chain_original_ssd.SSDPhotometricDistortions", "data_augmentation_chain_original_ssd.SSDExpand", "data_augmentation_chain_original_ssd.SSDRandomCrop", "data_generator.object_detection_2d_geometric_ops.RandomFlip", "data_generator.object_detection_2d_image_boxes_validation_utils.BoxFilter", "data_generator.object_detection_2d_geometric_ops.ResizeRandomInterp"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "img_height", "=", "300", ",", "\n", "img_width", "=", "300", ",", "\n", "background", "=", "(", "123", ",", "117", ",", "104", ")", ",", "\n", "labels_format", "=", "{", "'class_id'", ":", "0", ",", "'xmin'", ":", "1", ",", "'ymin'", ":", "2", ",", "'xmax'", ":", "3", ",", "'ymax'", ":", "4", "}", ")", ":", "\n", "        ", "'''\n        Arguments:\n            height (int): The desired height of the output images in pixels.\n            width (int): The desired width of the output images in pixels.\n            background (list/tuple, optional): A 3-tuple specifying the RGB color value of the\n                background pixels of the translated images.\n            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n        '''", "\n", "\n", "self", ".", "labels_format", "=", "labels_format", "\n", "\n", "self", ".", "photometric_distortions", "=", "SSDPhotometricDistortions", "(", ")", "\n", "self", ".", "expand", "=", "SSDExpand", "(", "background", "=", "background", ",", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "self", ".", "random_crop", "=", "SSDRandomCrop", "(", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "self", ".", "random_flip", "=", "RandomFlip", "(", "dim", "=", "'horizontal'", ",", "prob", "=", "0.5", ",", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n", "# This box filter makes sure that the resized images don't contain any degenerate boxes.", "\n", "# Resizing the images could lead the boxes to becomes smaller. For boxes that are already", "\n", "# pretty small, that might result in boxes with height and/or width zero, which we obviously", "\n", "# cannot allow.", "\n", "self", ".", "box_filter", "=", "BoxFilter", "(", "check_overlap", "=", "False", ",", "\n", "check_min_area", "=", "False", ",", "\n", "check_degenerate", "=", "True", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n", "self", ".", "resize", "=", "ResizeRandomInterp", "(", "height", "=", "img_height", ",", "\n", "width", "=", "img_width", ",", "\n", "interpolation_modes", "=", "[", "cv2", ".", "INTER_NEAREST", ",", "\n", "cv2", ".", "INTER_LINEAR", ",", "\n", "cv2", ".", "INTER_CUBIC", ",", "\n", "cv2", ".", "INTER_AREA", ",", "\n", "cv2", ".", "INTER_LANCZOS4", "]", ",", "\n", "box_filter", "=", "self", ".", "box_filter", ",", "\n", "labels_format", "=", "self", ".", "labels_format", ")", "\n", "\n", "self", ".", "sequence", "=", "[", "self", ".", "photometric_distortions", ",", "\n", "self", ".", "expand", ",", "\n", "self", ".", "random_crop", ",", "\n", "self", ".", "random_flip", ",", "\n", "self", ".", "resize", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.data_augmentation_chain_original_ssd.SSDDataAugmentation.__call__": [[262, 282], ["transform", "inverters.append", "transform", "inspect.signature"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "image", ",", "labels", ",", "return_inverter", "=", "False", ")", ":", "\n", "        ", "self", ".", "expand", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "self", ".", "random_crop", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "self", ".", "random_flip", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "self", ".", "resize", ".", "labels_format", "=", "self", ".", "labels_format", "\n", "\n", "inverters", "=", "[", "]", "\n", "\n", "for", "transform", "in", "self", ".", "sequence", ":", "\n", "            ", "if", "return_inverter", "and", "(", "'return_inverter'", "in", "inspect", ".", "signature", "(", "transform", ")", ".", "parameters", ")", ":", "\n", "                ", "image", ",", "labels", ",", "inverter", "=", "transform", "(", "image", ",", "labels", ",", "return_inverter", "=", "True", ")", "\n", "inverters", ".", "append", "(", "inverter", ")", "\n", "", "else", ":", "\n", "                ", "image", ",", "labels", "=", "transform", "(", "image", ",", "labels", ")", "\n", "aaa", "=", "1", "\n", "\n", "", "", "if", "return_inverter", ":", "\n", "            ", "return", "image", ",", "labels", ",", "inverters", "[", ":", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "return", "image", ",", "labels", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.ssd_output_decoder.greedy_nms": [[27, 76], ["numpy.copy", "y_pred_decoded_nms.append", "numpy.argmax", "numpy.copy", "maxima.append", "numpy.delete", "bounding_box_utils.bounding_box_utils.iou", "numpy.array"], "function", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.iou"], ["def", "greedy_nms", "(", "y_pred_decoded", ",", "iou_threshold", "=", "0.45", ",", "coords", "=", "'corners'", ",", "border_pixels", "=", "'half'", ")", ":", "\n", "    ", "'''\n    Perform greedy non-maximum suppression on the input boxes.\n\n    Greedy NMS works by selecting the box with the highest score and\n    removing all boxes around it that are too close to it measured by IoU-similarity.\n    Out of the boxes that are left over, once again the one with the highest\n    score is selected and so on, until no boxes with too much overlap are left.\n\n    Arguments:\n        y_pred_decoded (list): A batch of decoded predictions. For a given batch size `n` this\n            is a list of length `n` where each list element is a 2D Numpy array.\n            For a batch item with `k` predicted boxes this 2D Numpy array has\n            shape `(k, 6)`, where each row contains the coordinates of the respective\n            box in the format `[class_id, score, xmin, xmax, ymin, ymax]`.\n            Technically, the number of columns doesn't have to be 6, it can be\n            arbitrary as long as the first four elements of each row are\n            `xmin`, `xmax`, `ymin`, `ymax` (in this order) and the last element\n            is the score assigned to the prediction. Note that this function is\n            agnostic to the scale of the score or what it represents.\n        iou_threshold (float, optional): All boxes with a Jaccard similarity of\n            greater than `iou_threshold` with a locally maximal box will be removed\n            from the set of predictions, where 'maximal' refers to the box score.\n        coords (str, optional): The coordinate format of `y_pred_decoded`.\n            Can be one of the formats supported by `iou()`.\n        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n            to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n            If 'half', then one of each of the two horizontal and vertical borders belong\n            to the boxex, but not the other.\n\n    Returns:\n        The predictions after removing non-maxima. The format is the same as the input format.\n    '''", "\n", "y_pred_decoded_nms", "=", "[", "]", "\n", "for", "batch_item", "in", "y_pred_decoded", ":", "# For the labels of each batch item...", "\n", "        ", "boxes_left", "=", "np", ".", "copy", "(", "batch_item", ")", "\n", "maxima", "=", "[", "]", "# This is where we store the boxes that make it through the non-maximum suppression", "\n", "while", "boxes_left", ".", "shape", "[", "0", "]", ">", "0", ":", "# While there are still boxes left to compare...", "\n", "            ", "maximum_index", "=", "np", ".", "argmax", "(", "boxes_left", "[", ":", ",", "1", "]", ")", "# ...get the index of the next box with the highest confidence...", "\n", "maximum_box", "=", "np", ".", "copy", "(", "boxes_left", "[", "maximum_index", "]", ")", "# ...copy that box and...", "\n", "maxima", ".", "append", "(", "maximum_box", ")", "# ...append it to `maxima` because we'll definitely keep it", "\n", "boxes_left", "=", "np", ".", "delete", "(", "boxes_left", ",", "maximum_index", ",", "axis", "=", "0", ")", "# Now remove the maximum box from `boxes_left`", "\n", "if", "boxes_left", ".", "shape", "[", "0", "]", "==", "0", ":", "break", "# If there are no boxes left after this step, break. Otherwise...", "\n", "similarities", "=", "iou", "(", "boxes_left", "[", ":", ",", "2", ":", "]", ",", "maximum_box", "[", "2", ":", "]", ",", "coords", "=", "coords", ",", "mode", "=", "'element-wise'", ",", "border_pixels", "=", "border_pixels", ")", "# ...compare (IoU) the other left over boxes to the maximum box...", "\n", "boxes_left", "=", "boxes_left", "[", "similarities", "<=", "iou_threshold", "]", "# ...so that we can remove the ones that overlap too much with the maximum box", "\n", "", "y_pred_decoded_nms", ".", "append", "(", "np", ".", "array", "(", "maxima", ")", ")", "\n", "\n", "", "return", "y_pred_decoded_nms", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.ssd_output_decoder._greedy_nms": [[77, 93], ["numpy.copy", "numpy.array", "numpy.argmax", "numpy.copy", "maxima.append", "numpy.delete", "bounding_box_utils.bounding_box_utils.iou"], "function", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.iou"], ["", "def", "_greedy_nms", "(", "predictions", ",", "iou_threshold", "=", "0.45", ",", "coords", "=", "'corners'", ",", "border_pixels", "=", "'half'", ")", ":", "\n", "    ", "'''\n    The same greedy non-maximum suppression algorithm as above, but slightly modified for use as an internal\n    function for per-class NMS in `decode_detections()`.\n    '''", "\n", "boxes_left", "=", "np", ".", "copy", "(", "predictions", ")", "\n", "maxima", "=", "[", "]", "# This is where we store the boxes that make it through the non-maximum suppression", "\n", "while", "boxes_left", ".", "shape", "[", "0", "]", ">", "0", ":", "# While there are still boxes left to compare...", "\n", "        ", "maximum_index", "=", "np", ".", "argmax", "(", "boxes_left", "[", ":", ",", "0", "]", ")", "# ...get the index of the next box with the highest confidence...", "\n", "maximum_box", "=", "np", ".", "copy", "(", "boxes_left", "[", "maximum_index", "]", ")", "# ...copy that box and...", "\n", "maxima", ".", "append", "(", "maximum_box", ")", "# ...append it to `maxima` because we'll definitely keep it", "\n", "boxes_left", "=", "np", ".", "delete", "(", "boxes_left", ",", "maximum_index", ",", "axis", "=", "0", ")", "# Now remove the maximum box from `boxes_left`", "\n", "if", "boxes_left", ".", "shape", "[", "0", "]", "==", "0", ":", "break", "# If there are no boxes left after this step, break. Otherwise...", "\n", "similarities", "=", "iou", "(", "boxes_left", "[", ":", ",", "1", ":", "]", ",", "maximum_box", "[", "1", ":", "]", ",", "coords", "=", "coords", ",", "mode", "=", "'element-wise'", ",", "border_pixels", "=", "border_pixels", ")", "# ...compare (IoU) the other left over boxes to the maximum box...", "\n", "boxes_left", "=", "boxes_left", "[", "similarities", "<=", "iou_threshold", "]", "# ...so that we can remove the ones that overlap too much with the maximum box", "\n", "", "return", "np", ".", "array", "(", "maxima", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.ssd_output_decoder._greedy_nms2": [[94, 110], ["numpy.copy", "numpy.array", "numpy.argmax", "numpy.copy", "maxima.append", "numpy.delete", "bounding_box_utils.bounding_box_utils.iou"], "function", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.iou"], ["", "def", "_greedy_nms2", "(", "predictions", ",", "iou_threshold", "=", "0.45", ",", "coords", "=", "'corners'", ",", "border_pixels", "=", "'half'", ")", ":", "\n", "    ", "'''\n    The same greedy non-maximum suppression algorithm as above, but slightly modified for use as an internal\n    function in `decode_detections_fast()`.\n    '''", "\n", "boxes_left", "=", "np", ".", "copy", "(", "predictions", ")", "\n", "maxima", "=", "[", "]", "# This is where we store the boxes that make it through the non-maximum suppression", "\n", "while", "boxes_left", ".", "shape", "[", "0", "]", ">", "0", ":", "# While there are still boxes left to compare...", "\n", "        ", "maximum_index", "=", "np", ".", "argmax", "(", "boxes_left", "[", ":", ",", "1", "]", ")", "# ...get the index of the next box with the highest confidence...", "\n", "maximum_box", "=", "np", ".", "copy", "(", "boxes_left", "[", "maximum_index", "]", ")", "# ...copy that box and...", "\n", "maxima", ".", "append", "(", "maximum_box", ")", "# ...append it to `maxima` because we'll definitely keep it", "\n", "boxes_left", "=", "np", ".", "delete", "(", "boxes_left", ",", "maximum_index", ",", "axis", "=", "0", ")", "# Now remove the maximum box from `boxes_left`", "\n", "if", "boxes_left", ".", "shape", "[", "0", "]", "==", "0", ":", "break", "# If there are no boxes left after this step, break. Otherwise...", "\n", "similarities", "=", "iou", "(", "boxes_left", "[", ":", ",", "2", ":", "]", ",", "maximum_box", "[", "2", ":", "]", ",", "coords", "=", "coords", ",", "mode", "=", "'element-wise'", ",", "border_pixels", "=", "border_pixels", ")", "# ...compare (IoU) the other left over boxes to the maximum box...", "\n", "boxes_left", "=", "boxes_left", "[", "similarities", "<=", "iou_threshold", "]", "# ...so that we can remove the ones that overlap too much with the maximum box", "\n", "", "return", "np", ".", "array", "(", "maxima", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.ssd_output_decoder.decode_detections": [[111, 227], ["numpy.copy", "ValueError", "numpy.exp", "bounding_box_utils.bounding_box_utils.convert_coordinates", "range", "y_pred_decoded.append", "numpy.expand_dims", "numpy.expand_dims", "bounding_box_utils.bounding_box_utils.convert_coordinates", "numpy.concatenate", "numpy.array", "numpy.expand_dims", "numpy.expand_dims", "ValueError", "ssd_output_decoder._greedy_nms", "numpy.zeros", "np.array.append", "numpy.argpartition"], "function", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.convert_coordinates", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.convert_coordinates", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.ssd_output_decoder._greedy_nms"], ["", "def", "decode_detections", "(", "y_pred", ",", "\n", "confidence_thresh", "=", "0.01", ",", "\n", "iou_threshold", "=", "0.45", ",", "\n", "top_k", "=", "200", ",", "\n", "input_coords", "=", "'centroids'", ",", "\n", "normalize_coords", "=", "True", ",", "\n", "img_height", "=", "None", ",", "\n", "img_width", "=", "None", ",", "\n", "border_pixels", "=", "'half'", ")", ":", "\n", "    ", "'''\n    Convert model prediction output back to a format that contains only the positive box predictions\n    (i.e. the same format that `SSDInputEncoder` takes as input).\n\n    After the decoding, two stages of prediction filtering are performed for each class individually:\n    First confidence thresholding, then greedy non-maximum suppression. The filtering results for all\n    classes are concatenated and the `top_k` overall highest confidence results constitute the final\n    predictions for a given batch item. This procedure follows the original Caffe implementation.\n    For a slightly different and more efficient alternative to decode raw model output that performs\n    non-maximum suppresion globally instead of per class, see `decode_detections_fast()` below.\n\n    Arguments:\n        y_pred (array): The prediction output of the SSD model, expected to be a Numpy array\n            of shape `(batch_size, #boxes, #classes + 4 + 4 + 4)`, where `#boxes` is the total number of\n            boxes predicted by the model per image and the last axis contains\n            `[one-hot vector for the classes, 4 predicted coordinate offsets, 4 anchor box coordinates, 4 variances]`.\n        confidence_thresh (float, optional): A float in [0,1), the minimum classification confidence in a specific\n            positive class in order to be considered for the non-maximum suppression stage for the respective class.\n            A lower value will result in a larger part of the selection process being done by the non-maximum suppression\n            stage, while a larger value will result in a larger part of the selection process happening in the confidence\n            thresholding stage.\n        iou_threshold (float, optional): A float in [0,1]. All boxes with a Jaccard similarity of greater than `iou_threshold`\n            with a locally maximal box will be removed from the set of predictions for a given class, where 'maximal' refers\n            to the box score.\n        top_k (int, optional): The number of highest scoring predictions to be kept for each batch item after the\n            non-maximum suppression stage.\n        input_coords (str, optional): The box coordinate format that the model outputs. Can be either 'centroids'\n            for the format `(cx, cy, w, h)` (box center coordinates, width, and height), 'minmax' for the format\n            `(xmin, xmax, ymin, ymax)`, or 'corners' for the format `(xmin, ymin, xmax, ymax)`.\n        normalize_coords (bool, optional): Set to `True` if the model outputs relative coordinates (i.e. coordinates in [0,1])\n            and you wish to transform these relative coordinates back to absolute coordinates. If the model outputs\n            relative coordinates, but you do not want to convert them back to absolute coordinates, set this to `False`.\n            Do not set this to `True` if the model already outputs absolute coordinates, as that would result in incorrect\n            coordinates. Requires `img_height` and `img_width` if set to `True`.\n        img_height (int, optional): The height of the input images. Only needed if `normalize_coords` is `True`.\n        img_width (int, optional): The width of the input images. Only needed if `normalize_coords` is `True`.\n        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n            to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n            If 'half', then one of each of the two horizontal and vertical borders belong\n            to the boxex, but not the other.\n\n    Returns:\n        A python list of length `batch_size` where each list element represents the predicted boxes\n        for one image and contains a Numpy array of shape `(boxes, 6)` where each row is a box prediction for\n        a non-background class for the respective image in the format `[class_id, confidence, xmin, ymin, xmax, ymax]`.\n    '''", "\n", "if", "normalize_coords", "and", "(", "(", "img_height", "is", "None", ")", "or", "(", "img_width", "is", "None", ")", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"If relative box coordinates are supposed to be converted to absolute coordinates, the decoder needs the image size in order to decode the predictions, but `img_height == {}` and `img_width == {}`\"", ".", "format", "(", "img_height", ",", "img_width", ")", ")", "\n", "\n", "# 1: Convert the box coordinates from the predicted anchor box offsets to predicted absolute coordinates", "\n", "\n", "", "y_pred_decoded_raw", "=", "np", ".", "copy", "(", "y_pred", "[", ":", ",", ":", ",", ":", "-", "8", "]", ")", "# Slice out the classes and the four offsets, throw away the anchor coordinates and variances, resulting in a tensor of shape `[batch, n_boxes, n_classes + 4 coordinates]`", "\n", "\n", "if", "input_coords", "==", "'centroids'", ":", "\n", "        ", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "2", ",", "-", "1", "]", "]", "=", "np", ".", "exp", "(", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "2", ",", "-", "1", "]", "]", "*", "y_pred", "[", ":", ",", ":", ",", "[", "-", "2", ",", "-", "1", "]", "]", ")", "# exp(ln(w(pred)/w(anchor)) / w_variance * w_variance) == w(pred) / w(anchor), exp(ln(h(pred)/h(anchor)) / h_variance * h_variance) == h(pred) / h(anchor)", "\n", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "2", ",", "-", "1", "]", "]", "*=", "y_pred", "[", ":", ",", ":", ",", "[", "-", "6", ",", "-", "5", "]", "]", "# (w(pred) / w(anchor)) * w(anchor) == w(pred), (h(pred) / h(anchor)) * h(anchor) == h(pred)", "\n", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "4", ",", "-", "3", "]", "]", "*=", "y_pred", "[", ":", ",", ":", ",", "[", "-", "4", ",", "-", "3", "]", "]", "*", "y_pred", "[", ":", ",", ":", ",", "[", "-", "6", ",", "-", "5", "]", "]", "# (delta_cx(pred) / w(anchor) / cx_variance) * cx_variance * w(anchor) == delta_cx(pred), (delta_cy(pred) / h(anchor) / cy_variance) * cy_variance * h(anchor) == delta_cy(pred)", "\n", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "4", ",", "-", "3", "]", "]", "+=", "y_pred", "[", ":", ",", ":", ",", "[", "-", "8", ",", "-", "7", "]", "]", "# delta_cx(pred) + cx(anchor) == cx(pred), delta_cy(pred) + cy(anchor) == cy(pred)", "\n", "y_pred_decoded_raw", "=", "convert_coordinates", "(", "y_pred_decoded_raw", ",", "start_index", "=", "-", "4", ",", "conversion", "=", "'centroids2corners'", ")", "\n", "", "elif", "input_coords", "==", "'minmax'", ":", "\n", "        ", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "-", "4", ":", "]", "*=", "y_pred", "[", ":", ",", ":", ",", "-", "4", ":", "]", "# delta(pred) / size(anchor) / variance * variance == delta(pred) / size(anchor) for all four coordinates, where 'size' refers to w or h, respectively", "\n", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "4", ",", "-", "3", "]", "]", "*=", "np", ".", "expand_dims", "(", "y_pred", "[", ":", ",", ":", ",", "-", "7", "]", "-", "y_pred", "[", ":", ",", ":", ",", "-", "8", "]", ",", "axis", "=", "-", "1", ")", "# delta_xmin(pred) / w(anchor) * w(anchor) == delta_xmin(pred), delta_xmax(pred) / w(anchor) * w(anchor) == delta_xmax(pred)", "\n", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "2", ",", "-", "1", "]", "]", "*=", "np", ".", "expand_dims", "(", "y_pred", "[", ":", ",", ":", ",", "-", "5", "]", "-", "y_pred", "[", ":", ",", ":", ",", "-", "6", "]", ",", "axis", "=", "-", "1", ")", "# delta_ymin(pred) / h(anchor) * h(anchor) == delta_ymin(pred), delta_ymax(pred) / h(anchor) * h(anchor) == delta_ymax(pred)", "\n", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "-", "4", ":", "]", "+=", "y_pred", "[", ":", ",", ":", ",", "-", "8", ":", "-", "4", "]", "# delta(pred) + anchor == pred for all four coordinates", "\n", "y_pred_decoded_raw", "=", "convert_coordinates", "(", "y_pred_decoded_raw", ",", "start_index", "=", "-", "4", ",", "conversion", "=", "'minmax2corners'", ")", "\n", "", "elif", "input_coords", "==", "'corners'", ":", "\n", "        ", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "-", "4", ":", "]", "*=", "y_pred", "[", ":", ",", ":", ",", "-", "4", ":", "]", "# delta(pred) / size(anchor) / variance * variance == delta(pred) / size(anchor) for all four coordinates, where 'size' refers to w or h, respectively", "\n", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "4", ",", "-", "2", "]", "]", "*=", "np", ".", "expand_dims", "(", "y_pred", "[", ":", ",", ":", ",", "-", "6", "]", "-", "y_pred", "[", ":", ",", ":", ",", "-", "8", "]", ",", "axis", "=", "-", "1", ")", "# delta_xmin(pred) / w(anchor) * w(anchor) == delta_xmin(pred), delta_xmax(pred) / w(anchor) * w(anchor) == delta_xmax(pred)", "\n", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "3", ",", "-", "1", "]", "]", "*=", "np", ".", "expand_dims", "(", "y_pred", "[", ":", ",", ":", ",", "-", "5", "]", "-", "y_pred", "[", ":", ",", ":", ",", "-", "7", "]", ",", "axis", "=", "-", "1", ")", "# delta_ymin(pred) / h(anchor) * h(anchor) == delta_ymin(pred), delta_ymax(pred) / h(anchor) * h(anchor) == delta_ymax(pred)", "\n", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "-", "4", ":", "]", "+=", "y_pred", "[", ":", ",", ":", ",", "-", "8", ":", "-", "4", "]", "# delta(pred) + anchor == pred for all four coordinates", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unexpected value for `input_coords`. Supported input coordinate formats are 'minmax', 'corners' and 'centroids'.\"", ")", "\n", "\n", "# 2: If the model predicts normalized box coordinates and they are supposed to be converted back to absolute coordinates, do that", "\n", "\n", "", "if", "normalize_coords", ":", "\n", "        ", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "4", ",", "-", "2", "]", "]", "*=", "img_width", "# Convert xmin, xmax back to absolute coordinates", "\n", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "3", ",", "-", "1", "]", "]", "*=", "img_height", "# Convert ymin, ymax back to absolute coordinates", "\n", "\n", "# 3: Apply confidence thresholding and non-maximum suppression per class", "\n", "\n", "", "n_classes", "=", "y_pred_decoded_raw", ".", "shape", "[", "-", "1", "]", "-", "4", "# The number of classes is the length of the last axis minus the four box coordinates", "\n", "\n", "y_pred_decoded", "=", "[", "]", "# Store the final predictions in this list", "\n", "for", "batch_item", "in", "y_pred_decoded_raw", ":", "# `batch_item` has shape `[n_boxes, n_classes + 4 coords]`", "\n", "        ", "pred", "=", "[", "]", "# Store the final predictions for this batch item here", "\n", "for", "class_id", "in", "range", "(", "1", ",", "n_classes", ")", ":", "# For each class except the background class (which has class ID 0)...", "\n", "            ", "single_class", "=", "batch_item", "[", ":", ",", "[", "class_id", ",", "-", "4", ",", "-", "3", ",", "-", "2", ",", "-", "1", "]", "]", "# ...keep only the confidences for that class, making this an array of shape `[n_boxes, 5]` and...", "\n", "threshold_met", "=", "single_class", "[", "single_class", "[", ":", ",", "0", "]", ">", "confidence_thresh", "]", "# ...keep only those boxes with a confidence above the set threshold.", "\n", "if", "threshold_met", ".", "shape", "[", "0", "]", ">", "0", ":", "# If any boxes made the threshold...", "\n", "                ", "maxima", "=", "_greedy_nms", "(", "threshold_met", ",", "iou_threshold", "=", "iou_threshold", ",", "coords", "=", "'corners'", ",", "border_pixels", "=", "border_pixels", ")", "# ...perform NMS on them.", "\n", "maxima_output", "=", "np", ".", "zeros", "(", "(", "maxima", ".", "shape", "[", "0", "]", ",", "maxima", ".", "shape", "[", "1", "]", "+", "1", ")", ")", "# Expand the last dimension by one element to have room for the class ID. This is now an arrray of shape `[n_boxes, 6]`", "\n", "maxima_output", "[", ":", ",", "0", "]", "=", "class_id", "# Write the class ID to the first column...", "\n", "maxima_output", "[", ":", ",", "1", ":", "]", "=", "maxima", "# ...and write the maxima to the other columns...", "\n", "pred", ".", "append", "(", "maxima_output", ")", "# ...and append the maxima for this class to the list of maxima for this batch item.", "\n", "# Once we're through with all classes, keep only the `top_k` maxima with the highest scores", "\n", "", "", "if", "pred", ":", "# If there are any predictions left after confidence-thresholding...", "\n", "            ", "pred", "=", "np", ".", "concatenate", "(", "pred", ",", "axis", "=", "0", ")", "\n", "if", "top_k", "!=", "'all'", "and", "pred", ".", "shape", "[", "0", "]", ">", "top_k", ":", "# If we have more than `top_k` results left at this point, otherwise there is nothing to filter,...", "\n", "                ", "top_k_indices", "=", "np", ".", "argpartition", "(", "pred", "[", ":", ",", "1", "]", ",", "kth", "=", "pred", ".", "shape", "[", "0", "]", "-", "top_k", ",", "axis", "=", "0", ")", "[", "pred", ".", "shape", "[", "0", "]", "-", "top_k", ":", "]", "# ...get the indices of the `top_k` highest-score maxima...", "\n", "pred", "=", "pred", "[", "top_k_indices", "]", "# ...and keep only those entries of `pred`...", "\n", "", "", "else", ":", "\n", "            ", "pred", "=", "np", ".", "array", "(", "pred", ")", "# Even if empty, `pred` must become a Numpy array.", "\n", "", "y_pred_decoded", ".", "append", "(", "pred", ")", "# ...and now that we're done, append the array of final predictions for this batch item to the output list", "\n", "\n", "", "return", "y_pred_decoded", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.ssd_output_decoder.decode_detections_fast": [[228, 334], ["numpy.copy", "numpy.argmax", "numpy.amax", "ValueError", "numpy.exp", "bounding_box_utils.bounding_box_utils.convert_coordinates", "y_pred_decoded.append", "numpy.expand_dims", "numpy.expand_dims", "bounding_box_utils.bounding_box_utils.convert_coordinates", "ssd_output_decoder._greedy_nms2", "numpy.expand_dims", "numpy.expand_dims", "ValueError", "numpy.nonzero", "numpy.argpartition"], "function", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.convert_coordinates", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.convert_coordinates", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.ssd_output_decoder._greedy_nms2"], ["", "def", "decode_detections_fast", "(", "y_pred", ",", "\n", "confidence_thresh", "=", "0.5", ",", "\n", "iou_threshold", "=", "0.45", ",", "\n", "top_k", "=", "'all'", ",", "\n", "input_coords", "=", "'centroids'", ",", "\n", "normalize_coords", "=", "True", ",", "\n", "img_height", "=", "None", ",", "\n", "img_width", "=", "None", ",", "\n", "border_pixels", "=", "'half'", ")", ":", "\n", "    ", "'''\n    Convert model prediction output back to a format that contains only the positive box predictions\n    (i.e. the same format that `enconde_y()` takes as input).\n\n    Optionally performs confidence thresholding and greedy non-maximum suppression after the decoding stage.\n\n    Note that the decoding procedure used here is not the same as the procedure used in the original Caffe implementation.\n    For each box, the procedure used here assigns the box's highest confidence as its predicted class. Then it removes\n    all boxes for which the highest confidence is the background class. This results in less work for the subsequent\n    non-maximum suppression, because the vast majority of the predictions will be filtered out just by the fact that\n    their highest confidence is for the background class. It is much more efficient than the procedure of the original\n    implementation, but the results may also differ.\n\n    Arguments:\n        y_pred (array): The prediction output of the SSD model, expected to be a Numpy array\n            of shape `(batch_size, #boxes, #classes + 4 + 4 + 4)`, where `#boxes` is the total number of\n            boxes predicted by the model per image and the last axis contains\n            `[one-hot vector for the classes, 4 predicted coordinate offsets, 4 anchor box coordinates, 4 variances]`.\n        confidence_thresh (float, optional): A float in [0,1), the minimum classification confidence in any positive\n            class required for a given box to be considered a positive prediction. A lower value will result\n            in better recall, while a higher value will result in better precision. Do not use this parameter with the\n            goal to combat the inevitably many duplicates that an SSD will produce, the subsequent non-maximum suppression\n            stage will take care of those.\n        iou_threshold (float, optional): `None` or a float in [0,1]. If `None`, no non-maximum suppression will be\n            performed. If not `None`, greedy NMS will be performed after the confidence thresholding stage, meaning\n            all boxes with a Jaccard similarity of greater than `iou_threshold` with a locally maximal box will be removed\n            from the set of predictions, where 'maximal' refers to the box score.\n        top_k (int, optional): 'all' or an integer with number of highest scoring predictions to be kept for each batch item\n            after the non-maximum suppression stage. If 'all', all predictions left after the NMS stage will be kept.\n        input_coords (str, optional): The box coordinate format that the model outputs. Can be either 'centroids'\n            for the format `(cx, cy, w, h)` (box center coordinates, width, and height), 'minmax' for the format\n            `(xmin, xmax, ymin, ymax)`, or 'corners' for the format `(xmin, ymin, xmax, ymax)`.\n        normalize_coords (bool, optional): Set to `True` if the model outputs relative coordinates (i.e. coordinates in [0,1])\n            and you wish to transform these relative coordinates back to absolute coordinates. If the model outputs\n            relative coordinates, but you do not want to convert them back to absolute coordinates, set this to `False`.\n            Do not set this to `True` if the model already outputs absolute coordinates, as that would result in incorrect\n            coordinates. Requires `img_height` and `img_width` if set to `True`.\n        img_height (int, optional): The height of the input images. Only needed if `normalize_coords` is `True`.\n        img_width (int, optional): The width of the input images. Only needed if `normalize_coords` is `True`.\n        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n            to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n            If 'half', then one of each of the two horizontal and vertical borders belong\n            to the boxex, but not the other.\n\n    Returns:\n        A python list of length `batch_size` where each list element represents the predicted boxes\n        for one image and contains a Numpy array of shape `(boxes, 6)` where each row is a box prediction for\n        a non-background class for the respective image in the format `[class_id, confidence, xmin, xmax, ymin, ymax]`.\n    '''", "\n", "if", "normalize_coords", "and", "(", "(", "img_height", "is", "None", ")", "or", "(", "img_width", "is", "None", ")", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"If relative box coordinates are supposed to be converted to absolute coordinates, the decoder needs the image size in order to decode the predictions, but `img_height == {}` and `img_width == {}`\"", ".", "format", "(", "img_height", ",", "img_width", ")", ")", "\n", "\n", "# 1: Convert the classes from one-hot encoding to their class ID", "\n", "", "y_pred_converted", "=", "np", ".", "copy", "(", "y_pred", "[", ":", ",", ":", ",", "-", "14", ":", "-", "8", "]", ")", "# Slice out the four offset predictions plus two elements whereto we'll write the class IDs and confidences in the next step", "\n", "y_pred_converted", "[", ":", ",", ":", ",", "0", "]", "=", "np", ".", "argmax", "(", "y_pred", "[", ":", ",", ":", ",", ":", "-", "12", "]", ",", "axis", "=", "-", "1", ")", "# The indices of the highest confidence values in the one-hot class vectors are the class ID", "\n", "y_pred_converted", "[", ":", ",", ":", ",", "1", "]", "=", "np", ".", "amax", "(", "y_pred", "[", ":", ",", ":", ",", ":", "-", "12", "]", ",", "axis", "=", "-", "1", ")", "# Store the confidence values themselves, too", "\n", "\n", "# 2: Convert the box coordinates from the predicted anchor box offsets to predicted absolute coordinates", "\n", "if", "input_coords", "==", "'centroids'", ":", "\n", "        ", "y_pred_converted", "[", ":", ",", ":", ",", "[", "4", ",", "5", "]", "]", "=", "np", ".", "exp", "(", "y_pred_converted", "[", ":", ",", ":", ",", "[", "4", ",", "5", "]", "]", "*", "y_pred", "[", ":", ",", ":", ",", "[", "-", "2", ",", "-", "1", "]", "]", ")", "# exp(ln(w(pred)/w(anchor)) / w_variance * w_variance) == w(pred) / w(anchor), exp(ln(h(pred)/h(anchor)) / h_variance * h_variance) == h(pred) / h(anchor)", "\n", "y_pred_converted", "[", ":", ",", ":", ",", "[", "4", ",", "5", "]", "]", "*=", "y_pred", "[", ":", ",", ":", ",", "[", "-", "6", ",", "-", "5", "]", "]", "# (w(pred) / w(anchor)) * w(anchor) == w(pred), (h(pred) / h(anchor)) * h(anchor) == h(pred)", "\n", "y_pred_converted", "[", ":", ",", ":", ",", "[", "2", ",", "3", "]", "]", "*=", "y_pred", "[", ":", ",", ":", ",", "[", "-", "4", ",", "-", "3", "]", "]", "*", "y_pred", "[", ":", ",", ":", ",", "[", "-", "6", ",", "-", "5", "]", "]", "# (delta_cx(pred) / w(anchor) / cx_variance) * cx_variance * w(anchor) == delta_cx(pred), (delta_cy(pred) / h(anchor) / cy_variance) * cy_variance * h(anchor) == delta_cy(pred)", "\n", "y_pred_converted", "[", ":", ",", ":", ",", "[", "2", ",", "3", "]", "]", "+=", "y_pred", "[", ":", ",", ":", ",", "[", "-", "8", ",", "-", "7", "]", "]", "# delta_cx(pred) + cx(anchor) == cx(pred), delta_cy(pred) + cy(anchor) == cy(pred)", "\n", "y_pred_converted", "=", "convert_coordinates", "(", "y_pred_converted", ",", "start_index", "=", "-", "4", ",", "conversion", "=", "'centroids2corners'", ")", "\n", "", "elif", "input_coords", "==", "'minmax'", ":", "\n", "        ", "y_pred_converted", "[", ":", ",", ":", ",", "2", ":", "]", "*=", "y_pred", "[", ":", ",", ":", ",", "-", "4", ":", "]", "# delta(pred) / size(anchor) / variance * variance == delta(pred) / size(anchor) for all four coordinates, where 'size' refers to w or h, respectively", "\n", "y_pred_converted", "[", ":", ",", ":", ",", "[", "2", ",", "3", "]", "]", "*=", "np", ".", "expand_dims", "(", "y_pred", "[", ":", ",", ":", ",", "-", "7", "]", "-", "y_pred", "[", ":", ",", ":", ",", "-", "8", "]", ",", "axis", "=", "-", "1", ")", "# delta_xmin(pred) / w(anchor) * w(anchor) == delta_xmin(pred), delta_xmax(pred) / w(anchor) * w(anchor) == delta_xmax(pred)", "\n", "y_pred_converted", "[", ":", ",", ":", ",", "[", "4", ",", "5", "]", "]", "*=", "np", ".", "expand_dims", "(", "y_pred", "[", ":", ",", ":", ",", "-", "5", "]", "-", "y_pred", "[", ":", ",", ":", ",", "-", "6", "]", ",", "axis", "=", "-", "1", ")", "# delta_ymin(pred) / h(anchor) * h(anchor) == delta_ymin(pred), delta_ymax(pred) / h(anchor) * h(anchor) == delta_ymax(pred)", "\n", "y_pred_converted", "[", ":", ",", ":", ",", "2", ":", "]", "+=", "y_pred", "[", ":", ",", ":", ",", "-", "8", ":", "-", "4", "]", "# delta(pred) + anchor == pred for all four coordinates", "\n", "y_pred_converted", "=", "convert_coordinates", "(", "y_pred_converted", ",", "start_index", "=", "-", "4", ",", "conversion", "=", "'minmax2corners'", ")", "\n", "", "elif", "input_coords", "==", "'corners'", ":", "\n", "        ", "y_pred_converted", "[", ":", ",", ":", ",", "2", ":", "]", "*=", "y_pred", "[", ":", ",", ":", ",", "-", "4", ":", "]", "# delta(pred) / size(anchor) / variance * variance == delta(pred) / size(anchor) for all four coordinates, where 'size' refers to w or h, respectively", "\n", "y_pred_converted", "[", ":", ",", ":", ",", "[", "2", ",", "4", "]", "]", "*=", "np", ".", "expand_dims", "(", "y_pred", "[", ":", ",", ":", ",", "-", "6", "]", "-", "y_pred", "[", ":", ",", ":", ",", "-", "8", "]", ",", "axis", "=", "-", "1", ")", "# delta_xmin(pred) / w(anchor) * w(anchor) == delta_xmin(pred), delta_xmax(pred) / w(anchor) * w(anchor) == delta_xmax(pred)", "\n", "y_pred_converted", "[", ":", ",", ":", ",", "[", "3", ",", "5", "]", "]", "*=", "np", ".", "expand_dims", "(", "y_pred", "[", ":", ",", ":", ",", "-", "5", "]", "-", "y_pred", "[", ":", ",", ":", ",", "-", "7", "]", ",", "axis", "=", "-", "1", ")", "# delta_ymin(pred) / h(anchor) * h(anchor) == delta_ymin(pred), delta_ymax(pred) / h(anchor) * h(anchor) == delta_ymax(pred)", "\n", "y_pred_converted", "[", ":", ",", ":", ",", "2", ":", "]", "+=", "y_pred", "[", ":", ",", ":", ",", "-", "8", ":", "-", "4", "]", "# delta(pred) + anchor == pred for all four coordinates", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unexpected value for `coords`. Supported values are 'minmax', 'corners' and 'centroids'.\"", ")", "\n", "\n", "# 3: If the model predicts normalized box coordinates and they are supposed to be converted back to absolute coordinates, do that", "\n", "", "if", "normalize_coords", ":", "\n", "        ", "y_pred_converted", "[", ":", ",", ":", ",", "[", "2", ",", "4", "]", "]", "*=", "img_width", "# Convert xmin, xmax back to absolute coordinates", "\n", "y_pred_converted", "[", ":", ",", ":", ",", "[", "3", ",", "5", "]", "]", "*=", "img_height", "# Convert ymin, ymax back to absolute coordinates", "\n", "\n", "# 4: Decode our huge `(batch, #boxes, 6)` tensor into a list of length `batch` where each list entry is an array containing only the positive predictions", "\n", "", "y_pred_decoded", "=", "[", "]", "\n", "for", "batch_item", "in", "y_pred_converted", ":", "# For each image in the batch...", "\n", "        ", "boxes", "=", "batch_item", "[", "np", ".", "nonzero", "(", "batch_item", "[", ":", ",", "0", "]", ")", "]", "# ...get all boxes that don't belong to the background class,...", "\n", "boxes", "=", "boxes", "[", "boxes", "[", ":", ",", "1", "]", ">=", "confidence_thresh", "]", "# ...then filter out those positive boxes for which the prediction confidence is too low and after that...", "\n", "if", "iou_threshold", ":", "# ...if an IoU threshold is set...", "\n", "            ", "boxes", "=", "_greedy_nms2", "(", "boxes", ",", "iou_threshold", "=", "iou_threshold", ",", "coords", "=", "'corners'", ",", "border_pixels", "=", "border_pixels", ")", "# ...perform NMS on the remaining boxes.", "\n", "", "if", "top_k", "!=", "'all'", "and", "boxes", ".", "shape", "[", "0", "]", ">", "top_k", ":", "# If we have more than `top_k` results left at this point...", "\n", "            ", "top_k_indices", "=", "np", ".", "argpartition", "(", "boxes", "[", ":", ",", "1", "]", ",", "kth", "=", "boxes", ".", "shape", "[", "0", "]", "-", "top_k", ",", "axis", "=", "0", ")", "[", "boxes", ".", "shape", "[", "0", "]", "-", "top_k", ":", "]", "# ...get the indices of the `top_k` highest-scoring boxes...", "\n", "boxes", "=", "boxes", "[", "top_k_indices", "]", "# ...and keep only those boxes...", "\n", "", "y_pred_decoded", ".", "append", "(", "boxes", ")", "# ...and now that we're done, append the array of final predictions for this batch item to the output list", "\n", "\n", "", "return", "y_pred_decoded", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.ssd_output_decoder.decode_detections_debug": [[342, 468], ["numpy.copy", "numpy.zeros", "numpy.arange", "ValueError", "bounding_box_utils.bounding_box_utils.convert_coordinates", "range", "numpy.concatenate", "y_pred_decoded.append", "numpy.expand_dims", "numpy.expand_dims", "bounding_box_utils.bounding_box_utils.convert_coordinates", "numpy.exp", "numpy.exp", "numpy.expand_dims", "numpy.expand_dims", "ValueError", "ssd_output_decoder._greedy_nms_debug", "numpy.zeros", "np.concatenate.append", "numpy.argpartition"], "function", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.convert_coordinates", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.convert_coordinates", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.ssd_output_decoder._greedy_nms_debug"], ["", "def", "decode_detections_debug", "(", "y_pred", ",", "\n", "confidence_thresh", "=", "0.01", ",", "\n", "iou_threshold", "=", "0.45", ",", "\n", "top_k", "=", "200", ",", "\n", "input_coords", "=", "'centroids'", ",", "\n", "normalize_coords", "=", "True", ",", "\n", "img_height", "=", "None", ",", "\n", "img_width", "=", "None", ",", "\n", "variance_encoded_in_target", "=", "False", ",", "\n", "border_pixels", "=", "'half'", ")", ":", "\n", "    ", "'''\n    This decoder performs the same processing as `decode_detections()`, but the output format for each left-over\n    predicted box is `[box_id, class_id, confidence, xmin, ymin, xmax, ymax]`.\n\n    That is, in addition to the usual data, each predicted box has the internal index of that box within\n    the model (`box_id`) prepended to it. This allows you to know exactly which part of the model made a given\n    box prediction; in particular, it allows you to know which predictor layer made a given prediction.\n    This can be useful for debugging.\n\n    Arguments:\n        y_pred (array): The prediction output of the SSD model, expected to be a Numpy array\n            of shape `(batch_size, #boxes, #classes + 4 + 4 + 4)`, where `#boxes` is the total number of\n            boxes predicted by the model per image and the last axis contains\n            `[one-hot vector for the classes, 4 predicted coordinate offsets, 4 anchor box coordinates, 4 variances]`.\n        confidence_thresh (float, optional): A float in [0,1), the minimum classification confidence in a specific\n            positive class in order to be considered for the non-maximum suppression stage for the respective class.\n            A lower value will result in a larger part of the selection process being done by the non-maximum suppression\n            stage, while a larger value will result in a larger part of the selection process happening in the confidence\n            thresholding stage.\n        iou_threshold (float, optional): A float in [0,1]. All boxes with a Jaccard similarity of greater than `iou_threshold`\n            with a locally maximal box will be removed from the set of predictions for a given class, where 'maximal' refers\n            to the box score.\n        top_k (int, optional): The number of highest scoring predictions to be kept for each batch item after the\n            non-maximum suppression stage.\n        input_coords (str, optional): The box coordinate format that the model outputs. Can be either 'centroids'\n            for the format `(cx, cy, w, h)` (box center coordinates, width, and height), 'minmax' for the format\n            `(xmin, xmax, ymin, ymax)`, or 'corners' for the format `(xmin, ymin, xmax, ymax)`.\n        normalize_coords (bool, optional): Set to `True` if the model outputs relative coordinates (i.e. coordinates in [0,1])\n            and you wish to transform these relative coordinates back to absolute coordinates. If the model outputs\n            relative coordinates, but you do not want to convert them back to absolute coordinates, set this to `False`.\n            Do not set this to `True` if the model already outputs absolute coordinates, as that would result in incorrect\n            coordinates. Requires `img_height` and `img_width` if set to `True`.\n        img_height (int, optional): The height of the input images. Only needed if `normalize_coords` is `True`.\n        img_width (int, optional): The width of the input images. Only needed if `normalize_coords` is `True`.\n        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n            to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n            If 'half', then one of each of the two horizontal and vertical borders belong\n            to the boxex, but not the other.\n\n    Returns:\n        A python list of length `batch_size` where each list element represents the predicted boxes\n        for one image and contains a Numpy array of shape `(boxes, 7)` where each row is a box prediction for\n        a non-background class for the respective image in the format `[box_id, class_id, confidence, xmin, ymin, xmax, ymax]`.\n    '''", "\n", "if", "normalize_coords", "and", "(", "(", "img_height", "is", "None", ")", "or", "(", "img_width", "is", "None", ")", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"If relative box coordinates are supposed to be converted to absolute coordinates, the decoder needs the image size in order to decode the predictions, but `img_height == {}` and `img_width == {}`\"", ".", "format", "(", "img_height", ",", "img_width", ")", ")", "\n", "\n", "# 1: Convert the box coordinates from the predicted anchor box offsets to predicted absolute coordinates", "\n", "\n", "", "y_pred_decoded_raw", "=", "np", ".", "copy", "(", "y_pred", "[", ":", ",", ":", ",", ":", "-", "8", "]", ")", "# Slice out the classes and the four offsets, throw away the anchor coordinates and variances, resulting in a tensor of shape `[batch, n_boxes, n_classes + 4 coordinates]`", "\n", "\n", "if", "input_coords", "==", "'centroids'", ":", "\n", "        ", "if", "variance_encoded_in_target", ":", "\n", "# Decode the predicted box center x and y coordinates.", "\n", "            ", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "4", ",", "-", "3", "]", "]", "=", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "4", ",", "-", "3", "]", "]", "*", "y_pred", "[", ":", ",", ":", ",", "[", "-", "6", ",", "-", "5", "]", "]", "+", "y_pred", "[", ":", ",", ":", ",", "[", "-", "8", ",", "-", "7", "]", "]", "\n", "# Decode the predicted box width and heigt.", "\n", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "2", ",", "-", "1", "]", "]", "=", "np", ".", "exp", "(", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "2", ",", "-", "1", "]", "]", ")", "*", "y_pred", "[", ":", ",", ":", ",", "[", "-", "6", ",", "-", "5", "]", "]", "\n", "", "else", ":", "\n", "# Decode the predicted box center x and y coordinates.", "\n", "            ", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "4", ",", "-", "3", "]", "]", "=", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "4", ",", "-", "3", "]", "]", "*", "y_pred", "[", ":", ",", ":", ",", "[", "-", "6", ",", "-", "5", "]", "]", "*", "y_pred", "[", ":", ",", ":", ",", "[", "-", "4", ",", "-", "3", "]", "]", "+", "y_pred", "[", ":", ",", ":", ",", "[", "-", "8", ",", "-", "7", "]", "]", "\n", "# Decode the predicted box width and heigt.", "\n", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "2", ",", "-", "1", "]", "]", "=", "np", ".", "exp", "(", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "2", ",", "-", "1", "]", "]", "*", "y_pred", "[", ":", ",", ":", ",", "[", "-", "2", ",", "-", "1", "]", "]", ")", "*", "y_pred", "[", ":", ",", ":", ",", "[", "-", "6", ",", "-", "5", "]", "]", "\n", "", "y_pred_decoded_raw", "=", "convert_coordinates", "(", "y_pred_decoded_raw", ",", "start_index", "=", "-", "4", ",", "conversion", "=", "'centroids2corners'", ")", "\n", "", "elif", "input_coords", "==", "'minmax'", ":", "\n", "        ", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "-", "4", ":", "]", "*=", "y_pred", "[", ":", ",", ":", ",", "-", "4", ":", "]", "# delta(pred) / size(anchor) / variance * variance == delta(pred) / size(anchor) for all four coordinates, where 'size' refers to w or h, respectively", "\n", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "4", ",", "-", "3", "]", "]", "*=", "np", ".", "expand_dims", "(", "y_pred", "[", ":", ",", ":", ",", "-", "7", "]", "-", "y_pred", "[", ":", ",", ":", ",", "-", "8", "]", ",", "axis", "=", "-", "1", ")", "# delta_xmin(pred) / w(anchor) * w(anchor) == delta_xmin(pred), delta_xmax(pred) / w(anchor) * w(anchor) == delta_xmax(pred)", "\n", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "2", ",", "-", "1", "]", "]", "*=", "np", ".", "expand_dims", "(", "y_pred", "[", ":", ",", ":", ",", "-", "5", "]", "-", "y_pred", "[", ":", ",", ":", ",", "-", "6", "]", ",", "axis", "=", "-", "1", ")", "# delta_ymin(pred) / h(anchor) * h(anchor) == delta_ymin(pred), delta_ymax(pred) / h(anchor) * h(anchor) == delta_ymax(pred)", "\n", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "-", "4", ":", "]", "+=", "y_pred", "[", ":", ",", ":", ",", "-", "8", ":", "-", "4", "]", "# delta(pred) + anchor == pred for all four coordinates", "\n", "y_pred_decoded_raw", "=", "convert_coordinates", "(", "y_pred_decoded_raw", ",", "start_index", "=", "-", "4", ",", "conversion", "=", "'minmax2corners'", ")", "\n", "", "elif", "input_coords", "==", "'corners'", ":", "\n", "        ", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "-", "4", ":", "]", "*=", "y_pred", "[", ":", ",", ":", ",", "-", "4", ":", "]", "# delta(pred) / size(anchor) / variance * variance == delta(pred) / size(anchor) for all four coordinates, where 'size' refers to w or h, respectively", "\n", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "4", ",", "-", "2", "]", "]", "*=", "np", ".", "expand_dims", "(", "y_pred", "[", ":", ",", ":", ",", "-", "6", "]", "-", "y_pred", "[", ":", ",", ":", ",", "-", "8", "]", ",", "axis", "=", "-", "1", ")", "# delta_xmin(pred) / w(anchor) * w(anchor) == delta_xmin(pred), delta_xmax(pred) / w(anchor) * w(anchor) == delta_xmax(pred)", "\n", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "3", ",", "-", "1", "]", "]", "*=", "np", ".", "expand_dims", "(", "y_pred", "[", ":", ",", ":", ",", "-", "5", "]", "-", "y_pred", "[", ":", ",", ":", ",", "-", "7", "]", ",", "axis", "=", "-", "1", ")", "# delta_ymin(pred) / h(anchor) * h(anchor) == delta_ymin(pred), delta_ymax(pred) / h(anchor) * h(anchor) == delta_ymax(pred)", "\n", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "-", "4", ":", "]", "+=", "y_pred", "[", ":", ",", ":", ",", "-", "8", ":", "-", "4", "]", "# delta(pred) + anchor == pred for all four coordinates", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unexpected value for `input_coords`. Supported input coordinate formats are 'minmax', 'corners' and 'centroids'.\"", ")", "\n", "\n", "# 2: If the model predicts normalized box coordinates and they are supposed to be converted back to absolute coordinates, do that", "\n", "\n", "", "if", "normalize_coords", ":", "\n", "        ", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "4", ",", "-", "2", "]", "]", "*=", "img_width", "# Convert xmin, xmax back to absolute coordinates", "\n", "y_pred_decoded_raw", "[", ":", ",", ":", ",", "[", "-", "3", ",", "-", "1", "]", "]", "*=", "img_height", "# Convert ymin, ymax back to absolute coordinates", "\n", "\n", "# 3: For each batch item, prepend each box's internal index to its coordinates.", "\n", "\n", "", "y_pred_decoded_raw2", "=", "np", ".", "zeros", "(", "(", "y_pred_decoded_raw", ".", "shape", "[", "0", "]", ",", "y_pred_decoded_raw", ".", "shape", "[", "1", "]", ",", "y_pred_decoded_raw", ".", "shape", "[", "2", "]", "+", "1", ")", ")", "# Expand the last axis by one.", "\n", "y_pred_decoded_raw2", "[", ":", ",", ":", ",", "1", ":", "]", "=", "y_pred_decoded_raw", "\n", "y_pred_decoded_raw2", "[", ":", ",", ":", ",", "0", "]", "=", "np", ".", "arange", "(", "y_pred_decoded_raw", ".", "shape", "[", "1", "]", ")", "# Put the box indices as the first element for each box via broadcasting.", "\n", "y_pred_decoded_raw", "=", "y_pred_decoded_raw2", "\n", "\n", "# 4: Apply confidence thresholding and non-maximum suppression per class", "\n", "\n", "n_classes", "=", "y_pred_decoded_raw", ".", "shape", "[", "-", "1", "]", "-", "5", "# The number of classes is the length of the last axis minus the four box coordinates and minus the index", "\n", "\n", "y_pred_decoded", "=", "[", "]", "# Store the final predictions in this list", "\n", "for", "batch_item", "in", "y_pred_decoded_raw", ":", "# `batch_item` has shape `[n_boxes, n_classes + 4 coords]`", "\n", "        ", "pred", "=", "[", "]", "# Store the final predictions for this batch item here", "\n", "for", "class_id", "in", "range", "(", "1", ",", "n_classes", ")", ":", "# For each class except the background class (which has class ID 0)...", "\n", "            ", "single_class", "=", "batch_item", "[", ":", ",", "[", "0", ",", "class_id", "+", "1", ",", "-", "4", ",", "-", "3", ",", "-", "2", ",", "-", "1", "]", "]", "# ...keep only the confidences for that class, making this an array of shape `[n_boxes, 6]` and...", "\n", "threshold_met", "=", "single_class", "[", "single_class", "[", ":", ",", "1", "]", ">", "confidence_thresh", "]", "# ...keep only those boxes with a confidence above the set threshold.", "\n", "if", "threshold_met", ".", "shape", "[", "0", "]", ">", "0", ":", "# If any boxes made the threshold...", "\n", "                ", "maxima", "=", "_greedy_nms_debug", "(", "threshold_met", ",", "iou_threshold", "=", "iou_threshold", ",", "coords", "=", "'corners'", ",", "border_pixels", "=", "border_pixels", ")", "# ...perform NMS on them.", "\n", "maxima_output", "=", "np", ".", "zeros", "(", "(", "maxima", ".", "shape", "[", "0", "]", ",", "maxima", ".", "shape", "[", "1", "]", "+", "1", ")", ")", "# Expand the last dimension by one element to have room for the class ID. This is now an arrray of shape `[n_boxes, 6]`", "\n", "maxima_output", "[", ":", ",", "0", "]", "=", "maxima", "[", ":", ",", "0", "]", "# Write the box index to the first column...", "\n", "maxima_output", "[", ":", ",", "1", "]", "=", "class_id", "# ...and write the class ID to the second column...", "\n", "maxima_output", "[", ":", ",", "2", ":", "]", "=", "maxima", "[", ":", ",", "1", ":", "]", "# ...and write the rest of the maxima data to the other columns...", "\n", "pred", ".", "append", "(", "maxima_output", ")", "# ...and append the maxima for this class to the list of maxima for this batch item.", "\n", "# Once we're through with all classes, keep only the `top_k` maxima with the highest scores", "\n", "", "", "pred", "=", "np", ".", "concatenate", "(", "pred", ",", "axis", "=", "0", ")", "\n", "if", "pred", ".", "shape", "[", "0", "]", ">", "top_k", ":", "# If we have more than `top_k` results left at this point, otherwise there is nothing to filter,...", "\n", "            ", "top_k_indices", "=", "np", ".", "argpartition", "(", "pred", "[", ":", ",", "2", "]", ",", "kth", "=", "pred", ".", "shape", "[", "0", "]", "-", "top_k", ",", "axis", "=", "0", ")", "[", "pred", ".", "shape", "[", "0", "]", "-", "top_k", ":", "]", "# ...get the indices of the `top_k` highest-score maxima...", "\n", "pred", "=", "pred", "[", "top_k_indices", "]", "# ...and keep only those entries of `pred`...", "\n", "", "y_pred_decoded", ".", "append", "(", "pred", ")", "# ...and now that we're done, append the array of final predictions for this batch item to the output list", "\n", "\n", "", "return", "y_pred_decoded", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.ssd_output_decoder._greedy_nms_debug": [[469, 487], ["numpy.copy", "numpy.array", "numpy.argmax", "numpy.copy", "maxima.append", "numpy.delete", "bounding_box_utils.bounding_box_utils.iou"], "function", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.iou"], ["", "def", "_greedy_nms_debug", "(", "predictions", ",", "iou_threshold", "=", "0.45", ",", "coords", "=", "'corners'", ",", "border_pixels", "=", "'half'", ")", ":", "\n", "    ", "'''\n    The same greedy non-maximum suppression algorithm as above, but slightly modified for use as an internal\n    function for per-class NMS in `decode_detections_debug()`. The difference is that it keeps the indices of all\n    left-over boxes for each batch item, which allows you to know which predictor layer predicted a given output\n    box and is thus useful for debugging.\n    '''", "\n", "boxes_left", "=", "np", ".", "copy", "(", "predictions", ")", "\n", "maxima", "=", "[", "]", "# This is where we store the boxes that make it through the non-maximum suppression", "\n", "while", "boxes_left", ".", "shape", "[", "0", "]", ">", "0", ":", "# While there are still boxes left to compare...", "\n", "        ", "maximum_index", "=", "np", ".", "argmax", "(", "boxes_left", "[", ":", ",", "1", "]", ")", "# ...get the index of the next box with the highest confidence...", "\n", "maximum_box", "=", "np", ".", "copy", "(", "boxes_left", "[", "maximum_index", "]", ")", "# ...copy that box and...", "\n", "maxima", ".", "append", "(", "maximum_box", ")", "# ...append it to `maxima` because we'll definitely keep it", "\n", "boxes_left", "=", "np", ".", "delete", "(", "boxes_left", ",", "maximum_index", ",", "axis", "=", "0", ")", "# Now remove the maximum box from `boxes_left`", "\n", "if", "boxes_left", ".", "shape", "[", "0", "]", "==", "0", ":", "break", "# If there are no boxes left after this step, break. Otherwise...", "\n", "similarities", "=", "iou", "(", "boxes_left", "[", ":", ",", "2", ":", "]", ",", "maximum_box", "[", "2", ":", "]", ",", "coords", "=", "coords", ",", "mode", "=", "'element-wise'", ",", "border_pixels", "=", "border_pixels", ")", "# ...compare (IoU) the other left over boxes to the maximum box...", "\n", "boxes_left", "=", "boxes_left", "[", "similarities", "<=", "iou_threshold", "]", "# ...so that we can remove the ones that overlap too much with the maximum box", "\n", "", "return", "np", ".", "array", "(", "maxima", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.ssd_output_decoder.get_num_boxes_per_pred_layer": [[488, 502], ["range", "len", "num_boxes_per_pred_layer.append", "num_boxes_per_pred_layer.append", "len", "len"], "function", ["None"], ["", "def", "get_num_boxes_per_pred_layer", "(", "predictor_sizes", ",", "aspect_ratios", ",", "two_boxes_for_ar1", ")", ":", "\n", "    ", "'''\n    Returns a list of the number of boxes that each predictor layer predicts.\n\n    `aspect_ratios` must be a nested list, containing a list of aspect ratios\n    for each predictor layer.\n    '''", "\n", "num_boxes_per_pred_layer", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "predictor_sizes", ")", ")", ":", "\n", "        ", "if", "two_boxes_for_ar1", ":", "\n", "            ", "num_boxes_per_pred_layer", ".", "append", "(", "predictor_sizes", "[", "i", "]", "[", "0", "]", "*", "predictor_sizes", "[", "i", "]", "[", "1", "]", "*", "(", "len", "(", "aspect_ratios", "[", "i", "]", ")", "+", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "num_boxes_per_pred_layer", ".", "append", "(", "predictor_sizes", "[", "i", "]", "[", "0", "]", "*", "predictor_sizes", "[", "i", "]", "[", "1", "]", "*", "len", "(", "aspect_ratios", "[", "i", "]", ")", ")", "\n", "", "", "return", "num_boxes_per_pred_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.ssd_output_decoder.get_pred_layers": [[503, 531], ["numpy.cumsum", "pred_layers_all.append", "range", "ValueError", "len", "pred_layers.append"], "function", ["None"], ["", "def", "get_pred_layers", "(", "y_pred_decoded", ",", "num_boxes_per_pred_layer", ")", ":", "\n", "    ", "'''\n    For a given prediction tensor decoded with `decode_detections_debug()`, returns a list\n    with the indices of the predictor layers that made each predictions.\n\n    That is, this function lets you know which predictor layer is responsible\n    for a given prediction.\n\n    Arguments:\n        y_pred_decoded (array): The decoded model output tensor. Must have been\n            decoded with `decode_detections_debug()` so that it contains the internal box index\n            for each predicted box.\n        num_boxes_per_pred_layer (list): A list that contains the total number\n            of boxes that each predictor layer predicts.\n    '''", "\n", "pred_layers_all", "=", "[", "]", "\n", "cum_boxes_per_pred_layer", "=", "np", ".", "cumsum", "(", "num_boxes_per_pred_layer", ")", "\n", "for", "batch_item", "in", "y_pred_decoded", ":", "\n", "        ", "pred_layers", "=", "[", "]", "\n", "for", "prediction", "in", "batch_item", ":", "\n", "            ", "if", "(", "prediction", "[", "0", "]", "<", "0", ")", "or", "(", "prediction", "[", "0", "]", ">=", "cum_boxes_per_pred_layer", "[", "-", "1", "]", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\"Box index is out of bounds of the possible indices as given by the values in `num_boxes_per_pred_layer`.\"", ")", "\n", "", "for", "i", "in", "range", "(", "len", "(", "cum_boxes_per_pred_layer", ")", ")", ":", "\n", "                ", "if", "prediction", "[", "0", "]", "<", "cum_boxes_per_pred_layer", "[", "i", "]", ":", "\n", "                    ", "pred_layers", ".", "append", "(", "i", ")", "\n", "break", "\n", "", "", "", "pred_layers_all", ".", "append", "(", "pred_layers", ")", "\n", "", "return", "pred_layers_all", "\n", "", ""]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.matching_utils.match_bipartite_greedy": [[22, 80], ["numpy.copy", "list", "numpy.zeros", "range", "range", "numpy.argmax", "numpy.argmax"], "function", ["None"], ["def", "match_bipartite_greedy", "(", "weight_matrix", ")", ":", "\n", "    ", "'''\n    Returns a bipartite matching according to the given weight matrix.\n\n    The algorithm works as follows:\n\n    Let the first axis of `weight_matrix` represent ground truth boxes\n    and the second axis anchor boxes.\n    The ground truth box that has the greatest similarity with any\n    anchor box will be matched first, then out of the remaining ground\n    truth boxes, the ground truth box that has the greatest similarity\n    with any of the remaining anchor boxes will be matched second, and\n    so on. That is, the ground truth boxes will be matched in descending\n    order by maximum similarity with any of the respectively remaining\n    anchor boxes.\n    The runtime complexity is O(m^2 * n), where `m` is the number of\n    ground truth boxes and `n` is the number of anchor boxes.\n\n    Arguments:\n        weight_matrix (array): A 2D Numpy array that represents the weight matrix\n            for the matching process. If `(m,n)` is the shape of the weight matrix,\n            it must be `m <= n`. The weights can be integers or floating point\n            numbers. The matching process will maximize, i.e. larger weights are\n            preferred over smaller weights.\n\n    Returns:\n        A 1D Numpy array of length `weight_matrix.shape[0]` that represents\n        the matched index along the second axis of `weight_matrix` for each index\n        along the first axis.\n    '''", "\n", "\n", "weight_matrix", "=", "np", ".", "copy", "(", "weight_matrix", ")", "# We'll modify this array.", "\n", "num_ground_truth_boxes", "=", "weight_matrix", ".", "shape", "[", "0", "]", "\n", "all_gt_indices", "=", "list", "(", "range", "(", "num_ground_truth_boxes", ")", ")", "# Only relevant for fancy-indexing below.", "\n", "\n", "# This 1D array will contain for each ground truth box the index of", "\n", "# the matched anchor box.", "\n", "matches", "=", "np", ".", "zeros", "(", "num_ground_truth_boxes", ",", "dtype", "=", "np", ".", "int", ")", "\n", "\n", "# In each iteration of the loop below, exactly one ground truth box", "\n", "# will be matched to one anchor box.", "\n", "for", "_", "in", "range", "(", "num_ground_truth_boxes", ")", ":", "\n", "\n", "# Find the maximal anchor-ground truth pair in two steps: First, reduce", "\n", "# over the anchor boxes and then reduce over the ground truth boxes.", "\n", "        ", "anchor_indices", "=", "np", ".", "argmax", "(", "weight_matrix", ",", "axis", "=", "1", ")", "# Reduce along the anchor box axis.", "\n", "overlaps", "=", "weight_matrix", "[", "all_gt_indices", ",", "anchor_indices", "]", "\n", "ground_truth_index", "=", "np", ".", "argmax", "(", "overlaps", ")", "# Reduce along the ground truth box axis.", "\n", "anchor_index", "=", "anchor_indices", "[", "ground_truth_index", "]", "\n", "matches", "[", "ground_truth_index", "]", "=", "anchor_index", "# Set the match.", "\n", "\n", "# Set the row of the matched ground truth box and the column of the matched", "\n", "# anchor box to all zeros. This ensures that those boxes will not be matched again,", "\n", "# because they will never be the best matches for any other boxes.", "\n", "weight_matrix", "[", "ground_truth_index", "]", "=", "0", "\n", "weight_matrix", "[", ":", ",", "anchor_index", "]", "=", "0", "\n", "\n", "", "return", "matches", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.matching_utils.match_multi": [[81, 117], ["list", "numpy.argmax", "range", "numpy.nonzero"], "function", ["None"], ["", "def", "match_multi", "(", "weight_matrix", ",", "threshold", ")", ":", "\n", "    ", "'''\n    Matches all elements along the second axis of `weight_matrix` to their best\n    matches along the first axis subject to the constraint that the weight of a match\n    must be greater than or equal to `threshold` in order to produce a match.\n\n    If the weight matrix contains elements that should be ignored, the row or column\n    representing the respective elemet should be set to a value below `threshold`.\n\n    Arguments:\n        weight_matrix (array): A 2D Numpy array that represents the weight matrix\n            for the matching process. If `(m,n)` is the shape of the weight matrix,\n            it must be `m <= n`. The weights can be integers or floating point\n            numbers. The matching process will maximize, i.e. larger weights are\n            preferred over smaller weights.\n        threshold (float): A float that represents the threshold (i.e. lower bound)\n            that must be met by a pair of elements to produce a match.\n\n    Returns:\n        Two 1D Numpy arrays of equal length that represent the matched indices. The first\n        array contains the indices along the first axis of `weight_matrix`, the second array\n        contains the indices along the second axis.\n    '''", "\n", "\n", "num_anchor_boxes", "=", "weight_matrix", ".", "shape", "[", "1", "]", "\n", "all_anchor_indices", "=", "list", "(", "range", "(", "num_anchor_boxes", ")", ")", "# Only relevant for fancy-indexing below.", "\n", "\n", "# Find the best ground truth match for every anchor box.", "\n", "ground_truth_indices", "=", "np", ".", "argmax", "(", "weight_matrix", ",", "axis", "=", "0", ")", "# Array of shape (weight_matrix.shape[1],)", "\n", "overlaps", "=", "weight_matrix", "[", "ground_truth_indices", ",", "all_anchor_indices", "]", "# Array of shape (weight_matrix.shape[1],)", "\n", "\n", "# Filter out the matches with a weight below the threshold.", "\n", "anchor_indices_thresh_met", "=", "np", ".", "nonzero", "(", "overlaps", ">=", "threshold", ")", "[", "0", "]", "\n", "gt_indices_thresh_met", "=", "ground_truth_indices", "[", "anchor_indices_thresh_met", "]", "\n", "\n", "return", "gt_indices_thresh_met", ",", "anchor_indices_thresh_met", "\n", "", ""]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.ssd_input_encoder.SSDInputEncoder.__init__": [[36, 276], ["numpy.array", "numpy.array", "numpy.any", "range", "numpy.expand_dims", "ValueError", "numpy.array", "numpy.any", "numpy.any", "len", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "numpy.linspace", "len", "ssd_input_encoder.SSDInputEncoder.generate_anchor_boxes_for_layer", "ssd_input_encoder.SSDInputEncoder.boxes_list.append", "ssd_input_encoder.SSDInputEncoder.wh_list_diag.append", "ssd_input_encoder.SSDInputEncoder.steps_diag.append", "ssd_input_encoder.SSDInputEncoder.offsets_diag.append", "ssd_input_encoder.SSDInputEncoder.centers_diag.append", "len", "ValueError", "ValueError", "ValueError", "len", "ValueError", "numpy.any", "ValueError", "ValueError", "len", "len", "len", "ValueError", "numpy.array", "len", "len", "ssd_input_encoder.SSDInputEncoder.n_boxes.append", "ssd_input_encoder.SSDInputEncoder.n_boxes.append", "len", "len", "len", "len", "numpy.array", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.ssd_input_encoder.SSDInputEncoder.generate_anchor_boxes_for_layer"], ["def", "__init__", "(", "self", ",", "\n", "img_height", ",", "\n", "img_width", ",", "\n", "n_classes", ",", "\n", "predictor_sizes", ",", "\n", "min_scale", "=", "0.1", ",", "\n", "max_scale", "=", "0.9", ",", "\n", "scales", "=", "None", ",", "\n", "aspect_ratios_global", "=", "[", "0.5", ",", "1.0", ",", "2.0", "]", ",", "\n", "aspect_ratios_per_layer", "=", "None", ",", "\n", "two_boxes_for_ar1", "=", "True", ",", "\n", "steps", "=", "None", ",", "\n", "offsets", "=", "None", ",", "\n", "clip_boxes", "=", "False", ",", "\n", "variances", "=", "[", "0.1", ",", "0.1", ",", "0.2", ",", "0.2", "]", ",", "\n", "matching_type", "=", "'multi'", ",", "\n", "pos_iou_threshold", "=", "0.5", ",", "\n", "neg_iou_limit", "=", "0.3", ",", "\n", "border_pixels", "=", "'half'", ",", "\n", "coords", "=", "'centroids'", ",", "\n", "normalize_coords", "=", "True", ",", "\n", "background_id", "=", "0", ")", ":", "\n", "        ", "'''\n        Arguments:\n            img_height (int): The height of the input images.\n            img_width (int): The width of the input images.\n            n_classes (int): The number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO.\n            predictor_sizes (list): A list of int-tuples of the format `(height, width)`\n                containing the output heights and widths of the convolutional predictor layers.\n            min_scale (float, optional): The smallest scaling factor for the size of the anchor boxes as a fraction\n                of the shorter side of the input images. Note that you should set the scaling factors\n                such that the resulting anchor box sizes correspond to the sizes of the objects you are trying\n                to detect. Must be >0.\n            max_scale (float, optional): The largest scaling factor for the size of the anchor boxes as a fraction\n                of the shorter side of the input images. All scaling factors between the smallest and the\n                largest will be linearly interpolated. Note that the second to last of the linearly interpolated\n                scaling factors will actually be the scaling factor for the last predictor layer, while the last\n                scaling factor is used for the second box for aspect ratio 1 in the last predictor layer\n                if `two_boxes_for_ar1` is `True`. Note that you should set the scaling factors\n                such that the resulting anchor box sizes correspond to the sizes of the objects you are trying\n                to detect. Must be greater than or equal to `min_scale`.\n            scales (list, optional): A list of floats >0 containing scaling factors per convolutional predictor layer.\n                This list must be one element longer than the number of predictor layers. The first `k` elements are the\n                scaling factors for the `k` predictor layers, while the last element is used for the second box\n                for aspect ratio 1 in the last predictor layer if `two_boxes_for_ar1` is `True`. This additional\n                last scaling factor must be passed either way, even if it is not being used. If a list is passed,\n                this argument overrides `min_scale` and `max_scale`. All scaling factors must be greater than zero.\n                Note that you should set the scaling factors such that the resulting anchor box sizes correspond to\n                the sizes of the objects you are trying to detect.\n            aspect_ratios_global (list, optional): The list of aspect ratios for which anchor boxes are to be\n                generated. This list is valid for all prediction layers. Note that you should set the aspect ratios such\n                that the resulting anchor box shapes roughly correspond to the shapes of the objects you are trying to detect.\n            aspect_ratios_per_layer (list, optional): A list containing one aspect ratio list for each prediction layer.\n                If a list is passed, it overrides `aspect_ratios_global`. Note that you should set the aspect ratios such\n                that the resulting anchor box shapes very roughly correspond to the shapes of the objects you are trying to detect.\n            two_boxes_for_ar1 (bool, optional): Only relevant for aspect ratios lists that contain 1. Will be ignored otherwise.\n                If `True`, two anchor boxes will be generated for aspect ratio 1. The first will be generated\n                using the scaling factor for the respective layer, the second one will be generated using\n                geometric mean of said scaling factor and next bigger scaling factor.\n            steps (list, optional): `None` or a list with as many elements as there are predictor layers. The elements can be\n                either ints/floats or tuples of two ints/floats. These numbers represent for each predictor layer how many\n                pixels apart the anchor box center points should be vertically and horizontally along the spatial grid over\n                the image. If the list contains ints/floats, then that value will be used for both spatial dimensions.\n                If the list contains tuples of two ints/floats, then they represent `(step_height, step_width)`.\n                If no steps are provided, then they will be computed such that the anchor box center points will form an\n                equidistant grid within the image dimensions.\n            offsets (list, optional): `None` or a list with as many elements as there are predictor layers. The elements can be\n                either floats or tuples of two floats. These numbers represent for each predictor layer how many\n                pixels from the top and left boarders of the image the top-most and left-most anchor box center points should be\n                as a fraction of `steps`. The last bit is important: The offsets are not absolute pixel values, but fractions\n                of the step size specified in the `steps` argument. If the list contains floats, then that value will\n                be used for both spatial dimensions. If the list contains tuples of two floats, then they represent\n                `(vertical_offset, horizontal_offset)`. If no offsets are provided, then they will default to 0.5 of the step size.\n            clip_boxes (bool, optional): If `True`, limits the anchor box coordinates to stay within image boundaries.\n            variances (list, optional): A list of 4 floats >0. The anchor box offset for each coordinate will be divided by\n                its respective variance value.\n            matching_type (str, optional): Can be either 'multi' or 'bipartite'. In 'bipartite' mode, each ground truth box will\n                be matched only to the one anchor box with the highest IoU overlap. In 'multi' mode, in addition to the aforementioned\n                bipartite matching, all anchor boxes with an IoU overlap greater than or equal to the `pos_iou_threshold` will be\n                matched to a given ground truth box.\n            pos_iou_threshold (float, optional): The intersection-over-union similarity threshold that must be\n                met in order to match a given ground truth box to a given anchor box.\n            neg_iou_limit (float, optional): The maximum allowed intersection-over-union similarity of an\n                anchor box with any ground truth box to be labeled a negative (i.e. background) box. If an\n                anchor box is neither a positive, nor a negative box, it will be ignored during training.\n            border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n                Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n                to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n                If 'half', then one of each of the two horizontal and vertical borders belong\n                to the boxex, but not the other.\n            coords (str, optional): The box coordinate format to be used internally by the model (i.e. this is not the input format\n                of the ground truth labels). Can be either 'centroids' for the format `(cx, cy, w, h)` (box center coordinates, width,\n                and height), 'minmax' for the format `(xmin, xmax, ymin, ymax)`, or 'corners' for the format `(xmin, ymin, xmax, ymax)`.\n            normalize_coords (bool, optional): If `True`, the encoder uses relative instead of absolute coordinates.\n                This means instead of using absolute tartget coordinates, the encoder will scale all coordinates to be within [0,1].\n                This way learning becomes independent of the input image size.\n            background_id (int, optional): Determines which class ID is for the background class.\n        '''", "\n", "predictor_sizes", "=", "np", ".", "array", "(", "predictor_sizes", ")", "\n", "if", "predictor_sizes", ".", "ndim", "==", "1", ":", "\n", "            ", "predictor_sizes", "=", "np", ".", "expand_dims", "(", "predictor_sizes", ",", "axis", "=", "0", ")", "\n", "\n", "##################################################################################", "\n", "# Handle exceptions.", "\n", "##################################################################################", "\n", "\n", "", "if", "(", "min_scale", "is", "None", "or", "max_scale", "is", "None", ")", "and", "scales", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Either `min_scale` and `max_scale` or `scales` need to be specified.\"", ")", "\n", "\n", "", "if", "scales", ":", "\n", "            ", "if", "(", "len", "(", "scales", ")", "!=", "predictor_sizes", ".", "shape", "[", "0", "]", "+", "1", ")", ":", "# Must be two nested `if` statements since `list` and `bool` cannot be combined by `&`", "\n", "                ", "raise", "ValueError", "(", "\"It must be either scales is None or len(scales) == len(predictor_sizes)+1, but len(scales) == {} and len(predictor_sizes)+1 == {}\"", ".", "format", "(", "len", "(", "scales", ")", ",", "len", "(", "predictor_sizes", ")", "+", "1", ")", ")", "\n", "", "scales", "=", "np", ".", "array", "(", "scales", ")", "\n", "if", "np", ".", "any", "(", "scales", "<=", "0", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\"All values in `scales` must be greater than 0, but the passed list of scales is {}\"", ".", "format", "(", "scales", ")", ")", "\n", "", "", "else", ":", "# If no list of scales was passed, we need to make sure that `min_scale` and `max_scale` are valid values.", "\n", "            ", "if", "not", "0", "<", "min_scale", "<=", "max_scale", ":", "\n", "                ", "raise", "ValueError", "(", "\"It must be 0 < min_scale <= max_scale, but it is min_scale = {} and max_scale = {}\"", ".", "format", "(", "min_scale", ",", "max_scale", ")", ")", "\n", "\n", "", "", "if", "not", "(", "aspect_ratios_per_layer", "is", "None", ")", ":", "\n", "            ", "if", "(", "len", "(", "aspect_ratios_per_layer", ")", "!=", "predictor_sizes", ".", "shape", "[", "0", "]", ")", ":", "# Must be two nested `if` statements since `list` and `bool` cannot be combined by `&`", "\n", "                ", "raise", "ValueError", "(", "\"It must be either aspect_ratios_per_layer is None or len(aspect_ratios_per_layer) == len(predictor_sizes), but len(aspect_ratios_per_layer) == {} and len(predictor_sizes) == {}\"", ".", "format", "(", "len", "(", "aspect_ratios_per_layer", ")", ",", "len", "(", "predictor_sizes", ")", ")", ")", "\n", "", "for", "aspect_ratios", "in", "aspect_ratios_per_layer", ":", "\n", "                ", "if", "np", ".", "any", "(", "np", ".", "array", "(", "aspect_ratios", ")", "<=", "0", ")", ":", "\n", "                    ", "raise", "ValueError", "(", "\"All aspect ratios must be greater than zero.\"", ")", "\n", "", "", "", "else", ":", "\n", "            ", "if", "(", "aspect_ratios_global", "is", "None", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\"At least one of `aspect_ratios_global` and `aspect_ratios_per_layer` must not be `None`.\"", ")", "\n", "", "if", "np", ".", "any", "(", "np", ".", "array", "(", "aspect_ratios_global", ")", "<=", "0", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\"All aspect ratios must be greater than zero.\"", ")", "\n", "\n", "", "", "if", "len", "(", "variances", ")", "!=", "4", ":", "\n", "            ", "raise", "ValueError", "(", "\"4 variance values must be pased, but {} values were received.\"", ".", "format", "(", "len", "(", "variances", ")", ")", ")", "\n", "", "variances", "=", "np", ".", "array", "(", "variances", ")", "\n", "if", "np", ".", "any", "(", "variances", "<=", "0", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"All variances must be >0, but the variances given are {}\"", ".", "format", "(", "variances", ")", ")", "\n", "\n", "", "if", "not", "(", "coords", "==", "'minmax'", "or", "coords", "==", "'centroids'", "or", "coords", "==", "'corners'", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unexpected value for `coords`. Supported values are 'minmax', 'corners' and 'centroids'.\"", ")", "\n", "\n", "", "if", "(", "not", "(", "steps", "is", "None", ")", ")", "and", "(", "len", "(", "steps", ")", "!=", "predictor_sizes", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"You must provide at least one step value per predictor layer.\"", ")", "\n", "\n", "", "if", "(", "not", "(", "offsets", "is", "None", ")", ")", "and", "(", "len", "(", "offsets", ")", "!=", "predictor_sizes", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"You must provide at least one offset value per predictor layer.\"", ")", "\n", "\n", "##################################################################################", "\n", "# Set or compute members.", "\n", "##################################################################################", "\n", "\n", "", "self", ".", "img_height", "=", "img_height", "\n", "self", ".", "img_width", "=", "img_width", "\n", "self", ".", "n_classes", "=", "n_classes", "+", "1", "# + 1 for the background class", "\n", "self", ".", "predictor_sizes", "=", "predictor_sizes", "\n", "self", ".", "min_scale", "=", "min_scale", "\n", "self", ".", "max_scale", "=", "max_scale", "\n", "# If `scales` is None, compute the scaling factors by linearly interpolating between", "\n", "# `min_scale` and `max_scale`. If an explicit list of `scales` is given, however,", "\n", "# then it takes precedent over `min_scale` and `max_scale`.", "\n", "if", "(", "scales", "is", "None", ")", ":", "\n", "            ", "self", ".", "scales", "=", "np", ".", "linspace", "(", "self", ".", "min_scale", ",", "self", ".", "max_scale", ",", "len", "(", "self", ".", "predictor_sizes", ")", "+", "1", ")", "\n", "", "else", ":", "\n", "# If a list of scales is given explicitly, we'll use that instead of computing it from `min_scale` and `max_scale`.", "\n", "            ", "self", ".", "scales", "=", "scales", "\n", "# If `aspect_ratios_per_layer` is None, then we use the same list of aspect ratios", "\n", "# `aspect_ratios_global` for all predictor layers. If `aspect_ratios_per_layer` is given,", "\n", "# however, then it takes precedent over `aspect_ratios_global`.", "\n", "", "if", "(", "aspect_ratios_per_layer", "is", "None", ")", ":", "\n", "            ", "self", ".", "aspect_ratios", "=", "[", "aspect_ratios_global", "]", "*", "predictor_sizes", ".", "shape", "[", "0", "]", "\n", "", "else", ":", "\n", "# If aspect ratios are given per layer, we'll use those.", "\n", "            ", "self", ".", "aspect_ratios", "=", "aspect_ratios_per_layer", "\n", "", "self", ".", "two_boxes_for_ar1", "=", "two_boxes_for_ar1", "\n", "if", "not", "(", "steps", "is", "None", ")", ":", "\n", "            ", "self", ".", "steps", "=", "steps", "\n", "", "else", ":", "\n", "            ", "self", ".", "steps", "=", "[", "None", "]", "*", "predictor_sizes", ".", "shape", "[", "0", "]", "\n", "", "if", "not", "(", "offsets", "is", "None", ")", ":", "\n", "            ", "self", ".", "offsets", "=", "offsets", "\n", "", "else", ":", "\n", "            ", "self", ".", "offsets", "=", "[", "None", "]", "*", "predictor_sizes", ".", "shape", "[", "0", "]", "\n", "", "self", ".", "clip_boxes", "=", "clip_boxes", "\n", "self", ".", "variances", "=", "variances", "\n", "self", ".", "matching_type", "=", "matching_type", "\n", "self", ".", "pos_iou_threshold", "=", "pos_iou_threshold", "\n", "self", ".", "neg_iou_limit", "=", "neg_iou_limit", "\n", "self", ".", "border_pixels", "=", "border_pixels", "\n", "self", ".", "coords", "=", "coords", "\n", "self", ".", "normalize_coords", "=", "normalize_coords", "\n", "self", ".", "background_id", "=", "background_id", "\n", "\n", "# Compute the number of boxes per spatial location for each predictor layer.", "\n", "# For example, if a predictor layer has three different aspect ratios, [1.0, 0.5, 2.0], and is", "\n", "# supposed to predict two boxes of slightly different size for aspect ratio 1.0, then that predictor", "\n", "# layer predicts a total of four boxes at every spatial location across the feature map.", "\n", "if", "not", "(", "aspect_ratios_per_layer", "is", "None", ")", ":", "\n", "            ", "self", ".", "n_boxes", "=", "[", "]", "\n", "for", "aspect_ratios", "in", "aspect_ratios_per_layer", ":", "\n", "                ", "if", "(", "1", "in", "aspect_ratios", ")", "&", "two_boxes_for_ar1", ":", "\n", "                    ", "self", ".", "n_boxes", ".", "append", "(", "len", "(", "aspect_ratios", ")", "+", "1", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "n_boxes", ".", "append", "(", "len", "(", "aspect_ratios", ")", ")", "\n", "", "", "", "else", ":", "\n", "            ", "if", "(", "1", "in", "aspect_ratios_global", ")", "&", "two_boxes_for_ar1", ":", "\n", "                ", "self", ".", "n_boxes", "=", "len", "(", "aspect_ratios_global", ")", "+", "1", "\n", "", "else", ":", "\n", "                ", "self", ".", "n_boxes", "=", "len", "(", "aspect_ratios_global", ")", "\n", "\n", "##################################################################################", "\n", "# Compute the anchor boxes for each predictor layer.", "\n", "##################################################################################", "\n", "\n", "# Compute the anchor boxes for each predictor layer. We only have to do this once", "\n", "# since the anchor boxes depend only on the model configuration, not on the input data.", "\n", "# For each predictor layer (i.e. for each scaling factor) the tensors for that layer's", "\n", "# anchor boxes will have the shape `(feature_map_height, feature_map_width, n_boxes, 4)`.", "\n", "\n", "", "", "self", ".", "boxes_list", "=", "[", "]", "# This will store the anchor boxes for each predicotr layer.", "\n", "\n", "# The following lists just store diagnostic information. Sometimes it's handy to have the", "\n", "# boxes' center points, heights, widths, etc. in a list.", "\n", "self", ".", "wh_list_diag", "=", "[", "]", "# Box widths and heights for each predictor layer", "\n", "self", ".", "steps_diag", "=", "[", "]", "# Horizontal and vertical distances between any two boxes for each predictor layer", "\n", "self", ".", "offsets_diag", "=", "[", "]", "# Offsets for each predictor layer", "\n", "self", ".", "centers_diag", "=", "[", "]", "# Anchor box center points as `(cy, cx)` for each predictor layer", "\n", "\n", "# Iterate over all predictor layers and compute the anchor boxes for each one.", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "predictor_sizes", ")", ")", ":", "\n", "            ", "boxes", ",", "center", ",", "wh", ",", "step", ",", "offset", "=", "self", ".", "generate_anchor_boxes_for_layer", "(", "feature_map_size", "=", "self", ".", "predictor_sizes", "[", "i", "]", ",", "\n", "aspect_ratios", "=", "self", ".", "aspect_ratios", "[", "i", "]", ",", "\n", "this_scale", "=", "self", ".", "scales", "[", "i", "]", ",", "\n", "next_scale", "=", "self", ".", "scales", "[", "i", "+", "1", "]", ",", "\n", "this_steps", "=", "self", ".", "steps", "[", "i", "]", ",", "\n", "this_offsets", "=", "self", ".", "offsets", "[", "i", "]", ",", "\n", "diagnostics", "=", "True", ")", "\n", "self", ".", "boxes_list", ".", "append", "(", "boxes", ")", "\n", "self", ".", "wh_list_diag", ".", "append", "(", "wh", ")", "\n", "self", ".", "steps_diag", ".", "append", "(", "step", ")", "\n", "self", ".", "offsets_diag", ".", "append", "(", "offset", ")", "\n", "self", ".", "centers_diag", ".", "append", "(", "center", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.ssd_input_encoder.SSDInputEncoder.__call__": [[277, 433], ["len", "ssd_input_encoder.SSDInputEncoder.generate_encoding_template", "numpy.eye", "range", "ground_truth_labels[].astype", "numpy.concatenate", "bounding_box_utils.bounding_box_utils.iou", "ssd_encoder_decoder.matching_utils.match_bipartite_greedy", "numpy.amax", "numpy.copy", "numpy.any", "numpy.any", "ssd_input_encoder.DegenerateBoxError", "bounding_box_utils.bounding_box_utils.convert_coordinates", "ssd_encoder_decoder.matching_utils.match_multi", "numpy.nonzero", "numpy.log", "numpy.expand_dims", "numpy.expand_dims", "bounding_box_utils.bounding_box_utils.convert_coordinates", "labels[].astype", "numpy.expand_dims", "numpy.expand_dims"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.ssd_input_encoder.SSDInputEncoder.generate_encoding_template", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.iou", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.matching_utils.match_bipartite_greedy", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.convert_coordinates", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.matching_utils.match_multi", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.convert_coordinates"], ["", "", "def", "__call__", "(", "self", ",", "ground_truth_labels", ",", "diagnostics", "=", "False", ")", ":", "\n", "        ", "'''\n        Converts ground truth bounding box data into a suitable format to train an SSD model.\n\n        Arguments:\n            ground_truth_labels (list): A python list of length `batch_size` that contains one 2D Numpy array\n                for each batch image. Each such array has `k` rows for the `k` ground truth bounding boxes belonging\n                to the respective image, and the data for each ground truth bounding box has the format\n                `(class_id, xmin, ymin, xmax, ymax)` (i.e. the 'corners' coordinate format), and `class_id` must be\n                an integer greater than 0 for all boxes as class ID 0 is reserved for the background class.\n\n            ground_truth_sample_weight (list): (batch_size, number of gt boxes) LLLLLLOOOOOONNNNNGGGGGG\n\n            diagnostics (bool, optional): If `True`, not only the encoded ground truth tensor will be returned,\n                but also a copy of it with anchor box coordinates in place of the ground truth coordinates.\n                This can be very useful if you want to visualize which anchor boxes got matched to which ground truth\n                boxes.\n\n        Returns:\n            `y_encoded`, a 3D numpy array of shape `(batch_size, #boxes, #classes + 4 + 4 + 4)` that serves as the\n            ground truth label tensor for training, where `#boxes` is the total number of boxes predicted by the\n            model per image, and the classes are one-hot-encoded. The four elements after the class vecotrs in\n            the last axis are the box coordinates, the next four elements after that are just dummy elements, and\n            the last four elements are the variances.\n        '''", "\n", "batch_size", "=", "len", "(", "ground_truth_labels", ")", "\n", "# sample_weights = []", "\n", "# for i in range(batch_size):", "\n", "#     sample_weight = ground_truth_labels[i]", "\n", "#     sample_weight = sample_weight[:, 0]", "\n", "#     sample_weights.append(sample_weight)", "\n", "# sample_weights = ground_truth_sample_weights", "\n", "\n", "# Mapping to define which indices represent which coordinates in the ground truth.", "\n", "class_id", "=", "1", "\n", "xmin", "=", "2", "\n", "ymin", "=", "3", "\n", "xmax", "=", "4", "\n", "ymax", "=", "5", "\n", "\n", "##################################################################################", "\n", "# Generate the template for y_encoded.", "\n", "##################################################################################", "\n", "\n", "\n", "y_encoded", "=", "self", ".", "generate_encoding_template", "(", "batch_size", "=", "batch_size", ",", "diagnostics", "=", "False", ")", "\n", "##################################################################################", "\n", "# Match ground truth boxes to anchor boxes.", "\n", "##################################################################################", "\n", "\n", "# Match the ground truth boxes to the anchor boxes. Every anchor box that does not have", "\n", "# a ground truth match and for which the maximal IoU overlap with any ground truth box is less", "\n", "# than or equal to `neg_iou_limit` will be a negative (background) box.", "\n", "\n", "# y_encoded[:, :, self.background_id] = 1 # All boxes are background boxes by default.", "\n", "y_encoded", "[", ":", ",", ":", ",", "1", "]", "=", "1", "\n", "n_boxes", "=", "y_encoded", ".", "shape", "[", "1", "]", "# The total number of boxes that the model predicts per batch item", "\n", "class_vectors", "=", "np", ".", "eye", "(", "self", ".", "n_classes", ")", "# An identity matrix that we'll use as one-hot class vectors", "\n", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "# For each batch item...", "\n", "\n", "            ", "if", "ground_truth_labels", "[", "i", "]", ".", "size", "==", "0", ":", "continue", "\n", "labels", "=", "ground_truth_labels", "[", "i", "]", ".", "astype", "(", "np", ".", "float", ")", "\n", "\n", "# Check for degenerate ground truth bounding boxes before attempting any computations.", "\n", "if", "np", ".", "any", "(", "labels", "[", ":", ",", "[", "xmax", "]", "]", "-", "labels", "[", ":", ",", "[", "xmin", "]", "]", "<=", "0", ")", "or", "np", ".", "any", "(", "labels", "[", ":", ",", "[", "ymax", "]", "]", "-", "labels", "[", ":", ",", "[", "ymin", "]", "]", "<=", "0", ")", ":", "\n", "                ", "raise", "DegenerateBoxError", "(", "\"SSDInputEncoder detected degenerate ground truth bounding boxes for batch item {} with bounding boxes {}, \"", ".", "format", "(", "i", ",", "labels", ")", "+", "\n", "\"i.e. bounding boxes where xmax <= xmin and/or ymax <= ymin. Degenerate ground truth \"", "+", "\n", "\"bounding boxes will lead to NaN errors during the training.\"", ")", "\n", "\n", "# Maybe normalize the box coordinates.", "\n", "", "if", "self", ".", "normalize_coords", ":", "\n", "                ", "labels", "[", ":", ",", "[", "ymin", ",", "ymax", "]", "]", "/=", "self", ".", "img_height", "# Normalize ymin and ymax relative to the image height", "\n", "labels", "[", ":", ",", "[", "xmin", ",", "xmax", "]", "]", "/=", "self", ".", "img_width", "# Normalize xmin and xmax relative to the image width", "\n", "\n", "# Maybe convert the box coordinate format.", "\n", "", "if", "self", ".", "coords", "==", "'centroids'", ":", "\n", "                ", "labels", "=", "convert_coordinates", "(", "labels", ",", "start_index", "=", "xmin", ",", "conversion", "=", "'corners2centroids'", ",", "border_pixels", "=", "self", ".", "border_pixels", ")", "\n", "", "elif", "self", ".", "coords", "==", "'minmax'", ":", "\n", "                ", "labels", "=", "convert_coordinates", "(", "labels", ",", "start_index", "=", "xmin", ",", "conversion", "=", "'corners2minmax'", ")", "\n", "\n", "", "classes_one_hot", "=", "class_vectors", "[", "labels", "[", ":", ",", "class_id", "]", ".", "astype", "(", "np", ".", "int", ")", "]", "# The one-hot class IDs for the ground truth boxes of this batch item", "\n", "labels_one_hot", "=", "np", ".", "concatenate", "(", "[", "classes_one_hot", ",", "labels", "[", ":", ",", "[", "xmin", ",", "ymin", ",", "xmax", ",", "ymax", "]", "]", "]", ",", "axis", "=", "-", "1", ")", "# The one-hot version of the labels for this batch item", "\n", "\n", "# Compute the IoU similarities between all anchor boxes and all ground truth boxes for this batch item.", "\n", "# This is a matrix of shape `(num_ground_truth_boxes, num_anchor_boxes)`.", "\n", "similarities", "=", "iou", "(", "labels", "[", ":", ",", "[", "xmin", ",", "ymin", ",", "xmax", ",", "ymax", "]", "]", ",", "y_encoded", "[", "i", ",", ":", ",", "-", "12", ":", "-", "8", "]", ",", "coords", "=", "self", ".", "coords", ",", "mode", "=", "'outer_product'", ",", "border_pixels", "=", "self", ".", "border_pixels", ")", "\n", "\n", "\n", "# First: Do bipartite matching, i.e. match each ground truth box to the one anchor box with the highest IoU.", "\n", "#        This ensures that each ground truth box will have at least one good match.", "\n", "\n", "# For each ground truth box, get the anchor box to match with it.", "\n", "bipartite_matches", "=", "match_bipartite_greedy", "(", "weight_matrix", "=", "similarities", ")", "\n", "# Write the ground truth data to the matched anchor boxes.", "\n", "y_encoded", "[", "i", ",", "bipartite_matches", ",", "-", "16", ":", "-", "8", "]", "=", "labels_one_hot", "\n", "# y_encoded[i, bipartite_matches, :-8] = labels_one_hot", "\n", "\n", "# sample_weight = np.array(sample_weights[i])", "\n", "# sample_weights_boxes[i, bipartite_matches] = sample_weight", "\n", "\n", "# Set the columns of the matched anchor boxes to zero to indicate that they were matched.", "\n", "similarities", "[", ":", ",", "bipartite_matches", "]", "=", "0", "\n", "# Second: Maybe do 'multi' matching, where each remaining anchor box will be matched to its most similar", "\n", "#         ground truth box with an IoU of at least `pos_iou_threshold`, or not matched if there is no", "\n", "#         such ground truth box.", "\n", "\n", "if", "self", ".", "matching_type", "==", "'multi'", ":", "\n", "# Get all matches that satisfy the IoU threshold.", "\n", "                ", "matches", "=", "match_multi", "(", "weight_matrix", "=", "similarities", ",", "threshold", "=", "self", ".", "pos_iou_threshold", ")", "\n", "# Write the ground truth data to the matched anchor boxes.", "\n", "y_encoded", "[", "i", ",", "matches", "[", "1", "]", ",", "-", "16", ":", "-", "8", "]", "=", "labels_one_hot", "[", "matches", "[", "0", "]", "]", "\n", "# y_encoded[i, matches[1], :-8] = labels_one_hot[matches[0]]", "\n", "# sample_weights_boxes[i, matches[1]] = sample_weight[matches[0]]", "\n", "\n", "# Set the columns of the matched anchor boxes to zero to indicate that they were matched.", "\n", "similarities", "[", ":", ",", "matches", "[", "1", "]", "]", "=", "0", "\n", "\n", "# Third: Now after the matching is done, all negative (background) anchor boxes that have", "\n", "#        an IoU of `neg_iou_limit` or more with any ground truth box will be set to netral,", "\n", "#        i.e. they will no longer be background boxes. These anchors are \"too close\" to a", "\n", "#        ground truth box to be valid background boxes.", "\n", "\n", "", "max_background_similarities", "=", "np", ".", "amax", "(", "similarities", ",", "axis", "=", "0", ")", "\n", "neutral_boxes", "=", "np", ".", "nonzero", "(", "max_background_similarities", ">=", "self", ".", "neg_iou_limit", ")", "[", "0", "]", "\n", "y_encoded", "[", "i", ",", "neutral_boxes", ",", "1", "]", "=", "0", "\n", "# y_encoded[i, neutral_boxes, self.background_id] = 0", "\n", "\n", "\n", "##################################################################################", "\n", "# Convert box coordinates to anchor box offsets.", "\n", "##################################################################################", "\n", "\n", "", "if", "self", ".", "coords", "==", "'centroids'", ":", "\n", "            ", "y_encoded", "[", ":", ",", ":", ",", "[", "-", "12", ",", "-", "11", "]", "]", "-=", "y_encoded", "[", ":", ",", ":", ",", "[", "-", "8", ",", "-", "7", "]", "]", "# cx(gt) - cx(anchor), cy(gt) - cy(anchor)", "\n", "y_encoded", "[", ":", ",", ":", ",", "[", "-", "12", ",", "-", "11", "]", "]", "/=", "y_encoded", "[", ":", ",", ":", ",", "[", "-", "6", ",", "-", "5", "]", "]", "*", "y_encoded", "[", ":", ",", ":", ",", "[", "-", "4", ",", "-", "3", "]", "]", "# (cx(gt) - cx(anchor)) / w(anchor) / cx_variance, (cy(gt) - cy(anchor)) / h(anchor) / cy_variance", "\n", "y_encoded", "[", ":", ",", ":", ",", "[", "-", "10", ",", "-", "9", "]", "]", "/=", "y_encoded", "[", ":", ",", ":", ",", "[", "-", "6", ",", "-", "5", "]", "]", "# w(gt) / w(anchor), h(gt) / h(anchor)", "\n", "y_encoded", "[", ":", ",", ":", ",", "[", "-", "10", ",", "-", "9", "]", "]", "=", "np", ".", "log", "(", "y_encoded", "[", ":", ",", ":", ",", "[", "-", "10", ",", "-", "9", "]", "]", ")", "/", "y_encoded", "[", ":", ",", ":", ",", "[", "-", "2", ",", "-", "1", "]", "]", "# ln(w(gt) / w(anchor)) / w_variance, ln(h(gt) / h(anchor)) / h_variance (ln == natural logarithm)", "\n", "", "elif", "self", ".", "coords", "==", "'corners'", ":", "\n", "            ", "y_encoded", "[", ":", ",", ":", ",", "-", "12", ":", "-", "8", "]", "-=", "y_encoded", "[", ":", ",", ":", ",", "-", "8", ":", "-", "4", "]", "# (gt - anchor) for all four coordinates", "\n", "y_encoded", "[", ":", ",", ":", ",", "[", "-", "12", ",", "-", "10", "]", "]", "/=", "np", ".", "expand_dims", "(", "y_encoded", "[", ":", ",", ":", ",", "-", "6", "]", "-", "y_encoded", "[", ":", ",", ":", ",", "-", "8", "]", ",", "axis", "=", "-", "1", ")", "# (xmin(gt) - xmin(anchor)) / w(anchor), (xmax(gt) - xmax(anchor)) / w(anchor)", "\n", "y_encoded", "[", ":", ",", ":", ",", "[", "-", "11", ",", "-", "9", "]", "]", "/=", "np", ".", "expand_dims", "(", "y_encoded", "[", ":", ",", ":", ",", "-", "5", "]", "-", "y_encoded", "[", ":", ",", ":", ",", "-", "7", "]", ",", "axis", "=", "-", "1", ")", "# (ymin(gt) - ymin(anchor)) / h(anchor), (ymax(gt) - ymax(anchor)) / h(anchor)", "\n", "y_encoded", "[", ":", ",", ":", ",", "-", "12", ":", "-", "8", "]", "/=", "y_encoded", "[", ":", ",", ":", ",", "-", "4", ":", "]", "# (gt - anchor) / size(anchor) / variance for all four coordinates, where 'size' refers to w and h respectively", "\n", "", "elif", "self", ".", "coords", "==", "'minmax'", ":", "\n", "            ", "y_encoded", "[", ":", ",", ":", ",", "-", "12", ":", "-", "8", "]", "-=", "y_encoded", "[", ":", ",", ":", ",", "-", "8", ":", "-", "4", "]", "# (gt - anchor) for all four coordinates", "\n", "y_encoded", "[", ":", ",", ":", ",", "[", "-", "12", ",", "-", "11", "]", "]", "/=", "np", ".", "expand_dims", "(", "y_encoded", "[", ":", ",", ":", ",", "-", "7", "]", "-", "y_encoded", "[", ":", ",", ":", ",", "-", "8", "]", ",", "axis", "=", "-", "1", ")", "# (xmin(gt) - xmin(anchor)) / w(anchor), (xmax(gt) - xmax(anchor)) / w(anchor)", "\n", "y_encoded", "[", ":", ",", ":", ",", "[", "-", "10", ",", "-", "9", "]", "]", "/=", "np", ".", "expand_dims", "(", "y_encoded", "[", ":", ",", ":", ",", "-", "5", "]", "-", "y_encoded", "[", ":", ",", ":", ",", "-", "6", "]", ",", "axis", "=", "-", "1", ")", "# (ymin(gt) - ymin(anchor)) / h(anchor), (ymax(gt) - ymax(anchor)) / h(anchor)", "\n", "y_encoded", "[", ":", ",", ":", ",", "-", "12", ":", "-", "8", "]", "/=", "y_encoded", "[", ":", ",", ":", ",", "-", "4", ":", "]", "# (gt - anchor) / size(anchor) / variance for all four coordinates, where 'size' refers to w and h respectively", "\n", "\n", "", "if", "diagnostics", ":", "\n", "# Here we'll save the matched anchor boxes (i.e. anchor boxes that were matched to a ground truth box, but keeping the anchor box coordinates).", "\n", "            ", "y_matched_anchors", "=", "np", ".", "copy", "(", "y_encoded", ")", "\n", "y_matched_anchors", "[", ":", ",", ":", ",", "-", "12", ":", "-", "8", "]", "=", "0", "# Keeping the anchor box coordinates means setting the offsets to zero.", "\n", "return", "y_encoded", ",", "y_matched_anchors", "\n", "", "else", ":", "\n", "            ", "return", "y_encoded", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.ssd_input_encoder.SSDInputEncoder.generate_anchor_boxes_for_layer": [[434, 563], ["min", "numpy.array", "len", "numpy.linspace", "numpy.linspace", "numpy.meshgrid", "numpy.expand_dims", "numpy.expand_dims", "numpy.zeros", "numpy.tile", "numpy.tile", "bounding_box_utils.bounding_box_utils.convert_coordinates", "bounding_box_utils.bounding_box_utils.convert_coordinates", "numpy.array.append", "numpy.array.append", "isinstance", "isinstance", "isinstance", "isinstance", "bounding_box_utils.bounding_box_utils.convert_coordinates", "numpy.array.append", "numpy.sqrt", "numpy.sqrt", "len", "len", "numpy.sqrt"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.convert_coordinates", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.convert_coordinates", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.convert_coordinates"], ["", "", "def", "generate_anchor_boxes_for_layer", "(", "self", ",", "\n", "feature_map_size", ",", "\n", "aspect_ratios", ",", "\n", "this_scale", ",", "\n", "next_scale", ",", "\n", "this_steps", "=", "None", ",", "\n", "this_offsets", "=", "None", ",", "\n", "diagnostics", "=", "False", ")", ":", "\n", "        ", "'''\n        Computes an array of the spatial positions and sizes of the anchor boxes for one predictor layer\n        of size `feature_map_size == [feature_map_height, feature_map_width]`.\n\n        Arguments:\n            feature_map_size (tuple): A list or tuple `[feature_map_height, feature_map_width]` with the spatial\n                dimensions of the feature map for which to generate the anchor boxes.\n            aspect_ratios (list): A list of floats, the aspect ratios for which anchor boxes are to be generated.\n                All list elements must be unique.\n            this_scale (float): A float in [0, 1], the scaling factor for the size of the generate anchor boxes\n                as a fraction of the shorter side of the input image.\n            next_scale (float): A float in [0, 1], the next larger scaling factor. Only relevant if\n                `self.two_boxes_for_ar1 == True`.\n            diagnostics (bool, optional): If true, the following additional outputs will be returned:\n                1) A list of the center point `x` and `y` coordinates for each spatial location.\n                2) A list containing `(width, height)` for each box aspect ratio.\n                3) A tuple containing `(step_height, step_width)`\n                4) A tuple containing `(offset_height, offset_width)`\n                This information can be useful to understand in just a few numbers what the generated grid of\n                anchor boxes actually looks like, i.e. how large the different boxes are and how dense\n                their spatial distribution is, in order to determine whether the box grid covers the input images\n                appropriately and whether the box sizes are appropriate to fit the sizes of the objects\n                to be detected.\n\n        Returns:\n            A 4D Numpy tensor of shape `(feature_map_height, feature_map_width, n_boxes_per_cell, 4)` where the\n            last dimension contains `(xmin, xmax, ymin, ymax)` for each anchor box in each cell of the feature map.\n        '''", "\n", "# Compute box width and height for each aspect ratio.", "\n", "\n", "# The shorter side of the image will be used to compute `w` and `h` using `scale` and `aspect_ratios`.", "\n", "size", "=", "min", "(", "self", ".", "img_height", ",", "self", ".", "img_width", ")", "\n", "# Compute the box widths and and heights for all aspect ratios", "\n", "wh_list", "=", "[", "]", "\n", "for", "ar", "in", "aspect_ratios", ":", "\n", "            ", "if", "(", "ar", "==", "1", ")", ":", "\n", "# Compute the regular anchor box for aspect ratio 1.", "\n", "                ", "box_height", "=", "box_width", "=", "this_scale", "*", "size", "\n", "wh_list", ".", "append", "(", "(", "box_width", ",", "box_height", ")", ")", "\n", "if", "self", ".", "two_boxes_for_ar1", ":", "\n", "# Compute one slightly larger version using the geometric mean of this scale value and the next.", "\n", "                    ", "box_height", "=", "box_width", "=", "np", ".", "sqrt", "(", "this_scale", "*", "next_scale", ")", "*", "size", "\n", "wh_list", ".", "append", "(", "(", "box_width", ",", "box_height", ")", ")", "\n", "", "", "else", ":", "\n", "                ", "box_width", "=", "this_scale", "*", "size", "*", "np", ".", "sqrt", "(", "ar", ")", "\n", "box_height", "=", "this_scale", "*", "size", "/", "np", ".", "sqrt", "(", "ar", ")", "\n", "wh_list", ".", "append", "(", "(", "box_width", ",", "box_height", ")", ")", "\n", "", "", "wh_list", "=", "np", ".", "array", "(", "wh_list", ")", "\n", "n_boxes", "=", "len", "(", "wh_list", ")", "\n", "\n", "# Compute the grid of box center points. They are identical for all aspect ratios.", "\n", "\n", "# Compute the step sizes, i.e. how far apart the anchor box center points will be vertically and horizontally.", "\n", "if", "(", "this_steps", "is", "None", ")", ":", "\n", "            ", "step_height", "=", "self", ".", "img_height", "/", "feature_map_size", "[", "0", "]", "\n", "step_width", "=", "self", ".", "img_width", "/", "feature_map_size", "[", "1", "]", "\n", "", "else", ":", "\n", "            ", "if", "isinstance", "(", "this_steps", ",", "(", "list", ",", "tuple", ")", ")", "and", "(", "len", "(", "this_steps", ")", "==", "2", ")", ":", "\n", "                ", "step_height", "=", "this_steps", "[", "0", "]", "\n", "step_width", "=", "this_steps", "[", "1", "]", "\n", "", "elif", "isinstance", "(", "this_steps", ",", "(", "int", ",", "float", ")", ")", ":", "\n", "                ", "step_height", "=", "this_steps", "\n", "step_width", "=", "this_steps", "\n", "# Compute the offsets, i.e. at what pixel values the first anchor box center point will be from the top and from the left of the image.", "\n", "", "", "if", "(", "this_offsets", "is", "None", ")", ":", "\n", "            ", "offset_height", "=", "0.5", "\n", "offset_width", "=", "0.5", "\n", "", "else", ":", "\n", "            ", "if", "isinstance", "(", "this_offsets", ",", "(", "list", ",", "tuple", ")", ")", "and", "(", "len", "(", "this_offsets", ")", "==", "2", ")", ":", "\n", "                ", "offset_height", "=", "this_offsets", "[", "0", "]", "\n", "offset_width", "=", "this_offsets", "[", "1", "]", "\n", "", "elif", "isinstance", "(", "this_offsets", ",", "(", "int", ",", "float", ")", ")", ":", "\n", "                ", "offset_height", "=", "this_offsets", "\n", "offset_width", "=", "this_offsets", "\n", "# Now that we have the offsets and step sizes, compute the grid of anchor box center points.", "\n", "", "", "cy", "=", "np", ".", "linspace", "(", "offset_height", "*", "step_height", ",", "(", "offset_height", "+", "feature_map_size", "[", "0", "]", "-", "1", ")", "*", "step_height", ",", "feature_map_size", "[", "0", "]", ")", "\n", "cx", "=", "np", ".", "linspace", "(", "offset_width", "*", "step_width", ",", "(", "offset_width", "+", "feature_map_size", "[", "1", "]", "-", "1", ")", "*", "step_width", ",", "feature_map_size", "[", "1", "]", ")", "\n", "cx_grid", ",", "cy_grid", "=", "np", ".", "meshgrid", "(", "cx", ",", "cy", ")", "\n", "cx_grid", "=", "np", ".", "expand_dims", "(", "cx_grid", ",", "-", "1", ")", "# This is necessary for np.tile() to do what we want further down", "\n", "cy_grid", "=", "np", ".", "expand_dims", "(", "cy_grid", ",", "-", "1", ")", "# This is necessary for np.tile() to do what we want further down", "\n", "\n", "# Create a 4D tensor template of shape `(feature_map_height, feature_map_width, n_boxes, 4)`", "\n", "# where the last dimension will contain `(cx, cy, w, h)`", "\n", "boxes_tensor", "=", "np", ".", "zeros", "(", "(", "feature_map_size", "[", "0", "]", ",", "feature_map_size", "[", "1", "]", ",", "n_boxes", ",", "4", ")", ")", "\n", "\n", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "0", "]", "=", "np", ".", "tile", "(", "cx_grid", ",", "(", "1", ",", "1", ",", "n_boxes", ")", ")", "# Set cx", "\n", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "1", "]", "=", "np", ".", "tile", "(", "cy_grid", ",", "(", "1", ",", "1", ",", "n_boxes", ")", ")", "# Set cy", "\n", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "2", "]", "=", "wh_list", "[", ":", ",", "0", "]", "# Set w", "\n", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "3", "]", "=", "wh_list", "[", ":", ",", "1", "]", "# Set h", "\n", "\n", "# Convert `(cx, cy, w, h)` to `(xmin, ymin, xmax, ymax)`", "\n", "boxes_tensor", "=", "convert_coordinates", "(", "boxes_tensor", ",", "start_index", "=", "0", ",", "conversion", "=", "'centroids2corners'", ")", "\n", "\n", "# If `clip_boxes` is enabled, clip the coordinates to lie within the image boundaries", "\n", "if", "self", ".", "clip_boxes", ":", "\n", "            ", "x_coords", "=", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "[", "0", ",", "2", "]", "]", "\n", "x_coords", "[", "x_coords", ">=", "self", ".", "img_width", "]", "=", "self", ".", "img_width", "-", "1", "\n", "x_coords", "[", "x_coords", "<", "0", "]", "=", "0", "\n", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "[", "0", ",", "2", "]", "]", "=", "x_coords", "\n", "y_coords", "=", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "[", "1", ",", "3", "]", "]", "\n", "y_coords", "[", "y_coords", ">=", "self", ".", "img_height", "]", "=", "self", ".", "img_height", "-", "1", "\n", "y_coords", "[", "y_coords", "<", "0", "]", "=", "0", "\n", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "[", "1", ",", "3", "]", "]", "=", "y_coords", "\n", "\n", "# `normalize_coords` is enabled, normalize the coordinates to be within [0,1]", "\n", "", "if", "self", ".", "normalize_coords", ":", "\n", "            ", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "[", "0", ",", "2", "]", "]", "/=", "self", ".", "img_width", "\n", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "[", "1", ",", "3", "]", "]", "/=", "self", ".", "img_height", "\n", "\n", "# TODO: Implement box limiting directly for `(cx, cy, w, h)` so that we don't have to unnecessarily convert back and forth.", "\n", "", "if", "self", ".", "coords", "==", "'centroids'", ":", "\n", "# Convert `(xmin, ymin, xmax, ymax)` back to `(cx, cy, w, h)`.", "\n", "            ", "boxes_tensor", "=", "convert_coordinates", "(", "boxes_tensor", ",", "start_index", "=", "0", ",", "conversion", "=", "'corners2centroids'", ",", "border_pixels", "=", "'half'", ")", "\n", "", "elif", "self", ".", "coords", "==", "'minmax'", ":", "\n", "# Convert `(xmin, ymin, xmax, ymax)` to `(xmin, xmax, ymin, ymax).", "\n", "            ", "boxes_tensor", "=", "convert_coordinates", "(", "boxes_tensor", ",", "start_index", "=", "0", ",", "conversion", "=", "'corners2minmax'", ",", "border_pixels", "=", "'half'", ")", "\n", "\n", "", "if", "diagnostics", ":", "\n", "            ", "return", "boxes_tensor", ",", "(", "cy", ",", "cx", ")", ",", "wh_list", ",", "(", "step_height", ",", "step_width", ")", ",", "(", "offset_height", ",", "offset_width", ")", "\n", "", "else", ":", "\n", "            ", "return", "boxes_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.ssd_input_encoder.SSDInputEncoder.generate_encoding_template": [[564, 627], ["numpy.concatenate", "numpy.zeros", "numpy.zeros_like", "numpy.ones", "numpy.concatenate", "numpy.expand_dims", "numpy.tile", "numpy.reshape", "boxes_batch.append"], "methods", ["None"], ["", "", "def", "generate_encoding_template", "(", "self", ",", "batch_size", ",", "diagnostics", "=", "False", ")", ":", "\n", "        ", "'''\n        Produces an encoding template for the ground truth label tensor for a given batch.\n\n        Note that all tensor creation, reshaping and concatenation operations performed in this function\n        and the sub-functions it calls are identical to those performed inside the SSD model. This, of course,\n        must be the case in order to preserve the spatial meaning of each box prediction, but it's useful to make\n        yourself aware of this fact and why it is necessary.\n\n        In other words, the boxes in `y_encoded` must have a specific order in order correspond to the right spatial\n        positions and scales of the boxes predicted by the model. The sequence of operations here ensures that `y_encoded`\n        has this specific form.\n\n        Arguments:\n            batch_size (int): The batch size.\n            diagnostics (bool, optional): See the documnentation for `generate_anchor_boxes()`. The diagnostic output\n                here is similar, just for all predictor conv layers.\n\n        Returns:\n            A Numpy array of shape `(batch_size, #boxes, #classes + 12)`, the template into which to encode\n            the ground truth labels for training. The last axis has length `#classes + 12` because the model\n            output contains not only the 4 predicted box coordinate offsets, but also the 4 coordinates for\n            the anchor boxes and the 4 variance values.\n        '''", "\n", "# Tile the anchor boxes for each predictor layer across all batch items.", "\n", "boxes_batch", "=", "[", "]", "\n", "for", "boxes", "in", "self", ".", "boxes_list", ":", "\n", "# Prepend one dimension to `self.boxes_list` to account for the batch size and tile it along.", "\n", "# The result will be a 5D tensor of shape `(batch_size, feature_map_height, feature_map_width, n_boxes, 4)`", "\n", "            ", "boxes", "=", "np", ".", "expand_dims", "(", "boxes", ",", "axis", "=", "0", ")", "\n", "boxes", "=", "np", ".", "tile", "(", "boxes", ",", "(", "batch_size", ",", "1", ",", "1", ",", "1", ",", "1", ")", ")", "\n", "\n", "# Now reshape the 5D tensor above into a 3D tensor of shape", "\n", "# `(batch, feature_map_height * feature_map_width * n_boxes, 4)`. The resulting", "\n", "# order of the tensor content will be identical to the order obtained from the reshaping operation", "\n", "# in our Keras model (we're using the Tensorflow backend, and tf.reshape() and np.reshape()", "\n", "# use the same default index order, which is C-like index ordering)", "\n", "boxes", "=", "np", ".", "reshape", "(", "boxes", ",", "(", "batch_size", ",", "-", "1", ",", "4", ")", ")", "\n", "boxes_batch", ".", "append", "(", "boxes", ")", "\n", "\n", "# Concatenate the anchor tensors from the individual layers to one.", "\n", "", "boxes_tensor", "=", "np", ".", "concatenate", "(", "boxes_batch", ",", "axis", "=", "1", ")", "\n", "\n", "# 3: Create a template tensor to hold the one-hot class encodings of shape `(batch, #boxes, #classes)`", "\n", "#    It will contain all zeros for now, the classes will be set in the matching process that follows", "\n", "classes_tensor", "=", "np", ".", "zeros", "(", "(", "batch_size", ",", "boxes_tensor", ".", "shape", "[", "1", "]", ",", "self", ".", "n_classes", ")", ")", "\n", "\n", "# 4: Create a tensor to contain the variances. This tensor has the same shape as `boxes_tensor` and simply", "\n", "#    contains the same 4 variance values for every position in the last axis.", "\n", "variances_tensor", "=", "np", ".", "zeros_like", "(", "boxes_tensor", ")", "\n", "variances_tensor", "+=", "self", ".", "variances", "# Long live broadcasting", "\n", "\n", "# 4: Concatenate the classes, boxes and variances tensors to get our final template for y_encoded. We also need", "\n", "#    another tensor of the shape of `boxes_tensor` as a space filler so that `y_encoding_template` has the same", "\n", "#    shape as the SSD model output tensor. The content of this tensor is irrelevant, we'll just use", "\n", "#    `boxes_tensor` a second time.", "\n", "sample_weights", "=", "np", ".", "ones", "(", "[", "classes_tensor", ".", "shape", "[", "0", "]", ",", "classes_tensor", ".", "shape", "[", "1", "]", ",", "1", "]", ")", "\n", "y_encoding_template", "=", "np", ".", "concatenate", "(", "(", "sample_weights", ",", "classes_tensor", ",", "boxes_tensor", ",", "boxes_tensor", ",", "variances_tensor", ")", ",", "axis", "=", "2", ")", "\n", "\n", "if", "diagnostics", ":", "\n", "            ", "return", "y_encoding_template", ",", "self", ".", "centers_diag", ",", "self", ".", "wh_list_diag", ",", "self", ".", "steps_diag", ",", "self", ".", "offsets_diag", "\n", "", "else", ":", "\n", "            ", "return", "y_encoding_template", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_L2Normalization.L2Normalization.__init__": [[46, 53], ["keras.engine.topology.Layer.__init__", "keras.image_dim_ordering"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_train.Evaluator.__init__"], ["def", "__init__", "(", "self", ",", "gamma_init", "=", "20", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "K", ".", "image_dim_ordering", "(", ")", "==", "'tf'", ":", "\n", "            ", "self", ".", "axis", "=", "3", "\n", "", "else", ":", "\n", "            ", "self", ".", "axis", "=", "1", "\n", "", "self", ".", "gamma_init", "=", "gamma_init", "\n", "super", "(", "L2Normalization", ",", "self", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_L2Normalization.L2Normalization.build": [[54, 60], ["keras.variable", "super().build", "keras.engine.topology.InputSpec", "numpy.ones"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DeepAnchorBoxes.AnchorBoxes.build"], ["", "def", "build", "(", "self", ",", "input_shape", ")", ":", "\n", "        ", "self", ".", "input_spec", "=", "[", "InputSpec", "(", "shape", "=", "input_shape", ")", "]", "\n", "gamma", "=", "self", ".", "gamma_init", "*", "np", ".", "ones", "(", "(", "input_shape", "[", "self", ".", "axis", "]", ",", ")", ")", "\n", "self", ".", "gamma", "=", "K", ".", "variable", "(", "gamma", ",", "name", "=", "'{}_gamma'", ".", "format", "(", "self", ".", "name", ")", ")", "\n", "self", ".", "trainable_weights", "=", "[", "self", ".", "gamma", "]", "\n", "super", "(", "L2Normalization", ",", "self", ")", ".", "build", "(", "input_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_L2Normalization.L2Normalization.call": [[61, 64], ["keras.l2_normalize"], "methods", ["None"], ["", "def", "call", "(", "self", ",", "x", ",", "mask", "=", "None", ")", ":", "\n", "        ", "output", "=", "K", ".", "l2_normalize", "(", "x", ",", "self", ".", "axis", ")", "\n", "return", "output", "*", "self", ".", "gamma", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_L2Normalization.L2Normalization.get_config": [[65, 71], ["super().get_config", "dict", "list", "list", "super().get_config.items", "config.items"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DeepAnchorBoxes.AnchorBoxes.get_config"], ["", "def", "get_config", "(", "self", ")", ":", "\n", "        ", "config", "=", "{", "\n", "'gamma_init'", ":", "self", ".", "gamma_init", "\n", "}", "\n", "base_config", "=", "super", "(", "L2Normalization", ",", "self", ")", ".", "get_config", "(", ")", "\n", "return", "dict", "(", "list", "(", "base_config", ".", "items", "(", ")", ")", "+", "list", "(", "config", ".", "items", "(", ")", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_AnchorBoxes.AnchorBoxes.__init__": [[44, 114], ["numpy.array", "numpy.any", "keras.engine.topology.Layer.__init__", "keras.backend", "TypeError", "ValueError", "len", "ValueError", "ValueError", "len", "len", "keras.backend", "len"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_train.Evaluator.__init__"], ["def", "__init__", "(", "self", ",", "\n", "img_height", ",", "\n", "img_width", ",", "\n", "this_scale", ",", "\n", "next_scale", ",", "\n", "aspect_ratios", "=", "[", "0.5", ",", "1.0", ",", "2.0", "]", ",", "\n", "two_boxes_for_ar1", "=", "True", ",", "\n", "this_steps", "=", "None", ",", "\n", "this_offsets", "=", "None", ",", "\n", "clip_boxes", "=", "False", ",", "\n", "variances", "=", "[", "0.1", ",", "0.1", ",", "0.2", ",", "0.2", "]", ",", "\n", "coords", "=", "'centroids'", ",", "\n", "normalize_coords", "=", "False", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "'''\n        All arguments need to be set to the same values as in the box encoding process, otherwise the behavior is undefined.\n        Some of these arguments are explained in more detail in the documentation of the `SSDBoxEncoder` class.\n\n        Arguments:\n            img_height (int): The height of the input images.\n            img_width (int): The width of the input images.\n            this_scale (float): A float in [0, 1], the scaling factor for the size of the generated anchor boxes\n                as a fraction of the shorter side of the input image.\n            next_scale (float): A float in [0, 1], the next larger scaling factor. Only relevant if\n                `self.two_boxes_for_ar1 == True`.\n            aspect_ratios (list, optional): The list of aspect ratios for which default boxes are to be\n                generated for this layer.\n            two_boxes_for_ar1 (bool, optional): Only relevant if `aspect_ratios` contains 1.\n                If `True`, two default boxes will be generated for aspect ratio 1. The first will be generated\n                using the scaling factor for the respective layer, the second one will be generated using\n                geometric mean of said scaling factor and next bigger scaling factor.\n            clip_boxes (bool, optional): If `True`, clips the anchor box coordinates to stay within image boundaries.\n            variances (list, optional): A list of 4 floats >0. The anchor box offset for each coordinate will be divided by\n                its respective variance value.\n            coords (str, optional): The box coordinate format to be used internally in the model (i.e. this is not the input format\n                of the ground truth labels). Can be either 'centroids' for the format `(cx, cy, w, h)` (box center coordinates, width, and height),\n                'corners' for the format `(xmin, ymin, xmax,  ymax)`, or 'minmax' for the format `(xmin, xmax, ymin, ymax)`.\n            normalize_coords (bool, optional): Set to `True` if the model uses relative instead of absolute coordinates,\n                i.e. if the model predicts box coordinates within [0,1] instead of absolute coordinates.\n        '''", "\n", "if", "K", ".", "backend", "(", ")", "!=", "'tensorflow'", ":", "\n", "            ", "raise", "TypeError", "(", "\"This layer only supports TensorFlow at the moment, but you are using the {} backend.\"", ".", "format", "(", "K", ".", "backend", "(", ")", ")", ")", "\n", "\n", "", "if", "(", "this_scale", "<", "0", ")", "or", "(", "next_scale", "<", "0", ")", "or", "(", "this_scale", ">", "1", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`this_scale` must be in [0, 1] and `next_scale` must be >0, but `this_scale` == {}, `next_scale` == {}\"", ".", "format", "(", "this_scale", ",", "next_scale", ")", ")", "\n", "\n", "", "if", "len", "(", "variances", ")", "!=", "4", ":", "\n", "            ", "raise", "ValueError", "(", "\"4 variance values must be pased, but {} values were received.\"", ".", "format", "(", "len", "(", "variances", ")", ")", ")", "\n", "", "variances", "=", "np", ".", "array", "(", "variances", ")", "\n", "if", "np", ".", "any", "(", "variances", "<=", "0", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"All variances must be >0, but the variances given are {}\"", ".", "format", "(", "variances", ")", ")", "\n", "\n", "", "self", ".", "img_height", "=", "img_height", "\n", "self", ".", "img_width", "=", "img_width", "\n", "self", ".", "this_scale", "=", "this_scale", "\n", "self", ".", "next_scale", "=", "next_scale", "\n", "self", ".", "aspect_ratios", "=", "aspect_ratios", "\n", "self", ".", "two_boxes_for_ar1", "=", "two_boxes_for_ar1", "\n", "self", ".", "this_steps", "=", "this_steps", "\n", "self", ".", "this_offsets", "=", "this_offsets", "\n", "self", ".", "clip_boxes", "=", "clip_boxes", "\n", "self", ".", "variances", "=", "variances", "\n", "self", ".", "coords", "=", "coords", "\n", "self", ".", "normalize_coords", "=", "normalize_coords", "\n", "# Compute the number of boxes per cell", "\n", "if", "(", "1", "in", "aspect_ratios", ")", "and", "two_boxes_for_ar1", ":", "\n", "            ", "self", ".", "n_boxes", "=", "len", "(", "aspect_ratios", ")", "+", "1", "\n", "", "else", ":", "\n", "            ", "self", ".", "n_boxes", "=", "len", "(", "aspect_ratios", ")", "\n", "", "super", "(", "AnchorBoxes", ",", "self", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_AnchorBoxes.AnchorBoxes.build": [[115, 118], ["super().build", "keras.engine.topology.InputSpec"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DeepAnchorBoxes.AnchorBoxes.build"], ["", "def", "build", "(", "self", ",", "input_shape", ")", ":", "\n", "        ", "self", ".", "input_spec", "=", "[", "InputSpec", "(", "shape", "=", "input_shape", ")", "]", "\n", "super", "(", "AnchorBoxes", ",", "self", ")", ".", "build", "(", "input_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_AnchorBoxes.AnchorBoxes.call": [[119, 242], ["min", "numpy.array", "numpy.linspace", "numpy.linspace", "numpy.meshgrid", "numpy.expand_dims", "numpy.expand_dims", "numpy.zeros", "numpy.tile", "numpy.tile", "bounding_box_utils.bounding_box_utils.convert_coordinates", "numpy.zeros_like", "numpy.concatenate", "numpy.expand_dims", "keras.tile", "keras.image_dim_ordering", "bounding_box_utils.bounding_box_utils.convert_coordinates", "keras.constant", "numpy.array.append", "numpy.array.append", "isinstance", "isinstance", "isinstance", "isinstance", "bounding_box_utils.bounding_box_utils.convert_coordinates", "numpy.array.append", "numpy.sqrt", "numpy.sqrt", "len", "len", "keras.shape", "numpy.sqrt"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.convert_coordinates", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.convert_coordinates", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.convert_coordinates"], ["", "def", "call", "(", "self", ",", "x", ",", "mask", "=", "None", ")", ":", "\n", "        ", "'''\n        Return an anchor box tensor based on the shape of the input tensor.\n\n        The logic implemented here is identical to the logic in the module `ssd_box_encode_decode_utils.py`.\n\n        Note that this tensor does not participate in any graph computations at runtime. It is being created\n        as a constant once during graph creation and is just being output along with the rest of the model output\n        during runtime. Because of this, all logic is implemented as Numpy array operations and it is sufficient\n        to convert the resulting Numpy array into a Keras tensor at the very end before outputting it.\n\n        Arguments:\n            x (tensor): 4D tensor of shape `(batch, channels, height, width)` if `dim_ordering = 'th'`\n                or `(batch, height, width, channels)` if `dim_ordering = 'tf'`. The input for this\n                layer must be the output of the localization predictor layer.\n        '''", "\n", "\n", "# Compute box width and height for each aspect ratio", "\n", "# The shorter side of the image will be used to compute `w` and `h` using `scale` and `aspect_ratios`.", "\n", "size", "=", "min", "(", "self", ".", "img_height", ",", "self", ".", "img_width", ")", "\n", "# Compute the box widths and and heights for all aspect ratios", "\n", "wh_list", "=", "[", "]", "\n", "for", "ar", "in", "self", ".", "aspect_ratios", ":", "\n", "            ", "if", "(", "ar", "==", "1", ")", ":", "\n", "# Compute the regular anchor box for aspect ratio 1.", "\n", "                ", "box_height", "=", "box_width", "=", "self", ".", "this_scale", "*", "size", "\n", "wh_list", ".", "append", "(", "(", "box_width", ",", "box_height", ")", ")", "\n", "if", "self", ".", "two_boxes_for_ar1", ":", "\n", "# Compute one slightly larger version using the geometric mean of this scale value and the next.", "\n", "                    ", "box_height", "=", "box_width", "=", "np", ".", "sqrt", "(", "self", ".", "this_scale", "*", "self", ".", "next_scale", ")", "*", "size", "\n", "wh_list", ".", "append", "(", "(", "box_width", ",", "box_height", ")", ")", "\n", "", "", "else", ":", "\n", "                ", "box_height", "=", "self", ".", "this_scale", "*", "size", "/", "np", ".", "sqrt", "(", "ar", ")", "\n", "box_width", "=", "self", ".", "this_scale", "*", "size", "*", "np", ".", "sqrt", "(", "ar", ")", "\n", "wh_list", ".", "append", "(", "(", "box_width", ",", "box_height", ")", ")", "\n", "", "", "wh_list", "=", "np", ".", "array", "(", "wh_list", ")", "\n", "\n", "# We need the shape of the input tensor", "\n", "if", "K", ".", "image_dim_ordering", "(", ")", "==", "'tf'", ":", "\n", "            ", "batch_size", ",", "feature_map_height", ",", "feature_map_width", ",", "feature_map_channels", "=", "x", ".", "_keras_shape", "\n", "", "else", ":", "# Not yet relevant since TensorFlow is the only supported backend right now, but it can't harm to have this in here for the future", "\n", "            ", "batch_size", ",", "feature_map_channels", ",", "feature_map_height", ",", "feature_map_width", "=", "x", ".", "_keras_shape", "\n", "\n", "# Compute the grid of box center points. They are identical for all aspect ratios.", "\n", "\n", "# Compute the step sizes, i.e. how far apart the anchor box center points will be vertically and horizontally.", "\n", "", "if", "(", "self", ".", "this_steps", "is", "None", ")", ":", "\n", "            ", "step_height", "=", "self", ".", "img_height", "/", "feature_map_height", "\n", "step_width", "=", "self", ".", "img_width", "/", "feature_map_width", "\n", "", "else", ":", "\n", "            ", "if", "isinstance", "(", "self", ".", "this_steps", ",", "(", "list", ",", "tuple", ")", ")", "and", "(", "len", "(", "self", ".", "this_steps", ")", "==", "2", ")", ":", "\n", "                ", "step_height", "=", "self", ".", "this_steps", "[", "0", "]", "\n", "step_width", "=", "self", ".", "this_steps", "[", "1", "]", "\n", "", "elif", "isinstance", "(", "self", ".", "this_steps", ",", "(", "int", ",", "float", ")", ")", ":", "\n", "                ", "step_height", "=", "self", ".", "this_steps", "\n", "step_width", "=", "self", ".", "this_steps", "\n", "# Compute the offsets, i.e. at what pixel values the first anchor box center point will be from the top and from the left of the image.", "\n", "", "", "if", "(", "self", ".", "this_offsets", "is", "None", ")", ":", "\n", "            ", "offset_height", "=", "0.5", "\n", "offset_width", "=", "0.5", "\n", "", "else", ":", "\n", "            ", "if", "isinstance", "(", "self", ".", "this_offsets", ",", "(", "list", ",", "tuple", ")", ")", "and", "(", "len", "(", "self", ".", "this_offsets", ")", "==", "2", ")", ":", "\n", "                ", "offset_height", "=", "self", ".", "this_offsets", "[", "0", "]", "\n", "offset_width", "=", "self", ".", "this_offsets", "[", "1", "]", "\n", "", "elif", "isinstance", "(", "self", ".", "this_offsets", ",", "(", "int", ",", "float", ")", ")", ":", "\n", "                ", "offset_height", "=", "self", ".", "this_offsets", "\n", "offset_width", "=", "self", ".", "this_offsets", "\n", "# Now that we have the offsets and step sizes, compute the grid of anchor box center points.", "\n", "", "", "cy", "=", "np", ".", "linspace", "(", "offset_height", "*", "step_height", ",", "(", "offset_height", "+", "feature_map_height", "-", "1", ")", "*", "step_height", ",", "feature_map_height", ")", "\n", "cx", "=", "np", ".", "linspace", "(", "offset_width", "*", "step_width", ",", "(", "offset_width", "+", "feature_map_width", "-", "1", ")", "*", "step_width", ",", "feature_map_width", ")", "\n", "cx_grid", ",", "cy_grid", "=", "np", ".", "meshgrid", "(", "cx", ",", "cy", ")", "\n", "cx_grid", "=", "np", ".", "expand_dims", "(", "cx_grid", ",", "-", "1", ")", "# This is necessary for np.tile() to do what we want further down", "\n", "cy_grid", "=", "np", ".", "expand_dims", "(", "cy_grid", ",", "-", "1", ")", "# This is necessary for np.tile() to do what we want further down", "\n", "\n", "# Create a 4D tensor template of shape `(feature_map_height, feature_map_width, n_boxes, 4)`", "\n", "# where the last dimension will contain `(cx, cy, w, h)`", "\n", "boxes_tensor", "=", "np", ".", "zeros", "(", "(", "feature_map_height", ",", "feature_map_width", ",", "self", ".", "n_boxes", ",", "4", ")", ")", "\n", "\n", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "0", "]", "=", "np", ".", "tile", "(", "cx_grid", ",", "(", "1", ",", "1", ",", "self", ".", "n_boxes", ")", ")", "# Set cx", "\n", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "1", "]", "=", "np", ".", "tile", "(", "cy_grid", ",", "(", "1", ",", "1", ",", "self", ".", "n_boxes", ")", ")", "# Set cy", "\n", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "2", "]", "=", "wh_list", "[", ":", ",", "0", "]", "# Set w", "\n", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "3", "]", "=", "wh_list", "[", ":", ",", "1", "]", "# Set h", "\n", "\n", "# Convert `(cx, cy, w, h)` to `(xmin, xmax, ymin, ymax)`", "\n", "boxes_tensor", "=", "convert_coordinates", "(", "boxes_tensor", ",", "start_index", "=", "0", ",", "conversion", "=", "'centroids2corners'", ")", "\n", "\n", "# If `clip_boxes` is enabled, clip the coordinates to lie within the image boundaries", "\n", "if", "self", ".", "clip_boxes", ":", "\n", "            ", "x_coords", "=", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "[", "0", ",", "2", "]", "]", "\n", "x_coords", "[", "x_coords", ">=", "self", ".", "img_width", "]", "=", "self", ".", "img_width", "-", "1", "\n", "x_coords", "[", "x_coords", "<", "0", "]", "=", "0", "\n", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "[", "0", ",", "2", "]", "]", "=", "x_coords", "\n", "y_coords", "=", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "[", "1", ",", "3", "]", "]", "\n", "y_coords", "[", "y_coords", ">=", "self", ".", "img_height", "]", "=", "self", ".", "img_height", "-", "1", "\n", "y_coords", "[", "y_coords", "<", "0", "]", "=", "0", "\n", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "[", "1", ",", "3", "]", "]", "=", "y_coords", "\n", "\n", "# If `normalize_coords` is enabled, normalize the coordinates to be within [0,1]", "\n", "", "if", "self", ".", "normalize_coords", ":", "\n", "            ", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "[", "0", ",", "2", "]", "]", "/=", "self", ".", "img_width", "\n", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "[", "1", ",", "3", "]", "]", "/=", "self", ".", "img_height", "\n", "\n", "# TODO: Implement box limiting directly for `(cx, cy, w, h)` so that we don't have to unnecessarily convert back and forth.", "\n", "", "if", "self", ".", "coords", "==", "'centroids'", ":", "\n", "# Convert `(xmin, ymin, xmax, ymax)` back to `(cx, cy, w, h)`.", "\n", "            ", "boxes_tensor", "=", "convert_coordinates", "(", "boxes_tensor", ",", "start_index", "=", "0", ",", "conversion", "=", "'corners2centroids'", ",", "border_pixels", "=", "'half'", ")", "\n", "", "elif", "self", ".", "coords", "==", "'minmax'", ":", "\n", "# Convert `(xmin, ymin, xmax, ymax)` to `(xmin, xmax, ymin, ymax).", "\n", "            ", "boxes_tensor", "=", "convert_coordinates", "(", "boxes_tensor", ",", "start_index", "=", "0", ",", "conversion", "=", "'corners2minmax'", ",", "border_pixels", "=", "'half'", ")", "\n", "\n", "# Create a tensor to contain the variances and append it to `boxes_tensor`. This tensor has the same shape", "\n", "# as `boxes_tensor` and simply contains the same 4 variance values for every position in the last axis.", "\n", "", "variances_tensor", "=", "np", ".", "zeros_like", "(", "boxes_tensor", ")", "# Has shape `(feature_map_height, feature_map_width, n_boxes, 4)`", "\n", "variances_tensor", "+=", "self", ".", "variances", "# Long live broadcasting", "\n", "# Now `boxes_tensor` becomes a tensor of shape `(feature_map_height, feature_map_width, n_boxes, 8)`", "\n", "boxes_tensor", "=", "np", ".", "concatenate", "(", "(", "boxes_tensor", ",", "variances_tensor", ")", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# Now prepend one dimension to `boxes_tensor` to account for the batch size and tile it along", "\n", "# The result will be a 5D tensor of shape `(batch_size, feature_map_height, feature_map_width, n_boxes, 8)`", "\n", "boxes_tensor", "=", "np", ".", "expand_dims", "(", "boxes_tensor", ",", "axis", "=", "0", ")", "\n", "boxes_tensor", "=", "K", ".", "tile", "(", "K", ".", "constant", "(", "boxes_tensor", ",", "dtype", "=", "'float32'", ")", ",", "(", "K", ".", "shape", "(", "x", ")", "[", "0", "]", ",", "1", ",", "1", ",", "1", ",", "1", ")", ")", "\n", "\n", "return", "boxes_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_AnchorBoxes.AnchorBoxes.compute_output_shape": [[243, 249], ["keras.image_dim_ordering"], "methods", ["None"], ["", "def", "compute_output_shape", "(", "self", ",", "input_shape", ")", ":", "\n", "        ", "if", "K", ".", "image_dim_ordering", "(", ")", "==", "'tf'", ":", "\n", "            ", "batch_size", ",", "feature_map_height", ",", "feature_map_width", ",", "feature_map_channels", "=", "input_shape", "\n", "", "else", ":", "# Not yet relevant since TensorFlow is the only supported backend right now, but it can't harm to have this in here for the future", "\n", "            ", "batch_size", ",", "feature_map_channels", ",", "feature_map_height", ",", "feature_map_width", "=", "input_shape", "\n", "", "return", "(", "batch_size", ",", "feature_map_height", ",", "feature_map_width", ",", "self", ".", "n_boxes", ",", "8", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_AnchorBoxes.AnchorBoxes.get_config": [[250, 265], ["super().get_config", "dict", "list", "list", "list", "list", "super().get_config.items", "config.items"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DeepAnchorBoxes.AnchorBoxes.get_config"], ["", "def", "get_config", "(", "self", ")", ":", "\n", "        ", "config", "=", "{", "\n", "'img_height'", ":", "self", ".", "img_height", ",", "\n", "'img_width'", ":", "self", ".", "img_width", ",", "\n", "'this_scale'", ":", "self", ".", "this_scale", ",", "\n", "'next_scale'", ":", "self", ".", "next_scale", ",", "\n", "'aspect_ratios'", ":", "list", "(", "self", ".", "aspect_ratios", ")", ",", "\n", "'two_boxes_for_ar1'", ":", "self", ".", "two_boxes_for_ar1", ",", "\n", "'clip_boxes'", ":", "self", ".", "clip_boxes", ",", "\n", "'variances'", ":", "list", "(", "self", ".", "variances", ")", ",", "\n", "'coords'", ":", "self", ".", "coords", ",", "\n", "'normalize_coords'", ":", "self", ".", "normalize_coords", "\n", "}", "\n", "base_config", "=", "super", "(", "AnchorBoxes", ",", "self", ")", ".", "get_config", "(", ")", "\n", "return", "dict", "(", "list", "(", "base_config", ".", "items", "(", ")", ")", "+", "list", "(", "config", ".", "items", "(", ")", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DecodeDetections.DecodeDetections.__init__": [[38, 104], ["tensorflow.constant", "tensorflow.constant", "tensorflow.constant", "tensorflow.constant", "tensorflow.constant", "tensorflow.constant", "tensorflow.constant", "keras.engine.topology.Layer.__init__", "keras.backend", "TypeError", "ValueError", "ValueError", "keras.backend"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_train.Evaluator.__init__"], ["def", "__init__", "(", "self", ",", "\n", "confidence_thresh", "=", "0.01", ",", "\n", "iou_threshold", "=", "0.45", ",", "\n", "top_k", "=", "200", ",", "\n", "nms_max_output_size", "=", "400", ",", "\n", "coords", "=", "'centroids'", ",", "\n", "normalize_coords", "=", "True", ",", "\n", "img_height", "=", "None", ",", "\n", "img_width", "=", "None", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "'''\n        All default argument values follow the Caffe implementation.\n\n        Arguments:\n            confidence_thresh (float, optional): A float in [0,1), the minimum classification confidence in a specific\n                positive class in order to be considered for the non-maximum suppression stage for the respective class.\n                A lower value will result in a larger part of the selection process being done by the non-maximum suppression\n                stage, while a larger value will result in a larger part of the selection process happening in the confidence\n                thresholding stage.\n            iou_threshold (float, optional): A float in [0,1]. All boxes with a Jaccard similarity of greater than `iou_threshold`\n                with a locally maximal box will be removed from the set of predictions for a given class, where 'maximal' refers\n                to the box score.\n            top_k (int, optional): The number of highest scoring predictions to be kept for each batch item after the\n                non-maximum suppression stage.\n            nms_max_output_size (int, optional): The maximum number of predictions that will be left after performing non-maximum\n                suppression.\n            coords (str, optional): The box coordinate format that the model outputs. Must be 'centroids'\n                i.e. the format `(cx, cy, w, h)` (box center coordinates, width, and height). Other coordinate formats are\n                currently not supported.\n            normalize_coords (bool, optional): Set to `True` if the model outputs relative coordinates (i.e. coordinates in [0,1])\n                and you wish to transform these relative coordinates back to absolute coordinates. If the model outputs\n                relative coordinates, but you do not want to convert them back to absolute coordinates, set this to `False`.\n                Do not set this to `True` if the model already outputs absolute coordinates, as that would result in incorrect\n                coordinates. Requires `img_height` and `img_width` if set to `True`.\n            img_height (int, optional): The height of the input images. Only needed if `normalize_coords` is `True`.\n            img_width (int, optional): The width of the input images. Only needed if `normalize_coords` is `True`.\n        '''", "\n", "if", "K", ".", "backend", "(", ")", "!=", "'tensorflow'", ":", "\n", "            ", "raise", "TypeError", "(", "\"This layer only supports TensorFlow at the moment, but you are using the {} backend.\"", ".", "format", "(", "K", ".", "backend", "(", ")", ")", ")", "\n", "\n", "", "if", "normalize_coords", "and", "(", "(", "img_height", "is", "None", ")", "or", "(", "img_width", "is", "None", ")", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"If relative box coordinates are supposed to be converted to absolute coordinates, the decoder needs the image size in order to decode the predictions, but `img_height == {}` and `img_width == {}`\"", ".", "format", "(", "img_height", ",", "img_width", ")", ")", "\n", "\n", "", "if", "coords", "!=", "'centroids'", ":", "\n", "            ", "raise", "ValueError", "(", "\"The DetectionOutput layer currently only supports the 'centroids' coordinate format.\"", ")", "\n", "\n", "# We need these members for the config.", "\n", "", "self", ".", "confidence_thresh", "=", "confidence_thresh", "\n", "self", ".", "iou_threshold", "=", "iou_threshold", "\n", "self", ".", "top_k", "=", "top_k", "\n", "self", ".", "normalize_coords", "=", "normalize_coords", "\n", "self", ".", "img_height", "=", "img_height", "\n", "self", ".", "img_width", "=", "img_width", "\n", "self", ".", "coords", "=", "coords", "\n", "self", ".", "nms_max_output_size", "=", "nms_max_output_size", "\n", "\n", "# We need these members for TensorFlow.", "\n", "self", ".", "tf_confidence_thresh", "=", "tf", ".", "constant", "(", "self", ".", "confidence_thresh", ",", "name", "=", "'confidence_thresh'", ")", "\n", "self", ".", "tf_iou_threshold", "=", "tf", ".", "constant", "(", "self", ".", "iou_threshold", ",", "name", "=", "'iou_threshold'", ")", "\n", "self", ".", "tf_top_k", "=", "tf", ".", "constant", "(", "self", ".", "top_k", ",", "name", "=", "'top_k'", ")", "\n", "self", ".", "tf_normalize_coords", "=", "tf", ".", "constant", "(", "self", ".", "normalize_coords", ",", "name", "=", "'normalize_coords'", ")", "\n", "self", ".", "tf_img_height", "=", "tf", ".", "constant", "(", "self", ".", "img_height", ",", "dtype", "=", "tf", ".", "float32", ",", "name", "=", "'img_height'", ")", "\n", "self", ".", "tf_img_width", "=", "tf", ".", "constant", "(", "self", ".", "img_width", ",", "dtype", "=", "tf", ".", "float32", ",", "name", "=", "'img_width'", ")", "\n", "self", ".", "tf_nms_max_output_size", "=", "tf", ".", "constant", "(", "self", ".", "nms_max_output_size", ",", "name", "=", "'nms_max_output_size'", ")", "\n", "\n", "super", "(", "DecodeDetections", ",", "self", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DecodeDetections.DecodeDetections.build": [[105, 108], ["super().build", "keras.engine.topology.InputSpec"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DeepAnchorBoxes.AnchorBoxes.build"], ["", "def", "build", "(", "self", ",", "input_shape", ")", ":", "\n", "        ", "self", ".", "input_spec", "=", "[", "InputSpec", "(", "shape", "=", "input_shape", ")", "]", "\n", "super", "(", "DecodeDetections", ",", "self", ")", ".", "build", "(", "input_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DecodeDetections.DecodeDetections.call": [[109, 267], ["tensorflow.cond", "tensorflow.concat", "tensorflow.range", "tensorflow.map_fn", "tensorflow.exp", "tensorflow.exp", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.shape", "tensorflow.shape", "tensorflow.map_fn", "tensorflow.reshape", "tensorflow.cond", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.fill", "tensorflow.concat", "tensorflow.boolean_mask", "tensorflow.cond", "tensorflow.pad", "tensorflow.gather", "tensorflow.pad", "tensorflow.gather", "tensorflow.greater_equal", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.concat", "tensorflow.image.non_max_suppression", "tensorflow.gather", "tensorflow.constant", "tensorflow.equal", "tensorflow.range", "keras_layer_DecodeDetections.DecodeDetections.call.filter_predictions"], "methods", ["None"], ["", "def", "call", "(", "self", ",", "y_pred", ",", "mask", "=", "None", ")", ":", "\n", "        ", "'''\n        Returns:\n            3D tensor of shape `(batch_size, top_k, 6)`. The second axis is zero-padded\n            to always yield `top_k` predictions per batch item. The last axis contains\n            the coordinates for each predicted box in the format\n            `[class_id, confidence, xmin, ymin, xmax, ymax]`.\n        '''", "\n", "\n", "#####################################################################################", "\n", "# 1. Convert the box coordinates from predicted anchor box offsets to predicted", "\n", "#    absolute coordinates", "\n", "#####################################################################################", "\n", "\n", "# Convert anchor box offsets to image offsets.", "\n", "cx", "=", "y_pred", "[", "...", ",", "-", "12", "]", "*", "y_pred", "[", "...", ",", "-", "4", "]", "*", "y_pred", "[", "...", ",", "-", "6", "]", "+", "y_pred", "[", "...", ",", "-", "8", "]", "# cx = cx_pred * cx_variance * w_anchor + cx_anchor", "\n", "cy", "=", "y_pred", "[", "...", ",", "-", "11", "]", "*", "y_pred", "[", "...", ",", "-", "3", "]", "*", "y_pred", "[", "...", ",", "-", "5", "]", "+", "y_pred", "[", "...", ",", "-", "7", "]", "# cy = cy_pred * cy_variance * h_anchor + cy_anchor", "\n", "w", "=", "tf", ".", "exp", "(", "y_pred", "[", "...", ",", "-", "10", "]", "*", "y_pred", "[", "...", ",", "-", "2", "]", ")", "*", "y_pred", "[", "...", ",", "-", "6", "]", "# w = exp(w_pred * variance_w) * w_anchor", "\n", "h", "=", "tf", ".", "exp", "(", "y_pred", "[", "...", ",", "-", "9", "]", "*", "y_pred", "[", "...", ",", "-", "1", "]", ")", "*", "y_pred", "[", "...", ",", "-", "5", "]", "# h = exp(h_pred * variance_h) * h_anchor", "\n", "\n", "# Convert 'centroids' to 'corners'.", "\n", "xmin", "=", "cx", "-", "0.5", "*", "w", "\n", "ymin", "=", "cy", "-", "0.5", "*", "h", "\n", "xmax", "=", "cx", "+", "0.5", "*", "w", "\n", "ymax", "=", "cy", "+", "0.5", "*", "h", "\n", "\n", "# If the model predicts box coordinates relative to the image dimensions and they are supposed", "\n", "# to be converted back to absolute coordinates, do that.", "\n", "def", "normalized_coords", "(", ")", ":", "\n", "            ", "xmin1", "=", "tf", ".", "expand_dims", "(", "xmin", "*", "self", ".", "tf_img_width", ",", "axis", "=", "-", "1", ")", "\n", "ymin1", "=", "tf", ".", "expand_dims", "(", "ymin", "*", "self", ".", "tf_img_height", ",", "axis", "=", "-", "1", ")", "\n", "xmax1", "=", "tf", ".", "expand_dims", "(", "xmax", "*", "self", ".", "tf_img_width", ",", "axis", "=", "-", "1", ")", "\n", "ymax1", "=", "tf", ".", "expand_dims", "(", "ymax", "*", "self", ".", "tf_img_height", ",", "axis", "=", "-", "1", ")", "\n", "return", "xmin1", ",", "ymin1", ",", "xmax1", ",", "ymax1", "\n", "", "def", "non_normalized_coords", "(", ")", ":", "\n", "            ", "return", "tf", ".", "expand_dims", "(", "xmin", ",", "axis", "=", "-", "1", ")", ",", "tf", ".", "expand_dims", "(", "ymin", ",", "axis", "=", "-", "1", ")", ",", "tf", ".", "expand_dims", "(", "xmax", ",", "axis", "=", "-", "1", ")", ",", "tf", ".", "expand_dims", "(", "ymax", ",", "axis", "=", "-", "1", ")", "\n", "\n", "", "xmin", ",", "ymin", ",", "xmax", ",", "ymax", "=", "tf", ".", "cond", "(", "self", ".", "tf_normalize_coords", ",", "normalized_coords", ",", "non_normalized_coords", ")", "\n", "\n", "# Concatenate the one-hot class confidences and the converted box coordinates to form the decoded predictions tensor.", "\n", "y_pred", "=", "tf", ".", "concat", "(", "values", "=", "[", "y_pred", "[", "...", ",", ":", "-", "12", "]", ",", "xmin", ",", "ymin", ",", "xmax", ",", "ymax", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "#####################################################################################", "\n", "# 2. Perform confidence thresholding, per-class non-maximum suppression, and", "\n", "#    top-k filtering.", "\n", "#####################################################################################", "\n", "\n", "batch_size", "=", "tf", ".", "shape", "(", "y_pred", ")", "[", "0", "]", "# Output dtype: tf.int32", "\n", "n_boxes", "=", "tf", ".", "shape", "(", "y_pred", ")", "[", "1", "]", "\n", "n_classes", "=", "y_pred", ".", "shape", "[", "2", "]", "-", "4", "\n", "class_indices", "=", "tf", ".", "range", "(", "1", ",", "n_classes", ")", "\n", "\n", "# Create a function that filters the predictions for the given batch item. Specifically, it performs:", "\n", "# - confidence thresholding", "\n", "# - non-maximum suppression (NMS)", "\n", "# - top-k filtering", "\n", "def", "filter_predictions", "(", "batch_item", ")", ":", "\n", "\n", "# Create a function that filters the predictions for one single class.", "\n", "            ", "def", "filter_single_class", "(", "index", ")", ":", "\n", "\n", "# From a tensor of shape (n_boxes, n_classes + 4 coordinates) extract", "\n", "# a tensor of shape (n_boxes, 1 + 4 coordinates) that contains the", "\n", "# confidnece values for just one class, determined by `index`.", "\n", "                ", "confidences", "=", "tf", ".", "expand_dims", "(", "batch_item", "[", "...", ",", "index", "]", ",", "axis", "=", "-", "1", ")", "\n", "class_id", "=", "tf", ".", "fill", "(", "dims", "=", "tf", ".", "shape", "(", "confidences", ")", ",", "value", "=", "tf", ".", "to_float", "(", "index", ")", ")", "\n", "box_coordinates", "=", "batch_item", "[", "...", ",", "-", "4", ":", "]", "\n", "\n", "\n", "single_class", "=", "tf", ".", "concat", "(", "[", "class_id", ",", "confidences", ",", "box_coordinates", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# Apply confidence thresholding with respect to the class defined by `index`.", "\n", "threshold_met", "=", "single_class", "[", ":", ",", "1", "]", ">", "self", ".", "tf_confidence_thresh", "\n", "single_class", "=", "tf", ".", "boolean_mask", "(", "tensor", "=", "single_class", ",", "\n", "mask", "=", "threshold_met", ")", "\n", "\n", "# If any boxes made the threshold, perform NMS.", "\n", "def", "perform_nms", "(", ")", ":", "\n", "                    ", "scores", "=", "single_class", "[", "...", ",", "1", "]", "\n", "\n", "# `tf.image.non_max_suppression()` needs the box coordinates in the format `(ymin, xmin, ymax, xmax)`.", "\n", "xmin", "=", "tf", ".", "expand_dims", "(", "single_class", "[", "...", ",", "-", "4", "]", ",", "axis", "=", "-", "1", ")", "\n", "ymin", "=", "tf", ".", "expand_dims", "(", "single_class", "[", "...", ",", "-", "3", "]", ",", "axis", "=", "-", "1", ")", "\n", "xmax", "=", "tf", ".", "expand_dims", "(", "single_class", "[", "...", ",", "-", "2", "]", ",", "axis", "=", "-", "1", ")", "\n", "ymax", "=", "tf", ".", "expand_dims", "(", "single_class", "[", "...", ",", "-", "1", "]", ",", "axis", "=", "-", "1", ")", "\n", "boxes", "=", "tf", ".", "concat", "(", "values", "=", "[", "ymin", ",", "xmin", ",", "ymax", ",", "xmax", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "maxima_indices", "=", "tf", ".", "image", ".", "non_max_suppression", "(", "boxes", "=", "boxes", ",", "\n", "scores", "=", "scores", ",", "\n", "max_output_size", "=", "self", ".", "tf_nms_max_output_size", ",", "\n", "iou_threshold", "=", "self", ".", "iou_threshold", ",", "\n", "name", "=", "'non_maximum_suppresion'", ")", "\n", "maxima", "=", "tf", ".", "gather", "(", "params", "=", "single_class", ",", "\n", "indices", "=", "maxima_indices", ",", "\n", "axis", "=", "0", ")", "\n", "return", "maxima", "\n", "\n", "", "def", "no_confident_predictions", "(", ")", ":", "\n", "                    ", "return", "tf", ".", "constant", "(", "value", "=", "0.0", ",", "shape", "=", "(", "1", ",", "6", ")", ")", "\n", "\n", "", "single_class_nms", "=", "tf", ".", "cond", "(", "tf", ".", "equal", "(", "tf", ".", "size", "(", "single_class", ")", ",", "0", ")", ",", "no_confident_predictions", ",", "perform_nms", ")", "\n", "\n", "# Make sure `single_class` is exactly `self.nms_max_output_size` elements long.", "\n", "padded_single_class", "=", "tf", ".", "pad", "(", "tensor", "=", "single_class_nms", ",", "\n", "paddings", "=", "[", "[", "0", ",", "self", ".", "tf_nms_max_output_size", "-", "tf", ".", "shape", "(", "single_class_nms", ")", "[", "0", "]", "]", ",", "[", "0", ",", "0", "]", "]", ",", "\n", "mode", "=", "'CONSTANT'", ",", "\n", "constant_values", "=", "0.0", ")", "\n", "\n", "return", "padded_single_class", "\n", "\n", "# Iterate `filter_single_class()` over all class indices.", "\n", "", "filtered_single_classes", "=", "tf", ".", "map_fn", "(", "fn", "=", "lambda", "i", ":", "filter_single_class", "(", "i", ")", ",", "\n", "elems", "=", "tf", ".", "range", "(", "1", ",", "n_classes", ")", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "parallel_iterations", "=", "128", ",", "\n", "back_prop", "=", "False", ",", "\n", "swap_memory", "=", "False", ",", "\n", "infer_shape", "=", "True", ",", "\n", "name", "=", "'loop_over_classes'", ")", "\n", "\n", "# Concatenate the filtered results for all individual classes to one tensor.", "\n", "filtered_predictions", "=", "tf", ".", "reshape", "(", "tensor", "=", "filtered_single_classes", ",", "shape", "=", "(", "-", "1", ",", "6", ")", ")", "\n", "\n", "# Perform top-k filtering for this batch item or pad it in case there are", "\n", "# fewer than `self.top_k` boxes left at this point. Either way, produce a", "\n", "# tensor of length `self.top_k`. By the time we return the final results tensor", "\n", "# for the whole batch, all batch items must have the same number of predicted", "\n", "# boxes so that the tensor dimensions are homogenous. If fewer than `self.top_k`", "\n", "# predictions are left after the filtering process above, we pad the missing", "\n", "# predictions with zeros as dummy entries.", "\n", "def", "top_k", "(", ")", ":", "\n", "                ", "return", "tf", ".", "gather", "(", "params", "=", "filtered_predictions", ",", "\n", "indices", "=", "tf", ".", "nn", ".", "top_k", "(", "filtered_predictions", "[", ":", ",", "1", "]", ",", "k", "=", "self", ".", "tf_top_k", ",", "sorted", "=", "True", ")", ".", "indices", ",", "\n", "axis", "=", "0", ")", "\n", "", "def", "pad_and_top_k", "(", ")", ":", "\n", "                ", "padded_predictions", "=", "tf", ".", "pad", "(", "tensor", "=", "filtered_predictions", ",", "\n", "paddings", "=", "[", "[", "0", ",", "self", ".", "tf_top_k", "-", "tf", ".", "shape", "(", "filtered_predictions", ")", "[", "0", "]", "]", ",", "[", "0", ",", "0", "]", "]", ",", "\n", "mode", "=", "'CONSTANT'", ",", "\n", "constant_values", "=", "0.0", ")", "\n", "return", "tf", ".", "gather", "(", "params", "=", "padded_predictions", ",", "\n", "indices", "=", "tf", ".", "nn", ".", "top_k", "(", "padded_predictions", "[", ":", ",", "1", "]", ",", "k", "=", "self", ".", "tf_top_k", ",", "sorted", "=", "True", ")", ".", "indices", ",", "\n", "axis", "=", "0", ")", "\n", "\n", "", "top_k_boxes", "=", "tf", ".", "cond", "(", "tf", ".", "greater_equal", "(", "tf", ".", "shape", "(", "filtered_predictions", ")", "[", "0", "]", ",", "self", ".", "tf_top_k", ")", ",", "top_k", ",", "pad_and_top_k", ")", "\n", "\n", "return", "top_k_boxes", "\n", "\n", "# Iterate `filter_predictions()` over all batch items.", "\n", "", "output_tensor", "=", "tf", ".", "map_fn", "(", "fn", "=", "lambda", "x", ":", "filter_predictions", "(", "x", ")", ",", "\n", "elems", "=", "y_pred", ",", "\n", "dtype", "=", "None", ",", "\n", "parallel_iterations", "=", "128", ",", "\n", "back_prop", "=", "False", ",", "\n", "swap_memory", "=", "False", ",", "\n", "infer_shape", "=", "True", ",", "\n", "name", "=", "'loop_over_batch'", ")", "\n", "\n", "return", "output_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DecodeDetections.DecodeDetections.compute_output_shape": [[268, 271], ["None"], "methods", ["None"], ["", "def", "compute_output_shape", "(", "self", ",", "input_shape", ")", ":", "\n", "        ", "batch_size", ",", "n_boxes", ",", "last_axis", "=", "input_shape", "\n", "return", "(", "batch_size", ",", "self", ".", "tf_top_k", ",", "6", ")", "# Last axis: (class_ID, confidence, 4 box coordinates)", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DecodeDetections.DecodeDetections.get_config": [[272, 285], ["super().get_config", "dict", "list", "list", "super().get_config.items", "config.items"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DeepAnchorBoxes.AnchorBoxes.get_config"], ["", "def", "get_config", "(", "self", ")", ":", "\n", "        ", "config", "=", "{", "\n", "'confidence_thresh'", ":", "self", ".", "confidence_thresh", ",", "\n", "'iou_threshold'", ":", "self", ".", "iou_threshold", ",", "\n", "'top_k'", ":", "self", ".", "top_k", ",", "\n", "'nms_max_output_size'", ":", "self", ".", "nms_max_output_size", ",", "\n", "'coords'", ":", "self", ".", "coords", ",", "\n", "'normalize_coords'", ":", "self", ".", "normalize_coords", ",", "\n", "'img_height'", ":", "self", ".", "img_height", ",", "\n", "'img_width'", ":", "self", ".", "img_width", ",", "\n", "}", "\n", "base_config", "=", "super", "(", "DecodeDetections", ",", "self", ")", ".", "get_config", "(", ")", "\n", "return", "dict", "(", "list", "(", "base_config", ".", "items", "(", ")", ")", "+", "list", "(", "config", ".", "items", "(", ")", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DecodeDetectionsFast.DecodeDetectionsFast.__init__": [[40, 106], ["tensorflow.constant", "tensorflow.constant", "tensorflow.constant", "tensorflow.constant", "tensorflow.constant", "tensorflow.constant", "tensorflow.constant", "keras.engine.topology.Layer.__init__", "keras.backend", "TypeError", "ValueError", "ValueError", "keras.backend"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_train.Evaluator.__init__"], ["def", "__init__", "(", "self", ",", "\n", "confidence_thresh", "=", "0.01", ",", "\n", "iou_threshold", "=", "0.45", ",", "\n", "top_k", "=", "200", ",", "\n", "nms_max_output_size", "=", "400", ",", "\n", "coords", "=", "'centroids'", ",", "\n", "normalize_coords", "=", "True", ",", "\n", "img_height", "=", "None", ",", "\n", "img_width", "=", "None", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "'''\n        All default argument values follow the Caffe implementation.\n\n        Arguments:\n            confidence_thresh (float, optional): A float in [0,1), the minimum classification confidence in a specific\n                positive class in order to be considered for the non-maximum suppression stage for the respective class.\n                A lower value will result in a larger part of the selection process being done by the non-maximum suppression\n                stage, while a larger value will result in a larger part of the selection process happening in the confidence\n                thresholding stage.\n            iou_threshold (float, optional): A float in [0,1]. All boxes with a Jaccard similarity of greater than `iou_threshold`\n                with a locally maximal box will be removed from the set of predictions for a given class, where 'maximal' refers\n                to the box score.\n            top_k (int, optional): The number of highest scoring predictions to be kept for each batch item after the\n                non-maximum suppression stage.\n            nms_max_output_size (int, optional): The maximum number of predictions that will be left after performing non-maximum\n                suppression.\n            coords (str, optional): The box coordinate format that the model outputs. Must be 'centroids'\n                i.e. the format `(cx, cy, w, h)` (box center coordinates, width, and height). Other coordinate formats are\n                currently not supported.\n            normalize_coords (bool, optional): Set to `True` if the model outputs relative coordinates (i.e. coordinates in [0,1])\n                and you wish to transform these relative coordinates back to absolute coordinates. If the model outputs\n                relative coordinates, but you do not want to convert them back to absolute coordinates, set this to `False`.\n                Do not set this to `True` if the model already outputs absolute coordinates, as that would result in incorrect\n                coordinates. Requires `img_height` and `img_width` if set to `True`.\n            img_height (int, optional): The height of the input images. Only needed if `normalize_coords` is `True`.\n            img_width (int, optional): The width of the input images. Only needed if `normalize_coords` is `True`.\n        '''", "\n", "if", "K", ".", "backend", "(", ")", "!=", "'tensorflow'", ":", "\n", "            ", "raise", "TypeError", "(", "\"This layer only supports TensorFlow at the moment, but you are using the {} backend.\"", ".", "format", "(", "K", ".", "backend", "(", ")", ")", ")", "\n", "\n", "", "if", "normalize_coords", "and", "(", "(", "img_height", "is", "None", ")", "or", "(", "img_width", "is", "None", ")", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"If relative box coordinates are supposed to be converted to absolute coordinates, the decoder needs the image size in order to decode the predictions, but `img_height == {}` and `img_width == {}`\"", ".", "format", "(", "img_height", ",", "img_width", ")", ")", "\n", "\n", "", "if", "coords", "!=", "'centroids'", ":", "\n", "            ", "raise", "ValueError", "(", "\"The DetectionOutput layer currently only supports the 'centroids' coordinate format.\"", ")", "\n", "\n", "# We need these members for the config.", "\n", "", "self", ".", "confidence_thresh", "=", "confidence_thresh", "\n", "self", ".", "iou_threshold", "=", "iou_threshold", "\n", "self", ".", "top_k", "=", "top_k", "\n", "self", ".", "normalize_coords", "=", "normalize_coords", "\n", "self", ".", "img_height", "=", "img_height", "\n", "self", ".", "img_width", "=", "img_width", "\n", "self", ".", "coords", "=", "coords", "\n", "self", ".", "nms_max_output_size", "=", "nms_max_output_size", "\n", "\n", "# We need these members for TensorFlow.", "\n", "self", ".", "tf_confidence_thresh", "=", "tf", ".", "constant", "(", "self", ".", "confidence_thresh", ",", "name", "=", "'confidence_thresh'", ")", "\n", "self", ".", "tf_iou_threshold", "=", "tf", ".", "constant", "(", "self", ".", "iou_threshold", ",", "name", "=", "'iou_threshold'", ")", "\n", "self", ".", "tf_top_k", "=", "tf", ".", "constant", "(", "self", ".", "top_k", ",", "name", "=", "'top_k'", ")", "\n", "self", ".", "tf_normalize_coords", "=", "tf", ".", "constant", "(", "self", ".", "normalize_coords", ",", "name", "=", "'normalize_coords'", ")", "\n", "self", ".", "tf_img_height", "=", "tf", ".", "constant", "(", "self", ".", "img_height", ",", "dtype", "=", "tf", ".", "float32", ",", "name", "=", "'img_height'", ")", "\n", "self", ".", "tf_img_width", "=", "tf", ".", "constant", "(", "self", ".", "img_width", ",", "dtype", "=", "tf", ".", "float32", ",", "name", "=", "'img_width'", ")", "\n", "self", ".", "tf_nms_max_output_size", "=", "tf", ".", "constant", "(", "self", ".", "nms_max_output_size", ",", "name", "=", "'nms_max_output_size'", ")", "\n", "\n", "super", "(", "DecodeDetectionsFast", ",", "self", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DecodeDetectionsFast.DecodeDetectionsFast.build": [[107, 110], ["super().build", "keras.engine.topology.InputSpec"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DeepAnchorBoxes.AnchorBoxes.build"], ["", "def", "build", "(", "self", ",", "input_shape", ")", ":", "\n", "        ", "self", ".", "input_spec", "=", "[", "InputSpec", "(", "shape", "=", "input_shape", ")", "]", "\n", "super", "(", "DecodeDetectionsFast", ",", "self", ")", ".", "build", "(", "input_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DecodeDetectionsFast.DecodeDetectionsFast.call": [[111, 249], ["tensorflow.expand_dims", "tensorflow.reduce_max", "tensorflow.cond", "tensorflow.concat", "tensorflow.range", "tensorflow.map_fn", "tensorflow.to_float", "tensorflow.exp", "tensorflow.exp", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.shape", "tensorflow.shape", "tensorflow.not_equal", "tensorflow.boolean_mask", "tensorflow.cond", "tensorflow.cond", "tensorflow.cond", "tensorflow.argmax", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.boolean_mask", "tensorflow.constant", "tensorflow.equal", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.concat", "tensorflow.image.non_max_suppression", "tensorflow.gather", "tensorflow.constant", "tensorflow.equal", "tensorflow.gather", "tensorflow.pad", "tensorflow.gather", "tensorflow.greater_equal", "tensorflow.size", "tensorflow.size", "keras_layer_DecodeDetectionsFast.DecodeDetectionsFast.call.filter_predictions"], "methods", ["None"], ["", "def", "call", "(", "self", ",", "y_pred", ",", "mask", "=", "None", ")", ":", "\n", "        ", "'''\n        Returns:\n            3D tensor of shape `(batch_size, top_k, 6)`. The second axis is zero-padded\n            to always yield `top_k` predictions per batch item. The last axis contains\n            the coordinates for each predicted box in the format\n            `[class_id, confidence, xmin, ymin, xmax, ymax]`.\n        '''", "\n", "\n", "#####################################################################################", "\n", "# 1. Convert the box coordinates from predicted anchor box offsets to predicted", "\n", "#    absolute coordinates", "\n", "#####################################################################################", "\n", "\n", "# Extract the predicted class IDs as the indices of the highest confidence values.", "\n", "class_ids", "=", "tf", ".", "expand_dims", "(", "tf", ".", "to_float", "(", "tf", ".", "argmax", "(", "y_pred", "[", "...", ",", ":", "-", "12", "]", ",", "axis", "=", "-", "1", ")", ")", ",", "axis", "=", "-", "1", ")", "\n", "# Extract the confidences of the maximal classes.", "\n", "confidences", "=", "tf", ".", "reduce_max", "(", "y_pred", "[", "...", ",", ":", "-", "12", "]", ",", "axis", "=", "-", "1", ",", "keep_dims", "=", "True", ")", "\n", "\n", "# Convert anchor box offsets to image offsets.", "\n", "cx", "=", "y_pred", "[", "...", ",", "-", "12", "]", "*", "y_pred", "[", "...", ",", "-", "4", "]", "*", "y_pred", "[", "...", ",", "-", "6", "]", "+", "y_pred", "[", "...", ",", "-", "8", "]", "# cx = cx_pred * cx_variance * w_anchor + cx_anchor", "\n", "cy", "=", "y_pred", "[", "...", ",", "-", "11", "]", "*", "y_pred", "[", "...", ",", "-", "3", "]", "*", "y_pred", "[", "...", ",", "-", "5", "]", "+", "y_pred", "[", "...", ",", "-", "7", "]", "# cy = cy_pred * cy_variance * h_anchor + cy_anchor", "\n", "w", "=", "tf", ".", "exp", "(", "y_pred", "[", "...", ",", "-", "10", "]", "*", "y_pred", "[", "...", ",", "-", "2", "]", ")", "*", "y_pred", "[", "...", ",", "-", "6", "]", "# w = exp(w_pred * variance_w) * w_anchor", "\n", "h", "=", "tf", ".", "exp", "(", "y_pred", "[", "...", ",", "-", "9", "]", "*", "y_pred", "[", "...", ",", "-", "1", "]", ")", "*", "y_pred", "[", "...", ",", "-", "5", "]", "# h = exp(h_pred * variance_h) * h_anchor", "\n", "\n", "# Convert 'centroids' to 'corners'.", "\n", "xmin", "=", "cx", "-", "0.5", "*", "w", "\n", "ymin", "=", "cy", "-", "0.5", "*", "h", "\n", "xmax", "=", "cx", "+", "0.5", "*", "w", "\n", "ymax", "=", "cy", "+", "0.5", "*", "h", "\n", "\n", "# If the model predicts box coordinates relative to the image dimensions and they are supposed", "\n", "# to be converted back to absolute coordinates, do that.", "\n", "def", "normalized_coords", "(", ")", ":", "\n", "            ", "xmin1", "=", "tf", ".", "expand_dims", "(", "xmin", "*", "self", ".", "tf_img_width", ",", "axis", "=", "-", "1", ")", "\n", "ymin1", "=", "tf", ".", "expand_dims", "(", "ymin", "*", "self", ".", "tf_img_height", ",", "axis", "=", "-", "1", ")", "\n", "xmax1", "=", "tf", ".", "expand_dims", "(", "xmax", "*", "self", ".", "tf_img_width", ",", "axis", "=", "-", "1", ")", "\n", "ymax1", "=", "tf", ".", "expand_dims", "(", "ymax", "*", "self", ".", "tf_img_height", ",", "axis", "=", "-", "1", ")", "\n", "return", "xmin1", ",", "ymin1", ",", "xmax1", ",", "ymax1", "\n", "", "def", "non_normalized_coords", "(", ")", ":", "\n", "            ", "return", "tf", ".", "expand_dims", "(", "xmin", ",", "axis", "=", "-", "1", ")", ",", "tf", ".", "expand_dims", "(", "ymin", ",", "axis", "=", "-", "1", ")", ",", "tf", ".", "expand_dims", "(", "xmax", ",", "axis", "=", "-", "1", ")", ",", "tf", ".", "expand_dims", "(", "ymax", ",", "axis", "=", "-", "1", ")", "\n", "\n", "", "xmin", ",", "ymin", ",", "xmax", ",", "ymax", "=", "tf", ".", "cond", "(", "self", ".", "tf_normalize_coords", ",", "normalized_coords", ",", "non_normalized_coords", ")", "\n", "\n", "# Concatenate the one-hot class confidences and the converted box coordinates to form the decoded predictions tensor.", "\n", "y_pred", "=", "tf", ".", "concat", "(", "values", "=", "[", "class_ids", ",", "confidences", ",", "xmin", ",", "ymin", ",", "xmax", ",", "ymax", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "#####################################################################################", "\n", "# 2. Perform confidence thresholding, non-maximum suppression, and top-k filtering.", "\n", "#####################################################################################", "\n", "\n", "batch_size", "=", "tf", ".", "shape", "(", "y_pred", ")", "[", "0", "]", "# Output dtype: tf.int32", "\n", "n_boxes", "=", "tf", ".", "shape", "(", "y_pred", ")", "[", "1", "]", "\n", "n_classes", "=", "y_pred", ".", "shape", "[", "2", "]", "-", "4", "\n", "class_indices", "=", "tf", ".", "range", "(", "1", ",", "n_classes", ")", "\n", "\n", "# Create a function that filters the predictions for the given batch item. Specifically, it performs:", "\n", "# - confidence thresholding", "\n", "# - non-maximum suppression (NMS)", "\n", "# - top-k filtering", "\n", "def", "filter_predictions", "(", "batch_item", ")", ":", "\n", "\n", "# Keep only the non-background boxes.", "\n", "            ", "positive_boxes", "=", "tf", ".", "not_equal", "(", "batch_item", "[", "...", ",", "0", "]", ",", "0.0", ")", "\n", "predictions", "=", "tf", ".", "boolean_mask", "(", "tensor", "=", "batch_item", ",", "\n", "mask", "=", "positive_boxes", ")", "\n", "\n", "def", "perform_confidence_thresholding", "(", ")", ":", "\n", "# Apply confidence thresholding.", "\n", "                ", "threshold_met", "=", "predictions", "[", ":", ",", "1", "]", ">", "self", ".", "tf_confidence_thresh", "\n", "return", "tf", ".", "boolean_mask", "(", "tensor", "=", "predictions", ",", "\n", "mask", "=", "threshold_met", ")", "\n", "", "def", "no_positive_boxes", "(", ")", ":", "\n", "                ", "return", "tf", ".", "constant", "(", "value", "=", "0.0", ",", "shape", "=", "(", "1", ",", "6", ")", ")", "\n", "\n", "# If there are any positive predictions, perform confidence thresholding.", "\n", "", "predictions_conf_thresh", "=", "tf", ".", "cond", "(", "tf", ".", "equal", "(", "tf", ".", "size", "(", "predictions", ")", ",", "0", ")", ",", "no_positive_boxes", ",", "perform_confidence_thresholding", ")", "\n", "\n", "def", "perform_nms", "(", ")", ":", "\n", "                ", "scores", "=", "predictions_conf_thresh", "[", "...", ",", "1", "]", "\n", "\n", "# `tf.image.non_max_suppression()` needs the box coordinates in the format `(ymin, xmin, ymax, xmax)`.", "\n", "xmin", "=", "tf", ".", "expand_dims", "(", "predictions_conf_thresh", "[", "...", ",", "-", "4", "]", ",", "axis", "=", "-", "1", ")", "\n", "ymin", "=", "tf", ".", "expand_dims", "(", "predictions_conf_thresh", "[", "...", ",", "-", "3", "]", ",", "axis", "=", "-", "1", ")", "\n", "xmax", "=", "tf", ".", "expand_dims", "(", "predictions_conf_thresh", "[", "...", ",", "-", "2", "]", ",", "axis", "=", "-", "1", ")", "\n", "ymax", "=", "tf", ".", "expand_dims", "(", "predictions_conf_thresh", "[", "...", ",", "-", "1", "]", ",", "axis", "=", "-", "1", ")", "\n", "boxes", "=", "tf", ".", "concat", "(", "values", "=", "[", "ymin", ",", "xmin", ",", "ymax", ",", "xmax", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "maxima_indices", "=", "tf", ".", "image", ".", "non_max_suppression", "(", "boxes", "=", "boxes", ",", "\n", "scores", "=", "scores", ",", "\n", "max_output_size", "=", "self", ".", "tf_nms_max_output_size", ",", "\n", "iou_threshold", "=", "self", ".", "iou_threshold", ",", "\n", "name", "=", "'non_maximum_suppresion'", ")", "\n", "maxima", "=", "tf", ".", "gather", "(", "params", "=", "predictions_conf_thresh", ",", "\n", "indices", "=", "maxima_indices", ",", "\n", "axis", "=", "0", ")", "\n", "return", "maxima", "\n", "", "def", "no_confident_predictions", "(", ")", ":", "\n", "                ", "return", "tf", ".", "constant", "(", "value", "=", "0.0", ",", "shape", "=", "(", "1", ",", "6", ")", ")", "\n", "\n", "# If any boxes made the threshold, perform NMS.", "\n", "", "predictions_nms", "=", "tf", ".", "cond", "(", "tf", ".", "equal", "(", "tf", ".", "size", "(", "predictions_conf_thresh", ")", ",", "0", ")", ",", "no_confident_predictions", ",", "perform_nms", ")", "\n", "\n", "# Perform top-k filtering for this batch item or pad it in case there are", "\n", "# fewer than `self.top_k` boxes left at this point. Either way, produce a", "\n", "# tensor of length `self.top_k`. By the time we return the final results tensor", "\n", "# for the whole batch, all batch items must have the same number of predicted", "\n", "# boxes so that the tensor dimensions are homogenous. If fewer than `self.top_k`", "\n", "# predictions are left after the filtering process above, we pad the missing", "\n", "# predictions with zeros as dummy entries.", "\n", "def", "top_k", "(", ")", ":", "\n", "                ", "return", "tf", ".", "gather", "(", "params", "=", "predictions_nms", ",", "\n", "indices", "=", "tf", ".", "nn", ".", "top_k", "(", "predictions_nms", "[", ":", ",", "1", "]", ",", "k", "=", "self", ".", "tf_top_k", ",", "sorted", "=", "True", ")", ".", "indices", ",", "\n", "axis", "=", "0", ")", "\n", "", "def", "pad_and_top_k", "(", ")", ":", "\n", "                ", "padded_predictions", "=", "tf", ".", "pad", "(", "tensor", "=", "predictions_nms", ",", "\n", "paddings", "=", "[", "[", "0", ",", "self", ".", "tf_top_k", "-", "tf", ".", "shape", "(", "predictions_nms", ")", "[", "0", "]", "]", ",", "[", "0", ",", "0", "]", "]", ",", "\n", "mode", "=", "'CONSTANT'", ",", "\n", "constant_values", "=", "0.0", ")", "\n", "return", "tf", ".", "gather", "(", "params", "=", "padded_predictions", ",", "\n", "indices", "=", "tf", ".", "nn", ".", "top_k", "(", "padded_predictions", "[", ":", ",", "1", "]", ",", "k", "=", "self", ".", "tf_top_k", ",", "sorted", "=", "True", ")", ".", "indices", ",", "\n", "axis", "=", "0", ")", "\n", "\n", "", "top_k_boxes", "=", "tf", ".", "cond", "(", "tf", ".", "greater_equal", "(", "tf", ".", "shape", "(", "predictions_nms", ")", "[", "0", "]", ",", "self", ".", "tf_top_k", ")", ",", "top_k", ",", "pad_and_top_k", ")", "\n", "\n", "return", "top_k_boxes", "\n", "\n", "# Iterate `filter_predictions()` over all batch items.", "\n", "", "output_tensor", "=", "tf", ".", "map_fn", "(", "fn", "=", "lambda", "x", ":", "filter_predictions", "(", "x", ")", ",", "\n", "elems", "=", "y_pred", ",", "\n", "dtype", "=", "None", ",", "\n", "parallel_iterations", "=", "128", ",", "\n", "back_prop", "=", "False", ",", "\n", "swap_memory", "=", "False", ",", "\n", "infer_shape", "=", "True", ",", "\n", "name", "=", "'loop_over_batch'", ")", "\n", "\n", "return", "output_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DecodeDetectionsFast.DecodeDetectionsFast.compute_output_shape": [[250, 253], ["None"], "methods", ["None"], ["", "def", "compute_output_shape", "(", "self", ",", "input_shape", ")", ":", "\n", "        ", "batch_size", ",", "n_boxes", ",", "last_axis", "=", "input_shape", "\n", "return", "(", "batch_size", ",", "self", ".", "tf_top_k", ",", "6", ")", "# Last axis: (class_ID, confidence, 4 box coordinates)", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DecodeDetectionsFast.DecodeDetectionsFast.get_config": [[254, 267], ["super().get_config", "dict", "list", "list", "super().get_config.items", "config.items"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DeepAnchorBoxes.AnchorBoxes.get_config"], ["", "def", "get_config", "(", "self", ")", ":", "\n", "        ", "config", "=", "{", "\n", "'confidence_thresh'", ":", "self", ".", "confidence_thresh", ",", "\n", "'iou_threshold'", ":", "self", ".", "iou_threshold", ",", "\n", "'top_k'", ":", "self", ".", "top_k", ",", "\n", "'nms_max_output_size'", ":", "self", ".", "nms_max_output_size", ",", "\n", "'coords'", ":", "self", ".", "coords", ",", "\n", "'normalize_coords'", ":", "self", ".", "normalize_coords", ",", "\n", "'img_height'", ":", "self", ".", "img_height", ",", "\n", "'img_width'", ":", "self", ".", "img_width", ",", "\n", "}", "\n", "base_config", "=", "super", "(", "DecodeDetectionsFast", ",", "self", ")", ".", "get_config", "(", ")", "\n", "return", "dict", "(", "list", "(", "base_config", ".", "items", "(", ")", ")", "+", "list", "(", "config", ".", "items", "(", ")", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DeepAnchorBoxes.AnchorBoxes.__init__": [[37, 106], ["numpy.array", "numpy.any", "keras.engine.topology.Layer.__init__", "keras.backend", "TypeError", "ValueError", "len", "ValueError", "ValueError", "len", "len", "keras.backend", "len"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_train.Evaluator.__init__"], ["def", "__init__", "(", "self", ",", "\n", "img_height", ",", "\n", "img_width", ",", "\n", "this_scale", ",", "\n", "next_scale", ",", "\n", "aspect_ratios", "=", "[", "0.5", ",", "1.0", ",", "2.0", "]", ",", "\n", "two_boxes_for_ar1", "=", "True", ",", "\n", "this_steps", "=", "None", ",", "\n", "this_offsets", "=", "None", ",", "\n", "clip_boxes", "=", "False", ",", "\n", "variances", "=", "[", "0.1", ",", "0.1", ",", "0.2", ",", "0.2", "]", ",", "\n", "coords", "=", "'centroids'", ",", "\n", "normalize_coords", "=", "False", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "'''\n        All arguments need to be set to the same values as in the box encoding process, otherwise the behavior is undefined.\n        Some of these arguments are explained in more detail in the documentation of the `SSDBoxEncoder` class.\n        Arguments:\n            img_height (int): The height of the input images.\n            img_width (int): The width of the input images.\n            this_scale (float): A float in [0, 1], the scaling factor for the size of the generated anchor boxes\n                as a fraction of the shorter side of the input image.\n            next_scale (float): A float in [0, 1], the next larger scaling factor. Only relevant if\n                `self.two_boxes_for_ar1 == True`.\n            aspect_ratios (list, optional): The list of aspect ratios for which default boxes are to be\n                generated for this layer.\n            two_boxes_for_ar1 (bool, optional): Only relevant if `aspect_ratios` contains 1.\n                If `True`, two default boxes will be generated for aspect ratio 1. The first will be generated\n                using the scaling factor for the respective layer, the second one will be generated using\n                geometric mean of said scaling factor and next bigger scaling factor.\n            clip_boxes (bool, optional): If `True`, clips the anchor box coordinates to stay within image boundaries.\n            variances (list, optional): A list of 4 floats >0. The anchor box offset for each coordinate will be divided by\n                its respective variance value.\n            coords (str, optional): The box coordinate format to be used internally in the model (i.e. this is not the input format\n                of the ground truth labels). Can be either 'centroids' for the format `(cx, cy, w, h)` (box center coordinates, width, and height),\n                'corners' for the format `(xmin, ymin, xmax,  ymax)`, or 'minmax' for the format `(xmin, xmax, ymin, ymax)`.\n            normalize_coords (bool, optional): Set to `True` if the model uses relative instead of absolute coordinates,\n                i.e. if the model predicts box coordinates within [0,1] instead of absolute coordinates.\n        '''", "\n", "if", "K", ".", "backend", "(", ")", "!=", "'tensorflow'", ":", "\n", "            ", "raise", "TypeError", "(", "\"This layer only supports TensorFlow at the moment, but you are using the {} backend.\"", ".", "format", "(", "K", ".", "backend", "(", ")", ")", ")", "\n", "\n", "", "if", "(", "this_scale", "<", "0", ")", "or", "(", "next_scale", "<", "0", ")", "or", "(", "this_scale", ">", "1", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`this_scale` must be in [0, 1] and `next_scale` must be >0, but `this_scale` == {}, `next_scale` == {}\"", ".", "format", "(", "this_scale", ",", "next_scale", ")", ")", "\n", "\n", "", "if", "len", "(", "variances", ")", "!=", "4", ":", "\n", "            ", "raise", "ValueError", "(", "\"4 variance values must be pased, but {} values were received.\"", ".", "format", "(", "len", "(", "variances", ")", ")", ")", "\n", "", "variances", "=", "np", ".", "array", "(", "variances", ")", "\n", "if", "np", ".", "any", "(", "variances", "<=", "0", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"All variances must be >0, but the variances given are {}\"", ".", "format", "(", "variances", ")", ")", "\n", "\n", "", "self", ".", "img_height", "=", "img_height", "\n", "self", ".", "img_width", "=", "img_width", "\n", "self", ".", "this_scale", "=", "this_scale", "\n", "self", ".", "next_scale", "=", "next_scale", "\n", "self", ".", "aspect_ratios", "=", "aspect_ratios", "\n", "self", ".", "two_boxes_for_ar1", "=", "two_boxes_for_ar1", "\n", "self", ".", "this_steps", "=", "this_steps", "\n", "self", ".", "this_offsets", "=", "this_offsets", "\n", "self", ".", "clip_boxes", "=", "clip_boxes", "\n", "self", ".", "variances", "=", "variances", "\n", "self", ".", "coords", "=", "coords", "\n", "self", ".", "normalize_coords", "=", "normalize_coords", "\n", "# Compute the number of boxes per cell", "\n", "if", "(", "1", "in", "aspect_ratios", ")", "and", "two_boxes_for_ar1", ":", "\n", "            ", "self", ".", "n_boxes", "=", "len", "(", "aspect_ratios", ")", "+", "1", "\n", "", "else", ":", "\n", "            ", "self", ".", "n_boxes", "=", "len", "(", "aspect_ratios", ")", "\n", "", "super", "(", "AnchorBoxes", ",", "self", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DeepAnchorBoxes.AnchorBoxes.build": [[107, 110], ["super().build", "keras.engine.topology.InputSpec"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DeepAnchorBoxes.AnchorBoxes.build"], ["", "def", "build", "(", "self", ",", "input_shape", ")", ":", "\n", "        ", "self", ".", "input_spec", "=", "[", "InputSpec", "(", "shape", "=", "input_shape", ")", "]", "\n", "super", "(", "AnchorBoxes", ",", "self", ")", ".", "build", "(", "input_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DeepAnchorBoxes.AnchorBoxes.call": [[111, 234], ["min", "numpy.array", "numpy.linspace", "numpy.linspace", "numpy.meshgrid", "numpy.expand_dims", "numpy.expand_dims", "numpy.zeros", "numpy.tile", "numpy.tile", "bounding_box_utils.bounding_box_utils.convert_coordinates", "numpy.zeros_like", "numpy.concatenate", "numpy.expand_dims", "keras.tile", "keras.image_dim_ordering", "bounding_box_utils.bounding_box_utils.convert_coordinates", "keras.constant", "numpy.array.append", "numpy.array.append", "isinstance", "isinstance", "isinstance", "isinstance", "bounding_box_utils.bounding_box_utils.convert_coordinates", "numpy.array.append", "numpy.sqrt", "numpy.sqrt", "len", "len", "keras.shape", "numpy.sqrt"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.convert_coordinates", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.convert_coordinates", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.convert_coordinates"], ["", "def", "call", "(", "self", ",", "x", ",", "mask", "=", "None", ")", ":", "\n", "        ", "'''\n        Return an anchor box tensor based on the shape of the input tensor.\n\n        The logic implemented here is identical to the logic in the module `ssd_box_encode_decode_utils.py`.\n\n        Note that this tensor does not participate in any graph computations at runtime. It is being created\n        as a constant once during graph creation and is just being output along with the rest of the model output\n        during runtime. Because of this, all logic is implemented as Numpy array operations and it is sufficient\n        to convert the resulting Numpy array into a Keras tensor at the very end before outputting it.\n\n        Arguments:\n            x (tensor): 4D tensor of shape `(batch, channels, height, width)` if `dim_ordering = 'th'`\n                or `(batch, height, width, channels)` if `dim_ordering = 'tf'`. The input for this\n                layer must be the output of the localization predictor layer.\n        '''", "\n", "\n", "# Compute box width and height for each aspect ratio", "\n", "# The shorter side of the image will be used to compute `w` and `h` using `scale` and `aspect_ratios`.", "\n", "size", "=", "min", "(", "self", ".", "img_height", ",", "self", ".", "img_width", ")", "\n", "# Compute the box widths and and heights for all aspect ratios", "\n", "wh_list", "=", "[", "]", "\n", "for", "ar", "in", "self", ".", "aspect_ratios", ":", "\n", "            ", "if", "(", "ar", "==", "1", ")", ":", "\n", "# Compute the regular anchor box for aspect ratio 1.", "\n", "                ", "box_height", "=", "box_width", "=", "self", ".", "this_scale", "*", "size", "\n", "wh_list", ".", "append", "(", "(", "box_width", ",", "box_height", ")", ")", "\n", "if", "self", ".", "two_boxes_for_ar1", ":", "\n", "# Compute one slightly larger version using the geometric mean of this scale value and the next.", "\n", "                    ", "box_height", "=", "box_width", "=", "np", ".", "sqrt", "(", "self", ".", "this_scale", "*", "self", ".", "next_scale", ")", "*", "size", "\n", "wh_list", ".", "append", "(", "(", "box_width", ",", "box_height", ")", ")", "\n", "", "", "else", ":", "\n", "                ", "box_height", "=", "self", ".", "this_scale", "*", "size", "/", "np", ".", "sqrt", "(", "ar", ")", "\n", "box_width", "=", "self", ".", "this_scale", "*", "size", "*", "np", ".", "sqrt", "(", "ar", ")", "\n", "wh_list", ".", "append", "(", "(", "box_width", ",", "box_height", ")", ")", "\n", "", "", "wh_list", "=", "np", ".", "array", "(", "wh_list", ")", "\n", "\n", "# We need the shape of the input tensor", "\n", "if", "K", ".", "image_dim_ordering", "(", ")", "==", "'tf'", ":", "\n", "            ", "batch_size", ",", "feature_map_height", ",", "feature_map_width", ",", "feature_map_channels", "=", "x", ".", "_keras_shape", "\n", "", "else", ":", "# Not yet relevant since TensorFlow is the only supported backend right now, but it can't harm to have this in here for the future", "\n", "            ", "batch_size", ",", "feature_map_channels", ",", "feature_map_height", ",", "feature_map_width", "=", "x", ".", "_keras_shape", "\n", "\n", "# Compute the grid of box center points. They are identical for all aspect ratios.", "\n", "\n", "# Compute the step sizes, i.e. how far apart the anchor box center points will be vertically and horizontally.", "\n", "", "if", "(", "self", ".", "this_steps", "is", "None", ")", ":", "\n", "            ", "step_height", "=", "self", ".", "img_height", "/", "feature_map_height", "\n", "step_width", "=", "self", ".", "img_width", "/", "feature_map_width", "\n", "", "else", ":", "\n", "            ", "if", "isinstance", "(", "self", ".", "this_steps", ",", "(", "list", ",", "tuple", ")", ")", "and", "(", "len", "(", "self", ".", "this_steps", ")", "==", "2", ")", ":", "\n", "                ", "step_height", "=", "self", ".", "this_steps", "[", "0", "]", "\n", "step_width", "=", "self", ".", "this_steps", "[", "1", "]", "\n", "", "elif", "isinstance", "(", "self", ".", "this_steps", ",", "(", "int", ",", "float", ")", ")", ":", "\n", "                ", "step_height", "=", "self", ".", "this_steps", "\n", "step_width", "=", "self", ".", "this_steps", "\n", "# Compute the offsets, i.e. at what pixel values the first anchor box center point will be from the top and from the left of the image.", "\n", "", "", "if", "(", "self", ".", "this_offsets", "is", "None", ")", ":", "\n", "            ", "offset_height", "=", "0.5", "\n", "offset_width", "=", "0.5", "\n", "", "else", ":", "\n", "            ", "if", "isinstance", "(", "self", ".", "this_offsets", ",", "(", "list", ",", "tuple", ")", ")", "and", "(", "len", "(", "self", ".", "this_offsets", ")", "==", "2", ")", ":", "\n", "                ", "offset_height", "=", "self", ".", "this_offsets", "[", "0", "]", "\n", "offset_width", "=", "self", ".", "this_offsets", "[", "1", "]", "\n", "", "elif", "isinstance", "(", "self", ".", "this_offsets", ",", "(", "int", ",", "float", ")", ")", ":", "\n", "                ", "offset_height", "=", "self", ".", "this_offsets", "\n", "offset_width", "=", "self", ".", "this_offsets", "\n", "# Now that we have the offsets and step sizes, compute the grid of anchor box center points.", "\n", "", "", "cy", "=", "np", ".", "linspace", "(", "offset_height", "*", "step_height", ",", "(", "offset_height", "+", "feature_map_height", "-", "1", ")", "*", "step_height", ",", "feature_map_height", ")", "\n", "cx", "=", "np", ".", "linspace", "(", "offset_width", "*", "step_width", ",", "(", "offset_width", "+", "feature_map_width", "-", "1", ")", "*", "step_width", ",", "feature_map_width", ")", "\n", "cx_grid", ",", "cy_grid", "=", "np", ".", "meshgrid", "(", "cx", ",", "cy", ")", "\n", "cx_grid", "=", "np", ".", "expand_dims", "(", "cx_grid", ",", "-", "1", ")", "# This is necessary for np.tile() to do what we want further down", "\n", "cy_grid", "=", "np", ".", "expand_dims", "(", "cy_grid", ",", "-", "1", ")", "# This is necessary for np.tile() to do what we want further down", "\n", "\n", "# Create a 4D tensor template of shape `(feature_map_height, feature_map_width, n_boxes, 4)`", "\n", "# where the last dimension will contain `(cx, cy, w, h)`", "\n", "boxes_tensor", "=", "np", ".", "zeros", "(", "(", "feature_map_height", ",", "feature_map_width", ",", "self", ".", "n_boxes", ",", "4", ")", ")", "\n", "\n", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "0", "]", "=", "np", ".", "tile", "(", "cx_grid", ",", "(", "1", ",", "1", ",", "self", ".", "n_boxes", ")", ")", "# Set cx", "\n", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "1", "]", "=", "np", ".", "tile", "(", "cy_grid", ",", "(", "1", ",", "1", ",", "self", ".", "n_boxes", ")", ")", "# Set cy", "\n", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "2", "]", "=", "wh_list", "[", ":", ",", "0", "]", "# Set w", "\n", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "3", "]", "=", "wh_list", "[", ":", ",", "1", "]", "# Set h", "\n", "\n", "# Convert `(cx, cy, w, h)` to `(xmin, xmax, ymin, ymax)`", "\n", "boxes_tensor", "=", "convert_coordinates", "(", "boxes_tensor", ",", "start_index", "=", "0", ",", "conversion", "=", "'centroids2corners'", ")", "\n", "\n", "# If `clip_boxes` is enabled, clip the coordinates to lie within the image boundaries", "\n", "if", "self", ".", "clip_boxes", ":", "\n", "            ", "x_coords", "=", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "[", "0", ",", "2", "]", "]", "\n", "x_coords", "[", "x_coords", ">=", "self", ".", "img_width", "]", "=", "self", ".", "img_width", "-", "1", "\n", "x_coords", "[", "x_coords", "<", "0", "]", "=", "0", "\n", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "[", "0", ",", "2", "]", "]", "=", "x_coords", "\n", "y_coords", "=", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "[", "1", ",", "3", "]", "]", "\n", "y_coords", "[", "y_coords", ">=", "self", ".", "img_height", "]", "=", "self", ".", "img_height", "-", "1", "\n", "y_coords", "[", "y_coords", "<", "0", "]", "=", "0", "\n", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "[", "1", ",", "3", "]", "]", "=", "y_coords", "\n", "\n", "# If `normalize_coords` is enabled, normalize the coordinates to be within [0,1]", "\n", "", "if", "self", ".", "normalize_coords", ":", "\n", "            ", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "[", "0", ",", "2", "]", "]", "/=", "self", ".", "img_width", "\n", "boxes_tensor", "[", ":", ",", ":", ",", ":", ",", "[", "1", ",", "3", "]", "]", "/=", "self", ".", "img_height", "\n", "\n", "# TODO: Implement box limiting directly for `(cx, cy, w, h)` so that we don't have to unnecessarily convert back and forth.", "\n", "", "if", "self", ".", "coords", "==", "'centroids'", ":", "\n", "# Convert `(xmin, ymin, xmax, ymax)` back to `(cx, cy, w, h)`.", "\n", "            ", "boxes_tensor", "=", "convert_coordinates", "(", "boxes_tensor", ",", "start_index", "=", "0", ",", "conversion", "=", "'corners2centroids'", ",", "border_pixels", "=", "'half'", ")", "\n", "", "elif", "self", ".", "coords", "==", "'minmax'", ":", "\n", "# Convert `(xmin, ymin, xmax, ymax)` to `(xmin, xmax, ymin, ymax).", "\n", "            ", "boxes_tensor", "=", "convert_coordinates", "(", "boxes_tensor", ",", "start_index", "=", "0", ",", "conversion", "=", "'corners2minmax'", ",", "border_pixels", "=", "'half'", ")", "\n", "\n", "# Create a tensor to contain the variances and append it to `boxes_tensor`. This tensor has the same shape", "\n", "# as `boxes_tensor` and simply contains the same 4 variance values for every position in the last axis.", "\n", "", "variances_tensor", "=", "np", ".", "zeros_like", "(", "boxes_tensor", ")", "# Has shape `(feature_map_height, feature_map_width, n_boxes, 4)`", "\n", "variances_tensor", "+=", "self", ".", "variances", "# Long live broadcasting", "\n", "# Now `boxes_tensor` becomes a tensor of shape `(feature_map_height, feature_map_width, n_boxes, 8)`", "\n", "boxes_tensor", "=", "np", ".", "concatenate", "(", "(", "boxes_tensor", ",", "variances_tensor", ")", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# Now prepend one dimension to `boxes_tensor` to account for the batch size and tile it along", "\n", "# The result will be a 5D tensor of shape `(batch_size, feature_map_height, feature_map_width, n_boxes, 8)`", "\n", "boxes_tensor", "=", "np", ".", "expand_dims", "(", "boxes_tensor", ",", "axis", "=", "0", ")", "\n", "boxes_tensor", "=", "K", ".", "tile", "(", "K", ".", "constant", "(", "boxes_tensor", ",", "dtype", "=", "'float32'", ")", ",", "(", "K", ".", "shape", "(", "x", ")", "[", "0", "]", ",", "1", ",", "1", ",", "1", ",", "1", ")", ")", "\n", "\n", "return", "boxes_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DeepAnchorBoxes.AnchorBoxes.compute_output_shape": [[235, 241], ["keras.image_dim_ordering"], "methods", ["None"], ["", "def", "compute_output_shape", "(", "self", ",", "input_shape", ")", ":", "\n", "        ", "if", "K", ".", "image_dim_ordering", "(", ")", "==", "'tf'", ":", "\n", "            ", "batch_size", ",", "feature_map_height", ",", "feature_map_width", ",", "feature_map_channels", "=", "input_shape", "\n", "", "else", ":", "# Not yet relevant since TensorFlow is the only supported backend right now, but it can't harm to have this in here for the future", "\n", "            ", "batch_size", ",", "feature_map_channels", ",", "feature_map_height", ",", "feature_map_width", "=", "input_shape", "\n", "", "return", "(", "batch_size", ",", "feature_map_height", ",", "feature_map_width", ",", "self", ".", "n_boxes", ",", "8", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DeepAnchorBoxes.AnchorBoxes.get_config": [[242, 257], ["super().get_config", "dict", "list", "list", "list", "list", "super().get_config.items", "config.items"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.keras_layers.keras_layer_DeepAnchorBoxes.AnchorBoxes.get_config"], ["", "def", "get_config", "(", "self", ")", ":", "\n", "        ", "config", "=", "{", "\n", "'img_height'", ":", "self", ".", "img_height", ",", "\n", "'img_width'", ":", "self", ".", "img_width", ",", "\n", "'this_scale'", ":", "self", ".", "this_scale", ",", "\n", "'next_scale'", ":", "self", ".", "next_scale", ",", "\n", "'aspect_ratios'", ":", "list", "(", "self", ".", "aspect_ratios", ")", ",", "\n", "'two_boxes_for_ar1'", ":", "self", ".", "two_boxes_for_ar1", ",", "\n", "'clip_boxes'", ":", "self", ".", "clip_boxes", ",", "\n", "'variances'", ":", "list", "(", "self", ".", "variances", ")", ",", "\n", "'coords'", ":", "self", ".", "coords", ",", "\n", "'normalize_coords'", ":", "self", ".", "normalize_coords", "\n", "}", "\n", "base_config", "=", "super", "(", "AnchorBoxes", ",", "self", ")", ".", "get_config", "(", ")", "\n", "return", "dict", "(", "list", "(", "base_config", ".", "items", "(", ")", ")", "+", "list", "(", "config", ".", "items", "(", ")", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_test.Evaluator.__init__": [[31, 79], ["isinstance", "warnings.warn"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "model", ",", "\n", "modelindex", ",", "\n", "n_classes", ",", "\n", "data_generator", ",", "\n", "model_mode", "=", "'inference'", ",", "\n", "detection_mode", "=", "'test'", ",", "\n", "pred_format", "=", "{", "'class_id'", ":", "0", ",", "'conf'", ":", "1", ",", "'xmin'", ":", "2", ",", "'ymin'", ":", "3", ",", "'xmax'", ":", "4", ",", "'ymax'", ":", "5", "}", ",", "\n", "gt_format", "=", "{", "'class_id'", ":", "1", ",", "'xmin'", ":", "2", ",", "'ymin'", ":", "3", ",", "'xmax'", ":", "4", ",", "'ymax'", ":", "5", "}", ")", ":", "\n", "        ", "'''\n        Arguments:\n            model (Keras model): A Keras SSD model object.\n            n_classes (int): The number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO.\n            data_generator (DataGenerator): A `DataGenerator` object with the evaluation dataset.\n            model_mode (str, optional): The mode in which the model was created, i.e. 'training', 'inference' or 'inference_fast'.\n                This is needed in order to know whether the model output is already decoded or still needs to be decoded. Refer to\n                the model documentation for the meaning of the individual modes.\n            pred_format (dict, optional): A dictionary that defines which index in the last axis of the model's decoded predictions\n                contains which bounding box coordinate. The dictionary must map the keywords 'class_id', 'conf' (for the confidence),\n                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis.\n            gt_format (list, optional): A dictionary that defines which index of a ground truth bounding box contains which of the five\n                items class ID, xmin, ymin, xmax, ymax. The expected strings are 'xmin', 'ymin', 'xmax', 'ymax', 'class_id'.\n        '''", "\n", "\n", "if", "not", "isinstance", "(", "data_generator", ",", "DataGenerator", ")", ":", "\n", "            ", "warnings", ".", "warn", "(", "\"`data_generator` is not a `DataGenerator` object, which will cause undefined behavior.\"", ")", "\n", "\n", "", "self", ".", "model", "=", "model", "\n", "self", ".", "modelindex", "=", "modelindex", "\n", "self", ".", "data_generator", "=", "data_generator", "\n", "self", ".", "n_classes", "=", "n_classes", "\n", "self", ".", "model_mode", "=", "model_mode", "\n", "self", ".", "detection_mode", "=", "detection_mode", "\n", "self", ".", "pred_format", "=", "pred_format", "\n", "self", ".", "gt_format", "=", "gt_format", "\n", "\n", "# The following lists all contain per-class data, i.e. all list have the length `n_classes + 1`,", "\n", "# where one element is for the background class, i.e. that element is just a dummy entry.", "\n", "self", ".", "prediction_results", "=", "None", "\n", "self", ".", "num_gt_per_class", "=", "None", "\n", "self", ".", "true_positives", "=", "None", "\n", "self", ".", "false_positives", "=", "None", "\n", "self", ".", "cumulative_true_positives", "=", "None", "\n", "self", ".", "cumulative_false_positives", "=", "None", "\n", "self", ".", "cumulative_precisions", "=", "None", "# \"Cumulative\" means that the i-th element in each list represents the precision for the first i highest condidence predictions for that class.", "\n", "self", ".", "cumulative_recalls", "=", "None", "# \"Cumulative\" means that the i-th element in each list represents the recall for the first i highest condidence predictions for that class.", "\n", "self", ".", "average_precisions", "=", "None", "\n", "self", ".", "mean_average_precision", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_test.Evaluator.__call__": [[80, 188], ["average_precision_evaluator_test.Evaluator.predict_on_dataset"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_train.Evaluator.predict_on_dataset"], ["", "def", "__call__", "(", "self", ",", "\n", "img_height", ",", "\n", "img_width", ",", "\n", "batch_size", ",", "\n", "data_generator_mode", "=", "'resize'", ",", "\n", "round_confidences", "=", "False", ",", "\n", "matching_iou_threshold", "=", "0.5", ",", "\n", "border_pixels", "=", "'include'", ",", "\n", "sorting_algorithm", "=", "'quicksort'", ",", "\n", "average_precision_mode", "=", "'sample'", ",", "\n", "num_recall_points", "=", "11", ",", "\n", "ignore_neutral_boxes", "=", "True", ",", "\n", "return_precisions", "=", "False", ",", "\n", "return_recalls", "=", "False", ",", "\n", "return_average_precisions", "=", "False", ",", "\n", "verbose", "=", "True", ",", "\n", "decoding_confidence_thresh", "=", "0.01", ",", "\n", "decoding_iou_threshold", "=", "0.45", ",", "\n", "decoding_top_k", "=", "200", ",", "\n", "decoding_pred_coords", "=", "'centroids'", ",", "\n", "decoding_normalize_coords", "=", "True", ")", ":", "\n", "        ", "'''\n        Computes the mean average precision of the given Keras SSD model on the given dataset.\n\n        Optionally also returns the averages precisions, precisions, and recalls.\n\n        All the individual steps of the overall evaluation algorithm can also be called separately\n        (check out the other methods of this class), but this runs the overall algorithm all at once.\n\n        Arguments:\n            img_height (int): The input image height for the model.\n            img_width (int): The input image width for the model.\n            batch_size (int): The batch size for the evaluation.\n            data_generator_mode (str, optional): Either of 'resize' and 'pad'. If 'resize', the input images will\n                be resized (i.e. warped) to `(img_height, img_width)`. This mode does not preserve the aspect ratios of the images.\n                If 'pad', the input images will be first padded so that they have the aspect ratio defined by `img_height`\n                and `img_width` and then resized to `(img_height, img_width)`. This mode preserves the aspect ratios of the images.\n            round_confidences (int, optional): `False` or an integer that is the number of decimals that the prediction\n                confidences will be rounded to. If `False`, the confidences will not be rounded.\n            matching_iou_threshold (float, optional): A prediction will be considered a true positive if it has a Jaccard overlap\n                of at least `matching_iou_threshold` with any ground truth bounding box of the same class.\n            border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n                Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n                to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n                If 'half', then one of each of the two horizontal and vertical borders belong\n                to the boxex, but not the other.\n            sorting_algorithm (str, optional): Which sorting algorithm the matching algorithm should use. This argument accepts\n                any valid sorting algorithm for Numpy's `argsort()` function. You will usually want to choose between 'quicksort'\n                (fastest and most memory efficient, but not stable) and 'mergesort' (slight slower and less memory efficient, but stable).\n                The official Matlab evaluation algorithm uses a stable sorting algorithm, so this algorithm is only guaranteed\n                to behave identically if you choose 'mergesort' as the sorting algorithm, but it will almost always behave identically\n                even if you choose 'quicksort' (but no guarantees).\n            average_precision_mode (str, optional): Can be either 'sample' or 'integrate'. In the case of 'sample', the average precision\n                will be computed according to the Pascal VOC formula that was used up until VOC 2009, where the precision will be sampled\n                for `num_recall_points` recall values. In the case of 'integrate', the average precision will be computed according to the\n                Pascal VOC formula that was used from VOC 2010 onward, where the average precision will be computed by numerically integrating\n                over the whole preciscion-recall curve instead of sampling individual points from it. 'integrate' mode is basically just\n                the limit case of 'sample' mode as the number of sample points increases.\n            num_recall_points (int, optional): The number of points to sample from the precision-recall-curve to compute the average\n                precisions. In other words, this is the number of equidistant recall values for which the resulting precision will be\n                computed. 11 points is the value used in the official Pascal VOC 2007 detection evaluation algorithm.\n            ignore_neutral_boxes (bool, optional): In case the data generator provides annotations indicating whether a ground truth\n                bounding box is supposed to either count or be neutral for the evaluation, this argument decides what to do with these\n                annotations. If `False`, even boxes that are annotated as neutral will be counted into the evaluation. If `True`,\n                neutral boxes will be ignored for the evaluation. An example for evaluation-neutrality are the ground truth boxes\n                annotated as \"difficult\" in the Pascal VOC datasets, which are usually treated as neutral for the evaluation.\n            return_precisions (bool, optional): If `True`, returns a nested list containing the cumulative precisions for each class.\n            return_recalls (bool, optional): If `True`, returns a nested list containing the cumulative recalls for each class.\n            return_average_precisions (bool, optional): If `True`, returns a list containing the average precision for each class.\n            verbose (bool, optional): If `True`, will print out the progress during runtime.\n            decoding_confidence_thresh (float, optional): Only relevant if the model is in 'training' mode.\n                A float in [0,1), the minimum classification confidence in a specific positive class in order to be considered\n                for the non-maximum suppression stage for the respective class. A lower value will result in a larger part of the\n                selection process being done by the non-maximum suppression stage, while a larger value will result in a larger\n                part of the selection process happening in the confidence thresholding stage.\n            decoding_iou_threshold (float, optional): Only relevant if the model is in 'training' mode. A float in [0,1].\n                All boxes with a Jaccard similarity of greater than `iou_threshold` with a locally maximal box will be removed\n                from the set of predictions for a given class, where 'maximal' refers to the box score.\n            decoding_top_k (int, optional): Only relevant if the model is in 'training' mode. The number of highest scoring\n                predictions to be kept for each batch item after the non-maximum suppression stage.\n            decoding_input_coords (str, optional): Only relevant if the model is in 'training' mode. The box coordinate format\n                that the model outputs. Can be either 'centroids' for the format `(cx, cy, w, h)` (box center coordinates, width, and height),\n                'minmax' for the format `(xmin, xmax, ymin, ymax)`, or 'corners' for the format `(xmin, ymin, xmax, ymax)`.\n            decoding_normalize_coords (bool, optional): Only relevant if the model is in 'training' mode. Set to `True` if the model\n                outputs relative coordinates. Do not set this to `True` if the model already outputs absolute coordinates,\n                as that would result in incorrect coordinates.\n\n        Returns:\n            A float, the mean average precision, plus any optional returns specified in the arguments.\n        '''", "\n", "\n", "#############################################################################################", "\n", "# Predict on the entire dataset.", "\n", "#############################################################################################", "\n", "\n", "self", ".", "predict_on_dataset", "(", "img_height", "=", "img_height", ",", "\n", "img_width", "=", "img_width", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "data_generator_mode", "=", "data_generator_mode", ",", "\n", "decoding_confidence_thresh", "=", "decoding_confidence_thresh", ",", "\n", "decoding_iou_threshold", "=", "decoding_iou_threshold", ",", "\n", "decoding_top_k", "=", "decoding_top_k", ",", "\n", "decoding_pred_coords", "=", "decoding_pred_coords", ",", "\n", "decoding_normalize_coords", "=", "decoding_normalize_coords", ",", "\n", "decoding_border_pixels", "=", "border_pixels", ",", "\n", "round_confidences", "=", "round_confidences", ",", "\n", "verbose", "=", "verbose", ",", "\n", "ret", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_test.Evaluator.predict_on_dataset": [[189, 365], ["data_generator.object_detection_2d_photometric_ops.ConvertTo3Channels", "data_generator.object_detection_2d_geometric_ops.Resize", "average_precision_evaluator_test.Evaluator.data_generator.generate", "average_precision_evaluator_test.Evaluator.data_generator.get_dataset_size", "int", "list", "list", "math.ceil", "print", "print", "tqdm.trange", "range.set_description", "range", "next", "average_precision_evaluator_test.Evaluator.model.predict", "data_generator.object_detection_2d_misc_utils.apply_inverse_transforms", "enumerate", "data_generator.object_detection_2d_patch_sampling_ops.RandomPadFixedAR", "ValueError", "range", "range", "ssd_encoder_decoder.ssd_output_decoder.decode_detections", "range", "os.path.join", "open", "open.close", "average_precision_evaluator_test.Evaluator.data_generator.get_dataset_size", "len", "y_pred_filtered.append", "str", "os.path.exists", "os.mkdir", "os.path.exists", "os.mknod", "int", "round", "round", "round", "round", "open.write", "results[].append", "round", "str", "str", "str", "str", "str"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_data_generator.DataGenerator.generate", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_data_generator.DataGenerator.get_dataset_size", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_misc_utils.apply_inverse_transforms", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.ssd_output_decoder.decode_detections", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_data_generator.DataGenerator.get_dataset_size"], ["", "def", "predict_on_dataset", "(", "self", ",", "\n", "img_height", ",", "\n", "img_width", ",", "\n", "batch_size", ",", "\n", "data_generator_mode", "=", "'resize'", ",", "\n", "decoding_confidence_thresh", "=", "0.01", ",", "\n", "decoding_iou_threshold", "=", "0.45", ",", "\n", "decoding_top_k", "=", "200", ",", "\n", "decoding_pred_coords", "=", "'centroids'", ",", "\n", "decoding_normalize_coords", "=", "True", ",", "\n", "decoding_border_pixels", "=", "'include'", ",", "\n", "round_confidences", "=", "False", ",", "\n", "verbose", "=", "True", ",", "\n", "ret", "=", "False", ")", ":", "\n", "        ", "'''\n        Runs predictions for the given model over the entire dataset given by `data_generator`.\n\n        Arguments:\n            img_height (int): The input image height for the model.\n            img_width (int): The input image width for the model.\n            batch_size (int): The batch size for the evaluation.\n            data_generator_mode (str, optional): Either of 'resize' and 'pad'. If 'resize', the input images will\n                be resized (i.e. warped) to `(img_height, img_width)`. This mode does not preserve the aspect ratios of the images.\n                If 'pad', the input images will be first padded so that they have the aspect ratio defined by `img_height`\n                and `img_width` and then resized to `(img_height, img_width)`. This mode preserves the aspect ratios of the images.\n            decoding_confidence_thresh (float, optional): Only relevant if the model is in 'training' mode.\n                A float in [0,1), the minimum classification confidence in a specific positive class in order to be considered\n                for the non-maximum suppression stage for the respective class. A lower value will result in a larger part of the\n                selection process being done by the non-maximum suppression stage, while a larger value will result in a larger\n                part of the selection process happening in the confidence thresholding stage.\n            decoding_iou_threshold (float, optional): Only relevant if the model is in 'training' mode. A float in [0,1].\n                All boxes with a Jaccard similarity of greater than `iou_threshold` with a locally maximal box will be removed\n                from the set of predictions for a given class, where 'maximal' refers to the box score.\n            decoding_top_k (int, optional): Only relevant if the model is in 'training' mode. The number of highest scoring\n                predictions to be kept for each batch item after the non-maximum suppression stage.\n            decoding_input_coords (str, optional): Only relevant if the model is in 'training' mode. The box coordinate format\n                that the model outputs. Can be either 'centroids' for the format `(cx, cy, w, h)` (box center coordinates, width, and height),\n                'minmax' for the format `(xmin, xmax, ymin, ymax)`, or 'corners' for the format `(xmin, ymin, xmax, ymax)`.\n            decoding_normalize_coords (bool, optional): Only relevant if the model is in 'training' mode. Set to `True` if the model\n                outputs relative coordinates. Do not set this to `True` if the model already outputs absolute coordinates,\n                as that would result in incorrect coordinates.\n            round_confidences (int, optional): `False` or an integer that is the number of decimals that the prediction\n                confidences will be rounded to. If `False`, the confidences will not be rounded.\n            verbose (bool, optional): If `True`, will print out the progress during runtime.\n            ret (bool, optional): If `True`, returns the predictions.\n\n        Returns:\n            None by default. Optionally, a nested list containing the predictions for each class.\n        '''", "\n", "\n", "class_id_pred", "=", "self", ".", "pred_format", "[", "'class_id'", "]", "\n", "conf_pred", "=", "self", ".", "pred_format", "[", "'conf'", "]", "\n", "xmin_pred", "=", "self", ".", "pred_format", "[", "'xmin'", "]", "\n", "ymin_pred", "=", "self", ".", "pred_format", "[", "'ymin'", "]", "\n", "xmax_pred", "=", "self", ".", "pred_format", "[", "'xmax'", "]", "\n", "ymax_pred", "=", "self", ".", "pred_format", "[", "'ymax'", "]", "\n", "\n", "#############################################################################################", "\n", "# Configure the data generator for the evaluation.", "\n", "#############################################################################################", "\n", "\n", "convert_to_3_channels", "=", "ConvertTo3Channels", "(", ")", "\n", "resize", "=", "Resize", "(", "height", "=", "img_height", ",", "width", "=", "img_width", ",", "labels_format", "=", "self", ".", "gt_format", ")", "\n", "if", "data_generator_mode", "==", "'resize'", ":", "\n", "            ", "transformations", "=", "[", "convert_to_3_channels", ",", "\n", "resize", "]", "\n", "", "elif", "data_generator_mode", "==", "'pad'", ":", "\n", "            ", "random_pad", "=", "RandomPadFixedAR", "(", "patch_aspect_ratio", "=", "img_width", "/", "img_height", ",", "labels_format", "=", "self", ".", "gt_format", ")", "\n", "transformations", "=", "[", "convert_to_3_channels", ",", "\n", "random_pad", ",", "\n", "resize", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"`data_generator_mode` can be either of 'resize' or 'pad', but received '{}'.\"", ".", "format", "(", "data_generator_mode", ")", ")", "\n", "\n", "# Set the generator parameters.", "\n", "", "generator", "=", "self", ".", "data_generator", ".", "generate", "(", "batch_size", "=", "batch_size", ",", "\n", "shuffle", "=", "False", ",", "\n", "transformations", "=", "transformations", ",", "\n", "label_encoder", "=", "None", ",", "\n", "returns", "=", "{", "'processed_images'", ",", "\n", "'image_ids'", ",", "\n", "'evaluation-neutral'", ",", "\n", "'inverse_transform'", ",", "\n", "'original_labels'", "}", ",", "\n", "keep_images_without_gt", "=", "True", ",", "\n", "degenerate_box_handling", "=", "'remove'", ")", "\n", "\n", "if", "self", ".", "data_generator", ".", "image_ids", "is", "None", ":", "\n", "            ", "self", ".", "data_generator", ".", "image_ids", "=", "list", "(", "range", "(", "self", ".", "data_generator", ".", "get_dataset_size", "(", ")", ")", ")", "\n", "\n", "#############################################################################################", "\n", "# Predict over all batches of the dataset and store the predictions.", "\n", "#############################################################################################", "\n", "\n", "# We have to generate a separate results list for each class.", "\n", "", "results", "=", "[", "list", "(", ")", "for", "_", "in", "range", "(", "self", ".", "n_classes", "+", "1", ")", "]", "\n", "\n", "# Create a dictionary that maps image IDs to ground truth annotations.", "\n", "# We'll need it below.", "\n", "image_ids_to_labels", "=", "{", "}", "\n", "\n", "# Compute the number of batches to iterate over the entire dataset.", "\n", "n_images", "=", "self", ".", "data_generator", ".", "get_dataset_size", "(", ")", "\n", "n_batches", "=", "int", "(", "ceil", "(", "n_images", "/", "batch_size", ")", ")", "\n", "if", "verbose", ":", "\n", "            ", "print", "(", "\"Number of images in the evaluation dataset: {}\"", ".", "format", "(", "n_images", ")", ")", "\n", "print", "(", ")", "\n", "tr", "=", "trange", "(", "n_batches", ",", "file", "=", "sys", ".", "stdout", ")", "\n", "tr", ".", "set_description", "(", "'Producing predictions batch-wise'", ")", "\n", "", "else", ":", "\n", "            ", "tr", "=", "range", "(", "n_batches", ")", "\n", "\n", "", "for", "j", "in", "tr", ":", "\n", "# Generate batch.", "\n", "            ", "batch_X", ",", "batch_image_ids", ",", "batch_eval_neutral", ",", "batch_inverse_transforms", ",", "batch_orig_labels", "=", "next", "(", "generator", ")", "\n", "# Predict.", "\n", "y_pred", "=", "self", ".", "model", ".", "predict", "(", "batch_X", ")", "\n", "# If the model was created in 'training' mode, the raw predictions need to", "\n", "# be decoded and filtered, otherwise that's already taken care of.", "\n", "if", "self", ".", "model_mode", "==", "'training'", ":", "\n", "# Decode.", "\n", "                ", "y_pred", "=", "decode_detections", "(", "y_pred", ",", "\n", "confidence_thresh", "=", "decoding_confidence_thresh", ",", "\n", "iou_threshold", "=", "decoding_iou_threshold", ",", "\n", "top_k", "=", "decoding_top_k", ",", "\n", "input_coords", "=", "decoding_pred_coords", ",", "\n", "normalize_coords", "=", "decoding_normalize_coords", ",", "\n", "img_height", "=", "img_height", ",", "\n", "img_width", "=", "img_width", ",", "\n", "border_pixels", "=", "decoding_border_pixels", ")", "\n", "", "else", ":", "\n", "# Filter out the all-zeros dummy elements of `y_pred`.", "\n", "                ", "y_pred_filtered", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "y_pred", ")", ")", ":", "\n", "                    ", "y_pred_filtered", ".", "append", "(", "y_pred", "[", "i", "]", "[", "y_pred", "[", "i", ",", ":", ",", "0", "]", "!=", "0", "]", ")", "\n", "", "y_pred", "=", "y_pred_filtered", "\n", "# Convert the predicted box coordinates for the original images.", "\n", "", "y_pred", "=", "apply_inverse_transforms", "(", "y_pred", ",", "batch_inverse_transforms", ")", "\n", "\n", "# Iterate over all batch items.", "\n", "for", "k", ",", "batch_item", "in", "enumerate", "(", "y_pred", ")", ":", "\n", "                ", "image_id", "=", "batch_image_ids", "[", "k", "]", "\n", "path", "=", "'/data/deeplearn/SWEIPENet/dataset/Detections/detection'", "+", "str", "(", "self", ".", "modelindex", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "                    ", "os", ".", "mkdir", "(", "path", ")", "\n", "", "txtpath", "=", "os", ".", "path", ".", "join", "(", "path", ",", "image_id", "+", "'.txt'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "txtpath", ")", ":", "\n", "                    ", "os", ".", "mknod", "(", "txtpath", ")", "\n", "", "file_fid", "=", "open", "(", "txtpath", ",", "'w'", ")", "\n", "for", "box", "in", "batch_item", ":", "\n", "                    ", "class_id", "=", "int", "(", "box", "[", "class_id_pred", "]", ")", "\n", "# Round the box coordinates to reduce the required memory.", "\n", "if", "round_confidences", ":", "\n", "                        ", "confidence", "=", "round", "(", "box", "[", "conf_pred", "]", ",", "round_confidences", ")", "\n", "", "else", ":", "\n", "                        ", "confidence", "=", "box", "[", "conf_pred", "]", "\n", "", "xmin", "=", "round", "(", "box", "[", "xmin_pred", "]", ",", "1", ")", "\n", "ymin", "=", "round", "(", "box", "[", "ymin_pred", "]", ",", "1", ")", "\n", "xmax", "=", "round", "(", "box", "[", "xmax_pred", "]", ",", "1", ")", "\n", "ymax", "=", "round", "(", "box", "[", "ymax_pred", "]", ",", "1", ")", "\n", "prediction", "=", "(", "image_id", ",", "confidence", ",", "xmin", ",", "ymin", ",", "xmax", ",", "ymax", ")", "\n", "# write detections of each image into Detections/imname.txt", "\n", "if", "class_id", "==", "1", ":", "\n", "                        ", "class_name", "=", "'seacucumber'", "\n", "", "if", "class_id", "==", "2", ":", "\n", "                        ", "class_name", "=", "'seaurchin'", "\n", "", "if", "class_id", "==", "3", ":", "\n", "                        ", "class_name", "=", "'scallop'", "\n", "", "boxstr", "=", "class_name", "+", "' '", "+", "str", "(", "confidence", ")", "+", "' '", "+", "str", "(", "xmin", ")", "+", "' '", "+", "str", "(", "ymin", ")", "+", "' '", "+", "str", "(", "xmax", ")", "+", "' '", "+", "str", "(", "ymax", ")", "\n", "file_fid", ".", "write", "(", "boxstr", "+", "'\\n'", ")", "\n", "\n", "results", "[", "class_id", "]", ".", "append", "(", "prediction", ")", "\n", "", "file_fid", ".", "close", "(", ")", "\n", "", "", "self", ".", "prediction_results", "=", "results", "\n", "if", "ret", ":", "\n", "            ", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_test.Evaluator.write_predictions_to_txt": [[367, 417], ["range", "ValueError", "open", "open.close", "print", "print", "list", "round", "open.write", "map"], "methods", ["None"], ["", "", "def", "write_predictions_to_txt", "(", "self", ",", "\n", "classes", "=", "None", ",", "\n", "out_file_prefix", "=", "'comp3_det_test_'", ",", "\n", "verbose", "=", "True", ")", ":", "\n", "        ", "'''\n        Writes the predictions for all classes to separate text files according to the Pascal VOC results format.\n\n        Arguments:\n            classes (list, optional): `None` or a list of strings containing the class names of all classes in the dataset,\n                including some arbitrary name for the background class. This list will be used to name the output text files.\n                The ordering of the names in the list represents the ordering of the classes as they are predicted by the model,\n                i.e. the element with index 3 in this list should correspond to the class with class ID 3 in the model's predictions.\n                If `None`, the output text files will be named by their class IDs.\n            out_file_prefix (str, optional): A prefix for the output text file names. The suffix to each output text file name will\n                be the respective class name followed by the `.txt` file extension. This string is also how you specify the directory\n                in which the results are to be saved.\n            verbose (bool, optional): If `True`, will print out the progress during runtime.\n\n        Returns:\n            None.\n        '''", "\n", "\n", "if", "self", ".", "prediction_results", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"There are no prediction results. You must run `predict_on_dataset()` before calling this method.\"", ")", "\n", "\n", "# We generate a separate results file for each class.", "\n", "", "for", "class_id", "in", "range", "(", "1", ",", "self", ".", "n_classes", "+", "1", ")", ":", "\n", "\n", "            ", "if", "verbose", ":", "\n", "                ", "print", "(", "\"Writing results file for class {}/{}.\"", ".", "format", "(", "class_id", ",", "self", ".", "n_classes", ")", ")", "\n", "\n", "", "if", "classes", "is", "None", ":", "\n", "                ", "class_suffix", "=", "'{:04d}'", ".", "format", "(", "class_id", ")", "\n", "", "else", ":", "\n", "                ", "class_suffix", "=", "classes", "[", "class_id", "]", "\n", "\n", "", "results_file", "=", "open", "(", "'{}{}.txt'", ".", "format", "(", "out_file_prefix", ",", "class_suffix", ")", ",", "'w'", ")", "\n", "\n", "for", "prediction", "in", "self", ".", "prediction_results", "[", "class_id", "]", ":", "\n", "\n", "                ", "prediction_list", "=", "list", "(", "prediction", ")", "\n", "prediction_list", "[", "0", "]", "=", "'{}'", ".", "format", "(", "prediction_list", "[", "0", "]", ")", "\n", "prediction_list", "[", "1", "]", "=", "round", "(", "prediction_list", "[", "1", "]", ",", "4", ")", "\n", "prediction_txt", "=", "' '", ".", "join", "(", "map", "(", "str", ",", "prediction_list", ")", ")", "+", "'\\n'", "\n", "results_file", ".", "write", "(", "prediction_txt", ")", "\n", "\n", "", "results_file", ".", "close", "(", ")", "\n", "\n", "", "if", "verbose", ":", "\n", "            ", "print", "(", "\"All results files saved.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_test.Evaluator.get_num_gt_per_class": [[418, 478], ["numpy.zeros", "ValueError", "print", "tqdm.trange", "range", "numpy.asarray", "range", "len", "len"], "methods", ["None"], ["", "", "def", "get_num_gt_per_class", "(", "self", ",", "\n", "ignore_neutral_boxes", "=", "True", ",", "\n", "verbose", "=", "True", ",", "\n", "ret", "=", "False", ")", ":", "\n", "        ", "'''\n        Counts the number of ground truth boxes for each class across the dataset.\n\n        Arguments:\n            ignore_neutral_boxes (bool, optional): In case the data generator provides annotations indicating whether a ground truth\n                bounding box is supposed to either count or be neutral for the evaluation, this argument decides what to do with these\n                annotations. If `True`, only non-neutral ground truth boxes will be counted, otherwise all ground truth boxes will\n                be counted.\n            verbose (bool, optional): If `True`, will print out the progress during runtime.\n            ret (bool, optional): If `True`, returns the list of counts.\n\n        Returns:\n            None by default. Optionally, a list containing a count of the number of ground truth boxes for each class across the\n            entire dataset.\n        '''", "\n", "\n", "if", "self", ".", "data_generator", ".", "labels", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Computing the number of ground truth boxes per class not possible, no ground truth given.\"", ")", "\n", "\n", "", "num_gt_per_class", "=", "np", ".", "zeros", "(", "shape", "=", "(", "self", ".", "n_classes", "+", "1", ")", ",", "dtype", "=", "np", ".", "int", ")", "\n", "\n", "class_id_index", "=", "self", ".", "gt_format", "[", "'class_id'", "]", "\n", "\n", "ground_truth", "=", "self", ".", "data_generator", ".", "labels", "\n", "\n", "if", "verbose", ":", "\n", "            ", "print", "(", "'Computing the number of positive ground truth boxes per class.'", ")", "\n", "tr", "=", "trange", "(", "len", "(", "ground_truth", ")", ",", "file", "=", "sys", ".", "stdout", ")", "\n", "", "else", ":", "\n", "            ", "tr", "=", "range", "(", "len", "(", "ground_truth", ")", ")", "\n", "\n", "# Iterate over the ground truth for all images in the dataset.", "\n", "", "for", "i", "in", "tr", ":", "\n", "\n", "            ", "boxes", "=", "np", ".", "asarray", "(", "ground_truth", "[", "i", "]", ")", "\n", "\n", "# Iterate over all ground truth boxes for the current image.", "\n", "for", "j", "in", "range", "(", "boxes", ".", "shape", "[", "0", "]", ")", ":", "\n", "\n", "                ", "if", "ignore_neutral_boxes", "and", "not", "(", "self", ".", "data_generator", ".", "eval_neutral", "is", "None", ")", ":", "\n", "                    ", "if", "not", "self", ".", "data_generator", ".", "eval_neutral", "[", "i", "]", "[", "j", "]", ":", "\n", "# If this box is not supposed to be evaluation-neutral,", "\n", "# increment the counter for the respective class ID.", "\n", "                        ", "class_id", "=", "boxes", "[", "j", ",", "class_id_index", "]", "\n", "num_gt_per_class", "[", "class_id", "]", "+=", "1", "\n", "", "", "else", ":", "\n", "# If there is no such thing as evaluation-neutral boxes for", "\n", "# our dataset, always increment the counter for the respective", "\n", "# class ID.", "\n", "                    ", "class_id", "=", "boxes", "[", "j", ",", "class_id_index", "]", "\n", "num_gt_per_class", "[", "class_id", "]", "+=", "1", "\n", "\n", "", "", "", "self", ".", "num_gt_per_class", "=", "num_gt_per_class", "\n", "\n", "if", "ret", ":", "\n", "            ", "return", "num_gt_per_class", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_test.Evaluator.match_predictions": [[479, 678], ["range", "range", "ValueError", "ValueError", "len", "str", "numpy.zeros", "numpy.zeros", "numpy.dtype", "numpy.array", "numpy.argsort", "true_positives.append", "false_positives.append", "numpy.cumsum", "numpy.cumsum", "cumulative_true_positives.append", "cumulative_false_positives.append", "numpy.asarray", "len", "len", "len", "print", "true_positives.append", "false_positives.append", "len", "tqdm.trange", "range.set_description", "range", "numpy.asarray", "numpy.asarray", "bounding_box_utils.bounding_box_utils.iou", "numpy.argmax", "numpy.asarray", "numpy.asarray", "str", "len", "len", "list", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.iou"], ["", "", "def", "match_predictions", "(", "self", ",", "\n", "ignore_neutral_boxes", "=", "True", ",", "\n", "matching_iou_threshold", "=", "0.5", ",", "\n", "border_pixels", "=", "'include'", ",", "\n", "sorting_algorithm", "=", "'quicksort'", ",", "\n", "verbose", "=", "True", ",", "\n", "ret", "=", "False", ")", ":", "\n", "        ", "'''\n        Matches predictions to ground truth boxes.\n\n        Note that `predict_on_dataset()` must be called before calling this method.\n\n        Arguments:\n            ignore_neutral_boxes (bool, optional): In case the data generator provides annotations indicating whether a ground truth\n                bounding box is supposed to either count or be neutral for the evaluation, this argument decides what to do with these\n                annotations. If `False`, even boxes that are annotated as neutral will be counted into the evaluation. If `True`,\n                neutral boxes will be ignored for the evaluation. An example for evaluation-neutrality are the ground truth boxes\n                annotated as \"difficult\" in the Pascal VOC datasets, which are usually treated as neutral for the evaluation.\n            matching_iou_threshold (float, optional): A prediction will be considered a true positive if it has a Jaccard overlap\n                of at least `matching_iou_threshold` with any ground truth bounding box of the same class.\n            border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n                Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n                to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n                If 'half', then one of each of the two horizontal and vertical borders belong\n                to the boxex, but not the other.\n            sorting_algorithm (str, optional): Which sorting algorithm the matching algorithm should use. This argument accepts\n                any valid sorting algorithm for Numpy's `argsort()` function. You will usually want to choose between 'quicksort'\n                (fastest and most memory efficient, but not stable) and 'mergesort' (slight slower and less memory efficient, but stable).\n                The official Matlab evaluation algorithm uses a stable sorting algorithm, so this algorithm is only guaranteed\n                to behave identically if you choose 'mergesort' as the sorting algorithm, but it will almost always behave identically\n                even if you choose 'quicksort' (but no guarantees).\n            verbose (bool, optional): If `True`, will print out the progress during runtime.\n            ret (bool, optional): If `True`, returns the true and false positives.\n\n        Returns:\n            None by default. Optionally, four nested lists containing the true positives, false positives, cumulative true positives,\n            and cumulative false positives for each class.\n        '''", "\n", "\n", "if", "self", ".", "data_generator", ".", "labels", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Matching predictions to ground truth boxes not possible, no ground truth given.\"", ")", "\n", "\n", "", "if", "self", ".", "prediction_results", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"There are no prediction results. You must run `predict_on_dataset()` before calling this method.\"", ")", "\n", "\n", "", "class_id_gt", "=", "self", ".", "gt_format", "[", "'class_id'", "]", "\n", "xmin_gt", "=", "self", ".", "gt_format", "[", "'xmin'", "]", "\n", "ymin_gt", "=", "self", ".", "gt_format", "[", "'ymin'", "]", "\n", "xmax_gt", "=", "self", ".", "gt_format", "[", "'xmax'", "]", "\n", "ymax_gt", "=", "self", ".", "gt_format", "[", "'ymax'", "]", "\n", "\n", "# Convert the ground truth to a more efficient format for what we need", "\n", "# to do, which is access ground truth by image ID repeatedly.", "\n", "ground_truth", "=", "{", "}", "\n", "eval_neutral_available", "=", "not", "(", "self", ".", "data_generator", ".", "eval_neutral", "is", "None", ")", "# Whether or not we have annotations to decide whether ground truth boxes should be neutral or not.", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "data_generator", ".", "image_ids", ")", ")", ":", "\n", "            ", "image_id", "=", "str", "(", "self", ".", "data_generator", ".", "image_ids", "[", "i", "]", ")", "\n", "labels", "=", "self", ".", "data_generator", ".", "labels", "[", "i", "]", "\n", "if", "ignore_neutral_boxes", "and", "eval_neutral_available", ":", "\n", "                ", "ground_truth", "[", "image_id", "]", "=", "(", "np", ".", "asarray", "(", "labels", ")", ",", "np", ".", "asarray", "(", "self", ".", "data_generator", ".", "eval_neutral", "[", "i", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "ground_truth", "[", "image_id", "]", "=", "np", ".", "asarray", "(", "labels", ")", "\n", "\n", "", "", "true_positives", "=", "[", "[", "]", "]", "# The false positives for each class, sorted by descending confidence.", "\n", "false_positives", "=", "[", "[", "]", "]", "# The true positives for each class, sorted by descending confidence.", "\n", "cumulative_true_positives", "=", "[", "[", "]", "]", "\n", "cumulative_false_positives", "=", "[", "[", "]", "]", "\n", "\n", "# Iterate over all classes.", "\n", "for", "class_id", "in", "range", "(", "1", ",", "self", ".", "n_classes", "+", "1", ")", ":", "\n", "\n", "            ", "predictions", "=", "self", ".", "prediction_results", "[", "class_id", "]", "\n", "\n", "# Store the matching results in these lists:", "\n", "true_pos", "=", "np", ".", "zeros", "(", "len", "(", "predictions", ")", ",", "dtype", "=", "np", ".", "int", ")", "# 1 for every prediction that is a true positive, 0 otherwise", "\n", "false_pos", "=", "np", ".", "zeros", "(", "len", "(", "predictions", ")", ",", "dtype", "=", "np", ".", "int", ")", "# 1 for every prediction that is a false positive, 0 otherwise", "\n", "\n", "# In case there are no predictions at all for this class, we're done here.", "\n", "if", "len", "(", "predictions", ")", "==", "0", ":", "\n", "                ", "print", "(", "\"No predictions for class {}/{}\"", ".", "format", "(", "class_id", ",", "self", ".", "n_classes", ")", ")", "\n", "true_positives", ".", "append", "(", "true_pos", ")", "\n", "false_positives", ".", "append", "(", "false_pos", ")", "\n", "continue", "\n", "\n", "# Convert the predictions list for this class into a structured array so that we can sort it by confidence.", "\n", "\n", "# Get the number of characters needed to store the image ID strings in the structured array.", "\n", "", "num_chars_per_image_id", "=", "len", "(", "str", "(", "predictions", "[", "0", "]", "[", "0", "]", ")", ")", "+", "6", "# Keep a few characters buffer in case some image IDs are longer than others.", "\n", "# Create the data type for the structured array.", "\n", "preds_data_type", "=", "np", ".", "dtype", "(", "[", "(", "'image_id'", ",", "'U{}'", ".", "format", "(", "num_chars_per_image_id", ")", ")", ",", "\n", "(", "'confidence'", ",", "'f4'", ")", ",", "\n", "(", "'xmin'", ",", "'f4'", ")", ",", "\n", "(", "'ymin'", ",", "'f4'", ")", ",", "\n", "(", "'xmax'", ",", "'f4'", ")", ",", "\n", "(", "'ymax'", ",", "'f4'", ")", "]", ")", "\n", "# Create the structured array", "\n", "predictions", "=", "np", ".", "array", "(", "predictions", ",", "dtype", "=", "preds_data_type", ")", "\n", "\n", "# Sort the detections by decreasing confidence.", "\n", "descending_indices", "=", "np", ".", "argsort", "(", "-", "predictions", "[", "'confidence'", "]", ",", "kind", "=", "sorting_algorithm", ")", "\n", "predictions_sorted", "=", "predictions", "[", "descending_indices", "]", "\n", "\n", "if", "verbose", ":", "\n", "                ", "tr", "=", "trange", "(", "len", "(", "predictions", ")", ",", "file", "=", "sys", ".", "stdout", ")", "\n", "tr", ".", "set_description", "(", "\"Matching predictions to ground truth, class {}/{}.\"", ".", "format", "(", "class_id", ",", "self", ".", "n_classes", ")", ")", "\n", "", "else", ":", "\n", "                ", "tr", "=", "range", "(", "len", "(", "predictions", ".", "shape", ")", ")", "\n", "\n", "# Keep track of which ground truth boxes were already matched to a detection.", "\n", "", "gt_matched", "=", "{", "}", "\n", "\n", "# Iterate over all predictions.", "\n", "for", "i", "in", "tr", ":", "\n", "\n", "                ", "prediction", "=", "predictions_sorted", "[", "i", "]", "\n", "image_id", "=", "prediction", "[", "'image_id'", "]", "\n", "pred_box", "=", "np", ".", "asarray", "(", "list", "(", "prediction", "[", "[", "'xmin'", ",", "'ymin'", ",", "'xmax'", ",", "'ymax'", "]", "]", ")", ")", "# Convert the structured array element to a regular array.", "\n", "\n", "# Get the relevant ground truth boxes for this prediction,", "\n", "# i.e. all ground truth boxes that match the prediction's", "\n", "# image ID and class ID.", "\n", "\n", "# The ground truth could either be a tuple with `(ground_truth_boxes, eval_neutral_boxes)`", "\n", "# or only `ground_truth_boxes`.", "\n", "if", "ignore_neutral_boxes", "and", "eval_neutral_available", ":", "\n", "                    ", "gt", ",", "eval_neutral", "=", "ground_truth", "[", "image_id", "]", "\n", "", "else", ":", "\n", "                    ", "gt", "=", "ground_truth", "[", "image_id", "]", "\n", "", "gt", "=", "np", ".", "asarray", "(", "gt", ")", "\n", "class_mask", "=", "gt", "[", ":", ",", "class_id_gt", "]", "==", "class_id", "\n", "gt", "=", "gt", "[", "class_mask", "]", "\n", "if", "ignore_neutral_boxes", "and", "eval_neutral_available", ":", "\n", "                    ", "eval_neutral", "=", "eval_neutral", "[", "class_mask", "]", "\n", "\n", "", "if", "gt", ".", "size", "==", "0", ":", "\n", "# If the image doesn't contain any objects of this class,", "\n", "# the prediction becomes a false positive.", "\n", "                    ", "false_pos", "[", "i", "]", "=", "1", "\n", "continue", "\n", "\n", "# Compute the IoU of this prediction with all ground truth boxes of the same class.", "\n", "", "overlaps", "=", "iou", "(", "boxes1", "=", "gt", "[", ":", ",", "[", "xmin_gt", ",", "ymin_gt", ",", "xmax_gt", ",", "ymax_gt", "]", "]", ",", "\n", "boxes2", "=", "pred_box", ",", "\n", "coords", "=", "'corners'", ",", "\n", "mode", "=", "'element-wise'", ",", "\n", "border_pixels", "=", "border_pixels", ")", "\n", "\n", "# For each detection, match the ground truth box with the highest overlap.", "\n", "# It's possible that the same ground truth box will be matched to multiple", "\n", "# detections.", "\n", "gt_match_index", "=", "np", ".", "argmax", "(", "overlaps", ")", "\n", "gt_match_overlap", "=", "overlaps", "[", "gt_match_index", "]", "\n", "\n", "if", "gt_match_overlap", "<", "matching_iou_threshold", ":", "\n", "# False positive, IoU threshold violated:", "\n", "# Those predictions whose matched overlap is below the threshold become", "\n", "# false positives.", "\n", "                    ", "false_pos", "[", "i", "]", "=", "1", "\n", "", "else", ":", "\n", "                    ", "if", "not", "(", "ignore_neutral_boxes", "and", "eval_neutral_available", ")", "or", "(", "eval_neutral", "[", "gt_match_index", "]", "==", "False", ")", ":", "\n", "# If this is not a ground truth that is supposed to be evaluation-neutral", "\n", "# (i.e. should be skipped for the evaluation) or if we don't even have the", "\n", "# concept of neutral boxes.", "\n", "                        ", "if", "not", "(", "image_id", "in", "gt_matched", ")", ":", "\n", "# True positive:", "\n", "# If the matched ground truth box for this prediction hasn't been matched to a", "\n", "# different prediction already, we have a true positive.", "\n", "                            ", "true_pos", "[", "i", "]", "=", "1", "\n", "gt_matched", "[", "image_id", "]", "=", "np", ".", "zeros", "(", "shape", "=", "(", "gt", ".", "shape", "[", "0", "]", ")", ",", "dtype", "=", "np", ".", "bool", ")", "\n", "gt_matched", "[", "image_id", "]", "[", "gt_match_index", "]", "=", "True", "\n", "", "elif", "not", "gt_matched", "[", "image_id", "]", "[", "gt_match_index", "]", ":", "\n", "# True positive:", "\n", "# If the matched ground truth box for this prediction hasn't been matched to a", "\n", "# different prediction already, we have a true positive.", "\n", "                            ", "true_pos", "[", "i", "]", "=", "1", "\n", "gt_matched", "[", "image_id", "]", "[", "gt_match_index", "]", "=", "True", "\n", "", "else", ":", "\n", "# False positive, duplicate detection:", "\n", "# If the matched ground truth box for this prediction has already been matched", "\n", "# to a different prediction previously, it is a duplicate detection for an", "\n", "# already detected object, which counts as a false positive.", "\n", "                            ", "false_pos", "[", "i", "]", "=", "1", "\n", "\n", "", "", "", "", "true_positives", ".", "append", "(", "true_pos", ")", "\n", "false_positives", ".", "append", "(", "false_pos", ")", "\n", "\n", "cumulative_true_pos", "=", "np", ".", "cumsum", "(", "true_pos", ")", "# Cumulative sums of the true positives", "\n", "cumulative_false_pos", "=", "np", ".", "cumsum", "(", "false_pos", ")", "# Cumulative sums of the false positives", "\n", "\n", "cumulative_true_positives", ".", "append", "(", "cumulative_true_pos", ")", "\n", "cumulative_false_positives", ".", "append", "(", "cumulative_false_pos", ")", "\n", "\n", "", "self", ".", "true_positives", "=", "true_positives", "\n", "self", ".", "false_positives", "=", "false_positives", "\n", "self", ".", "cumulative_true_positives", "=", "cumulative_true_positives", "\n", "self", ".", "cumulative_false_positives", "=", "cumulative_false_positives", "\n", "\n", "if", "ret", ":", "\n", "            ", "return", "true_positives", ",", "false_positives", ",", "cumulative_true_positives", ",", "cumulative_false_positives", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_test.Evaluator.compute_precision_recall": [[679, 726], ["average_precision_evaluator_test.Evaluator.write_predictions_to_txt", "range", "ValueError", "ValueError", "numpy.where", "cumulative_precisions.append", "cumulative_recalls.append", "print"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_train.Evaluator.write_predictions_to_txt"], ["", "", "def", "compute_precision_recall", "(", "self", ",", "verbose", "=", "True", ",", "ret", "=", "False", ")", ":", "\n", "        ", "'''\n        Computes the precisions and recalls for all classes.\n\n        Note that `match_predictions()` must be called before calling this method.\n\n        Arguments:\n            verbose (bool, optional): If `True`, will print out the progress during runtime.\n            ret (bool, optional): If `True`, returns the precisions and recalls.\n\n        Returns:\n            None by default. Optionally, two nested lists containing the cumulative precisions and recalls for each class.\n        '''", "\n", "\n", "if", "(", "self", ".", "cumulative_true_positives", "is", "None", ")", "or", "(", "self", ".", "cumulative_false_positives", "is", "None", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"True and false positives not available. You must run `match_predictions()` before you call this method.\"", ")", "\n", "\n", "", "if", "(", "self", ".", "num_gt_per_class", "is", "None", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Number of ground truth boxes per class not available. You must run `get_num_gt_per_class()` before you call this method.\"", ")", "\n", "\n", "", "cumulative_precisions", "=", "[", "[", "]", "]", "\n", "cumulative_recalls", "=", "[", "[", "]", "]", "\n", "\n", "classes", "=", "[", "'background'", ",", "'seacucumber'", ",", "'seaurchin'", ",", "'scallop'", "]", "\n", "self", ".", "write_predictions_to_txt", "(", "classes", ",", "out_file_prefix", "=", "'comp3_det_test_'", ",", "verbose", "=", "True", ")", "\n", "\n", "# Iterate over all classes.", "\n", "for", "class_id", "in", "range", "(", "1", ",", "self", ".", "n_classes", "+", "1", ")", ":", "\n", "\n", "            ", "if", "verbose", ":", "\n", "                ", "print", "(", "\"Computing precisions and recalls, class {}/{}\"", ".", "format", "(", "class_id", ",", "self", ".", "n_classes", ")", ")", "\n", "\n", "", "tp", "=", "self", ".", "cumulative_true_positives", "[", "class_id", "]", "\n", "fp", "=", "self", ".", "cumulative_false_positives", "[", "class_id", "]", "\n", "\n", "\n", "cumulative_precision", "=", "np", ".", "where", "(", "tp", "+", "fp", ">", "0", ",", "tp", "/", "(", "tp", "+", "fp", ")", ",", "0", ")", "# 1D array with shape `(num_predictions,)`", "\n", "cumulative_recall", "=", "tp", "/", "self", ".", "num_gt_per_class", "[", "class_id", "]", "# 1D array with shape `(num_predictions,)`", "\n", "\n", "cumulative_precisions", ".", "append", "(", "cumulative_precision", ")", "\n", "cumulative_recalls", ".", "append", "(", "cumulative_recall", ")", "\n", "\n", "", "self", ".", "cumulative_precisions", "=", "cumulative_precisions", "\n", "self", ".", "cumulative_recalls", "=", "cumulative_recalls", "\n", "\n", "if", "ret", ":", "\n", "            ", "return", "cumulative_precisions", ",", "cumulative_recalls", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_test.Evaluator.compute_average_precisions": [[727, 829], ["range", "ValueError", "ValueError", "average_precisions.append", "print", "numpy.linspace", "numpy.unique", "numpy.zeros_like", "numpy.zeros_like", "range", "numpy.sum", "numpy.amax", "numpy.maximum", "len", "numpy.amax"], "methods", ["None"], ["", "", "def", "compute_average_precisions", "(", "self", ",", "mode", "=", "'sample'", ",", "num_recall_points", "=", "11", ",", "verbose", "=", "True", ",", "ret", "=", "False", ")", ":", "\n", "        ", "'''\n        Computes the average precision for each class.\n\n        Can compute the Pascal-VOC-style average precision in both the pre-2010 (k-point sampling)\n        and post-2010 (integration) algorithm versions.\n\n        Note that `compute_precision_recall()` must be called before calling this method.\n\n        Arguments:\n            mode (str, optional): Can be either 'sample' or 'integrate'. In the case of 'sample', the average precision will be computed\n                according to the Pascal VOC formula that was used up until VOC 2009, where the precision will be sampled for `num_recall_points`\n                recall values. In the case of 'integrate', the average precision will be computed according to the Pascal VOC formula that\n                was used from VOC 2010 onward, where the average precision will be computed by numerically integrating over the whole\n                preciscion-recall curve instead of sampling individual points from it. 'integrate' mode is basically just the limit case\n                of 'sample' mode as the number of sample points increases. For details, see the references below.\n            num_recall_points (int, optional): Only relevant if mode is 'sample'. The number of points to sample from the precision-recall-curve\n                to compute the average precisions. In other words, this is the number of equidistant recall values for which the resulting\n                precision will be computed. 11 points is the value used in the official Pascal VOC pre-2010 detection evaluation algorithm.\n            verbose (bool, optional): If `True`, will print out the progress during runtime.\n            ret (bool, optional): If `True`, returns the average precisions.\n\n        Returns:\n            None by default. Optionally, a list containing average precision for each class.\n\n        References:\n            http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/devkit_doc.html#sec:ap\n        '''", "\n", "\n", "if", "(", "self", ".", "cumulative_precisions", "is", "None", ")", "or", "(", "self", ".", "cumulative_recalls", "is", "None", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Precisions and recalls not available. You must run `compute_precision_recall()` before you call this method.\"", ")", "\n", "\n", "", "if", "not", "(", "mode", "in", "{", "'sample'", ",", "'integrate'", "}", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`mode` can be either 'sample' or 'integrate', but received '{}'\"", ".", "format", "(", "mode", ")", ")", "\n", "\n", "", "average_precisions", "=", "[", "0.0", "]", "\n", "\n", "# Iterate over all classes.", "\n", "for", "class_id", "in", "range", "(", "1", ",", "self", ".", "n_classes", "+", "1", ")", ":", "\n", "\n", "            ", "if", "verbose", ":", "\n", "                ", "print", "(", "\"Computing average precision, class {}/{}\"", ".", "format", "(", "class_id", ",", "self", ".", "n_classes", ")", ")", "\n", "\n", "", "cumulative_precision", "=", "self", ".", "cumulative_precisions", "[", "class_id", "]", "\n", "cumulative_recall", "=", "self", ".", "cumulative_recalls", "[", "class_id", "]", "\n", "average_precision", "=", "0.0", "\n", "\n", "if", "mode", "==", "'sample'", ":", "\n", "\n", "                ", "for", "t", "in", "np", ".", "linspace", "(", "start", "=", "0", ",", "stop", "=", "1", ",", "num", "=", "num_recall_points", ",", "endpoint", "=", "True", ")", ":", "\n", "\n", "                    ", "cum_prec_recall_greater_t", "=", "cumulative_precision", "[", "cumulative_recall", ">=", "t", "]", "\n", "\n", "if", "cum_prec_recall_greater_t", ".", "size", "==", "0", ":", "\n", "                        ", "precision", "=", "0.0", "\n", "", "else", ":", "\n", "                        ", "precision", "=", "np", ".", "amax", "(", "cum_prec_recall_greater_t", ")", "\n", "\n", "", "average_precision", "+=", "precision", "\n", "\n", "", "average_precision", "/=", "num_recall_points", "\n", "\n", "", "elif", "mode", "==", "'integrate'", ":", "\n", "\n", "# We will compute the precision at all unique recall values.", "\n", "                ", "unique_recalls", ",", "unique_recall_indices", ",", "unique_recall_counts", "=", "np", ".", "unique", "(", "cumulative_recall", ",", "return_index", "=", "True", ",", "return_counts", "=", "True", ")", "\n", "\n", "# Store the maximal precision for each recall value and the absolute difference", "\n", "# between any two unique recal values in the lists below. The products of these", "\n", "# two nummbers constitute the rectangular areas whose sum will be our numerical", "\n", "# integral.", "\n", "maximal_precisions", "=", "np", ".", "zeros_like", "(", "unique_recalls", ")", "\n", "recall_deltas", "=", "np", ".", "zeros_like", "(", "unique_recalls", ")", "\n", "\n", "# Iterate over all unique recall values in reverse order. This saves a lot of computation:", "\n", "# For each unique recall value `r`, we want to get the maximal precision value obtained", "\n", "# for any recall value `r* >= r`. Once we know the maximal precision for the last `k` recall", "\n", "# values after a given iteration, then in the next iteration, in order compute the maximal", "\n", "# precisions for the last `l > k` recall values, we only need to compute the maximal precision", "\n", "# for `l - k` recall values and then take the maximum between that and the previously computed", "\n", "# maximum instead of computing the maximum over all `l` values.", "\n", "# We skip the very last recall value, since the precision after between the last recall value", "\n", "# recall 1.0 is defined to be zero.", "\n", "for", "i", "in", "range", "(", "len", "(", "unique_recalls", ")", "-", "2", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "                    ", "begin", "=", "unique_recall_indices", "[", "i", "]", "\n", "end", "=", "unique_recall_indices", "[", "i", "+", "1", "]", "\n", "# When computing the maximal precisions, use the maximum of the previous iteration to", "\n", "# avoid unnecessary repeated computation over the same precision values.", "\n", "# The maximal precisions are the heights of the rectangle areas of our integral under", "\n", "# the precision-recall curve.", "\n", "maximal_precisions", "[", "i", "]", "=", "np", ".", "maximum", "(", "np", ".", "amax", "(", "cumulative_precision", "[", "begin", ":", "end", "]", ")", ",", "maximal_precisions", "[", "i", "+", "1", "]", ")", "\n", "# The differences between two adjacent recall values are the widths of our rectangle areas.", "\n", "recall_deltas", "[", "i", "]", "=", "unique_recalls", "[", "i", "+", "1", "]", "-", "unique_recalls", "[", "i", "]", "\n", "\n", "", "average_precision", "=", "np", ".", "sum", "(", "maximal_precisions", "*", "recall_deltas", ")", "\n", "\n", "", "average_precisions", ".", "append", "(", "average_precision", ")", "\n", "\n", "", "self", ".", "average_precisions", "=", "average_precisions", "\n", "\n", "if", "ret", ":", "\n", "            ", "return", "average_precisions", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_test.Evaluator.compute_mean_average_precision": [[830, 851], ["numpy.average", "ValueError"], "methods", ["None"], ["", "", "def", "compute_mean_average_precision", "(", "self", ",", "ret", "=", "True", ")", ":", "\n", "        ", "'''\n        Computes the mean average precision over all classes.\n\n        Note that `compute_average_precisions()` must be called before calling this method.\n\n        Arguments:\n            ret (bool, optional): If `True`, returns the mean average precision.\n\n        Returns:\n            A float, the mean average precision, by default. Optionally, None.\n        '''", "\n", "\n", "if", "self", ".", "average_precisions", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Average precisions not available. You must run `compute_average_precisions()` before you call this method.\"", ")", "\n", "\n", "", "mean_average_precision", "=", "np", ".", "average", "(", "self", ".", "average_precisions", "[", "1", ":", "]", ")", "# The first element is for the background class, so skip it.", "\n", "self", ".", "mean_average_precision", "=", "mean_average_precision", "\n", "\n", "if", "ret", ":", "\n", "            ", "return", "mean_average_precision", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.coco_utils.get_coco_category_maps": [[30, 61], ["classes_to_names.append", "enumerate", "open", "json.load", "classes_to_names.append"], "function", ["None"], ["def", "get_coco_category_maps", "(", "annotations_file", ")", ":", "\n", "    ", "'''\n    Builds dictionaries that map between MS COCO category IDs, transformed category IDs, and category names.\n    The original MS COCO category IDs are not consecutive unfortunately: The 80 category IDs are spread\n    across the integers 1 through 90 with some integers skipped. Since we usually use a one-hot\n    class representation in neural networks, we need to map these non-consecutive original COCO category\n    IDs (let's call them 'cats') to consecutive category IDs (let's call them 'classes').\n\n    Arguments:\n        annotations_file (str): The filepath to any MS COCO annotations JSON file.\n\n    Returns:\n        1) cats_to_classes: A dictionary that maps between the original (keys) and the transformed category IDs (values).\n        2) classes_to_cats: A dictionary that maps between the transformed (keys) and the original category IDs (values).\n        3) cats_to_names: A dictionary that maps between original category IDs (keys) and the respective category names (values).\n        4) classes_to_names: A list of the category names (values) with their indices representing the transformed IDs.\n    '''", "\n", "with", "open", "(", "annotations_file", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "annotations", "=", "json", ".", "load", "(", "f", ")", "\n", "", "cats_to_classes", "=", "{", "}", "\n", "classes_to_cats", "=", "{", "}", "\n", "cats_to_names", "=", "{", "}", "\n", "classes_to_names", "=", "[", "]", "\n", "classes_to_names", ".", "append", "(", "'background'", ")", "# Need to add the background class first so that the indexing is right.", "\n", "for", "i", ",", "cat", "in", "enumerate", "(", "annotations", "[", "'categories'", "]", ")", ":", "\n", "        ", "cats_to_classes", "[", "cat", "[", "'id'", "]", "]", "=", "i", "+", "1", "\n", "classes_to_cats", "[", "i", "+", "1", "]", "=", "cat", "[", "'id'", "]", "\n", "cats_to_names", "[", "cat", "[", "'id'", "]", "]", "=", "cat", "[", "'name'", "]", "\n", "classes_to_names", ".", "append", "(", "cat", "[", "'name'", "]", ")", "\n", "\n", "", "return", "cats_to_classes", ",", "classes_to_cats", ",", "cats_to_names", ",", "classes_to_names", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.coco_utils.predict_all_to_json": [[62, 201], ["data_generator.object_detection_2d_photometric_ops.ConvertTo3Channels", "data_generator.object_detection_2d_geometric_ops.Resize", "data_generator.generate", "data_generator.get_dataset_size", "print", "int", "tqdm.trange", "tqdm.trange.set_description", "print", "math.ceil", "next", "model.predict", "data_generator.object_detection_2d_misc_utils.apply_inverse_transforms", "enumerate", "open", "json.dump", "data_generator.object_detection_2d_patch_sampling_ops.RandomPadFixedAR", "ValueError", "ssd_encoder_decoder.ssd_output_decoder.decode_detections", "range", "len", "y_pred_filtered.append", "float", "float", "float", "float", "float", "results.append", "round", "round", "round", "round", "round"], "function", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_data_generator.DataGenerator.generate", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_data_generator.DataGenerator.get_dataset_size", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_misc_utils.apply_inverse_transforms", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.ssd_output_decoder.decode_detections"], ["", "def", "predict_all_to_json", "(", "out_file", ",", "\n", "model", ",", "\n", "img_height", ",", "\n", "img_width", ",", "\n", "classes_to_cats", ",", "\n", "data_generator", ",", "\n", "batch_size", ",", "\n", "data_generator_mode", "=", "'resize'", ",", "\n", "model_mode", "=", "'training'", ",", "\n", "confidence_thresh", "=", "0.01", ",", "\n", "iou_threshold", "=", "0.45", ",", "\n", "top_k", "=", "200", ",", "\n", "pred_coords", "=", "'centroids'", ",", "\n", "normalize_coords", "=", "True", ")", ":", "\n", "    ", "'''\n    Runs detection predictions over the whole dataset given a model and saves them in a JSON file\n    in the MS COCO detection results format.\n\n    Arguments:\n        out_file (str): The file name (full path) under which to save the results JSON file.\n        model (Keras model): A Keras SSD model object.\n        img_height (int): The input image height for the model.\n        img_width (int): The input image width for the model.\n        classes_to_cats (dict): A dictionary that maps the consecutive class IDs predicted by the model\n            to the non-consecutive original MS COCO category IDs.\n        data_generator (DataGenerator): A `DataGenerator` object with the evaluation dataset.\n        batch_size (int): The batch size for the evaluation.\n        data_generator_mode (str, optional): Either of 'resize' or 'pad'. If 'resize', the input images will\n            be resized (i.e. warped) to `(img_height, img_width)`. This mode does not preserve the aspect ratios of the images.\n            If 'pad', the input images will be first padded so that they have the aspect ratio defined by `img_height`\n            and `img_width` and then resized to `(img_height, img_width)`. This mode preserves the aspect ratios of the images.\n        model_mode (str, optional): The mode in which the model was created, i.e. 'training', 'inference' or 'inference_fast'.\n            This is needed in order to know whether the model output is already decoded or still needs to be decoded. Refer to\n            the model documentation for the meaning of the individual modes.\n        confidence_thresh (float, optional): A float in [0,1), the minimum classification confidence in a specific\n            positive class in order to be considered for the non-maximum suppression stage for the respective class.\n            A lower value will result in a larger part of the selection process being done by the non-maximum suppression\n            stage, while a larger value will result in a larger part of the selection process happening in the confidence\n            thresholding stage.\n        iou_threshold (float, optional): A float in [0,1]. All boxes with a Jaccard similarity of greater than `iou_threshold`\n            with a locally maximal box will be removed from the set of predictions for a given class, where 'maximal' refers\n            to the box score.\n        top_k (int, optional): The number of highest scoring predictions to be kept for each batch item after the\n            non-maximum suppression stage. Defaults to 200, following the paper.\n        input_coords (str, optional): The box coordinate format that the model outputs. Can be either 'centroids'\n            for the format `(cx, cy, w, h)` (box center coordinates, width, and height), 'minmax' for the format\n            `(xmin, xmax, ymin, ymax)`, or 'corners' for the format `(xmin, ymin, xmax, ymax)`.\n        normalize_coords (bool, optional): Set to `True` if the model outputs relative coordinates (i.e. coordinates in [0,1])\n            and you wish to transform these relative coordinates back to absolute coordinates. If the model outputs\n            relative coordinates, but you do not want to convert them back to absolute coordinates, set this to `False`.\n            Do not set this to `True` if the model already outputs absolute coordinates, as that would result in incorrect\n            coordinates. Requires `img_height` and `img_width` if set to `True`.\n\n    Returns:\n        None.\n    '''", "\n", "\n", "convert_to_3_channels", "=", "ConvertTo3Channels", "(", ")", "\n", "resize", "=", "Resize", "(", "height", "=", "img_height", ",", "width", "=", "img_width", ")", "\n", "if", "data_generator_mode", "==", "'resize'", ":", "\n", "        ", "transformations", "=", "[", "convert_to_3_channels", ",", "\n", "resize", "]", "\n", "", "elif", "data_generator_mode", "==", "'pad'", ":", "\n", "        ", "random_pad", "=", "RandomPadFixedAR", "(", "patch_aspect_ratio", "=", "img_width", "/", "img_height", ",", "clip_boxes", "=", "False", ")", "\n", "transformations", "=", "[", "convert_to_3_channels", ",", "\n", "random_pad", ",", "\n", "resize", "]", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unexpected argument value: `data_generator_mode` can be either of 'resize' or 'pad', but received '{}'.\"", ".", "format", "(", "data_generator_mode", ")", ")", "\n", "\n", "# Set the generator parameters.", "\n", "", "generator", "=", "data_generator", ".", "generate", "(", "batch_size", "=", "batch_size", ",", "\n", "shuffle", "=", "False", ",", "\n", "transformations", "=", "transformations", ",", "\n", "label_encoder", "=", "None", ",", "\n", "returns", "=", "{", "'processed_images'", ",", "\n", "'image_ids'", ",", "\n", "'inverse_transform'", "}", ",", "\n", "keep_images_without_gt", "=", "True", ")", "\n", "# Put the results in this list.", "\n", "results", "=", "[", "]", "\n", "# Compute the number of batches to iterate over the entire dataset.", "\n", "n_images", "=", "data_generator", ".", "get_dataset_size", "(", ")", "\n", "print", "(", "\"Number of images in the evaluation dataset: {}\"", ".", "format", "(", "n_images", ")", ")", "\n", "n_batches", "=", "int", "(", "ceil", "(", "n_images", "/", "batch_size", ")", ")", "\n", "# Loop over all batches.", "\n", "tr", "=", "trange", "(", "n_batches", ",", "file", "=", "sys", ".", "stdout", ")", "\n", "tr", ".", "set_description", "(", "'Producing results file'", ")", "\n", "for", "i", "in", "tr", ":", "\n", "# Generate batch.", "\n", "        ", "batch_X", ",", "batch_image_ids", ",", "batch_inverse_transforms", "=", "next", "(", "generator", ")", "\n", "# Predict.", "\n", "y_pred", "=", "model", ".", "predict", "(", "batch_X", ")", "\n", "# If the model was created in 'training' mode, the raw predictions need to", "\n", "# be decoded and filtered, otherwise that's already taken care of.", "\n", "if", "model_mode", "==", "'training'", ":", "\n", "# Decode.", "\n", "            ", "y_pred", "=", "decode_detections", "(", "y_pred", ",", "\n", "confidence_thresh", "=", "confidence_thresh", ",", "\n", "iou_threshold", "=", "iou_threshold", ",", "\n", "top_k", "=", "top_k", ",", "\n", "input_coords", "=", "pred_coords", ",", "\n", "normalize_coords", "=", "normalize_coords", ",", "\n", "img_height", "=", "img_height", ",", "\n", "img_width", "=", "img_width", ")", "\n", "", "else", ":", "\n", "# Filter out the all-zeros dummy elements of `y_pred`.", "\n", "            ", "y_pred_filtered", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "y_pred", ")", ")", ":", "\n", "                ", "y_pred_filtered", ".", "append", "(", "y_pred", "[", "i", "]", "[", "y_pred", "[", "i", ",", ":", ",", "0", "]", "!=", "0", "]", ")", "\n", "", "y_pred", "=", "y_pred_filtered", "\n", "# Convert the predicted box coordinates for the original images.", "\n", "", "y_pred", "=", "apply_inverse_transforms", "(", "y_pred", ",", "batch_inverse_transforms", ")", "\n", "\n", "# Convert each predicted box into the results format.", "\n", "for", "k", ",", "batch_item", "in", "enumerate", "(", "y_pred", ")", ":", "\n", "            ", "for", "box", "in", "batch_item", ":", "\n", "                ", "class_id", "=", "box", "[", "0", "]", "\n", "# Transform the consecutive class IDs back to the original COCO category IDs.", "\n", "cat_id", "=", "classes_to_cats", "[", "class_id", "]", "\n", "# Round the box coordinates to reduce the JSON file size.", "\n", "xmin", "=", "float", "(", "round", "(", "box", "[", "2", "]", ",", "1", ")", ")", "\n", "ymin", "=", "float", "(", "round", "(", "box", "[", "3", "]", ",", "1", ")", ")", "\n", "xmax", "=", "float", "(", "round", "(", "box", "[", "4", "]", ",", "1", ")", ")", "\n", "ymax", "=", "float", "(", "round", "(", "box", "[", "5", "]", ",", "1", ")", ")", "\n", "width", "=", "xmax", "-", "xmin", "\n", "height", "=", "ymax", "-", "ymin", "\n", "bbox", "=", "[", "xmin", ",", "ymin", ",", "width", ",", "height", "]", "\n", "result", "=", "{", "}", "\n", "result", "[", "'image_id'", "]", "=", "batch_image_ids", "[", "k", "]", "\n", "result", "[", "'category_id'", "]", "=", "cat_id", "\n", "result", "[", "'score'", "]", "=", "float", "(", "round", "(", "box", "[", "1", "]", ",", "3", ")", ")", "\n", "result", "[", "'bbox'", "]", "=", "bbox", "\n", "results", ".", "append", "(", "result", ")", "\n", "\n", "", "", "", "with", "open", "(", "out_file", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "results", ",", "f", ")", "\n", "\n", "", "print", "(", "\"Prediction results saved in '{}'\"", ".", "format", "(", "out_file", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_train.Evaluator.__init__": [[20, 66], ["isinstance", "warnings.warn"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "\n", "model", ",", "\n", "n_classes", ",", "\n", "data_generator", ",", "\n", "model_mode", "=", "'inference'", ",", "\n", "detection_mode", "=", "'test'", ",", "\n", "pred_format", "=", "{", "'class_id'", ":", "0", ",", "'conf'", ":", "1", ",", "'xmin'", ":", "2", ",", "'ymin'", ":", "3", ",", "'xmax'", ":", "4", ",", "'ymax'", ":", "5", "}", ",", "\n", "gt_format", "=", "{", "'class_id'", ":", "1", ",", "'xmin'", ":", "2", ",", "'ymin'", ":", "3", ",", "'xmax'", ":", "4", ",", "'ymax'", ":", "5", "}", ")", ":", "\n", "        ", "'''\n        Arguments:\n            model (Keras model): A Keras SSD model object.\n            n_classes (int): The number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO.\n            data_generator (DataGenerator): A `DataGenerator` object with the evaluation dataset.\n            model_mode (str, optional): The mode in which the model was created, i.e. 'training', 'inference' or 'inference_fast'.\n                This is needed in order to know whether the model output is already decoded or still needs to be decoded. Refer to\n                the model documentation for the meaning of the individual modes.\n            pred_format (dict, optional): A dictionary that defines which index in the last axis of the model's decoded predictions\n                contains which bounding box coordinate. The dictionary must map the keywords 'class_id', 'conf' (for the confidence),\n                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis.\n            gt_format (list, optional): A dictionary that defines which index of a ground truth bounding box contains which of the five\n                items class ID, xmin, ymin, xmax, ymax. The expected strings are 'xmin', 'ymin', 'xmax', 'ymax', 'class_id'.\n        '''", "\n", "\n", "if", "not", "isinstance", "(", "data_generator", ",", "DataGenerator", ")", ":", "\n", "            ", "warnings", ".", "warn", "(", "\"`data_generator` is not a `DataGenerator` object, which will cause undefined behavior.\"", ")", "\n", "\n", "", "self", ".", "model", "=", "model", "\n", "self", ".", "data_generator", "=", "data_generator", "\n", "self", ".", "n_classes", "=", "n_classes", "\n", "self", ".", "model_mode", "=", "model_mode", "\n", "self", ".", "detection_mode", "=", "detection_mode", "\n", "self", ".", "pred_format", "=", "pred_format", "\n", "self", ".", "gt_format", "=", "gt_format", "\n", "\n", "# The following lists all contain per-class data, i.e. all list have the length `n_classes + 1`,", "\n", "# where one element is for the background class, i.e. that element is just a dummy entry.", "\n", "self", ".", "prediction_results", "=", "None", "\n", "self", ".", "num_gt_per_class", "=", "None", "\n", "self", ".", "true_positives", "=", "None", "\n", "self", ".", "false_positives", "=", "None", "\n", "self", ".", "cumulative_true_positives", "=", "None", "\n", "self", ".", "cumulative_false_positives", "=", "None", "\n", "self", ".", "cumulative_precisions", "=", "None", "# \"Cumulative\" means that the i-th element in each list represents the precision for the first i highest condidence predictions for that class.", "\n", "self", ".", "cumulative_recalls", "=", "None", "# \"Cumulative\" means that the i-th element in each list represents the recall for the first i highest condidence predictions for that class.", "\n", "self", ".", "average_precisions", "=", "None", "\n", "self", ".", "mean_average_precision", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_train.Evaluator.__call__": [[67, 175], ["average_precision_evaluator_train.Evaluator.predict_on_dataset"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_train.Evaluator.predict_on_dataset"], ["", "def", "__call__", "(", "self", ",", "\n", "img_height", ",", "\n", "img_width", ",", "\n", "batch_size", ",", "\n", "data_generator_mode", "=", "'resize'", ",", "\n", "round_confidences", "=", "False", ",", "\n", "matching_iou_threshold", "=", "0.5", ",", "\n", "border_pixels", "=", "'include'", ",", "\n", "sorting_algorithm", "=", "'quicksort'", ",", "\n", "average_precision_mode", "=", "'sample'", ",", "\n", "num_recall_points", "=", "11", ",", "\n", "ignore_neutral_boxes", "=", "True", ",", "\n", "return_precisions", "=", "False", ",", "\n", "return_recalls", "=", "False", ",", "\n", "return_average_precisions", "=", "False", ",", "\n", "verbose", "=", "True", ",", "\n", "decoding_confidence_thresh", "=", "0.01", ",", "\n", "decoding_iou_threshold", "=", "0.45", ",", "\n", "decoding_top_k", "=", "200", ",", "\n", "decoding_pred_coords", "=", "'centroids'", ",", "\n", "decoding_normalize_coords", "=", "True", ")", ":", "\n", "        ", "'''\n        Computes the mean average precision of the given Keras SSD model on the given dataset.\n\n        Optionally also returns the averages precisions, precisions, and recalls.\n\n        All the individual steps of the overall evaluation algorithm can also be called separately\n        (check out the other methods of this class), but this runs the overall algorithm all at once.\n\n        Arguments:\n            img_height (int): The input image height for the model.\n            img_width (int): The input image width for the model.\n            batch_size (int): The batch size for the evaluation.\n            data_generator_mode (str, optional): Either of 'resize' and 'pad'. If 'resize', the input images will\n                be resized (i.e. warped) to `(img_height, img_width)`. This mode does not preserve the aspect ratios of the images.\n                If 'pad', the input images will be first padded so that they have the aspect ratio defined by `img_height`\n                and `img_width` and then resized to `(img_height, img_width)`. This mode preserves the aspect ratios of the images.\n            round_confidences (int, optional): `False` or an integer that is the number of decimals that the prediction\n                confidences will be rounded to. If `False`, the confidences will not be rounded.\n            matching_iou_threshold (float, optional): A prediction will be considered a true positive if it has a Jaccard overlap\n                of at least `matching_iou_threshold` with any ground truth bounding box of the same class.\n            border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n                Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n                to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n                If 'half', then one of each of the two horizontal and vertical borders belong\n                to the boxex, but not the other.\n            sorting_algorithm (str, optional): Which sorting algorithm the matching algorithm should use. This argument accepts\n                any valid sorting algorithm for Numpy's `argsort()` function. You will usually want to choose between 'quicksort'\n                (fastest and most memory efficient, but not stable) and 'mergesort' (slight slower and less memory efficient, but stable).\n                The official Matlab evaluation algorithm uses a stable sorting algorithm, so this algorithm is only guaranteed\n                to behave identically if you choose 'mergesort' as the sorting algorithm, but it will almost always behave identically\n                even if you choose 'quicksort' (but no guarantees).\n            average_precision_mode (str, optional): Can be either 'sample' or 'integrate'. In the case of 'sample', the average precision\n                will be computed according to the Pascal VOC formula that was used up until VOC 2009, where the precision will be sampled\n                for `num_recall_points` recall values. In the case of 'integrate', the average precision will be computed according to the\n                Pascal VOC formula that was used from VOC 2010 onward, where the average precision will be computed by numerically integrating\n                over the whole preciscion-recall curve instead of sampling individual points from it. 'integrate' mode is basically just\n                the limit case of 'sample' mode as the number of sample points increases.\n            num_recall_points (int, optional): The number of points to sample from the precision-recall-curve to compute the average\n                precisions. In other words, this is the number of equidistant recall values for which the resulting precision will be\n                computed. 11 points is the value used in the official Pascal VOC 2007 detection evaluation algorithm.\n            ignore_neutral_boxes (bool, optional): In case the data generator provides annotations indicating whether a ground truth\n                bounding box is supposed to either count or be neutral for the evaluation, this argument decides what to do with these\n                annotations. If `False`, even boxes that are annotated as neutral will be counted into the evaluation. If `True`,\n                neutral boxes will be ignored for the evaluation. An example for evaluation-neutrality are the ground truth boxes\n                annotated as \"difficult\" in the Pascal VOC datasets, which are usually treated as neutral for the evaluation.\n            return_precisions (bool, optional): If `True`, returns a nested list containing the cumulative precisions for each class.\n            return_recalls (bool, optional): If `True`, returns a nested list containing the cumulative recalls for each class.\n            return_average_precisions (bool, optional): If `True`, returns a list containing the average precision for each class.\n            verbose (bool, optional): If `True`, will print out the progress during runtime.\n            decoding_confidence_thresh (float, optional): Only relevant if the model is in 'training' mode.\n                A float in [0,1), the minimum classification confidence in a specific positive class in order to be considered\n                for the non-maximum suppression stage for the respective class. A lower value will result in a larger part of the\n                selection process being done by the non-maximum suppression stage, while a larger value will result in a larger\n                part of the selection process happening in the confidence thresholding stage.\n            decoding_iou_threshold (float, optional): Only relevant if the model is in 'training' mode. A float in [0,1].\n                All boxes with a Jaccard similarity of greater than `iou_threshold` with a locally maximal box will be removed\n                from the set of predictions for a given class, where 'maximal' refers to the box score.\n            decoding_top_k (int, optional): Only relevant if the model is in 'training' mode. The number of highest scoring\n                predictions to be kept for each batch item after the non-maximum suppression stage.\n            decoding_input_coords (str, optional): Only relevant if the model is in 'training' mode. The box coordinate format\n                that the model outputs. Can be either 'centroids' for the format `(cx, cy, w, h)` (box center coordinates, width, and height),\n                'minmax' for the format `(xmin, xmax, ymin, ymax)`, or 'corners' for the format `(xmin, ymin, xmax, ymax)`.\n            decoding_normalize_coords (bool, optional): Only relevant if the model is in 'training' mode. Set to `True` if the model\n                outputs relative coordinates. Do not set this to `True` if the model already outputs absolute coordinates,\n                as that would result in incorrect coordinates.\n\n        Returns:\n            A float, the mean average precision, plus any optional returns specified in the arguments.\n        '''", "\n", "\n", "#############################################################################################", "\n", "# Predict on the entire dataset.", "\n", "#############################################################################################", "\n", "\n", "self", ".", "predict_on_dataset", "(", "img_height", "=", "img_height", ",", "\n", "img_width", "=", "img_width", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "data_generator_mode", "=", "data_generator_mode", ",", "\n", "decoding_confidence_thresh", "=", "decoding_confidence_thresh", ",", "\n", "decoding_iou_threshold", "=", "decoding_iou_threshold", ",", "\n", "decoding_top_k", "=", "decoding_top_k", ",", "\n", "decoding_pred_coords", "=", "decoding_pred_coords", ",", "\n", "decoding_normalize_coords", "=", "decoding_normalize_coords", ",", "\n", "decoding_border_pixels", "=", "border_pixels", ",", "\n", "round_confidences", "=", "round_confidences", ",", "\n", "verbose", "=", "verbose", ",", "\n", "ret", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_train.Evaluator.predict_on_dataset": [[178, 370], ["data_generator.object_detection_2d_photometric_ops.ConvertTo3Channels", "data_generator.object_detection_2d_geometric_ops.Resize", "average_precision_evaluator_train.Evaluator.data_generator.generate", "average_precision_evaluator_train.Evaluator.data_generator.get_dataset_size", "int", "open", "open.close", "average_precision_evaluator_train.Evaluator.Updateweight", "print", "list", "list", "math.ceil", "print", "print", "tqdm.trange", "range.set_description", "range", "next", "average_precision_evaluator_train.Evaluator.model.predict", "data_generator.object_detection_2d_misc_utils.apply_inverse_transforms", "enumerate", "data_generator.object_detection_2d_patch_sampling_ops.RandomPadFixedAR", "ValueError", "range", "range", "ssd_encoder_decoder.ssd_output_decoder.decode_detections", "range", "open", "open.close", "numpy.array", "average_precision_evaluator_train.Evaluator.CheckDetected", "range", "average_precision_evaluator_train.Evaluator.data_generator.get_dataset_size", "len", "y_pred_filtered.append", "os.path.exists", "os.mknod", "int", "round", "round", "round", "round", "open.write", "results[].append", "len", "round", "str", "predictions.append", "open.writelines", "open.writelines", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_data_generator.DataGenerator.generate", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_data_generator.DataGenerator.get_dataset_size", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_train.Evaluator.Updateweight", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_misc_utils.apply_inverse_transforms", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.ssd_encoder_decoder.ssd_output_decoder.decode_detections", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_train.Evaluator.CheckDetected", "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.data_generator.object_detection_2d_data_generator.DataGenerator.get_dataset_size"], ["", "def", "predict_on_dataset", "(", "self", ",", "\n", "img_height", ",", "\n", "img_width", ",", "\n", "batch_size", ",", "\n", "data_generator_mode", "=", "'resize'", ",", "\n", "decoding_confidence_thresh", "=", "0.01", ",", "\n", "decoding_iou_threshold", "=", "0.45", ",", "\n", "decoding_top_k", "=", "200", ",", "\n", "decoding_pred_coords", "=", "'centroids'", ",", "\n", "decoding_normalize_coords", "=", "True", ",", "\n", "decoding_border_pixels", "=", "'include'", ",", "\n", "round_confidences", "=", "False", ",", "\n", "verbose", "=", "True", ",", "\n", "ret", "=", "False", ")", ":", "\n", "        ", "'''\n        Runs predictions for the given model over the entire dataset given by `data_generator`.\n\n        Arguments:\n            img_height (int): The input image height for the model.\n            img_width (int): The input image width for the model.\n            batch_size (int): The batch size for the evaluation.\n            data_generator_mode (str, optional): Either of 'resize' and 'pad'. If 'resize', the input images will\n                be resized (i.e. warped) to `(img_height, img_width)`. This mode does not preserve the aspect ratios of the images.\n                If 'pad', the input images will be first padded so that they have the aspect ratio defined by `img_height`\n                and `img_width` and then resized to `(img_height, img_width)`. This mode preserves the aspect ratios of the images.\n            decoding_confidence_thresh (float, optional): Only relevant if the model is in 'training' mode.\n                A float in [0,1), the minimum classification confidence in a specific positive class in order to be considered\n                for the non-maximum suppression stage for the respective class. A lower value will result in a larger part of the\n                selection process being done by the non-maximum suppression stage, while a larger value will result in a larger\n                part of the selection process happening in the confidence thresholding stage.\n            decoding_iou_threshold (float, optional): Only relevant if the model is in 'training' mode. A float in [0,1].\n                All boxes with a Jaccard similarity of greater than `iou_threshold` with a locally maximal box will be removed\n                from the set of predictions for a given class, where 'maximal' refers to the box score.\n            decoding_top_k (int, optional): Only relevant if the model is in 'training' mode. The number of highest scoring\n                predictions to be kept for each batch item after the non-maximum suppression stage.\n            decoding_input_coords (str, optional): Only relevant if the model is in 'training' mode. The box coordinate format\n                that the model outputs. Can be either 'centroids' for the format `(cx, cy, w, h)` (box center coordinates, width, and height),\n                'minmax' for the format `(xmin, xmax, ymin, ymax)`, or 'corners' for the format `(xmin, ymin, xmax, ymax)`.\n            decoding_normalize_coords (bool, optional): Only relevant if the model is in 'training' mode. Set to `True` if the model\n                outputs relative coordinates. Do not set this to `True` if the model already outputs absolute coordinates,\n                as that would result in incorrect coordinates.\n            round_confidences (int, optional): `False` or an integer that is the number of decimals that the prediction\n                confidences will be rounded to. If `False`, the confidences will not be rounded.\n            verbose (bool, optional): If `True`, will print out the progress during runtime.\n            ret (bool, optional): If `True`, returns the predictions.\n\n        Returns:\n            None by default. Optionally, a nested list containing the predictions for each class.\n        '''", "\n", "\n", "class_id_pred", "=", "self", ".", "pred_format", "[", "'class_id'", "]", "\n", "conf_pred", "=", "self", ".", "pred_format", "[", "'conf'", "]", "\n", "xmin_pred", "=", "self", ".", "pred_format", "[", "'xmin'", "]", "\n", "ymin_pred", "=", "self", ".", "pred_format", "[", "'ymin'", "]", "\n", "xmax_pred", "=", "self", ".", "pred_format", "[", "'xmax'", "]", "\n", "ymax_pred", "=", "self", ".", "pred_format", "[", "'ymax'", "]", "\n", "\n", "#############################################################################################", "\n", "# Configure the data generator for the evaluation.", "\n", "#############################################################################################", "\n", "\n", "convert_to_3_channels", "=", "ConvertTo3Channels", "(", ")", "\n", "resize", "=", "Resize", "(", "height", "=", "img_height", ",", "width", "=", "img_width", ",", "labels_format", "=", "self", ".", "gt_format", ")", "\n", "if", "data_generator_mode", "==", "'resize'", ":", "\n", "            ", "transformations", "=", "[", "convert_to_3_channels", ",", "\n", "resize", "]", "\n", "", "elif", "data_generator_mode", "==", "'pad'", ":", "\n", "            ", "random_pad", "=", "RandomPadFixedAR", "(", "patch_aspect_ratio", "=", "img_width", "/", "img_height", ",", "labels_format", "=", "self", ".", "gt_format", ")", "\n", "transformations", "=", "[", "convert_to_3_channels", ",", "\n", "random_pad", ",", "\n", "resize", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"`data_generator_mode` can be either of 'resize' or 'pad', but received '{}'.\"", ".", "format", "(", "data_generator_mode", ")", ")", "\n", "\n", "# Set the generator parameters.", "\n", "", "generator", "=", "self", ".", "data_generator", ".", "generate", "(", "batch_size", "=", "batch_size", ",", "\n", "shuffle", "=", "False", ",", "\n", "transformations", "=", "transformations", ",", "\n", "label_encoder", "=", "None", ",", "\n", "returns", "=", "{", "'processed_images'", ",", "\n", "'image_ids'", ",", "\n", "'evaluation-neutral'", ",", "\n", "'inverse_transform'", ",", "\n", "'original_labels'", "}", ",", "\n", "keep_images_without_gt", "=", "True", ",", "\n", "degenerate_box_handling", "=", "'remove'", ")", "\n", "\n", "if", "self", ".", "data_generator", ".", "image_ids", "is", "None", ":", "\n", "            ", "self", ".", "data_generator", ".", "image_ids", "=", "list", "(", "range", "(", "self", ".", "data_generator", ".", "get_dataset_size", "(", ")", ")", ")", "\n", "\n", "#############################################################################################", "\n", "# Predict over all batches of the dataset and store the predictions.", "\n", "#############################################################################################", "\n", "\n", "# We have to generate a separate results list for each class.", "\n", "", "results", "=", "[", "list", "(", ")", "for", "_", "in", "range", "(", "self", ".", "n_classes", "+", "1", ")", "]", "\n", "\n", "# Create a dictionary that maps image IDs to ground truth annotations.", "\n", "# We'll need it below.", "\n", "image_ids_to_labels", "=", "{", "}", "\n", "\n", "# Compute the number of batches to iterate over the entire dataset.", "\n", "n_images", "=", "self", ".", "data_generator", ".", "get_dataset_size", "(", ")", "\n", "n_batches", "=", "int", "(", "ceil", "(", "n_images", "/", "batch_size", ")", ")", "\n", "if", "verbose", ":", "\n", "            ", "print", "(", "\"Number of images in the evaluation dataset: {}\"", ".", "format", "(", "n_images", ")", ")", "\n", "print", "(", ")", "\n", "tr", "=", "trange", "(", "n_batches", ",", "file", "=", "sys", ".", "stdout", ")", "\n", "tr", ".", "set_description", "(", "'Producing predictions batch-wise'", ")", "\n", "", "else", ":", "\n", "            ", "tr", "=", "range", "(", "n_batches", ")", "\n", "\n", "", "overallindex", "=", "0", "\n", "fid", "=", "open", "(", "'/data/deeplearn/SWEIPENet/dataset/detections.txt'", ",", "'w'", ")", "\n", "for", "j", "in", "tr", ":", "\n", "# Generate batch.", "\n", "            ", "batch_X", ",", "batch_image_ids", ",", "batch_eval_neutral", ",", "batch_inverse_transforms", ",", "batch_orig_labels", "=", "next", "(", "generator", ")", "\n", "# Predict.", "\n", "y_pred", "=", "self", ".", "model", ".", "predict", "(", "batch_X", ")", "\n", "# If the model was created in 'training' mode, the raw predictions need to", "\n", "# be decoded and filtered, otherwise that's already taken care of.", "\n", "if", "self", ".", "model_mode", "==", "'training'", ":", "\n", "# Decode.", "\n", "                ", "y_pred", "=", "decode_detections", "(", "y_pred", ",", "\n", "confidence_thresh", "=", "decoding_confidence_thresh", ",", "\n", "iou_threshold", "=", "decoding_iou_threshold", ",", "\n", "top_k", "=", "decoding_top_k", ",", "\n", "input_coords", "=", "decoding_pred_coords", ",", "\n", "normalize_coords", "=", "decoding_normalize_coords", ",", "\n", "img_height", "=", "img_height", ",", "\n", "img_width", "=", "img_width", ",", "\n", "border_pixels", "=", "decoding_border_pixels", ")", "\n", "", "else", ":", "\n", "# Filter out the all-zeros dummy elements of `y_pred`.", "\n", "                ", "y_pred_filtered", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "y_pred", ")", ")", ":", "\n", "                    ", "y_pred_filtered", ".", "append", "(", "y_pred", "[", "i", "]", "[", "y_pred", "[", "i", ",", ":", ",", "0", "]", "!=", "0", "]", ")", "\n", "", "y_pred", "=", "y_pred_filtered", "\n", "# Convert the predicted box coordinates for the original images.", "\n", "", "y_pred", "=", "apply_inverse_transforms", "(", "y_pred", ",", "batch_inverse_transforms", ")", "\n", "\n", "# Iterate over all batch items.", "\n", "for", "k", ",", "batch_item", "in", "enumerate", "(", "y_pred", ")", ":", "\n", "                ", "image_id", "=", "batch_image_ids", "[", "k", "]", "\n", "path", "=", "'/data/deeplearn/SWEIPENet/dataset/Detections/'", "\n", "txtpath", "=", "path", "+", "image_id", "+", "'.txt'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "txtpath", ")", ":", "\n", "                    ", "os", ".", "mknod", "(", "txtpath", ")", "\n", "", "file_fid", "=", "open", "(", "txtpath", ",", "'w'", ")", "\n", "\n", "predictions", "=", "[", "]", "\n", "for", "box", "in", "batch_item", ":", "\n", "                    ", "class_id", "=", "int", "(", "box", "[", "class_id_pred", "]", ")", "\n", "# Round the box coordinates to reduce the required memory.", "\n", "if", "round_confidences", ":", "\n", "                        ", "confidence", "=", "round", "(", "box", "[", "conf_pred", "]", ",", "round_confidences", ")", "\n", "", "else", ":", "\n", "                        ", "confidence", "=", "box", "[", "conf_pred", "]", "\n", "", "xmin", "=", "round", "(", "box", "[", "xmin_pred", "]", ",", "1", ")", "\n", "ymin", "=", "round", "(", "box", "[", "ymin_pred", "]", ",", "1", ")", "\n", "xmax", "=", "round", "(", "box", "[", "xmax_pred", "]", ",", "1", ")", "\n", "ymax", "=", "round", "(", "box", "[", "ymax_pred", "]", ",", "1", ")", "\n", "prediction", "=", "(", "image_id", ",", "confidence", ",", "xmin", ",", "ymin", ",", "xmax", ",", "ymax", ")", "\n", "# write detections of each image into Detections/imname.txt", "\n", "if", "class_id", "==", "1", ":", "\n", "                        ", "class_name", "=", "'seacucumber'", "\n", "", "if", "class_id", "==", "2", ":", "\n", "                        ", "class_name", "=", "'seaurchin'", "\n", "", "if", "class_id", "==", "3", ":", "\n", "                        ", "class_name", "=", "'scallop'", "\n", "", "boxstr", "=", "class_name", "+", "' '", "+", "str", "(", "confidence", ")", "+", "' '", "+", "str", "(", "xmin", ")", "+", "' '", "+", "str", "(", "ymin", ")", "+", "' '", "+", "str", "(", "xmax", ")", "+", "' '", "+", "str", "(", "ymax", ")", "\n", "file_fid", ".", "write", "(", "boxstr", "+", "'\\n'", ")", "\n", "if", "confidence", ">", "0", ":", "\n", "                        ", "predictions", ".", "append", "(", "[", "xmin", ",", "ymin", ",", "xmax", ",", "ymax", "]", ")", "\n", "", "results", "[", "class_id", "]", ".", "append", "(", "prediction", ")", "\n", "\n", "", "file_fid", ".", "close", "(", ")", "\n", "gtlabels", "=", "self", ".", "data_generator", ".", "labels", "[", "overallindex", "]", "\n", "gtlabels", "=", "np", ".", "array", "(", "gtlabels", ")", "\n", "gtlabels", "=", "gtlabels", "[", ":", ",", "2", ":", "]", "\n", "overallindex", "=", "overallindex", "+", "1", "\n", "detect", "=", "self", ".", "CheckDetected", "(", "gtlabels", ",", "predictions", ")", "\n", "# write whether current ground truth boxes have been detected or not in detections.txt", "\n", "for", "i", "in", "range", "(", "len", "(", "detect", ")", ")", ":", "\n", "                    ", "if", "detect", "[", "i", "]", ":", "\n", "                        ", "fid", ".", "writelines", "(", "image_id", "+", "' '", "+", "str", "(", "gtlabels", "[", "i", ",", "0", "]", ")", "+", "' '", "+", "str", "(", "gtlabels", "[", "i", ",", "1", "]", ")", "+", "' '", "+", "str", "(", "gtlabels", "[", "i", ",", "2", "]", ")", "+", "' '", "+", "str", "(", "gtlabels", "[", "i", ",", "3", "]", ")", "+", "', 1\\n'", ")", "\n", "", "else", ":", "\n", "                        ", "fid", ".", "writelines", "(", "image_id", "+", "' '", "+", "str", "(", "gtlabels", "[", "i", ",", "0", "]", ")", "+", "' '", "+", "str", "(", "gtlabels", "[", "i", ",", "1", "]", ")", "+", "' '", "+", "str", "(", "gtlabels", "[", "i", ",", "2", "]", ")", "+", "' '", "+", "str", "(", "gtlabels", "[", "i", ",", "3", "]", ")", "+", "', 0\\n'", ")", "\n", "", "", "", "", "self", ".", "prediction_results", "=", "results", "\n", "fid", ".", "close", "(", ")", "\n", "self", ".", "Updateweight", "(", "gtlabels", ",", "predictions", ")", "\n", "print", "(", "'Finishing update the weight!!!!!!!'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_train.Evaluator.Updateweight": [[371, 401], ["open", "open", "open.close", "len", "numpy.sum", "numpy.exp", "numpy.sum", "open", "range", "line.strip().split", "int", "line.strip().split", "float", "numpy.ones", "numpy.dot", "numpy.sum", "numpy.log", "numpy.log", "len", "open.writelines", "line.strip", "line.strip", "str"], "methods", ["None"], ["", "def", "Updateweight", "(", "self", ",", "gtlabels", ",", "predictions", ")", ":", "\n", "        ", "n_classes", "=", "3", "\n", "detfid", "=", "open", "(", "'/data/deeplearn/SWEIPENet/dataset/detections.txt'", ",", "'r'", ")", "\n", "weightfid", "=", "open", "(", "'/data/deeplearn/SWEIPENet/dataset/weights.txt'", ",", "'r+'", ")", "\n", "ydet", "=", "[", "line", ".", "strip", "(", ")", ".", "split", "(", "','", ")", "for", "line", "in", "detfid", "]", "\n", "ydetobj", "=", "[", "t", "[", "0", "]", "for", "t", "in", "ydet", "]", "\n", "ylabel", "=", "[", "int", "(", "t", "[", "1", "]", ")", "for", "t", "in", "ydet", "]", "\n", "\n", "ywei", "=", "[", "line", ".", "strip", "(", ")", ".", "split", "(", "','", ")", "for", "line", "in", "weightfid", "]", "\n", "yweiobj", "=", "[", "t", "[", "0", "]", "for", "t", "in", "ywei", "]", "\n", "yweivalue", "=", "[", "float", "(", "t", "[", "1", "]", ")", "for", "t", "in", "ywei", "]", "\n", "weightfid", ".", "close", "(", ")", "\n", "objnum", "=", "len", "(", "yweiobj", ")", "\n", "# label is True means detetecd", "\n", "label", "=", "ylabel", "==", "np", ".", "ones", "(", "objnum", ")", "\n", "estimator_error", "=", "np", ".", "dot", "(", "~", "label", ",", "yweivalue", ")", "/", "np", ".", "sum", "(", "yweivalue", ",", "axis", "=", "0", ")", "\n", "# update estimator_weight", "\n", "estimator_weight", "=", "np", ".", "log", "(", "(", "1", "-", "estimator_error", ")", "/", "estimator_error", ")", "+", "np", ".", "log", "(", "n_classes", "-", "1", ")", "\n", "# convert SWIPENET weights into IMA weights", "\n", "weight_sum", "=", "np", ".", "sum", "(", "yweivalue", ",", "axis", "=", "0", ")", "\n", "yweivalue", "/=", "weight_sum", "\n", "# update sample weight", "\n", "yweivalue", "*=", "np", ".", "exp", "(", "estimator_weight", "*", "label", ")", "\n", "sample_weight_sum", "=", "np", ".", "sum", "(", "yweivalue", ",", "axis", "=", "0", ")", "\n", "# normalize sample weight", "\n", "yweivalue", "/=", "sample_weight_sum", "\n", "yweivalue", "=", "yweivalue", "*", "objnum", "\n", "newweightfid", "=", "open", "(", "'/data/deeplearn/SWEIPENet/dataset/weights.txt'", ",", "'w'", ")", "\n", "for", "weighindex", "in", "range", "(", "len", "(", "yweiobj", ")", ")", ":", "\n", "            ", "newweightfid", ".", "writelines", "(", "yweiobj", "[", "weighindex", "]", "+", "', '", "+", "str", "(", "\"{:.4f}\"", ".", "format", "(", "yweivalue", "[", "weighindex", "]", ")", ")", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_train.Evaluator.CheckDetected": [[402, 421], ["numpy.array", "numpy.array", "bounding_box_utils.bounding_box_utils.iou", "numpy.argmax", "detect.append", "detect.append"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.iou"], ["", "", "def", "CheckDetected", "(", "self", ",", "gtlabels", "=", "None", ",", "predictions", "=", "None", ")", ":", "\n", "        ", "detect", "=", "[", "]", "\n", "predictions", "=", "np", ".", "array", "(", "predictions", ")", "\n", "for", "gt_box", "in", "gtlabels", ":", "\n", "            ", "gt_box", "=", "np", ".", "array", "(", "gt_box", ")", "\n", "overlaps", "=", "iou", "(", "boxes1", "=", "predictions", ",", "\n", "boxes2", "=", "gt_box", ",", "\n", "coords", "=", "'corners'", ",", "\n", "mode", "=", "'element-wise'", ",", "\n", "border_pixels", "=", "'include'", ")", "\n", "\n", "gt_match_index", "=", "np", ".", "argmax", "(", "overlaps", ")", "\n", "gt_match_overlap", "=", "overlaps", "[", "gt_match_index", "]", "\n", "threshold_match", "=", "0", "\n", "if", "gt_match_overlap", ">", "threshold_match", ":", "\n", "                ", "detect", ".", "append", "(", "True", ")", "\n", "", "else", ":", "\n", "                ", "detect", ".", "append", "(", "False", ")", "\n", "", "", "return", "detect", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_train.Evaluator.write_predictions_to_txt": [[422, 472], ["range", "ValueError", "open", "open.close", "print", "print", "list", "round", "open.write", "map"], "methods", ["None"], ["", "def", "write_predictions_to_txt", "(", "self", ",", "\n", "classes", "=", "None", ",", "\n", "out_file_prefix", "=", "'comp3_det_test_'", ",", "\n", "verbose", "=", "True", ")", ":", "\n", "        ", "'''\n        Writes the predictions for all classes to separate text files according to the Pascal VOC results format.\n\n        Arguments:\n            classes (list, optional): `None` or a list of strings containing the class names of all classes in the dataset,\n                including some arbitrary name for the background class. This list will be used to name the output text files.\n                The ordering of the names in the list represents the ordering of the classes as they are predicted by the model,\n                i.e. the element with index 3 in this list should correspond to the class with class ID 3 in the model's predictions.\n                If `None`, the output text files will be named by their class IDs.\n            out_file_prefix (str, optional): A prefix for the output text file names. The suffix to each output text file name will\n                be the respective class name followed by the `.txt` file extension. This string is also how you specify the directory\n                in which the results are to be saved.\n            verbose (bool, optional): If `True`, will print out the progress during runtime.\n\n        Returns:\n            None.\n        '''", "\n", "\n", "if", "self", ".", "prediction_results", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"There are no prediction results. You must run `predict_on_dataset()` before calling this method.\"", ")", "\n", "\n", "# We generate a separate results file for each class.", "\n", "", "for", "class_id", "in", "range", "(", "1", ",", "self", ".", "n_classes", "+", "1", ")", ":", "\n", "\n", "            ", "if", "verbose", ":", "\n", "                ", "print", "(", "\"Writing results file for class {}/{}.\"", ".", "format", "(", "class_id", ",", "self", ".", "n_classes", ")", ")", "\n", "\n", "", "if", "classes", "is", "None", ":", "\n", "                ", "class_suffix", "=", "'{:04d}'", ".", "format", "(", "class_id", ")", "\n", "", "else", ":", "\n", "                ", "class_suffix", "=", "classes", "[", "class_id", "]", "\n", "\n", "", "results_file", "=", "open", "(", "'{}{}.txt'", ".", "format", "(", "out_file_prefix", ",", "class_suffix", ")", ",", "'w'", ")", "\n", "\n", "for", "prediction", "in", "self", ".", "prediction_results", "[", "class_id", "]", ":", "\n", "\n", "                ", "prediction_list", "=", "list", "(", "prediction", ")", "\n", "prediction_list", "[", "0", "]", "=", "'{}'", ".", "format", "(", "prediction_list", "[", "0", "]", ")", "\n", "prediction_list", "[", "1", "]", "=", "round", "(", "prediction_list", "[", "1", "]", ",", "4", ")", "\n", "prediction_txt", "=", "' '", ".", "join", "(", "map", "(", "str", ",", "prediction_list", ")", ")", "+", "'\\n'", "\n", "results_file", ".", "write", "(", "prediction_txt", ")", "\n", "\n", "", "results_file", ".", "close", "(", ")", "\n", "\n", "", "if", "verbose", ":", "\n", "            ", "print", "(", "\"All results files saved.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_train.Evaluator.get_num_gt_per_class": [[473, 533], ["numpy.zeros", "ValueError", "print", "tqdm.trange", "range", "numpy.asarray", "range", "len", "len"], "methods", ["None"], ["", "", "def", "get_num_gt_per_class", "(", "self", ",", "\n", "ignore_neutral_boxes", "=", "True", ",", "\n", "verbose", "=", "True", ",", "\n", "ret", "=", "False", ")", ":", "\n", "        ", "'''\n        Counts the number of ground truth boxes for each class across the dataset.\n\n        Arguments:\n            ignore_neutral_boxes (bool, optional): In case the data generator provides annotations indicating whether a ground truth\n                bounding box is supposed to either count or be neutral for the evaluation, this argument decides what to do with these\n                annotations. If `True`, only non-neutral ground truth boxes will be counted, otherwise all ground truth boxes will\n                be counted.\n            verbose (bool, optional): If `True`, will print out the progress during runtime.\n            ret (bool, optional): If `True`, returns the list of counts.\n\n        Returns:\n            None by default. Optionally, a list containing a count of the number of ground truth boxes for each class across the\n            entire dataset.\n        '''", "\n", "\n", "if", "self", ".", "data_generator", ".", "labels", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Computing the number of ground truth boxes per class not possible, no ground truth given.\"", ")", "\n", "\n", "", "num_gt_per_class", "=", "np", ".", "zeros", "(", "shape", "=", "(", "self", ".", "n_classes", "+", "1", ")", ",", "dtype", "=", "np", ".", "int", ")", "\n", "\n", "class_id_index", "=", "self", ".", "gt_format", "[", "'class_id'", "]", "\n", "\n", "ground_truth", "=", "self", ".", "data_generator", ".", "labels", "\n", "\n", "if", "verbose", ":", "\n", "            ", "print", "(", "'Computing the number of positive ground truth boxes per class.'", ")", "\n", "tr", "=", "trange", "(", "len", "(", "ground_truth", ")", ",", "file", "=", "sys", ".", "stdout", ")", "\n", "", "else", ":", "\n", "            ", "tr", "=", "range", "(", "len", "(", "ground_truth", ")", ")", "\n", "\n", "# Iterate over the ground truth for all images in the dataset.", "\n", "", "for", "i", "in", "tr", ":", "\n", "\n", "            ", "boxes", "=", "np", ".", "asarray", "(", "ground_truth", "[", "i", "]", ")", "\n", "\n", "# Iterate over all ground truth boxes for the current image.", "\n", "for", "j", "in", "range", "(", "boxes", ".", "shape", "[", "0", "]", ")", ":", "\n", "\n", "                ", "if", "ignore_neutral_boxes", "and", "not", "(", "self", ".", "data_generator", ".", "eval_neutral", "is", "None", ")", ":", "\n", "                    ", "if", "not", "self", ".", "data_generator", ".", "eval_neutral", "[", "i", "]", "[", "j", "]", ":", "\n", "# If this box is not supposed to be evaluation-neutral,", "\n", "# increment the counter for the respective class ID.", "\n", "                        ", "class_id", "=", "boxes", "[", "j", ",", "class_id_index", "]", "\n", "num_gt_per_class", "[", "class_id", "]", "+=", "1", "\n", "", "", "else", ":", "\n", "# If there is no such thing as evaluation-neutral boxes for", "\n", "# our dataset, always increment the counter for the respective", "\n", "# class ID.", "\n", "                    ", "class_id", "=", "boxes", "[", "j", ",", "class_id_index", "]", "\n", "num_gt_per_class", "[", "class_id", "]", "+=", "1", "\n", "\n", "", "", "", "self", ".", "num_gt_per_class", "=", "num_gt_per_class", "\n", "\n", "if", "ret", ":", "\n", "            ", "return", "num_gt_per_class", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_train.Evaluator.match_predictions": [[534, 734], ["range", "range", "ValueError", "ValueError", "len", "str", "numpy.zeros", "numpy.zeros", "numpy.dtype", "numpy.array", "numpy.argsort", "true_positives.append", "false_positives.append", "numpy.cumsum", "numpy.cumsum", "cumulative_true_positives.append", "cumulative_false_positives.append", "numpy.asarray", "len", "len", "len", "print", "true_positives.append", "false_positives.append", "len", "tqdm.trange", "range.set_description", "range", "numpy.asarray", "numpy.asarray", "bounding_box_utils.bounding_box_utils.iou", "numpy.argmax", "numpy.asarray", "numpy.asarray", "str", "len", "len", "list", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.bounding_box_utils.bounding_box_utils.iou"], ["", "", "def", "match_predictions", "(", "self", ",", "\n", "ignore_neutral_boxes", "=", "True", ",", "\n", "matching_iou_threshold", "=", "0.5", ",", "\n", "border_pixels", "=", "'include'", ",", "\n", "sorting_algorithm", "=", "'quicksort'", ",", "\n", "verbose", "=", "True", ",", "\n", "ret", "=", "False", ")", ":", "\n", "        ", "'''\n        Matches predictions to ground truth boxes.\n\n        Note that `predict_on_dataset()` must be called before calling this method.\n\n        Arguments:\n            ignore_neutral_boxes (bool, optional): In case the data generator provides annotations indicating whether a ground truth\n                bounding box is supposed to either count or be neutral for the evaluation, this argument decides what to do with these\n                annotations. If `False`, even boxes that are annotated as neutral will be counted into the evaluation. If `True`,\n                neutral boxes will be ignored for the evaluation. An example for evaluation-neutrality are the ground truth boxes\n                annotated as \"difficult\" in the Pascal VOC datasets, which are usually treated as neutral for the evaluation.\n            matching_iou_threshold (float, optional): A prediction will be considered a true positive if it has a Jaccard overlap\n                of at least `matching_iou_threshold` with any ground truth bounding box of the same class.\n            border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n                Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n                to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n                If 'half', then one of each of the two horizontal and vertical borders belong\n                to the boxex, but not the other.\n            sorting_algorithm (str, optional): Which sorting algorithm the matching algorithm should use. This argument accepts\n                any valid sorting algorithm for Numpy's `argsort()` function. You will usually want to choose between 'quicksort'\n                (fastest and most memory efficient, but not stable) and 'mergesort' (slight slower and less memory efficient, but stable).\n                The official Matlab evaluation algorithm uses a stable sorting algorithm, so this algorithm is only guaranteed\n                to behave identically if you choose 'mergesort' as the sorting algorithm, but it will almost always behave identically\n                even if you choose 'quicksort' (but no guarantees).\n            verbose (bool, optional): If `True`, will print out the progress during runtime.\n            ret (bool, optional): If `True`, returns the true and false positives.\n\n        Returns:\n            None by default. Optionally, four nested lists containing the true positives, false positives, cumulative true positives,\n            and cumulative false positives for each class.\n        '''", "\n", "\n", "if", "self", ".", "data_generator", ".", "labels", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Matching predictions to ground truth boxes not possible, no ground truth given.\"", ")", "\n", "\n", "", "if", "self", ".", "prediction_results", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"There are no prediction results. You must run `predict_on_dataset()` before calling this method.\"", ")", "\n", "\n", "", "class_id_gt", "=", "self", ".", "gt_format", "[", "'class_id'", "]", "\n", "xmin_gt", "=", "self", ".", "gt_format", "[", "'xmin'", "]", "\n", "ymin_gt", "=", "self", ".", "gt_format", "[", "'ymin'", "]", "\n", "xmax_gt", "=", "self", ".", "gt_format", "[", "'xmax'", "]", "\n", "ymax_gt", "=", "self", ".", "gt_format", "[", "'ymax'", "]", "\n", "\n", "# Convert the ground truth to a more efficient format for what we need", "\n", "# to do, which is access ground truth by image ID repeatedly.", "\n", "ground_truth", "=", "{", "}", "\n", "eval_neutral_available", "=", "not", "(", "self", ".", "data_generator", ".", "eval_neutral", "is", "None", ")", "# Whether or not we have annotations to decide whether ground truth boxes should be neutral or not.", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "data_generator", ".", "image_ids", ")", ")", ":", "\n", "            ", "image_id", "=", "str", "(", "self", ".", "data_generator", ".", "image_ids", "[", "i", "]", ")", "\n", "labels", "=", "self", ".", "data_generator", ".", "labels", "[", "i", "]", "\n", "if", "ignore_neutral_boxes", "and", "eval_neutral_available", ":", "\n", "                ", "ground_truth", "[", "image_id", "]", "=", "(", "np", ".", "asarray", "(", "labels", ")", ",", "np", ".", "asarray", "(", "self", ".", "data_generator", ".", "eval_neutral", "[", "i", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "ground_truth", "[", "image_id", "]", "=", "np", ".", "asarray", "(", "labels", ")", "\n", "\n", "", "", "true_positives", "=", "[", "[", "]", "]", "# The false positives for each class, sorted by descending confidence.", "\n", "false_positives", "=", "[", "[", "]", "]", "# The true positives for each class, sorted by descending confidence.", "\n", "cumulative_true_positives", "=", "[", "[", "]", "]", "\n", "cumulative_false_positives", "=", "[", "[", "]", "]", "\n", "\n", "# Iterate over all classes.", "\n", "for", "class_id", "in", "range", "(", "1", ",", "self", ".", "n_classes", "+", "1", ")", ":", "\n", "\n", "            ", "predictions", "=", "self", ".", "prediction_results", "[", "class_id", "]", "\n", "\n", "# Store the matching results in these lists:", "\n", "true_pos", "=", "np", ".", "zeros", "(", "len", "(", "predictions", ")", ",", "dtype", "=", "np", ".", "int", ")", "# 1 for every prediction that is a true positive, 0 otherwise", "\n", "false_pos", "=", "np", ".", "zeros", "(", "len", "(", "predictions", ")", ",", "dtype", "=", "np", ".", "int", ")", "# 1 for every prediction that is a false positive, 0 otherwise", "\n", "\n", "# In case there are no predictions at all for this class, we're done here.", "\n", "if", "len", "(", "predictions", ")", "==", "0", ":", "\n", "                ", "print", "(", "\"No predictions for class {}/{}\"", ".", "format", "(", "class_id", ",", "self", ".", "n_classes", ")", ")", "\n", "true_positives", ".", "append", "(", "true_pos", ")", "\n", "false_positives", ".", "append", "(", "false_pos", ")", "\n", "continue", "\n", "\n", "# Convert the predictions list for this class into a structured array so that we can sort it by confidence.", "\n", "\n", "# Get the number of characters needed to store the image ID strings in the structured array.", "\n", "", "num_chars_per_image_id", "=", "len", "(", "str", "(", "predictions", "[", "0", "]", "[", "0", "]", ")", ")", "+", "6", "# Keep a few characters buffer in case some image IDs are longer than others.", "\n", "# Create the data type for the structured array.", "\n", "preds_data_type", "=", "np", ".", "dtype", "(", "[", "(", "'image_id'", ",", "'U{}'", ".", "format", "(", "num_chars_per_image_id", ")", ")", ",", "\n", "(", "'confidence'", ",", "'f4'", ")", ",", "\n", "(", "'xmin'", ",", "'f4'", ")", ",", "\n", "(", "'ymin'", ",", "'f4'", ")", ",", "\n", "(", "'xmax'", ",", "'f4'", ")", ",", "\n", "(", "'ymax'", ",", "'f4'", ")", "]", ")", "\n", "# Create the structured array", "\n", "predictions", "=", "np", ".", "array", "(", "predictions", ",", "dtype", "=", "preds_data_type", ")", "\n", "\n", "# Sort the detections by decreasing confidence.", "\n", "descending_indices", "=", "np", ".", "argsort", "(", "-", "predictions", "[", "'confidence'", "]", ",", "kind", "=", "sorting_algorithm", ")", "\n", "predictions_sorted", "=", "predictions", "[", "descending_indices", "]", "\n", "\n", "if", "verbose", ":", "\n", "                ", "tr", "=", "trange", "(", "len", "(", "predictions", ")", ",", "file", "=", "sys", ".", "stdout", ")", "\n", "tr", ".", "set_description", "(", "\"Matching predictions to ground truth, class {}/{}.\"", ".", "format", "(", "class_id", ",", "self", ".", "n_classes", ")", ")", "\n", "", "else", ":", "\n", "                ", "tr", "=", "range", "(", "len", "(", "predictions", ".", "shape", ")", ")", "\n", "\n", "# Keep track of which ground truth boxes were already matched to a detection.", "\n", "", "gt_matched", "=", "{", "}", "\n", "\n", "# Iterate over all predictions.", "\n", "for", "i", "in", "tr", ":", "\n", "\n", "                ", "prediction", "=", "predictions_sorted", "[", "i", "]", "\n", "image_id", "=", "prediction", "[", "'image_id'", "]", "\n", "pred_box", "=", "np", ".", "asarray", "(", "list", "(", "prediction", "[", "[", "'xmin'", ",", "'ymin'", ",", "'xmax'", ",", "'ymax'", "]", "]", ")", ")", "# Convert the structured array element to a regular array.", "\n", "\n", "# Get the relevant ground truth boxes for this prediction,", "\n", "# i.e. all ground truth boxes that match the prediction's", "\n", "# image ID and class ID.", "\n", "\n", "# The ground truth could either be a tuple with `(ground_truth_boxes, eval_neutral_boxes)`", "\n", "# or only `ground_truth_boxes`.", "\n", "if", "ignore_neutral_boxes", "and", "eval_neutral_available", ":", "\n", "                    ", "gt", ",", "eval_neutral", "=", "ground_truth", "[", "image_id", "]", "\n", "", "else", ":", "\n", "                    ", "gt", "=", "ground_truth", "[", "image_id", "]", "\n", "", "gt", "=", "np", ".", "asarray", "(", "gt", ")", "\n", "class_mask", "=", "gt", "[", ":", ",", "class_id_gt", "]", "==", "class_id", "\n", "gt", "=", "gt", "[", "class_mask", "]", "\n", "if", "ignore_neutral_boxes", "and", "eval_neutral_available", ":", "\n", "                    ", "eval_neutral", "=", "eval_neutral", "[", "class_mask", "]", "\n", "\n", "", "if", "gt", ".", "size", "==", "0", ":", "\n", "# If the image doesn't contain any objects of this class,", "\n", "# the prediction becomes a false positive.", "\n", "                    ", "false_pos", "[", "i", "]", "=", "1", "\n", "continue", "\n", "\n", "# Compute the IoU of this prediction with all ground truth boxes of the same class.", "\n", "", "aaaa", "=", "gt", "[", ":", ",", "[", "xmin_gt", ",", "ymin_gt", ",", "xmax_gt", ",", "ymax_gt", "]", "]", "\n", "overlaps", "=", "iou", "(", "boxes1", "=", "gt", "[", ":", ",", "[", "xmin_gt", ",", "ymin_gt", ",", "xmax_gt", ",", "ymax_gt", "]", "]", ",", "\n", "boxes2", "=", "pred_box", ",", "\n", "coords", "=", "'corners'", ",", "\n", "mode", "=", "'element-wise'", ",", "\n", "border_pixels", "=", "border_pixels", ")", "\n", "\n", "# For each detection, match the ground truth box with the highest overlap.", "\n", "# It's possible that the same ground truth box will be matched to multiple", "\n", "# detections.", "\n", "gt_match_index", "=", "np", ".", "argmax", "(", "overlaps", ")", "\n", "gt_match_overlap", "=", "overlaps", "[", "gt_match_index", "]", "\n", "\n", "if", "gt_match_overlap", "<", "matching_iou_threshold", ":", "\n", "# False positive, IoU threshold violated:", "\n", "# Those predictions whose matched overlap is below the threshold become", "\n", "# false positives.", "\n", "                    ", "false_pos", "[", "i", "]", "=", "1", "\n", "", "else", ":", "\n", "                    ", "if", "not", "(", "ignore_neutral_boxes", "and", "eval_neutral_available", ")", "or", "(", "eval_neutral", "[", "gt_match_index", "]", "==", "False", ")", ":", "\n", "# If this is not a ground truth that is supposed to be evaluation-neutral", "\n", "# (i.e. should be skipped for the evaluation) or if we don't even have the", "\n", "# concept of neutral boxes.", "\n", "                        ", "if", "not", "(", "image_id", "in", "gt_matched", ")", ":", "\n", "# True positive:", "\n", "# If the matched ground truth box for this prediction hasn't been matched to a", "\n", "# different prediction already, we have a true positive.", "\n", "                            ", "true_pos", "[", "i", "]", "=", "1", "\n", "gt_matched", "[", "image_id", "]", "=", "np", ".", "zeros", "(", "shape", "=", "(", "gt", ".", "shape", "[", "0", "]", ")", ",", "dtype", "=", "np", ".", "bool", ")", "\n", "gt_matched", "[", "image_id", "]", "[", "gt_match_index", "]", "=", "True", "\n", "", "elif", "not", "gt_matched", "[", "image_id", "]", "[", "gt_match_index", "]", ":", "\n", "# True positive:", "\n", "# If the matched ground truth box for this prediction hasn't been matched to a", "\n", "# different prediction already, we have a true positive.", "\n", "                            ", "true_pos", "[", "i", "]", "=", "1", "\n", "gt_matched", "[", "image_id", "]", "[", "gt_match_index", "]", "=", "True", "\n", "", "else", ":", "\n", "# False positive, duplicate detection:", "\n", "# If the matched ground truth box for this prediction has already been matched", "\n", "# to a different prediction previously, it is a duplicate detection for an", "\n", "# already detected object, which counts as a false positive.", "\n", "                            ", "false_pos", "[", "i", "]", "=", "1", "\n", "\n", "", "", "", "", "true_positives", ".", "append", "(", "true_pos", ")", "\n", "false_positives", ".", "append", "(", "false_pos", ")", "\n", "\n", "cumulative_true_pos", "=", "np", ".", "cumsum", "(", "true_pos", ")", "# Cumulative sums of the true positives", "\n", "cumulative_false_pos", "=", "np", ".", "cumsum", "(", "false_pos", ")", "# Cumulative sums of the false positives", "\n", "\n", "cumulative_true_positives", ".", "append", "(", "cumulative_true_pos", ")", "\n", "cumulative_false_positives", ".", "append", "(", "cumulative_false_pos", ")", "\n", "\n", "", "self", ".", "true_positives", "=", "true_positives", "\n", "self", ".", "false_positives", "=", "false_positives", "\n", "self", ".", "cumulative_true_positives", "=", "cumulative_true_positives", "\n", "self", ".", "cumulative_false_positives", "=", "cumulative_false_positives", "\n", "\n", "if", "ret", ":", "\n", "            ", "return", "true_positives", ",", "false_positives", ",", "cumulative_true_positives", ",", "cumulative_false_positives", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_train.Evaluator.compute_precision_recall": [[735, 782], ["average_precision_evaluator_train.Evaluator.write_predictions_to_txt", "range", "ValueError", "ValueError", "numpy.where", "cumulative_precisions.append", "cumulative_recalls.append", "print"], "methods", ["home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_train.Evaluator.write_predictions_to_txt"], ["", "", "def", "compute_precision_recall", "(", "self", ",", "verbose", "=", "True", ",", "ret", "=", "False", ")", ":", "\n", "        ", "'''\n        Computes the precisions and recalls for all classes.\n\n        Note that `match_predictions()` must be called before calling this method.\n\n        Arguments:\n            verbose (bool, optional): If `True`, will print out the progress during runtime.\n            ret (bool, optional): If `True`, returns the precisions and recalls.\n\n        Returns:\n            None by default. Optionally, two nested lists containing the cumulative precisions and recalls for each class.\n        '''", "\n", "\n", "if", "(", "self", ".", "cumulative_true_positives", "is", "None", ")", "or", "(", "self", ".", "cumulative_false_positives", "is", "None", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"True and false positives not available. You must run `match_predictions()` before you call this method.\"", ")", "\n", "\n", "", "if", "(", "self", ".", "num_gt_per_class", "is", "None", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Number of ground truth boxes per class not available. You must run `get_num_gt_per_class()` before you call this method.\"", ")", "\n", "\n", "", "cumulative_precisions", "=", "[", "[", "]", "]", "\n", "cumulative_recalls", "=", "[", "[", "]", "]", "\n", "\n", "classes", "=", "[", "'background'", ",", "'seacucumber'", ",", "'seaurchin'", ",", "'scallop'", "]", "\n", "self", ".", "write_predictions_to_txt", "(", "classes", ",", "out_file_prefix", "=", "'comp3_det_test_'", ",", "verbose", "=", "True", ")", "\n", "\n", "# Iterate over all classes.", "\n", "for", "class_id", "in", "range", "(", "1", ",", "self", ".", "n_classes", "+", "1", ")", ":", "\n", "\n", "            ", "if", "verbose", ":", "\n", "                ", "print", "(", "\"Computing precisions and recalls, class {}/{}\"", ".", "format", "(", "class_id", ",", "self", ".", "n_classes", ")", ")", "\n", "\n", "", "tp", "=", "self", ".", "cumulative_true_positives", "[", "class_id", "]", "\n", "fp", "=", "self", ".", "cumulative_false_positives", "[", "class_id", "]", "\n", "\n", "\n", "cumulative_precision", "=", "np", ".", "where", "(", "tp", "+", "fp", ">", "0", ",", "tp", "/", "(", "tp", "+", "fp", ")", ",", "0", ")", "# 1D array with shape `(num_predictions,)`", "\n", "cumulative_recall", "=", "tp", "/", "self", ".", "num_gt_per_class", "[", "class_id", "]", "# 1D array with shape `(num_predictions,)`", "\n", "\n", "cumulative_precisions", ".", "append", "(", "cumulative_precision", ")", "\n", "cumulative_recalls", ".", "append", "(", "cumulative_recall", ")", "\n", "\n", "", "self", ".", "cumulative_precisions", "=", "cumulative_precisions", "\n", "self", ".", "cumulative_recalls", "=", "cumulative_recalls", "\n", "\n", "if", "ret", ":", "\n", "            ", "return", "cumulative_precisions", ",", "cumulative_recalls", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_train.Evaluator.compute_average_precisions": [[783, 885], ["range", "ValueError", "ValueError", "average_precisions.append", "print", "numpy.linspace", "numpy.unique", "numpy.zeros_like", "numpy.zeros_like", "range", "numpy.sum", "numpy.amax", "numpy.maximum", "len", "numpy.amax"], "methods", ["None"], ["", "", "def", "compute_average_precisions", "(", "self", ",", "mode", "=", "'sample'", ",", "num_recall_points", "=", "11", ",", "verbose", "=", "True", ",", "ret", "=", "False", ")", ":", "\n", "        ", "'''\n        Computes the average precision for each class.\n\n        Can compute the Pascal-VOC-style average precision in both the pre-2010 (k-point sampling)\n        and post-2010 (integration) algorithm versions.\n\n        Note that `compute_precision_recall()` must be called before calling this method.\n\n        Arguments:\n            mode (str, optional): Can be either 'sample' or 'integrate'. In the case of 'sample', the average precision will be computed\n                according to the Pascal VOC formula that was used up until VOC 2009, where the precision will be sampled for `num_recall_points`\n                recall values. In the case of 'integrate', the average precision will be computed according to the Pascal VOC formula that\n                was used from VOC 2010 onward, where the average precision will be computed by numerically integrating over the whole\n                preciscion-recall curve instead of sampling individual points from it. 'integrate' mode is basically just the limit case\n                of 'sample' mode as the number of sample points increases. For details, see the references below.\n            num_recall_points (int, optional): Only relevant if mode is 'sample'. The number of points to sample from the precision-recall-curve\n                to compute the average precisions. In other words, this is the number of equidistant recall values for which the resulting\n                precision will be computed. 11 points is the value used in the official Pascal VOC pre-2010 detection evaluation algorithm.\n            verbose (bool, optional): If `True`, will print out the progress during runtime.\n            ret (bool, optional): If `True`, returns the average precisions.\n\n        Returns:\n            None by default. Optionally, a list containing average precision for each class.\n\n        References:\n            http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/devkit_doc.html#sec:ap\n        '''", "\n", "\n", "if", "(", "self", ".", "cumulative_precisions", "is", "None", ")", "or", "(", "self", ".", "cumulative_recalls", "is", "None", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Precisions and recalls not available. You must run `compute_precision_recall()` before you call this method.\"", ")", "\n", "\n", "", "if", "not", "(", "mode", "in", "{", "'sample'", ",", "'integrate'", "}", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`mode` can be either 'sample' or 'integrate', but received '{}'\"", ".", "format", "(", "mode", ")", ")", "\n", "\n", "", "average_precisions", "=", "[", "0.0", "]", "\n", "\n", "# Iterate over all classes.", "\n", "for", "class_id", "in", "range", "(", "1", ",", "self", ".", "n_classes", "+", "1", ")", ":", "\n", "\n", "            ", "if", "verbose", ":", "\n", "                ", "print", "(", "\"Computing average precision, class {}/{}\"", ".", "format", "(", "class_id", ",", "self", ".", "n_classes", ")", ")", "\n", "\n", "", "cumulative_precision", "=", "self", ".", "cumulative_precisions", "[", "class_id", "]", "\n", "cumulative_recall", "=", "self", ".", "cumulative_recalls", "[", "class_id", "]", "\n", "average_precision", "=", "0.0", "\n", "\n", "if", "mode", "==", "'sample'", ":", "\n", "\n", "                ", "for", "t", "in", "np", ".", "linspace", "(", "start", "=", "0", ",", "stop", "=", "1", ",", "num", "=", "num_recall_points", ",", "endpoint", "=", "True", ")", ":", "\n", "\n", "                    ", "cum_prec_recall_greater_t", "=", "cumulative_precision", "[", "cumulative_recall", ">=", "t", "]", "\n", "\n", "if", "cum_prec_recall_greater_t", ".", "size", "==", "0", ":", "\n", "                        ", "precision", "=", "0.0", "\n", "", "else", ":", "\n", "                        ", "precision", "=", "np", ".", "amax", "(", "cum_prec_recall_greater_t", ")", "\n", "\n", "", "average_precision", "+=", "precision", "\n", "\n", "", "average_precision", "/=", "num_recall_points", "\n", "\n", "", "elif", "mode", "==", "'integrate'", ":", "\n", "\n", "# We will compute the precision at all unique recall values.", "\n", "                ", "unique_recalls", ",", "unique_recall_indices", ",", "unique_recall_counts", "=", "np", ".", "unique", "(", "cumulative_recall", ",", "return_index", "=", "True", ",", "return_counts", "=", "True", ")", "\n", "\n", "# Store the maximal precision for each recall value and the absolute difference", "\n", "# between any two unique recal values in the lists below. The products of these", "\n", "# two nummbers constitute the rectangular areas whose sum will be our numerical", "\n", "# integral.", "\n", "maximal_precisions", "=", "np", ".", "zeros_like", "(", "unique_recalls", ")", "\n", "recall_deltas", "=", "np", ".", "zeros_like", "(", "unique_recalls", ")", "\n", "\n", "# Iterate over all unique recall values in reverse order. This saves a lot of computation:", "\n", "# For each unique recall value `r`, we want to get the maximal precision value obtained", "\n", "# for any recall value `r* >= r`. Once we know the maximal precision for the last `k` recall", "\n", "# values after a given iteration, then in the next iteration, in order compute the maximal", "\n", "# precisions for the last `l > k` recall values, we only need to compute the maximal precision", "\n", "# for `l - k` recall values and then take the maximum between that and the previously computed", "\n", "# maximum instead of computing the maximum over all `l` values.", "\n", "# We skip the very last recall value, since the precision after between the last recall value", "\n", "# recall 1.0 is defined to be zero.", "\n", "for", "i", "in", "range", "(", "len", "(", "unique_recalls", ")", "-", "2", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "                    ", "begin", "=", "unique_recall_indices", "[", "i", "]", "\n", "end", "=", "unique_recall_indices", "[", "i", "+", "1", "]", "\n", "# When computing the maximal precisions, use the maximum of the previous iteration to", "\n", "# avoid unnecessary repeated computation over the same precision values.", "\n", "# The maximal precisions are the heights of the rectangle areas of our integral under", "\n", "# the precision-recall curve.", "\n", "maximal_precisions", "[", "i", "]", "=", "np", ".", "maximum", "(", "np", ".", "amax", "(", "cumulative_precision", "[", "begin", ":", "end", "]", ")", ",", "maximal_precisions", "[", "i", "+", "1", "]", ")", "\n", "# The differences between two adjacent recall values are the widths of our rectangle areas.", "\n", "recall_deltas", "[", "i", "]", "=", "unique_recalls", "[", "i", "+", "1", "]", "-", "unique_recalls", "[", "i", "]", "\n", "\n", "", "average_precision", "=", "np", ".", "sum", "(", "maximal_precisions", "*", "recall_deltas", ")", "\n", "\n", "", "average_precisions", ".", "append", "(", "average_precision", ")", "\n", "\n", "", "self", ".", "average_precisions", "=", "average_precisions", "\n", "\n", "if", "ret", ":", "\n", "            ", "return", "average_precisions", "\n", "\n"]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.eval_utils.average_precision_evaluator_train.Evaluator.compute_mean_average_precision": [[886, 907], ["numpy.average", "ValueError"], "methods", ["None"], ["", "", "def", "compute_mean_average_precision", "(", "self", ",", "ret", "=", "True", ")", ":", "\n", "        ", "'''\n        Computes the mean average precision over all classes.\n\n        Note that `compute_average_precisions()` must be called before calling this method.\n\n        Arguments:\n            ret (bool, optional): If `True`, returns the mean average precision.\n\n        Returns:\n            A float, the mean average precision, by default. Optionally, None.\n        '''", "\n", "\n", "if", "self", ".", "average_precisions", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Average precisions not available. You must run `compute_average_precisions()` before you call this method.\"", ")", "\n", "\n", "", "mean_average_precision", "=", "np", ".", "average", "(", "self", ".", "average_precisions", "[", "1", ":", "]", ")", "# The first element is for the background class, so skip it.", "\n", "self", ".", "mean_average_precision", "=", "mean_average_precision", "\n", "\n", "if", "ret", ":", "\n", "            ", "return", "mean_average_precision", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.LongChenCV_SWIPENet.misc_utils.tensor_sampling_utils.sample_tensors": [[21, 178], ["enumerate", "numpy.copy", "subsampled_weights_list.append", "ValueError", "ValueError", "isinstance", "len", "range", "numpy.array", "upsampled_weights_list.append", "isinstance", "len", "len", "len", "numpy.amax", "sampling_slices.append", "np.array.append", "isinstance", "len", "subsampled_weights_list.append", "numpy.random.normal", "numpy.arange", "numpy.array", "numpy.sort", "numpy.concatenate", "len", "range", "numpy.array", "ValueError", "numpy.array", "len", "np.array.append", "ValueError", "numpy.ix_", "numpy.copy", "numpy.zeros", "ValueError", "numpy.random.choice", "numpy.ix_", "len", "upsampled_weights_list.append", "numpy.arange", "sampling_slices.append", "numpy.arange", "numpy.random.normal", "numpy.array", "numpy.sort", "numpy.concatenate", "sampling_slices.append", "numpy.arange", "sampling_slices.append", "up_sample.append", "type", "numpy.zeros", "ValueError", "numpy.ix_", "numpy.random.choice", "numpy.ix_", "numpy.arange"], "function", ["None"], ["def", "sample_tensors", "(", "weights_list", ",", "sampling_instructions", ",", "axes", "=", "None", ",", "init", "=", "None", ",", "mean", "=", "0.0", ",", "stddev", "=", "0.005", ")", ":", "\n", "    ", "'''\n    Can sub-sample and/or up-sample individual dimensions of the tensors in the given list\n    of input tensors.\n\n    It is possible to sub-sample some dimensions and up-sample other dimensions at the same time.\n\n    The tensors in the list will be sampled consistently, i.e. for any given dimension that\n    corresponds among all tensors in the list, the same elements will be picked for every tensor\n    along that dimension.\n\n    For dimensions that are being sub-sampled, you can either provide a list of the indices\n    that should be picked, or you can provide the number of elements to be sub-sampled, in which\n    case the elements will be chosen at random.\n\n    For dimensions that are being up-sampled, \"filler\" elements will be insterted at random\n    positions along the respective dimension. These filler elements will be initialized either\n    with zero or from a normal distribution with selectable mean and standard deviation.\n\n    Arguments:\n        weights_list (list): A list of Numpy arrays. Each array represents one of the tensors\n            to be sampled. The tensor with the greatest number of dimensions must be the first\n            element in the list. For example, in the case of the weights of a 2D convolutional\n            layer, the kernel must be the first element in the list and the bias the second,\n            not the other way around. For all tensors in the list after the first tensor, the\n            lengths of each of their axes must identical to the length of some axis of the\n            first tensor.\n        sampling_instructions (list): A list that contains the sampling instructions for each\n            dimension of the first tensor. If the first tensor has `n` dimensions, then this\n            must be a list of length `n`. That means, sampling instructions for every dimension\n            of the first tensor must still be given even if not all dimensions should be changed.\n            The elements of this list can be either lists of integers or integers. If the sampling\n            instruction for a given dimension is a list of integers, then these integers represent\n            the indices of the elements of that dimension that will be sub-sampled. If the sampling\n            instruction for a given dimension is an integer, then that number of elements will be\n            sampled along said dimension. If the integer is greater than the number of elements\n            of the input tensors in that dimension, that dimension will be up-sampled. If the integer\n            is smaller than the number of elements of the input tensors in that dimension, that\n            dimension will be sub-sampled. If the integer is equal to the number of elements\n            of the input tensors in that dimension, that dimension will remain the same.\n        axes (list, optional): Only relevant if `weights_list` contains more than one tensor.\n            This list contains a list for each additional tensor in `weights_list` beyond the first.\n            Each of these lists contains integers that determine to which axes of the first tensor\n            the axes of the respective tensor correspond. For example, let the first tensor be a\n            4D tensor and the second tensor in the list be a 2D tensor. If the first element of\n            `axis` is the list `[2,3]`, then that means that the two axes of the second tensor\n            correspond to the last two axes of the first tensor, in the same order. The point of\n            this list is for the program to know, if a given dimension of the first tensor is to\n            be sub- or up-sampled, which dimensions of the other tensors in the list must be\n            sub- or up-sampled accordingly.\n        init (list, optional): Only relevant for up-sampling. Must be `None` or a list of strings\n            that determines for each tensor in `weights_list` how the newly inserted values should\n            be initialized. The possible values are 'gaussian' for initialization from a normal\n            distribution with the selected mean and standard deviation (see the following two arguments),\n            or 'zeros' for zero-initialization. If `None`, all initializations default to\n            'gaussian'.\n        mean (float, optional): Only relevant for up-sampling. The mean of the values that will\n            be inserted into the tensors at random in the case of up-sampling.\n        stddev (float, optional): Only relevant for up-sampling. The standard deviation of the\n            values that will be inserted into the tensors at random in the case of up-sampling.\n\n    Returns:\n        A list containing the sampled tensors in the same order in which they were given.\n    '''", "\n", "\n", "first_tensor", "=", "weights_list", "[", "0", "]", "\n", "\n", "if", "(", "not", "isinstance", "(", "sampling_instructions", ",", "(", "list", ",", "tuple", ")", ")", ")", "or", "(", "len", "(", "sampling_instructions", ")", "!=", "first_tensor", ".", "ndim", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"The sampling instructions must be a list whose length is the number of dimensions of the first tensor in `weights_list`.\"", ")", "\n", "\n", "", "if", "(", "not", "init", "is", "None", ")", "and", "len", "(", "init", ")", "!=", "len", "(", "weights_list", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"`init` must either be `None` or a list of strings that has the same length as `weights_list`.\"", ")", "\n", "\n", "", "up_sample", "=", "[", "]", "# Store the dimensions along which we need to up-sample.", "\n", "out_shape", "=", "[", "]", "# Store the shape of the output tensor here.", "\n", "# Store two stages of the new (sub-sampled and/or up-sampled) weights tensors in the following two lists.", "\n", "subsampled_weights_list", "=", "[", "]", "# Tensors after sub-sampling, but before up-sampling (if any).", "\n", "upsampled_weights_list", "=", "[", "]", "# Sub-sampled tensors after up-sampling (if any), i.e. final output tensors.", "\n", "\n", "# Create the slicing arrays from the sampling instructions.", "\n", "sampling_slices", "=", "[", "]", "\n", "for", "i", ",", "sampling_inst", "in", "enumerate", "(", "sampling_instructions", ")", ":", "\n", "        ", "if", "isinstance", "(", "sampling_inst", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "amax", "=", "np", ".", "amax", "(", "np", ".", "array", "(", "sampling_inst", ")", ")", "\n", "if", "amax", ">=", "first_tensor", ".", "shape", "[", "i", "]", ":", "\n", "                ", "raise", "ValueError", "(", "\"The sample instructions for dimension {} contain index {}, which is greater than the length of that dimension.\"", ".", "format", "(", "i", ",", "amax", ")", ")", "\n", "", "sampling_slices", ".", "append", "(", "np", ".", "array", "(", "sampling_inst", ")", ")", "\n", "out_shape", ".", "append", "(", "len", "(", "sampling_inst", ")", ")", "\n", "", "elif", "isinstance", "(", "sampling_inst", ",", "int", ")", ":", "\n", "            ", "out_shape", ".", "append", "(", "sampling_inst", ")", "\n", "if", "sampling_inst", "==", "first_tensor", ".", "shape", "[", "i", "]", ":", "\n", "# Nothing to sample here, we're keeping the original number of elements along this axis.", "\n", "                ", "sampling_slice", "=", "np", ".", "arange", "(", "sampling_inst", ")", "\n", "sampling_slices", ".", "append", "(", "sampling_slice", ")", "\n", "", "elif", "sampling_inst", "<", "first_tensor", ".", "shape", "[", "i", "]", ":", "\n", "# We want to SUB-sample this dimension. Randomly pick `sample_inst` many elements from it.", "\n", "                ", "sampling_slice1", "=", "np", ".", "array", "(", "[", "0", "]", ")", "# We will always sample class 0, the background class.", "\n", "# Sample the rest of the classes.", "\n", "sampling_slice2", "=", "np", ".", "sort", "(", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "1", ",", "first_tensor", ".", "shape", "[", "i", "]", ")", ",", "sampling_inst", "-", "1", ",", "replace", "=", "False", ")", ")", "\n", "sampling_slice", "=", "np", ".", "concatenate", "(", "[", "sampling_slice1", ",", "sampling_slice2", "]", ")", "\n", "sampling_slices", ".", "append", "(", "sampling_slice", ")", "\n", "", "else", ":", "\n", "# We want to UP-sample. Pick all elements from this dimension.", "\n", "                ", "sampling_slice", "=", "np", ".", "arange", "(", "first_tensor", ".", "shape", "[", "i", "]", ")", "\n", "sampling_slices", ".", "append", "(", "sampling_slice", ")", "\n", "up_sample", ".", "append", "(", "i", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Each element of the sampling instructions must be either an integer or a list/tuple of integers, but received `{}`\"", ".", "format", "(", "type", "(", "sampling_inst", ")", ")", ")", "\n", "\n", "# Process the first tensor.", "\n", "", "", "subsampled_first_tensor", "=", "np", ".", "copy", "(", "first_tensor", "[", "np", ".", "ix_", "(", "*", "sampling_slices", ")", "]", ")", "\n", "subsampled_weights_list", ".", "append", "(", "subsampled_first_tensor", ")", "\n", "\n", "# Process the other tensors.", "\n", "if", "len", "(", "weights_list", ")", ">", "1", ":", "\n", "        ", "for", "j", "in", "range", "(", "1", ",", "len", "(", "weights_list", ")", ")", ":", "\n", "            ", "this_sampling_slices", "=", "[", "sampling_slices", "[", "i", "]", "for", "i", "in", "axes", "[", "j", "-", "1", "]", "]", "# Get the sampling slices for this tensor.", "\n", "subsampled_weights_list", ".", "append", "(", "np", ".", "copy", "(", "weights_list", "[", "j", "]", "[", "np", ".", "ix_", "(", "*", "this_sampling_slices", ")", "]", ")", ")", "\n", "\n", "", "", "if", "up_sample", ":", "\n", "# Take care of the dimensions that are to be up-sampled.", "\n", "\n", "        ", "out_shape", "=", "np", ".", "array", "(", "out_shape", ")", "\n", "\n", "# Process the first tensor.", "\n", "if", "init", "is", "None", "or", "init", "[", "0", "]", "==", "'gaussian'", ":", "\n", "            ", "upsampled_first_tensor", "=", "np", ".", "random", ".", "normal", "(", "loc", "=", "mean", ",", "scale", "=", "stddev", ",", "size", "=", "out_shape", ")", "\n", "", "elif", "init", "[", "0", "]", "==", "'zeros'", ":", "\n", "            ", "upsampled_first_tensor", "=", "np", ".", "zeros", "(", "out_shape", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Valid initializations are 'gaussian' and 'zeros', but received '{}'.\"", ".", "format", "(", "init", "[", "0", "]", ")", ")", "\n", "# Pick the indices of the elements in `upsampled_first_tensor` that should be occupied by `subsampled_first_tensor`.", "\n", "", "up_sample_slices", "=", "[", "np", ".", "arange", "(", "k", ")", "for", "k", "in", "subsampled_first_tensor", ".", "shape", "]", "\n", "for", "i", "in", "up_sample", ":", "\n", "# Randomly select across which indices of this dimension to scatter the elements of `new_weights_tensor` in this dimension.", "\n", "            ", "up_sample_slice1", "=", "np", ".", "array", "(", "[", "0", "]", ")", "\n", "up_sample_slice2", "=", "np", ".", "sort", "(", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "1", ",", "upsampled_first_tensor", ".", "shape", "[", "i", "]", ")", ",", "subsampled_first_tensor", ".", "shape", "[", "i", "]", "-", "1", ",", "replace", "=", "False", ")", ")", "\n", "up_sample_slices", "[", "i", "]", "=", "np", ".", "concatenate", "(", "[", "up_sample_slice1", ",", "up_sample_slice2", "]", ")", "\n", "", "upsampled_first_tensor", "[", "np", ".", "ix_", "(", "*", "up_sample_slices", ")", "]", "=", "subsampled_first_tensor", "\n", "upsampled_weights_list", ".", "append", "(", "upsampled_first_tensor", ")", "\n", "\n", "# Process the other tensors", "\n", "if", "len", "(", "weights_list", ")", ">", "1", ":", "\n", "            ", "for", "j", "in", "range", "(", "1", ",", "len", "(", "weights_list", ")", ")", ":", "\n", "                ", "if", "init", "is", "None", "or", "init", "[", "j", "]", "==", "'gaussian'", ":", "\n", "                    ", "upsampled_tensor", "=", "np", ".", "random", ".", "normal", "(", "loc", "=", "mean", ",", "scale", "=", "stddev", ",", "size", "=", "out_shape", "[", "axes", "[", "j", "-", "1", "]", "]", ")", "\n", "", "elif", "init", "[", "j", "]", "==", "'zeros'", ":", "\n", "                    ", "upsampled_tensor", "=", "np", ".", "zeros", "(", "out_shape", "[", "axes", "[", "j", "-", "1", "]", "]", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "\"Valid initializations are 'gaussian' and 'zeros', but received '{}'.\"", ".", "format", "(", "init", "[", "j", "]", ")", ")", "\n", "", "this_up_sample_slices", "=", "[", "up_sample_slices", "[", "i", "]", "for", "i", "in", "axes", "[", "j", "-", "1", "]", "]", "# Get the up-sampling slices for this tensor.", "\n", "upsampled_tensor", "[", "np", ".", "ix_", "(", "*", "this_up_sample_slices", ")", "]", "=", "subsampled_weights_list", "[", "j", "]", "\n", "upsampled_weights_list", ".", "append", "(", "upsampled_tensor", ")", "\n", "\n", "", "", "return", "upsampled_weights_list", "\n", "", "else", ":", "\n", "        ", "return", "subsampled_weights_list", "\n", "", "", ""]]}