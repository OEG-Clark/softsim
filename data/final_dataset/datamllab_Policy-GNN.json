{"home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.train_citeseer.main": [[10, 53], ["env.gcn.gcn_env", "env.gcn.gcn_env.seed", "dqn_agent_pytorch.DQNAgent", "print", "range", "print", "env.gcn.gcn_env", "env.gcn.gcn_env.seed", "env.gcn.gcn_env.reset2", "range", "dqn_agent_pytorch.DQNAgent.learn", "print", "copy.deepcopy.eval_step", "env.gcn.gcn_env.step2", "env.gcn.gcn_env.test_batch", "print", "int", "torch.device", "copy.deepcopy"], "function", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.seed", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.seed", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.reset2", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.DQNAgent.learn", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.DQNAgent.eval_step", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.step2", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.test_batch"], ["def", "main", "(", ")", ":", "\n", "    ", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", "=", "True", "\n", "### Experiment Settings ###", "\n", "# Cora", "\n", "max_timesteps", "=", "20", "\n", "dataset", "=", "'Citeseer'", "\n", "max_episodes", "=", "135", "\n", "### Experiment Settings ###", "\n", "\n", "env", "=", "gcn_env", "(", "dataset", "=", "dataset", ",", "max_layer", "=", "5", ")", "\n", "env", ".", "seed", "(", "0", ")", "\n", "agent", "=", "DQNAgent", "(", "scope", "=", "'dqn'", ",", "\n", "action_num", "=", "env", ".", "action_num", ",", "\n", "replay_memory_size", "=", "int", "(", "1e4", ")", ",", "\n", "replay_memory_init_size", "=", "500", ",", "\n", "norm_step", "=", "200", ",", "\n", "state_shape", "=", "env", ".", "observation_space", ".", "shape", ",", "\n", "mlp_layers", "=", "[", "32", ",", "64", ",", "128", ",", "64", ",", "32", "]", ",", "\n", "device", "=", "torch", ".", "device", "(", "'cpu'", ")", "\n", ")", "\n", "env", ".", "policy", "=", "agent", "\n", "last_val", "=", "0.0", "\n", "# Training:  meta-policy", "\n", "print", "(", "\"Training Meta-policy on Validation Set\"", ")", "\n", "for", "i_episode", "in", "range", "(", "1", ",", "max_episodes", "+", "1", ")", ":", "\n", "        ", "loss", ",", "reward", ",", "(", "val_acc", ",", "reward", ")", "=", "agent", ".", "learn", "(", "env", ",", "max_timesteps", ")", "# debug = (val_acc, reward)", "\n", "if", "val_acc", ">", "last_val", ":", "# check whether gain improvement on validation set", "\n", "            ", "best_policy", "=", "deepcopy", "(", "agent", ")", "# save the best policy", "\n", "", "last_val", "=", "val_acc", "\n", "print", "(", "\"Training Meta-policy:\"", ",", "i_episode", ",", "\"Val_Acc:\"", ",", "val_acc", ",", "\"Avg_reward:\"", ",", "reward", ")", "\n", "\n", "# Testing: Apply meta-policy to train a new GNN", "\n", "", "test_acc", "=", "0.0", "\n", "print", "(", "\"Training GNNs with learned meta-policy\"", ")", "\n", "new_env", "=", "gcn_env", "(", "dataset", "=", "dataset", ",", "max_layer", "=", "5", ")", "\n", "new_env", ".", "seed", "(", "0", ")", "\n", "new_env", ".", "policy", "=", "best_policy", "\n", "state", "=", "new_env", ".", "reset2", "(", ")", "\n", "for", "i_episode", "in", "range", "(", "1", ",", "1000", ")", ":", "\n", "        ", "action", "=", "best_policy", ".", "eval_step", "(", "state", ")", "\n", "state", ",", "reward", ",", "done", ",", "(", "val_acc", ",", "reward", ")", "=", "new_env", ".", "step2", "(", "action", ")", "\n", "test_acc", "=", "new_env", ".", "test_batch", "(", ")", "\n", "print", "(", "\"Training GNN\"", ",", "i_episode", ",", "\"; Val_Acc:\"", ",", "val_acc", ",", "\"; Test_Acc:\"", ",", "test_acc", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Normalizer.__init__": [[18, 26], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "''' Initialize a Normalizer instance.\n        '''", "\n", "self", ".", "mean", "=", "None", "\n", "self", ".", "std", "=", "None", "\n", "self", ".", "state_memory", "=", "[", "]", "\n", "self", ".", "max_size", "=", "1000", "\n", "self", ".", "length", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Normalizer.normalize": [[27, 39], ["None"], "methods", ["None"], ["", "def", "normalize", "(", "self", ",", "s", ")", ":", "\n", "        ", "''' Normalize the state with the running mean and std.\n\n        Args:\n            s (numpy.array): the input state\n\n        Returns:\n            a (int):  normalized state\n        '''", "\n", "if", "self", ".", "length", "==", "0", ":", "\n", "            ", "return", "s", "\n", "", "return", "(", "s", "-", "self", ".", "mean", ")", "/", "(", "self", ".", "std", "+", "1e-8", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Normalizer.append": [[40, 52], ["dqn_agent_pytorch.Normalizer.state_memory.append", "numpy.mean", "numpy.mean", "len", "len", "dqn_agent_pytorch.Normalizer.state_memory.pop"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Normalizer.append"], ["", "def", "append", "(", "self", ",", "s", ")", ":", "\n", "        ", "''' Append a new state and update the running statistics\n\n        Args:\n            s (numpy.array): the input state\n        '''", "\n", "if", "len", "(", "self", ".", "state_memory", ")", ">", "self", ".", "max_size", ":", "\n", "            ", "self", ".", "state_memory", ".", "pop", "(", "0", ")", "\n", "", "self", ".", "state_memory", ".", "append", "(", "s", ")", "\n", "self", ".", "mean", "=", "np", ".", "mean", "(", "self", ".", "state_memory", ",", "axis", "=", "0", ")", "\n", "self", ".", "std", "=", "np", ".", "mean", "(", "self", ".", "state_memory", ",", "axis", "=", "0", ")", "\n", "self", ".", "length", "=", "len", "(", "self", ".", "state_memory", ")", "\n", "", "", "class", "Memory", "(", "object", ")", ":", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Memory.__init__": [[56, 64], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "memory_size", ",", "batch_size", ")", ":", "\n", "        ", "''' Initialize\n        Args:\n            memory_size (int): the size of the memroy buffer\n        '''", "\n", "self", ".", "memory_size", "=", "memory_size", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "memory", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Memory.save": [[65, 79], ["Transition", "dqn_agent_pytorch.Memory.memory.append", "len", "dqn_agent_pytorch.Memory.memory.pop"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Normalizer.append"], ["", "def", "save", "(", "self", ",", "state", ",", "action", ",", "reward", ",", "next_state", ",", "done", ")", ":", "\n", "        ", "''' Save transition into memory\n\n        Args:\n            state (numpy.array): the current state\n            action (int): the performed action ID\n            reward (float): the reward received\n            next_state (numpy.array): the next state after performing the action\n            done (boolean): whether the episode is finished\n        '''", "\n", "if", "len", "(", "self", ".", "memory", ")", "==", "self", ".", "memory_size", ":", "\n", "            ", "self", ".", "memory", ".", "pop", "(", "0", ")", "\n", "", "transition", "=", "Transition", "(", "state", ",", "action", ",", "reward", ",", "next_state", ",", "done", ")", "\n", "self", ".", "memory", ".", "append", "(", "transition", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Memory.sample": [[80, 92], ["random.sample", "map", "zip"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Memory.sample"], ["", "def", "sample", "(", "self", ")", ":", "\n", "        ", "''' Sample a minibatch from the replay memory\n\n        Returns:\n            state_batch (list): a batch of states\n            action_batch (list): a batch of actions\n            reward_batch (list): a batch of rewards\n            next_state_batch (list): a batch of states\n            done_batch (list): a batch of dones\n        '''", "\n", "samples", "=", "random", ".", "sample", "(", "self", ".", "memory", ",", "self", ".", "batch_size", ")", "\n", "return", "map", "(", "np", ".", "array", ",", "zip", "(", "*", "samples", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.DQNAgent.__init__": [[94, 173], ["numpy.linspace", "dqn_agent_pytorch.Estimator", "dqn_agent_pytorch.Estimator", "dqn_agent_pytorch.Normalizer", "dqn_agent_pytorch.Memory", "torch.device", "torch.device", "torch.device", "torch.device", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "\n", "scope", ",", "\n", "replay_memory_size", "=", "2000", ",", "\n", "replay_memory_init_size", "=", "100", ",", "\n", "update_target_estimator_every", "=", "1000", ",", "\n", "discount_factor", "=", "0.95", ",", "\n", "epsilon_start", "=", "1.0", ",", "\n", "epsilon_end", "=", "0.2", ",", "\n", "epsilon_decay_steps", "=", "100", ",", "\n", "batch_size", "=", "128", ",", "\n", "action_num", "=", "2", ",", "\n", "state_shape", "=", "None", ",", "\n", "norm_step", "=", "100", ",", "\n", "mlp_layers", "=", "None", ",", "\n", "learning_rate", "=", "0.0005", ",", "\n", "device", "=", "None", ")", ":", "\n", "\n", "        ", "'''\n        Q-Learning algorithm for off-policy TD control using Function Approximation.\n        Finds the optimal greedy policy while following an epsilon-greedy policy.\n\n        Args:\n            scope (str): The name of the DQN agent\n            env (object): The Environment.\n            replay_memory_size (int): Size of the replay memory\n            replay_memory_init_size (int): Number of random experiences to sampel when initializing\n              the reply memory.\n            update_target_estimator_every (int): Copy parameters from the Q estimator to the\n              target estimator every N steps\n            discount_factor (float): Gamma discount factor\n            epsilon_start (int): Chance to sample a random action when taking an action.\n              Epsilon is decayed over time and this is the start value\n            epsilon_end (int): The final minimum value of epsilon after decaying is done\n            epsilon_decay_steps (int): Number of steps to decay epsilon over\n            batch_size (int): Size of batches to sample from the replay memory\n            evaluate_every (int): Evaluate every N steps\n            action_num (int): The number of the actions\n            state_space (list): The space of the state vector\n            norm_step (int): The number of the step used form noramlize state\n            mlp_layers (list): The layer number and the dimension of each layer in MLP\n            learning_rate (float): The learning rate of the DQN agent.\n            device (torch.device): whether to use the cpu or gpu\n        '''", "\n", "self", ".", "scope", "=", "scope", "\n", "self", ".", "replay_memory_init_size", "=", "replay_memory_init_size", "\n", "self", ".", "update_target_estimator_every", "=", "update_target_estimator_every", "\n", "self", ".", "discount_factor", "=", "discount_factor", "\n", "self", ".", "epsilon_decay_steps", "=", "epsilon_decay_steps", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "action_num", "=", "action_num", "\n", "self", ".", "norm_step", "=", "norm_step", "\n", "\n", "# Torch device", "\n", "if", "device", "is", "None", ":", "\n", "            ", "self", ".", "device", "=", "torch", ".", "device", "(", "'cuda:0'", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "device", "=", "device", "\n", "\n", "# Total timesteps", "\n", "", "self", ".", "total_t", "=", "0", "\n", "\n", "# Total training step", "\n", "self", ".", "train_t", "=", "0", "\n", "\n", "# The epsilon decay scheduler", "\n", "self", ".", "epsilons", "=", "np", ".", "linspace", "(", "epsilon_start", ",", "epsilon_end", ",", "epsilon_decay_steps", ")", "\n", "\n", "# Create estimators", "\n", "#with tf.variable_scope(scope):", "\n", "self", ".", "q_estimator", "=", "Estimator", "(", "action_num", "=", "action_num", ",", "learning_rate", "=", "learning_rate", ",", "state_shape", "=", "state_shape", ",", "mlp_layers", "=", "mlp_layers", ",", "device", "=", "self", ".", "device", ")", "\n", "self", ".", "target_estimator", "=", "Estimator", "(", "action_num", "=", "action_num", ",", "learning_rate", "=", "learning_rate", ",", "state_shape", "=", "state_shape", ",", "mlp_layers", "=", "mlp_layers", ",", "device", "=", "self", ".", "device", ")", "\n", "\n", "# Create normalizer", "\n", "self", ".", "normalizer", "=", "Normalizer", "(", ")", "\n", "\n", "# Create replay memory", "\n", "self", ".", "memory", "=", "Memory", "(", "replay_memory_size", ",", "batch_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.DQNAgent.learn": [[175, 189], ["env.reset2", "range", "dqn_agent_pytorch.DQNAgent.train", "dqn_agent_pytorch.DQNAgent.predict_batch", "numpy.random.choice", "env.step2", "zip", "numpy.arange", "dqn_agent_pytorch.DQNAgent.feed", "len"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.reset2", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.train", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.DQNAgent.predict_batch", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.step2", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.DQNAgent.feed"], ["", "def", "learn", "(", "self", ",", "env", ",", "total_timesteps", ")", ":", "\n", "        ", "done", "=", "[", "False", "]", "\n", "next_state_batch", "=", "env", ".", "reset2", "(", ")", "\n", "trajectories", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "total_timesteps", ")", ":", "\n", "            ", "A", "=", "self", ".", "predict_batch", "(", "next_state_batch", ")", "\n", "best_actions", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "len", "(", "A", ")", ")", ",", "p", "=", "A", ",", "size", "=", "next_state_batch", ".", "shape", "[", "0", "]", ")", "\n", "state_batch", "=", "next_state_batch", "\n", "next_state_batch", ",", "reward_batch", ",", "done_batch", ",", "debug", "=", "env", ".", "step2", "(", "best_actions", ")", "# debug = (val_acc, test_acc)", "\n", "trajectories", "=", "zip", "(", "state_batch", ",", "best_actions", ",", "reward_batch", ",", "next_state_batch", ",", "done_batch", ")", "\n", "for", "each", "in", "trajectories", ":", "\n", "                ", "self", ".", "feed", "(", "each", ")", "\n", "", "", "loss", "=", "self", ".", "train", "(", ")", "\n", "return", "loss", ",", "reward_batch", ",", "debug", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.DQNAgent.feed": [[190, 205], ["tuple", "dqn_agent_pytorch.DQNAgent.feed_norm", "dqn_agent_pytorch.DQNAgent.feed_memory"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.DQNAgent.feed_norm", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.DQNAgent.feed_memory"], ["", "def", "feed", "(", "self", ",", "ts", ")", ":", "\n", "        ", "''' Store data in to replay buffer and train the agent. There are two stages.\n            In stage 1, populate the Normalizer to calculate mean and std.\n            The transition is NOT stored in the memory\n            In stage 2, the transition is stored to the memory.\n\n        Args:\n            ts (list): a list of 5 elements that represent the transition\n        '''", "\n", "(", "state", ",", "action", ",", "reward", ",", "next_state", ",", "done", ")", "=", "tuple", "(", "ts", ")", "\n", "if", "self", ".", "total_t", "<", "self", ".", "norm_step", ":", "\n", "            ", "self", ".", "feed_norm", "(", "state", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "feed_memory", "(", "state", ",", "action", ",", "reward", ",", "next_state", ",", "done", ")", "\n", "", "self", ".", "total_t", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.DQNAgent.step": [[206, 219], ["dqn_agent_pytorch.DQNAgent.predict", "numpy.random.choice", "numpy.arange", "len"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.DQNAgent.predict"], ["", "def", "step", "(", "self", ",", "state", ")", ":", "\n", "        ", "''' Predict the action for genrating training data but\n            have the predictions disconnected from the computation graph\n\n        Args:\n            state (numpy.array): current state\n\n        Returns:\n            action (int): an action id\n        '''", "\n", "A", "=", "self", ".", "predict", "(", "state", ")", "\n", "action", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "len", "(", "A", ")", ")", ",", "p", "=", "A", ")", "\n", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.DQNAgent.eval_step": [[220, 232], ["dqn_agent_pytorch.DQNAgent.q_estimator.predict_nograd", "numpy.argmax", "dqn_agent_pytorch.DQNAgent.normalizer.normalize"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Estimator.predict_nograd", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Normalizer.normalize"], ["", "def", "eval_step", "(", "self", ",", "states", ")", ":", "\n", "        ", "''' Predict the action for evaluation purpose.\n\n        Args:\n            state (numpy.array): current state\n\n        Returns:\n            action (int): an action id\n        '''", "\n", "q_values", "=", "self", ".", "q_estimator", ".", "predict_nograd", "(", "self", ".", "normalizer", ".", "normalize", "(", "states", ")", ")", "\n", "best_actions", "=", "np", ".", "argmax", "(", "q_values", ",", "axis", "=", "1", ")", "\n", "return", "best_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.DQNAgent.predict": [[233, 249], ["numpy.argmax", "dqn_agent_pytorch.DQNAgent.q_estimator.predict_nograd", "min", "numpy.ones", "numpy.expand_dims", "dqn_agent_pytorch.DQNAgent.normalizer.normalize"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Estimator.predict_nograd", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Normalizer.normalize"], ["", "def", "predict", "(", "self", ",", "state", ")", ":", "\n", "        ", "''' Predict the action probabilities but have them\n            disconnected from the computation graph\n\n        Args:\n            state (numpy.array): current state\n\n        Returns:\n            q_values (numpy.array): a 1-d array where each entry represents a Q value\n        '''", "\n", "epsilon", "=", "self", ".", "epsilons", "[", "min", "(", "self", ".", "total_t", ",", "self", ".", "epsilon_decay_steps", "-", "1", ")", "]", "\n", "A", "=", "np", ".", "ones", "(", "self", ".", "action_num", ",", "dtype", "=", "float", ")", "*", "epsilon", "/", "self", ".", "action_num", "\n", "q_values", "=", "self", ".", "q_estimator", ".", "predict_nograd", "(", "np", ".", "expand_dims", "(", "self", ".", "normalizer", ".", "normalize", "(", "state", ")", ",", "0", ")", ")", "[", "0", "]", "\n", "best_action", "=", "np", ".", "argmax", "(", "q_values", ")", "\n", "A", "[", "best_action", "]", "+=", "(", "1.0", "-", "epsilon", ")", "\n", "return", "A", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.DQNAgent.predict_batch": [[250, 259], ["dqn_agent_pytorch.DQNAgent.q_estimator.predict_nograd", "numpy.argmax", "dqn_agent_pytorch.DQNAgent.normalizer.normalize", "A.sum", "min", "numpy.ones"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Estimator.predict_nograd", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Normalizer.normalize"], ["", "def", "predict_batch", "(", "self", ",", "states", ")", ":", "\n", "        ", "epsilon", "=", "self", ".", "epsilons", "[", "min", "(", "self", ".", "total_t", ",", "self", ".", "epsilon_decay_steps", "-", "1", ")", "]", "\n", "A", "=", "np", ".", "ones", "(", "self", ".", "action_num", ",", "dtype", "=", "float", ")", "*", "epsilon", "/", "self", ".", "action_num", "\n", "q_values", "=", "self", ".", "q_estimator", ".", "predict_nograd", "(", "self", ".", "normalizer", ".", "normalize", "(", "states", ")", ")", "\n", "best_action", "=", "np", ".", "argmax", "(", "q_values", ",", "axis", "=", "1", ")", "\n", "for", "a", "in", "best_action", ":", "\n", "            ", "A", "[", "best_action", "]", "+=", "(", "1.0", "-", "epsilon", ")", "\n", "", "A", "=", "A", "/", "A", ".", "sum", "(", ")", "\n", "return", "A", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.DQNAgent.train": [[260, 288], ["dqn_agent_pytorch.DQNAgent.memory.sample", "dqn_agent_pytorch.DQNAgent.q_estimator.predict_nograd", "numpy.argmax", "dqn_agent_pytorch.DQNAgent.target_estimator.predict_nograd", "numpy.array", "dqn_agent_pytorch.DQNAgent.q_estimator.update", "copy.deepcopy", "numpy.invert().astype", "numpy.invert", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Memory.sample", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Estimator.predict_nograd", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Estimator.predict_nograd", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Estimator.update"], ["", "def", "train", "(", "self", ")", ":", "\n", "        ", "''' Train the network\n\n        Returns:\n            loss (float): The loss of the current batch.\n        '''", "\n", "state_batch", ",", "action_batch", ",", "reward_batch", ",", "next_state_batch", ",", "done_batch", "=", "self", ".", "memory", ".", "sample", "(", ")", "\n", "\n", "# Calculate best next actions using Q-network (Double DQN)", "\n", "q_values_next", "=", "self", ".", "q_estimator", ".", "predict_nograd", "(", "next_state_batch", ")", "\n", "best_actions", "=", "np", ".", "argmax", "(", "q_values_next", ",", "axis", "=", "1", ")", "\n", "\n", "# Evaluate best next actions using Target-network (Double DQN)", "\n", "q_values_next_target", "=", "self", ".", "target_estimator", ".", "predict_nograd", "(", "next_state_batch", ")", "\n", "target_batch", "=", "reward_batch", "+", "np", ".", "invert", "(", "done_batch", ")", ".", "astype", "(", "np", ".", "float32", ")", "*", "self", ".", "discount_factor", "*", "q_values_next_target", "[", "np", ".", "arange", "(", "self", ".", "batch_size", ")", ",", "best_actions", "]", "\n", "\n", "# Perform gradient descent update", "\n", "state_batch", "=", "np", ".", "array", "(", "state_batch", ")", "\n", "\n", "loss", "=", "self", ".", "q_estimator", ".", "update", "(", "state_batch", ",", "action_batch", ",", "target_batch", ")", "\n", "\n", "# Update the target estimator", "\n", "if", "self", ".", "train_t", "%", "self", ".", "update_target_estimator_every", "==", "0", ":", "\n", "            ", "self", ".", "target_estimator", "=", "deepcopy", "(", "self", ".", "q_estimator", ")", "\n", "\n", "", "self", ".", "train_t", "+=", "1", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.DQNAgent.feed_norm": [[289, 296], ["dqn_agent_pytorch.DQNAgent.normalizer.append"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Normalizer.append"], ["", "def", "feed_norm", "(", "self", ",", "state", ")", ":", "\n", "        ", "''' Feed state to normalizer to collect statistics\n\n        Args:\n            state (numpy.array): the state that will be feed into normalizer\n        '''", "\n", "self", ".", "normalizer", ".", "append", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.DQNAgent.feed_memory": [[297, 308], ["dqn_agent_pytorch.DQNAgent.memory.save", "dqn_agent_pytorch.DQNAgent.normalizer.normalize", "dqn_agent_pytorch.DQNAgent.normalizer.normalize"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Memory.save", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Normalizer.normalize", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Normalizer.normalize"], ["", "def", "feed_memory", "(", "self", ",", "state", ",", "action", ",", "reward", ",", "next_state", ",", "done", ")", ":", "\n", "        ", "''' Feed transition to memory\n\n        Args:\n            state (numpy.array): the current state\n            action (int): the performed action ID\n            reward (float): the reward received\n            next_state (numpy.array): the next state after performing the action\n            done (boolean): whether the episode is finished\n        '''", "\n", "self", ".", "memory", ".", "save", "(", "self", ".", "normalizer", ".", "normalize", "(", "state", ")", ",", "action", ",", "reward", ",", "self", ".", "normalizer", ".", "normalize", "(", "next_state", ")", ",", "done", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Estimator.__init__": [[318, 349], ["dqn_agent_pytorch.EstimatorNetwork", "qnet.to.to.to", "dqn_agent_pytorch.Estimator.qnet.eval", "dqn_agent_pytorch.Estimator.qnet.parameters", "torch.MSELoss", "torch.MSELoss", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "dqn_agent_pytorch.Estimator.qnet.parameters", "len", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "action_num", "=", "2", ",", "learning_rate", "=", "0.001", ",", "state_shape", "=", "None", ",", "mlp_layers", "=", "None", ",", "device", "=", "None", ")", ":", "\n", "        ", "''' Initilalize an Estimator object.\n\n        Args:\n            action_num (int): the number output actions\n            state_shape (list): the shape of the state space\n            mlp_layers (list): size of outputs of mlp layers\n            device (torch.device): whether to use cpu or gpu\n        '''", "\n", "self", ".", "action_num", "=", "action_num", "\n", "self", ".", "learning_rate", "=", "learning_rate", "\n", "self", ".", "state_shape", "=", "state_shape", "\n", "self", ".", "mlp_layers", "=", "mlp_layers", "\n", "self", ".", "device", "=", "device", "\n", "\n", "# set up Q model and place it in eval mode", "\n", "qnet", "=", "EstimatorNetwork", "(", "action_num", ",", "state_shape", ",", "mlp_layers", ")", "\n", "qnet", "=", "qnet", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "qnet", "=", "qnet", "\n", "self", ".", "qnet", ".", "eval", "(", ")", "\n", "\n", "# initialize the weights using Xavier init", "\n", "for", "p", "in", "self", ".", "qnet", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "len", "(", "p", ".", "data", ".", "shape", ")", ">", "1", ":", "\n", "                ", "nn", ".", "init", ".", "xavier_uniform_", "(", "p", ".", "data", ")", "\n", "\n", "# set up loss function", "\n", "", "", "self", ".", "mse_loss", "=", "nn", ".", "MSELoss", "(", "reduction", "=", "'mean'", ")", "\n", "\n", "# set up optimizer", "\n", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "qnet", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "learning_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Estimator.predict_nograd": [[350, 366], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "dqn_agent_pytorch.Estimator.qnet().to().numpy", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "dqn_agent_pytorch.Estimator.qnet().to", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "dqn_agent_pytorch.Estimator.qnet"], "methods", ["None"], ["", "def", "predict_nograd", "(", "self", ",", "s", ")", ":", "\n", "        ", "''' Predicts action values, but prediction is not included\n            in the computation graph.  It is used to predict optimal next\n            actions in the Double-DQN algorithm.\n\n        Args:\n          s (np.ndarray): (batch, state_len)\n\n        Returns:\n          np.ndarray of shape (batch_size, NUM_VALID_ACTIONS) containing the estimated\n          action values.\n        '''", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "s", "=", "torch", ".", "from_numpy", "(", "s", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "q_as", "=", "self", ".", "qnet", "(", "s", ")", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "", "return", "q_as", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Estimator.update": [[367, 404], ["dqn_agent_pytorch.Estimator.optimizer.zero_grad", "dqn_agent_pytorch.Estimator.qnet.train", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().long().to", "torch.from_numpy().long().to", "torch.from_numpy().long().to", "torch.from_numpy().long().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "dqn_agent_pytorch.Estimator.qnet", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "dqn_agent_pytorch.Estimator.mse_loss", "batch_loss.item.item.backward", "dqn_agent_pytorch.Estimator.optimizer.step", "batch_loss.item.item.item", "dqn_agent_pytorch.Estimator.qnet.eval", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy().long().to.unsqueeze", "torch.from_numpy().long().to.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.train", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.step"], ["", "def", "update", "(", "self", ",", "s", ",", "a", ",", "y", ")", ":", "\n", "        ", "''' Updates the estimator towards the given targets.\n            In this case y is the target-network estimated\n            value of the Q-network optimal actions, which\n            is labeled y in Algorithm 1 of Minh et al. (2015)\n\n        Args:\n          s (np.ndarray): (batch, state_shape) state representation\n          a (np.ndarray): (batch,) integer sampled actions\n          y (np.ndarray): (batch,) value of optimal actions according to Q-target\n\n        Returns:\n          The calculated loss on the batch.\n        '''", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "self", ".", "qnet", ".", "train", "(", ")", "\n", "\n", "s", "=", "torch", ".", "from_numpy", "(", "s", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "a", "=", "torch", ".", "from_numpy", "(", "a", ")", ".", "long", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "y", "=", "torch", ".", "from_numpy", "(", "y", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# (batch, state_shape) -> (batch, action_num)", "\n", "q_as", "=", "self", ".", "qnet", "(", "s", ")", "\n", "\n", "# (batch, action_num) -> (batch, )", "\n", "Q", "=", "torch", ".", "gather", "(", "q_as", ",", "dim", "=", "-", "1", ",", "index", "=", "a", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "# update model", "\n", "batch_loss", "=", "self", ".", "mse_loss", "(", "Q", ",", "y", ")", "\n", "batch_loss", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "batch_loss", "=", "batch_loss", ".", "item", "(", ")", "\n", "\n", "self", ".", "qnet", ".", "eval", "(", ")", "\n", "\n", "return", "batch_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.EstimatorNetwork.__init__": [[410, 432], ["torch.Module.__init__", "range", "fc.append", "torch.Sequential", "torch.Sequential", "torch.Flatten", "torch.Flatten", "fc.append", "fc.append", "torch.Linear", "torch.Linear", "numpy.prod", "len", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.__init__", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Normalizer.append", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Normalizer.append", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Normalizer.append"], ["def", "__init__", "(", "self", ",", "action_num", "=", "2", ",", "state_shape", "=", "None", ",", "mlp_layers", "=", "None", ")", ":", "\n", "        ", "''' Initialize the Q network\n\n        Args:\n            action_num (int): number of legal actions\n            state_shape (list): shape of state tensor\n            mlp_layers (list): output size of each fc layer\n        '''", "\n", "super", "(", "EstimatorNetwork", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "action_num", "=", "action_num", "\n", "self", ".", "state_shape", "=", "state_shape", "\n", "self", ".", "mlp_layers", "=", "mlp_layers", "\n", "\n", "# build the Q network", "\n", "layer_dims", "=", "[", "np", ".", "prod", "(", "self", ".", "state_shape", ")", "]", "+", "self", ".", "mlp_layers", "\n", "fc", "=", "[", "nn", ".", "Flatten", "(", ")", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "layer_dims", ")", "-", "1", ")", ":", "\n", "            ", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "layer_dims", "[", "i", "]", ",", "layer_dims", "[", "i", "+", "1", "]", ",", "bias", "=", "True", ")", ")", "\n", "fc", ".", "append", "(", "nn", ".", "Tanh", "(", ")", ")", "\n", "", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "layer_dims", "[", "-", "1", "]", ",", "self", ".", "action_num", ",", "bias", "=", "True", ")", ")", "\n", "self", ".", "fc_layers", "=", "nn", ".", "Sequential", "(", "*", "fc", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.EstimatorNetwork.forward": [[433, 440], ["dqn_agent_pytorch.EstimatorNetwork.fc_layers"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "s", ")", ":", "\n", "        ", "''' Predict action values\n\n        Args:\n            s  (Tensor): (batch, state_shape)\n        '''", "\n", "return", "self", ".", "fc_layers", "(", "s", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.train_cora.main": [[10, 53], ["env.gcn.gcn_env", "env.gcn.gcn_env.seed", "dqn_agent_pytorch.DQNAgent", "print", "range", "print", "env.gcn.gcn_env", "env.gcn.gcn_env.seed", "env.gcn.gcn_env.reset2", "range", "dqn_agent_pytorch.DQNAgent.learn", "print", "copy.deepcopy.eval_step", "env.gcn.gcn_env.step2", "env.gcn.gcn_env.test_batch", "print", "int", "torch.device", "copy.deepcopy"], "function", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.seed", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.seed", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.reset2", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.DQNAgent.learn", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.DQNAgent.eval_step", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.step2", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.test_batch"], ["def", "main", "(", ")", ":", "\n", "    ", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", "=", "True", "\n", "### Experiment Settings ###", "\n", "# Cora", "\n", "max_timesteps", "=", "10", "\n", "dataset", "=", "'Cora'", "\n", "max_episodes", "=", "325", "\n", "### Experiment Settings ###", "\n", "\n", "env", "=", "gcn_env", "(", "dataset", "=", "dataset", ",", "max_layer", "=", "5", ")", "\n", "env", ".", "seed", "(", "0", ")", "\n", "agent", "=", "DQNAgent", "(", "scope", "=", "'dqn'", ",", "\n", "action_num", "=", "env", ".", "action_num", ",", "\n", "replay_memory_size", "=", "int", "(", "1e4", ")", ",", "\n", "replay_memory_init_size", "=", "500", ",", "\n", "norm_step", "=", "200", ",", "\n", "state_shape", "=", "env", ".", "observation_space", ".", "shape", ",", "\n", "mlp_layers", "=", "[", "32", ",", "64", ",", "128", ",", "64", ",", "32", "]", ",", "\n", "device", "=", "torch", ".", "device", "(", "'cpu'", ")", "\n", ")", "\n", "env", ".", "policy", "=", "agent", "\n", "last_val", "=", "0.0", "\n", "# Training: Learning meta-policy", "\n", "print", "(", "\"Training Meta-policy on Validation Set\"", ")", "\n", "for", "i_episode", "in", "range", "(", "1", ",", "max_episodes", "+", "1", ")", ":", "\n", "        ", "loss", ",", "reward", ",", "(", "val_acc", ",", "reward", ")", "=", "agent", ".", "learn", "(", "env", ",", "max_timesteps", ")", "# debug = (val_acc, reward)", "\n", "if", "val_acc", ">", "last_val", ":", "# check whether gain improvement on validation set", "\n", "            ", "best_policy", "=", "deepcopy", "(", "agent", ")", "# save the best policy", "\n", "", "last_val", "=", "val_acc", "\n", "print", "(", "\"Training Meta-policy:\"", ",", "i_episode", ",", "\"Val_Acc:\"", ",", "val_acc", ",", "\"Avg_reward:\"", ",", "reward", ")", "\n", "\n", "# Testing: Apply meta-policy to train a new GNN", "\n", "", "test_acc", "=", "0.0", "\n", "print", "(", "\"Training GNNs with learned meta-policy\"", ")", "\n", "new_env", "=", "gcn_env", "(", "dataset", "=", "dataset", ",", "max_layer", "=", "5", ")", "\n", "new_env", ".", "seed", "(", "0", ")", "\n", "new_env", ".", "policy", "=", "best_policy", "\n", "state", "=", "new_env", ".", "reset2", "(", ")", "\n", "for", "i_episode", "in", "range", "(", "1", ",", "1000", ")", ":", "\n", "        ", "action", "=", "best_policy", ".", "eval_step", "(", "state", ")", "\n", "state", ",", "reward", ",", "done", ",", "(", "val_acc", ",", "reward", ")", "=", "new_env", ".", "step2", "(", "action", ")", "\n", "test_acc", "=", "new_env", ".", "test_batch", "(", ")", "\n", "print", "(", "\"Training GNN\"", ",", "i_episode", ",", "\"; Val_Acc:\"", ",", "val_acc", ",", "\"; Test_Acc:\"", ",", "test_acc", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.Net.__init__": [[19, 26], ["super().__init__", "torch_geometric.nn.GCNConv", "range", "torch_geometric.nn.GCNConv", "gcn.Net.hidden.append", "torch_geometric.nn.GCNConv"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.__init__", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Normalizer.append"], ["    ", "def", "__init__", "(", "self", ",", "max_layer", "=", "10", ",", "dataset", "=", "'Cora'", ")", ":", "\n", "        ", "self", ".", "hidden", "=", "[", "]", "\n", "super", "(", "Net", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv1", "=", "GCNConv", "(", "dataset", ".", "num_features", ",", "16", ",", "cached", "=", "True", ")", "\n", "for", "i", "in", "range", "(", "max_layer", "-", "2", ")", ":", "\n", "            ", "self", ".", "hidden", ".", "append", "(", "GCNConv", "(", "16", ",", "16", ",", "cached", "=", "True", ")", ")", "\n", "", "self", ".", "conv2", "=", "GCNConv", "(", "16", ",", "dataset", ".", "num_classes", ",", "cached", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.Net.forward": [[28, 36], ["torch.relu", "torch.relu", "range", "gcn.Net.conv2", "torch.log_softmax", "torch.log_softmax", "gcn.Net.conv1", "torch.relu", "torch.relu", "torch.dropout", "torch.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "action", ",", "data", ")", ":", "\n", "        ", "x", ",", "edge_index", "=", "data", ".", "x", ",", "data", ".", "edge_index", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "conv1", "(", "x", ",", "edge_index", ")", ")", "\n", "for", "i", "in", "range", "(", "action", "-", "2", ")", ":", "\n", "            ", "x", "=", "F", ".", "relu", "(", "self", ".", "hidden", "[", "i", "]", "(", "x", ",", "edge_index", ")", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "training", "=", "self", ".", "training", ")", "\n", "", "self", ".", "embedding", "=", "self", ".", "conv2", "(", "x", ",", "edge_index", ")", "\n", "return", "F", ".", "log_softmax", "(", "self", ".", "embedding", ",", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.__init__": [[38, 74], ["os.join", "os.join", "torch_geometric.datasets.Planetoid", "numpy.array", "gcn.gcn_env.init_k_hop", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "gcn.gcn_env.data.train_mask.to().numpy", "gcn.gcn_env._set_action_space", "gcn.gcn_env.reset", "gcn.gcn_env._set_observation_space", "collections.defaultdict", "os.dirname", "os.dirname", "torch_geometric.NormalizeFeatures", "torch_geometric.utils.to_dense_adj().numpy", "Net().to", "data.to", "gcn.gcn_env.model.parameters", "numpy.where", "len", "os.realpath", "os.realpath", "numpy.sum", "gcn.gcn_env.data.train_mask.to", "torch_geometric.utils.to_dense_adj", "gcn.Net"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.init_k_hop", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env._set_action_space", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.reset", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env._set_observation_space"], ["    ", "def", "__init__", "(", "self", ",", "dataset", "=", "'Cora'", ",", "lr", "=", "0.01", ",", "weight_decay", "=", "5e-4", ",", "max_layer", "=", "10", ",", "batch_size", "=", "128", ",", "policy", "=", "\"\"", ")", ":", "\n", "        ", "device", "=", "'cpu'", "\n", "dataset", "=", "dataset", "\n", "path", "=", "osp", ".", "join", "(", "osp", ".", "dirname", "(", "osp", ".", "realpath", "(", "__file__", ")", ")", ",", "'..'", ",", "'data'", ",", "dataset", ")", "\n", "dataset", "=", "Planetoid", "(", "path", ",", "dataset", ",", "T", ".", "NormalizeFeatures", "(", ")", ")", "\n", "data", "=", "dataset", "[", "0", "]", "\n", "\n", "adj", "=", "to_dense_adj", "(", "data", ".", "edge_index", ")", ".", "numpy", "(", ")", "[", "0", "]", "\n", "norm", "=", "np", ".", "array", "(", "[", "np", ".", "sum", "(", "row", ")", "for", "row", "in", "adj", "]", ")", "\n", "self", ".", "adj", "=", "(", "adj", "/", "norm", ")", ".", "T", "\n", "self", ".", "init_k_hop", "(", "max_layer", ")", "\n", "\n", "self", ".", "model", ",", "self", ".", "data", "=", "Net", "(", "max_layer", ",", "dataset", ")", ".", "to", "(", "device", ")", ",", "data", ".", "to", "(", "device", ")", "\n", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "lr", ",", "weight_decay", "=", "weight_decay", ")", "\n", "train_mask", "=", "self", ".", "data", ".", "train_mask", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "self", ".", "train_indexes", "=", "np", ".", "where", "(", "train_mask", "==", "True", ")", "[", "0", "]", "\n", "self", ".", "batch_size", "=", "len", "(", "self", ".", "train_indexes", ")", "-", "1", "\n", "self", ".", "i", "=", "0", "\n", "self", ".", "val_acc", "=", "0.0", "\n", "self", ".", "_set_action_space", "(", "max_layer", ")", "\n", "obs", "=", "self", ".", "reset", "(", ")", "\n", "self", ".", "_set_observation_space", "(", "obs", ")", "\n", "self", ".", "policy", "=", "policy", "\n", "self", ".", "max_layer", "=", "max_layer", "\n", "\n", "# For Experiment #", "\n", "self", ".", "random", "=", "False", "\n", "self", ".", "gcn", "=", "False", "# GCN Baseline", "\n", "self", ".", "enable_skh", "=", "True", "# only when GCN is false will be useful", "\n", "self", ".", "enable_dlayer", "=", "True", "\n", "self", ".", "baseline_experience", "=", "50", "\n", "\n", "# buffers for updating", "\n", "#self.buffers = {i: [] for i in range(max_layer)}", "\n", "self", ".", "buffers", "=", "defaultdict", "(", "list", ")", "\n", "self", ".", "past_performance", "=", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.seed": [[75, 79], ["torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "random.seed", "numpy.random.seed"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.seed", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.seed"], ["", "def", "seed", "(", "self", ",", "random_seed", ")", ":", "\n", "        ", "torch", ".", "manual_seed", "(", "random_seed", ")", "\n", "random", ".", "seed", "(", "random_seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "random_seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.init_k_hop": [[80, 87], ["scipy.sparse.csr_matrix", "range", "gcn.gcn_env.adjs.append"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Normalizer.append"], ["", "def", "init_k_hop", "(", "self", ",", "max_hop", ")", ":", "\n", "        ", "sp_adj", "=", "csr_matrix", "(", "self", ".", "adj", ")", "\n", "dd", "=", "sp_adj", "\n", "self", ".", "adjs", "=", "[", "dd", "]", "\n", "for", "i", "in", "range", "(", "max_hop", ")", ":", "\n", "            ", "dd", "*=", "sp_adj", "\n", "self", ".", "adjs", ".", "append", "(", "dd", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.reset": [[88, 93], ["gcn.gcn_env.data.x[].to().numpy", "gcn.gcn_env.optimizer.zero_grad", "gcn.gcn_env.data.x[].to"], "methods", ["None"], ["", "", "def", "reset", "(", "self", ")", ":", "\n", "        ", "index", "=", "self", ".", "train_indexes", "[", "self", ".", "i", "]", "\n", "state", "=", "self", ".", "data", ".", "x", "[", "index", "]", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env._set_action_space": [[94, 97], ["gym.spaces.Discrete"], "methods", ["None"], ["", "def", "_set_action_space", "(", "self", ",", "_max", ")", ":", "\n", "        ", "self", ".", "action_num", "=", "_max", "\n", "self", ".", "action_space", "=", "Discrete", "(", "_max", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env._set_observation_space": [[98, 102], ["numpy.full", "numpy.full", "gym.spaces.Box", "float", "float"], "methods", ["None"], ["", "def", "_set_observation_space", "(", "self", ",", "observation", ")", ":", "\n", "        ", "low", "=", "np", ".", "full", "(", "observation", ".", "shape", ",", "-", "float", "(", "'inf'", ")", ")", "\n", "high", "=", "np", ".", "full", "(", "observation", ".", "shape", ",", "float", "(", "'inf'", ")", ")", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "low", "=", "low", ",", "high", "=", "high", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.step": [[103, 131], ["gcn.gcn_env.model.train", "gcn.gcn_env.optimizer.zero_grad", "pred.unsqueeze.unsqueeze.unsqueeze", "y.unsqueeze.unsqueeze.unsqueeze", "torch.nll_loss().backward", "torch.nll_loss().backward", "gcn.gcn_env.optimizer.step", "gcn.gcn_env.eval_batch", "gcn.gcn_env.data.x[].numpy", "random.randint", "gcn.gcn_env.model", "len", "torch.nll_loss", "torch.nll_loss"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.train", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.step", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.eval_batch"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "self", ".", "model", ".", "train", "(", ")", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "if", "self", ".", "random", "==", "True", ":", "\n", "            ", "action", "=", "random", ".", "randint", "(", "1", ",", "5", ")", "\n", "# train one step", "\n", "", "index", "=", "self", ".", "train_indexes", "[", "self", ".", "i", "]", "\n", "pred", "=", "self", ".", "model", "(", "action", ",", "self", ".", "data", ")", "[", "index", "]", "\n", "pred", "=", "pred", ".", "unsqueeze", "(", "0", ")", "\n", "y", "=", "self", ".", "data", ".", "y", "[", "index", "]", "\n", "y", "=", "y", ".", "unsqueeze", "(", "0", ")", "\n", "F", ".", "nll_loss", "(", "pred", ",", "y", ")", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "# get reward from validation set", "\n", "val_acc", "=", "self", ".", "eval_batch", "(", ")", "\n", "\n", "# get next state", "\n", "self", ".", "i", "+=", "1", "\n", "self", ".", "i", "=", "self", ".", "i", "%", "len", "(", "self", ".", "train_indexes", ")", "\n", "next_index", "=", "self", ".", "train_indexes", "[", "self", ".", "i", "]", "\n", "#next_state = self.data.x[next_index].to('cpu').numpy()", "\n", "next_state", "=", "self", ".", "data", ".", "x", "[", "next_index", "]", ".", "numpy", "(", ")", "\n", "if", "self", ".", "i", "==", "0", ":", "\n", "            ", "done", "=", "True", "\n", "", "else", ":", "\n", "            ", "done", "=", "False", "\n", "", "return", "next_state", ",", "val_acc", ",", "done", ",", "\"debug\"", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.reset2": [[132, 139], ["gcn.gcn_env.data.x[].to().numpy", "gcn.gcn_env.optimizer.zero_grad", "len", "gcn.gcn_env.data.x[].to"], "methods", ["None"], ["", "def", "reset2", "(", "self", ")", ":", "\n", "        ", "start", "=", "self", ".", "i", "\n", "end", "=", "(", "self", ".", "i", "+", "self", ".", "batch_size", ")", "%", "len", "(", "self", ".", "train_indexes", ")", "\n", "index", "=", "self", ".", "train_indexes", "[", "start", ":", "end", "]", "\n", "state", "=", "self", ".", "data", ".", "x", "[", "index", "]", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.step2": [[140, 174], ["gcn.gcn_env.model.train", "gcn.gcn_env.optimizer.zero_grad", "zip", "gcn.gcn_env.data.x[].to().numpy", "gcn.gcn_env.eval_batch", "gcn.gcn_env.test_batch", "numpy.mean", "gcn.gcn_env.past_performance.extend", "numpy.mean", "numpy.mean", "len", "gcn.gcn_env.buffers[].append", "min", "gcn.gcn_env.stochastic_k_hop", "numpy.array", "numpy.array", "len", "gcn.gcn_env.train", "len", "gcn.gcn_env.data.x[].to"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.train", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.eval_batch", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.test_batch", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Normalizer.append", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.stochastic_k_hop", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.train"], ["", "def", "step2", "(", "self", ",", "actions", ")", ":", "\n", "        ", "self", ".", "model", ".", "train", "(", ")", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "start", "=", "self", ".", "i", "\n", "end", "=", "(", "self", ".", "i", "+", "self", ".", "batch_size", ")", "%", "len", "(", "self", ".", "train_indexes", ")", "\n", "index", "=", "self", ".", "train_indexes", "[", "start", ":", "end", "]", "\n", "done", "=", "False", "\n", "for", "act", ",", "idx", "in", "zip", "(", "actions", ",", "index", ")", ":", "\n", "            ", "if", "self", ".", "gcn", "==", "True", "or", "self", ".", "enable_dlayer", "==", "False", ":", "\n", "                ", "act", "=", "self", ".", "max_layer", "\n", "", "self", ".", "buffers", "[", "act", "]", ".", "append", "(", "idx", ")", "\n", "if", "len", "(", "self", ".", "buffers", "[", "act", "]", ")", ">=", "self", ".", "batch_size", ":", "\n", "                ", "self", ".", "train", "(", "act", ",", "self", ".", "buffers", "[", "act", "]", ")", "\n", "self", ".", "buffers", "[", "act", "]", "=", "[", "]", "\n", "done", "=", "True", "\n", "", "", "if", "self", ".", "gcn", "==", "True", "or", "self", ".", "enable_skh", "==", "False", ":", "\n", "### Random ###", "\n", "            ", "self", ".", "i", "+=", "min", "(", "(", "self", ".", "i", "+", "self", ".", "batch_size", ")", "%", "self", ".", "batch_size", ",", "self", ".", "batch_size", ")", "\n", "start", "=", "self", ".", "i", "\n", "end", "=", "(", "self", ".", "i", "+", "self", ".", "batch_size", ")", "%", "len", "(", "self", ".", "train_indexes", ")", "\n", "index", "=", "self", ".", "train_indexes", "[", "start", ":", "end", "]", "\n", "", "else", ":", "\n", "            ", "index", "=", "self", ".", "stochastic_k_hop", "(", "actions", ",", "index", ")", "\n", "", "next_state", "=", "self", ".", "data", ".", "x", "[", "index", "]", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "#next_state = self.data.x[index].numpy()", "\n", "val_acc_dict", "=", "self", ".", "eval_batch", "(", ")", "\n", "val_acc", "=", "[", "val_acc_dict", "[", "a", "]", "for", "a", "in", "actions", "]", "\n", "test_acc", "=", "self", ".", "test_batch", "(", ")", "\n", "baseline", "=", "np", ".", "mean", "(", "np", ".", "array", "(", "self", ".", "past_performance", "[", "-", "self", ".", "baseline_experience", ":", "]", ")", ")", "\n", "self", ".", "past_performance", ".", "extend", "(", "val_acc", ")", "\n", "reward", "=", "[", "100", "*", "(", "each", "-", "baseline", ")", "for", "each", "in", "val_acc", "]", "# FIXME: Reward Engineering", "\n", "r", "=", "np", ".", "mean", "(", "np", ".", "array", "(", "reward", ")", ")", "\n", "val_acc", "=", "np", ".", "mean", "(", "val_acc", ")", "\n", "return", "next_state", ",", "reward", ",", "[", "done", "]", "*", "self", ".", "batch_size", ",", "(", "val_acc", ",", "r", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.stochastic_k_hop": [[175, 183], ["zip", "gcn.gcn_env.adjs[].getrow().toarray().flatten", "numpy.array", "numpy.random.choice", "next_batch.append", "gcn.gcn_env.adjs[].getrow().toarray", "range", "gcn.gcn_env.adjs[].getrow", "len"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Normalizer.append"], ["", "def", "stochastic_k_hop", "(", "self", ",", "actions", ",", "index", ")", ":", "\n", "        ", "next_batch", "=", "[", "]", "\n", "for", "idx", ",", "act", "in", "zip", "(", "index", ",", "actions", ")", ":", "\n", "            ", "prob", "=", "self", ".", "adjs", "[", "act", "]", ".", "getrow", "(", "idx", ")", ".", "toarray", "(", ")", ".", "flatten", "(", ")", "\n", "cand", "=", "np", ".", "array", "(", "[", "i", "for", "i", "in", "range", "(", "len", "(", "prob", ")", ")", "]", ")", "\n", "next_cand", "=", "np", ".", "random", ".", "choice", "(", "cand", ",", "p", "=", "prob", ")", "\n", "next_batch", ".", "append", "(", "next_cand", ")", "\n", "", "return", "next_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.train": [[184, 190], ["gcn.gcn_env.model.train", "torch.nll_loss().backward", "torch.nll_loss().backward", "gcn.gcn_env.optimizer.step", "gcn.gcn_env.model", "torch.nll_loss", "torch.nll_loss"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.train", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.step"], ["", "def", "train", "(", "self", ",", "action", ",", "indexes", ")", ":", "\n", "        ", "self", ".", "model", ".", "train", "(", ")", "\n", "pred", "=", "self", ".", "model", "(", "action", ",", "self", ".", "data", ")", "[", "indexes", "]", "\n", "y", "=", "self", ".", "data", ".", "y", "[", "indexes", "]", "\n", "F", ".", "nll_loss", "(", "pred", ",", "y", ")", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.eval_batch": [[191, 217], ["gcn.gcn_env.model.eval", "gcn.gcn_env.data.x[].to().numpy", "zip", "batch_dict.keys", "numpy.where", "numpy.random.randint", "batch_dict[].append", "gcn.gcn_env.model", "gcn.gcn_env.data.x[].to", "len", "numpy.full", "gcn.gcn_env.policy.eval_step", "batch_dict.keys", "range", "logits[].max", "pred.eq().sum().item", "len", "gcn.gcn_env.data.val_mask.to().numpy", "len", "pred.eq().sum", "gcn.gcn_env.data.val_mask.to", "pred.eq"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Normalizer.append", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.DQNAgent.eval_step"], ["", "def", "eval_batch", "(", "self", ")", ":", "\n", "        ", "self", ".", "model", ".", "eval", "(", ")", "\n", "batch_dict", "=", "{", "}", "\n", "val_index", "=", "np", ".", "where", "(", "self", ".", "data", ".", "val_mask", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "==", "True", ")", "[", "0", "]", "\n", "val_states", "=", "self", ".", "data", ".", "x", "[", "val_index", "]", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "if", "self", ".", "random", "==", "True", ":", "\n", "            ", "val_acts", "=", "np", ".", "random", ".", "randint", "(", "1", ",", "5", ",", "len", "(", "val_index", ")", ")", "\n", "", "elif", "self", ".", "gcn", "==", "True", "or", "self", ".", "enable_dlayer", "==", "False", ":", "\n", "            ", "val_acts", "=", "np", ".", "full", "(", "len", "(", "val_index", ")", ",", "3", ")", "\n", "", "else", ":", "\n", "            ", "val_acts", "=", "self", ".", "policy", ".", "eval_step", "(", "val_states", ")", "\n", "", "s_a", "=", "zip", "(", "val_index", ",", "val_acts", ")", "\n", "for", "i", ",", "a", "in", "s_a", ":", "\n", "            ", "if", "a", "not", "in", "batch_dict", ".", "keys", "(", ")", ":", "\n", "                ", "batch_dict", "[", "a", "]", "=", "[", "]", "\n", "", "batch_dict", "[", "a", "]", ".", "append", "(", "i", ")", "\n", "#acc = 0.0", "\n", "", "acc", "=", "{", "a", ":", "0.0", "for", "a", "in", "range", "(", "self", ".", "max_layer", ")", "}", "\n", "for", "a", "in", "batch_dict", ".", "keys", "(", ")", ":", "\n", "            ", "idx", "=", "batch_dict", "[", "a", "]", "\n", "logits", "=", "self", ".", "model", "(", "a", ",", "self", ".", "data", ")", "\n", "pred", "=", "logits", "[", "idx", "]", ".", "max", "(", "1", ")", "[", "1", "]", "\n", "#acc += pred.eq(self.data.y[idx]).sum().item() / len(idx)", "\n", "acc", "[", "a", "]", "=", "pred", ".", "eq", "(", "self", ".", "data", ".", "y", "[", "idx", "]", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "len", "(", "idx", ")", "\n", "#acc = acc / len(batch_dict.keys())", "\n", "", "return", "acc", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.test_batch": [[218, 242], ["gcn.gcn_env.model.eval", "gcn.gcn_env.data.x[].to().numpy", "zip", "batch_dict.keys", "numpy.where", "numpy.random.randint", "batch_dict[].append", "gcn.gcn_env.model", "len", "gcn.gcn_env.data.x[].to", "len", "numpy.full", "gcn.gcn_env.policy.eval_step", "batch_dict.keys", "logits[].max", "pred.eq().sum().item", "len", "batch_dict.keys", "gcn.gcn_env.data.test_mask.to().numpy", "len", "pred.eq().sum", "gcn.gcn_env.data.test_mask.to", "pred.eq"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.Normalizer.append", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.DQNAgent.eval_step"], ["", "def", "test_batch", "(", "self", ")", ":", "\n", "        ", "self", ".", "model", ".", "eval", "(", ")", "\n", "batch_dict", "=", "{", "}", "\n", "test_index", "=", "np", ".", "where", "(", "self", ".", "data", ".", "test_mask", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "==", "True", ")", "[", "0", "]", "\n", "val_states", "=", "self", ".", "data", ".", "x", "[", "test_index", "]", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "if", "self", ".", "random", "==", "True", ":", "\n", "            ", "val_acts", "=", "np", ".", "random", ".", "randint", "(", "1", ",", "5", ",", "len", "(", "test_index", ")", ")", "\n", "", "elif", "self", ".", "gcn", "==", "True", "or", "self", ".", "enable_dlayer", "==", "False", ":", "\n", "            ", "val_acts", "=", "np", ".", "full", "(", "len", "(", "test_index", ")", ",", "3", ")", "\n", "", "else", ":", "\n", "            ", "val_acts", "=", "self", ".", "policy", ".", "eval_step", "(", "val_states", ")", "\n", "", "s_a", "=", "zip", "(", "test_index", ",", "val_acts", ")", "\n", "for", "i", ",", "a", "in", "s_a", ":", "\n", "            ", "if", "a", "not", "in", "batch_dict", ".", "keys", "(", ")", ":", "\n", "                ", "batch_dict", "[", "a", "]", "=", "[", "]", "\n", "", "batch_dict", "[", "a", "]", ".", "append", "(", "i", ")", "\n", "", "acc", "=", "0.0", "\n", "for", "a", "in", "batch_dict", ".", "keys", "(", ")", ":", "\n", "            ", "idx", "=", "batch_dict", "[", "a", "]", "\n", "logits", "=", "self", ".", "model", "(", "a", ",", "self", ".", "data", ")", "\n", "pred", "=", "logits", "[", "idx", "]", ".", "max", "(", "1", ")", "[", "1", "]", "\n", "acc", "+=", "pred", ".", "eq", "(", "self", ".", "data", ".", "y", "[", "idx", "]", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "len", "(", "idx", ")", "\n", "", "acc", "=", "acc", "/", "len", "(", "batch_dict", ".", "keys", "(", ")", ")", "\n", "return", "acc", "\n", "\n"]], "home.repos.pwc.inspect_result.datamllab_Policy-GNN.env.gcn.gcn_env.check": [[243, 258], ["gcn.gcn_env.model.eval", "gcn.gcn_env.data.x[].to().numpy", "gcn.gcn_env.policy.eval_step", "gcn.gcn_env.data.x[].to().numpy", "gcn.gcn_env.policy.eval_step", "gcn.gcn_env.data.x[].to().numpy", "gcn.gcn_env.policy.eval_step", "numpy.where", "numpy.where", "numpy.where", "gcn.gcn_env.data.x[].to", "gcn.gcn_env.data.x[].to", "gcn.gcn_env.data.x[].to", "gcn.gcn_env.data.train_mask.to().numpy", "gcn.gcn_env.data.val_mask.to().numpy", "gcn.gcn_env.data.test_mask.to().numpy", "gcn.gcn_env.data.train_mask.to", "gcn.gcn_env.data.val_mask.to", "gcn.gcn_env.data.test_mask.to"], "methods", ["home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.DQNAgent.eval_step", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.DQNAgent.eval_step", "home.repos.pwc.inspect_result.datamllab_Policy-GNN.None.dqn_agent_pytorch.DQNAgent.eval_step"], ["", "def", "check", "(", "self", ")", ":", "\n", "        ", "self", ".", "model", ".", "eval", "(", ")", "\n", "train_index", "=", "np", ".", "where", "(", "self", ".", "data", ".", "train_mask", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "==", "True", ")", "[", "0", "]", "\n", "tr_states", "=", "self", ".", "data", ".", "x", "[", "train_index", "]", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "tr_acts", "=", "self", ".", "policy", ".", "eval_step", "(", "tr_states", ")", "\n", "\n", "val_index", "=", "np", ".", "where", "(", "self", ".", "data", ".", "val_mask", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "==", "True", ")", "[", "0", "]", "\n", "val_states", "=", "self", ".", "data", ".", "x", "[", "val_index", "]", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "val_acts", "=", "self", ".", "policy", ".", "eval_step", "(", "val_states", ")", "\n", "\n", "test_index", "=", "np", ".", "where", "(", "self", ".", "data", ".", "test_mask", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "==", "True", ")", "[", "0", "]", "\n", "test_states", "=", "self", ".", "data", ".", "x", "[", "test_index", "]", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "test_acts", "=", "self", ".", "policy", ".", "eval_step", "(", "test_states", ")", "\n", "\n", "return", "(", "train_index", ",", "tr_states", ",", "tr_acts", ")", ",", "(", "val_index", ",", "val_states", ",", "val_acts", ")", ",", "(", "test_index", ",", "test_states", ",", "test_acts", ")", "\n", "\n"]]}