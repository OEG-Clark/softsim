{"home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.code.utils.sample_top": [[11, 18], ["numpy.random.choice", "numpy.argsort", "numpy.sum"], "function", ["None"], ["def", "sample_top", "(", "a", "=", "[", "]", ",", "top_k", "=", "10", ")", ":", "\n", "    ", "idx", "=", "np", ".", "argsort", "(", "a", ")", "[", ":", ":", "-", "1", "]", "\n", "idx", "=", "idx", "[", ":", "top_k", "]", "\n", "probs", "=", "a", "[", "idx", "]", "\n", "probs", "=", "probs", "/", "np", ".", "sum", "(", "probs", ")", "\n", "choice", "=", "np", ".", "random", ".", "choice", "(", "idx", ",", "p", "=", "probs", ")", "\n", "return", "choice", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.code.utils.html_header": [[20, 123], ["None"], "function", ["None"], ["", "def", "html_header", "(", ")", ":", "\n", "    ", "header", "=", "\"\"\"\n    <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\"> \n    <html>\n\n    <head>\n      <meta name=viewport content=\u201cwidth=650\u201d>\n      <meta name=\"generator\" content=\"HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org\">\n      <style type=\"text/css\">\n        /* Color scheme stolen from Sergey Karayev */\n\n        a {\n          color: #1772d0;\n          text-decoration: none;\n        }\n\n        a:focus,\n        a:hover {\n          color: #f09228;\n          text-decoration: none;\n        }\n\n        body,\n        td,\n        th,\n        tr,\n        p,\n        a {\n          font-family: \"Lato\", Verdana, Helvetica, sans-serif;\n          font-size: 14px\n        }\n\n        strong {\n          font-family: \"Lato\", Verdana, Helvetica, sans-serif;\n          font-size: 14px;\n        }\n\n        heading {\n          font-family: \"Lato\", Verdana, Helvetica, sans-serif;\n          font-size: 22px;\n        }\n\n        papertitle {\n          font-family: \"Lato\", Verdana, Helvetica, sans-serif;\n          font-size: 14px;\n          font-weight: 700\n        }\n\n        name {\n          font-family: \"Lato\", Verdana, Helvetica, sans-serif; \n          }\n\n        .one {\n          width: 160px;\n          height: 160px;\n          position: relative;\n        }\n\n        .two {\n          width: 160px;\n          height: 160px;\n          position: absolute;\n          transition: opacity .2s ease-in-out;\n          -moz-transition: opacity .2s ease-in-out;\n          -webkit-transition: opacity .2s ease-in-out;\n        }\n\n        .fade {\n          transition: opacity .2s ease-in-out;\n          -moz-transition: opacity .2s ease-in-out;\n          -webkit-transition: opacity .2s ease-in-out;\n        }\n\n        .my {\n          border: 1px solid;\n          width: 650px; \n          align: \"center\"; \n          cellspacing: 0;\n          cellpadding: 0; \n        }  \n\n\n\n        span.highlight {\n          background-color: #ffffd0;\n        }\n      </style>\n      <link rel=\"icon\" type=\"image/png\" href=\"seal_icon.png\">\n      <title>CNN-Units-in-NLP</title>\n      <meta http-equiv=\"Content-Type\" content=\"text/html; charset=us-ascii\">\n      <link href=\"http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic\" rel='stylesheet' type='text/css'>\n    </head>\n    <body>\n\n    <table width=\"650\" border=\"0\" align=\"center\" cellspacing=\"0\" cellpadding=\"0\">\n        <tr>\n            <td align=\"right\">[#] denotes morpheme concept</td>\n        </tr>\n        <tr>\n            <td align=\"right\"> M=3 concepts are aligned per unit\n        </tr>\n    \"\"\"", "\n", "return", "header", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.code.utils.html_per_unit": [[125, 139], ["range", "html.decode", "concept.replace.replace"], "function", ["None"], ["", "def", "html_per_unit", "(", "task", ",", "layer", ",", "unit", ",", "alignment", ",", "num_align", ")", ":", "\n", "    ", "html", "=", "\"\"\"\n    <tr>\n        <td align=\"left\">[%s / layer %02d / Unit %04d]<br>\n    \"\"\"", "%", "(", "task", ",", "layer", ",", "unit", ")", "\n", "\n", "for", "i", "in", "range", "(", "num_align", ")", ":", "\n", "        ", "concept", ",", "doa", "=", "alignment", "[", "unit", "]", "[", "i", "]", "\n", "concept", "=", "concept", ".", "replace", "(", "'MORPH_'", ",", "'[#]'", ")", "\n", "\n", "html", "+=", "'<span style=\"background-color: %s\" >%s</span> (%.2lf) / '", "%", "(", "colors", "[", "i", "]", ",", "concept", ",", "doa", ")", "\n", "", "html", "+=", "\"</tr>\"", "\n", "\n", "return", "html", ".", "decode", "(", "\"utf-8\"", ",", "errors", "=", "\"ignore\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.code.utils.html_per_tas": [[140, 189], ["html.decode", "enumerate", "len", "concept.replace.replace", "concept.replace.lower", "sentence.lower", "sentence.lower().index", "concept.replace.lower", "sentence.lower", "len", "len"], "function", ["None"], ["", "def", "html_per_tas", "(", "unit", ",", "tas", ",", "alignment", ",", "num_visualized_tas", ",", "max_sent_len", ")", ":", "\n", "    ", "html", "=", "\"\"\"\n    <tr>\n        <td>\n            <table class=my>\n    \"\"\"", "\n", "\n", "for", "activ", ",", "sentence", "in", "tas", "[", "unit", "]", "[", ":", "num_visualized_tas", "]", ":", "\n", "        ", "if", "len", "(", "sentence", ")", ">", "max_sent_len", ":", "\n", "            ", "sentence", "=", "sentence", "[", ":", "max_sent_len", "]", "+", "' (...)'", "\n", "\n", "\n", "\n", "", "for", "idx", ",", "(", "concept", ",", "_", ")", "in", "enumerate", "(", "alignment", "[", "unit", "]", ")", ":", "\n", "            ", "concept", "=", "concept", ".", "replace", "(", "'MORPH_'", ",", "''", ")", "\n", "\n", "# set max length for visualization", "\n", "concept", "=", "concept", "[", ":", "25", "]", "\n", "\n", "try", ":", "\n", "                ", "if", "concept", ".", "lower", "(", ")", "in", "sentence", ".", "lower", "(", ")", ":", "\n", "                    ", "s_idx", "=", "sentence", ".", "lower", "(", ")", ".", "index", "(", "concept", ".", "lower", "(", ")", ")", "\n", "sentence", "=", "sentence", "[", ":", "s_idx", "]", "+", "'<span style=\"background-color: %s\" >'", "%", "colors", "[", "idx", "]", "+", "sentence", "[", "s_idx", ":", "s_idx", "+", "len", "(", "concept", ")", "]", "+", "'</span>'", "+", "sentence", "[", "s_idx", "+", "len", "(", "concept", ")", ":", "]", "\n", "\n", "", "", "except", ":", "\n", "#from IPython import embed; embed()", "\n", "                ", "continue", "\n", "\n", "", "", "html", "+=", "\"\"\"\n        <tr>\n            <td align=\"left\"><li>{}</li></td>\n        </tr>\n        \"\"\"", ".", "format", "(", "sentence", ")", "\n", "\n", "", "html", "+=", "\"\"\"\n            </table>\n        </td>\n    </tr>\n\n    <tr>\n        <td><br></td>\n    </tr>\n    \"\"\"", "\n", "return", "html", ".", "decode", "(", "'utf-8'", ",", "errors", "=", "'ignore'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.data_loader.Data_Loader.__init__": [[10, 125], ["open", "f.read().decode().split", "open", "f.read().decode().split", "data_loader.Data_Loader.build_vocab", "data_loader.Data_Loader.build_vocab", "print", "print", "print", "os.path.join", "print", "len", "len", "len", "print", "numpy.load", "print", "enumerate", "numpy.array", "numpy.array", "numpy.array", "f.read().decode", "f.read().decode", "os.listdir", "os.path.isfile", "list", "type", "print", "cPickle.load", "enumerate", "open", "os.path.join", "open().read", "open", "list", "f.readline", "json.loads", "text.append", "f.read", "f.read", "open().read", "review[].replace", "int", "rating.append", "data_loader.Data_Loader.string_to_indices", "open", "int", "rating.append", "review[].replace", "open", "len"], "methods", ["home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.data_loader.Data_Loader.build_vocab", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.data_loader.Data_Loader.build_vocab", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.data_loader.Data_Loader.string_to_indices"], ["    ", "def", "__init__", "(", "self", ",", "options", ",", "split", "=", "'train'", ",", "vocab", "=", "None", ",", "cache", "=", "None", ")", ":", "\n", "        ", "if", "options", "[", "'model_type'", "]", "==", "'translation'", ":", "\n", "            ", "source_file", "=", "options", "[", "'source_file'", "]", "+", "'.'", "+", "split", "\n", "target_file", "=", "options", "[", "'target_file'", "]", "+", "'.'", "+", "split", "\n", "\n", "self", ".", "max_sentences", "=", "None", "\n", "if", "'max_sentences'", "in", "options", ":", "\n", "                ", "self", ".", "max_sentences", "=", "options", "[", "'max_sentences'", "]", "\n", "\n", "", "with", "open", "(", "source_file", ")", "as", "f", ":", "\n", "                ", "self", ".", "source_lines", "=", "f", ".", "read", "(", ")", ".", "decode", "(", "\"utf-8\"", ",", "errors", "=", "'ignore'", ")", ".", "split", "(", "'\\n'", ")", "\n", "# for temporally covering error in inference_translation.py", "\n", "", "with", "open", "(", "target_file", ")", "as", "f", ":", "\n", "                ", "self", ".", "target_lines", "=", "f", ".", "read", "(", ")", ".", "decode", "(", "\"utf-8\"", ",", "errors", "=", "'ignore'", ")", ".", "split", "(", "'\\n'", ")", "\n", "\n", "\n", "", "if", "self", ".", "max_sentences", ":", "\n", "                ", "self", ".", "source_lines", "=", "self", ".", "source_lines", "[", "0", ":", "self", ".", "max_sentences", "]", "\n", "self", ".", "target_lines", "=", "self", ".", "target_lines", "[", "0", ":", "self", ".", "max_sentences", "]", "\n", "\n", "#print(\"Source Sentences\", len(self.source_lines))", "\n", "#print(\"Target Sentences\", len(self.target_lines))", "\n", "\n", "", "self", ".", "bucket_quant", "=", "options", "[", "'bucket_quant'", "]", "\n", "if", "split", "==", "\"train\"", ":", "\n", "                ", "self", ".", "source_vocab", "=", "self", ".", "build_vocab", "(", "self", ".", "source_lines", ")", "\n", "self", ".", "target_vocab", "=", "self", ".", "build_vocab", "(", "self", ".", "target_lines", ")", "\n", "", "else", ":", "\n", "                ", "'''\n                if vocab is None:\n                    raise Exception(\"split={}: need vocab from training data\"\n                                    % split)\n                with open(join(vocab, \"source_vocab.pkl\"), \"rb\") as f:\n                    self.source_vocab = pickle.load(f)\n                with open(join(vocab, \"target_vocab.pkl\"), \"rb\") as f:\n                    self.target_vocab = pickle.load(f)\n                '''", "\n", "pass", "\n", "\n", "\n", "#print(\"SOURCE VOCAB SIZE\", len(self.source_vocab))", "\n", "#print(\"TARGET VOCAB SIZE\", len(self.target_vocab))", "\n", "\n", "", "", "elif", "options", "[", "'model_type'", "]", "==", "'generator'", ":", "\n", "            ", "dir_name", "=", "options", "[", "'dir_name'", "]", "\n", "files", "=", "[", "join", "(", "dir_name", ",", "f", ")", "for", "f", "in", "listdir", "(", "dir_name", ")", "if", "isfile", "(", "join", "(", "dir_name", ",", "f", ")", ")", "]", "\n", "\n", "text", "=", "[", "]", "\n", "\n", "# construct same vocab set both for train/valid data", "\n", "if", "vocab", "==", "None", ":", "\n", "                ", "print", "(", "'There is no vocab file. Construct it from scratch.'", ")", "\n", "for", "f", "in", "files", ":", "\n", "                    ", "text", "+=", "list", "(", "open", "(", "f", ")", ".", "read", "(", ")", ")", "\n", "\n", "", "vocab", "=", "{", "ch", ":", "True", "for", "ch", "in", "text", "}", "\n", "\n", "# If there is vocab cache, load it only!", "\n", "", "elif", "type", "(", "vocab", ")", "==", "str", ":", "\n", "                ", "print", "(", "'Loading presaved vocab file | {}'", ".", "format", "(", "vocab", ")", ")", "\n", "vocab", "=", "pickle", ".", "load", "(", "open", "(", "vocab", ")", ")", "\n", "\n", "", "else", ":", "\n", "                ", "for", "f", "in", "files", ":", "\n", "                    ", "text", "+=", "list", "(", "open", "(", "f", ")", ".", "read", "(", ")", ")", "\n", "\n", "\n", "", "", "self", ".", "vocab", "=", "vocab", "\n", "\n", "print", "(", "\"Bool vocab\"", ",", "len", "(", "vocab", ")", ")", "\n", "self", ".", "vocab_list", "=", "[", "ch", "for", "ch", "in", "vocab", "]", "\n", "print", "(", "\"vocab list\"", ",", "len", "(", "self", ".", "vocab_list", ")", ")", "\n", "self", ".", "vocab_indexed", "=", "{", "ch", ":", "i", "for", "i", ",", "ch", "in", "enumerate", "(", "self", ".", "vocab_list", ")", "}", "\n", "print", "(", "\"vocab_indexed\"", ",", "len", "(", "self", ".", "vocab_indexed", ")", ")", "\n", "\n", "if", "cache", ":", "\n", "              ", "print", "(", "'text cache file [{}] is started to load...'", ".", "format", "(", "cache", ")", ")", "\n", "self", ".", "text", "=", "np", ".", "load", "(", "cache", ")", "\n", "print", "(", "'text cache file [{}] is loaded.'", ".", "format", "(", "cache", ")", ")", "\n", "\n", "", "else", ":", "\n", "              ", "for", "index", ",", "item", "in", "enumerate", "(", "text", ")", ":", "\n", "                  ", "text", "[", "index", "]", "=", "self", ".", "vocab_indexed", "[", "item", "]", "\n", "", "self", ".", "text", "=", "np", ".", "array", "(", "text", ",", "dtype", "=", "'int32'", ")", "\n", "\n", "", "", "elif", "options", "[", "'model_type'", "]", "==", "'classifier'", ":", "\n", "            ", "text", ",", "rating", "=", "[", "]", ",", "[", "]", "\n", "\n", "fname", "=", "options", "[", "'review_file'", "]", "+", "'.{}'", ".", "format", "(", "split", ")", "\n", "\n", "if", "not", "vocab", ":", "vocab_scratch", "=", "{", "'<p>'", ":", "0", "}", "\n", "\n", "with", "open", "(", "fname", ")", "as", "f", ":", "\n", "                ", "while", "True", ":", "\n", "                    ", "line", "=", "f", ".", "readline", "(", ")", "\n", "if", "not", "line", ":", "break", "\n", "review", "=", "json", ".", "loads", "(", "line", ")", "\n", "\n", "if", "not", "vocab", ":", "\n", "                        ", "for", "ch", "in", "review", "[", "'text'", "]", ".", "replace", "(", "'\\n'", ",", "''", ")", ":", "\n", "                            ", "if", "ch", "in", "vocab_scratch", ":", "continue", "\n", "else", ":", "vocab_scratch", "[", "ch", "]", "=", "len", "(", "vocab_scratch", ")", "\n", "\n", "# make polarity", "\n", "", "", "if", "int", "(", "review", "[", "'stars'", "]", ")", ">", "3", ":", "\n", "                        ", "rating", ".", "append", "(", "1", ")", "\n", "", "elif", "int", "(", "review", "[", "'stars'", "]", ")", "<", "3", ":", "\n", "                        ", "rating", ".", "append", "(", "0", ")", "\n", "", "else", ":", "continue", "\n", "\n", "text", ".", "append", "(", "self", ".", "string_to_indices", "(", "review", "[", "'text'", "]", ".", "replace", "(", "'\\n'", ",", "''", ")", ",", "vocab_scratch", ",", "pad", "=", "options", "[", "'seq_len'", "]", ")", ")", "\n", "\n", "", "", "self", ".", "text", "=", "np", ".", "array", "(", "text", ",", "dtype", "=", "'int32'", ")", "\n", "self", ".", "rating", "=", "np", ".", "array", "(", "rating", ",", "dtype", "=", "'int32'", ")", "\n", "self", ".", "vocab", "=", "vocab_scratch", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.data_loader.Data_Loader.load_generator_data": [[126, 132], ["text.reshape.reshape.reshape", "len", "len"], "methods", ["None"], ["", "", "def", "load_generator_data", "(", "self", ",", "sample_size", ")", ":", "\n", "        ", "text", "=", "self", ".", "text", "\n", "mod_size", "=", "len", "(", "text", ")", "-", "len", "(", "text", ")", "%", "sample_size", "\n", "text", "=", "text", "[", "0", ":", "mod_size", "]", "\n", "text", "=", "text", ".", "reshape", "(", "-", "1", ",", "sample_size", ")", "\n", "return", "text", ",", "self", ".", "vocab_indexed", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.data_loader.Data_Loader.load_translation_data": [[134, 150], ["range", "data_loader.Data_Loader.create_buckets", "len", "source_lines.append", "target_lines.append", "data_loader.Data_Loader.string_to_indices", "data_loader.Data_Loader.string_to_indices"], "methods", ["home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.data_loader.Data_Loader.create_buckets", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.data_loader.Data_Loader.string_to_indices", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.data_loader.Data_Loader.string_to_indices"], ["", "def", "load_translation_data", "(", "self", ")", ":", "\n", "        ", "source_lines", "=", "[", "]", "\n", "target_lines", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "source_lines", ")", ")", ":", "\n", "            ", "source_lines", ".", "append", "(", "self", ".", "string_to_indices", "(", "self", ".", "source_lines", "[", "i", "]", ",", "self", ".", "source_vocab", ")", ")", "\n", "target_lines", ".", "append", "(", "self", ".", "string_to_indices", "(", "self", ".", "target_lines", "[", "i", "]", ",", "self", ".", "target_vocab", ")", ")", "\n", "\n", "", "buckets", "=", "self", ".", "create_buckets", "(", "source_lines", ",", "target_lines", ")", "\n", "\n", "# frequent_keys = [ (-len(buckets[key]), key) for key in buckets ]", "\n", "# frequent_keys.sort()", "\n", "\n", "# print \"Source\", self.inidices_to_string( buckets[ frequent_keys[3][1] ][5][0], self.source_vocab)", "\n", "# print \"Target\", self.inidices_to_string( buckets[ frequent_keys[3][1] ][5][1], self.target_vocab)", "\n", "\n", "return", "buckets", ",", "self", ".", "source_vocab", ",", "self", ".", "target_vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.data_loader.Data_Loader.load_classifier_data": [[151, 153], ["None"], "methods", ["None"], ["", "def", "load_classifier_data", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "text", ",", "self", ".", "rating", ",", "self", ".", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.data_loader.Data_Loader.create_buckets": [[155, 197], ["xrange", "len", "numpy.concatenate", "numpy.concatenate", "len", "len", "max", "numpy.array", "numpy.array", "numpy.concatenate", "numpy.concatenate", "buckets[].append", "xrange", "xrange"], "methods", ["None"], ["", "def", "create_buckets", "(", "self", ",", "source_lines", ",", "target_lines", ")", ":", "\n", "\n", "        ", "bucket_quant", "=", "self", ".", "bucket_quant", "\n", "source_vocab", "=", "self", ".", "source_vocab", "\n", "target_vocab", "=", "self", ".", "target_vocab", "\n", "\n", "buckets", "=", "{", "}", "\n", "for", "i", "in", "xrange", "(", "len", "(", "source_lines", ")", ")", ":", "\n", "\n", "# source = source + <EOL>", "\n", "# target = <BOL> + target + <EOL>", "\n", "            ", "source_lines", "[", "i", "]", "=", "np", ".", "concatenate", "(", "(", "source_lines", "[", "i", "]", ",", "[", "source_vocab", "[", "'eol'", "]", "]", ")", ")", "\n", "target_lines", "[", "i", "]", "=", "np", ".", "concatenate", "(", "(", "[", "target_vocab", "[", "'init'", "]", "]", ",", "target_lines", "[", "i", "]", ",", "[", "target_vocab", "[", "'eol'", "]", "]", ")", ")", "\n", "\n", "sl", "=", "len", "(", "source_lines", "[", "i", "]", ")", "\n", "tl", "=", "len", "(", "target_lines", "[", "i", "]", ")", "\n", "\n", "new_length", "=", "max", "(", "sl", ",", "tl", ")", "\n", "\n", "# fitting new_length to neareast upperbound of bucket_quant", "\n", "# e.g. bucket_quant=50 -> new_length = 50, 100, 150, ...", "\n", "\n", "if", "new_length", "%", "bucket_quant", ">", "0", ":", "\n", "                ", "new_length", "=", "(", "(", "new_length", "/", "bucket_quant", ")", "+", "1", ")", "*", "bucket_quant", "\n", "\n", "", "s_padding", "=", "np", ".", "array", "(", "[", "source_vocab", "[", "'padding'", "]", "for", "ctr", "in", "xrange", "(", "sl", ",", "new_length", ")", "]", ")", "\n", "\n", "# NEED EXTRA PADDING FOR TRAINING.. ", "\n", "t_padding", "=", "np", ".", "array", "(", "[", "target_vocab", "[", "'padding'", "]", "for", "ctr", "in", "xrange", "(", "tl", ",", "new_length", "+", "1", ")", "]", ")", "\n", "\n", "source_lines", "[", "i", "]", "=", "np", ".", "concatenate", "(", "[", "source_lines", "[", "i", "]", ",", "s_padding", "]", ")", "\n", "target_lines", "[", "i", "]", "=", "np", ".", "concatenate", "(", "[", "target_lines", "[", "i", "]", ",", "t_padding", "]", ")", "\n", "\n", "if", "new_length", "in", "buckets", ":", "\n", "                ", "buckets", "[", "new_length", "]", ".", "append", "(", "(", "source_lines", "[", "i", "]", ",", "target_lines", "[", "i", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "buckets", "[", "new_length", "]", "=", "[", "(", "source_lines", "[", "i", "]", ",", "target_lines", "[", "i", "]", ")", "]", "\n", "\n", "#if i%100000 == 0 and i > 0:", "\n", "#    print(\"Loading\", i)", "\n", "\n", "", "", "return", "buckets", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.data_loader.Data_Loader.create_buckets_only_src": [[199, 230], ["xrange", "len", "numpy.concatenate", "len", "numpy.array", "numpy.concatenate", "buckets[].append", "xrange"], "methods", ["None"], ["", "def", "create_buckets_only_src", "(", "self", ",", "source_lines", ")", ":", "\n", "\n", "        ", "bucket_quant", "=", "self", ".", "bucket_quant", "\n", "source_vocab", "=", "self", ".", "source_vocab", "\n", "\n", "buckets", "=", "{", "}", "\n", "for", "i", "in", "xrange", "(", "len", "(", "source_lines", ")", ")", ":", "\n", "\n", "# source = source + <EOL>", "\n", "# target = <BOL> + target + <EOL>", "\n", "            ", "source_lines", "[", "i", "]", "=", "np", ".", "concatenate", "(", "(", "source_lines", "[", "i", "]", ",", "[", "source_vocab", "[", "'eol'", "]", "]", ")", ")", "\n", "\n", "sl", "=", "len", "(", "source_lines", "[", "i", "]", ")", "\n", "new_length", "=", "sl", "\n", "\n", "# fitting new_length to neareast upperbound of bucket_quant", "\n", "# e.g. bucket_quant=50 -> new_length = 50, 100, 150, ...", "\n", "if", "new_length", "%", "bucket_quant", ">", "0", ":", "\n", "                ", "new_length", "=", "(", "(", "new_length", "/", "bucket_quant", ")", "+", "1", ")", "*", "bucket_quant", "\n", "\n", "", "s_padding", "=", "np", ".", "array", "(", "[", "source_vocab", "[", "'padding'", "]", "for", "ctr", "in", "xrange", "(", "sl", ",", "new_length", ")", "]", ")", "\n", "\n", "source_lines", "[", "i", "]", "=", "np", ".", "concatenate", "(", "[", "source_lines", "[", "i", "]", ",", "s_padding", "]", ")", "\n", "\n", "if", "new_length", "in", "buckets", ":", "\n", "                ", "buckets", "[", "new_length", "]", ".", "append", "(", "source_lines", "[", "i", "]", ")", "\n", "", "else", ":", "\n", "                ", "buckets", "[", "new_length", "]", "=", "[", "source_lines", "[", "i", "]", "]", "\n", "\n", "\n", "", "", "return", "buckets", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.data_loader.Data_Loader.build_vocab": [[232, 247], ["None"], "methods", ["None"], ["", "def", "build_vocab", "(", "self", ",", "sentences", ")", ":", "\n", "        ", "vocab", "=", "{", "}", "\n", "ctr", "=", "0", "\n", "for", "st", "in", "sentences", ":", "\n", "            ", "for", "ch", "in", "st", ":", "\n", "                ", "if", "ch", "not", "in", "vocab", ":", "\n", "                    ", "vocab", "[", "ch", "]", "=", "ctr", "\n", "ctr", "+=", "1", "\n", "\n", "# SOME SPECIAL CHARACTERS", "\n", "", "", "", "vocab", "[", "'eol'", "]", "=", "ctr", "# end of line", "\n", "vocab", "[", "'padding'", "]", "=", "ctr", "+", "1", "# padding", "\n", "vocab", "[", "'init'", "]", "=", "ctr", "+", "2", "# init", "\n", "\n", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.data_loader.Data_Loader.string_to_indices": [[248, 263], ["indices.append", "len", "indices.extend", "xrange", "len"], "methods", ["None"], ["", "def", "string_to_indices", "(", "self", ",", "sentence", ",", "vocab", ",", "pad", "=", "-", "1", ")", ":", "\n", "        ", "indices", "=", "[", "]", "\n", "for", "s", "in", "sentence", ":", "\n", "            ", "try", ":", "indices", ".", "append", "(", "vocab", "[", "s", "]", ")", "\n", "except", ":", "pass", "\n", "#indices = [ vocab[s] for s in sentence ]", "\n", "\n", "", "if", "pad", ">", "-", "1", ":", "\n", "            ", "if", "len", "(", "indices", ")", ">", "pad", ":", "\n", "                ", "indices", "=", "indices", "[", ":", "pad", "]", "\n", "", "else", ":", "\n", "                ", "padding", "=", "[", "vocab", "[", "'<p>'", "]", "for", "_", "in", "xrange", "(", "len", "(", "indices", ")", ",", "pad", ")", "]", "\n", "indices", ".", "extend", "(", "padding", ")", "\n", "\n", "", "", "return", "indices", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.data_loader.Data_Loader.inidices_to_string": [[264, 273], ["None"], "methods", ["None"], ["", "def", "inidices_to_string", "(", "self", ",", "sentence", ",", "vocab", ")", ":", "\n", "        ", "id_ch", "=", "{", "vocab", "[", "ch", "]", ":", "ch", "for", "ch", "in", "vocab", "}", "\n", "sent", "=", "[", "]", "\n", "for", "c", "in", "sentence", ":", "\n", "            ", "if", "id_ch", "[", "c", "]", "==", "'eol'", ":", "\n", "                ", "break", "\n", "", "sent", "+=", "id_ch", "[", "c", "]", "\n", "\n", "", "return", "\"\"", ".", "join", "(", "sent", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.data_loader.Data_Loader.get_batch_from_pairs": [[274, 282], ["source_sentences.append", "target_sentences.append", "numpy.array", "numpy.array"], "methods", ["None"], ["", "def", "get_batch_from_pairs", "(", "self", ",", "pair_list", ")", ":", "\n", "        ", "source_sentences", "=", "[", "]", "\n", "target_sentences", "=", "[", "]", "\n", "for", "s", ",", "t", "in", "pair_list", ":", "\n", "            ", "source_sentences", ".", "append", "(", "s", ")", "\n", "target_sentences", ".", "append", "(", "t", ")", "\n", "\n", "", "return", "np", ".", "array", "(", "source_sentences", ",", "dtype", "=", "'int32'", ")", ",", "np", ".", "array", "(", "target_sentences", ",", "dtype", "=", "'int32'", ")", ",", "pair_list", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.data_loader.main": [[284, 300], ["data_loader.Data_Loader", "data_loader.Data_Loader.load_translation_data", "embed"], "function", ["home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.data_loader.Data_Loader.load_translation_data"], ["", "", "def", "main", "(", ")", ":", "\n", "# FOR TESTING ONLY", "\n", "    ", "trans_options", "=", "{", "\n", "'model_type'", ":", "'translation'", ",", "\n", "'source_file'", ":", "'Data/translator_training_data/news-commentary-v9.fr-en.en'", ",", "\n", "'target_file'", ":", "'Data/translator_training_data/news-commentary-v9.fr-en.fr'", ",", "\n", "'bucket_quant'", ":", "25", ",", "\n", "}", "\n", "gen_options", "=", "{", "\n", "'model_type'", ":", "'generator'", ",", "\n", "'dir_name'", ":", "'Data'", ",", "\n", "}", "\n", "\n", "dl", "=", "Data_Loader", "(", "trans_options", ")", "\n", "buckets", ",", "source_vocab", ",", "target_vocab", "=", "dl", ".", "load_translation_data", "(", ")", "\n", "from", "IPython", "import", "embed", ";", "embed", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.translator_protocol.ByteNetCNN.__init__": [[12, 81], ["os.path.join", "os.path.join", "data_loader.Data_Loader", "translator_protocol.ByteNetCNN.dl.load_translation_data", "ByteNet.translator.ByteNet_Translator", "translator_protocol.ByteNetCNN.translator_model.build_model", "translator_protocol.ByteNetCNN.translator_model.build_translator", "tensorflow.Session", "tensorflow.train.Saver", "range", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "len", "len", "tensorflow.train.Saver.restore", "int", "tensorflow.get_default_graph().get_tensor_by_name", "os.path.realpath().split", "math.pow", "tensorflow.get_default_graph", "os.path.realpath"], "methods", ["home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.data_loader.Data_Loader.load_translation_data", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.translator.ByteNet_Translator.build_model", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.translator.ByteNet_Translator.build_translator"], ["    ", "def", "__init__", "(", "self", ",", "task", ")", ":", "\n", "        ", "bucket_quant", "=", "10", "\n", "\n", "current_path", "=", "'/'", ".", "join", "(", "os", ".", "path", ".", "realpath", "(", "__file__", ")", ".", "split", "(", "'/'", ")", "[", ":", "-", "1", "]", ")", "\n", "translator_root_path", "=", "join", "(", "current_path", ",", "'pretrained_models'", ")", "\n", "\n", "model_path", "=", "{", "\n", "'en-de-news'", ":", "join", "(", "translator_root_path", ",", "'en-de-news'", ",", "'model_epoch_4_145000.ckpt'", ")", ",", "\n", "'en-fr-news'", ":", "join", "(", "translator_root_path", ",", "'en-fr-news'", ",", "'model_epoch_4_90000.ckpt'", ")", ",", "\n", "'en-cs-news'", ":", "join", "(", "translator_root_path", ",", "'en-cs-news'", ",", "'model_epoch_4_70000.ckpt'", ")", ",", "\n", "'en-de-europarl'", ":", "join", "(", "translator_root_path", ",", "'en-de-europarl'", ",", "'model_epoch_1_440000.ckpt'", ")", "\n", "}", "\n", "\n", "data_root_path", "=", "join", "(", "current_path", ",", "'Data'", ",", "'translator_training_data'", ")", "\n", "source_file", "=", "{", "\n", "'en-de-europarl'", ":", "join", "(", "data_root_path", ",", "'europarl-v7.de-en.en'", ")", ",", "\n", "'en-de-news'", ":", "join", "(", "data_root_path", ",", "'news-commentary-v12.de-en.en'", ")", ",", "\n", "'en-fr-news'", ":", "join", "(", "data_root_path", ",", "'news-commentary-v9.fr-en.en'", ")", ",", "\n", "'en-cs-news'", ":", "join", "(", "data_root_path", ",", "'news-commentary-v9.cs-en.en'", ")", "\n", "}", "\n", "\n", "target_file", "=", "{", "\n", "'en-de-europarl'", ":", "join", "(", "data_root_path", ",", "'europarl-v7.de-en.de'", ")", ",", "\n", "'en-de-news'", ":", "join", "(", "data_root_path", ",", "'news-commentary-v12.de-en.de'", ")", ",", "\n", "'en-fr-news'", ":", "join", "(", "data_root_path", ",", "'news-commentary-v9.fr-en.fr'", ")", ",", "\n", "'en-cs-news'", ":", "join", "(", "data_root_path", ",", "'news-commentary-v9.cs-en.cs'", ")", "\n", "}", "\n", "\n", "data_loader_options", "=", "{", "\n", "'model_type'", ":", "'translation'", ",", "\n", "'source_file'", ":", "source_file", "[", "task", "]", ",", "\n", "'target_file'", ":", "target_file", "[", "task", "]", ",", "\n", "'bucket_quant'", ":", "bucket_quant", "\n", "}", "\n", "\n", "\n", "self", ".", "dl", "=", "data_loader", ".", "Data_Loader", "(", "data_loader_options", ")", "\n", "self", ".", "buckets", ",", "self", ".", "source_vocab", ",", "self", ".", "target_vocab", "=", "self", ".", "dl", ".", "load_translation_data", "(", ")", "\n", "\n", "config", "=", "model_config", ".", "translator_config", "\n", "\n", "model_options", "=", "{", "\n", "'source_vocab_size'", ":", "len", "(", "self", ".", "source_vocab", ")", ",", "\n", "'target_vocab_size'", ":", "len", "(", "self", ".", "target_vocab", ")", ",", "\n", "'residual_channels'", ":", "config", "[", "'residual_channels'", "]", ",", "\n", "'decoder_dilations'", ":", "config", "[", "'decoder_dilations'", "]", ",", "\n", "'encoder_dilations'", ":", "config", "[", "'encoder_dilations'", "]", ",", "\n", "'decoder_filter_width'", ":", "config", "[", "'decoder_filter_width'", "]", ",", "\n", "'encoder_filter_width'", ":", "config", "[", "'encoder_filter_width'", "]", ",", "\n", "'layer_norm'", ":", "config", "[", "'layer_norm'", "]", "\n", "}", "\n", "\n", "self", ".", "translator_model", "=", "translator", ".", "ByteNet_Translator", "(", "model_options", ")", "\n", "self", ".", "translator_model", ".", "build_model", "(", ")", "\n", "self", ".", "translator_model", ".", "build_translator", "(", "reuse", "=", "True", ")", "\n", "\n", "self", ".", "sess", "=", "tf", ".", "Session", "(", ")", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "\n", "\n", "if", "model_path", "[", "task", "]", ":", "\n", "            ", "saver", ".", "restore", "(", "self", ".", "sess", ",", "model_path", "[", "task", "]", ")", "\n", "\n", "", "self", ".", "features", "=", "{", "}", "\n", "for", "layer_index", "in", "range", "(", "15", ")", ":", "\n", "            ", "dilation", "=", "int", "(", "math", ".", "pow", "(", "2", ",", "layer_index", "%", "5", ")", ")", "\n", "\n", "layer_tensor_name", "=", "\"bytenet_encoder_layer_%d_%d/add:0\"", "%", "(", "layer_index", ",", "dilation", ")", "\n", "layer_name", "=", "\"bytenet_encoder_layer_%d_%d\"", "%", "(", "layer_index", ",", "dilation", ")", "\n", "self", ".", "features", "[", "layer_name", "]", "=", "tf", ".", "get_default_graph", "(", ")", ".", "get_tensor_by_name", "(", "layer_tensor_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.translator_protocol.ByteNetCNN.get_layer_name": [[82, 86], ["int", "math.pow"], "methods", ["None"], ["", "", "def", "get_layer_name", "(", "self", ",", "layer_index", ")", ":", "\n", "        ", "dilation", "=", "int", "(", "math", ".", "pow", "(", "2", ",", "layer_index", "%", "5", ")", ")", "\n", "layer_name", "=", "'bytenet_encoder_layer_%d_%d'", "%", "(", "layer_index", ",", "dilation", ")", "\n", "return", "layer_name", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.translator_protocol.ByteNetCNN.make_feed": [[87, 95], ["translator_protocol.ByteNetCNN.dl.create_buckets_only_src", "numpy.array", "translator_protocol.ByteNetCNN.keys", "translator_protocol.ByteNetCNN.dl.string_to_indices"], "methods", ["home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.data_loader.Data_Loader.create_buckets_only_src", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.data_loader.Data_Loader.string_to_indices"], ["", "def", "make_feed", "(", "self", ",", "x", ")", ":", "\n", "        ", "buckets", "=", "self", ".", "dl", ".", "create_buckets_only_src", "(", "\n", "[", "self", ".", "dl", ".", "string_to_indices", "(", "x", ",", "self", ".", "source_vocab", ")", "]", ")", "\n", "\n", "bucket_size", "=", "buckets", ".", "keys", "(", ")", "[", "0", "]", "\n", "feed_text", "=", "np", ".", "array", "(", "buckets", "[", "bucket_size", "]", ",", "dtype", "=", "'int32'", ")", "\n", "\n", "return", "feed_text", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.translator_protocol.ByteNetCNN.forward": [[96, 110], ["translator_protocol.ByteNetCNN.make_feed", "translator_protocol.ByteNetCNN.sess.run"], "methods", ["home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.classifier_protocol.VeryDeepCNN.make_feed"], ["", "def", "forward", "(", "self", ",", "layer_name", ",", "x", ")", ":", "\n", "        ", "feed_text", "=", "self", ".", "make_feed", "(", "x", ")", "\n", "\n", "activation", "=", "self", ".", "sess", ".", "run", "(", "\n", "self", ".", "features", "[", "layer_name", "]", ",", "\n", "feed_dict", "=", "{", "\n", "self", ".", "translator_model", ".", "source_sentence", ":", "feed_text", "\n", "}", "\n", ")", "\n", "\n", "# activation.shape = (len, # units)", "\n", "activation", "=", "activation", "[", "0", "]", "\n", "\n", "return", "activation", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.translator_protocol.ByteNetCNN.get_loss": [[111, 129], ["numpy.reshape", "numpy.reshape", "numpy.array", "numpy.array", "translator_protocol.ByteNetCNN.sess.run"], "methods", ["None"], ["", "def", "get_loss", "(", "self", ",", "src", ",", "tgt", ")", ":", "\n", "\n", "        ", "src", "=", "np", ".", "reshape", "(", "src", ",", "[", "1", ",", "-", "1", "]", ")", "\n", "tgt", "=", "np", ".", "reshape", "(", "tgt", ",", "[", "1", ",", "-", "1", "]", ")", "\n", "\n", "src", "=", "np", ".", "array", "(", "src", ",", "dtype", "=", "'int32'", ")", "\n", "tgt", "=", "np", ".", "array", "(", "tgt", ",", "dtype", "=", "'int32'", ")", "\n", "\n", "\n", "loss", "=", "self", ".", "sess", ".", "run", "(", "\n", "self", ".", "translator_model", ".", "loss", ",", "\n", "feed_dict", "=", "{", "\n", "self", ".", "translator_model", ".", "source_sentence", ":", "src", ",", "\n", "self", ".", "translator_model", ".", "target_sentence", ":", "tgt", "\n", "}", "\n", ")", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.bytenet.translator_protocol.ByteNetCNN.get_grad": [[130, 157], ["numpy.reshape", "numpy.reshape", "numpy.array", "numpy.array", "translator_protocol.ByteNetCNN.sess.run", "numpy.average", "tensorflow.gradients", "tensorflow.global_variables"], "methods", ["None"], ["", "def", "get_grad", "(", "self", ",", "src", ",", "tgt", ")", ":", "\n", "        ", "src", "=", "np", ".", "reshape", "(", "src", ",", "[", "1", ",", "-", "1", "]", ")", "\n", "tgt", "=", "np", ".", "reshape", "(", "tgt", ",", "[", "1", ",", "-", "1", "]", ")", "\n", "\n", "src", "=", "np", ".", "array", "(", "src", ",", "dtype", "=", "'int32'", ")", "\n", "tgt", "=", "np", ".", "array", "(", "tgt", ",", "dtype", "=", "'int32'", ")", "\n", "\n", "src_embed", "=", "[", "v", "for", "v", "in", "tf", ".", "global_variables", "(", ")", "if", "'source_embedding'", "in", "v", ".", "name", "]", "[", "0", "]", "\n", "\n", "grad", "=", "self", ".", "sess", ".", "run", "(", "\n", "tf", ".", "gradients", "(", "self", ".", "translator_model", ".", "loss", ",", "src_embed", ")", ",", "\n", "feed_dict", "=", "{", "\n", "self", ".", "translator_model", ".", "source_sentence", ":", "src", ",", "\n", "self", ".", "translator_model", ".", "target_sentence", ":", "tgt", "\n", "}", "\n", ")", "\n", "\n", "# grad.shape = (l, d)", "\n", "grad", "=", "grad", "[", "0", "]", ".", "values", "\n", "\n", "# grad.shape = (l)", "\n", "grad", "=", "np", ".", "average", "(", "\n", "grad", ",", "\n", "axis", "=", "1", "\n", ")", "\n", "\n", "return", "grad", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.fully_connected": [[4, 16], ["tensorflow.variable_scope", "input_.get_shape", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.matmul", "tensorflow.contrib.layers.xavier_initializer", "tensorflow.constant_initializer"], "function", ["None"], ["def", "fully_connected", "(", "input_", ",", "output_nodes", ",", "name", ",", "stddev", "=", "0.02", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "name", ")", ":", "\n", "        ", "input_shape", "=", "input_", ".", "get_shape", "(", ")", "\n", "input_nodes", "=", "input_shape", "[", "-", "1", "]", "\n", "w", "=", "tf", ".", "get_variable", "(", "'w'", ",", "[", "input_nodes", ",", "output_nodes", "]", ",", "\n", "initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer", "(", ")", ")", "\n", "#initializer=tf.random_normal_initializer())", "\n", "#initializer=tf.truncated_normal_initializer(stddev=0.02))", "\n", "biases", "=", "tf", ".", "get_variable", "(", "'b'", ",", "[", "output_nodes", "]", ",", "\n", "initializer", "=", "tf", ".", "constant_initializer", "(", "0.0", ")", ")", "\n", "res", "=", "tf", ".", "matmul", "(", "input_", ",", "w", ")", "+", "biases", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.conv1d": [[19, 40], ["tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.squeeze", "tensorflow.pad", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.contrib.layers.xavier_initializer_conv2d", "tensorflow.constant_initializer", "tensorflow.nn.atrous_conv2d", "tensorflow.nn.atrous_conv2d", "input_.get_shape"], "function", ["None"], ["", "", "def", "conv1d", "(", "input_", ",", "output_channels", ",", "\n", "dilation", "=", "1", ",", "filter_width", "=", "1", ",", "causal", "=", "False", ",", "\n", "name", "=", "\"dilated_conv\"", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "name", ")", ":", "\n", "        ", "w", "=", "tf", ".", "get_variable", "(", "'w'", ",", "[", "1", ",", "filter_width", ",", "input_", ".", "get_shape", "(", ")", "[", "-", "1", "]", ",", "output_channels", "]", ",", "\n", "initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer_conv2d", "(", ")", ")", "\n", "#initializer=tf.random_normal_initializer())", "\n", "#initializer=tf.truncated_normal_initializer(stddev=0.02))", "\n", "b", "=", "tf", ".", "get_variable", "(", "'b'", ",", "[", "output_channels", "]", ",", "\n", "initializer", "=", "tf", ".", "constant_initializer", "(", "0.0", ")", ")", "\n", "\n", "if", "causal", ":", "\n", "            ", "padding", "=", "[", "[", "0", ",", "0", "]", ",", "[", "(", "filter_width", "-", "1", ")", "*", "dilation", ",", "0", "]", ",", "[", "0", ",", "0", "]", "]", "\n", "padded", "=", "tf", ".", "pad", "(", "input_", ",", "padding", ")", "\n", "input_expanded", "=", "tf", ".", "expand_dims", "(", "padded", ",", "axis", "=", "1", ")", "\n", "out", "=", "tf", ".", "nn", ".", "atrous_conv2d", "(", "input_expanded", ",", "w", ",", "rate", "=", "dilation", ",", "padding", "=", "'VALID'", ")", "+", "b", "\n", "", "else", ":", "\n", "            ", "input_expanded", "=", "tf", ".", "expand_dims", "(", "input_", ",", "axis", "=", "1", ")", "\n", "out", "=", "tf", ".", "nn", ".", "atrous_conv2d", "(", "input_expanded", ",", "w", ",", "rate", "=", "dilation", ",", "padding", "=", "'SAME'", ")", "+", "b", "\n", "\n", "", "return", "tf", ".", "squeeze", "(", "out", ",", "[", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.layer_normalization": [[41, 54], ["tensorflow.variable_scope", "x.get_shape", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.moments", "tensorflow.sqrt", "int", "tensorflow.constant_initializer", "int", "tensorflow.constant_initializer", "len"], "function", ["None"], ["", "", "def", "layer_normalization", "(", "x", ",", "name", ",", "epsilon", "=", "1e-8", ",", "trainable", "=", "True", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "name", ")", ":", "\n", "        ", "shape", "=", "x", ".", "get_shape", "(", ")", "\n", "beta", "=", "tf", ".", "get_variable", "(", "'beta'", ",", "[", "int", "(", "shape", "[", "-", "1", "]", ")", "]", ",", "\n", "initializer", "=", "tf", ".", "constant_initializer", "(", "0", ")", ",", "trainable", "=", "trainable", ")", "\n", "gamma", "=", "tf", ".", "get_variable", "(", "'gamma'", ",", "[", "int", "(", "shape", "[", "-", "1", "]", ")", "]", ",", "\n", "initializer", "=", "tf", ".", "constant_initializer", "(", "1", ")", ",", "trainable", "=", "trainable", ")", "\n", "\n", "mean", ",", "variance", "=", "tf", ".", "nn", ".", "moments", "(", "x", ",", "axes", "=", "[", "len", "(", "shape", ")", "-", "1", "]", ",", "keep_dims", "=", "True", ")", "\n", "\n", "x", "=", "(", "x", "-", "mean", ")", "/", "tf", ".", "sqrt", "(", "variance", "+", "epsilon", ")", "\n", "\n", "return", "gamma", "*", "x", "+", "beta", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.byetenet_residual_block_without_ln": [[55, 76], ["tensorflow.variable_scope", "ops.layer_normalization", "tensorflow.nn.relu", "ops.conv1d", "ops.layer_normalization", "tensorflow.nn.relu", "ops.conv1d", "tensorflow.nn.relu", "ops.conv1d"], "function", ["home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.layer_normalization", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.conv1d", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.layer_normalization", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.conv1d", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.conv1d"], ["", "", "def", "byetenet_residual_block_without_ln", "(", "input_", ",", "dilation", ",", "layer_no", ",", "\n", "residual_channels", ",", "filter_width", ",", "\n", "causal", "=", "True", ",", "train", "=", "True", ")", ":", "\n", "        ", "block_type", "=", "\"decoder\"", "if", "causal", "else", "\"encoder\"", "\n", "block_name", "=", "\"bytenet_{}_layer_{}_{}\"", ".", "format", "(", "block_type", ",", "layer_no", ",", "dilation", ")", "\n", "with", "tf", ".", "variable_scope", "(", "block_name", ")", ":", "\n", "            ", "input_ln", "=", "layer_normalization", "(", "input_", ",", "name", "=", "\"ln1\"", ",", "trainable", "=", "train", ")", "\n", "relu1", "=", "tf", ".", "nn", ".", "relu", "(", "input_ln", ")", "\n", "conv1", "=", "conv1d", "(", "relu1", ",", "residual_channels", ",", "name", "=", "\"conv1d_1\"", ")", "\n", "conv1", "=", "layer_normalization", "(", "conv1", ",", "name", "=", "\"ln2\"", ",", "trainable", "=", "train", ")", "\n", "relu2", "=", "tf", ".", "nn", ".", "relu", "(", "conv1", ")", "\n", "\n", "dilated_conv", "=", "conv1d", "(", "relu2", ",", "residual_channels", ",", "\n", "dilation", ",", "filter_width", ",", "\n", "causal", "=", "causal", ",", "\n", "name", "=", "\"dilated_conv\"", "\n", ")", "\n", "#dilated_conv = layer_normalization(dilated_conv, name=\"ln3\", trainable = train)", "\n", "relu3", "=", "tf", ".", "nn", ".", "relu", "(", "dilated_conv", ")", "\n", "conv2", "=", "conv1d", "(", "relu3", ",", "2", "*", "residual_channels", ",", "name", "=", "'conv1d_2'", ")", "\n", "return", "input_", "+", "conv2", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.byetenet_residual_block": [[77, 99], ["tensorflow.variable_scope", "ops.layer_normalization", "tensorflow.nn.relu", "ops.conv1d", "ops.layer_normalization", "tensorflow.nn.relu", "ops.conv1d", "ops.layer_normalization", "tensorflow.nn.relu", "ops.conv1d"], "function", ["home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.layer_normalization", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.conv1d", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.layer_normalization", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.conv1d", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.layer_normalization", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.conv1d"], ["", "", "def", "byetenet_residual_block", "(", "input_", ",", "dilation", ",", "layer_no", ",", "\n", "residual_channels", ",", "filter_width", ",", "\n", "causal", "=", "True", ",", "train", "=", "True", ")", ":", "\n", "        ", "block_type", "=", "\"decoder\"", "if", "causal", "else", "\"encoder\"", "\n", "block_name", "=", "\"bytenet_{}_layer_{}_{}\"", ".", "format", "(", "block_type", ",", "layer_no", ",", "dilation", ")", "\n", "with", "tf", ".", "variable_scope", "(", "block_name", ")", ":", "\n", "            ", "input_ln", "=", "layer_normalization", "(", "input_", ",", "name", "=", "\"ln1\"", ",", "trainable", "=", "train", ")", "\n", "relu1", "=", "tf", ".", "nn", ".", "relu", "(", "input_ln", ")", "\n", "conv1", "=", "conv1d", "(", "relu1", ",", "residual_channels", ",", "name", "=", "\"conv1d_1\"", ")", "\n", "conv1", "=", "layer_normalization", "(", "conv1", ",", "name", "=", "\"ln2\"", ",", "trainable", "=", "train", ")", "\n", "relu2", "=", "tf", ".", "nn", ".", "relu", "(", "conv1", ")", "\n", "\n", "dilated_conv", "=", "conv1d", "(", "relu2", ",", "residual_channels", ",", "\n", "dilation", ",", "filter_width", ",", "\n", "causal", "=", "causal", ",", "\n", "name", "=", "\"dilated_conv\"", "\n", ")", "\n", "dilated_conv", "=", "layer_normalization", "(", "dilated_conv", ",", "name", "=", "\"ln3\"", ",", "trainable", "=", "train", ")", "\n", "relu3", "=", "tf", ".", "nn", ".", "relu", "(", "dilated_conv", ")", "\n", "conv2", "=", "conv1d", "(", "relu3", ",", "2", "*", "residual_channels", ",", "name", "=", "'conv1d_2'", ")", "\n", "\n", "return", "input_", "+", "conv2", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.init_weight": [[100, 102], ["tensorflow.Variable", "tensorflow.truncated_normal", "math.sqrt", "float"], "function", ["None"], ["", "", "def", "init_weight", "(", "dim_in", ",", "dim_out", ",", "name", "=", "None", ",", "stddev", "=", "1.0", ")", ":", "\n", "    ", "return", "tf", ".", "Variable", "(", "tf", ".", "truncated_normal", "(", "[", "dim_in", ",", "dim_out", "]", ",", "stddev", "=", "stddev", "/", "math", ".", "sqrt", "(", "float", "(", "dim_in", ")", ")", ")", ",", "name", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.init_bias": [[103, 105], ["tensorflow.Variable", "tensorflow.zeros"], "function", ["None"], ["", "def", "init_bias", "(", "dim_out", ",", "name", "=", "None", ")", ":", "\n", "    ", "return", "tf", ".", "Variable", "(", "tf", ".", "zeros", "(", "[", "dim_out", "]", ")", ",", "name", "=", "name", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.translator.ByteNet_Translator.__init__": [[5, 16], ["tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.truncated_normal_initializer", "tensorflow.truncated_normal_initializer"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "options", ")", ":", "\n", "        ", "self", ".", "options", "=", "options", "\n", "embedding_channels", "=", "2", "*", "options", "[", "'residual_channels'", "]", "\n", "\n", "self", ".", "w_source_embedding", "=", "tf", ".", "get_variable", "(", "'w_source_embedding'", ",", "\n", "[", "options", "[", "'source_vocab_size'", "]", ",", "embedding_channels", "]", ",", "\n", "initializer", "=", "tf", ".", "truncated_normal_initializer", "(", "stddev", "=", "0.02", ")", ")", "\n", "\n", "self", ".", "w_target_embedding", "=", "tf", ".", "get_variable", "(", "'w_target_embedding'", ",", "\n", "[", "options", "[", "'target_vocab_size'", "]", ",", "embedding_channels", "]", ",", "\n", "initializer", "=", "tf", ".", "truncated_normal_initializer", "(", "stddev", "=", "0.02", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.translator.ByteNet_Translator.build_model": [[17, 64], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.nn.embedding_lookup", "tensorflow.nn.embedding_lookup", "enumerate", "enumerate", "ops.conv1d", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.reduce_mean", "tensorflow.argmax", "tensorflow.summary.scalar", "ops.byetenet_residual_block", "tensorflow.nn.relu", "ops.byetenet_residual_block", "ops.byetenet_residual_block_without_ln"], "methods", ["home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.conv1d", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.byetenet_residual_block", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.byetenet_residual_block", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.byetenet_residual_block_without_ln"], ["", "def", "build_model", "(", "self", ")", ":", "\n", "        ", "options", "=", "self", ".", "options", "\n", "self", ".", "source_sentence", "=", "tf", ".", "placeholder", "(", "'int32'", ",", "\n", "[", "None", ",", "None", "]", ",", "name", "=", "'source_sentence'", ")", "\n", "self", ".", "target_sentence", "=", "tf", ".", "placeholder", "(", "'int32'", ",", "\n", "[", "None", ",", "None", "]", ",", "name", "=", "'target_sentence'", ")", "\n", "\n", "target_1", "=", "self", ".", "target_sentence", "[", ":", ",", "0", ":", "-", "1", "]", "\n", "target_2", "=", "self", ".", "target_sentence", "[", ":", ",", "1", ":", "]", "\n", "\n", "source_embedding", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "self", ".", "w_source_embedding", ",", "\n", "self", ".", "source_sentence", ",", "name", "=", "\"source_embedding\"", ")", "\n", "target_1_embedding", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "self", ".", "w_target_embedding", ",", "\n", "target_1", ",", "name", "=", "\"target_1_embedding\"", ")", "\n", "\n", "\n", "curr_input", "=", "source_embedding", "\n", "for", "layer_no", ",", "dilation", "in", "enumerate", "(", "options", "[", "'encoder_dilations'", "]", ")", ":", "\n", "            ", "if", "options", "[", "'layer_norm'", "]", ":", "\n", "                ", "curr_input", "=", "ops", ".", "byetenet_residual_block", "(", "curr_input", ",", "dilation", ",", "\n", "layer_no", ",", "options", "[", "'residual_channels'", "]", ",", "\n", "options", "[", "'encoder_filter_width'", "]", ",", "causal", "=", "False", ",", "train", "=", "True", ")", "\n", "", "else", ":", "\n", "                ", "curr_input", "=", "ops", ".", "byetenet_residual_block_without_ln", "(", "curr_input", ",", "dilation", ",", "\n", "layer_no", ",", "options", "[", "'residual_channels'", "]", ",", "\n", "options", "[", "'encoder_filter_width'", "]", ",", "causal", "=", "False", ",", "train", "=", "True", ")", "\n", "\n", "\n", "", "", "encoder_output", "=", "curr_input", "\n", "combined_embedding", "=", "target_1_embedding", "+", "encoder_output", "\n", "curr_input", "=", "combined_embedding", "\n", "for", "layer_no", ",", "dilation", "in", "enumerate", "(", "options", "[", "'decoder_dilations'", "]", ")", ":", "\n", "            ", "curr_input", "=", "ops", ".", "byetenet_residual_block", "(", "curr_input", ",", "dilation", ",", "\n", "layer_no", ",", "options", "[", "'residual_channels'", "]", ",", "\n", "options", "[", "'decoder_filter_width'", "]", ",", "causal", "=", "True", ",", "train", "=", "True", ")", "\n", "\n", "\n", "", "logits", "=", "ops", ".", "conv1d", "(", "tf", ".", "nn", ".", "relu", "(", "curr_input", ")", ",", "\n", "options", "[", "'target_vocab_size'", "]", ",", "name", "=", "'logits'", ")", "\n", "logits_flat", "=", "tf", ".", "reshape", "(", "logits", ",", "[", "-", "1", ",", "options", "[", "'target_vocab_size'", "]", "]", ")", "\n", "target_flat", "=", "tf", ".", "reshape", "(", "target_2", ",", "[", "-", "1", "]", ")", "\n", "loss", "=", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "\n", "labels", "=", "target_flat", ",", "logits", "=", "logits_flat", ")", "\n", "\n", "self", ".", "loss", "=", "tf", ".", "reduce_mean", "(", "loss", ")", "\n", "self", ".", "arg_max_prediction", "=", "tf", ".", "argmax", "(", "logits_flat", ",", "1", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "'loss'", ",", "self", ".", "loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.translator.ByteNet_Translator.build_translator": [[65, 108], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.nn.embedding_lookup", "tensorflow.nn.embedding_lookup", "enumerate", "enumerate", "ops.conv1d", "tensorflow.reshape", "tensorflow.nn.softmax", "tensorflow.reshape", "tensorflow.get_variable_scope().reuse_variables", "ops.byetenet_residual_block", "tensorflow.nn.relu", "ops.byetenet_residual_block", "ops.byetenet_residual_block_without_ln", "tensorflow.get_variable_scope", "tensorflow.shape", "tensorflow.shape"], "methods", ["home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.conv1d", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.byetenet_residual_block", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.byetenet_residual_block", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.byetenet_residual_block_without_ln"], ["", "def", "build_translator", "(", "self", ",", "reuse", "=", "False", ")", ":", "\n", "        ", "if", "reuse", ":", "\n", "            ", "tf", ".", "get_variable_scope", "(", ")", ".", "reuse_variables", "(", ")", "\n", "\n", "", "options", "=", "self", ".", "options", "\n", "self", ".", "t_source_sentence", "=", "tf", ".", "placeholder", "(", "'int32'", ",", "\n", "[", "None", ",", "None", "]", ",", "name", "=", "'source_sentence'", ")", "\n", "self", ".", "t_target_sentence", "=", "tf", ".", "placeholder", "(", "'int32'", ",", "\n", "[", "None", ",", "None", "]", ",", "name", "=", "'target_sentence'", ")", "\n", "\n", "source_embedding", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "self", ".", "w_source_embedding", ",", "\n", "self", ".", "t_source_sentence", ",", "name", "=", "\"source_embedding\"", ")", "\n", "target_embedding", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "self", ".", "w_target_embedding", ",", "\n", "self", ".", "t_target_sentence", ",", "name", "=", "\"target_embedding\"", ")", "\n", "\n", "curr_input", "=", "source_embedding", "\n", "for", "layer_no", ",", "dilation", "in", "enumerate", "(", "options", "[", "'encoder_dilations'", "]", ")", ":", "\n", "            ", "if", "options", "[", "'layer_norm'", "]", ":", "\n", "                ", "curr_input", "=", "ops", ".", "byetenet_residual_block", "(", "curr_input", ",", "dilation", ",", "\n", "layer_no", ",", "options", "[", "'residual_channels'", "]", ",", "\n", "options", "[", "'encoder_filter_width'", "]", ",", "causal", "=", "False", ",", "train", "=", "False", ")", "\n", "", "else", ":", "\n", "                ", "curr_input", "=", "ops", ".", "byetenet_residual_block_without_ln", "(", "curr_input", ",", "dilation", ",", "\n", "layer_no", ",", "options", "[", "'residual_channels'", "]", ",", "\n", "options", "[", "'encoder_filter_width'", "]", ",", "causal", "=", "False", ",", "train", "=", "False", ")", "\n", "\n", "\n", "", "", "encoder_output", "=", "curr_input", "[", ":", ",", "0", ":", "tf", ".", "shape", "(", "self", ".", "t_target_sentence", ")", "[", "1", "]", ",", ":", "]", "\n", "\n", "combined_embedding", "=", "target_embedding", "+", "encoder_output", "\n", "curr_input", "=", "combined_embedding", "\n", "for", "layer_no", ",", "dilation", "in", "enumerate", "(", "options", "[", "'decoder_dilations'", "]", ")", ":", "\n", "            ", "curr_input", "=", "ops", ".", "byetenet_residual_block", "(", "curr_input", ",", "dilation", ",", "\n", "layer_no", ",", "options", "[", "'residual_channels'", "]", ",", "\n", "options", "[", "'decoder_filter_width'", "]", ",", "causal", "=", "True", ",", "train", "=", "False", ")", "\n", "\n", "", "logits", "=", "ops", ".", "conv1d", "(", "tf", ".", "nn", ".", "relu", "(", "curr_input", ")", ",", "\n", "options", "[", "'target_vocab_size'", "]", ",", "name", "=", "'logits'", ")", "\n", "logits_flat", "=", "tf", ".", "reshape", "(", "logits", ",", "[", "-", "1", ",", "options", "[", "'target_vocab_size'", "]", "]", ")", "\n", "probs_flat", "=", "tf", ".", "nn", ".", "softmax", "(", "logits_flat", ")", "\n", "\n", "self", ".", "t_probs", "=", "tf", ".", "reshape", "(", "probs_flat", ",", "\n", "[", "-", "1", ",", "tf", ".", "shape", "(", "logits", ")", "[", "1", "]", ",", "options", "[", "'target_vocab_size'", "]", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.translator.main": [[109, 127], ["translator.ByteNet_Translator", "translator.ByteNet_Translator.build_model", "ByteNet_Translator.build_translator"], "function", ["home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.translator.ByteNet_Translator.build_model", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.translator.ByteNet_Translator.build_translator"], ["", "", "def", "main", "(", ")", ":", "\n", "    ", "options", "=", "{", "\n", "'source_vocab_size'", ":", "250", ",", "\n", "'target_vocab_size'", ":", "250", ",", "\n", "'residual_channels'", ":", "512", ",", "\n", "'encoder_dilations'", ":", "[", "1", ",", "2", ",", "4", ",", "8", ",", "16", ",", "\n", "1", ",", "2", ",", "4", ",", "8", ",", "16", "\n", "]", ",", "\n", "'decoder_dilations'", ":", "[", "1", ",", "2", ",", "4", ",", "8", ",", "16", ",", "\n", "1", ",", "2", ",", "4", ",", "8", ",", "16", "\n", "]", ",", "\n", "'encoder_filter_width'", ":", "3", ",", "\n", "'decoder_filter_width'", ":", "3", ",", "\n", "'layer_norm'", ":", "False", "\n", "}", "\n", "md", "=", "ByteNet_Translator", "(", "options", ")", "\n", "md", ".", "build_model", "(", ")", "\n", "md", ".", "build_translator", "(", "reuse", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.data_helper.data_helper.__init__": [[17, 27], ["unicode", "enumerate"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "sequence_max_length", "=", "1024", ",", "use_title", "=", "False", ")", ":", "\n", "        ", "self", ".", "alphabet", "=", "unicode", "(", "'abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:\u2019\"/|_#$%\u02c6&*\u02dc\u2018+=<>()[]{} '", ",", "'utf-8'", ")", "\n", "#self.alphabet = 'abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:\u2019\"/|_#$%\u02c6&*\u02dc\u2018+=<>()[]{} '", "\n", "\n", "self", ".", "char_dict", "=", "{", "}", "\n", "self", ".", "sequence_max_length", "=", "sequence_max_length", "\n", "for", "i", ",", "c", "in", "enumerate", "(", "self", ".", "alphabet", ")", ":", "\n", "            ", "self", ".", "char_dict", "[", "c", "]", "=", "i", "+", "1", "\n", "\n", "", "self", ".", "use_title", "=", "use_title", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.data_helper.data_helper.char2vec": [[28, 39], ["numpy.zeros", "range", "len", "len"], "methods", ["None"], ["", "def", "char2vec", "(", "self", ",", "text", ")", ":", "\n", "        ", "data", "=", "np", ".", "zeros", "(", "self", ".", "sequence_max_length", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "text", ")", ")", ":", "\n", "            ", "if", "i", ">=", "self", ".", "sequence_max_length", ":", "\n", "                ", "return", "data", "\n", "", "elif", "text", "[", "i", "]", "in", "self", ".", "char_dict", ":", "\n", "                ", "data", "[", "i", "]", "=", "self", ".", "char_dict", "[", "text", "[", "i", "]", "]", "\n", "", "else", ":", "\n", "# unknown character set to be last index + 1", "\n", "                ", "data", "[", "i", "]", "=", "len", "(", "self", ".", "char_dict", ")", "+", "1", "\n", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.data_helper.data_helper.load_csv_file": [[40, 65], ["f.close", "open", "csv.DictReader", "numpy.array", "numpy.array", "numpy.zeros", "labels.append", "texts.append", "all_data.append", "numpy.ones", "[].lower().replace", "[].lower().replace", "data_helper.data_helper.char2vec", "int", "[].lower", "[].lower"], "methods", ["home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.data_helper.data_helper.char2vec"], ["", "def", "load_csv_file", "(", "self", ",", "filename", ",", "num_classes", ")", ":", "\n", "        ", "\"\"\"\n        Load CSV file, generate one-hot labels and process text data as Paper did.\n        \"\"\"", "\n", "all_data", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "texts", "=", "[", "]", "\n", "with", "open", "(", "filename", ")", "as", "f", ":", "\n", "            ", "reader", "=", "csv", ".", "DictReader", "(", "f", ",", "fieldnames", "=", "[", "'class'", "]", ",", "restkey", "=", "'fields'", ")", "\n", "for", "row", "in", "reader", ":", "\n", "# One-hot", "\n", "                ", "one_hot", "=", "np", ".", "zeros", "(", "num_classes", ")", "\n", "one_hot", "[", "int", "(", "row", "[", "'class'", "]", ")", "-", "1", "]", "=", "1", "\n", "labels", ".", "append", "(", "one_hot", ")", "\n", "# Char2vec", "\n", "data", "=", "np", ".", "ones", "(", "self", ".", "sequence_max_length", ")", "*", "68", "\n", "\n", "if", "self", ".", "use_title", ":", "\n", "                    ", "text", "=", "row", "[", "'fields'", "]", "[", "0", "]", ".", "lower", "(", ")", ".", "replace", "(", "'\"'", ",", "''", ")", "\n", "", "else", ":", "\n", "                    ", "text", "=", "row", "[", "'fields'", "]", "[", "-", "1", "]", ".", "lower", "(", ")", ".", "replace", "(", "'\"'", ",", "''", ")", "\n", "", "texts", ".", "append", "(", "text", ")", "\n", "all_data", ".", "append", "(", "self", ".", "char2vec", "(", "text", ")", ")", "\n", "", "", "f", ".", "close", "(", ")", "\n", "return", "np", ".", "array", "(", "all_data", ")", ",", "np", ".", "array", "(", "labels", ")", ",", "texts", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.data_helper.data_helper.load_dataset": [[66, 80], ["f.close", "len", "data_helper.data_helper.load_csv_file", "data_helper.data_helper.load_csv_file", "open", "os.path.join", "os.path.join", "os.path.join", "classes.append", "line.strip"], "methods", ["home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.data_helper.data_helper.load_csv_file", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.data_helper.data_helper.load_csv_file"], ["", "def", "load_dataset", "(", "self", ",", "dataset_path", ")", ":", "\n", "# Read Classes Info", "\n", "        ", "with", "open", "(", "opj", "(", "dataset_path", ",", "\"classes.txt\"", ")", ")", "as", "f", ":", "\n", "            ", "classes", "=", "[", "]", "\n", "for", "line", "in", "f", ":", "\n", "                ", "classes", ".", "append", "(", "line", ".", "strip", "(", ")", ")", "\n", "", "", "f", ".", "close", "(", ")", "\n", "num_classes", "=", "len", "(", "classes", ")", "\n", "# Read CSV Info", "\n", "train_data", ",", "train_label", ",", "train_texts", "=", "self", ".", "load_csv_file", "(", "\n", "opj", "(", "dataset_path", ",", "'train.csv'", ")", ",", "num_classes", ")", "\n", "test_data", ",", "test_label", ",", "test_texts", "=", "self", ".", "load_csv_file", "(", "\n", "opj", "(", "dataset_path", ",", "'test.csv'", ")", ",", "num_classes", ")", "\n", "return", "train_data", ",", "train_label", ",", "train_texts", ",", "test_data", ",", "test_label", ",", "test_texts", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.data_helper.data_helper.batch_iter": [[81, 101], ["numpy.array", "len", "range", "len", "int", "range", "len", "numpy.random.permutation", "min", "numpy.arange", "len"], "methods", ["None"], ["", "def", "batch_iter", "(", "self", ",", "data", ",", "texts", ",", "batch_size", ",", "num_epochs", ",", "shuffle", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Generates a batch iterator for a dataset.\n        \"\"\"", "\n", "data", "=", "np", ".", "array", "(", "data", ")", "\n", "data_size", "=", "len", "(", "data", ")", "\n", "assert", "data_size", "==", "len", "(", "texts", ")", ",", "'len(data) = %d, len(texts) = %d'", "%", "(", "\n", "data_size", ",", "len", "(", "texts", ")", ")", "\n", "num_batches_per_epoch", "=", "int", "(", "(", "len", "(", "data", ")", "-", "1", ")", "/", "batch_size", ")", "+", "1", "\n", "for", "epoch", "in", "range", "(", "num_epochs", ")", ":", "\n", "# Shuffle the data at each epoch", "\n", "            ", "if", "shuffle", ":", "\n", "                ", "shuffle_indices", "=", "np", ".", "random", ".", "permutation", "(", "np", ".", "arange", "(", "data_size", ")", ")", "\n", "shuffled_data", "=", "data", "[", "shuffle_indices", "]", "\n", "", "else", ":", "\n", "                ", "shuffled_data", "=", "data", "\n", "", "for", "batch_num", "in", "range", "(", "num_batches_per_epoch", ")", ":", "\n", "                ", "start_index", "=", "batch_num", "*", "batch_size", "\n", "end_index", "=", "min", "(", "(", "batch_num", "+", "1", ")", "*", "batch_size", ",", "data_size", ")", "\n", "yield", "shuffled_data", "[", "start_index", ":", "end_index", "]", ",", "texts", "[", "start_index", ":", "end_index", "]", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.vdcnn_cam.VDCNN.__init__": [[66, 196], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "print", "vdcnn_cam.VDCNN.layers.append", "range", "vdcnn_cam.downsampling", "vdcnn_cam.VDCNN.layers.append", "print", "range", "vdcnn_cam.downsampling", "vdcnn_cam.VDCNN.layers.append", "print", "range", "vdcnn_cam.downsampling", "vdcnn_cam.VDCNN.layers.append", "print", "range", "tensorflow.reduce_mean", "tensorflow.device", "tensorflow.name_scope", "tensorflow.nn.embedding_lookup", "print", "print", "print", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.nn.conv1d", "tensorflow.nn.conv1d.get_shape", "vdcnn_cam.Convolutional_Block", "vdcnn_cam.VDCNN.layers.append", "downsampling.get_shape", "vdcnn_cam.Convolutional_Block", "vdcnn_cam.VDCNN.layers.append", "downsampling.get_shape", "vdcnn_cam.Convolutional_Block", "vdcnn_cam.VDCNN.layers.append", "downsampling.get_shape", "vdcnn_cam.Convolutional_Block", "vdcnn_cam.VDCNN.layers.append", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.matmul", "tensorflow.name_scope", "tensorflow.argmax", "tensorflow.nn.softmax_cross_entropy_with_logits", "tensorflow.get_collection", "tensorflow.name_scope", "tensorflow.equal", "tensorflow.reduce_mean", "tensorflow.get_variable", "tensorflow.Variable", "vdcnn_cam.VDCNN.embedded_characters.get_shape", "tensorflow.reduce_mean", "sum", "tensorflow.argmax", "tensorflow.cast", "tensorflow.random_uniform", "str", "str", "str", "str", "ValueError", "tensorflow.keras.initializers.he_uniform", "vdcnn_cam.VDCNN.gap.get_shape"], "methods", ["home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.vdcnn.downsampling", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.vdcnn.downsampling", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.vdcnn.downsampling", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.conv1d", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.vdcnn.Convolutional_Block", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.vdcnn.Convolutional_Block", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.vdcnn.Convolutional_Block", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.vdcnn.Convolutional_Block"], ["    ", "def", "__init__", "(", "self", ",", "num_classes", ",", "sequence_max_length", "=", "1024", ",", "num_quantized_chars", "=", "69", ",", "embedding_size", "=", "16", ",", "\n", "depth", "=", "9", ",", "downsampling_type", "=", "'maxpool'", ",", "use_he_uniform", "=", "True", ",", "optional_shortcut", "=", "False", ")", ":", "\n", "\n", "# Depth to No. Layers", "\n", "        ", "if", "depth", "==", "9", ":", "\n", "            ", "num_layers", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", "\n", "", "elif", "depth", "==", "17", ":", "\n", "            ", "num_layers", "=", "[", "4", ",", "4", ",", "4", ",", "4", "]", "\n", "", "elif", "depth", "==", "29", ":", "\n", "            ", "num_layers", "=", "[", "10", ",", "10", ",", "4", ",", "4", "]", "\n", "", "elif", "depth", "==", "49", ":", "\n", "            ", "num_layers", "=", "[", "16", ",", "16", ",", "10", ",", "6", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'depth=%g is a not a valid setting!'", "%", "depth", ")", "\n", "\n", "# input tensors", "\n", "", "self", ".", "input_x", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", ",", "sequence_max_length", "]", ",", "name", "=", "\"input_x\"", ")", "\n", "self", ".", "input_y", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", ",", "num_classes", "]", ",", "name", "=", "\"input_y\"", ")", "\n", "self", ".", "is_training", "=", "tf", ".", "placeholder", "(", "tf", ".", "bool", ")", "\n", "\n", "# Embedding Lookup 16", "\n", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ",", "tf", ".", "name_scope", "(", "\"embedding\"", ")", ":", "\n", "            ", "if", "use_he_uniform", ":", "\n", "                ", "self", ".", "embedding_W", "=", "tf", ".", "get_variable", "(", "name", "=", "'lookup_W'", ",", "shape", "=", "[", "num_quantized_chars", ",", "embedding_size", "]", ",", "initializer", "=", "tf", ".", "keras", ".", "initializers", ".", "he_uniform", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "embedding_W", "=", "tf", ".", "Variable", "(", "tf", ".", "random_uniform", "(", "[", "num_quantized_chars", ",", "embedding_size", "]", ",", "-", "1.0", ",", "1.0", ")", ",", "name", "=", "\"embedding_W\"", ")", "\n", "", "self", ".", "embedded_characters", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "self", ".", "embedding_W", ",", "self", ".", "input_x", ")", "\n", "print", "(", "\"-\"", "*", "20", ")", "\n", "print", "(", "\"Embedded Lookup:\"", ",", "self", ".", "embedded_characters", ".", "get_shape", "(", ")", ")", "\n", "print", "(", "\"-\"", "*", "20", ")", "\n", "\n", "", "self", ".", "layers", "=", "[", "]", "\n", "\n", "# Temp(First) Conv Layer", "\n", "with", "tf", ".", "variable_scope", "(", "\"temp_conv\"", ")", "as", "scope", ":", "\n", "            ", "filter_shape", "=", "[", "3", ",", "embedding_size", ",", "64", "]", "\n", "W", "=", "tf", ".", "get_variable", "(", "name", "=", "'W_1'", ",", "shape", "=", "filter_shape", ",", "\n", "initializer", "=", "he_normal", ",", "\n", "regularizer", "=", "regularizer", ")", "\n", "inputs", "=", "tf", ".", "nn", ".", "conv1d", "(", "self", ".", "embedded_characters", ",", "W", ",", "stride", "=", "1", ",", "padding", "=", "\"SAME\"", ")", "\n", "#inputs = tf.nn.relu(inputs)", "\n", "", "print", "(", "\"Temp Conv\"", ",", "inputs", ".", "get_shape", "(", ")", ")", "\n", "self", ".", "layers", ".", "append", "(", "inputs", ")", "\n", "\n", "# Conv Block 64", "\n", "for", "i", "in", "range", "(", "num_layers", "[", "0", "]", ")", ":", "\n", "            ", "if", "i", "<", "num_layers", "[", "0", "]", "-", "1", "and", "optional_shortcut", ":", "\n", "                ", "shortcut", "=", "self", ".", "layers", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "shortcut", "=", "None", "\n", "", "conv_block", "=", "Convolutional_Block", "(", "inputs", "=", "self", ".", "layers", "[", "-", "1", "]", ",", "shortcut", "=", "shortcut", ",", "num_filters", "=", "64", ",", "is_training", "=", "self", ".", "is_training", ",", "name", "=", "str", "(", "i", "+", "1", ")", ")", "\n", "self", ".", "layers", ".", "append", "(", "conv_block", ")", "\n", "", "pool1", "=", "downsampling", "(", "self", ".", "layers", "[", "-", "1", "]", ",", "downsampling_type", "=", "downsampling_type", ",", "name", "=", "'pool1'", ",", "optional_shortcut", "=", "optional_shortcut", ",", "shortcut", "=", "self", ".", "layers", "[", "-", "2", "]", ")", "\n", "self", ".", "layers", ".", "append", "(", "pool1", ")", "\n", "print", "(", "\"Pooling:\"", ",", "pool1", ".", "get_shape", "(", ")", ")", "\n", "\n", "# Conv Block 128", "\n", "for", "i", "in", "range", "(", "num_layers", "[", "1", "]", ")", ":", "\n", "            ", "if", "i", "<", "num_layers", "[", "1", "]", "-", "1", "and", "optional_shortcut", ":", "\n", "                ", "shortcut", "=", "self", ".", "layers", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "shortcut", "=", "None", "\n", "", "conv_block", "=", "Convolutional_Block", "(", "inputs", "=", "self", ".", "layers", "[", "-", "1", "]", ",", "shortcut", "=", "shortcut", ",", "num_filters", "=", "128", ",", "is_training", "=", "self", ".", "is_training", ",", "name", "=", "str", "(", "i", "+", "1", ")", ")", "\n", "self", ".", "layers", ".", "append", "(", "conv_block", ")", "\n", "", "pool2", "=", "downsampling", "(", "self", ".", "layers", "[", "-", "1", "]", ",", "downsampling_type", "=", "downsampling_type", ",", "name", "=", "'pool2'", ",", "optional_shortcut", "=", "optional_shortcut", ",", "shortcut", "=", "self", ".", "layers", "[", "-", "2", "]", ")", "\n", "self", ".", "layers", ".", "append", "(", "pool2", ")", "\n", "print", "(", "\"Pooling:\"", ",", "pool2", ".", "get_shape", "(", ")", ")", "\n", "\n", "# Conv Block 256", "\n", "for", "i", "in", "range", "(", "num_layers", "[", "2", "]", ")", ":", "\n", "            ", "if", "i", "<", "num_layers", "[", "2", "]", "-", "1", "and", "optional_shortcut", ":", "\n", "                ", "shortcut", "=", "self", ".", "layers", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "shortcut", "=", "None", "\n", "", "conv_block", "=", "Convolutional_Block", "(", "inputs", "=", "self", ".", "layers", "[", "-", "1", "]", ",", "shortcut", "=", "shortcut", ",", "num_filters", "=", "256", ",", "is_training", "=", "self", ".", "is_training", ",", "name", "=", "str", "(", "i", "+", "1", ")", ")", "\n", "self", ".", "layers", ".", "append", "(", "conv_block", ")", "\n", "", "pool3", "=", "downsampling", "(", "self", ".", "layers", "[", "-", "1", "]", ",", "downsampling_type", "=", "downsampling_type", ",", "name", "=", "'pool3'", ",", "optional_shortcut", "=", "optional_shortcut", ",", "shortcut", "=", "self", ".", "layers", "[", "-", "2", "]", ")", "\n", "self", ".", "layers", ".", "append", "(", "pool3", ")", "\n", "print", "(", "\"Pooling:\"", ",", "pool3", ".", "get_shape", "(", ")", ")", "\n", "\n", "# Conv Block 512", "\n", "for", "i", "in", "range", "(", "num_layers", "[", "3", "]", ")", ":", "\n", "            ", "if", "i", "<", "num_layers", "[", "3", "]", "-", "1", "and", "optional_shortcut", ":", "\n", "                ", "shortcut", "=", "self", ".", "layers", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "shortcut", "=", "None", "\n", "", "conv_block", "=", "Convolutional_Block", "(", "inputs", "=", "self", ".", "layers", "[", "-", "1", "]", ",", "shortcut", "=", "shortcut", ",", "num_filters", "=", "512", ",", "is_training", "=", "self", ".", "is_training", ",", "name", "=", "str", "(", "i", "+", "1", ")", ")", "\n", "self", ".", "layers", ".", "append", "(", "conv_block", ")", "\n", "\n", "", "'''\n        # Extract 8 most features as mentioned in paper\n        self.k_pooled = tf.nn.top_k(tf.transpose(self.layers[-1], [0,2,1]), k=8, name='k_pool', sorted=False)[0]\n        print(\"8-maxpooling:\", self.k_pooled.get_shape())\n        self.flatten = tf.reshape(self.k_pooled, (-1, 512*8))\n\n        # fc1\n        with tf.variable_scope('fc1'):\n            w = tf.get_variable('w', [self.flatten.get_shape()[1], 2048], initializer=he_normal,\n                regularizer=regularizer)\n            b = tf.get_variable('b', [2048], initializer=tf.constant_initializer(1.0))\n            out = tf.matmul(self.flatten, w) + b\n            self.fc1 = tf.nn.relu(out)\n\n        # fc2\n        with tf.variable_scope('fc2'):\n            w = tf.get_variable('w', [self.fc1.get_shape()[1], 2048], initializer=he_normal,\n                regularizer=regularizer)\n            b = tf.get_variable('b', [2048], initializer=tf.constant_initializer(1.0))\n            out = tf.matmul(self.fc1, w) + b\n            self.fc2 = tf.nn.relu(out)\n\n        '''", "\n", "self", ".", "gap", "=", "tf", ".", "reduce_mean", "(", "self", ".", "layers", "[", "-", "1", "]", ",", "axis", "=", "1", ")", "\n", "# fc3", "\n", "with", "tf", ".", "variable_scope", "(", "'fc3'", ")", ":", "\n", "            ", "w", "=", "tf", ".", "get_variable", "(", "'w'", ",", "[", "self", ".", "gap", ".", "get_shape", "(", ")", "[", "1", "]", ",", "num_classes", "]", ",", "initializer", "=", "he_normal", ",", "\n", "regularizer", "=", "regularizer", ")", "\n", "self", ".", "fc3", "=", "tf", ".", "matmul", "(", "self", ".", "gap", ",", "w", ")", "\n", "\n", "# Calculate Mean cross-entropy loss", "\n", "", "with", "tf", ".", "name_scope", "(", "\"loss\"", ")", ":", "\n", "            ", "self", ".", "predictions", "=", "tf", ".", "argmax", "(", "self", ".", "fc3", ",", "1", ",", "name", "=", "\"predictions\"", ")", "\n", "losses", "=", "tf", ".", "nn", ".", "softmax_cross_entropy_with_logits", "(", "logits", "=", "self", ".", "fc3", ",", "labels", "=", "self", ".", "input_y", ")", "\n", "regularization_losses", "=", "tf", ".", "get_collection", "(", "tf", ".", "GraphKeys", ".", "REGULARIZATION_LOSSES", ")", "\n", "self", ".", "loss", "=", "tf", ".", "reduce_mean", "(", "losses", ")", "+", "sum", "(", "regularization_losses", ")", "\n", "\n", "# Accuracy", "\n", "", "with", "tf", ".", "name_scope", "(", "\"accuracy\"", ")", ":", "\n", "            ", "correct_predictions", "=", "tf", ".", "equal", "(", "self", ".", "predictions", ",", "tf", ".", "argmax", "(", "self", ".", "input_y", ",", "1", ")", ")", "\n", "self", ".", "accuracy", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "cast", "(", "correct_predictions", ",", "\"float\"", ")", ",", "name", "=", "\"accuracy\"", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.vdcnn_cam.Convolutional_Block": [[9, 32], ["print", "print", "print", "print", "str", "tensorflow.variable_scope", "range", "print", "print", "print", "shortcut.get_shape", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.nn.conv1d", "tensorflow.nn.relu", "print", "tf.nn.relu.get_shape", "str", "str", "tf.nn.relu.get_shape"], "function", ["home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.conv1d"], ["def", "Convolutional_Block", "(", "inputs", ",", "shortcut", ",", "num_filters", ",", "name", ",", "is_training", ")", ":", "\n", "    ", "print", "(", "\"-\"", "*", "20", ")", "\n", "print", "(", "\"Convolutional Block\"", ",", "str", "(", "num_filters", ")", ",", "name", ")", "\n", "print", "(", "\"-\"", "*", "20", ")", "\n", "with", "tf", ".", "variable_scope", "(", "\"conv_block_\"", "+", "str", "(", "num_filters", ")", "+", "\"_\"", "+", "name", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "\"conv1d_%s\"", "%", "str", "(", "i", ")", ")", ":", "\n", "                ", "filter_shape", "=", "[", "3", ",", "inputs", ".", "get_shape", "(", ")", "[", "2", "]", ",", "num_filters", "]", "\n", "W", "=", "tf", ".", "get_variable", "(", "name", "=", "'W'", ",", "shape", "=", "filter_shape", ",", "\n", "initializer", "=", "he_normal", ",", "\n", "regularizer", "=", "regularizer", ")", "\n", "inputs", "=", "tf", ".", "nn", ".", "conv1d", "(", "inputs", ",", "W", ",", "stride", "=", "1", ",", "padding", "=", "\"SAME\"", ")", "\n", "#inputs = tf.layers.batch_normalization(inputs=inputs, momentum=0.997, epsilon=1e-5, ", "\n", "#                                center=True, scale=True, training=is_training)", "\n", "inputs", "=", "tf", ".", "nn", ".", "relu", "(", "inputs", ")", "\n", "print", "(", "\"Conv1D:\"", ",", "inputs", ".", "get_shape", "(", ")", ")", "\n", "", "", "", "print", "(", "\"-\"", "*", "20", ")", "\n", "if", "shortcut", "is", "not", "None", ":", "\n", "        ", "print", "(", "\"-\"", "*", "5", ")", "\n", "print", "(", "\"Optional Shortcut:\"", ",", "shortcut", ".", "get_shape", "(", ")", ")", "\n", "print", "(", "\"-\"", "*", "5", ")", "\n", "return", "inputs", "+", "shortcut", "\n", "", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.vdcnn_cam.downsampling": [[34, 57], ["vdcnn_cam.fixed_padding", "tensorflow.layers.conv1d", "math.ceil", "tensorflow.transpose", "tensorflow.layers.conv1d", "print", "print", "print", "tensorflow.nn.top_k", "tensorflow.layers.conv1d", "tensorflow.layers.max_pooling1d", "tf.layers.conv1d.get_shape", "int", "tensorflow.transpose", "tf.layers.conv1d.get_shape", "tf.layers.max_pooling1d.get_shape", "inputs.get_shape", "inputs.get_shape"], "function", ["home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.vdcnn.fixed_padding", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.conv1d", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.conv1d", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.conv1d"], ["", "def", "downsampling", "(", "inputs", ",", "downsampling_type", ",", "name", ",", "optional_shortcut", "=", "False", ",", "shortcut", "=", "None", ")", ":", "\n", "# k-maxpooling", "\n", "    ", "if", "downsampling_type", "==", "'k-maxpool'", ":", "\n", "        ", "k", "=", "math", ".", "ceil", "(", "int", "(", "inputs", ".", "get_shape", "(", ")", "[", "1", "]", ")", "/", "2", ")", "\n", "pool", "=", "tf", ".", "nn", ".", "top_k", "(", "tf", ".", "transpose", "(", "inputs", ",", "[", "0", ",", "2", ",", "1", "]", ")", ",", "k", "=", "k", ",", "name", "=", "name", ",", "sorted", "=", "False", ")", "[", "0", "]", "\n", "pool", "=", "tf", ".", "transpose", "(", "pool", ",", "[", "0", ",", "2", ",", "1", "]", ")", "\n", "# Linear", "\n", "", "elif", "downsampling_type", "==", "'linear'", ":", "\n", "        ", "pool", "=", "tf", ".", "layers", ".", "conv1d", "(", "inputs", "=", "inputs", ",", "filters", "=", "inputs", ".", "get_shape", "(", ")", "[", "2", "]", ",", "kernel_size", "=", "3", ",", "\n", "strides", "=", "2", ",", "padding", "=", "'same'", ",", "use_bias", "=", "False", ")", "\n", "# Maxpooling", "\n", "", "else", ":", "\n", "        ", "pool", "=", "tf", ".", "layers", ".", "max_pooling1d", "(", "inputs", "=", "inputs", ",", "pool_size", "=", "3", ",", "strides", "=", "2", ",", "padding", "=", "'same'", ",", "name", "=", "name", ")", "\n", "", "if", "optional_shortcut", ":", "\n", "        ", "shortcut", "=", "tf", ".", "layers", ".", "conv1d", "(", "inputs", "=", "shortcut", ",", "filters", "=", "shortcut", ".", "get_shape", "(", ")", "[", "2", "]", ",", "kernel_size", "=", "1", ",", "\n", "strides", "=", "2", ",", "padding", "=", "'same'", ",", "use_bias", "=", "False", ")", "\n", "print", "(", "\"-\"", "*", "5", ")", "\n", "print", "(", "\"Optional Shortcut:\"", ",", "shortcut", ".", "get_shape", "(", ")", ")", "\n", "print", "(", "\"-\"", "*", "5", ")", "\n", "pool", "+=", "shortcut", "\n", "", "pool", "=", "fixed_padding", "(", "inputs", "=", "pool", ")", "\n", "return", "tf", ".", "layers", ".", "conv1d", "(", "inputs", "=", "pool", ",", "filters", "=", "pool", ".", "get_shape", "(", ")", "[", "2", "]", "*", "2", ",", "kernel_size", "=", "1", ",", "\n", "strides", "=", "1", ",", "padding", "=", "'valid'", ",", "use_bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.vdcnn_cam.fixed_padding": [[58, 64], ["tensorflow.pad"], "function", ["None"], ["", "def", "fixed_padding", "(", "inputs", ",", "kernel_size", "=", "3", ")", ":", "\n", "    ", "pad_total", "=", "kernel_size", "-", "1", "\n", "pad_beg", "=", "pad_total", "//", "2", "\n", "pad_end", "=", "pad_total", "-", "pad_beg", "\n", "padded_inputs", "=", "tf", ".", "pad", "(", "inputs", ",", "[", "[", "0", ",", "0", "]", ",", "[", "pad_beg", ",", "pad_end", "]", ",", "[", "0", ",", "0", "]", "]", ")", "\n", "return", "padded_inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.vdcnn.VDCNN.__init__": [[55, 175], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "vdcnn.VDCNN.layers.append", "range", "vdcnn.downsampling", "vdcnn.VDCNN.layers.append", "range", "vdcnn.downsampling", "vdcnn.VDCNN.layers.append", "range", "vdcnn.downsampling", "vdcnn.VDCNN.layers.append", "range", "tensorflow.reshape", "tensorflow.device", "tensorflow.name_scope", "tensorflow.nn.embedding_lookup", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.nn.conv1d", "vdcnn.Convolutional_Block", "vdcnn.VDCNN.layers.append", "vdcnn.Convolutional_Block", "vdcnn.VDCNN.layers.append", "vdcnn.Convolutional_Block", "vdcnn.VDCNN.layers.append", "vdcnn.Convolutional_Block", "vdcnn.VDCNN.layers.append", "tensorflow.nn.top_k", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.relu", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.relu", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.name_scope", "tensorflow.argmax", "tensorflow.nn.softmax_cross_entropy_with_logits", "tensorflow.get_collection", "tensorflow.name_scope", "tensorflow.equal", "tensorflow.reduce_mean", "tensorflow.get_variable", "tensorflow.Variable", "tensorflow.transpose", "tensorflow.matmul", "tensorflow.matmul", "tensorflow.matmul", "tensorflow.reduce_mean", "sum", "tensorflow.argmax", "tensorflow.cast", "tensorflow.random_uniform", "str", "str", "str", "str", "tensorflow.constant_initializer", "tensorflow.constant_initializer", "tensorflow.constant_initializer", "ValueError", "tensorflow.keras.initializers.he_uniform", "vdcnn.VDCNN.flatten.get_shape", "vdcnn.VDCNN.fc1.get_shape", "vdcnn.VDCNN.fc2.get_shape"], "methods", ["home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.vdcnn.downsampling", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.vdcnn.downsampling", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.vdcnn.downsampling", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.conv1d", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.vdcnn.Convolutional_Block", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.vdcnn.Convolutional_Block", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.vdcnn.Convolutional_Block", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.vdcnn.Convolutional_Block"], ["    ", "def", "__init__", "(", "self", ",", "num_classes", ",", "sequence_max_length", "=", "1024", ",", "num_quantized_chars", "=", "69", ",", "embedding_size", "=", "16", ",", "\n", "depth", "=", "9", ",", "downsampling_type", "=", "'maxpool'", ",", "use_he_uniform", "=", "True", ",", "optional_shortcut", "=", "False", ")", ":", "\n", "\n", "# Depth to No. Layers", "\n", "        ", "if", "depth", "==", "9", ":", "\n", "            ", "num_layers", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", "\n", "", "elif", "depth", "==", "17", ":", "\n", "            ", "num_layers", "=", "[", "4", ",", "4", ",", "4", ",", "4", "]", "\n", "", "elif", "depth", "==", "29", ":", "\n", "            ", "num_layers", "=", "[", "10", ",", "10", ",", "4", ",", "4", "]", "\n", "", "elif", "depth", "==", "49", ":", "\n", "            ", "num_layers", "=", "[", "16", ",", "16", ",", "10", ",", "6", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'depth=%g is a not a valid setting!'", "%", "depth", ")", "\n", "\n", "# input tensors", "\n", "", "self", ".", "input_x", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", ",", "sequence_max_length", "]", ",", "name", "=", "\"input_x\"", ")", "\n", "self", ".", "input_y", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", ",", "num_classes", "]", ",", "name", "=", "\"input_y\"", ")", "\n", "self", ".", "is_training", "=", "tf", ".", "placeholder", "(", "tf", ".", "bool", ")", "\n", "\n", "# Embedding Lookup 16", "\n", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ",", "tf", ".", "name_scope", "(", "\"embedding\"", ")", ":", "\n", "            ", "if", "use_he_uniform", ":", "\n", "                ", "self", ".", "embedding_W", "=", "tf", ".", "get_variable", "(", "name", "=", "'lookup_W'", ",", "shape", "=", "[", "num_quantized_chars", ",", "embedding_size", "]", ",", "initializer", "=", "tf", ".", "keras", ".", "initializers", ".", "he_uniform", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "embedding_W", "=", "tf", ".", "Variable", "(", "tf", ".", "random_uniform", "(", "[", "num_quantized_chars", ",", "embedding_size", "]", ",", "-", "1.0", ",", "1.0", ")", ",", "name", "=", "\"embedding_W\"", ")", "\n", "", "self", ".", "embedded_characters", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "self", ".", "embedding_W", ",", "self", ".", "input_x", ")", "\n", "\n", "", "self", ".", "layers", "=", "[", "]", "\n", "\n", "# Temp(First) Conv Layer", "\n", "with", "tf", ".", "variable_scope", "(", "\"temp_conv\"", ")", "as", "scope", ":", "\n", "            ", "filter_shape", "=", "[", "3", ",", "embedding_size", ",", "64", "]", "\n", "W", "=", "tf", ".", "get_variable", "(", "name", "=", "'W_1'", ",", "shape", "=", "filter_shape", ",", "\n", "initializer", "=", "he_normal", ",", "\n", "regularizer", "=", "regularizer", ")", "\n", "inputs", "=", "tf", ".", "nn", ".", "conv1d", "(", "self", ".", "embedded_characters", ",", "W", ",", "stride", "=", "1", ",", "padding", "=", "\"SAME\"", ")", "\n", "#inputs = tf.nn.relu(inputs)", "\n", "", "self", ".", "layers", ".", "append", "(", "inputs", ")", "\n", "\n", "# Conv Block 64", "\n", "for", "i", "in", "range", "(", "num_layers", "[", "0", "]", ")", ":", "\n", "            ", "if", "i", "<", "num_layers", "[", "0", "]", "-", "1", "and", "optional_shortcut", ":", "\n", "                ", "shortcut", "=", "self", ".", "layers", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "shortcut", "=", "None", "\n", "", "conv_block", "=", "Convolutional_Block", "(", "inputs", "=", "self", ".", "layers", "[", "-", "1", "]", ",", "shortcut", "=", "shortcut", ",", "num_filters", "=", "64", ",", "is_training", "=", "self", ".", "is_training", ",", "name", "=", "str", "(", "i", "+", "1", ")", ")", "\n", "self", ".", "layers", ".", "append", "(", "conv_block", ")", "\n", "", "pool1", "=", "downsampling", "(", "self", ".", "layers", "[", "-", "1", "]", ",", "downsampling_type", "=", "downsampling_type", ",", "name", "=", "'pool1'", ",", "optional_shortcut", "=", "optional_shortcut", ",", "shortcut", "=", "self", ".", "layers", "[", "-", "2", "]", ")", "\n", "self", ".", "layers", ".", "append", "(", "pool1", ")", "\n", "\n", "# Conv Block 128", "\n", "for", "i", "in", "range", "(", "num_layers", "[", "1", "]", ")", ":", "\n", "            ", "if", "i", "<", "num_layers", "[", "1", "]", "-", "1", "and", "optional_shortcut", ":", "\n", "                ", "shortcut", "=", "self", ".", "layers", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "shortcut", "=", "None", "\n", "", "conv_block", "=", "Convolutional_Block", "(", "inputs", "=", "self", ".", "layers", "[", "-", "1", "]", ",", "shortcut", "=", "shortcut", ",", "num_filters", "=", "128", ",", "is_training", "=", "self", ".", "is_training", ",", "name", "=", "str", "(", "i", "+", "1", ")", ")", "\n", "self", ".", "layers", ".", "append", "(", "conv_block", ")", "\n", "", "pool2", "=", "downsampling", "(", "self", ".", "layers", "[", "-", "1", "]", ",", "downsampling_type", "=", "downsampling_type", ",", "name", "=", "'pool2'", ",", "optional_shortcut", "=", "optional_shortcut", ",", "shortcut", "=", "self", ".", "layers", "[", "-", "2", "]", ")", "\n", "self", ".", "layers", ".", "append", "(", "pool2", ")", "\n", "\n", "# Conv Block 256", "\n", "for", "i", "in", "range", "(", "num_layers", "[", "2", "]", ")", ":", "\n", "            ", "if", "i", "<", "num_layers", "[", "2", "]", "-", "1", "and", "optional_shortcut", ":", "\n", "                ", "shortcut", "=", "self", ".", "layers", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "shortcut", "=", "None", "\n", "", "conv_block", "=", "Convolutional_Block", "(", "inputs", "=", "self", ".", "layers", "[", "-", "1", "]", ",", "shortcut", "=", "shortcut", ",", "num_filters", "=", "256", ",", "is_training", "=", "self", ".", "is_training", ",", "name", "=", "str", "(", "i", "+", "1", ")", ")", "\n", "self", ".", "layers", ".", "append", "(", "conv_block", ")", "\n", "", "pool3", "=", "downsampling", "(", "self", ".", "layers", "[", "-", "1", "]", ",", "downsampling_type", "=", "downsampling_type", ",", "name", "=", "'pool3'", ",", "optional_shortcut", "=", "optional_shortcut", ",", "shortcut", "=", "self", ".", "layers", "[", "-", "2", "]", ")", "\n", "self", ".", "layers", ".", "append", "(", "pool3", ")", "\n", "\n", "# Conv Block 512", "\n", "for", "i", "in", "range", "(", "num_layers", "[", "3", "]", ")", ":", "\n", "            ", "if", "i", "<", "num_layers", "[", "3", "]", "-", "1", "and", "optional_shortcut", ":", "\n", "                ", "shortcut", "=", "self", ".", "layers", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "shortcut", "=", "None", "\n", "", "conv_block", "=", "Convolutional_Block", "(", "inputs", "=", "self", ".", "layers", "[", "-", "1", "]", ",", "shortcut", "=", "shortcut", ",", "num_filters", "=", "512", ",", "is_training", "=", "self", ".", "is_training", ",", "name", "=", "str", "(", "i", "+", "1", ")", ")", "\n", "self", ".", "layers", ".", "append", "(", "conv_block", ")", "\n", "\n", "# Extract 8 most features as mentioned in paper", "\n", "", "self", ".", "k_pooled", "=", "tf", ".", "nn", ".", "top_k", "(", "tf", ".", "transpose", "(", "self", ".", "layers", "[", "-", "1", "]", ",", "[", "0", ",", "2", ",", "1", "]", ")", ",", "k", "=", "8", ",", "name", "=", "'k_pool'", ",", "sorted", "=", "False", ")", "[", "0", "]", "\n", "self", ".", "flatten", "=", "tf", ".", "reshape", "(", "self", ".", "k_pooled", ",", "(", "-", "1", ",", "512", "*", "8", ")", ")", "\n", "\n", "# fc1", "\n", "with", "tf", ".", "variable_scope", "(", "'fc1'", ")", ":", "\n", "            ", "w", "=", "tf", ".", "get_variable", "(", "'w'", ",", "[", "self", ".", "flatten", ".", "get_shape", "(", ")", "[", "1", "]", ",", "2048", "]", ",", "initializer", "=", "he_normal", ",", "\n", "regularizer", "=", "regularizer", ")", "\n", "b", "=", "tf", ".", "get_variable", "(", "'b'", ",", "[", "2048", "]", ",", "initializer", "=", "tf", ".", "constant_initializer", "(", "1.0", ")", ")", "\n", "out", "=", "tf", ".", "matmul", "(", "self", ".", "flatten", ",", "w", ")", "+", "b", "\n", "self", ".", "fc1", "=", "tf", ".", "nn", ".", "relu", "(", "out", ")", "\n", "\n", "# fc2", "\n", "", "with", "tf", ".", "variable_scope", "(", "'fc2'", ")", ":", "\n", "            ", "w", "=", "tf", ".", "get_variable", "(", "'w'", ",", "[", "self", ".", "fc1", ".", "get_shape", "(", ")", "[", "1", "]", ",", "2048", "]", ",", "initializer", "=", "he_normal", ",", "\n", "regularizer", "=", "regularizer", ")", "\n", "b", "=", "tf", ".", "get_variable", "(", "'b'", ",", "[", "2048", "]", ",", "initializer", "=", "tf", ".", "constant_initializer", "(", "1.0", ")", ")", "\n", "out", "=", "tf", ".", "matmul", "(", "self", ".", "fc1", ",", "w", ")", "+", "b", "\n", "self", ".", "fc2", "=", "tf", ".", "nn", ".", "relu", "(", "out", ")", "\n", "\n", "# fc3", "\n", "", "with", "tf", ".", "variable_scope", "(", "'fc3'", ")", ":", "\n", "            ", "w", "=", "tf", ".", "get_variable", "(", "'w'", ",", "[", "self", ".", "fc2", ".", "get_shape", "(", ")", "[", "1", "]", ",", "num_classes", "]", ",", "initializer", "=", "he_normal", ",", "\n", "regularizer", "=", "regularizer", ")", "\n", "b", "=", "tf", ".", "get_variable", "(", "'b'", ",", "[", "num_classes", "]", ",", "initializer", "=", "tf", ".", "constant_initializer", "(", "1.0", ")", ")", "\n", "self", ".", "fc3", "=", "tf", ".", "matmul", "(", "self", ".", "fc2", ",", "w", ")", "+", "b", "\n", "\n", "# Calculate Mean cross-entropy loss", "\n", "", "with", "tf", ".", "name_scope", "(", "\"loss\"", ")", ":", "\n", "            ", "self", ".", "predictions", "=", "tf", ".", "argmax", "(", "self", ".", "fc3", ",", "1", ",", "name", "=", "\"predictions\"", ")", "\n", "losses", "=", "tf", ".", "nn", ".", "softmax_cross_entropy_with_logits", "(", "logits", "=", "self", ".", "fc3", ",", "labels", "=", "self", ".", "input_y", ")", "\n", "regularization_losses", "=", "tf", ".", "get_collection", "(", "tf", ".", "GraphKeys", ".", "REGULARIZATION_LOSSES", ")", "\n", "self", ".", "loss", "=", "tf", ".", "reduce_mean", "(", "losses", ")", "+", "sum", "(", "regularization_losses", ")", "\n", "\n", "# Accuracy", "\n", "", "with", "tf", ".", "name_scope", "(", "\"accuracy\"", ")", ":", "\n", "            ", "correct_predictions", "=", "tf", ".", "equal", "(", "self", ".", "predictions", ",", "tf", ".", "argmax", "(", "self", ".", "input_y", ",", "1", ")", ")", "\n", "self", ".", "accuracy", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "cast", "(", "correct_predictions", ",", "\"float\"", ")", ",", "name", "=", "\"accuracy\"", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.vdcnn.Convolutional_Block": [[9, 24], ["tensorflow.variable_scope", "range", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.nn.conv1d", "tensorflow.nn.relu", "str", "str", "tf.nn.relu.get_shape"], "function", ["home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.conv1d"], ["def", "Convolutional_Block", "(", "inputs", ",", "shortcut", ",", "num_filters", ",", "name", ",", "is_training", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "\"conv_block_\"", "+", "str", "(", "num_filters", ")", "+", "\"_\"", "+", "name", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "\"conv1d_%s\"", "%", "str", "(", "i", ")", ")", ":", "\n", "                ", "filter_shape", "=", "[", "3", ",", "inputs", ".", "get_shape", "(", ")", "[", "2", "]", ",", "num_filters", "]", "\n", "W", "=", "tf", ".", "get_variable", "(", "name", "=", "'W'", ",", "shape", "=", "filter_shape", ",", "\n", "initializer", "=", "he_normal", ",", "\n", "regularizer", "=", "regularizer", ")", "\n", "inputs", "=", "tf", ".", "nn", ".", "conv1d", "(", "inputs", ",", "W", ",", "stride", "=", "1", ",", "padding", "=", "\"SAME\"", ")", "\n", "#inputs = tf.layers.batch_normalization(inputs=inputs, momentum=0.997, epsilon=1e-5, ", "\n", "#                                center=True, scale=True, training=is_training)", "\n", "inputs", "=", "tf", ".", "nn", ".", "relu", "(", "inputs", ")", "\n", "", "", "", "if", "shortcut", "is", "not", "None", ":", "\n", "        ", "return", "inputs", "+", "shortcut", "\n", "", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.vdcnn.downsampling": [[26, 46], ["vdcnn.fixed_padding", "tensorflow.layers.conv1d", "math.ceil", "tensorflow.transpose", "tensorflow.layers.conv1d", "tensorflow.nn.top_k", "tensorflow.layers.conv1d", "tensorflow.layers.max_pooling1d", "int", "tensorflow.transpose", "tf.layers.conv1d.get_shape", "tf.layers.max_pooling1d.get_shape", "inputs.get_shape", "inputs.get_shape"], "function", ["home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.vdcnn.fixed_padding", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.conv1d", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.conv1d", "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.ByteNet.ops.conv1d"], ["", "def", "downsampling", "(", "inputs", ",", "downsampling_type", ",", "name", ",", "optional_shortcut", "=", "False", ",", "shortcut", "=", "None", ")", ":", "\n", "# k-maxpooling", "\n", "    ", "if", "downsampling_type", "==", "'k-maxpool'", ":", "\n", "        ", "k", "=", "math", ".", "ceil", "(", "int", "(", "inputs", ".", "get_shape", "(", ")", "[", "1", "]", ")", "/", "2", ")", "\n", "pool", "=", "tf", ".", "nn", ".", "top_k", "(", "tf", ".", "transpose", "(", "inputs", ",", "[", "0", ",", "2", ",", "1", "]", ")", ",", "k", "=", "k", ",", "name", "=", "name", ",", "sorted", "=", "False", ")", "[", "0", "]", "\n", "pool", "=", "tf", ".", "transpose", "(", "pool", ",", "[", "0", ",", "2", ",", "1", "]", ")", "\n", "# Linear", "\n", "", "elif", "downsampling_type", "==", "'linear'", ":", "\n", "        ", "pool", "=", "tf", ".", "layers", ".", "conv1d", "(", "inputs", "=", "inputs", ",", "filters", "=", "inputs", ".", "get_shape", "(", ")", "[", "2", "]", ",", "kernel_size", "=", "3", ",", "\n", "strides", "=", "2", ",", "padding", "=", "'same'", ",", "use_bias", "=", "False", ")", "\n", "# Maxpooling", "\n", "", "else", ":", "\n", "        ", "pool", "=", "tf", ".", "layers", ".", "max_pooling1d", "(", "inputs", "=", "inputs", ",", "pool_size", "=", "3", ",", "strides", "=", "2", ",", "padding", "=", "'same'", ",", "name", "=", "name", ")", "\n", "", "if", "optional_shortcut", ":", "\n", "        ", "shortcut", "=", "tf", ".", "layers", ".", "conv1d", "(", "inputs", "=", "shortcut", ",", "filters", "=", "shortcut", ".", "get_shape", "(", ")", "[", "2", "]", ",", "kernel_size", "=", "1", ",", "\n", "strides", "=", "2", ",", "padding", "=", "'same'", ",", "use_bias", "=", "False", ")", "\n", "pool", "+=", "shortcut", "\n", "", "pool", "=", "fixed_padding", "(", "inputs", "=", "pool", ")", "\n", "return", "tf", ".", "layers", ".", "conv1d", "(", "inputs", "=", "pool", ",", "filters", "=", "pool", ".", "get_shape", "(", ")", "[", "2", "]", "*", "2", ",", "kernel_size", "=", "1", ",", "\n", "strides", "=", "1", ",", "padding", "=", "'valid'", ",", "use_bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.vdcnn.fixed_padding": [[47, 53], ["tensorflow.pad"], "function", ["None"], ["", "def", "fixed_padding", "(", "inputs", ",", "kernel_size", "=", "3", ")", ":", "\n", "    ", "pad_total", "=", "kernel_size", "-", "1", "\n", "pad_beg", "=", "pad_total", "//", "2", "\n", "pad_end", "=", "pad_total", "-", "pad_beg", "\n", "padded_inputs", "=", "tf", ".", "pad", "(", "inputs", ",", "[", "[", "0", ",", "0", "]", ",", "[", "pad_beg", ",", "pad_end", "]", ",", "[", "0", ",", "0", "]", "]", ")", "\n", "return", "padded_inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.train_cam.train_step": [[67, 79], ["sess.run", "datetime.datetime.now().isoformat", "print", "datetime.datetime.now", "int"], "function", ["None"], ["def", "train_step", "(", "x_batch", ",", "y_batch", ")", ":", "\n", "    ", "\"\"\"\n    A single training step\n    \"\"\"", "\n", "feed_dict", "=", "{", "cnn", ".", "input_x", ":", "x_batch", ",", "\n", "cnn", ".", "input_y", ":", "y_batch", ",", "\n", "cnn", ".", "is_training", ":", "True", "}", "\n", "_", ",", "step", ",", "loss", ",", "accuracy", "=", "sess", ".", "run", "(", "[", "train_op", ",", "global_step", ",", "cnn", ".", "loss", ",", "cnn", ".", "accuracy", "]", ",", "feed_dict", ")", "\n", "time_str", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "isoformat", "(", ")", "\n", "\n", "if", "step", "%", "100", "==", "0", ":", "\n", "        ", "print", "(", "\"{}: Step {}, Epoch {}, Loss {:g}, Acc {:g}\"", ".", "format", "(", "time_str", ",", "step", ",", "int", "(", "step", "//", "num_batches_per_epoch", ")", "+", "1", ",", "loss", ",", "accuracy", ")", ")", "\n", "#if step%FLAGS.evaluate_every == 0 and FLAGS.enable_tensorboard:", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.train_cam.test_step": [[83, 93], ["sess.run", "datetime.datetime.now().isoformat", "datetime.datetime.now"], "function", ["None"], ["", "", "def", "test_step", "(", "x_batch", ",", "y_batch", ")", ":", "\n", "\t", "\"\"\"\n\tEvaluates model on a dev set\n\t\"\"\"", "\n", "feed_dict", "=", "{", "cnn", ".", "input_x", ":", "x_batch", ",", "\n", "cnn", ".", "input_y", ":", "y_batch", ",", "\n", "cnn", ".", "is_training", ":", "False", "}", "\n", "loss", ",", "preds", "=", "sess", ".", "run", "(", "[", "cnn", ".", "loss", ",", "cnn", ".", "predictions", "]", ",", "feed_dict", ")", "\n", "time_str", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "isoformat", "(", ")", "\n", "return", "preds", ",", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.train.train_step": [[71, 83], ["sess.run", "datetime.datetime.now().isoformat", "print", "datetime.datetime.now", "int"], "function", ["None"], ["def", "train_step", "(", "x_batch", ",", "y_batch", ")", ":", "\n", "    ", "\"\"\"\n    A single training step\n    \"\"\"", "\n", "feed_dict", "=", "{", "cnn", ".", "input_x", ":", "x_batch", ",", "\n", "cnn", ".", "input_y", ":", "y_batch", ",", "\n", "cnn", ".", "is_training", ":", "True", "}", "\n", "_", ",", "step", ",", "loss", ",", "accuracy", "=", "sess", ".", "run", "(", "[", "train_op", ",", "global_step", ",", "cnn", ".", "loss", ",", "cnn", ".", "accuracy", "]", ",", "feed_dict", ")", "\n", "time_str", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "isoformat", "(", ")", "\n", "\n", "if", "step", "%", "100", "==", "0", ":", "\n", "        ", "print", "(", "\"{}: Step {}, Epoch {}, Loss {:g}, Acc {:g}\"", ".", "format", "(", "time_str", ",", "step", ",", "int", "(", "step", "//", "num_batches_per_epoch", ")", "+", "1", ",", "loss", ",", "accuracy", ")", ")", "\n", "#if step%FLAGS.evaluate_every == 0 and FLAGS.enable_tensorboard:", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.train.test_step": [[87, 97], ["sess.run", "datetime.datetime.now().isoformat", "datetime.datetime.now"], "function", ["None"], ["", "", "def", "test_step", "(", "x_batch", ",", "y_batch", ")", ":", "\n", "\t", "\"\"\"\n\tEvaluates model on a dev set\n\t\"\"\"", "\n", "feed_dict", "=", "{", "cnn", ".", "input_x", ":", "x_batch", ",", "\n", "cnn", ".", "input_y", ":", "y_batch", ",", "\n", "cnn", ".", "is_training", ":", "False", "}", "\n", "loss", ",", "preds", "=", "sess", ".", "run", "(", "[", "cnn", ".", "loss", ",", "cnn", ".", "predictions", "]", ",", "feed_dict", ")", "\n", "time_str", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "isoformat", "(", ")", "\n", "return", "preds", ",", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.classifier_protocol.VeryDeepCNN.__init__": [[11, 65], ["os.path.join", "vdcnn.VDCNN", "data_helper.data_helper.data_helper", "tensorflow.Session", "tensorflow.train.Saver", "tensorflow.train.Saver.restore", "range", "os.path.join", "os.path.join", "os.path.join", "tensorflow.get_default_graph().get_tensor_by_name", "os.path.realpath().split", "tensorflow.get_default_graph", "os.path.realpath"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "task", "=", "'ag'", ")", ":", "\n", "        ", "sequence_max_length", "=", "200", "\n", "downsampling_type", "=", "'maxpool'", "\n", "depth", "=", "9", "\n", "num_layers", "=", "4", "\n", "use_he_uniform", "=", "True", "\n", "optional_shortcut", "=", "True", "\n", "\n", "current_path", "=", "'/'", ".", "join", "(", "os", ".", "path", ".", "realpath", "(", "__file__", ")", ".", "split", "(", "'/'", ")", "[", ":", "-", "1", "]", ")", "\n", "model_root", "=", "opj", "(", "current_path", ",", "'pretrained_models'", ")", "\n", "\n", "model_path", "=", "{", "\n", "'ag'", ":", "opj", "(", "model_root", ",", "'ag'", ",", "'model-step92000.ckpt'", ")", ",", "\n", "'yelp'", ":", "opj", "(", "model_root", ",", "'yelp'", ",", "'model-step282000.ckpt'", ")", ",", "\n", "'ag-cam'", ":", "'pretrained_models/ag-cam/model-step10000.ckpt'", ",", "\n", "'yelp-cam'", ":", "'pretrained_models/yelp-cam/model-step54000.ckpt'", ",", "\n", "'dbpedia'", ":", "opj", "(", "model_root", ",", "'dbpedia'", ",", "'model-step40000.ckpt'", ")", "\n", "}", "\n", "\n", "num_classes", "=", "{", "\n", "'ag'", ":", "4", ",", "\n", "'yelp'", ":", "2", ",", "\n", "'ag-cam'", ":", "4", ",", "\n", "'yelp-cam'", ":", "2", ",", "\n", "'dbpedia'", ":", "14", "\n", "}", "\n", "\n", "self", ".", "cnn", "=", "VDCNN", "(", "\n", "num_classes", "=", "num_classes", "[", "task", "]", ",", "\n", "depth", "=", "depth", ",", "\n", "sequence_max_length", "=", "sequence_max_length", ",", "\n", "use_he_uniform", "=", "use_he_uniform", ",", "\n", "optional_shortcut", "=", "optional_shortcut", "\n", ")", "\n", "\n", "self", ".", "data_helper", "=", "data_helper", "(", "\n", "sequence_max_length", "=", "sequence_max_length", "\n", ")", "\n", "\n", "self", ".", "sess", "=", "tf", ".", "Session", "(", ")", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "\n", "saver", ".", "restore", "(", "self", ".", "sess", ",", "model_path", "[", "task", "]", ")", "\n", "\n", "self", ".", "features", "=", "{", "}", "\n", "for", "layer_index", "in", "range", "(", "num_layers", ")", ":", "\n", "            ", "if", "layer_index", "==", "0", ":", "\n", "                ", "layer_tensor_name", "=", "'add:0'", "\n", "", "else", ":", "\n", "                ", "layer_tensor_name", "=", "'add_%d:0'", "%", "(", "layer_index", "*", "2", ")", "\n", "\n", "", "layer_name", "=", "'conv_%d'", "%", "layer_index", "\n", "layer_tensor", "=", "tf", ".", "get_default_graph", "(", ")", ".", "get_tensor_by_name", "(", "layer_tensor_name", ")", "\n", "\n", "self", ".", "features", "[", "layer_name", "]", "=", "layer_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.classifier_protocol.VeryDeepCNN.make_feed": [[66, 76], ["classifier_protocol.VeryDeepCNN.data_helper.char2vec", "numpy.expand_dims", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.data_helper.data_helper.char2vec"], ["", "", "def", "make_feed", "(", "self", ",", "x", ")", ":", "\n", "        ", "feed_text", "=", "self", ".", "data_helper", ".", "char2vec", "(", "x", ")", "\n", "feed_text", "=", "np", ".", "expand_dims", "(", "np", ".", "array", "(", "feed_text", ",", "dtype", "=", "'int32'", ")", ",", "axis", "=", "0", ")", "\n", "\n", "feed_dict", "=", "{", "\n", "self", ".", "cnn", ".", "input_x", ":", "feed_text", ",", "\n", "self", ".", "cnn", ".", "is_training", ":", "False", "\n", "}", "\n", "\n", "return", "feed_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.classifier_protocol.VeryDeepCNN.forward": [[77, 89], ["classifier_protocol.VeryDeepCNN.make_feed", "classifier_protocol.VeryDeepCNN.sess.run"], "methods", ["home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.classifier_protocol.VeryDeepCNN.make_feed"], ["", "def", "forward", "(", "self", ",", "layer_name", ",", "x", ")", ":", "\n", "        ", "feed", "=", "self", ".", "make_feed", "(", "x", ")", "\n", "\n", "activation", "=", "self", ".", "sess", ".", "run", "(", "\n", "self", ".", "features", "[", "layer_name", "]", ",", "\n", "feed_dict", "=", "feed", "\n", ")", "\n", "\n", "activation", "=", "activation", "[", "0", "]", "\n", "\n", "#activation = np.average(activation[0, :len(x), :], )", "\n", "return", "activation", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.classifier_protocol.VeryDeepCNN.get_loss": [[90, 103], ["numpy.expand_dims", "numpy.expand_dims", "classifier_protocol.VeryDeepCNN.sess.run"], "methods", ["None"], ["", "def", "get_loss", "(", "self", ",", "x", ",", "label", ")", ":", "\n", "        ", "x", "=", "np", ".", "expand_dims", "(", "x", ",", "axis", "=", "0", ")", "\n", "label", "=", "np", ".", "expand_dims", "(", "label", ",", "axis", "=", "0", ")", "\n", "loss", "=", "self", ".", "sess", ".", "run", "(", "\n", "self", ".", "cnn", ".", "loss", ",", "\n", "feed_dict", "=", "{", "\n", "self", ".", "cnn", ".", "input_x", ":", "x", ",", "\n", "self", ".", "cnn", ".", "input_y", ":", "label", ",", "\n", "self", ".", "cnn", ".", "is_training", ":", "False", "\n", "}", "\n", ")", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.classifier_protocol.VeryDeepCNN.get_grad": [[105, 127], ["classifier_protocol.VeryDeepCNN.make_feed", "tensorflow.gradients", "classifier_protocol.VeryDeepCNN.sess.run", "numpy.average", "classifier_protocol.VeryDeepCNN.sess.run"], "methods", ["home.repos.pwc.inspect_result.seilna_CNN-Units-in-NLP.vdcnn.classifier_protocol.VeryDeepCNN.make_feed"], ["", "def", "get_grad", "(", "self", ",", "layer_name", ",", "x", ")", ":", "\n", "        ", "feed", "=", "self", ".", "make_feed", "(", "x", ")", "\n", "\n", "# TODO: replace target to Ground-Truth", "\n", "prediction", "=", "self", ".", "sess", ".", "run", "(", "\n", "self", ".", "cnn", ".", "predictions", ",", "\n", "feed_dict", "=", "feed", "\n", ")", "[", "0", "]", "\n", "\n", "grad_tensor", "=", "tf", ".", "gradients", "(", "\n", "ys", "=", "self", ".", "cnn", ".", "fc3", "[", "0", ",", "prediction", "]", ",", "\n", "xs", "=", "self", ".", "features", "[", "layer_name", "]", "\n", ")", "\n", "\n", "\n", "grad", "=", "self", ".", "sess", ".", "run", "(", "\n", "grad_tensor", ",", "\n", "feed_dict", "=", "feed", ")", "\n", "\n", "grad_per_units", "=", "np", ".", "average", "(", "grad", "[", "0", "]", "[", "0", "]", ",", "axis", "=", "0", ")", "\n", "\n", "return", "prediction", ",", "grad_per_units", "\n", "\n"]]}