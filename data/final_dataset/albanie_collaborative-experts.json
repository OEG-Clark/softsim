{"home.repos.pwc.inspect_result.albanie_collaborative-experts.None.train.run_exp": [[32, 174], ["warnings.filterwarnings", "config.get_logger", "pathlib.Path().parent.mkdir", "utils.util.compute_dims", "utils.util.compute_trn_config", "socket.gethostname().endswith", "enumerate", "print", "print", "open", "print", "os.system", "time.time", "config.get_logger.info", "utils.set_seeds", "config.init", "config.get_logger.info", "config.init", "config.get", "config.init", "filter", "utils.util.update_src_web_video_dir", "config.init", "trainer.Trainer", "trainer.Trainer.train", "time.strftime", "config.get_logger.info", "config._config.get", "len", "logger.log_parser.log_summary", "int", "int", "socket.gethostname", "str", "config.get_logger.info", "config.init.apply", "getattr", "config.init.parameters", "config.init", "config.init", "config.init", "time.gmtime", "copy.deepcopy", "mergedeep.merge", "test.evaluation", "pathlib.Path", "config._args.seeds.split", "[].get", "config.get", "config[].get", "[].get", "config.get", "config[].get", "config.get", "config.get", "config.get", "config.get", "config[].get", "isinstance", "config.init", "config[].get", "config.get", "config.get", "config.get", "config[].get", "config[].get", "set", "str", "pathlib.Path.home", "len", "torch.nn.init.xavier_uniform", "torch.nn.init.xavier_uniform", "m.bias.data.fill_", "config.init", "config.init", "config.get", "time.time"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get_logger", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdir", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.compute_dims", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.compute_trn_config", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.set_seeds", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.init", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.init", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.init", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.update_src_web_video_dir", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.init", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.train", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.logger.log_parser.log_summary", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.init", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.init", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.init", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.test.evaluation", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.init", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.init", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.init", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get"], ["def", "run_exp", "(", "config", ")", ":", "\n", "    ", "warnings", ".", "filterwarnings", "(", "'ignore'", ")", "\n", "logger", "=", "config", ".", "get_logger", "(", "'train'", ")", "\n", "\n", "leaderboard_path", "=", "config", ".", "_args", ".", "leaderboard", "\n", "Path", "(", "leaderboard_path", ")", ".", "parent", ".", "mkdir", "(", "exist_ok", "=", "True", ",", "parents", "=", "True", ")", "\n", "with", "open", "(", "leaderboard_path", ",", "'a'", ")", "as", "f", ":", "\n", "        ", "txt_path", "=", "f\"{config._log_dir}/preds.txt\"", "\n", "print", "(", "txt_path", ",", "file", "=", "f", ",", "flush", "=", "True", ")", "\n", "\n", "", "expert_dims", ",", "raw_input_dims", ",", "text_dim", "=", "compute_dims", "(", "config", ",", "logger", ")", "\n", "trn_config", "=", "compute_trn_config", "(", "config", ")", "\n", "\n", "if", "config", ".", "_args", ".", "group_seed", ":", "\n", "        ", "seeds", "=", "[", "int", "(", "config", ".", "_args", ".", "group_seed", ")", "]", "\n", "", "else", ":", "\n", "        ", "seeds", "=", "[", "int", "(", "x", ")", "for", "x", "in", "config", ".", "_args", ".", "seeds", ".", "split", "(", "\",\"", ")", "]", "\n", "\n", "# set up local filesystem on the cluster", "\n", "", "if", "socket", ".", "gethostname", "(", ")", ".", "endswith", "(", "\"cluster\"", ")", ":", "\n", "        ", "os", ".", "system", "(", "str", "(", "Path", ".", "home", "(", ")", "/", "\"configure_tmp_data.sh\"", ")", ")", "\n", "\n", "", "for", "ii", ",", "seed", "in", "enumerate", "(", "seeds", ")", ":", "\n", "        ", "tic", "=", "time", ".", "time", "(", ")", "\n", "logger", ".", "info", "(", "f\"{ii + 1}/{len(seeds)} Setting experiment random seed to {seed}\"", ")", "\n", "set_seeds", "(", "seed", ")", "\n", "config", "[", "\"seed\"", "]", "=", "seed", "\n", "\n", "model", "=", "config", ".", "init", "(", "\n", "name", "=", "'arch'", ",", "\n", "module", "=", "module_arch", ",", "\n", "expert_dims", "=", "expert_dims", ",", "\n", "text_dim", "=", "text_dim", ",", "\n", "disable_nan_checks", "=", "config", "[", "\"disable_nan_checks\"", "]", ",", "\n", "spatial_feats", "=", "config", "[", "\"data_loader\"", "]", "[", "\"args\"", "]", ".", "get", "(", "\"spatial_feats\"", ",", "False", ")", ",", "\n", "task", "=", "config", ".", "get", "(", "\"task\"", ",", "\"retrieval\"", ")", ",", "\n", "ce_shared_dim", "=", "config", "[", "\"experts\"", "]", ".", "get", "(", "\"ce_shared_dim\"", ",", "None", ")", ",", "\n", "feat_aggregation", "=", "config", "[", "\"data_loader\"", "]", "[", "\"args\"", "]", "[", "\"feat_aggregation\"", "]", ",", "\n", "trn_config", "=", "trn_config", ",", "\n", "trn_cat", "=", "config", "[", "\"data_loader\"", "]", "[", "\"args\"", "]", ".", "get", "(", "\"trn_cat\"", ",", "0", ")", ",", "\n", ")", "\n", "logger", ".", "info", "(", "model", ")", "\n", "\n", "data_loaders", "=", "config", ".", "init", "(", "\n", "name", "=", "'data_loader'", ",", "\n", "module", "=", "module_data", ",", "\n", "logger", "=", "logger", ",", "\n", "raw_input_dims", "=", "raw_input_dims", ",", "\n", "challenge_mode", "=", "config", ".", "get", "(", "\"challenge_mode\"", ",", "False", ")", ",", "\n", "text_dim", "=", "text_dim", ",", "\n", "text_feat", "=", "config", "[", "\"experts\"", "]", "[", "\"text_feat\"", "]", ",", "\n", "text_agg", "=", "config", "[", "\"experts\"", "]", "[", "\"text_agg\"", "]", ",", "\n", "use_zeros_for_missing", "=", "config", "[", "\"experts\"", "]", ".", "get", "(", "\"use_zeros_for_missing\"", ",", "False", ")", ",", "\n", "task", "=", "config", ".", "get", "(", "\"task\"", ",", "\"retrieval\"", ")", ",", "\n", "eval_only", "=", "False", ",", "\n", "distil_params", "=", "config", ".", "get", "(", "\"distil_params\"", ",", "None", ")", ",", "\n", "training_file", "=", "config", ".", "get", "(", "\"training_file\"", ",", "None", ")", ",", "\n", "caption_masks", "=", "config", ".", "get", "(", "\"caption_masks\"", ",", "None", ")", ",", "\n", "ce_shared_dim", "=", "config", "[", "\"experts\"", "]", ".", "get", "(", "\"ce_shared_dim\"", ",", "None", ")", ",", "\n", ")", "\n", "\n", "if", "config", ".", "get", "(", "\"manual_linear_init\"", ",", "False", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"manually setting init for linear layers\"", ")", "\n", "\n", "def", "init_weights", "(", "m", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "                    ", "torch", ".", "nn", ".", "init", ".", "xavier_uniform", "(", "m", ".", "weight", ")", "\n", "m", ".", "bias", ".", "data", ".", "fill_", "(", "0.01", ")", "\n", "", "", "model", ".", "apply", "(", "init_weights", ")", "\n", "\n", "", "loss", "=", "config", ".", "init", "(", "name", "=", "\"loss\"", ",", "module", "=", "module_loss", ")", "\n", "metrics", "=", "[", "getattr", "(", "module_metric", ",", "met", ")", "for", "met", "in", "config", "[", "'metrics'", "]", "]", "\n", "trainable_params", "=", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "model", ".", "parameters", "(", ")", ")", "\n", "\n", "if", "config", "[", "\"optimizer\"", "]", "[", "\"type\"", "]", "==", "\"RAdam\"", ":", "\n", "            ", "optimizer", "=", "config", ".", "init", "(", "'optimizer'", ",", "radam", ",", "trainable_params", ")", "\n", "", "elif", "config", "[", "\"optimizer\"", "]", "[", "\"type\"", "]", "==", "\"Ranger\"", ":", "\n", "            ", "optimizer", "=", "config", ".", "init", "(", "'optimizer'", ",", "ranger", ",", "trainable_params", ")", "\n", "", "elif", "config", "[", "\"optimizer\"", "]", "[", "\"type\"", "]", "==", "\"SWATS\"", ":", "\n", "            ", "optimizer", "=", "config", ".", "init", "(", "'optimizer'", ",", "swats", ",", "trainable_params", ")", "\n", "", "else", ":", "\n", "            ", "optimizer", "=", "config", ".", "init", "(", "'optimizer'", ",", "torch", ".", "optim", ",", "trainable_params", ")", "\n", "\n", "", "if", "config", "[", "\"lr_scheduler\"", "]", "[", "\"type\"", "]", "==", "\"StepLR\"", ":", "\n", "            ", "lr_scheduler", "=", "config", ".", "init", "(", "'lr_scheduler'", ",", "torch", ".", "optim", ".", "lr_scheduler", ",", "\n", "optimizer", ")", "\n", "", "else", ":", "\n", "            ", "lr_scheduler", "=", "config", ".", "init", "(", "'lr_scheduler'", ",", "cos_restart", ",", "optimizer", ")", "\n", "\n", "", "update_src_web_video_dir", "(", "config", ")", "\n", "visualizer", "=", "config", ".", "init", "(", "\n", "name", "=", "'visualizer'", ",", "\n", "module", "=", "module_vis", ",", "\n", "exp_name", "=", "config", ".", "_exper_name", ",", "\n", "web_dir", "=", "config", ".", "_web_log_dir", ",", "\n", ")", "\n", "\n", "trainer", "=", "Trainer", "(", "\n", "model", ",", "\n", "loss", ",", "\n", "metrics", ",", "\n", "optimizer", ",", "\n", "config", "=", "config", ",", "\n", "data_loaders", "=", "data_loaders", ",", "\n", "lr_scheduler", "=", "lr_scheduler", ",", "\n", "mini_train", "=", "config", ".", "_args", ".", "mini_train", ",", "\n", "disable_nan_checks", "=", "config", "[", "\"disable_nan_checks\"", "]", ",", "\n", "visualizer", "=", "visualizer", ",", "\n", "val_freq", "=", "config", "[", "\"trainer\"", "]", ".", "get", "(", "\"val_freq\"", ",", "1", ")", ",", "\n", "distil_loss", "=", "config", ".", "get", "(", "\"distil_loss\"", ",", "False", ")", ",", "\n", "distil_params", "=", "config", ".", "get", "(", "\"distil_params\"", ",", "None", ")", ",", "\n", "force_cpu_val", "=", "config", ".", "get", "(", "\"force_cpu_val\"", ",", "False", ")", ",", "\n", "skip_first_n_saves", "=", "config", "[", "\"trainer\"", "]", ".", "get", "(", "\"skip_first_n_saves\"", ",", "0", ")", ",", "\n", "include_optim_in_ckpts", "=", "config", "[", "\"trainer\"", "]", ".", "get", "(", "\"include_optim_in_ckpts\"", ",", "1", ")", ",", "\n", "cache_targets", "=", "set", "(", "config", ".", "get", "(", "\"cache_targets\"", ",", "[", "]", ")", ")", ",", "\n", ")", "\n", "trainer", ".", "train", "(", ")", "\n", "best_ckpt_path", "=", "config", ".", "save_dir", "/", "\"trained_model.pth\"", "\n", "duration", "=", "time", ".", "strftime", "(", "'%Hh%Mm%Ss'", ",", "time", ".", "gmtime", "(", "time", ".", "time", "(", ")", "-", "tic", ")", ")", "\n", "logger", ".", "info", "(", "f\"Training took {duration}\"", ")", "\n", "\n", "if", "config", ".", "_config", ".", "get", "(", "\"eval_settings\"", ",", "False", ")", ":", "\n", "            ", "eval_config", "=", "copy", ".", "deepcopy", "(", "config", ")", "\n", "merge", "(", "eval_config", ".", "_config", ",", "config", "[", "\"eval_settings\"", "]", ",", "strategy", "=", "Strategy", ".", "REPLACE", ")", "\n", "eval_config", ".", "_args", ".", "resume", "=", "best_ckpt_path", "\n", "evaluation", "(", "eval_config", ",", "logger", "=", "logger", ",", "trainer", "=", "trainer", ")", "\n", "\n", "# If multiple runs were conducted, report relevant statistics", "\n", "", "", "if", "len", "(", "seeds", ")", ">", "1", ":", "\n", "        ", "log_summary", "(", "\n", "logger", "=", "logger", ",", "\n", "log_path", "=", "config", ".", "log_path", ",", "\n", "eval_mode", "=", "config", "[", "\"eval_mode\"", "]", ",", "\n", "fixed_num_epochs", "=", "config", "[", "\"trainer\"", "]", "[", "\"epochs\"", "]", ",", "\n", ")", "\n", "", "print", "(", "f\"Log file stored at {config.log_path}\"", ")", "\n", "\n", "# Report the location of the \"best\" checkpoint of the final seeded run (here", "\n", "# \"best\" corresponds to the model with the highest geometric mean over the", "\n", "# R@1, R@5 and R@10 metrics when a validation set is used, or simply the final", "\n", "# epoch of training for fixed-length schedules).", "\n", "print", "(", "f\"The best performing ckpt can be found at {str(best_ckpt_path)}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.train.main": [[176, 208], ["argparse.ArgumentParser", "parse_config.ConfigParser.add_argument", "parse_config.ConfigParser.add_argument", "parse_config.ConfigParser.add_argument", "parse_config.ConfigParser.add_argument", "parse_config.ConfigParser.add_argument", "parse_config.ConfigParser.add_argument", "parse_config.ConfigParser.add_argument", "parse_config.ConfigParser.add_argument", "parse_config.ConfigParser.add_argument", "parse_config.ConfigParser.add_argument", "parse_config.ConfigParser.add_argument", "parse_config.ConfigParser.add_argument", "parse_config.ConfigParser.add_mutually_exclusive_group", "args.add_mutually_exclusive_group.add_argument", "args.add_mutually_exclusive_group.add_argument", "parse_config.ConfigParser", "print", "print", "train.run_exp"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.train.run_exp"], ["", "def", "main", "(", ")", ":", "\n", "    ", "args", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Main entry point for training'", ")", "\n", "args", ".", "add_argument", "(", "'--config'", ",", "help", "=", "'config file path'", ")", "\n", "args", ".", "add_argument", "(", "'--resume'", ",", "help", "=", "'path to latest checkpoint (default: None)'", ")", "\n", "args", ".", "add_argument", "(", "'--leaderboard'", ",", "default", "=", "\"data/leaderboards/exp.txt\"", ",", "\n", "help", "=", "'path we want to draw on leadboard'", ")", "\n", "args", ".", "add_argument", "(", "'--device'", ",", "help", "=", "\"indices of GPUs to enable\"", ")", "\n", "args", ".", "add_argument", "(", "'--mini_train'", ",", "action", "=", "\"store_true\"", ")", "\n", "args", ".", "add_argument", "(", "'--group_id'", ",", "help", "=", "\"if supplied, group these experiments\"", ")", "\n", "args", ".", "add_argument", "(", "'--disable_workers'", ",", "action", "=", "\"store_true\"", ")", "\n", "args", ".", "add_argument", "(", "'--refresh_lru_cache'", ",", "action", "=", "\"store_true\"", ")", "\n", "args", ".", "add_argument", "(", "'--train_single_epoch'", ",", "action", "=", "\"store_true\"", ")", "\n", "args", ".", "add_argument", "(", "'--purge_exp_dir'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"remove all previous experiments with the given config\"", ")", "\n", "args", ".", "add_argument", "(", "\"--dbg\"", ",", "default", "=", "\"ipdb.set_trace\"", ")", "\n", "args", ".", "add_argument", "(", "\"--custom_args\"", ",", "help", "=", "\"qualified key,val pairs\"", ")", "\n", "\n", "# Seeds can either be passed directly as a comma separated list at the command line,", "\n", "# or individually for separate experiments as a group (used for slurm experiments)", "\n", "seed_args", "=", "args", ".", "add_mutually_exclusive_group", "(", ")", "\n", "seed_args", ".", "add_argument", "(", "'--seeds'", ",", "default", "=", "\"0\"", ",", "help", "=", "\"comma separated list of seeds\"", ")", "\n", "seed_args", ".", "add_argument", "(", "'--group_seed'", ",", "help", "=", "\"seed for group member\"", ")", "\n", "args", "=", "ConfigParser", "(", "args", ")", "\n", "os", ".", "environ", "[", "\"PYTHONBREAKPOINT\"", "]", "=", "args", ".", "_args", ".", "dbg", "\n", "args", "[", "\"data_loader\"", "]", "[", "\"args\"", "]", "[", "\"refresh_lru_cache\"", "]", "=", "args", ".", "_args", ".", "refresh_lru_cache", "\n", "msg", "=", "(", "f\"Expected the number of training epochs ({args['trainer']['epochs']})\"", "\n", "f\"to exceed the save period ({args['trainer']['save_period']}), otherwise\"", "\n", "\" no checkpoints will be saved.\"", ")", "\n", "assert", "args", "[", "\"trainer\"", "]", "[", "\"epochs\"", "]", ">=", "args", "[", "\"trainer\"", "]", "[", "\"save_period\"", "]", ",", "msg", "\n", "print", "(", "\"Launching experiment with config:\"", ")", "\n", "print", "(", "args", ")", "\n", "run_exp", "(", "config", "=", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.sent_feat_demo.sent_feat": [[1, 54], ["gensim.models.KeyedVectors.load_word2vec_format", "text.split", "np.asarray", "logging.basicConfig", "OpenAIGPTTokenizer.from_pretrained", "OpenAIGPTModel.from_pretrained", "OpenAIGPTModel.from_pretrained.eval", "OpenAIGPTModel.from_pretrained.to", "OpenAIGPTTokenizer.from_pretrained.tokenize", "OpenAIGPTTokenizer.from_pretrained.convert_tokens_to_ids", "torch.tensor", "tokens_tensor.to.to", "print", "hidden_states[].cpu().numpy.append", "torch.no_grad", "OpenAIGPTModel.from_pretrained.", "hidden_states[].cpu().numpy", "OpenAIGPTModel.from_pretrained.get_vector", "hidden_states[].cpu"], "function", ["None"], ["def", "sent_feat", "(", "text", ",", "feat_type", ")", ":", "\n", "\n", "    ", "if", "feat_type", "==", "'w2v'", ":", "\n", "        ", "import", "gensim", "\n", "import", "numpy", "as", "np", "\n", "model", "=", "gensim", ".", "models", ".", "KeyedVectors", ".", "load_word2vec_format", "(", "'/scratch/shared/slow/yangl/w2v/GoogleNews-vectors-negative300.bin'", ",", "binary", "=", "True", ")", "\n", "final_feats", "=", "[", "]", "\n", "for", "word", "in", "(", "text", ".", "split", "(", "' '", ")", ")", ":", "\n", "            ", "if", "(", "word", "!=", "'a'", ")", "and", "(", "word", "in", "model", ".", "vocab", ")", ":", "\n", "                ", "final_feats", ".", "append", "(", "model", ".", "get_vector", "(", "word", ")", ")", "\n", "\n", "", "", "final_feats", "=", "np", ".", "asarray", "(", "final_feats", ")", "\n", "\n", "", "elif", "feat_type", "==", "'openai'", ":", "\n", "        ", "import", "json", "\n", "import", "torch", "\n", "from", "pytorch_pretrained_bert", "import", "OpenAIGPTTokenizer", ",", "OpenAIGPTModel", ",", "OpenAIGPTLMHeadModel", "\n", "import", "logging", "\n", "\n", "logging", ".", "basicConfig", "(", "level", "=", "logging", ".", "INFO", ")", "\n", "\n", "# Load pre-trained model tokenizer (vocabulary)", "\n", "tokenizer", "=", "OpenAIGPTTokenizer", ".", "from_pretrained", "(", "'openai-gpt'", ")", "\n", "\n", "# Tokenized input", "\n", "#text = \"Who was Jim Henson ? Jim Henson was a puppeteer\"", "\n", "model", "=", "OpenAIGPTModel", ".", "from_pretrained", "(", "'openai-gpt'", ")", "\n", "model", ".", "eval", "(", ")", "\n", "model", ".", "to", "(", "'cuda'", ")", "\n", "\n", "tokenized_text", "=", "tokenizer", ".", "tokenize", "(", "text", ")", "\n", "\n", "# Convert token to vocabulary indices", "\n", "indexed_tokens", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenized_text", ")", "\n", "\n", "# Convert inputs to PyTorch tensors", "\n", "tokens_tensor", "=", "torch", ".", "tensor", "(", "[", "indexed_tokens", "]", ")", "\n", "\n", "\n", "# If you have a GPU, put everything on cuda", "\n", "tokens_tensor", "=", "tokens_tensor", ".", "to", "(", "'cuda'", ")", "\n", "\n", "\n", "# Predict hidden states features for each layer", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "hidden_states", "=", "model", "(", "tokens_tensor", ")", "\n", "final_feats", "=", "hidden_states", "[", "0", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "\n", "", "", "else", ":", "\n", "        ", "print", "(", "'Unrecognised FEAT_TYPE.'", ")", "\n", "\n", "", "return", "final_feats", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.test.compress_predictions": [[23, 48], ["numpy.argsort", "query_masks.flatten().astype", "query_masks.flatten"], "function", ["None"], ["@", "typechecked", "\n", "def", "compress_predictions", "(", "query_masks", ":", "np", ".", "ndarray", ",", "sims", ":", "np", ".", "ndarray", ",", "topk", ":", "int", "=", "10", ")", ":", "\n", "    ", "\"\"\"We store the indices of the top-k predictions, rather than the full similarity\n    matrix, to reduce storage requirements.\n\n    NOTE: The similarity matrix contains `num_queries x num_videos` elements, where\n    `num_queries = num_videos x max_num_queries_per_video`.  We first mask out\n    locations in the similarity matrix that correspond to invalid queries (these are\n    produced by videos with fewer than `max_num_queries_per_video` descriptions).\n    \"\"\"", "\n", "\n", "# validate the input shapes", "\n", "assert", "query_masks", ".", "ndim", "==", "2", ",", "\"Expected query_masks to be a matrix\"", "\n", "query_num_videos", ",", "query_max_per_video", "=", "query_masks", ".", "shape", "\n", "sims_queries", ",", "sims_num_videos", "=", "sims", ".", "shape", "\n", "msg", "=", "(", "f\"Expected sims and query masks to represent the same number of videos \"", "\n", "f\"(found {sims_num_videos} v {query_num_videos}\"", ")", "\n", "assert", "query_num_videos", "==", "sims_num_videos", ",", "msg", "\n", "msg", "=", "(", "f\"Expected sims and query masks to represent the same number of queries \"", "\n", "f\"(found {sims_queries} v {query_num_videos * query_max_per_video}\"", ")", "\n", "assert", "query_max_per_video", "*", "query_num_videos", "==", "sims_queries", ",", "msg", "\n", "\n", "valid_sims", "=", "sims", "[", "query_masks", ".", "flatten", "(", ")", ".", "astype", "(", "np", ".", "bool", ")", "]", "\n", "ranks", "=", "np", ".", "argsort", "(", "-", "valid_sims", ",", "axis", "=", "1", ")", "\n", "return", "ranks", "[", ":", ",", ":", "topk", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.test.get_model_and_data_loaders": [[50, 106], ["utils.util.compute_dims", "config.init", "utils.util.compute_trn_config", "config.init", "logger.info", "torch.load", "torch.nn.DataParallel.load_state_dict", "torch.nn.DataParallel", "config.get", "config[].get", "config.get", "config.get", "config.get", "config.get", "config[].get", "config.get", "config[].get", "[].get", "print", "state_dict.pop"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.compute_dims", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.init", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.compute_trn_config", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.init", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get"], ["", "@", "typechecked", "\n", "def", "get_model_and_data_loaders", "(", "\n", "config", ":", "ConfigParser", ",", "\n", "logger", ":", "logging", ".", "Logger", ",", "\n", "ckpt_path", ":", "Path", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "nn", ".", "Module", ",", "module_data", ".", "ExpertDataLoader", "]", ":", "\n", "    ", "expert_dims", ",", "raw_input_dims", ",", "text_dim", "=", "compute_dims", "(", "config", ")", "\n", "\n", "data_loaders", "=", "config", ".", "init", "(", "\n", "name", "=", "'data_loader'", ",", "\n", "module", "=", "module_data", ",", "\n", "logger", "=", "logger", ",", "\n", "raw_input_dims", "=", "raw_input_dims", ",", "\n", "challenge_mode", "=", "config", ".", "get", "(", "\"challenge_mode\"", ",", "False", ")", ",", "\n", "text_dim", "=", "text_dim", ",", "\n", "text_feat", "=", "config", "[", "\"experts\"", "]", "[", "\"text_feat\"", "]", ",", "\n", "text_agg", "=", "config", "[", "\"experts\"", "]", "[", "\"text_agg\"", "]", ",", "\n", "use_zeros_for_missing", "=", "config", "[", "\"experts\"", "]", ".", "get", "(", "\"use_zeros_for_missing\"", ",", "False", ")", ",", "\n", "task", "=", "config", ".", "get", "(", "\"task\"", ",", "\"retrieval\"", ")", ",", "\n", "eval_only", "=", "True", ",", "\n", "distil_params", "=", "config", ".", "get", "(", "\"distil_params\"", ",", "None", ")", ",", "\n", "training_file", "=", "config", ".", "get", "(", "\"training_file\"", ",", "None", ")", ",", "\n", "caption_masks", "=", "config", ".", "get", "(", "\"caption_masks\"", ",", "None", ")", ",", "\n", "ce_shared_dim", "=", "config", "[", "\"experts\"", "]", ".", "get", "(", "\"ce_shared_dim\"", ",", "None", ")", ",", "\n", ")", "\n", "\n", "trn_config", "=", "compute_trn_config", "(", "config", ")", "\n", "model", "=", "config", ".", "init", "(", "\n", "name", "=", "'arch'", ",", "\n", "module", "=", "module_arch", ",", "\n", "trn_config", "=", "trn_config", ",", "\n", "expert_dims", "=", "expert_dims", ",", "\n", "text_dim", "=", "text_dim", ",", "\n", "disable_nan_checks", "=", "config", "[", "\"disable_nan_checks\"", "]", ",", "\n", "task", "=", "config", ".", "get", "(", "\"task\"", ",", "\"retrieval\"", ")", ",", "\n", "ce_shared_dim", "=", "config", "[", "\"experts\"", "]", ".", "get", "(", "\"ce_shared_dim\"", ",", "None", ")", ",", "\n", "feat_aggregation", "=", "config", "[", "\"data_loader\"", "]", "[", "\"args\"", "]", "[", "\"feat_aggregation\"", "]", ",", "\n", "trn_cat", "=", "config", "[", "\"data_loader\"", "]", "[", "\"args\"", "]", ".", "get", "(", "\"trn_cat\"", ",", "0", ")", ",", "\n", ")", "\n", "ckpt_path", "=", "config", ".", "_args", ".", "resume", "\n", "logger", ".", "info", "(", "f\"Loading checkpoint: {ckpt_path} ...\"", ")", "\n", "checkpoint", "=", "torch", ".", "load", "(", "ckpt_path", ")", "\n", "state_dict", "=", "checkpoint", "[", "'state_dict'", "]", "\n", "if", "config", "[", "'n_gpu'", "]", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "# support backwards compatibility", "\n", "", "deprecated", "=", "[", "\"ce.moe_fc_bottleneck1\"", ",", "\"ce.moe_cg\"", ",", "\"ce.moe_fc_proj\"", "]", "\n", "for", "mod", "in", "deprecated", ":", "\n", "        ", "for", "suffix", "in", "(", "\"weight\"", ",", "\"bias\"", ")", ":", "\n", "            ", "key", "=", "f\"{mod}.{suffix}\"", "\n", "if", "key", "in", "state_dict", ":", "\n", "                ", "print", "(", "f\"WARNING: Removing deprecated key {key} from model\"", ")", "\n", "state_dict", ".", "pop", "(", "key", ")", "\n", "", "", "", "model", ".", "load_state_dict", "(", "state_dict", ")", "\n", "\n", "return", "model", ",", "data_loaders", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.test.evaluation": [[107, 213], ["getattr", "config.get_logger.info", "config.get_logger.info", "config.get_logger.info", "random.seed", "numpy.random.seed", "torch.manual_seed", "test.get_model_and_data_loaders", "config.get_logger.info", "utils.util.update_src_web_video_dir", "config.init", "config.get", "config.get_logger.info", "model.to.to", "model.to.eval", "nested_metrics.items", "log.items", "config.get_logger", "copy.deepcopy", "mergedeep.merge", "getattr", "torch.cuda.is_available", "torch.no_grad", "output[].data.cpu().float().numpy", "config.init.visualize_ranking", "subval.items", "config.get_logger.info", "pathlib.Path", "config.get", "trainer.ctxt_mgr", "model.to.", "test.compress_predictions", "numpy.savetxt", "print", "print", "metric", "trainer.verbose", "output[].data.cpu().float", "trainer.log_metrics", "str", "trainer.writer.set_step", "output[].data.cpu"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.test.get_model_and_data_loaders", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.update_src_web_video_dir", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.init", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get_logger", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.visualizer.Visualizer.visualize_ranking", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.trainer.trainer.ctxt_mgr", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.test.compress_predictions", "home.repos.pwc.inspect_result.albanie_collaborative-experts.trainer.trainer.verbose", "home.repos.pwc.inspect_result.albanie_collaborative-experts.trainer.trainer.Trainer.log_metrics", "home.repos.pwc.inspect_result.albanie_collaborative-experts.logger.visualization.TensorboardWriter.set_step"], ["", "def", "evaluation", "(", "config", ",", "logger", "=", "None", ",", "trainer", "=", "None", ")", ":", "\n", "\n", "    ", "if", "logger", "is", "None", ":", "\n", "        ", "logger", "=", "config", ".", "get_logger", "(", "'test'", ")", "\n", "\n", "", "if", "getattr", "(", "config", ".", "_args", ",", "\"eval_from_training_config\"", ",", "False", ")", ":", "\n", "        ", "eval_conf", "=", "copy", ".", "deepcopy", "(", "config", ")", "\n", "merge", "(", "eval_conf", ".", "_config", ",", "config", "[", "\"eval_settings\"", "]", ",", "strategy", "=", "Strategy", ".", "REPLACE", ")", "\n", "config", "=", "eval_conf", "\n", "\n", "", "logger", ".", "info", "(", "\"Running evaluation with configuration:\"", ")", "\n", "logger", ".", "info", "(", "config", ")", "\n", "\n", "# Set the random initial seeds", "\n", "seed", "=", "config", "[", "\"seed\"", "]", "\n", "logger", ".", "info", "(", "f\"Setting experiment random seed to {seed}\"", ")", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "\n", "model", ",", "data_loaders", "=", "get_model_and_data_loaders", "(", "\n", "config", "=", "config", ",", "\n", "logger", "=", "logger", ",", "\n", "ckpt_path", "=", "Path", "(", "config", ".", "_args", ".", "resume", ")", ",", "\n", ")", "\n", "logger", ".", "info", "(", "model", ")", "\n", "\n", "update_src_web_video_dir", "(", "config", ")", "\n", "visualizer", "=", "config", ".", "init", "(", "\n", "name", "=", "'visualizer'", ",", "\n", "module", "=", "module_vis", ",", "\n", "exp_name", "=", "config", ".", "_exper_name", ",", "\n", "web_dir", "=", "config", ".", "_web_log_dir", ",", "\n", ")", "\n", "\n", "metrics", "=", "[", "getattr", "(", "module_metric", ",", "met", ")", "for", "met", "in", "config", "[", "'metrics'", "]", "]", "\n", "challenge_mode", "=", "config", ".", "get", "(", "\"challenge_mode\"", ",", "False", ")", "\n", "challenge_msg", "=", "(", "\n", "\"\\n\"", "\n", "\"Evaluation ran on challenge features. To obtain a score, upload the similarity\"", "\n", "\"matrix for each dataset to the test server after running the \"", "\n", "\"`misc/cvpr2020-challenge/prepare_submission.py` script and following the \"", "\n", "\"instructions at: \"", "\n", "\"https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/\"", "\n", "\"\\n\"", "\n", ")", "\n", "\n", "# prepare model for testing.  Note that some datasets fail to fit the retrieval", "\n", "# set on the GPU, so we run them on the CPU", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "config", ".", "get", "(", "\"disable_gpu\"", ",", "True", ")", ":", "\n", "        ", "device", "=", "\"cuda\"", "\n", "", "else", ":", "\n", "        ", "device", "=", "\"cpu\"", "\n", "", "logger", ".", "info", "(", "f\"Running evaluation on {device}\"", ")", "\n", "\n", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "samples", ",", "meta", "=", "data_loaders", "[", "\"retrieval\"", "]", "\n", "\n", "# To use the nan-checks safely, we need make temporary copies of the data", "\n", "disable_nan_checks", "=", "config", ".", "_config", "[", "\"disable_nan_checks\"", "]", "\n", "with", "ctxt_mgr", "(", "samples", ",", "device", ",", "disable_nan_checks", ")", "as", "valid", ":", "\n", "            ", "output", "=", "model", "(", "**", "valid", ")", "\n", "\n", "", "sims", "=", "output", "[", "\"cross_view_conf_matrix\"", "]", ".", "data", ".", "cpu", "(", ")", ".", "float", "(", ")", ".", "numpy", "(", ")", "\n", "dataset", "=", "data_loaders", ".", "dataset_name", "\n", "if", "challenge_mode", ":", "\n", "            ", "split", "=", "data_loaders", ".", "dataloaders", "[", "\"dataset\"", "]", ".", "split_name", "\n", "prediction_path", "=", "config", ".", "_log_dir", "/", "f\"{dataset}-{split}-predictions.csv\"", "\n", "compressed_preds", "=", "compress_predictions", "(", "\n", "query_masks", "=", "meta", "[", "\"query_masks\"", "]", ",", "\n", "sims", "=", "sims", ",", "\n", ")", "\n", "np", ".", "savetxt", "(", "prediction_path", ",", "compressed_preds", ",", "delimiter", "=", "','", ",", "fmt", "=", "\"%d\"", ")", "\n", "print", "(", "f\"Saved similarity matrix predictions to {prediction_path}\"", ")", "\n", "print", "(", "challenge_msg", ")", "\n", "return", "\n", "\n", "", "nested_metrics", "=", "{", "}", "\n", "for", "metric", "in", "metrics", ":", "\n", "            ", "metric_name", "=", "metric", ".", "__name__", "\n", "res", "=", "metric", "(", "sims", ",", "query_masks", "=", "meta", "[", "\"query_masks\"", "]", ")", "\n", "verbose", "(", "epoch", "=", "0", ",", "metrics", "=", "res", ",", "name", "=", "dataset", ",", "mode", "=", "metric_name", ")", "\n", "if", "trainer", "is", "not", "None", ":", "\n", "                ", "if", "not", "trainer", ".", "mini_train", ":", "\n", "                    ", "trainer", ".", "writer", ".", "set_step", "(", "step", "=", "0", ",", "mode", "=", "\"val\"", ")", "\n", "# avoid tensboard folding by prefixing", "\n", "", "metric_name_", "=", "f\"test_{metric_name}\"", "\n", "trainer", ".", "log_metrics", "(", "res", ",", "metric_name", "=", "metric_name_", ",", "mode", "=", "\"val\"", ")", "\n", "", "nested_metrics", "[", "metric_name", "]", "=", "res", "\n", "\n", "", "", "if", "data_loaders", ".", "num_test_captions", "==", "1", ":", "\n", "        ", "visualizer", ".", "visualize_ranking", "(", "\n", "sims", "=", "sims", ",", "\n", "meta", "=", "meta", ",", "\n", "epoch", "=", "0", ",", "\n", "nested_metrics", "=", "nested_metrics", ",", "\n", ")", "\n", "", "log", "=", "{", "}", "\n", "for", "subkey", ",", "subval", "in", "nested_metrics", ".", "items", "(", ")", ":", "\n", "        ", "for", "subsubkey", ",", "subsubval", "in", "subval", ".", "items", "(", ")", ":", "\n", "            ", "log", "[", "f\"test_{subkey}_{subsubkey}\"", "]", "=", "subsubval", "\n", "", "", "for", "key", ",", "value", "in", "log", ".", "items", "(", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\" {:15s}: {}\"", ".", "format", "(", "str", "(", "key", ")", ",", "value", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.__init__": [[20, 105], ["pathlib.Path", "zsvision.zs_utils.load_json_config", "parse_config._update_config", "parse_config.ConfigParser._config.get", "parse_config.ConfigParser.set_exper_name", "getattr", "parse_config.ConfigParser._config.get", "vars().get", "parse_config.ConfigParser.save_dir.mkdir", "parse_config.ConfigParser.log_dir.mkdir", "utils.write_json", "args.parse_args.parse_args.add_argument", "args.parse_args.parse_args.parse_args", "isinstance", "pathlib.Path", "parse_config.ConfigParser._config.get", "pathlib.Path().exists", "pathlib.Path", "pathlib.Path", "datetime.datetime.datetime.now().strftime", "logger.setup_logging", "args.parse_args.parse_args.parse_args", "vars", "list", "print", "time.time", "os.system", "print", "pathlib.Path", "datetime.datetime.datetime.now", "pathlib.Path", "config_dir.glob", "parse_config.ConfigParser._config.get", "len", "time.time"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config._update_config", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.set_exper_name", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdir", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdir", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.write_json", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.logger.logger.setup_logging", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "options", "=", "''", ",", "timestamp", "=", "True", ",", "slave_mode", "=", "False", ")", ":", "\n", "# slave_mode - when calling the config parser form an existing process, we", "\n", "# avoid reinitialising the logger and ignore sys.argv when argparsing.", "\n", "\n", "# parse default and custom cli options", "\n", "        ", "for", "opt", "in", "options", ":", "\n", "            ", "args", ".", "add_argument", "(", "*", "opt", ".", "flags", ",", "default", "=", "None", ",", "type", "=", "opt", ".", "type", ")", "\n", "\n", "", "if", "slave_mode", ":", "\n", "            ", "args", "=", "args", ".", "parse_args", "(", "args", "=", "[", "]", ")", "\n", "", "elif", "isinstance", "(", "args", ",", "mock", ".", "Mock", ")", ":", "\n", "            ", "pass", "\n", "", "else", ":", "\n", "            ", "args", "=", "args", ".", "parse_args", "(", ")", "\n", "\n", "", "if", "args", ".", "device", ":", "\n", "            ", "os", ".", "environ", "[", "\"CUDA_VISIBLE_DEVICES\"", "]", "=", "args", ".", "device", "\n", "\n", "", "if", "args", ".", "resume", "and", "not", "slave_mode", ":", "\n", "            ", "self", ".", "resume", "=", "Path", "(", "args", ".", "resume", ")", "\n", "# self.cfg_fname = self.resume.parent / 'config.json'", "\n", "", "else", ":", "\n", "            ", "msg_no_cfg", "=", "\"Config file must be specified\"", "\n", "assert", "args", ".", "config", "is", "not", "None", ",", "msg_no_cfg", "\n", "self", ".", "resume", "=", "None", "\n", "", "self", ".", "cfg_fname", "=", "Path", "(", "args", ".", "config", ")", "\n", "\n", "config", "=", "load_json_config", "(", "self", ".", "cfg_fname", ")", "\n", "self", ".", "_config", "=", "_update_config", "(", "config", ",", "options", ",", "args", ")", "\n", "\n", "if", "self", ".", "_config", ".", "get", "(", "\"eval_config\"", ",", "False", ")", ":", "\n", "# validate path to evaluation file", "\n", "            ", "eval_cfg_path", "=", "self", ".", "_config", ".", "get", "(", "\"eval_config\"", ")", "\n", "msg", "=", "f\"eval_config was specified, but `{eval_cfg_path}` does not exist\"", "\n", "assert", "Path", "(", "self", ".", "_config", ".", "get", "(", "\"eval_config\"", ")", ")", ".", "exists", "(", ")", ",", "msg", "\n", "\n", "# set save_dir where trained model and log will be saved.", "\n", "", "if", "\"tester\"", "in", "self", ".", "config", ":", "\n", "            ", "save_dir", "=", "Path", "(", "self", ".", "config", "[", "'tester'", "]", "[", "'save_dir'", "]", ")", "\n", "", "else", ":", "\n", "            ", "save_dir", "=", "Path", "(", "self", ".", "config", "[", "'trainer'", "]", "[", "'save_dir'", "]", ")", "\n", "", "timestamp", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "r\"%Y-%m-%d_%H-%M-%S\"", ")", "if", "timestamp", "else", "\"\"", "\n", "\n", "if", "slave_mode", ":", "\n", "            ", "timestamp", "=", "f\"{timestamp}-eval-worker\"", "\n", "\n", "", "exper_name", "=", "self", ".", "set_exper_name", "(", "args", ",", "config", "=", "config", ")", "\n", "\n", "if", "getattr", "(", "args", ",", "\"group_id\"", ",", "False", ")", ":", "\n", "            ", "subdir", "=", "Path", "(", "args", ".", "group_id", ")", "/", "f\"seed-{args.group_seed}\"", "/", "timestamp", "\n", "", "else", ":", "\n", "            ", "subdir", "=", "timestamp", "\n", "\n", "# store challenge experiments in a further subdirectory", "\n", "", "if", "self", ".", "_config", ".", "get", "(", "\"challenge_mode\"", ",", "False", ")", ":", "\n", "            ", "challenge_tag", "=", "\"cvpr2020-challenge\"", "\n", "exper_name", "=", "f\"{challenge_tag}/{exper_name}\"", "\n", "\n", "", "self", ".", "_save_dir", "=", "save_dir", "/", "'models'", "/", "exper_name", "/", "subdir", "\n", "self", ".", "_web_log_dir", "=", "save_dir", "/", "'web'", "/", "exper_name", "/", "subdir", "\n", "self", ".", "_log_dir", "=", "save_dir", "/", "'log'", "/", "exper_name", "/", "subdir", "\n", "self", ".", "_exper_name", "=", "exper_name", "\n", "self", ".", "_args", "=", "args", "\n", "\n", "# if set, remove all previous experiments with the current config", "\n", "if", "vars", "(", "args", ")", ".", "get", "(", "\"purge_exp_dir\"", ",", "False", ")", ":", "\n", "            ", "for", "dirpath", "in", "(", "self", ".", "_save_dir", ",", "self", ".", "_log_dir", ",", "self", ".", "_web_log_dir", ")", ":", "\n", "                ", "config_dir", "=", "dirpath", ".", "parent", "\n", "existing", "=", "list", "(", "config_dir", ".", "glob", "(", "\"*\"", ")", ")", "\n", "print", "(", "f\"purging {len(existing)} directories from config_dir...\"", ")", "\n", "tic", "=", "time", ".", "time", "(", ")", "\n", "os", ".", "system", "(", "f\"rm -rf {config_dir}\"", ")", "\n", "print", "(", "f\"Finished purge in {time.time() - tic:.3f}s\"", ")", "\n", "\n", "", "", "self", ".", "save_dir", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "self", ".", "log_dir", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "\n", "# save updated config file to the checkpoint dir", "\n", "write_json", "(", "self", ".", "config", ",", "self", ".", "save_dir", "/", "'config.json'", ")", "\n", "\n", "# configure logging module", "\n", "if", "not", "slave_mode", ":", "\n", "            ", "self", ".", "log_path", "=", "setup_logging", "(", "self", ".", "log_dir", ")", "\n", "\n", "", "self", ".", "log_levels", "=", "{", "0", ":", "logging", ".", "WARNING", ",", "1", ":", "logging", ".", "INFO", ",", "2", ":", "logging", ".", "DEBUG", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.set_exper_name": [[106, 135], ["getattr", "getattr", "args.custom_args.split", "print", "print", "print", "key_val_pair.split", "zsvision.zs_utils.set_nested_key_val", "key.replace", "val.replace.replace.replace", "key.replace.split"], "methods", ["None"], ["", "def", "set_exper_name", "(", "self", ",", "args", ",", "config", ")", ":", "\n", "# We assume that the config files are organised into directories such that", "\n", "# each directory has the name of the dataset.", "\n", "        ", "dataset_name", "=", "self", ".", "cfg_fname", ".", "parent", ".", "stem", "\n", "exper_name", "=", "f\"{dataset_name}-{self.cfg_fname.stem}\"", "\n", "if", "args", ".", "custom_args", ":", "\n", "            ", "key_val_lists", "=", "args", ".", "custom_args", ".", "split", "(", "\"+\"", ")", "\n", "for", "key_val_pair", "in", "key_val_lists", ":", "\n", "                ", "print", "(", "f\"parsing key-val pair : {key_val_pair}\"", ")", "\n", "key", ",", "val", "=", "key_val_pair", ".", "split", "(", "\"@\"", ")", "\n", "set_nested_key_val", "(", "key", ",", "val", ",", "self", ".", "_config", ")", "\n", "# remove periods from key names", "\n", "key_", "=", "key", ".", "replace", "(", "\"_.\"", ",", "\"--\"", ")", "\n", "# remove commas from value names", "\n", "val", "=", "val", ".", "replace", "(", "\",\"", ",", "\"--\"", ")", "\n", "custom_tag", "=", "\"-\"", ".", "join", "(", "key_", ".", "split", "(", "\".\"", ")", "[", "-", "2", ":", "]", ")", "\n", "exper_name", "=", "f\"{exper_name}-{custom_tag}-{val}\"", "\n", "\n", "", "", "if", "getattr", "(", "args", ",", "\"disable_workers\"", ",", "False", ")", ":", "\n", "            ", "print", "(", "\"Disabling data loader workers....\"", ")", "\n", "config", "[", "\"data_loader\"", "]", "[", "\"args\"", "]", "[", "\"num_workers\"", "]", "=", "0", "\n", "\n", "", "if", "getattr", "(", "args", ",", "\"train_single_epoch\"", ",", "False", ")", ":", "\n", "            ", "print", "(", "\"Restricting training to a single epoch....\"", ")", "\n", "config", "[", "\"trainer\"", "]", "[", "\"epochs\"", "]", "=", "1", "\n", "config", "[", "\"trainer\"", "]", "[", "\"save_period\"", "]", "=", "1", "\n", "config", "[", "\"trainer\"", "]", "[", "\"skip_first_n_saves\"", "]", "=", "0", "\n", "exper_name", "=", "f\"{exper_name}-train-single-epoch\"", "\n", "", "return", "exper_name", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.init": [[136, 149], ["dict", "all", "dict.update", "getattr"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update"], ["", "def", "init", "(", "self", ",", "name", ",", "module", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Finds a function handle with the name given as 'type' in config, and returns\n        the instance initialized with corresponding keyword args given as 'args'.\n        \"\"\"", "\n", "module_name", "=", "self", "[", "name", "]", "[", "'type'", "]", "\n", "module_args", "=", "dict", "(", "self", "[", "name", "]", "[", "'args'", "]", ")", "\n", "msg", "=", "(", "f\"Fail for {module_name}\\n\"", "\n", "f\"overwriting kwargs given in config file is not allowed\\n\"", "\n", "f\"passed kwargs: {kwargs}\\n\"", "\n", "f\"for module_args: {module_args})\"", ")", "\n", "assert", "all", "(", "[", "k", "not", "in", "module_args", "for", "k", "in", "kwargs", "]", ")", ",", "msg", "\n", "module_args", ".", "update", "(", "kwargs", ")", "\n", "return", "getattr", "(", "module", ",", "module_name", ")", "(", "*", "args", ",", "**", "module_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.__getitem__": [[150, 152], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "name", ")", ":", "\n", "        ", "return", "self", ".", "config", "[", "name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.__len__": [[153, 157], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "# NOTE: This is used for boolean checking deep inside ray.tune, so we required it", "\n", "# to be defined.", "\n", "        ", "return", "len", "(", "self", ".", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.__setitem__": [[158, 160], ["None"], "methods", ["None"], ["", "def", "__setitem__", "(", "self", ",", "name", ",", "value", ")", ":", "\n", "        ", "self", ".", "config", "[", "name", "]", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.__contains__": [[161, 163], ["None"], "methods", ["None"], ["", "def", "__contains__", "(", "self", ",", "name", ")", ":", "\n", "        ", "return", "name", "in", "self", ".", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get": [[164, 166], ["parse_config.ConfigParser.config.get"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get"], ["", "def", "get", "(", "self", ",", "name", ",", "default", ")", ":", "\n", "        ", "return", "self", ".", "config", ".", "get", "(", "name", ",", "default", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys": [[167, 169], ["parse_config.ConfigParser.config.keys"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys"], ["", "def", "keys", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "config", ".", "keys", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get_logger": [[170, 177], ["msg_verbosity.format.format.format", "logging.getLogger", "logging.getLogger.setLevel", "parse_config.ConfigParser.log_levels.keys"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys"], ["", "def", "get_logger", "(", "self", ",", "name", ",", "verbosity", "=", "2", ")", ":", "\n", "        ", "msg_verbosity", "=", "\"verbosity option {} is invalid. Valid options are {}.\"", "\n", "msg_verbosity", "=", "msg_verbosity", ".", "format", "(", "verbosity", ",", "self", ".", "log_levels", ".", "keys", "(", ")", ")", "\n", "assert", "verbosity", "in", "self", ".", "log_levels", ",", "msg_verbosity", "\n", "logger", "=", "logging", ".", "getLogger", "(", "name", ")", "\n", "logger", ".", "setLevel", "(", "self", ".", "log_levels", "[", "verbosity", "]", ")", "\n", "return", "logger", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.config": [[179, 182], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "config", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_config", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.save_dir": [[183, 186], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "save_dir", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_save_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.log_dir": [[187, 190], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "log_dir", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_log_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.__repr__": [[191, 193], ["pprint.PrettyPrinter().pformat", "pprint.PrettyPrinter"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "pprint", ".", "PrettyPrinter", "(", ")", ".", "pformat", "(", "self", ".", "__dict__", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items": [[194, 196], ["parse_config.ConfigParser._config.items"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["", "def", "items", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_config", ".", "items", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config._update_config": [[199, 205], ["getattr", "parse_config._get_opt_name", "parse_config._set_by_path"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config._get_opt_name", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config._set_by_path"], ["", "", "def", "_update_config", "(", "config", ",", "options", ",", "args", ")", ":", "\n", "    ", "for", "opt", "in", "options", ":", "\n", "        ", "value", "=", "getattr", "(", "args", ",", "_get_opt_name", "(", "opt", ".", "flags", ")", ")", "\n", "if", "value", "is", "not", "None", ":", "\n", "            ", "_set_by_path", "(", "config", ",", "opt", ".", "target", ",", "value", ")", "\n", "", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config._get_opt_name": [[207, 212], ["flags[].replace", "flg.startswith", "flg.replace"], "function", ["None"], ["", "def", "_get_opt_name", "(", "flags", ")", ":", "\n", "    ", "for", "flg", "in", "flags", ":", "\n", "        ", "if", "flg", ".", "startswith", "(", "'--'", ")", ":", "\n", "            ", "return", "flg", ".", "replace", "(", "'--'", ",", "''", ")", "\n", "", "", "return", "flags", "[", "0", "]", ".", "replace", "(", "'--'", ",", "''", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config._set_by_path": [[214, 217], ["parse_config._get_by_path"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config._get_by_path"], ["", "def", "_set_by_path", "(", "tree", ",", "keys", ",", "value", ")", ":", "\n", "    ", "\"\"\"Set a value in a nested object in tree by sequence of keys.\"\"\"", "\n", "_get_by_path", "(", "tree", ",", "keys", "[", ":", "-", "1", "]", ")", "[", "keys", "[", "-", "1", "]", "]", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config._get_by_path": [[219, 222], ["functools.reduce"], "function", ["None"], ["", "def", "_get_by_path", "(", "tree", ",", "keys", ")", ":", "\n", "    ", "\"\"\"Access a nested object in tree by sequence of keys.\"\"\"", "\n", "return", "reduce", "(", "getitem", ",", "keys", ",", "tree", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.html.HTML.__init__": [[16, 37], ["os.path.join", "dominate.document", "os.path.exists", "os.makedirs", "os.path.exists", "os.makedirs", "dominate.tags.meta", "str"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "web_dir", ",", "title", ",", "refresh", "=", "0", ")", ":", "\n", "        ", "\"\"\"Initialize the HTML classes\n\n        Parameters:\n            web_dir (str) -- a directory that stores the webpage. HTML file will be\n            created at <web_dir>/index.html; images will be saved at <web_dir/images/\n            title (str)   -- the webpage name\n            reflect (int) -- how often the website refresh itself; if 0; no refreshing\n        \"\"\"", "\n", "self", ".", "title", "=", "title", "\n", "self", ".", "web_dir", "=", "web_dir", "\n", "self", ".", "img_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "web_dir", ",", "\"images\"", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "web_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "web_dir", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "img_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "img_dir", ")", "\n", "\n", "", "self", ".", "doc", "=", "dominate", ".", "document", "(", "title", "=", "title", ")", "\n", "if", "refresh", ">", "0", ":", "\n", "            ", "with", "self", ".", "doc", ".", "head", ":", "\n", "                ", "meta", "(", "http_equiv", "=", "\"refresh\"", ",", "content", "=", "str", "(", "refresh", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.html.HTML.get_image_dir": [[38, 41], ["None"], "methods", ["None"], ["", "", "", "def", "get_image_dir", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the directory that stores images\"\"\"", "\n", "return", "self", ".", "img_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.html.HTML.add_header": [[42, 50], ["dominate.tags.h3"], "methods", ["None"], ["", "def", "add_header", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Insert a header to the HTML file\n\n        Parameters:\n            text (str) -- the header text\n        \"\"\"", "\n", "with", "self", ".", "doc", ":", "\n", "            ", "h3", "(", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.html.HTML.add_videos": [[51, 90], ["dominate.tags.table", "html.HTML.doc.add", "dominate.tags.tr", "zip", "dominate.tags.td", "dominate.tags.p", "str", "dominate.tags.br", "txt.split", "enumerate", "p_style.format.format.format", "dominate.tags.p", "dominate.tags.span", "dominate.tags.a", "row.startswith", "dominate.tags.video", "dominate.tags.attr", "dominate.tags.source", "str", "len", "len"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.add"], ["", "", "def", "add_videos", "(", "self", ",", "vids", ",", "txts", ",", "links", ",", "width", "=", "400", ",", "hidden_tag", "=", "\"hidden\"", ")", ":", "\n", "        ", "\"\"\"add images to the HTML file\n\n        Parameters:\n            vids (str list)   -- a list of image paths\n            txts (str list)  -- a list of image names shown on the website\n            links (str list) --  a list of hyperref links; when you click an image,\n            it will redirect you to a new page\n        \"\"\"", "\n", "self", ".", "t", "=", "table", "(", "border", "=", "1", ",", "style", "=", "\"table-layout: fixed;\"", ")", "# Insert a table", "\n", "self", ".", "doc", ".", "add", "(", "self", ".", "t", ")", "\n", "colors", "=", "[", "\"red\"", ",", "\"blue\"", ",", "\"gold\"", ",", "\"salman\"", "]", "\n", "with", "self", ".", "t", ":", "\n", "            ", "with", "tr", "(", ")", ":", "\n", "                ", "for", "vid", ",", "txt", ",", "link", "in", "zip", "(", "vids", ",", "txts", ",", "links", ")", ":", "\n", "                    ", "td_style", "=", "\"word-wrap: break-word; width:{}px\"", ".", "format", "(", "width", ")", "\n", "with", "td", "(", "style", "=", "td_style", ",", "halign", "=", "\"center\"", ",", "valign", "=", "\"top\"", ")", ":", "\n", "                        ", "with", "p", "(", ")", ":", "\n", "                            ", "vid_path", "=", "str", "(", "vid", ")", "\n", "if", "vid_path", "==", "hidden_tag", ":", "\n", "                                ", "p_style", "=", "\"font-weight: bold; width:{}px;\"", "\n", "p_style", "=", "p_style", ".", "format", "(", "width", "*", "3", ")", "\n", "p", "(", "\"hidden video\"", ",", "style", "=", "p_style", ")", "\n", "", "else", ":", "\n", "                                ", "with", "a", "(", "href", "=", "str", "(", "link", ")", ")", ":", "\n", "                                    ", "with", "video", "(", ")", ":", "\n", "                                        ", "attr", "(", "controls", "=", "\"controls\"", ")", "\n", "source", "(", "src", "=", "vid_path", ",", "type", "=", "\"video/mp4\"", ")", "\n", "", "", "", "br", "(", ")", "\n", "rows", "=", "txt", ".", "split", "(", "\"<br>\"", ")", "\n", "for", "idx", ",", "row", "in", "enumerate", "(", "rows", ")", ":", "\n", "                                ", "color", "=", "colors", "[", "idx", "%", "len", "(", "colors", ")", "]", "\n", "bold_tag", "=", "\"<b>\"", "\n", "if", "not", "row", ".", "startswith", "(", "bold_tag", ")", ":", "\n", "                                    ", "s_style", "=", "\"color:{};\"", ".", "format", "(", "color", ")", "\n", "", "else", ":", "\n", "                                    ", "s_style", "=", "\"color:black; font-weight: bold;\"", "\n", "row", "=", "row", "[", "len", "(", "bold_tag", ")", ":", "]", "\n", "", "span", "(", "row", ",", "style", "=", "s_style", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.html.HTML.add_images": [[91, 115], ["dominate.tags.table", "html.HTML.doc.add", "dominate.tags.tr", "zip", "dominate.tags.td", "dominate.tags.p", "dominate.tags.br", "dominate.tags.p", "dominate.tags.a", "dominate.tags.img", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.add"], ["", "", "", "", "", "", "", "def", "add_images", "(", "self", ",", "ims", ",", "txts", ",", "links", ",", "width", "=", "400", ")", ":", "\n", "        ", "\"\"\"add images to the HTML file\n\n        Parameters:\n            ims (str list)   -- a list of image paths\n            txts (str list)  -- a list of image names shown on the website\n            links (str list) --  a list of hyperref links; when you click an image,\n            it will redirect you to a new page\n        \"\"\"", "\n", "self", ".", "t", "=", "table", "(", "border", "=", "1", ",", "style", "=", "\"table-layout: fixed;\"", ")", "# Insert a table", "\n", "self", ".", "doc", ".", "add", "(", "self", ".", "t", ")", "\n", "with", "self", ".", "t", ":", "\n", "            ", "with", "tr", "(", ")", ":", "\n", "                ", "for", "im", ",", "txt", ",", "link", "in", "zip", "(", "ims", ",", "txts", ",", "links", ")", ":", "\n", "                    ", "td_style", "=", "\"word-wrap: break-word;\"", "\n", "with", "td", "(", "style", "=", "td_style", ",", "halign", "=", "\"center\"", ",", "valign", "=", "\"top\"", ")", ":", "\n", "                        ", "with", "p", "(", ")", ":", "\n", "                            ", "with", "a", "(", "href", "=", "os", ".", "path", ".", "join", "(", "\"images\"", ",", "link", ")", ")", ":", "\n", "                                ", "img", "(", "\n", "style", "=", "\"width:%dpx\"", "%", "width", ",", "\n", "src", "=", "os", ".", "path", ".", "join", "(", "\"images\"", ",", "im", ")", ",", "\n", ")", "\n", "", "br", "(", ")", "\n", "p", "(", "txt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.html.HTML.save": [[116, 122], ["open", "open.write", "open.close", "html.HTML.doc.render"], "methods", ["None"], ["", "", "", "", "", "", "def", "save", "(", "self", ")", ":", "\n", "        ", "\"\"\"save the current content to the HMTL file\"\"\"", "\n", "html_file", "=", "\"%s/index.html\"", "%", "self", ".", "web_dir", "\n", "f", "=", "open", "(", "html_file", ",", "\"wt\"", ")", "\n", "f", ".", "write", "(", "self", ".", "doc", ".", "render", "(", ")", ")", "\n", "f", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.visualizer.Visualizer.__init__": [[19, 36], ["os.path.join", "print", "util.mkdirs", "pathlib.Path().absolute", "print", "sym_dir.is_symlink", "sym_dir.symlink_to", "os.remove", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdirs"], ["def", "__init__", "(", "self", ",", "exp_name", ",", "web_dir", ",", "src_video_dir", ",", "vis_vid_freq", ",", "num_samples", "=", "50", ")", ":", "\n", "        ", "\"\"\"Initialize the Visualizer class\n        Create an HTML object for saveing HTML filters\n        \"\"\"", "\n", "self", ".", "name", "=", "exp_name", "\n", "self", ".", "web_dir", "=", "web_dir", "\n", "self", ".", "vis_vid_freq", "=", "vis_vid_freq", "\n", "self", ".", "img_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "web_dir", ",", "\"images\"", ")", "\n", "self", ".", "num_samples", "=", "num_samples", "\n", "print", "(", "f\"create web directory {self.web_dir}...\"", ")", "\n", "util", ".", "mkdirs", "(", "[", "self", ".", "web_dir", ",", "self", ".", "img_dir", "]", ")", "\n", "src_dir", "=", "Path", "(", "src_video_dir", ")", ".", "absolute", "(", ")", "\n", "print", "(", "f\"symlinking videos from {src_dir}...\"", ")", "\n", "sym_dir", "=", "(", "Path", "(", "self", ".", "web_dir", ")", "/", "\"videos\"", ")", ".", "absolute", "(", ")", "\n", "if", "sym_dir", ".", "is_symlink", "(", ")", ":", "\n", "            ", "os", ".", "remove", "(", "sym_dir", ")", "\n", "", "sym_dir", ".", "symlink_to", "(", "src_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.visualizer.Visualizer.visualize_ranking": [[37, 72], ["numpy.random.seed", "numpy.argsort", "numpy.diag", "numpy.random.choice", "visualizer.Visualizer.display_current_results", "numpy.arange", "rankings.append", "numpy.array", "numpy.where"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.visualizer.Visualizer.display_current_results"], ["", "def", "visualize_ranking", "(", "self", ",", "sims", ",", "epoch", ",", "meta", ",", "nested_metrics", ")", ":", "\n", "        ", "if", "not", "(", "self", ".", "vis_vid_freq", "and", "epoch", "%", "self", ".", "vis_vid_freq", "==", "0", ")", ":", "\n", "            ", "return", "\n", "\n", "", "dists", "=", "-", "sims", "\n", "np", ".", "random", ".", "seed", "(", "0", ")", "\n", "sorted_ranks", "=", "np", ".", "argsort", "(", "dists", ",", "axis", "=", "1", ")", "\n", "gt_dists", "=", "np", ".", "diag", "(", "dists", ")", "\n", "rankings", "=", "[", "]", "\n", "vis_top_k", "=", "5", "\n", "hide_gt", "=", "False", "\n", "# num_indep_samples = 1", "\n", "# random_seeds = np.arange(num_indep_samples)", "\n", "sample", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "dists", ".", "shape", "[", "0", "]", ")", ",", "size", "=", "self", ".", "num_samples", ",", "\n", "replace", "=", "False", ")", "\n", "for", "ii", "in", "sample", ":", "\n", "            ", "ranked_idx", "=", "sorted_ranks", "[", "ii", "]", "[", ":", "vis_top_k", "]", "\n", "gt_captions", "=", "meta", "[", "\"raw_captions\"", "]", "[", "ii", "]", "\n", "# if args.sample_single_gt_caption:", "\n", "#     gt_captions = np.random.choice(gt_captions, 1).tolist()", "\n", "\n", "datum", "=", "{", "\n", "\"gt-sim\"", ":", "-", "gt_dists", "[", "ii", "]", ",", "\n", "\"gt-captions\"", ":", "gt_captions", ",", "\n", "\"gt-rank\"", ":", "np", ".", "where", "(", "sorted_ranks", "[", "ii", "]", "==", "ii", ")", "[", "0", "]", "[", "0", "]", ",", "\n", "\"gt-path\"", ":", "meta", "[", "\"paths\"", "]", "[", "ii", "]", ",", "\n", "\"top-k-sims\"", ":", "-", "dists", "[", "ii", "]", "[", "ranked_idx", "]", ",", "\n", "\"top-k-paths\"", ":", "np", ".", "array", "(", "meta", "[", "\"paths\"", "]", ")", "[", "ranked_idx", "]", ",", "\n", "\"hide-gt\"", ":", "hide_gt", ",", "\n", "}", "\n", "rankings", ".", "append", "(", "datum", ")", "\n", "", "self", ".", "display_current_results", "(", "\n", "rankings", ",", "\n", "epoch", "=", "epoch", ",", "\n", "metrics", "=", "nested_metrics", "[", "\"t2v_metrics\"", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.visualizer.Visualizer.display_current_results": [[74, 129], ["print", "html.HTML", "html.HTML.add_header", "html.HTML.add_header", "print", "print", "html.HTML.save", "pathlib.Path().exists", "pathlib.Path().mkdir", "print", "enumerate", "html.HTML.add_videos", "txts.append", "links.append", "vids.append", "txts.append", "links.append", "vids.append", "zip", "pathlib.Path", "txts.append", "vids.append", "links.append", "pathlib.Path", "pathlib.Path", "len", "len", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.html.HTML.add_header", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.html.HTML.add_header", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.html.HTML.save", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdir", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.html.HTML.add_videos"], ["", "def", "display_current_results", "(", "self", ",", "rankings", ",", "epoch", ",", "metrics", ")", ":", "\n", "        ", "\"\"\"Display current results on visdom; save current results to an HTML file.\n\n        Parameters:\n            visuals (OrderedDict) - - dictionary of images to display or save\n            epoch (int) - - the current epoch\n            save_result (bool) - - if save the current results to an HTML file\n        \"\"\"", "\n", "if", "not", "Path", "(", "self", ".", "web_dir", ")", ".", "exists", "(", ")", ":", "\n", "            ", "Path", "(", "self", ".", "web_dir", ")", ".", "mkdir", "(", "exist_ok", "=", "True", ",", "parents", "=", "True", ")", "\n", "", "print", "(", "f\"updating webpage at {self.web_dir}\"", ")", "\n", "title", "=", "f\"Experiment name = {self.name}\"", "\n", "refresh", "=", "True", "\n", "if", "not", "refresh", ":", "\n", "            ", "print", "(", "\"DISABLING WEB PAGE REFRESH\"", ")", "\n", "", "webpage", "=", "html", ".", "HTML", "(", "web_dir", "=", "self", ".", "web_dir", ",", "title", "=", "title", ",", "refresh", "=", "refresh", ")", "\n", "\n", "msg", "=", "f\"epoch [{epoch}] - {self.name}\"", "\n", "webpage", ".", "add_header", "(", "msg", ")", "\n", "msg", "=", "(", "f\"R1: {metrics['R1']:.1f}, \"", "\n", "f\"R5: {metrics['R5']:.1f}, \"", "\n", "f\"R10: {metrics['R10']:.1f}, \"", "\n", "f\"MedR: {metrics['MedR']}\"", ")", "\n", "webpage", ".", "add_header", "(", "msg", ")", "\n", "print", "(", "f\"Top {len(rankings[0])} retreived videos at epoch: {epoch}\"", ")", "\n", "\n", "for", "ranking", "in", "rankings", ":", "\n", "            ", "vids", ",", "txts", ",", "links", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "gt_vid_path", "=", "ranking", "[", "\"gt-path\"", "]", "\n", "gt_captions", "=", "[", "\" \"", ".", "join", "(", "x", ")", "for", "x", "in", "ranking", "[", "\"gt-captions\"", "]", "]", "\n", "gt_captions", "=", "\"<br>\"", ".", "join", "(", "gt_captions", ")", "\n", "if", "ranking", "[", "\"hide-gt\"", "]", ":", "\n", "                ", "txts", ".", "append", "(", "gt_captions", ")", "\n", "links", ".", "append", "(", "\"hidden\"", ")", "\n", "vids", ".", "append", "(", "\"hidden\"", ")", "\n", "", "else", ":", "\n", "                ", "txt", "=", "(", "f\"{gt_captions}<br><b>Rank: {ranking['gt-rank']}, \"", "\n", "f\"Sim: {ranking['gt-sim']:.3f} [{Path(ranking['gt-path']).stem}]\"", ")", "\n", "txts", ".", "append", "(", "txt", ")", "\n", "links", ".", "append", "(", "gt_vid_path", ")", "\n", "vids", ".", "append", "(", "gt_vid_path", ")", "\n", "\n", "", "for", "idx", ",", "(", "vid_path", ",", "sim", ")", "in", "enumerate", "(", "zip", "(", "ranking", "[", "\"top-k-paths\"", "]", ",", "\n", "ranking", "[", "\"top-k-sims\"", "]", ")", ")", ":", "\n", "                ", "vid_path", "=", "Path", "(", "vid_path", ")", "\n", "if", "ranking", "[", "\"hide-gt\"", "]", ":", "\n", "                    ", "txt", "=", "f\"choice: {idx}\"", "\n", "", "else", ":", "\n", "                    ", "txt", "=", "f\"<b>Rank: {idx}, Sim: {sim:.3f}, [{Path(vid_path).stem}]\"", "\n", "", "txts", ".", "append", "(", "txt", ")", "\n", "vids", ".", "append", "(", "vid_path", ")", "\n", "links", ".", "append", "(", "vid_path", ")", "\n", "", "webpage", ".", "add_videos", "(", "vids", ",", "txts", ",", "links", ",", "width", "=", "200", ")", "\n", "", "print", "(", "f\"added {len(vids)} videos\"", ")", "\n", "webpage", ".", "save", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.datastructures.ExpertStore.__init__": [[16, 24], ["numpy.zeros", "set", "datastructures.ExpertStore.rebuild_keymap", "len"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.datastructures.ExpertStore.rebuild_keymap"], ["    ", "def", "__init__", "(", "self", ",", "keylist", ",", "dim", ",", "dtype", "=", "np", ".", "float16", ")", ":", "\n", "        ", "self", ".", "keys", "=", "keylist", "\n", "self", ".", "dim", "=", "dim", "\n", "self", ".", "store_dtype", "=", "dtype", "\n", "self", ".", "store", "=", "np", ".", "zeros", "(", "(", "len", "(", "keylist", ")", ",", "dim", ")", ",", "dtype", "=", "dtype", ")", "\n", "self", ".", "keymap", "=", "{", "}", "\n", "self", ".", "missing", "=", "set", "(", ")", "\n", "self", ".", "rebuild_keymap", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.datastructures.ExpertStore.__setitem__": [[25, 33], ["isinstance", "numpy.isnan"], "methods", ["None"], ["", "def", "__setitem__", "(", "self", ",", "key", ",", "value", ")", ":", "\n", "        ", "idx", "=", "self", ".", "keymap", "[", "key", "]", "\n", "if", "isinstance", "(", "value", ",", "np", ".", "ndarray", ")", ":", "\n", "# non-nan values must be vectors of the appropriate size", "\n", "            ", "assert", "value", ".", "size", "==", "self", ".", "dim", ",", "f\"cannot set value with size {value.size}\"", "\n", "", "else", ":", "\n", "            ", "assert", "np", ".", "isnan", "(", "value", ")", "\n", "", "self", ".", "store", "[", "idx", "]", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.datastructures.ExpertStore.rebuild_keymap": [[34, 37], ["enumerate"], "methods", ["None"], ["", "def", "rebuild_keymap", "(", "self", ")", ":", "\n", "        ", "for", "idx", ",", "key", "in", "enumerate", "(", "self", ".", "keys", ")", ":", "\n", "            ", "self", ".", "keymap", "[", "key", "]", "=", "idx", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.datastructures.ExpertStore.filter_keys": [[38, 58], ["set", "print", "numpy.array", "print", "datastructures.ExpertStore.rebuild_keymap", "set", "missing.intersection", "print", "print", "numpy.array", "set", "list", "len", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.datastructures.ExpertStore.rebuild_keymap"], ["", "", "def", "filter_keys", "(", "self", ",", "keys", ",", "tag", ",", "allow_mismatch", "=", "\"\"", ",", "exceptions", "=", "None", ")", ":", "\n", "        ", "keyset", "=", "set", "(", "keys", ")", "\n", "missing", "=", "keyset", "-", "set", "(", "self", ".", "keys", ")", "\n", "if", "exceptions", "is", "not", "None", "and", "missing", ":", "\n", "            ", "excluded", "=", "missing", ".", "intersection", "(", "set", "(", "exceptions", ")", ")", "\n", "print", "(", "f\"filter_keys >>> applying exceptions for {len(excluded)} videos\"", ")", "\n", "missing", "=", "missing", "-", "excluded", "\n", "", "print", "(", "f\"filter_keys >>> {tag}\"", ")", "\n", "if", "allow_mismatch", "and", "missing", ":", "\n", "            ", "print", "(", "f\"Key mismatch (missing {len(missing)}) {allow_mismatch}\"", ")", "\n", "", "else", ":", "\n", "            ", "samples", "=", "list", "(", "missing", ")", "[", ":", "3", "]", "\n", "msg", "=", "f\"cannot apply filter since missing {len(missing)} keys e.g. {samples}\"", "\n", "assert", "not", "missing", ",", "msg", "\n", "", "keep", "=", "np", ".", "array", "(", "[", "x", "in", "keyset", "for", "x", "in", "self", ".", "keys", "]", ")", "\n", "filtered_keys", "=", "np", ".", "array", "(", "self", ".", "keys", ")", "[", "keep", "]", "\n", "print", "(", "f\"Filtering from {len(self.keys)} keys to {len(filtered_keys)} keys\"", ")", "\n", "self", ".", "keys", "=", "filtered_keys", "\n", "self", ".", "store", "=", "self", ".", "store", "[", "keep", "]", "\n", "self", ".", "rebuild_keymap", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.datastructures.ExpertStore.__getitem__": [[59, 61], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "key", ")", ":", "\n", "        ", "return", "self", ".", "store", "[", "self", ".", "keymap", "[", "key", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.datastructures.ExpertStore.__len__": [[62, 64], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "keys", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.datastructures.ExpertStore.__repr__": [[65, 75], ["list", "datastructures.ExpertStore.keymap.items", "len", "humanize.naturalsize"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "keep_samples", "=", "3", "\n", "samples", "=", "list", "(", "self", ".", "keymap", ".", "items", "(", ")", ")", "[", ":", "keep_samples", "]", "\n", "sample_str", "=", "\"\\n\"", ".", "join", "(", "[", "f\"{key}: {val}\"", "for", "key", ",", "val", "in", "samples", "]", ")", "\n", "summary", "=", "(", "\n", "f\"ExpertStore object with {len(self.keys)} features (dim: {self.dim})\"", "\n", "f\" (storage is using {humanize.naturalsize(self.store.nbytes)})\"", "\n", "f\"\\nFirst {keep_samples} elements of keymap: \\n{sample_str}\"", "\n", ")", "\n", "return", "summary", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.datastructures.gen_dict_store": [[77, 82], ["dict", "numpy.random.rand().astype", "numpy.random.rand"], "function", ["None"], ["", "", "def", "gen_dict_store", "(", "keylist", ",", "dim", ")", ":", "\n", "    ", "store", "=", "dict", "(", ")", "\n", "for", "key", "in", "keylist", ":", "\n", "        ", "store", "[", "key", "]", "=", "np", ".", "random", ".", "rand", "(", "1", ",", "dim", ")", ".", "astype", "(", "np", ".", "float16", ")", "\n", "", "return", "store", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.datastructures.main": [[84, 106], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "get_data_paths", "open", "sorted", "pickle.dumps", "print", "f.read().splitlines", "datastructures.gen_dict_store", "numpy.random.rand().astype", "f.read", "datastructures.ExpertStore", "print", "humanize.naturalsize", "numpy.random.rand", "len", "len"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.datastructures.gen_dict_store"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--dataset\"", ",", "default", "=", "\"moments-in-time\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dim\"", ",", "type", "=", "int", ",", "default", "=", "2048", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "from", "config", "import", "get_data_paths", "\n", "data_paths", "=", "get_data_paths", "(", "args", ".", "dataset", ")", "\n", "relevant_path", "=", "data_paths", "[", "\"relevant-id-list\"", "]", "\n", "with", "open", "(", "relevant_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "relevant_ids", "=", "sorted", "(", "f", ".", "read", "(", ")", ".", "splitlines", "(", ")", ")", "\n", "\n", "", "for", "store_name", "in", "\"dict\"", ",", "\"np\"", ",", "\"expert_store\"", ":", "\n", "        ", "if", "store_name", "==", "\"dict\"", ":", "\n", "            ", "store", "=", "gen_dict_store", "(", "keylist", "=", "relevant_ids", ",", "dim", "=", "args", ".", "dim", ")", "\n", "", "elif", "store_name", "==", "\"np\"", ":", "\n", "            ", "store", "=", "np", ".", "random", ".", "rand", "(", "len", "(", "relevant_ids", ")", ",", "args", ".", "dim", ")", ".", "astype", "(", "np", ".", "float16", ")", "\n", "", "elif", "store_name", "==", "\"expert_store\"", ":", "\n", "            ", "store", "=", "ExpertStore", "(", "keylist", "=", "relevant_ids", ",", "dim", "=", "args", ".", "dim", ")", "\n", "print", "(", "store", ")", "\n", "", "serialised", "=", "pickle", ".", "dumps", "(", "store", ")", "\n", "print", "(", "f\"Memory needs for {store_name}: {humanize.naturalsize(len(serialised))}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.cos_restart.CosineAnnealingWithRestartsLR.__init__": [[32, 40], ["torch.optim.lr_scheduler._LRScheduler.__init__"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["def", "__init__", "(", "self", ",", "optimizer", ",", "T_max", ",", "eta_min", "=", "0", ",", "last_epoch", "=", "-", "1", ",", "T_mult", "=", "1", ")", ":", "\n", "        ", "self", ".", "T_max", "=", "T_max", "\n", "self", ".", "T_mult", "=", "T_mult", "\n", "self", ".", "restart_every", "=", "T_max", "\n", "self", ".", "eta_min", "=", "eta_min", "\n", "self", ".", "restarts", "=", "0", "\n", "self", ".", "restarted_at", "=", "0", "\n", "super", "(", ")", ".", "__init__", "(", "optimizer", ",", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.cos_restart.CosineAnnealingWithRestartsLR.restart": [[41, 44], ["None"], "methods", ["None"], ["", "def", "restart", "(", "self", ")", ":", "\n", "        ", "self", ".", "restart_every", "*=", "self", ".", "T_mult", "\n", "self", ".", "restarted_at", "=", "self", ".", "last_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.cos_restart.CosineAnnealingWithRestartsLR.cosine": [[45, 48], ["math.cos"], "methods", ["None"], ["", "def", "cosine", "(", "self", ",", "base_lr", ")", ":", "\n", "        ", "return", "self", ".", "eta_min", "+", "(", "base_lr", "-", "self", ".", "eta_min", ")", "*", "(", "1", "+", "math", ".", "cos", "(", "math", ".", "pi", "*", "self", ".", "step_n", "/", "self", ".", "restart_every", ")", ")", "/", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.cos_restart.CosineAnnealingWithRestartsLR.step_n": [[49, 52], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "step_n", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "last_epoch", "-", "self", ".", "restarted_at", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.cos_restart.CosineAnnealingWithRestartsLR.get_lr": [[53, 57], ["cos_restart.CosineAnnealingWithRestartsLR.restart", "cos_restart.CosineAnnealingWithRestartsLR.cosine"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.cos_restart.CosineAnnealingWithRestartsLR.restart", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.cos_restart.CosineAnnealingWithRestartsLR.cosine"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "step_n", ">=", "self", ".", "restart_every", ":", "\n", "            ", "self", ".", "restart", "(", ")", "\n", "", "return", "[", "self", ".", "cosine", "(", "base_lr", ")", "for", "base_lr", "in", "self", ".", "base_lrs", "]", "", "", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.radam.RAdam.__init__": [[7, 11], ["dict", "torch.optim.optimizer.Optimizer.__init__", "range"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "weight_decay", "=", "0", ")", ":", "\n", "        ", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ")", "\n", "self", ".", "buffer", "=", "[", "[", "None", ",", "None", ",", "None", "]", "for", "ind", "in", "range", "(", "10", ")", "]", "\n", "super", "(", "RAdam", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.radam.RAdam.__setstate__": [[12, 14], ["super().__setstate__"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.ranger.Ranger.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "RAdam", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.radam.RAdam.step": [[15, 79], ["closure", "p.grad.data.float", "p.data.float", "exp_avg_sq.mul_().addcmul_", "exp_avg.mul_().add_", "p.data.copy_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "state[].type_as", "state[].type_as", "p.data.float.add_", "exp_avg_sq.sqrt().add_", "p.data.float.addcdiv_", "p.data.float.add_", "exp_avg_sq.mul_", "exp_avg.mul_", "int", "math.sqrt", "exp_avg_sq.sqrt"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "\n", "        ", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", ".", "float", "(", ")", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'RAdam does not support sparse gradients'", ")", "\n", "\n", "", "p_data_fp32", "=", "p", ".", "data", ".", "float", "(", ")", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p_data_fp32", ")", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p_data_fp32", ")", "\n", "", "else", ":", "\n", "                    ", "state", "[", "'exp_avg'", "]", "=", "state", "[", "'exp_avg'", "]", ".", "type_as", "(", "p_data_fp32", ")", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "state", "[", "'exp_avg_sq'", "]", ".", "type_as", "(", "p_data_fp32", ")", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "buffered", "=", "self", ".", "buffer", "[", "int", "(", "state", "[", "'step'", "]", "%", "10", ")", "]", "\n", "if", "state", "[", "'step'", "]", "==", "buffered", "[", "0", "]", ":", "\n", "                    ", "N_sma", ",", "step_size", "=", "buffered", "[", "1", "]", ",", "buffered", "[", "2", "]", "\n", "", "else", ":", "\n", "                    ", "buffered", "[", "0", "]", "=", "state", "[", "'step'", "]", "\n", "beta2_t", "=", "beta2", "**", "state", "[", "'step'", "]", "\n", "N_sma_max", "=", "2", "/", "(", "1", "-", "beta2", ")", "-", "1", "\n", "N_sma", "=", "N_sma_max", "-", "2", "*", "state", "[", "'step'", "]", "*", "beta2_t", "/", "(", "1", "-", "beta2_t", ")", "\n", "buffered", "[", "1", "]", "=", "N_sma", "\n", "\n", "# more conservative since it's an approximated value", "\n", "if", "N_sma", ">=", "5", ":", "\n", "                        ", "step_size", "=", "math", ".", "sqrt", "(", "(", "1", "-", "beta2_t", ")", "*", "(", "N_sma", "-", "4", ")", "/", "(", "N_sma_max", "-", "4", ")", "*", "(", "N_sma", "-", "2", ")", "/", "N_sma", "*", "N_sma_max", "/", "(", "N_sma_max", "-", "2", ")", ")", "/", "(", "1", "-", "beta1", "**", "state", "[", "'step'", "]", ")", "\n", "", "else", ":", "\n", "                        ", "step_size", "=", "1.0", "/", "(", "1", "-", "beta1", "**", "state", "[", "'step'", "]", ")", "\n", "", "buffered", "[", "2", "]", "=", "step_size", "\n", "\n", "", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p_data_fp32", ".", "add_", "(", "-", "group", "[", "'weight_decay'", "]", "*", "group", "[", "'lr'", "]", ",", "p_data_fp32", ")", "\n", "\n", "# more conservative since it's an approximated value", "\n", "", "if", "N_sma", ">=", "5", ":", "\n", "                    ", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "p_data_fp32", ".", "addcdiv_", "(", "-", "step_size", "*", "group", "[", "'lr'", "]", ",", "exp_avg", ",", "denom", ")", "\n", "", "else", ":", "\n", "                    ", "p_data_fp32", ".", "add_", "(", "-", "step_size", "*", "group", "[", "'lr'", "]", ",", "exp_avg", ")", "\n", "\n", "", "p", ".", "data", ".", "copy_", "(", "p_data_fp32", ")", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.radam.PlainRAdam.__init__": [[82, 86], ["dict", "torch.optim.optimizer.Optimizer.__init__"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "weight_decay", "=", "0", ")", ":", "\n", "        ", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ")", "\n", "\n", "super", "(", "PlainRAdam", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.radam.PlainRAdam.__setstate__": [[87, 89], ["super().__setstate__"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.ranger.Ranger.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "PlainRAdam", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.radam.PlainRAdam.step": [[90, 143], ["closure", "p.grad.data.float", "p.data.float", "exp_avg_sq.mul_().addcmul_", "exp_avg.mul_().add_", "p.data.copy_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "state[].type_as", "state[].type_as", "p.data.float.add_", "exp_avg_sq.sqrt().add_", "p.data.float.addcdiv_", "p.data.float.add_", "exp_avg_sq.mul_", "exp_avg.mul_", "math.sqrt", "exp_avg_sq.sqrt"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "\n", "        ", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", ".", "float", "(", ")", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'RAdam does not support sparse gradients'", ")", "\n", "\n", "", "p_data_fp32", "=", "p", ".", "data", ".", "float", "(", ")", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p_data_fp32", ")", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p_data_fp32", ")", "\n", "", "else", ":", "\n", "                    ", "state", "[", "'exp_avg'", "]", "=", "state", "[", "'exp_avg'", "]", ".", "type_as", "(", "p_data_fp32", ")", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "state", "[", "'exp_avg_sq'", "]", ".", "type_as", "(", "p_data_fp32", ")", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "beta2_t", "=", "beta2", "**", "state", "[", "'step'", "]", "\n", "N_sma_max", "=", "2", "/", "(", "1", "-", "beta2", ")", "-", "1", "\n", "N_sma", "=", "N_sma_max", "-", "2", "*", "state", "[", "'step'", "]", "*", "beta2_t", "/", "(", "1", "-", "beta2_t", ")", "\n", "\n", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p_data_fp32", ".", "add_", "(", "-", "group", "[", "'weight_decay'", "]", "*", "group", "[", "'lr'", "]", ",", "p_data_fp32", ")", "\n", "\n", "# more conservative since it's an approximated value", "\n", "", "if", "N_sma", ">=", "5", ":", "\n", "                    ", "step_size", "=", "group", "[", "'lr'", "]", "*", "math", ".", "sqrt", "(", "(", "1", "-", "beta2_t", ")", "*", "(", "N_sma", "-", "4", ")", "/", "(", "N_sma_max", "-", "4", ")", "*", "(", "N_sma", "-", "2", ")", "/", "N_sma", "*", "N_sma_max", "/", "(", "N_sma_max", "-", "2", ")", ")", "/", "(", "1", "-", "beta1", "**", "state", "[", "'step'", "]", ")", "\n", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "p_data_fp32", ".", "addcdiv_", "(", "-", "step_size", ",", "exp_avg", ",", "denom", ")", "\n", "", "else", ":", "\n", "                    ", "step_size", "=", "group", "[", "'lr'", "]", "/", "(", "1", "-", "beta1", "**", "state", "[", "'step'", "]", ")", "\n", "p_data_fp32", ".", "add_", "(", "-", "step_size", ",", "exp_avg", ")", "\n", "\n", "", "p", ".", "data", ".", "copy_", "(", "p_data_fp32", ")", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.radam.AdamW.__init__": [[147, 151], ["dict", "torch.optim.optimizer.Optimizer.__init__"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "weight_decay", "=", "0", ",", "warmup", "=", "0", ")", ":", "\n", "        ", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "warmup", "=", "warmup", ")", "\n", "super", "(", "AdamW", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.radam.AdamW.__setstate__": [[152, 154], ["super().__setstate__"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.ranger.Ranger.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "AdamW", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.radam.AdamW.step": [[155, 208], ["closure", "p.grad.data.float", "p.data.float", "exp_avg_sq.mul_().addcmul_", "exp_avg.mul_().add_", "exp_avg_sq.sqrt().add_", "p.data.float.addcdiv_", "p.data.copy_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "state[].type_as", "state[].type_as", "p.data.float.add_", "exp_avg_sq.mul_", "exp_avg.mul_", "exp_avg_sq.sqrt", "math.sqrt"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", ".", "float", "(", ")", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Adam does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "p_data_fp32", "=", "p", ".", "data", ".", "float", "(", ")", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p_data_fp32", ")", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p_data_fp32", ")", "\n", "", "else", ":", "\n", "                    ", "state", "[", "'exp_avg'", "]", "=", "state", "[", "'exp_avg'", "]", ".", "type_as", "(", "p_data_fp32", ")", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "state", "[", "'exp_avg_sq'", "]", ".", "type_as", "(", "p_data_fp32", ")", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "\n", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "bias_correction1", "=", "1", "-", "beta1", "**", "state", "[", "'step'", "]", "\n", "bias_correction2", "=", "1", "-", "beta2", "**", "state", "[", "'step'", "]", "\n", "\n", "if", "group", "[", "'warmup'", "]", ">", "state", "[", "'step'", "]", ":", "\n", "                    ", "scheduled_lr", "=", "1e-8", "+", "state", "[", "'step'", "]", "*", "group", "[", "'lr'", "]", "/", "group", "[", "'warmup'", "]", "\n", "", "else", ":", "\n", "                    ", "scheduled_lr", "=", "group", "[", "'lr'", "]", "\n", "\n", "", "step_size", "=", "scheduled_lr", "*", "math", ".", "sqrt", "(", "bias_correction2", ")", "/", "bias_correction1", "\n", "\n", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p_data_fp32", ".", "add_", "(", "-", "group", "[", "'weight_decay'", "]", "*", "scheduled_lr", ",", "p_data_fp32", ")", "\n", "\n", "", "p_data_fp32", ".", "addcdiv_", "(", "-", "step_size", ",", "exp_avg", ",", "denom", ")", "\n", "\n", "p", ".", "data", ".", "copy_", "(", "p_data_fp32", ")", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.Timer.__init__": [[298, 300], ["datetime.datetime.datetime.now"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "cache", "=", "datetime", ".", "now", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.Timer.check": [[301, 306], ["datetime.datetime.datetime.now", "duration.total_seconds"], "methods", ["None"], ["", "def", "check", "(", "self", ")", ":", "\n", "        ", "now", "=", "datetime", ".", "now", "(", ")", "\n", "duration", "=", "now", "-", "self", ".", "cache", "\n", "self", ".", "cache", "=", "now", "\n", "return", "duration", ".", "total_seconds", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.Timer.reset": [[307, 309], ["datetime.datetime.datetime.now"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "cache", "=", "datetime", ".", "now", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.filter_cmd_args": [[25, 38], ["sorted", "cmd_args.index", "drop.append", "cmd_args.pop", "drop.append", "len", "cmd_args[].startswith"], "function", ["None"], ["@", "typechecked", "\n", "def", "filter_cmd_args", "(", "cmd_args", ":", "List", "[", "str", "]", ",", "remove", ":", "List", "[", "str", "]", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "drop", "=", "[", "]", "\n", "for", "key", "in", "remove", ":", "\n", "        ", "if", "key", "not", "in", "cmd_args", ":", "\n", "            ", "continue", "\n", "", "pos", "=", "cmd_args", ".", "index", "(", "key", ")", "\n", "drop", ".", "append", "(", "pos", ")", "\n", "if", "len", "(", "cmd_args", ")", ">", "(", "pos", "+", "1", ")", "and", "not", "cmd_args", "[", "pos", "+", "1", "]", ".", "startswith", "(", "\"--\"", ")", ":", "\n", "            ", "drop", ".", "append", "(", "pos", "+", "1", ")", "\n", "", "", "for", "pos", "in", "sorted", "(", "drop", ",", "reverse", "=", "True", ")", ":", "\n", "        ", "cmd_args", ".", "pop", "(", "pos", ")", "\n", "", "return", "cmd_args", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.get_short_uuid": [[40, 48], ["str().split", "str", "uuid.uuid4"], "function", ["None"], ["", "@", "typechecked", "\n", "def", "get_short_uuid", "(", ")", "->", "str", ":", "\n", "    ", "\"\"\"Return a 7 alpha-numeric character random string.  We could use the full uuid()\n    for better uniqueness properties, but it makes the filenames long and its not\n    needed for our purpose (simply grouping experiments that were run with the same\n    configuration).\n    \"\"\"", "\n", "return", "str", "(", "uuid", ".", "uuid4", "(", ")", ")", ".", "split", "(", "\"-\"", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.parse_grid": [[50, 82], ["x.split", "util.get_short_uuid", "enumerate", "grid_opts.items", "list", "grid_idx.append", "list.append", "itertools.product", "copy.deepcopy", "zip", "copy.deepcopy.append", "parsed.append", "token.split"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.get_short_uuid", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["", "@", "typechecked", "\n", "def", "parse_grid", "(", "x", ":", "str", ")", "->", "Dict", "[", "str", ",", "List", "[", "str", "]", "]", ":", "\n", "    ", "\"\"\"Parse compact command line strings of the form:\n        --key1 val_a|val_b --key2 val_c|val_d\n\n    (here a vertical bar represents multiple values)\n\n    into a grid of separate strings e.g:\n        --key1 val_a --key2 val_c\n        --key1 val_a --key2 val_d\n        --key1 val_b --key2 val_c\n        --key1 val_b --key2 val_d\n\n    \"\"\"", "\n", "args", "=", "x", ".", "split", "(", "\" \"", ")", "\n", "group_id", "=", "get_short_uuid", "(", ")", "\n", "grid_opts", ",", "parsed", "=", "{", "}", ",", "[", "]", "\n", "for", "ii", ",", "token", "in", "enumerate", "(", "args", ")", ":", "\n", "        ", "if", "\"|\"", "in", "token", ":", "\n", "            ", "grid_opts", "[", "ii", "]", "=", "token", ".", "split", "(", "\"|\"", ")", "\n", "", "", "grid_idx", ",", "grid_vals", "=", "[", "]", ",", "[", "]", "\n", "for", "ii", ",", "val", "in", "grid_opts", ".", "items", "(", ")", ":", "\n", "        ", "grid_idx", ".", "append", "(", "ii", ")", "\n", "grid_vals", ".", "append", "(", "val", ")", "\n", "", "grid_vals", "=", "list", "(", "itertools", ".", "product", "(", "*", "grid_vals", ")", ")", "\n", "for", "cfg", "in", "grid_vals", ":", "\n", "        ", "base", "=", "copy", ".", "deepcopy", "(", "args", ")", "\n", "for", "ii", ",", "val", "in", "zip", "(", "grid_idx", ",", "cfg", ")", ":", "\n", "            ", "base", "[", "ii", "]", "=", "val", "\n", "", "base", ".", "append", "(", "f\"--group_id {group_id}\"", ")", "\n", "parsed", ".", "append", "(", "\" \"", ".", "join", "(", "base", ")", ")", "\n", "", "return", "{", "group_id", ":", "parsed", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.set_seeds": [[84, 94], ["random.seed", "numpy.random.seed", "torch.manual_seed"], "function", ["None"], ["", "@", "typechecked", "\n", "def", "set_seeds", "(", "seed", ":", "int", ")", ":", "\n", "    ", "\"\"\"Set seeds for randomisation libraries.\n\n    Args:\n        seed: the seed value\n    \"\"\"", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.update_src_web_video_dir": [[96, 117], ["pathlib.Path", "pathlib.Path", "str", "pathlib.Path"], "function", ["None"], ["", "def", "update_src_web_video_dir", "(", "config", ")", ":", "\n", "    ", "\"\"\"Provide backwards compatible support for web directories\n\n    Args:\n        config: a configuration object containing experiment paths\n    \"\"\"", "\n", "src_video_dir", "=", "Path", "(", "config", "[", "\"visualizer\"", "]", "[", "\"args\"", "]", "[", "\"src_video_dir\"", "]", ")", "\n", "dataset", "=", "config", "[", "\"data_loader\"", "]", "[", "\"args\"", "]", "[", "\"dataset_name\"", "]", "\n", "if", "dataset", "not", "in", "str", "(", "src_video_dir", ")", ":", "\n", "        ", "lookup", "=", "{", "\n", "\"ActivityNet\"", ":", "\"activity-net/videos\"", ",", "\n", "\"MSRVTT\"", ":", "\"MSRVTT/videos/all\"", ",", "\n", "\"MSVD\"", ":", "\"MSVD/videos\"", ",", "\n", "\"DiDeMo\"", ":", "\"DiDeMo/videos\"", ",", "\n", "\"LSMDC\"", ":", "\"LSMDC/videos\"", ",", "\n", "\"YouCook2\"", ":", "\"YouCook2/videos\"", ",", "\n", "\"QuerYD\"", ":", "\"QuerYD/videos\"", ",", "\n", "\"QuerYDSegments\"", ":", "\"QuerYDSegments/videos\"", ",", "\n", "}", "\n", "src_video_dir", "=", "Path", "(", "src_video_dir", ".", "parts", "[", "0", "]", ")", "/", "lookup", "[", "dataset", "]", "\n", "", "config", "[", "\"visualizer\"", "]", "[", "\"args\"", "]", "[", "\"src_video_dir\"", "]", "=", "Path", "(", "src_video_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.memory_summary": [[119, 126], ["psutil.virtual_memory", "print", "humanize.naturalsize", "humanize.naturalsize"], "function", ["None"], ["", "def", "memory_summary", "(", ")", ":", "\n", "    ", "vmem", "=", "psutil", ".", "virtual_memory", "(", ")", "\n", "msg", "=", "(", "\n", "f\">>> Currently using {vmem.percent}% of system memory \"", "\n", "f\"{humanize.naturalsize(vmem.used)}/{humanize.naturalsize(vmem.available)}\"", "\n", ")", "\n", "print", "(", "msg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.flatten_dict": [[128, 138], ["x.items", "isinstance", "util.flatten_dict", "flat_dict.update", "flat_dict.update", "flatten_dict.items"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.flatten_dict", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["", "def", "flatten_dict", "(", "x", ",", "keysep", "=", "\"-\"", ")", ":", "\n", "    ", "flat_dict", "=", "{", "}", "\n", "for", "key", ",", "val", "in", "x", ".", "items", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "val", ",", "dict", ")", ":", "\n", "            ", "flat_subdict", "=", "flatten_dict", "(", "val", ")", "\n", "flat_dict", ".", "update", "(", "{", "f\"{key}{keysep}{subkey}\"", ":", "subval", "\n", "for", "subkey", ",", "subval", "in", "flat_subdict", ".", "items", "(", ")", "}", ")", "\n", "", "else", ":", "\n", "            ", "flat_dict", ".", "update", "(", "{", "key", ":", "val", "}", ")", "\n", "", "", "return", "flat_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.expert_tensor_storage": [[140, 160], ["feat_aggregation.items", "expert_storage.items", "set", "set", "set", "config.get", "value.intersection", "expert_storage[].add", "all", "expert_storage[].add", "set", "expert_storage[].add", "ValueError", "config[].split"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.add", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.add", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.add"], ["", "def", "expert_tensor_storage", "(", "experts", ",", "feat_aggregation", ")", ":", "\n", "    ", "expert_storage", "=", "{", "\"fixed\"", ":", "set", "(", ")", ",", "\"variable\"", ":", "set", "(", ")", ",", "\"flaky\"", ":", "set", "(", ")", "}", "\n", "# fixed_sz_experts, variable_sz_experts, flaky_experts = set(), set(), set()", "\n", "for", "expert", ",", "config", "in", "feat_aggregation", ".", "items", "(", ")", ":", "\n", "        ", "if", "config", "[", "\"temporal\"", "]", "in", "{", "\"vlad\"", "}", ":", "\n", "            ", "expert_storage", "[", "\"variable\"", "]", ".", "add", "(", "expert", ")", "\n", "", "elif", "all", "(", "[", "x", "in", "{", "\"avg\"", ",", "\"max\"", ",", "\"ent\"", ",", "\"std\"", "}", "for", "x", "in", "config", "[", "\"temporal\"", "]", ".", "split", "(", "\"-\"", ")", "]", ")", ":", "\n", "            ", "expert_storage", "[", "\"fixed\"", "]", ".", "add", "(", "expert", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "f\"unknown temporal strategy: {config['temporal']}\"", ")", "\n", "# some \"flaky\" experts are only available for a fraction of videos - we need", "\n", "# to pass this information (in the form of indices) into the network for any", "\n", "# experts present in the current dataset", "\n", "", "if", "config", ".", "get", "(", "\"flaky\"", ",", "False", ")", ":", "\n", "            ", "expert_storage", "[", "\"flaky\"", "]", ".", "add", "(", "expert", ")", "\n", "\n", "# we only allocate storage for experts used by the current dataset", "\n", "", "", "for", "key", ",", "value", "in", "expert_storage", ".", "items", "(", ")", ":", "\n", "        ", "expert_storage", "[", "key", "]", "=", "value", ".", "intersection", "(", "set", "(", "experts", ")", ")", "\n", "", "return", "expert_storage", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.read_json": [[162, 165], ["fname.open", "json.load"], "function", ["None"], ["", "def", "read_json", "(", "fname", ")", ":", "\n", "    ", "with", "fname", ".", "open", "(", "'rt'", ")", "as", "handle", ":", "\n", "        ", "return", "json", ".", "load", "(", "handle", ",", "object_hook", "=", "OrderedDict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.path2str": [[167, 174], ["x.items", "isinstance", "util.path2str", "isinstance", "str"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.path2str"], ["", "", "def", "path2str", "(", "x", ")", ":", "\n", "    ", "\"\"\"Recursively convert pathlib objects to strings to enable serialization\"\"\"", "\n", "for", "key", ",", "val", "in", "x", ".", "items", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "val", ",", "dict", ")", ":", "\n", "            ", "path2str", "(", "val", ")", "\n", "", "elif", "isinstance", "(", "val", ",", "Path", ")", ":", "\n", "            ", "x", "[", "key", "]", "=", "str", "(", "val", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.write_json": [[176, 181], ["util.path2str", "fname.open", "json.dump"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.path2str"], ["", "", "", "def", "write_json", "(", "content", ",", "fname", ",", "paths2strs", "=", "False", ")", ":", "\n", "    ", "if", "paths2strs", ":", "\n", "        ", "path2str", "(", "content", ")", "\n", "", "with", "fname", ".", "open", "(", "'wt'", ")", "as", "handle", ":", "\n", "        ", "json", ".", "dump", "(", "content", ",", "handle", ",", "indent", "=", "4", ",", "sort_keys", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.inf_loop": [[183, 187], ["itertools.repeat"], "function", ["None"], ["", "", "def", "inf_loop", "(", "data_loader", ")", ":", "\n", "    ", "''' wrapper function for endless data loader. '''", "\n", "for", "loader", "in", "itertools", ".", "repeat", "(", "data_loader", ")", ":", "\n", "        ", "yield", "from", "loader", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.compute_trn_config": [[189, 197], ["feat_agg.keys", "feat_agg[].keys"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys"], ["", "", "def", "compute_trn_config", "(", "config", ",", "logger", "=", "None", ")", ":", "\n", "    ", "trn_config", "=", "{", "}", "\n", "feat_agg", "=", "config", "[", "\"data_loader\"", "]", "[", "\"args\"", "]", "[", "\"feat_aggregation\"", "]", "\n", "for", "static_expert", "in", "feat_agg", ".", "keys", "(", ")", ":", "\n", "        ", "if", "static_expert", "in", "feat_agg", ":", "\n", "            ", "if", "\"trn_seg\"", "in", "feat_agg", "[", "static_expert", "]", ".", "keys", "(", ")", ":", "\n", "                ", "trn_config", "[", "static_expert", "]", "=", "feat_agg", "[", "static_expert", "]", "[", "\"trn_seg\"", "]", "\n", "", "", "", "return", "trn_config", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.compute_dims": [[199, 289], ["sorted", "collections.OrderedDict", "collections.OrderedDict", "collections.OrderedDict.items", "config.get_logger", "experts[].split", "config.get_logger.info", "dims.append", "hasattr", "open", "json.load", "arch_args.get", "arch_args.get", "vlad_clusters.get", "arch_args[].get", "[].get", "arch_args[].get", "arch_args[].get", "len", "[].split"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get_logger", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get"], ["", "@", "typechecked", "\n", "def", "compute_dims", "(", "\n", "config", ",", "\n", "logger", ":", "logging", ".", "Logger", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "Dict", "[", "str", ",", "Tuple", "[", "int", ",", "int", "]", "]", ",", "Dict", "[", "str", ",", "int", "]", ",", "int", "]", ":", "\n", "    ", "if", "logger", "is", "None", ":", "\n", "        ", "logger", "=", "config", ".", "get_logger", "(", "'utils'", ")", "\n", "\n", "", "experts", "=", "config", "[", "\"experts\"", "]", "\n", "# TODO(Samuel): clean up the logic since it's a little convoluted", "\n", "ordered", "=", "sorted", "(", "config", "[", "\"experts\"", "]", "[", "\"modalities\"", "]", ")", "\n", "\n", "if", "experts", "[", "\"drop_feats\"", "]", ":", "\n", "        ", "to_drop", "=", "experts", "[", "\"drop_feats\"", "]", ".", "split", "(", "\",\"", ")", "\n", "logger", ".", "info", "(", "f\"dropping: {to_drop}\"", ")", "\n", "ordered", "=", "[", "x", "for", "x", "in", "ordered", "if", "x", "not", "in", "to_drop", "]", "\n", "\n", "", "feat_agg", "=", "config", "[", "\"data_loader\"", "]", "[", "\"args\"", "]", "[", "\"feat_aggregation\"", "]", "\n", "dims", "=", "[", "]", "\n", "arch_args", "=", "config", "[", "\"arch\"", "]", "[", "\"args\"", "]", "\n", "vlad_clusters", "=", "arch_args", "[", "\"vlad_clusters\"", "]", "\n", "msg", "=", "f\"It is not valid to use both the `use_ce` and `mimic_ce_dims` options\"", "\n", "assert", "not", "(", "arch_args", "[", "\"use_ce\"", "]", "and", "arch_args", ".", "get", "(", "\"mimic_ce_dims\"", ",", "False", ")", ")", ",", "msg", "\n", "for", "expert", "in", "ordered", ":", "\n", "        ", "temporal", "=", "feat_agg", "[", "expert", "]", "[", "\"temporal\"", "]", "\n", "if", "expert", "==", "\"face\"", ":", "\n", "            ", "in_dim", ",", "out_dim", "=", "experts", "[", "\"face_dim\"", "]", ",", "experts", "[", "\"face_dim\"", "]", "\n", "", "elif", "expert", "in", "{", "\"audio\"", ",", "\"audio.vggish.0\"", "}", "and", "temporal", "==", "\"vlad\"", ":", "\n", "            ", "in_dim", ",", "out_dim", "=", "128", "*", "vlad_clusters", "[", "\"audio\"", "]", ",", "128", "\n", "", "elif", "expert", "==", "\"speech\"", "and", "temporal", "==", "\"vlad\"", ":", "\n", "            ", "in_dim", ",", "out_dim", "=", "300", "*", "vlad_clusters", "[", "\"speech\"", "]", ",", "300", "\n", "", "elif", "expert", "==", "\"ocr\"", "and", "temporal", "==", "\"vlad\"", ":", "\n", "            ", "in_dim", ",", "out_dim", "=", "300", "*", "vlad_clusters", "[", "\"ocr\"", "]", ",", "300", "\n", "", "elif", "expert", "==", "\"detection\"", ":", "\n", "# allow for avg pooling", "\n", "            ", "det_clusters", "=", "arch_args", "[", "\"vlad_clusters\"", "]", ".", "get", "(", "\"detection\"", ",", "1", ")", "\n", "in_dim", ",", "out_dim", "=", "1541", "*", "det_clusters", ",", "1541", "\n", "", "elif", "expert", "==", "\"detection-sem\"", ":", "\n", "            ", "if", "config", "[", "\"data_loader\"", "]", "[", "\"args\"", "]", ".", "get", "(", "\"spatial_feats\"", ",", "False", ")", ":", "\n", "                ", "base", "=", "300", "+", "16", "\n", "", "else", ":", "\n", "                ", "base", "=", "300", "+", "5", "\n", "", "det_clusters", "=", "arch_args", "[", "\"vlad_clusters\"", "]", ".", "get", "(", "\"detection-sem\"", ",", "1", ")", "\n", "in_dim", ",", "out_dim", "=", "base", "*", "det_clusters", ",", "base", "\n", "", "elif", "expert", "==", "\"openpose\"", ":", "\n", "            ", "base", "=", "54", "\n", "det_clusters", "=", "arch_args", "[", "\"vlad_clusters\"", "]", ".", "get", "(", "\"openpose\"", ",", "1", ")", "\n", "in_dim", ",", "out_dim", "=", "base", "*", "det_clusters", ",", "base", "\n", "", "else", ":", "\n", "            ", "common_dim", "=", "feat_agg", "[", "expert", "]", "[", "\"feat_dims\"", "]", "[", "feat_agg", "[", "expert", "]", "[", "\"type\"", "]", "]", "\n", "# account for aggregation of multilpe forms (e.g. avg + max pooling)", "\n", "common_dim", "=", "common_dim", "*", "len", "(", "feat_agg", "[", "expert", "]", "[", "\"temporal\"", "]", ".", "split", "(", "\"-\"", ")", ")", "\n", "in_dim", ",", "out_dim", "=", "common_dim", ",", "common_dim", "\n", "\n", "# For the CE architecture, we need to project all features to a common", "\n", "# dimensionality", "\n", "", "is_ce", "=", "config", "[", "\"arch\"", "]", "[", "\"type\"", "]", "==", "\"CENet\"", "\n", "if", "is_ce", "and", "(", "arch_args", "[", "\"use_ce\"", "]", "or", "arch_args", ".", "get", "(", "\"mimic_ce_dims\"", ",", "False", ")", ")", ":", "\n", "            ", "out_dim", "=", "experts", "[", "\"ce_shared_dim\"", "]", "\n", "\n", "", "dims", ".", "append", "(", "(", "expert", ",", "(", "in_dim", ",", "out_dim", ")", ")", ")", "\n", "", "expert_dims", "=", "OrderedDict", "(", "dims", ")", "\n", "\n", "if", "vlad_clusters", "[", "\"text\"", "]", "==", "0", ":", "\n", "        ", "msg", "=", "\"vlad can only be disabled for text with single tokens\"", "\n", "assert", "config", "[", "\"data_loader\"", "]", "[", "\"args\"", "]", "[", "\"max_tokens\"", "]", "[", "\"text\"", "]", "==", "1", ",", "msg", "\n", "\n", "", "if", "config", "[", "\"experts\"", "]", "[", "\"text_agg\"", "]", "==", "\"avg\"", ":", "\n", "        ", "if", "hasattr", "(", "config", "[", "\"arch\"", "]", "[", "\"args\"", "]", ",", "\"vlad_clusters\"", ")", ":", "\n", "            ", "msg", "=", "\"averaging can only be performed with text using single tokens\"", "\n", "assert", "config", "[", "\"arch\"", "]", "[", "\"args\"", "]", "[", "\"vlad_clusters\"", "]", "[", "\"text\"", "]", "==", "0", ",", "msg", "\n", "", "assert", "config", "[", "\"data_loader\"", "]", "[", "\"args\"", "]", "[", "\"max_tokens\"", "]", "[", "\"text\"", "]", "==", "1", "\n", "\n", "# To remove the dependency of dataloader on the model architecture, we create a", "\n", "# second copy of the expert dimensions which accounts for the number of vlad", "\n", "# clusters", "\n", "", "raw_input_dims", "=", "OrderedDict", "(", ")", "\n", "for", "expert", ",", "dim_pair", "in", "expert_dims", ".", "items", "(", ")", ":", "\n", "        ", "raw_dim", "=", "dim_pair", "[", "0", "]", "\n", "if", "expert", "in", "{", "\"audio\"", ",", "\"speech\"", ",", "\"ocr\"", ",", "\"detection\"", ",", "\"detection-sem\"", ",", "\"openpose\"", ",", "\n", "\"speech.mozilla.0\"", "}", ":", "\n", "            ", "if", "feat_agg", "[", "expert", "]", "[", "\"temporal\"", "]", "==", "\"vlad\"", ":", "\n", "                ", "raw_dim", "=", "raw_dim", "//", "vlad_clusters", ".", "get", "(", "expert", ",", "1", ")", "\n", "", "", "raw_input_dims", "[", "expert", "]", "=", "raw_dim", "\n", "\n", "", "with", "open", "(", "config", "[", "\"text_embedding_model_configs\"", "]", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "text_embedding_model_configs", "=", "json", ".", "load", "(", "f", ")", "\n", "", "text_dim", "=", "text_embedding_model_configs", "[", "experts", "[", "\"text_feat\"", "]", "]", "[", "\"dim\"", "]", "\n", "\n", "return", "expert_dims", ",", "raw_input_dims", ",", "text_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.ensure_tensor": [[291, 295], ["isinstance", "torch.from_numpy"], "function", ["None"], ["", "def", "ensure_tensor", "(", "x", ")", ":", "\n", "    ", "if", "not", "isinstance", "(", "x", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "x", "=", "torch", ".", "from_numpy", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.tensor2im": [[311, 332], ["np.tile.astype", "isinstance", "isinstance", "image_tensor[].cpu().float().numpy", "numpy.tile", "image_tensor[].cpu().float", "numpy.transpose", "image_tensor[].cpu"], "function", ["None"], ["", "", "def", "tensor2im", "(", "input_image", ",", "imtype", "=", "np", ".", "uint8", ")", ":", "\n", "    ", "\"\"\"\"Converts a Tensor array into a numpy image array.\n\n    Parameters:\n        input_image (tensor) --  the input image tensor array\n        imtype (type)        --  the desired type of the converted numpy array\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "input_image", ",", "np", ".", "ndarray", ")", ":", "\n", "        ", "if", "isinstance", "(", "input_image", ",", "torch", ".", "Tensor", ")", ":", "# get the data from a variable", "\n", "            ", "image_tensor", "=", "input_image", ".", "data", "\n", "", "else", ":", "\n", "            ", "return", "input_image", "\n", "# convert it into a numpy array", "\n", "", "image_numpy", "=", "image_tensor", "[", "0", "]", ".", "cpu", "(", ")", ".", "float", "(", ")", ".", "numpy", "(", ")", "\n", "if", "image_numpy", ".", "shape", "[", "0", "]", "==", "1", ":", "# grayscale to RGB", "\n", "            ", "image_numpy", "=", "np", ".", "tile", "(", "image_numpy", ",", "(", "3", ",", "1", ",", "1", ")", ")", "\n", "# post-processing: tranpose and scaling", "\n", "", "image_numpy", "=", "(", "np", ".", "transpose", "(", "image_numpy", ",", "(", "1", ",", "2", ",", "0", ")", ")", "+", "1", ")", "/", "2.0", "*", "255.0", "\n", "", "else", ":", "# if it is a numpy array, do nothing", "\n", "        ", "image_numpy", "=", "input_image", "\n", "", "return", "image_numpy", ".", "astype", "(", "imtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.save_image": [[334, 343], ["PIL.Image.fromarray", "Image.fromarray.save"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.html.HTML.save"], ["", "def", "save_image", "(", "image_numpy", ",", "image_path", ")", ":", "\n", "    ", "\"\"\"Save a numpy image to the disk\n\n    Parameters:\n        image_numpy (numpy array) -- input numpy array\n        image_path (str)          -- the path of the image\n    \"\"\"", "\n", "image_pil", "=", "Image", ".", "fromarray", "(", "image_numpy", ")", "\n", "image_pil", ".", "save", "(", "image_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.print_numpy": [[345, 359], ["x.flatten.astype", "print", "x.flatten.flatten", "print", "numpy.mean", "numpy.min", "numpy.max", "numpy.median", "numpy.std"], "function", ["None"], ["", "def", "print_numpy", "(", "x", ",", "val", "=", "True", ",", "shp", "=", "False", ")", ":", "\n", "    ", "\"\"\"Print the mean, min, max, median, std, and size of a numpy array\n\n    Parameters:\n        val (bool) -- if print the values of the numpy array\n        shp (bool) -- if print the shape of the numpy array\n    \"\"\"", "\n", "x", "=", "x", ".", "astype", "(", "np", ".", "float64", ")", "\n", "if", "shp", ":", "\n", "        ", "print", "(", "'shape,'", ",", "x", ".", "shape", ")", "\n", "", "if", "val", ":", "\n", "        ", "x", "=", "x", ".", "flatten", "(", ")", "\n", "print", "(", "'mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f'", "%", "(", "\n", "np", ".", "mean", "(", "x", ")", ",", "np", ".", "min", "(", "x", ")", ",", "np", ".", "max", "(", "x", ")", ",", "np", ".", "median", "(", "x", ")", ",", "np", ".", "std", "(", "x", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdirs": [[361, 372], ["isinstance", "util.mkdir", "isinstance", "util.mkdir"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdir", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdir"], ["", "", "def", "mkdirs", "(", "paths", ")", ":", "\n", "    ", "\"\"\"create empty directories if they don't exist\n\n    Parameters:\n        paths (str list) -- a list of directory paths\n    \"\"\"", "\n", "if", "isinstance", "(", "paths", ",", "list", ")", "and", "not", "isinstance", "(", "paths", ",", "str", ")", ":", "\n", "        ", "for", "path", "in", "paths", ":", "\n", "            ", "mkdir", "(", "path", ")", "\n", "", "", "else", ":", "\n", "        ", "mkdir", "(", "paths", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdir": [[374, 382], ["os.path.exists", "os.makedirs"], "function", ["None"], ["", "", "def", "mkdir", "(", "path", ")", ":", "\n", "    ", "\"\"\"create a single empty directory if it didn't exist\n\n    Parameters:\n        path (str) -- a single directory path\n    \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "path", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.gen_ablations_for_dataset.handle_moee_config": [[13, 23], ["None"], "function", ["None"], ["def", "handle_moee_config", "(", "config", ")", ":", "\n", "    ", "\"\"\"For the official ablations on MSRVTT, we provide MoEE with the same hyperparam\n    budget as CE and run a search to find the best hyperparams.  For the unofficial\n    ablations, we use the same padding/VLAD settings as CE.\n    \"\"\"", "\n", "config", "=", "{", "\n", "\"inherit_from\"", ":", "config", "[", "\"inherit_from\"", "]", ",", "\n", "\"arch\"", ":", "{", "\"type\"", ":", "\"CENet\"", ",", "\"args\"", ":", "{", "\"use_ce\"", ":", "\"\"", "}", "}", ",", "\n", "}", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.gen_ablations_for_dataset.remove_audio_streams": [[25, 41], ["dest_path.replace.replace", "[].remove"], "function", ["None"], ["", "def", "remove_audio_streams", "(", "config", ",", "dest_path", ")", ":", "\n", "    ", "\"\"\"Prune audio-based features from the config and dest_path name (necessary for\n    datasets like MSVD which do not possess sound.)  If the audio feature was the control\n    variable in the experiment, we return False for the dest_path, such that the ablation\n    is removed altogether.\n    \"\"\"", "\n", "audio_tags", "=", "[", "\"audio\"", ",", "\"speech\"", "]", "\n", "for", "audio_tag", "in", "audio_tags", ":", "\n", "        ", "if", "f\"-{audio_tag}.\"", "in", "dest_path", ":", "\n", "            ", "return", "config", ",", "False", "\n", "\n", "", "dest_path", "=", "dest_path", ".", "replace", "(", "f\"-{audio_tag}\"", ",", "\"\"", ")", "\n", "if", "\"experts\"", "in", "config", "and", "\"modalities\"", "in", "config", "[", "\"experts\"", "]", ":", "\n", "            ", "if", "audio_tag", "in", "config", "[", "\"experts\"", "]", "[", "\"modalities\"", "]", ":", "\n", "                ", "config", "[", "\"experts\"", "]", "[", "\"modalities\"", "]", ".", "remove", "(", "audio_tag", ")", "\n", "", "", "", "return", "config", ",", "dest_path", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.gen_ablations_for_dataset.main": [[43, 90], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "print", "pathlib.Path", "open", "parser.parse_args.exp_list.replace", "pathlib.Path.exists", "print", "row.split", "any", "config_path.replace", "config[].replace", "print", "output_rows.append", "print", "open", "json.load", "gen_ablations_for_dataset.handle_moee_config", "gen_ablations_for_dataset.remove_audio_streams", "open", "json.dump", "open", "sorted", "f.read().splitlines", "len", "pathlib.Path", "list", "f.write", "set", "f.read"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.gen_ablations_for_dataset.handle_moee_config", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.gen_ablations_for_dataset.remove_audio_streams"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--refresh'", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "'--update_ablation_list'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--src_dataset'", ",", "default", "=", "\"msrvtt\"", ")", "\n", "parser", ".", "add_argument", "(", "'--dest_dataset'", ",", "default", "=", "\"lsmdc\"", ")", "\n", "parser", ".", "add_argument", "(", "'--exp_list'", ",", "default", "=", "\"slurm/msrvtt-ablations.txt\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "with", "open", "(", "args", ".", "exp_list", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "exps", "=", "[", "x", "for", "x", "in", "f", ".", "read", "(", ")", ".", "splitlines", "(", ")", "if", "x", "]", "\n", "\n", "", "print", "(", "f\"Found {len(exps)} experiments in {args.exp_list}\"", ")", "\n", "dest_exp_path", "=", "Path", "(", "args", ".", "exp_list", ".", "replace", "(", "\"msrvtt\"", ",", "args", ".", "dest_dataset", ")", ")", "\n", "if", "dest_exp_path", ".", "exists", "(", ")", "and", "not", "args", ".", "refresh", ":", "\n", "        ", "print", "(", "f\"experiment list found at {dest_exp_path}, skipping...\"", ")", "\n", "return", "\n", "\n", "", "output_rows", "=", "[", "]", "\n", "exclude", "=", "[", "\"miech\"", ",", "\"jsfusion\"", "]", "\n", "for", "row", "in", "exps", ":", "\n", "        ", "flag", ",", "config_path", ",", "seed_flag", ",", "seed_opts", "=", "row", ".", "split", "(", ")", "\n", "if", "any", "(", "[", "x", "in", "config_path", "for", "x", "in", "exclude", "]", ")", ":", "\n", "            ", "continue", "\n", "", "with", "open", "(", "config_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "config", "=", "json", ".", "load", "(", "f", ")", "\n", "", "if", "Path", "(", "config_path", ")", ".", "stem", "==", "\"train-full-moee\"", ":", "\n", "            ", "config", "=", "handle_moee_config", "(", "config", ")", "\n", "", "dest_path", "=", "config_path", ".", "replace", "(", "args", ".", "src_dataset", ",", "args", ".", "dest_dataset", ")", "\n", "config", "[", "\"inherit_from\"", "]", "=", "config", "[", "\"inherit_from\"", "]", ".", "replace", "(", "args", ".", "src_dataset", ",", "\n", "args", ".", "dest_dataset", ")", "\n", "if", "args", ".", "dest_dataset", "==", "\"msvd\"", ":", "\n", "            ", "config", ",", "dest_path", "=", "remove_audio_streams", "(", "config", ",", "dest_path", ")", "\n", "if", "not", "dest_path", ":", "\n", "                ", "continue", "\n", "\n", "", "", "print", "(", "f\"writing config to {dest_path}\"", ")", "\n", "with", "open", "(", "dest_path", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "config", ",", "f", ",", "indent", "=", "4", ",", "sort_keys", "=", "False", ")", "\n", "", "output_rows", ".", "append", "(", "[", "flag", ",", "dest_path", ",", "seed_flag", ",", "seed_opts", "]", ")", "\n", "\n", "", "if", "args", ".", "update_ablation_list", ":", "\n", "        ", "print", "(", "f\"Writing new experiment list to {dest_exp_path}\"", ")", "\n", "output_rows", "=", "[", "\" \"", ".", "join", "(", "x", ")", "for", "x", "in", "output_rows", "]", "\n", "with", "open", "(", "dest_exp_path", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "for", "row", "in", "sorted", "(", "list", "(", "set", "(", "output_rows", ")", ")", ")", ":", "\n", "                ", "f", ".", "write", "(", "f\"{row}\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.ranger.Ranger.__init__": [[28, 63], ["dict", "torch.optim.optimizer.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "range"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "alpha", "=", "0.5", ",", "k", "=", "6", ",", "N_sma_threshhold", "=", "5", ",", "betas", "=", "(", ".95", ",", "0.999", ")", ",", "eps", "=", "1e-5", ",", "weight_decay", "=", "0", ")", ":", "\n", "#parameter checks", "\n", "        ", "if", "not", "0.0", "<=", "alpha", "<=", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "f'Invalid slow update rate: {alpha}'", ")", "\n", "", "if", "not", "1", "<=", "k", ":", "\n", "            ", "raise", "ValueError", "(", "f'Invalid lookahead steps: {k}'", ")", "\n", "", "if", "not", "lr", ">", "0", ":", "\n", "            ", "raise", "ValueError", "(", "f'Invalid Learning Rate: {lr}'", ")", "\n", "", "if", "not", "eps", ">", "0", ":", "\n", "            ", "raise", "ValueError", "(", "f'Invalid eps: {eps}'", ")", "\n", "\n", "#parameter comments:", "\n", "# beta1 (momentum) of .95 seems to work better than .90...", "\n", "#N_sma_threshold of 5 seems better in testing than 4.", "\n", "#In both cases, worth testing on your dataset (.90 vs .95, 4 vs 5) to make sure which works best for you.", "\n", "\n", "#prep defaults and init torch.optim base", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "alpha", "=", "alpha", ",", "k", "=", "k", ",", "step_counter", "=", "0", ",", "betas", "=", "betas", ",", "N_sma_threshhold", "=", "N_sma_threshhold", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ")", "\n", "super", "(", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n", "#adjustable threshold", "\n", "self", ".", "N_sma_threshhold", "=", "N_sma_threshhold", "\n", "\n", "#now we can get to work...", "\n", "#removed as we now use step from RAdam...no need for duplicate step counting", "\n", "#for group in self.param_groups:", "\n", "#    group[\"step_counter\"] = 0", "\n", "#print(\"group step counter init\")", "\n", "\n", "#look ahead params", "\n", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "k", "=", "k", "\n", "\n", "#radam buffer for state", "\n", "self", ".", "radam_buffer", "=", "[", "[", "None", ",", "None", ",", "None", "]", "for", "ind", "in", "range", "(", "10", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.ranger.Ranger.__setstate__": [[77, 80], ["print", "super().__setstate__"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.ranger.Ranger.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "print", "(", "\"set state called\"", ")", "\n", "super", "(", "Ranger", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.ranger.Ranger.step": [[82, 166], ["p.grad.data.float", "p.data.float", "exp_avg_sq.mul_().addcmul_", "exp_avg.mul_().add_", "p.data.copy_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "torch.empty_like", "state[].copy_", "state[].type_as", "state[].type_as", "p.data.float.add_", "exp_avg_sq.sqrt().add_", "p.data.float.addcdiv_", "p.data.float.add_", "slow_p.add_", "p.data.copy_", "exp_avg_sq.mul_", "exp_avg.mul_", "int", "math.sqrt", "exp_avg_sq.sqrt"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "loss", "=", "None", "\n", "#note - below is commented out b/c I have other work that passes back the loss as a float, and thus not a callable closure.  ", "\n", "#Uncomment if you need to use the actual closure...", "\n", "\n", "#if closure is not None:", "\n", "#loss = closure()", "\n", "\n", "#Evaluate averages and grad, update param tensors", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", ".", "float", "(", ")", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Ranger optimizer does not support sparse gradients'", ")", "\n", "\n", "", "p_data_fp32", "=", "p", ".", "data", ".", "float", "(", ")", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "#get state dict for this param", "\n", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "#if first time to run...init dictionary with our desired entries", "\n", "#if self.first_run_check==0:", "\n", "#self.first_run_check=1", "\n", "#print(\"Initializing slow buffer...should not see this at load from saved model!\")", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p_data_fp32", ")", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p_data_fp32", ")", "\n", "\n", "#look ahead weight storage now in state dict ", "\n", "state", "[", "'slow_buffer'", "]", "=", "torch", ".", "empty_like", "(", "p", ".", "data", ")", "\n", "state", "[", "'slow_buffer'", "]", ".", "copy_", "(", "p", ".", "data", ")", "\n", "\n", "", "else", ":", "\n", "                    ", "state", "[", "'exp_avg'", "]", "=", "state", "[", "'exp_avg'", "]", ".", "type_as", "(", "p_data_fp32", ")", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "state", "[", "'exp_avg_sq'", "]", ".", "type_as", "(", "p_data_fp32", ")", "\n", "\n", "#begin computations ", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "\n", "#compute variance mov avg", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "#compute mean moving avg", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "\n", "buffered", "=", "self", ".", "radam_buffer", "[", "int", "(", "state", "[", "'step'", "]", "%", "10", ")", "]", "\n", "if", "state", "[", "'step'", "]", "==", "buffered", "[", "0", "]", ":", "\n", "                    ", "N_sma", ",", "step_size", "=", "buffered", "[", "1", "]", ",", "buffered", "[", "2", "]", "\n", "", "else", ":", "\n", "                    ", "buffered", "[", "0", "]", "=", "state", "[", "'step'", "]", "\n", "beta2_t", "=", "beta2", "**", "state", "[", "'step'", "]", "\n", "N_sma_max", "=", "2", "/", "(", "1", "-", "beta2", ")", "-", "1", "\n", "N_sma", "=", "N_sma_max", "-", "2", "*", "state", "[", "'step'", "]", "*", "beta2_t", "/", "(", "1", "-", "beta2_t", ")", "\n", "buffered", "[", "1", "]", "=", "N_sma", "\n", "if", "N_sma", ">", "self", ".", "N_sma_threshhold", ":", "\n", "                        ", "step_size", "=", "math", ".", "sqrt", "(", "(", "1", "-", "beta2_t", ")", "*", "(", "N_sma", "-", "4", ")", "/", "(", "N_sma_max", "-", "4", ")", "*", "(", "N_sma", "-", "2", ")", "/", "N_sma", "*", "N_sma_max", "/", "(", "N_sma_max", "-", "2", ")", ")", "/", "(", "1", "-", "beta1", "**", "state", "[", "'step'", "]", ")", "\n", "", "else", ":", "\n", "                        ", "step_size", "=", "1.0", "/", "(", "1", "-", "beta1", "**", "state", "[", "'step'", "]", ")", "\n", "", "buffered", "[", "2", "]", "=", "step_size", "\n", "\n", "", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p_data_fp32", ".", "add_", "(", "-", "group", "[", "'weight_decay'", "]", "*", "group", "[", "'lr'", "]", ",", "p_data_fp32", ")", "\n", "\n", "", "if", "N_sma", ">", "self", ".", "N_sma_threshhold", ":", "\n", "                    ", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "p_data_fp32", ".", "addcdiv_", "(", "-", "step_size", "*", "group", "[", "'lr'", "]", ",", "exp_avg", ",", "denom", ")", "\n", "", "else", ":", "\n", "                    ", "p_data_fp32", ".", "add_", "(", "-", "step_size", "*", "group", "[", "'lr'", "]", ",", "exp_avg", ")", "\n", "\n", "", "p", ".", "data", ".", "copy_", "(", "p_data_fp32", ")", "\n", "\n", "#integrated look ahead...", "\n", "#we do it at the param level instead of group level", "\n", "if", "state", "[", "'step'", "]", "%", "group", "[", "'k'", "]", "==", "0", ":", "\n", "                    ", "slow_p", "=", "state", "[", "'slow_buffer'", "]", "#get access to slow param tensor", "\n", "slow_p", ".", "add_", "(", "self", ".", "alpha", ",", "p", ".", "data", "-", "slow_p", ")", "#(fast weights - slow weights) * alpha", "\n", "p", ".", "data", ".", "copy_", "(", "slow_p", ")", "#copy interpolated weights to RAdam param tensor", "\n", "\n", "", "", "", "return", "loss", "", "", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.mil_nce_net.MNNet.__init__": [[11, 23], ["list", "base.BaseModel.__init__", "torch.nn.Parameter", "expert_dims.keys", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys"], ["    ", "@", "typechecked", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "text_dim", ":", "int", ",", "\n", "expert_dims", ":", "Dict", "[", "str", ",", "Tuple", "[", "int", ",", "int", "]", "]", ",", "\n", "**", "_unused", ",", "\n", ")", ":", "\n", "        ", "self", ".", "text_dim", "=", "text_dim", "\n", "self", ".", "expert_dims", "=", "expert_dims", "\n", "self", ".", "modalities", "=", "list", "(", "expert_dims", ".", "keys", "(", ")", ")", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dummy_param", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "1", ")", "*", "1E-5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.mil_nce_net.MNNet.forward": [[24, 45], ["mil_nce_net.MNNet.sanity_checks", "next", "text.view.view.view", "torch.matmul", "iter", "next.t", "experts.values"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.sanity_checks"], ["", "@", "typechecked", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "text", ":", "torch", ".", "Tensor", ",", "\n", "ind", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "experts", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "**", "_unused", ",", "\n", ")", ":", "\n", "        ", "self", ".", "sanity_checks", "(", "text", "=", "text", ",", "experts", "=", "experts", ",", "ind", "=", "ind", ")", "\n", "vid_embedding", "=", "next", "(", "iter", "(", "experts", ".", "values", "(", ")", ")", ")", "\n", "vid_embedding", "=", "self", ".", "dummy_param", "+", "vid_embedding", "\n", "text", "=", "text", ".", "view", "(", "text", ".", "shape", "[", "0", "]", "*", "text", ".", "shape", "[", "1", "]", ",", "text", ".", "shape", "[", "-", "1", "]", ")", "\n", "# text = text / torch.norm(text, p=2, dim=1).reshape(-1, 1)", "\n", "# vid_embedding = vid_embedding / torch.norm(vid_embedding, p=2,", "\n", "#                                            dim=1).reshape(-1, 1)", "\n", "sims", "=", "torch", ".", "matmul", "(", "text", ",", "vid_embedding", ".", "t", "(", ")", ")", "\n", "return", "{", "\n", "\"modalities\"", ":", "self", ".", "modalities", ",", "\n", "\"cross_view_conf_matrix\"", ":", "sims", ",", "\n", "\"text_embds\"", ":", "{", "self", ".", "modalities", "[", "0", "]", ":", "text", "}", ",", "\n", "\"vid_embds\"", ":", "{", "self", ".", "modalities", "[", "0", "]", ":", "vid_embedding", "}", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.mil_nce_net.MNNet.sanity_checks": [[47, 66], ["len", "len", "ind[].sum", "len"], "methods", ["None"], ["", "@", "typechecked", "\n", "def", "sanity_checks", "(", "\n", "self", ",", "\n", "text", ":", "torch", ".", "Tensor", ",", "\n", "ind", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "experts", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", ")", ":", "\n", "        ", "msg", "=", "f\"Text dim {text.shape[-1]} did not match expected {self.text_dim}\"", "\n", "assert", "text", ".", "shape", "[", "-", "1", "]", "==", "self", ".", "text_dim", ",", "msg", "\n", "assert", "len", "(", "experts", ")", "==", "1", ",", "\"Expected single modality experts\"", "\n", "assert", "len", "(", "text", ".", "shape", ")", "==", "4", ",", "\"Expected four axes for text input\"", "\n", "assert", "text", ".", "shape", "[", "2", "]", "==", "1", ",", "\"Expected singleton for text input on dim 2\"", "\n", "for", "expert", "in", "self", ".", "expert_dims", ":", "\n", "            ", "msg", "=", "f\"Expected all features to be present for {expert}\"", "\n", "assert", "ind", "[", "expert", "]", ".", "sum", "(", ")", "==", "len", "(", "ind", "[", "expert", "]", ")", ",", "msg", "\n", "feats", "=", "experts", "[", "expert", "]", "\n", "expected", "=", "self", ".", "expert_dims", "[", "expert", "]", "\n", "msg", "=", "f\"Feature shape {feats.shape[1]} did not match expected {expected}\"", "\n", "assert", "feats", ".", "shape", "[", "1", "]", "==", "expected", "[", "-", "1", "]", ",", "msg", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.net_vlad.NetVLAD.__init__": [[26, 43], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "math.sqrt", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "cluster_size", ",", "feature_size", ",", "ghost_clusters", "=", "0", ",", "\n", "add_batch_norm", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "feature_size", "=", "feature_size", "\n", "self", ".", "cluster_size", "=", "cluster_size", "\n", "self", ".", "ghost_clusters", "=", "ghost_clusters", "\n", "\n", "init_sc", "=", "(", "1", "/", "math", ".", "sqrt", "(", "feature_size", ")", ")", "\n", "clusters", "=", "cluster_size", "+", "ghost_clusters", "\n", "\n", "# The `clusters` weights are the `(w,b)` in the paper", "\n", "self", ".", "clusters", "=", "nn", ".", "Parameter", "(", "init_sc", "*", "th", ".", "randn", "(", "feature_size", ",", "clusters", ")", ")", "\n", "self", ".", "batch_norm", "=", "nn", ".", "BatchNorm1d", "(", "clusters", ")", "if", "add_batch_norm", "else", "None", "\n", "# The `clusters2` weights are the visual words `c_k` in the paper", "\n", "self", ".", "clusters2", "=", "nn", ".", "Parameter", "(", "init_sc", "*", "th", ".", "randn", "(", "1", ",", "feature_size", ",", "cluster_size", ")", ")", "\n", "self", ".", "out_dim", "=", "self", ".", "cluster_size", "*", "feature_size", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.net_vlad.NetVLAD.forward": [[44, 88], ["net_vlad.NetVLAD.sanity_checks", "x.view.view.view", "torch.matmul", "torch.matmul", "torch.matmul", "torch.softmax", "torch.softmax", "torch.softmax", "net_vlad.NetVLAD.view", "torch.sum", "torch.sum", "torch.sum", "net_vlad.NetVLAD.transpose", "x.view.view.view", "torch.matmul", "torch.matmul", "torch.matmul", "torch.normalize.transpose", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize.reshape", "torch.normalize", "torch.normalize", "torch.normalize", "x.view.view.size", "ValueError", "net_vlad.NetVLAD.batch_norm"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.sanity_checks"], ["", "def", "forward", "(", "self", ",", "x", ",", "mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"Aggregates feature maps into a fixed size representation.  In the following\n        notation, B = batch_size, N = num_features, K = num_clusters, D = feature_size.\n\n        Args:\n            x (th.Tensor): B x N x D\n\n        Returns:\n            (th.Tensor): B x DK\n        \"\"\"", "\n", "self", ".", "sanity_checks", "(", "x", ")", "\n", "max_sample", "=", "x", ".", "size", "(", ")", "[", "1", "]", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "self", ".", "feature_size", ")", "# B x N x D -> BN x D", "\n", "\n", "if", "x", ".", "device", "!=", "self", ".", "clusters", ".", "device", ":", "\n", "            ", "msg", "=", "f\"x.device {x.device} != cluster.device {self.clusters.device}\"", "\n", "raise", "ValueError", "(", "msg", ")", "\n", "\n", "", "assignment", "=", "th", ".", "matmul", "(", "x", ",", "self", ".", "clusters", ")", "# (BN x D) x (D x (K+G)) -> BN x (K+G)", "\n", "\n", "if", "self", ".", "batch_norm", ":", "\n", "            ", "assignment", "=", "self", ".", "batch_norm", "(", "assignment", ")", "\n", "\n", "", "assignment", "=", "F", ".", "softmax", "(", "assignment", ",", "dim", "=", "1", ")", "# BN x (K+G) -> BN x (K+G)", "\n", "# remove ghost assigments", "\n", "assignment", "=", "assignment", "[", ":", ",", ":", "self", ".", "cluster_size", "]", "\n", "assignment", "=", "assignment", ".", "view", "(", "-", "1", ",", "max_sample", ",", "self", ".", "cluster_size", ")", "# -> B x N x K", "\n", "a_sum", "=", "th", ".", "sum", "(", "assignment", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "# B x N x K -> B x 1 x K", "\n", "a", "=", "a_sum", "*", "self", ".", "clusters2", "\n", "\n", "assignment", "=", "assignment", ".", "transpose", "(", "1", ",", "2", ")", "# B x N x K -> B x K x N", "\n", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "max_sample", ",", "self", ".", "feature_size", ")", "# BN x D -> B x N x D", "\n", "vlad", "=", "th", ".", "matmul", "(", "assignment", ",", "x", ")", "# (B x K x N) x (B x N x D) -> B x K x D", "\n", "vlad", "=", "vlad", ".", "transpose", "(", "1", ",", "2", ")", "# -> B x D x K", "\n", "vlad", "=", "vlad", "-", "a", "\n", "\n", "# L2 intra norm", "\n", "vlad", "=", "F", ".", "normalize", "(", "vlad", ")", "\n", "\n", "# flattening + L2 norm", "\n", "vlad", "=", "vlad", ".", "reshape", "(", "-", "1", ",", "self", ".", "cluster_size", "*", "self", ".", "feature_size", ")", "# -> B x DK", "\n", "vlad", "=", "F", ".", "normalize", "(", "vlad", ")", "\n", "return", "vlad", "# B x DK", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.net_vlad.NetVLAD.sanity_checks": [[89, 97], ["torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.sum", "torch.sum", "torch.sum", "print", "ipdb.set_trace", "print", "ipdb.set_trace"], "methods", ["None"], ["", "def", "sanity_checks", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Catch any nans in the inputs/clusters\"\"\"", "\n", "if", "th", ".", "isnan", "(", "th", ".", "sum", "(", "x", ")", ")", ":", "\n", "            ", "print", "(", "\"nan inputs\"", ")", "\n", "ipdb", ".", "set_trace", "(", ")", "\n", "", "if", "th", ".", "isnan", "(", "self", ".", "clusters", "[", "0", "]", "[", "0", "]", ")", ":", "\n", "            ", "print", "(", "\"nan clusters\"", ")", "\n", "ipdb", ".", "set_trace", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.text.TextEmbedding.__init__": [[28, 40], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "model", ":", "Callable", ",", "\n", "tokenizer", ":", "Union", "[", "Callable", ",", "None", "]", ",", "\n", "dim", ":", "int", ",", "\n", "remove_stopwords", ":", "bool", ",", "\n", ")", ":", "\n", "        ", "self", ".", "dim", "=", "dim", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "remove_stopwords", "=", "remove_stopwords", "\n", "self", ".", "device", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.text.TextEmbedding.text2vec": [[41, 57], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "text2vec", "(", "self", ",", "text", ":", "str", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"Convert a string of text into an embedding.\n\n        Args:\n            text: the content to be embedded\n\n        Returns:\n            (d x n) array, where d is the dimensionality of the embedding and `n` is the\n                number of words that were successfully parsed from the text string.\n\n        NOTE: For some text embedding models (such as word2vec), not all words are\n        converted to vectors (e.g. certain kinds of stop words) - these are dropped from\n        the output.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.text.TextEmbedding.set_device": [[58, 62], ["text.TextEmbedding.model.to"], "methods", ["None"], ["", "@", "typechecked", "\n", "def", "set_device", "(", "self", ",", "device", ":", "torch", ".", "device", ")", ":", "\n", "        ", "self", ".", "model", "=", "self", ".", "model", ".", "to", "(", "device", ")", "\n", "self", ".", "device", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.text.LookupEmbedding.__init__": [[66, 88], ["text.Tokenizer", "numpy.mean", "text.TextEmbedding.__init__", "zsvision.zs_utils.BlockTimer", "numpy.zeros", "enumerate", "sorted", "model", "min", "len"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["    ", "@", "typechecked", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "model", ":", "Callable", ",", "\n", "dim", ":", "int", ",", "\n", "remove_stopwords", ":", "bool", ",", "\n", "num_samples_for_unknown", ":", "int", "=", "50000", ",", "\n", ")", ":", "\n", "        ", "tokenizer", "=", "Tokenizer", "(", "vocab", "=", "model", ".", "vocab", ")", "\n", "with", "BlockTimer", "(", "\"generating unknown vector\"", ")", ":", "\n", "            ", "vecs", "=", "np", ".", "zeros", "(", "(", "min", "(", "num_samples_for_unknown", ",", "len", "(", "model", ".", "vocab", ")", ")", ",", "dim", ")", ")", "\n", "for", "ii", ",", "key", "in", "enumerate", "(", "sorted", "(", "model", ".", "vocab", ")", ")", ":", "\n", "                ", "if", "ii", ">=", "num_samples_for_unknown", ":", "\n", "                    ", "break", "\n", "", "vecs", "[", "ii", "]", "=", "model", "(", "key", ")", "\n", "", "", "self", ".", "unknown_vector", "=", "np", ".", "mean", "(", "vecs", ",", "0", ")", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "dim", "=", "dim", ",", "\n", "model", "=", "model", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "remove_stopwords", "=", "remove_stopwords", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.text.LookupEmbedding.set_device": [[90, 94], ["type"], "methods", ["None"], ["", "@", "typechecked", "\n", "def", "set_device", "(", "self", ",", "device", ":", "torch", ".", "device", ")", ":", "\n", "        ", "msg", "=", "f\"{type(self)} only supports CPU-based execution found {device.type}\"", "\n", "assert", "device", ".", "type", "==", "\"cpu\"", ",", "msg", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.text.LookupEmbedding.text2vec": [[95, 112], ["text.LookupEmbedding.tokenizer", "numpy.array", "gensim.parsing.preprocessing.remove_stopwords", "gensim.parsing.preprocessing.remove_stopwords", "gensim.parsing.preprocessing.remove_stopwords", "gensim.parsing.preprocessing.remove_stopwords", "numpy.array.append", "print", "numpy.array", "text.LookupEmbedding.model"], "methods", ["None"], ["", "@", "typechecked", "\n", "def", "text2vec", "(", "self", ",", "text", ":", "str", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "if", "self", ".", "remove_stopwords", ":", "\n", "            ", "processed_string", "=", "gensim", ".", "parsing", ".", "preprocessing", ".", "remove_stopwords", "(", "text", ")", "\n", "", "else", ":", "\n", "            ", "processed_string", "=", "text", "\n", "", "tokens", ",", "failed", "=", "self", ".", "tokenizer", "(", "processed_string", ")", "\n", "embeddings", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "embeddings", ".", "append", "(", "self", ".", "model", "(", "token", ")", ")", "\n", "", "embeddings", "=", "np", ".", "array", "(", "embeddings", ")", "\n", "msg", "=", "(", "f\"Failed to embed any tokens! (text: {text}, processed_string: \"", "\n", "f\"{processed_string}, failed: {failed})\"", ")", "\n", "if", "embeddings", ".", "size", "==", "0", ":", "\n", "            ", "print", "(", "f\"Warning: {msg}, falling back to unknown vector\"", ")", "\n", "embeddings", "=", "np", ".", "array", "(", "[", "self", ".", "unknown_vector", "]", ")", "\n", "", "return", "embeddings", ",", "failed", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.text.Tokenizer.__init__": [[120, 141], ["zsvision.zs_utils.BlockTimer", "spacy.load", "symspellpy.SymSpell", "pkg_resources.resource_filename", "symspellpy.SymSpell.load_dictionary"], "methods", ["None"], ["@", "typechecked", "\n", "def", "__init__", "(", "self", ",", "vocab", ":", "Set", "[", "str", "]", ")", ":", "\n", "        ", "with", "BlockTimer", "(", "\"preparing tokenizer dicionaries\"", ")", ":", "\n", "# we only use spacy for lemmatising, so we don't need NER or the parser.", "\n", "# NOTE: all pronouns are mapped to -PRON-, because it's not clear what their", "\n", "# lemma should be (we try to handle these via the spellchecker)", "\n", "            ", "self", ".", "nlp", "=", "spacy", ".", "load", "(", "'en'", ",", "disable", "=", "[", "'parser'", ",", "'ner'", "]", ")", "\n", "\n", "# Symspell is, in theory, a fast spell checker:", "\n", "sym_spell", "=", "SymSpell", "(", "max_dictionary_edit_distance", "=", "2", ",", "prefix_length", "=", "7", ")", "\n", "dictionary_path", "=", "pkg_resources", ".", "resource_filename", "(", "\n", "\"symspellpy\"", ",", "\"frequency_dictionary_en_82_765.txt\"", ")", "\n", "# term_index is the column of the term and count_index is the", "\n", "# column of the term frequency", "\n", "sym_spell", ".", "load_dictionary", "(", "dictionary_path", ",", "term_index", "=", "0", ",", "count_index", "=", "1", ")", "\n", "self", ".", "sym_spell", "=", "sym_spell", "\n", "self", ".", "vocab", "=", "vocab", "\n", "\n", "# For a small number of cases, the tokenization fails.", "\n", "self", ".", "custom", "=", "{", "\n", "\"roundtable\"", ":", "[", "\"round\"", ",", "\"table\"", "]", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.text.Tokenizer.__call__": [[143, 173], ["text.Tokenizer.nlp", "str", "tokens.append", "tokens.append", "text.Tokenizer.sym_spell.lookup", "failed.append", "tokens.append", "failed.append", "tokens.append", "str"], "methods", ["None"], ["", "", "def", "__call__", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "str", "]", ":", "\n", "        ", "doc", "=", "self", ".", "nlp", "(", "text", ")", "\n", "tokens", ",", "failed", "=", "[", "]", ",", "[", "]", "\n", "for", "token", "in", "doc", ":", "\n", "            ", "token", ",", "lemma", "=", "str", "(", "token", ")", ",", "token", ".", "lemma_", "\n", "if", "token", "in", "self", ".", "vocab", ":", "\n", "                ", "tokens", ".", "append", "(", "token", ")", "\n", "", "elif", "lemma", "in", "self", ".", "vocab", ":", "\n", "                ", "tokens", ".", "append", "(", "lemma", ")", "\n", "", "elif", "lemma", "in", "self", ".", "custom", ":", "\n", "                ", "for", "subtoken", "in", "self", ".", "custom", "[", "lemma", "]", ":", "\n", "                    ", "if", "subtoken", "in", "self", ".", "vocab", ":", "\n", "                        ", "tokens", ".", "append", "(", "subtoken", ")", "\n", "", "else", ":", "\n", "                        ", "failed", ".", "append", "(", "subtoken", ")", "\n", "", "", "", "else", ":", "\n", "                ", "suggestions", "=", "self", ".", "sym_spell", ".", "lookup", "(", "\n", "phrase", "=", "token", ",", "\n", "verbosity", "=", "Verbosity", ".", "CLOSEST", ",", "\n", "max_edit_distance", "=", "2", ",", "\n", ")", "\n", "success", "=", "False", "\n", "for", "suggestion", "in", "suggestions", ":", "\n", "                    ", "if", "suggestion", ".", "term", "in", "self", ".", "vocab", ":", "\n", "                        ", "success", "=", "True", "\n", "tokens", ".", "append", "(", "suggestion", ".", "term", ")", "\n", "break", "\n", "", "", "if", "not", "success", ":", "\n", "                    ", "failed", ".", "append", "(", "str", "(", "token", ")", ")", "\n", "", "", "", "return", "tokens", ",", "failed", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.text.W2VEmbedding.__init__": [[207, 239], ["text.load_w2v_model_from_cache", "W2V_Lookup", "text.LookupEmbedding.__init__", "weights_path.exists", "text.fetch_model", "ValueError", "set", "text.W2VEmbedding.w2v.get_vector", "load_w2v_model_from_cache.vocab.keys"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.text.load_w2v_model_from_cache", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.text.fetch_model", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys"], ["@", "typechecked", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "dim", ":", "int", ",", "\n", "mirror", ":", "str", ",", "\n", "embedding_name", ":", "str", ",", "\n", "weights_path", ":", "Path", ",", "\n", "remove_stopwords", ":", "bool", ",", "\n", "fetch_weights", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "if", "not", "weights_path", ".", "exists", "(", ")", ":", "\n", "            ", "if", "fetch_weights", ":", "\n", "                ", "fetch_model", "(", "url", "=", "mirror", ",", "weights_path", "=", "weights_path", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "f\"w2v weights missing at {weights_path}\"", ")", "\n", "\n", "", "", "class", "W2V_Lookup", ":", "\n", "            ", "def", "__init__", "(", "self", ",", "w2v", ")", ":", "\n", "                ", "self", ".", "w2v", "=", "w2v", "\n", "self", ".", "vocab", "=", "set", "(", "w2v", ".", "vocab", ".", "keys", "(", ")", ")", "\n", "\n", "", "def", "__call__", "(", "self", ",", "key", ")", ":", "\n", "                ", "return", "self", ".", "w2v", ".", "get_vector", "(", "key", ")", "\n", "\n", "", "", "w2v", "=", "load_w2v_model_from_cache", "(", "weights_path", ")", "\n", "self", ".", "embedding_name", "=", "embedding_name", "\n", "model", "=", "W2V_Lookup", "(", "w2v", "=", "w2v", ")", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "dim", "=", "dim", ",", "\n", "model", "=", "model", ",", "\n", "remove_stopwords", "=", "remove_stopwords", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.text.GrOVLE.__init__": [[257, 304], ["Lookup", "text.LookupEmbedding.__init__", "weights_path.exists", "zsvision.zs_utils.BlockTimer", "zipfile.ZipFile", "zipfile.ZipFile.read().decode().splitlines", "any", "row.split", "numpy.array", "text.fetch_model", "ValueError", "set", "zipfile.ZipFile.read().decode", "text.GrOVLE.keys", "float", "weights.split", "zipfile.ZipFile.read"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.text.fetch_model", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys"], ["@", "typechecked", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "dim", ":", "int", ",", "\n", "mirror", ":", "str", ",", "\n", "embedding_name", ":", "str", ",", "\n", "weights_path", ":", "Path", ",", "\n", "remove_stopwords", ":", "bool", ",", "\n", "fetch_weights", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "if", "not", "weights_path", ".", "exists", "(", ")", ":", "\n", "            ", "if", "fetch_weights", ":", "\n", "                ", "public_url", "=", "f\"{mirror}/{weights_path.name}\"", "\n", "fetch_model", "(", "url", "=", "public_url", ",", "weights_path", "=", "weights_path", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "f\"GrOVLE weights missing at {weights_path}\"", ")", "\n", "\n", "", "", "with", "BlockTimer", "(", "f\"Reading weight contents from {weights_path}\"", ")", ":", "\n", "            ", "zipped", "=", "zipfile", ".", "ZipFile", "(", "weights_path", ")", "\n", "rows", "=", "zipped", ".", "read", "(", "f\"{embedding_name}.txt\"", ")", ".", "decode", "(", "\"utf-8\"", ")", ".", "splitlines", "(", ")", "\n", "\n", "# To maintain a consistent interface with w2v, we add a matching `get_vector`", "\n", "# method to the dictionary baseclass", "\n", "", "class", "Lookup", "(", "dict", ")", ":", "\n", "            ", "def", "__call__", "(", "self", ",", "key", ")", ":", "\n", "                ", "return", "self", "[", "key", "]", "\n", "\n", "", "@", "property", "\n", "def", "vocab", "(", "self", ")", ":", "\n", "                ", "return", "set", "(", "self", ".", "keys", "(", ")", ")", "\n", "\n", "", "", "model", "=", "Lookup", "(", ")", "\n", "\n", "punctuation", "=", "{", "'\"'", ",", "\"?\"", ",", "\")\"", ",", "\"(\"", "}", "\n", "for", "row", "in", "rows", ":", "\n", "# exclude puncutation", "\n", "            ", "if", "any", "(", "[", "punc", "in", "row", "for", "punc", "in", "punctuation", "]", ")", ":", "\n", "                ", "msg", "=", "f\"Only expected HGLMM models to have punctuation\"", "\n", "assert", "embedding_name", "in", "{", "\"hglmm_300d\"", ",", "\"hglmm_6kd\"", "}", ",", "msg", "\n", "continue", "\n", "", "key", ",", "weights", "=", "row", ".", "split", "(", "\" \"", ",", "1", ")", "\n", "model", "[", "key", "]", "=", "np", ".", "array", "(", "[", "float", "(", "x", ")", "for", "x", "in", "weights", ".", "split", "(", "\" \"", ")", "]", ")", "\n", "\n", "", "super", "(", ")", ".", "__init__", "(", "\n", "dim", "=", "dim", ",", "\n", "model", "=", "model", ",", "\n", "remove_stopwords", "=", "remove_stopwords", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.text.HowTo100M_MIL_NCE.__init__": [[316, 342], ["model.s3dg.S3D.s3dg.S3D", "model.s3dg.S3D.s3dg.S3D.load_state_dict", "model.s3dg.S3D.s3dg.S3D.eval", "text.TextEmbedding.__init__", "torch.load", "path.exists", "text.fetch_model", "ValueError"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.text.fetch_model"], ["def", "__init__", "(", "\n", "self", ",", "\n", "word_dict_path", ":", "Path", ",", "\n", "weights_path", ":", "Path", ",", "\n", "embedding_name", ":", "str", ",", "\n", "dim", ":", "int", ",", "\n", "mirror", ":", "str", ",", "\n", "fetch_weights", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "for", "path", "in", "[", "word_dict_path", ",", "weights_path", "]", ":", "\n", "            ", "if", "not", "path", ".", "exists", "(", ")", ":", "\n", "                ", "if", "fetch_weights", ":", "\n", "                    ", "public_url", "=", "f\"{mirror}/{path.name}\"", "\n", "fetch_model", "(", "url", "=", "public_url", ",", "weights_path", "=", "path", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "f\"howto100m weights missing at {path}\"", ")", "\n", "\n", "", "", "", "model", "=", "S3D", "(", "word_dict_path", ",", "dim", ")", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "weights_path", ")", ")", "\n", "self", ".", "embedding_name", "=", "embedding_name", "\n", "model", ".", "eval", "(", ")", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "dim", "=", "dim", ",", "\n", "model", "=", "model", ",", "\n", "tokenizer", "=", "None", ",", "\n", "remove_stopwords", "=", "False", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.text.HowTo100M_MIL_NCE.text2vec": [[344, 349], ["torch.no_grad", "text.HowTo100M_MIL_NCE.model.text_module", "embedding[].cpu().numpy", "embedding[].cpu"], "methods", ["None"], ["", "@", "typechecked", "\n", "def", "text2vec", "(", "self", ",", "text", ":", "str", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "embedding", "=", "self", ".", "model", ".", "text_module", "(", "[", "text", "]", ",", "device", "=", "self", ".", "device", ")", "\n", "", "return", "embedding", "[", "\"text_embedding\"", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.text.HuggingFaceWrapper.__init__": [[358, 412], ["collections.defaultdict", "collections.defaultdict", "transformer_keys.get", "tokenizers[].from_pretrained", "models[].from_pretrained", "text.TextEmbedding.__init__"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["def", "__init__", "(", "self", ",", "dim", ":", "int", ",", "embedding_name", ":", "str", ")", ":", "\n", "        ", "tokenizers", "=", "{", "\n", "\"openai-gpt\"", ":", "transformers", ".", "OpenAIGPTTokenizer", ",", "\n", "\"bert-base-uncased\"", ":", "transformers", ".", "BertTokenizer", ",", "\n", "\"ctrl\"", ":", "transformers", ".", "CTRLTokenizer", ",", "\n", "\"transfo-xl-wt103\"", ":", "transformers", ".", "TransfoXLTokenizer", ",", "\n", "\"electra\"", ":", "transformers", ".", "ElectraTokenizer", ",", "\n", "}", "\n", "models", "=", "{", "\n", "\"openai-gpt\"", ":", "transformers", ".", "OpenAIGPTModel", ",", "\n", "\"bert-base-uncased\"", ":", "transformers", ".", "BertModel", ",", "\n", "\"ctrl\"", ":", "transformers", ".", "CTRLModel", ",", "\n", "\"transfo-xl-wt103\"", ":", "transformers", ".", "TransfoXLModel", ",", "\n", "\"electra\"", ":", "transformers", ".", "ElectraModel", ",", "\n", "}", "\n", "add_special_tokens", "=", "defaultdict", "(", "lambda", ":", "True", ")", "\n", "add_decoder_input_ids", "=", "defaultdict", "(", "lambda", ":", "False", ")", "\n", "\n", "for", "name", "in", "[", "\"gpt2\"", ",", "\"gpt2-medium\"", ",", "\"gpt2-large\"", ",", "\"gpt2-xl\"", ",", "\"gpt2-xl-finetune\"", "]", ":", "\n", "            ", "tokenizers", "[", "name", "]", "=", "transformers", ".", "GPT2Tokenizer", "\n", "models", "[", "name", "]", "=", "transformers", ".", "GPT2Model", "\n", "\n", "", "for", "name", "in", "[", "\"t5-small\"", ",", "\"t5-base\"", ",", "\"t5-large\"", ",", "\"t5-3b\"", ",", "\"t5-11b\"", "]", ":", "\n", "            ", "tokenizers", "[", "name", "]", "=", "transformers", ".", "T5Tokenizer", "\n", "models", "[", "name", "]", "=", "transformers", ".", "T5Model", "\n", "add_special_tokens", "[", "name", "]", "=", "False", "\n", "add_decoder_input_ids", "[", "name", "]", "=", "True", "\n", "\n", "", "for", "name", "in", "[", "\"albert-base-v2\"", ",", "\"albert-large-v2\"", ",", "\"albert-xlarge-v2\"", "]", ":", "\n", "            ", "tokenizers", "[", "name", "]", "=", "transformers", ".", "AlbertTokenizer", "\n", "models", "[", "name", "]", "=", "transformers", ".", "AlbertModel", "\n", "\n", "", "for", "name", "in", "[", "\"roberta-base\"", ",", "\"roberta-large\"", "]", ":", "\n", "            ", "tokenizers", "[", "name", "]", "=", "transformers", ".", "RobertaTokenizer", "\n", "models", "[", "name", "]", "=", "transformers", ".", "RobertaModel", "\n", "\n", "", "for", "name", "in", "[", "\"xlnet-base-cased\"", ",", "\"xlnet-large-cased\"", "]", ":", "\n", "            ", "tokenizers", "[", "name", "]", "=", "transformers", ".", "XLNetTokenizer", "\n", "models", "[", "name", "]", "=", "transformers", ".", "XLNetModel", "\n", "add_special_tokens", "[", "name", "]", "=", "False", "\n", "\n", "# handle inconsistent naming scheme for electra", "\n", "", "transformer_keys", "=", "{", "\"electra\"", ":", "\"google/electra-small-discriminator\"", "}", "\n", "transformer_key", "=", "transformer_keys", ".", "get", "(", "embedding_name", ",", "embedding_name", ")", "\n", "tokenizer", "=", "tokenizers", "[", "embedding_name", "]", ".", "from_pretrained", "(", "transformer_key", ")", "\n", "model", "=", "models", "[", "embedding_name", "]", ".", "from_pretrained", "(", "transformer_key", ")", "\n", "\n", "self", ".", "add_special_tokens", "=", "add_special_tokens", "[", "embedding_name", "]", "\n", "self", ".", "add_decoder_input_ids", "=", "add_decoder_input_ids", "[", "embedding_name", "]", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "model", "=", "model", ",", "\n", "dim", "=", "dim", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "remove_stopwords", "=", "False", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.text.HuggingFaceWrapper.text2vec": [[414, 429], ["text.HuggingFaceWrapper.tokenizer.encode", "torch.LongTensor().to", "torch.LongTensor().to.unsqueeze", "torch.LongTensor().to.unsqueeze", "torch.no_grad", "text.HuggingFaceWrapper.model", "hidden_states[].cpu().numpy", "hidden_states[].cpu().numpy.squeeze", "torch.LongTensor", "hidden_states[].cpu"], "methods", ["None"], ["", "@", "typechecked", "\n", "def", "text2vec", "(", "self", ",", "text", ":", "str", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "tokens", "=", "self", ".", "tokenizer", ".", "encode", "(", "\n", "text", ",", "\n", "add_special_tokens", "=", "self", ".", "add_special_tokens", ",", "\n", "add_space_before_punct_symbol", "=", "True", ",", "\n", ")", "\n", "input_idx", "=", "torch", ".", "LongTensor", "(", "tokens", ")", ".", "to", "(", "self", ".", "model", ".", "device", ")", "\n", "kwargs", "=", "{", "\"input_ids\"", ":", "input_idx", ".", "unsqueeze", "(", "0", ")", "}", "\n", "if", "self", ".", "add_decoder_input_ids", ":", "\n", "            ", "kwargs", "[", "\"decoder_input_ids\"", "]", "=", "input_idx", ".", "unsqueeze", "(", "0", ")", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "hidden_states", "=", "self", ".", "model", "(", "**", "kwargs", ")", "\n", "embeddings", "=", "hidden_states", "[", "0", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "return", "embeddings", ".", "squeeze", "(", "0", ")", ",", "[", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.text.load_w2v_model_from_cache": [[175, 185], ["functools.lru_cache", "zsvision.zs_utils.BlockTimer", "gensim.models.KeyedVectors.load_word2vec_format", "gensim.models.KeyedVectors.load_word2vec_format"], "function", ["None"], ["", "", "@", "functools", ".", "lru_cache", "(", "maxsize", "=", "64", ",", "typed", "=", "False", ")", "\n", "def", "load_w2v_model_from_cache", "(", "\n", "w2v_weights", ":", "Path", ",", "\n", ")", "->", "gensim", ".", "models", ".", "keyedvectors", ".", "Word2VecKeyedVectors", ":", "\n", "    ", "with", "BlockTimer", "(", "\"Loading w2v from disk\"", ")", ":", "\n", "        ", "model", "=", "gensim", ".", "models", ".", "KeyedVectors", ".", "load_word2vec_format", "(", "\n", "fname", "=", "w2v_weights", ",", "\n", "binary", "=", "True", ",", "\n", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.text.fetch_model": [[187, 194], ["weights_path.parent.mkdir", "zsvision.zs_utils.BlockTimer", "requests.get", "open", "f.write"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdir", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get"], ["", "@", "typechecked", "\n", "def", "fetch_model", "(", "url", ":", "str", ",", "weights_path", ":", "Path", ")", ":", "\n", "    ", "weights_path", ".", "parent", ".", "mkdir", "(", "exist_ok", "=", "True", ",", "parents", "=", "True", ")", "\n", "with", "BlockTimer", "(", "f\"Fetching weights {url} -> {weights_path}\"", ")", ":", "\n", "        ", "resp", "=", "requests", ".", "get", "(", "url", ",", "verify", "=", "False", ")", "\n", "with", "open", "(", "weights_path", ",", "\"wb\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "resp", ".", "content", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.loss.MaxMarginRankingLoss.__init__": [[31, 36], ["torch.Module.__init__", "torch.nn.MarginRankingLoss", "torch.nn.MarginRankingLoss", "torch.nn.MarginRankingLoss"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "margin", "=", "1", ",", "fix_norm", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fix_norm", "=", "fix_norm", "\n", "self", ".", "loss", "=", "th", ".", "nn", ".", "MarginRankingLoss", "(", "margin", ")", "\n", "self", ".", "margin", "=", "margin", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.loss.MaxMarginRankingLoss.forward": [[37, 65], ["torch.diag", "torch.diag", "torch.diag", "torch.cat.unsqueeze", "torch.cat.expand", "torch.cat.contiguous().view", "torch.cat", "torch.cat", "torch.cat", "x.view", "x.transpose().contiguous().view", "torch.cat", "torch.cat", "torch.cat", "torch.relu", "torch.relu", "torch.relu", "torch.relu.mean", "x.size", "keep.view", "keep.transpose().contiguous().view", "torch.nonzero().flatten", "torch.nonzero().flatten", "torch.nonzero().flatten", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.relu", "torch.relu", "torch.relu", "torch.cat.contiguous", "x.transpose().contiguous", "torch.ones", "torch.ones", "torch.ones", "torch.eye", "torch.eye", "torch.eye", "keep_idx.cuda.cuda.cuda", "keep.transpose().contiguous", "torch.nonzero", "torch.nonzero", "torch.nonzero", "x.transpose", "torch.cat().flatten", "torch.cat().flatten", "torch.cat().flatten", "keep.transpose", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "n", "=", "x", ".", "size", "(", ")", "[", "0", "]", "\n", "\n", "x1", "=", "th", ".", "diag", "(", "x", ")", "\n", "x1", "=", "x1", ".", "unsqueeze", "(", "1", ")", "\n", "x1", "=", "x1", ".", "expand", "(", "n", ",", "n", ")", "\n", "x1", "=", "x1", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "x1", "=", "th", ".", "cat", "(", "(", "x1", ",", "x1", ")", ",", "0", ")", "\n", "\n", "x2", "=", "x", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "x3", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "\n", "x2", "=", "th", ".", "cat", "(", "(", "x2", ",", "x3", ")", ",", "0", ")", "\n", "max_margin", "=", "F", ".", "relu", "(", "self", ".", "margin", "-", "(", "x1", "-", "x2", ")", ")", "\n", "\n", "if", "self", ".", "fix_norm", ":", "\n", "# remove the elements from the diagonal", "\n", "            ", "keep", "=", "th", ".", "ones", "(", "x", ".", "shape", ")", "-", "th", ".", "eye", "(", "x", ".", "shape", "[", "0", "]", ")", "# 128 x 128", "\n", "keep1", "=", "keep", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "keep2", "=", "keep", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "keep_idx", "=", "th", ".", "nonzero", "(", "th", ".", "cat", "(", "(", "keep1", ",", "keep2", ")", ",", "0", ")", ".", "flatten", "(", ")", ")", ".", "flatten", "(", ")", "\n", "if", "x1", ".", "is_cuda", ":", "\n", "                ", "keep_idx", "=", "keep_idx", ".", "cuda", "(", ")", "\n", "", "x1_", "=", "th", ".", "index_select", "(", "x1", ",", "dim", "=", "0", ",", "index", "=", "keep_idx", ")", "\n", "x2_", "=", "th", ".", "index_select", "(", "x2", ",", "dim", "=", "0", ",", "index", "=", "keep_idx", ")", "\n", "max_margin", "=", "F", ".", "relu", "(", "self", ".", "margin", "-", "(", "x1_", "-", "x2_", ")", ")", "\n", "\n", "", "return", "max_margin", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.loss.BCEWithLogitsLoss.__init__": [[69, 72], ["torch.Module.__init__", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "weight", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "loss", "=", "th", ".", "nn", ".", "BCEWithLogitsLoss", "(", "weight", "=", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.loss.BCEWithLogitsLoss.forward": [[73, 75], ["loss.BCEWithLogitsLoss.loss"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "target", ")", ":", "\n", "        ", "return", "self", ".", "loss", "(", "x", ",", "target", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.loss.CrossEntropyLoss.__init__": [[79, 82], ["torch.Module.__init__", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "weight", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "loss", "=", "th", ".", "nn", ".", "CrossEntropyLoss", "(", "weight", "=", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.loss.CrossEntropyLoss.forward": [[83, 85], ["loss.CrossEntropyLoss.loss", "target.long().to", "target.long"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "target", ")", ":", "\n", "        ", "return", "self", ".", "loss", "(", "x", ",", "target", ".", "long", "(", ")", ".", "to", "(", "x", ".", "device", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.s3dg.InceptionBlock.__init__": [[36, 68], ["torch.Module.__init__", "s3dg.STConv3D", "s3dg.STConv3D", "s3dg.STConv3D", "s3dg.STConv3D", "s3dg.STConv3D", "torch.nn.MaxPool3d", "torch.nn.MaxPool3d", "torch.nn.MaxPool3d", "s3dg.STConv3D", "s3dg.SelfGating", "s3dg.SelfGating", "s3dg.SelfGating", "s3dg.SelfGating"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "input_dim", ",", "\n", "num_outputs_0_0a", ",", "\n", "num_outputs_1_0a", ",", "\n", "num_outputs_1_0b", ",", "\n", "num_outputs_2_0a", ",", "\n", "num_outputs_2_0b", ",", "\n", "num_outputs_3_0b", ",", "\n", "gating", "=", "True", ",", "\n", ")", ":", "\n", "        ", "super", "(", "InceptionBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv_b0", "=", "STConv3D", "(", "input_dim", ",", "num_outputs_0_0a", ",", "[", "1", ",", "1", ",", "1", "]", ")", "\n", "self", ".", "conv_b1_a", "=", "STConv3D", "(", "input_dim", ",", "num_outputs_1_0a", ",", "[", "1", ",", "1", ",", "1", "]", ")", "\n", "self", ".", "conv_b1_b", "=", "STConv3D", "(", "\n", "num_outputs_1_0a", ",", "num_outputs_1_0b", ",", "[", "3", ",", "3", ",", "3", "]", ",", "padding", "=", "1", ",", "separable", "=", "True", "\n", ")", "\n", "self", ".", "conv_b2_a", "=", "STConv3D", "(", "input_dim", ",", "num_outputs_2_0a", ",", "[", "1", ",", "1", ",", "1", "]", ")", "\n", "self", ".", "conv_b2_b", "=", "STConv3D", "(", "\n", "num_outputs_2_0a", ",", "num_outputs_2_0b", ",", "[", "3", ",", "3", ",", "3", "]", ",", "padding", "=", "1", ",", "separable", "=", "True", "\n", ")", "\n", "self", ".", "maxpool_b3", "=", "th", ".", "nn", ".", "MaxPool3d", "(", "(", "3", ",", "3", ",", "3", ")", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "self", ".", "conv_b3_b", "=", "STConv3D", "(", "input_dim", ",", "num_outputs_3_0b", ",", "[", "1", ",", "1", ",", "1", "]", ")", "\n", "self", ".", "gating", "=", "gating", "\n", "self", ".", "output_dim", "=", "(", "\n", "num_outputs_0_0a", "+", "num_outputs_1_0b", "+", "num_outputs_2_0b", "+", "num_outputs_3_0b", "\n", ")", "\n", "if", "gating", ":", "\n", "            ", "self", ".", "gating_b0", "=", "SelfGating", "(", "num_outputs_0_0a", ")", "\n", "self", ".", "gating_b1", "=", "SelfGating", "(", "num_outputs_1_0b", ")", "\n", "self", ".", "gating_b2", "=", "SelfGating", "(", "num_outputs_2_0b", ")", "\n", "self", ".", "gating_b3", "=", "SelfGating", "(", "num_outputs_3_0b", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.s3dg.InceptionBlock.forward": [[69, 85], ["s3dg.InceptionBlock.conv_b0", "s3dg.InceptionBlock.conv_b1_a", "s3dg.InceptionBlock.conv_b1_b", "s3dg.InceptionBlock.conv_b2_a", "s3dg.InceptionBlock.conv_b2_b", "s3dg.InceptionBlock.maxpool_b3", "s3dg.InceptionBlock.conv_b3_b", "torch.cat", "torch.cat", "torch.cat", "s3dg.InceptionBlock.gating_b0", "s3dg.InceptionBlock.gating_b1", "s3dg.InceptionBlock.gating_b2", "s3dg.InceptionBlock.gating_b3"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "\"\"\"Inception block\n      \"\"\"", "\n", "b0", "=", "self", ".", "conv_b0", "(", "input", ")", "\n", "b1", "=", "self", ".", "conv_b1_a", "(", "input", ")", "\n", "b1", "=", "self", ".", "conv_b1_b", "(", "b1", ")", "\n", "b2", "=", "self", ".", "conv_b2_a", "(", "input", ")", "\n", "b2", "=", "self", ".", "conv_b2_b", "(", "b2", ")", "\n", "b3", "=", "self", ".", "maxpool_b3", "(", "input", ")", "\n", "b3", "=", "self", ".", "conv_b3_b", "(", "b3", ")", "\n", "if", "self", ".", "gating", ":", "\n", "            ", "b0", "=", "self", ".", "gating_b0", "(", "b0", ")", "\n", "b1", "=", "self", ".", "gating_b1", "(", "b1", ")", "\n", "b2", "=", "self", ".", "gating_b2", "(", "b2", ")", "\n", "b3", "=", "self", ".", "gating_b3", "(", "b3", ")", "\n", "", "return", "th", ".", "cat", "(", "(", "b0", ",", "b1", ",", "b2", ",", "b3", ")", ",", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.s3dg.SelfGating.__init__": [[88, 91], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_dim", ")", ":", "\n", "        ", "super", "(", "SelfGating", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "input_dim", ",", "input_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.s3dg.SelfGating.forward": [[92, 99], ["torch.mean", "torch.mean", "torch.mean", "s3dg.SelfGating.fc", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ")", ":", "\n", "        ", "\"\"\"Feature gating as used in S3D-G.\n      \"\"\"", "\n", "spatiotemporal_average", "=", "th", ".", "mean", "(", "input_tensor", ",", "dim", "=", "[", "2", ",", "3", ",", "4", "]", ")", "\n", "weights", "=", "self", ".", "fc", "(", "spatiotemporal_average", ")", "\n", "weights", "=", "th", ".", "sigmoid", "(", "weights", ")", "\n", "return", "weights", "[", ":", ",", ":", ",", "None", ",", "None", ",", "None", "]", "*", "input_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.s3dg.STConv3D.__init__": [[102, 153], ["torch.Module.__init__", "torch.ReLU", "torch.ReLU", "torch.ReLU", "len", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.BatchNorm3d", "torch.BatchNorm3d", "torch.BatchNorm3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.BatchNorm3d", "torch.BatchNorm3d", "torch.BatchNorm3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.BatchNorm3d", "torch.BatchNorm3d", "torch.BatchNorm3d", "isinstance", "isinstance", "len", "len"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "input_dim", ",", "output_dim", ",", "kernel_size", ",", "stride", "=", "1", ",", "padding", "=", "0", ",", "separable", "=", "False", "\n", ")", ":", "\n", "        ", "super", "(", "STConv3D", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "separable", "=", "separable", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "assert", "len", "(", "kernel_size", ")", "==", "3", "\n", "if", "separable", "and", "kernel_size", "[", "0", "]", "!=", "1", ":", "\n", "            ", "spatial_kernel_size", "=", "[", "1", ",", "kernel_size", "[", "1", "]", ",", "kernel_size", "[", "2", "]", "]", "\n", "temporal_kernel_size", "=", "[", "kernel_size", "[", "0", "]", ",", "1", ",", "1", "]", "\n", "if", "isinstance", "(", "stride", ",", "list", ")", "and", "len", "(", "stride", ")", "==", "3", ":", "\n", "                ", "spatial_stride", "=", "[", "1", ",", "stride", "[", "1", "]", ",", "stride", "[", "2", "]", "]", "\n", "temporal_stride", "=", "[", "stride", "[", "0", "]", ",", "1", ",", "1", "]", "\n", "", "else", ":", "\n", "                ", "spatial_stride", "=", "[", "1", ",", "stride", ",", "stride", "]", "\n", "temporal_stride", "=", "[", "stride", ",", "1", ",", "1", "]", "\n", "", "if", "isinstance", "(", "padding", ",", "list", ")", "and", "len", "(", "padding", ")", "==", "3", ":", "\n", "                ", "spatial_padding", "=", "[", "0", ",", "padding", "[", "1", "]", ",", "padding", "[", "2", "]", "]", "\n", "temporal_padding", "=", "[", "padding", "[", "0", "]", ",", "0", ",", "0", "]", "\n", "", "else", ":", "\n", "                ", "spatial_padding", "=", "[", "0", ",", "padding", ",", "padding", "]", "\n", "temporal_padding", "=", "[", "padding", ",", "0", ",", "0", "]", "\n", "", "", "if", "separable", ":", "\n", "            ", "self", ".", "conv1", "=", "nn", ".", "Conv3d", "(", "\n", "input_dim", ",", "\n", "output_dim", ",", "\n", "kernel_size", "=", "spatial_kernel_size", ",", "\n", "stride", "=", "spatial_stride", ",", "\n", "padding", "=", "spatial_padding", ",", "\n", "bias", "=", "False", ",", "\n", ")", "\n", "self", ".", "bn1", "=", "nn", ".", "BatchNorm3d", "(", "output_dim", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv3d", "(", "\n", "output_dim", ",", "\n", "output_dim", ",", "\n", "kernel_size", "=", "temporal_kernel_size", ",", "\n", "stride", "=", "temporal_stride", ",", "\n", "padding", "=", "temporal_padding", ",", "\n", "bias", "=", "False", ",", "\n", ")", "\n", "self", ".", "bn2", "=", "nn", ".", "BatchNorm3d", "(", "output_dim", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv1", "=", "nn", ".", "Conv3d", "(", "\n", "input_dim", ",", "\n", "output_dim", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "stride", "=", "stride", ",", "\n", "padding", "=", "padding", ",", "\n", "bias", "=", "False", ",", "\n", ")", "\n", "self", ".", "bn1", "=", "nn", ".", "BatchNorm3d", "(", "output_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.s3dg.STConv3D.forward": [[154, 159], ["s3dg.STConv3D.relu", "s3dg.STConv3D.bn1", "s3dg.STConv3D.relu", "s3dg.STConv3D.conv1", "s3dg.STConv3D.bn2", "s3dg.STConv3D.conv2"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "out", "=", "self", ".", "relu", "(", "self", ".", "bn1", "(", "self", ".", "conv1", "(", "input", ")", ")", ")", "\n", "if", "self", ".", "separable", ":", "\n", "            ", "out", "=", "self", ".", "relu", "(", "self", ".", "bn2", "(", "self", ".", "conv2", "(", "out", ")", ")", ")", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.s3dg.MaxPool3dTFPadding.__init__": [[162, 169], ["super().__init__", "torch.nn.MaxPool3d", "torch.nn.MaxPool3d", "torch.nn.MaxPool3d", "s3dg.MaxPool3dTFPadding._get_padding_shape", "torch.nn.ConstantPad3d", "torch.nn.ConstantPad3d", "torch.nn.ConstantPad3d"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.s3dg.MaxPool3dTFPadding._get_padding_shape"], ["    ", "def", "__init__", "(", "self", ",", "kernel_size", ",", "stride", "=", "None", ",", "padding", "=", "\"SAME\"", ")", ":", "\n", "        ", "super", "(", "MaxPool3dTFPadding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "padding", "==", "\"SAME\"", ":", "\n", "            ", "padding_shape", "=", "self", ".", "_get_padding_shape", "(", "kernel_size", ",", "stride", ")", "\n", "self", ".", "padding_shape", "=", "padding_shape", "\n", "self", ".", "pad", "=", "th", ".", "nn", ".", "ConstantPad3d", "(", "padding_shape", ",", "0", ")", "\n", "", "self", ".", "pool", "=", "th", ".", "nn", ".", "MaxPool3d", "(", "kernel_size", ",", "stride", ",", "ceil_mode", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.s3dg.MaxPool3dTFPadding._get_padding_shape": [[170, 187], ["zip", "padding_shape.pop", "padding_shape.pop", "padding_shape.append", "padding_shape.append", "tuple", "max", "s3dg.MaxPool3dTFPadding._get_padding_shape._pad_top_bottom"], "methods", ["None"], ["", "def", "_get_padding_shape", "(", "self", ",", "filter_shape", ",", "stride", ")", ":", "\n", "        ", "def", "_pad_top_bottom", "(", "filter_dim", ",", "stride_val", ")", ":", "\n", "            ", "pad_along", "=", "max", "(", "filter_dim", "-", "stride_val", ",", "0", ")", "\n", "pad_top", "=", "pad_along", "//", "2", "\n", "pad_bottom", "=", "pad_along", "-", "pad_top", "\n", "return", "pad_top", ",", "pad_bottom", "\n", "\n", "", "padding_shape", "=", "[", "]", "\n", "for", "filter_dim", ",", "stride_val", "in", "zip", "(", "filter_shape", ",", "stride", ")", ":", "\n", "            ", "pad_top", ",", "pad_bottom", "=", "_pad_top_bottom", "(", "filter_dim", ",", "stride_val", ")", "\n", "padding_shape", ".", "append", "(", "pad_top", ")", "\n", "padding_shape", ".", "append", "(", "pad_bottom", ")", "\n", "", "depth_top", "=", "padding_shape", ".", "pop", "(", "0", ")", "\n", "depth_bottom", "=", "padding_shape", ".", "pop", "(", "0", ")", "\n", "padding_shape", ".", "append", "(", "depth_top", ")", "\n", "padding_shape", ".", "append", "(", "depth_bottom", ")", "\n", "return", "tuple", "(", "padding_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.s3dg.MaxPool3dTFPadding.forward": [[188, 192], ["s3dg.MaxPool3dTFPadding.pad", "s3dg.MaxPool3dTFPadding.pool"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inp", ")", ":", "\n", "        ", "inp", "=", "self", ".", "pad", "(", "inp", ")", "\n", "out", "=", "self", ".", "pool", "(", "inp", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.s3dg.Sentence_Embedding.__init__": [[195, 213], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "numpy.load", "enumerate"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "embd_dim", ",", "\n", "num_embeddings", "=", "66250", ",", "\n", "word_embedding_dim", "=", "300", ",", "\n", "token_to_word_path", "=", "\"dict.npy\"", ",", "\n", "max_words", "=", "16", ",", "\n", "output_dim", "=", "2048", ",", "\n", ")", ":", "\n", "        ", "super", "(", "Sentence_Embedding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embd", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "word_embedding_dim", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "word_embedding_dim", ",", "output_dim", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "output_dim", ",", "embd_dim", ")", "\n", "self", ".", "word_to_token", "=", "{", "}", "\n", "self", ".", "max_words", "=", "max_words", "\n", "token_to_word", "=", "np", ".", "load", "(", "token_to_word_path", ")", "\n", "for", "i", ",", "t", "in", "enumerate", "(", "token_to_word", ")", ":", "\n", "            ", "self", ".", "word_to_token", "[", "t", "]", "=", "i", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.s3dg.Sentence_Embedding._zero_pad_tensor_token": [[214, 220], ["len", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.cat", "torch.cat", "torch.cat", "torch.zeros", "torch.zeros", "torch.zeros", "len"], "methods", ["None"], ["", "", "def", "_zero_pad_tensor_token", "(", "self", ",", "tensor", ",", "size", ")", ":", "\n", "        ", "if", "len", "(", "tensor", ")", ">=", "size", ":", "\n", "            ", "return", "tensor", "[", ":", "size", "]", "\n", "", "else", ":", "\n", "            ", "zero", "=", "th", ".", "zeros", "(", "size", "-", "len", "(", "tensor", ")", ")", ".", "long", "(", ")", "\n", "return", "th", ".", "cat", "(", "(", "tensor", ",", "zero", ")", ",", "dim", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.s3dg.Sentence_Embedding._split_text": [[221, 224], ["re.findall", "str"], "methods", ["None"], ["", "", "def", "_split_text", "(", "self", ",", "sentence", ")", ":", "\n", "        ", "w", "=", "re", ".", "findall", "(", "r\"[\\w']+\"", ",", "str", "(", "sentence", ")", ")", "\n", "return", "w", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.s3dg.Sentence_Embedding._words_to_token": [[225, 234], ["s3dg.Sentence_Embedding._zero_pad_tensor_token", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.s3dg.Sentence_Embedding._zero_pad_tensor_token"], ["", "def", "_words_to_token", "(", "self", ",", "words", ")", ":", "\n", "        ", "words", "=", "[", "\n", "self", ".", "word_to_token", "[", "word", "]", "for", "word", "in", "words", "if", "word", "in", "self", ".", "word_to_token", "\n", "]", "\n", "if", "words", ":", "\n", "            ", "we", "=", "self", ".", "_zero_pad_tensor_token", "(", "th", ".", "LongTensor", "(", "words", ")", ",", "self", ".", "max_words", ")", "\n", "return", "we", "\n", "", "else", ":", "\n", "            ", "return", "th", ".", "zeros", "(", "self", ".", "max_words", ")", ".", "long", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.s3dg.Sentence_Embedding._words_to_ids": [[235, 238], ["torch.stack", "torch.stack", "torch.stack", "s3dg.Sentence_Embedding._words_to_token", "s3dg.Sentence_Embedding._split_text", "sent.lower"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.s3dg.Sentence_Embedding._words_to_token", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.s3dg.Sentence_Embedding._split_text"], ["", "", "def", "_words_to_ids", "(", "self", ",", "x", ")", ":", "\n", "        ", "split_x", "=", "[", "self", ".", "_words_to_token", "(", "self", ".", "_split_text", "(", "sent", ".", "lower", "(", ")", ")", ")", "for", "sent", "in", "x", "]", "\n", "return", "th", ".", "stack", "(", "split_x", ",", "dim", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.s3dg.Sentence_Embedding.forward": [[239, 248], ["s3dg.Sentence_Embedding._words_to_ids", "s3dg.Sentence_Embedding.word_embd", "torch.relu", "torch.relu", "torch.relu", "s3dg.Sentence_Embedding.fc2", "x.to.to.to", "s3dg.Sentence_Embedding.fc1", "torch.max", "torch.max", "torch.max"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.s3dg.Sentence_Embedding._words_to_ids"], ["", "def", "forward", "(", "self", ",", "x", ",", "device", ":", "th", ".", "device", "=", "None", ")", ":", "\n", "        ", "x", "=", "self", ".", "_words_to_ids", "(", "x", ")", "\n", "if", "device", ":", "\n", "            ", "x", "=", "x", ".", "to", "(", "device", ")", "\n", "", "x", "=", "self", ".", "word_embd", "(", "x", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "x", "=", "th", ".", "max", "(", "x", ",", "dim", "=", "1", ")", "[", "0", "]", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "return", "{", "'text_embedding'", ":", "x", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.s3dg.S3D.__init__": [[251, 307], ["torch.Module.__init__", "s3dg.STConv3D", "s3dg.STConv3D", "s3dg.SelfGating", "s3dg.MaxPool3dTFPadding", "s3dg.MaxPool3dTFPadding", "s3dg.InceptionBlock", "s3dg.InceptionBlock", "s3dg.MaxPool3dTFPadding", "s3dg.InceptionBlock", "s3dg.InceptionBlock", "s3dg.InceptionBlock", "s3dg.InceptionBlock", "s3dg.InceptionBlock", "s3dg.MaxPool3dTFPadding", "s3dg.InceptionBlock", "s3dg.InceptionBlock", "torch.Linear", "torch.Linear", "torch.Linear", "s3dg.Sentence_Embedding", "s3dg.STConv3D", "s3dg.STConv3D"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dict_path", ",", "num_classes", "=", "512", ",", "gating", "=", "True", ",", "space_to_depth", "=", "True", ")", ":", "\n", "        ", "super", "(", "S3D", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "gating", "=", "gating", "\n", "self", ".", "space_to_depth", "=", "space_to_depth", "\n", "if", "space_to_depth", ":", "\n", "            ", "self", ".", "conv1", "=", "STConv3D", "(", "\n", "24", ",", "64", ",", "[", "2", ",", "4", ",", "4", "]", ",", "stride", "=", "1", ",", "padding", "=", "(", "1", ",", "2", ",", "2", ")", ",", "separable", "=", "False", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv1", "=", "STConv3D", "(", "\n", "3", ",", "64", ",", "[", "3", ",", "7", ",", "7", "]", ",", "stride", "=", "2", ",", "padding", "=", "(", "1", ",", "3", ",", "3", ")", ",", "separable", "=", "False", "\n", ")", "\n", "", "self", ".", "conv_2b", "=", "STConv3D", "(", "64", ",", "64", ",", "[", "1", ",", "1", ",", "1", "]", ",", "separable", "=", "False", ")", "\n", "self", ".", "conv_2c", "=", "STConv3D", "(", "64", ",", "192", ",", "[", "3", ",", "3", ",", "3", "]", ",", "padding", "=", "1", ",", "separable", "=", "True", ")", "\n", "self", ".", "gating", "=", "SelfGating", "(", "192", ")", "\n", "self", ".", "maxpool_2a", "=", "MaxPool3dTFPadding", "(", "\n", "kernel_size", "=", "(", "1", ",", "3", ",", "3", ")", ",", "stride", "=", "(", "1", ",", "2", ",", "2", ")", ",", "padding", "=", "\"SAME\"", "\n", ")", "\n", "self", ".", "maxpool_3a", "=", "MaxPool3dTFPadding", "(", "\n", "kernel_size", "=", "(", "1", ",", "3", ",", "3", ")", ",", "stride", "=", "(", "1", ",", "2", ",", "2", ")", ",", "padding", "=", "\"SAME\"", "\n", ")", "\n", "self", ".", "mixed_3b", "=", "InceptionBlock", "(", "192", ",", "64", ",", "96", ",", "128", ",", "16", ",", "32", ",", "32", ")", "\n", "self", ".", "mixed_3c", "=", "InceptionBlock", "(", "\n", "self", ".", "mixed_3b", ".", "output_dim", ",", "128", ",", "128", ",", "192", ",", "32", ",", "96", ",", "64", "\n", ")", "\n", "self", ".", "maxpool_4a", "=", "MaxPool3dTFPadding", "(", "\n", "kernel_size", "=", "(", "3", ",", "3", ",", "3", ")", ",", "stride", "=", "(", "2", ",", "2", ",", "2", ")", ",", "padding", "=", "\"SAME\"", "\n", ")", "\n", "self", ".", "mixed_4b", "=", "InceptionBlock", "(", "\n", "self", ".", "mixed_3c", ".", "output_dim", ",", "192", ",", "96", ",", "208", ",", "16", ",", "48", ",", "64", "\n", ")", "\n", "self", ".", "mixed_4c", "=", "InceptionBlock", "(", "\n", "self", ".", "mixed_4b", ".", "output_dim", ",", "160", ",", "112", ",", "224", ",", "24", ",", "64", ",", "64", "\n", ")", "\n", "self", ".", "mixed_4d", "=", "InceptionBlock", "(", "\n", "self", ".", "mixed_4c", ".", "output_dim", ",", "128", ",", "128", ",", "256", ",", "24", ",", "64", ",", "64", "\n", ")", "\n", "self", ".", "mixed_4e", "=", "InceptionBlock", "(", "\n", "self", ".", "mixed_4d", ".", "output_dim", ",", "112", ",", "144", ",", "288", ",", "32", ",", "64", ",", "64", "\n", ")", "\n", "self", ".", "mixed_4f", "=", "InceptionBlock", "(", "\n", "self", ".", "mixed_4e", ".", "output_dim", ",", "256", ",", "160", ",", "320", ",", "32", ",", "128", ",", "128", "\n", ")", "\n", "self", ".", "maxpool_5a", "=", "self", ".", "maxPool3d_5a_2x2", "=", "MaxPool3dTFPadding", "(", "\n", "kernel_size", "=", "(", "2", ",", "2", ",", "2", ")", ",", "stride", "=", "(", "2", ",", "2", ",", "2", ")", ",", "padding", "=", "\"SAME\"", "\n", ")", "\n", "self", ".", "mixed_5b", "=", "InceptionBlock", "(", "\n", "self", ".", "mixed_4f", ".", "output_dim", ",", "256", ",", "160", ",", "320", ",", "32", ",", "128", ",", "128", "\n", ")", "\n", "self", ".", "mixed_5c", "=", "InceptionBlock", "(", "\n", "self", ".", "mixed_5b", ".", "output_dim", ",", "384", ",", "192", ",", "384", ",", "48", ",", "128", ",", "128", "\n", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "self", ".", "mixed_5c", ".", "output_dim", ",", "num_classes", ")", "\n", "self", ".", "text_module", "=", "Sentence_Embedding", "(", "num_classes", ",", "\n", "token_to_word_path", "=", "dict_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.s3dg.S3D._space_to_depth": [[308, 316], ["input.contiguous().view.contiguous().view.view", "input.contiguous().view.contiguous().view.permute", "input.contiguous().view.contiguous().view.contiguous().view", "input.contiguous().view.contiguous().view.contiguous"], "methods", ["None"], ["", "def", "_space_to_depth", "(", "self", ",", "input", ")", ":", "\n", "        ", "\"\"\"3D space to depth trick for TPU optimization.\n      \"\"\"", "\n", "B", ",", "C", ",", "T", ",", "H", ",", "W", "=", "input", ".", "shape", "\n", "input", "=", "input", ".", "view", "(", "B", ",", "C", ",", "T", "//", "2", ",", "2", ",", "H", "//", "2", ",", "2", ",", "W", "//", "2", ",", "2", ")", "\n", "input", "=", "input", ".", "permute", "(", "0", ",", "3", ",", "5", ",", "7", ",", "1", ",", "2", ",", "4", ",", "6", ")", "\n", "input", "=", "input", ".", "contiguous", "(", ")", ".", "view", "(", "B", ",", "8", "*", "C", ",", "T", "//", "2", ",", "H", "//", "2", ",", "W", "//", "2", ")", "\n", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.s3dg.S3D.forward": [[317, 345], ["s3dg.S3D.conv1", "s3dg.S3D.maxpool_2a", "s3dg.S3D.conv_2b", "s3dg.S3D.conv_2c", "s3dg.S3D.maxpool_3a", "s3dg.S3D.mixed_3b", "s3dg.S3D.mixed_3c", "s3dg.S3D.maxpool_4a", "s3dg.S3D.mixed_4b", "s3dg.S3D.mixed_4c", "s3dg.S3D.mixed_4d", "s3dg.S3D.mixed_4e", "s3dg.S3D.mixed_4f", "s3dg.S3D.maxpool_5a", "s3dg.S3D.mixed_5b", "s3dg.S3D.mixed_5c", "torch.mean", "torch.mean", "torch.mean", "s3dg.S3D._space_to_depth", "s3dg.S3D.gating", "s3dg.S3D.fc"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.s3dg.S3D._space_to_depth"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "\"\"\"Defines the S3DG base architecture.\n      \"\"\"", "\n", "if", "self", ".", "space_to_depth", ":", "\n", "            ", "inputs", "=", "self", ".", "_space_to_depth", "(", "inputs", ")", "\n", "", "net", "=", "self", ".", "conv1", "(", "inputs", ")", "\n", "if", "self", ".", "space_to_depth", ":", "\n", "# we need to replicate 'SAME' tensorflow padding", "\n", "            ", "net", "=", "net", "[", ":", ",", ":", ",", "1", ":", ",", "1", ":", ",", "1", ":", "]", "\n", "", "net", "=", "self", ".", "maxpool_2a", "(", "net", ")", "\n", "net", "=", "self", ".", "conv_2b", "(", "net", ")", "\n", "net", "=", "self", ".", "conv_2c", "(", "net", ")", "\n", "if", "self", ".", "gating", ":", "\n", "            ", "net", "=", "self", ".", "gating", "(", "net", ")", "\n", "", "net", "=", "self", ".", "maxpool_3a", "(", "net", ")", "\n", "net", "=", "self", ".", "mixed_3b", "(", "net", ")", "\n", "net", "=", "self", ".", "mixed_3c", "(", "net", ")", "\n", "net", "=", "self", ".", "maxpool_4a", "(", "net", ")", "\n", "net", "=", "self", ".", "mixed_4b", "(", "net", ")", "\n", "net", "=", "self", ".", "mixed_4c", "(", "net", ")", "\n", "net", "=", "self", ".", "mixed_4d", "(", "net", ")", "\n", "net", "=", "self", ".", "mixed_4e", "(", "net", ")", "\n", "net", "=", "self", ".", "mixed_4f", "(", "net", ")", "\n", "net", "=", "self", ".", "maxpool_5a", "(", "net", ")", "\n", "net", "=", "self", ".", "mixed_5b", "(", "net", ")", "\n", "net", "=", "self", ".", "mixed_5c", "(", "net", ")", "\n", "net", "=", "th", ".", "mean", "(", "net", ",", "dim", "=", "[", "2", ",", "3", ",", "4", "]", ")", "\n", "return", "{", "'video_embedding'", ":", "self", ".", "fc", "(", "net", ")", ",", "'mixed_5c'", ":", "net", "}", "\n", "", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.Mish.forward": [[41, 46], ["torch.tanh", "torch.tanh", "torch.tanh", "torch.softplus", "torch.softplus", "torch.softplus"], "methods", ["None"], ["def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "'''\n        Forward pass of the function.\n        '''", "\n", "return", "input", "*", "th", ".", "tanh", "(", "F", ".", "softplus", "(", "input", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.CENet.__init__": [[87, 239], ["base.BaseModel.__init__", "torch.ModuleDict", "torch.ModuleDict", "torch.ModuleDict", "pre_vlad_feat_sizes.items", "utils.expert_tensor_storage", "model.CEModule", "set", "set", "model.SpatialMLP", "pre_vlad_feat_sizes.items", "vlad_clusters.items", "expert_dims.keys", "model.net_vlad.NetVLAD", "torch.Sequential", "torch.Sequential", "torch.Sequential", "model.net_vlad.NetVLAD", "model.CENet.expert_dims.keys", "torch.Linear", "torch.Linear", "torch.Linear", "randomise_feats.split"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.expert_tensor_storage", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "task", ",", "\n", "use_ce", ",", "\n", "text_dim", ",", "\n", "l2renorm", ",", "\n", "expert_dims", ",", "\n", "vlad_clusters", ",", "\n", "ghost_clusters", ",", "\n", "disable_nan_checks", ",", "\n", "keep_missing_modalities", ",", "\n", "test_caption_mode", ",", "\n", "randomise_feats", ",", "\n", "feat_aggregation", ",", "\n", "ce_shared_dim", ",", "\n", "trn_config", ",", "\n", "trn_cat", ",", "\n", "include_self", ",", "\n", "use_mish", ",", "\n", "use_bn_reason", ",", "\n", "num_h_layers", ",", "\n", "num_g_layers", ",", "\n", "kron_dets", "=", "False", ",", "\n", "freeze_weights", "=", "False", ",", "\n", "geometric_mlp", "=", "False", ",", "\n", "rand_proj", "=", "False", ",", "\n", "mimic_ce_dims", "=", "False", ",", "\n", "coord_dets", "=", "False", ",", "\n", "concat_experts", "=", "False", ",", "\n", "spatial_feats", "=", "False", ",", "\n", "concat_mix_experts", "=", "False", ",", "\n", "verbose", "=", "False", ",", "\n", "num_classes", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "l2renorm", "=", "l2renorm", "\n", "self", ".", "task", "=", "task", "\n", "self", ".", "geometric_mlp", "=", "geometric_mlp", "\n", "self", ".", "feat_aggregation", "=", "feat_aggregation", "\n", "self", ".", "expert_dims", "=", "expert_dims", "\n", "self", ".", "num_h_layers", "=", "num_h_layers", "\n", "self", ".", "num_g_layers", "=", "num_g_layers", "\n", "self", ".", "use_mish", "=", "use_mish", "\n", "self", ".", "use_bn_resaon", "=", "use_bn_reason", "\n", "self", ".", "include_self", "=", "include_self", "\n", "self", ".", "kron_dets", "=", "kron_dets", "\n", "self", ".", "rand_proj", "=", "rand_proj", "\n", "self", ".", "coord_dets", "=", "coord_dets", "\n", "self", ".", "disable_nan_checks", "=", "disable_nan_checks", "\n", "self", ".", "trn_config", "=", "trn_config", "\n", "self", ".", "trn_cat", "=", "trn_cat", "\n", "if", "randomise_feats", ":", "\n", "            ", "self", ".", "random_feats", "=", "set", "(", "[", "x", "for", "x", "in", "randomise_feats", ".", "split", "(", "\",\"", ")", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "random_feats", "=", "set", "(", ")", "\n", "\n", "# sanity checks on the features that may be vladded", "\n", "", "pre_vlad_feat_sizes", "=", "{", "\"ocr\"", ":", "300", ",", "\"audio\"", ":", "128", ",", "\"speech\"", ":", "300", "}", "\n", "pre_vlad_feat_sizes", "=", "{", "key", ":", "val", "for", "key", ",", "val", "in", "pre_vlad_feat_sizes", ".", "items", "(", ")", "\n", "if", "feat_aggregation", "[", "key", "]", "[", "\"temporal\"", "]", "==", "\"vlad\"", "}", "\n", "\n", "# we basically disable safety checks for detection-sem", "\n", "if", "spatial_feats", ":", "\n", "            ", "spatial_feat_dim", "=", "16", "\n", "", "else", ":", "\n", "            ", "spatial_feat_dim", "=", "5", "\n", "", "if", "self", ".", "geometric_mlp", ":", "\n", "            ", "self", ".", "geometric_mlp_model", "=", "SpatialMLP", "(", "spatial_feat_dim", ")", "\n", "", "if", "kron_dets", ":", "\n", "            ", "sem_det_dim", "=", "300", "*", "spatial_feat_dim", "\n", "", "elif", "coord_dets", ":", "\n", "            ", "sem_det_dim", "=", "spatial_feat_dim", "\n", "", "elif", "rand_proj", ":", "\n", "            ", "sem_det_dim", "=", "300", "+", "300", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "spatial_feat_dim", ",", "300", ")", "\n", "# random proj", "\n", "# th.nn.init.normal_(self.proj.weight, mean=0, std=1)", "\n", "# for param in self.proj.parameters():", "\n", "#     param.requires_grad = False", "\n", "# self.proj.bias.mul_(0)", "\n", "", "else", ":", "\n", "            ", "sem_det_dim", "=", "300", "+", "spatial_feat_dim", "\n", "", "self", ".", "spatial_feat_dim", "=", "spatial_feat_dim", "\n", "pre_vlad_feat_sizes", "[", "\"detection-sem\"", "]", "=", "sem_det_dim", "\n", "if", "\"detection-sem\"", "in", "expert_dims", ":", "\n", "            ", "new_in_dim", "=", "sem_det_dim", "*", "vlad_clusters", "[", "\"detection-sem\"", "]", "\n", "expert_dims", "[", "\"detection-sem\"", "]", "=", "(", "new_in_dim", ",", "expert_dims", "[", "\"detection-sem\"", "]", "[", "1", "]", ")", "\n", "\n", "", "vlad_feat_sizes", "=", "{", "key", ":", "val", "for", "key", ",", "val", "in", "vlad_clusters", ".", "items", "(", ")", "}", "\n", "\n", "self", ".", "pooling", "=", "nn", ".", "ModuleDict", "(", ")", "\n", "for", "mod", ",", "expected", "in", "pre_vlad_feat_sizes", ".", "items", "(", ")", ":", "\n", "            ", "if", "mod", "in", "expert_dims", ".", "keys", "(", ")", ":", "\n", "                ", "feature_size", "=", "expert_dims", "[", "mod", "]", "[", "0", "]", "//", "vlad_clusters", "[", "mod", "]", "\n", "msg", "=", "f\"expected {expected} for {mod} features atm\"", "\n", "assert", "feature_size", "==", "expected", ",", "msg", "\n", "self", ".", "pooling", "[", "mod", "]", "=", "NetVLAD", "(", "\n", "feature_size", "=", "feature_size", ",", "\n", "cluster_size", "=", "vlad_clusters", "[", "mod", "]", ",", "\n", ")", "\n", "", "", "if", "\"retrieval\"", "in", "self", ".", "task", ":", "\n", "            ", "if", "vlad_clusters", "[", "\"text\"", "]", "==", "0", ":", "\n", "                ", "self", ".", "text_pooling", "=", "nn", ".", "Sequential", "(", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "text_pooling", "=", "NetVLAD", "(", "\n", "feature_size", "=", "text_dim", ",", "\n", "cluster_size", "=", "vlad_clusters", "[", "\"text\"", "]", ",", "\n", "ghost_clusters", "=", "ghost_clusters", "[", "\"text\"", "]", ",", "\n", ")", "\n", "text_dim", "=", "self", ".", "text_pooling", ".", "out_dim", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "num_classes", "=", "num_classes", "\n", "text_dim", "=", "None", "\n", "\n", "", "self", ".", "tensor_storage", "=", "expert_tensor_storage", "(", "\n", "experts", "=", "self", ".", "expert_dims", ".", "keys", "(", ")", ",", "\n", "feat_aggregation", "=", "self", ".", "feat_aggregation", ",", "\n", ")", "\n", "# self.fixed_sz_experts = [\"rgb\", \"scene\", \"face\", \"flow\"]", "\n", "# self.variable_sz_experts = [\"audio\", \"speech\", \"ocr\"]", "\n", "# # handle features which can be either", "\n", "# for expert in {\"detection\", \"detection-sem\", \"openpose\"}:", "\n", "#     if expert in self.pooling:", "\n", "#         self.variable_sz_experts.append(expert)", "\n", "#     else:", "\n", "#         self.fixed_sz_experts.append(expert)", "\n", "\n", "self", ".", "ce", "=", "CEModule", "(", "\n", "use_ce", "=", "use_ce", ",", "\n", "task", "=", "self", ".", "task", ",", "\n", "verbose", "=", "verbose", ",", "\n", "l2renorm", "=", "l2renorm", ",", "\n", "trn_cat", "=", "self", ".", "trn_cat", ",", "\n", "trn_config", "=", "self", ".", "trn_config", ",", "\n", "random_feats", "=", "self", ".", "random_feats", ",", "\n", "freeze_weights", "=", "freeze_weights", ",", "\n", "text_dim", "=", "text_dim", ",", "\n", "test_caption_mode", "=", "test_caption_mode", ",", "\n", "concat_experts", "=", "concat_experts", ",", "\n", "concat_mix_experts", "=", "concat_mix_experts", ",", "\n", "expert_dims", "=", "expert_dims", ",", "\n", "vlad_feat_sizes", "=", "vlad_feat_sizes", ",", "\n", "disable_nan_checks", "=", "disable_nan_checks", ",", "\n", "keep_missing_modalities", "=", "keep_missing_modalities", ",", "\n", "mimic_ce_dims", "=", "mimic_ce_dims", ",", "\n", "include_self", "=", "include_self", ",", "\n", "use_mish", "=", "use_mish", ",", "\n", "use_bn_reason", "=", "use_bn_reason", ",", "\n", "num_h_layers", "=", "num_h_layers", ",", "\n", "num_g_layers", "=", "num_g_layers", ",", "\n", "num_classes", "=", "num_classes", ",", "\n", "same_dim", "=", "ce_shared_dim", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.CENet.randomise_feats": [[241, 250], ["torch.isnan", "torch.isnan", "torch.isnan", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor.to", "float"], "methods", ["None"], ["", "def", "randomise_feats", "(", "self", ",", "experts", ",", "key", ")", ":", "\n", "        ", "if", "key", "in", "self", ".", "random_feats", ":", "\n", "# keep expected nans", "\n", "            ", "nan_mask", "=", "th", ".", "isnan", "(", "experts", "[", "key", "]", ")", "\n", "experts", "[", "key", "]", "=", "th", ".", "randn_like", "(", "experts", "[", "key", "]", ")", "\n", "if", "not", "self", ".", "disable_nan_checks", ":", "\n", "                ", "nans", "=", "th", ".", "tensor", "(", "float", "(", "'nan'", ")", ")", "# pylint: disable=not-callable", "\n", "experts", "[", "key", "]", "[", "nan_mask", "]", "=", "nans", ".", "to", "(", "experts", "[", "key", "]", ".", "device", ")", "\n", "", "", "return", "experts", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.CENet.forward": [[251, 334], ["collections.OrderedDict", "model.CENet.ce", "model.CENet.randomise_feats", "model.drop_nans", "text.view.view.size", "text.view.view.view", "isinstance", "model.CENet.text_pooling", "text.view.view.view", "model.CENet.view.view", "model.CENet.geometric_mlp_model", "model.CENet.view", "model.kronecker_prod", "model.CENet.view.contiguous", "model.CENet.view.contiguous", "model.CENet.proj", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "sem_feats.contiguous", "sem_feats.contiguous"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.CENet.randomise_feats", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.drop_nans", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.kronecker_prod"], ["", "def", "forward", "(", "self", ",", "experts", ",", "ind", ",", "text", "=", "None", ",", "raw_captions", "=", "None", ",", "text_token_mask", "=", "None", ")", ":", "\n", "        ", "aggregated_experts", "=", "OrderedDict", "(", ")", "\n", "\n", "if", "\"detection-sem\"", "in", "self", ".", "expert_dims", ":", "\n", "            ", "det_sem", "=", "experts", "[", "\"detection-sem\"", "]", "\n", "box_feats", "=", "det_sem", "[", ":", ",", ":", ",", ":", "self", ".", "spatial_feat_dim", "]", "\n", "sem_feats", "=", "det_sem", "[", ":", ",", ":", ",", "self", ".", "spatial_feat_dim", ":", "]", "\n", "if", "self", ".", "geometric_mlp", ":", "\n", "                ", "x", "=", "box_feats", ".", "view", "(", "-", "1", ",", "box_feats", ".", "shape", "[", "-", "1", "]", ")", "\n", "x", "=", "self", ".", "geometric_mlp_model", "(", "x", ")", "\n", "box_feats", "=", "x", ".", "view", "(", "box_feats", ".", "shape", ")", "\n", "\n", "", "if", "self", ".", "kron_dets", ":", "\n", "                ", "feats", "=", "kronecker_prod", "(", "box_feats", ",", "sem_feats", ")", "\n", "", "elif", "self", ".", "coord_dets", ":", "\n", "                ", "feats", "=", "box_feats", ".", "contiguous", "(", ")", "\n", "", "elif", "self", ".", "rand_proj", ":", "\n", "                ", "feats", "=", "box_feats", ".", "contiguous", "(", ")", "\n", "projected", "=", "self", ".", "proj", "(", "feats", ")", "\n", "feats", "=", "th", ".", "cat", "(", "(", "projected", ",", "sem_feats", ".", "contiguous", "(", ")", ")", ",", "dim", "=", "2", ")", "\n", "", "else", ":", "\n", "                ", "feats", "=", "th", ".", "cat", "(", "(", "box_feats", ",", "sem_feats", ".", "contiguous", "(", ")", ")", ",", "dim", "=", "2", ")", "\n", "", "experts", "[", "\"detection-sem\"", "]", "=", "feats", "\n", "\n", "# Handle all nan-checks", "\n", "", "for", "mod", "in", "self", ".", "expert_dims", ":", "\n", "            ", "experts", "=", "self", ".", "randomise_feats", "(", "experts", ",", "mod", ")", "\n", "experts", "[", "mod", "]", "=", "drop_nans", "(", "x", "=", "experts", "[", "mod", "]", ",", "ind", "=", "ind", "[", "mod", "]", ",", "validate_missing", "=", "True", ")", "\n", "if", "mod", "in", "self", ".", "tensor_storage", "[", "\"fixed\"", "]", ":", "\n", "                ", "aggregated_experts", "[", "mod", "]", "=", "experts", "[", "mod", "]", "\n", "", "elif", "mod", "in", "self", ".", "tensor_storage", "[", "\"variable\"", "]", ":", "\n", "                ", "aggregated_experts", "[", "mod", "]", "=", "self", ".", "pooling", "[", "mod", "]", "(", "experts", "[", "mod", "]", ")", "\n", "\n", "# for mod in variable_sz_experts:", "\n", "#     if mod in self.expert_dims.keys():", "\n", "#         experts[mod] = drop_nans(x=experts[mod], ind=ind[mod],", "\n", "#                                  validate_missing=True)", "\n", "#         experts = self.randomise_feats(experts, mod)", "\n", "\n", "# if \"rgb\" in self.expert_dims.keys():", "\n", "#     experts = self.randomise_feats(experts, \"rgb\")", "\n", "#     # If only average pooling has been performed, we will have an input of the", "\n", "#     # form N x 1 x D, so we need to flatten out the middle dimension to", "\n", "#     # maintain consistency", "\n", "#     aggregated_experts[\"rgb\"] = experts[\"rgb\"].view(experts[\"rgb\"].shape[0], -1)", "\n", "\n", "", "", "if", "\"retrieval\"", "in", "self", ".", "task", ":", "\n", "# When pooling multiple captions for a single video, we treat them as separate", "\n", "# members of the minibatch, so the total pooling op does the following:", "\n", "# pooling: B x captions_per_video x max_sentence_length x text_feat_dim", "\n", "# -> B x captions_per_video (cluster_dim * text_feat_dim)", "\n", "            ", "B", ",", "captions_per_video", ",", "max_words", ",", "text_feat_dim", "=", "text", ".", "size", "(", ")", "\n", "text", "=", "text", ".", "view", "(", "B", "*", "captions_per_video", ",", "max_words", ",", "text_feat_dim", ")", "\n", "\n", "# avoid OOM errors on old GPUs during inference", "\n", "# if not self.text_pooling.training:", "\n", "#     safe = 5000", "\n", "#     if text.shape[0] > safe:", "\n", "#         splits = [safe] * (text.shape[0] // safe)", "\n", "#         splits += [text.shape[0] - sum(splits)]", "\n", "#         texts = th.split(text, split_size_or_sections=splits)", "\n", "#         texts = [self.text_pooling(text) for text in texts]", "\n", "#         text = th.cat(texts, dim=0)", "\n", "#         import ipdb; ipdb.set_trace()", "\n", "#     else:", "\n", "# safe = 5000", "\n", "# if text.shape[0] > safe:", "\n", "#     dev = text.device", "\n", "#     print(\"pooling text features on cpu\")", "\n", "#     text, self.text_pooling = text.to(\"cpu\"), self.text_pooling.to(\"cpu\")", "\n", "#     text = self.text_pooling(text)", "\n", "#     text, self.text_pooling = text.to(dev), self.text_pooling.to(dev)", "\n", "#     import ipdb; ipdb.set_trace()", "\n", "# else:", "\n", "if", "isinstance", "(", "self", ".", "text_pooling", ",", "NetVLAD", ")", ":", "\n", "                ", "kwargs", "=", "{", "\"mask\"", ":", "text_token_mask", "}", "\n", "", "else", ":", "\n", "                ", "kwargs", "=", "{", "}", "\n", "", "text", "=", "self", ".", "text_pooling", "(", "text", ",", "**", "kwargs", ")", "\n", "text", "=", "text", ".", "view", "(", "B", ",", "captions_per_video", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "text", "=", "None", "\n", "", "return", "self", ".", "ce", "(", "text", ",", "aggregated_experts", ",", "ind", ",", "raw_captions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.TemporalAttention.__init__": [[337, 344], ["super().__init__", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "img_feature_dim", ",", "num_attention", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "weight", "=", "Variable", "(", "\n", "th", ".", "randn", "(", "img_feature_dim", ",", "num_attention", ")", ",", "\n", "requires_grad", "=", "True", ")", ".", "cuda", "(", ")", "# d*seg", "\n", "self", ".", "img_feature_dim", "=", "img_feature_dim", "\n", "self", ".", "num_attention", "=", "num_attention", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.TemporalAttention.forward": [[345, 362], ["torch.mean", "torch.mean", "torch.mean", "torch.max", "torch.max", "torch.max", "record.append", "record.append", "torch.matmul", "torch.matmul", "torch.matmul", "torch.softmax", "torch.softmax", "torch.softmax", "range", "torch.cat", "torch.cat", "torch.cat", "input.clone", "input.clone", "torch.sum", "torch.sum", "torch.sum", "temp_output.div.div.norm", "temp_output.div.div.div", "record.append", "temp.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "B", ",", "T", ",", "D", "=", "input", ".", "shape", "\n", "record", "=", "[", "]", "\n", "input_avg", "=", "th", ".", "mean", "(", "input", ".", "clone", "(", ")", ",", "dim", "=", "1", ")", "\n", "input_max", "=", "th", ".", "max", "(", "input", ".", "clone", "(", ")", ",", "dim", "=", "1", ")", "\n", "record", ".", "append", "(", "input_avg", ")", "\n", "record", ".", "append", "(", "input_max", "[", "0", "]", ")", "\n", "output", "=", "th", ".", "matmul", "(", "input", ",", "self", ".", "weight", ")", "\n", "attentions", "=", "F", ".", "softmax", "(", "output", ",", "dim", "=", "1", ")", "\n", "for", "idx", "in", "range", "(", "attentions", ".", "shape", "[", "-", "1", "]", ")", ":", "\n", "            ", "temp", "=", "attentions", "[", ":", ",", ":", ",", "idx", "]", "\n", "temp_output", "=", "th", ".", "sum", "(", "temp", ".", "unsqueeze", "(", "2", ")", "*", "input", ",", "dim", "=", "1", ")", "\n", "norm", "=", "temp_output", ".", "norm", "(", "p", "=", "2", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "temp_output", "=", "temp_output", ".", "div", "(", "norm", ")", "\n", "record", ".", "append", "(", "temp_output", ")", "\n", "", "act_all", "=", "th", ".", "cat", "(", "(", "record", ")", ",", "1", ")", "\n", "return", "act_all", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.RelationModuleMultiScale.__init__": [[367, 398], ["super().__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "print", "model.RelationModuleMultiScale.return_relationset", "model.RelationModuleMultiScale.relations_scales.append", "model.RelationModuleMultiScale.subsample_scales.append", "len", "torch.Sequential", "torch.Sequential", "torch.Sequential", "range", "min", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "len"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.RelationModuleMultiScale_Cat.return_relationset"], ["    ", "def", "__init__", "(", "self", ",", "img_feature_dim", ",", "num_frames", ",", "num_class", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "subsample_num", "=", "3", "# how many relations selected to sum up", "\n", "self", ".", "img_feature_dim", "=", "img_feature_dim", "\n", "# generate the multiple frame relations", "\n", "self", ".", "scales", "=", "[", "i", "for", "i", "in", "range", "(", "num_frames", ",", "1", ",", "-", "1", ")", "]", "\n", "\n", "self", ".", "relations_scales", "=", "[", "]", "\n", "self", ".", "subsample_scales", "=", "[", "]", "\n", "for", "scale", "in", "self", ".", "scales", ":", "\n", "            ", "relations_scale", "=", "self", ".", "return_relationset", "(", "num_frames", ",", "scale", ")", "\n", "self", ".", "relations_scales", ".", "append", "(", "relations_scale", ")", "\n", "# how many samples of relation to select in each forward pass", "\n", "self", ".", "subsample_scales", ".", "append", "(", "min", "(", "self", ".", "subsample_num", ",", "len", "(", "relations_scale", ")", ")", ")", "\n", "\n", "", "self", ".", "num_class", "=", "num_class", "\n", "self", ".", "num_frames", "=", "num_frames", "\n", "num_bottleneck", "=", "256", "\n", "self", ".", "fc_fusion_scales", "=", "nn", ".", "ModuleList", "(", ")", "# high-tech modulelist", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "scales", ")", ")", ":", "\n", "            ", "scale", "=", "self", ".", "scales", "[", "i", "]", "\n", "fc_fusion", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "scale", "*", "self", ".", "img_feature_dim", ",", "num_bottleneck", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "num_bottleneck", ",", "self", ".", "num_class", ")", ",", "\n", ")", "\n", "self", ".", "fc_fusion_scales", "+=", "[", "fc_fusion", "]", "\n", "\n", "", "msg", "=", "'Multi-Scale Temporal Relation Network Module in use'", "\n", "print", "(", "msg", ",", "[", "'%d-frame relation'", "%", "i", "for", "i", "in", "self", ".", "scales", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.RelationModuleMultiScale.forward": [[399, 418], ["act_all.view.view.view", "range", "act_all.view.view.size", "len", "numpy.random.choice", "len", "act_relation.view.view.view", "act_relation.view.view.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "# the first one is the largest scale", "\n", "        ", "act_all", "=", "input", "[", ":", ",", "self", ".", "relations_scales", "[", "0", "]", "[", "0", "]", ",", ":", "]", "\n", "act_all", "=", "act_all", ".", "view", "(", "act_all", ".", "size", "(", "0", ")", ",", "self", ".", "scales", "[", "0", "]", "*", "self", ".", "img_feature_dim", ")", "\n", "act_all", "=", "self", ".", "fc_fusion_scales", "[", "0", "]", "(", "act_all", ")", "\n", "\n", "for", "scaleID", "in", "range", "(", "1", ",", "len", "(", "self", ".", "scales", ")", ")", ":", "\n", "# iterate over the scales", "\n", "            ", "idx_relations_randomsample", "=", "np", ".", "random", ".", "choice", "(", "\n", "len", "(", "self", ".", "relations_scales", "[", "scaleID", "]", ")", ",", "\n", "self", ".", "subsample_scales", "[", "scaleID", "]", ",", "\n", "replace", "=", "False", ",", "\n", ")", "\n", "for", "idx", "in", "idx_relations_randomsample", ":", "\n", "                ", "act_relation", "=", "input", "[", ":", ",", "self", ".", "relations_scales", "[", "scaleID", "]", "[", "idx", "]", ",", ":", "]", "\n", "act_relation", "=", "act_relation", ".", "view", "(", "act_relation", ".", "size", "(", "0", ")", ",", "self", ".", "scales", "[", "scaleID", "]", "*", "self", ".", "img_feature_dim", ")", "\n", "act_relation", "=", "self", ".", "fc_fusion_scales", "[", "scaleID", "]", "(", "act_relation", ")", "\n", "act_all", "+=", "act_relation", "\n", "", "", "return", "act_all", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.RelationModuleMultiScale.return_relationset": [[419, 422], ["list", "itertools.combinations", "range"], "methods", ["None"], ["", "def", "return_relationset", "(", "self", ",", "num_frames", ",", "num_frames_relation", ")", ":", "\n", "        ", "import", "itertools", "\n", "return", "list", "(", "itertools", ".", "combinations", "(", "[", "i", "for", "i", "in", "range", "(", "num_frames", ")", "]", ",", "num_frames_relation", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.RelationModuleMultiScale_Cat.__init__": [[427, 456], ["super().__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "print", "model.RelationModuleMultiScale_Cat.return_relationset", "model.RelationModuleMultiScale_Cat.relations_scales.append", "model.RelationModuleMultiScale_Cat.subsample_scales.append", "len", "torch.Sequential", "torch.Sequential", "torch.Sequential", "range", "min", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "len"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.RelationModuleMultiScale_Cat.return_relationset"], ["    ", "def", "__init__", "(", "self", ",", "img_feature_dim", ",", "num_frames", ",", "num_class", ")", ":", "\n", "        ", "super", "(", "RelationModuleMultiScale_Cat", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "subsample_num", "=", "3", "# how many relations selected to sum up", "\n", "self", ".", "img_feature_dim", "=", "img_feature_dim", "\n", "self", ".", "scales", "=", "[", "i", "for", "i", "in", "range", "(", "num_frames", ",", "1", ",", "-", "1", ")", "]", "# generate the multiple frame relations", "\n", "\n", "self", ".", "relations_scales", "=", "[", "]", "\n", "self", ".", "subsample_scales", "=", "[", "]", "\n", "for", "scale", "in", "self", ".", "scales", ":", "\n", "            ", "relations_scale", "=", "self", ".", "return_relationset", "(", "num_frames", ",", "scale", ")", "\n", "self", ".", "relations_scales", ".", "append", "(", "relations_scale", ")", "\n", "self", ".", "subsample_scales", ".", "append", "(", "min", "(", "self", ".", "subsample_num", ",", "len", "(", "relations_scale", ")", ")", ")", "# how many samples of relation to select in each forward pass", "\n", "\n", "", "self", ".", "num_class", "=", "num_class", "\n", "self", ".", "num_frames", "=", "num_frames", "\n", "num_bottleneck", "=", "256", "\n", "self", ".", "fc_fusion_scales", "=", "nn", ".", "ModuleList", "(", ")", "# high-tech modulelist", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "scales", ")", ")", ":", "\n", "            ", "scale", "=", "self", ".", "scales", "[", "i", "]", "\n", "fc_fusion", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "scale", "*", "self", ".", "img_feature_dim", ",", "num_bottleneck", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "num_bottleneck", ",", "self", ".", "num_class", ")", ",", "\n", ")", "\n", "\n", "self", ".", "fc_fusion_scales", "+=", "[", "fc_fusion", "]", "\n", "\n", "", "print", "(", "'Multi-Scale Temporal Relation Network Module in use'", ",", "[", "'%d-frame relation'", "%", "i", "for", "i", "in", "self", ".", "scales", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.RelationModuleMultiScale_Cat.forward": [[457, 482], ["act_all.div.div.view", "act_all.div.div.norm", "act_all.div.div.div", "record.append", "range", "torch.cat", "torch.cat", "torch.cat", "act_all.div.div.size", "len", "numpy.random.choice", "act_all.div.div.norm", "act_all.div.div.div", "record.append", "len", "act_relation.view.view.view", "act_relation.view.view.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "record", "=", "[", "]", "\n", "# the first one is the largest scale", "\n", "act_all", "=", "input", "[", ":", ",", "self", ".", "relations_scales", "[", "0", "]", "[", "0", "]", ",", ":", "]", "\n", "act_all", "=", "act_all", ".", "view", "(", "act_all", ".", "size", "(", "0", ")", ",", "self", ".", "scales", "[", "0", "]", "*", "self", ".", "img_feature_dim", ")", "\n", "act_all", "=", "self", ".", "fc_fusion_scales", "[", "0", "]", "(", "act_all", ")", "\n", "norm", "=", "act_all", ".", "norm", "(", "p", "=", "2", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "act_all", "=", "act_all", ".", "div", "(", "norm", ")", "\n", "record", ".", "append", "(", "act_all", ")", "\n", "\n", "for", "scaleID", "in", "range", "(", "1", ",", "len", "(", "self", ".", "scales", ")", ")", ":", "\n", "# iterate over the scales", "\n", "            ", "idx_relations_randomsample", "=", "np", ".", "random", ".", "choice", "(", "len", "(", "self", ".", "relations_scales", "[", "scaleID", "]", ")", ",", "self", ".", "subsample_scales", "[", "scaleID", "]", ",", "replace", "=", "False", ")", "\n", "act_all", "=", "0", "\n", "for", "idx", "in", "idx_relations_randomsample", ":", "\n", "                ", "act_relation", "=", "input", "[", ":", ",", "self", ".", "relations_scales", "[", "scaleID", "]", "[", "idx", "]", ",", ":", "]", "\n", "act_relation", "=", "act_relation", ".", "view", "(", "act_relation", ".", "size", "(", "0", ")", ",", "self", ".", "scales", "[", "scaleID", "]", "*", "self", ".", "img_feature_dim", ")", "\n", "act_relation", "=", "self", ".", "fc_fusion_scales", "[", "scaleID", "]", "(", "act_relation", ")", "\n", "act_all", "+=", "act_relation", "\n", "", "norm", "=", "act_all", ".", "norm", "(", "p", "=", "2", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "act_all", "=", "act_all", ".", "div", "(", "norm", ")", "\n", "record", ".", "append", "(", "act_all", ")", "\n", "\n", "", "act_all", "=", "th", ".", "cat", "(", "(", "record", ")", ",", "1", ")", "\n", "return", "act_all", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.RelationModuleMultiScale_Cat.return_relationset": [[483, 487], ["list", "itertools.combinations", "range"], "methods", ["None"], ["", "def", "return_relationset", "(", "self", ",", "num_frames", ",", "num_frames_relation", ")", ":", "\n", "        ", "import", "itertools", "\n", "return", "list", "(", "itertools", ".", "combinations", "(", "[", "i", "for", "i", "in", "range", "(", "num_frames", ")", "]", ",", "\n", "num_frames_relation", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.CEModule.__init__": [[490, 670], ["torch.Module.__init__", "list", "print", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "expert_dims.keys", "model.Mish", "torch.ReLU", "torch.ReLU", "torch.ReLU", "len", "torch.Linear", "torch.Linear", "torch.Linear", "print", "model.CEModule.trn_config.keys", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "print", "model.CEModule.expert_dims.keys", "print", "torch.Linear", "torch.Linear", "torch.Linear", "len", "torch.ones", "torch.ones", "torch.ones", "print", "model.CEModule.trn_config.keys", "model.ReduceDim", "torch.Linear", "torch.Linear", "torch.Linear", "g_reason_shared.append", "g_reason_shared.append", "h_reason.append", "h_reason.append", "model.GatedEmbeddingUnitReasoning", "model.TemporalAttention", "len", "print", "model.CEModule.trn_config.keys", "NotImplementedError", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "g_reason_shared.append", "torch.Linear", "torch.Linear", "torch.Linear", "h_reason.append", "torch.Linear", "torch.Linear", "torch.Linear", "model.MimicCEGatedEmbeddingUnit", "torch.Sequential", "torch.Sequential", "torch.Sequential", "model.RelationModuleMultiScale_Cat", "model.G_reason", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "ValueError", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "sum", "sum", "model.GatedEmbeddingUnit", "model.GatedEmbeddingUnit", "model.GatedEmbeddingUnit", "model.RelationModuleMultiScale", "model.GatedEmbeddingUnit", "sum", "range", "zip"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys"], ["    ", "def", "__init__", "(", "self", ",", "expert_dims", ",", "text_dim", ",", "use_ce", ",", "verbose", ",", "l2renorm", ",", "num_classes", ",", "\n", "trn_config", ",", "trn_cat", ",", "use_mish", ",", "include_self", ",", "num_h_layers", ",", "num_g_layers", ",", "\n", "disable_nan_checks", ",", "random_feats", ",", "test_caption_mode", ",", "mimic_ce_dims", ",", "\n", "concat_experts", ",", "concat_mix_experts", ",", "freeze_weights", ",", "task", ",", "\n", "keep_missing_modalities", ",", "vlad_feat_sizes", ",", "same_dim", ",", "use_bn_reason", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "modalities", "=", "list", "(", "expert_dims", ".", "keys", "(", ")", ")", "\n", "self", ".", "expert_dims", "=", "expert_dims", "\n", "self", ".", "modalities", "=", "modalities", "\n", "self", ".", "disable_nan_checks", "=", "disable_nan_checks", "\n", "self", ".", "mimic_ce_dims", "=", "mimic_ce_dims", "\n", "self", ".", "concat_experts", "=", "concat_experts", "\n", "self", ".", "same_dim", "=", "same_dim", "\n", "self", ".", "use_mish", "=", "use_mish", "\n", "self", ".", "use_bn_reason", "=", "use_bn_reason", "\n", "self", ".", "num_h_layers", "=", "num_h_layers", "\n", "self", ".", "num_g_layers", "=", "num_g_layers", "\n", "self", ".", "include_self", "=", "include_self", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "task", "=", "task", "\n", "self", ".", "vlad_feat_sizes", "=", "vlad_feat_sizes", "\n", "self", ".", "concat_mix_experts", "=", "concat_mix_experts", "\n", "self", ".", "test_caption_mode", "=", "test_caption_mode", "\n", "self", ".", "reduce_dim", "=", "64", "\n", "self", ".", "moe_cg", "=", "ContextGating", "\n", "self", ".", "freeze_weights", "=", "freeze_weights", "\n", "self", ".", "random_feats", "=", "random_feats", "\n", "self", ".", "use_ce", "=", "use_ce", "\n", "self", ".", "verbose", "=", "verbose", "\n", "self", ".", "keep_missing_modalities", "=", "keep_missing_modalities", "\n", "self", ".", "l2renorm", "=", "l2renorm", "\n", "self", ".", "trn_config", "=", "trn_config", "\n", "self", ".", "trn_cat", "=", "trn_cat", "\n", "print", "(", "\"trn_config is {}\"", ".", "format", "(", "self", ".", "trn_config", ")", ")", "\n", "\n", "if", "self", ".", "use_mish", ":", "\n", "            ", "self", ".", "non_lin", "=", "Mish", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "non_lin", "=", "nn", ".", "ReLU", "(", ")", "\n", "\n", "", "if", "\"retrieval\"", "in", "self", ".", "task", ":", "\n", "            ", "num_mods", "=", "len", "(", "expert_dims", ")", "\n", "self", ".", "moe_fc", "=", "nn", ".", "Linear", "(", "text_dim", ",", "len", "(", "expert_dims", ")", ")", "\n", "# self.moe_fc_bottleneck1 = nn.Linear(text_dim, text_dim // 4)", "\n", "# self.moe_cg = nn.Linear(text_dim // 4, text_dim // 4)", "\n", "# self.moe_fc_proj = nn.Linear(text_dim // 4, len(expert_dims))", "\n", "self", ".", "moe_weights", "=", "th", ".", "ones", "(", "1", ",", "num_mods", ")", "/", "num_mods", "\n", "\n", "# The batch size of the face input can vary (due to missing inputs), so we", "\n", "# probably shouldn't use BN on this branch. It's probably fine to leave it", "\n", "# n for the corresponding text inputs, (but we should switch to GN)", "\n", "", "use_bns", "=", "[", "True", "for", "modality", "in", "self", ".", "modalities", "]", "\n", "\n", "# NOTE: When use_ce is not used, the text features are projected to", "\n", "# subspaces of different dimensions.  When use_ce is used, they must all", "\n", "# be projected to `same_dim` (to allow fusion). The only excpetion is for an", "\n", "# ablation in which we mimic the `same_dim` reduction to measure whether this", "\n", "# projection influences overall performance.", "\n", "self", ".", "trn_list", "=", "nn", ".", "ModuleList", "(", ")", "\n", "\n", "self", ".", "repeat_temporal", "=", "{", "}", "\n", "for", "mod", "in", "modalities", ":", "\n", "            ", "self", ".", "repeat_temporal", "[", "mod", "]", "=", "1", "\n", "\n", "", "if", "self", ".", "trn_cat", "==", "2", ":", "\n", "            ", "print", "(", "\"Performing concat between random temporal attention\"", ")", "\n", "for", "mod", "in", "self", ".", "trn_config", ".", "keys", "(", ")", ":", "\n", "                ", "img_feature_dim", "=", "expert_dims", "[", "mod", "]", "[", "0", "]", "# 365", "\n", "num_frames", "=", "self", ".", "trn_config", "[", "\n", "mod", "]", "# This is exatcly how many different attention", "\n", "num_frames", "=", "1", "# mimic simple avg and max based on segments", "\n", "# num_class = expert_dims[mod][0]", "\n", "self", ".", "trn_list", "+=", "[", "TemporalAttention", "(", "img_feature_dim", ",", "num_frames", ")", "]", "\n", "self", ".", "repeat_temporal", "[", "mod", "]", "=", "num_frames", "+", "2", "\n", "", "", "elif", "self", ".", "trn_cat", "==", "1", ":", "\n", "            ", "print", "(", "\"Performing concat between segments\"", ")", "\n", "for", "mod", "in", "self", ".", "trn_config", ".", "keys", "(", ")", ":", "\n", "                ", "img_feature_dim", "=", "expert_dims", "[", "mod", "]", "[", "0", "]", "# 365", "\n", "num_frames", "=", "self", ".", "trn_config", "[", "mod", "]", "# hard code", "\n", "num_class", "=", "expert_dims", "[", "mod", "]", "[", "0", "]", "\n", "self", ".", "trn_list", "+=", "[", "\n", "RelationModuleMultiScale_Cat", "(", "img_feature_dim", ",", "num_frames", ",", "num_class", ")", "\n", "]", "\n", "self", ".", "repeat_temporal", "[", "mod", "]", "=", "len", "(", "\n", "[", "i", "for", "i", "in", "range", "(", "num_frames", ",", "1", ",", "-", "1", ")", "]", ")", "\n", "", "", "elif", "self", ".", "trn_cat", "==", "0", ":", "\n", "            ", "print", "(", "\"Performing Conventional TRN (sum) segments\"", ")", "\n", "for", "mod", "in", "self", ".", "trn_config", ".", "keys", "(", ")", ":", "\n", "                ", "img_feature_dim", "=", "expert_dims", "[", "mod", "]", "[", "0", "]", "# 365", "\n", "num_frames", "=", "self", ".", "trn_config", "[", "mod", "]", "# hard code", "\n", "num_class", "=", "expert_dims", "[", "mod", "]", "[", "0", "]", "\n", "self", ".", "trn_list", "+=", "[", "\n", "RelationModuleMultiScale", "(", "img_feature_dim", ",", "num_frames", ",", "\n", "num_class", ")", "\n", "]", "\n", "", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", ")", "\n", "\n", "", "in_dims", "=", "[", "expert_dims", "[", "mod", "]", "[", "0", "]", "*", "self", ".", "repeat_temporal", "[", "mod", "]", "for", "mod", "in", "modalities", "]", "\n", "agg_dims", "=", "[", "expert_dims", "[", "mod", "]", "[", "1", "]", "*", "self", ".", "repeat_temporal", "[", "mod", "]", "for", "mod", "in", "modalities", "]", "\n", "\n", "if", "self", ".", "use_ce", "or", "self", ".", "mimic_ce_dims", ":", "\n", "            ", "dim_reducers", "=", "[", "ReduceDim", "(", "in_dim", ",", "same_dim", ")", "for", "in_dim", "in", "in_dims", "]", "\n", "self", ".", "video_dim_reduce", "=", "nn", ".", "ModuleList", "(", "dim_reducers", ")", "\n", "", "if", "self", ".", "use_ce", ":", "\n", "# The g_reason module has a first layer that is specific to the design choice", "\n", "# (e.g. triplet vs pairwise), then a shared component which is common to all", "\n", "# designs.", "\n", "            ", "if", "self", ".", "use_ce", "in", "{", "\"pairwise\"", ",", "\"pairwise-star\"", ",", "\"triplet\"", "}", ":", "\n", "                ", "num_inputs", "=", "3", "if", "self", ".", "use_ce", "==", "\"triplet\"", "else", "2", "\n", "self", ".", "g_reason_1", "=", "nn", ".", "Linear", "(", "same_dim", "*", "num_inputs", ",", "same_dim", ")", "\n", "", "elif", "self", ".", "use_ce", "==", "\"pairwise-star-specific\"", ":", "\n", "                ", "num_inputs", "=", "2", "\n", "g_reason_unshared_weights", "=", "[", "G_reason", "(", "same_dim", ",", "num_inputs", ",", "self", ".", "non_lin", ")", "\n", "for", "mod", "in", "modalities", "]", "\n", "self", ".", "g_reason_unshared_weights", "=", "nn", ".", "ModuleList", "(", "g_reason_unshared_weights", ")", "\n", "", "elif", "self", ".", "use_ce", "in", "{", "\"pairwise-star-tensor\"", "}", ":", "\n", "                ", "reduce_dim", "=", "self", ".", "reduce_dim", "\n", "self", ".", "dim_reduce", "=", "nn", ".", "Linear", "(", "same_dim", ",", "reduce_dim", ")", "\n", "self", ".", "g_reason_1", "=", "nn", ".", "Linear", "(", "self", ".", "reduce_dim", "*", "reduce_dim", ",", "same_dim", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "f\"unrecognised CE config: {self.use_ce}\"", ")", "\n", "\n", "", "g_reason_shared", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "self", ".", "num_g_layers", "-", "1", ")", ":", "\n", "                ", "if", "self", ".", "use_bn_reason", ":", "\n", "                    ", "g_reason_shared", ".", "append", "(", "nn", ".", "BatchNorm1d", "(", "same_dim", ")", ")", "\n", "", "g_reason_shared", ".", "append", "(", "self", ".", "non_lin", ")", "\n", "g_reason_shared", ".", "append", "(", "nn", ".", "Linear", "(", "same_dim", ",", "same_dim", ")", ")", "\n", "", "self", ".", "g_reason_shared", "=", "nn", ".", "Sequential", "(", "*", "g_reason_shared", ")", "\n", "\n", "h_reason", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "self", ".", "num_h_layers", ")", ":", "\n", "                ", "if", "self", ".", "use_bn_reason", ":", "\n", "                    ", "h_reason", ".", "append", "(", "nn", ".", "BatchNorm1d", "(", "same_dim", ")", ")", "\n", "", "h_reason", ".", "append", "(", "self", ".", "non_lin", ")", "\n", "h_reason", ".", "append", "(", "nn", ".", "Linear", "(", "same_dim", ",", "same_dim", ")", ")", "\n", "", "self", ".", "h_reason", "=", "nn", ".", "Sequential", "(", "*", "h_reason", ")", "\n", "\n", "gated_vid_embds", "=", "[", "GatedEmbeddingUnitReasoning", "(", "same_dim", ")", "for", "_", "in", "in_dims", "]", "\n", "text_out_dims", "=", "[", "same_dim", "for", "_", "in", "agg_dims", "]", "\n", "", "elif", "self", ".", "mimic_ce_dims", ":", "# ablation study", "\n", "            ", "gated_vid_embds", "=", "[", "MimicCEGatedEmbeddingUnit", "(", "same_dim", ",", "same_dim", ",", "use_bn", "=", "True", ")", "\n", "for", "_", "in", "modalities", "]", "\n", "text_out_dims", "=", "[", "same_dim", "for", "_", "in", "agg_dims", "]", "\n", "", "elif", "self", ".", "concat_mix_experts", ":", "# ablation study", "\n", "# use a single large GEU to mix the experts - the output will be the sum", "\n", "# of the aggregation sizes", "\n", "            ", "in_dim", ",", "out_dim", "=", "sum", "(", "in_dims", ")", ",", "sum", "(", "agg_dims", ")", "\n", "gated_vid_embds", "=", "[", "GatedEmbeddingUnit", "(", "in_dim", ",", "out_dim", ",", "use_bn", "=", "True", ")", "]", "\n", "", "elif", "self", ".", "concat_experts", ":", "# ablation study", "\n", "# We do not use learnable parameters for the video combination, (we simply", "\n", "# use a high dimensional inner product).", "\n", "            ", "gated_vid_embds", "=", "[", "]", "\n", "", "else", ":", "\n", "            ", "gated_vid_embds", "=", "[", "GatedEmbeddingUnit", "(", "in_dim", ",", "dim", ",", "use_bn", ")", "for", "\n", "in_dim", ",", "dim", ",", "use_bn", "in", "zip", "(", "in_dims", ",", "agg_dims", ",", "use_bns", ")", "]", "\n", "text_out_dims", "=", "agg_dims", "\n", "", "self", ".", "video_GU", "=", "nn", ".", "ModuleList", "(", "gated_vid_embds", ")", "\n", "\n", "if", "\"retrieval\"", "in", "self", ".", "task", ":", "\n", "            ", "if", "self", ".", "concat_experts", ":", "\n", "                ", "gated_text_embds", "=", "[", "nn", ".", "Sequential", "(", ")", "]", "\n", "", "elif", "self", ".", "concat_mix_experts", ":", "\n", "# As with the video inputs, we similiarly use a single large GEU for the", "\n", "# text embedding", "\n", "                ", "gated_text_embds", "=", "[", "GatedEmbeddingUnit", "(", "text_dim", ",", "sum", "(", "agg_dims", ")", ",", "\n", "use_bn", "=", "True", ")", "]", "\n", "", "else", ":", "\n", "                ", "gated_text_embds", "=", "[", "GatedEmbeddingUnit", "(", "text_dim", ",", "dim", ",", "use_bn", "=", "True", ")", "for", "\n", "dim", "in", "text_out_dims", "]", "\n", "", "self", ".", "text_GU", "=", "nn", ".", "ModuleList", "(", "gated_text_embds", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"V. simple classifier, should update....\"", ")", "\n", "total_dim", "=", "0", "\n", "for", "mod", "in", "self", ".", "expert_dims", ".", "keys", "(", ")", ":", "\n", "                ", "total_dim", "+=", "self", ".", "expert_dims", "[", "mod", "]", "[", "1", "]", "*", "self", ".", "repeat_temporal", "[", "mod", "]", "\n", "", "print", "(", "f\"Total dim is {total_dim}\"", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "total_dim", ",", "self", ".", "num_classes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.CEModule.compute_moe_weights": [[671, 710], ["len", "text.view.view.view", "model.CEModule.moe_weights.repeat", "model.CEModule.moe_fc", "torch.softmax", "torch.softmax", "torch.softmax", "moe_weights.cuda.cuda.view", "print", "enumerate", "moe_weights.cuda.cuda.cuda", "msg.format.format.format", "print", "moe_weights[].mean().item", "moe_weights[].std().item", "moe_weights[].min().item", "moe_weights[].max().item", "moe_weights[].mean", "moe_weights[].std", "moe_weights[].min", "moe_weights[].max"], "methods", ["None"], ["", "", "def", "compute_moe_weights", "(", "self", ",", "text", ",", "ind", ")", ":", "\n", "# compute weights for all captions (including when assigned K captions to", "\n", "# the same video)", "\n", "        ", "B", ",", "K", ",", "D", "=", "text", ".", "shape", "\n", "M", "=", "len", "(", "self", ".", "modalities", ")", "\n", "msg", "=", "f\"expected between 1 and 10 modalities, found {M} ({self.modalities})\"", "\n", "assert", "1", "<=", "M", "<=", "10", ",", "msg", "\n", "\n", "# Treat each caption independently in the softmax (which runs over modalities)", "\n", "text", "=", "text", ".", "view", "(", "B", "*", "K", ",", "D", ")", "\n", "if", "self", ".", "freeze_weights", ":", "\n", "            ", "moe_weights", "=", "self", ".", "moe_weights", ".", "repeat", "(", "B", ",", "K", ",", "1", ")", "\n", "if", "text", ".", "is_cuda", ":", "\n", "                ", "moe_weights", "=", "moe_weights", ".", "cuda", "(", ")", "\n", "", "", "else", ":", "\n", "# if False:", "\n", "#     print(\"USING BIGGER WEIGHT PREDS\")", "\n", "#     moe_weights = self.moe_fc_bottleneck1(text)", "\n", "#     moe_weights = self.moe_cg(moe_weights)", "\n", "#     moe_weights = self.moe_fc_proj(moe_weights)", "\n", "#     moe_weights = moe_weights * 1", "\n", "# else:", "\n", "            ", "moe_weights", "=", "self", ".", "moe_fc", "(", "text", ")", "# BK x D -> BK x M", "\n", "moe_weights", "=", "F", ".", "softmax", "(", "moe_weights", ",", "dim", "=", "1", ")", "\n", "moe_weights", "=", "moe_weights", ".", "view", "(", "B", ",", "K", ",", "M", ")", "\n", "\n", "", "if", "self", ".", "verbose", ":", "\n", "            ", "print", "(", "\"--------------------------------\"", ")", "\n", "for", "idx", ",", "key", "in", "enumerate", "(", "self", ".", "modalities", ")", ":", "\n", "                ", "msg", "=", "\"{}: mean: {:.3f}, std: {:.3f}, min: {:.3f}, max: {:.3f}\"", "\n", "msg", "=", "msg", ".", "format", "(", "\n", "key", ",", "\n", "moe_weights", "[", ":", ",", ":", ",", "idx", "]", ".", "mean", "(", ")", ".", "item", "(", ")", ",", "\n", "moe_weights", "[", ":", ",", ":", ",", "idx", "]", ".", "std", "(", ")", ".", "item", "(", ")", ",", "\n", "moe_weights", "[", ":", ",", ":", ",", "idx", "]", ".", "min", "(", ")", ".", "item", "(", ")", ",", "\n", "moe_weights", "[", ":", ",", ":", ",", "idx", "]", ".", "max", "(", ")", ".", "item", "(", ")", ",", "\n", ")", "\n", "print", "(", "msg", ")", "\n", "", "", "return", "moe_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.CEModule.forward": [[711, 906], ["zip", "hasattr", "text.view.view.size", "text.view.view.view", "zip", "text.view.view.view", "model.CEModule.compute_moe_weights", "layer", "zip", "enumerate", "torch.cat", "torch.cat", "torch.cat", "model.CEModule.classifier", "layer", "torch.rand_like.view", "experts[].norm", "experts[].div", "layer", "list", "list", "len", "experts.keys", "model.CEModule.h_reason", "l", "torch.cat", "torch.cat", "torch.cat", "text_embd_.view.view.view", "tuple", "torch.matmul", "torch.matmul", "torch.matmul", "torch.rand_like", "torch.rand_like", "torch.rand_like", "itertools.product", "itertools.permutations", "ind[].float().to().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "model.CEModule.g_reason_1", "model.CEModule.g_reason_shared", "tuple", "torch.cat", "torch.cat", "torch.cat", "text_embd_.view.view.view", "zip", "experts.values", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cat.t", "model.sharded_cross_view_inner_product", "ind[].float().to().unsqueeze", "ind[].float().to().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "model.CEModule.g_reason_shared", "experts.values", "tuple", "layer", "ind[].float().to", "ind[].float().to().unsqueeze", "model.CEModule.dim_reduce", "mod0_reduce.unsqueeze.unsqueeze.unsqueeze", "model.CEModule.dim_reduce", "mod1_reduce.unsqueeze.unsqueeze.unsqueeze", "torch.matmul().view", "torch.matmul().view", "torch.matmul().view", "model.CEModule.g_reason_1", "model.CEModule.g_reason_shared", "experts.values", "ind[].float().to", "ind[].float().to", "ind[].float().to().unsqueeze", "torch.div", "torch.div", "torch.div", "ValueError", "ind[].float", "ind[].float().to", "torch.matmul", "torch.matmul", "torch.matmul", "ind[].float", "ind[].float", "ind[].float().to", "avai_dict[].unsqueeze", "ind[].float", "torch.cat", "torch.cat", "torch.cat", "model.CEModule.g_reason_1", "model.CEModule.g_reason_shared", "avail.to", "ind[].float", "ind[].float", "ind[].float", "torch.cat", "torch.cat", "torch.cat", "model.CEModule.g_reason_1", "model.CEModule.g_reason_shared", "ind[].float", "ind[].float", "ind[].float"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.CEModule.compute_moe_weights", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.sharded_cross_view_inner_product"], ["", "def", "forward", "(", "self", ",", "text", ",", "experts", ",", "ind", ",", "raw_captions", ")", ":", "\n", "        ", "\"\"\"Compute joint embeddings and, if requested, a confusion matrix between\n        video and text representations in the minibatch.\n\n        Notation: B = batch size, M = number of modalities\n        \"\"\"", "\n", "if", "\"retrieval\"", "in", "self", ".", "task", ":", "\n", "# Pass text embeddings through gated units", "\n", "            ", "text_embd", "=", "{", "}", "\n", "\n", "# Unroll repeated captions into present minibatch", "\n", "B", ",", "captions_per_video", ",", "feat_dim", "=", "text", ".", "size", "(", ")", "\n", "text", "=", "text", ".", "view", "(", "B", "*", "captions_per_video", ",", "feat_dim", ")", "\n", "for", "modality", ",", "layer", "in", "zip", "(", "self", ".", "modalities", ",", "self", ".", "text_GU", ")", ":", "\n", "# NOTE: Due to the batch norm, the gated units are sensitive to passing", "\n", "# in a lot of zeroes, so we do the masking step after the forwards pass", "\n", "                ", "text_", "=", "layer", "(", "text", ")", "\n", "\n", "# We always assume that text is available for retrieval", "\n", "text_", "=", "text_", ".", "view", "(", "B", ",", "captions_per_video", ",", "-", "1", ")", "\n", "\n", "if", "\"text\"", "in", "self", ".", "random_feats", ":", "\n", "                    ", "text_", "=", "th", ".", "rand_like", "(", "text_", ")", "\n", "", "text_embd", "[", "modality", "]", "=", "text_", "\n", "", "text", "=", "text", ".", "view", "(", "B", ",", "captions_per_video", ",", "-", "1", ")", "\n", "\n", "# vladded nans are handled earlier (during pooling)", "\n", "# We also avoid zeroing random features, since this will leak information", "\n", "# exclude = list(self.vlad_feat_sizes.keys()) + list(self.random_feats)", "\n", "# experts = self.mask_missing_embeddings(experts, ind, exclude=exclude)", "\n", "\n", "# MOE weights computation + normalization - note that we use the first caption", "\n", "# sample to predict the weights", "\n", "moe_weights", "=", "self", ".", "compute_moe_weights", "(", "text", ",", "ind", "=", "ind", ")", "\n", "\n", "", "if", "self", ".", "l2renorm", ":", "\n", "            ", "for", "modality", "in", "self", ".", "modalities", ":", "\n", "                ", "norm", "=", "experts", "[", "modality", "]", ".", "norm", "(", "p", "=", "2", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "experts", "[", "modality", "]", "=", "experts", "[", "modality", "]", ".", "div", "(", "norm", ")", "\n", "\n", "", "", "for", "modality", ",", "layer", "in", "zip", "(", "self", ".", "modalities", ",", "self", ".", "trn_list", ")", ":", "\n", "            ", "experts", "[", "modality", "]", "=", "layer", "(", "experts", "[", "modality", "]", ")", "\n", "\n", "", "if", "hasattr", "(", "self", ",", "\"video_dim_reduce\"", ")", ":", "\n", "# Embed all features to a common dimension", "\n", "            ", "for", "modality", ",", "layer", "in", "zip", "(", "self", ".", "modalities", ",", "self", ".", "video_dim_reduce", ")", ":", "\n", "                ", "experts", "[", "modality", "]", "=", "layer", "(", "experts", "[", "modality", "]", ")", "\n", "\n", "", "", "if", "self", ".", "use_ce", ":", "\n", "            ", "dev", "=", "experts", "[", "self", ".", "modalities", "[", "0", "]", "]", ".", "device", "\n", "if", "self", ".", "include_self", ":", "\n", "                ", "all_combinations", "=", "list", "(", "itertools", ".", "product", "(", "experts", ",", "repeat", "=", "2", ")", ")", "\n", "", "else", ":", "\n", "                ", "all_combinations", "=", "list", "(", "itertools", ".", "permutations", "(", "experts", ",", "2", ")", ")", "\n", "", "assert", "len", "(", "self", ".", "modalities", ")", ">", "1", ",", "\"use_ce requires multiple modalities\"", "\n", "\n", "if", "self", ".", "use_ce", "in", "{", "\"pairwise-star\"", ",", "\"pairwise-star-specific\"", ",", "\n", "\"pairwise-star-tensor\"", "}", ":", "\n", "                ", "sum_all", "=", "0", "\n", "sum_ind", "=", "0", "\n", "for", "mod0", "in", "experts", ".", "keys", "(", ")", ":", "\n", "                    ", "sum_all", "+=", "(", "experts", "[", "mod0", "]", "*", "ind", "[", "mod0", "]", ".", "float", "(", ")", ".", "to", "(", "dev", ")", ".", "unsqueeze", "(", "1", ")", ")", "\n", "sum_ind", "+=", "ind", "[", "mod0", "]", ".", "float", "(", ")", ".", "to", "(", "dev", ")", ".", "unsqueeze", "(", "1", ")", "\n", "", "avg_modality", "=", "sum_all", "/", "sum_ind", "\n", "\n", "", "for", "ii", ",", "l", "in", "enumerate", "(", "self", ".", "video_GU", ")", ":", "\n", "\n", "                ", "mask_num", "=", "0", "\n", "curr_mask", "=", "0", "\n", "temp_dict", "=", "{", "}", "\n", "avai_dict", "=", "{", "}", "\n", "curr_modality", "=", "self", ".", "modalities", "[", "ii", "]", "\n", "\n", "if", "self", ".", "use_ce", "==", "\"pairwise-star\"", ":", "\n", "                    ", "fused", "=", "th", ".", "cat", "(", "(", "experts", "[", "curr_modality", "]", ",", "avg_modality", ")", ",", "1", ")", "# -> B x 2D", "\n", "temp", "=", "self", ".", "g_reason_1", "(", "fused", ")", "# B x 2D -> B x D", "\n", "temp", "=", "self", ".", "g_reason_shared", "(", "temp", ")", "# B x D -> B x D", "\n", "curr_mask", "=", "temp", "*", "ind", "[", "curr_modality", "]", ".", "float", "(", ")", ".", "to", "(", "dev", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "", "elif", "self", ".", "use_ce", "==", "\"pairwise-star-specific\"", ":", "\n", "                    ", "fused", "=", "th", ".", "cat", "(", "(", "experts", "[", "curr_modality", "]", ",", "avg_modality", ")", ",", "1", ")", "# -> B x 2D", "\n", "temp", "=", "self", ".", "g_reason_unshared_weights", "[", "ii", "]", "(", "fused", ")", "\n", "temp", "=", "self", ".", "g_reason_shared", "(", "temp", ")", "# B x D -> B x D", "\n", "curr_mask", "=", "temp", "*", "ind", "[", "curr_modality", "]", ".", "float", "(", ")", ".", "to", "(", "dev", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "", "elif", "self", ".", "use_ce", "==", "\"pairwise-star-tensor\"", ":", "\n", "                    ", "mod0_reduce", "=", "self", ".", "dim_reduce", "(", "experts", "[", "curr_modality", "]", ")", "\n", "mod0_reduce", "=", "mod0_reduce", ".", "unsqueeze", "(", "2", ")", "# B x reduced_dim x1", "\n", "mod1_reduce", "=", "self", ".", "dim_reduce", "(", "avg_modality", ")", "\n", "mod1_reduce", "=", "mod1_reduce", ".", "unsqueeze", "(", "1", ")", "# B x1 x reduced_dim", "\n", "flat_dim", "=", "self", ".", "reduce_dim", "*", "self", ".", "reduce_dim", "\n", "fused", "=", "th", ".", "matmul", "(", "mod0_reduce", ",", "mod1_reduce", ")", ".", "view", "(", "-", "1", ",", "flat_dim", ")", "\n", "temp", "=", "self", ".", "g_reason_1", "(", "fused", ")", "# B x 2D -> B x D", "\n", "temp", "=", "self", ".", "g_reason_shared", "(", "temp", ")", "# B x D -> B x D", "\n", "curr_mask", "=", "temp", "*", "ind", "[", "curr_modality", "]", ".", "float", "(", ")", ".", "to", "(", "dev", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "", "elif", "self", ".", "use_ce", "in", "{", "\"pairwise\"", ",", "\"triplet\"", "}", ":", "\n", "                    ", "for", "modality_pair", "in", "all_combinations", ":", "\n", "                        ", "mod0", ",", "mod1", "=", "modality_pair", "\n", "if", "self", ".", "use_ce", "==", "\"pairwise\"", ":", "\n", "                            ", "if", "mod0", "==", "curr_modality", ":", "\n", "                                ", "new_key", "=", "f\"{mod0}_{mod1}\"", "\n", "fused", "=", "th", ".", "cat", "(", "(", "experts", "[", "mod0", "]", ",", "experts", "[", "mod1", "]", ")", ",", "1", ")", "\n", "temp", "=", "self", ".", "g_reason_1", "(", "fused", ")", "# B x 2D -> B x D", "\n", "temp", "=", "self", ".", "g_reason_shared", "(", "temp", ")", "\n", "temp_dict", "[", "new_key", "]", "=", "temp", "\n", "avail", "=", "(", "ind", "[", "mod0", "]", ".", "float", "(", ")", "*", "ind", "[", "mod1", "]", ".", "float", "(", ")", ")", "\n", "avai_dict", "[", "new_key", "]", "=", "avail", ".", "to", "(", "dev", ")", "\n", "", "", "elif", "self", ".", "use_ce", "==", "\"triplet\"", ":", "\n", "                            ", "if", "(", "curr_modality", "not", "in", "{", "mod0", ",", "mod1", "}", ")", "or", "self", ".", "include_self", ":", "\n", "                                ", "new_key", "=", "f\"{curr_modality}_{mod0}_{mod1}\"", "\n", "fused", "=", "th", ".", "cat", "(", "(", "experts", "[", "curr_modality", "]", ",", "experts", "[", "mod0", "]", ",", "\n", "experts", "[", "mod1", "]", ")", ",", "1", ")", "# -> B x 2D", "\n", "temp", "=", "self", ".", "g_reason_1", "(", "fused", ")", "# B x 2D -> B x D", "\n", "temp", "=", "self", ".", "g_reason_shared", "(", "temp", ")", "\n", "temp_dict", "[", "new_key", "]", "=", "temp", "\n", "avail", "=", "(", "ind", "[", "curr_modality", "]", ".", "float", "(", ")", "*", "ind", "[", "mod0", "]", ".", "float", "(", ")", "*", "\n", "ind", "[", "mod1", "]", ".", "float", "(", ")", ")", ".", "to", "(", "dev", ")", "\n", "avai_dict", "[", "new_key", "]", "=", "avail", "\n", "\n", "# Combine the paired features into a mask through elementwise sum", "\n", "", "", "", "for", "mm", "in", "temp_dict", ":", "\n", "                        ", "curr_mask", "+=", "temp_dict", "[", "mm", "]", "*", "avai_dict", "[", "mm", "]", ".", "unsqueeze", "(", "1", ")", "\n", "mask_num", "+=", "avai_dict", "[", "mm", "]", "\n", "", "curr_mask", "=", "th", ".", "div", "(", "curr_mask", ",", "(", "mask_num", "+", "0.00000000001", ")", ".", "unsqueeze", "(", "1", ")", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "f\"Unknown CE mechanism: {self.use_ce}\"", ")", "\n", "", "curr_mask", "=", "self", ".", "h_reason", "(", "curr_mask", ")", "\n", "experts", "[", "curr_modality", "]", "=", "l", "(", "experts", "[", "curr_modality", "]", ",", "curr_mask", ")", "\n", "\n", "", "", "elif", "self", ".", "concat_mix_experts", ":", "\n", "            ", "concatenated", "=", "th", ".", "cat", "(", "tuple", "(", "experts", ".", "values", "(", ")", ")", ",", "dim", "=", "1", ")", "\n", "vid_embd_", "=", "self", ".", "video_GU", "[", "0", "]", "(", "concatenated", ")", "\n", "text_embd_", "=", "text_embd", "[", "self", ".", "modalities", "[", "0", "]", "]", "\n", "text_embd_", "=", "text_embd_", ".", "view", "(", "-", "1", ",", "text_embd_", ".", "shape", "[", "-", "1", "]", ")", "\n", "\n", "", "elif", "self", ".", "concat_experts", ":", "\n", "            ", "vid_embd_", "=", "th", ".", "cat", "(", "tuple", "(", "experts", ".", "values", "(", ")", ")", ",", "dim", "=", "1", ")", "\n", "text_embd_", "=", "text_embd", "[", "self", ".", "modalities", "[", "0", "]", "]", "\n", "text_embd_", "=", "text_embd_", ".", "view", "(", "-", "1", ",", "text_embd_", ".", "shape", "[", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "for", "modality", ",", "layer", "in", "zip", "(", "self", ".", "modalities", ",", "self", ".", "video_GU", ")", ":", "\n", "                ", "experts", "[", "modality", "]", "=", "layer", "(", "experts", "[", "modality", "]", ")", "\n", "\n", "", "", "if", "self", ".", "training", ":", "\n", "            ", "merge_caption_similiarities", "=", "\"avg\"", "\n", "", "else", ":", "\n", "            ", "merge_caption_similiarities", "=", "self", ".", "test_caption_mode", "\n", "\n", "", "if", "self", ".", "task", "==", "\"classification\"", ":", "\n", "# for modality, layer in zip(self.modalities, self.video_dim_reduce_later):", "\n", "#     attempt to perform affordable classifier, might be removed later", "\n", "#     experts[modality] = layer(experts[modality])", "\n", "            ", "concatenated", "=", "th", ".", "cat", "(", "tuple", "(", "experts", ".", "values", "(", ")", ")", ",", "dim", "=", "1", ")", "\n", "preds", "=", "self", ".", "classifier", "(", "concatenated", ")", "\n", "return", "{", "\"modalities\"", ":", "self", ".", "modalities", ",", "\"class_preds\"", ":", "preds", "}", "\n", "", "elif", "self", ".", "concat_experts", "or", "self", ".", "concat_mix_experts", ":", "\n", "# zero pad to accommodate mismatch in sizes (after first setting the number", "\n", "# of VLAD clusters for the text to get the two vectors as close as possible", "\n", "# in size)", "\n", "            ", "if", "text_embd_", ".", "shape", "[", "1", "]", ">", "vid_embd_", ".", "shape", "[", "1", "]", ":", "\n", "                ", "sz", "=", "(", "vid_embd_", ".", "shape", "[", "0", "]", ",", "text_embd_", ".", "shape", "[", "1", "]", ")", "\n", "dtype", ",", "device", "=", "text_embd_", ".", "dtype", ",", "text_embd_", ".", "device", "\n", "vid_embd_padded", "=", "th", ".", "zeros", "(", "size", "=", "sz", ",", "dtype", "=", "dtype", ",", "device", "=", "device", ")", "\n", "# try:", "\n", "#     vid_embd_padded[:, :vid_embd_.shape[1]] = vid_embd_", "\n", "# except:", "\n", "#     import ipdb; ipdb.set_trace()", "\n", "vid_embd_", "=", "vid_embd_padded", "\n", "", "else", ":", "\n", "                ", "sz", "=", "(", "text_embd_", ".", "shape", "[", "0", "]", ",", "vid_embd_", ".", "shape", "[", "1", "]", ")", "\n", "dtype", ",", "device", "=", "text_embd_", ".", "dtype", ",", "text_embd_", ".", "device", "\n", "text_embd_padded", "=", "th", ".", "zeros", "(", "size", "=", "sz", ",", "dtype", "=", "dtype", ",", "device", "=", "device", ")", "\n", "text_embd_padded", "[", ":", ",", ":", "text_embd_", ".", "shape", "[", "1", "]", "]", "=", "text_embd_", "\n", "text_embd_", "=", "text_embd_padded", "\n", "", "cross_view_conf_matrix", "=", "th", ".", "matmul", "(", "text_embd_", ",", "vid_embd_", ".", "t", "(", ")", ")", "\n", "", "elif", "self", ".", "task", "==", "\"compute_video_embeddings\"", ":", "\n", "            ", "return", "{", "\"modalities\"", ":", "self", ".", "modalities", ",", "\"embeddings\"", ":", "experts", "}", "\n", "", "else", ":", "\n", "            ", "cross_view_conf_matrix", "=", "sharded_cross_view_inner_product", "(", "\n", "ind", "=", "ind", ",", "\n", "vid_embds", "=", "experts", ",", "\n", "text_embds", "=", "text_embd", ",", "\n", "keep_missing_modalities", "=", "self", ".", "keep_missing_modalities", ",", "\n", "l2renorm", "=", "self", ".", "l2renorm", ",", "\n", "text_weights", "=", "moe_weights", ",", "\n", "subspaces", "=", "self", ".", "modalities", ",", "\n", "raw_captions", "=", "raw_captions", ",", "\n", "merge_caption_similiarities", "=", "merge_caption_similiarities", ",", "\n", ")", "\n", "", "return", "{", "\n", "\"modalities\"", ":", "self", ".", "modalities", ",", "\n", "\"cross_view_conf_matrix\"", ":", "cross_view_conf_matrix", ",", "\n", "\"text_embds\"", ":", "text_embd", ",", "\n", "\"vid_embds\"", ":", "experts", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.GatedEmbeddingUnit.__init__": [[910, 915], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "model.ContextGating"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_dimension", ",", "output_dimension", ",", "use_bn", ")", ":", "\n", "        ", "super", "(", "GatedEmbeddingUnit", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "input_dimension", ",", "output_dimension", ")", "\n", "self", ".", "cg", "=", "ContextGating", "(", "output_dimension", ",", "add_batch_norm", "=", "use_bn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.GatedEmbeddingUnit.forward": [[916, 921], ["model.GatedEmbeddingUnit.fc", "model.GatedEmbeddingUnit.cg", "torch.normalize", "torch.normalize", "torch.normalize"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "fc", "(", "x", ")", "\n", "x", "=", "self", ".", "cg", "(", "x", ")", "\n", "x", "=", "F", ".", "normalize", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.MimicCEGatedEmbeddingUnit.__init__": [[924, 927], ["torch.Module.__init__", "model.ContextGating"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_dimension", ",", "output_dimension", ",", "use_bn", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "cg", "=", "ContextGating", "(", "input_dimension", ",", "add_batch_norm", "=", "use_bn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.MimicCEGatedEmbeddingUnit.forward": [[928, 932], ["model.MimicCEGatedEmbeddingUnit.cg", "torch.normalize", "torch.normalize", "torch.normalize"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "cg", "(", "x", ")", "\n", "x", "=", "F", ".", "normalize", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.ReduceDim.__init__": [[935, 938], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_dimension", ",", "output_dimension", ")", ":", "\n", "        ", "super", "(", "ReduceDim", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "input_dimension", ",", "output_dimension", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.ReduceDim.forward": [[942, 947], ["model.ReduceDim.fc", "torch.normalize", "torch.normalize", "torch.normalize"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "fc", "(", "x", ")", "\n", "#         x = self.fc2(F.relu(x))", "\n", "x", "=", "F", ".", "normalize", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.ContextGating.__init__": [[950, 955], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.BatchNorm1d"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dimension", ",", "add_batch_norm", "=", "True", ")", ":", "\n", "        ", "super", "(", "ContextGating", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "dimension", ",", "dimension", ")", "\n", "self", ".", "add_batch_norm", "=", "add_batch_norm", "\n", "self", ".", "batch_norm", "=", "nn", ".", "BatchNorm1d", "(", "dimension", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.ContextGating.forward": [[956, 962], ["model.ContextGating.fc", "torch.cat", "torch.cat", "torch.cat", "torch.glu", "torch.glu", "torch.glu", "model.ContextGating.batch_norm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x1", "=", "self", ".", "fc", "(", "x", ")", "\n", "if", "self", ".", "add_batch_norm", ":", "\n", "            ", "x1", "=", "self", ".", "batch_norm", "(", "x1", ")", "\n", "", "x", "=", "th", ".", "cat", "(", "(", "x", ",", "x1", ")", ",", "1", ")", "\n", "return", "F", ".", "glu", "(", "x", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.GatedEmbeddingUnitReasoning.__init__": [[965, 968], ["torch.Module.__init__", "model.ContextGatingReasoning"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "output_dimension", ")", ":", "\n", "        ", "super", "(", "GatedEmbeddingUnitReasoning", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "cg", "=", "ContextGatingReasoning", "(", "output_dimension", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.GatedEmbeddingUnitReasoning.forward": [[969, 973], ["model.GatedEmbeddingUnitReasoning.cg", "torch.normalize", "torch.normalize", "torch.normalize"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "mask", ")", ":", "\n", "        ", "x", "=", "self", ".", "cg", "(", "x", ",", "mask", ")", "\n", "x", "=", "F", ".", "normalize", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.SpatialMLP.__init__": [[976, 980], ["torch.Module.__init__", "model.ContextGating", "model.ContextGating"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dimension", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "cg1", "=", "ContextGating", "(", "dimension", ")", "\n", "self", ".", "cg2", "=", "ContextGating", "(", "dimension", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.SpatialMLP.forward": [[981, 984], ["model.SpatialMLP.cg1", "model.SpatialMLP.cg2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "cg1", "(", "x", ")", "\n", "return", "self", ".", "cg2", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.ContextGatingReasoning.__init__": [[987, 993], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.BatchNorm1d"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dimension", ",", "add_batch_norm", "=", "True", ")", ":", "\n", "        ", "super", "(", "ContextGatingReasoning", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "dimension", ",", "dimension", ")", "\n", "self", ".", "add_batch_norm", "=", "add_batch_norm", "\n", "self", ".", "batch_norm", "=", "nn", ".", "BatchNorm1d", "(", "dimension", ")", "\n", "self", ".", "batch_norm2", "=", "nn", ".", "BatchNorm1d", "(", "dimension", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.ContextGatingReasoning.forward": [[994, 1016], ["model.ContextGatingReasoning.fc", "torch.cat", "torch.cat", "torch.cat", "torch.glu", "torch.glu", "torch.glu", "model.ContextGatingReasoning.batch_norm", "model.ContextGatingReasoning.batch_norm2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "x1", ")", ":", "\n", "\n", "        ", "x2", "=", "self", ".", "fc", "(", "x", ")", "\n", "\n", "# t = x1 + x2", "\n", "if", "self", ".", "add_batch_norm", ":", "\n", "            ", "x1", "=", "self", ".", "batch_norm", "(", "x1", ")", "\n", "x2", "=", "self", ".", "batch_norm2", "(", "x2", ")", "\n", "# t = self.batch_norm (t)", "\n", "\n", "# t = (F.sigmoid(x1) + F.sigmoid (x2))/2", "\n", "\n", "", "t", "=", "x1", "+", "x2", "\n", "\n", "# t = (t > 0.2).float() * 1", "\n", "# t = th.trunc(2*F.sigmoid (t)-0.5)", "\n", "# print (t)", "\n", "# return x*F.sigmoid(t)", "\n", "\n", "# return t  (curr no sigmoid hoho!)", "\n", "x", "=", "th", ".", "cat", "(", "(", "x", ",", "t", ")", ",", "1", ")", "\n", "return", "F", ".", "glu", "(", "x", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.G_reason.__init__": [[1019, 1024], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "same_dim", ",", "num_inputs", ",", "non_lin", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "g_reason_1_specific", "=", "nn", ".", "Linear", "(", "same_dim", "*", "num_inputs", ",", "same_dim", ")", "\n", "self", ".", "g_reason_2_specific", "=", "nn", ".", "Linear", "(", "same_dim", ",", "same_dim", ")", "\n", "self", ".", "non_lin", "=", "non_lin", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.G_reason.forward": [[1025, 1030], ["model.G_reason.g_reason_1_specific", "model.G_reason.non_lin", "model.G_reason.g_reason_2_specific"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "g_reason_1_specific", "(", "x", ")", "# B x 2D -> B x D", "\n", "x", "=", "self", ".", "non_lin", "(", "x", ")", "\n", "x", "=", "self", ".", "g_reason_2_specific", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.kronecker_prod": [[48, 52], ["torch.bmm", "th.bmm.view", "t1.view", "t2.contiguous().view", "t1.size", "t2.size", "t2.contiguous"], "function", ["None"], ["", "", "def", "kronecker_prod", "(", "t1", ",", "t2", ")", ":", "\n", "# kronecker is performed along the last dim", "\n", "    ", "kron", "=", "th", ".", "bmm", "(", "t1", ".", "view", "(", "-", "1", ",", "t1", ".", "size", "(", "-", "1", ")", ",", "1", ")", ",", "t2", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "1", ",", "t2", ".", "size", "(", "-", "1", ")", ")", ")", "\n", "return", "kron", ".", "view", "(", "t1", ".", "shape", "[", "0", "]", ",", "t1", ".", "shape", "[", "1", "]", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.drop_nans": [[54, 84], ["torch.nonzero().flatten", "th.nonzero().flatten.numel", "torch.isnan().sum", "ipdb.set_trace", "torch.nonzero", "torch.isnan", "ipdb.set_trace", "vals.view", "torch.isnan", "vals.view"], "function", ["None"], ["", "def", "drop_nans", "(", "x", ",", "ind", ",", "validate_missing", ")", ":", "\n", "    ", "\"\"\"Remove nans, which we expect to find at missing indices.\n\n    Args:\n        x (th.Tensor): features\n        ind (th.Tensor): binary values denoting whether or not a given feature is\n            present\n        validate_missing (bool): whether to validate that the missing location contains\n            a nan.\n\n    Returns:\n        (th.tensor): the features, with the missing values masked to zero.\n    \"\"\"", "\n", "missing", "=", "th", ".", "nonzero", "(", "ind", "==", "0", ")", ".", "flatten", "(", ")", "\n", "if", "missing", ".", "numel", "(", ")", ":", "\n", "        ", "if", "validate_missing", ":", "\n", "            ", "vals", "=", "x", "[", "missing", "[", "0", "]", "]", "\n", "if", "not", "th", ".", "isnan", "(", "vals", ".", "view", "(", "-", "1", ")", "[", "0", "]", ")", ":", "\n", "                ", "ipdb", ".", "set_trace", "(", ")", "\n", "", "assert", "vals", ".", "view", "(", "-", "1", ")", "[", "0", "]", ",", "\"expected nans at missing locations\"", "\n", "# Prevent overwrite of the original tensor", "\n", "# x_ = x.clone()", "\n", "# TODO(samuel): This doesn't do anything, so can remove it", "\n", "", "x_", "=", "x", "\n", "x_", "[", "missing", "]", "=", "0", "\n", "x", "=", "x_", "\n", "", "if", "th", ".", "isnan", "(", "x", ")", ".", "sum", "(", ")", ">", "0", ":", "\n", "# slow, but might help us solve the mystery", "\n", "        ", "ipdb", ".", "set_trace", "(", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.sharded_cross_view_inner_product": [[1032, 1152], ["vid_embds[].size", "text_embds[].size", "torch.zeros", "text_weights.view.view", "enumerate", "torch.isnan().sum().item", "zip", "print", "torch.ones", "torch.ones", "enumerate", "available.to.to", "set().issubset", "th.div.sum().view", "torch.div", "NotImplementedError", "enumerate", "torch.sqrt().unsqueeze", "torch.sqrt().unsqueeze", "text_embds[].view", "ipdb.set_trace", "ValueError", "round", "round", "len", "len", "set", "text_weights.view.view", "vid_embd_.reshape().pow().sum", "text_embd_.reshape.reshape", "text_embd_.reshape.pow().sum", "vid_embds[].reshape", "torch.matmul", "sims.view.max", "sims.view.min", "torch.isnan().sum", "sims.view.view", "torch.mean", "sims.view.view", "text_weights.view.mean().detach().cpu().numpy().tolist", "text_weights.view.std().detach().cpu().numpy().tolist", "set", "len", "th.div.sum", "len", "len", "torch.sqrt", "torch.sqrt", "vid_embd_.t", "ValueError", "torch.unique().cpu().numpy", "vid_embd_.size", "vid_embd_.reshape().pow", "text_embd_.reshape.size", "text_embd_.reshape.pow", "th.sqrt().unsqueeze.clamp", "th.sqrt().unsqueeze.clamp", "torch.isnan", "msg.format", "text_weights.view.mean().detach().cpu().numpy", "text_weights.view.std().detach().cpu().numpy", "torch.unique().cpu", "vid_embd_.reshape", "text_weights.view.mean().detach().cpu", "text_weights.view.std().detach().cpu", "torch.unique", "text_weights.view.mean().detach", "text_weights.view.std().detach", "text_weights.view.mean", "text_weights.view.std"], "function", ["None"], ["", "", "def", "sharded_cross_view_inner_product", "(", "vid_embds", ",", "text_embds", ",", "text_weights", ",", "\n", "subspaces", ",", "l2renorm", ",", "ind", ",", "\n", "keep_missing_modalities", ",", "\n", "merge_caption_similiarities", "=", "\"avg\"", ",", "tol", "=", "1E-5", ",", "\n", "raw_captions", "=", "None", ")", ":", "\n", "    ", "\"\"\"Compute a similarity matrix from sharded vectors.\n\n    Args:\n        embds1 (dict[str:th.Tensor]): the set of sub-embeddings that, when\n            concatenated, form the whole. The ith shard has shape `B x K x F_i`\n            (i.e. they can differ in the last dimension).\n        embds2 (dict[str:th.Tensor]): same format.\n        weights2 (th.Tensor): weights for the shards in `embds2`.\n        l2norm (bool::True): whether to l2 renormalize the full embeddings.\n\n    Returns:\n        (th.tensor): similarity matrix of size `BK x BK`.\n\n    NOTE: If multiple captions are provided, we can aggregate their similarities to\n    provide a single video-text similarity score.\n    \"\"\"", "\n", "B", "=", "vid_embds", "[", "subspaces", "[", "0", "]", "]", ".", "size", "(", "0", ")", "\n", "T", ",", "num_caps", ",", "_", "=", "text_embds", "[", "subspaces", "[", "0", "]", "]", ".", "size", "(", ")", "\n", "device", "=", "vid_embds", "[", "subspaces", "[", "0", "]", "]", ".", "device", "\n", "\n", "# unroll separate captions onto first dimension and treat them separately", "\n", "sims", "=", "th", ".", "zeros", "(", "T", "*", "num_caps", ",", "B", ",", "device", "=", "device", ")", "\n", "text_weights", "=", "text_weights", ".", "view", "(", "T", "*", "num_caps", ",", "-", "1", ")", "\n", "if", "False", ":", "\n", "        ", "mus", "=", "[", "round", "(", "x", ",", "3", ")", "for", "x", "in", "text_weights", ".", "mean", "(", "0", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "]", "\n", "stds", "=", "[", "round", "(", "x", ",", "3", ")", "for", "x", "in", "text_weights", ".", "std", "(", "0", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "]", "\n", "summary", "=", "\">>>\"", "\n", "for", "mod", ",", "mu", ",", "std", "in", "zip", "(", "subspaces", ",", "mus", ",", "stds", ")", ":", "\n", "            ", "summary", "+=", "f\"{mod}: {mu} +/- {std} \"", "\n", "", "print", "(", "summary", ")", "\n", "\n", "", "if", "keep_missing_modalities", ":", "\n", "# assign every expert/text inner product the same weight, even if the expert", "\n", "# is missing", "\n", "        ", "text_weight_tensor", "=", "th", ".", "ones", "(", "T", "*", "num_caps", ",", "B", ",", "len", "(", "subspaces", ")", ",", "\n", "dtype", "=", "text_weights", ".", "dtype", ",", "\n", "device", "=", "text_weights", ".", "device", ")", "\n", "", "else", ":", "\n", "# mark expert availabilities along the second axis", "\n", "        ", "available", "=", "th", ".", "ones", "(", "1", ",", "B", ",", "len", "(", "subspaces", ")", ",", "dtype", "=", "text_weights", ".", "dtype", ")", "\n", "for", "ii", ",", "modality", "in", "enumerate", "(", "subspaces", ")", ":", "\n", "            ", "available", "[", ":", ",", ":", ",", "ii", "]", "=", "ind", "[", "modality", "]", "\n", "", "available", "=", "available", ".", "to", "(", "text_weights", ".", "device", ")", "\n", "msg", "=", "\"expected `available` modality mask to only contain 0s or 1s\"", "\n", "assert", "set", "(", "th", ".", "unique", "(", "available", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", ".", "issubset", "(", "set", "(", "[", "0", ",", "1", "]", ")", ")", ",", "msg", "\n", "# set the text weights along the first axis and combine with availabilities to", "\n", "# produce a <T x B x num_experts> tensor", "\n", "text_weight_tensor", "=", "text_weights", ".", "view", "(", "T", "*", "num_caps", ",", "1", ",", "len", "(", "subspaces", ")", ")", "*", "available", "\n", "# normalise to account for missing experts", "\n", "normalising_weights", "=", "text_weight_tensor", ".", "sum", "(", "2", ")", ".", "view", "(", "T", "*", "num_caps", ",", "B", ",", "1", ")", "\n", "text_weight_tensor", "=", "th", ".", "div", "(", "text_weight_tensor", ",", "normalising_weights", ")", "\n", "\n", "", "if", "l2renorm", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Do not use renorm until availability fix is complete\"", ")", "\n", "l2_mass_vid", ",", "l2_mass_text", "=", "0", ",", "0", "\n", "for", "idx", ",", "modality", "in", "enumerate", "(", "subspaces", ")", ":", "\n", "            ", "vid_embd_", "=", "vid_embds", "[", "modality", "]", "\n", "assert", "len", "(", "vid_embd_", ".", "size", "(", ")", ")", "==", "2", ",", "\"expected B x feat_dim format\"", "\n", "l2_mass_vid", "+=", "vid_embd_", ".", "reshape", "(", "B", ",", "-", "1", ")", ".", "pow", "(", "2", ")", ".", "sum", "(", "1", ")", "\n", "text_embd_", "=", "text_embds", "[", "modality", "]", "\n", "assert", "len", "(", "text_embd_", ".", "size", "(", ")", ")", "==", "3", ",", "\"expected B x caps x feat_dim format\"", "\n", "text_embd_", "=", "text_embd_", ".", "reshape", "(", "B", "*", "num_caps", ",", "-", "1", ")", "\n", "text_embd_", "=", "text_weights", "[", ":", ",", "idx", ":", "idx", "+", "1", "]", "*", "text_embd_", "\n", "l2_mass_text", "+=", "text_embd_", ".", "pow", "(", "2", ")", ".", "sum", "(", "1", ")", "\n", "", "l2_mass_vid", "=", "th", ".", "sqrt", "(", "l2_mass_vid", ".", "clamp", "(", "min", "=", "1E-6", ")", ")", ".", "unsqueeze", "(", "1", ")", "\n", "l2_mass_text", "=", "th", ".", "sqrt", "(", "l2_mass_text", ".", "clamp", "(", "min", "=", "1E-6", ")", ")", ".", "unsqueeze", "(", "1", ")", "\n", "", "else", ":", "\n", "        ", "l2_mass_text", ",", "l2_mass_vid", "=", "1", ",", "1", "\n", "\n", "", "for", "idx", ",", "modality", "in", "enumerate", "(", "subspaces", ")", ":", "\n", "        ", "vid_embd_", "=", "vid_embds", "[", "modality", "]", ".", "reshape", "(", "B", ",", "-", "1", ")", "/", "l2_mass_vid", "\n", "text_embd_", "=", "text_embds", "[", "modality", "]", ".", "view", "(", "T", "*", "num_caps", ",", "-", "1", ")", "\n", "msg", "=", "\"expected weights to be applied to text embeddings\"", "\n", "assert", "text_embd_", ".", "shape", "[", "0", "]", "==", "text_weights", ".", "shape", "[", "0", "]", ",", "msg", "\n", "text_embd_", "=", "text_embd_", "/", "l2_mass_text", "\n", "weighting", "=", "text_weight_tensor", "[", ":", ",", ":", ",", "idx", "]", "\n", "sims", "+=", "weighting", "*", "th", ".", "matmul", "(", "text_embd_", ",", "vid_embd_", ".", "t", "(", ")", ")", "# (T x num_caps) x (B)", "\n", "\n", "", "if", "l2renorm", ":", "\n", "# if not (sims.max() < 1 + tol):", "\n", "#     import ipdb; ipdb.set_trace()", "\n", "        ", "assert", "sims", ".", "max", "(", ")", "<", "1", "+", "tol", ",", "\"expected cosine similarities to be < 1\"", "\n", "assert", "sims", ".", "min", "(", ")", ">", "-", "1", "-", "tol", ",", "\"expected cosine similarities to be > -1\"", "\n", "\n", "", "if", "th", ".", "isnan", "(", "sims", ")", ".", "sum", "(", ")", ".", "item", "(", ")", ":", "\n", "        ", "import", "ipdb", ";", "ipdb", ".", "set_trace", "(", ")", "\n", "raise", "ValueError", "(", "\"Found nans in similarity matrix!\"", ")", "\n", "\n", "", "if", "num_caps", ">", "1", ":", "\n", "# aggregate similarities from different captions", "\n", "        ", "if", "merge_caption_similiarities", "==", "\"avg\"", ":", "\n", "            ", "sims", "=", "sims", ".", "view", "(", "B", ",", "num_caps", ",", "B", ")", "\n", "# vis = False", "\n", "# if vis:", "\n", "#     import sys", "\n", "#     from pathlib import Path", "\n", "#     import matplotlib", "\n", "#     matplotlib.use(\"Agg\")", "\n", "#     import matplotlib.pyplot as plt", "\n", "#     sys.path.insert(0, str(Path.home() / \"coding/src/zsvision/python\"))", "\n", "#     from zsvision.zs_iterm import zs_dispFig # NOQA", "\n", "#     sim_stds = th.std(sims, dim=1)", "\n", "#     sim_stds_ = sim_stds - sim_stds.min()", "\n", "#     sim_stds_ = sim_stds_ / sim_stds_.max()", "\n", "#     plt.matshow(sim_stds_)", "\n", "#     zs_dispFig()", "\n", "\n", "sims", "=", "th", ".", "mean", "(", "sims", ",", "dim", "=", "1", ")", "\n", "sims", "=", "sims", ".", "view", "(", "B", ",", "B", ")", "\n", "", "elif", "merge_caption_similiarities", "==", "\"indep\"", ":", "\n", "            ", "pass", "\n", "", "else", ":", "\n", "            ", "msg", "=", "\"unrecognised merge mode: {}\"", "\n", "raise", "ValueError", "(", "msg", ".", "format", "(", "merge_caption_similiarities", ")", ")", "\n", "", "", "return", "sims", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.model.sharded_single_view_inner_product": [[1154, 1205], ["list", "torch.zeros", "enumerate", "torch.isnan().sum().item", "embds.keys", "len", "enumerate", "torch.sqrt().unsqueeze", "torch.matmul", "ValueError", "len", "ValueError", "embds[].reshape", "embds[].reshape.pow().sum", "embds[].reshape.reshape", "embds[].reshape.t", "torch.isnan().sum", "torch.sqrt", "len", "embds[].reshape.pow", "th.sqrt().unsqueeze.clamp", "torch.isnan"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys"], ["", "def", "sharded_single_view_inner_product", "(", "embds", ",", "subspaces", ",", "text_weights", "=", "None", ",", "\n", "l2renorm", "=", "True", ")", ":", "\n", "    ", "\"\"\"Compute a similarity matrix from sharded vectors.\n\n    Args:\n        embds (dict[str:th.Tensor]): the set of sub-embeddings that, when concatenated,\n            form the whole. The ith shard has shape `B x K x F_i` (i.e. they can\n            differ in the last dimension), or shape `B x F_i`\n        l2norm (bool::True): whether to l2 normalize the full embedding.\n\n    Returns:\n        (th.tensor): similarity matrix of size `BK x BK`.\n    \"\"\"", "\n", "subspaces", "=", "list", "(", "embds", ".", "keys", "(", ")", ")", "\n", "device", "=", "embds", "[", "subspaces", "[", "0", "]", "]", ".", "device", "\n", "shape", "=", "embds", "[", "subspaces", "[", "0", "]", "]", ".", "shape", "\n", "if", "len", "(", "shape", ")", "==", "3", ":", "\n", "        ", "B", ",", "K", ",", "_", "=", "shape", "\n", "num_embds", "=", "B", "*", "K", "\n", "assert", "text_weights", "is", "not", "None", ",", "\"Expected 3-dim tensors for text (+ weights)\"", "\n", "assert", "text_weights", ".", "shape", "[", "0", "]", "==", "B", "\n", "assert", "text_weights", ".", "shape", "[", "1", "]", "==", "K", "\n", "", "elif", "len", "(", "shape", ")", "==", "2", ":", "\n", "        ", "B", ",", "_", "=", "shape", "\n", "num_embds", "=", "B", "\n", "assert", "text_weights", "is", "None", ",", "\"Expected 2-dim tensors for non-text (no weights)\"", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"input tensor with {} dims unrecognised\"", ".", "format", "(", "len", "(", "shape", ")", ")", ")", "\n", "", "sims", "=", "th", ".", "zeros", "(", "num_embds", ",", "num_embds", ",", "device", "=", "device", ")", "\n", "if", "l2renorm", ":", "\n", "        ", "l2_mass", "=", "0", "\n", "for", "idx", ",", "modality", "in", "enumerate", "(", "subspaces", ")", ":", "\n", "            ", "embd_", "=", "embds", "[", "modality", "]", "\n", "if", "text_weights", "is", "not", "None", ":", "\n", "# text_weights (i.e. moe_weights) are shared among subspace for video", "\n", "                ", "embd_", "=", "text_weights", "[", ":", ",", ":", ",", "idx", ":", "idx", "+", "1", "]", "*", "embd_", "\n", "", "embd_", "=", "embds", "[", "modality", "]", ".", "reshape", "(", "num_embds", ",", "-", "1", ")", "\n", "l2_mass", "+=", "embd_", ".", "pow", "(", "2", ")", ".", "sum", "(", "1", ")", "\n", "", "l2_mass", "=", "th", ".", "sqrt", "(", "l2_mass", ".", "clamp", "(", "min", "=", "1E-6", ")", ")", ".", "unsqueeze", "(", "1", ")", "\n", "", "else", ":", "\n", "        ", "l2_mass", "=", "1", "\n", "\n", "", "for", "idx", ",", "modality", "in", "enumerate", "(", "subspaces", ")", ":", "\n", "        ", "embd_", "=", "embds", "[", "modality", "]", "\n", "if", "text_weights", "is", "not", "None", ":", "\n", "            ", "embd_", "=", "text_weights", "[", ":", ",", ":", ",", "idx", ":", "idx", "+", "1", "]", "*", "embd_", "\n", "", "embd_", "=", "embd_", ".", "reshape", "(", "num_embds", ",", "-", "1", ")", "/", "l2_mass", "\n", "sims", "+=", "th", ".", "matmul", "(", "embd_", ",", "embd_", ".", "t", "(", ")", ")", "\n", "", "if", "th", ".", "isnan", "(", "sims", ")", ".", "sum", "(", ")", ".", "item", "(", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"Found nans in similarity matrix!\"", ")", "\n", "", "return", "sims", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.__init__": [[309, 313], ["metric.AverageMeter.reset"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.reset"], ["def", "__init__", "(", "self", ",", "name", ",", "fmt", "=", "':f'", ")", ":", "\n", "        ", "self", ".", "name", "=", "name", "\n", "self", ".", "fmt", "=", "fmt", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.reset": [[314, 319], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "val", "=", "0", "\n", "self", ".", "avg", "=", "0", "\n", "self", ".", "sum", "=", "0", "\n", "self", ".", "count", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update": [[320, 325], ["None"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "val", ",", "n", "=", "1", ")", ":", "\n", "        ", "self", ".", "val", "=", "val", "\n", "self", ".", "sum", "+=", "val", "*", "n", "\n", "self", ".", "count", "+=", "n", "\n", "self", ".", "avg", "=", "self", ".", "sum", "/", "self", ".", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.__str__": [[326, 329], ["fmtstr.format"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "fmtstr", "=", "'{name} {val'", "+", "self", ".", "fmt", "+", "'} ({avg'", "+", "self", ".", "fmt", "+", "'})'", "\n", "return", "fmtstr", ".", "format", "(", "**", "self", ".", "__dict__", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.Meter.reset": [[336, 339], ["None"], "methods", ["None"], ["def", "reset", "(", "self", ")", ":", "\n", "        ", "'''Resets the meter to default settings.'''", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.Meter.add": [[340, 346], ["None"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "value", ")", ":", "\n", "        ", "'''Log a new value to the meter\n        Args:\n            value: Next restult to include.\n        '''", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.Meter.value": [[347, 350], ["None"], "methods", ["None"], ["", "def", "value", "(", "self", ")", ":", "\n", "        ", "'''Get the value of the meter in the current state.'''", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.APMeter.__init__": [[366, 369], ["object.__init__", "metric.APMeter.reset"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.reset"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "APMeter", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.APMeter.reset": [[370, 375], ["torch.FloatTensor", "torch.LongTensor", "torch.FloatTensor", "torch.FloatStorage", "torch.LongStorage", "torch.FloatStorage"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"Resets the meter with empty member variables\"\"\"", "\n", "self", ".", "scores", "=", "torch", ".", "FloatTensor", "(", "torch", ".", "FloatStorage", "(", ")", ")", "\n", "self", ".", "targets", "=", "torch", ".", "LongTensor", "(", "torch", ".", "LongStorage", "(", ")", ")", "\n", "self", ".", "weights", "=", "torch", ".", "FloatTensor", "(", "torch", ".", "FloatStorage", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.APMeter.add": [[376, 441], ["torch.equal", "metric.APMeter.scores.resize_", "metric.APMeter.targets.resize_", "metric.APMeter.scores.narrow().copy_", "metric.APMeter.targets.narrow().copy_", "torch.is_tensor", "torch.from_numpy", "torch.is_tensor", "torch.from_numpy", "torch.from_numpy.squeeze", "output.view.view.dim", "output.view.view.view", "target.view.view.dim", "target.view.view.view", "metric.APMeter.scores.numel", "metric.APMeter.scores.storage().size", "math.ceil", "math.ceil", "metric.APMeter.scores.storage().resize_", "metric.APMeter.targets.storage().resize_", "metric.APMeter.scores.size", "output.view.view.size", "target.view.view.size", "metric.APMeter.weights.resize_", "metric.APMeter.weights.narrow().copy_", "torch.is_tensor", "torch.from_numpy", "output.view.view.dim", "target.view.view.dim", "torch.from_numpy.dim", "torch.from_numpy.numel", "target.view.view.size", "torch.min", "target.view.view.size", "metric.APMeter.targets.size", "metric.APMeter.scores.numel", "output.view.view.numel", "int", "int", "metric.APMeter.weights.storage().resize_", "metric.APMeter.scores.dim", "output.view.view.size", "target.view.view.size", "metric.APMeter.scores.narrow", "metric.APMeter.targets.narrow", "metric.APMeter.scores.storage", "metric.APMeter.scores.storage().size", "metric.APMeter.weights.storage().size", "metric.APMeter.scores.storage", "metric.APMeter.targets.storage", "int", "output.view.view.size", "target.view.view.size", "torch.from_numpy.size", "metric.APMeter.weights.narrow", "output.view.view.numel", "output.view.view.numel", "metric.APMeter.weights.storage", "torch.from_numpy.size", "metric.APMeter.scores.storage", "metric.APMeter.weights.storage", "output.view.view.size"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "output", ",", "target", ",", "weight", "=", "None", ")", ":", "\n", "        ", "\"\"\"Add a new observation\n        Args:\n            output (Tensor): NxK tensor that for each of the N examples\n                indicates the probability of the example belonging to each of\n                the K classes, according to the model. The probabilities should\n                sum to one over all classes\n            target (Tensor): binary NxK tensort that encodes which of the K\n                classes are associated with the N-th input\n                (eg: a row [0, 1, 0, 1] indicates that the example is\n                associated with classes 2 and 4)\n            weight (optional, Tensor): Nx1 tensor representing the weight for\n                each example (each weight > 0)\n        \"\"\"", "\n", "if", "not", "torch", ".", "is_tensor", "(", "output", ")", ":", "\n", "            ", "output", "=", "torch", ".", "from_numpy", "(", "output", ")", "\n", "", "if", "not", "torch", ".", "is_tensor", "(", "target", ")", ":", "\n", "            ", "target", "=", "torch", ".", "from_numpy", "(", "target", ")", "\n", "\n", "", "if", "weight", "is", "not", "None", ":", "\n", "            ", "if", "not", "torch", ".", "is_tensor", "(", "weight", ")", ":", "\n", "                ", "weight", "=", "torch", ".", "from_numpy", "(", "weight", ")", "\n", "", "weight", "=", "weight", ".", "squeeze", "(", ")", "\n", "", "if", "output", ".", "dim", "(", ")", "==", "1", ":", "\n", "            ", "output", "=", "output", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "", "else", ":", "\n", "            ", "assert", "output", ".", "dim", "(", ")", "==", "2", ",", "'wrong output size (should be 1D or 2D with one column \\\n                per class)'", "\n", "", "if", "target", ".", "dim", "(", ")", "==", "1", ":", "\n", "            ", "target", "=", "target", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "", "else", ":", "\n", "            ", "assert", "target", ".", "dim", "(", ")", "==", "2", ",", "'wrong target size (should be 1D or 2D with one column \\\n                per class)'", "\n", "", "if", "weight", "is", "not", "None", ":", "\n", "            ", "assert", "weight", ".", "dim", "(", ")", "==", "1", ",", "'Weight dimension should be 1'", "\n", "assert", "weight", ".", "numel", "(", ")", "==", "target", ".", "size", "(", "0", ")", ",", "'Weight dimension 1 should be the same as that of target'", "\n", "assert", "torch", ".", "min", "(", "weight", ")", ">=", "0", ",", "'Weight should be non-negative only'", "\n", "", "assert", "torch", ".", "equal", "(", "target", "**", "2", ",", "target", ")", ",", "'targets should be binary (0 or 1)'", "\n", "if", "self", ".", "scores", ".", "numel", "(", ")", ">", "0", ":", "\n", "            ", "assert", "target", ".", "size", "(", "1", ")", "==", "self", ".", "targets", ".", "size", "(", "1", ")", ",", "'dimensions for output should match previously added examples.'", "\n", "\n", "# make sure storage is of sufficient size", "\n", "", "if", "self", ".", "scores", ".", "storage", "(", ")", ".", "size", "(", ")", "<", "self", ".", "scores", ".", "numel", "(", ")", "+", "output", ".", "numel", "(", ")", ":", "\n", "            ", "new_size", "=", "math", ".", "ceil", "(", "self", ".", "scores", ".", "storage", "(", ")", ".", "size", "(", ")", "*", "1.5", ")", "\n", "new_weight_size", "=", "math", ".", "ceil", "(", "self", ".", "weights", ".", "storage", "(", ")", ".", "size", "(", ")", "*", "1.5", ")", "\n", "self", ".", "scores", ".", "storage", "(", ")", ".", "resize_", "(", "int", "(", "new_size", "+", "output", ".", "numel", "(", ")", ")", ")", "\n", "self", ".", "targets", ".", "storage", "(", ")", ".", "resize_", "(", "int", "(", "new_size", "+", "output", ".", "numel", "(", ")", ")", ")", "\n", "if", "weight", "is", "not", "None", ":", "\n", "                ", "self", ".", "weights", ".", "storage", "(", ")", ".", "resize_", "(", "int", "(", "new_weight_size", "+", "output", ".", "size", "(", "0", ")", ")", ")", "\n", "\n", "# store scores and targets", "\n", "", "", "offset", "=", "self", ".", "scores", ".", "size", "(", "0", ")", "if", "self", ".", "scores", ".", "dim", "(", ")", ">", "0", "else", "0", "\n", "self", ".", "scores", ".", "resize_", "(", "offset", "+", "output", ".", "size", "(", "0", ")", ",", "output", ".", "size", "(", "1", ")", ")", "\n", "self", ".", "targets", ".", "resize_", "(", "offset", "+", "target", ".", "size", "(", "0", ")", ",", "target", ".", "size", "(", "1", ")", ")", "\n", "self", ".", "scores", ".", "narrow", "(", "0", ",", "offset", ",", "output", ".", "size", "(", "0", ")", ")", ".", "copy_", "(", "output", ")", "\n", "self", ".", "targets", ".", "narrow", "(", "0", ",", "offset", ",", "target", ".", "size", "(", "0", ")", ")", ".", "copy_", "(", "target", ")", "\n", "\n", "if", "weight", "is", "not", "None", ":", "\n", "            ", "self", ".", "weights", ".", "resize_", "(", "offset", "+", "weight", ".", "size", "(", "0", ")", ")", "\n", "self", ".", "weights", ".", "narrow", "(", "0", ",", "offset", ",", "weight", ".", "size", "(", "0", ")", ")", ".", "copy_", "(", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.APMeter.value": [[442, 483], ["torch.zeros", "hasattr", "range", "metric.APMeter.scores.numel", "metric.APMeter.scores.size", "torch.arange().float", "torch.range().float", "metric.APMeter.weights.numel", "metric.APMeter.weights.new", "metric.APMeter.weights.new", "metric.APMeter.scores.size", "torch.sort", "truth.float().cumsum.div", "metric.APMeter.weights.size", "metric.APMeter.weights.size", "metric.APMeter.weights.numel", "metric.APMeter.cumsum", "metric.APMeter.weights.numel", "metric.APMeter.cumsum", "truth.float().cumsum", "precision[].sum", "max", "torch.arange", "torch.range", "truth.float", "float", "metric.APMeter.scores.size", "truth.float", "truth.sum", "metric.APMeter.scores.size", "truth.byte"], "methods", ["None"], ["", "", "def", "value", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the model's average precision for each class\n        Return:\n            ap (FloatTensor): 1xK tensor, with avg precision for each class k\n        \"\"\"", "\n", "\n", "if", "self", ".", "scores", ".", "numel", "(", ")", "==", "0", ":", "\n", "            ", "return", "0", "\n", "", "ap", "=", "torch", ".", "zeros", "(", "self", ".", "scores", ".", "size", "(", "1", ")", ")", "\n", "if", "hasattr", "(", "torch", ",", "\"arange\"", ")", ":", "\n", "            ", "rg", "=", "torch", ".", "arange", "(", "1", ",", "self", ".", "scores", ".", "size", "(", "0", ")", "+", "1", ")", ".", "float", "(", ")", "\n", "", "else", ":", "\n", "            ", "rg", "=", "torch", ".", "range", "(", "1", ",", "self", ".", "scores", ".", "size", "(", "0", ")", ")", ".", "float", "(", ")", "\n", "", "if", "self", ".", "weights", ".", "numel", "(", ")", ">", "0", ":", "\n", "            ", "weight", "=", "self", ".", "weights", ".", "new", "(", "self", ".", "weights", ".", "size", "(", ")", ")", "\n", "weighted_truth", "=", "self", ".", "weights", ".", "new", "(", "self", ".", "weights", ".", "size", "(", ")", ")", "\n", "\n", "# compute average precision for each class", "\n", "", "for", "k", "in", "range", "(", "self", ".", "scores", ".", "size", "(", "1", ")", ")", ":", "\n", "# sort scores", "\n", "            ", "scores", "=", "self", ".", "scores", "[", ":", ",", "k", "]", "\n", "targets", "=", "self", ".", "targets", "[", ":", ",", "k", "]", "\n", "_", ",", "sortind", "=", "torch", ".", "sort", "(", "scores", ",", "0", ",", "True", ")", "\n", "truth", "=", "targets", "[", "sortind", "]", "\n", "if", "self", ".", "weights", ".", "numel", "(", ")", ">", "0", ":", "\n", "                ", "weight", "=", "self", ".", "weights", "[", "sortind", "]", "\n", "weighted_truth", "=", "truth", ".", "float", "(", ")", "*", "weight", "\n", "rg", "=", "weight", ".", "cumsum", "(", "0", ")", "\n", "\n", "# compute true positive sums", "\n", "", "if", "self", ".", "weights", ".", "numel", "(", ")", ">", "0", ":", "\n", "                ", "tp", "=", "weighted_truth", ".", "cumsum", "(", "0", ")", "\n", "", "else", ":", "\n", "                ", "tp", "=", "truth", ".", "float", "(", ")", ".", "cumsum", "(", "0", ")", "\n", "\n", "# compute precision curve", "\n", "", "precision", "=", "tp", ".", "div", "(", "rg", ")", "\n", "\n", "# compute average precision", "\n", "ap", "[", "k", "]", "=", "precision", "[", "truth", ".", "byte", "(", ")", "]", ".", "sum", "(", ")", "/", "max", "(", "float", "(", "truth", ".", "sum", "(", ")", ")", ",", "1", ")", "\n", "", "return", "ap", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.APMeterChallenge.value": [[499, 518], ["range", "torch.from_numpy", "metric.APMeterChallenge.scores.numel", "metric.APMeterChallenge.scores.cpu().numpy", "metric.APMeterChallenge.targets.cpu().numpy", "numpy.argsort", "numpy.squeeze", "numpy.squeeze", "range", "sklearn.metrics.average_precision_score", "numpy.asarray", "metric.APMeterChallenge.scores.cpu", "metric.APMeterChallenge.targets.cpu"], "methods", ["None"], ["def", "value", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the model's average precision for each class\n        Return:\n            ap (FloatTensor): 1xK tensor, with avg precision for each class k\n        \"\"\"", "\n", "if", "not", "self", ".", "scores", ".", "numel", "(", ")", ":", "\n", "            ", "return", "0", "\n", "\n", "", "mAP", "=", "0.0", "\n", "scores_np", ",", "target_np", "=", "self", ".", "scores", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "self", ".", "targets", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "for", "ii", "in", "range", "(", "self", ".", "targets", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "sorted_ind", "=", "np", ".", "argsort", "(", "-", "1", "*", "scores_np", "[", "ii", ",", ":", "]", ")", "\n", "gt_label", "=", "np", ".", "squeeze", "(", "target_np", "[", "ii", ",", ":", "]", ")", "\n", "pred_label", "=", "np", ".", "squeeze", "(", "scores_np", "[", "ii", ",", ":", "]", ")", "\n", "for", "jj", "in", "range", "(", "target_np", ".", "shape", "[", "1", "]", ")", ":", "\n", "                ", "pred_label", "[", "sorted_ind", "[", "jj", "]", "]", "=", "target_np", ".", "shape", "[", "1", "]", "-", "1", "-", "jj", "\n", "", "mAP", "+=", "average_precision_score", "(", "gt_label", ",", "pred_label", ",", "average", "=", "'macro'", ")", "\n", "", "ap", "=", "mAP", "/", "target_np", ".", "shape", "[", "0", "]", "\n", "return", "torch", ".", "from_numpy", "(", "np", ".", "asarray", "(", "ap", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.__init__": [[521, 526], ["object.__init__", "numpy.sort", "metric.ClassErrorMeter.reset"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.reset"], ["    ", "def", "__init__", "(", "self", ",", "topk", "=", "[", "1", ",", "5", ",", "10", ",", "50", "]", ",", "accuracy", "=", "True", ")", ":", "\n", "        ", "super", "(", "ClassErrorMeter", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "topk", "=", "np", ".", "sort", "(", "topk", ")", "\n", "self", ".", "accuracy", "=", "accuracy", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.reset": [[527, 530], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "sum", "=", "{", "v", ":", "0", "for", "v", "in", "self", ".", "topk", "}", "\n", "self", ".", "n", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.add": [[531, 557], ["torch.is_tensor", "torch.is_tensor", "int", "[].numpy", "output.cpu().squeeze().numpy.cpu().squeeze().numpy.cpu().squeeze().numpy", "numpy.atleast_1d", "isinstance", "numpy.ndim", "target[].repeat", "numpy.asarray.cpu().squeeze().numpy", "numpy.asarray", "numpy.ndim", "numpy.ndim", "correct[].sum", "output.cpu().squeeze().numpy.cpu().squeeze().numpy.cpu().squeeze", "torch.from_numpy().topk", "numpy.asarray.cpu().squeeze", "output.cpu().squeeze().numpy.cpu().squeeze().numpy.cpu", "torch.from_numpy", "numpy.asarray.cpu"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "output", ",", "target", ")", ":", "\n", "        ", "if", "torch", ".", "is_tensor", "(", "output", ")", ":", "\n", "            ", "output", "=", "output", ".", "cpu", "(", ")", ".", "squeeze", "(", ")", ".", "numpy", "(", ")", "\n", "", "if", "torch", ".", "is_tensor", "(", "target", ")", ":", "\n", "            ", "target", "=", "np", ".", "atleast_1d", "(", "target", ".", "cpu", "(", ")", ".", "squeeze", "(", ")", ".", "numpy", "(", ")", ")", "\n", "", "elif", "isinstance", "(", "target", ",", "numbers", ".", "Number", ")", ":", "\n", "            ", "target", "=", "np", ".", "asarray", "(", "[", "target", "]", ")", "\n", "", "if", "np", ".", "ndim", "(", "output", ")", "==", "1", ":", "\n", "            ", "output", "=", "output", "[", "np", ".", "newaxis", "]", "\n", "", "else", ":", "\n", "            ", "assert", "np", ".", "ndim", "(", "output", ")", "==", "2", ",", "'wrong output size (1D or 2D expected)'", "\n", "assert", "np", ".", "ndim", "(", "target", ")", "==", "1", ",", "'target and output do not match'", "\n", "", "assert", "target", ".", "shape", "[", "0", "]", "==", "output", ".", "shape", "[", "0", "]", ",", "'target and output do not match'", "\n", "topk", "=", "self", ".", "topk", "\n", "maxk", "=", "int", "(", "topk", "[", "-", "1", "]", ")", "# seems like Python3 wants int and not np.int64", "\n", "no", "=", "output", ".", "shape", "[", "0", "]", "\n", "\n", "pred", "=", "torch", ".", "from_numpy", "(", "output", ")", ".", "topk", "(", "maxk", ",", "1", ",", "True", ",", "True", ")", "[", "1", "]", ".", "numpy", "(", ")", "\n", "correct", "=", "pred", "==", "target", "[", ":", ",", "np", ".", "newaxis", "]", ".", "repeat", "(", "pred", ".", "shape", "[", "1", "]", ",", "1", ")", "\n", "\n", "for", "k", "in", "topk", ":", "\n", "            ", "self", ".", "sum", "[", "k", "]", "+=", "no", "-", "correct", "[", ":", ",", "0", ":", "k", "]", ".", "sum", "(", ")", "\n", "", "self", ".", "n", "+=", "no", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.value": [[558, 568], ["metric.ClassErrorMeter.sum.keys", "metric.ClassErrorMeter.value", "float", "float"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.value"], ["", "def", "value", "(", "self", ",", "k", "=", "-", "1", ")", ":", "\n", "        ", "if", "k", "!=", "-", "1", ":", "\n", "            ", "assert", "k", "in", "self", ".", "sum", ".", "keys", "(", ")", ",", "'invalid k (this k was not provided at construction time)'", "\n", "if", "self", ".", "accuracy", ":", "\n", "                ", "return", "(", "1.", "-", "float", "(", "self", ".", "sum", "[", "k", "]", ")", "/", "self", ".", "n", ")", "*", "100.0", "\n", "", "else", ":", "\n", "                ", "return", "float", "(", "self", ".", "sum", "[", "k", "]", ")", "/", "self", ".", "n", "*", "100.0", "\n", "", "", "else", ":", "\n", "            ", "return", "[", "self", ".", "value", "(", "k_", ")", "for", "k_", "in", "self", ".", "topk", "]", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.t2v_metrics": [[14, 130], ["numpy.sort", "numpy.array", "numpy.where", "metric.cols2metrics", "matplotlib.use", "sys.path.insert", "plt.matshow", "zs_dispFig", "ipdb.set_trace", "dists.reshape", "ipdb.set_trace", "print", "numpy.zeros_like", "np.zeros_like.sum", "numpy.diag", "numpy.where", "numpy.array_equal", "str", "numpy.ravel_multi_index", "range", "np.array.reshape", "numpy.unique", "np.zeros_like.sum", "range", "numpy.unique", "numpy.argwhere", "numpy.diff", "numpy.insert", "numpy.add.reduceat", "numpy.diff", "np.zeros_like.reshape().astype", "pathlib.Path.home", "numpy.append", "print", "numpy.array_equal", "print", "numpy.nonzero", "numpy.mean", "np.zeros_like.reshape", "range"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.cols2metrics"], ["def", "t2v_metrics", "(", "sims", ",", "query_masks", "=", "None", ")", ":", "\n", "    ", "\"\"\"Compute retrieval metrics from a similiarity matrix.\n\n    Args:\n        sims (th.Tensor): N x M matrix of similarities between embeddings, where\n             x_{i,j} = <text_embd[i], vid_embed[j]>\n        query_masks (th.Tensor): mask any missing queries from the dataset (two videos\n             in MSRVTT only have 19, rather than 20 captions)\n\n    Returns:\n        (dict[str:float]): retrieval metrics\n    \"\"\"", "\n", "assert", "sims", ".", "ndim", "==", "2", ",", "\"expected a matrix\"", "\n", "num_queries", ",", "num_vids", "=", "sims", ".", "shape", "\n", "dists", "=", "-", "sims", "\n", "sorted_dists", "=", "np", ".", "sort", "(", "dists", ",", "axis", "=", "1", ")", "\n", "\n", "if", "False", ":", "\n", "        ", "import", "sys", "\n", "import", "matplotlib", "\n", "from", "pathlib", "import", "Path", "\n", "matplotlib", ".", "use", "(", "\"Agg\"", ")", "\n", "import", "matplotlib", ".", "pyplot", "as", "plt", "\n", "sys", ".", "path", ".", "insert", "(", "0", ",", "str", "(", "Path", ".", "home", "(", ")", "/", "\"coding/src/zsvision/python\"", ")", ")", "\n", "from", "zsvision", ".", "zs_iterm", "import", "zs_dispFig", "# NOQA", "\n", "plt", ".", "matshow", "(", "dists", ")", "\n", "zs_dispFig", "(", ")", "\n", "import", "ipdb", ";", "ipdb", ".", "set_trace", "(", ")", "\n", "\n", "# The indices are computed such that they slice out the ground truth distances", "\n", "# from the psuedo-rectangular dist matrix", "\n", "", "queries_per_video", "=", "num_queries", "//", "num_vids", "\n", "gt_idx", "=", "[", "[", "np", ".", "ravel_multi_index", "(", "[", "ii", ",", "jj", "]", ",", "(", "num_queries", ",", "num_vids", ")", ")", "\n", "for", "ii", "in", "range", "(", "jj", "*", "queries_per_video", ",", "(", "jj", "+", "1", ")", "*", "queries_per_video", ")", "]", "\n", "for", "jj", "in", "range", "(", "num_vids", ")", "]", "\n", "gt_idx", "=", "np", ".", "array", "(", "gt_idx", ")", "\n", "gt_dists", "=", "dists", ".", "reshape", "(", "-", "1", ")", "[", "gt_idx", ".", "reshape", "(", "-", "1", ")", "]", "\n", "gt_dists", "=", "gt_dists", "[", ":", ",", "np", ".", "newaxis", "]", "\n", "rows", ",", "cols", "=", "np", ".", "where", "(", "(", "sorted_dists", "-", "gt_dists", ")", "==", "0", ")", "# find column position of GT", "\n", "\n", "# --------------------------------", "\n", "# NOTE: Breaking ties", "\n", "# --------------------------------", "\n", "# We sometimes need to break ties (in general, these should occur extremely rarely,", "\n", "# but there are pathological cases when they can distort the scores, such as when", "\n", "# the similarity matrix is all zeros). Previous implementations (e.g. the t2i", "\n", "# evaluation function used", "\n", "# here: https://github.com/niluthpol/multimodal_vtt/blob/master/evaluation.py and", "\n", "# here: https://github.com/linxd5/VSE_Pytorch/blob/master/evaluation.py#L87) generally", "\n", "# break ties \"optimistically\".  However, if the similarity matrix is constant this", "\n", "# can evaluate to a perfect ranking. A principled option is to average over all", "\n", "# possible partial orderings implied by the ties. See # this paper for a discussion:", "\n", "#    McSherry, Frank, and Marc Najork,", "\n", "#    \"Computing information retrieval performance measures efficiently in the presence", "\n", "#    of tied scores.\" European conference on information retrieval. Springer, Berlin, ", "\n", "#    Heidelberg, 2008.", "\n", "# http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.8892&rep=rep1&type=pdf", "\n", "\n", "# break_ties = \"optimistically\"", "\n", "break_ties", "=", "\"averaging\"", "\n", "\n", "if", "rows", ".", "size", ">", "num_queries", ":", "\n", "        ", "assert", "np", ".", "unique", "(", "rows", ")", ".", "size", "==", "num_queries", ",", "\"issue in metric evaluation\"", "\n", "if", "break_ties", "==", "\"optimistically\"", ":", "\n", "            ", "_", ",", "idx", "=", "np", ".", "unique", "(", "rows", ",", "return_index", "=", "True", ")", "\n", "cols", "=", "cols", "[", "idx", "]", "\n", "", "elif", "break_ties", "==", "\"averaging\"", ":", "\n", "# fast implementation, based on this code:", "\n", "# https://stackoverflow.com/a/49239335", "\n", "            ", "locs", "=", "np", ".", "argwhere", "(", "(", "sorted_dists", "-", "gt_dists", ")", "==", "0", ")", "\n", "\n", "# Find the split indices", "\n", "steps", "=", "np", ".", "diff", "(", "locs", "[", ":", ",", "0", "]", ")", "\n", "splits", "=", "np", ".", "nonzero", "(", "steps", ")", "[", "0", "]", "+", "1", "\n", "splits", "=", "np", ".", "insert", "(", "splits", ",", "0", ",", "0", ")", "\n", "\n", "# Compute the result columns", "\n", "summed_cols", "=", "np", ".", "add", ".", "reduceat", "(", "locs", "[", ":", ",", "1", "]", ",", "splits", ")", "\n", "counts", "=", "np", ".", "diff", "(", "np", ".", "append", "(", "splits", ",", "locs", ".", "shape", "[", "0", "]", ")", ")", "\n", "avg_cols", "=", "summed_cols", "/", "counts", "\n", "if", "False", ":", "\n", "                ", "print", "(", "\"Running slower code to verify rank averaging across ties\"", ")", "\n", "# slow, but more interpretable version, used for testing", "\n", "avg_cols_slow", "=", "[", "np", ".", "mean", "(", "cols", "[", "rows", "==", "idx", "]", ")", "for", "idx", "in", "range", "(", "num_queries", ")", "]", "\n", "assert", "np", ".", "array_equal", "(", "avg_cols", ",", "avg_cols_slow", ")", ",", "\"slow vs fast difference\"", "\n", "print", "(", "\"passed num check\"", ")", "\n", "", "cols", "=", "avg_cols", "\n", "\n", "", "", "msg", "=", "\"expected ranks to match queries ({} vs {}) \"", "\n", "if", "cols", ".", "size", "!=", "num_queries", ":", "\n", "        ", "import", "ipdb", ";", "ipdb", ".", "set_trace", "(", ")", "\n", "", "assert", "cols", ".", "size", "==", "num_queries", ",", "msg", "\n", "\n", "if", "False", ":", "\n", "# overload mask to check that we can recover the scores for single-query", "\n", "# retrieval", "\n", "        ", "print", "(", "\"DEBUGGING MODE\"", ")", "\n", "query_masks", "=", "np", ".", "zeros_like", "(", "query_masks", ")", "\n", "query_masks", "[", ":", ",", "0", "]", "=", "1", "# recover single query score", "\n", "\n", "", "if", "query_masks", "is", "not", "None", ":", "\n", "# remove invalid queries", "\n", "        ", "assert", "query_masks", ".", "size", "==", "num_queries", ",", "\"invalid query mask shape\"", "\n", "cols", "=", "cols", "[", "query_masks", ".", "reshape", "(", "-", "1", ")", ".", "astype", "(", "np", ".", "bool", ")", "]", "\n", "assert", "cols", ".", "size", "==", "query_masks", ".", "sum", "(", ")", ",", "\"masking was not applied correctly\"", "\n", "# update number of queries to account for those that were missing", "\n", "num_queries", "=", "query_masks", ".", "sum", "(", ")", "\n", "\n", "", "if", "False", ":", "\n", "# sanity check against old logic for square matrices", "\n", "        ", "gt_dists_old", "=", "np", ".", "diag", "(", "dists", ")", "\n", "gt_dists_old", "=", "gt_dists_old", "[", ":", ",", "np", ".", "newaxis", "]", "\n", "_", ",", "cols_old", "=", "np", ".", "where", "(", "(", "sorted_dists", "-", "gt_dists_old", ")", "==", "0", ")", "\n", "assert", "np", ".", "array_equal", "(", "cols_old", ",", "cols", ")", ",", "\"new metric doesn't match\"", "\n", "\n", "", "return", "cols2metrics", "(", "cols", ",", "num_queries", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.v2t_metrics": [[132, 222], ["range", "numpy.array", "metric.cols2metrics", "numpy.ones", "numpy.sort", "range", "np.array.append", "numpy.sort", "numpy.diag", "numpy.where", "numpy.array_equal", "matplotlib.use", "sys.path.insert", "plt.matshow", "zs_dispFig", "numpy.unique", "str", "numpy.where", "numpy.logical_not", "ranks.mean", "pathlib.Path.home", "query_masks.reshape"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.cols2metrics"], ["", "def", "v2t_metrics", "(", "sims", ",", "query_masks", "=", "None", ")", ":", "\n", "    ", "\"\"\"Compute retrieval metrics from a similiarity matrix.\n\n    Args:\n        sims (th.Tensor): N x M matrix of similarities between embeddings, where\n             x_{i,j} = <text_embd[i], vid_embed[j]>\n        query_masks (th.Tensor): mask any missing captions from the dataset\n\n    Returns:\n        (dict[str:float]): retrieval metrics\n\n    NOTES: We find the closest \"GT caption\" in the style of VSE, which corresponds\n    to finding the rank of the closest relevant caption in embedding space:\n    github.com/ryankiros/visual-semantic-embedding/blob/master/evaluation.py#L52-L56\n    \"\"\"", "\n", "# switch axes of text and video", "\n", "sims", "=", "sims", ".", "T", "\n", "\n", "if", "False", ":", "\n", "# experiment with toy example", "\n", "        ", "sims", "=", "np", ".", "ones", "(", "(", "3", ",", "3", ")", ")", "\n", "sims", "[", "0", ",", "0", "]", "=", "2", "\n", "sims", "[", "1", ",", "1", ":", "2", "]", "=", "2", "\n", "sims", "[", "2", ",", ":", "]", "=", "2", "\n", "query_masks", "=", "None", "\n", "\n", "", "assert", "sims", ".", "ndim", "==", "2", ",", "\"expected a matrix\"", "\n", "num_queries", ",", "num_caps", "=", "sims", ".", "shape", "\n", "dists", "=", "-", "sims", "\n", "caps_per_video", "=", "num_caps", "//", "num_queries", "\n", "break_ties", "=", "\"averaging\"", "\n", "\n", "MISSING_VAL", "=", "1E8", "\n", "query_ranks", "=", "[", "]", "\n", "for", "ii", "in", "range", "(", "num_queries", ")", ":", "\n", "        ", "row_dists", "=", "dists", "[", "ii", ",", ":", "]", "\n", "if", "query_masks", "is", "not", "None", ":", "\n", "# Set missing queries to have a distance of infinity.  A missing query", "\n", "# refers to a query position `n` for a video that had less than `n`", "\n", "# captions (for example, a few MSRVTT videos only have 19 queries)", "\n", "            ", "row_dists", "[", "np", ".", "logical_not", "(", "query_masks", ".", "reshape", "(", "-", "1", ")", ")", "]", "=", "MISSING_VAL", "\n", "\n", "# NOTE: Using distance subtraction to perform the ranking is easier to make", "\n", "# deterministic than using argsort, which suffers from the issue of defining", "\n", "# \"stability\" for equal distances.  Example of distance subtraction code:", "\n", "# github.com/antoine77340/Mixture-of-Embedding-Experts/blob/master/train.py", "\n", "", "sorted_dists", "=", "np", ".", "sort", "(", "row_dists", ")", "\n", "\n", "min_rank", "=", "np", ".", "inf", "\n", "for", "jj", "in", "range", "(", "ii", "*", "caps_per_video", ",", "(", "ii", "+", "1", ")", "*", "caps_per_video", ")", ":", "\n", "            ", "if", "row_dists", "[", "jj", "]", "==", "MISSING_VAL", ":", "\n", "# skip rankings of missing captions", "\n", "                ", "continue", "\n", "", "ranks", "=", "np", ".", "where", "(", "(", "sorted_dists", "-", "row_dists", "[", "jj", "]", ")", "==", "0", ")", "[", "0", "]", "\n", "if", "break_ties", "==", "\"optimistically\"", ":", "\n", "                ", "rank", "=", "ranks", "[", "0", "]", "\n", "", "elif", "break_ties", "==", "\"averaging\"", ":", "\n", "# NOTE: If there is more than one caption per video, its possible for the", "\n", "# method to do \"worse than chance\" in the degenerate case when all", "\n", "# similarities are tied.  TODO(Samuel): Address this case.", "\n", "                ", "rank", "=", "ranks", ".", "mean", "(", ")", "\n", "", "if", "rank", "<", "min_rank", ":", "\n", "                ", "min_rank", "=", "rank", "\n", "", "", "query_ranks", ".", "append", "(", "min_rank", ")", "\n", "", "query_ranks", "=", "np", ".", "array", "(", "query_ranks", ")", "\n", "\n", "# sanity check against old version of code", "\n", "if", "False", ":", "\n", "        ", "sorted_dists", "=", "np", ".", "sort", "(", "dists", ",", "axis", "=", "1", ")", "\n", "gt_dists_old", "=", "np", ".", "diag", "(", "dists", ")", "\n", "gt_dists_old", "=", "gt_dists_old", "[", ":", ",", "np", ".", "newaxis", "]", "\n", "rows_old", ",", "cols_old", "=", "np", ".", "where", "(", "(", "sorted_dists", "-", "gt_dists_old", ")", "==", "0", ")", "\n", "if", "rows_old", ".", "size", ">", "num_queries", ":", "\n", "            ", "_", ",", "idx", "=", "np", ".", "unique", "(", "rows_old", ",", "return_index", "=", "True", ")", "\n", "cols_old", "=", "cols_old", "[", "idx", "]", "\n", "", "num_diffs", "=", "(", "1", "-", "(", "cols_old", "==", "query_ranks", ")", ")", ".", "sum", "(", ")", "\n", "msg", "=", "f\"new metric doesn't match in {num_diffs} places\"", "\n", "assert", "np", ".", "array_equal", "(", "cols_old", ",", "query_ranks", ")", ",", "msg", "\n", "\n", "# visualise the distance matrix", "\n", "import", "sys", "\n", "import", "matplotlib", "\n", "matplotlib", ".", "use", "(", "\"Agg\"", ")", "\n", "import", "matplotlib", ".", "pyplot", "as", "plt", "\n", "sys", ".", "path", ".", "insert", "(", "0", ",", "str", "(", "Path", ".", "home", "(", ")", "/", "\"coding/src/zsvision/python\"", ")", ")", "\n", "from", "zsvision", ".", "zs_iterm", "import", "zs_dispFig", "# NOQA", "\n", "plt", ".", "matshow", "(", "dists", ")", "\n", "zs_dispFig", "(", ")", "\n", "\n", "", "return", "cols2metrics", "(", "query_ranks", ",", "num_queries", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.retrieval_as_classification": [[223, 286], ["range", "numpy.array", "metric.cols2metrics", "numpy.sort", "np.array.extend", "matplotlib.use", "sys.path.insert", "plt.hist", "plt.grid", "zs_dispFig", "ipdb.set_trace", "numpy.where", "label_ranks.append", "str", "len", "numpy.where", "enumerate", "ranks.mean", "ValueError", "pathlib.Path.home"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.cols2metrics"], ["", "def", "retrieval_as_classification", "(", "sims", ",", "query_masks", "=", "None", ")", ":", "\n", "    ", "\"\"\"Compute classification metrics from a similiarity matrix.\n    \"\"\"", "\n", "assert", "sims", ".", "ndim", "==", "2", ",", "\"expected a matrix\"", "\n", "\n", "# switch axes of query-labels and video", "\n", "sims", "=", "sims", ".", "T", "\n", "query_masks", "=", "query_masks", ".", "T", "\n", "dists", "=", "-", "sims", "\n", "num_queries", ",", "num_labels", "=", "sims", ".", "shape", "\n", "break_ties", "=", "\"averaging\"", "\n", "\n", "query_ranks", "=", "[", "]", "\n", "for", "ii", "in", "range", "(", "num_queries", ")", ":", "\n", "        ", "row_dists", "=", "dists", "[", "ii", ",", ":", "]", "\n", "\n", "# NOTE: Using distance subtraction to perform the ranking is easier to make", "\n", "# deterministic than using argsort, which suffers from the issue of defining", "\n", "# \"stability\" for equal distances.  Example of distance subtraction code:", "\n", "# github.com/antoine77340/Mixture-of-Embedding-Experts/blob/master/train.py", "\n", "sorted_dists", "=", "np", ".", "sort", "(", "row_dists", ")", "\n", "\n", "# min_rank = np.inf", "\n", "label_ranks", "=", "[", "]", "\n", "for", "gt_label", "in", "np", ".", "where", "(", "query_masks", "[", "ii", ",", ":", "]", ")", "[", "0", "]", ":", "\n", "            ", "ranks", "=", "np", ".", "where", "(", "(", "sorted_dists", "-", "row_dists", "[", "gt_label", "]", ")", "==", "0", ")", "[", "0", "]", "\n", "if", "break_ties", "==", "\"optimistically\"", ":", "\n", "                ", "rank", "=", "ranks", "[", "0", "]", "\n", "", "elif", "break_ties", "==", "\"averaging\"", ":", "\n", "# NOTE: If there is more than one caption per video, its possible for the", "\n", "# method to do \"worse than chance\" in the degenerate case when all", "\n", "# similarities are tied.  TODO(Samuel): Address this case.", "\n", "                ", "rank", "=", "ranks", ".", "mean", "(", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "f\"unknown tie-breaking method: {break_ties}\"", ")", "\n", "", "label_ranks", ".", "append", "(", "rank", ")", "\n", "# Avoid penalising for assigning higher similarity to other gt labels. This is", "\n", "# done by subtracting out the better ranked query labels.  Note that this step", "\n", "# introduces a slight skew in favour of videos with lots of labels.  We can", "\n", "# address this later with a normalisation step if needed.", "\n", "", "label_ranks", "=", "[", "x", "-", "idx", "for", "idx", ",", "x", "in", "enumerate", "(", "label_ranks", ")", "]", "\n", "\n", "# Include all labels in the final calculation", "\n", "query_ranks", ".", "extend", "(", "label_ranks", ")", "\n", "", "query_ranks", "=", "np", ".", "array", "(", "query_ranks", ")", "\n", "\n", "# sanity check against old version of code", "\n", "if", "False", ":", "\n", "# visualise the distance matrix", "\n", "        ", "import", "sys", "\n", "import", "matplotlib", "\n", "matplotlib", ".", "use", "(", "\"Agg\"", ")", "\n", "import", "matplotlib", ".", "pyplot", "as", "plt", "\n", "sys", ".", "path", ".", "insert", "(", "0", ",", "str", "(", "Path", ".", "home", "(", ")", "/", "\"coding/src/zsvision/python\"", ")", ")", "\n", "from", "zsvision", ".", "zs_iterm", "import", "zs_dispFig", "# NOQA", "\n", "# plt.matshow(dists)", "\n", "# zs_dispFig()", "\n", "plt", ".", "hist", "(", "query_ranks", ",", "bins", "=", "313", ",", "alpha", "=", "0.5", ")", "\n", "plt", ".", "grid", "(", ")", "\n", "zs_dispFig", "(", ")", "\n", "import", "ipdb", ";", "ipdb", ".", "set_trace", "(", ")", "\n", "\n", "", "return", "cols2metrics", "(", "query_ranks", ",", "num_queries", "=", "len", "(", "query_ranks", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.cols2metrics": [[288, 299], ["scipy.stats.mstats.gmean", "numpy.median", "numpy.mean", "float", "float", "float", "float", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum"], "function", ["None"], ["", "def", "cols2metrics", "(", "cols", ",", "num_queries", ")", ":", "\n", "    ", "metrics", "=", "{", "}", "\n", "metrics", "[", "\"R1\"", "]", "=", "100", "*", "float", "(", "np", ".", "sum", "(", "cols", "==", "0", ")", ")", "/", "num_queries", "\n", "metrics", "[", "\"R5\"", "]", "=", "100", "*", "float", "(", "np", ".", "sum", "(", "cols", "<", "5", ")", ")", "/", "num_queries", "\n", "metrics", "[", "\"R10\"", "]", "=", "100", "*", "float", "(", "np", ".", "sum", "(", "cols", "<", "10", ")", ")", "/", "num_queries", "\n", "metrics", "[", "\"R50\"", "]", "=", "100", "*", "float", "(", "np", ".", "sum", "(", "cols", "<", "50", ")", ")", "/", "num_queries", "\n", "metrics", "[", "\"MedR\"", "]", "=", "np", ".", "median", "(", "cols", ")", "+", "1", "\n", "metrics", "[", "\"MeanR\"", "]", "=", "np", ".", "mean", "(", "cols", ")", "+", "1", "\n", "stats", "=", "[", "metrics", "[", "x", "]", "for", "x", "in", "(", "\"R1\"", ",", "\"R5\"", ",", "\"R10\"", ")", "]", "\n", "metrics", "[", "\"geometric_mean_R1-R5-R10\"", "]", "=", "scipy", ".", "stats", ".", "mstats", ".", "gmean", "(", "stats", ")", "\n", "return", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.mean_average_precision": [[301, 305], ["metric.APMeter", "metric.APMeter.add", "APMeter.value().mean", "metric.APMeter.value"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.add", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.value"], ["", "def", "mean_average_precision", "(", "sims", ",", "query_masks", "=", "None", ")", ":", "\n", "    ", "ap_meter", "=", "APMeter", "(", ")", "\n", "ap_meter", ".", "add", "(", "output", "=", "sims", ".", "T", ",", "target", "=", "query_masks", ".", "T", ")", "\n", "return", "{", "\"mAP\"", ":", "ap_meter", ".", "value", "(", ")", ".", "mean", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.MSVD_dataset.MSVD.dataset_paths": [[16, 63], ["test_splits.items", "base.base_dataset.BaseDataset.common_feat_names", "base.base_dataset.BaseDataset.common_text_feat_paths", "pathlib.Path", "base.base_dataset.BaseDataset.common_text_feat_paths.items", "text_feat_names.items"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.common_feat_names", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.common_text_feat_paths", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["    ", "@", "staticmethod", "\n", "@", "typechecked", "\n", "def", "dataset_paths", "(", "training_file", "=", "None", ")", "->", "Dict", "[", "str", ",", "Union", "[", "str", ",", "List", "[", "str", "]", ",", "Path", ",", "Dict", "]", "]", ":", "\n", "        ", "subset_paths", "=", "{", "}", "\n", "test_splits", "=", "{", "\n", "\"dev\"", ":", "\"val_list.txt\"", ",", "\n", "\"official\"", ":", "\"test_list.txt\"", ",", "\n", "\"public_server_val\"", ":", "\"public_server_val.txt\"", ",", "\n", "\"public_server_test\"", ":", "\"public_server_test.txt\"", ",", "\n", "}", "\n", "\n", "for", "split_name", ",", "fname", "in", "test_splits", ".", "items", "(", ")", ":", "\n", "            ", "if", "training_file", "is", "not", "None", ":", "\n", "                ", "subset_paths", "[", "split_name", "]", "=", "{", "\"train\"", ":", "training_file", ",", "\"val\"", ":", "fname", "}", "\n", "", "else", ":", "\n", "                ", "subset_paths", "[", "split_name", "]", "=", "{", "\"train\"", ":", "\"train_list.txt\"", ",", "\"val\"", ":", "fname", "}", "\n", "\n", "", "", "feature_names", "=", "BaseDataset", ".", "common_feat_names", "(", ")", "\n", "custom_paths", "=", "{", "\n", "\"face\"", ":", "[", "\"aggregated_face_feats/face-avg.pickle\"", "]", ",", "\n", "\"ocr\"", ":", "[", "\"aggregated_ocr_feats/ocr-w2v.pickle\"", "]", ",", "\n", "\"r2p1d.r2p1d-ig65m-kinetics.0\"", ":", "[", "\"aggregated_r2p1d_30fps_256px_stride8_offset0_inner_stride1/r2p1d-ig65m-kinetics-avg.pickle\"", "]", ",", "\n", "}", "\n", "text_feat_paths", "=", "BaseDataset", ".", "common_text_feat_paths", "(", ")", "\n", "text_feat_dir", "=", "Path", "(", "\"aggregated_text_feats\"", ")", "\n", "\n", "text_feat_paths", "=", "{", "key", ":", "text_feat_dir", "/", "fname", "\n", "for", "key", ",", "fname", "in", "text_feat_paths", ".", "items", "(", ")", "}", "\n", "challenge_text_feat_paths", "=", "{", "}", "\n", "for", "text_feat", "in", "(", "\"openai\"", ",", ")", ":", "\n", "            ", "text_feat_names", "=", "{", "key", ":", "f\"{text_feat}-caption-{key}\"", "\n", "for", "key", "in", "{", "\"train\"", ",", "\"val\"", ",", "\"test\"", "}", "}", "\n", "text_feat_paths", "[", "text_feat", "]", "=", "{", "key", ":", "f\"aggregated_text_feats/{val}.pkl\"", "\n", "for", "key", ",", "val", "in", "text_feat_names", ".", "items", "(", ")", "}", "\n", "challenge_text_feat_paths", "[", "text_feat", "]", "=", "f\"aggregated_text_feats/{text_feat}.pkl\"", "\n", "\n", "", "feature_info", "=", "{", "\n", "\"subset_list_paths\"", ":", "subset_paths", ",", "\n", "\"feature_names\"", ":", "feature_names", ",", "\n", "\"custom_paths\"", ":", "custom_paths", ",", "\n", "\"text_feat_paths\"", ":", "text_feat_paths", ",", "\n", "\"challenge_text_feat_paths\"", ":", "challenge_text_feat_paths", ",", "\n", "\"raw_captions_path\"", ":", "\"raw-captions.pkl\"", ",", "\n", "\"dict_youtube_mapping_path\"", ":", "\"dict_youtube_mapping.pkl\"", "\n", "}", "\n", "return", "feature_info", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.MSVD_dataset.MSVD.load_features": [[64, 143], ["pathlib.Path", "feat_names.update", "feat_names.items", "list", "enumerate", "MSVD_dataset.MSVD.visual_feat_paths", "tuple", "MSVD_dataset.MSVD.load_challenge_text_features", "isinstance", "zsvision.zs_utils.memcache", "MSVD_dataset.MSVD.features[].items", "MSVD_dataset.MSVD.features[].items", "map", "zsvision.zs_utils.memcache", "len", "zsvision.zs_utils.memcache", "print", "zsvision.zs_utils.concat_features.cache_info", "print", "zsvision.zs_utils.concat_features", "utils.util.memory_summary", "copy.deepcopy", "zsvision.zs_utils.memcache", "zsvision.zs_utils.memcache.update", "zsvision.zs_utils.memcache", "isinstance", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate.transpose().reshape", "collections.defaultdict", "collections.defaultdict.update", "zsvision.zs_utils.memcache", "zsvision.zs_utils.memcache", "TypeError", "zsvision.zs_utils.memcache.items", "zsvision.zs_utils.memcache.items", "conf.reshape", "numpy.concatenate.transpose", "pathlib.Path", "numpy.zeros", "type"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.visual_feat_paths", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.load_challenge_text_features", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.memory_summary", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["", "def", "load_features", "(", "self", ")", ":", "\n", "        ", "root_feat", "=", "Path", "(", "self", ".", "root_feat", ")", "\n", "if", "self", ".", "distil_params", "is", "not", "None", ":", "\n", "            ", "self", ".", "distil_features", "=", "{", "}", "\n", "d_base_path", "=", "self", ".", "distil_params", "[", "'base_path'", "]", "\n", "\n", "teachers", "=", "list", "(", "map", "(", "lambda", "x", ":", "root_feat", "/", "Path", "(", "d_base_path", "+", "x", ")", ",", "self", ".", "distil_params", "[", "'teachers'", "]", ")", ")", "\n", "\n", "for", "i", ",", "f_name", "in", "enumerate", "(", "teachers", ")", ":", "\n", "                ", "self", ".", "distil_features", "[", "i", "]", "=", "memcache", "(", "f_name", ")", "\n", "\n", "", "", "feat_names", "=", "{", "key", ":", "self", ".", "visual_feat_paths", "(", "key", ")", "for", "key", "in", "\n", "self", ".", "paths", "[", "\"feature_names\"", "]", "}", "\n", "feat_names", ".", "update", "(", "self", ".", "paths", "[", "\"custom_paths\"", "]", ")", "\n", "\n", "features", "=", "{", "}", "\n", "for", "expert", ",", "rel_names", "in", "feat_names", ".", "items", "(", ")", ":", "\n", "            ", "if", "expert", "not", "in", "self", ".", "ordered_experts", ":", "\n", "                ", "continue", "\n", "", "feat_paths", "=", "tuple", "(", "[", "root_feat", "/", "rel_name", "for", "rel_name", "in", "rel_names", "]", ")", "\n", "if", "len", "(", "feat_paths", ")", "==", "1", ":", "\n", "                ", "features", "[", "expert", "]", "=", "memcache", "(", "feat_paths", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "# support multiple forms of feature (e.g. max and avg pooling). For", "\n", "# now, we only support direct concatenation", "\n", "                ", "msg", "=", "f\"{expert}: Only direct concat of muliple feats is possible\"", "\n", "print", "(", "f\"Concatenating aggregates for {expert}....\"", ")", "\n", "assert", "self", ".", "feat_aggregation", "[", "expert", "]", "[", "\"aggregate\"", "]", "==", "\"concat\"", ",", "msg", "\n", "axis", "=", "self", ".", "feat_aggregation", "[", "expert", "]", "[", "\"aggregate-axis\"", "]", "\n", "x", "=", "concat_features", ".", "cache_info", "(", ")", "# pylint: disable=no-value-for-parameter", "\n", "print", "(", "f\"concat cache info: {x}\"", ")", "\n", "features_", "=", "concat_features", "(", "feat_paths", ",", "axis", "=", "axis", ")", "\n", "memory_summary", "(", ")", "\n", "\n", "if", "expert", "==", "\"speech\"", ":", "\n", "                    ", "features_defaults", "=", "defaultdict", "(", "lambda", ":", "np", ".", "zeros", "(", "(", "1", ",", "300", ")", ")", ")", "\n", "features_defaults", ".", "update", "(", "features_", ")", "\n", "features_", "=", "features_defaults", "\n", "\n", "# Make separate feature copies for each split to allow in-place filtering", "\n", "", "features", "[", "expert", "]", "=", "copy", ".", "deepcopy", "(", "features_", ")", "\n", "\n", "", "", "self", ".", "features", "=", "features", "\n", "if", "self", ".", "challenge_mode", ":", "\n", "            ", "self", ".", "load_challenge_text_features", "(", ")", "\n", "", "else", ":", "\n", "            ", "text_feat_paths", "=", "self", ".", "paths", "[", "\"text_feat_paths\"", "]", "[", "self", ".", "text_feat", "]", "\n", "if", "isinstance", "(", "text_feat_paths", ",", "dict", ")", ":", "\n", "                ", "text_features", "=", "memcache", "(", "root_feat", "/", "text_feat_paths", "[", "\"train\"", "]", ")", "\n", "split_names", "=", "{", "\"dev\"", ":", "\"val\"", ",", "\"official\"", ":", "\"test\"", "}", "\n", "text_features", ".", "update", "(", "memcache", "(", "\n", "root_feat", "/", "text_feat_paths", "[", "split_names", "[", "self", ".", "split_name", "]", "]", ")", ")", "\n", "key_map", "=", "memcache", "(", "root_feat", "/", "self", ".", "paths", "[", "\"dict_youtube_mapping_path\"", "]", ")", "\n", "inverse_map", "=", "{", "val", ":", "key", "for", "key", ",", "val", "in", "key_map", ".", "items", "(", ")", "}", "\n", "text_features", "=", "{", "inverse_map", "[", "key", "]", ":", "val", "for", "key", ",", "val", "in", "\n", "text_features", ".", "items", "(", ")", "}", "\n", "", "elif", "isinstance", "(", "text_feat_paths", ",", "(", "Path", ",", "str", ")", ")", ":", "\n", "                ", "text_features", "=", "memcache", "(", "root_feat", "/", "text_feat_paths", ")", "\n", "", "else", ":", "\n", "                ", "raise", "TypeError", "(", "f\"Unexpected type {type(text_feat_paths)}\"", ")", "\n", "", "self", ".", "text_features", "=", "text_features", "\n", "self", ".", "raw_captions", "=", "memcache", "(", "root_feat", "/", "self", ".", "paths", "[", "\"raw_captions_path\"", "]", ")", "\n", "\n", "", "if", "\"detection\"", "in", "self", ".", "ordered_experts", ":", "\n", "# Example processing", "\n", "            ", "processed", "=", "{", "}", "\n", "for", "key", ",", "subdict", "in", "self", ".", "features", "[", "\"detection\"", "]", ".", "items", "(", ")", ":", "\n", "                ", "box", ",", "conf", "=", "subdict", "[", "\"detection_boxes\"", "]", ",", "subdict", "[", "\"detection_scores\"", "]", "\n", "raw", "=", "subdict", "[", "\"raw_feats_avg\"", "]", "\n", "processed", "[", "key", "]", "=", "np", ".", "concatenate", "(", "(", "box", ",", "conf", ".", "reshape", "(", "-", "1", ",", "1", ")", ",", "raw", ")", ",", "axis", "=", "1", ")", "\n", "", "self", ".", "features", "[", "\"detection\"", "]", "=", "processed", "\n", "\n", "", "if", "\"openpose\"", "in", "self", ".", "ordered_experts", ":", "\n", "# Example processing", "\n", "            ", "processed", "=", "{", "}", "\n", "for", "key", ",", "subdict", "in", "self", ".", "features", "[", "\"openpose\"", "]", ".", "items", "(", ")", ":", "\n", "                ", "raw", "=", "np", ".", "concatenate", "(", "subdict", "[", "\"matrix\"", "]", ",", "axis", "=", "1", ")", "\n", "processed", "[", "key", "]", "=", "raw", ".", "transpose", "(", "1", ",", "0", ",", "2", ")", ".", "reshape", "(", "-", "1", ",", "3", "*", "18", ")", "\n", "", "self", ".", "features", "[", "\"openpose\"", "]", "=", "processed", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.MSVD_dataset.MSVD.sanity_checks": [[144, 146], ["None"], "methods", ["None"], ["", "", "def", "sanity_checks", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "num_test_captions", "==", "81", ",", "\"Expected to have 81 test caps for MSVD\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.LSMDC_dataset.LSMDC.dataset_paths": [[16, 63], ["test_splits.items", "test_splits.items", "base.base_dataset.BaseDataset.common_feat_names", "base.base_dataset.BaseDataset.common_text_feat_paths", "pathlib.Path", "base.base_dataset.BaseDataset.common_text_feat_paths.items", "pathlib.Path", "base.base_dataset.BaseDataset.common_text_feat_paths.items"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.common_feat_names", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.common_text_feat_paths", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["    ", "@", "staticmethod", "\n", "@", "typechecked", "\n", "def", "dataset_paths", "(", "training_file", "=", "None", ")", "->", "Dict", "[", "str", ",", "Union", "[", "Path", ",", "str", ",", "Dict", ",", "List", "[", "str", "]", "]", "]", ":", "\n", "        ", "subset_paths", "=", "{", "}", "\n", "test_splits", "=", "{", "\n", "\"full-val\"", ":", "\"val_list.txt\"", ",", "\n", "\"full-test\"", ":", "\"test_list.txt\"", ",", "\n", "\"public_server_val\"", ":", "\"public_server_val.txt\"", ",", "\n", "\"public_server_test\"", ":", "\"public_server_test.txt\"", ",", "\n", "}", "\n", "for", "split_name", ",", "fname", "in", "test_splits", ".", "items", "(", ")", ":", "\n", "            ", "subset_paths", "[", "split_name", "]", "=", "{", "\"train\"", ":", "\"train_list.txt\"", ",", "\"val\"", ":", "fname", "}", "\n", "", "for", "split_name", ",", "fname", "in", "test_splits", ".", "items", "(", ")", ":", "\n", "            ", "if", "training_file", "is", "None", ":", "\n", "                ", "subset_paths", "[", "split_name", "]", "=", "{", "\"train\"", ":", "\"train_list.txt\"", ",", "\"val\"", ":", "fname", "}", "\n", "", "else", ":", "\n", "                ", "subset_paths", "[", "split_name", "]", "=", "{", "\"train\"", ":", "training_file", ",", "\"val\"", ":", "fname", "}", "\n", "\n", "\n", "", "", "feature_names", "=", "BaseDataset", ".", "common_feat_names", "(", ")", "\n", "custom_paths", "=", "{", "\n", "\"audio\"", ":", "[", "\"aggregated_audio/vggish-raw.pickle\"", "]", ",", "\n", "\"ocr\"", ":", "[", "\"aggregated_ocr_feats/ocr-w2v.pkl\"", "]", ",", "\n", "\"face\"", ":", "[", "\"antoine/face-max-with-blank-val.pickle\"", "]", ",", "\n", "\"speech\"", ":", "[", "\"aggregated_speech/speech-w2v.pickle\"", "]", ",", "\n", "}", "\n", "# old w2v", "\n", "# text_feat_paths = {\"openai\": \"openai-text.pickle\", \"w2v\": \"w2v-text.pickle\"}", "\n", "text_feat_paths", "=", "BaseDataset", ".", "common_text_feat_paths", "(", ")", "\n", "text_feat_paths", "[", "\"openai\"", "]", "=", "\"openai-text.pickle\"", "\n", "text_feat_dir", "=", "Path", "(", "\"aggregated_text_feats\"", ")", "\n", "\n", "text_feat_paths", "=", "{", "key", ":", "text_feat_dir", "/", "fname", "\n", "for", "key", ",", "fname", "in", "text_feat_paths", ".", "items", "(", ")", "}", "\n", "challenge_text_feat_paths", "=", "{", "\n", "key", ":", "Path", "(", "\"aggregated_text_feats\"", ")", "/", "f\"{key}{fname.suffix}\"", "\n", "for", "key", ",", "fname", "in", "text_feat_paths", ".", "items", "(", ")", "\n", "}", "\n", "feature_info", "=", "{", "\n", "\"custom_paths\"", ":", "custom_paths", ",", "\n", "\"feature_names\"", ":", "feature_names", ",", "\n", "\"subset_list_paths\"", ":", "subset_paths", ",", "\n", "\"text_feat_paths\"", ":", "text_feat_paths", ",", "\n", "\"challenge_text_feat_paths\"", ":", "challenge_text_feat_paths", ",", "\n", "\"raw_captions_path\"", ":", "\"raw-captions.pkl\"", ",", "\n", "}", "\n", "return", "feature_info", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.LSMDC_dataset.LSMDC.load_features": [[64, 107], ["pathlib.Path", "feat_names.update", "feat_names.items", "list", "enumerate", "LSMDC_dataset.LSMDC.visual_feat_paths", "tuple", "LSMDC_dataset.LSMDC.load_challenge_text_features", "zsvision.zs_utils.memcache", "zsvision.zs_utils.memcache", "map", "zsvision.zs_utils.memcache", "len", "zsvision.zs_utils.memcache", "print", "zsvision.zs_utils.concat_features.cache_info", "print", "zsvision.zs_utils.concat_features", "utils.memory_summary", "copy.deepcopy", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.visual_feat_paths", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.load_challenge_text_features", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.memory_summary"], ["", "def", "load_features", "(", "self", ")", ":", "\n", "        ", "root_feat", "=", "Path", "(", "self", ".", "root_feat", ")", "\n", "if", "self", ".", "distil_params", "is", "not", "None", ":", "\n", "            ", "self", ".", "distil_features", "=", "{", "}", "\n", "d_base_path", "=", "self", ".", "distil_params", "[", "'base_path'", "]", "\n", "\n", "teachers", "=", "list", "(", "map", "(", "lambda", "x", ":", "root_feat", "/", "Path", "(", "d_base_path", "+", "x", ")", ",", "self", ".", "distil_params", "[", "'teachers'", "]", ")", ")", "\n", "\n", "for", "i", ",", "f_name", "in", "enumerate", "(", "teachers", ")", ":", "\n", "                ", "self", ".", "distil_features", "[", "i", "]", "=", "memcache", "(", "f_name", ")", "\n", "\n", "", "", "feat_names", "=", "{", "key", ":", "self", ".", "visual_feat_paths", "(", "key", ")", "for", "key", "in", "\n", "self", ".", "paths", "[", "\"feature_names\"", "]", "}", "\n", "feat_names", ".", "update", "(", "self", ".", "paths", "[", "\"custom_paths\"", "]", ")", "\n", "features", "=", "{", "}", "\n", "for", "expert", ",", "rel_names", "in", "feat_names", ".", "items", "(", ")", ":", "\n", "            ", "if", "expert", "not", "in", "self", ".", "ordered_experts", ":", "\n", "                ", "continue", "\n", "", "feat_paths", "=", "tuple", "(", "[", "root_feat", "/", "rel_name", "for", "rel_name", "in", "rel_names", "]", ")", "\n", "if", "len", "(", "feat_paths", ")", "==", "1", ":", "\n", "                ", "features", "[", "expert", "]", "=", "memcache", "(", "feat_paths", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "# support multiple forms of feature (e.g. max and avg pooling). For", "\n", "# now, we only support direct concatenation", "\n", "                ", "msg", "=", "f\"{expert}: Only direct concatenation of muliple feats is possible\"", "\n", "print", "(", "f\"Concatenating aggregates for {expert}....\"", ")", "\n", "assert", "self", ".", "feat_aggregation", "[", "expert", "]", "[", "\"aggregate\"", "]", "==", "\"concat\"", ",", "msg", "\n", "axis", "=", "self", ".", "feat_aggregation", "[", "expert", "]", "[", "\"aggregate-axis\"", "]", "\n", "x", "=", "concat_features", ".", "cache_info", "(", ")", "# pylint: disable=no-value-for-parameter", "\n", "print", "(", "f\"concat cache info: {x}\"", ")", "\n", "features_", "=", "concat_features", "(", "feat_paths", ",", "axis", "=", "axis", ")", "\n", "memory_summary", "(", ")", "\n", "\n", "# Make separate feature copies for each split to allow in-place filtering", "\n", "features", "[", "expert", "]", "=", "copy", ".", "deepcopy", "(", "features_", ")", "\n", "\n", "", "", "self", ".", "features", "=", "features", "\n", "if", "self", ".", "challenge_mode", ":", "\n", "            ", "self", ".", "load_challenge_text_features", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "raw_captions", "=", "memcache", "(", "root_feat", "/", "self", ".", "paths", "[", "\"raw_captions_path\"", "]", ")", "\n", "text_feat_path", "=", "root_feat", "/", "self", ".", "paths", "[", "\"text_feat_paths\"", "]", "[", "self", ".", "text_feat", "]", "\n", "self", ".", "text_features", "=", "memcache", "(", "text_feat_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.LSMDC_dataset.LSMDC.sanity_checks": [[108, 112], ["None"], "methods", ["None"], ["", "", "def", "sanity_checks", "(", "self", ")", ":", "\n", "        ", "msg", "=", "(", "f\"Expected to have single test caption for LSMDC, since we assume \"", "\n", "f\"that the captions are fused (but using {self.num_test_captions})\"", ")", "\n", "assert", "self", ".", "num_test_captions", "==", "1", ",", "msg", "\n", "", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.MSRVTT_dataset.MSRVTT.dataset_paths": [[14, 93], ["splits.update", "base.base_dataset.BaseDataset.common_feat_names", "custom_paths.copy", "custom_paths.copy.update", "base.base_dataset.BaseDataset.common_text_feat_paths", "pathlib.Path", "base.base_dataset.BaseDataset.common_text_feat_paths.items", "ValueError", "msg.format"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.common_feat_names", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.common_text_feat_paths", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["    ", "@", "staticmethod", "\n", "@", "typechecked", "\n", "def", "dataset_paths", "(", "training_file", "=", "None", ")", "->", "Dict", "[", "str", ",", "Union", "[", "str", ",", "List", "[", "str", "]", ",", "Path", ",", "Dict", "]", "]", ":", "\n", "        ", "subset_paths", "=", "{", "}", "\n", "js_test_cap_idx_path", "=", "None", "\n", "challenge_splits", "=", "{", "\"val\"", ",", "\"public_server_val\"", ",", "\"public_server_test\"", "}", "\n", "splits", "=", "{", "\"full-val\"", ",", "\"full-test\"", ",", "\"miech\"", ",", "\"jsfusion\"", "}", "\n", "splits", ".", "update", "(", "challenge_splits", ")", "\n", "for", "split_name", "in", "splits", ":", "\n", "            ", "if", "split_name", "==", "\"miech\"", ":", "\n", "# For now, we follow Antoine's approach of using the first text caption", "\n", "# for the retreival task when evaluating on his custom split.", "\n", "                ", "train_list_path", "=", "\"train_list_miech.txt\"", "\n", "test_list_path", "=", "\"test_list_miech.txt\"", "\n", "", "elif", "split_name", "in", "\"jsfusion\"", ":", "\n", "                ", "train_list_path", "=", "\"train_list_jsfusion.txt\"", "\n", "test_list_path", "=", "\"val_list_jsfusion.txt\"", "\n", "# NOTE: The JSFusion split (referred to as 1k-A in the paper) uses all", "\n", "# videos, but randomly samples a single caption per video from the test", "\n", "# set for evaluation. To reproduce this evaluation, we use the indices", "\n", "# of the test captions, and restrict to this subset during eval.", "\n", "js_test_cap_idx_path", "=", "\"jsfusion_val_caption_idx.pkl\"", "\n", "", "elif", "split_name", "in", "{", "\"full-val\"", ",", "\"full-test\"", "}", ":", "\n", "                ", "if", "training_file", "is", "None", ":", "\n", "                    ", "train_list_path", "=", "\"train_list_full.txt\"", "\n", "", "else", ":", "\n", "                    ", "train_list_path", "=", "training_file", "\n", "", "if", "split_name", "==", "\"full-val\"", ":", "\n", "                    ", "test_list_path", "=", "\"val_list_full.txt\"", "\n", "", "else", ":", "\n", "                    ", "test_list_path", "=", "\"test_list_full.txt\"", "\n", "", "", "elif", "split_name", "in", "challenge_splits", ":", "\n", "                ", "train_list_path", "=", "\"train_list.txt\"", "\n", "if", "split_name", "==", "\"val\"", ":", "\n", "                    ", "test_list_path", "=", "f\"{split_name}_list.txt\"", "\n", "", "else", ":", "\n", "                    ", "test_list_path", "=", "f\"{split_name}.txt\"", "\n", "", "", "else", ":", "\n", "                ", "msg", "=", "\"unrecognised MSRVTT split: {}\"", "\n", "raise", "ValueError", "(", "msg", ".", "format", "(", "split_name", ")", ")", "\n", "", "subset_paths", "[", "split_name", "]", "=", "{", "\"train\"", ":", "train_list_path", ",", "\"val\"", ":", "test_list_path", "}", "\n", "", "feature_names", "=", "BaseDataset", ".", "common_feat_names", "(", ")", "\n", "custom_paths", "=", "{", "\n", "\"audio\"", ":", "[", "\"aggregated_audio_feats/Audio_MSRVTT_new.pickle\"", "]", ",", "\n", "\"face\"", ":", "[", "\"aggregated_face_feats/facefeats-avg.pickle\"", "]", ",", "\n", "\"ocr\"", ":", "[", "\"aggregated_ocr_feats/ocr-raw.pickle\"", "]", ",", "\n", "\"speech\"", ":", "[", "\"aggregated_speech/speech-w2v.pickle\"", "]", ",", "\n", "\n", "}", "\n", "custom_miech_paths", "=", "custom_paths", ".", "copy", "(", ")", "\n", "custom_miech_paths", ".", "update", "(", "{", "\n", "\"antoine-rgb\"", ":", "[", "\"antoine/resnet_features.pickle\"", "]", ",", "\n", "\"audio\"", ":", "[", "\"antoine/audio_features.pickle\"", "]", ",", "\n", "\"flow\"", ":", "[", "\"antoine/flow_features.pickle\"", "]", ",", "\n", "\"face\"", ":", "[", "\"antoine/facefeats-clone.pickle\"", "]", ",", "\n", "}", ")", "\n", "\n", "text_feat_paths", "=", "BaseDataset", ".", "common_text_feat_paths", "(", ")", "\n", "# include non-standard text features", "\n", "text_feat_paths", "[", "\"openai\"", "]", "=", "\"w2v_MSRVTT_openAIGPT.pickle\"", "\n", "text_feat_dir", "=", "Path", "(", "\"aggregated_text_feats\"", ")", "\n", "\n", "text_feat_paths", "=", "{", "key", ":", "text_feat_dir", "/", "fname", "\n", "for", "key", ",", "fname", "in", "text_feat_paths", ".", "items", "(", ")", "}", "\n", "\n", "challenge_text_feat_paths", "=", "{", "key", ":", "f\"aggregated_text_feats/{key}.pickle\"", "\n", "for", "key", "in", "text_feat_paths", "}", "\n", "\n", "feature_info", "=", "{", "\n", "\"custom_paths\"", ":", "custom_paths", ",", "\n", "\"custom_miech_paths\"", ":", "custom_miech_paths", ",", "\n", "\"feature_names\"", ":", "feature_names", ",", "\n", "\"subset_list_paths\"", ":", "subset_paths", ",", "\n", "\"text_feat_paths\"", ":", "text_feat_paths", ",", "\n", "\"challenge_text_feat_paths\"", ":", "challenge_text_feat_paths", ",", "\n", "\"raw_captions_path\"", ":", "\"raw-captions.pkl\"", ",", "\n", "\"js_test_cap_idx_path\"", ":", "js_test_cap_idx_path", ",", "\n", "}", "\n", "return", "feature_info", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.MSRVTT_dataset.MSRVTT.load_features": [[94, 167], ["pathlib.Path", "feat_names.update", "feat_names.items", "MSRVTT_dataset.MSRVTT.summary_stats", "list", "enumerate", "MSRVTT_dataset.MSRVTT.visual_feat_paths", "tuple", "MSRVTT_dataset.MSRVTT.load_challenge_text_features", "zsvision.zs_utils.memcache", "zsvision.zs_utils.memcache", "map", "zsvision.zs_utils.memcache", "len", "zsvision.zs_utils.memcache", "print", "MSRVTT_dataset.MSRVTT.log_assert", "zsvision.zs_utils.concat_features.cache_info", "print", "zsvision.zs_utils.concat_features", "utils.memory_summary", "copy.deepcopy", "zsvision.zs_utils.memcache", "set", "MSRVTT_dataset.MSRVTT.text_features.items", "MSRVTT_dataset.MSRVTT.log_assert", "pathlib.Path", "isinstance", "len"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.summary_stats", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.visual_feat_paths", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.load_challenge_text_features", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.log_assert", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.memory_summary", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.log_assert"], ["", "def", "load_features", "(", "self", ")", ":", "\n", "        ", "root_feat", "=", "Path", "(", "self", ".", "root_feat", ")", "\n", "if", "self", ".", "distil_params", "is", "not", "None", ":", "\n", "            ", "self", ".", "distil_features", "=", "{", "}", "\n", "d_base_path", "=", "self", ".", "distil_params", "[", "'base_path'", "]", "\n", "\n", "teachers", "=", "list", "(", "map", "(", "lambda", "x", ":", "root_feat", "/", "Path", "(", "d_base_path", "+", "x", ")", ",", "self", ".", "distil_params", "[", "'teachers'", "]", ")", ")", "\n", "\n", "for", "i", ",", "f_name", "in", "enumerate", "(", "teachers", ")", ":", "\n", "                ", "self", ".", "distil_features", "[", "i", "]", "=", "memcache", "(", "f_name", ")", "\n", "\n", "", "", "feat_names", "=", "{", "key", ":", "self", ".", "visual_feat_paths", "(", "key", ")", "for", "key", "in", "\n", "self", ".", "paths", "[", "\"feature_names\"", "]", "}", "\n", "\n", "if", "self", ".", "split_name", "==", "\"miech\"", ":", "\n", "            ", "custom_path_key", "=", "\"custom_miech_paths\"", "\n", "", "else", ":", "\n", "            ", "custom_path_key", "=", "\"custom_paths\"", "\n", "", "feat_names", ".", "update", "(", "self", ".", "paths", "[", "custom_path_key", "]", ")", "\n", "features", "=", "{", "}", "\n", "for", "expert", ",", "rel_names", "in", "feat_names", ".", "items", "(", ")", ":", "\n", "            ", "if", "expert", "not", "in", "self", ".", "ordered_experts", ":", "\n", "                ", "continue", "\n", "\n", "", "feat_paths", "=", "tuple", "(", "[", "root_feat", "/", "rel_name", "for", "rel_name", "in", "rel_names", "]", ")", "\n", "if", "len", "(", "feat_paths", ")", "==", "1", ":", "\n", "                ", "features", "[", "expert", "]", "=", "memcache", "(", "feat_paths", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "# support multiple forms of feature (e.g. max and avg pooling). For", "\n", "# now, we only support direct concatenation", "\n", "                ", "msg", "=", "f\"{expert}: Only direct concatenation of muliple feats is possible\"", "\n", "print", "(", "f\"Concatenating aggregates for {expert}....\"", ")", "\n", "is_concat", "=", "self", ".", "feat_aggregation", "[", "expert", "]", "[", "\"aggregate\"", "]", "==", "\"concat\"", "\n", "self", ".", "log_assert", "(", "is_concat", ",", "msg", "=", "msg", ")", "\n", "axis", "=", "self", ".", "feat_aggregation", "[", "expert", "]", "[", "\"aggregate-axis\"", "]", "\n", "x", "=", "concat_features", ".", "cache_info", "(", ")", "# pylint: disable=no-value-for-parameter", "\n", "print", "(", "f\"concat cache info: {x}\"", ")", "\n", "features_", "=", "concat_features", "(", "feat_paths", ",", "axis", "=", "axis", ")", "\n", "memory_summary", "(", ")", "\n", "\n", "# Make separate feature copies for each split to allow in-place filtering", "\n", "features", "[", "expert", "]", "=", "copy", ".", "deepcopy", "(", "features_", ")", "\n", "", "", "self", ".", "features", "=", "features", "\n", "if", "self", ".", "challenge_mode", ":", "\n", "            ", "self", ".", "load_challenge_text_features", "(", ")", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "split_name", "==", "\"jsfusion\"", ":", "\n", "                ", "self", ".", "restrict_test_captions", "=", "memcache", "(", "\n", "root_feat", "/", "self", ".", "paths", "[", "\"js_test_cap_idx_path\"", "]", ")", "\n", "", "self", ".", "raw_captions", "=", "memcache", "(", "root_feat", "/", "self", ".", "paths", "[", "\"raw_captions_path\"", "]", ")", "\n", "text_feat_path", "=", "root_feat", "/", "self", ".", "paths", "[", "\"text_feat_paths\"", "]", "[", "self", ".", "text_feat", "]", "\n", "self", ".", "text_features", "=", "memcache", "(", "text_feat_path", ")", "\n", "\n", "if", "self", ".", "restrict_train_captions", ":", "\n", "# hash the video names to avoid O(n) lookups in long lists", "\n", "                ", "train_list", "=", "set", "(", "self", ".", "partition_lists", "[", "\"train\"", "]", ")", "\n", "for", "key", ",", "val", "in", "self", ".", "text_features", ".", "items", "(", ")", ":", "\n", "                    ", "if", "key", "not", "in", "train_list", ":", "\n", "                        ", "continue", "\n", "\n", "", "if", "not", "self", ".", "split_name", "==", "\"full-test\"", ":", "\n", "# Note that we do not perform this sanity check for the full-test", "\n", "# split, because the text features in the cached dataset will", "\n", "# already have been cropped to the specified", "\n", "# `resstrict_train_captions`", "\n", "                        ", "expect", "=", "{", "19", ",", "20", "}", "\n", "msg", "=", "f\"expected train text feats as lists with length {expect}\"", "\n", "has_expected_feats", "=", "isinstance", "(", "val", ",", "list", ")", "and", "len", "(", "val", ")", "in", "expect", "\n", "self", ".", "log_assert", "(", "has_expected_feats", ",", "msg", "=", "msg", ")", "\n", "\n", "# restrict to the first N captions (deterministic)", "\n", "", "self", ".", "text_features", "[", "key", "]", "=", "val", "[", ":", "self", ".", "restrict_train_captions", "]", "\n", "", "", "", "self", ".", "summary_stats", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.MSRVTT_dataset.MSRVTT.sanity_checks": [[168, 183], ["MSRVTT_dataset.MSRVTT.log_assert", "len", "MSRVTT_dataset.MSRVTT.query_masks.sum", "len", "len", "ValueError"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.log_assert"], ["", "def", "sanity_checks", "(", "self", ")", ":", "\n", "\n", "        ", "if", "self", ".", "num_test_captions", "==", "20", ":", "\n", "            ", "if", "len", "(", "self", ".", "partition_lists", "[", "\"val\"", "]", ")", "==", "2990", ":", "\n", "                ", "missing", "=", "6", "\n", "", "elif", "len", "(", "self", ".", "partition_lists", "[", "\"val\"", "]", ")", "==", "1000", ":", "\n", "                ", "missing", "=", "2", "\n", "", "elif", "len", "(", "self", ".", "partition_lists", "[", "\"val\"", "]", ")", "==", "497", ":", "\n", "                ", "missing", "=", "0", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"unrecognised test set\"", ")", "\n", "", "found_missing", "=", "self", ".", "query_masks", ".", "size", "-", "self", ".", "query_masks", ".", "sum", "(", ")", "\n", "msg", "=", "f\"Expected {missing} missing queries in MSRVTT, found {found_missing}\"", "\n", "correct_missing", "=", "found_missing", "==", "missing", "\n", "self", ".", "log_assert", "(", "correct_missing", ",", "msg", "=", "msg", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.YouCook2_dataset.YouCook2.dataset_paths": [[14, 54], ["test_splits.items", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["    ", "@", "staticmethod", "\n", "@", "typechecked", "\n", "def", "dataset_paths", "(", ")", "->", "Dict", "[", "str", ",", "Union", "[", "str", ",", "List", "[", "str", "]", ",", "Path", ",", "Dict", "]", "]", ":", "\n", "        ", "subset_paths", "=", "{", "}", "\n", "test_splits", "=", "{", "\n", "\"val\"", ":", "\"val_list.txt\"", ",", "\n", "\"public_server_val\"", ":", "\"public_server_val.txt\"", ",", "\n", "\"public_server_test\"", ":", "\"public_server_test.txt\"", ",", "\n", "}", "\n", "for", "split_name", ",", "fname", "in", "test_splits", ".", "items", "(", ")", ":", "\n", "            ", "subset_paths", "[", "split_name", "]", "=", "{", "\"train\"", ":", "\"train_list.txt\"", ",", "\"val\"", ":", "fname", "}", "\n", "\n", "", "feature_names", "=", "[", "\n", "\"imagenet.senet154.0\"", ",", "\n", "\"scene.densenet161.0\"", ",", "\n", "\"i3d.i3d.0\"", ",", "\n", "\"imagenet.resnext101_32x48d.0\"", ",", "\n", "\"r2p1d.r2p1d-ig65m.0\"", ",", "\n", "\"r2p1d.r2p1d-ig65m-kinetics.0\"", ",", "\n", "\"s3dg.s3dg.0\"", ",", "\n", "\"audio.vggish.0\"", ",", "\n", "]", "\n", "custom_paths", "=", "{", "\n", "\"audio\"", ":", "[", "\"aggregated_audio/vggish-raw.pickle\"", "]", ",", "\n", "\"ocr\"", ":", "[", "\"aggregated_ocr_feats/ocr-w2v.pkl\"", "]", ",", "\n", "}", "\n", "text_feats", "=", "(", "\"w2v\"", ",", "\"openai\"", ")", "\n", "text_feat_paths", "=", "{", "key", ":", "Path", "(", "\"aggregated_text_feats\"", ")", "/", "f\"{key}.pkl\"", "\n", "for", "key", "in", "text_feats", "}", "\n", "challenge_text_feat_paths", "=", "{", "key", ":", "f\"aggregated_text_feats/{key}.pkl\"", "\n", "for", "key", "in", "text_feat_paths", "}", "\n", "feature_info", "=", "{", "\n", "\"custom_paths\"", ":", "custom_paths", ",", "\n", "\"feature_names\"", ":", "feature_names", ",", "\n", "\"subset_list_paths\"", ":", "subset_paths", ",", "\n", "\"text_feat_paths\"", ":", "text_feat_paths", ",", "\n", "\"challenge_text_feat_paths\"", ":", "challenge_text_feat_paths", ",", "\n", "\"raw_captions_path\"", ":", "\"raw-captions.pkl\"", ",", "\n", "}", "\n", "return", "feature_info", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.YouCook2_dataset.YouCook2.load_features": [[55, 93], ["feat_names.update", "feat_names.items", "YouCook2_dataset.YouCook2.visual_feat_paths", "tuple", "YouCook2_dataset.YouCook2.load_challenge_text_features", "zsvision.zs_utils.memcache", "zsvision.zs_utils.memcache", "len", "zsvision.zs_utils.memcache", "print", "zsvision.zs_utils.concat_features.cache_info", "print", "zsvision.zs_utils.concat_features", "utils.memory_summary", "copy.deepcopy", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.visual_feat_paths", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.load_challenge_text_features", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.memory_summary"], ["", "def", "load_features", "(", "self", ")", ":", "\n", "        ", "root_feat", "=", "self", ".", "root_feat", "\n", "feat_names", "=", "{", "key", ":", "self", ".", "visual_feat_paths", "(", "key", ")", "for", "key", "in", "\n", "self", ".", "paths", "[", "\"feature_names\"", "]", "}", "\n", "feat_names", ".", "update", "(", "self", ".", "paths", "[", "\"custom_paths\"", "]", ")", "\n", "features", "=", "{", "}", "\n", "for", "expert", ",", "rel_names", "in", "feat_names", ".", "items", "(", ")", ":", "\n", "            ", "if", "expert", "not", "in", "self", ".", "ordered_experts", ":", "\n", "                ", "continue", "\n", "", "feat_paths", "=", "tuple", "(", "[", "Path", "(", "root_feat", ")", "/", "rel_name", "for", "rel_name", "in", "rel_names", "]", ")", "\n", "if", "len", "(", "feat_paths", ")", "==", "1", ":", "\n", "                ", "features", "[", "expert", "]", "=", "memcache", "(", "feat_paths", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "# support multiple forms of feature (e.g. max and avg pooling). For", "\n", "# now, we only support direct concatenation", "\n", "                ", "msg", "=", "f\"{expert}: Only direct concatenation of muliple feats is possible\"", "\n", "print", "(", "f\"Concatenating aggregates for {expert}....\"", ")", "\n", "assert", "self", ".", "feat_aggregation", "[", "expert", "]", "[", "\"aggregate\"", "]", "==", "\"concat\"", ",", "msg", "\n", "axis", "=", "self", ".", "feat_aggregation", "[", "expert", "]", "[", "\"aggregate-axis\"", "]", "\n", "x", "=", "concat_features", ".", "cache_info", "(", ")", "# pylint: disable=no-value-for-parameter", "\n", "print", "(", "f\"concat cache info: {x}\"", ")", "\n", "features_", "=", "concat_features", "(", "feat_paths", ",", "axis", "=", "axis", ")", "\n", "memory_summary", "(", ")", "\n", "\n", "# Make separate feature copies for each split to allow in-place filtering", "\n", "features", "[", "expert", "]", "=", "copy", ".", "deepcopy", "(", "features_", ")", "\n", "\n", "", "", "self", ".", "features", "=", "features", "\n", "if", "self", ".", "challenge_mode", ":", "\n", "            ", "self", ".", "load_challenge_text_features", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "raw_captions", "=", "memcache", "(", "root_feat", "/", "self", ".", "paths", "[", "\"raw_captions_path\"", "]", ")", "\n", "text_feat_path", "=", "root_feat", "/", "self", ".", "paths", "[", "\"text_feat_paths\"", "]", "[", "self", ".", "text_feat", "]", "\n", "self", ".", "text_features", "=", "memcache", "(", "text_feat_path", ")", "\n", "\n", "# overload video paths, which are structured differently for YouCook2", "\n", "", "self", ".", "video_path_retrieval", "=", "[", "f\"videos/validation/{x}.mp4\"", "\n", "for", "x", "in", "self", ".", "partition_lists", "[", "\"val\"", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.YouCook2_dataset.YouCook2.sanity_checks": [[94, 98], ["None"], "methods", ["None"], ["", "def", "sanity_checks", "(", "self", ")", ":", "\n", "        ", "msg", "=", "(", "f\"Expected to have single test caption for YouCook2, since we assume\"", "\n", "f\"that the captions are fused (but using {self.num_test_captions})\"", ")", "\n", "assert", "self", ".", "num_test_captions", "==", "1", ",", "msg", "\n", "", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.QuerYDSegments_dataset.QuerYDSegments.dataset_paths": [[14, 44], ["test_splits.items", "base.base_dataset.BaseDataset.common_feat_names", "base.base_dataset.BaseDataset.common_feat_names.append", "base.base_dataset.BaseDataset.common_text_feat_paths", "pathlib.Path", "base.base_dataset.BaseDataset.common_text_feat_paths.items"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.common_feat_names", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.common_text_feat_paths", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["    ", "@", "staticmethod", "\n", "@", "typechecked", "\n", "def", "dataset_paths", "(", "training_file", "=", "None", ")", "->", "Dict", "[", "str", ",", "Union", "[", "str", ",", "List", "[", "str", "]", ",", "Path", ",", "Dict", "]", "]", ":", "\n", "        ", "subset_paths", "=", "{", "}", "\n", "test_splits", "=", "{", "\n", "\"val\"", ":", "\"val_list.txt\"", ",", "\n", "\"test\"", ":", "\"test_list.txt\"", ",", "\n", "}", "\n", "for", "split_name", ",", "fname", "in", "test_splits", ".", "items", "(", ")", ":", "\n", "            ", "subset_paths", "[", "split_name", "]", "=", "{", "\"train\"", ":", "\"train_list.txt\"", ",", "\"val\"", ":", "fname", "}", "\n", "\n", "", "feature_names", "=", "BaseDataset", ".", "common_feat_names", "(", ")", "\n", "feature_names", ".", "append", "(", "\"audio.vggish.0\"", ")", "\n", "text_feat_paths", "=", "BaseDataset", ".", "common_text_feat_paths", "(", ")", "\n", "text_feat_paths", "=", "{", "key", ":", "Path", "(", "\"text_embeddings\"", ")", "/", "fname", "\n", "for", "key", ",", "fname", "in", "text_feat_paths", ".", "items", "(", ")", "}", "\n", "challenge_text_feat_paths", "=", "{", "key", ":", "f\"text_embeddings/{key}.pkl\"", "\n", "for", "key", "in", "text_feat_paths", "}", "\n", "custom_paths", "=", "{", "\n", "\"audio\"", ":", "[", "\"aggregated_audio/vggish-raw.hickle\"", "]", ",", "\n", "}", "\n", "feature_info", "=", "{", "\n", "\"custom_paths\"", ":", "custom_paths", ",", "\n", "\"feature_names\"", ":", "feature_names", ",", "\n", "\"subset_list_paths\"", ":", "subset_paths", ",", "\n", "\"text_feat_paths\"", ":", "text_feat_paths", ",", "\n", "\"challenge_text_feat_paths\"", ":", "challenge_text_feat_paths", ",", "\n", "\"raw_captions_path\"", ":", "\"structured-symlinks/split_raw_captions_filtered.pkl\"", ",", "\n", "}", "\n", "return", "feature_info", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.QuerYDSegments_dataset.QuerYDSegments.load_features": [[45, 84], ["feat_names.update", "feat_names.items", "QuerYDSegments_dataset.QuerYDSegments.visual_feat_paths", "tuple", "QuerYDSegments_dataset.QuerYDSegments.load_challenge_text_features", "zsvision.zs_utils.memcache", "zsvision.zs_utils.memcache", "len", "zsvision.zs_utils.memcache", "print", "zsvision.zs_utils.concat_features.cache_info", "print", "zsvision.zs_utils.concat_features", "utils.memory_summary", "copy.deepcopy", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.visual_feat_paths", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.load_challenge_text_features", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.memory_summary"], ["", "def", "load_features", "(", "self", ")", ":", "\n", "        ", "root_feat", "=", "self", ".", "root_feat", "\n", "feat_names", "=", "{", "key", ":", "self", ".", "visual_feat_paths", "(", "key", ")", "for", "key", "in", "\n", "self", ".", "paths", "[", "\"feature_names\"", "]", "}", "\n", "feat_names", ".", "update", "(", "self", ".", "paths", "[", "\"custom_paths\"", "]", ")", "\n", "features", "=", "{", "}", "\n", "for", "expert", ",", "rel_names", "in", "feat_names", ".", "items", "(", ")", ":", "\n", "            ", "if", "expert", "not", "in", "self", ".", "ordered_experts", ":", "\n", "                ", "continue", "\n", "", "feat_paths", "=", "tuple", "(", "[", "Path", "(", "root_feat", ")", "/", "rel_name", "for", "rel_name", "in", "rel_names", "]", ")", "\n", "if", "len", "(", "feat_paths", ")", "==", "1", ":", "\n", "                ", "features", "[", "expert", "]", "=", "memcache", "(", "feat_paths", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "# support multiple forms of feature (e.g. max and avg pooling). For", "\n", "# now, we only support direct concatenation", "\n", "                ", "msg", "=", "f\"{expert}: Only direct concatenation of muliple feats is possible\"", "\n", "print", "(", "f\"Concatenating aggregates for {expert}....\"", ")", "\n", "assert", "self", ".", "feat_aggregation", "[", "expert", "]", "[", "\"aggregate\"", "]", "==", "\"concat\"", ",", "msg", "\n", "axis", "=", "self", ".", "feat_aggregation", "[", "expert", "]", "[", "\"aggregate-axis\"", "]", "\n", "x", "=", "concat_features", ".", "cache_info", "(", ")", "# pylint: disable=no-value-for-parameter", "\n", "print", "(", "f\"concat cache info: {x}\"", ")", "\n", "features_", "=", "concat_features", "(", "feat_paths", ",", "axis", "=", "axis", ")", "\n", "memory_summary", "(", ")", "\n", "\n", "# Make separate feature copies for each split to allow in-place filtering", "\n", "features", "[", "expert", "]", "=", "copy", ".", "deepcopy", "(", "features_", ")", "\n", "\n", "", "", "self", ".", "features", "=", "features", "\n", "if", "self", ".", "challenge_mode", ":", "\n", "            ", "self", ".", "load_challenge_text_features", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "raw_captions", "=", "memcache", "(", "root_feat", "/", "self", ".", "paths", "[", "\"raw_captions_path\"", "]", ")", "\n", "text_feat_path", "=", "root_feat", "/", "self", ".", "paths", "[", "\"text_feat_paths\"", "]", "[", "self", ".", "text_feat", "]", "\n", "self", ".", "text_features", "=", "memcache", "(", "text_feat_path", ")", "\n", "\n", "\n", "# overload video paths", "\n", "", "self", ".", "video_path_retrieval", "=", "[", "f\"videos/{x}.mp4\"", "\n", "for", "x", "in", "self", ".", "partition_lists", "[", "\"val\"", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.QuerYDSegments_dataset.QuerYDSegments.sanity_checks": [[85, 89], ["None"], "methods", ["None"], ["", "def", "sanity_checks", "(", "self", ")", ":", "\n", "        ", "msg", "=", "(", "f\"Expected to have single test caption for QuerYD, since we assume\"", "\n", "f\"that the captions are fused (but using {self.num_test_captions})\"", ")", "\n", "assert", "self", ".", "num_test_captions", "==", "1", ",", "msg", "\n", "", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.data_loaders.ExpertDataLoader.__init__": [[104, 219], ["zsvision.zs_data_structures.HashableOrderedDict", "zsvision.zs_data_structures.HashableDict", "zsvision.zs_data_structures.HashableDict", "dict", "logger.info", "zsvision.zs_data_structures.HashableDict", "logger.info", "dataset_loader.cache_clear", "torch.cuda.empty_cache", "zsvision.zs_utils.memcache.cache_clear", "NotImplementedError", "data_loaders.dataset_loader", "dataset_loader.cache_info", "logger.info", "dataset_loader.get_retrieval_data", "torch.utils.data.DataLoader", "data_loaders.dataset_loader", "dataset_loader.cache_info", "logger.info", "torch.utils.data.DataLoader"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.data_loaders.dataset_loader", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.get_retrieval_data", "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.data_loaders.dataset_loader"], ["    ", "@", "typechecked", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "eval_only", ":", "bool", ",", "\n", "fuse_captions", ":", "bool", ",", "\n", "challenge_mode", ":", "bool", ",", "\n", "use_zeros_for_missing", ":", "bool", ",", "\n", "trn_cat", ":", "int", ",", "\n", "text_dim", ":", "int", ",", "\n", "batch_size", ":", "int", ",", "\n", "num_workers", ":", "int", ",", "\n", "num_test_captions", ":", "int", ",", "\n", "task", ":", "str", ",", "\n", "data_dir", ":", "str", ",", "\n", "text_agg", ":", "str", ",", "\n", "text_feat", ":", "str", ",", "\n", "split_name", ":", "str", ",", "\n", "dataset_name", ":", "str", ",", "\n", "root_feat_folder", ":", "str", ",", "\n", "text_dropout", ":", "float", ",", "\n", "max_tokens", ":", "Dict", "[", "str", ",", "int", "]", ",", "\n", "raw_input_dims", ":", "Dict", "[", "str", ",", "int", "]", ",", "\n", "feat_aggregation", ":", "Dict", "[", "str", ",", "Dict", "]", ",", "\n", "logger", ":", "logging", ".", "Logger", ",", "\n", "spatial_feats", ":", "bool", "=", "False", ",", "\n", "restrict_train_captions", ":", "int", "=", "0", ",", "\n", "drop_last", ":", "bool", "=", "False", ",", "\n", "refresh_lru_cache", ":", "bool", "=", "False", ",", "\n", "cls_partitions", ":", "List", "[", "str", "]", "=", "[", "\"train\"", ",", "\"val\"", ",", "\"tiny\"", ",", "\"challenge\"", "]", ",", "\n", "challenge_test_root_feat_folder", ":", "str", "=", "\"challenge\"", ",", "\n", "distil_params", ":", "Union", "[", "None", ",", "Dict", "]", "=", "None", ",", "\n", "training_file", ":", "Union", "[", "None", ",", "str", "]", "=", "None", ",", "\n", "caption_masks", ":", "Union", "[", "None", ",", "str", "]", "=", "None", ",", "\n", "ce_shared_dim", ":", "Union", "[", "None", ",", "int", "]", "=", "None", ",", "\n", ")", ":", "\n", "\n", "# Ensure that the dictionaries are hashable to allow use of caching", "\n", "        ", "raw_input_dims", "=", "HashableOrderedDict", "(", "raw_input_dims", ")", "\n", "feat_aggregation", "=", "HashableDict", "(", "feat_aggregation", ")", "\n", "if", "distil_params", "is", "not", "None", ":", "\n", "            ", "distil_params", "=", "HashableDict", "(", "distil_params", ")", "\n", "", "max_tokens", "=", "HashableDict", "(", "max_tokens", ")", "\n", "\n", "if", "refresh_lru_cache", ":", "\n", "            ", "logger", ".", "info", "(", "\"Explicitly refreshing dataloader and cuda cache\"", ")", "\n", "dataset_loader", ".", "cache_clear", "(", ")", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "memcache", ".", "cache_clear", "(", ")", "\n", "\n", "", "if", "trn_cat", ":", "\n", "            ", "raise", "NotImplementedError", "(", "f\"Support for trn cat will need to be re-added\"", ")", "\n", "\n", "", "common_kwargs", "=", "dict", "(", "\n", "task", "=", "task", ",", "\n", "logger", "=", "logger", ",", "\n", "data_dir", "=", "data_dir", ",", "\n", "text_dim", "=", "text_dim", ",", "\n", "text_agg", "=", "text_agg", ",", "\n", "eval_only", "=", "eval_only", ",", "\n", "text_feat", "=", "text_feat", ",", "\n", "max_tokens", "=", "max_tokens", ",", "\n", "dataset_name", "=", "dataset_name", ",", "\n", "text_dropout", "=", "text_dropout", ",", "\n", "fuse_captions", "=", "fuse_captions", ",", "\n", "spatial_feats", "=", "spatial_feats", ",", "\n", "split_name", "=", "split_name", ",", "\n", "challenge_mode", "=", "challenge_mode", ",", "\n", "root_feat_folder", "=", "root_feat_folder", ",", "\n", "use_zeros_for_missing", "=", "use_zeros_for_missing", ",", "\n", "challenge_test_root_feat_folder", "=", "challenge_test_root_feat_folder", ",", "\n", "num_test_captions", "=", "num_test_captions", ",", "\n", "raw_input_dims", "=", "raw_input_dims", ",", "\n", "feat_aggregation", "=", "feat_aggregation", ",", "\n", "restrict_train_captions", "=", "restrict_train_captions", ",", "\n", "distil_params", "=", "distil_params", ",", "\n", "training_file", "=", "training_file", ",", "\n", "caption_masks", "=", "caption_masks", ",", "\n", "ce_shared_dim", "=", "ce_shared_dim", ",", "\n", ")", "\n", "\n", "if", "\"retrieval\"", "in", "task", ":", "\n", "            ", "dataset", "=", "dataset_loader", "(", "cls_partition", "=", "\"train\"", ",", "**", "common_kwargs", ")", "\n", "x", "=", "dataset_loader", ".", "cache_info", "(", ")", "# pylint: disable=no-value-for-parameter", "\n", "logger", ".", "info", "(", "f\"cache info {x}\"", ")", "\n", "self", ".", "dataloaders", "=", "{", "\"dataset\"", ":", "dataset", "}", "\n", "self", ".", "dataloaders", "[", "\"retrieval\"", "]", "=", "dataset", ".", "get_retrieval_data", "(", ")", "\n", "if", "not", "eval_only", ":", "\n", "                ", "train_loader", "=", "DataLoader", "(", "\n", "dataset", "=", "dataset", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", "collate_fn", "=", "dataset", ".", "collate_data", ",", "\n", "drop_last", "=", "drop_last", ",", "\n", "shuffle", "=", "True", ",", "\n", ")", "\n", "self", ".", "dataloaders", "[", "\"train\"", "]", "=", "train_loader", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "dataloaders", "=", "{", "}", "\n", "for", "cls_partition", "in", "cls_partitions", ":", "\n", "                ", "cls_dataset", "=", "dataset_loader", "(", "cls_partition", "=", "cls_partition", ",", "**", "common_kwargs", ")", "\n", "x", "=", "dataset_loader", ".", "cache_info", "(", ")", "# pylint: disable=no-value-for-parameter", "\n", "logger", ".", "info", "(", "f\"cache info [{cls_partition}] {x}\"", ")", "\n", "loader", "=", "DataLoader", "(", "\n", "dataset", "=", "cls_dataset", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", "collate_fn", "=", "cls_dataset", ".", "collate_data", ",", "\n", "drop_last", "=", "False", ",", "\n", "shuffle", "=", "False", ",", "\n", ")", "\n", "self", ".", "dataloaders", "[", "cls_partition", "]", "=", "loader", "\n", "\n", "", "", "logger", ".", "info", "(", "f\"Loading data loaders with {num_workers} workers\"", ")", "\n", "self", ".", "num_test_captions", "=", "num_test_captions", "\n", "self", ".", "dataset_name", "=", "dataset_name", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.data_loaders.ExpertDataLoader.__getitem__": [[220, 222], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "key", ")", ":", "\n", "        ", "return", "self", ".", "dataloaders", "[", "key", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.data_loaders.dataset_loader": [[22, 100], ["functools.lru_cache", "print", "dict", "data_loader.MSRVTT_dataset.MSRVTT", "pathlib.Path", "data_loader.LSMDC_dataset.LSMDC", "data_loader.MSVD_dataset.MSVD", "data_loader.DiDeMo_dataset.DiDeMo", "data_loader.ActivityNet_dataset.ActivityNet", "data_loader.YouCook2_dataset.YouCook2", "data_loader.QuerYD_dataset.QuerYD", "data_loader.QuerYDSegments_dataset.QuerYDSegments", "data_loader.VaTeX_dataset.VaTeX"], "function", ["None"], ["@", "functools", ".", "lru_cache", "(", "maxsize", "=", "64", ",", "typed", "=", "False", ")", "\n", "def", "dataset_loader", "(", "\n", "text_dropout", ":", "float", ",", "\n", "fuse_captions", ":", "bool", ",", "\n", "spatial_feats", ":", "bool", ",", "\n", "use_zeros_for_missing", ":", "bool", ",", "\n", "challenge_mode", ":", "bool", ",", "\n", "eval_only", ":", "bool", ",", "\n", "task", ":", "str", ",", "\n", "data_dir", ":", "str", ",", "\n", "text_agg", ":", "str", ",", "\n", "text_feat", ":", "str", ",", "\n", "split_name", ":", "str", ",", "\n", "dataset_name", ":", "str", ",", "\n", "cls_partition", ":", "str", ",", "\n", "root_feat_folder", ":", "str", ",", "\n", "challenge_test_root_feat_folder", ":", "str", ",", "\n", "text_dim", ":", "int", ",", "\n", "num_test_captions", ":", "int", ",", "\n", "restrict_train_captions", ":", "int", ",", "\n", "logger", ":", "logging", ".", "Logger", ",", "\n", "max_tokens", ":", "Dict", "[", "str", ",", "int", "]", ",", "\n", "raw_input_dims", ":", "HashableOrderedDict", ",", "\n", "feat_aggregation", ":", "HashableDict", ",", "\n", "distil_params", ":", "Union", "[", "None", ",", "Dict", "]", ",", "\n", "training_file", ":", "Union", "[", "None", ",", "str", "]", ",", "\n", "caption_masks", ":", "Union", "[", "None", ",", "str", "]", ",", "\n", "ce_shared_dim", ":", "Union", "[", "None", ",", "int", "]", ",", "\n", "**", "args", ",", "\n", ")", ":", "\n", "    ", "print", "(", "f\"refreshing cache for {dataset_name} data loader [{split_name}]\"", ")", "\n", "kwargs", "=", "dict", "(", "\n", "task", "=", "task", ",", "\n", "data_dir", "=", "Path", "(", "data_dir", ")", ",", "\n", "text_dim", "=", "text_dim", ",", "\n", "logger", "=", "logger", ",", "\n", "eval_only", "=", "eval_only", ",", "\n", "text_agg", "=", "text_agg", ",", "\n", "text_feat", "=", "text_feat", ",", "\n", "max_tokens", "=", "max_tokens", ",", "\n", "split_name", "=", "split_name", ",", "\n", "cls_partition", "=", "cls_partition", ",", "\n", "spatial_feats", "=", "spatial_feats", ",", "\n", "text_dropout", "=", "text_dropout", ",", "\n", "fuse_captions", "=", "fuse_captions", ",", "\n", "raw_input_dims", "=", "raw_input_dims", ",", "\n", "challenge_mode", "=", "challenge_mode", ",", "\n", "root_feat_folder", "=", "root_feat_folder", ",", "\n", "feat_aggregation", "=", "feat_aggregation", ",", "\n", "num_test_captions", "=", "num_test_captions", ",", "\n", "use_zeros_for_missing", "=", "use_zeros_for_missing", ",", "\n", "restrict_train_captions", "=", "restrict_train_captions", ",", "\n", "challenge_test_root_feat_folder", "=", "challenge_test_root_feat_folder", ",", "\n", "distil_params", "=", "distil_params", ",", "\n", "training_file", "=", "training_file", ",", "\n", "caption_masks", "=", "caption_masks", ",", "\n", "ce_shared_dim", "=", "ce_shared_dim", ",", "\n", "**", "args", ",", "\n", ")", "\n", "if", "dataset_name", "==", "\"MSRVTT\"", ":", "\n", "        ", "dataset", "=", "MSRVTT", "(", "**", "kwargs", ")", "\n", "", "elif", "dataset_name", "==", "\"LSMDC\"", ":", "\n", "        ", "dataset", "=", "LSMDC", "(", "**", "kwargs", ")", "\n", "", "elif", "dataset_name", "==", "\"MSVD\"", ":", "\n", "        ", "dataset", "=", "MSVD", "(", "**", "kwargs", ")", "\n", "", "elif", "dataset_name", "==", "\"DiDeMo\"", ":", "\n", "        ", "dataset", "=", "DiDeMo", "(", "**", "kwargs", ")", "\n", "", "elif", "dataset_name", "==", "\"ActivityNet\"", ":", "\n", "        ", "dataset", "=", "ActivityNet", "(", "**", "kwargs", ")", "\n", "", "elif", "dataset_name", "==", "\"YouCook2\"", ":", "\n", "        ", "dataset", "=", "YouCook2", "(", "**", "kwargs", ")", "\n", "", "elif", "dataset_name", "==", "\"QuerYD\"", ":", "\n", "        ", "dataset", "=", "QuerYD", "(", "**", "kwargs", ")", "\n", "", "elif", "dataset_name", "==", "\"QuerYDSegments\"", ":", "\n", "        ", "dataset", "=", "QuerYDSegments", "(", "**", "kwargs", ")", "\n", "", "elif", "dataset_name", "==", "\"VaTeX\"", ":", "\n", "        ", "dataset", "=", "VaTeX", "(", "**", "kwargs", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.VaTeX_dataset.VaTeX.dataset_paths": [[16, 50], ["test_splits.items", "base.base_dataset.BaseDataset.common_feat_names", "base.base_dataset.BaseDataset.common_text_feat_paths", "pathlib.Path", "base.base_dataset.BaseDataset.common_text_feat_paths.items", "pathlib.Path", "base.base_dataset.BaseDataset.common_text_feat_paths.items"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.common_feat_names", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.common_text_feat_paths", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["    ", "@", "staticmethod", "\n", "@", "typechecked", "\n", "def", "dataset_paths", "(", "training_file", "=", "-", "1", ")", "->", "Dict", "[", "str", ",", "Union", "[", "Path", ",", "str", ",", "Dict", ",", "List", "[", "str", "]", "]", "]", ":", "\n", "        ", "subset_paths", "=", "{", "}", "\n", "test_splits", "=", "{", "\n", "\"full-val\"", ":", "\"val_list_split1.txt\"", ",", "\n", "\"full-test\"", ":", "\"val_list_split2.txt\"", ",", "\n", "}", "\n", "for", "split_name", ",", "fname", "in", "test_splits", ".", "items", "(", ")", ":", "\n", "            ", "subset_paths", "[", "split_name", "]", "=", "{", "\"train\"", ":", "\"train_list.txt\"", ",", "\"val\"", ":", "fname", "}", "\n", "\n", "", "feature_names", "=", "BaseDataset", ".", "common_feat_names", "(", ")", "\n", "custom_paths", "=", "{", "\n", "\"audio\"", ":", "[", "\"aggregated_audio/vggish-raw.hickle\"", "]", ",", "\n", "}", "\n", "text_feat_paths", "=", "BaseDataset", ".", "common_text_feat_paths", "(", ")", "\n", "\n", "text_feat_dir", "=", "Path", "(", "\"text-embeddings\"", ")", "\n", "\n", "text_feat_paths", "=", "{", "key", ":", "text_feat_dir", "/", "fname", "\n", "for", "key", ",", "fname", "in", "text_feat_paths", ".", "items", "(", ")", "}", "\n", "challenge_text_feat_paths", "=", "{", "\n", "key", ":", "Path", "(", "\"aggregated_text_feats\"", ")", "/", "f\"{key}{fname.suffix}\"", "\n", "for", "key", ",", "fname", "in", "text_feat_paths", ".", "items", "(", ")", "\n", "}", "\n", "feature_info", "=", "{", "\n", "\"custom_paths\"", ":", "custom_paths", ",", "\n", "\"feature_names\"", ":", "feature_names", ",", "\n", "\"subset_list_paths\"", ":", "subset_paths", ",", "\n", "\"text_feat_paths\"", ":", "text_feat_paths", ",", "\n", "\"challenge_text_feat_paths\"", ":", "challenge_text_feat_paths", ",", "\n", "\"raw_captions_path\"", ":", "\"raw-captions.pkl\"", ",", "\n", "}", "\n", "return", "feature_info", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.VaTeX_dataset.VaTeX.load_features": [[51, 94], ["pathlib.Path", "feat_names.update", "feat_names.items", "list", "enumerate", "VaTeX_dataset.VaTeX.visual_feat_paths", "tuple", "VaTeX_dataset.VaTeX.load_challenge_text_features", "zsvision.zs_utils.memcache", "zsvision.zs_utils.memcache", "map", "zsvision.zs_utils.memcache", "len", "zsvision.zs_utils.memcache", "print", "zsvision.zs_utils.concat_features.cache_info", "print", "zsvision.zs_utils.concat_features", "utils.memory_summary", "copy.deepcopy"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.visual_feat_paths", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.load_challenge_text_features", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.memory_summary"], ["", "def", "load_features", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "distil_params", "is", "not", "None", ":", "\n", "            ", "self", ".", "distil_features", "=", "{", "}", "\n", "d_base_path", "=", "self", ".", "distil_params", "[", "'base_path'", "]", "\n", "\n", "teachers", "=", "list", "(", "map", "(", "lambda", "x", ":", "d_base_path", "+", "x", ",", "self", ".", "distil_params", "[", "'teachers'", "]", ")", ")", "\n", "\n", "for", "i", ",", "f_name", "in", "enumerate", "(", "teachers", ")", ":", "\n", "                ", "self", ".", "distil_features", "[", "i", "]", "=", "memcache", "(", "f_name", ")", "\n", "\n", "", "", "root_feat", "=", "Path", "(", "self", ".", "root_feat", ")", "\n", "feat_names", "=", "{", "key", ":", "self", ".", "visual_feat_paths", "(", "key", ")", "for", "key", "in", "\n", "self", ".", "paths", "[", "\"feature_names\"", "]", "}", "\n", "feat_names", ".", "update", "(", "self", ".", "paths", "[", "\"custom_paths\"", "]", ")", "\n", "features", "=", "{", "}", "\n", "for", "expert", ",", "rel_names", "in", "feat_names", ".", "items", "(", ")", ":", "\n", "            ", "if", "expert", "not", "in", "self", ".", "ordered_experts", ":", "\n", "                ", "continue", "\n", "", "feat_paths", "=", "tuple", "(", "[", "root_feat", "/", "rel_name", "for", "rel_name", "in", "rel_names", "]", ")", "\n", "if", "len", "(", "feat_paths", ")", "==", "1", ":", "\n", "                ", "features", "[", "expert", "]", "=", "memcache", "(", "feat_paths", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "# support multiple forms of feature (e.g. max and avg pooling). For", "\n", "# now, we only support direct concatenation", "\n", "                ", "msg", "=", "f\"{expert}: Only direct concatenation of muliple feats is possible\"", "\n", "print", "(", "f\"Concatenating aggregates for {expert}....\"", ")", "\n", "assert", "self", ".", "feat_aggregation", "[", "expert", "]", "[", "\"aggregate\"", "]", "==", "\"concat\"", ",", "msg", "\n", "axis", "=", "self", ".", "feat_aggregation", "[", "expert", "]", "[", "\"aggregate-axis\"", "]", "\n", "x", "=", "concat_features", ".", "cache_info", "(", ")", "# pylint: disable=no-value-for-parameter", "\n", "print", "(", "f\"concat cache info: {x}\"", ")", "\n", "features_", "=", "concat_features", "(", "feat_paths", ",", "axis", "=", "axis", ")", "\n", "memory_summary", "(", ")", "\n", "\n", "# Make separate feature copies for each split to allow in-place filtering", "\n", "features", "[", "expert", "]", "=", "copy", ".", "deepcopy", "(", "features_", ")", "\n", "\n", "", "", "self", ".", "features", "=", "features", "\n", "if", "self", ".", "challenge_mode", ":", "\n", "            ", "self", ".", "load_challenge_text_features", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "raw_captions", "=", "memcache", "(", "root_feat", "/", "self", ".", "paths", "[", "\"raw_captions_path\"", "]", ")", "\n", "text_feat_path", "=", "root_feat", "/", "self", ".", "paths", "[", "\"text_feat_paths\"", "]", "[", "self", ".", "text_feat", "]", "\n", "self", ".", "text_features", "=", "memcache", "(", "text_feat_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.VaTeX_dataset.VaTeX.sanity_checks": [[95, 99], ["None"], "methods", ["None"], ["", "", "def", "sanity_checks", "(", "self", ")", ":", "\n", "        ", "msg", "=", "(", "f\"Expected to have single test caption for VaTeX, since we assume \"", "\n", "f\"that the captions are fused (but using {self.num_test_captions})\"", ")", "\n", "assert", "self", ".", "num_test_captions", "==", "10", ",", "msg", "\n", "", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.DiDeMo_dataset.DiDeMo.dataset_paths": [[14, 57], ["test_splits.items", "base.base_dataset.BaseDataset.common_feat_names", "base.base_dataset.BaseDataset.common_text_feat_paths", "pathlib.Path", "base.base_dataset.BaseDataset.common_text_feat_paths.items", "pathlib.Path", "base.base_dataset.BaseDataset.common_text_feat_paths.items"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.common_feat_names", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.common_text_feat_paths", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["    ", "@", "staticmethod", "\n", "@", "typechecked", "\n", "def", "dataset_paths", "(", "training_file", "=", "None", ")", "->", "Dict", "[", "str", ",", "Union", "[", "Path", ",", "str", ",", "Dict", ",", "List", "[", "str", "]", "]", "]", ":", "\n", "        ", "subset_paths", "=", "{", "}", "\n", "test_splits", "=", "{", "\n", "\"val\"", ":", "\"val_list.txt\"", ",", "\n", "\"test\"", ":", "\"test_list.txt\"", ",", "\n", "\"public_server_val\"", ":", "\"public_server_val.txt\"", ",", "\n", "\"public_server_test\"", ":", "\"public_server_test.txt\"", ",", "\n", "}", "\n", "for", "split_name", ",", "fname", "in", "test_splits", ".", "items", "(", ")", ":", "\n", "            ", "if", "training_file", "is", "None", ":", "\n", "                ", "subset_paths", "[", "split_name", "]", "=", "{", "\"train\"", ":", "\"train_list.txt\"", ",", "\"val\"", ":", "fname", "}", "\n", "", "else", ":", "\n", "                ", "subset_paths", "[", "split_name", "]", "=", "{", "\"train\"", ":", "training_file", ",", "\"val\"", ":", "fname", "}", "\n", "\n", "", "", "feature_names", "=", "BaseDataset", ".", "common_feat_names", "(", ")", "\n", "custom_paths", "=", "{", "\n", "\"audio\"", ":", "[", "\"aggregated_audio_feats/vggish-audio-raw.pickle\"", "]", ",", "\n", "\"ocr\"", ":", "[", "\"aggregated_ocr_feats/ocr-feats.pkl\"", "]", ",", "\n", "\"speech\"", ":", "[", "\"aggregated_speech_feats/stt_w2v.pickle\"", "]", ",", "\n", "\"face\"", ":", "[", "\"aggregated_facefeats_25fps_256px_stride1/face-avg.pickle\"", "]", ",", "\n", "}", "\n", "\n", "text_feat_paths", "=", "BaseDataset", ".", "common_text_feat_paths", "(", ")", "\n", "# include non-standard text features", "\n", "text_feat_paths", "[", "\"openai\"", "]", "=", "\"openai-feats.pkl\"", "\n", "text_feat_paths", "=", "{", "key", ":", "Path", "(", "\"aggregated_text_feats\"", ")", "/", "fname", "\n", "for", "key", ",", "fname", "in", "text_feat_paths", ".", "items", "(", ")", "}", "\n", "\n", "challenge_text_feat_paths", "=", "{", "\n", "key", ":", "Path", "(", "\"aggregated_text_feats\"", ")", "/", "f\"{key}{fname.suffix}\"", "\n", "for", "key", ",", "fname", "in", "text_feat_paths", ".", "items", "(", ")", "\n", "}", "\n", "feature_info", "=", "{", "\n", "\"custom_paths\"", ":", "custom_paths", ",", "\n", "\"feature_names\"", ":", "feature_names", ",", "\n", "\"subset_list_paths\"", ":", "subset_paths", ",", "\n", "\"text_feat_paths\"", ":", "text_feat_paths", ",", "\n", "\"challenge_text_feat_paths\"", ":", "challenge_text_feat_paths", ",", "\n", "\"raw_captions_path\"", ":", "\"raw-captions.pkl\"", ",", "\n", "}", "\n", "return", "feature_info", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.DiDeMo_dataset.DiDeMo.load_features": [[58, 101], ["pathlib.Path", "feat_names.update", "feat_names.items", "list", "enumerate", "DiDeMo_dataset.DiDeMo.visual_feat_paths", "tuple", "DiDeMo_dataset.DiDeMo.load_challenge_text_features", "zsvision.zs_utils.memcache", "zsvision.zs_utils.memcache", "map", "zsvision.zs_utils.memcache", "len", "zsvision.zs_utils.memcache", "print", "zsvision.zs_utils.concat_features.cache_info", "print", "zsvision.zs_utils.concat_features", "utils.memory_summary", "copy.deepcopy", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.visual_feat_paths", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.load_challenge_text_features", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.memory_summary"], ["", "def", "load_features", "(", "self", ")", ":", "\n", "        ", "root_feat", "=", "Path", "(", "self", ".", "root_feat", ")", "\n", "if", "self", ".", "distil_params", "is", "not", "None", ":", "\n", "            ", "self", ".", "distil_features", "=", "{", "}", "\n", "d_base_path", "=", "self", ".", "distil_params", "[", "'base_path'", "]", "\n", "\n", "teachers", "=", "list", "(", "map", "(", "lambda", "x", ":", "root_feat", "/", "Path", "(", "d_base_path", "+", "x", ")", ",", "self", ".", "distil_params", "[", "'teachers'", "]", ")", ")", "\n", "\n", "for", "i", ",", "f_name", "in", "enumerate", "(", "teachers", ")", ":", "\n", "                ", "self", ".", "distil_features", "[", "i", "]", "=", "memcache", "(", "f_name", ")", "\n", "\n", "", "", "feat_names", "=", "{", "key", ":", "self", ".", "visual_feat_paths", "(", "key", ")", "for", "key", "in", "\n", "self", ".", "paths", "[", "\"feature_names\"", "]", "}", "\n", "feat_names", ".", "update", "(", "self", ".", "paths", "[", "\"custom_paths\"", "]", ")", "\n", "features", "=", "{", "}", "\n", "for", "expert", ",", "rel_names", "in", "feat_names", ".", "items", "(", ")", ":", "\n", "            ", "if", "expert", "not", "in", "self", ".", "ordered_experts", ":", "\n", "                ", "continue", "\n", "", "feat_paths", "=", "tuple", "(", "[", "root_feat", "/", "rel_name", "for", "rel_name", "in", "rel_names", "]", ")", "\n", "if", "len", "(", "feat_paths", ")", "==", "1", ":", "\n", "                ", "features", "[", "expert", "]", "=", "memcache", "(", "feat_paths", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "# support multiple forms of feature (e.g. max and avg pooling). For", "\n", "# now, we only support direct concatenation", "\n", "                ", "msg", "=", "f\"{expert}: Only direct concatenation of muliple feats is possible\"", "\n", "print", "(", "f\"Concatenating aggregates for {expert}....\"", ")", "\n", "assert", "self", ".", "feat_aggregation", "[", "expert", "]", "[", "\"aggregate\"", "]", "==", "\"concat\"", ",", "msg", "\n", "axis", "=", "self", ".", "feat_aggregation", "[", "expert", "]", "[", "\"aggregate-axis\"", "]", "\n", "x", "=", "concat_features", ".", "cache_info", "(", ")", "# pylint: disable=no-value-for-parameter", "\n", "print", "(", "f\"concat cache info: {x}\"", ")", "\n", "features_", "=", "concat_features", "(", "feat_paths", ",", "axis", "=", "axis", ")", "\n", "memory_summary", "(", ")", "\n", "\n", "# Make separate feature copies for each split to allow in-place filtering", "\n", "features", "[", "expert", "]", "=", "copy", ".", "deepcopy", "(", "features_", ")", "\n", "\n", "", "", "self", ".", "features", "=", "features", "\n", "if", "self", ".", "challenge_mode", ":", "\n", "            ", "self", ".", "load_challenge_text_features", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "raw_captions", "=", "memcache", "(", "root_feat", "/", "self", ".", "paths", "[", "\"raw_captions_path\"", "]", ")", "\n", "text_feat_path", "=", "root_feat", "/", "self", ".", "paths", "[", "\"text_feat_paths\"", "]", "[", "self", ".", "text_feat", "]", "\n", "self", ".", "text_features", "=", "memcache", "(", "text_feat_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.DiDeMo_dataset.DiDeMo.sanity_checks": [[102, 106], ["None"], "methods", ["None"], ["", "", "def", "sanity_checks", "(", "self", ")", ":", "\n", "        ", "msg", "=", "(", "f\"Expected to have single test caption for DiDemo, since we assume\"", "\n", "f\"that the captions are fused (but using {self.num_test_captions})\"", ")", "\n", "assert", "self", ".", "num_test_captions", "==", "1", ",", "msg", "\n", "", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.ActivityNet_dataset.ActivityNet.dataset_paths": [[14, 61], ["test_splits.items", "base.base_dataset.BaseDataset.common_feat_names", "base.base_dataset.BaseDataset.common_text_feat_paths", "pathlib.Path", "base.base_dataset.BaseDataset.common_text_feat_paths.items", "text_feat_names.items"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.common_feat_names", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.common_text_feat_paths", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["    ", "@", "staticmethod", "\n", "@", "typechecked", "\n", "def", "dataset_paths", "(", "training_file", "=", "None", ")", "->", "Dict", "[", "str", ",", "Union", "[", "str", ",", "List", "[", "str", "]", ",", "Path", ",", "Dict", "]", "]", ":", "\n", "        ", "subset_paths", "=", "{", "}", "\n", "test_splits", "=", "{", "\n", "\"val1\"", ":", "\"val_1_list.txt\"", ",", "\n", "\"val\"", ":", "\"val_list.txt\"", ",", "\n", "\"public_server_val\"", ":", "\"public_server_val.txt\"", ",", "\n", "\"public_server_test\"", ":", "\"public_server_test.txt\"", ",", "\n", "}", "\n", "for", "split_name", ",", "fname", "in", "test_splits", ".", "items", "(", ")", ":", "\n", "            ", "if", "training_file", "is", "None", ":", "\n", "                ", "subset_paths", "[", "split_name", "]", "=", "{", "\"train\"", ":", "\"train_list.txt\"", ",", "\"val\"", ":", "fname", "}", "\n", "", "else", ":", "\n", "                ", "subset_paths", "[", "split_name", "]", "=", "{", "\"train\"", ":", "training_file", ",", "\"val\"", ":", "fname", "}", "\n", "\n", "\n", "", "", "feature_names", "=", "BaseDataset", ".", "common_feat_names", "(", ")", "\n", "custom_paths", "=", "{", "\n", "\"audio\"", ":", "[", "\"aggregated_audio/vggish-audio-raw.pickle\"", "]", ",", "\n", "\"speech\"", ":", "[", "\"aggregated_speech/goog_w2v-speech-raw.pickle\"", "]", ",", "\n", "\"ocr\"", ":", "[", "\"aggregated_ocr_feats/ocr-w2v.pkl\"", "]", ",", "\n", "\"face\"", ":", "[", "\"aggregated_facefeats_25fps_256px_stride1/face-avg.pickle\"", "]", ",", "\n", "}", "\n", "text_feat_paths", "=", "BaseDataset", ".", "common_text_feat_paths", "(", ")", "\n", "text_feat_dir", "=", "Path", "(", "\"aggregated_text_feats\"", ")", "\n", "\n", "text_feat_paths", "=", "{", "key", ":", "text_feat_dir", "/", "fname", "\n", "for", "key", ",", "fname", "in", "text_feat_paths", ".", "items", "(", ")", "}", "\n", "challenge_text_feat_paths", "=", "{", "}", "\n", "# include non-standard text features", "\n", "for", "text_feat", "in", "(", "\"openai\"", ",", ")", ":", "\n", "            ", "text_feat_names", "=", "{", "key", ":", "f\"{text_feat}-{key}\"", "\n", "for", "key", "in", "{", "\"train\"", ",", "\"val1\"", "}", "}", "\n", "text_feat_paths", "[", "text_feat", "]", "=", "{", "key", ":", "f\"aggregated_text_feats/{val}.pkl\"", "\n", "for", "key", ",", "val", "in", "text_feat_names", ".", "items", "(", ")", "}", "\n", "challenge_text_feat_paths", "[", "text_feat", "]", "=", "f\"aggregated_text_feats/{text_feat}.pkl\"", "\n", "", "feature_info", "=", "{", "\n", "\"custom_paths\"", ":", "custom_paths", ",", "\n", "\"feature_names\"", ":", "feature_names", ",", "\n", "\"subset_list_paths\"", ":", "subset_paths", ",", "\n", "\"text_feat_paths\"", ":", "text_feat_paths", ",", "\n", "\"challenge_text_feat_paths\"", ":", "challenge_text_feat_paths", ",", "\n", "\"raw_captions_path\"", ":", "\"raw-captions-train-val_1.pkl\"", ",", "\n", "}", "\n", "return", "feature_info", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.ActivityNet_dataset.ActivityNet.load_features": [[62, 113], ["feat_names.update", "feat_names.items", "list", "enumerate", "ActivityNet_dataset.ActivityNet.visual_feat_paths", "tuple", "ActivityNet_dataset.ActivityNet.load_challenge_text_features", "isinstance", "zsvision.zs_utils.memcache", "map", "zsvision.zs_utils.memcache", "len", "zsvision.zs_utils.memcache", "print", "zsvision.zs_utils.concat_features.cache_info", "print", "zsvision.zs_utils.concat_features", "utils.memory_summary", "copy.deepcopy", "zsvision.zs_utils.memcache", "zsvision.zs_utils.memcache.update", "isinstance", "zsvision.zs_utils.memcache", "zsvision.zs_utils.memcache", "TypeError", "pathlib.Path", "pathlib.Path", "type"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.visual_feat_paths", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.load_challenge_text_features", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.memory_summary", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update"], ["", "def", "load_features", "(", "self", ")", ":", "\n", "        ", "root_feat", "=", "self", ".", "root_feat", "\n", "if", "self", ".", "distil_params", "is", "not", "None", ":", "\n", "            ", "self", ".", "distil_features", "=", "{", "}", "\n", "d_base_path", "=", "self", ".", "distil_params", "[", "'base_path'", "]", "\n", "\n", "teachers", "=", "list", "(", "map", "(", "lambda", "x", ":", "root_feat", "/", "Path", "(", "d_base_path", "+", "x", ")", ",", "self", ".", "distil_params", "[", "'teachers'", "]", ")", ")", "\n", "\n", "for", "i", ",", "f_name", "in", "enumerate", "(", "teachers", ")", ":", "\n", "                ", "self", ".", "distil_features", "[", "i", "]", "=", "memcache", "(", "f_name", ")", "\n", "\n", "", "", "feat_names", "=", "{", "key", ":", "self", ".", "visual_feat_paths", "(", "key", ")", "for", "key", "in", "\n", "self", ".", "paths", "[", "\"feature_names\"", "]", "}", "\n", "feat_names", ".", "update", "(", "self", ".", "paths", "[", "\"custom_paths\"", "]", ")", "\n", "features", "=", "{", "}", "\n", "for", "expert", ",", "rel_names", "in", "feat_names", ".", "items", "(", ")", ":", "\n", "            ", "if", "expert", "not", "in", "self", ".", "ordered_experts", ":", "\n", "                ", "continue", "\n", "", "feat_paths", "=", "tuple", "(", "[", "Path", "(", "root_feat", ")", "/", "rel_name", "for", "rel_name", "in", "rel_names", "]", ")", "\n", "if", "len", "(", "feat_paths", ")", "==", "1", ":", "\n", "                ", "features", "[", "expert", "]", "=", "memcache", "(", "feat_paths", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "# support multiple forms of feature (e.g. max and avg pooling). For", "\n", "# now, we only support direct concatenation", "\n", "                ", "msg", "=", "f\"{expert}: Only direct concatenation of muliple feats is possible\"", "\n", "print", "(", "f\"Concatenating aggregates for {expert}....\"", ")", "\n", "assert", "self", ".", "feat_aggregation", "[", "expert", "]", "[", "\"aggregate\"", "]", "==", "\"concat\"", ",", "msg", "\n", "axis", "=", "self", ".", "feat_aggregation", "[", "expert", "]", "[", "\"aggregate-axis\"", "]", "\n", "x", "=", "concat_features", ".", "cache_info", "(", ")", "# pylint: disable=no-value-for-parameter", "\n", "print", "(", "f\"concat cache info: {x}\"", ")", "\n", "features_", "=", "concat_features", "(", "feat_paths", ",", "axis", "=", "axis", ")", "\n", "memory_summary", "(", ")", "\n", "\n", "# Make separate feature copies for each split to allow in-place filtering", "\n", "features", "[", "expert", "]", "=", "copy", ".", "deepcopy", "(", "features_", ")", "\n", "\n", "", "", "self", ".", "features", "=", "features", "\n", "if", "self", ".", "challenge_mode", ":", "\n", "            ", "self", ".", "load_challenge_text_features", "(", ")", "\n", "", "else", ":", "\n", "            ", "text_feat_paths", "=", "self", ".", "paths", "[", "\"text_feat_paths\"", "]", "[", "self", ".", "text_feat", "]", "\n", "if", "isinstance", "(", "text_feat_paths", ",", "dict", ")", ":", "\n", "                ", "text_features", "=", "memcache", "(", "root_feat", "/", "text_feat_paths", "[", "\"train\"", "]", ")", "\n", "text_features", ".", "update", "(", "memcache", "(", "\n", "root_feat", "/", "text_feat_paths", "[", "self", ".", "split_name", "]", ")", ")", "\n", "", "elif", "isinstance", "(", "text_feat_paths", ",", "(", "Path", ",", "str", ")", ")", ":", "\n", "                ", "text_features", "=", "memcache", "(", "root_feat", "/", "text_feat_paths", ")", "\n", "", "else", ":", "\n", "                ", "raise", "TypeError", "(", "f\"Unexpected type {type(text_feat_paths)}\"", ")", "\n", "", "self", ".", "text_features", "=", "text_features", "\n", "self", ".", "raw_captions", "=", "memcache", "(", "root_feat", "/", "self", ".", "paths", "[", "\"raw_captions_path\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.ActivityNet_dataset.ActivityNet.sanity_checks": [[114, 118], ["None"], "methods", ["None"], ["", "", "def", "sanity_checks", "(", "self", ")", ":", "\n", "        ", "msg", "=", "(", "f\"Expected to have single test caption for ANet, since we assume\"", "\n", "f\"that the captions are fused (but using {self.num_test_captions})\"", ")", "\n", "assert", "self", ".", "num_test_captions", "==", "1", ",", "msg", "\n", "", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.QuerYD_dataset.QuerYD.dataset_paths": [[14, 44], ["test_splits.items", "base.base_dataset.BaseDataset.common_feat_names", "base.base_dataset.BaseDataset.common_feat_names.append", "base.base_dataset.BaseDataset.common_text_feat_paths", "pathlib.Path", "base.base_dataset.BaseDataset.common_text_feat_paths.items"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.common_feat_names", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.common_text_feat_paths", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["    ", "@", "staticmethod", "\n", "@", "typechecked", "\n", "def", "dataset_paths", "(", "training_file", "=", "None", ")", "->", "Dict", "[", "str", ",", "Union", "[", "str", ",", "List", "[", "str", "]", ",", "Path", ",", "Dict", "]", "]", ":", "\n", "        ", "subset_paths", "=", "{", "}", "\n", "test_splits", "=", "{", "\n", "\"val\"", ":", "\"val_list.txt\"", ",", "\n", "\"test\"", ":", "\"test_list.txt\"", ",", "\n", "}", "\n", "for", "split_name", ",", "fname", "in", "test_splits", ".", "items", "(", ")", ":", "\n", "            ", "subset_paths", "[", "split_name", "]", "=", "{", "\"train\"", ":", "\"train_list.txt\"", ",", "\"val\"", ":", "fname", "}", "\n", "\n", "", "feature_names", "=", "BaseDataset", ".", "common_feat_names", "(", ")", "\n", "feature_names", ".", "append", "(", "\"audio.vggish.0\"", ")", "\n", "text_feat_paths", "=", "BaseDataset", ".", "common_text_feat_paths", "(", ")", "\n", "text_feat_paths", "=", "{", "key", ":", "Path", "(", "\"text_embeddings\"", ")", "/", "fname", "\n", "for", "key", ",", "fname", "in", "text_feat_paths", ".", "items", "(", ")", "}", "\n", "challenge_text_feat_paths", "=", "{", "key", ":", "f\"text_embeddings/{key}.pkl\"", "\n", "for", "key", "in", "text_feat_paths", "}", "\n", "custom_paths", "=", "{", "\n", "\"audio\"", ":", "[", "\"aggregated_audio/vggish-raw.hickle\"", "]", ",", "\n", "}", "\n", "feature_info", "=", "{", "\n", "\"custom_paths\"", ":", "custom_paths", ",", "\n", "\"feature_names\"", ":", "feature_names", ",", "\n", "\"subset_list_paths\"", ":", "subset_paths", ",", "\n", "\"text_feat_paths\"", ":", "text_feat_paths", ",", "\n", "\"challenge_text_feat_paths\"", ":", "challenge_text_feat_paths", ",", "\n", "\"raw_captions_path\"", ":", "\"structured-symlinks/raw_captions_combined_filtered.pkl\"", ",", "\n", "}", "\n", "return", "feature_info", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.QuerYD_dataset.QuerYD.load_features": [[45, 88], ["feat_names.update", "feat_names.items", "QuerYD_dataset.QuerYD.visual_feat_paths", "tuple", "QuerYD_dataset.QuerYD.load_challenge_text_features", "zsvision.zs_utils.memcache", "zsvision.zs_utils.memcache", "len", "zsvision.zs_utils.memcache", "print", "zsvision.zs_utils.concat_features.cache_info", "print", "zsvision.zs_utils.concat_features", "utils.memory_summary", "copy.deepcopy", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.visual_feat_paths", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.load_challenge_text_features", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.memory_summary"], ["", "def", "load_features", "(", "self", ")", ":", "\n", "        ", "root_feat", "=", "self", ".", "root_feat", "\n", "feat_names", "=", "{", "key", ":", "self", ".", "visual_feat_paths", "(", "key", ")", "for", "key", "in", "\n", "self", ".", "paths", "[", "\"feature_names\"", "]", "}", "\n", "feat_names", ".", "update", "(", "self", ".", "paths", "[", "\"custom_paths\"", "]", ")", "\n", "features", "=", "{", "}", "\n", "for", "expert", ",", "rel_names", "in", "feat_names", ".", "items", "(", ")", ":", "\n", "            ", "if", "expert", "not", "in", "self", ".", "ordered_experts", ":", "\n", "                ", "continue", "\n", "", "feat_paths", "=", "tuple", "(", "[", "Path", "(", "root_feat", ")", "/", "rel_name", "for", "rel_name", "in", "rel_names", "]", ")", "\n", "if", "len", "(", "feat_paths", ")", "==", "1", ":", "\n", "                ", "features", "[", "expert", "]", "=", "memcache", "(", "feat_paths", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "# support multiple forms of feature (e.g. max and avg pooling). For", "\n", "# now, we only support direct concatenation", "\n", "                ", "msg", "=", "f\"{expert}: Only direct concatenation of muliple feats is possible\"", "\n", "print", "(", "f\"Concatenating aggregates for {expert}....\"", ")", "\n", "assert", "self", ".", "feat_aggregation", "[", "expert", "]", "[", "\"aggregate\"", "]", "==", "\"concat\"", ",", "msg", "\n", "axis", "=", "self", ".", "feat_aggregation", "[", "expert", "]", "[", "\"aggregate-axis\"", "]", "\n", "x", "=", "concat_features", ".", "cache_info", "(", ")", "# pylint: disable=no-value-for-parameter", "\n", "print", "(", "f\"concat cache info: {x}\"", ")", "\n", "features_", "=", "concat_features", "(", "feat_paths", ",", "axis", "=", "axis", ")", "\n", "memory_summary", "(", ")", "\n", "\n", "# Make separate feature copies for each split to allow in-place filtering", "\n", "features", "[", "expert", "]", "=", "copy", ".", "deepcopy", "(", "features_", ")", "\n", "\n", "", "", "self", ".", "features", "=", "features", "\n", "if", "self", ".", "challenge_mode", ":", "\n", "            ", "self", ".", "load_challenge_text_features", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "raw_captions", "=", "memcache", "(", "root_feat", "/", "self", ".", "paths", "[", "\"raw_captions_path\"", "]", ")", "\n", "# keys = list(raw_captions.keys())", "\n", "# raw_captions_fused = {}", "\n", "# for key in keys:", "\n", "#     raw_captions_fused[key] = list(itertools.chain.from_iterable(raw_captions[key]))", "\n", "# self.raw_captions = raw_captions_fused", "\n", "text_feat_path", "=", "root_feat", "/", "self", ".", "paths", "[", "\"text_feat_paths\"", "]", "[", "self", ".", "text_feat", "]", "\n", "self", ".", "text_features", "=", "memcache", "(", "text_feat_path", ")", "\n", "\n", "# overload video paths, which are structured differently for YouCook2", "\n", "", "self", ".", "video_path_retrieval", "=", "[", "f\"videos/{x}.mp4\"", "\n", "for", "x", "in", "self", ".", "partition_lists", "[", "\"val\"", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.data_loader.QuerYD_dataset.QuerYD.sanity_checks": [[89, 93], ["None"], "methods", ["None"], ["", "def", "sanity_checks", "(", "self", ")", ":", "\n", "        ", "msg", "=", "(", "f\"Expected to have single test caption for QuerYD, since we assume\"", "\n", "f\"that the captions are fused (but using {self.num_test_captions})\"", ")", "\n", "assert", "self", ".", "num_test_captions", "==", "1", ",", "msg", "\n", "", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.logger.log_parser.log_summary": [[8, 105], ["open", "f.read().splitlines", "logger.info", "logger.info", "logger.info", "logger.info", "collections.defaultdict", "scores.items", "agg_scores.items", "str", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "NotImplementedError", "list", "subdict.items", "logger.info", "logger.info", "f.read", "re.search", "row.split", "scores[].keys", "scores.items", "numpy.array", "scipy.stats.mstats.gmean", "numpy.argmax", "agg_scores[].append", "len", "len", "re.search.groups", "re.search.groups", "float", "[].append", "geometric_stats[].append", "numpy.mean", "numpy.std", "row.split.index", "ValueError"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["def", "log_summary", "(", "logger", ",", "log_path", ",", "eval_mode", "=", "\"test_run\"", ",", "fixed_num_epochs", "=", "None", ")", ":", "\n", "    ", "\"\"\"Extract performace statistics from experiment log files.\n\n    Args:\n        logger (logger): reference to primary logging instance\n        log_path (Path): the path to the log file\n        eval_mode (str): the method use to collect the statistics. Can be one of:\n            `test_run`, `fixed_num_epochs` or `geometric_mean`\n\n    NOTE: The `eval_mode` argument differs by dataset: for datasets which provide a\n    validation set, we use validation set performance to complete a single test run.  For\n    datasets where no validation set is available, we aim to match prior work by either\n    fixing the number of training epochs, or selecting directly from validation set\n    performance (Details can be found in the supplementary material of the paper.)\n    \"\"\"", "\n", "with", "open", "(", "str", "(", "log_path", ")", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "log", "=", "f", ".", "read", "(", ")", ".", "splitlines", "(", ")", "\n", "\n", "# keep track of the random seed used for the part of the logfile being processed", "\n", "", "current_seed", "=", "None", "\n", "\n", "# Regex tag for finding the seed", "\n", "seed_tag", "=", "\"Setting experiment random seed to\"", "\n", "\n", "if", "eval_mode", "==", "\"test_run\"", ":", "\n", "        ", "subset", "=", "\"test\"", "\n", "", "else", ":", "\n", "        ", "subset", "=", "\"val\"", "\n", "\n", "", "for", "mode", "in", "\"t2v\"", ",", "\"v2t\"", ":", "\n", "        ", "logger", ".", "info", "(", "\"\"", ")", "\n", "logger", ".", "info", "(", "\"----------------------------------------------------\"", ")", "\n", "logger", ".", "info", "(", "f\"[{mode}] loaded log file with {len(log)} lines....\"", ")", "\n", "logger", ".", "info", "(", "\"----------------------------------------------------\"", ")", "\n", "\n", "# Search for the following metrics", "\n", "scores", "=", "{", "\n", "\"R1\"", ":", "defaultdict", "(", "list", ")", ",", "\n", "\"R5\"", ":", "defaultdict", "(", "list", ")", ",", "\n", "\"R10\"", ":", "defaultdict", "(", "list", ")", ",", "\n", "\"R50\"", ":", "defaultdict", "(", "list", ")", ",", "\n", "\"MedR\"", ":", "defaultdict", "(", "list", ")", ",", "\n", "\"MeanR\"", ":", "defaultdict", "(", "list", ")", ",", "\n", "}", "\n", "\n", "for", "row", "in", "log", ":", "\n", "            ", "if", "seed_tag", "in", "row", ":", "\n", "# Search for the log file entry describing the current random seed", "\n", "                ", "match", "=", "re", ".", "search", "(", "seed_tag", "+", "\" (\\d+)$\"", ",", "row", ")", "# NOQA", "\n", "assert", "len", "(", "match", ".", "groups", "(", ")", ")", "==", "1", ",", "\"expected a single regex match\"", "\n", "current_seed", "=", "match", ".", "groups", "(", ")", "[", "0", "]", "\n", "\n", "", "if", "f\"{subset}_{mode}_metrics\"", "in", "row", ":", "\n", "                ", "tokens", "=", "row", ".", "split", "(", "\" \"", ")", "\n", "for", "key", "in", "scores", ":", "\n", "                    ", "tag", "=", "f\"{subset}_{mode}_metrics_{key}:\"", "\n", "if", "tag", "in", "tokens", ":", "\n", "                        ", "pos", "=", "tokens", ".", "index", "(", "tag", ")", "+", "1", "\n", "val", "=", "tokens", "[", "pos", "]", "\n", "val", "=", "float", "(", "val", ")", "\n", "assert", "current_seed", "is", "not", "None", ",", "\"failed to determine the seed\"", "\n", "scores", "[", "key", "]", "[", "current_seed", "]", ".", "append", "(", "val", ")", "\n", "\n", "", "", "", "", "agg_scores", "=", "{", "\"R1\"", ":", "[", "]", ",", "\"R5\"", ":", "[", "]", ",", "\"R10\"", ":", "[", "]", ",", "\"R50\"", ":", "[", "]", ",", "\"MedR\"", ":", "[", "]", ",", "\"MeanR\"", ":", "[", "]", "}", "\n", "\n", "# compute the best performance for a single epoch (i.e. sharing the same model", "\n", "# to compute all stats)", "\n", "geometric_stats", "=", "defaultdict", "(", "list", ")", "\n", "best_epochs", "=", "{", "}", "\n", "if", "eval_mode", "==", "\"geometric_mean\"", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"Need to fix this for new log format\"", ")", "\n", "consider", "=", "[", "\"R1\"", ",", "\"R5\"", ",", "\"R10\"", "]", "\n", "seeds", "=", "list", "(", "scores", "[", "\"R1\"", "]", ".", "keys", "(", ")", ")", "\n", "for", "seed", "in", "seeds", ":", "\n", "                ", "for", "metric", ",", "subdict", "in", "scores", ".", "items", "(", ")", ":", "\n", "                    ", "if", "metric", "in", "consider", ":", "\n", "                        ", "geometric_stats", "[", "seed", "]", ".", "append", "(", "subdict", "[", "seed", "]", ")", "\n", "", "", "gms_raw", "=", "np", ".", "array", "(", "geometric_stats", "[", "seed", "]", ")", "\n", "geo_means", "=", "scipy", ".", "stats", ".", "mstats", ".", "gmean", "(", "gms_raw", ",", "axis", "=", "0", ")", "\n", "best_epochs", "[", "seed", "]", "=", "np", ".", "argmax", "(", "geo_means", ")", "\n", "\n", "", "", "for", "metric", ",", "subdict", "in", "scores", ".", "items", "(", ")", ":", "\n", "            ", "for", "seed", ",", "values", "in", "subdict", ".", "items", "(", ")", ":", "\n", "                ", "if", "eval_mode", "==", "\"test_run\"", ":", "\n", "                    ", "stat", "=", "values", "[", "0", "]", "\n", "", "elif", "eval_mode", "==", "\"fixed_num_epochs\"", ":", "\n", "                    ", "stat", "=", "values", "[", "fixed_num_epochs", "-", "1", "]", "\n", "", "elif", "\"LSMDC\"", "in", "log_path", "and", "eval_mode", "==", "\"geometric_mean\"", ":", "\n", "                    ", "stat", "=", "values", "[", "best_epochs", "[", "seed", "]", "]", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "f\"unrecognised eval_mode: {eval_mode}\"", ")", "\n", "", "agg_scores", "[", "metric", "]", ".", "append", "(", "stat", ")", "\n", "\n", "", "", "if", "eval_mode", "==", "\"fixed_num_epochs\"", ":", "\n", "            ", "logger", ".", "info", "(", "f\"Reporting stats with fixed training length: {fixed_num_epochs}\"", ")", "\n", "", "for", "metric", ",", "values", "in", "agg_scores", ".", "items", "(", ")", ":", "\n", "            ", "logger", ".", "info", "(", "f\"{metric}: {np.mean(values):.1f}, {np.std(values, ddof=1):.1f}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.logger.visualization.TensorboardWriter.__init__": [[6, 43], ["utils.Timer", "str", "logger.warning", "importlib.import_module().SummaryWriter", "importlib.import_module"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "log_dir", ",", "logger", ",", "enabled", ")", ":", "\n", "        ", "self", ".", "writer", "=", "None", "\n", "self", ".", "selected_module", "=", "\"\"", "\n", "\n", "if", "enabled", ":", "\n", "            ", "log_dir", "=", "str", "(", "log_dir", ")", "\n", "\n", "# Retrieve vizualization writer", "\n", "succeeded", "=", "False", "\n", "for", "module", "in", "[", "\"torch.utils.tensorboard\"", ",", "\"tensorboardX\"", "]", ":", "\n", "                ", "try", ":", "\n", "                    ", "self", ".", "writer", "=", "importlib", ".", "import_module", "(", "module", ")", ".", "SummaryWriter", "(", "log_dir", ")", "\n", "succeeded", "=", "True", "\n", "break", "\n", "", "except", "ImportError", ":", "\n", "                    ", "succeeded", "=", "False", "\n", "", "self", ".", "selected_module", "=", "module", "\n", "\n", "", "if", "not", "succeeded", ":", "\n", "                ", "message", "=", "(", "\"Warning: visualization (Tensorboard) is configured to use, \"", "\n", "\"but currently not installed on this machine. Please install\"", "\n", "\" either TensorboardX with 'pip install tensorboardx', \"", "\n", "\" upgrade PyTorch to version >= 1.1 for using \"", "\n", "\"'torch.utils.tensorboard' or turn off the option in \"", "\n", "\"the 'config.json' file.\"", ")", "\n", "logger", ".", "warning", "(", "message", ")", "\n", "\n", "", "", "self", ".", "step", "=", "0", "\n", "self", ".", "mode", "=", "''", "\n", "\n", "self", ".", "tb_writer_ftns", "=", "{", "\n", "'add_scalar'", ",", "'add_scalars'", ",", "'add_image'", ",", "'add_images'", ",", "'add_audio'", ",", "\n", "'add_text'", ",", "'add_histogram'", ",", "'add_pr_curve'", ",", "'add_embedding'", "\n", "}", "\n", "self", ".", "tag_mode_exceptions", "=", "{", "'add_histogram'", ",", "'add_embedding'", "}", "\n", "\n", "self", ".", "timer", "=", "Timer", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.logger.visualization.TensorboardWriter.set_step": [[44, 52], ["visualization.TensorboardWriter.timer.reset", "visualization.TensorboardWriter.timer.check", "visualization.TensorboardWriter.add_scalar"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.reset", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.Timer.check"], ["", "def", "set_step", "(", "self", ",", "step", ",", "mode", "=", "'train'", ")", ":", "\n", "        ", "self", ".", "mode", "=", "mode", "\n", "self", ".", "step", "=", "step", "\n", "if", "step", "==", "0", ":", "\n", "            ", "self", ".", "timer", ".", "reset", "(", ")", "\n", "", "else", ":", "\n", "            ", "duration", "=", "self", ".", "timer", ".", "check", "(", ")", "\n", "self", ".", "add_scalar", "(", "'steps_per_sec'", ",", "1", "/", "duration", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.logger.visualization.TensorboardWriter.__getattr__": [[53, 80], ["getattr", "object.__getattr__", "getattr.", "AttributeError", "msg.format"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.logger.visualization.TensorboardWriter.__getattr__"], ["", "", "def", "__getattr__", "(", "self", ",", "name", ")", ":", "\n", "        ", "\"\"\"\n        If visualization is configured to use:\n            return add_data() methods of tensorboard with additional information\n            (step, tag) added.\n        Otherwise:\n            return a blank function handle that does nothing\n        \"\"\"", "\n", "if", "name", "in", "self", ".", "tb_writer_ftns", ":", "\n", "            ", "add_data", "=", "getattr", "(", "self", ".", "writer", ",", "name", ",", "None", ")", "\n", "\n", "def", "wrapper", "(", "tag", ",", "data", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "                ", "if", "add_data", "is", "not", "None", ":", "\n", "# add mode(train/valid) tag", "\n", "                    ", "if", "name", "not", "in", "self", ".", "tag_mode_exceptions", ":", "\n", "                        ", "tag", "=", "'{}/{}'", ".", "format", "(", "tag", ",", "self", ".", "mode", ")", "\n", "", "add_data", "(", "tag", ",", "data", ",", "self", ".", "step", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "", "return", "wrapper", "\n", "", "else", ":", "\n", "# default action for returning methods defined in this class, set_step()", "\n", "# for instance.", "\n", "            ", "try", ":", "\n", "                ", "attr", "=", "object", ".", "__getattr__", "(", "name", ")", "\n", "", "except", "AttributeError", ":", "\n", "                ", "msg", "=", "\"type object '{}' has no attribute '{}'\"", "\n", "raise", "AttributeError", "(", "msg", ".", "format", "(", "self", ".", "selected_module", ",", "name", ")", ")", "\n", "", "return", "attr", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.logger.logger.setup_logging": [[8, 26], ["print", "pathlib.Path", "print", "pathlib.Path.is_file", "os.getcwd", "utils.read_json", "config[].items", "logging.config.dictConfig", "logging.config.dictConfig", "print", "logging.basicConfig", "logging.basicConfig", "pathlib.Path.exists", "str"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.read_json", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["def", "setup_logging", "(", "save_dir", ",", "log_config", "=", "'logger/logger_config.json'", ",", "\n", "default_level", "=", "logging", ".", "INFO", ")", ":", "\n", "    ", "\"\"\"Setup logging configuration.\"\"\"", "\n", "print", "(", "os", ".", "getcwd", "(", ")", ")", "\n", "log_config", "=", "Path", "(", "log_config", ")", "\n", "print", "(", "f\"log config: {log_config} exists: {log_config.exists()}\"", ")", "\n", "if", "log_config", ".", "is_file", "(", ")", ":", "\n", "        ", "config", "=", "read_json", "(", "log_config", ")", "\n", "# modify logging paths based on run config", "\n", "for", "_", ",", "handler", "in", "config", "[", "'handlers'", "]", ".", "items", "(", ")", ":", "\n", "            ", "if", "'filename'", "in", "handler", ":", "\n", "                ", "handler", "[", "'filename'", "]", "=", "str", "(", "save_dir", "/", "handler", "[", "'filename'", "]", ")", "\n", "\n", "", "", "logging", ".", "config", ".", "dictConfig", "(", "config", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "f\"Warning: logging configuration file is not found in {log_config}.\"", ")", "\n", "logging", ".", "basicConfig", "(", "level", "=", "default_level", ")", "\n", "", "return", "config", "[", "\"handlers\"", "]", "[", "\"info_file_handler\"", "]", "[", "\"filename\"", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.trainer.trainer.Trainer.__init__": [[55, 78], ["set", "base.BaseTrainer.__init__", "len", "int", "torch.nn.SmoothL1Loss", "numpy.sqrt"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__"], ["def", "__init__", "(", "self", ",", "model", ",", "loss", ",", "metrics", ",", "optimizer", ",", "config", ",", "data_loaders", ",", "\n", "lr_scheduler", ",", "visualizer", ",", "disable_nan_checks", ",", "skip_first_n_saves", ",", "\n", "include_optim_in_ckpts", ",", "force_cpu_val", ",", "distil_loss", ",", "distil_params", ",", "cache_targets", "=", "set", "(", ")", ",", "\n", "num_keep_ckpts", "=", "3", ",", "mini_train", "=", "False", ",", "val_freq", "=", "1", ",", "skip_tboard", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model", ",", "loss", ",", "metrics", ",", "optimizer", ",", "config", ",", "mini_train", "=", "mini_train", ",", "\n", "skip_tboard", "=", "skip_tboard", ",", "num_keep_ckpts", "=", "num_keep_ckpts", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "cache_targets", "=", "cache_targets", "\n", "self", ".", "data_loaders", "=", "data_loaders", "\n", "self", ".", "lr_scheduler", "=", "lr_scheduler", "\n", "self", ".", "mini_train", "=", "mini_train", "\n", "self", ".", "disable_nan_checks", "=", "disable_nan_checks", "\n", "self", ".", "len_epoch", "=", "len", "(", "self", ".", "data_loaders", "[", "\"train\"", "]", ")", "\n", "self", ".", "log_step", "=", "int", "(", "np", ".", "sqrt", "(", "data_loaders", "[", "\"train\"", "]", ".", "batch_size", ")", ")", "\n", "self", ".", "visualizer", "=", "visualizer", "\n", "self", ".", "force_cpu_val", "=", "force_cpu_val", "\n", "self", ".", "val_freq", "=", "val_freq", "\n", "self", ".", "skip_first_n_saves", "=", "skip_first_n_saves", "\n", "self", ".", "include_optim_in_ckpts", "=", "include_optim_in_ckpts", "\n", "self", ".", "seen", "=", "{", "\"train\"", ":", "0", ",", "\"val\"", ":", "0", "}", "\n", "self", ".", "distil_loss", "=", "distil_loss", "\n", "self", ".", "distil_params", "=", "distil_params", "\n", "self", ".", "tt_loss", "=", "torch", ".", "nn", ".", "SmoothL1Loss", "(", "reduction", "=", "\"elementwise_mean\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.trainer.trainer.Trainer._train_epoch": [[79, 178], ["trainer.Trainer.model.train", "utils.memory_summary", "enumerate", "trainer.Trainer.logger.info", "minibatch[].items", "trainer.Trainer.optimizer.zero_grad", "trainer.Trainer.model", "trainer.Trainer.backward", "trainer.Trainer.optimizer.step", "trainer.Trainer.item", "trainer.Trainer._valid_epoch", "log.update", "trainer.Trainer.logger.info", "trainer.Trainer.lr_scheduler.step", "val.to", "minibatch.pop().to", "minibatch.pop", "minibatch.pop", "trainer.Trainer.loss", "trainer.Trainer.loss", "trainer.Trainer.tt_loss", "list", "trainer.Trainer.writer.set_step", "trainer.Trainer.writer.add_scalar", "trainer.Trainer._progress", "trainer.Trainer.logger.info", "minibatch[].to", "torch.no_grad", "minibatch[].keys", "trainer.Trainer.item", "trainer.Trainer.lr_scheduler.get_lr", "minibatch.pop", "len", "[].to", "[].to", "torch.matmul", "minibatch.pop.keys", "trainer.Trainer.item", "[].view", "[].t"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.train", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.memory_summary", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.ranger.Ranger.step", "home.repos.pwc.inspect_result.albanie_collaborative-experts.trainer.trainer.Trainer._valid_epoch", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.ranger.Ranger.step", "home.repos.pwc.inspect_result.albanie_collaborative-experts.logger.visualization.TensorboardWriter.set_step", "home.repos.pwc.inspect_result.albanie_collaborative-experts.trainer.trainer.Trainer._progress", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.cos_restart.CosineAnnealingWithRestartsLR.get_lr", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys"], ["", "def", "_train_epoch", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "\"\"\"\n        Training logic for an epoch\n\n        :param epoch: Current training epoch.\n        :return: A log that contains all information you want to save.\n\n        Note:\n            If you have additional information to record, for example:\n                > additional_log = {\"x\": x, \"y\": y}\n            merge it with log before return. i.e.\n                > log = {**log, **additional_log}\n                > return log\n\n            The metrics in log must have the key 'metrics'.\n        \"\"\"", "\n", "total_loss", "=", "0", "\n", "self", ".", "model", ".", "train", "(", ")", "\n", "memory_summary", "(", ")", "\n", "\n", "for", "batch_idx", ",", "minibatch", "in", "enumerate", "(", "self", ".", "data_loaders", "[", "\"train\"", "]", ")", ":", "\n", "            ", "for", "key", ",", "val", "in", "minibatch", "[", "\"experts\"", "]", ".", "items", "(", ")", ":", "\n", "                ", "minibatch", "[", "\"experts\"", "]", "[", "key", "]", "=", "val", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "", "for", "key", "in", "{", "\"text\"", ",", "\"text_token_mask\"", "}", ":", "\n", "                ", "if", "key", "in", "minibatch", ":", "\n", "                    ", "minibatch", "[", "key", "]", "=", "minibatch", "[", "key", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "", "", "if", "\"labels\"", "in", "minibatch", ":", "\n", "                ", "labels", "=", "minibatch", ".", "pop", "(", "\"labels\"", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "", "if", "\"distil_video\"", "in", "minibatch", ":", "\n", "                ", "distil", "=", "minibatch", ".", "pop", "(", "\"distil_video\"", ")", "\n", "distil_text", "=", "minibatch", ".", "pop", "(", "\"distil_text\"", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "new_sims", "=", "None", "\n", "for", "t", "in", "distil", ":", "\n", "                        ", "t_sim", "=", "None", "\n", "\n", "for", "new_mod", "in", "distil", "[", "t", "]", ":", "\n", "                            ", "distil_text", "[", "t", "]", "[", "new_mod", "]", "=", "distil_text", "[", "t", "]", "[", "new_mod", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "distil", "[", "t", "]", "[", "new_mod", "]", "=", "distil", "[", "t", "]", "[", "new_mod", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "tmp_sim", "=", "torch", ".", "matmul", "(", "distil_text", "[", "t", "]", "[", "new_mod", "]", ".", "view", "(", "-", "1", ",", "distil_text", "[", "t", "]", "[", "new_mod", "]", ".", "shape", "[", "-", "1", "]", ")", ",", "distil", "[", "t", "]", "[", "new_mod", "]", ".", "t", "(", ")", ")", "\n", "\n", "if", "t_sim", "is", "None", ":", "\n", "                                ", "t_sim", "=", "tmp_sim", "\n", "", "else", ":", "\n", "                                ", "t_sim", "=", "t_sim", "+", "tmp_sim", "\n", "\n", "", "", "if", "new_sims", "is", "None", ":", "\n", "                            ", "new_sims", "=", "t_sim", "\n", "", "else", ":", "\n", "                            ", "new_sims", "=", "new_sims", "+", "t_sim", "\n", "\n", "", "", "new_sims", "=", "new_sims", "/", "len", "(", "distil", ".", "keys", "(", ")", ")", "\n", "\n", "", "", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "output", "=", "self", ".", "model", "(", "**", "minibatch", ")", "\n", "if", "\"retrieval\"", "in", "self", ".", "data_loaders", ".", "dataloaders", ":", "\n", "                ", "loss", "=", "self", ".", "loss", "(", "output", "[", "\"cross_view_conf_matrix\"", "]", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "self", ".", "loss", "(", "x", "=", "output", "[", "\"class_preds\"", "]", ",", "target", "=", "labels", ")", "\n", "\n", "", "if", "self", ".", "distil_loss", ":", "\n", "                ", "loss", "+=", "self", ".", "tt_loss", "(", "output", "[", "\"cross_view_conf_matrix\"", "]", ",", "new_sims", ")", "\n", "", "loss", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "sample_key", "=", "list", "(", "minibatch", "[", "\"experts\"", "]", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "batch_size", "=", "minibatch", "[", "\"experts\"", "]", "[", "sample_key", "]", ".", "shape", "[", "0", "]", "\n", "self", ".", "seen", "[", "\"train\"", "]", "+=", "batch_size", "\n", "\n", "if", "not", "self", ".", "skip_tboard", ":", "\n", "# self.writer.set_step((epoch - 1) * self.len_epoch + batch_idx)", "\n", "                ", "self", ".", "writer", ".", "set_step", "(", "self", ".", "seen", "[", "\"train\"", "]", ",", "mode", "=", "\"train\"", ")", "\n", "self", ".", "writer", ".", "add_scalar", "(", "'loss'", ",", "loss", ".", "item", "(", ")", ")", "\n", "", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "if", "batch_idx", "%", "self", ".", "log_step", "==", "0", ":", "\n", "                ", "prog", "=", "self", ".", "_progress", "(", "batch_idx", ")", "\n", "self", ".", "logger", ".", "info", "(", "f\"Train Epoch: {epoch} {prog} Loss: {loss.item():.6f}\"", ")", "\n", "\n", "", "if", "batch_idx", "==", "self", ".", "len_epoch", "or", "(", "self", ".", "mini_train", "and", "batch_idx", ">", "3", ")", ":", "\n", "                ", "break", "\n", "\n", "", "", "log", "=", "{", "'loss'", ":", "total_loss", "/", "self", ".", "len_epoch", "}", "\n", "if", "epoch", "%", "self", ".", "val_freq", "==", "0", ":", "\n", "            ", "nested_log", ",", "cached_preds", "=", "self", ".", "_valid_epoch", "(", "epoch", ")", "\n", "log", ".", "update", "(", "nested_log", ")", "\n", "", "else", ":", "\n", "            ", "nested_log", ",", "cached_preds", "=", "{", "}", ",", "None", "\n", "self", ".", "logger", ".", "info", "(", "f\"skipping val for epoch: {epoch}\"", ")", "\n", "\n", "", "if", "self", ".", "lr_scheduler", "is", "not", "None", ":", "\n", "            ", "self", ".", "lr_scheduler", ".", "step", "(", ")", "\n", "\n", "", "self", ".", "logger", ".", "info", "(", "f\"LR {self.lr_scheduler.get_lr()}\"", ")", "\n", "return", "log", ",", "cached_preds", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.trainer.trainer.Trainer.log_metrics": [[179, 185], ["print", "trainer.Trainer.writer.set_step", "metric_store.items", "trainer.Trainer.writer.add_scalar"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.logger.visualization.TensorboardWriter.set_step", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["", "def", "log_metrics", "(", "self", ",", "metric_store", ",", "metric_name", ",", "mode", ")", ":", "\n", "        ", "if", "not", "self", ".", "skip_tboard", ":", "\n", "            ", "print", "(", "f\"logging metrics: {metric_name}\"", ")", "\n", "self", ".", "writer", ".", "set_step", "(", "step", "=", "self", ".", "seen", "[", "mode", "]", ",", "mode", "=", "mode", ")", "\n", "for", "key", ",", "value", "in", "metric_store", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "writer", ".", "add_scalar", "(", "f\"{metric_name}/{key}\"", ",", "value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.trainer.trainer.Trainer._valid_epoch": [[186, 323], ["trainer.Trainer.model.eval", "torch.no_grad", "torch.from_numpy().view", "trainer.Trainer.loss", "list", "int", "int", "copy.deepcopy", "range", "torch.cat().data.cpu().float().numpy", "trainer.Trainer.model.to", "output[].data.cpu().float().numpy", "sims_[].contiguous", "trainer.Trainer.writer.add_scalar", "metric", "trainer.Trainer.log_metrics", "enumerate", "samples[].keys", "numpy.ceil", "numpy.ceil", "sim_chunks.append", "trainer.ctxt_mgr", "trainer.Trainer.model", "torch.from_numpy", "trainer.Trainer.item", "print", "trainer.verbose", "trainer.Trainer.visualizer.visualize_ranking", "x", "minibatch[].items", "minibatch.pop().to", "minibatch.pop", "trainer.Trainer.model", "hasattr", "enumerate", "cached_preds[].items", "trainer.ctxt_mgr", "trainer.Trainer.model", "torch.cat().data.cpu().float", "output[].data.cpu().float", "val.to", "[].append", "[].append", "metric.add", "trainer.Trainer._progress", "trainer.Trainer.logger.info", "trainer.Trainer.log_metrics", "isinstance", "minibatch[].items", "[].append", "trainer.Trainer.model", "[].append", "torch.cat().cpu().numpy", "minibatch.pop", "trainer.Trainer.log_metrics", "isinstance", "val.to", "[].append", "minibatch.pop", "torch.cat().data.cpu", "output[].data.cpu", "zip", "metric.value().mean", "trainer.Trainer.log_metrics", "ValueError", "minibatch.pop", "torch.cat().cpu", "metric.value", "metric.value().mean", "metric.value", "torch.cat", "torch.cat", "metric.value", "type"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.trainer.trainer.Trainer.log_metrics", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys", "home.repos.pwc.inspect_result.albanie_collaborative-experts.trainer.trainer.ctxt_mgr", "home.repos.pwc.inspect_result.albanie_collaborative-experts.trainer.trainer.verbose", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.visualizer.Visualizer.visualize_ranking", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.trainer.trainer.ctxt_mgr", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.add", "home.repos.pwc.inspect_result.albanie_collaborative-experts.trainer.trainer.Trainer._progress", "home.repos.pwc.inspect_result.albanie_collaborative-experts.trainer.trainer.Trainer.log_metrics", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.trainer.trainer.Trainer.log_metrics", "home.repos.pwc.inspect_result.albanie_collaborative-experts.trainer.trainer.Trainer.log_metrics", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.value", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.value", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.value"], ["", "", "", "def", "_valid_epoch", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "\"\"\"Validate model after an epoch of training and store results to disk.\n\n        Args:\n            epoch (int): the current epoch\n\n        Returns:\n            A log that contains information about validation\n\n        NOTE: The validation metrics in log must have the key 'val_metrics'.\n        \"\"\"", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "if", "not", "self", ".", "skip_tboard", ":", "\n", "            ", "self", ".", "writer", ".", "mode", "=", "\"val\"", "\n", "", "cached_preds", "=", "{", "key", ":", "{", "\"vid_name\"", ":", "[", "]", ",", "\"preds\"", ":", "[", "]", ",", "\"labels\"", ":", "[", "]", "}", "\n", "for", "key", "in", "self", ".", "cache_targets", "}", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "\"retrieval\"", "in", "self", ".", "data_loaders", ".", "dataloaders", ":", "\n", "                ", "samples", ",", "meta", "=", "self", ".", "data_loaders", "[", "\"retrieval\"", "]", "\n", "\n", "sample_key", "=", "list", "(", "samples", "[", "\"experts\"", "]", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "batch_size", "=", "samples", "[", "\"experts\"", "]", "[", "sample_key", "]", ".", "shape", "[", "0", "]", "\n", "self", ".", "seen", "[", "\"val\"", "]", "+=", "batch_size", "\n", "\n", "num_queries", "=", "samples", "[", "\"text\"", "]", ".", "shape", "[", "0", "]", "*", "samples", "[", "\"text\"", "]", ".", "shape", "[", "1", "]", "\n", "safe_queries", "=", "256", "\n", "if", "num_queries", ">", "safe_queries", ":", "\n", "                    ", "partitions", "=", "int", "(", "np", ".", "ceil", "(", "num_queries", "/", "safe_queries", ")", ")", "\n", "chunk_size", "=", "int", "(", "np", ".", "ceil", "(", "samples", "[", "\"text\"", "]", ".", "shape", "[", "0", "]", "/", "partitions", ")", ")", "\n", "texts", "=", "copy", ".", "deepcopy", "(", "samples", "[", "\"text\"", "]", ")", "\n", "sim_chunks", "=", "[", "]", "\n", "for", "chunk_idx", "in", "range", "(", "partitions", ")", ":", "\n", "                        ", "chunk_start", "=", "chunk_idx", "*", "chunk_size", "\n", "chunk_stop", "=", "(", "chunk_idx", "+", "1", ")", "*", "chunk_size", "\n", "samples", "[", "\"text\"", "]", "=", "texts", "[", "chunk_start", ":", "chunk_stop", "]", "\n", "if", "samples", "[", "'text'", "]", ".", "shape", "[", "0", "]", "==", "0", ":", "\n", "                            ", "continue", "\n", "", "with", "ctxt_mgr", "(", "samples", ",", "self", ".", "device", ",", "\n", "self", ".", "disable_nan_checks", ")", "as", "xx", ":", "\n", "                            ", "output", "=", "self", ".", "model", "(", "**", "xx", ")", "\n", "", "sims", "=", "output", "[", "\"cross_view_conf_matrix\"", "]", ".", "data", "\n", "sim_chunks", ".", "append", "(", "sims", ")", "\n", "\n", "", "samples", "[", "\"text\"", "]", "=", "texts", "# restore pointer to original tensor", "\n", "del", "texts", "\n", "sims", "=", "torch", ".", "cat", "(", "sim_chunks", ",", "dim", "=", "0", ")", ".", "data", ".", "cpu", "(", ")", ".", "float", "(", ")", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "                    ", "with", "ctxt_mgr", "(", "samples", ",", "self", ".", "device", ",", "self", ".", "disable_nan_checks", ")", "as", "xx", ":", "\n", "                        ", "output", "=", "self", ".", "model", "(", "**", "xx", ")", "\n", "", "self", ".", "model", "=", "self", ".", "model", ".", "to", "(", "self", ".", "device", ")", "\n", "sims", "=", "output", "[", "\"cross_view_conf_matrix\"", "]", ".", "data", ".", "cpu", "(", ")", ".", "float", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "# sample the loss (using only the first query for each video)", "\n", "", "queries_per_vid", "=", "meta", "[", "\"query_masks\"", "]", ".", "shape", "[", "1", "]", "\n", "sims_", "=", "torch", ".", "from_numpy", "(", "sims", ")", ".", "view", "(", "-", "1", ",", "queries_per_vid", ",", "sims", ".", "shape", "[", "-", "1", "]", ")", "\n", "loss", "=", "self", ".", "loss", "(", "sims_", "[", ":", ",", "0", ",", ":", "]", ".", "contiguous", "(", ")", ")", "\n", "if", "not", "self", ".", "skip_tboard", ":", "\n", "                    ", "self", ".", "writer", ".", "add_scalar", "(", "'first-query-loss'", ",", "loss", ".", "item", "(", ")", ")", "\n", "", "dataset", "=", "self", ".", "data_loaders", ".", "dataset_name", "\n", "nested_metrics", "=", "{", "}", "\n", "for", "metric", "in", "self", ".", "metrics", ":", "\n", "                    ", "metric_name", "=", "metric", ".", "__name__", "\n", "res", "=", "metric", "(", "sims", ",", "query_masks", "=", "meta", "[", "\"query_masks\"", "]", ")", "\n", "if", "metric_name", "==", "\"mean_average_precision\"", ":", "\n", "                        ", "print", "(", "f\"Epoch: {epoch}, mean AP: {res['mAP']}\"", ")", "\n", "", "else", ":", "\n", "                        ", "verbose", "(", "epoch", "=", "epoch", ",", "metrics", "=", "res", ",", "name", "=", "dataset", ",", "mode", "=", "metric_name", ")", "\n", "", "self", ".", "log_metrics", "(", "res", ",", "metric_name", "=", "metric_name", ",", "mode", "=", "\"val\"", ")", "\n", "nested_metrics", "[", "metric_name", "]", "=", "res", "\n", "\n", "# TODO(Samuel) disabled visualisation for now, simple to add in later", "\n", "", "num_test_caps", "=", "self", ".", "data_loaders", ".", "num_test_captions", "\n", "if", "num_test_caps", "==", "1", "and", "meta", "[", "\"raw_captions\"", "]", "is", "not", "None", ":", "\n", "                    ", "if", "self", ".", "visualizer", "is", "not", "None", ":", "\n", "                        ", "self", ".", "visualizer", ".", "visualize_ranking", "(", "\n", "sims", "=", "sims", ",", "\n", "meta", "=", "meta", ",", "\n", "epoch", "=", "epoch", ",", "\n", "nested_metrics", "=", "nested_metrics", ",", "\n", ")", "\n", "", "", "return", "{", "\"nested_val_metrics\"", ":", "nested_metrics", "}", ",", "cached_preds", "\n", "\n", "", "elif", "\"val\"", "in", "self", ".", "data_loaders", ".", "dataloaders", ":", "\n", "                ", "metrics", "=", "[", "x", "(", ")", "for", "x", "in", "self", ".", "metrics", "]", "\n", "for", "batch_idx", ",", "minibatch", "in", "enumerate", "(", "self", ".", "data_loaders", "[", "\"val\"", "]", ")", ":", "\n", "                    ", "for", "key", ",", "val", "in", "minibatch", "[", "\"experts\"", "]", ".", "items", "(", ")", ":", "\n", "                        ", "minibatch", "[", "\"experts\"", "]", "[", "key", "]", "=", "val", ".", "to", "(", "self", ".", "device", ")", "\n", "", "labels", "=", "minibatch", ".", "pop", "(", "\"labels\"", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "vid_name", "=", "minibatch", ".", "pop", "(", "\"vid_name\"", ")", "\n", "output", "=", "self", ".", "model", "(", "**", "minibatch", ")", "\n", "if", "\"val\"", "in", "self", ".", "cache_targets", ":", "\n", "                        ", "cached_preds", "[", "\"val\"", "]", "[", "\"vid_name\"", "]", ".", "append", "(", "vid_name", ")", "\n", "cached_preds", "[", "\"val\"", "]", "[", "\"preds\"", "]", ".", "append", "(", "output", "[", "\"class_preds\"", "]", ")", "\n", "\n", "", "for", "metric", "in", "metrics", ":", "\n", "                        ", "metric", ".", "add", "(", "output", "=", "output", "[", "\"class_preds\"", "]", ",", "target", "=", "labels", ")", "\n", "", "if", "batch_idx", "%", "self", ".", "log_step", "==", "0", ":", "\n", "                        ", "prog", "=", "self", ".", "_progress", "(", "batch_idx", ")", "\n", "self", ".", "logger", ".", "info", "(", "f\"Val Epoch: {epoch} {prog}\"", ")", "\n", "\n", "", "", "nested_metrics", "=", "{", "}", "\n", "for", "metric", "in", "metrics", ":", "\n", "                    ", "if", "hasattr", "(", "metric", ",", "\"topk\"", ")", ":", "\n", "                        ", "res", "=", "{", "f\"top{key}\"", ":", "val", "for", "key", ",", "val", "in", "\n", "zip", "(", "metric", ".", "topk", ",", "metric", ".", "value", "(", ")", ")", "}", "\n", "self", ".", "log_metrics", "(", "res", ",", "mode", "=", "\"val\"", ",", "metric_name", "=", "\"accuracy\"", ")", "\n", "nested_metrics", "[", "\"accuracy\"", "]", "=", "res", "\n", "", "elif", "isinstance", "(", "metric", ",", "APMeter", ")", ":", "\n", "                        ", "res", "=", "{", "\"mAP\"", ":", "metric", ".", "value", "(", ")", ".", "mean", "(", ")", "}", "\n", "self", ".", "log_metrics", "(", "res", ",", "mode", "=", "\"val\"", ",", "\n", "metric_name", "=", "\"mean_ap_non_challenge\"", ")", "\n", "nested_metrics", "[", "\"mean_ap_non_challenge\"", "]", "=", "res", "\n", "", "elif", "isinstance", "(", "metric", ",", "APMeterChallenge", ")", ":", "\n", "                        ", "res", "=", "{", "\"mAP\"", ":", "metric", ".", "value", "(", ")", ".", "mean", "(", ")", "}", "\n", "self", ".", "log_metrics", "(", "res", ",", "mode", "=", "\"val\"", ",", "\n", "metric_name", "=", "\"mean_average_precision\"", ")", "\n", "nested_metrics", "[", "\"mean_ap\"", "]", "=", "res", "\n", "", "else", ":", "\n", "                        ", "raise", "ValueError", "(", "f\"unsupported mettric: {type(metric)}\"", ")", "\n", "", "", "nested", "=", "{", "\"nested_val_metrics\"", ":", "nested_metrics", "}", "\n", "\n", "for", "target", "in", "self", ".", "cache_targets", "-", "{", "\"val\"", "}", ":", "\n", "                    ", "for", "batch_idx", ",", "minibatch", "in", "enumerate", "(", "self", ".", "data_loaders", "[", "\"tiny\"", "]", ")", ":", "\n", "                        ", "for", "key", ",", "val", "in", "minibatch", "[", "\"experts\"", "]", ".", "items", "(", ")", ":", "\n", "                            ", "minibatch", "[", "\"experts\"", "]", "[", "key", "]", "=", "val", ".", "to", "(", "self", ".", "device", ")", "\n", "", "if", "\"labels\"", "in", "minibatch", ":", "\n", "                            ", "cached_preds", "[", "target", "]", "[", "\"labels\"", "]", ".", "append", "(", "minibatch", ".", "pop", "(", "\"labels\"", ")", ")", "\n", "", "cached_preds", "[", "target", "]", "[", "\"vid_name\"", "]", ".", "append", "(", "minibatch", ".", "pop", "(", "\"vid_name\"", ")", ")", "\n", "output", "=", "self", ".", "model", "(", "**", "minibatch", ")", "\n", "cached_preds", "[", "target", "]", "[", "\"preds\"", "]", ".", "append", "(", "output", "[", "\"class_preds\"", "]", ")", "\n", "\n", "# aggregate all cached predictions", "\n", "", "", "for", "target", "in", "self", ".", "cache_targets", ":", "\n", "                    ", "for", "key", ",", "val", "in", "cached_preds", "[", "target", "]", ".", "items", "(", ")", ":", "\n", "                        ", "cached_preds", "[", "key", "]", "=", "torch", ".", "cat", "(", "val", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "", "return", "nested", ",", "cached_preds", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.trainer.trainer.Trainer._progress": [[324, 333], ["hasattr", "base.format"], "methods", ["None"], ["", "", "", "def", "_progress", "(", "self", ",", "batch_idx", ")", ":", "\n", "        ", "base", "=", "'[{}/{} ({:.0f}%)]'", "\n", "if", "hasattr", "(", "self", ".", "data_loaders", ",", "'n_samples'", ")", ":", "\n", "            ", "current", "=", "batch_idx", "*", "self", ".", "data_loaders", ".", "batch_size", "\n", "total", "=", "self", ".", "data_loaders", ".", "n_samples", "\n", "", "else", ":", "\n", "            ", "current", "=", "batch_idx", "\n", "total", "=", "self", ".", "len_epoch", "\n", "", "return", "base", ".", "format", "(", "current", ",", "total", ",", "100.0", "*", "current", "/", "total", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.trainer.trainer.verbose": [[11, 17], ["print"], "function", ["None"], ["def", "verbose", "(", "epoch", ",", "metrics", ",", "mode", ",", "name", "=", "\"TEST\"", ")", ":", "\n", "    ", "r1", ",", "r5", ",", "r10", ",", "r50", "=", "metrics", "[", "\"R1\"", "]", ",", "metrics", "[", "\"R5\"", "]", ",", "metrics", "[", "\"R10\"", "]", ",", "metrics", "[", "\"R50\"", "]", "\n", "msg", "=", "f\"[{mode}]{name:s} epoch {epoch}, R@1: {r1:.1f}\"", "\n", "msg", "+=", "f\", R@5: {r5:.1f}, R@10 {r10:.1f}, R@50 {r50:.1f}\"", "\n", "msg", "+=", "f\"MedR: {metrics['MedR']:g}, MeanR: {metrics['MeanR']:.1f}\"", "\n", "print", "(", "msg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.trainer.trainer.ctxt_mgr": [[19, 46], ["print", "samples[].items", "val.clone().to", "samples[].to", "samples[].to", "val.clone"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["", "@", "contextmanager", "\n", "def", "ctxt_mgr", "(", "samples", ",", "device", ",", "disable_nan_checks", ")", ":", "\n", "    ", "\"\"\"Provide a context for managing temporary, cloned copies of retrieval\n    sample tensors.\n\n    The rationale here is that to use nan-checking in the model (to validate the\n    positions of missing experts), we need to modify the underlying tensors. This\n    function lets the evaluation code run (and modify) temporary copies, without\n    modifying the originals.\n    \"\"\"", "\n", "if", "disable_nan_checks", ":", "\n", "        ", "print", "(", "\"running without nan checks\"", ")", "\n", "yield", "samples", "\n", "", "else", ":", "\n", "        ", "exp_dict", "=", "samples", "[", "\"experts\"", "]", ".", "items", "(", ")", "\n", "experts", "=", "{", "key", ":", "val", ".", "clone", "(", ")", ".", "to", "(", "device", ")", "for", "key", ",", "val", "in", "exp_dict", "}", "\n", "samples_", "=", "{", "\n", "\"experts\"", ":", "experts", ",", "\n", "\"ind\"", ":", "samples", "[", "\"ind\"", "]", ",", "\n", "\"text\"", ":", "samples", "[", "\"text\"", "]", ".", "to", "(", "device", ")", ",", "\n", "}", "\n", "if", "\"text_token_mask\"", "in", "samples", ":", "\n", "            ", "samples_", "[", "\"text_token_mask\"", "]", "=", "samples", "[", "\"text_token_mask\"", "]", ".", "to", "(", "device", ")", "\n", "", "try", ":", "\n", "            ", "yield", "samples_", "\n", "", "finally", ":", "\n", "            ", "del", "samples_", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_exps.generate_configs": [[12, 34], ["datetime.datetime.now().strftime", "list", "list", "pathlib.Path().parent.mkdir", "print", "print", "itertools.product", "grid.keys", "job_queue.append", "open", "f.write", "open", "f.write", "datetime.datetime.now", "grid.values", "str", "str", "zip", "pathlib.Path", "len", "pathlib.Path", "len"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdir", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys"], ["def", "generate_configs", "(", "base_config", ",", "grid", ")", ":", "\n", "    ", "job_queue", "=", "[", "]", "\n", "timestamp", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "r\"%Y-%m-%d_%H-%M-%S\"", ")", "\n", "hparam_vals", "=", "[", "x", "for", "x", "in", "grid", ".", "values", "(", ")", "]", "\n", "grid_vals", "=", "list", "(", "itertools", ".", "product", "(", "*", "hparam_vals", ")", ")", "\n", "hparams", "=", "list", "(", "grid", ".", "keys", "(", ")", ")", "\n", "\n", "for", "cfg_vals", "in", "grid_vals", ":", "\n", "        ", "custom_tokens", "=", "[", "f\"{hparam}@{val}\"", "for", "hparam", ",", "val", "in", "zip", "(", "hparams", ",", "cfg_vals", ")", "]", "\n", "custom_args", "=", "\"+\"", ".", "join", "(", "custom_tokens", ")", "\n", "job", "=", "f\"--config {base_config} --custom_args {custom_args}\"", "\n", "job_queue", ".", "append", "(", "job", ")", "\n", "\n", "", "job_queue_path", "=", "f\"data/job-queues/latest.txt\"", "\n", "Path", "(", "job_queue_path", ")", ".", "parent", ".", "mkdir", "(", "exist_ok", "=", "True", ",", "parents", "=", "True", ")", "\n", "with", "open", "(", "str", "(", "job_queue_path", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "\"\\n\"", ".", "join", "(", "job_queue", ")", ")", "\n", "", "print", "(", "f\"Wrote {len(job_queue)} jobs to queue at {job_queue_path}\"", ")", "\n", "job_queue_path", "=", "f\"data/job-queues/{Path(base_config).stem}-{timestamp}.txt\"", "\n", "with", "open", "(", "str", "(", "job_queue_path", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "\"\\n\"", ".", "join", "(", "job_queue", ")", ")", "\n", "", "print", "(", "f\"Wrote backup {len(job_queue)} jobs to queue at {job_queue_path}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_exps.parse_grid": [[36, 52], ["print", "key_val_strs.split", "collections.OrderedDict", "pair.split", "vals.append", "val_str.split", "token.split"], "function", ["None"], ["", "def", "parse_grid", "(", "key_val_strs", ")", ":", "\n", "    ", "print", "(", "f\"parsing grid str: {key_val_strs}\"", ")", "\n", "key_val_pairs", "=", "key_val_strs", ".", "split", "(", "\"+\"", ")", "\n", "parsed", "=", "OrderedDict", "(", ")", "\n", "for", "pair", "in", "key_val_pairs", ":", "\n", "        ", "key", ",", "val_str", "=", "pair", ".", "split", "(", "\"@\"", ")", "\n", "vals", "=", "[", "]", "\n", "opts", "=", "[", "x", "for", "x", "in", "val_str", ".", "split", "(", "\":\"", ")", "]", "\n", "for", "token", "in", "opts", ":", "\n", "            ", "if", "\",\"", "in", "token", ":", "\n", "                ", "val", "=", "[", "x", "for", "x", "in", "token", ".", "split", "(", "\",\"", ")", "if", "x", "]", "\n", "", "else", ":", "\n", "                ", "val", "=", "token", "\n", "", "vals", ".", "append", "(", "val", ")", "\n", "", "parsed", "[", "key", "]", "=", "vals", "\n", "", "return", "parsed", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_exps.main": [[54, 64], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "generate_exps.parse_grid", "generate_exps.generate_configs"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_exps.parse_grid", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_exps.generate_configs"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--grid'", ",", "default", "=", "\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'--config'", ",", "default", "=", "\"configs/msrvtt/only-i3d.json\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "grid", "=", "parse_grid", "(", "args", ".", "grid", ")", "\n", "generate_configs", "(", "\n", "grid", "=", "grid", ",", "\n", "base_config", "=", "args", ".", "config", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.prepare_text_embeddings.prepare_embedding_model": [[44, 63], ["collections.defaultdict", "collections.defaultdict.update", "key.endswith", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update"], ["@", "typechecked", "\n", "def", "prepare_embedding_model", "(", "\n", "embedding_name", ":", "str", ",", "\n", "text_embedding_config", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "Union", "[", "str", ",", "int", "]", "]", "]", ",", "\n", ")", "->", "TextEmbedding", ":", "\n", "    ", "conf", "=", "text_embedding_config", "[", "embedding_name", "]", "\n", "for", "key", "in", "conf", ":", "\n", "        ", "if", "key", ".", "endswith", "(", "\"_path\"", ")", ":", "\n", "            ", "conf", "[", "key", "]", "=", "Path", "(", "conf", "[", "key", "]", ")", "\n", "", "", "cls_map", "=", "defaultdict", "(", "lambda", ":", "HuggingFaceWrapper", ")", "\n", "cls_map", ".", "update", "(", "{", "\n", "\"w2v\"", ":", "W2VEmbedding", ",", "\n", "\"grovle\"", ":", "GrOVLE", ",", "\n", "\"mt_grovle\"", ":", "GrOVLE", ",", "\n", "\"hglmm_300d\"", ":", "GrOVLE", ",", "\n", "\"hglmm_6kd\"", ":", "GrOVLE", ",", "\n", "\"howto100m_mil_nce\"", ":", "HowTo100M_MIL_NCE", ",", "\n", "}", ")", "\n", "return", "cls_map", "[", "embedding_name", "]", "(", "embedding_name", "=", "embedding_name", ",", "**", "conf", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.prepare_text_embeddings.validate_embeddings_against_reference": [[65, 91], ["misc.gen_readme.dataset_paths", "[].values", "print", "tqdm.tqdm", "reference_dict.update", "zsvision.zs_utils.memcache", "computed_embeddings.items", "zip", "zsvision.zs_utils.memcache", "len", "len", "zsvision.zs_utils.memcache.items", "reference_dict.items", "len", "len", "numpy.abs().max", "numpy.abs"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.dataset_paths", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["", "@", "typechecked", "\n", "def", "validate_embeddings_against_reference", "(", "\n", "computed_embeddings", ":", "Dict", "[", "str", ",", "List", "[", "np", ".", "ndarray", "]", "]", ",", "\n", "embedding_name", ":", "str", ",", "\n", "dataset", ":", "str", ",", "\n", ")", ":", "\n", "    ", "root_feat", ",", "paths", "=", "dataset_paths", "(", "dataset", ")", "\n", "reference_dict", "=", "{", "}", "\n", "for", "path", "in", "paths", "[", "\"text_feat_paths\"", "]", "[", "embedding_name", "]", ".", "values", "(", ")", ":", "\n", "        ", "reference_dict", ".", "update", "(", "memcache", "(", "root_feat", "/", "path", ")", ")", "\n", "\n", "# We handle MSVD as a special case, because video keys != feature keys", "\n", "", "if", "dataset", "==", "\"MSVD\"", ":", "\n", "        ", "key_map", "=", "memcache", "(", "root_feat", "/", "paths", "[", "\"dict_youtube_mapping_path\"", "]", ")", "\n", "inverse_map", "=", "{", "val", ":", "key", "for", "key", ",", "val", "in", "key_map", ".", "items", "(", ")", "}", "\n", "reference_dict", "=", "{", "inverse_map", "[", "key", "]", ":", "val", "for", "key", ",", "val", "in", "reference_dict", ".", "items", "(", ")", "}", "\n", "\n", "", "print", "(", "f\"Validating embeddings against reference....\"", ")", "\n", "for", "key", ",", "val", "in", "tqdm", ".", "tqdm", "(", "computed_embeddings", ".", "items", "(", ")", ")", ":", "\n", "        ", "ref_val", "=", "reference_dict", "[", "key", "]", "\n", "msg", "=", "(", "f\"[{embedding_name}] {key} Different number of \"", "\n", "f\"embeddings {len(ref_val)} vs {len(val)}\"", ")", "\n", "assert", "len", "(", "ref_val", ")", "==", "len", "(", "val", ")", ",", "msg", "\n", "msg", "=", "f\"[{embedding_name}] Embedding mismatch for {key}\"", "\n", "for", "vec", ",", "ref_vec", "in", "zip", "(", "val", ",", "ref_val", ")", ":", "\n", "            ", "assert", "np", ".", "abs", "(", "vec", "-", "ref_vec", ")", ".", "max", "(", ")", "<", "1E-5", ",", "msg", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.prepare_text_embeddings.extract_embeddings_for_video": [[93, 109], ["isinstance", "isinstance", "model.text2vec", "embeddings_for_video.append", "failed_tokens.extend", "type"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.text.HuggingFaceWrapper.text2vec"], ["", "", "", "def", "extract_embeddings_for_video", "(", "\n", "descriptions", ":", "List", "[", "str", "]", ",", "\n", "key", ":", "str", ",", "\n", "model", ",", "\n", ")", "->", "Tuple", "[", "List", "[", "np", ".", "ndarray", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "    ", "embeddings_for_video", ",", "failed_tokens", "=", "[", "]", ",", "[", "]", "\n", "for", "description", "in", "descriptions", ":", "\n", "        ", "msg", "=", "(", "f\"Expected descripton to be a list of string tokens, \"", "\n", "f\" but was {type(description)} instead for {key}\"", ")", "\n", "assert", "isinstance", "(", "description", ",", "List", ")", ",", "msg", "\n", "assert", "isinstance", "(", "description", "[", "0", "]", ",", "str", ")", ",", "msg", "\n", "description_str", "=", "\" \"", ".", "join", "(", "description", ")", "\n", "embedded", ",", "failed", "=", "model", ".", "text2vec", "(", "description_str", ")", "\n", "embeddings_for_video", ".", "append", "(", "embedded", ")", "\n", "failed_tokens", ".", "extend", "(", "failed", ")", "\n", "", "return", "embeddings_for_video", ",", "failed_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.prepare_text_embeddings.extract_embeddings": [[111, 204], ["dest_dir.mkdir", "zsvision.zs_utils.memcache", "text_embedding_config[].pop", "torch.device", "prepare_text_embeddings.prepare_embedding_model", "prepare_embedding_model.set_device", "tqdm.tqdm", "print", "print", "dest_path.exists", "print", "open", "json.load", "set", "zsvision.zs_utils.memcache.items", "kwarg_list.append", "ray.remote", "ray.init", "ray.put", "zip", "tqdm.tqdm", "tqdm.tqdm", "len", "len", "numpy.sum", "prepare_text_embeddings.validate_embeddings_against_reference", "zsvision.zs_utils.BlockTimer", "ray.remote.remote", "prepare_text_embeddings.extract_embeddings.to_iterator"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdir", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.prepare_text_embeddings.prepare_embedding_model", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.text.LookupEmbedding.set_device", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.init", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.prepare_text_embeddings.validate_embeddings_against_reference"], ["", "@", "typechecked", "\n", "def", "extract_embeddings", "(", "\n", "text_embedding_config_path", ":", "Path", ",", "\n", "rel_dest_dir", ":", "Path", ",", "\n", "data_dir", ":", "Path", ",", "\n", "refresh", ":", "bool", ",", "\n", "validate_embeddings", ":", "bool", ",", "\n", "limit", ":", "int", ",", "\n", "processes", ":", "int", ",", "\n", "embedding_name", ":", "str", ",", "\n", "datasets", ":", "List", "[", "str", "]", ",", "\n", ")", ":", "\n", "    ", "for", "dataset", "in", "datasets", ":", "\n", "        ", "dest_dir", "=", "data_dir", "/", "dataset", "/", "rel_dest_dir", "\n", "dest_name", "=", "embedding_name", "\n", "if", "limit", ":", "\n", "            ", "dest_name", "=", "f\"{embedding_name}-limit{limit}\"", "\n", "", "dest_path", "=", "dest_dir", "/", "f\"{dest_name}.pkl\"", "\n", "\n", "if", "dest_path", ".", "exists", "(", ")", "and", "not", "refresh", ":", "\n", "            ", "print", "(", "f\"Found existing text embeddings at {dest_path}, skipping....\"", ")", "\n", "return", "\n", "\n", "", "dest_dir", ".", "mkdir", "(", "exist_ok", "=", "True", ",", "parents", "=", "True", ")", "\n", "# handle the activity-net exception", "\n", "if", "dataset", "==", "\"activity-net\"", ":", "\n", "            ", "fname", "=", "\"raw-captions-train-val_1.pkl\"", "\n", "", "else", ":", "\n", "            ", "fname", "=", "\"raw-captions.pkl\"", "\n", "", "captions_path", "=", "data_dir", "/", "dataset", "/", "\"structured-symlinks\"", "/", "fname", "\n", "video_descriptions", "=", "memcache", "(", "captions_path", ")", "\n", "with", "open", "(", "text_embedding_config_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "text_embedding_config", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "force_cpu", "=", "text_embedding_config", "[", "embedding_name", "]", ".", "pop", "(", "\"force_cpu\"", ",", "False", ")", "\n", "dev_name", "=", "\"cuda:0\"", "if", "torch", ".", "cuda", ".", "device_count", "(", ")", ">", "0", "and", "not", "force_cpu", "else", "\"cpu\"", "\n", "device", "=", "torch", ".", "device", "(", "dev_name", ")", "\n", "\n", "model", "=", "prepare_embedding_model", "(", "embedding_name", ",", "text_embedding_config", ")", "\n", "model", ".", "set_device", "(", "device", ")", "\n", "if", "limit", ":", "\n", "            ", "keep", "=", "set", "(", "list", "(", "video_descriptions", ".", "keys", "(", ")", ")", "[", ":", "limit", "]", ")", "\n", "video_descriptions", "=", "{", "key", ":", "val", "for", "key", ",", "val", "in", "video_descriptions", ".", "items", "(", ")", "\n", "if", "key", "in", "keep", "}", "\n", "\n", "", "computed_embeddings", "=", "{", "}", "\n", "kwarg_list", "=", "[", "]", "\n", "for", "key", ",", "descriptions", "in", "tqdm", ".", "tqdm", "(", "video_descriptions", ".", "items", "(", ")", ")", ":", "\n", "            ", "kwarg_list", ".", "append", "(", "{", "\"key\"", ":", "key", ",", "\"descriptions\"", ":", "descriptions", "}", ")", "\n", "\n", "", "all_failed_tokens", "=", "[", "]", "\n", "func", "=", "extract_embeddings_for_video", "\n", "if", "processes", ">", "1", ":", "\n", "# Note: An experimental approach with Ray.  Unfortunately, it seems that", "\n", "# the overhead is too great to justify this approach (it's slower than", "\n", "# using a single process). TODO(Samuel): revisit.", "\n", "            ", "func", "=", "ray", ".", "remote", "(", "extract_embeddings_for_video", ")", "\n", "ray", ".", "init", "(", "num_cpus", "=", "processes", ")", "\n", "\n", "# Store model in shared memory object store to avoid multiple copies", "\n", "model_id", "=", "ray", ".", "put", "(", "model", ")", "\n", "\n", "def", "to_iterator", "(", "obj_ids", ")", ":", "\n", "                ", "while", "obj_ids", ":", "\n", "                    ", "done", ",", "obj_ids", "=", "ray", ".", "wait", "(", "obj_ids", ")", "\n", "yield", "ray", ".", "get", "(", "done", "[", "0", "]", ")", "\n", "\n", "", "", "result_ids", "=", "[", "func", ".", "remote", "(", "model", "=", "model_id", ",", "**", "kwargs", ")", "for", "kwargs", "in", "kwarg_list", "]", "\n", "zipped", "=", "zip", "(", "to_iterator", "(", "result_ids", ")", ",", "kwarg_list", ")", "\n", "for", "(", "embeddings", ",", "failed", ")", ",", "kwargs", "in", "tqdm", ".", "tqdm", "(", "zipped", ",", "total", "=", "len", "(", "result_ids", ")", ")", ":", "\n", "                ", "computed_embeddings", "[", "kwargs", "[", "\"key\"", "]", "]", "=", "embeddings", "\n", "all_failed_tokens", ".", "extend", "(", "failed", ")", "\n", "", "", "else", ":", "\n", "            ", "for", "kwargs", "in", "tqdm", ".", "tqdm", "(", "kwarg_list", ")", ":", "\n", "                ", "embeddings_for_video", ",", "failed_tokens", "=", "func", "(", "**", "kwargs", ",", "model", "=", "model", ")", "\n", "computed_embeddings", "[", "kwargs", "[", "\"key\"", "]", "]", "=", "embeddings_for_video", "\n", "all_failed_tokens", ".", "extend", "(", "failed_tokens", ")", "\n", "\n", "", "", "stats", "=", "[", "len", "(", "x", ")", "for", "sublist", "in", "computed_embeddings", ".", "values", "(", ")", "for", "x", "in", "sublist", "]", "\n", "print", "(", "f\"Average num embedding tokens: {np.mean(stats):.1f} tokens\"", ")", "\n", "fail_rate", "=", "len", "(", "all_failed_tokens", ")", "/", "np", ".", "sum", "(", "stats", ")", "\n", "stat_str", "=", "f\"{len(all_failed_tokens)}/{np.sum(stats)} [{100 * fail_rate:.1f}%]\"", "\n", "print", "(", "f\"Failed tokens: {stat_str} tokens\"", ")", "\n", "\n", "if", "validate_embeddings", ":", "\n", "            ", "validate_embeddings_against_reference", "(", "\n", "computed_embeddings", "=", "computed_embeddings", ",", "\n", "embedding_name", "=", "embedding_name", ",", "\n", "dataset", "=", "dataset", ",", "\n", ")", "\n", "", "with", "BlockTimer", "(", "f\"Writing embeddings to {dest_path}\"", ")", ":", "\n", "            ", "with", "open", "(", "dest_path", ",", "\"wb\"", ")", "as", "f", ":", "\n", "                ", "pickle", ".", "dump", "(", "computed_embeddings", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.prepare_text_embeddings.prepare_text_with_yaspi": [[206, 238], ["utils.util.filter_cmd_args", "list", "yaspi.yaspi.Yaspi", "yaspi.yaspi.Yaspi.submit", "prepare_text_embeddings.extract_embeddings", "embedding_acronyms.append", "itertools.product", "job_queue.append", "len", "x[].upper", "embedding_name.split"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.filter_cmd_args", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.prepare_text_embeddings.extract_embeddings"], ["", "", "", "", "@", "typechecked", "\n", "def", "prepare_text_with_yaspi", "(", "\n", "yaspi_defaults", ":", "Dict", "[", "str", ",", "Union", "[", "str", ",", "int", "]", "]", ",", "\n", "common_kwargs", ":", "Dict", ",", "\n", "datasets", ":", "List", "[", "str", "]", ",", "\n", "embedding_names", ":", "List", "[", "str", "]", ",", "\n", ")", ":", "\n", "    ", "cmd_args", "=", "sys", ".", "argv", "\n", "remove", "=", "[", "\"--yaspify\"", ",", "\"--datasets\"", ",", "\"--embedding_name\"", "]", "\n", "cmd_args", "=", "filter_cmd_args", "(", "cmd_args", ",", "remove", "=", "remove", ")", "\n", "base_cmd", "=", "f\"python {' '.join(cmd_args)} --slurm\"", "\n", "# avoid filename limit", "\n", "embedding_acronyms", "=", "[", "]", "\n", "for", "embedding_name", "in", "embedding_names", ":", "\n", "        ", "acronym", "=", "\"\"", ".", "join", "(", "[", "x", "[", "0", "]", ".", "upper", "(", ")", "for", "x", "in", "embedding_name", ".", "split", "(", "\"-\"", ")", "]", ")", "\n", "embedding_acronyms", ".", "append", "(", "acronym", ")", "\n", "\n", "", "job_name", "=", "f\"prepare-text-{'-'.join(datasets)}-{'-'.join(embedding_acronyms)}\"", "\n", "pairs", "=", "list", "(", "itertools", ".", "product", "(", "embedding_names", ",", "datasets", ")", ")", "\n", "job_queue", "=", "[", "]", "\n", "for", "embedding_name", ",", "dataset", "in", "pairs", ":", "\n", "        ", "job_queue", ".", "append", "(", "f'\"--embedding_name {embedding_name} --datasets {dataset}\"'", ")", "\n", "", "job_queue", "=", "\" \"", ".", "join", "(", "job_queue", ")", "\n", "job", "=", "Yaspi", "(", "\n", "cmd", "=", "base_cmd", ",", "\n", "job_queue", "=", "job_queue", ",", "\n", "job_name", "=", "job_name", ",", "\n", "job_array_size", "=", "len", "(", "pairs", ")", ",", "\n", "**", "yaspi_defaults", ",", "\n", ")", "\n", "job", ".", "submit", "(", "watch", "=", "True", ",", "conserve_resources", "=", "5", ")", "\n", "extract_embeddings", "(", "**", "common_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.prepare_text_embeddings.main": [[240, 334], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "dict", "prepare_text_embeddings.prepare_text_with_yaspi", "prepare_text_embeddings.extract_embeddings", "open", "json.load", "json.load.update", "os.system", "print", "open", "json.load().items", "str", "json.load", "vals.get", "embedding_names.append", "pathlib.Path.home", "socket.gethostname"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.prepare_text_embeddings.prepare_text_with_yaspi", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.prepare_text_embeddings.extract_embeddings", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\"Prepare text embeddings for a given dataset\"", ")", "\n", "parser", ".", "add_argument", "(", "'--refresh'", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "'--limit'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--processes'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "type", "=", "Path", ",", "default", "=", "\"data\"", ",", "\n", "help", "=", "\"the location of the data directory\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dataset\"", ",", "type", "=", "str", ",", "default", "=", "\"MSVD\"", ",", "\n", "help", "=", "\"the name of the dataset to process\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--datasets\"", ",", "nargs", "=", "\"+\"", ",", "\n", "default", "=", "[", "\"MSRVTT\"", ",", "\"MSVD\"", ",", "\"DiDeMo\"", ",", "\"YouCook2\"", ",", "\"activity-net\"", ",", "\n", "\"LSMDC\"", ",", "\"YouDescribe\"", ",", "\"YouDescribeSegments\"", ",", "\n", "\"ActivityNetSegments\"", ",", "\"DiDeMoSegments\"", "]", ",", "\n", "help", "=", "\"The datasets to prepare text embeddings for\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--rel_dest_dir\"", ",", "type", "=", "Path", ",", "\n", "default", "=", "\"processing/text_embeddings\"", ",", "\n", "help", "=", "\"the relative path of destination folder\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--embedding_name\"", ",", "type", "=", "str", ",", "default", "=", "\"all-but-slow\"", ",", "\n", "choices", "=", "[", "\n", "\"w2v\"", ",", "\"bert\"", ",", "\"grovle\"", ",", "\"mt_grovle\"", ",", "\n", "\"electra\"", ",", "\"howto100m_mil_nce\"", ",", "\"hglmm_300d\"", ",", "\n", "\"hglmm_6kd\"", ",", "\"openai-gpt\"", ",", "\"gpt2\"", ",", "\"gpt2-medium\"", ",", "\n", "\"gpt2-large\"", ",", "\"gpt2-xl\"", ",", "\"bert-base-uncased\"", ",", "\"t5-small\"", ",", "\n", "\"t5-base\"", ",", "\"t5-large\"", ",", "\"t5-3b\"", ",", "\"t5-11b\"", ",", "\"ctrl\"", ",", "\n", "\"albert-base-v2\"", ",", "\"albert-large-v2\"", ",", "\"albert-xlarge-v2\"", ",", "\n", "\"roberta-base\"", ",", "\"roberta-large\"", ",", "\"xlnet-base-cased\"", ",", "\n", "\"xlnet-large-cased\"", ",", "\"transfo-xl-wt103\"", ",", "\"all\"", ",", "\n", "\"all-but-slow\"", "\n", "]", ",", "\n", "help", "=", "\"the name of the embedding model to prepare\"", ")", "\n", "parser", ".", "add_argument", "(", "'--device'", ",", "default", "=", "\"0\"", ",", "help", "=", "\"indices of GPUs to enable\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--yaspify\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"launch via slurm\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--slurm\"", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--use_cnodes\"", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--text_embedding_config_path\"", ",", "type", "=", "Path", ",", "\n", "default", "=", "\"model/text_embedding_models.json\"", ",", "\n", "help", "=", "\"the location of the config file containing the model paths\"", ")", "\n", "parser", ".", "add_argument", "(", "'--validate_embeddings'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"If given, compare the embeddings to a reference set\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--yaspi_defaults_path\"", ",", "type", "=", "Path", ",", "\n", "default", "=", "\"misc/yaspi_gpu_defaults.json\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "os", ".", "environ", "[", "\"CUDA_VISIBLE_DEVICES\"", "]", "=", "args", ".", "device", "\n", "\n", "common_kwargs", "=", "dict", "(", "\n", "refresh", "=", "args", ".", "refresh", ",", "\n", "limit", "=", "args", ".", "limit", ",", "\n", "processes", "=", "args", ".", "processes", ",", "\n", "data_dir", "=", "args", ".", "data_dir", ",", "\n", "rel_dest_dir", "=", "args", ".", "rel_dest_dir", ",", "\n", "datasets", "=", "args", ".", "datasets", ",", "\n", "embedding_name", "=", "args", ".", "embedding_name", ",", "\n", "text_embedding_config_path", "=", "args", ".", "text_embedding_config_path", ",", "\n", "validate_embeddings", "=", "args", ".", "validate_embeddings", ",", "\n", ")", "\n", "\n", "if", "args", ".", "yaspify", ":", "\n", "        ", "slow_models", "=", "{", "\n", "\"ctrl\"", ",", "\n", "\"t5-3b\"", ",", "\n", "\"t5-11b\"", ",", "\n", "\"xlnet-base-cased\"", ",", "\n", "\"xlnet-large-cased\"", ",", "\n", "\"transfo-xl-wt103\"", ",", "\n", "\"roberta-large\"", ",", "\n", "}", "\n", "with", "open", "(", "args", ".", "yaspi_defaults_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "yaspi_defaults", "=", "json", ".", "load", "(", "f", ")", "\n", "", "if", "args", ".", "use_cnodes", ":", "\n", "            ", "yaspi_defaults", ".", "update", "(", "{", "\"partition\"", ":", "\"compute\"", ",", "\"gpus_per_task\"", ":", "0", "}", ")", "\n", "", "if", "args", ".", "embedding_name", "in", "{", "\"all\"", ",", "\"all-but-slow\"", "}", ":", "\n", "            ", "embedding_names", "=", "[", "]", "\n", "with", "open", "(", "args", ".", "text_embedding_config_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "                ", "for", "name", ",", "vals", "in", "json", ".", "load", "(", "f", ")", ".", "items", "(", ")", ":", "\n", "                    ", "if", "args", ".", "embedding_name", "==", "\"all-but-slow\"", ":", "\n", "                        ", "if", "name", "in", "slow_models", ":", "\n", "                            ", "continue", "\n", "", "", "if", "not", "vals", ".", "get", "(", "\"custom_pipeline\"", ",", "False", ")", ":", "\n", "                        ", "embedding_names", ".", "append", "(", "name", ")", "\n", "", "", "", "", "else", ":", "\n", "            ", "embedding_names", "=", "[", "args", ".", "embedding_name", "]", "\n", "\n", "", "prepare_text_with_yaspi", "(", "\n", "datasets", "=", "args", ".", "datasets", ",", "\n", "common_kwargs", "=", "common_kwargs", ",", "\n", "yaspi_defaults", "=", "yaspi_defaults", ",", "\n", "embedding_names", "=", "embedding_names", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "if", "args", ".", "slurm", ":", "\n", "            ", "os", ".", "system", "(", "str", "(", "Path", ".", "home", "(", ")", "/", "\"configure_tmp_data.sh\"", ")", ")", "\n", "print", "(", "f\"Preparing embeddings via slurm on {socket.gethostname()}\"", ")", "\n", "", "extract_embeddings", "(", "**", "common_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.sync_experts.get_archive_name": [[23, 38], ["release.startswith", "release.startswith", "NotImplementedError"], "function", ["None"], ["@", "typechecked", "\n", "def", "get_archive_name", "(", "dataset", ":", "str", ",", "release", ":", "str", ",", "archive_type", ":", "str", ")", "->", "str", ":", "\n", "    ", "if", "release", ".", "startswith", "(", "\"challenge-release\"", ")", ":", "\n", "        ", "archive_name", "=", "f\"{release}-{dataset}-experts.tar.gz\"", "\n", "", "elif", "release", ".", "startswith", "(", "\"high-quality\"", ")", ":", "\n", "        ", "archive_name", "=", "f\"{release}-{dataset}-experts.tar.gz\"", "\n", "", "else", ":", "\n", "        ", "archive_name", "=", "f\"{dataset}-experts.tar.gz\"", "\n", "", "if", "archive_type", "==", "\"features\"", ":", "\n", "        ", "pass", "\n", "", "elif", "archive_type", "==", "\"videos\"", ":", "\n", "        ", "archive_name", "=", "f\"{archive_type}-{archive_name}\"", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "f\"Unsupported archive type: {archive_type}\"", ")", "\n", "", "return", "archive_name", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.sync_experts.upload_to_server": [[40, 112], ["release.startswith", "subprocess.call", "release.startswith", "zip", "tar_lists.items", "release.startswith", "time.time", "subprocess.call", "time.strftime", "print", "str", "pathlib.Path", "tar_includes.append", "sync_experts.get_archive_name", "compressed_paths.append", "sync_experts.get_archive_name", "sync_experts.get_archive_name", "compressed_path.parent.exists", "compressed_path.parent.mkdir", "print", "time.time", "os.system", "time.strftime", "print", "print", "str", "rsync_args.insert", "time.gmtime", "pathlib.Path", "pathlib.Path().exists", "time.gmtime", "str", "pathlib.Path", "str", "time.time", "pathlib.Path", "dataset.lower", "pathlib.Path", "dataset.lower", "pathlib.Path", "time.time", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.sync_experts.get_archive_name", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.sync_experts.get_archive_name", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.sync_experts.get_archive_name", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdir"], ["", "@", "typechecked", "\n", "def", "upload_to_server", "(", "\n", "web_dir", ":", "Path", ",", "\n", "dataset", ":", "str", ",", "\n", "release", ":", "str", ",", "\n", "webserver", ":", "str", ",", "\n", "refresh", ":", "Dict", "[", "str", ",", "bool", "]", ",", "\n", ")", ":", "\n", "    ", "if", "release", ".", "startswith", "(", "\"high-quality\"", ")", ":", "\n", "        ", "server_dir", "=", "web_dir", "/", "\"data-hq\"", "/", "release", "\n", "", "else", ":", "\n", "        ", "server_dir", "=", "web_dir", "/", "\"data\"", "/", "release", "\n", "", "subprocess", ".", "call", "(", "[", "\"ssh\"", ",", "webserver", ",", "\"mkdir -p\"", ",", "str", "(", "server_dir", ")", "]", ")", "\n", "if", "release", ".", "startswith", "(", "\"challenge-release\"", ")", ":", "\n", "        ", "dataset_dir", "=", "Path", "(", "\"misc/cvpr2020_challenge/datasets\"", ")", "/", "dataset", "\n", "# tar_include = dataset_dir / release / \"tar_include.txt\"", "\n", "tar_lists", "=", "{", "\n", "\"features\"", ":", "\"tar_include.txt\"", ",", "\n", "\"videos\"", ":", "\"video_tar_include.txt\"", ",", "\n", "}", "\n", "tar_includes", ",", "compressed_paths", "=", "[", "]", ",", "[", "]", "\n", "for", "key", ",", "tar_list", "in", "tar_lists", ".", "items", "(", ")", ":", "\n", "            ", "tar_includes", ".", "append", "(", "dataset_dir", "/", "release", "/", "tar_list", ")", "\n", "compressed_file", "=", "get_archive_name", "(", "\n", "dataset", "=", "dataset", ",", "\n", "release", "=", "release", ",", "\n", "archive_type", "=", "key", ",", "\n", ")", "\n", "compressed_path", "=", "Path", "(", "f\"data/{dataset}/webserver-files\"", ")", "/", "compressed_file", "\n", "compressed_paths", ".", "append", "(", "compressed_path", ")", "\n", "", "", "elif", "release", ".", "startswith", "(", "\"high-quality\"", ")", ":", "\n", "        ", "tar_includes", "=", "[", "Path", "(", "\"misc/datasets\"", ")", "/", "dataset", ".", "lower", "(", ")", "/", "\"tar_include_hq.txt\"", "]", "\n", "compressed_file", "=", "get_archive_name", "(", "\n", "dataset", "=", "dataset", ",", "\n", "release", "=", "release", ",", "\n", "archive_type", "=", "\"features\"", ",", "\n", ")", "\n", "#compressed_paths = [Path(\"data\") / dataset / \"webserver-files\" / compressed_file]", "\n", "compressed_paths", "=", "[", "Path", "(", "\"/scratch/shared/beegfs/ioana/webserver-files/\"", ")", "/", "compressed_file", "]", "\n", "", "else", ":", "\n", "        ", "tar_includes", "=", "[", "Path", "(", "\"misc/datasets\"", ")", "/", "dataset", ".", "lower", "(", ")", "/", "\"tar_include.txt\"", "]", "\n", "compressed_file", "=", "get_archive_name", "(", "\n", "dataset", "=", "dataset", ",", "\n", "release", "=", "release", ",", "\n", "archive_type", "=", "\"features\"", ",", "\n", ")", "\n", "compressed_paths", "=", "[", "Path", "(", "\"data\"", ")", "/", "dataset", "/", "\"webserver-files\"", "/", "compressed_file", "]", "\n", "\n", "", "for", "tar_include", ",", "compressed_path", "in", "zip", "(", "tar_includes", ",", "compressed_paths", ")", ":", "\n", "        ", "if", "not", "compressed_path", ".", "parent", ".", "exists", "(", ")", ":", "\n", "            ", "compressed_path", ".", "parent", ".", "mkdir", "(", "exist_ok", "=", "True", ",", "parents", "=", "True", ")", "\n", "", "if", "not", "Path", "(", "compressed_path", ")", ".", "exists", "(", ")", "or", "refresh", "[", "\"compression\"", "]", ":", "\n", "            ", "compression_args", "=", "(", "f\"tar --dereference --create --verbose\"", "\n", "f\" --file={str(compressed_path)}\"", "\n", "f\" --use-compress-program=pigz\"", "\n", "f\" --files-from={tar_include}\"", ")", "\n", "print", "(", "f\"running command {compression_args}\"", ")", "\n", "tic", "=", "time", ".", "time", "(", ")", "\n", "os", ".", "system", "(", "compression_args", ")", "\n", "duration", "=", "time", ".", "strftime", "(", "'%Hh%Mm%Ss'", ",", "time", ".", "gmtime", "(", "time", ".", "time", "(", ")", "-", "tic", ")", ")", "\n", "print", "(", "f\"Finished tar contents features in {duration}\"", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "f\"Found existing compressed file at {compressed_path}, skipping....\"", ")", "\n", "\n", "", "dest", "=", "f\"{webserver}:{str(server_dir / compressed_path.name)}\"", "\n", "rsync_args", "=", "[", "\"rsync\"", ",", "\"-av\"", ",", "\"--progress\"", ",", "str", "(", "compressed_path", ")", ",", "dest", "]", "\n", "if", "not", "refresh", "[", "\"server\"", "]", ":", "\n", "            ", "rsync_args", ".", "insert", "(", "1", ",", "\"--ignore-existing\"", ")", "\n", "", "tic", "=", "time", ".", "time", "(", ")", "\n", "subprocess", ".", "call", "(", "rsync_args", ")", "\n", "duration", "=", "time", ".", "strftime", "(", "'%Hh%Mm%Ss'", ",", "time", ".", "gmtime", "(", "time", ".", "time", "(", ")", "-", "tic", ")", ")", "\n", "print", "(", "f\"Finished transferring tar file in {duration}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.sync_experts.upload_models_to_robots": [[117, 162], ["json.load.items", "open", "json.load", "os.listdir", "subprocess.call", "subprocess.call", "time.time", "subprocess.call", "time.strftime", "print", "time.time", "subprocess.call", "time.strftime", "print", "time.time", "subprocess.call", "time.strftime", "print", "sorted", "pathlib.Path", "str", "time.gmtime", "str", "time.gmtime", "str", "time.gmtime", "os.listdir", "pathlib.Path", "str", "str", "str", "str", "pathlib.Path", "pathlib.Path", "time.time", "pathlib.Path", "pathlib.Path", "time.time", "time.time", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["", "", "@", "typechecked", "\n", "def", "upload_models_to_robots", "(", "web_dir", ":", "Path", ",", "experiments", ":", "Path", ",", "\n", "save_dir", ":", "Path", ",", "webserver", ":", "str", ")", ":", "\n", "    ", "with", "open", "(", "experiments", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "experiments", "=", "json", ".", "load", "(", "f", ")", "\n", "", "experiments_items", "=", "experiments", ".", "items", "(", ")", "\n", "server_dir", "=", "web_dir", "/", "\"data\"", "\n", "for", "exp_name", ",", "meta", "in", "experiments_items", ":", "\n", "        ", "if", "\"queryd\"", "in", "exp_name", ":", "\n", "            ", "group_id", ",", "timestamp", "=", "meta", "\n", "seed_folder", "=", "sorted", "(", "os", ".", "listdir", "(", "Path", "(", "save_dir", ")", "/", "\"log\"", "/", "Path", "(", "exp_name", ")", "/", "group_id", ")", ")", "[", "0", "]", "\n", "files_in_seed_folder", "=", "os", ".", "listdir", "(", "Path", "(", "save_dir", ")", "/", "\"log\"", "/", "Path", "(", "exp_name", ")", "/", "group_id", "/", "seed_folder", "/", "Path", "(", "timestamp", ")", ")", "\n", "for", "file", "in", "files_in_seed_folder", ":", "\n", "                ", "if", "\".json\"", "in", "file", "and", "\".bak\"", "not", "in", "file", ":", "\n", "                    ", "fname", "=", "file", "\n", "break", "\n", "", "", "rel_path", "=", "Path", "(", "exp_name", ")", "/", "group_id", "/", "seed_folder", "/", "Path", "(", "timestamp", ")", "\n", "log_path", "=", "Path", "(", "save_dir", ")", "/", "\"log\"", "/", "rel_path", "/", "fname", "\n", "server_log_path", "=", "server_dir", "/", "\"log\"", "/", "rel_path", "\n", "model_config_path", "=", "server_dir", "/", "\"models\"", "/", "rel_path", "\n", "\n", "subprocess", ".", "call", "(", "[", "\"ssh\"", ",", "webserver", ",", "\"mkdir -p\"", ",", "str", "(", "server_log_path", ")", "]", ")", "\n", "subprocess", ".", "call", "(", "[", "\"ssh\"", ",", "webserver", ",", "\"mkdir -p\"", ",", "str", "(", "model_config_path", ")", "]", ")", "\n", "dest_log", "=", "f\"{webserver}:{str(server_log_path)}\"", "\n", "rsync_args_log", "=", "[", "\"rsync\"", ",", "\"-av\"", ",", "\"--progress\"", ",", "\"--ignore-existing\"", ",", "str", "(", "log_path", ")", ",", "dest_log", "]", "\n", "\n", "tic", "=", "time", ".", "time", "(", ")", "\n", "subprocess", ".", "call", "(", "rsync_args_log", ")", "\n", "duration", "=", "time", ".", "strftime", "(", "'%Hh%Mm%Ss'", ",", "time", ".", "gmtime", "(", "time", ".", "time", "(", ")", "-", "tic", ")", ")", "\n", "print", "(", "f\"Finished transferring log file for experiment {exp_name} in {duration}\"", ")", "\n", "\n", "model_path", "=", "Path", "(", "save_dir", ")", "/", "\"models\"", "/", "rel_path", "/", "\"trained_model.pth\"", "\n", "config_path", "=", "Path", "(", "save_dir", ")", "/", "\"models\"", "/", "rel_path", "/", "\"config.json\"", "\n", "\n", "dest_model_config", "=", "f\"{webserver}:{str(model_config_path)}\"", "\n", "rsync_args_model", "=", "[", "\"rsync\"", ",", "\"-av\"", ",", "\"--progress\"", ",", "\"--ignore-existing\"", ",", "str", "(", "model_path", ")", ",", "dest_model_config", "]", "\n", "tic", "=", "time", ".", "time", "(", ")", "\n", "subprocess", ".", "call", "(", "rsync_args_model", ")", "\n", "duration", "=", "time", ".", "strftime", "(", "'%Hh%Mm%Ss'", ",", "time", ".", "gmtime", "(", "time", ".", "time", "(", ")", "-", "tic", ")", ")", "\n", "print", "(", "f\"Finished transferring model for experiment {exp_name} in {duration}\"", ")", "\n", "rsync_args_config", "=", "[", "\"rsync\"", ",", "\"-av\"", ",", "\"--progress\"", ",", "\"--ignore-existing\"", ",", "str", "(", "config_path", ")", ",", "dest_model_config", "]", "\n", "tic", "=", "time", ".", "time", "(", ")", "\n", "subprocess", ".", "call", "(", "rsync_args_config", ")", "\n", "duration", "=", "time", ".", "strftime", "(", "'%Hh%Mm%Ss'", ",", "time", ".", "gmtime", "(", "time", ".", "time", "(", ")", "-", "tic", ")", ")", "\n", "print", "(", "f\"Finished transferring config file for experiment {exp_name} in {duration}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.sync_experts.fetch_from_server": [[165, 199], ["local_data_dir.mkdir", "sync_experts.get_archive_name", "subprocess.call", "pathlib.Path", "symlinked_feats_dir.exists", "print", "local_archive.exists", "print", "subprocess.call", "print", "str", "local_archive.unlink", "hashlib.sha256().hexdigest", "str", "hashlib.sha256", "access_code.encode"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdir", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.sync_experts.get_archive_name"], ["", "", "", "@", "typechecked", "\n", "def", "fetch_from_server", "(", "\n", "dataset", ":", "str", ",", "\n", "root_url", ":", "str", ",", "\n", "purge_tar_file", ":", "bool", ",", "\n", "release", ":", "str", ",", "\n", "refresh", ":", "Dict", "[", "str", ",", "bool", "]", ",", "\n", "access_code", ":", "str", "=", "None", ",", "\n", ")", ":", "\n", "    ", "local_data_dir", "=", "Path", "(", "\"data\"", ")", "/", "dataset", "\n", "symlinked_feats_dir", "=", "local_data_dir", "/", "\"symlinked-feats\"", "\n", "if", "symlinked_feats_dir", ".", "exists", "(", ")", "and", "not", "refresh", "[", "\"symlinked-feats\"", "]", ":", "\n", "        ", "print", "(", "f\"Found symlinked feats at {symlinked_feats_dir}, skipping\"", ")", "\n", "return", "\n", "\n", "", "local_data_dir", ".", "mkdir", "(", "exist_ok", "=", "True", ",", "parents", "=", "True", ")", "\n", "archive_name", "=", "get_archive_name", "(", "dataset", ",", "release", "=", "release", ",", "archive_type", "=", "\"features\"", ")", "\n", "local_archive", "=", "local_data_dir", "/", "archive_name", "\n", "if", "not", "local_archive", ".", "exists", "(", ")", ":", "\n", "        ", "if", "access_code", ":", "\n", "            ", "access_hash", "=", "hashlib", ".", "sha256", "(", "access_code", ".", "encode", "(", "\"utf-8\"", ")", ")", ".", "hexdigest", "(", ")", "[", ":", "10", "]", "\n", "archive_name", "=", "f\"{access_hash}-{archive_name}\"", "\n", "", "src_url", "=", "f\"{root_url}/{release}/{archive_name}\"", "\n", "wget_args", "=", "[", "\"wget\"", ",", "f\"--output-document={str(local_archive)}\"", ",", "src_url", "]", "\n", "print", "(", "f\"running command: {' '.join(wget_args)}\"", ")", "\n", "subprocess", ".", "call", "(", "wget_args", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "f\"found archive at {local_archive}, skipping...\"", ")", "\n", "\n", "# unpack the archive and optionally clean up", "\n", "", "untar_args", "=", "[", "\"tar\"", ",", "\"-xvf\"", ",", "str", "(", "local_archive", ")", "]", "\n", "subprocess", ".", "call", "(", "untar_args", ")", "\n", "if", "purge_tar_file", ":", "\n", "        ", "local_archive", ".", "unlink", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.sync_experts.main": [[201, 267], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "sync_experts.upload_to_server", "sync_experts.fetch_from_server", "sync_experts.upload_models_to_robots", "ValueError"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.sync_experts.upload_to_server", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.sync_experts.fetch_from_server", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.sync_experts.upload_models_to_robots"], ["", "", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--dataset\"", ",", "nargs", "=", "\"+\"", ",", "\n", "default", "=", "[", "\"MSRVTT\"", ",", "\"MSVD\"", ",", "\"DiDeMo\"", ",", "\"activity-net\"", ",", "\"YouCook2\"", "]", ",", "\n", "choices", "=", "[", "\"LSMDC\"", ",", "\"MSRVTT\"", ",", "\"MSVD\"", ",", "\"DiDeMo\"", ",", "\"activity-net\"", ",", "\n", "\"YouCook2\"", ",", "\"QuerYD\"", ",", "\"QuerYDSegments\"", "]", ")", "\n", "parser", ".", "add_argument", "(", "\"--action\"", ",", "default", "=", "\"fetch\"", ",", "choices", "=", "[", "\"upload\"", ",", "\"fetch\"", ",", "\"model\"", "]", ")", "\n", "parser", ".", "add_argument", "(", "\"--webserver\"", ",", "default", "=", "\"login.robots.ox.ac.uk\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--refresh_compression\"", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--refresh_server\"", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--refresh_symlinked_feats\"", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--purge_tar_file\"", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--experiments_path\"", ",", "type", "=", "Path", ",", "default", "=", "\"misc/experiments.json\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_dir\"", ",", "default", "=", "\"data/saved\"", ",", "type", "=", "Path", ")", "\n", "parser", ".", "add_argument", "(", "\"--release\"", ",", "default", "=", "\"features-v2\"", ",", "\n", "choices", "=", "[", "\"features-v2\"", ",", "\"challenge-release-1\"", ",", "\n", "\"challenge-release-2\"", ",", "\"high-quality\"", "]", ",", "\n", "help", "=", "(", "\"The features to fetch (features-v2 refers to the features\"", "\n", "\" that can be used to reproduce the collaborative experts\"", "\n", "\"paper\"", ")", ")", "\n", "parser", ".", "add_argument", "(", "\"--access_code\"", ",", "help", "=", "\"Code to access LSMDC\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--web_dir\"", ",", "type", "=", "Path", ",", "\n", "default", "=", "\"/projects/vgg/vgg/WWW/research/collaborative-experts\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--root_url\"", ",", "\n", "default", "=", "\"http://www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data\"", ",", "\n", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "refresh_targets", "=", "{", "\n", "\"server\"", ":", "args", ".", "refresh_server", ",", "\n", "\"compression\"", ":", "args", ".", "refresh_compression", ",", "\n", "\"symlinked-feats\"", ":", "args", ".", "refresh_symlinked_feats", ",", "\n", "}", "\n", "\n", "for", "dataset", "in", "args", ".", "dataset", ":", "\n", "        ", "if", "dataset", "==", "\"LSMDC\"", ":", "\n", "            ", "msg", "=", "(", "\"To download LSMDC, you must obtain an access code (please see \"", "\n", "\"README.md for details\"", ")", "\n", "assert", "args", ".", "access_code", ",", "msg", "\n", "", "if", "args", ".", "action", "==", "\"upload\"", ":", "\n", "            ", "upload_to_server", "(", "\n", "web_dir", "=", "args", ".", "web_dir", ",", "\n", "dataset", "=", "dataset", ",", "\n", "refresh", "=", "refresh_targets", ",", "\n", "webserver", "=", "args", ".", "webserver", ",", "\n", "release", "=", "args", ".", "release", ",", "\n", ")", "\n", "", "elif", "args", ".", "action", "==", "\"fetch\"", ":", "\n", "            ", "fetch_from_server", "(", "\n", "dataset", "=", "dataset", ",", "\n", "release", "=", "args", ".", "release", ",", "\n", "root_url", "=", "args", ".", "root_url", ",", "\n", "refresh", "=", "refresh_targets", ",", "\n", "purge_tar_file", "=", "args", ".", "purge_tar_file", ",", "\n", "access_code", "=", "args", ".", "access_code", ",", "\n", ")", "\n", "", "elif", "args", ".", "action", "==", "\"model\"", ":", "\n", "            ", "upload_models_to_robots", "(", "\n", "web_dir", "=", "args", ".", "web_dir", ",", "\n", "experiments", "=", "args", ".", "experiments_path", ",", "\n", "save_dir", "=", "args", ".", "save_dir", ",", "\n", "webserver", "=", "args", ".", "webserver", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "f\"unknown action: {args.action}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_tar_lists.generate_tar_lists": [[16, 78], ["tqdm.tqdm", "all_feat_paths.items", "experiments.items", "set", "gen_readme.dataset_paths", "gen_readme.model_specs2path", "all_feat_paths[].update", "paths[].items", "all_feat_paths[].update", "all_feat_paths[].add", "tar_include_list.parent.mkdir", "exp_name.split", "set", "split_names.append", "set", "all_feat_paths[].add", "set", "all_feat_paths[].update", "tar_include_list.exists", "print", "open", "sorted", "open", "json.load", "x.lower", "feat_aggregation.items", "all_feat_paths[].add", "pathlib.Path", "pathlib.Path", "open", "json.load", "pathlib.Path", "str", "print", "f.write", "[].values", "pathlib.Path", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.dataset_paths", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.model_specs2path", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.add", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdir", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.add", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.add"], ["@", "beartype", "\n", "def", "generate_tar_lists", "(", "\n", "save_dir", ":", "Path", ",", "\n", "experiments", ":", "Dict", "[", "str", ",", "Tuple", "[", "str", ",", "str", "]", "]", ",", "\n", "datasets", ":", "List", "[", "str", "]", ",", "\n", "refresh", ":", "bool", ",", "\n", ")", ":", "\n", "    ", "all_feat_paths", "=", "{", "}", "\n", "for", "exp_name", ",", "(", "group_id", ",", "timestamp", ")", "in", "tqdm", ".", "tqdm", "(", "experiments", ".", "items", "(", ")", ")", ":", "\n", "        ", "rel_path", "=", "Path", "(", "group_id", ")", "/", "\"seed-0\"", "/", "timestamp", "/", "\"config.json\"", "\n", "config_path", "=", "Path", "(", "save_dir", ")", "/", "\"models\"", "/", "exp_name", "/", "rel_path", "\n", "try", ":", "\n", "            ", "with", "open", "(", "config_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "                ", "config", "=", "json", ".", "load", "(", "f", ")", "\n", "", "", "except", "FileNotFoundError", ":", "\n", "            ", "rel_path", "=", "Path", "(", "group_id", ")", "/", "\"seed-1\"", "/", "timestamp", "/", "\"config.json\"", "\n", "config_path", "=", "Path", "(", "save_dir", ")", "/", "\"models\"", "/", "exp_name", "/", "rel_path", "\n", "with", "open", "(", "config_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "                ", "config", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "", "feat_aggregation", "=", "config", "[", "\"data_loader\"", "]", "[", "\"args\"", "]", "[", "\"feat_aggregation\"", "]", "\n", "dataset_name", "=", "exp_name", ".", "split", "(", "\"-train\"", ")", "[", "0", "]", "\n", "if", "dataset_name", "not", "in", "[", "x", ".", "lower", "(", ")", "for", "x", "in", "datasets", "]", ":", "\n", "            ", "continue", "\n", "", "if", "dataset_name", "not", "in", "all_feat_paths", ":", "\n", "            ", "all_feat_paths", "[", "dataset_name", "]", "=", "set", "(", ")", "\n", "", "split_names", "=", "[", "config", "[", "\"data_loader\"", "]", "[", "\"args\"", "]", "[", "\"split_name\"", "]", "]", "\n", "if", "\"eval_settings\"", "in", "config", "and", "config", "[", "\"eval_settings\"", "]", ":", "\n", "            ", "test_split", "=", "config", "[", "\"eval_settings\"", "]", "[", "\"data_loader\"", "]", "[", "\"args\"", "]", "[", "\"split_name\"", "]", "\n", "split_names", ".", "append", "(", "test_split", ")", "\n", "", "keep", "=", "set", "(", "config", "[", "\"experts\"", "]", "[", "\"modalities\"", "]", ")", "\n", "text_feat", "=", "config", "[", "\"experts\"", "]", "[", "\"text_feat\"", "]", "\n", "root_feat", ",", "paths", "=", "dataset_paths", "(", "dataset_name", ")", "\n", "modern_feat_agg", "=", "{", "key", ":", "val", "for", "key", ",", "val", "in", "feat_aggregation", ".", "items", "(", ")", "\n", "if", "key", "in", "paths", "[", "\"feature_names\"", "]", "}", "\n", "feat_paths", "=", "model_specs2path", "(", "modern_feat_agg", ",", "keep", ")", "\n", "all_feat_paths", "[", "dataset_name", "]", ".", "update", "(", "{", "root_feat", "/", "x", "for", "x", "in", "feat_paths", "}", ")", "\n", "for", "key", ",", "feat_list", "in", "paths", "[", "\"custom_paths\"", "]", ".", "items", "(", ")", ":", "\n", "            ", "for", "feat_path", "in", "feat_list", ":", "\n", "                ", "all_feat_paths", "[", "dataset_name", "]", ".", "add", "(", "root_feat", "/", "feat_path", ")", "\n", "", "", "text_paths", "=", "[", "root_feat", "/", "paths", "[", "\"text_feat_paths\"", "]", "[", "text_feat", "]", "]", "\n", "all_feat_paths", "[", "dataset_name", "]", ".", "update", "(", "set", "(", "text_paths", ")", ")", "\n", "all_feat_paths", "[", "dataset_name", "]", ".", "add", "(", "root_feat", "/", "paths", "[", "\"raw_captions_path\"", "]", ")", "\n", "if", "\"dict_youtube_mapping_path\"", "in", "paths", ":", "\n", "            ", "all_feat_paths", "[", "dataset_name", "]", ".", "add", "(", "\n", "root_feat", "/", "paths", "[", "\"dict_youtube_mapping_path\"", "]", ")", "\n", "", "for", "split_name", "in", "split_names", ":", "\n", "            ", "split_paths", "=", "set", "(", "root_feat", "/", "x", "for", "x", "in", "\n", "paths", "[", "\"subset_list_paths\"", "]", "[", "split_name", "]", ".", "values", "(", ")", ")", "\n", "all_feat_paths", "[", "dataset_name", "]", ".", "update", "(", "split_paths", ")", "\n", "\n", "", "", "for", "dataset_name", ",", "paths", "in", "all_feat_paths", ".", "items", "(", ")", ":", "\n", "        ", "tar_include_list", "=", "Path", "(", "\"misc\"", ")", "/", "\"datasets\"", "/", "dataset_name", "/", "\"tar_include.txt\"", "\n", "tar_include_list", ".", "parent", ".", "mkdir", "(", "exist_ok", "=", "True", ",", "parents", "=", "True", ")", "\n", "if", "tar_include_list", ".", "exists", "(", ")", "and", "not", "refresh", ":", "\n", "            ", "print", "(", "f\"Found existing tar include list at {tar_include_list}, skipping...\"", ")", "\n", "continue", "\n", "", "with", "open", "(", "tar_include_list", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "for", "path", "in", "sorted", "(", "paths", ")", ":", "\n", "                ", "if", "\"aggregated_speech\"", "not", "in", "str", "(", "path", ")", ":", "\n", "                    ", "print", "(", "f\"Writing {path} to {tar_include_list}\"", ")", "\n", "f", ".", "write", "(", "f\"{path}\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_tar_lists.generate_tar_lists_for_challenge": [[80, 166], ["pathlib.Path", "tar_include_list.parent.mkdir", "list", "print", "zsvision.zs_utils.load_json_config", "set", "gen_readme.dataset_paths", "set", "print", "print", "tar_include_list.exists", "video_tar_include_list.exists", "print", "src_folder.glob", "isinstance", "any", "open", "print", "sorted", "open", "print", "sorted", "dataset.lower", "pathlib.Path", "feat_aggregation.items", "copy.deepcopy", "set.update", "set.add", "isinstance", "filtered_rel_paths.append", "f.write", "f.write", "len", "gen_readme.model_specs2path", "paths.values", "TypeError", "str().endswith", "len", "len", "str().startswith", "x.is_file", "len", "isinstance", "str", "str", "set.update", "isinstance", "str", "str", "val.values", "set.add", "isinstance", "type", "set.update", "TypeError", "type"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdir", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.dataset_paths", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.add", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.model_specs2path", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.add", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update"], ["", "", "", "", "", "@", "beartype", "\n", "def", "generate_tar_lists_for_challenge", "(", "\n", "refresh", ":", "bool", ",", "\n", "datasets", ":", "List", "[", "str", "]", ",", "\n", "challenge_phase", ":", "str", ",", "\n", "data_dir", ":", "Path", ",", "\n", ")", ":", "\n", "    ", "phase_dirs", "=", "{", "\n", "\"public_server_val\"", ":", "\"challenge-release-1\"", ",", "\n", "\"public_server_test\"", ":", "\"challenge-release-2\"", ",", "\n", "}", "\n", "base", "=", "Path", "(", "\"misc/cvpr2020_challenge/datasets\"", ")", "\n", "challenge_dir", "=", "phase_dirs", "[", "challenge_phase", "]", "\n", "for", "dataset", "in", "datasets", ":", "\n", "        ", "tar_include_list", "=", "base", "/", "dataset", "/", "challenge_dir", "/", "\"tar_include.txt\"", "\n", "video_tar_include_list", "=", "base", "/", "dataset", "/", "challenge_dir", "/", "\"video_tar_include.txt\"", "\n", "if", "tar_include_list", ".", "exists", "(", ")", "and", "video_tar_include_list", ".", "exists", "(", ")", "and", "not", "refresh", ":", "\n", "            ", "print", "(", "f\"Found lists at {tar_include_list}/{video_tar_include_list}, skipping\"", ")", "\n", "continue", "\n", "", "tar_include_list", ".", "parent", ".", "mkdir", "(", "exist_ok", "=", "True", ",", "parents", "=", "True", ")", "\n", "src_folder", "=", "data_dir", "/", "dataset", "/", "challenge_dir", "\n", "rel_paths", "=", "list", "(", "src_folder", ".", "glob", "(", "\"**/*\"", ")", ")", "\n", "\n", "print", "(", "f\"Found {len(rel_paths)} files in {src_folder}\"", ")", "\n", "fname", "=", "f\"data_loader_{dataset.lower()}.json\"", "\n", "config_path", "=", "Path", "(", "\"configs\"", ")", "/", "\"cvpr2020-challenge\"", "/", "fname", "\n", "config", "=", "load_json_config", "(", "config_path", ")", "\n", "\n", "keep", "=", "set", "(", "config", "[", "\"experts\"", "]", "[", "\"modalities\"", "]", ")", "\n", "feat_aggregation", "=", "config", "[", "\"data_loader\"", "]", "[", "\"args\"", "]", "[", "\"feat_aggregation\"", "]", "\n", "_", ",", "all_dataset_paths", "=", "dataset_paths", "(", "dataset", "=", "dataset", ")", "\n", "modern_feat_agg", "=", "{", "key", ":", "val", "for", "key", ",", "val", "in", "feat_aggregation", ".", "items", "(", ")", "\n", "if", "key", "in", "all_dataset_paths", "[", "\"feature_names\"", "]", "}", "\n", "expected_feat_paths", "=", "set", "(", ")", "\n", "for", "feat_type", "in", "(", "\"embed\"", ",", "\"logits\"", ")", ":", "\n", "            ", "for", "agg_type", "in", "(", "\"avg\"", ",", "\"max\"", ",", "\"fixed_seg\"", ")", ":", "\n", "                ", "modern_feat_agg_", "=", "copy", ".", "deepcopy", "(", "modern_feat_agg", ")", "\n", "for", "feat_key", "in", "modern_feat_agg_", ":", "\n", "                    ", "modern_feat_agg_", "[", "feat_key", "]", "[", "\"temporal\"", "]", "=", "agg_type", "\n", "modern_feat_agg_", "[", "feat_key", "]", "[", "\"type\"", "]", "=", "feat_type", "\n", "", "expected_feat_paths", ".", "update", "(", "(", "model_specs2path", "(", "modern_feat_agg_", ",", "keep", ")", ")", ")", "\n", "", "", "keep", "=", "{", "\n", "\"custom_paths\"", ",", "\n", "\"challenge_text_feat_paths\"", ",", "\n", "\"raw_captions_path\"", ",", "\n", "\"subset_list_paths\"", ",", "\n", "}", "\n", "for", "key", "in", "keep", ":", "\n", "            ", "paths", "=", "all_dataset_paths", "[", "key", "]", "\n", "if", "isinstance", "(", "paths", ",", "str", ")", ":", "\n", "                ", "expected_feat_paths", ".", "add", "(", "paths", ")", "\n", "", "elif", "isinstance", "(", "paths", ",", "dict", ")", ":", "\n", "                ", "for", "val", "in", "paths", ".", "values", "(", ")", ":", "\n", "                    ", "if", "isinstance", "(", "val", ",", "dict", ")", ":", "\n", "                        ", "expected_feat_paths", ".", "update", "(", "val", ".", "values", "(", ")", ")", "\n", "", "elif", "isinstance", "(", "val", ",", "(", "str", ",", "Path", ")", ")", ":", "\n", "                        ", "expected_feat_paths", ".", "add", "(", "val", ")", "\n", "", "elif", "isinstance", "(", "val", ",", "list", ")", ":", "\n", "                        ", "expected_feat_paths", ".", "update", "(", "val", ")", "\n", "", "else", ":", "\n", "                        ", "raise", "TypeError", "(", "f\"Unexpected type: {type(val)}\"", ")", "\n", "", "", "", "else", ":", "\n", "                ", "raise", "TypeError", "(", "f\"Unexpected type: {type(paths)}\"", ")", "\n", "\n", "# filter to relevant features", "\n", "", "", "filtered_rel_paths", "=", "[", "]", "\n", "for", "rel_path", "in", "rel_paths", ":", "\n", "            ", "if", "any", "(", "[", "str", "(", "rel_path", ")", ".", "endswith", "(", "str", "(", "x", ")", ")", "for", "x", "in", "expected_feat_paths", "]", ")", ":", "\n", "                ", "filtered_rel_paths", ".", "append", "(", "rel_path", ")", "\n", "", "", "print", "(", "f\"[{dataset}] Filtered from {len(rel_paths)} to {len(filtered_rel_paths)}\"", ")", "\n", "\n", "with", "open", "(", "tar_include_list", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "print", "(", "f\"Writing paths to {tar_include_list}\"", ")", "\n", "for", "path", "in", "sorted", "(", "filtered_rel_paths", ")", ":", "\n", "                ", "f", ".", "write", "(", "f\"{path}\\n\"", ")", "\n", "\n", "# select video paths", "\n", "", "", "video_dir", "=", "src_folder", "/", "\"videos\"", "\n", "video_paths", "=", "[", "x", "for", "x", "in", "rel_paths", "if", "str", "(", "x", ")", ".", "startswith", "(", "str", "(", "video_dir", ")", ")", "\n", "and", "x", ".", "is_file", "(", ")", "]", "\n", "print", "(", "f\"[{dataset}] Found {len(video_paths)} video paths\"", ")", "\n", "\n", "with", "open", "(", "video_tar_include_list", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "print", "(", "f\"Writing video paths to {video_tar_include_list}\"", ")", "\n", "for", "path", "in", "sorted", "(", "video_paths", ")", ":", "\n", "                ", "f", ".", "write", "(", "f\"{path}\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_tar_lists.main": [[168, 199], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "open", "json.load", "gen_tar_lists.generate_tar_lists", "gen_tar_lists.generate_tar_lists_for_challenge"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.generate_tar_lists", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_tar_lists.generate_tar_lists_for_challenge"], ["", "", "", "", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_dir\"", ",", "default", "=", "\"data/saved\"", ",", "type", "=", "Path", ")", "\n", "parser", ".", "add_argument", "(", "\"--refresh\"", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--experiments_path\"", ",", "default", "=", "\"misc/experiments.json\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--target\"", ",", "default", "=", "\"main\"", ",", "\n", "choices", "=", "[", "\"main\"", ",", "\"cvpr2020_challenge\"", "]", ")", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "type", "=", "Path", ",", "default", "=", "\"data\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--challenge_phase\"", ",", "default", "=", "\"public_server_val\"", ",", "\n", "choices", "=", "[", "\"public_server_val\"", ",", "\"public_server_test\"", "]", ")", "\n", "parser", ".", "add_argument", "(", "\"--datasets\"", ",", "nargs", "=", "\"+\"", ",", "\n", "default", "=", "[", "\"MSRVTT\"", ",", "\"MSVD\"", ",", "\"DiDeMo\"", ",", "\"activity-net\"", ",", "\"YouCook2\"", ",", "\n", "\"QuerYD\"", ",", "\"QuerYDSegments\"", "]", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "with", "open", "(", "args", ".", "experiments_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "experiments", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "if", "args", ".", "target", "==", "\"main\"", ":", "\n", "        ", "generate_tar_lists", "(", "\n", "save_dir", "=", "args", ".", "save_dir", ",", "\n", "datasets", "=", "args", ".", "datasets", ",", "\n", "experiments", "=", "experiments", ",", "\n", "refresh", "=", "args", ".", "refresh", ",", "\n", ")", "\n", "", "elif", "args", ".", "target", "==", "\"cvpr2020_challenge\"", ":", "\n", "        ", "generate_tar_lists_for_challenge", "(", "\n", "refresh", "=", "args", ".", "refresh", ",", "\n", "datasets", "=", "args", ".", "datasets", ",", "\n", "data_dir", "=", "args", ".", "data_dir", ",", "\n", "challenge_phase", "=", "args", ".", "challenge_phase", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.find_latest_checkpoints.formatted_summary": [[10, 35], ["print", "sorted", "list", "summary.relative_to", "latest.items", "print", "pathlib.Path().glob", "list", "datetime.datetime.strptime", "datetime.datetime.strptime", "pathlib.Path().glob", "len", "pathlib.Path", "str", "pathlib.Path", "str"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["def", "formatted_summary", "(", "dataset", ",", "exp_root", ",", "fname", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "summaries", "=", "list", "(", "Path", "(", "exp_root", ")", ".", "glob", "(", "f\"**/*{fname}\"", ")", ")", "\n", "summaries", "=", "[", "x", "for", "x", "in", "summaries", "if", "dataset", "in", "str", "(", "x", ")", "]", "\n", "", "except", "FileNotFoundError", ":", "\n", "        ", "fname", "=", "\"summary-seed-1_seed-2_seed-3.json\"", "\n", "summaries", "=", "list", "(", "Path", "(", "exp_root", ")", ".", "glob", "(", "f\"**/*{fname}\"", ")", ")", "\n", "summaries", "=", "[", "x", "for", "x", "in", "summaries", "if", "dataset", "in", "str", "(", "x", ")", "]", "\n", "", "print", "(", "f\"Found {len(summaries)}\"", ")", "\n", "latest", "=", "{", "}", "\n", "time_format", "=", "\"%Y-%m-%d_%H-%M-%S\"", "\n", "for", "summary", "in", "summaries", ":", "\n", "        ", "rel_path", "=", "summary", ".", "relative_to", "(", "exp_root", ")", "\n", "key", ",", "group", ",", "timestamp", "=", "rel_path", ".", "parts", "[", "0", "]", ",", "rel_path", ".", "parts", "[", "1", "]", ",", "rel_path", ".", "parts", "[", "3", "]", "\n", "val", "=", "{", "\"timestamp\"", ":", "timestamp", ",", "\"group\"", ":", "group", "}", "\n", "if", "key", "in", "latest", ":", "\n", "            ", "prev_ts", "=", "datetime", ".", "strptime", "(", "latest", "[", "key", "]", "[", "\"timestamp\"", "]", ",", "time_format", ")", "\n", "curr_ts", "=", "datetime", ".", "strptime", "(", "timestamp", ",", "time_format", ")", "\n", "if", "curr_ts", ">", "prev_ts", ":", "\n", "                ", "latest", "[", "key", "]", "=", "val", "\n", "", "", "else", ":", "\n", "            ", "latest", "[", "key", "]", "=", "val", "\n", "", "", "for", "key", ",", "val", "in", "sorted", "(", "latest", ".", "items", "(", ")", ")", ":", "\n", "        ", "ts", ",", "group", "=", "val", "[", "\"timestamp\"", "]", ",", "val", "[", "\"group\"", "]", "\n", "print", "(", "f'\"{key}\": [\"{group}\", \"{ts}\"],'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.find_latest_checkpoints.main": [[37, 48], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "find_latest_checkpoints.formatted_summary"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.find_latest_checkpoints.formatted_summary"], ["", "", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--dataset\"", ",", "default", "=", "\"lsmdc\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--exp_root\"", ",", "default", "=", "\"data/saved/log\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--fname\"", ",", "default", "=", "\"summary-seed-0_seed-1_seed-2.json\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "formatted_summary", "(", "\n", "fname", "=", "args", ".", "fname", ",", "\n", "dataset", "=", "args", ".", "dataset", ",", "\n", "exp_root", "=", "args", ".", "exp_root", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.gen_latex_version_of_table": [[34, 86], ["content[].startswith", "list", "pylatex.Tabular", "pylatex.Tabular.add_hline", "pylatex.Tabular.add_hline", "pylatex.Tabular.add_row", "pylatex.Tabular.add_hline", "pylatex.Tabular.dumps", "latex_table_dir.mkdir", "pathlib.Path", "reversed", "x.strip", "col_names[].lower", "col_names.pop", "tuple", "pylatex.Tabular.add_row", "pylatex.Tabular.add_hline", "open", "f.write", "x.strip", "tokens.pop", "re.findall", "row_contents.append", "tuple", "x.startswith", "markdown_table[].split", "range", "re.findall", "pylatex.NoEscape", "reversed", "len", "row.split", "len", "len"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdir"], ["@", "typechecked", "\n", "def", "gen_latex_version_of_table", "(", "\n", "latex_table_dir", ":", "Path", ",", "\n", "content", ":", "List", "[", "str", "]", ",", "\n", "table_name", ":", "str", ",", "\n", "branch_name", ":", "str", "=", "\"dev\"", ",", "\n", ")", "->", "Path", ":", "\n", "    ", "msg", "=", "\"Expected latexify tag to be placed directly following a table\"", "\n", "assert", "content", "[", "-", "1", "]", ".", "startswith", "(", "\"|\"", ")", ",", "msg", "\n", "num_table_rows", "=", "[", "x", ".", "startswith", "(", "\"|\"", ")", "for", "x", "in", "reversed", "(", "content", ")", "]", ".", "index", "(", "False", ")", "\n", "assert", "num_table_rows", ">", "2", ",", "\"expected at least three table rows (including header)\"", "\n", "markdown_table", "=", "list", "(", "reversed", "(", "content", "[", "-", "1", ":", "-", "(", "num_table_rows", "+", "1", ")", ":", "-", "1", "]", ")", ")", "\n", "col_names", "=", "[", "x", ".", "strip", "(", ")", "for", "x", "in", "markdown_table", "[", "0", "]", ".", "split", "(", "\"|\"", ")", "[", "1", ":", "-", "1", "]", "]", "\n", "\n", "# remove last column of links", "\n", "remove_links", "=", "col_names", "[", "-", "1", "]", ".", "lower", "(", ")", "==", "\"links\"", "\n", "if", "remove_links", ":", "\n", "        ", "col_names", ".", "pop", "(", ")", "\n", "", "cols", "=", "\"|\"", ".", "join", "(", "[", "\"c\"", "for", "_", "in", "range", "(", "len", "(", "col_names", ")", ")", "]", ")", "\n", "table", "=", "pylatex", ".", "Tabular", "(", "cols", ")", "\n", "table", ".", "add_hline", "(", ")", "\n", "table", ".", "add_hline", "(", ")", "\n", "table", ".", "add_row", "(", "tuple", "(", "col_names", ")", ")", "\n", "table", ".", "add_hline", "(", ")", "\n", "for", "row", "in", "markdown_table", "[", "2", ":", "]", ":", "\n", "        ", "tokens", "=", "[", "x", ".", "strip", "(", ")", "for", "x", "in", "row", ".", "split", "(", "\"|\"", ")", "[", "1", ":", "-", "1", "]", "]", "\n", "if", "remove_links", ":", "\n", "            ", "tokens", ".", "pop", "(", ")", "\n", "", "row_contents", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "mean_regexp", "=", "r\"<sub><sup>([0-9]+[.][0-9]+)<sub>\"", "\n", "# std_regexp = r\"<sub>\\(([0-9]+[.][0-9]+|[a-z]+)\\)<\\/sub>\"", "\n", "std_regexp", "=", "r\"<sub>\\(([0-9]+[.][0-9]+e*-*[0-9]*|[a-z]+|)\\)<\\/sub>\"", "\n", "mean_strs", "=", "re", ".", "findall", "(", "mean_regexp", ",", "token", ")", "\n", "if", "mean_strs", ":", "\n", "                ", "assert", "len", "(", "mean_strs", ")", "==", "1", ",", "\"expected a unique mean specifier\"", "\n", "std_strs", "=", "re", ".", "findall", "(", "std_regexp", ",", "token", ")", "\n", "assert", "len", "(", "std_strs", ")", "==", "1", ",", "\"expected a unique std specifier\"", "\n", "mean_str", ",", "std_str", "=", "mean_strs", "[", "0", "]", ",", "std_strs", "[", "0", "]", "\n", "raw_str", "=", "\"$\"", "+", "mean_str", "+", "r\"_{\\pm\"", "+", "std_str", "+", "r\"}$\"", "\n", "token", "=", "pylatex", ".", "NoEscape", "(", "raw_str", ")", "\n", "", "row_contents", ".", "append", "(", "token", ")", "\n", "", "table", ".", "add_row", "(", "tuple", "(", "row_contents", ")", ")", "\n", "table", ".", "add_hline", "(", ")", "\n", "", "latex_str", "=", "table", ".", "dumps", "(", ")", "\n", "latex_table_dir", ".", "mkdir", "(", "exist_ok", "=", "True", ",", "parents", "=", "True", ")", "\n", "dest_path", "=", "latex_table_dir", "/", "f\"{table_name}.txt\"", "\n", "with", "open", "(", "dest_path", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "latex_str", ")", "\n", "", "github_project_root", "=", "f\"/../../tree/{branch_name}/\"", "\n", "markdown_link", "=", "Path", "(", "f\"{github_project_root}{dest_path}\"", ")", "\n", "return", "markdown_link", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.generate_url": [[88, 104], ["str", "pathlib.Path", "pathlib.Path"], "function", ["None"], ["", "@", "typechecked", "\n", "def", "generate_url", "(", "root_url", ":", "str", ",", "target", ":", "str", ",", "\n", "exp_name", ":", "str", ",", "experiments", ":", "Dict", ",", "\n", "fnames", ":", "dict", ",", "seed_folders", ":", "dict", ")", "->", "str", ":", "\n", "    ", "path_store", "=", "{", "\n", "\"log\"", ":", "{", "\"parent\"", ":", "\"log\"", ",", "\"fname\"", ":", "fnames", "[", "exp_name", "]", "}", ",", "\n", "\"log_TT\"", ":", "{", "\"parent\"", ":", "\"log\"", ",", "\"fname\"", ":", "fnames", "[", "exp_name", "]", "}", ",", "\n", "\"config\"", ":", "{", "\"parent\"", ":", "\"models\"", ",", "\"fname\"", ":", "\"config.json\"", "}", ",", "\n", "\"config_TT\"", ":", "{", "\"parent\"", ":", "\"models\"", ",", "\"fname\"", ":", "\"config.json\"", "}", ",", "\n", "\"model\"", ":", "{", "\"parent\"", ":", "\"models\"", ",", "\"fname\"", ":", "\"trained_model.pth\"", "}", ",", "\n", "\"model_TT\"", ":", "{", "\"parent\"", ":", "\"models\"", ",", "\"fname\"", ":", "\"trained_model.pth\"", "}", "\n", "}", "\n", "paths", "=", "path_store", "[", "target", "]", "\n", "group_id", ",", "timestamp", "=", "experiments", "[", "exp_name", "]", "\n", "rel_path", "=", "Path", "(", "group_id", ")", "/", "seed_folders", "[", "exp_name", "]", "/", "timestamp", "/", "paths", "[", "\"fname\"", "]", "\n", "return", "str", "(", "Path", "(", "root_url", ")", "/", "paths", "[", "\"parent\"", "]", "/", "exp_name", "/", "rel_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.small_font_str": [[106, 109], ["None"], "function", ["None"], ["", "def", "small_font_str", "(", "tokens", ")", ":", "\n", "    ", "tokens", "=", "[", "f\"<sub><sup>{x}</sup></sub>\"", "for", "x", "in", "tokens", "]", "\n", "return", "\" | \"", ".", "join", "(", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.sync_files": [[111, 139], ["experiments.items", "filetypes.items", "timestamp.startswith", "print", "subprocess.call", "print", "subprocess.call", "pathlib.Path", "pathlib.Path.exists", "str", "pathlib.Path.exists", "str", "str().replace", "str", "str", "pathlib.Path", "pathlib.Path", "pathlib.Path().expanduser", "str", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["", "def", "sync_files", "(", "experiments", ",", "save_dir", ",", "webserver", ",", "web_dir", ")", ":", "\n", "    ", "filetypes", "=", "{", "\n", "\"log\"", ":", "[", "\"summary-seed-1_seed-2_seed-3.json\"", "]", ",", "\n", "\"log_TT\"", ":", "[", "\"summary-seed-1_seed-2_seed-3.json\"", "]", ",", "\n", "\"models\"", ":", "[", "\"trained_model.pth\"", ",", "\"config.json\"", "]", ",", "\n", "\"models_TT\"", ":", "[", "\"trained_model.pth\"", ",", "\"config.json\"", "]", "\n", "}", "\n", "for", "key", ",", "(", "group_id", ",", "timestamp", ")", "in", "experiments", ".", "items", "(", ")", ":", "\n", "# copy experiment artifacts", "\n", "        ", "for", "filetype", ",", "fnames", "in", "filetypes", ".", "items", "(", ")", ":", "\n", "            ", "for", "fname", "in", "fnames", ":", "\n", "                ", "if", "timestamp", ".", "startswith", "(", "\"TODO\"", ")", ":", "\n", "                    ", "continue", "\n", "", "rel_path", "=", "Path", "(", "group_id", ")", "/", "\"seed-1\"", "/", "timestamp", "/", "fname", "\n", "local_path", "=", "Path", "(", "save_dir", ")", "/", "filetype", "/", "key", "/", "rel_path", "\n", "server_path", "=", "Path", "(", "web_dir", ")", ".", "expanduser", "(", ")", "/", "filetype", "/", "key", "/", "rel_path", "\n", "if", "not", "local_path", ".", "exists", "(", ")", "and", "\"/log/\"", "in", "str", "(", "local_path", ")", ":", "\n", "# try historical logs", "\n", "                    ", "old", ",", "new", "=", "\"/log/\"", ",", "\"/log-includes-some-final-exps/\"", "\n", "local_path", "=", "Path", "(", "str", "(", "local_path", ")", ".", "replace", "(", "old", ",", "new", ")", ")", "\n", "msg", "=", "f\"neither original log nor historical data exist ({local_path})\"", "\n", "assert", "local_path", ".", "exists", "(", ")", ",", "msg", "\n", "", "dest", "=", "f\"{webserver}:{str(server_path)}\"", "\n", "print", "(", "f\"{key} -> {webserver} [{local_path} -> {server_path}]\"", ")", "\n", "subprocess", ".", "call", "(", "[", "\"ssh\"", ",", "webserver", ",", "\"mkdir -p\"", ",", "str", "(", "server_path", ".", "parent", ")", "]", ")", "\n", "rsync_args", "=", "[", "\"rsync\"", ",", "\"-hvrPt\"", ",", "str", "(", "local_path", ")", ",", "dest", "]", "\n", "print", "(", "f\"running command {' '.join(rsync_args)}\"", ")", "\n", "subprocess", ".", "call", "(", "rsync_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.model_specs2path": [[141, 169], ["feat_aggregation.items", "model_spec.split", "aggs[].split", "aggs.get", "feat_paths.append", "feat_type.replace", "aggs.get", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get"], ["", "", "", "", "@", "typechecked", "\n", "def", "model_specs2path", "(", "feat_aggregation", ":", "Dict", ",", "keep", ":", "set", ",", "tag", ":", "str", "=", "None", ")", "->", "List", "[", "Path", "]", ":", "\n", "    ", "feat_paths", "=", "[", "]", "\n", "for", "model_spec", ",", "aggs", "in", "feat_aggregation", ".", "items", "(", ")", ":", "\n", "        ", "if", "model_spec", "not", "in", "keep", ":", "\n", "            ", "continue", "\n", "\n", "", "feat_type", ",", "model_name", ",", "_", "=", "model_spec", ".", "split", "(", "\".\"", ")", "\n", "base", "=", "f\"aggregated_{feat_type.replace('-', '_')}\"", "\n", "required", "=", "(", "\"fps\"", ",", "\"pixel_dim\"", ",", "\"stride\"", ")", "\n", "fps", ",", "pixel_dim", ",", "stride", "=", "[", "aggs", ".", "get", "(", "x", ",", "None", ")", "for", "x", "in", "required", "]", "\n", "if", "feat_type", "in", "{", "\"facecrops\"", ",", "\"faceboxes\"", "}", ":", "\n", "            ", "base", "=", "f\"{base}_{fps}fps_{pixel_dim}px_stride{stride}\"", "\n", "", "elif", "feat_type", "not", "in", "{", "\"ocr\"", ",", "\"speech\"", ",", "\"audio\"", "}", ":", "\n", "            ", "base", "=", "f\"{base}_{fps}fps_{pixel_dim}px_stride{stride}\"", "\n", "\n", "", "for", "option", "in", "\"offset\"", ",", "\"inner_stride\"", ",", "\"num_segments\"", ":", "\n", "            ", "if", "aggs", ".", "get", "(", "option", ",", "None", ")", "is", "not", "None", ":", "\n", "                ", "base", "+=", "f\"_{option}{aggs[option]}\"", "\n", "\n", "", "", "for", "agg", "in", "aggs", "[", "\"temporal\"", "]", ".", "split", "(", "\"-\"", ")", ":", "\n", "            ", "fname", "=", "f\"{model_name}-{agg}\"", "\n", "if", "aggs", "[", "\"type\"", "]", "==", "\"logits\"", ":", "\n", "                ", "fname", "=", "f\"{fname}-logits\"", "\n", "", "if", "tag", "is", "not", "None", ":", "\n", "                ", "fname", "+=", "f\"-{tag}\"", "\n", "", "feat_paths", ".", "append", "(", "Path", "(", "base", ")", "/", "f\"{fname}.pickle\"", ")", "\n", "", "", "return", "feat_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.dataset_paths": [[171, 198], ["importlib.import_module", "getattr", "pathlib.Path", "getattr.", "set", "getattr", "name_map.values"], "function", ["None"], ["", "@", "typechecked", "\n", "def", "dataset_paths", "(", "\n", "dataset", ":", "str", "\n", ")", "->", "Tuple", "[", "Path", ",", "Dict", "[", "str", ",", "Union", "[", "str", ",", "List", "[", "str", "]", ",", "Dict", ",", "Path", "]", "]", "]", ":", "\n", "    ", "name_map", "=", "{", "\n", "\"msvd\"", ":", "\"MSVD\"", ",", "\n", "\"lsmdc\"", ":", "\"LSMDC\"", ",", "\n", "\"msrvtt\"", ":", "\"MSRVTT\"", ",", "\n", "\"didemo\"", ":", "\"DiDeMo\"", ",", "\n", "\"activity-net\"", ":", "\"ActivityNet\"", ",", "\n", "\"youcook2\"", ":", "\"YouCook2\"", ",", "\n", "\"queryd\"", ":", "\"QuerYD\"", ",", "\n", "\"querydsegments\"", ":", "\"QuerYDSegments\"", "\n", "}", "\n", "if", "dataset", "in", "set", "(", "name_map", ".", "values", "(", ")", ")", ":", "\n", "        ", "class_name", "=", "dataset", "\n", "", "else", ":", "\n", "        ", "class_name", "=", "name_map", "[", "dataset", "]", "\n", "", "mod", "=", "importlib", ".", "import_module", "(", "f\"data_loader.{class_name}_dataset\"", ")", "\n", "get_dataset_paths", "=", "getattr", "(", "getattr", "(", "mod", ",", "class_name", ")", ",", "\"dataset_paths\"", ")", "\n", "if", "dataset", "==", "\"activity-net\"", ":", "\n", "        ", "data_dir", "=", "dataset", "\n", "", "else", ":", "\n", "        ", "data_dir", "=", "class_name", "\n", "", "root_feat", "=", "Path", "(", "f\"data/{data_dir}/structured-symlinks\"", ")", "\n", "paths", "=", "get_dataset_paths", "(", ")", "\n", "return", "root_feat", ",", "paths", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.generate_tar_lists": [[200, 244], ["tqdm.tqdm", "all_feat_paths.items", "experiments.items", "set", "gen_readme.dataset_paths", "gen_readme.model_specs2path", "all_feat_paths[].update", "paths[].items", "all_feat_paths[].update", "all_feat_paths[].add", "tar_include_list.parent.mkdir", "open", "json.load", "exp_name.split", "set", "split_names.append", "set", "all_feat_paths[].add", "set", "all_feat_paths[].update", "open", "sorted", "feat_aggregation.items", "all_feat_paths[].add", "[].values", "print", "f.write", "pathlib.Path", "pathlib.Path", "pathlib.Path", "[].values"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.dataset_paths", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.model_specs2path", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.add", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdir", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.add", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.ClassErrorMeter.add"], ["", "def", "generate_tar_lists", "(", "save_dir", ",", "experiments", ")", ":", "\n", "    ", "all_feat_paths", "=", "{", "}", "\n", "for", "exp_name", ",", "(", "group_id", ",", "timestamp", ")", "in", "tqdm", ".", "tqdm", "(", "experiments", ".", "items", "(", ")", ")", ":", "\n", "        ", "rel_path", "=", "Path", "(", "group_id", ")", "/", "\"seed-1\"", "/", "timestamp", "/", "\"config.json\"", "\n", "config_path", "=", "Path", "(", "save_dir", ")", "/", "\"models\"", "/", "exp_name", "/", "rel_path", "\n", "with", "open", "(", "config_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "config", "=", "json", ".", "load", "(", "f", ")", "\n", "", "feat_aggregation", "=", "config", "[", "\"data_loader\"", "]", "[", "\"args\"", "]", "[", "\"feat_aggregation\"", "]", "\n", "dataset_name", "=", "exp_name", ".", "split", "(", "\"-train\"", ")", "[", "0", "]", "\n", "if", "dataset_name", "not", "in", "all_feat_paths", ":", "\n", "            ", "all_feat_paths", "[", "dataset_name", "]", "=", "set", "(", ")", "\n", "", "split_names", "=", "[", "config", "[", "\"data_loader\"", "]", "[", "\"args\"", "]", "[", "\"split_name\"", "]", "]", "\n", "if", "\"eval_settings\"", "in", "config", "and", "config", "[", "\"eval_settings\"", "]", ":", "\n", "            ", "test_split", "=", "config", "[", "\"eval_settings\"", "]", "[", "\"data_loader\"", "]", "[", "\"args\"", "]", "[", "\"split_name\"", "]", "\n", "split_names", ".", "append", "(", "test_split", ")", "\n", "", "keep", "=", "set", "(", "config", "[", "\"experts\"", "]", "[", "\"modalities\"", "]", ")", "\n", "text_feat", "=", "config", "[", "\"experts\"", "]", "[", "\"text_feat\"", "]", "\n", "root_feat", ",", "paths", "=", "dataset_paths", "(", "dataset_name", ")", "\n", "modern_feat_agg", "=", "{", "key", ":", "val", "for", "key", ",", "val", "in", "feat_aggregation", ".", "items", "(", ")", "\n", "if", "key", "in", "paths", "[", "\"feature_names\"", "]", "}", "\n", "feat_paths", "=", "model_specs2path", "(", "modern_feat_agg", ",", "keep", ")", "\n", "all_feat_paths", "[", "dataset_name", "]", ".", "update", "(", "{", "root_feat", "/", "x", "for", "x", "in", "feat_paths", "}", ")", "\n", "for", "key", ",", "feat_list", "in", "paths", "[", "\"custom_paths\"", "]", ".", "items", "(", ")", ":", "\n", "            ", "for", "feat_path", "in", "feat_list", ":", "\n", "                ", "all_feat_paths", "[", "dataset_name", "]", ".", "add", "(", "root_feat", "/", "feat_path", ")", "\n", "", "", "text_paths", "=", "[", "root_feat", "/", "rel_path", "for", "rel_path", "in", "\n", "paths", "[", "\"text_feat_paths\"", "]", "[", "text_feat", "]", ".", "values", "(", ")", "]", "\n", "all_feat_paths", "[", "dataset_name", "]", ".", "update", "(", "set", "(", "text_paths", ")", ")", "\n", "all_feat_paths", "[", "dataset_name", "]", ".", "add", "(", "root_feat", "/", "paths", "[", "\"raw_captions_path\"", "]", ")", "\n", "if", "\"dict_youtube_mapping_path\"", "in", "paths", ":", "\n", "            ", "all_feat_paths", "[", "dataset_name", "]", ".", "add", "(", "\n", "root_feat", "/", "paths", "[", "\"dict_youtube_mapping_path\"", "]", ")", "\n", "", "for", "split_name", "in", "split_names", ":", "\n", "            ", "split_paths", "=", "set", "(", "root_feat", "/", "x", "for", "x", "in", "\n", "paths", "[", "\"subset_list_paths\"", "]", "[", "split_name", "]", ".", "values", "(", ")", ")", "\n", "all_feat_paths", "[", "dataset_name", "]", ".", "update", "(", "split_paths", ")", "\n", "\n", "", "", "for", "dataset_name", ",", "paths", "in", "all_feat_paths", ".", "items", "(", ")", ":", "\n", "        ", "tar_include_list", "=", "Path", "(", "\"misc\"", ")", "/", "\"datasets\"", "/", "dataset_name", "/", "\"tar_include.txt\"", "\n", "tar_include_list", ".", "parent", ".", "mkdir", "(", "exist_ok", "=", "True", ",", "parents", "=", "True", ")", "\n", "with", "open", "(", "tar_include_list", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "for", "path", "in", "sorted", "(", "paths", ")", ":", "\n", "                ", "print", "(", "f\"Writing {path} to {tar_include_list}\"", ")", "\n", "f", ".", "write", "(", "f\"{path}\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.parse_geom_means_from_val_runs": [[245, 287], ["scores.items", "zip", "sum", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "subdict.items", "geometric_means.append", "re.search", "row.split", "agg_scores[].append", "scipy.stats.mstats.gmean", "len", "re.search.groups", "re.search.groups", "float", "[].append", "row.split.index"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["", "", "", "", "@", "typechecked", "\n", "def", "parse_geom_means_from_val_runs", "(", "log", ":", "List", "[", "str", "]", ",", "group", ":", "str", ")", "->", "List", "[", "float", "]", ":", "\n", "    ", "\"\"\"TODO: Samuel - this is redundant due to log_summary() func in log_parser\n    should refactor after deadline.\n    \"\"\"", "\n", "subset", "=", "\"val\"", "\n", "# sanity check, should not be used for experiments with test sets", "\n", "assert", "sum", "(", "[", "\"test_t2v\"", "in", "x", "for", "x", "in", "log", "]", ")", "==", "0", ",", "\"should not parse test runs\"", "\n", "scores", "=", "{", "\n", "\"R1\"", ":", "defaultdict", "(", "list", ")", ",", "\n", "\"R5\"", ":", "defaultdict", "(", "list", ")", ",", "\n", "\"R10\"", ":", "defaultdict", "(", "list", ")", ",", "\n", "}", "\n", "# Regex tag for finding the seed", "\n", "seed_tag", "=", "\"Setting experiment random seed to\"", "\n", "\n", "for", "row", "in", "log", ":", "\n", "        ", "if", "seed_tag", "in", "row", ":", "\n", "# Search for the log file entry describing the current random seed", "\n", "            ", "match", "=", "re", ".", "search", "(", "seed_tag", "+", "\" (\\d+)$\"", ",", "row", ")", "# NOQA", "\n", "assert", "len", "(", "match", ".", "groups", "(", ")", ")", "==", "1", ",", "\"expected a single regex match\"", "\n", "current_seed", "=", "match", ".", "groups", "(", ")", "[", "0", "]", "\n", "\n", "", "if", "f\"{subset}_{group}_metrics\"", "in", "row", ":", "\n", "            ", "tokens", "=", "row", ".", "split", "(", "\" \"", ")", "\n", "for", "key", "in", "scores", ":", "\n", "                ", "tag", "=", "f\"{subset}_{group}_metrics_{key}:\"", "\n", "if", "tag", "in", "tokens", ":", "\n", "                    ", "pos", "=", "tokens", ".", "index", "(", "tag", ")", "+", "1", "\n", "val", "=", "tokens", "[", "pos", "]", "\n", "val", "=", "float", "(", "val", ")", "\n", "assert", "current_seed", "is", "not", "None", ",", "\"failed to determine the seed\"", "\n", "scores", "[", "key", "]", "[", "current_seed", "]", ".", "append", "(", "val", ")", "\n", "# keep last score", "\n", "", "", "", "", "agg_scores", "=", "{", "key", ":", "[", "]", "for", "key", "in", "scores", "}", "\n", "for", "metric", ",", "subdict", "in", "scores", ".", "items", "(", ")", ":", "\n", "        ", "for", "seed", ",", "values", "in", "subdict", ".", "items", "(", ")", ":", "\n", "            ", "agg_scores", "[", "metric", "]", ".", "append", "(", "values", "[", "-", "1", "]", ")", "\n", "", "", "geometric_means", "=", "[", "]", "\n", "for", "r1", ",", "r5", ",", "r10", "in", "zip", "(", "agg_scores", "[", "\"R1\"", "]", ",", "agg_scores", "[", "\"R5\"", "]", ",", "agg_scores", "[", "\"R10\"", "]", ")", ":", "\n", "        ", "geometric_means", ".", "append", "(", "scipy", ".", "stats", ".", "mstats", ".", "gmean", "(", "[", "r1", ",", "r5", ",", "r10", "]", ")", ")", "\n", "", "return", "geometric_means", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.parse_log": [[289, 339], ["open", "f.read().splitlines", "collections.OrderedDict", "[].item", "zip", "sum", "row.replace.replace", "row.replace.split", "len", "len", "numpy.std", "round", "round", "[].replace", "int", "f.read", "sum", "ValueError", "float", "float", "float", "len", "print", "sum", "any", "gen_readme.parse_geom_means_from_val_runs", "numpy.mean", "numpy.where", "len", "[].replace", "tokens[].split", "row.replace.split", "len", "str", "x.split"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.parse_geom_means_from_val_runs"], ["", "def", "parse_log", "(", "log_path", ")", ":", "\n", "    ", "with", "open", "(", "log_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "log", "=", "f", ".", "read", "(", ")", ".", "splitlines", "(", ")", "\n", "", "results", "=", "{", "}", "\n", "for", "group", "in", "{", "\"t2v\"", ",", "\"v2t\"", "}", ":", "\n", "        ", "tag", "=", "f\"[{group}] loaded log file\"", "\n", "results", "[", "group", "]", "=", "OrderedDict", "(", ")", "\n", "presence", "=", "[", "tag", "in", "row", "for", "row", "in", "log", "]", "\n", "msg", "=", "f\"expected single occurence of log tag, found {sum(presence)} in {log_path}\"", "\n", "assert", "sum", "(", "presence", ")", "==", "1", ",", "msg", "\n", "metrics", "=", "[", "\"R1\"", ",", "\"R5\"", ",", "\"R10\"", ",", "\"R50\"", ",", "\"MedR\"", ",", "\"MeanR\"", "]", "\n", "pos", "=", "np", ".", "where", "(", "presence", ")", "[", "0", "]", ".", "item", "(", ")", "\n", "if", "\"fixed training length\"", "in", "log", "[", "pos", "+", "2", "]", ":", "\n", "            ", "pos", "+=", "3", "\n", "", "else", ":", "\n", "            ", "pos", "+=", "2", "\n", "", "rows", "=", "log", "[", "pos", ":", "pos", "+", "len", "(", "metrics", ")", "]", "\n", "for", "row", ",", "metric", "in", "zip", "(", "rows", ",", "metrics", ")", ":", "\n", "            ", "row", "=", "row", ".", "replace", "(", "\"INFO:summary:\"", ",", "\"\"", ")", "\n", "tokens", "=", "row", ".", "split", "(", "\" \"", ")", "\n", "if", "tokens", "[", "-", "3", "]", "!=", "f\"{metric}:\"", ":", "\n", "                ", "raise", "ValueError", "(", "f\"Unexpteced log format [{row}]\"", ")", "\n", "", "assert", "tokens", "[", "-", "3", "]", "==", "f\"{metric}:\"", ",", "f\"unexpected row format {row}\"", "\n", "mean", ",", "std", "=", "float", "(", "tokens", "[", "-", "2", "]", ".", "split", "(", "\",\"", ")", "[", "0", "]", ")", ",", "float", "(", "tokens", "[", "-", "1", "]", ")", "\n", "results", "[", "group", "]", "[", "metric", "]", "=", "(", "mean", ",", "std", ")", "\n", "# geometric means are recomputed from summaries", "\n", "", "tag", "=", "f\"test_{group}_metrics_geometric_mean\"", "\n", "nan_tag", "=", "\"INFO:summary:R1: nan\"", "\n", "matches", "=", "[", "x", "for", "x", "in", "log", "if", "tag", "in", "x", "]", "\n", "if", "len", "(", "matches", ")", "in", "{", "1", ",", "2", ",", "3", "}", ":", "\n", "            ", "geoms", "=", "[", "float", "(", "x", ".", "split", "(", ")", "[", "-", "1", "]", ".", "replace", "(", "\"INFO:summary:\"", ",", "\"\"", ")", ")", "for", "x", "in", "matches", "]", "\n", "if", "len", "(", "matches", ")", "<", "3", ":", "\n", "                ", "print", "(", "f\"WARNING: Getting stds from {len(matches)} runs for {log_path}!\"", ")", "\n", "", "", "elif", "sum", "(", "[", "nan_tag", "in", "x", "for", "x", "in", "log", "]", ")", ">", "0", ":", "\n", "            ", "geoms", "=", "[", "np", ".", "nan", ",", "np", ".", "nan", ",", "np", ".", "nan", "]", "\n", "", "else", ":", "\n", "            ", "valid_exceptions", "=", "[", "\"miechfeats-moee\"", ",", "\"miech-ce\"", ",", "\"jsfusion\"", "]", "\n", "msg", "=", "f\"Did not expect fixed length training for {log_path}\"", "\n", "assert", "any", "(", "[", "x", "in", "str", "(", "log_path", ")", "for", "x", "in", "valid_exceptions", "]", ")", ",", "msg", "\n", "geoms", "=", "parse_geom_means_from_val_runs", "(", "log", ",", "group", "=", "group", ")", "\n", "", "if", "len", "(", "geoms", ")", "==", "1", ":", "\n", "            ", "std", "=", "np", ".", "nan", "\n", "", "else", ":", "\n", "            ", "std", "=", "np", ".", "std", "(", "geoms", ")", "\n", "", "results", "[", "group", "]", "[", "\"geom\"", "]", "=", "(", "round", "(", "np", ".", "mean", "(", "geoms", ")", ",", "1", ")", ",", "round", "(", "std", ",", "1", ")", ")", "\n", "", "for", "row", "in", "log", ":", "\n", "        ", "if", "\"Trainable parameters\"", "in", "row", ":", "\n", "            ", "param_token", "=", "row", ".", "split", "(", "\" \"", ")", "[", "-", "1", "]", ".", "replace", "(", "\"INFO:summary:\"", ",", "\"\"", ")", "\n", "results", "[", "\"params\"", "]", "=", "int", "(", "param_token", ")", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.multiprocessing_parsing": [[340, 369], ["os.path.exists", "timestamp.startswith", "print", "os.listdir", "gen_readme.parse_log", "open", "pickle.dump", "print", "sorted", "pathlib.Path", "aggregate_logs_and_stats.summarise", "pathlib.Path", "os.listdir", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.parse_log", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.aggregate_logs_and_stats.summarise"], ["", "@", "typechecked", "\n", "def", "multiprocessing_parsing", "(", "exp_name", ":", "str", ",", "meta", ":", "list", ",", "\n", "save_dir", ":", "Path", ",", "refresh_summaries", ":", "bool", ",", "teachText", ":", "bool", ",", "pickle_files", ":", "str", ")", ":", "\n", "    ", "if", "os", ".", "path", ".", "exists", "(", "Path", "(", "save_dir", ")", "/", "pickle_files", "/", "f'log_results_{exp_name}.pkl'", ")", "is", "False", ":", "\n", "        ", "group_id", ",", "timestamp", "=", "meta", "\n", "_log_path", "=", "\"log_old\"", "\n", "if", "teachText", ":", "\n", "            ", "_log_path", "=", "\"log\"", "\n", "", "if", "timestamp", ".", "startswith", "(", "\"TODO\"", ")", ":", "\n", "            ", "log_results", "[", "exp_name", "]", "=", "{", "\"timestamp\"", ":", "\"TODO\"", ",", "\"results\"", ":", "{", "}", "}", "\n", "", "else", ":", "\n", "            ", "seed_folder", "=", "sorted", "(", "os", ".", "listdir", "(", "Path", "(", "save_dir", ")", "/", "_log_path", "/", "Path", "(", "exp_name", ")", "/", "group_id", ")", ")", "[", "0", "]", "\n", "files_in_seed_folder", "=", "os", ".", "listdir", "(", "Path", "(", "save_dir", ")", "/", "_log_path", "/", "Path", "(", "exp_name", ")", "/", "group_id", "/", "seed_folder", "/", "Path", "(", "timestamp", ")", ")", "\n", "for", "file", "in", "files_in_seed_folder", ":", "\n", "                ", "if", "\".json\"", "in", "file", "and", "\".bak\"", "not", "in", "file", ":", "\n", "                    ", "fname", "=", "file", "\n", "break", "\n", "", "", "rel_fname", "=", "Path", "(", "timestamp", ")", "/", "fname", "\n", "rel_path", "=", "Path", "(", "exp_name", ")", "/", "group_id", "/", "seed_folder", "/", "rel_fname", "\n", "log_path", "=", "Path", "(", "save_dir", ")", "/", "_log_path", "/", "rel_path", "\n", "if", "refresh_summaries", ":", "\n", "                ", "summarise", "(", "group_id", "=", "group_id", ",", "log_dir", "=", "Path", "(", "save_dir", ")", "/", "_log_path", ")", "\n", "", "results", "=", "parse_log", "(", "log_path", ")", "\n", "log_results", "=", "{", "\"timestamp\"", ":", "timestamp", ",", "\"results\"", ":", "results", "}", "\n", "", "with", "open", "(", "Path", "(", "save_dir", ")", "/", "pickle_files", "/", "f'log_results_{exp_name}.pkl'", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "[", "log_results", ",", "fname", ",", "seed_folder", "]", ",", "f", ")", "\n", "print", "(", "f\"Saved experiment {exp_name}\"", ")", "\n", "", "", "else", ":", "\n", "        ", "print", "(", "f\"Experiment log_results_{exp_name}.pkl already saved\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.parse_results": [[370, 414], ["time.time", "experiments.items", "print", "os.path.exists", "os.mkdir", "multiprocessing.Process", "processes.append", "multiprocessing.Process.start", "process.join", "open", "pickle.load", "pathlib.Path", "pathlib.Path", "time.time", "open", "pickle.dump", "open", "pickle.dump", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdir"], ["", "", "@", "typechecked", "\n", "def", "parse_results", "(", "\n", "experiments", ":", "Dict", "[", "str", ",", "List", "[", "str", "]", "]", ",", "\n", "save_dir", ":", "Path", ",", "\n", "refresh_summaries", ":", "bool", ",", "\n", "teachText", ":", "bool", ",", "\n", ")", "->", "(", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "Union", "[", "str", ",", "Dict", "]", "]", "]", ",", "\n", "dict", ",", "dict", ")", ":", "\n", "    ", "starttime", "=", "time", ".", "time", "(", ")", "\n", "processes", "=", "[", "]", "\n", "experiments_items", "=", "experiments", ".", "items", "(", ")", "\n", "pickle_files", "=", "\"pickle_files\"", "\n", "if", "teachText", ":", "\n", "        ", "pickle_files", "=", "\"pickle_files_teachText\"", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "Path", "(", "save_dir", ")", "/", "pickle_files", ")", "is", "False", ":", "\n", "        ", "os", ".", "mkdir", "(", "Path", "(", "save_dir", ")", "/", "pickle_files", ")", "\n", "", "for", "exp_name", ",", "meta", "in", "experiments_items", ":", "\n", "        ", "p", "=", "multiprocessing", ".", "Process", "(", "target", "=", "multiprocessing_parsing", ",", "\n", "args", "=", "(", "exp_name", ",", "meta", ",", "\n", "save_dir", ",", "refresh_summaries", ",", "teachText", ",", "pickle_files", ")", ")", "\n", "processes", ".", "append", "(", "p", ")", "\n", "p", ".", "start", "(", ")", "\n", "", "for", "process", "in", "processes", ":", "\n", "        ", "process", ".", "join", "(", ")", "\n", "", "print", "(", "'That took {} seconds'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "starttime", ")", ")", "\n", "log_results", "=", "{", "}", "\n", "fnames", "=", "{", "}", "\n", "seed_folders", "=", "{", "}", "\n", "for", "exp_name", ",", "_", "in", "experiments_items", ":", "\n", "        ", "with", "open", "(", "Path", "(", "save_dir", ")", "/", "pickle_files", "/", "f'log_results_{exp_name}.pkl'", ",", "\n", "'rb'", ")", "as", "f", ":", "\n", "            ", "log_results", "[", "exp_name", "]", ",", "fnames", "[", "exp_name", "]", ",", "seed_folders", "[", "exp_name", "]", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "if", "not", "teachText", ":", "\n", "            ", "with", "open", "(", "Path", "(", "save_dir", ")", "/", "'log_results2.pkl'", ",", "'wb'", ")", "as", "f", ":", "\n", "                ", "pickle", ".", "dump", "(", "[", "log_results", ",", "fnames", ",", "seed_folders", "]", ",", "f", ")", "\n", "", "", "else", ":", "\n", "            ", "with", "open", "(", "Path", "(", "save_dir", ")", "/", "'log_results_teachText.pkl'", ",", "'wb'", ")", "as", "f", ":", "\n", "                ", "pickle", ".", "dump", "(", "[", "log_results", ",", "fnames", ",", "seed_folders", "]", ",", "f", ")", "\n", "\n", "\n", "\n", "", "", "", "return", "log_results", ",", "fnames", ",", "seed_folders", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.generate_results_string": [[416, 434], ["print", "stats.items", "gen_readme.small_font_str", "print", "tokens.append", "str_tokens.insert"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.small_font_str"], ["", "def", "generate_results_string", "(", "target", ",", "exp_name", ",", "results", ",", "latexify", ",", "drop", "=", "None", ")", ":", "\n", "    ", "stats", "=", "results", "[", "exp_name", "]", "[", "\"results\"", "]", "[", "target", "]", "\n", "print", "(", "f\"Filling template values for {exp_name}\"", ")", "\n", "tokens", "=", "[", "]", "\n", "prepad", "=", "False", "\n", "for", "metric", ",", "values", "in", "stats", ".", "items", "(", ")", ":", "\n", "        ", "mean", ",", "std", "=", "values", "\n", "if", "drop", "and", "metric", "in", "drop", ":", "\n", "            ", "continue", "\n", "", "print", "(", "f\"{metric}: {mean} ({std})\"", ")", "\n", "if", "latexify", ":", "\n", "            ", "str_tokens", "=", "[", "\"&$\"", ",", "f\"{mean}_{{\\\\pm{std}}}$\"", "]", "\n", "if", "prepad", ":", "\n", "                ", "str_tokens", ".", "insert", "(", "1", ",", "r\"\\prepad\"", ")", "\n", "", "tokens", ".", "append", "(", "\" \"", ".", "join", "(", "str_tokens", ")", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "[", "f\"{mean}<sub>({std})</sub>\"", "]", "\n", "", "", "return", "small_font_str", "(", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.generate_readme": [[436, 558], ["zip", "open", "f.read().splitlines", "re.finditer", "re.finditer", "generated.append", "match.groups", "groups[].split", "full_readme.extend", "full_readme.append", "match.groups", "groups[].split", "target.startswith", "edits.append", "itertools.zip_longest", "open", "f.write", "open", "f.write", "f.read", "len", "open", "f.read().splitlines", "subrow.replace.replace", "subrow.replace.replace", "len", "gen_readme.gen_latex_version_of_table", "str", "src_name.upper", "dest_name.upper", "subrows.append", "match.span", "zip", "f.read", "subrow.replace.replace", "gen_readme.generate_url", "len", "print", "gen_readme.generate_url", "gen_readme.generate_results_string", "target.split", "gen_readme.generate_results_string", "target.split", "millify.millify"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.gen_latex_version_of_table", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.generate_url", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.generate_url", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.generate_results_string", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.generate_results_string"], ["", "def", "generate_readme", "(", "\n", "experiments", ":", "Dict", "[", "str", ",", "List", "[", "str", "]", "]", ",", "\n", "root_url", ":", "str", ",", "\n", "readme_templates", ":", "List", "[", "Path", "]", ",", "\n", "readme_dests", ":", "List", "[", "Path", "]", ",", "\n", "results_path", ":", "Path", ",", "\n", "latex_table_dir", ":", "Path", ",", "\n", "save_dir", ":", "Path", ",", "\n", "latexify", ":", "bool", ",", "\n", "keep_mnr", ":", "bool", ",", "\n", "refresh_summaries", ":", "bool", ",", "\n", "results", ":", "Dict", ",", "\n", "fnames", ":", "Dict", ",", "\n", "seed_folders", ":", "Dict", ",", "\n", "append_to_existing_readme", ":", "bool", ",", "\n", ")", ":", "\n", "    ", "for", "readme_template", ",", "readme_dest", "in", "zip", "(", "readme_templates", ",", "readme_dests", ")", ":", "\n", "        ", "with", "open", "(", "readme_template", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "readme", "=", "f", ".", "read", "(", ")", ".", "splitlines", "(", ")", "\n", "\n", "# insert sub-templates", "\n", "", "full_readme", "=", "[", "]", "\n", "for", "row", "in", "readme", ":", "\n", "            ", "regex", "=", "r\"\\<\\<(.*?)\\>\\>\"", "\n", "matched", "=", "False", "\n", "for", "match", "in", "re", ".", "finditer", "(", "regex", ",", "row", ")", ":", "\n", "                ", "matched", "=", "True", "\n", "groups", "=", "match", ".", "groups", "(", ")", "\n", "assert", "len", "(", "groups", ")", "==", "1", ",", "\"expected single group\"", "\n", "subtemplate_path", ",", "src_name", ",", "dest_name", "=", "groups", "[", "0", "]", ".", "split", "(", "\":\"", ")", "\n", "with", "open", "(", "subtemplate_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "                    ", "subtemplate", "=", "f", ".", "read", "(", ")", ".", "splitlines", "(", ")", "\n", "", "subrows", "=", "[", "]", "\n", "for", "subrow", "in", "subtemplate", ":", "\n", "                    ", "drop_subrow", "=", "False", "\n", "subrow", "=", "subrow", ".", "replace", "(", "src_name", ",", "dest_name", ")", "\n", "subrow", "=", "subrow", ".", "replace", "(", "src_name", ".", "upper", "(", ")", ",", "dest_name", ".", "upper", "(", ")", ")", "\n", "# Handle the missing audio modalities of MSVD", "\n", "if", "dest_name", "==", "\"msvd\"", ":", "\n", "                        ", "for", "tag", "in", "(", "\"speech\"", ",", "\"audio\"", ")", ":", "\n", "# drop experiments for which the audio/speech features form", "\n", "# the control variable", "\n", "                            ", "if", "f\"-{tag}.\"", "in", "subrow", ":", "\n", "                                ", "print", "(", "\"skipping\"", ",", "subrow", ")", "\n", "drop_subrow", "=", "True", "\n", "break", "\n", "# remove audio features from other experiments", "\n", "", "subrow", "=", "subrow", ".", "replace", "(", "f\"-{tag}\"", ",", "\"\"", ")", "\n", "\n", "", "", "if", "not", "drop_subrow", ":", "\n", "                        ", "subrows", ".", "append", "(", "subrow", ")", "\n", "", "", "full_readme", ".", "extend", "(", "subrows", ")", "\n", "", "if", "not", "matched", ":", "\n", "                ", "full_readme", ".", "append", "(", "row", ")", "\n", "\n", "", "", "generated", "=", "[", "]", "\n", "for", "row", "in", "full_readme", ":", "\n", "            ", "edits", "=", "[", "]", "\n", "regex", "=", "r\"\\{\\{(.*?)\\}\\}\"", "\n", "for", "match", "in", "re", ".", "finditer", "(", "regex", ",", "row", ")", ":", "\n", "                ", "groups", "=", "match", ".", "groups", "(", ")", "\n", "assert", "len", "(", "groups", ")", "==", "1", ",", "\"expected single group\"", "\n", "exp_name", ",", "target", "=", "groups", "[", "0", "]", ".", "split", "(", "\".\"", ")", "\n", "if", "target", ".", "startswith", "(", "\"latexify\"", ")", ":", "\n", "                    ", "latex_link", "=", "gen_latex_version_of_table", "(", "\n", "content", "=", "generated", "[", ":", "]", ",", "\n", "table_name", "=", "exp_name", ",", "\n", "latex_table_dir", "=", "latex_table_dir", ",", "\n", ")", "\n", "token", "=", "f\"[latex]({latex_link}) | | | | | | | |\"", "\n", "", "elif", "results", "[", "exp_name", "]", "[", "\"timestamp\"", "]", "==", "\"TODO\"", ":", "\n", "                    ", "token", "=", "\"TODO\"", "\n", "", "elif", "target", "in", "{", "\"config\"", ",", "\"model\"", ",", "\"log\"", "}", ":", "\n", "                    ", "token", "=", "generate_url", "(", "root_url", ",", "target", ",", "exp_name", ",", "\n", "experiments", "=", "experiments", ",", "\n", "fnames", "=", "fnames", ",", "\n", "seed_folders", "=", "seed_folders", ")", "\n", "", "elif", "target", "in", "{", "\"config_TT\"", ",", "\"model_TT\"", ",", "\"log_TT\"", "}", ":", "\n", "                    ", "token", "=", "generate_url", "(", "\"http://www.robots.ox.ac.uk/~vgg/research/teachtext/data-hq\"", ",", "target", ",", "exp_name", ",", "\n", "experiments", "=", "experiments", ",", "\n", "fnames", "=", "fnames", ",", "\n", "seed_folders", "=", "seed_folders", ")", "\n", "\n", "", "elif", "target", "in", "{", "\"t2v\"", ",", "\"v2t\"", ",", "\"geomt2v\"", ",", "\"geomv2t\"", "}", ":", "\n", "                    ", "if", "not", "\"geom\"", "in", "target", ":", "\n", "                        ", "drop", "=", "{", "\"geom\"", "}", "\n", "", "else", ":", "\n", "                        ", "drop", "=", "{", "}", "\n", "", "target_", "=", "target", ".", "split", "(", "\"geom\"", ")", "[", "-", "1", "]", "\n", "token", "=", "generate_results_string", "(", "target_", ",", "exp_name", ",", "results", ",", "\n", "drop", "=", "drop", ",", "latexify", "=", "latexify", ")", "\n", "", "elif", "target", "in", "{", "\"short-t2v\"", ",", "\"short-v2t\"", "}", ":", "\n", "                    ", "if", "keep_mnr", ":", "\n", "                        ", "drop", "=", "{", "\"R50\"", ",", "\"geom\"", "}", "\n", "", "else", ":", "\n", "                        ", "drop", "=", "{", "\"R50\"", ",", "\"MeanR\"", ",", "\"geom\"", "}", "\n", "", "target_", "=", "target", ".", "split", "(", "\"-\"", ")", "[", "1", "]", "\n", "token", "=", "generate_results_string", "(", "target_", ",", "exp_name", ",", "results", ",", "\n", "drop", "=", "drop", ",", "latexify", "=", "latexify", ")", "\n", "", "elif", "target", "in", "{", "\"params\"", "}", ":", "\n", "                    ", "token", "=", "millify", "(", "results", "[", "exp_name", "]", "[", "\"results\"", "]", "[", "\"params\"", "]", ",", "precision", "=", "2", ")", "\n", "\n", "", "edits", ".", "append", "(", "(", "match", ".", "span", "(", ")", ",", "token", ")", ")", "\n", "", "if", "edits", ":", "\n", "# invert the spans", "\n", "                ", "spans", "=", "[", "(", "None", ",", "0", ")", "]", "+", "[", "x", "[", "0", "]", "for", "x", "in", "edits", "]", "+", "[", "(", "len", "(", "row", ")", ",", "None", ")", "]", "\n", "inverse_spans", "=", "[", "(", "x", "[", "1", "]", ",", "y", "[", "0", "]", ")", "for", "x", ",", "y", "in", "zip", "(", "spans", ",", "spans", "[", "1", ":", "]", ")", "]", "\n", "tokens", "=", "[", "row", "[", "start", ":", "stop", "]", "for", "start", ",", "stop", "in", "inverse_spans", "]", "\n", "urls", "=", "[", "str", "(", "x", "[", "1", "]", ")", "for", "x", "in", "edits", "]", "\n", "new_row", "=", "\"\"", "\n", "for", "token", ",", "url", "in", "zip_longest", "(", "tokens", ",", "urls", ",", "fillvalue", "=", "\"\"", ")", ":", "\n", "                    ", "new_row", "+=", "token", "+", "url", "\n", "", "row", "=", "new_row", "\n", "\n", "", "generated", ".", "append", "(", "row", ")", "\n", "\n", "", "if", "not", "append_to_existing_readme", ":", "\n", "            ", "with", "open", "(", "readme_dest", ",", "\"w\"", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "\"\\n\"", ".", "join", "(", "generated", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "with", "open", "(", "readme_dest", ",", "\"a\"", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "\"\\n\"", ".", "join", "(", "generated", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.parse_generate_readme": [[561, 629], ["gen_readme.parse_results", "gen_readme.generate_readme", "open", "json.dump", "gen_readme.parse_results", "gen_readme.generate_readme", "open", "json.dump"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.parse_results", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.generate_readme", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.parse_results", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.generate_readme"], ["", "", "", "", "def", "parse_generate_readme", "(", "\n", "experiments", ":", "Dict", "[", "str", ",", "List", "[", "str", "]", "]", ",", "\n", "root_url", ":", "str", ",", "\n", "readme_templates", ":", "List", "[", "Path", "]", ",", "\n", "readme_dests", ":", "List", "[", "Path", "]", ",", "\n", "results_path", ":", "Path", ",", "\n", "latex_table_dir", ":", "Path", ",", "\n", "save_dir", ":", "Path", ",", "\n", "latexify", ":", "bool", ",", "\n", "keep_mnr", ":", "bool", ",", "\n", "refresh_summaries", ":", "bool", ",", "\n", "drop_experiments_hq", ":", "bool", ",", "\n", "results_path_teachText", ":", "Path", ",", "\n", "experiments_teachText", ":", "Dict", "[", "str", ",", "List", "[", "str", "]", "]", ",", "\n", "teachText_template", ":", "Path", ",", "\n", ")", ":", "\n", "\n", "    ", "results", ",", "fnames", ",", "seed_folders", "=", "parse_results", "(", "experiments", "=", "experiments", ",", "\n", "save_dir", "=", "save_dir", ",", "\n", "refresh_summaries", "=", "refresh_summaries", ",", "\n", "teachText", "=", "False", ",", "\n", ")", "\n", "\n", "append_to_existing_readme", "=", "False", "\n", "with", "open", "(", "results_path", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "results", ",", "f", ",", "indent", "=", "4", ",", "sort_keys", "=", "False", ")", "\n", "\n", "", "if", "not", "drop_experiments_hq", ":", "\n", "        ", "results_teachText", ",", "fnames_teachText", ",", "seed_folders_teachText", "=", "parse_results", "(", "experiments", "=", "experiments_teachText", ",", "\n", "save_dir", "=", "save_dir", ",", "\n", "refresh_summaries", "=", "refresh_summaries", ",", "\n", "teachText", "=", "True", ",", "\n", ")", "\n", "with", "open", "(", "results_path_teachText", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "results", ",", "f", ",", "indent", "=", "4", ",", "sort_keys", "=", "False", ")", "\n", "\n", "", "generate_readme", "(", "experiments", "=", "experiments_teachText", ",", "\n", "root_url", "=", "root_url", ",", "\n", "readme_templates", "=", "[", "teachText_template", "]", ",", "\n", "readme_dests", "=", "readme_dests", ",", "\n", "results_path", "=", "results_path_teachText", ",", "\n", "latex_table_dir", "=", "latex_table_dir", ",", "\n", "save_dir", "=", "save_dir", ",", "\n", "latexify", "=", "latexify", ",", "\n", "keep_mnr", "=", "keep_mnr", ",", "\n", "refresh_summaries", "=", "refresh_summaries", ",", "\n", "results", "=", "results_teachText", ",", "\n", "fnames", "=", "fnames_teachText", ",", "\n", "seed_folders", "=", "seed_folders_teachText", ",", "\n", "append_to_existing_readme", "=", "False", ",", "\n", ")", "\n", "\n", "append_to_existing_readme", "=", "True", "\n", "\n", "", "generate_readme", "(", "experiments", "=", "experiments", ",", "\n", "root_url", "=", "root_url", ",", "\n", "readme_templates", "=", "readme_templates", ",", "\n", "readme_dests", "=", "readme_dests", ",", "\n", "results_path", "=", "results_path", ",", "\n", "latex_table_dir", "=", "latex_table_dir", ",", "\n", "save_dir", "=", "save_dir", ",", "\n", "latexify", "=", "latexify", ",", "\n", "keep_mnr", "=", "keep_mnr", ",", "\n", "refresh_summaries", "=", "refresh_summaries", ",", "\n", "results", "=", "results", ",", "\n", "fnames", "=", "fnames", ",", "\n", "seed_folders", "=", "seed_folders", ",", "\n", "append_to_existing_readme", "=", "append_to_existing_readme", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.main": [[632, 705], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "open", "json.load", "open", "json.load", "gen_readme.sync_files", "gen_readme.parse_generate_readme"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.sync_files", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.gen_readme.parse_generate_readme"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_dir\"", ",", "default", "=", "\"data/saved\"", ",", "type", "=", "Path", ")", "\n", "parser", ".", "add_argument", "(", "\"--webserver\"", ",", "default", "=", "\"login.robots.ox.ac.uk\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--results_path\"", ",", "default", "=", "\"misc/results.json\"", ",", "type", "=", "Path", ")", "\n", "parser", ".", "add_argument", "(", "\"--results_path_teachText\"", ",", "default", "=", "\"misc/results_teachText.json\"", ",", "type", "=", "Path", ")", "\n", "parser", ".", "add_argument", "(", "\"--experiments_path\"", ",", "default", "=", "\"misc/experiments.json\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--experiments_teachText\"", ",", "default", "=", "\"misc/experiments_teachText.json\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--readme_template\"", ",", "default", "=", "\"misc/README-template.md\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--teachText_template\"", ",", "default", "=", "\"misc/README-teachText.md\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--latexify\"", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--drop_experiments_hq\"", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--keep_mnr\"", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--refresh_summaries\"", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--readme_dest\"", ",", "default", "=", "\"README.md\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--latex_table_dir\"", ",", "default", "=", "\"latex-tables\"", ",", "type", "=", "Path", ")", "\n", "parser", ".", "add_argument", "(", "\"--ablation_readme_dest\"", ",", "default", "=", "\"misc/ablations.md\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--challenge_readme_dest\"", ",", "default", "=", "\"misc/challenge.md\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--ablation_readme_template\"", ",", "\n", "default", "=", "\"misc/ablations-template.md\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--challenge_readme_template\"", ",", "\n", "default", "=", "\"misc/README-challenge-template.md\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--task\"", ",", "default", "=", "\"generate_readme\"", ",", "\n", "choices", "=", "[", "\"sync_files\"", ",", "\"generate_readme\"", "]", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--web_dir\"", ",", "\n", "default", "=", "\"/projects/vgg/vgg/WWW/research/collaborative-experts/data\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--root_url\"", ",", "\n", "default", "=", "\"http://www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data\"", ",", "\n", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "with", "open", "(", "args", ".", "experiments_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "experiments", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "with", "open", "(", "args", ".", "experiments_teachText", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "experiments_teachText", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "if", "args", ".", "task", "==", "\"sync_files\"", ":", "\n", "        ", "sync_files", "(", "\n", "web_dir", "=", "args", ".", "web_dir", ",", "\n", "save_dir", "=", "args", ".", "save_dir", ",", "\n", "webserver", "=", "args", ".", "webserver", ",", "\n", "experiments", "=", "experiments", ",", "\n", ")", "\n", "", "elif", "args", ".", "task", "==", "\"generate_readme\"", ":", "\n", "        ", "readme_dests", "=", "[", "\n", "args", ".", "readme_dest", ",", "\n", "args", ".", "ablation_readme_dest", ",", "\n", "args", ".", "challenge_readme_dest", ",", "\n", "]", "\n", "readme_templates", "=", "[", "\n", "args", ".", "readme_template", ",", "\n", "args", ".", "ablation_readme_template", ",", "\n", "args", ".", "challenge_readme_template", ",", "\n", "]", "\n", "parse_generate_readme", "(", "\n", "root_url", "=", "args", ".", "root_url", ",", "\n", "save_dir", "=", "args", ".", "save_dir", ",", "\n", "latexify", "=", "args", ".", "latexify", ",", "\n", "experiments", "=", "experiments", ",", "\n", "latex_table_dir", "=", "args", ".", "latex_table_dir", ",", "\n", "keep_mnr", "=", "args", ".", "keep_mnr", ",", "\n", "readme_dests", "=", "readme_dests", ",", "\n", "results_path", "=", "args", ".", "results_path", ",", "\n", "readme_templates", "=", "readme_templates", ",", "\n", "refresh_summaries", "=", "args", ".", "refresh_summaries", ",", "\n", "drop_experiments_hq", "=", "args", ".", "drop_experiments_hq", ",", "\n", "results_path_teachText", "=", "args", ".", "results_path_teachText", ",", "\n", "experiments_teachText", "=", "experiments_teachText", ",", "\n", "teachText_template", "=", "args", ".", "teachText_template", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_slurm_scripts.fill_template": [[31, 56], ["open", "f.read().splitlines", "re.finditer", "generated.append", "match.groups", "edits.append", "itertools.zip_longest", "f.read", "len", "str", "match.span", "zip", "len"], "function", ["None"], ["def", "fill_template", "(", "template_path", ",", "rules", ")", ":", "\n", "    ", "generated", "=", "[", "]", "\n", "with", "open", "(", "template_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "template", "=", "f", ".", "read", "(", ")", ".", "splitlines", "(", ")", "\n", "", "for", "row", "in", "template", ":", "\n", "        ", "edits", "=", "[", "]", "\n", "regex", "=", "r\"\\{\\{(.*?)\\}\\}\"", "\n", "for", "match", "in", "re", ".", "finditer", "(", "regex", ",", "row", ")", ":", "\n", "            ", "groups", "=", "match", ".", "groups", "(", ")", "\n", "assert", "len", "(", "groups", ")", "==", "1", ",", "\"expected single group\"", "\n", "key", "=", "groups", "[", "0", "]", "\n", "token", "=", "rules", "[", "key", "]", "\n", "edits", ".", "append", "(", "(", "match", ".", "span", "(", ")", ",", "token", ")", ")", "\n", "", "if", "edits", ":", "\n", "# invert the spans", "\n", "            ", "spans", "=", "[", "(", "None", ",", "0", ")", "]", "+", "[", "x", "[", "0", "]", "for", "x", "in", "edits", "]", "+", "[", "(", "len", "(", "row", ")", ",", "None", ")", "]", "\n", "inverse_spans", "=", "[", "(", "x", "[", "1", "]", ",", "y", "[", "0", "]", ")", "for", "x", ",", "y", "in", "zip", "(", "spans", ",", "spans", "[", "1", ":", "]", ")", "]", "\n", "tokens", "=", "[", "row", "[", "start", ":", "stop", "]", "for", "start", ",", "stop", "in", "inverse_spans", "]", "\n", "urls", "=", "[", "str", "(", "x", "[", "1", "]", ")", "for", "x", "in", "edits", "]", "\n", "new_row", "=", "\"\"", "\n", "for", "token", ",", "url", "in", "zip_longest", "(", "tokens", ",", "urls", ",", "fillvalue", "=", "\"\"", ")", ":", "\n", "                ", "new_row", "+=", "token", "+", "url", "\n", "", "row", "=", "new_row", "\n", "", "generated", ".", "append", "(", "row", ")", "\n", "", "return", "\"\\n\"", ".", "join", "(", "generated", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_slurm_scripts.parse_group_ids": [[58, 67], ["collections.OrderedDict", "enumerate", "cmd.split", "group_ids[].append", "cmd.split.index"], "function", ["None"], ["", "def", "parse_group_ids", "(", "parsed_cmds", ")", ":", "\n", "    ", "group_ids", "=", "OrderedDict", "(", ")", "\n", "for", "ii", ",", "cmd", "in", "enumerate", "(", "parsed_cmds", ")", ":", "\n", "        ", "tokens", "=", "cmd", ".", "split", "(", "\" \"", ")", "\n", "group_id", "=", "tokens", "[", "tokens", ".", "index", "(", "\"--group_id\"", ")", "+", "1", "]", "\n", "if", "group_id", "not", "in", "group_ids", ":", "\n", "            ", "group_ids", "[", "group_id", "]", "=", "[", "]", "\n", "", "group_ids", "[", "group_id", "]", ".", "append", "(", "ii", "+", "1", ")", "# slurm arrays are 1-indexed", "\n", "", "return", "group_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_slurm_scripts.generate_slurm_dependency_script": [[69, 82], ["aggregation_scripts.items", "generate_slurm_scripts.fill_template", "deps.append", "str"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_slurm_scripts.fill_template"], ["", "def", "generate_slurm_dependency_script", "(", "group_ids", ",", "dependency_template", ",", "aggregation_scripts", ",", "\n", "generated_script_paths", ")", ":", "\n", "    ", "deps", "=", "[", "]", "\n", "for", "group_id", ",", "aggregation_script", "in", "aggregation_scripts", ".", "items", "(", ")", ":", "\n", "        ", "array_id_list", "=", "group_ids", "[", "group_id", "]", "\n", "array_deps", "=", "\":\"", ".", "join", "(", "[", "f\"${{job_id}}_{x}\"", "for", "x", "in", "array_id_list", "]", ")", "\n", "dep", "=", "f\"sbatch --dependency=afterok:{array_deps} {aggregation_script}\"", "\n", "deps", ".", "append", "(", "dep", ")", "\n", "", "rules", "=", "{", "\n", "\"dependencies\"", ":", "\"\\n\"", ".", "join", "(", "deps", ")", ",", "\n", "\"job_script_path\"", ":", "str", "(", "generated_script_paths", "[", "\"array-job\"", "]", ")", ",", "\n", "}", "\n", "return", "fill_template", "(", "template_path", "=", "dependency_template", ",", "rules", "=", "rules", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_slurm_scripts.jobn_name2agg_log_path": [[84, 86], ["pathlib.Path"], "function", ["None"], ["", "def", "jobn_name2agg_log_path", "(", "exp_dir", ",", "job_name", ")", ":", "\n", "    ", "return", "Path", "(", "exp_dir", ")", "/", "\"data/slurm\"", "/", "job_name", "/", "\"log.txt\"", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_slurm_scripts.generate_aggregation_script": [[88, 95], ["generate_slurm_scripts.aggregation_script_path2job_name", "generate_slurm_scripts.jobn_name2agg_log_path", "jobn_name2agg_log_path.parent.mkdir", "generate_slurm_scripts.fill_template"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_slurm_scripts.aggregation_script_path2job_name", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_slurm_scripts.jobn_name2agg_log_path", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdir", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_slurm_scripts.fill_template"], ["", "def", "generate_aggregation_script", "(", "exp_dir", ",", "group_id", ",", "aggregation_template", ",", "\n", "aggregation_script_path", ")", ":", "\n", "    ", "job_name", "=", "aggregation_script_path2job_name", "(", "aggregation_script_path", ")", "\n", "log_path", "=", "jobn_name2agg_log_path", "(", "exp_dir", ",", "job_name", ")", "\n", "log_path", ".", "parent", ".", "mkdir", "(", "exist_ok", "=", "True", ",", "parents", "=", "True", ")", "\n", "rules", "=", "{", "\"job-name\"", ":", "job_name", ",", "\"group_id\"", ":", "group_id", ",", "\"log-path\"", ":", "log_path", "}", "\n", "return", "fill_template", "(", "template_path", "=", "aggregation_template", ",", "rules", "=", "rules", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_slurm_scripts.aggregation_script_path2job_name": [[97, 100], ["None"], "function", ["None"], ["", "def", "aggregation_script_path2job_name", "(", "aggregation_script_path", ")", ":", "\n", "    ", "job_name", "=", "f\"{aggregation_script_path.parent.stem}-{aggregation_script_path.stem}\"", "\n", "return", "job_name", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_slurm_scripts.generate_script": [[101, 198], ["len", "generate_slurm_scripts.parse_group_ids", "zip", "generated_script_paths.update", "array_log_path.parent.mkdir", "range", "aggregation_scripts.values", "zip", "print", "generated_script_paths.items", "open", "f.read().splitlines", "parsed.extend", "config.replace", "arg_list.replace.replace().replace().replace", "arg_list.replace.replace", "pathlib.Path", "watched_logs[].append", "watched_logs[].append", "generate_slurm_scripts.aggregation_script_path2job_name", "generate_slurm_scripts.jobn_name2agg_log_path", "watched_logs[].append", "watched_logs[].append", "jobn_name2agg_log_path.parent.mkdir", "open", "f.write", "dest_path.parent.mkdir", "utils.util.parse_grid", "pathlib.Path", "pathlib.Path", "parsed[].split", "pathlib.Path", "generated_script_paths.items", "pathlib.Path", "str().replace", "jobn_name2agg_log_path.exists", "print", "jobn_name2agg_log_path.touch", "generate_slurm_scripts.fill_template", "open", "print", "f.write", "f.read", "arg_list.replace.replace().replace", "pathlib.Path", "open", "f.write", "str", "generate_slurm_scripts.generate_aggregation_script", "str", "str", "str", "generate_slurm_scripts.generate_slurm_dependency_script", "arg_list.replace.replace", "str"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_slurm_scripts.parse_group_ids", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdir", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_slurm_scripts.aggregation_script_path2job_name", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_slurm_scripts.jobn_name2agg_log_path", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdir", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdir", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_exps.parse_grid", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_slurm_scripts.fill_template", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_slurm_scripts.generate_aggregation_script", "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_slurm_scripts.generate_slurm_dependency_script"], ["", "def", "generate_script", "(", "template_path", ",", "slurm_script_dir", ",", "job_queue", ",", "exp_dir", ",", "\n", "monitor_script", ",", "constraints", ",", "dependency_template", ",", "\n", "aggregation_template", ")", ":", "\n", "\n", "    ", "with", "open", "(", "job_queue", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "custom_args", "=", "f", ".", "read", "(", ")", ".", "splitlines", "(", ")", "\n", "# remove blank lines", "\n", "", "custom_args", "=", "[", "x", "for", "x", "in", "custom_args", "if", "x", "]", "\n", "parsed", "=", "[", "]", "\n", "for", "line", "in", "custom_args", ":", "\n", "        ", "parsed", ".", "extend", "(", "parse_grid", "(", "line", ")", ")", "\n", "", "num_array_workers", "=", "len", "(", "parsed", ")", "\n", "\n", "if", "Path", "(", "job_queue", ")", ".", "stem", "!=", "\"latest\"", ":", "\n", "        ", "array_job_name", "=", "Path", "(", "job_queue", ")", ".", "stem", "\n", "", "else", ":", "\n", "        ", "config", "=", "parsed", "[", "0", "]", ".", "split", "(", "\" \"", ")", "[", "1", "]", "\n", "array_job_name", "=", "config", ".", "replace", "(", "\"/\"", ",", "\"-\"", ")", "\n", "\n", "", "generated_script_paths", "=", "{", "\n", "\"main\"", ":", "\"slurm-dependencies.sh\"", ",", "\n", "\"array-job\"", ":", "\"slurm-job.sh\"", ",", "\n", "\"backup\"", ":", "f\"{array_job_name}.sh\"", ",", "\n", "}", "\n", "group_ids", "=", "parse_group_ids", "(", "parsed", ")", "\n", "generated_script_paths", "=", "{", "key", ":", "Path", "(", "slurm_script_dir", ")", "/", "val", "\n", "for", "key", ",", "val", "in", "generated_script_paths", ".", "items", "(", ")", "}", "\n", "\n", "aggregation_scripts", "=", "{", "}", "\n", "for", "group_id", ",", "arg_list", "in", "zip", "(", "group_ids", ",", "custom_args", ")", ":", "\n", "        ", "arg_list", "=", "arg_list", ".", "replace", "(", "\"--\"", ",", "\"\"", ")", ".", "replace", "(", "\" \"", ",", "\"_\"", ")", ".", "replace", "(", "\".json\"", ",", "\"\"", ")", "\n", "arg_list", "=", "arg_list", ".", "replace", "(", "\"|\"", ",", "\"_\"", ")", "\n", "fname", "=", "f\"{array_job_name}-{arg_list}_agg_{group_id}.sh\"", "\n", "path", "=", "Path", "(", "slurm_script_dir", ")", "/", "fname", "\n", "aggregation_scripts", "[", "group_id", "]", "=", "path", "\n", "", "generated_script_paths", ".", "update", "(", "aggregation_scripts", ")", "\n", "\n", "# worker logs", "\n", "array_log_path", "=", "Path", "(", "exp_dir", ")", "/", "\"data/slurm\"", "/", "array_job_name", "/", "\"%4a-log.txt\"", "\n", "array_log_path", ".", "parent", ".", "mkdir", "(", "exist_ok", "=", "True", ",", "parents", "=", "True", ")", "\n", "watched_logs", "=", "{", "\"paths\"", ":", "[", "]", ",", "\"dividers\"", ":", "[", "]", "}", "\n", "\n", "for", "idx", "in", "range", "(", "num_array_workers", ")", ":", "\n", "        ", "slurm_id", "=", "idx", "+", "1", "\n", "watched_log", "=", "Path", "(", "str", "(", "array_log_path", ")", ".", "replace", "(", "\"%4a\"", ",", "f\"{slurm_id:04d}\"", ")", ")", "\n", "msg", "=", "f\">>  START OF NEW JOB [{idx}/{num_array_workers}] <<\\n\"", "\n", "watched_logs", "[", "\"paths\"", "]", ".", "append", "(", "watched_log", ")", "\n", "watched_logs", "[", "\"dividers\"", "]", ".", "append", "(", "msg", ")", "\n", "\n", "", "for", "aggregation_script_path", "in", "aggregation_scripts", ".", "values", "(", ")", ":", "\n", "        ", "job_name", "=", "aggregation_script_path2job_name", "(", "aggregation_script_path", ")", "\n", "watched_log", "=", "jobn_name2agg_log_path", "(", "exp_dir", ",", "job_name", ")", "\n", "watched_logs", "[", "\"paths\"", "]", ".", "append", "(", "watched_log", ")", "\n", "watched_logs", "[", "\"dividers\"", "]", ".", "append", "(", "f\">>  STARTING AGGREGATION job [{job_name}] <<\\n\"", ")", "\n", "\n", "", "for", "watched_log", ",", "divider", "in", "zip", "(", "watched_logs", "[", "\"paths\"", "]", ",", "watched_logs", "[", "\"dividers\"", "]", ")", ":", "\n", "        ", "watched_log", ".", "parent", ".", "mkdir", "(", "exist_ok", "=", "True", ",", "parents", "=", "True", ")", "\n", "if", "not", "watched_log", ".", "exists", "(", ")", ":", "\n", "            ", "print", "(", "f\"Creating watch log: {watched_log} for the first time\"", ")", "\n", "watched_log", ".", "touch", "(", ")", "\n", "", "else", ":", "\n", "            ", "with", "open", "(", "str", "(", "watched_log", ")", ",", "\"a\"", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "divider", ")", "\n", "\n", "", "", "", "with", "open", "(", "monitor_script", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "cmd", "=", "f\"watchlogs {','.join([str(x) for x in watched_logs['paths']])}\"", "\n", "f", ".", "write", "(", "f\"{cmd}\\n\"", ")", "\n", "", "print", "(", "f\"Watching logs: {','.join(watched_logs)}\"", ")", "\n", "for", "script_name", ",", "dest_path", "in", "generated_script_paths", ".", "items", "(", ")", ":", "\n", "        ", "dest_path", ".", "parent", ".", "mkdir", "(", "exist_ok", "=", "True", ",", "parents", "=", "True", ")", "\n", "if", "script_name", "in", "{", "\"array-job\"", ",", "\"backup\"", "}", ":", "\n", "            ", "rules", "=", "{", "\n", "\"job-name\"", ":", "array_job_name", ",", "\n", "\"job_queue\"", ":", "\" \"", ".", "join", "(", "[", "f'\"{x}\"'", "for", "x", "in", "parsed", "]", ")", ",", "\n", "\"constraints\"", ":", "constraints", ",", "\n", "\"array-range\"", ":", "f\"1-{num_array_workers}\"", ",", "\n", "\"log-path\"", ":", "str", "(", "array_log_path", ")", ",", "\n", "\n", "}", "\n", "script", "=", "fill_template", "(", "template_path", ",", "rules", ")", "\n", "", "elif", "script_name", "in", "aggregation_scripts", ":", "\n", "            ", "script", "=", "generate_aggregation_script", "(", "\n", "exp_dir", "=", "exp_dir", ",", "\n", "group_id", "=", "script_name", ",", "\n", "aggregation_script_path", "=", "dest_path", ",", "\n", "aggregation_template", "=", "aggregation_template", ",", "\n", ")", "\n", "", "elif", "script_name", "==", "\"main\"", ":", "\n", "            ", "script", "=", "generate_slurm_dependency_script", "(", "\n", "group_ids", "=", "group_ids", ",", "\n", "generated_script_paths", "=", "generated_script_paths", ",", "\n", "aggregation_scripts", "=", "aggregation_scripts", ",", "\n", "dependency_template", "=", "dependency_template", ",", "\n", ")", "\n", "", "with", "open", "(", "str", "(", "dest_path", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "print", "(", "f\"Writing slurm script ({script_name}) to {dest_path}\"", ")", "\n", "f", ".", "write", "(", "script", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_slurm_scripts.main": [[200, 225], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "generate_slurm_scripts.generate_script"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.generate_slurm_scripts.generate_script"], ["", "", "", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--job_queue\"", ",", "default", "=", "\"data/job-queues/latest.txt\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--slurm_script_dir\"", ",", "default", "=", "\"data/slurm/scripts\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--slurm_template\"", ",", "default", "=", "\"misc/slurm/gpu-template_v2.sh\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dependency_template\"", ",", "default", "=", "\"misc/slurm/dependencies.sh\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--aggregation_template\"", ",", "\n", "default", "=", "\"misc/slurm/aggregate-logs-and-stats.sh\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--constraints\"", ",", "default", "=", "\"\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--exp_dir\"", ",", "\n", "#default=\"/users/albanie/coding/libs/pt/collaborative-experts\")", "\n", "# default=\"/users/ioana/collaborative-experts-internal/collaborative-experts-internal\")", "\n", "default", "=", "\"/scratch/shared/beegfs/oncescu/shared-datasets/QuerYD/collaborative\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "monitor_script", "=", "f\"slurm/monitor-jobs.sh\"", "\n", "generate_script", "(", "\n", "exp_dir", "=", "args", ".", "exp_dir", ",", "\n", "job_queue", "=", "args", ".", "job_queue", ",", "\n", "monitor_script", "=", "monitor_script", ",", "\n", "template_path", "=", "args", ".", "slurm_template", ",", "\n", "slurm_script_dir", "=", "args", ".", "slurm_script_dir", ",", "\n", "dependency_template", "=", "args", ".", "dependency_template", ",", "\n", "aggregation_template", "=", "args", ".", "aggregation_template", ",", "\n", "constraints", "=", "args", ".", "constraints", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.aggregate_logs_and_stats.summarise": [[14, 62], ["sorted", "print", "collections.OrderedDict", "collections.OrderedDict.items", "print", "config_path.exists", "utils.util.read_json", "logging.getLogger", "logging.basicConfig", "logger.log_parser.log_summary", "list", "len", "list", "summary_log.extend", "list", "open", "f.write", "first_info_log.relative_to", "logging.root.removeHandler", "logging.getLogger.addHandler", "pathlib.Path().glob", "pathlib.Path().glob", "len", "open", "f.read().splitlines", "collections.OrderedDict.values", "pathlib.Path", "logging.StreamHandler", "len", "len", "list", "pathlib.Path", "pathlib.Path", "f.read", "collections.OrderedDict.keys"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.read_json", "home.repos.pwc.inspect_result.albanie_collaborative-experts.logger.log_parser.log_summary", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys"], ["def", "summarise", "(", "group_id", ",", "log_dir", "=", "\"data/saved/log\"", ",", "model_dir", "=", "\"data/saved/models\"", ")", ":", "\n", "    ", "seeded_runs", "=", "sorted", "(", "list", "(", "Path", "(", "log_dir", ")", ".", "glob", "(", "f\"**/{group_id}/seed-*\"", ")", ")", ")", "\n", "print", "(", "f\"Found a total of {len(seeded_runs)} seed runs in {group_id}\"", ")", "\n", "msg", "=", "f\"Found no seeded runs for group_id: {group_id} in {log_dir}\"", "\n", "assert", "len", "(", "seeded_runs", ")", ">", "0", ",", "msg", "\n", "\n", "info_logs", "=", "OrderedDict", "(", ")", "\n", "for", "seeded_run", "in", "seeded_runs", ":", "\n", "        ", "info_log_matches", "=", "list", "(", "Path", "(", "seeded_run", ")", ".", "glob", "(", "\"**/info.log\"", ")", ")", "\n", "msg", "=", "f\"expected to find a single info.log file, found {len(info_log_matches)}\"", "\n", "assert", "len", "(", "info_log_matches", ")", "==", "1", ",", "msg", "\n", "info_logs", "[", "seeded_run", ".", "stem", "]", "=", "info_log_matches", "[", "0", "]", "\n", "\n", "", "summary_log", "=", "[", "]", "\n", "for", "seeded_run", ",", "info_log_path", "in", "info_logs", ".", "items", "(", ")", ":", "\n", "        ", "with", "open", "(", "info_log_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "log", "=", "f", ".", "read", "(", ")", ".", "splitlines", "(", ")", "\n", "", "summary_log", ".", "extend", "(", "log", ")", "\n", "", "first_info_log", "=", "list", "(", "info_logs", ".", "values", "(", ")", ")", "[", "0", "]", "\n", "summary_log_name", "=", "f\"summary-{'_'.join(list(info_logs.keys()))}.json\"", "\n", "summary_log_path", "=", "first_info_log", ".", "parent", "/", "summary_log_name", "\n", "with", "open", "(", "summary_log_path", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "\"\\n\"", ".", "join", "(", "summary_log", ")", ")", "\n", "", "print", "(", "f\"Wrote concatenated logs to {summary_log_path}\"", ")", "\n", "\n", "# retrieve the config from the first run", "\n", "rel_path", "=", "first_info_log", ".", "relative_to", "(", "log_dir", ")", ".", "parent", "\n", "config_path", "=", "Path", "(", "model_dir", ")", "/", "rel_path", "/", "\"config.json\"", "\n", "assert", "config_path", ".", "exists", "(", ")", ",", "f\"Could not find config at {config_path}\"", "\n", "config", "=", "read_json", "(", "config_path", ")", "\n", "\n", "logger", "=", "logging", ".", "getLogger", "(", "\"summary\"", ")", "\n", "\n", "# some care is required with logging to avoid sending all experiment logs", "\n", "# to the same file.  We avoid this by essentially resetting the logging utility", "\n", "\n", "# Remove all handlers associated with the root logger object", "\n", "for", "handler", "in", "logging", ".", "root", ".", "handlers", "[", ":", "]", ":", "\n", "        ", "logging", ".", "root", ".", "removeHandler", "(", "handler", ")", "\n", "", "logging", ".", "basicConfig", "(", "filename", "=", "summary_log_path", ",", "level", "=", "logging", ".", "INFO", ")", "\n", "if", "not", "logger", ".", "handlers", ":", "\n", "        ", "logger", ".", "addHandler", "(", "logging", ".", "StreamHandler", "(", ")", ")", "\n", "\n", "", "log_summary", "(", "\n", "logger", "=", "logger", ",", "\n", "log_path", "=", "summary_log_path", ",", "\n", "eval_mode", "=", "config", "[", "\"eval_mode\"", "]", ",", "\n", "fixed_num_epochs", "=", "config", "[", "\"trainer\"", "]", "[", "\"epochs\"", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.aggregate_logs_and_stats.main": [[65, 70], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "aggregate_logs_and_stats.summarise"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.misc.aggregate_logs_and_stats.summarise"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--group_id\"", ",", "default", "=", "\"ed53d01d\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "summarise", "(", "group_id", "=", "args", ".", "group_id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.prepare_submission.get_dataset_num_queries": [[20, 52], ["None"], "function", ["None"], ["@", "typechecked", "\n", "def", "get_dataset_num_queries", "(", "dataset", ":", "str", ",", "challenge_phase", ":", "str", ")", "->", "int", ":", "\n", "    ", "stats", "=", "{", "\n", "\"max_descriptions_per_video\"", ":", "{", "\n", "\"MSRVTT\"", ":", "20", ",", "\n", "\"LSMDC\"", ":", "1", ",", "\n", "\"MSVD\"", ":", "81", ",", "\n", "\"DiDeMo\"", ":", "1", ",", "\n", "\"YouCook2\"", ":", "1", ",", "\n", "\"ActivityNet\"", ":", "1", ",", "\n", "}", ",", "\n", "\"expected_videos\"", ":", "{", "\n", "\"MSRVTT\"", ":", "{", "\"public_server_val\"", ":", "497", ",", "\"public_server_test\"", ":", "2990", "}", ",", "\n", "\"LSMDC\"", ":", "{", "\"public_server_val\"", ":", "7408", ",", "\"public_server_test\"", ":", "1000", "}", ",", "\n", "\"YouCook2\"", ":", "{", "\"public_server_val\"", ":", "969", ",", "\"public_server_test\"", ":", "3310", "}", ",", "\n", "\"MSVD\"", ":", "{", "\"public_server_val\"", ":", "100", ",", "\"public_server_test\"", ":", "670", "}", ",", "\n", "\"DiDeMo\"", ":", "{", "\"public_server_val\"", ":", "1065", ",", "\"public_server_test\"", ":", "1004", "}", ",", "\n", "\"ActivityNet\"", ":", "{", "\"public_server_val\"", ":", "1001", ",", "\"public_server_test\"", ":", "4917", "}", "\n", "}", ",", "\n", "\"expected_invalid_queries\"", ":", "{", "\n", "\"MSRVTT\"", ":", "{", "\"public_server_val\"", ":", "0", ",", "\"public_server_test\"", ":", "6", "}", ",", "\n", "\"LSMDC\"", ":", "{", "\"public_server_val\"", ":", "0", ",", "\"public_server_test\"", ":", "0", "}", ",", "\n", "\"YouCook2\"", ":", "{", "\"public_server_val\"", ":", "0", ",", "\"public_server_test\"", ":", "0", "}", ",", "\n", "\"MSVD\"", ":", "{", "\"public_server_val\"", ":", "3810", ",", "\"public_server_test\"", ":", "26507", "}", ",", "\n", "\"DiDeMo\"", ":", "{", "\"public_server_val\"", ":", "0", ",", "\"public_server_test\"", ":", "0", "}", ",", "\n", "\"ActivityNet\"", ":", "{", "\"public_server_val\"", ":", "0", ",", "\"public_server_test\"", ":", "0", "}", ",", "\n", "}", ",", "\n", "}", "\n", "num_videos", "=", "stats", "[", "\"expected_videos\"", "]", "[", "dataset", "]", "[", "challenge_phase", "]", "\n", "num_queries", "=", "num_videos", "*", "stats", "[", "\"max_descriptions_per_video\"", "]", "[", "dataset", "]", "\n", "invalid_queries", "=", "stats", "[", "\"expected_invalid_queries\"", "]", "[", "dataset", "]", "[", "challenge_phase", "]", "\n", "return", "num_queries", "-", "invalid_queries", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.prepare_submission.validate_predictions": [[54, 67], ["prepare_submission.get_dataset_num_queries", "print"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.prepare_submission.get_dataset_num_queries"], ["", "@", "typechecked", "\n", "def", "validate_predictions", "(", "\n", "preds", ":", "np", ".", "ndarray", ",", "\n", "dataset", ":", "str", ",", "\n", "challenge_phase", ":", "str", ",", "\n", "topk", ":", "int", "=", "10", ",", "\n", ")", ":", "\n", "    ", "shape", "=", "preds", ".", "shape", "\n", "num_queries", "=", "get_dataset_num_queries", "(", "dataset", ",", "challenge_phase", "=", "challenge_phase", ")", "\n", "expected", "=", "(", "num_queries", ",", "topk", ")", "\n", "msg", "=", "f\"Expected ranks with shape {expected}, but found {shape} for {dataset}\"", "\n", "assert", "shape", "==", "expected", ",", "msg", "\n", "print", "(", "f\"Found valid rank matrix for {dataset} [shape: {shape}]\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.prepare_submission.generate_predictions": [[69, 105], ["datetime.datetime.now().strftime", "dest_dir.mkdir", "humanize.naturalsize", "print", "open", "json.load", "sorted", "hashlib.sha256().hexdigest", "list", "print", "zipfile.ZipFile", "enumerate", "datetime.datetime.now", "dest_dir.glob", "json.load.items", "pathlib.Path", "print", "f.write", "dest_path.stat", "hashlib.sha256", "numpy.loadtxt", "humanize.naturalsize", "print", "prepare_submission.validate_predictions", "json.load.values", "contents.encode", "pathlib.Path.stat", "len"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdir", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.prepare_submission.validate_predictions"], ["", "@", "typechecked", "\n", "def", "generate_predictions", "(", "\n", "refresh", ":", "bool", ",", "\n", "validate", ":", "bool", ",", "\n", "dest_dir", ":", "Path", ",", "\n", "challenge_phase", ":", "str", ",", "\n", "predictions_path", ":", "Path", ",", "\n", ")", ":", "\n", "    ", "\"\"\"Generate a zip file for a submission in the challenge format.\"\"\"", "\n", "with", "open", "(", "predictions_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "predictions", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "# generate a filename from a hash of the prediction locations and the current date", "\n", "", "contents", "=", "\"\"", ".", "join", "(", "sorted", "(", "[", "val", "[", "challenge_phase", "]", "for", "val", "in", "predictions", ".", "values", "(", ")", "]", ")", ")", "\n", "content_hash", "=", "hashlib", ".", "sha256", "(", "contents", ".", "encode", "(", "\"utf-8\"", ")", ")", ".", "hexdigest", "(", ")", "[", ":", "10", "]", "\n", "dt", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%Y-%m-%d_%H-%M-%S\"", ")", "\n", "dest_name", "=", "f\"{challenge_phase}-{content_hash}-{dt}.zip\"", "\n", "\n", "dest_path", "=", "dest_dir", "/", "dest_name", "\n", "dest_dir", ".", "mkdir", "(", "exist_ok", "=", "True", ",", "parents", "=", "True", ")", "\n", "if", "list", "(", "dest_dir", ".", "glob", "(", "f\"{challenge_phase}-{content_hash}*.zip\"", ")", ")", "and", "not", "refresh", ":", "\n", "        ", "print", "(", "f\"Found existing submission for {predictions_path}, skipping\"", ")", "\n", "return", "\n", "", "with", "zipfile", ".", "ZipFile", "(", "dest_path", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "ii", ",", "(", "dataset", ",", "paths", ")", "in", "enumerate", "(", "predictions", ".", "items", "(", ")", ")", ":", "\n", "            ", "pred_path", "=", "Path", "(", "paths", "[", "challenge_phase", "]", ")", "\n", "if", "validate", ":", "\n", "                ", "preds", "=", "np", ".", "loadtxt", "(", "pred_path", ",", "dtype", "=", "int", ",", "delimiter", "=", "\",\"", ")", "\n", "pred_size", "=", "humanize", ".", "naturalsize", "(", "pred_path", ".", "stat", "(", ")", ".", "st_size", ")", "\n", "print", "(", "f\"Validating predictions from [{pred_path}] [{pred_size}]\"", ")", "\n", "validate_predictions", "(", "preds", ",", "dataset", ",", "challenge_phase", "=", "challenge_phase", ")", "\n", "", "print", "(", "f\"{ii}/{len(predictions)} Writing predictions for {dataset} \"", "\n", "f\"from {pred_path} to {dest_path} as {pred_path.name}\"", ")", "\n", "f", ".", "write", "(", "pred_path", ",", "arcname", "=", "pred_path", ".", "name", ")", "\n", "", "", "zip_size", "=", "humanize", ".", "naturalsize", "(", "dest_path", ".", "stat", "(", ")", ".", "st_size", ")", "\n", "print", "(", "f\"Zipfile {dest_path} [{zip_size}] can now be uploaded to CodaLab\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.prepare_submission.main": [[107, 131], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "prepare_submission.generate_predictions", "bool"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.prepare_submission.generate_predictions"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Prepare challenge submission'", ")", "\n", "parser", ".", "add_argument", "(", "\"--refresh\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"whether to overwrite existing submission files\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--validate\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"whether to validate the submission shapes\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--predictions_path\"", ",", "type", "=", "Path", ",", "\n", "default", "=", "\"misc/cvpr2020-challenge/predictions.json\"", ",", "\n", "help", "=", "\"location of file containing paths to predictions\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dest_dir\"", ",", "type", "=", "Path", ",", "\n", "default", "=", "\"data/cvpr2020-challenge-submissions\"", ",", "\n", "help", "=", "\"directory location where submissions will be stored\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--challenge_phase\"", ",", "default", "=", "\"public_server_val\"", ",", "\n", "choices", "=", "[", "\"public_server_val\"", ",", "\"public_server_test\"", "]", ",", "\n", "help", "=", "(", "\"whether the submission is for the validation phase of the\"", "\n", "\" competition, or the final test phase\"", ")", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "generate_predictions", "(", "\n", "refresh", "=", "args", ".", "refresh", ",", "\n", "validate", "=", "bool", "(", "args", ".", "validate", ")", ",", "\n", "dest_dir", "=", "args", ".", "dest_dir", ",", "\n", "challenge_phase", "=", "args", ".", "challenge_phase", ",", "\n", "predictions_path", "=", "args", ".", "predictions_path", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.test_baselines.json_key2dataset_name": [[27, 42], ["None"], "function", ["None"], ["@", "typechecked", "\n", "def", "json_key2dataset_name", "(", "json_key", ":", "str", ")", "->", "str", ":", "\n", "    ", "\"\"\"Convert json key used for a given datset into the name used in the codebase.\n\n    Args:\n        json_key: the key to be converted\n\n    Returns:\n        the name of the dataset\n\n    NOTE: This is used to patch around inconsistency in the naming scheme used for\n    ActivityNet and every other datsaet.\n    \"\"\"", "\n", "dataset_name", "=", "{", "\"ActivityNet\"", ":", "\"activity-net\"", "}", ".", "get", "(", "json_key", ",", "json_key", ")", "\n", "return", "dataset_name", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.test_baselines.evaluate_from_ckpts": [[44, 85], ["datetime.datetime.now().strftime", "json.load.items", "print", "misc.cvpr2020_challenge.prepare_submission.generate_predictions", "open", "json.load", "test_baselines.json_key2dataset_name", "datasets.append", "json_key2dataset_name.lower", "print", "misc.cvpr2020_challenge.train_baselines.launch_and_monitor_cmd", "misc.cvpr2020_challenge.train_baselines.parse_paths_from_logs", "open", "json.dump", "datetime.datetime.now"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.prepare_submission.generate_predictions", "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.test_baselines.json_key2dataset_name", "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.train_baselines.launch_and_monitor_cmd", "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.train_baselines.parse_paths_from_logs"], ["", "@", "typechecked", "\n", "def", "evaluate_from_ckpts", "(", "\n", "ckpt_list_path", ":", "Path", ",", "\n", "challenge_config_dir", ":", "Path", ",", "\n", "dest_dir", ":", "Path", ",", "\n", "device", ":", "int", ",", "\n", ")", ":", "\n", "    ", "with", "open", "(", "ckpt_list_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "ckpts", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "challenge_phase", "=", "\"public_server_test\"", "\n", "timestamp", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "r\"%Y-%m-%d_%H-%M-%S\"", ")", "\n", "\n", "test_set_prediction_paths", "=", "{", "}", "\n", "datasets", "=", "[", "]", "\n", "for", "json_key", ",", "ckpt_path", "in", "ckpts", ".", "items", "(", ")", ":", "\n", "        ", "dataset", "=", "json_key2dataset_name", "(", "json_key", ")", "\n", "datasets", ".", "append", "(", "dataset", ")", "\n", "folder", "=", "dataset", ".", "lower", "(", ")", "\n", "config_path", "=", "challenge_config_dir", "/", "folder", "/", "\"baseline-public-test.json\"", "\n", "flags", "=", "(", "f\" --config {config_path} --device {device} --eval_from_training_config\"", "\n", "f\" --resume {ckpt_path}\"", ")", "\n", "cmd", "=", "f\"python -u test.py {flags}\"", "\n", "print", "(", "f\"Launching baseline evaluation for {dataset} with command: {cmd}\"", ")", "\n", "logs", "=", "launch_and_monitor_cmd", "(", "cmd", "=", "cmd", ")", "\n", "exp_paths", "=", "parse_paths_from_logs", "(", "logs", ",", "queries", "=", "[", "\"predictions\"", "]", ")", "\n", "test_set_prediction_paths", "[", "json_key", "]", "=", "{", "challenge_phase", ":", "exp_paths", "[", "\"predictions\"", "]", "}", "\n", "\n", "", "fname", "=", "f\"baselines-{timestamp}-{challenge_phase}-{'-'.join(datasets)}.json\"", "\n", "dest_path", "=", "dest_dir", "/", "f\"predictions-{fname}\"", "\n", "\n", "print", "(", "f\"Writing baseline predicitions list to {dest_path}\"", ")", "\n", "with", "open", "(", "dest_path", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "test_set_prediction_paths", ",", "f", ",", "indent", "=", "4", ",", "sort_keys", "=", "True", ")", "\n", "\n", "", "generate_predictions", "(", "\n", "refresh", "=", "True", ",", "\n", "validate", "=", "True", ",", "\n", "dest_dir", "=", "dest_dir", ",", "\n", "challenge_phase", "=", "challenge_phase", ",", "\n", "predictions_path", "=", "dest_path", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.test_baselines.main": [[88, 105], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "test_baselines.evaluate_from_ckpts"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.test_baselines.evaluate_from_ckpts"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"evaluate baselines from ckpts\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--device\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"gpu device to use for training\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--ckpt_list_path\"", ",", "type", "=", "Path", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path to json file containing model checkpoint locations\"", ")", "\n", "parser", ".", "add_argument", "(", "'--challenge_config_dir'", ",", "type", "=", "Path", ",", "\n", "default", "=", "\"configs/cvpr2020-challenge\"", ")", "\n", "parser", ".", "add_argument", "(", "'--dest_dir'", ",", "type", "=", "Path", ",", "\n", "default", "=", "\"data/cvpr2020-challenge-submissions\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "evaluate_from_ckpts", "(", "\n", "device", "=", "args", ".", "device", ",", "\n", "dest_dir", "=", "args", ".", "dest_dir", ",", "\n", "challenge_config_dir", "=", "args", ".", "challenge_config_dir", ",", "\n", "ckpt_list_path", "=", "args", ".", "ckpt_list_path", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.train_baselines.launch_and_monitor_cmd": [[22, 30], ["subprocess.Popen", "cmd.split", "print", "lines.append"], "function", ["None"], ["@", "typechecked", "\n", "def", "launch_and_monitor_cmd", "(", "cmd", ":", "str", ")", "->", "List", ":", "\n", "    ", "lines", "=", "[", "]", "\n", "with", "Popen", "(", "cmd", ".", "split", "(", ")", ",", "stdout", "=", "PIPE", ",", "bufsize", "=", "1", ",", "universal_newlines", "=", "True", ")", "as", "proc", ":", "\n", "        ", "for", "line", "in", "proc", ".", "stdout", ":", "\n", "            ", "print", "(", "line", ",", "end", "=", "''", ")", "\n", "lines", ".", "append", "(", "line", ")", "\n", "", "", "return", "lines", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.train_baselines.dataset_name2json_key": [[32, 38], ["None"], "function", ["None"], ["", "@", "typechecked", "\n", "def", "dataset_name2json_key", "(", "dataset", ":", "str", ")", "->", "str", ":", "\n", "# Ensure that ActivityNet dataset key is stored with CamelCase convention when", "\n", "# storing in jsons", "\n", "    ", "json_key", "=", "{", "\"activity-net\"", ":", "\"ActivityNet\"", "}", ".", "get", "(", "dataset", ",", "dataset", ")", "\n", "return", "json_key", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.train_baselines.parse_paths_from_logs": [[40, 55], ["sum", "matches.index", "x.startswith", "logs[].rstrip().split", "logs[].rstrip"], "function", ["None"], ["", "@", "typechecked", "\n", "def", "parse_paths_from_logs", "(", "logs", ":", "List", "[", "str", "]", ",", "queries", ":", "List", "[", "str", "]", ",", ")", "->", "Dict", "[", "str", ",", "str", "]", ":", "\n", "    ", "prefixes", "=", "{", "\n", "\"predictions\"", ":", "\"Saved similarity matrix predictions to\"", ",", "\n", "\"ckpts\"", ":", "\"The best performing ckpt can be found at\"", ",", "\n", "}", "\n", "paths", "=", "{", "}", "\n", "for", "key", "in", "queries", ":", "\n", "        ", "prefix", "=", "prefixes", "[", "key", "]", "\n", "matches", "=", "[", "x", ".", "startswith", "(", "prefix", ")", "for", "x", "in", "logs", "]", "\n", "found", "=", "sum", "(", "matches", ")", "\n", "assert", "found", "==", "1", ",", "f\"Expected to find one match for `{prefix}`, found {found}\"", "\n", "pos", "=", "matches", ".", "index", "(", "True", ")", "\n", "paths", "[", "key", "]", "=", "logs", "[", "pos", "]", ".", "rstrip", "(", ")", ".", "split", "(", "\" \"", ")", "[", "-", "1", "]", "\n", "", "return", "paths", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.train_baselines.train_baseline_for_dataset": [[57, 77], ["dataset.lower", "print", "train_baselines.launch_and_monitor_cmd", "train_baselines.parse_paths_from_logs"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.train_baselines.launch_and_monitor_cmd", "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.train_baselines.parse_paths_from_logs"], ["", "@", "typechecked", "\n", "def", "train_baseline_for_dataset", "(", "\n", "challenge_config_dir", ":", "Path", ",", "\n", "mini_train", ":", "bool", ",", "\n", "train_single_epoch", ":", "bool", ",", "\n", "device", ":", "int", ",", "\n", "dataset", ":", "str", ",", "\n", ")", "->", "Dict", "[", "str", ",", "str", "]", ":", "\n", "    ", "folder", "=", "dataset", ".", "lower", "(", ")", "\n", "config_path", "=", "challenge_config_dir", "/", "folder", "/", "\"baseline-public-trainval.json\"", "\n", "flags", "=", "f\" --config {config_path} --device {device}\"", "\n", "if", "mini_train", ":", "\n", "        ", "flags", "+=", "f\" --mini_train\"", "\n", "", "if", "train_single_epoch", ":", "\n", "        ", "flags", "+=", "f\" --train_single_epoch\"", "\n", "", "cmd", "=", "f\"python -u train.py {flags}\"", "\n", "print", "(", "f\"Launching baseline for {dataset} with command: {cmd}\"", ")", "\n", "logs", "=", "launch_and_monitor_cmd", "(", "cmd", "=", "cmd", ")", "\n", "exp_paths", "=", "parse_paths_from_logs", "(", "logs", ",", "queries", "=", "[", "\"predictions\"", ",", "\"ckpts\"", "]", ")", "\n", "return", "exp_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.train_baselines.train_baselines": [[79, 129], ["dest_dir.mkdir", "dest_paths.items", "print", "misc.cvpr2020_challenge.prepare_submission.generate_predictions", "train_baselines.train_baseline_for_dataset", "train_baselines.dataset_name2json_key", "open", "json.dump", "open", "outputs[].update", "json.load"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.mkdir", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.prepare_submission.generate_predictions", "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.train_baselines.train_baseline_for_dataset", "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.train_baselines.dataset_name2json_key", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update"], ["", "@", "typechecked", "\n", "def", "train_baselines", "(", "\n", "dest_dir", ":", "Path", ",", "\n", "challenge_config_dir", ":", "Path", ",", "\n", "datasets", ":", "List", "[", "str", "]", ",", "\n", "slurm", ":", "bool", ",", "\n", "mini_train", ":", "bool", ",", "\n", "train_single_epoch", ":", "bool", ",", "\n", "device", ":", "int", ",", "\n", "timestamp", ":", "str", ",", "\n", "aggregate", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "    ", "challenge_phase", "=", "\"public_server_val\"", "\n", "fname", "=", "f\"baselines-{timestamp}-{challenge_phase}-{'-'.join(datasets)}.json\"", "\n", "outputs", "=", "{", "key", ":", "{", "}", "for", "key", "in", "(", "\"predictions\"", ",", "\"ckpts\"", ")", "}", "\n", "dest_dir", ".", "mkdir", "(", "exist_ok", "=", "True", ",", "parents", "=", "True", ")", "\n", "dest_paths", "=", "{", "key", ":", "dest_dir", "/", "f\"{key}-{fname}\"", "for", "key", "in", "outputs", "}", "\n", "\n", "if", "aggregate", ":", "\n", "        ", "for", "dataset", "in", "datasets", ":", "\n", "            ", "fname", "=", "f\"baselines-{timestamp}-{challenge_phase}-{dataset}.json\"", "\n", "for", "key", "in", "outputs", ":", "\n", "                ", "with", "open", "(", "dest_dir", "/", "f\"{key}-{fname}\"", ",", "\"r\"", ")", "as", "f", ":", "\n", "                    ", "outputs", "[", "key", "]", ".", "update", "(", "json", ".", "load", "(", "f", ")", ")", "\n", "", "", "", "", "else", ":", "\n", "        ", "for", "dataset", "in", "datasets", ":", "\n", "            ", "exp_paths", "=", "train_baseline_for_dataset", "(", "\n", "dataset", "=", "dataset", ",", "\n", "device", "=", "device", ",", "\n", "mini_train", "=", "mini_train", ",", "\n", "train_single_epoch", "=", "train_single_epoch", ",", "\n", "challenge_config_dir", "=", "challenge_config_dir", ",", "\n", ")", "\n", "dataset_key", "=", "dataset_name2json_key", "(", "dataset", ")", "\n", "outputs", "[", "\"ckpts\"", "]", "[", "dataset_key", "]", "=", "exp_paths", "[", "\"ckpts\"", "]", "\n", "outputs", "[", "\"predictions\"", "]", "[", "dataset_key", "]", "=", "{", "challenge_phase", ":", "\n", "exp_paths", "[", "\"predictions\"", "]", "}", "\n", "\n", "", "", "for", "key", ",", "dest_path", "in", "dest_paths", ".", "items", "(", ")", ":", "\n", "        ", "print", "(", "f\"Writing baseline {key} list to {dest_path}\"", ")", "\n", "with", "open", "(", "dest_path", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "outputs", "[", "key", "]", ",", "f", ",", "indent", "=", "4", ",", "sort_keys", "=", "True", ")", "\n", "\n", "", "", "if", "not", "slurm", ":", "\n", "        ", "generate_predictions", "(", "\n", "refresh", "=", "True", ",", "\n", "validate", "=", "True", ",", "\n", "dest_dir", "=", "dest_dir", ",", "\n", "challenge_phase", "=", "challenge_phase", ",", "\n", "predictions_path", "=", "dest_paths", "[", "\"predictions\"", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.train_baselines.train_baselines_with_yaspi": [[132, 158], ["utils.util.filter_cmd_args", "utils.util.filter_cmd_args.extend", "yaspi.yaspi.Yaspi", "yaspi.yaspi.Yaspi.submit", "train_baselines.train_baselines", "open", "json.load", "len"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.filter_cmd_args", "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.train_baselines.train_baselines"], ["", "", "@", "typechecked", "\n", "def", "train_baselines_with_yaspi", "(", "\n", "yaspi_defaults_path", ":", "Path", ",", "\n", "common_kwargs", ":", "Dict", ",", "\n", "timestamp", ":", "str", ",", "\n", "datasets", ":", "List", "[", "str", "]", ",", "\n", ")", ":", "\n", "    ", "with", "open", "(", "yaspi_defaults_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "yaspi_defaults", "=", "json", ".", "load", "(", "f", ")", "\n", "", "cmd_args", "=", "sys", ".", "argv", "\n", "remove", "=", "[", "\"--yaspify\"", ",", "\"--datasets\"", "]", "\n", "cmd_args", "=", "filter_cmd_args", "(", "cmd_args", ",", "remove", "=", "remove", ")", "\n", "cmd_args", ".", "extend", "(", "[", "\"--timestamp\"", ",", "timestamp", "]", ")", "\n", "base_cmd", "=", "f\"python {' '.join(cmd_args)}\"", "\n", "job_name", "=", "f\"baselines-{timestamp}\"", "\n", "job_queue", "=", "[", "f'\"--datasets {dataset}\"'", "for", "dataset", "in", "datasets", "]", "\n", "job_queue", "=", "\" \"", ".", "join", "(", "job_queue", ")", "\n", "job", "=", "Yaspi", "(", "\n", "cmd", "=", "base_cmd", ",", "\n", "job_queue", "=", "job_queue", ",", "\n", "job_name", "=", "job_name", ",", "\n", "job_array_size", "=", "len", "(", "datasets", ")", ",", "\n", "**", "yaspi_defaults", ",", "\n", ")", "\n", "job", ".", "submit", "(", "watch", "=", "True", ",", "conserve_resources", "=", "5", ")", "\n", "train_baselines", "(", "**", "common_kwargs", ",", "aggregate", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.train_baselines.main": [[160, 206], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "dict", "datetime.datetime.now().strftime", "train_baselines.train_baselines_with_yaspi", "train_baselines.train_baselines", "datetime.datetime.now"], "function", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.train_baselines.train_baselines_with_yaspi", "home.repos.pwc.inspect_result.albanie_collaborative-experts.cvpr2020_challenge.train_baselines.train_baselines"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"train baselines\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--debug\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"launch in debug mode\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--timestamp\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--yaspify\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"launch via slurm\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--slurm\"", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--device\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"gpu device to use for training\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--datasets\"", ",", "nargs", "=", "\"+\"", ",", "\n", "default", "=", "[", "\"MSRVTT\"", ",", "\"MSVD\"", ",", "\"DiDeMo\"", ",", "\"YouCook2\"", ",", "\"activity-net\"", "]", ",", "\n", "help", "=", "\"The challenge datasets to train baselines for\"", ")", "\n", "parser", ".", "add_argument", "(", "'--mini_train'", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "'--train_single_epoch'", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "'--dest_dir'", ",", "type", "=", "Path", ",", "\n", "default", "=", "\"data/cvpr2020-challenge-submissions\"", ")", "\n", "parser", ".", "add_argument", "(", "'--challenge_config_dir'", ",", "type", "=", "Path", ",", "\n", "default", "=", "\"configs/cvpr2020-challenge\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--yaspi_defaults_path\"", ",", "type", "=", "Path", ",", "\n", "default", "=", "\"misc/yaspi_gpu_defaults.json\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "timestamp", ":", "\n", "        ", "timestamp", "=", "args", ".", "timestamp", "\n", "", "else", ":", "\n", "        ", "timestamp", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "r\"%Y-%m-%d_%H-%M-%S\"", ")", "\n", "\n", "", "common_kwargs", "=", "dict", "(", "\n", "device", "=", "args", ".", "device", ",", "\n", "timestamp", "=", "timestamp", ",", "\n", "dest_dir", "=", "args", ".", "dest_dir", ",", "\n", "datasets", "=", "args", ".", "datasets", ",", "\n", "slurm", "=", "args", ".", "slurm", ",", "\n", "mini_train", "=", "args", ".", "mini_train", ",", "\n", "train_single_epoch", "=", "args", ".", "train_single_epoch", ",", "\n", "challenge_config_dir", "=", "args", ".", "challenge_config_dir", ",", "\n", ")", "\n", "\n", "if", "args", ".", "yaspify", ":", "\n", "        ", "train_baselines_with_yaspi", "(", "\n", "datasets", "=", "args", ".", "datasets", ",", "\n", "timestamp", "=", "timestamp", ",", "\n", "common_kwargs", "=", "common_kwargs", ",", "\n", "yaspi_defaults_path", "=", "args", ".", "yaspi_defaults_path", "\n", ")", "\n", "", "else", ":", "\n", "        ", "train_baselines", "(", "**", "common_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.dataset_paths": [[29, 36], ["None"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "@", "abstractmethod", "\n", "@", "typechecked", "\n", "def", "dataset_paths", "(", "training_file", "=", "None", ")", "->", "Dict", "[", "str", ",", "Union", "[", "Path", ",", "str", "]", "]", ":", "\n", "        ", "\"\"\"Generates a datastructure containing all the paths required to load features\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.sanity_checks": [[37, 42], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "sanity_checks", "(", "self", ")", ":", "\n", "        ", "\"\"\"Run sanity checks on loaded data\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.load_features": [[43, 48], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "load_features", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load features from disk\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.__init__": [[49, 287], ["set", "list", "base_dataset.BaseDataset.configure_train_test_splits", "base_dataset.BaseDataset.logger.info", "len", "len", "base_dataset.BaseDataset.load_features", "utils.util.expert_tensor_storage", "retrieval.update", "enumerate", "base_dataset.BaseDataset.sanity_checks", "raw_input_dims.keys", "raw_input_dims.keys", "pickle.load", "len", "base_dataset.BaseDataset.logger.info", "base_dataset.BaseDataset.logger.info", "base_dataset.BaseDataset.text_features.items", "base_dataset.BaseDataset.logger.info", "numpy.zeros", "torch.ones", "len", "numpy.zeros", "numpy.zeros", "enumerate", "base_dataset.BaseDataset.label_features.items", "base_dataset.BaseDataset.tensor_storage[].intersection", "base_dataset.BaseDataset.tensor_storage[].intersection", "open", "numpy.zeros", "numpy.mean", "numpy.zeros", "numpy.zeros", "numpy.zeros", "ValueError", "base_dataset.BaseDataset.has_missing_values", "base_dataset.BaseDataset.feat_aggregation[].get", "base_dataset.BaseDataset.has_missing_values", "base_dataset.BaseDataset.feat_aggregation[].get", "numpy.mean", "base_dataset.BaseDataset.feat_aggregation[].keys", "numpy.logical_not", "numpy.ones_like", "numpy.logical_not", "numpy.ones_like", "min", "numpy.vstack", "min", "range", "pathlib.Path", "numpy.isnan", "numpy.isnan", "len", "len", "min", "numpy.array", "len", "len", "len", "base_dataset.BaseDataset.logger.info", "ValueError", "len", "len"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.configure_train_test_splits", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.load_features", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.expert_tensor_storage", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.sanity_checks", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.has_missing_values", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.has_missing_values", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys"], ["", "@", "typechecked", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "data_dir", ":", "Path", ",", "\n", "fuse_captions", ":", "bool", ",", "\n", "spatial_feats", ":", "bool", ",", "\n", "challenge_mode", ":", "bool", ",", "\n", "eval_only", ":", "bool", ",", "\n", "use_zeros_for_missing", ":", "bool", ",", "\n", "task", ":", "str", ",", "\n", "text_agg", ":", "str", ",", "\n", "text_feat", ":", "str", ",", "\n", "split_name", ":", "str", ",", "\n", "cls_partition", ":", "str", ",", "\n", "root_feat_folder", ":", "str", ",", "\n", "challenge_test_root_feat_folder", ":", "str", ",", "\n", "text_dim", ":", "int", ",", "\n", "num_test_captions", ":", "int", ",", "\n", "restrict_train_captions", ":", "int", ",", "\n", "max_tokens", ":", "Dict", "[", "str", ",", "int", "]", ",", "\n", "text_dropout", ":", "float", ",", "\n", "logger", ":", "logging", ".", "Logger", ",", "\n", "raw_input_dims", ":", "Dict", "[", "str", ",", "int", "]", ",", "\n", "feat_aggregation", ":", "Dict", "[", "str", ",", "Dict", "]", ",", "\n", "distil_params", ":", "Union", "[", "None", ",", "Dict", "]", ",", "\n", "training_file", ":", "Union", "[", "None", ",", "str", "]", ",", "\n", "caption_masks", ":", "Union", "[", "None", ",", "str", "]", ",", "\n", "ce_shared_dim", ":", "Union", "[", "None", ",", "int", "]", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "self", ".", "task", "=", "task", "\n", "self", ".", "eval_only", "=", "eval_only", "\n", "self", ".", "logger", "=", "logger", "\n", "self", ".", "challenge_mode", "=", "challenge_mode", "\n", "self", ".", "text_feat", "=", "text_feat", "\n", "self", ".", "data_dir", "=", "data_dir", "\n", "self", ".", "text_dim", "=", "text_dim", "\n", "self", ".", "spatial_feats", "=", "spatial_feats", "\n", "self", ".", "text_dropout", "=", "text_dropout", "\n", "self", ".", "restrict_train_captions", "=", "restrict_train_captions", "\n", "self", ".", "max_tokens", "=", "max_tokens", "\n", "self", ".", "cls_partition", "=", "cls_partition", "\n", "self", ".", "fuse_captions", "=", "fuse_captions", "\n", "self", ".", "num_test_captions", "=", "num_test_captions", "\n", "self", ".", "feat_aggregation", "=", "feat_aggregation", "\n", "self", ".", "root_feat", "=", "data_dir", "/", "root_feat_folder", "\n", "self", ".", "challenge_test_root_feat_folder", "=", "data_dir", "/", "challenge_test_root_feat_folder", "\n", "self", ".", "experts", "=", "set", "(", "raw_input_dims", ".", "keys", "(", ")", ")", "\n", "self", ".", "distil_params", "=", "distil_params", "\n", "self", ".", "training_file", "=", "training_file", "\n", "\n", "# This attributes can be overloaded by different datasets, so it must be set", "\n", "# before the `load_features() method call`", "\n", "self", ".", "restrict_test_captions", "=", "None", "\n", "self", ".", "text_features", "=", "None", "\n", "self", ".", "label_features", "=", "None", "\n", "self", ".", "video_labels", "=", "None", "\n", "self", ".", "distil_features", "=", "None", "\n", "self", ".", "raw_captions", "=", "None", "\n", "self", ".", "features", "=", "None", "\n", "\n", "# Use a single caption per video when forming training minibatches (different", "\n", "# captions from the same video may still be used across different minibatches)", "\n", "self", ".", "captions_per_video", "=", "1", "\n", "\n", "# TODO(Samuel) - is a global fixed ordering still necessary?", "\n", "self", ".", "ordered_experts", "=", "list", "(", "raw_input_dims", ".", "keys", "(", ")", ")", "\n", "\n", "# Training and test lists are set by dataset-specific subclasses", "\n", "self", ".", "partition_lists", "=", "{", "}", "\n", "self", ".", "configure_train_test_splits", "(", "split_name", "=", "split_name", ")", "\n", "\n", "# All retrieval-based tasks use a single dataloader (and handle the retrieval", "\n", "# data separately), whereas for classification we use one dataloader for", "\n", "# training and one for validation.", "\n", "self", ".", "logger", ".", "info", "(", "\"The current task is {}\"", ".", "format", "(", "self", ".", "task", ")", ")", "\n", "self", ".", "sample_list", "=", "self", ".", "partition_lists", "[", "\"train\"", "]", "\n", "\n", "self", ".", "num_samples", "=", "len", "(", "self", ".", "sample_list", ")", "\n", "num_val", "=", "len", "(", "self", ".", "partition_lists", "[", "\"val\"", "]", ")", "\n", "\n", "self", ".", "ce_shared_dim", "=", "ce_shared_dim", "\n", "if", "caption_masks", "is", "not", "None", ":", "\n", "            ", "self", ".", "caption_masks", "=", "pickle", ".", "load", "(", "open", "(", "self", ".", "root_feat", "/", "Path", "(", "caption_masks", ")", ",", "\"rb\"", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "caption_masks", "=", "None", "\n", "\n", "", "if", "self", ".", "task", "==", "\"classification\"", ":", "\n", "            ", "self", ".", "sample_list", "=", "self", ".", "partition_lists", "[", "self", ".", "cls_partition", "]", "\n", "self", ".", "num_samples", "=", "len", "(", "self", ".", "sample_list", ")", "\n", "self", ".", "logger", ".", "info", "(", "\"The current cls_partition is {}\"", ".", "format", "(", "self", ".", "cls_partition", ")", ")", "\n", "\n", "# The number of classes and class type (i.e. single or multi-label) must be", "\n", "# overriden in the subclass", "\n", "self", ".", "num_classes", "=", "None", "\n", "self", ".", "class_type", "=", "None", "\n", "\n", "", "self", ".", "raw_input_dims", "=", "raw_input_dims", "\n", "\n", "# we store default paths to enable visualisations (this can be overloaded by", "\n", "# dataset-specific classes)", "\n", "self", ".", "video_path_retrieval", "=", "[", "f\"videos/{x}.mp4\"", "for", "x", "\n", "in", "self", ".", "partition_lists", "[", "\"val\"", "]", "]", "\n", "\n", "# NOTE: We use nans rather than zeros to indicate missing faces, unless we wish", "\n", "# to test single modality strength, which requires passing zeroed features for", "\n", "# missing videos", "\n", "if", "use_zeros_for_missing", ":", "\n", "            ", "self", ".", "MISSING_VAL", "=", "0", "\n", "", "else", ":", "\n", "            ", "self", ".", "MISSING_VAL", "=", "np", ".", "nan", "\n", "\n", "# load the dataset-specific features into memory", "\n", "", "self", ".", "load_features", "(", ")", "\n", "\n", "if", "text_agg", "==", "\"avg\"", ":", "\n", "            ", "self", ".", "logger", ".", "info", "(", "\"averaging the text features...\"", ")", "\n", "for", "key", ",", "val", "in", "self", ".", "text_features", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "text_features", "[", "key", "]", "=", "[", "np", ".", "mean", "(", "x", ",", "0", ",", "keepdims", "=", "1", ")", "for", "x", "in", "val", "]", "\n", "", "self", ".", "logger", ".", "info", "(", "\"finished averaging the text features\"", ")", "\n", "\n", "", "self", ".", "trn_config", "=", "{", "}", "\n", "self", ".", "raw_config", "=", "{", "}", "\n", "self", ".", "tensor_storage", "=", "expert_tensor_storage", "(", "self", ".", "experts", ",", "self", ".", "feat_aggregation", ")", "\n", "for", "static_expert", "in", "self", ".", "tensor_storage", "[", "\"fixed\"", "]", ":", "\n", "            ", "if", "static_expert", "in", "self", ".", "feat_aggregation", ":", "\n", "                ", "if", "\"trn_seg\"", "in", "self", ".", "feat_aggregation", "[", "static_expert", "]", ".", "keys", "(", ")", ":", "\n", "                    ", "self", ".", "trn_config", "[", "static_expert", "]", "=", "self", ".", "feat_aggregation", "[", "static_expert", "]", "[", "\"trn_seg\"", "]", "\n", "", "if", "\"raw\"", "in", "self", ".", "feat_aggregation", "[", "static_expert", "]", "[", "\"temporal\"", "]", ":", "\n", "                    ", "self", ".", "raw_config", "[", "static_expert", "]", "=", "1", "\n", "\n", "", "", "", "if", "self", ".", "task", "==", "\"classification\"", ":", "\n", "# for classification we don't need to preload additional features", "\n", "            ", "return", "\n", "\n", "", "retrieval", "=", "{", "\n", "expert", ":", "np", ".", "zeros", "(", "(", "num_val", ",", "self", ".", "max_tokens", "[", "expert", "]", ",", "raw_input_dims", "[", "expert", "]", ")", ")", "\n", "for", "expert", "in", "self", ".", "tensor_storage", "[", "\"variable\"", "]", "\n", "}", "\n", "\n", "retrieval", ".", "update", "(", "{", "expert", ":", "np", ".", "zeros", "(", "(", "num_val", ",", "raw_input_dims", "[", "expert", "]", ")", ")", "\n", "for", "expert", "in", "self", ".", "tensor_storage", "[", "\"fixed\"", "]", "}", ")", "\n", "self", ".", "retrieval", "=", "retrieval", "\n", "self", ".", "test_ind", "=", "{", "expert", ":", "th", ".", "ones", "(", "num_val", ")", "for", "expert", "in", "self", ".", "experts", "}", "\n", "self", ".", "raw_captions_retrieval", "=", "[", "None", "]", "*", "num_val", "\n", "\n", "if", "self", ".", "task", "==", "\"retrieval-as-classification\"", ":", "\n", "# Treat each category label as a query", "\n", "            ", "num_labels", "=", "len", "(", "self", ".", "label_features", ")", "\n", "self", ".", "text_retrieval", "=", "np", ".", "zeros", "(", "(", "num_labels", ",", "1", ",", "1", ",", "self", ".", "text_dim", ")", ")", "\n", "self", ".", "query_masks", "=", "np", ".", "zeros", "(", "(", "num_labels", ",", "num_val", ")", ")", "\n", "for", "ii", ",", "video_name", "in", "enumerate", "(", "self", ".", "partition_lists", "[", "\"val\"", "]", ")", ":", "\n", "                ", "labels", "=", "self", ".", "video_labels", "[", "video_name", "]", "\n", "self", ".", "query_masks", "[", "np", ".", "array", "(", "labels", ")", ",", "ii", "]", "=", "1", "\n", "\n", "# Perform a single loop over the categories and encode the average label", "\n", "# as queries", "\n", "", "for", "ii", ",", "embedding", "in", "self", ".", "label_features", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "text_retrieval", "[", "ii", ",", ":", ",", ":", ",", ":", "]", "=", "np", ".", "mean", "(", "embedding", ",", "axis", "=", "0", ",", "keepdims", "=", "1", ")", "\n", "\n", "", "", "elif", "self", ".", "task", "==", "\"retrieval\"", ":", "\n", "# avoid evaluation on missing queries", "\n", "            ", "self", ".", "query_masks", "=", "np", ".", "zeros", "(", "(", "num_val", ",", "num_test_captions", ")", ")", "\n", "self", ".", "text_token_mask", "=", "np", ".", "zeros", "(", "(", "num_val", ",", "num_test_captions", ")", ")", "\n", "self", ".", "text_retrieval", "=", "np", ".", "zeros", "(", "(", "num_val", ",", "self", ".", "num_test_captions", ",", "\n", "self", ".", "max_tokens", "[", "\"text\"", "]", ",", "self", ".", "text_dim", ")", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Unrecognised task: {self.task}\"", ")", "\n", "\n", "", "for", "ii", ",", "video_name", "in", "enumerate", "(", "self", ".", "partition_lists", "[", "\"val\"", "]", ")", ":", "\n", "\n", "            ", "self", ".", "raw_captions_retrieval", "[", "ii", "]", "=", "self", ".", "raw_captions", "[", "video_name", "]", "\n", "for", "expert", "in", "self", ".", "tensor_storage", "[", "\"fixed\"", "]", ".", "intersection", "(", "self", ".", "experts", ")", ":", "\n", "                ", "feats", "=", "self", ".", "features", "[", "expert", "]", "[", "video_name", "]", "\n", "drop", "=", "self", ".", "has_missing_values", "(", "feats", ")", "\n", "\n", "self", ".", "test_ind", "[", "expert", "]", "[", "ii", "]", "=", "not", "drop", "\n", "self", ".", "retrieval", "[", "expert", "]", "[", "ii", "]", "=", "feats", "\n", "if", "drop", ":", "\n", "                    ", "self", ".", "retrieval", "[", "expert", "]", "[", "ii", "]", "[", ":", "]", "=", "self", ".", "MISSING_VAL", "\n", "", "if", "self", ".", "feat_aggregation", "[", "expert", "]", ".", "get", "(", "\"binarise\"", ",", "False", ")", ":", "\n", "                    ", "keep", "=", "np", ".", "logical_not", "(", "np", ".", "isnan", "(", "self", ".", "retrieval", "[", "expert", "]", "[", ":", ",", "0", ",", "0", "]", ")", ")", "\n", "marker", "=", "np", ".", "ones_like", "(", "self", ".", "retrieval", "[", "expert", "]", "[", "keep", "]", ")", "\n", "self", ".", "retrieval", "[", "expert", "]", "[", "keep", "]", "=", "marker", "\n", "\n", "", "", "for", "expert", "in", "self", ".", "tensor_storage", "[", "\"variable\"", "]", ".", "intersection", "(", "self", ".", "experts", ")", ":", "\n", "                ", "feats", "=", "self", ".", "features", "[", "expert", "]", "[", "video_name", "]", "\n", "drop", "=", "self", ".", "has_missing_values", "(", "feats", ")", "\n", "self", ".", "test_ind", "[", "expert", "]", "[", "ii", "]", "=", "not", "drop", "\n", "if", "drop", ":", "\n", "                    ", "self", ".", "retrieval", "[", "expert", "]", "[", "ii", "]", "[", ":", "]", "=", "self", ".", "MISSING_VAL", "\n", "", "if", "self", ".", "feat_aggregation", "[", "expert", "]", ".", "get", "(", "\"binarise\"", ",", "False", ")", ":", "\n", "                    ", "keep", "=", "np", ".", "logical_not", "(", "np", ".", "isnan", "(", "self", ".", "retrieval", "[", "expert", "]", "[", ":", ",", "0", ",", "0", "]", ")", ")", "\n", "marker", "=", "np", ".", "ones_like", "(", "self", ".", "retrieval", "[", "expert", "]", "[", "keep", "]", ")", "\n", "self", ".", "retrieval", "[", "expert", "]", "[", "keep", "]", "=", "marker", "\n", "", "if", "self", ".", "test_ind", "[", "expert", "]", "[", "ii", "]", ":", "\n", "                    ", "keep", "=", "min", "(", "self", ".", "max_tokens", "[", "expert", "]", ",", "len", "(", "feats", ")", ")", "\n", "self", ".", "retrieval", "[", "expert", "]", "[", "ii", ",", ":", "keep", ",", ":", "]", "=", "feats", "[", ":", "keep", "]", "\n", "\n", "", "", "if", "self", ".", "task", "==", "\"retrieval\"", ":", "\n", "                ", "candidates_sentences", "=", "self", ".", "text_features", "[", "video_name", "]", "\n", "\n", "if", "self", ".", "restrict_test_captions", "is", "not", "None", ":", "\n", "                    ", "keep_sent_idx", "=", "self", ".", "restrict_test_captions", "[", "video_name", "]", "\n", "candidates_sentences", "=", "[", "candidates_sentences", "[", "keep_sent_idx", "]", "]", "\n", "\n", "", "self", ".", "query_masks", "[", "ii", ",", ":", "len", "(", "candidates_sentences", ")", "]", "=", "1", "\n", "\n", "if", "self", ".", "fuse_captions", ":", "\n", "# fuse into a single caption", "\n", "                    ", "text_feats", "=", "np", ".", "vstack", "(", "candidates_sentences", ")", "\n", "keep", "=", "min", "(", "len", "(", "text_feats", ")", ",", "self", ".", "max_tokens", "[", "\"text\"", "]", ")", "\n", "self", ".", "text_retrieval", "[", "ii", ",", "0", ",", ":", "keep", ",", ":", "]", "=", "text_feats", "[", ":", "keep", ",", ":", "]", "\n", "self", ".", "text_token_mask", "[", "ii", ",", "0", "]", "=", "keep", "\n", "self", ".", "query_masks", "[", "ii", ",", ":", "]", "=", "1", "\n", "", "else", ":", "\n", "                    ", "for", "test_caption_idx", "in", "range", "(", "self", ".", "num_test_captions", ")", ":", "\n", "                        ", "if", "len", "(", "candidates_sentences", ")", "<=", "test_caption_idx", ":", "\n", "                            ", "break", "\n", "", "keep", "=", "min", "(", "len", "(", "candidates_sentences", "[", "test_caption_idx", "]", ")", ",", "\n", "self", ".", "max_tokens", "[", "\"text\"", "]", ")", "\n", "self", ".", "text_token_mask", "[", "ii", ",", "test_caption_idx", "]", "=", "keep", "\n", "if", "ii", "%", "500", "==", "0", "and", "test_caption_idx", "==", "0", ":", "\n", "                            ", "msg", "=", "(", "\n", "f\"{ii}/{len(self.partition_lists['val'])} will evaluate \"", "\n", "f\"sentence {test_caption_idx} out of \"", "\n", "f\"{len(candidates_sentences)} (has {keep} words) \"", "\n", "f\"{video_name}\"", "\n", ")", "\n", "self", ".", "logger", ".", "info", "(", "msg", ")", "\n", "", "text_feats", "=", "candidates_sentences", "[", "test_caption_idx", "]", "[", ":", "keep", "]", "\n", "if", "text_feats", ".", "size", "==", "0", ":", "\n", "                            ", "text_feats", "=", "0", "\n", "raise", "ValueError", "(", "\"empty text features!\"", ")", "\n", "", "self", ".", "text_retrieval", "[", "ii", ",", "test_caption_idx", ",", ":", "keep", ",", ":", "]", "=", "text_feats", "\n", "\n", "", "", "", "", "self", ".", "sanity_checks", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.configure_train_test_splits": [[288, 315], ["type().dataset_paths", "print", "time.time", "[].items", "print", "type", "pathlib.Path", "pathlib.Path", "isinstance", "open", "f.read().splitlines", "time.time", "f.read", "x.strip().split", "x.strip"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.dataset_paths", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["", "def", "configure_train_test_splits", "(", "self", ",", "split_name", ")", ":", "\n", "        ", "\"\"\"Partition the datset into train/val/test splits.\n\n        Args:\n            split_name (str): the name of the split\n        \"\"\"", "\n", "self", ".", "paths", "=", "type", "(", "self", ")", ".", "dataset_paths", "(", "self", ".", "training_file", ")", "\n", "print", "(", "\"loading training/val splits....\"", ")", "\n", "tic", "=", "time", ".", "time", "(", ")", "\n", "for", "subset", ",", "path", "in", "self", ".", "paths", "[", "\"subset_list_paths\"", "]", "[", "split_name", "]", ".", "items", "(", ")", ":", "\n", "            ", "if", "self", ".", "challenge_mode", "and", "split_name", "==", "\"public_server_test\"", "and", "subset", "==", "\"val\"", ":", "\n", "                ", "root_feat", "=", "Path", "(", "self", ".", "challenge_test_root_feat_folder", ")", "\n", "", "else", ":", "\n", "                ", "root_feat", "=", "Path", "(", "self", ".", "root_feat", ")", "\n", "", "subset_list_path", "=", "root_feat", "/", "path", "\n", "if", "subset", "==", "\"train\"", "and", "self", ".", "eval_only", ":", "\n", "                ", "rows", "=", "[", "]", "\n", "", "else", ":", "\n", "                ", "with", "open", "(", "subset_list_path", ")", "as", "f", ":", "\n", "                    ", "rows", "=", "f", ".", "read", "(", ")", ".", "splitlines", "(", ")", "\n", "", "if", "isinstance", "(", "self", ",", "data_loader", ".", "DiDeMo_dataset", ".", "DiDeMo", ")", ":", "\n", "# For DiDeMo, we need to remove additional video suffixes", "\n", "                    ", "rows", "=", "[", "x", ".", "strip", "(", ")", ".", "split", "(", "\".\"", ")", "[", "0", "]", "for", "x", "in", "rows", "]", "\n", "", "", "self", ".", "partition_lists", "[", "subset", "]", "=", "rows", "\n", "", "print", "(", "\"done in {:.3f}s\"", ".", "format", "(", "time", ".", "time", "(", ")", "-", "tic", ")", ")", "\n", "self", ".", "split_name", "=", "split_name", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.collate_data": [[316, 432], ["len", "tensors.update", "enumerate", "collections.OrderedDict", "numpy.zeros", "numpy.zeros", "numpy.zeros", "utils.util.ensure_tensor", "base_dataset.BaseDataset.feat_aggregation[].get", "torch.from_numpy().float", "torch.from_numpy", "base_dataset.BaseDataset.trn_config.keys", "numpy.zeros", "numpy.zeros", "[].keys", "numpy.zeros", "numpy.zeros", "range", "ind.items", "collections.OrderedDict", "collections.OrderedDict", "numpy.logical_not", "torch.ones_like", "numpy.zeros", "min", "min", "vid_name.append", "torch.from_numpy().float", "torch.isnan", "torch.from_numpy", "torch.from_numpy().float", "isinstance", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "len", "len", "torch.from_numpy", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy", "isinstance", "torch.from_numpy", "torch.from_numpy", "list", "base_dataset.BaseDataset.distil_features[].keys"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.ensure_tensor", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys"], ["", "def", "collate_data", "(", "self", ",", "data", ")", ":", "\n", "        ", "batch_size", "=", "len", "(", "data", ")", "\n", "tensors", "=", "{", "}", "\n", "for", "expert", "in", "self", ".", "tensor_storage", "[", "\"fixed\"", "]", ":", "\n", "            ", "if", "expert", "in", "self", ".", "trn_config", ".", "keys", "(", ")", ":", "\n", "                ", "tensors", "[", "expert", "]", "=", "np", ".", "zeros", "(", "(", "batch_size", ",", "self", ".", "trn_config", "[", "expert", "]", ",", "\n", "self", ".", "raw_input_dims", "[", "expert", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "tensors", "[", "expert", "]", "=", "np", ".", "zeros", "(", "(", "batch_size", ",", "self", ".", "raw_input_dims", "[", "expert", "]", ")", ")", "\n", "", "", "if", "self", ".", "distil_features", "is", "not", "None", ":", "\n", "            ", "distil", "=", "{", "}", "\n", "distil_text", "=", "{", "}", "\n", "\n", "for", "t", "in", "self", ".", "distil_features", ":", "\n", "                ", "distil", "[", "t", "]", "=", "{", "}", "\n", "distil_text", "[", "t", "]", "=", "{", "}", "\n", "\n", "check_moee", "=", "False", "\n", "if", "self", ".", "distil_params", "is", "not", "None", "and", "'moee'", "in", "self", ".", "distil_params", ":", "\n", "                    ", "if", "isinstance", "(", "self", ".", "distil_params", "[", "'moee'", "]", ",", "list", ")", "and", "self", ".", "distil_params", "[", "'moee'", "]", "[", "t", "]", "==", "1", ":", "\n", "                        ", "check_moee", "=", "True", "\n", "", "elif", "not", "isinstance", "(", "self", ".", "distil_params", "[", "'moee'", "]", ",", "list", ")", "and", "self", ".", "distil_params", "[", "'moee'", "]", "==", "True", ":", "\n", "                        ", "check_moee", "=", "True", "\n", "", "", "for", "mod", "in", "self", ".", "distil_features", "[", "t", "]", "[", "list", "(", "self", ".", "distil_features", "[", "t", "]", ".", "keys", "(", ")", ")", "[", "0", "]", "]", "[", "'vid_embds'", "]", ".", "keys", "(", ")", ":", "\n", "                    ", "if", "check_moee", ":", "\n", "                        ", "size", "=", "self", ".", "raw_input_dims", "[", "mod", "]", "\n", "distil", "[", "t", "]", "[", "mod", "]", "=", "np", ".", "zeros", "(", "(", "batch_size", ",", "size", ")", ")", "\n", "distil_text", "[", "t", "]", "[", "mod", "]", "=", "np", ".", "zeros", "(", "(", "batch_size", ",", "1", ",", "size", ")", ")", "\n", "", "else", ":", "\n", "                        ", "distil", "[", "t", "]", "[", "mod", "]", "=", "np", ".", "zeros", "(", "(", "batch_size", ",", "self", ".", "ce_shared_dim", ")", ")", "\n", "distil_text", "[", "t", "]", "[", "mod", "]", "=", "np", ".", "zeros", "(", "(", "batch_size", ",", "1", ",", "self", ".", "ce_shared_dim", ")", ")", "\n", "\n", "# Track which indices of each modality are available in the present batch", "\n", "", "", "", "", "ind", "=", "{", "expert", ":", "np", ".", "zeros", "(", "batch_size", ")", "for", "expert", "in", "self", ".", "experts", "}", "\n", "tensors", ".", "update", "(", "{", "expert", ":", "np", ".", "zeros", "(", "\n", "(", "batch_size", ",", "self", ".", "max_tokens", "[", "expert", "]", ",", "self", ".", "raw_input_dims", "[", "expert", "]", ")", "\n", ")", "for", "expert", "in", "self", ".", "tensor_storage", "[", "\"variable\"", "]", "}", ")", "\n", "\n", "if", "\"retrieval\"", "in", "self", ".", "task", ":", "\n", "            ", "text_tensor", "=", "np", ".", "zeros", "(", "(", "batch_size", ",", "self", ".", "captions_per_video", ",", "\n", "self", ".", "max_tokens", "[", "\"text\"", "]", ",", "self", ".", "text_dim", ")", ")", "\n", "text_token_mask", "=", "np", ".", "zeros", "(", "(", "batch_size", ",", "self", ".", "captions_per_video", ")", ")", "\n", "", "elif", "\"classification\"", "in", "self", ".", "task", "and", "self", ".", "class_type", "==", "\"single_label\"", ":", "\n", "            ", "label_tensor", "=", "np", ".", "zeros", "(", "batch_size", ")", "\n", "vid_name", "=", "[", "]", "\n", "", "elif", "\"classification\"", "in", "self", ".", "task", "and", "self", ".", "class_type", "==", "\"multi_label\"", ":", "\n", "            ", "label_tensor", "=", "np", ".", "zeros", "(", "(", "batch_size", ",", "self", ".", "num_classes", ")", ")", "\n", "vid_name", "=", "[", "]", "\n", "\n", "", "for", "ii", ",", "_", "in", "enumerate", "(", "data", ")", ":", "\n", "            ", "datum", "=", "data", "[", "ii", "]", "\n", "for", "expert", "in", "self", ".", "experts", ":", "\n", "                ", "ind", "[", "expert", "]", "[", "ii", "]", "=", "datum", "[", "f\"{expert}_ind\"", "]", "\n", "", "for", "expert", "in", "self", ".", "tensor_storage", "[", "\"fixed\"", "]", ":", "\n", "                ", "tensors", "[", "expert", "]", "[", "ii", "]", "=", "datum", "[", "expert", "]", "\n", "", "for", "expert", "in", "self", ".", "tensor_storage", "[", "\"variable\"", "]", ":", "\n", "                ", "if", "ind", "[", "expert", "]", "[", "ii", "]", ":", "\n", "                    ", "keep", "=", "min", "(", "len", "(", "datum", "[", "expert", "]", ")", ",", "self", ".", "max_tokens", "[", "expert", "]", ")", "\n", "if", "keep", ":", "\n", "                        ", "tensors", "[", "expert", "]", "[", "ii", ",", ":", "keep", ",", ":", "]", "=", "datum", "[", "expert", "]", "[", ":", "keep", "]", "\n", "", "", "else", ":", "\n", "                    ", "tensors", "[", "expert", "]", "[", "ii", ",", ":", ",", ":", "]", "=", "self", ".", "MISSING_VAL", "\n", "\n", "", "", "if", "self", ".", "distil_features", "is", "not", "None", ":", "\n", "                ", "for", "t", "in", "datum", "[", "'distil_mods'", "]", ":", "\n", "                    ", "for", "mod", "in", "datum", "[", "'distil_mods'", "]", "[", "t", "]", ":", "\n", "                        ", "distil", "[", "t", "]", "[", "mod", "]", "[", "ii", "]", "=", "datum", "[", "'distil_mods'", "]", "[", "t", "]", "[", "mod", "]", "\n", "", "", "for", "t", "in", "datum", "[", "'distil_mods'", "]", ":", "\n", "                    ", "for", "mod", "in", "datum", "[", "'distil_texts'", "]", "[", "t", "]", ":", "\n", "                        ", "distil_text", "[", "t", "]", "[", "mod", "]", "[", "ii", "]", "=", "datum", "[", "'distil_texts'", "]", "[", "t", "]", "[", "mod", "]", "\n", "\n", "", "", "", "if", "\"retrieval\"", "in", "self", ".", "task", ":", "\n", "                ", "text", "=", "datum", "[", "\"text\"", "]", "\n", "for", "jj", "in", "range", "(", "self", ".", "captions_per_video", ")", ":", "\n", "                    ", "keep", "=", "min", "(", "len", "(", "text", "[", "jj", "]", ")", ",", "self", ".", "max_tokens", "[", "\"text\"", "]", ")", "\n", "text_tensor", "[", "ii", ",", "jj", ",", ":", "keep", ",", ":", "]", "=", "text", "[", "jj", "]", "[", ":", "keep", "]", "\n", "text_token_mask", "[", "ii", ",", "jj", "]", "=", "keep", "\n", "", "", "elif", "self", ".", "task", "==", "\"classification\"", ":", "\n", "                ", "if", "self", ".", "cls_partition", "!=", "'test'", ":", "\n", "                    ", "label_tensor", "[", "ii", "]", "=", "datum", "[", "\"labels\"", "]", "\n", "", "vid_name", ".", "append", "(", "datum", "[", "\"vid\"", "]", ")", "\n", "\n", "", "", "ind", "=", "{", "key", ":", "ensure_tensor", "(", "val", ")", "for", "key", ",", "val", "in", "ind", ".", "items", "(", ")", "}", "\n", "experts", "=", "OrderedDict", "(", "\n", "(", "expert", ",", "th", ".", "from_numpy", "(", "tensors", "[", "expert", "]", ")", ".", "float", "(", ")", ")", "\n", "for", "expert", "in", "self", ".", "ordered_experts", ")", "\n", "if", "self", ".", "distil_features", "is", "not", "None", ":", "\n", "            ", "for", "t", "in", "distil", ":", "\n", "                ", "distil", "[", "t", "]", "=", "OrderedDict", "(", "\n", "(", "expert", ",", "th", ".", "from_numpy", "(", "distil", "[", "t", "]", "[", "expert", "]", ")", ".", "float", "(", ")", ")", "\n", "for", "expert", "in", "distil", "[", "t", "]", ")", "\n", "distil_text", "[", "t", "]", "=", "OrderedDict", "(", "\n", "(", "expert", ",", "th", ".", "from_numpy", "(", "distil_text", "[", "t", "]", "[", "expert", "]", ")", ".", "float", "(", ")", ")", "\n", "for", "expert", "in", "distil_text", "[", "t", "]", ")", "\n", "\n", "", "", "for", "expert", "in", "self", ".", "experts", ":", "\n", "            ", "if", "self", ".", "feat_aggregation", "[", "expert", "]", ".", "get", "(", "\"binarise\"", ",", "False", ")", ":", "\n", "                ", "replace", "=", "np", ".", "logical_not", "(", "th", ".", "isnan", "(", "experts", "[", "expert", "]", "[", ":", ",", "0", ",", "0", "]", ")", ")", "\n", "experts", "[", "expert", "]", "[", "replace", "]", "=", "th", ".", "ones_like", "(", "experts", "[", "expert", "]", "[", "replace", "]", ")", "\n", "\n", "", "", "minibatch", "=", "{", "\"experts\"", ":", "experts", ",", "\"ind\"", ":", "ind", "}", "\n", "if", "self", ".", "distil_features", "is", "not", "None", ":", "\n", "            ", "minibatch", "[", "\"distil_video\"", "]", "=", "distil", "\n", "minibatch", "[", "\"distil_text\"", "]", "=", "distil_text", "\n", "", "if", "\"retrieval\"", "in", "self", ".", "task", ":", "\n", "            ", "minibatch", "[", "\"text\"", "]", "=", "th", ".", "from_numpy", "(", "text_tensor", ")", ".", "float", "(", ")", "\n", "minibatch", "[", "\"text_token_mask\"", "]", "=", "th", ".", "from_numpy", "(", "text_token_mask", ")", "\n", "\n", "", "elif", "self", ".", "task", "==", "\"classification\"", ":", "\n", "            ", "if", "self", ".", "cls_partition", "!=", "'test'", ":", "\n", "                ", "minibatch", "[", "\"labels\"", "]", "=", "th", ".", "from_numpy", "(", "label_tensor", ")", ".", "float", "(", ")", "\n", "", "if", "self", ".", "cls_partition", "!=", "\"train\"", ":", "\n", "# we only pass the video names for visualisation and making predictions", "\n", "# on the val/test set", "\n", "                ", "minibatch", "[", "\"vid_name\"", "]", "=", "vid_name", "\n", "", "", "return", "minibatch", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.__len__": [[433, 435], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.__getitem__": [[436, 557], ["sample.update", "sample.update", "sample.update", "ValueError", "base_dataset.BaseDataset.trn_config.keys", "isinstance", "numpy.random.random", "ind.items", "numpy.multiply", "numpy.random.randint", "numpy.zeros", "enumerate", "base_dataset.BaseDataset.has_missing_values", "numpy.vstack", "numpy.random.choice", "numpy.random.random", "NotImplementedError", "len", "ValueError", "base_dataset.BaseDataset.raw_config.keys", "numpy.mean", "ipdb.set_trace", "list", "numpy.random.choice", "numpy.random.choice", "numpy.array", "len", "numpy.zeros", "range", "numpy.sum", "len", "len", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.has_missing_values", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.keys"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "if", "idx", "<", "self", ".", "num_samples", ":", "\n", "            ", "vid", "=", "self", ".", "sample_list", "[", "idx", "]", "\n", "if", "self", ".", "caption_masks", "is", "not", "None", ":", "\n", "                ", "caption_masks", "=", "self", ".", "caption_masks", "[", "vid", "]", "\n", "", "else", ":", "\n", "                ", "caption_masks", "=", "None", "\n", "", "if", "self", ".", "distil_features", "is", "not", "None", ":", "\n", "                ", "distil_mod_feats", "=", "{", "}", "\n", "distil_text_feats", "=", "{", "}", "\n", "\n", "for", "t", "in", "self", ".", "distil_features", ":", "\n", "                    ", "distil_mod_feats", "[", "t", "]", "=", "{", "}", "\n", "distil_text_feats", "[", "t", "]", "=", "{", "}", "\n", "for", "k", "in", "self", ".", "distil_features", "[", "t", "]", "[", "vid", "]", ":", "\n", "                        ", "for", "mod", "in", "self", ".", "distil_features", "[", "t", "]", "[", "vid", "]", "[", "k", "]", ":", "\n", "                            ", "if", "k", "==", "'vid_embds'", ":", "\n", "                                ", "distil_mod_feats", "[", "t", "]", "[", "mod", "]", "=", "self", ".", "distil_features", "[", "t", "]", "[", "vid", "]", "[", "k", "]", "[", "mod", "]", "\n", "", "elif", "k", "==", "'text_embds'", ":", "\n", "                                ", "distil_text_feats", "[", "t", "]", "[", "mod", "]", "=", "self", ".", "distil_features", "[", "t", "]", "[", "vid", "]", "[", "k", "]", "[", "mod", "]", "\n", "\n", "# try:", "\n", "", "", "", "", "", "features", "=", "{", "}", "\n", "for", "expert", "in", "self", ".", "experts", ":", "\n", "                ", "if", "expert", "not", "in", "self", ".", "trn_config", ".", "keys", "(", ")", ":", "\n", "                    ", "try", ":", "\n", "                        ", "if", "expert", "in", "self", ".", "raw_config", ".", "keys", "(", ")", ":", "\n", "                            ", "features", "[", "expert", "]", "=", "np", ".", "mean", "(", "self", ".", "features", "[", "expert", "]", "[", "vid", "]", ",", "axis", "=", "0", ")", "\n", "", "else", ":", "\n", "                            ", "features", "[", "expert", "]", "=", "self", ".", "features", "[", "expert", "]", "[", "vid", "]", "\n", "", "", "except", ":", "\n", "                        ", "import", "ipdb", ";", "ipdb", ".", "set_trace", "(", ")", "\n", "", "", "else", ":", "\n", "# ------------------------------------", "\n", "# Yang's implementation for TRN inputs", "\n", "# ------------------------------------", "\n", "                    ", "raw_frame_feats", "=", "self", ".", "features", "[", "expert", "]", "[", "vid", "]", "\n", "new_length", "=", "1", "\n", "num_frames", "=", "raw_frame_feats", ".", "shape", "[", "0", "]", "\n", "avg_duration", "=", "(", "(", "num_frames", "-", "new_length", "+", "1", ")", "\n", "//", "self", ".", "trn_config", "[", "expert", "]", ")", "\n", "assert", "avg_duration", ">", "0", ",", "\"average duration must be positive\"", "\n", "if", "avg_duration", ">", "0", ":", "\n", "# maybe we could change to use average for each tiny segment", "\n", "# seems like use everything per iter", "\n", "                        ", "offsets", "=", "np", ".", "multiply", "(", "list", "(", "range", "(", "self", ".", "trn_config", "[", "expert", "]", ")", ")", ",", "\n", "avg_duration", ")", "\n", "offsets", "+=", "randint", "(", "avg_duration", ",", "size", "=", "self", ".", "trn_config", "[", "expert", "]", ")", "\n", "new_frame_feats", "=", "np", ".", "zeros", "(", "(", "self", ".", "trn_config", "[", "expert", "]", ",", "\n", "raw_frame_feats", ".", "shape", "[", "1", "]", ")", ")", "\n", "for", "idx", ",", "xx", "in", "enumerate", "(", "offsets", ")", ":", "\n", "# yang! you might want to change back", "\n", "                            ", "new_frame_feats", "[", "idx", ",", ":", "]", "=", "raw_frame_feats", "[", "xx", ",", ":", "]", "\n", "", "msg", "=", "\"returning a wrong feature != segment num\"", "\n", "assert", "new_frame_feats", ".", "shape", "[", "0", "]", "==", "self", ".", "trn_config", "[", "expert", "]", ",", "msg", "\n", "features", "[", "expert", "]", "=", "new_frame_feats", "\n", "\n", "", "", "", "ind", "=", "{", "}", "\n", "for", "expert", "in", "self", ".", "ordered_experts", ":", "\n", "                ", "if", "expert", "in", "self", ".", "tensor_storage", "[", "\"flaky\"", "]", ":", "\n", "                    ", "ind", "[", "expert", "]", "=", "not", "self", ".", "has_missing_values", "(", "features", "[", "expert", "]", ")", "\n", "", "else", ":", "\n", "                    ", "ind", "[", "expert", "]", "=", "1", "\n", "\n", "", "", "if", "self", ".", "task", "in", "{", "\"retrieval\"", ",", "\"retrieval-as-classification\"", "}", ":", "\n", "# Handle some inconsistencies between how the text features are stored", "\n", "                ", "text", "=", "self", ".", "text_features", "[", "vid", "]", "\n", "if", "self", ".", "fuse_captions", ":", "\n", "                    ", "text", "=", "[", "np", ".", "vstack", "(", "text", ")", "]", "\n", "pick", "=", "None", "\n", "", "elif", "isinstance", "(", "text", ",", "list", ")", ":", "\n", "                    ", "if", "caption_masks", "is", "not", "None", ":", "\n", "                        ", "probability", "=", "caption_masks", "/", "np", ".", "sum", "(", "caption_masks", ")", "\n", "pick", "=", "np", ".", "random", ".", "choice", "(", "len", "(", "text", ")", ",", "size", "=", "self", ".", "captions_per_video", ",", "p", "=", "probability", ")", "\n", "assert", "caption_masks", "[", "pick", "]", "==", "1", "\n", "", "else", ":", "\n", "                        ", "pick", "=", "np", ".", "random", ".", "choice", "(", "len", "(", "text", ")", ",", "size", "=", "self", ".", "captions_per_video", ")", "\n", "\n", "", "text", "=", "np", ".", "array", "(", "text", ")", "[", "pick", "]", "\n", "if", "self", ".", "distil_features", "is", "not", "None", ":", "\n", "# Select appropriate text embd for teacher data", "\n", "                        ", "for", "t", "in", "distil_text_feats", ":", "\n", "                            ", "for", "mod", "in", "distil_text_feats", "[", "t", "]", ":", "\n", "                                ", "distil_text_feats", "[", "t", "]", "[", "mod", "]", "=", "np", ".", "array", "(", "distil_text_feats", "[", "t", "]", "[", "mod", "]", ")", "[", "pick", "]", "\n", "", "", "", "", "else", ":", "\n", "                    ", "pick", "=", "None", "\n", "text", "=", "np", ".", "random", ".", "choice", "(", "text", ",", "size", "=", "self", ".", "captions_per_video", ")", "\n", "\n", "", "if", "np", ".", "random", ".", "random", "(", ")", "<", "self", ".", "text_dropout", ":", "\n", "                    ", "if", "pick", "is", "not", "None", ":", "\n", "                        ", "mask", "=", "np", ".", "random", ".", "random", "(", "len", "(", "text", "[", "0", "]", ")", ")", "\n", "text", "=", "[", "text", "[", "0", "]", "[", "mask", ">", "0.5", "]", "]", "\n", "", "else", ":", "\n", "                        ", "raise", "NotImplementedError", "(", "\"TODO: Add dropouot for picked text\"", ")", "\n", "\n", "# Return both the missing indices as well as the tensors", "\n", "", "", "", "", "if", "self", ".", "task", "in", "{", "\"retrieval\"", ",", "\"retrieval-as-classification\"", "}", ":", "\n", "            ", "if", "self", ".", "distil_features", "is", "not", "None", ":", "\n", "                ", "sample", "=", "{", "\"text\"", ":", "text", ",", "\"distil_mods\"", ":", "distil_mod_feats", ",", "\"distil_texts\"", ":", "distil_text_feats", "}", "\n", "", "else", ":", "\n", "                ", "sample", "=", "{", "\"text\"", ":", "text", "}", "\n", "", "", "elif", "self", ".", "task", "==", "\"classification\"", ":", "\n", "            ", "if", "self", ".", "class_type", "==", "\"single_label\"", ":", "\n", "                ", "labels", "=", "self", ".", "video_labels", "[", "vid", "]", "\n", "assert", "len", "(", "labels", ")", "==", "1", ",", "\"expected single label\"", "\n", "labels", "=", "labels", "[", "0", "]", "\n", "", "elif", "self", ".", "class_type", "==", "\"multi_label\"", ":", "\n", "                ", "if", "self", ".", "cls_partition", "!=", "'test'", ":", "\n", "                    ", "labels", "=", "np", ".", "zeros", "(", "self", ".", "num_classes", ")", "\n", "labels", "[", "self", ".", "video_labels", "[", "vid", "]", "]", "=", "1", "\n", "", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "f\"unknown label class type: {self.class_type}\"", ")", "\n", "", "sample", "=", "{", "}", "\n", "if", "self", ".", "cls_partition", "!=", "'test'", ":", "\n", "                ", "sample", "=", "{", "\"labels\"", ":", "labels", "}", "\n", "", "sample", ".", "update", "(", "{", "\"vid\"", ":", "vid", "}", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "f\"unknown task: {self.task}\"", ")", "\n", "", "sample", ".", "update", "(", "{", "f\"{key}_ind\"", ":", "val", "for", "key", ",", "val", "in", "ind", ".", "items", "(", ")", "}", ")", "\n", "sample", ".", "update", "(", "features", ")", "\n", "return", "sample", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.get_retrieval_data": [[558, 574], ["collections.OrderedDict", "utils.util.ensure_tensor().float", "torch.from_numpy().float", "utils.util.ensure_tensor", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.util.ensure_tensor"], ["", "def", "get_retrieval_data", "(", "self", ")", ":", "\n", "        ", "experts", "=", "OrderedDict", "(", "\n", "(", "expert", ",", "th", ".", "from_numpy", "(", "self", ".", "retrieval", "[", "expert", "]", ")", ".", "float", "(", ")", ")", "\n", "for", "expert", "in", "self", ".", "ordered_experts", "\n", ")", "\n", "retrieval_data", "=", "{", "\n", "\"text\"", ":", "ensure_tensor", "(", "self", ".", "text_retrieval", ")", ".", "float", "(", ")", ",", "\n", "\"experts\"", ":", "experts", ",", "\n", "\"ind\"", ":", "self", ".", "test_ind", ",", "\n", "}", "\n", "meta", "=", "{", "\n", "\"query_masks\"", ":", "self", ".", "query_masks", ",", "\n", "\"raw_captions\"", ":", "self", ".", "raw_captions_retrieval", ",", "\n", "\"paths\"", ":", "self", ".", "video_path_retrieval", ",", "\n", "}", "\n", "return", "retrieval_data", ",", "meta", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.has_missing_values": [[575, 577], ["isinstance", "numpy.isnan"], "methods", ["None"], ["", "def", "has_missing_values", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "isinstance", "(", "x", ",", "float", ")", "and", "np", ".", "isnan", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.visual_feat_paths": [[578, 608], ["model_spec.split", "aggs[].split", "base_dataset.BaseDataset.logger.info", "aggs.get", "feat_paths.append", "feat_type.replace", "aggs.get", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get"], ["", "def", "visual_feat_paths", "(", "self", ",", "model_spec", ",", "tag", "=", "None", ")", ":", "\n", "        ", "\"\"\"Canonical path lookup for visual features\n        \"\"\"", "\n", "if", "model_spec", "not", "in", "self", ".", "ordered_experts", ":", "\n", "            ", "self", ".", "logger", ".", "info", "(", "f\"Skipping load for {model_spec} (feature not requested)\"", ")", "\n", "return", "f\"SKIPPED-{model_spec}\"", "\n", "\n", "", "feat_type", ",", "model_name", ",", "_", "=", "model_spec", ".", "split", "(", "\".\"", ")", "\n", "aggs", "=", "self", ".", "feat_aggregation", "[", "model_spec", "]", "\n", "base", "=", "f\"aggregated_{feat_type.replace('-', '_')}\"", "\n", "required", "=", "(", "\"fps\"", ",", "\"pixel_dim\"", ",", "\"stride\"", ")", "\n", "fps", ",", "pixel_dim", ",", "stride", "=", "[", "aggs", ".", "get", "(", "x", ",", "None", ")", "for", "x", "in", "required", "]", "\n", "if", "feat_type", "in", "{", "\"facecrops\"", ",", "\"faceboxes\"", "}", ":", "\n", "            ", "base", "=", "f\"{base}_{fps}fps_{pixel_dim}px_stride{stride}\"", "\n", "", "elif", "feat_type", "not", "in", "{", "\"ocr\"", ",", "\"speech\"", ",", "\"audio\"", "}", ":", "\n", "            ", "base", "=", "f\"{base}_{fps}fps_{pixel_dim}px_stride{stride}\"", "\n", "\n", "", "for", "option", "in", "\"offset\"", ",", "\"inner_stride\"", ",", "\"num_segments\"", ":", "\n", "            ", "if", "aggs", ".", "get", "(", "option", ",", "None", ")", "is", "not", "None", ":", "\n", "                ", "base", "+=", "f\"_{option}{aggs[option]}\"", "\n", "\n", "", "", "feat_paths", "=", "[", "]", "\n", "for", "agg", "in", "aggs", "[", "\"temporal\"", "]", ".", "split", "(", "\"-\"", ")", ":", "\n", "            ", "fname", "=", "f\"{model_name}-{agg}\"", "\n", "if", "aggs", "[", "\"type\"", "]", "==", "\"logits\"", ":", "\n", "                ", "fname", "=", "f\"{fname}-logits\"", "\n", "", "if", "tag", "is", "not", "None", ":", "\n", "                ", "fname", "+=", "f\"-{tag}\"", "\n", "", "feat_paths", ".", "append", "(", "Path", "(", "base", ")", "/", "f\"{fname}.pickle\"", ")", "\n", "", "return", "feat_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.log_assert": [[609, 629], ["base_dataset.BaseDataset.logger.debug", "AssertionError", "inspect.stack", "open().readlines", "[].strip", "open"], "methods", ["None"], ["", "def", "log_assert", "(", "self", ",", "bool_", ",", "msg", "=", "\"\"", ",", "verbose", "=", "True", ")", ":", "\n", "        ", "\"\"\"Use assertions that will be written to the logs. This is a recipe from:\n        http://code.activestate.com/recipes/577074-logging-asserts/\n        \"\"\"", "\n", "try", ":", "\n", "            ", "assert", "bool_", ",", "msg", "\n", "", "except", "AssertionError", ":", "\n", "# construct an exception message from the code of the calling frame", "\n", "            ", "last_stackframe", "=", "inspect", ".", "stack", "(", ")", "[", "-", "2", "]", "\n", "source_file", ",", "line_no", ",", "func", "=", "last_stackframe", "[", "1", ":", "4", "]", "\n", "source", "=", "f\"Traceback (most recent call last):\\n\"", "+", "f\" File {source_file}, line {line_no}, in {func}\\n\"", "\n", "if", "verbose", ":", "\n", "# include more lines than that where the statement was made", "\n", "                ", "source_code", "=", "open", "(", "source_file", ")", ".", "readlines", "(", ")", "\n", "source", "+=", "\"\"", ".", "join", "(", "source_code", "[", "line_no", "-", "3", ":", "line_no", "+", "1", "]", ")", "\n", "", "else", ":", "\n", "                ", "source", "+=", "last_stackframe", "[", "-", "2", "]", "[", "0", "]", ".", "strip", "(", ")", "\n", "", "self", ".", "logger", ".", "debug", "(", "f\"{msg}\\n{source}\"", ")", "\n", "raise", "AssertionError", "(", "f\"{msg}\\n{source}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.summary_stats": [[630, 657], ["base_dataset.BaseDataset.logger.info", "base_dataset.BaseDataset.partition_lists.items", "set", "print", "base_dataset.BaseDataset.has_missing_values", "print", "sizes.append", "len", "numpy.min", "numpy.max", "numpy.mean"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.has_missing_values"], ["", "", "def", "summary_stats", "(", "self", ")", ":", "\n", "        ", "\"\"\"Report basic statistics about feature availability and variable lengths\n        across the different subsets of the data.\n        \"\"\"", "\n", "self", ".", "logger", ".", "info", "(", "\"Computing feature stats...\"", ")", "\n", "queries", "=", "self", ".", "ordered_experts", "+", "[", "\"text\"", "]", "\n", "for", "subset", ",", "keep", "in", "self", ".", "partition_lists", ".", "items", "(", ")", ":", "\n", "            ", "keep", "=", "set", "(", "keep", ")", "\n", "print", "(", "f\"Summary for {subset}\"", ")", "\n", "for", "expert", "in", "queries", ":", "\n", "                ", "if", "expert", "in", "self", ".", "features", ":", "\n", "                    ", "feats", "=", "self", ".", "features", "[", "expert", "]", "\n", "", "else", ":", "\n", "                    ", "feats", "=", "self", ".", "text_features", "\n", "", "vals", "=", "[", "feats", "[", "key", "]", "for", "key", "in", "keep", "]", "\n", "missing", "=", "0", "\n", "sizes", "=", "[", "]", "\n", "for", "val", "in", "vals", ":", "\n", "                    ", "if", "self", ".", "has_missing_values", "(", "val", ")", ":", "\n", "                        ", "missing", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "sizes", ".", "append", "(", "len", "(", "val", ")", ")", "\n", "", "", "if", "sizes", ":", "\n", "                    ", "stat_str", "=", "(", "f\"min: {np.min(sizes):4}, \"", "\n", "f\"max: {np.max(sizes):4}, \"", "\n", "f\"mean: {np.mean(sizes):.1f}\"", ")", "\n", "print", "(", "f\"{subset}: missing: {missing:4}, {stat_str} {expert}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.common_text_feat_paths": [[658, 666], ["open", "json.load"], "methods", ["None"], ["", "", "", "", "@", "staticmethod", "\n", "@", "typechecked", "\n", "def", "common_text_feat_paths", "(", ")", "->", "Dict", "[", "str", ",", "str", "]", ":", "\n", "        ", "\"\"\"Load the text features and raw captions to be used in the challenge.\n        \"\"\"", "\n", "with", "open", "(", "\"model/text_embedding_models.json\"", ")", "as", "f", ":", "\n", "            ", "supported_text_embeddings", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "{", "name", ":", "f\"{name}.pkl\"", "for", "name", "in", "supported_text_embeddings", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.common_feat_names": [[667, 690], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "@", "typechecked", "\n", "def", "common_feat_names", "(", ")", "->", "List", "[", "str", "]", ":", "\n", "        ", "\"\"\"Produce a common collection of feature names shared amongst datasets.\n        \"\"\"", "\n", "feature_names", "=", "[", "\n", "\"imagenet.senet154.0\"", ",", "\n", "\"scene.densenet161.0\"", ",", "\n", "\"imagenet.resnext101_32x48d.0\"", ",", "\n", "\"trn.moments-trn.0\"", ",", "\n", "\"moments_2d.resnet50.0\"", ",", "\n", "\"i3d.i3d.0\"", ",", "\n", "\"i3d.i3d.1\"", ",", "\n", "\"s3dg.s3dg.0\"", ",", "\n", "\"s3dg.s3dg.1\"", ",", "\n", "\"r2p1d.r2p1d-ig65m.0\"", ",", "\n", "\"r2p1d.r2p1d-ig65m.1\"", ",", "\n", "\"r2p1d.r2p1d-ig65m-kinetics.0\"", ",", "\n", "\"r2p1d.r2p1d-ig65m-kinetics.1\"", ",", "\n", "\"moments_3d.moments-resnet3d50.0\"", ",", "\n", "\"moments_3d.moments-resnet3d50.1\"", "\n", "]", "\n", "return", "feature_names", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_dataset.BaseDataset.load_challenge_text_features": [[691, 704], ["zsvision.zs_utils.memcache", "zsvision.zs_utils.memcache", "pathlib.Path", "pathlib.Path"], "methods", ["None"], ["", "def", "load_challenge_text_features", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load the text features and raw captions to be used in the challenge.\n        \"\"\"", "\n", "text_feat_path", "=", "self", ".", "paths", "[", "\"challenge_text_feat_paths\"", "]", "[", "self", ".", "text_feat", "]", "\n", "if", "self", ".", "split_name", "==", "\"public_server_test\"", ":", "\n", "            ", "text_path", "=", "self", ".", "challenge_test_root_feat_folder", "/", "text_feat_path", "\n", "caption_path", "=", "(", "self", ".", "challenge_test_root_feat_folder", "/", "\n", "self", ".", "paths", "[", "\"raw_captions_path\"", "]", ")", "\n", "", "else", ":", "\n", "            ", "text_path", "=", "Path", "(", "self", ".", "root_feat", ")", "/", "text_feat_path", "\n", "caption_path", "=", "Path", "(", "self", ".", "root_feat", ")", "/", "self", ".", "paths", "[", "\"raw_captions_path\"", "]", "\n", "", "self", ".", "text_features", "=", "memcache", "(", "text_path", ")", "\n", "self", ".", "raw_captions", "=", "memcache", "(", "caption_path", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.__init__": [[17, 68], ["config.get_logger", "base_trainer.BaseTrainer._prepare_device", "model.to", "cfg_trainer.get", "cfg_trainer.get", "config[].get", "len", "torch.nn.DataParallel", "base_trainer.BaseTrainer.monitor.split", "cfg_trainer.get", "logger.TensorboardWriter", "base_trainer.BaseTrainer._resume_checkpoint"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get_logger", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer._prepare_device", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.get", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer._resume_checkpoint"], ["def", "__init__", "(", "self", ",", "model", ",", "loss", ",", "metrics", ",", "optimizer", ",", "config", ",", "mini_train", ",", "\n", "num_keep_ckpts", ",", "skip_tboard", ")", ":", "\n", "        ", "self", ".", "config", "=", "config", "\n", "self", ".", "logger", "=", "config", ".", "get_logger", "(", "\n", "'trainer'", ",", "config", "[", "'trainer'", "]", "[", "'verbosity'", "]", ")", "\n", "\n", "# setup GPU device if available, move model into configured device", "\n", "self", ".", "device", ",", "device_ids", "=", "self", ".", "_prepare_device", "(", "config", "[", "'n_gpu'", "]", ")", "\n", "self", ".", "model", "=", "model", ".", "to", "(", "self", ".", "device", ")", "\n", "if", "len", "(", "device_ids", ")", ">", "1", ":", "\n", "            ", "self", ".", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ",", "device_ids", "=", "device_ids", ")", "\n", "\n", "", "self", ".", "loss", "=", "loss", "\n", "self", ".", "metrics", "=", "metrics", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "num_keep_ckpts", "=", "num_keep_ckpts", "\n", "self", ".", "skip_tboard", "=", "skip_tboard", "or", "mini_train", "\n", "\n", "# This property can be overriden in the subclass", "\n", "self", ".", "skip_first_n_saves", "=", "0", "\n", "\n", "cfg_trainer", "=", "config", "[", "'trainer'", "]", "\n", "self", ".", "epochs", "=", "cfg_trainer", "[", "'epochs'", "]", "\n", "self", ".", "save_period", "=", "cfg_trainer", "[", "'save_period'", "]", "\n", "self", ".", "monitor", "=", "cfg_trainer", ".", "get", "(", "'monitor'", ",", "'off'", ")", "\n", "self", ".", "save_only_best", "=", "cfg_trainer", ".", "get", "(", "\"save_only_best\"", ",", "True", ")", "\n", "\n", "# configuration to monitor model performance and save best", "\n", "if", "self", ".", "monitor", "==", "'off'", ":", "\n", "            ", "self", ".", "mnt_mode", "=", "'off'", "\n", "self", ".", "mnt_best", "=", "0", "\n", "", "else", ":", "\n", "            ", "self", ".", "mnt_mode", ",", "self", ".", "mnt_metric", "=", "self", ".", "monitor", ".", "split", "(", ")", "\n", "assert", "self", ".", "mnt_mode", "in", "[", "'min'", ",", "'max'", "]", "\n", "\n", "self", ".", "mnt_best", "=", "np", ".", "inf", "if", "self", ".", "mnt_mode", "==", "'min'", "else", "-", "np", ".", "inf", "\n", "self", ".", "early_stop", "=", "cfg_trainer", ".", "get", "(", "'early_stop'", ",", "np", ".", "inf", ")", "\n", "\n", "", "self", ".", "start_epoch", "=", "1", "\n", "\n", "self", ".", "checkpoint_dir", "=", "config", ".", "save_dir", "\n", "\n", "# setup visualization writer instance", "\n", "if", "not", "self", ".", "skip_tboard", ":", "\n", "            ", "summary_dir", "=", "config", ".", "log_dir", "/", "f\"seed-{config['seed']}\"", "\n", "self", ".", "writer", "=", "TensorboardWriter", "(", "summary_dir", ",", "self", ".", "logger", ",", "\n", "cfg_trainer", "[", "'tensorboard'", "]", ")", "\n", "\n", "", "self", ".", "include_optim_in_ckpts", "=", "config", "[", "\"trainer\"", "]", ".", "get", "(", "\"include_optim_in_ckpts\"", ",", "1", ")", "\n", "if", "config", ".", "resume", "is", "not", "None", ":", "\n", "            ", "self", ".", "_resume_checkpoint", "(", "config", ".", "resume", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer._train_epoch": [[69, 76], ["None"], "methods", ["None"], ["", "", "@", "abstractmethod", "\n", "def", "_train_epoch", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "\"\"\"Training logic for an epoch\n\n        :param epoch: Current epoch number\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.train": [[77, 192], ["range", "base_trainer.BaseTrainer._train_epoch", "result.items", "log.items", "base_trainer.BaseTrainer.logger.info", "base_trainer.BaseTrainer.logger.info", "base_trainer.BaseTrainer._save_checkpoint", "print", "base_trainer.BaseTrainer.purge_stale_checkpoints", "log.update", "copy.deepcopy().cpu", "base_trainer.BaseTrainer.logger.info", "print", "base_trainer.BaseTrainer._save_checkpoint", "cached_preds.items", "log.update", "str", "base_trainer.BaseTrainer.logger.warning", "ValueError", "pathlib.Path", "numpy.save", "numpy.save", "base_trainer.BaseTrainer.logger.info", "base_trainer.BaseTrainer.logger.info", "value.items", "msg.format", "copy.deepcopy", "cached[].cpu().numpy", "numpy.argsort", "open", "range", "open", "pickle.dump", "enumerate", "subval.items", "str", "print", "vid_names.append", "enumerate", "cached[].cpu", "str", "str", "vid_name[].split"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer._train_epoch", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer._save_checkpoint", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.purge_stale_checkpoints", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer._save_checkpoint", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.model.metric.AverageMeter.update", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.html.HTML.save", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.html.HTML.save", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items", "home.repos.pwc.inspect_result.albanie_collaborative-experts.None.parse_config.ConfigParser.items"], ["", "def", "train", "(", "self", ")", ":", "\n", "        ", "\"\"\"Full training logic.  Responsible for iterating over epochs, early stopping,\n        checkpointing and logging metrics.\n        \"\"\"", "\n", "for", "epoch", "in", "range", "(", "self", ".", "start_epoch", ",", "self", ".", "epochs", "+", "1", ")", ":", "\n", "            ", "result", ",", "cached_preds", "=", "self", ".", "_train_epoch", "(", "epoch", ")", "\n", "\n", "# save logged informations into log dict", "\n", "log", "=", "{", "'epoch'", ":", "epoch", "}", "\n", "for", "key", ",", "value", "in", "result", ".", "items", "(", ")", ":", "\n", "                ", "if", "key", "==", "'metrics'", ":", "\n", "                    ", "log", ".", "update", "(", "{", "mtr", ".", "__name__", ":", "value", "[", "i", "]", "\n", "for", "i", ",", "mtr", "in", "enumerate", "(", "self", ".", "metrics", ")", "}", ")", "\n", "", "elif", "key", "==", "'val_metrics'", ":", "\n", "                    ", "log", ".", "update", "(", "{", "'val_'", "+", "mtr", ".", "__name__", ":", "value", "[", "i", "]", "\n", "for", "i", ",", "mtr", "in", "enumerate", "(", "self", ".", "metrics", ")", "}", ")", "\n", "", "elif", "key", "==", "'nested_val_metrics'", ":", "\n", "# NOTE: currently only supports two layers of nesting", "\n", "                    ", "for", "subkey", ",", "subval", "in", "value", ".", "items", "(", ")", ":", "\n", "                        ", "for", "subsubkey", ",", "subsubval", "in", "subval", ".", "items", "(", ")", ":", "\n", "                            ", "log", "[", "f\"val_{subkey}_{subsubkey}\"", "]", "=", "subsubval", "\n", "", "", "", "else", ":", "\n", "                    ", "log", "[", "key", "]", "=", "value", "\n", "\n", "# print logged informations to the screen", "\n", "", "", "for", "key", ",", "value", "in", "log", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "logger", ".", "info", "(", "'    {:15s}: {}'", ".", "format", "(", "str", "(", "key", ")", ",", "value", ")", ")", "\n", "\n", "# eval model according to configured metric, save best # ckpt as trained_model", "\n", "", "not_improved_count", "=", "0", "\n", "best", "=", "False", "\n", "if", "self", ".", "mnt_mode", "!=", "'off'", ":", "\n", "                ", "try", ":", "\n", "# check whether specified metric improved or not, according to", "\n", "# specified metric(mnt_metric)", "\n", "                    ", "lower", "=", "log", "[", "self", ".", "mnt_metric", "]", "<=", "self", ".", "mnt_best", "\n", "higher", "=", "log", "[", "self", ".", "mnt_metric", "]", ">=", "self", ".", "mnt_best", "\n", "improved", "=", "(", "self", ".", "mnt_mode", "==", "'min'", "and", "lower", ")", "or", "(", "self", ".", "mnt_mode", "==", "'max'", "and", "higher", ")", "\n", "", "except", "KeyError", ":", "\n", "                    ", "msg", "=", "\"Warning: Metric '{}' not found, perf monitoring is disabled.\"", "\n", "self", ".", "logger", ".", "warning", "(", "msg", ".", "format", "(", "self", ".", "mnt_metric", ")", ")", "\n", "self", ".", "mnt_mode", "=", "'off'", "\n", "improved", "=", "False", "\n", "not_improved_count", "=", "0", "\n", "raise", "ValueError", "(", "\"Pick a metric that will save checkpoints!!!!!!!!\"", ")", "\n", "\n", "", "if", "improved", ":", "\n", "                    ", "self", ".", "mnt_best", "=", "log", "[", "self", ".", "mnt_metric", "]", "\n", "# TODO(Samuel): refactor the code so that we don't move the model", "\n", "# off the GPU or duplicate on the GPU (we should be able to safely", "\n", "# copy the state dict directly to CPU)", "\n", "cpu_model", "=", "copy", ".", "deepcopy", "(", "self", ".", "model", ")", ".", "cpu", "(", ")", "\n", "self", ".", "best_checkpoint", "=", "{", "\"epoch\"", ":", "epoch", ",", "\"model\"", ":", "cpu_model", "}", "\n", "not_improved_count", "=", "0", "\n", "best", "=", "True", "\n", "", "else", ":", "\n", "                    ", "not_improved_count", "+=", "1", "\n", "\n", "", "if", "not_improved_count", ">", "self", ".", "early_stop", ":", "\n", "                    ", "self", ".", "logger", ".", "info", "(", "\"Val performance didn\\'t improve for {} epochs. \"", "\n", "\"Training stops.\"", ".", "format", "(", "self", ".", "early_stop", ")", ")", "\n", "break", "\n", "\n", "", "", "if", "self", ".", "save_only_best", ":", "\n", "                ", "if", "epoch", "==", "self", ".", "epochs", ":", "\n", "                    ", "best_ckpt", "=", "self", ".", "best_checkpoint", "\n", "self", ".", "model", "=", "best_ckpt", "[", "\"model\"", "]", "\n", "print", "(", "f\"saving the best ckpt to disk (epoch {epoch})\"", ")", "\n", "self", ".", "_save_checkpoint", "(", "best_ckpt", "[", "\"epoch\"", "]", ",", "save_best", "=", "True", ")", "\n", "", "continue", "\n", "\n", "# If checkpointing is done intermittently, still save models that outperform", "\n", "# the best metric", "\n", "# save_best = best and not self.mnt_metric == \"epoch\"", "\n", "", "save_best", "=", "True", "\n", "\n", "# Due to the fast runtime/slow HDD combination, checkpointing can dominate", "\n", "# the total training time, so we optionally skip checkpoints for some of", "\n", "# the first epochs", "\n", "if", "epoch", "<", "self", ".", "skip_first_n_saves", "and", "not", "self", ".", "save_only_best", ":", "\n", "                ", "msg", "=", "f\"Skipping ckpt save at epoch {epoch} <= {self.skip_first_n_saves}\"", "\n", "self", ".", "logger", ".", "info", "(", "msg", ")", "\n", "continue", "\n", "\n", "", "if", "epoch", "%", "self", ".", "save_period", "==", "0", "and", "save_best", ":", "\n", "                ", "self", ".", "_save_checkpoint", "(", "epoch", ",", "save_best", "=", "best", ")", "\n", "print", "(", "\"This epoch, the save best :{}\"", ".", "format", "(", "best", ")", ")", "\n", "if", "best", ":", "\n", "                    ", "for", "key", ",", "cached", "in", "cached_preds", ".", "items", "(", ")", ":", "\n", "                        ", "log_dir", "=", "Path", "(", "self", ".", "config", ".", "log_dir", ")", "\n", "prediction_path", "=", "log_dir", "/", "f\"{key}_preds.txt\"", "\n", "prediction_logits_path", "=", "log_dir", "/", "f\"{key}_preds_logits.npy\"", "\n", "np", ".", "save", "(", "prediction_logits_path", ",", "cached", "[", "\"preds\"", "]", ")", "\n", "gt_logits_path", "=", "log_dir", "/", "f\"{key}_gt_logits.npy\"", "\n", "np", ".", "save", "(", "gt_logits_path", ",", "cached", "[", "\"labels\"", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "vid_names", "=", "[", "]", "\n", "sort_predict", "=", "np", ".", "argsort", "(", "cached", "[", "\"preds\"", "]", ")", "[", ":", ",", ":", ":", "-", "1", "]", "\n", "with", "open", "(", "str", "(", "prediction_path", ")", ",", "'w'", ")", "as", "f", ":", "\n", "                            ", "for", "kk", "in", "range", "(", "cached", "[", "\"preds\"", "]", ".", "shape", "[", "0", "]", ")", ":", "\n", "                                ", "pred_classes", "=", "[", "str", "(", "v", ")", "for", "v", "in", "sort_predict", "[", "kk", ",", ":", "]", "]", "\n", "vid_name", "=", "cached", "[", "\"vid_name\"", "]", "[", "kk", "]", "\n", "if", "key", "==", "\"test\"", ":", "\n", "                                    ", "vid_name", "=", "vid_name", "[", "kk", "]", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", "+", "'.mp4'", "\n", "", "row", "=", "f\"{vid_name} {' '.join(pred_classes)}\"", "\n", "print", "(", "row", ",", "file", "=", "f", ")", "\n", "vid_names", ".", "append", "(", "vid_name", ")", "\n", "", "", "save_name_path", "=", "log_dir", "/", "f\"{key}_vid_name.pkl\"", "\n", "with", "open", "(", "save_name_path", ",", "'wb'", ")", "as", "f", ":", "\n", "                            ", "pickle", ".", "dump", "(", "vid_names", ",", "f", ")", "\n", "", "self", ".", "logger", ".", "info", "(", "f\"All {key} preds saved\"", ")", "\n", "self", ".", "logger", ".", "info", "(", "f\"Wrote result to: {str(prediction_path)}\"", ")", "\n", "\n", "", "", "", "if", "epoch", ">", "self", ".", "num_keep_ckpts", ":", "\n", "                ", "self", ".", "purge_stale_checkpoints", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer.purge_stale_checkpoints": [[193, 219], ["list", "list", "sorted", "base_trainer.BaseTrainer.checkpoint_dir.glob", "base_trainer.BaseTrainer.checkpoint_dir.glob", "len", "print", "int", "list", "time.time", "stale_ckpt.unlink", "base_trainer.BaseTrainer.logger.info", "len", "len", "zip", "re.search().groups", "time.time", "re.search", "str"], "methods", ["None"], ["", "", "", "def", "purge_stale_checkpoints", "(", "self", ")", ":", "\n", "        ", "\"\"\"Remove checkpoints that are no longer neededself.\n\n        NOTE: This function assumes that the `best` checkpoint has already been renamed\n        to have a format that differs from `checkpoint-epoch<num>.pth`\n        \"\"\"", "\n", "all_ckpts", "=", "list", "(", "self", ".", "checkpoint_dir", ".", "glob", "(", "\"*.pth\"", ")", ")", "\n", "found_epoch_ckpts", "=", "list", "(", "self", ".", "checkpoint_dir", ".", "glob", "(", "\"checkpoint-epoch*.pth\"", ")", ")", "\n", "if", "len", "(", "all_ckpts", ")", "<=", "self", ".", "num_keep_ckpts", ":", "\n", "            ", "return", "\n", "\n", "", "msg", "=", "\"Expected at the best checkpoint to have been renamed to a different format\"", "\n", "if", "not", "len", "(", "all_ckpts", ")", ">", "len", "(", "found_epoch_ckpts", ")", ":", "\n", "            ", "print", "(", "\"Warning, purging ckpt, but the best epoch was not saved!\"", ")", "\n", "# assert len(all_ckpts) > len(found_epoch_ckpts), msg", "\n", "\n", "# purge the oldest checkpoints", "\n", "", "regex", "=", "r\".*checkpoint-epoch(\\d+)[.]pth$\"", "\n", "epochs", "=", "[", "int", "(", "re", ".", "search", "(", "regex", ",", "str", "(", "x", ")", ")", ".", "groups", "(", ")", "[", "0", "]", ")", "for", "x", "in", "found_epoch_ckpts", "]", "\n", "sorted_ckpts", "=", "sorted", "(", "list", "(", "zip", "(", "epochs", ",", "found_epoch_ckpts", ")", ")", ",", "key", "=", "lambda", "x", ":", "-", "x", "[", "0", "]", ")", "\n", "\n", "for", "epoch", ",", "stale_ckpt", "in", "sorted_ckpts", "[", "self", ".", "num_keep_ckpts", ":", "]", ":", "\n", "            ", "tic", "=", "time", ".", "time", "(", ")", "\n", "stale_ckpt", ".", "unlink", "(", ")", "\n", "msg", "=", "f\"removing stale ckpt [epoch {epoch}] [took {time.time() - tic:.2f}s]\"", "\n", "self", ".", "logger", ".", "info", "(", "msg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer._prepare_device": [[220, 237], ["torch.cuda.device_count", "torch.device", "list", "base_trainer.BaseTrainer.logger.warning", "base_trainer.BaseTrainer.logger.warning", "range"], "methods", ["None"], ["", "", "def", "_prepare_device", "(", "self", ",", "n_gpu_use", ")", ":", "\n", "        ", "\"\"\"\n        setup GPU device if available, move model into configured device\n        \"\"\"", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "if", "n_gpu_use", ">", "0", "and", "n_gpu", "==", "0", ":", "\n", "            ", "self", ".", "logger", ".", "warning", "(", "\"Warning: There\\'s no GPU available on this machine,\"", "\n", "\"training will be performed on CPU.\"", ")", "\n", "n_gpu_use", "=", "0", "\n", "", "if", "n_gpu_use", ">", "n_gpu", ":", "\n", "            ", "self", ".", "logger", ".", "warning", "(", "\"Warning: The number of GPU\\'s configured to use is {}\"", "\n", "\", but only {} are available \"", "\n", "\"on this machine.\"", ".", "format", "(", "n_gpu_use", ",", "n_gpu", ")", ")", "\n", "n_gpu_use", "=", "n_gpu", "\n", "", "device", "=", "torch", ".", "device", "(", "'cuda:0'", "if", "n_gpu_use", ">", "0", "else", "'cpu'", ")", "\n", "list_ids", "=", "list", "(", "range", "(", "n_gpu_use", ")", ")", "\n", "return", "device", ",", "list_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer._save_checkpoint": [[238, 267], ["str", "time.time", "base_trainer.BaseTrainer.logger.info", "torch.save", "base_trainer.BaseTrainer.logger.info", "type", "base_trainer.BaseTrainer.model.state_dict", "base_trainer.BaseTrainer.optimizer.state_dict", "base_trainer.BaseTrainer.logger.info", "str", "torch.save", "base_trainer.BaseTrainer.logger.info", "time.time", "time.time"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.html.HTML.save", "home.repos.pwc.inspect_result.albanie_collaborative-experts.utils.html.HTML.save"], ["", "def", "_save_checkpoint", "(", "self", ",", "epoch", ",", "save_best", "=", "False", ")", ":", "\n", "        ", "\"\"\"Saving checkpoints\n\n        :param epoch: current epoch number\n        :param log: logging information of the epoch\n        :param save_best: if True, rename the saved checkpoint to 'trained_model.pth'\n        \"\"\"", "\n", "arch", "=", "type", "(", "self", ".", "model", ")", ".", "__name__", "\n", "state", "=", "{", "\n", "'arch'", ":", "arch", ",", "\n", "'epoch'", ":", "epoch", ",", "\n", "'state_dict'", ":", "self", ".", "model", ".", "state_dict", "(", ")", ",", "\n", "'monitor_best'", ":", "self", ".", "mnt_best", ",", "\n", "'config'", ":", "self", ".", "config", "\n", "}", "\n", "if", "self", ".", "include_optim_in_ckpts", ":", "\n", "            ", "state", "[", "\"optimizer\"", "]", "=", "self", ".", "optimizer", ".", "state_dict", "(", ")", "\n", "\n", "", "filename", "=", "str", "(", "self", ".", "checkpoint_dir", "/", "\n", "'checkpoint-epoch{}.pth'", ".", "format", "(", "epoch", ")", ")", "\n", "tic", "=", "time", ".", "time", "(", ")", "\n", "self", ".", "logger", ".", "info", "(", "\"Saving checkpoint: {} ...\"", ".", "format", "(", "filename", ")", ")", "\n", "torch", ".", "save", "(", "state", ",", "filename", ")", "\n", "self", ".", "logger", ".", "info", "(", "f\"Done in {time.time() - tic:.3f}s\"", ")", "\n", "if", "save_best", ":", "\n", "            ", "self", ".", "logger", ".", "info", "(", "\"Updating 'best' checkpoint: {} ...\"", ".", "format", "(", "filename", ")", ")", "\n", "best_path", "=", "str", "(", "self", ".", "checkpoint_dir", "/", "'trained_model.pth'", ")", "\n", "torch", ".", "save", "(", "state", ",", "best_path", ")", "\n", "self", ".", "logger", ".", "info", "(", "f\"Done in {time.time() - tic:.3f}s\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_trainer.BaseTrainer._resume_checkpoint": [[268, 298], ["str", "base_trainer.BaseTrainer.logger.info", "torch.load", "base_trainer.BaseTrainer.model.load_state_dict", "base_trainer.BaseTrainer.logger.info", "base_trainer.BaseTrainer.logger.warning", "base_trainer.BaseTrainer.logger.warning", "base_trainer.BaseTrainer.optimizer.load_state_dict"], "methods", ["None"], ["", "", "def", "_resume_checkpoint", "(", "self", ",", "resume_path", ")", ":", "\n", "        ", "\"\"\" Resume from saved checkpoints\n\n        :param resume_path: Checkpoint path to be resumed\n        \"\"\"", "\n", "resume_path", "=", "str", "(", "resume_path", ")", "\n", "self", ".", "logger", ".", "info", "(", "\"Loading checkpoint: {} ...\"", ".", "format", "(", "resume_path", ")", ")", "\n", "checkpoint", "=", "torch", ".", "load", "(", "resume_path", ")", "\n", "self", ".", "start_epoch", "=", "checkpoint", "[", "'epoch'", "]", "+", "1", "\n", "self", ".", "mnt_best", "=", "checkpoint", "[", "'monitor_best'", "]", "\n", "\n", "# load architecture params from checkpoint.", "\n", "if", "checkpoint", "[", "'config'", "]", "[", "'arch'", "]", "!=", "self", ".", "config", "[", "'arch'", "]", ":", "\n", "            ", "msg", "=", "(", "\"Warning: Architecture configuration given in config file is\"", "\n", "\"different from that of checkpoint. This may yield an exception\"", "\n", "\" while state_dict is being loaded.\"", ")", "\n", "self", ".", "logger", ".", "warning", "(", "msg", ")", "\n", "", "self", ".", "model", ".", "load_state_dict", "(", "checkpoint", "[", "'state_dict'", "]", ")", "\n", "\n", "if", "self", ".", "include_optim_in_ckpts", ":", "\n", "# load optimizer state from ckpt only when optimizer type is not changed.", "\n", "            ", "optim_args", "=", "checkpoint", "[", "'config'", "]", "[", "'optimizer'", "]", "\n", "if", "optim_args", "[", "'type'", "]", "!=", "self", ".", "config", "[", "'optimizer'", "]", "[", "'type'", "]", ":", "\n", "                ", "msg", "=", "(", "\"Warning: Optimizer type given in config file differs from that\"", "\n", "\" of checkpoint. Optimizer parameters not being resumed.\"", ")", "\n", "self", ".", "logger", ".", "warning", "(", "msg", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "optimizer", ".", "load_state_dict", "(", "checkpoint", "[", "'optimizer'", "]", ")", "\n", "\n", "", "", "self", ".", "logger", ".", "info", "(", "f\"Ckpt loaded. Resume training from epoch {self.start_epoch}\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_model.BaseModel.forward": [[10, 18], ["None"], "methods", ["None"], ["@", "abstractmethod", "\n", "def", "forward", "(", "self", ",", "*", "inputs", ")", ":", "\n", "        ", "\"\"\"\n        Forward pass logic\n\n        :return: Model output\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_model.BaseModel.__str__": [[19, 26], ["filter", "sum", "base_model.BaseModel.parameters", "torch.Module.__str__", "numpy.prod", "p.size"], "methods", ["home.repos.pwc.inspect_result.albanie_collaborative-experts.base.base_model.BaseModel.__str__"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Model prints with number of trainable parameters\n        \"\"\"", "\n", "model_parameters", "=", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "self", ".", "parameters", "(", ")", ")", "\n", "params", "=", "sum", "(", "[", "np", ".", "prod", "(", "p", ".", "size", "(", ")", ")", "for", "p", "in", "model_parameters", "]", ")", "\n", "return", "super", "(", ")", ".", "__str__", "(", ")", "+", "f\"\\nTrainable parameters: {params}\"", "\n", "", "", ""]]}