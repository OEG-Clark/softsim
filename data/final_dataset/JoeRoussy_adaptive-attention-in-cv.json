{"home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.config.get_logger": [[8, 27], ["logging.getLogger", "logging.getLogger", "logging.getLogger.setLevel", "logging.Formatter", "logging.Formatter", "logging.FileHandler", "logging.FileHandler", "logging.StreamHandler", "logging.StreamHandler", "logging.FileHandler.setFormatter", "logging.StreamHandler.setFormatter", "logging.getLogger.addHandler", "logging.getLogger.addHandler", "os.path.isdir", "os.mkdir"], "function", ["None"], ["def", "get_logger", "(", "filename", ")", ":", "\n", "    ", "logger", "=", "logging", ".", "getLogger", "(", "'logger'", ")", "\n", "logger", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "\n", "formatter", "=", "logging", ".", "Formatter", "(", "'[%(levelname)s | %(filename)s:%(lineno)s] %(asctime)s: %(message)s'", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "'log'", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "'log'", ")", "\n", "\n", "", "file_handler", "=", "logging", ".", "FileHandler", "(", "'./log/'", "+", "filename", "+", "'.log'", ")", "\n", "stream_handler", "=", "logging", ".", "StreamHandler", "(", ")", "\n", "\n", "file_handler", ".", "setFormatter", "(", "formatter", ")", "\n", "stream_handler", ".", "setFormatter", "(", "formatter", ")", "\n", "\n", "logger", ".", "addHandler", "(", "file_handler", ")", "\n", "logger", ".", "addHandler", "(", "stream_handler", ")", "\n", "\n", "return", "logger", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.config.get_args": [[29, 101], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["", "def", "get_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "'parameters'", ")", "\n", "\n", "#ATTENTION VARS", "\n", "parser", ".", "add_argument", "(", "'--all_attention'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "help", "=", "'Use local self attention instead of convolutions'", ")", "\n", "parser", ".", "add_argument", "(", "'--groups'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "\n", "#Global Attention", "\n", "parser", ".", "add_argument", "(", "'--dk'", ",", "type", "=", "int", ",", "default", "=", "40", ",", "help", "=", "'Dimensions of the query and key vectors. Note that this will be split amoung each attention head'", ")", "\n", "parser", ".", "add_argument", "(", "'--dv'", ",", "type", "=", "int", ",", "default", "=", "4", ",", "help", "=", "'Dimensions of the value vectors. Note that this will be split amoung each attention head'", ")", "\n", "parser", ".", "add_argument", "(", "'--attention_conv'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "help", "=", "'Use attention augmented convolutions'", ")", "\n", "\n", "#Adaptive Attention", "\n", "parser", ".", "add_argument", "(", "'--R'", ",", "type", "=", "float", ",", "default", "=", "3.0", ",", "help", "=", "'Variable R in masking function (controls decay of mask to 0)'", ")", "\n", "parser", ".", "add_argument", "(", "'--z_init'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "help", "=", "'mask variable which controls distance of no mask'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive_span'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--span_penalty'", ",", "type", "=", "float", ",", "default", "=", "0.001", ",", "\n", "help", "=", "'L1 regularizer coefficient for attention span variables'", ")", "\n", "parser", ".", "add_argument", "(", "'--attention_kernel'", ",", "type", "=", "int", ",", "default", "=", "3", ")", "\n", "\n", "# learning rate for adam", "\n", "parser", ".", "add_argument", "(", "'--decay_factor'", ",", "type", "=", "float", ",", "default", "=", "0.3", ",", "help", "=", "'factor to decay lr by'", ")", "\n", "parser", ".", "add_argument", "(", "'--use_adam'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "help", "=", "'Whether or not to use Adam optimizer'", ")", "\n", "parser", ".", "add_argument", "(", "'--adam_lr'", ",", "type", "=", "float", ",", "default", "=", "0.001", ")", "\n", "\n", "\n", "parser", ".", "add_argument", "(", "'--dataset'", ",", "type", "=", "str", ",", "default", "=", "'CIFAR10'", ",", "help", "=", "'CIFAR10, CIFAR100, MNIST, TinyImageNet'", ")", "\n", "parser", ".", "add_argument", "(", "'--subset'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "help", "=", "'proportion of dataset to use'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--test'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "help", "=", "'Whether or not on test set'", ")", "\n", "parser", ".", "add_argument", "(", "'--small_version'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--smallest_version'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--model-name'", ",", "type", "=", "str", ",", "default", "=", "'ResNet26'", ",", "help", "=", "'ResNet26, ResNet38, ResNet50'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch-size'", ",", "type", "=", "int", ",", "default", "=", "25", ")", "\n", "parser", ".", "add_argument", "(", "'--num-workers'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "100", ")", "\n", "\n", "# scheduler config", "\n", "parser", ".", "add_argument", "(", "'--no_annealing'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--T_max'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "help", "=", "'default equals total number of epochs'", ")", "\n", "parser", ".", "add_argument", "(", "'--eta_min'", ",", "type", "=", "float", ",", "default", "=", "0.", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup_epochs'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--start_scheduler'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'which epoch to start the scheduler, by default it starts as soon as warmup finishes'", ")", "\n", "parser", ".", "add_argument", "(", "'--force_cosine_annealing'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "\n", "help", "=", "'Force a warmup with cosine annealing learning rate schedule regardless of model type'", ")", "\n", "\n", "# learning rate for SGD", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "0.1", ")", "\n", "parser", ".", "add_argument", "(", "'--momentum'", ",", "type", "=", "float", ",", "default", "=", "0.9", ")", "\n", "parser", ".", "add_argument", "(", "'--weight-decay'", ",", "type", "=", "float", ",", "default", "=", "1e-4", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--print-interval'", ",", "type", "=", "int", ",", "default", "=", "100", ")", "\n", "parser", ".", "add_argument", "(", "'--cuda'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--pretrained-model'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--distributed'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu-devices'", ",", "type", "=", "int", ",", "nargs", "=", "'+'", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu'", ",", "type", "=", "int", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'--rank'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'current process number'", ")", "\n", "parser", ".", "add_argument", "(", "'--world-size'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'Total number of processes to be used (number of gpus)'", ")", "\n", "parser", ".", "add_argument", "(", "'--dist-backend'", ",", "type", "=", "str", ",", "default", "=", "'nccl'", ")", "\n", "parser", ".", "add_argument", "(", "'--dist-url'", ",", "default", "=", "'tcp://127.0.0.1:3456'", ",", "type", "=", "str", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--xpid'", ",", "default", "=", "'example'", ",", "help", "=", "'Experiment ID, default = example'", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# TODO: Remove these comments", "\n", "logger", "=", "None", "#get_logger('train')", "\n", "#logger.info(vars(args))", "\n", "\n", "return", "args", ",", "logger", "\n", "", ""]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention_augmented_conv.AugmentedConv.__init__": [[9, 36], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "str", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.file_writer.FileWriter.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "dk", ",", "dv", ",", "Nh", ",", "shape", "=", "0", ",", "relative", "=", "False", ",", "stride", "=", "1", ")", ":", "\n", "        ", "super", "(", "AugmentedConv", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "out_channels", "=", "out_channels", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "self", ".", "dk", "=", "dk", "\n", "self", ".", "dv", "=", "dv", "\n", "self", ".", "Nh", "=", "Nh", "\n", "self", ".", "shape", "=", "shape", "\n", "self", ".", "relative", "=", "relative", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "padding", "=", "(", "self", ".", "kernel_size", "-", "1", ")", "//", "2", "\n", "\n", "assert", "self", ".", "Nh", "!=", "0", ",", "\"integer division or modulo by zero, Nh >= 1\"", "\n", "assert", "self", ".", "dk", "%", "self", ".", "Nh", "==", "0", ",", "\"dk should be divided by Nh. (example: out_channels: 20, dk: 40, Nh: 4)\"", "\n", "assert", "self", ".", "dv", "%", "self", ".", "Nh", "==", "0", ",", "\"dv should be divided by Nh. (example: out_channels: 20, dv: 4, Nh: 4)\"", "\n", "assert", "stride", "in", "[", "1", ",", "2", "]", ",", "str", "(", "stride", ")", "+", "\" Up to 2 strides are allowed.\"", "\n", "\n", "self", ".", "conv_out", "=", "nn", ".", "Conv2d", "(", "self", ".", "in_channels", ",", "self", ".", "out_channels", "-", "self", ".", "dv", ",", "self", ".", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "self", ".", "padding", ")", "\n", "\n", "self", ".", "qkv_conv", "=", "nn", ".", "Conv2d", "(", "self", ".", "in_channels", ",", "2", "*", "self", ".", "dk", "+", "self", ".", "dv", ",", "kernel_size", "=", "self", ".", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "self", ".", "padding", ")", "\n", "\n", "self", ".", "attn_out", "=", "nn", ".", "Conv2d", "(", "self", ".", "dv", ",", "self", ".", "dv", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", "\n", "if", "self", ".", "relative", ":", "\n", "            ", "self", ".", "key_rel_w", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "(", "2", "*", "self", ".", "shape", "-", "1", ",", "dk", "//", "Nh", ")", ",", "requires_grad", "=", "True", ")", ")", "\n", "self", ".", "key_rel_h", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "(", "2", "*", "self", ".", "shape", "-", "1", ",", "dk", "//", "Nh", ")", ",", "requires_grad", "=", "True", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention_augmented_conv.AugmentedConv.forward": [[37, 70], ["attention_augmented_conv.AugmentedConv.conv_out", "attention_augmented_conv.AugmentedConv.size", "attention_augmented_conv.AugmentedConv.compute_flat_qkv", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.softmax", "torch.softmax", "torch.softmax", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "attention_augmented_conv.AugmentedConv.combine_heads_2d", "attention_augmented_conv.AugmentedConv.attn_out", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "flat_q.transpose", "attention_augmented_conv.AugmentedConv.relative_logits", "flat_v.transpose"], "methods", ["home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention_augmented_conv.AugmentedConv.compute_flat_qkv", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention_augmented_conv.AugmentedConv.combine_heads_2d", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention_augmented_conv.AugmentedConv.relative_logits"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# Input x", "\n", "# (batch_size, channels, height, width)", "\n", "# batch, _, height, width = x.size()", "\n", "\n", "# conv_out", "\n", "# (batch_size, out_channels, height, width)", "\n", "        ", "conv_out", "=", "self", ".", "conv_out", "(", "x", ")", "\n", "batch", ",", "_", ",", "height", ",", "width", "=", "conv_out", ".", "size", "(", ")", "\n", "\n", "# flat_q, flat_k, flat_v", "\n", "# (batch_size, Nh, height * width, dvh or dkh)", "\n", "# dvh = dv / Nh, dkh = dk / Nh", "\n", "# q, k, v", "\n", "# (batch_size, Nh, height, width, dv or dk)", "\n", "flat_q", ",", "flat_k", ",", "flat_v", ",", "q", ",", "k", ",", "v", "=", "self", ".", "compute_flat_qkv", "(", "x", ",", "self", ".", "dk", ",", "self", ".", "dv", ",", "self", ".", "Nh", ")", "\n", "logits", "=", "torch", ".", "matmul", "(", "flat_q", ".", "transpose", "(", "2", ",", "3", ")", ",", "flat_k", ")", "\n", "if", "self", ".", "relative", ":", "\n", "            ", "h_rel_logits", ",", "w_rel_logits", "=", "self", ".", "relative_logits", "(", "q", ")", "\n", "logits", "+=", "h_rel_logits", "\n", "logits", "+=", "w_rel_logits", "\n", "", "weights", "=", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# attn_out", "\n", "# (batch, Nh, height * width, dvh)", "\n", "attn_out", "=", "torch", ".", "matmul", "(", "weights", ",", "flat_v", ".", "transpose", "(", "2", ",", "3", ")", ")", "\n", "attn_out", "=", "torch", ".", "reshape", "(", "attn_out", ",", "(", "batch", ",", "self", ".", "Nh", ",", "self", ".", "dv", "//", "self", ".", "Nh", ",", "height", ",", "width", ")", ")", "\n", "# combine_heads_2d", "\n", "# (batch, out_channels, height, width)", "\n", "attn_out", "=", "self", ".", "combine_heads_2d", "(", "attn_out", ")", "\n", "attn_out", "=", "self", ".", "attn_out", "(", "attn_out", ")", "\n", "\n", "return", "torch", ".", "cat", "(", "(", "conv_out", ",", "attn_out", ")", ",", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention_augmented_conv.AugmentedConv.compute_flat_qkv": [[71, 85], ["attention_augmented_conv.AugmentedConv.qkv_conv", "attention_augmented_conv.AugmentedConv.size", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "attention_augmented_conv.AugmentedConv.split_heads_2d", "attention_augmented_conv.AugmentedConv.split_heads_2d", "attention_augmented_conv.AugmentedConv.split_heads_2d", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape"], "methods", ["home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention_augmented_conv.AugmentedConv.split_heads_2d", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention_augmented_conv.AugmentedConv.split_heads_2d", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention_augmented_conv.AugmentedConv.split_heads_2d"], ["", "def", "compute_flat_qkv", "(", "self", ",", "x", ",", "dk", ",", "dv", ",", "Nh", ")", ":", "\n", "        ", "qkv", "=", "self", ".", "qkv_conv", "(", "x", ")", "\n", "N", ",", "_", ",", "H", ",", "W", "=", "qkv", ".", "size", "(", ")", "\n", "q", ",", "k", ",", "v", "=", "torch", ".", "split", "(", "qkv", ",", "[", "dk", ",", "dk", ",", "dv", "]", ",", "dim", "=", "1", ")", "\n", "q", "=", "self", ".", "split_heads_2d", "(", "q", ",", "Nh", ")", "\n", "k", "=", "self", ".", "split_heads_2d", "(", "k", ",", "Nh", ")", "\n", "v", "=", "self", ".", "split_heads_2d", "(", "v", ",", "Nh", ")", "\n", "\n", "dkh", "=", "dk", "//", "Nh", "\n", "q", "*=", "dkh", "**", "-", "0.5", "\n", "flat_q", "=", "torch", ".", "reshape", "(", "q", ",", "(", "N", ",", "Nh", ",", "dk", "//", "Nh", ",", "H", "*", "W", ")", ")", "\n", "flat_k", "=", "torch", ".", "reshape", "(", "k", ",", "(", "N", ",", "Nh", ",", "dk", "//", "Nh", ",", "H", "*", "W", ")", ")", "\n", "flat_v", "=", "torch", ".", "reshape", "(", "v", ",", "(", "N", ",", "Nh", ",", "dv", "//", "Nh", ",", "H", "*", "W", ")", ")", "\n", "return", "flat_q", ",", "flat_k", ",", "flat_v", ",", "q", ",", "k", ",", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention_augmented_conv.AugmentedConv.split_heads_2d": [[86, 91], ["x.size", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape"], "methods", ["None"], ["", "def", "split_heads_2d", "(", "self", ",", "x", ",", "Nh", ")", ":", "\n", "        ", "batch", ",", "channels", ",", "height", ",", "width", "=", "x", ".", "size", "(", ")", "\n", "ret_shape", "=", "(", "batch", ",", "Nh", ",", "channels", "//", "Nh", ",", "height", ",", "width", ")", "\n", "split", "=", "torch", ".", "reshape", "(", "x", ",", "ret_shape", ")", "\n", "return", "split", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention_augmented_conv.AugmentedConv.combine_heads_2d": [[92, 96], ["x.size", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape"], "methods", ["None"], ["", "def", "combine_heads_2d", "(", "self", ",", "x", ")", ":", "\n", "        ", "batch", ",", "Nh", ",", "dv", ",", "H", ",", "W", "=", "x", ".", "size", "(", ")", "\n", "ret_shape", "=", "(", "batch", ",", "Nh", "*", "dv", ",", "H", ",", "W", ")", "\n", "return", "torch", ".", "reshape", "(", "x", ",", "ret_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention_augmented_conv.AugmentedConv.relative_logits": [[97, 105], ["torch.transpose().transpose.size", "torch.transpose().transpose.size", "torch.transpose().transpose.size", "torch.transpose().transpose", "torch.transpose().transpose", "torch.transpose().transpose", "torch.transpose().transpose", "torch.transpose().transpose", "torch.transpose().transpose", "torch.transpose().transpose", "torch.transpose().transpose", "torch.transpose().transpose", "attention_augmented_conv.AugmentedConv.relative_logits_1d", "attention_augmented_conv.AugmentedConv.relative_logits_1d", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose"], "methods", ["home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention_augmented_conv.AugmentedConv.relative_logits_1d", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention_augmented_conv.AugmentedConv.relative_logits_1d"], ["", "def", "relative_logits", "(", "self", ",", "q", ")", ":", "\n", "        ", "B", ",", "Nh", ",", "dk", ",", "H", ",", "W", "=", "q", ".", "size", "(", ")", "\n", "q", "=", "torch", ".", "transpose", "(", "q", ",", "2", ",", "4", ")", ".", "transpose", "(", "2", ",", "3", ")", "\n", "\n", "rel_logits_w", "=", "self", ".", "relative_logits_1d", "(", "q", ",", "self", ".", "key_rel_w", ",", "H", ",", "W", ",", "Nh", ",", "\"w\"", ")", "\n", "rel_logits_h", "=", "self", ".", "relative_logits_1d", "(", "torch", ".", "transpose", "(", "q", ",", "2", ",", "3", ")", ",", "self", ".", "key_rel_h", ",", "W", ",", "H", ",", "Nh", ",", "\"h\"", ")", "\n", "\n", "return", "rel_logits_h", ",", "rel_logits_w", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention_augmented_conv.AugmentedConv.relative_logits_1d": [[106, 121], ["torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "attention_augmented_conv.AugmentedConv.rel_to_abs", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.transpose().transpose().transpose.repeat", "torch.transpose().transpose().transpose.repeat", "torch.transpose().transpose().transpose.repeat", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose().transpose().transpose", "torch.transpose().transpose().transpose", "torch.transpose().transpose().transpose", "torch.transpose().transpose().transpose", "torch.transpose().transpose().transpose", "torch.transpose().transpose().transpose", "torch.transpose().transpose().transpose", "torch.transpose().transpose().transpose", "torch.transpose().transpose().transpose", "torch.transpose().transpose", "torch.transpose().transpose", "torch.transpose().transpose", "torch.transpose().transpose", "torch.transpose().transpose", "torch.transpose().transpose", "torch.transpose().transpose", "torch.transpose().transpose", "torch.transpose().transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose"], "methods", ["home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention_augmented_conv.AugmentedConv.rel_to_abs"], ["", "def", "relative_logits_1d", "(", "self", ",", "q", ",", "rel_k", ",", "H", ",", "W", ",", "Nh", ",", "case", ")", ":", "\n", "        ", "rel_logits", "=", "torch", ".", "einsum", "(", "'bhxyd,md->bhxym'", ",", "q", ",", "rel_k", ")", "\n", "rel_logits", "=", "torch", ".", "reshape", "(", "rel_logits", ",", "(", "-", "1", ",", "Nh", "*", "H", ",", "W", ",", "2", "*", "W", "-", "1", ")", ")", "\n", "rel_logits", "=", "self", ".", "rel_to_abs", "(", "rel_logits", ")", "\n", "\n", "rel_logits", "=", "torch", ".", "reshape", "(", "rel_logits", ",", "(", "-", "1", ",", "Nh", ",", "H", ",", "W", ",", "W", ")", ")", "\n", "rel_logits", "=", "torch", ".", "unsqueeze", "(", "rel_logits", ",", "dim", "=", "3", ")", "\n", "rel_logits", "=", "rel_logits", ".", "repeat", "(", "(", "1", ",", "1", ",", "1", ",", "H", ",", "1", ",", "1", ")", ")", "\n", "\n", "if", "case", "==", "\"w\"", ":", "\n", "            ", "rel_logits", "=", "torch", ".", "transpose", "(", "rel_logits", ",", "3", ",", "4", ")", "\n", "", "elif", "case", "==", "\"h\"", ":", "\n", "            ", "rel_logits", "=", "torch", ".", "transpose", "(", "rel_logits", ",", "2", ",", "4", ")", ".", "transpose", "(", "4", ",", "5", ")", ".", "transpose", "(", "3", ",", "5", ")", "\n", "", "rel_logits", "=", "torch", ".", "reshape", "(", "rel_logits", ",", "(", "-", "1", ",", "Nh", ",", "H", "*", "W", ",", "H", "*", "W", ")", ")", "\n", "return", "rel_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention_augmented_conv.AugmentedConv.rel_to_abs": [[122, 135], ["torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "rel_to_abs", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "Nh", ",", "L", ",", "_", "=", "x", ".", "size", "(", ")", "\n", "\n", "col_pad", "=", "torch", ".", "zeros", "(", "(", "B", ",", "Nh", ",", "L", ",", "1", ")", ")", ".", "to", "(", "x", ")", "\n", "x", "=", "torch", ".", "cat", "(", "(", "x", ",", "col_pad", ")", ",", "dim", "=", "3", ")", "\n", "\n", "flat_x", "=", "torch", ".", "reshape", "(", "x", ",", "(", "B", ",", "Nh", ",", "L", "*", "2", "*", "L", ")", ")", "\n", "flat_pad", "=", "torch", ".", "zeros", "(", "(", "B", ",", "Nh", ",", "L", "-", "1", ")", ")", ".", "to", "(", "x", ")", "\n", "flat_x_padded", "=", "torch", ".", "cat", "(", "(", "flat_x", ",", "flat_pad", ")", ",", "dim", "=", "2", ")", "\n", "\n", "final_x", "=", "torch", ".", "reshape", "(", "flat_x_padded", ",", "(", "B", ",", "Nh", ",", "L", "+", "1", ",", "2", "*", "L", "-", "1", ")", ")", "\n", "final_x", "=", "final_x", "[", ":", ",", ":", ",", ":", "L", ",", "L", "-", "1", ":", "]", "\n", "return", "final_x", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.main.adjust_learning_rate": [[33, 39], ["int", "int", "float", "float"], "function", ["None"], ["def", "adjust_learning_rate", "(", "optimizer", ",", "epoch", ",", "args", ")", ":", "\n", "    ", "\"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"", "\n", "#After 60% of we trim by factor args.decay_factor and at 80% we do this again", "\n", "if", "epoch", "==", "int", "(", "0.6", "*", "float", "(", "args", ".", "epochs", ")", ")", "or", "epoch", "==", "int", "(", "0.8", "*", "float", "(", "args", ".", "epochs", ")", ")", ":", "\n", "        ", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "            ", "param_group", "[", "'lr'", "]", "*=", "args", ".", "decay_factor", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.main.train": [[41, 66], ["model.train", "optimizer.zero_grad", "model", "model.module.get_span_l1", "loss.backward", "optimizer.step", "data.to", "target.to", "criterion", "model.module.clamp_span", "model.data.max", "print", "float", "len", "y_pred.eq().sum", "y_pred.eq"], "function", ["home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.main.train", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.model.Model.get_span_l1", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.model.Model.clamp_span"], ["", "", "", "def", "train", "(", "model", ",", "train_loader", ",", "optimizer", ",", "criterion", ",", "epoch", ",", "args", ",", "logger", ",", "device", ")", ":", "\n", "    ", "model", ".", "train", "(", ")", "\n", "\n", "train_acc", "=", "0.0", "\n", "step", "=", "0", "\n", "for", "data", ",", "target", "in", "train_loader", ":", "\n", "        ", "data", ",", "target", "=", "data", ".", "to", "(", "device", ")", ",", "target", ".", "to", "(", "device", ")", "\n", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "output", "=", "model", "(", "data", ")", "\n", "span_loss", "=", "model", ".", "module", ".", "get_span_l1", "(", "args", ")", "\n", "loss", "=", "criterion", "(", "output", ",", "target", ")", "+", "args", ".", "span_penalty", "*", "span_loss", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "if", "args", ".", "adaptive_span", ":", "\n", "            ", "model", ".", "module", ".", "clamp_span", "(", ")", "\n", "\n", "", "y_pred", "=", "output", ".", "data", ".", "max", "(", "1", ")", "[", "1", "]", "\n", "\n", "acc", "=", "float", "(", "y_pred", ".", "eq", "(", "target", ".", "data", ")", ".", "sum", "(", ")", ")", "/", "len", "(", "data", ")", "*", "100.", "\n", "train_acc", "+=", "acc", "\n", "step", "+=", "1", "\n", "if", "step", "%", "args", ".", "print_interval", "==", "0", ":", "\n", "            ", "print", "(", "\"[Epoch {0:4d}] Loss: {1:2.3f} Acc: {2:.3f}%\"", ".", "format", "(", "epoch", ",", "loss", ".", "data", ",", "acc", ")", ")", "\n", "#logger.info(\"[Epoch {0:4d}] Loss: {1:2.3f} Acc: {2:.3f}%\".format(epoch, loss.data, acc))", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.main.eval": [[69, 85], ["print", "model.eval", "print", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "len", "model", "prediction.eq().sum", "float", "data.to", "target.to", "model.data.max", "prediction.eq"], "function", ["home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.main.eval"], ["", "", "", "def", "eval", "(", "model", ",", "test_loader", ",", "args", ",", "is_valid", "=", "True", ",", "device", "=", "None", ")", ":", "\n", "    ", "print", "(", "'evaluation ...'", ")", "\n", "model", ".", "eval", "(", ")", "\n", "correct", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "data", ",", "target", "in", "test_loader", ":", "\n", "            ", "data", ",", "target", "=", "data", ".", "to", "(", "device", ")", ",", "target", ".", "to", "(", "device", ")", "\n", "\n", "output", "=", "model", "(", "data", ")", "\n", "prediction", "=", "output", ".", "data", ".", "max", "(", "1", ")", "[", "1", "]", "\n", "correct", "+=", "prediction", ".", "eq", "(", "target", ".", "data", ")", ".", "sum", "(", ")", "\n", "\n", "", "", "acc", "=", "100.", "*", "float", "(", "correct", ")", "/", "len", "(", "test_loader", ".", "dataset", ")", "\n", "data_set", "=", "'Validation'", "if", "is_valid", "else", "\"Test\"", "\n", "print", "(", "data_set", "+", "' acc: {0:.2f}'", ".", "format", "(", "acc", ")", ")", "\n", "return", "acc", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.main.get_model_parameters": [[87, 95], ["list", "model.parameters", "list", "layer.size"], "function", ["None"], ["", "def", "get_model_parameters", "(", "model", ")", ":", "\n", "    ", "total_parameters", "=", "0", "\n", "for", "layer", "in", "list", "(", "model", ".", "parameters", "(", ")", ")", ":", "\n", "        ", "layer_parameter", "=", "1", "\n", "for", "l", "in", "list", "(", "layer", ".", "size", "(", ")", ")", ":", "\n", "            ", "layer_parameter", "*=", "l", "\n", "", "total_parameters", "+=", "layer_parameter", "\n", "", "return", "total_parameters", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.main.main": [[97, 256], ["preprocess.load_data", "len", "print", "print", "torch.optim.lr_scheduler.CosineAnnealingLR", "torch.optim.lr_scheduler.CosineAnnealingLR", "torch.optim.lr_scheduler.CosineAnnealingLR", "torch.optim.lr_scheduler.CosineAnnealingLR", "torch.device", "torch.device", "torch.device", "torch.device", "print", "file_writer.FileWriter", "print", "torch.CrossEntropyLoss", "range", "file_writer.FileWriter.close", "print", "model.ResNet26", "torch.Adam", "torch.SGD", "print", "torch.load", "torch.load", "torch.load", "torch.load", "torch.DataParallel", "model.ResNet50.load_state_dict", "print", "model.ResNet50.to", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "dummy_input.to.to", "thop.profile", "print", "optim.SGD.load_state_dict", "torch.optim.lr_scheduler.CosineAnnealingLR.load_state_dict", "print", "torch.DataParallel", "model.ResNet50.to", "main.get_model_parameters", "print", "time.time", "main.train", "print", "main.eval", "dict", "file_writer.FileWriter.log", "max", "main.get_model_parameters", "print", "model.ResNet38", "model.ResNet50.parameters", "model.ResNet50.parameters", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "copy.deepcopy", "print", "main.eval", "print", "str", "os.path.dirname", "main.adjust_learning_rate", "os.path.isdir", "os.mkdir", "print", "torch.save", "torch.save", "torch.save", "torch.save", "print", "model.ResNet50", "str", "os.path.abspath", "time.time", "int", "print", "model.ResNet50.state_dict", "optim.SGD.state_dict", "torch.optim.lr_scheduler.CosineAnnealingLR.state_dict", "block.conv2[].adaptive_mask.get_current_max_size", "max_spans.append", "str", "str", "torch.optim.lr_scheduler.CosineAnnealingLR.step"], "function", ["home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.preprocess.load_data", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.file_writer.FileWriter.close", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.model.ResNet26", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.model.get_model_parameters", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.main.train", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.main.eval", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.file_writer.FileWriter.log", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.model.get_model_parameters", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.model.ResNet38", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.main.eval", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.main.adjust_learning_rate", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.model.ResNet50", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention.AdaptiveMask.get_current_max_size"], ["", "def", "main", "(", "args", ",", "logger", ")", ":", "\n", "    ", "train_loader", ",", "valid_loader", ",", "test_loader", "=", "load_data", "(", "args", ")", "\n", "\n", "# TODO : THIS WILL NOT WORK OUT FOR OTHER DATASETS DUE TO THE WAY WE SPLIT WITH SUBSET FLAG IN CIFAR100", "\n", "num_classes", "=", "len", "(", "train_loader", ".", "dataset", ".", "dataset", ".", "dataset", ".", "classes", ")", "\n", "\n", "# if args.dataset == 'CIFAR10':", "\n", "#     num_classes = 10", "\n", "# elif args.dataset == 'CIFAR100':", "\n", "#     num_classes = 100", "\n", "# elif args.dataset == 'TinyImageNet':", "\n", "#     num_classes = 200", "\n", "\n", "print", "(", "'dataset: {}, num_classes: {}'", ".", "format", "(", "args", ".", "dataset", ",", "num_classes", ")", ")", "\n", "model", "=", "None", "\n", "print", "(", "'ARGS: '", ",", "args", ")", "\n", "if", "args", ".", "model_name", "==", "'ResNet26'", ":", "\n", "        ", "print", "(", "'Model Name: {0}'", ".", "format", "(", "args", ".", "model_name", ")", ")", "\n", "model", "=", "ResNet26", "(", "num_classes", "=", "num_classes", ",", "args", "=", "args", ")", "\n", "", "elif", "args", ".", "model_name", "==", "'ResNet38'", ":", "\n", "        ", "print", "(", "'Model Name: {0}'", ".", "format", "(", "args", ".", "model_name", ")", ")", "\n", "model", "=", "ResNet38", "(", "num_classes", "=", "num_classes", ",", "all_attention", "=", "args", ".", "all_attention", ")", "\n", "", "elif", "args", ".", "model_name", "==", "'ResNet50'", ":", "\n", "        ", "print", "(", "'Model Name: {0}'", ".", "format", "(", "args", ".", "model_name", ")", ")", "\n", "model", "=", "ResNet50", "(", "num_classes", "=", "num_classes", ",", "all_attention", "=", "args", ".", "all_attention", ")", "\n", "\n", "", "if", "args", ".", "use_adam", ":", "\n", "        ", "optimizer", "=", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "args", ".", "adam_lr", ")", "# Try altering initial settings of Adam later.", "\n", "", "else", ":", "\n", "        ", "optimizer", "=", "optim", ".", "SGD", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "args", ".", "lr", ",", "momentum", "=", "args", ".", "momentum", ",", "\n", "weight_decay", "=", "args", ".", "weight_decay", ",", "nesterov", "=", "True", ")", "\n", "\n", "", "if", "args", ".", "T_max", "==", "-", "1", ":", "\n", "        ", "args", ".", "T_max", "=", "args", ".", "epochs", "\n", "", "scheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "CosineAnnealingLR", "(", "optimizer", ",", "\n", "T_max", "=", "args", ".", "T_max", ",", "\n", "eta_min", "=", "args", ".", "eta_min", ")", "\n", "\n", "start_epoch", "=", "1", "\n", "best_acc", "=", "0.0", "\n", "best_epoch", "=", "1", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda:0\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "if", "args", ".", "pretrained_model", "or", "args", ".", "test", ":", "\n", "        ", "filename", "=", "args", ".", "xpid", "+", "'_model_'", "+", "str", "(", "args", ".", "dataset", ")", "+", "'_'", "+", "str", "(", "args", ".", "model_name", ")", "+", "'_ckpt.tar'", "\n", "print", "(", "'filename :: '", ",", "filename", ")", "\n", "\n", "map_location", "=", "'cuda'", "if", "args", ".", "cuda", "else", "None", "\n", "checkpoint", "=", "torch", ".", "load", "(", "filename", ",", "map_location", "=", "map_location", ")", "\n", "\n", "model", "=", "nn", ".", "DataParallel", "(", "model", ")", "\n", "model", ".", "load_state_dict", "(", "checkpoint", "[", "'state_dict'", "]", ")", "\n", "print", "(", "'MADE IT'", ")", "\n", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "\n", "dummy_input", "=", "torch", ".", "randn", "(", "(", "2", ",", "3", ",", "32", ",", "32", ")", ")", "\n", "dummy_input", "=", "dummy_input", ".", "to", "(", "device", ")", "\n", "macs", ",", "params", "=", "profile", "(", "copy", ".", "deepcopy", "(", "model", ".", "module", ")", ",", "inputs", "=", "(", "dummy_input", ",", ")", ",", "custom_ops", "=", "{", "Bottleneck", ":", "count_bootleneck", "}", ",", "verbose", "=", "True", ")", "\n", "print", "(", "'FLOPS : {}, PARAMS : {}'", ".", "format", "(", "macs", ",", "params", ")", ")", "\n", "\n", "max_spans", "=", "[", "]", "\n", "if", "args", ".", "adaptive_span", ":", "\n", "#print out max z value per layer as a list", "\n", "            ", "for", "layer", "in", "model", ".", "module", ".", "layers", ":", "\n", "                ", "for", "block", "in", "layer", ":", "\n", "                    ", "max_span", "=", "block", ".", "conv2", "[", "0", "]", ".", "adaptive_mask", ".", "get_current_max_size", "(", ")", "\n", "max_spans", ".", "append", "(", "max_span", ")", "\n", "\n", "", "", "print", "(", "'MAX SPANS: '", ",", "max_spans", ")", "\n", "\n", "", "optimizer", ".", "load_state_dict", "(", "checkpoint", "[", "'optimizer'", "]", ")", "\n", "# reset to this learning rate given", "\n", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "args", ".", "lr", "\n", "\n", "scheduler", ".", "load_state_dict", "(", "checkpoint", "[", "'scheduler'", "]", ")", "\n", "\n", "start_epoch", "=", "checkpoint", "[", "'epoch'", "]", "\n", "best_acc", "=", "checkpoint", "[", "'best_acc'", "]", "\n", "best_epoch", "=", "start_epoch", "\n", "model_parameters", "=", "checkpoint", "[", "'parameters'", "]", "\n", "print", "(", "'Load model, Parameters: {0}, Start_epoch: {1}, Acc: {2}'", ".", "format", "(", "model_parameters", ",", "start_epoch", ",", "best_acc", ")", ")", "\n", "#logger.info('Load model, Parameters: {0}, Start_epoch: {1}, Acc: {2}'.format(model_parameters, start_epoch, best_acc))", "\n", "\n", "if", "args", ".", "test", ":", "\n", "#Compute test accuracy", "\n", "\n", "            ", "test_acc", "=", "eval", "(", "model", ",", "test_loader", ",", "args", ",", "is_valid", "=", "False", ",", "device", "=", "device", ")", "\n", "print", "(", "'TEST ACCURACY: '", ",", "test_acc", ")", "\n", "return", "\n", "\n", "\n", "", "", "if", "not", "args", ".", "pretrained_model", ":", "\n", "\n", "        ", "model", "=", "nn", ".", "DataParallel", "(", "model", ")", "\n", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "\n", "", "print", "(", "\"Number of model parameters: \"", ",", "get_model_parameters", "(", "model", ")", ")", "\n", "#logger.info(\"Number of model parameters: {0}\".format(get_model_parameters(model)))", "\n", "\n", "filename", "=", "args", ".", "xpid", "+", "'_model_'", "+", "str", "(", "args", ".", "dataset", ")", "+", "'_'", "+", "str", "(", "args", ".", "model_name", ")", "+", "'_ckpt.tar'", "\n", "plogger", "=", "file_writer", ".", "FileWriter", "(", "\n", "xpid", "=", "args", ".", "xpid", ",", "rootdir", "=", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "abspath", "(", "__file__", ")", ")", "\n", ")", "\n", "print", "(", "'will save model as filename :: '", ",", "filename", ")", "\n", "\n", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "\n", "\n", "for", "epoch", "in", "range", "(", "start_epoch", ",", "args", ".", "epochs", "+", "1", ")", ":", "\n", "\n", "        ", "if", "args", ".", "all_attention", "or", "args", ".", "attention_conv", "or", "args", ".", "force_cosine_annealing", ":", "\n", "            ", "if", "args", ".", "no_annealing", ":", "\n", "                ", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "args", ".", "lr", "\n", "", "elif", "epoch", "<", "args", ".", "warmup_epochs", ":", "\n", "                ", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "                    ", "param_group", "[", "'lr'", "]", "=", "args", ".", "lr", "*", "(", "epoch", "+", "1", ")", "/", "args", ".", "warmup_epochs", "\n", "\n", "", "", "elif", "epoch", ">=", "args", ".", "start_scheduler", ":", "\n", "                ", "scheduler", ".", "step", "(", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "adjust_learning_rate", "(", "optimizer", ",", "epoch", ",", "args", ")", "\n", "", "learning_rate", "=", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "# learning_rate = [x['lr'] for x in optimizer.param_groups]", "\n", "print", "(", "'Updated lr: '", ",", "learning_rate", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "train", "(", "model", ",", "train_loader", ",", "optimizer", ",", "criterion", ",", "epoch", ",", "args", ",", "logger", ",", "device", ")", "\n", "print", "(", "'Epoch took: '", ",", "time", ".", "time", "(", ")", "-", "start_time", ")", "\n", "\n", "eval_acc", "=", "eval", "(", "model", ",", "valid_loader", ",", "args", ",", "is_valid", "=", "True", ",", "device", "=", "device", ")", "\n", "to_log", "=", "dict", "(", "accuracy", "=", "eval_acc", ",", "learning_rate", "=", "learning_rate", ")", "\n", "plogger", ".", "log", "(", "to_log", ")", "\n", "\n", "is_best", "=", "eval_acc", ">", "best_acc", "\n", "best_acc", "=", "max", "(", "eval_acc", ",", "best_acc", ")", "\n", "if", "is_best", ":", "\n", "            ", "best_epoch", "=", "epoch", "\n", "", "elif", "epoch", "-", "best_epoch", ">", "int", "(", "args", ".", "epochs", "*", "0.2", ")", ":", "\n", "            ", "print", "(", "'EARLY STOPPING'", ")", "\n", "break", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "isdir", "(", "'checkpoint'", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "'checkpoint'", ")", "\n", "\n", "", "parameters", "=", "get_model_parameters", "(", "model", ")", "\n", "\n", "if", "is_best", ":", "\n", "            ", "print", "(", "'Saving best model'", ")", "\n", "state", "=", "{", "\n", "'epoch'", ":", "epoch", ",", "\n", "'arch'", ":", "args", ".", "model_name", ",", "\n", "'state_dict'", ":", "model", ".", "state_dict", "(", ")", ",", "\n", "'best_acc'", ":", "best_acc", ",", "\n", "'optimizer'", ":", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "'scheduler'", ":", "scheduler", ".", "state_dict", "(", ")", ",", "\n", "'parameters'", ":", "parameters", ",", "\n", "}", "\n", "torch", ".", "save", "(", "state", ",", "filename", ")", "\n", "", "", "plogger", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention.AdaptiveMask.__init__": [[27, 35], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "attention.AdaptiveMask.register_buffer", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "int", "int"], "methods", ["home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.file_writer.FileWriter.__init__", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.file_writer.FileWriter.__init__", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.file_writer.FileWriter.__init__", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.file_writer.FileWriter.__init__"], ["def", "__init__", "(", "self", ",", "max_size", ",", "ramp_size", ",", "init_val", "=", "0", ",", "shape", "=", "(", "1", ",", ")", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "self", ".", "_max_size", "=", "max_size", "\n", "self", ".", "_ramp_size", "=", "ramp_size", "\n", "self", ".", "current_val", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "*", "shape", ")", "+", "init_val", ")", "\n", "# print('INII ', self.current_val.device)", "\n", "mask_template", "=", "torch", ".", "linspace", "(", "1", "-", "int", "(", "max_size", ")", ",", "0", ",", "steps", "=", "int", "(", "max_size", ")", ")", "\n", "self", ".", "register_buffer", "(", "'mask_template'", ",", "mask_template", ")", "\n", "# self.mask_template = torch.linspace(1 - int(max_size), 0, steps=int(max_size))", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention.AdaptiveMask.forward": [[37, 79], ["one_d_mask.clamp.clamp.clamp", "one_d_mask.clamp.clamp.new_ones", "range", "mask.view.view.view", "zip", "one_d_mask[].unsqueeze", "x.sum", "range", "range", "range", "range"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "mask_len", ")", ":", "\n", "\n", "# self.current_val should be a fraction", "\n", "# import pdb", "\n", "# pdb.set_trace()", "\n", "# print('device : ', self.mask_template.device, self.current_val.device)", "\n", "        ", "one_d_mask", "=", "self", ".", "mask_template", "+", "self", ".", "current_val", "*", "self", ".", "_max_size", "\n", "one_d_mask", "=", "one_d_mask", "/", "self", ".", "_ramp_size", "+", "1", "\n", "one_d_mask", "=", "one_d_mask", ".", "clamp", "(", "0", ",", "1", ")", "\n", "# TODO Debug: Check that indexing right dim, this should be relative to x size", "\n", "#              if kernel is 3x3 then would expect x.shape[-1] to be 3", "\n", "\n", "# This line is to do the computation only for the mask_len size which are non zeros. Avoiding some compute here", "\n", "one_d_mask", "=", "one_d_mask", "[", ":", ",", "-", "mask_len", ":", "]", "\n", "kernel_size", "=", "2", "*", "mask_len", "+", "1", "\n", "# Now masking 'out' (how do we count distance? One way is to start at the center pixel of the kernel and", "\n", "# work outwards one square around it at a time filling in the mask.", "\n", "# For ex: the adjacent pixels to center pixel have same masking weight. Now pixels outside of those that are", "\n", "# adjacent have same weight and so on.", "\n", "mask", "=", "one_d_mask", ".", "new_ones", "(", "(", "self", ".", "current_val", ".", "shape", "[", "0", "]", ",", "kernel_size", ",", "kernel_size", ")", ")", "\n", "left", ",", "right", "=", "0", ",", "kernel_size", "-", "1", "\n", "\n", "for", "i", "in", "range", "(", "one_d_mask", ".", "shape", "[", "1", "]", ")", ":", "\n", "            ", "bottom", ",", "top", "=", "left", ",", "right", "\n", "indices", "=", "[", "[", "j", ",", "left", "]", "for", "j", "in", "range", "(", "bottom", ",", "top", "+", "1", ")", "]", "# left edge indices", "\n", "indices", "+=", "[", "[", "bottom", ",", "j", "]", "for", "j", "in", "range", "(", "left", "+", "1", ",", "right", "+", "1", ")", "]", "# bottom edge minus overlap with left", "\n", "indices", "+=", "[", "[", "top", ",", "j", "]", "for", "j", "in", "range", "(", "left", "+", "1", ",", "right", "+", "1", ")", "]", "# top minus overlap with left", "\n", "indices", "+=", "[", "[", "j", ",", "right", "]", "for", "j", "in", "range", "(", "bottom", "+", "1", ",", "top", ")", "]", "# right minus overlap with bottom and top", "\n", "rows", ",", "cols", "=", "zip", "(", "*", "indices", ")", "\n", "\n", "mask", "[", ":", ",", "rows", ",", "cols", "]", "=", "one_d_mask", "[", ":", ",", "i", "]", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "left", "+=", "1", "\n", "right", "-=", "1", "\n", "\n", "\n", "", "mask", "=", "mask", ".", "view", "(", "1", ",", "mask", ".", "shape", "[", "0", "]", ",", "1", ",", "1", ",", "-", "1", ")", "\n", "x", "=", "x", "*", "mask", "\n", "\n", "x", "=", "x", "/", "(", "x", ".", "sum", "(", "-", "1", ",", "keepdim", "=", "True", ")", "+", "1e-8", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention.AdaptiveMask.get_current_max_size": [[80, 86], ["math.ceil", "max", "min", "attention.AdaptiveMask.current_val.detach().max().item", "attention.AdaptiveMask.current_val.detach().max", "attention.AdaptiveMask.current_val.detach"], "methods", ["None"], ["", "def", "get_current_max_size", "(", "self", ",", "include_ramp", "=", "True", ")", ":", "\n", "        ", "current_size", "=", "math", ".", "ceil", "(", "self", ".", "current_val", ".", "detach", "(", ")", ".", "max", "(", ")", ".", "item", "(", ")", "*", "self", ".", "_max_size", ")", "\n", "if", "include_ramp", ":", "\n", "            ", "current_size", "+=", "self", ".", "_ramp_size", "\n", "", "current_size", "=", "max", "(", "0", ",", "min", "(", "self", ".", "_max_size", ",", "current_size", ")", ")", "\n", "return", "current_size", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention.AdaptiveMask.get_current_avg_size": [[87, 93], ["math.ceil", "max", "min", "attention.AdaptiveMask.current_val.mean().item", "attention.AdaptiveMask.current_val.mean"], "methods", ["None"], ["", "def", "get_current_avg_size", "(", "self", ",", "include_ramp", "=", "True", ")", ":", "\n", "        ", "current_size", "=", "math", ".", "ceil", "(", "self", ".", "current_val", ".", "mean", "(", ")", ".", "item", "(", ")", "*", "self", ".", "_max_size", ")", "\n", "if", "include_ramp", ":", "\n", "            ", "current_size", "+=", "self", ".", "_ramp_size", "\n", "", "current_size", "=", "max", "(", "0", ",", "min", "(", "self", ".", "_max_size", ",", "current_size", ")", ")", "\n", "return", "current_size", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention.AdaptiveMask.clamp_param": [[94, 97], ["attention.AdaptiveMask.current_val.data.clamp_"], "methods", ["None"], ["", "def", "clamp_param", "(", "self", ")", ":", "\n", "        ", "\"\"\"this need to be called after each update\"\"\"", "\n", "self", ".", "current_val", ".", "data", ".", "clamp_", "(", "0", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention.AttentionConv.__init__": [[103, 139], ["torch.Module.__init__", "attention.AdaptiveMask", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "attention.AttentionConv.reset_parameters", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.file_writer.FileWriter.__init__", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention.AttentionConv.reset_parameters"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", ",", "padding", "=", "0", ",", "groups", "=", "1", ",", "bias", "=", "False", ",", "\n", "R", "=", "3", ",", "z_init", "=", "0.3", ",", "image_size", "=", "32", ",", "adaptive_span", "=", "False", ")", ":", "\n", "\n", "        ", "super", "(", "AttentionConv", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "out_channels", "=", "out_channels", "\n", "\n", "# Note\u2013 we will assume a kernel_size x kernel_size context window for applying attention.", "\n", "# TODO: Debug: may want to use relative cosine embeddings instead so param count not so", "\n", "#               large when kernel size is large in adaptive span.", "\n", "# Reason for large kernel_size is that need relative embeddings to be large enough if our", "\n", "# model ends up wanting to attend to very large kernels. (easier if this is odd number later on)", "\n", "self", ".", "kernel_size", "=", "image_size", "+", "1", "if", "adaptive_span", "else", "kernel_size", "\n", "self", ".", "adaptive_span", "=", "adaptive_span", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "padding", "=", "padding", "\n", "self", ".", "groups", "=", "groups", "\n", "\n", "assert", "self", ".", "out_channels", "%", "self", ".", "groups", "==", "0", ",", "\"out_channels should be divisible by groups. (example: out_channels: 40, groups: 4)\"", "\n", "\n", "\n", "max_mask_size", "=", "image_size", "/", "2", "# TODO(Joe): Our images are all even sizes now so this works but we should force this to be an int, i.e. int(image_size / 2) or image_size // 2", "\n", "\n", "# TODO HIGH : This value of 5 is hardcoded for CIFAR100", "\n", "self", ".", "adaptive_mask", "=", "AdaptiveMask", "(", "5", ",", "R", ",", "init_val", "=", "z_init", ",", "shape", "=", "(", "groups", ",", "1", ")", ")", "\n", "\n", "# Note the different usage of kernel_size for rel_w and rel_h. They are 2 one dimensional arrays", "\n", "# Reason they divide by two is that in the paper they just concat rel_h and rel_w to be the positional", "\n", "# embedding vector", "\n", "self", ".", "rel_h", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "out_channels", "//", "2", ",", "1", ",", "1", ",", "self", ".", "kernel_size", ",", "1", ")", ",", "requires_grad", "=", "True", ")", "\n", "self", ".", "rel_w", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "out_channels", "//", "2", ",", "1", ",", "1", ",", "1", ",", "self", ".", "kernel_size", ")", ",", "requires_grad", "=", "True", ")", "\n", "\n", "self", ".", "key_conv", "=", "nn", ".", "Conv2d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "1", ",", "bias", "=", "bias", ")", "\n", "self", ".", "query_conv", "=", "nn", ".", "Conv2d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "1", ",", "bias", "=", "bias", ")", "\n", "self", ".", "value_conv", "=", "nn", ".", "Conv2d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "1", ",", "bias", "=", "bias", ")", "\n", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention.AttentionConv.forward": [[140, 245], ["x.size", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "attention.AttentionConv.query_conv", "attention.AttentionConv.key_conv", "attention.AttentionConv.value_conv", "k_out.contiguous().view.contiguous().view.unfold().unfold", "v_out.contiguous().view.contiguous().view.unfold().unfold", "time.time", "k_out.contiguous().view.contiguous().view.split", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "k_out.contiguous().view.contiguous().view.contiguous().view", "v_out.contiguous().view.contiguous().view.contiguous().view", "q_out.view.view.view", "time.time", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "attention.AttentionConv.adaptive_mask.get_current_max_size", "int", "int", "attention.AttentionConv.adaptive_mask", "k_out.contiguous().view.contiguous().view.unfold", "v_out.contiguous().view.contiguous().view.unfold", "k_out.contiguous().view.contiguous().view.contiguous", "v_out.contiguous().view.contiguous().view.contiguous", "int", "attention.AttentionConv.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention.AdaptiveMask.get_current_max_size"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "        ", "'''\n        Comment on 2d masking:\n        Have defined z and R as follows in this 2D case:\n        z: if have center pixel in kernel, then z is number of pixels to the right of this center pixel that\n            get a mask of 1, NOTE: This is a float (ie can be say 2.5 pixels away from center).\n            An example is if have 3x3 kernel which should not be masked, then z=2\n\n        R: is buffer around the non masked kernel to give soft masking which makes this differentiable (ie ramplength).\n            For ex: If R=1, and we want a 3x3 kernel to not be masked, then anything outside the 4x4 kernel\n            gets attention weight of 0.\n\n        Based on mask, we choose min kernel size we need to compute, and then pad x accordingly. To keep shape,\n        we just need padding=(kernel_size-1)/2 so need to choose kernel_size to be odd.\n\n        TODO: Add L1 regularizer for masking vars\n\n        When we add relative embeddings, look from center of rel_h and rel_w outwards to do this properly.\n        '''", "\n", "\n", "batch", ",", "channels", ",", "height", ",", "width", "=", "x", ".", "size", "(", ")", "\n", "max_size", "=", "None", "\n", "if", "self", ".", "adaptive_span", ":", "\n", "# print('z value ',self.adaptive_mask.current_val)", "\n", "            ", "max_size", "=", "self", ".", "adaptive_mask", ".", "get_current_max_size", "(", ")", "\n", "kernel_size", "=", "int", "(", "2", "*", "max_size", "+", "1", ")", "# compute smallest kernel_size we can compute based on mask", "\n", "padding", "=", "int", "(", "(", "kernel_size", "-", "1", ")", "/", "2", ")", "\n", "\n", "", "else", ":", "\n", "            ", "kernel_size", "=", "self", ".", "kernel_size", "\n", "padding", "=", "self", ".", "padding", "\n", "\n", "", "padded_x", "=", "F", ".", "pad", "(", "x", ",", "[", "padding", ",", "padding", ",", "padding", ",", "padding", "]", ")", "\n", "# the query should come from the pixel under consideration, while the keys and values should come from the", "\n", "# context window", "\n", "q_out", "=", "self", ".", "query_conv", "(", "x", ")", "\n", "k_out", "=", "self", ".", "key_conv", "(", "padded_x", ")", "\n", "v_out", "=", "self", ".", "value_conv", "(", "padded_x", ")", "\n", "\n", "# this line is dividing k_out into chunks of kernel_size with stride=self.stride", "\n", "k_out", "=", "k_out", ".", "unfold", "(", "2", ",", "kernel_size", ",", "self", ".", "stride", ")", ".", "unfold", "(", "3", ",", "kernel_size", ",", "self", ".", "stride", ")", "\n", "v_out", "=", "v_out", ".", "unfold", "(", "2", ",", "kernel_size", ",", "self", ".", "stride", ")", ".", "unfold", "(", "3", ",", "kernel_size", ",", "self", ".", "stride", ")", "\n", "#now k_out has shape (bsz, out_channels, height, width,3,3) where kernel =(3,3) (so has keys for each block which makes it easy to apply attention)", "\n", "\n", "#now we add relative height to the first half of the output channels and relative width to the second half", "\n", "if", "self", ".", "adaptive_span", ":", "\n", "# now we add relative height to the first half of the output channels and relative width to the second half", "\n", "# Index these relatives based on kernel size (we know kernel size is odd and length of self.rel_h is odd", "\n", "# so can start at center element of self.rel_h and work outwards in both directions equally until", "\n", "# using kernel_size elements.", "\n", "# TODO Debug: Ensure correctness of this indexing", "\n", "            ", "start_ind", "=", "(", "self", ".", "kernel_size", "//", "2", ")", "-", "(", "kernel_size", "//", "2", ")", "# remember self.kernel_size != kernel_size", "\n", "end_ind", "=", "(", "self", ".", "kernel_size", "//", "2", ")", "+", "(", "kernel_size", "//", "2", ")", "\n", "rel_h", "=", "self", ".", "rel_h", "[", ":", ",", ":", ",", ":", ",", "start_ind", ":", "end_ind", "+", "1", ",", ":", "]", "\n", "rel_w", "=", "self", ".", "rel_w", "[", ":", ",", ":", ",", ":", ",", ":", ",", "start_ind", ":", "end_ind", "+", "1", "]", "\n", "", "else", ":", "\n", "            ", "rel_h", "=", "self", ".", "rel_h", "\n", "rel_w", "=", "self", ".", "rel_w", "\n", "\n", "#TIME these splits", "\n", "", "starttime", "=", "time", ".", "time", "(", ")", "\n", "k_out_h", ",", "k_out_w", "=", "k_out", ".", "split", "(", "self", ".", "out_channels", "//", "2", ",", "dim", "=", "1", ")", "\n", "# print('shapes : k_out_h : {} rel_h : {} k_out_w : {} rel_w : {}'.format(k_out_h.shape, rel_h.shape, k_out_w.shape, rel_w.shape))", "\n", "k_out", "=", "torch", ".", "cat", "(", "(", "k_out_h", "+", "rel_h", ",", "k_out_w", "+", "rel_w", ")", ",", "dim", "=", "1", ")", "\n", "#print('time: ', time.time())", "\n", "#print('here')", "\n", "\n", "#for now suppose groups is 1, RETHINK THIS IF NOT", "\n", "#this operation just flattens the kernels in the last two dimensions (does this properly from example I did)", "\n", "# This operation also divides the k_out, and v_out among the different heads.", "\n", "k_out", "=", "k_out", ".", "contiguous", "(", ")", ".", "view", "(", "batch", ",", "self", ".", "groups", ",", "self", ".", "out_channels", "//", "self", ".", "groups", ",", "height", ",", "width", ",", "-", "1", ")", "\n", "v_out", "=", "v_out", ".", "contiguous", "(", ")", ".", "view", "(", "batch", ",", "self", ".", "groups", ",", "self", ".", "out_channels", "//", "self", ".", "groups", ",", "height", ",", "width", ",", "-", "1", ")", "\n", "\n", "q_out", "=", "q_out", ".", "view", "(", "batch", ",", "self", ".", "groups", ",", "self", ".", "out_channels", "//", "self", ".", "groups", ",", "height", ",", "width", ",", "1", ")", "\n", "\n", "# ORINGINAL IMPLEMENTATION", "\n", "# This is about 3x slower than our new implementation below for 1 group", "\n", "#start_time = time.time()", "\n", "#out = q_out * k_out", "\n", "#out = F.softmax(out, dim=-1)", "\n", "#out = torch.einsum('bnchwk,bnchwk -> bnchw', out, v_out).view(batch, -1, height, width)", "\n", "#print('Attention took: ', time.time()-start_time)", "\n", "# END ORIGINAL IMPLEMENTATOIN", "\n", "\n", "#OUR IMPLEMENTATION (DOES NOT WORK WITH groups > 1)", "\n", "# Why does orginal implementation work with many groups and ours does not?", "\n", "#I think way to do this is is multiply (broadcast over last dimension) then sum dim=2 (acts as dot product)", "\n", "#TO DO: Check that this still works with groups > 1 (I think may need to do a flattening after in this case)", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "out", "=", "(", "q_out", "*", "k_out", ")", ".", "sum", "(", "dim", "=", "2", ")", "# Original", "\n", "# All the channels are being merged into 1", "\n", "#out = (q_out*k_out).sum(dim=2).squeeze(dim=1)", "\n", "\n", "out2", "=", "F", ".", "softmax", "(", "out", ",", "dim", "=", "-", "1", ")", "\n", "if", "self", ".", "adaptive_span", ":", "\n", "#Note: Applying after softmax and then renormalize after mask", "\n", "            ", "out2", "=", "self", ".", "adaptive_mask", "(", "out2", ",", "int", "(", "max_size", ")", ")", "\n", "\n", "#out3 = (out2.unsqueeze(dim=2) * v_out).sum(dim=-1).squeeze(dim=1) #Check if can condense this in one einstein", "\n", "", "out3", "=", "(", "out2", ".", "unsqueeze", "(", "dim", "=", "2", ")", "*", "v_out", ")", ".", "sum", "(", "dim", "=", "-", "1", ")", ".", "view", "(", "batch", ",", "-", "1", ",", "height", ",", "width", ")", "\n", "\n", "#print('Attention took: ', time.time()-start_time)", "\n", "\n", "return", "out3", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention.AttentionConv.reset_parameters": [[247, 254], ["torch.kaiming_normal_", "torch.kaiming_normal_", "torch.kaiming_normal_", "torch.kaiming_normal_", "torch.kaiming_normal_", "torch.kaiming_normal_", "torch.kaiming_normal_", "torch.kaiming_normal_", "torch.kaiming_normal_", "torch.kaiming_normal_", "torch.kaiming_normal_", "torch.kaiming_normal_", "torch.normal_", "torch.normal_", "torch.normal_", "torch.normal_", "torch.normal_", "torch.normal_", "torch.normal_", "torch.normal_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "init", ".", "kaiming_normal_", "(", "self", ".", "key_conv", ".", "weight", ",", "mode", "=", "'fan_out'", ",", "nonlinearity", "=", "'relu'", ")", "\n", "init", ".", "kaiming_normal_", "(", "self", ".", "value_conv", ".", "weight", ",", "mode", "=", "'fan_out'", ",", "nonlinearity", "=", "'relu'", ")", "\n", "init", ".", "kaiming_normal_", "(", "self", ".", "query_conv", ".", "weight", ",", "mode", "=", "'fan_out'", ",", "nonlinearity", "=", "'relu'", ")", "\n", "\n", "init", ".", "normal_", "(", "self", ".", "rel_h", ",", "0", ",", "1", ")", "\n", "init", ".", "normal_", "(", "self", ".", "rel_w", ",", "0", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.flop_count.count_conv2d": [[11, 17], ["torch.zeros().numel", "torch.zeros().numel", "m", "m.nelement", "torch.zeros", "torch.zeros", "m.weight.size"], "function", ["None"], ["def", "count_conv2d", "(", "m", ",", "x", ")", ":", "\n", "    ", "kernel_ops", "=", "torch", ".", "zeros", "(", "m", ".", "weight", ".", "size", "(", ")", "[", "2", ":", "]", ")", ".", "numel", "(", ")", "# Kw x Kh", "\n", "# N x Cout x H x W x  (Cin x Kw x Kh + bias)", "\n", "y", "=", "m", "(", "x", ")", "\n", "total_ops", "=", "y", ".", "nelement", "(", ")", "*", "(", "m", ".", "in_channels", "//", "m", ".", "groups", "*", "kernel_ops", ")", "\n", "return", "total_ops", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.flop_count.count_softmax": [[19, 26], ["x.size"], "function", ["None"], ["", "def", "count_softmax", "(", "x", ")", ":", "\n", "    ", "batch_size", ",", "nfeatures", "=", "x", ".", "size", "(", ")", "\n", "total_exp", "=", "nfeatures", "\n", "total_add", "=", "nfeatures", "-", "1", "\n", "total_div", "=", "nfeatures", "\n", "total_ops", "=", "batch_size", "*", "(", "total_exp", "+", "total_add", "+", "total_div", ")", "\n", "return", "total_ops", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.flop_count.count_adaptive_flops": [[28, 67], ["torch.ones", "torch.ones", "x.numel", "m.mask_template.numel", "one_d_mask.numel", "x.sum", "x[].numel", "flop_count.count_softmax", "m.current_val.numel"], "function", ["home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.flop_count.count_softmax"], ["", "def", "count_adaptive_flops", "(", "m", ",", "x", ",", "mask_len", ")", ":", "\n", "    ", "total_ops", "=", "0", "\n", "one_d_mask", "=", "m", ".", "mask_template", "+", "m", ".", "current_val", "*", "m", ".", "_max_size", "\n", "# TODO HIGH Shakti : check if flops holds for multiple groups!", "\n", "total_ops", "=", "total_ops", "+", "1", "*", "m", ".", "current_val", ".", "numel", "(", ")", "+", "m", ".", "mask_template", ".", "numel", "(", ")", "\n", "\n", "# one_d_mask = one_d_mask / m._ramp_size + 1", "\n", "total_ops", "=", "total_ops", "+", "one_d_mask", ".", "numel", "(", ")", "\n", "\n", "# one_d_mask = one_d_mask.clamp(0, 1)", "\n", "# one_d_mask = one_d_mask[:, -mask_len:]", "\n", "\n", "kernel_size", "=", "2", "*", "mask_len", "+", "1", "\n", "mask", "=", "torch", ".", "ones", "(", "kernel_size", ",", "kernel_size", ")", "\n", "\n", "# the next block has no flops usage", "\n", "# left, right = 0, kernel_size - 1", "\n", "# for i in range(one_d_mask.shape[1]):", "\n", "#     bottom, top = left, right", "\n", "#     indices = [[j, left] for j in range(bottom, top + 1)]  # left edge indices", "\n", "#     indices += [[bottom, j] for j in range(left + 1, right + 1)]  # bottom edge minus overlap with left", "\n", "#     indices += [[top, j] for j in range(left + 1, right + 1)]  # top minus overlap with left", "\n", "#     indices += [[j, right] for j in range(bottom + 1, top)]  # right minus overlap with bottom and top", "\n", "#     rows, cols = zip(*indices)", "\n", "#     mask[rows, cols] = one_d_mask[0, i]", "\n", "#", "\n", "#     left += 1", "\n", "#     right -= 1", "\n", "\n", "# mask = mask.view(1, 1, 1, 1, -1)", "\n", "\n", "# next line doesnt add any value, is only needed for flops", "\n", "# x = x * mask", "\n", "total_ops", "+=", "x", ".", "numel", "(", ")", "\n", "\n", "x", "=", "x", "/", "(", "x", ".", "sum", "(", "-", "1", ",", "keepdim", "=", "True", ")", "+", "1e-8", ")", "\n", "total_ops", "=", "total_ops", "+", "x", "[", "0", ",", ":", ",", ":", ",", ":", ",", "0", "]", ".", "numel", "(", ")", "*", "count_softmax", "(", "x", "[", ":", ",", "0", ",", "0", ",", "0", ",", ":", "]", ")", "\n", "\n", "return", "total_ops", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.flop_count.count_attention_flops": [[69, 143], ["x.size", "torch.pad", "flop_count.count_conv2d", "flop_count.count_conv2d", "flop_count.count_conv2d", "k_out.contiguous().view.unfold().unfold", "v_out.contiguous().view.unfold().unfold", "k_out.contiguous().view.split", "torch.cat", "torch.cat", "k_out.contiguous().view.contiguous().view", "v_out.contiguous().view.contiguous().view", "q_out.view.view", "torch.softmax", "flop_count.count_softmax", "m.adaptive_mask.get_current_max_size", "int", "int", "q_out.view.numel", "k_out.contiguous().view.size", "out[].numel", "flop_count.count_adaptive_flops", "k_out.contiguous().view.unfold", "v_out.contiguous().view.unfold", "k_out.contiguous().view.contiguous", "v_out.contiguous().view.contiguous", "int"], "function", ["home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.flop_count.count_conv2d", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.flop_count.count_conv2d", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.flop_count.count_conv2d", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.flop_count.count_softmax", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention.AdaptiveMask.get_current_max_size", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.flop_count.count_adaptive_flops"], ["", "def", "count_attention_flops", "(", "m", ",", "x", ")", ":", "\n", "# x = x[0]", "\n", "    ", "total_count", "=", "0", "\n", "batch", ",", "channels", ",", "height", ",", "width", "=", "x", ".", "size", "(", ")", "\n", "max_size", "=", "None", "\n", "if", "m", ".", "adaptive_span", ":", "\n", "        ", "max_size", "=", "m", ".", "adaptive_mask", ".", "get_current_max_size", "(", ")", "\n", "kernel_size", "=", "int", "(", "2", "*", "max_size", "+", "1", ")", "\n", "padding", "=", "int", "(", "(", "kernel_size", "-", "1", ")", "/", "2", ")", "\n", "\n", "", "else", ":", "\n", "        ", "kernel_size", "=", "m", ".", "kernel_size", "\n", "padding", "=", "m", ".", "padding", "\n", "\n", "", "padded_x", "=", "F", ".", "pad", "(", "x", ",", "[", "padding", ",", "padding", ",", "padding", ",", "padding", "]", ")", "\n", "\n", "total_ops", ",", "q_out", "=", "count_conv2d", "(", "m", ".", "query_conv", ",", "x", ")", "\n", "total_count", "+=", "total_ops", "\n", "# m.total_ops += torch.DoubleTensor([int(total_ops)])", "\n", "\n", "total_ops", ",", "k_out", "=", "count_conv2d", "(", "m", ".", "key_conv", ",", "padded_x", ")", "\n", "# m.total_ops += torch.DoubleTensor([int(total_ops)])", "\n", "total_count", "+=", "total_ops", "\n", "\n", "\n", "total_ops", ",", "v_out", "=", "count_conv2d", "(", "m", ".", "value_conv", ",", "padded_x", ")", "\n", "# m.total_ops += torch.DoubleTensor([int(total_ops)])", "\n", "total_count", "+=", "total_ops", "\n", "\n", "k_out", "=", "k_out", ".", "unfold", "(", "2", ",", "kernel_size", ",", "m", ".", "stride", ")", ".", "unfold", "(", "3", ",", "kernel_size", ",", "m", ".", "stride", ")", "\n", "v_out", "=", "v_out", ".", "unfold", "(", "2", ",", "kernel_size", ",", "m", ".", "stride", ")", ".", "unfold", "(", "3", ",", "kernel_size", ",", "m", ".", "stride", ")", "\n", "\n", "if", "m", ".", "adaptive_span", ":", "\n", "        ", "start_ind", "=", "(", "m", ".", "kernel_size", "//", "2", ")", "-", "(", "kernel_size", "//", "2", ")", "\n", "end_ind", "=", "(", "m", ".", "kernel_size", "//", "2", ")", "+", "(", "kernel_size", "//", "2", ")", "\n", "rel_h", "=", "m", ".", "rel_h", "[", ":", ",", ":", ",", ":", ",", "start_ind", ":", "end_ind", "+", "1", ",", ":", "]", "\n", "rel_w", "=", "m", ".", "rel_w", "[", ":", ",", ":", ",", ":", ",", ":", ",", "start_ind", ":", "end_ind", "+", "1", "]", "\n", "", "else", ":", "\n", "        ", "rel_h", "=", "m", ".", "rel_h", "\n", "rel_w", "=", "m", ".", "rel_w", "\n", "\n", "", "k_out_h", ",", "k_out_w", "=", "k_out", ".", "split", "(", "m", ".", "out_channels", "//", "2", ",", "dim", "=", "1", ")", "\n", "k_out", "=", "torch", ".", "cat", "(", "(", "k_out_h", "+", "rel_h", ",", "k_out_w", "+", "rel_w", ")", ",", "dim", "=", "1", ")", "\n", "\n", "k_out", "=", "k_out", ".", "contiguous", "(", ")", ".", "view", "(", "batch", ",", "m", ".", "groups", ",", "m", ".", "out_channels", "//", "m", ".", "groups", ",", "height", ",", "width", ",", "-", "1", ")", "\n", "v_out", "=", "v_out", ".", "contiguous", "(", ")", ".", "view", "(", "batch", ",", "m", ".", "groups", ",", "m", ".", "out_channels", "//", "m", ".", "groups", ",", "height", ",", "width", ",", "-", "1", ")", "\n", "\n", "q_out", "=", "q_out", ".", "view", "(", "batch", ",", "m", ".", "groups", ",", "m", ".", "out_channels", "//", "m", ".", "groups", ",", "height", ",", "width", ",", "1", ")", "\n", "\n", "out", "=", "(", "q_out", "*", "k_out", ")", ".", "sum", "(", "dim", "=", "2", ")", "\n", "# TODO HIGH Shakti: CHeck if this multiplication is correct?", "\n", "total_ops", "=", "q_out", ".", "numel", "(", ")", "*", "k_out", ".", "size", "(", "-", "1", ")", "\n", "# m.total_ops += torch.DoubleTensor([int(total_ops)])", "\n", "total_count", "+=", "total_ops", "\n", "\n", "out2", "=", "F", ".", "softmax", "(", "out", ",", "dim", "=", "-", "1", ")", "\n", "# get the softmax count for one batch, one set of features", "\n", "total_ops", "=", "count_softmax", "(", "out", "[", ":", ",", "0", ",", "0", ",", "0", ",", ":", "]", ")", "\n", "# now multiply with the total groups, and width x height", "\n", "total_ops", "=", "total_ops", "*", "out", "[", "0", ",", ":", ",", ":", ",", ":", ",", "0", "]", ".", "numel", "(", ")", "\n", "# m.total_ops += torch.DoubleTensor([int(total_ops)])", "\n", "total_count", "+=", "total_ops", "\n", "\n", "\n", "if", "m", ".", "adaptive_span", ":", "\n", "        ", "total_ops", "=", "count_adaptive_flops", "(", "m", ".", "adaptive_mask", ",", "out2", ",", "int", "(", "max_size", ")", ")", "\n", "# m.total_ops += torch.DoubleTensor([int(total_ops)])", "\n", "total_count", "+=", "total_ops", "\n", "\n", "# out3 = (out2.unsqueeze(dim=2) * v_out).sum(dim=-1).view(batch, -1, height, width)", "\n", "# m.total_ops += v_out.numel()", "\n", "", "total_count", "+=", "total_ops", "\n", "\n", "return", "total_count", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.flop_count.count_batchnorm2d": [[144, 152], ["x.numel"], "function", ["None"], ["", "def", "count_batchnorm2d", "(", "m", ",", "x", ")", ":", "\n", "    ", "nelements", "=", "x", ".", "numel", "(", ")", "\n", "# TODO : Check this m.training", "\n", "if", "not", "m", ".", "training", ":", "\n", "# subtract, divide, gamma, beta", "\n", "        ", "total_ops", "=", "2", "*", "nelements", "\n", "\n", "", "return", "total_ops", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.flop_count.count_avgpool2d": [[154, 159], ["y.numel"], "function", ["None"], ["", "def", "count_avgpool2d", "(", "y", ")", ":", "\n", "    ", "kernel_ops", "=", "1", "\n", "num_elements", "=", "y", ".", "numel", "(", ")", "\n", "total_ops", "=", "kernel_ops", "*", "num_elements", "\n", "return", "total_ops", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.flop_count.count_bootleneck": [[161, 259], ["flop_count.count_conv2d", "torch.DoubleTensor", "torch.DoubleTensor", "flop_count.count_batchnorm2d", "torch.DoubleTensor", "torch.DoubleTensor", "torch.relu", "F.avg_pool2d.numel", "torch.DoubleTensor", "torch.DoubleTensor", "print", "flop_count.count_batchnorm2d", "torch.DoubleTensor", "torch.DoubleTensor", "torch.relu", "F.avg_pool2d.numel", "torch.DoubleTensor", "torch.DoubleTensor", "print", "flop_count.count_conv2d", "torch.DoubleTensor", "torch.DoubleTensor", "flop_count.count_batchnorm2d", "torch.DoubleTensor", "torch.DoubleTensor", "print", "F.avg_pool2d.numel", "torch.DoubleTensor", "torch.DoubleTensor", "torch.relu", "F.avg_pool2d.numel", "torch.DoubleTensor", "torch.DoubleTensor", "m.conv2[]._get_name().lower", "flop_count.count_attention_flops", "torch.DoubleTensor", "torch.DoubleTensor", "flop_count.count_conv2d", "torch.DoubleTensor", "torch.DoubleTensor", "flop_count.count_avgpool2d", "torch.avg_pool2d", "torch.DoubleTensor", "torch.DoubleTensor", "len", "flop_count.count_conv2d", "torch.DoubleTensor", "torch.DoubleTensor", "flop_count.count_batchnorm2d", "torch.DoubleTensor", "torch.DoubleTensor", "int", "int", "int", "int", "int", "int", "int", "int", "int", "m.conv2[]._get_name", "int", "int", "int", "int", "int"], "function", ["home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.flop_count.count_conv2d", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.flop_count.count_batchnorm2d", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.flop_count.count_batchnorm2d", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.flop_count.count_conv2d", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.flop_count.count_batchnorm2d", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.flop_count.count_attention_flops", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.flop_count.count_conv2d", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.flop_count.count_avgpool2d", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.flop_count.count_conv2d", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.flop_count.count_batchnorm2d"], ["", "def", "count_bootleneck", "(", "m", ",", "x", ",", "y", ")", ":", "\n", "    ", "x", "=", "x", "[", "0", "]", "\n", "# import pdb", "\n", "# pdb.set_trace()", "\n", "# out = m.conv1(x)", "\n", "# conv1 consists of 3 layers", "\n", "# self.conv1 = nn.Sequential(", "\n", "#     nn.Conv2d(in_channels, width, kernel_size=1, bias=False),", "\n", "#     nn.BatchNorm2d(width),", "\n", "#     nn.ReLU(),", "\n", "# )", "\n", "\n", "conv1_count", "=", "0", "\n", "total_ops", ",", "out", "=", "count_conv2d", "(", "m", ".", "conv1", "[", "0", "]", ",", "x", ")", "\n", "m", ".", "total_ops", "+=", "torch", ".", "DoubleTensor", "(", "[", "int", "(", "total_ops", ")", "]", ")", "\n", "conv1_count", "+=", "total_ops", "\n", "\n", "total_ops", "=", "count_batchnorm2d", "(", "m", ".", "conv1", "[", "1", "]", ",", "out", ")", "\n", "m", ".", "total_ops", "+=", "torch", ".", "DoubleTensor", "(", "[", "int", "(", "total_ops", ")", "]", ")", "\n", "out", "=", "m", ".", "conv1", "[", "1", "]", "(", "out", ")", "\n", "conv1_count", "+=", "total_ops", "\n", "\n", "out", "=", "F", ".", "relu", "(", "out", ")", "\n", "total_ops", "=", "out", ".", "numel", "(", ")", "\n", "m", ".", "total_ops", "+=", "torch", ".", "DoubleTensor", "(", "[", "int", "(", "total_ops", ")", "]", ")", "\n", "conv1_count", "+=", "total_ops", "\n", "print", "(", "'conv1 : '", ",", "conv1_count", ")", "\n", "\n", "\n", "conv2_count", "=", "0", "\n", "# conv2 consists of a layer, batchnorm and relu", "\n", "if", "'attention'", "in", "m", ".", "conv2", "[", "0", "]", ".", "_get_name", "(", ")", ".", "lower", "(", ")", ":", "\n", "# if args.all_attention:", "\n", "        ", "total_ops", "=", "count_attention_flops", "(", "m", ".", "conv2", "[", "0", "]", ",", "out", ")", "\n", "out", "=", "m", ".", "conv2", "[", "0", "]", "(", "out", ")", "\n", "m", ".", "total_ops", "+=", "torch", ".", "DoubleTensor", "(", "[", "int", "(", "total_ops", ")", "]", ")", "\n", "", "else", ":", "\n", "        ", "total_ops", ",", "out", "=", "count_conv2d", "(", "m", ".", "conv2", "[", "0", "]", ",", "out", ")", "\n", "m", ".", "total_ops", "+=", "torch", ".", "DoubleTensor", "(", "[", "int", "(", "total_ops", ")", "]", ")", "\n", "", "conv2_count", "+=", "total_ops", "\n", "\n", "total_ops", "=", "count_batchnorm2d", "(", "m", ".", "conv2", "[", "1", "]", ",", "out", ")", "\n", "m", ".", "total_ops", "+=", "torch", ".", "DoubleTensor", "(", "[", "int", "(", "total_ops", ")", "]", ")", "\n", "out", "=", "m", ".", "conv2", "[", "1", "]", "(", "out", ")", "\n", "conv2_count", "+=", "total_ops", "\n", "\n", "out", "=", "F", ".", "relu", "(", "out", ")", "\n", "total_ops", "=", "out", ".", "numel", "(", ")", "\n", "m", ".", "total_ops", "+=", "torch", ".", "DoubleTensor", "(", "[", "int", "(", "total_ops", ")", "]", ")", "\n", "conv2_count", "+=", "total_ops", "\n", "\n", "print", "(", "'conv2 : '", ",", "conv2_count", ")", "\n", "\n", "# out = m.conv3(out)", "\n", "# self.conv3 = nn.Sequential(", "\n", "#     nn.Conv2d(width, self.expansion * out_channels, kernel_size=1, bias=False),", "\n", "#     nn.BatchNorm2d(self.expansion * out_channels),", "\n", "# )", "\n", "conv3_count", "=", "0", "\n", "total_ops", ",", "out", "=", "count_conv2d", "(", "m", ".", "conv3", "[", "0", "]", ",", "out", ")", "\n", "m", ".", "total_ops", "+=", "torch", ".", "DoubleTensor", "(", "[", "int", "(", "total_ops", ")", "]", ")", "\n", "conv3_count", "+=", "total_ops", "\n", "\n", "total_ops", "=", "count_batchnorm2d", "(", "m", ".", "conv3", "[", "1", "]", ",", "out", ")", "\n", "m", ".", "total_ops", "+=", "torch", ".", "DoubleTensor", "(", "[", "int", "(", "total_ops", ")", "]", ")", "\n", "out", "=", "m", ".", "conv3", "[", "1", "]", "(", "out", ")", "\n", "conv3_count", "+=", "total_ops", "\n", "\n", "print", "(", "'conv3 : '", ",", "conv3_count", ")", "\n", "\n", "# import pdb", "\n", "# pdb.set_trace()", "\n", "\n", "if", "m", ".", "stride", ">=", "2", ":", "\n", "        ", "total_ops", "=", "count_avgpool2d", "(", "out", ")", "\n", "out", "=", "F", ".", "avg_pool2d", "(", "out", ",", "(", "m", ".", "stride", ",", "m", ".", "stride", ")", ")", "\n", "m", ".", "total_ops", "+=", "torch", ".", "DoubleTensor", "(", "[", "int", "(", "total_ops", ")", "]", ")", "\n", "\n", "# out += m.shortcut(x)", "\n", "# self.shortcut = nn.Sequential(", "\n", "#     nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),", "\n", "#     nn.BatchNorm2d(self.expansion * out_channels)", "\n", "# )", "\n", "\n", "", "if", "len", "(", "m", ".", "shortcut", ")", ">", "0", ":", "\n", "        ", "total_ops", ",", "out_1", "=", "count_conv2d", "(", "m", ".", "shortcut", "[", "0", "]", ",", "x", ")", "\n", "m", ".", "total_ops", "+=", "torch", ".", "DoubleTensor", "(", "[", "int", "(", "total_ops", ")", "]", ")", "\n", "\n", "total_ops", "=", "count_batchnorm2d", "(", "m", ".", "shortcut", "[", "1", "]", ",", "out_1", ")", "\n", "m", ".", "total_ops", "+=", "torch", ".", "DoubleTensor", "(", "[", "int", "(", "total_ops", ")", "]", ")", "\n", "out", "+=", "m", ".", "shortcut", "[", "1", "]", "(", "out_1", ")", "\n", "# out += m.shortcut(x)", "\n", "", "total_ops", "=", "out", ".", "numel", "(", ")", "\n", "m", ".", "total_ops", "+=", "torch", ".", "DoubleTensor", "(", "[", "int", "(", "total_ops", ")", "]", ")", "\n", "\n", "out", "=", "F", ".", "relu", "(", "out", ")", "\n", "total_ops", "=", "out", ".", "numel", "(", ")", "\n", "m", ".", "total_ops", "+=", "torch", ".", "DoubleTensor", "(", "[", "int", "(", "total_ops", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.preprocess.Args.__init__": [[193, 197], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "batch_size", "=", "32", "\n", "self", ".", "num_workers", "=", "1", "\n", "self", ".", "dataset", "=", "'CIFAR10'", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.preprocess.load_data": [[7, 189], ["print", "torchvision.transforms.Compose", "torchvision.transforms.Compose", "torchvision.datasets.CIFAR10", "int", "print", "torch.utils.data.random_split", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "len", "torchvision.datasets.CIFAR10", "torchvision.transforms.Compose", "torchvision.transforms.Compose", "torchvision.datasets.CIFAR100", "int", "torch.utils.data.random_split", "int", "print", "torch.utils.data.random_split", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torchvision.transforms.RandomCrop", "torchvision.transforms.RandomHorizontalFlip", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "len", "len", "torchvision.datasets.CIFAR100", "torchvision.transforms.Compose", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torchvision.transforms.RandomCrop", "torchvision.transforms.RandomHorizontalFlip", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "len", "len", "torchvision.datasets.MNIST", "torchvision.datasets.MNIST", "torchvision.transforms.Compose", "torchvision.transforms.Compose", "torchvision.datasets.ImageFolder", "int", "int", "int", "torch.utils.data.random_split", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "print", "print", "len", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "torchvision.transforms.RandomHorizontalFlip", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "len", "len", "len"], "function", ["None"], ["def", "load_data", "(", "args", ")", ":", "\n", "    ", "print", "(", "'Load Dataset :: {}'", ".", "format", "(", "args", ".", "dataset", ")", ")", "\n", "if", "args", ".", "dataset", "==", "'CIFAR10'", ":", "\n", "        ", "transform_train", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "RandomCrop", "(", "32", ",", "padding", "=", "4", ")", ",", "\n", "transforms", ".", "RandomHorizontalFlip", "(", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Normalize", "(", "\n", "mean", "=", "(", "0.4914", ",", "0.4822", ",", "0.4465", ")", ",", "\n", "std", "=", "(", "0.2470", ",", "0.2435", ",", "0.2616", ")", "\n", ")", "\n", "]", ")", "\n", "\n", "transform_test", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Normalize", "(", "\n", "mean", "=", "(", "0.4914", ",", "0.4822", ",", "0.4465", ")", ",", "\n", "std", "=", "(", "0.2470", ",", "0.2435", ",", "0.2616", ")", "\n", ")", "\n", "]", ")", "\n", "\n", "train_data", "=", "datasets", ".", "CIFAR10", "(", "'data'", ",", "train", "=", "True", ",", "download", "=", "True", ",", "transform", "=", "transform_train", ")", "\n", "train_len", "=", "int", "(", "len", "(", "train_data", ")", "*", "0.9", ")", "\n", "val_len", "=", "len", "(", "train_data", ")", "-", "train_len", "\n", "print", "(", "'Len Train: {}, Len Valid: {}'", ".", "format", "(", "train_len", ",", "val_len", ")", ")", "\n", "train_set", ",", "valid_set", "=", "torch", ".", "utils", ".", "data", ".", "random_split", "(", "train_data", ",", "[", "train_len", ",", "val_len", "]", ")", "\n", "valid_set", ".", "transform", "=", "transform_test", "#Don't want to apply flips and random crops to this", "\n", "\n", "train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "train_set", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "shuffle", "=", "True", ",", "\n", "num_workers", "=", "args", ".", "num_workers", "\n", ")", "\n", "\n", "valid_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "valid_set", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "shuffle", "=", "True", ",", "\n", "num_workers", "=", "args", ".", "num_workers", "\n", ")", "\n", "\n", "test_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "datasets", ".", "CIFAR10", "(", "'data'", ",", "train", "=", "False", ",", "transform", "=", "transform_test", ")", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "shuffle", "=", "False", ",", "\n", "num_workers", "=", "args", ".", "num_workers", "\n", ")", "\n", "\n", "", "elif", "args", ".", "dataset", "==", "'CIFAR100'", ":", "\n", "        ", "transform_train", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "RandomCrop", "(", "32", ",", "padding", "=", "4", ")", ",", "\n", "transforms", ".", "RandomHorizontalFlip", "(", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Normalize", "(", "\n", "mean", "=", "(", "0.5071", ",", "0.4865", ",", "0.4409", ")", ",", "\n", "std", "=", "(", "0.2673", ",", "0.2564", ",", "0.2762", ")", "\n", ")", ",", "\n", "]", ")", "\n", "transform_test", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Normalize", "(", "\n", "mean", "=", "(", "0.5071", ",", "0.4865", ",", "0.4409", ")", ",", "\n", "std", "=", "(", "0.2673", ",", "0.2564", ",", "0.2762", ")", "\n", ")", ",", "\n", "]", ")", "\n", "\n", "train_data", "=", "datasets", ".", "CIFAR100", "(", "'data'", ",", "train", "=", "True", ",", "download", "=", "True", ",", "transform", "=", "transform_train", ")", "\n", "\n", "actual_to_be_used", "=", "int", "(", "len", "(", "train_data", ")", "*", "args", ".", "subset", ")", "\n", "train_data", ",", "_", "=", "torch", ".", "utils", ".", "data", ".", "random_split", "(", "train_data", ",", "[", "actual_to_be_used", ",", "len", "(", "train_data", ")", "-", "actual_to_be_used", "]", ")", "\n", "\n", "train_len", "=", "int", "(", "len", "(", "train_data", ")", "*", "0.9", ")", "\n", "val_len", "=", "len", "(", "train_data", ")", "-", "train_len", "\n", "print", "(", "'Len Train: {}, Len Valid: {}'", ".", "format", "(", "train_len", ",", "val_len", ")", ")", "\n", "train_set", ",", "valid_set", "=", "torch", ".", "utils", ".", "data", ".", "random_split", "(", "train_data", ",", "[", "train_len", ",", "val_len", "]", ")", "\n", "valid_set", ".", "transform", "=", "transform_test", "#Don't want to apply flips and random crops to this", "\n", "\n", "train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "train_set", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "shuffle", "=", "True", ",", "\n", "num_workers", "=", "args", ".", "num_workers", "\n", ")", "\n", "\n", "valid_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "valid_set", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "shuffle", "=", "True", ",", "\n", "num_workers", "=", "args", ".", "num_workers", "\n", ")", "\n", "\n", "test_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "datasets", ".", "CIFAR100", "(", "'data'", ",", "train", "=", "False", ",", "transform", "=", "transform_test", ")", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "shuffle", "=", "False", ",", "\n", "num_workers", "=", "args", ".", "num_workers", "\n", ")", "\n", "\n", "", "elif", "args", ".", "dataset", "==", "'MNIST'", ":", "\n", "        ", "transform", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Normalize", "(", "\n", "mean", "=", "(", "0.1307", ",", ")", ",", "\n", "std", "=", "(", "0.3081", ",", ")", "\n", ")", "\n", "]", ")", "\n", "train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "datasets", ".", "MNIST", "(", "'data'", ",", "train", "=", "True", ",", "download", "=", "True", ",", "transform", "=", "transform", ")", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "shuffle", "=", "True", ",", "\n", "num_workers", "=", "args", ".", "num_workers", "\n", ")", "\n", "\n", "test_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "datasets", ".", "MNIST", "(", "'data'", ",", "train", "=", "False", ",", "transform", "=", "transform", ")", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "shuffle", "=", "False", ",", "\n", "num_workers", "=", "args", ".", "num_workers", "\n", ")", "\n", "\n", "", "elif", "args", ".", "dataset", "==", "'TinyImageNet'", ":", "\n", "# We use the normalization stats of the full ImageNet dataset as an estimate for the stats of the", "\n", "# TinyImageNet dataset", "\n", "        ", "transform_train", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "RandomHorizontalFlip", "(", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Normalize", "(", "\n", "mean", "=", "(", "0.485", ",", "0.456", ",", "0.406", ")", ",", "\n", "std", "=", "(", "0.229", ",", "0.224", ",", "0.225", ")", "\n", ")", "\n", "]", ")", "\n", "\n", "transform_test", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Normalize", "(", "\n", "mean", "=", "(", "0.485", ",", "0.456", ",", "0.406", ")", ",", "\n", "std", "=", "(", "0.229", ",", "0.224", ",", "0.225", ")", "\n", ")", "\n", "]", ")", "\n", "\n", "# Only the training data has labels so we will split it up to make training, testing, and validation sets", "\n", "train_data", "=", "datasets", ".", "ImageFolder", "(", "'./datasets/processed-tiny-imagenet'", ",", "transform", "=", "transform_train", ")", "\n", "\n", "train_len", "=", "int", "(", "len", "(", "train_data", ")", "*", "0.8", ")", "\n", "val_len", "=", "int", "(", "len", "(", "train_data", ")", "*", "0.1", ")", "\n", "test_len", "=", "int", "(", "len", "(", "train_data", ")", "*", "0.1", ")", "\n", "train_set", ",", "valid_set", ",", "test_set", "=", "torch", ".", "utils", ".", "data", ".", "random_split", "(", "train_data", ",", "[", "train_len", ",", "val_len", ",", "test_len", "]", ")", "\n", "\n", "# test_len = int(len(train_set)*0.1)", "\n", "# new_train_len = len(train_set) - test_len", "\n", "# train_set, test_set = torch.utils.data.random_split(train_set, [new_train_len, test_len])", "\n", "\n", "#Don't want to apply flips and random crops to this", "\n", "valid_set", ".", "transform", "=", "transform_test", "\n", "test_set", ".", "transform", "=", "transform_test", "\n", "\n", "train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "train_set", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "shuffle", "=", "True", ",", "\n", "num_workers", "=", "args", ".", "num_workers", "\n", ")", "\n", "\n", "valid_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "valid_set", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "shuffle", "=", "True", ",", "\n", "num_workers", "=", "args", ".", "num_workers", "\n", ")", "\n", "\n", "test_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "test_set", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "shuffle", "=", "False", ",", "\n", "num_workers", "=", "args", ".", "num_workers", "\n", ")", "\n", "\n", "print", "(", "'TinyImageNet Loader'", ")", "\n", "print", "(", "train_loader", ")", "\n", "\n", "", "return", "train_loader", ",", "valid_loader", ",", "test_loader", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.model.Bottleneck.__init__": [[13, 64], ["torch.Module.__init__", "int", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "attention_augmented_conv.AugmentedConv", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Sequential", "torch.Sequential", "torch.Sequential", "attention.AttentionConv", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d"], "methods", ["home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.file_writer.FileWriter.__init__"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "stride", "=", "1", ",", "base_width", "=", "64", ",", "args", "=", "None", ")", ":", "\n", "\n", "        ", "super", "(", "Bottleneck", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "stride", "=", "stride", "\n", "groups", "=", "args", ".", "groups", "# Number of attention heads", "\n", "\n", "'''\n\n        # TODO : Doubt in width, when base_width != 64?\n        width = int(out_channels * (base_width / 64.))\\\n            if args.attention_conv\\\n            else int(out_channels * (base_width / 64.)) * groups\n        '''", "\n", "width", "=", "out_channels", "\n", "\n", "additional_args", "=", "{", "'groups'", ":", "groups", ",", "'R'", ":", "args", ".", "R", ",", "'z_init'", ":", "args", ".", "z_init", ",", "'adaptive_span'", ":", "args", ".", "adaptive_span", "}", "if", "args", ".", "all_attention", "else", "{", "'bias'", ":", "False", "}", "\n", "\n", "kernel_size", "=", "args", ".", "attention_kernel", "if", "args", ".", "all_attention", "else", "3", "\n", "padding", "=", "int", "(", "(", "kernel_size", "-", "1", ")", "/", "2", ")", "\n", "\n", "layer", "=", "None", "\n", "\n", "if", "args", ".", "attention_conv", ":", "\n", "            ", "layer", "=", "AugmentedConv", "(", "width", ",", "width", ",", "kernel_size", ",", "args", ".", "dk", ",", "args", ".", "dv", ",", "groups", ",", "shape", "=", "width", ")", "\n", "", "elif", "args", ".", "all_attention", ":", "\n", "            ", "layer", "=", "AttentionConv", "(", "width", ",", "width", ",", "kernel_size", "=", "kernel_size", ",", "padding", "=", "padding", ",", "**", "additional_args", ")", "\n", "", "else", ":", "\n", "            ", "layer", "=", "nn", ".", "Conv2d", "(", "width", ",", "width", ",", "kernel_size", "=", "kernel_size", ",", "padding", "=", "padding", ",", "**", "additional_args", ")", "\n", "\n", "", "self", ".", "conv1", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "in_channels", ",", "width", ",", "kernel_size", "=", "1", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "width", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", ")", "\n", "\n", "self", ".", "conv2", "=", "nn", ".", "Sequential", "(", "\n", "layer", ",", "\n", "nn", ".", "BatchNorm2d", "(", "width", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", ")", "\n", "self", ".", "conv3", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "width", ",", "self", ".", "expansion", "*", "out_channels", ",", "kernel_size", "=", "1", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "self", ".", "expansion", "*", "out_channels", ")", ",", "\n", ")", "\n", "\n", "self", ".", "shortcut", "=", "nn", ".", "Sequential", "(", ")", "\n", "if", "stride", "!=", "1", "or", "in_channels", "!=", "self", ".", "expansion", "*", "out_channels", ":", "\n", "            ", "self", ".", "shortcut", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "in_channels", ",", "self", ".", "expansion", "*", "out_channels", ",", "kernel_size", "=", "1", ",", "stride", "=", "stride", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "self", ".", "expansion", "*", "out_channels", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.model.Bottleneck.forward": [[66, 77], ["model.Bottleneck.conv1", "model.Bottleneck.conv2", "model.Bottleneck.conv3", "model.Bottleneck.shortcut", "torch.relu", "torch.relu", "torch.relu", "torch.avg_pool2d", "torch.avg_pool2d", "torch.avg_pool2d"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "conv3", "(", "out", ")", "\n", "if", "self", ".", "stride", ">=", "2", ":", "\n", "            ", "out", "=", "F", ".", "avg_pool2d", "(", "out", ",", "(", "self", ".", "stride", ",", "self", ".", "stride", ")", ")", "\n", "\n", "", "out", "+=", "self", ".", "shortcut", "(", "x", ")", "\n", "out", "=", "F", ".", "relu", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.model.Model.__init__": [[80, 114], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "len", "model.Model.layers.append", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "model.Model._make_layer"], "methods", ["home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.file_writer.FileWriter.__init__", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.model.Model._make_layer"], ["    ", "def", "__init__", "(", "self", ",", "block", ",", "num_blocks", ",", "num_classes", "=", "1000", ",", "args", "=", "None", ")", ":", "\n", "        ", "super", "(", "Model", ",", "self", ")", ".", "__init__", "(", ")", "\n", "divider", "=", "2", "#if args.small_version else 1", "\n", "layer_channels", "=", "None", "#These two sets of channels give approximately equal #params between all_attention and all_conv", "\n", "\n", "if", "args", ".", "all_attention", ":", "\n", "#layer_channels = [64,128,128,256,256]", "\n", "            ", "layer_channels", "=", "[", "32", ",", "64", ",", "128", "]", "if", "args", ".", "smallest_version", "else", "[", "64", "//", "divider", ",", "128", "//", "divider", ",", "256", "//", "divider", ",", "512", "//", "divider", "]", "# [96, 128, 128, 256]", "\n", "", "else", ":", "\n", "            ", "layer_channels", "=", "[", "32", ",", "64", ",", "128", "]", "if", "args", ".", "smallest_version", "else", "[", "64", "//", "divider", ",", "128", "//", "divider", ",", "256", "//", "divider", ",", "512", "//", "divider", "]", "\n", "\n", "", "self", ".", "args", "=", "args", "\n", "self", ".", "in_places", "=", "64", "if", "args", ".", "dataset", "==", "'TinyImageNet'", "else", "32", "\n", "self", ".", "all_attention", "=", "args", ".", "all_attention", "\n", "self", ".", "attention_kernel", "=", "args", ".", "attention_kernel", "\n", "\n", "self", ".", "init", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "3", ",", "64", ",", "kernel_size", "=", "7", ",", "stride", "=", "2", ",", "padding", "=", "3", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "64", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", ",", "\n", ")", "if", "args", ".", "dataset", "==", "'TinyImageNet'", "else", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "3", ",", "64", "//", "divider", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "64", "//", "divider", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "\n", "strides", "=", "[", "2", "]", "*", "3", "if", "args", ".", "smallest_version", "else", "[", "1", "]", "+", "[", "2", "]", "*", "3", "\n", "for", "i", "in", "range", "(", "len", "(", "layer_channels", ")", ")", ":", "\n", "            ", "self", ".", "layers", ".", "append", "(", "self", ".", "_make_layer", "(", "block", ",", "layer_channels", "[", "i", "]", ",", "num_blocks", "[", "i", "]", ",", "stride", "=", "strides", "[", "i", "]", ")", ")", "\n", "\n", "", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "layer_channels", "[", "-", "1", "]", "*", "block", ".", "expansion", ",", "num_classes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.model.Model._make_layer": [[115, 122], ["torch.Sequential", "torch.Sequential", "torch.Sequential", "layers.append", "block"], "methods", ["None"], ["", "def", "_make_layer", "(", "self", ",", "block", ",", "planes", ",", "num_blocks", ",", "stride", ")", ":", "\n", "        ", "strides", "=", "[", "stride", "]", "+", "[", "1", "]", "*", "(", "num_blocks", "-", "1", ")", "\n", "layers", "=", "[", "]", "\n", "for", "stride", "in", "strides", ":", "\n", "            ", "layers", ".", "append", "(", "block", "(", "self", ".", "in_places", ",", "planes", ",", "stride", ",", "args", "=", "self", ".", "args", ")", ")", "#in_places is #input_channels", "\n", "self", ".", "in_places", "=", "planes", "*", "block", ".", "expansion", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.model.Model.get_span_l1": [[123, 132], ["l2.conv2[].adaptive_mask.current_val.abs().sum", "l2.conv2[].adaptive_mask.current_val.abs"], "methods", ["None"], ["", "def", "get_span_l1", "(", "self", ",", "args", ")", ":", "\n", "        ", "num_abs_spans", "=", "0", "\n", "if", "args", ".", "all_attention", ":", "\n", "            ", "for", "l", "in", "self", ".", "layers", ":", "\n", "                ", "for", "l2", "in", "l", ":", "\n", "                    ", "sum_layer", "=", "l2", ".", "conv2", "[", "0", "]", ".", "adaptive_mask", ".", "current_val", ".", "abs", "(", ")", ".", "sum", "(", ")", "\n", "num_abs_spans", "+=", "sum_layer", "\n", "\n", "", "", "", "return", "num_abs_spans", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.model.Model.clamp_span": [[133, 137], ["l2.conv2[].adaptive_mask.clamp_param"], "methods", ["home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.attention.AdaptiveMask.clamp_param"], ["", "def", "clamp_span", "(", "self", ")", ":", "\n", "        ", "for", "l", "in", "self", ".", "layers", ":", "\n", "            ", "for", "l2", "in", "l", ":", "\n", "                ", "l2", ".", "conv2", "[", "0", "]", ".", "adaptive_mask", ".", "clamp_param", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.model.Model.forward": [[138, 152], ["model.Model.init", "torch.avg_pool2d", "torch.avg_pool2d", "torch.avg_pool2d", "layer.view", "model.Model.dense", "layer", "layer.size"], "methods", ["None"], ["", "", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# TODO(Joe): See if there is some other modification we can make so we don't need to have different pooling kernels at the end of the model", "\n", "        ", "pooling_kernel_size", "=", "2", "if", "self", ".", "args", ".", "dataset", "==", "'TinyImageNet'", "else", "4", "\n", "\n", "out", "=", "self", ".", "init", "(", "x", ")", "\n", "\n", "for", "layer", "in", "self", ".", "layers", ":", "\n", "            ", "out", "=", "layer", "(", "out", ")", "\n", "\n", "", "out", "=", "F", ".", "avg_pool2d", "(", "out", ",", "pooling_kernel_size", ")", "\n", "out", "=", "out", ".", "view", "(", "out", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "out", "=", "self", ".", "dense", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.model.ResNet26": [[154, 165], ["model.Model"], "function", ["None"], ["", "", "def", "ResNet26", "(", "num_classes", "=", "1000", ",", "args", "=", "None", ")", ":", "\n", "    ", "if", "args", ".", "smallest_version", ":", "\n", "        ", "num_blocks", "=", "[", "1", "]", "*", "3", "\n", "", "elif", "args", ".", "small_version", ":", "\n", "#Decided to use same architecture for both all conv and all attention for better comparison", "\n", "        ", "num_blocks", "=", "[", "1", "]", "*", "4", "#[1, 2, 2, 1] if args.all_attention else [1]*4", "\n", "", "else", ":", "\n", "        ", "num_blocks", "=", "[", "1", ",", "3", ",", "4", ",", "1", "]", "#Now all attention is 3.02M and CNN is 3.09 M params", "\n", "#num_blocks = [1, 2, 4, 1]", "\n", "\n", "", "return", "Model", "(", "Bottleneck", ",", "num_blocks", ",", "num_classes", "=", "num_classes", ",", "args", "=", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.model.ResNet38": [[167, 169], ["model.Model"], "function", ["None"], ["", "def", "ResNet38", "(", "num_classes", "=", "1000", ",", "all_attention", "=", "False", ")", ":", "\n", "    ", "return", "Model", "(", "Bottleneck", ",", "[", "2", ",", "3", ",", "5", ",", "2", "]", ",", "num_classes", "=", "num_classes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.model.ResNet50": [[171, 173], ["model.Model"], "function", ["None"], ["", "def", "ResNet50", "(", "num_classes", "=", "1000", ",", "all_attention", "=", "False", ")", ":", "\n", "    ", "return", "Model", "(", "Bottleneck", ",", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "num_classes", "=", "num_classes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.model.get_model_parameters": [[175, 183], ["list", "model.parameters", "list", "layer.size"], "function", ["None"], ["", "def", "get_model_parameters", "(", "model", ")", ":", "\n", "    ", "total_parameters", "=", "0", "\n", "for", "layer", "in", "list", "(", "model", ".", "parameters", "(", ")", ")", ":", "\n", "        ", "layer_parameter", "=", "1", "\n", "for", "l", "in", "list", "(", "layer", ".", "size", "(", ")", ")", ":", "\n", "            ", "layer_parameter", "*=", "l", "\n", "", "total_parameters", "+=", "layer_parameter", "\n", "", "return", "total_parameters", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.file_writer.FileWriter.__init__": [[66, 174], ["file_writer.gather_metadata", "copy.deepcopy", "logging.Formatter", "logging.getLogger", "logging.StreamHandler", "logging.StreamHandler.setFormatter", "file_writer.FileWriter._logger.addHandler", "file_writer.FileWriter._logger.setLevel", "os.path.expandvars", "os.path.join", "dict", "file_writer.FileWriter._logger.info", "os.path.exists", "file_writer.FileWriter._logger.info", "os.path.exists", "logging.FileHandler", "logging.FileHandler.setFormatter", "file_writer.FileWriter._logger.addHandler", "file_writer.FileWriter._logger.info", "file_writer.FileWriter._logger.info", "os.path.exists", "open", "csv.writer", "open", "csv.DictWriter", "os.path.expanduser", "os.path.exists", "file_writer.FileWriter._logger.info", "os.makedirs", "file_writer.FileWriter._logger.info", "os.path.join", "file_writer.FileWriter._logger.warning", "file_writer.FileWriter._save_metadata", "file_writer.FileWriter._logger.warning", "file_writer.FileWriter._logger.warning", "os.path.islink", "open", "csv.reader", "list", "open", "csv.reader", "list", "os.getpid", "int", "os.remove", "os.path.exists", "os.symlink", "file_writer.FileWriter._logger.info", "len", "len", "time.time", "int"], "methods", ["home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.file_writer.gather_metadata", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.file_writer.FileWriter._save_metadata"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "xpid", ":", "str", "=", "None", ",", "\n", "xp_args", ":", "dict", "=", "None", ",", "\n", "rootdir", ":", "str", "=", "\"~/logs\"", ",", "\n", "symlink_to_latest", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "if", "not", "xpid", ":", "\n", "# Make unique id.", "\n", "            ", "xpid", "=", "\"{proc}_{unixtime}\"", ".", "format", "(", "\n", "proc", "=", "os", ".", "getpid", "(", ")", ",", "unixtime", "=", "int", "(", "time", ".", "time", "(", ")", ")", "\n", ")", "\n", "", "self", ".", "xpid", "=", "xpid", "\n", "self", ".", "_tick", "=", "0", "\n", "\n", "# Metadata gathering.", "\n", "if", "xp_args", "is", "None", ":", "\n", "            ", "xp_args", "=", "{", "}", "\n", "", "self", ".", "metadata", "=", "gather_metadata", "(", ")", "\n", "# We need to copy the args, otherwise when we close the file writer", "\n", "# (and rewrite the args) we might have non-serializable objects (or", "\n", "# other unwanted side-effects).", "\n", "self", ".", "metadata", "[", "\"args\"", "]", "=", "copy", ".", "deepcopy", "(", "xp_args", ")", "\n", "self", ".", "metadata", "[", "\"xpid\"", "]", "=", "self", ".", "xpid", "\n", "\n", "formatter", "=", "logging", ".", "Formatter", "(", "\"%(message)s\"", ")", "\n", "self", ".", "_logger", "=", "logging", ".", "getLogger", "(", "\"logs/out\"", ")", "\n", "\n", "# To stdout handler.", "\n", "shandle", "=", "logging", ".", "StreamHandler", "(", ")", "\n", "shandle", ".", "setFormatter", "(", "formatter", ")", "\n", "self", ".", "_logger", ".", "addHandler", "(", "shandle", ")", "\n", "self", ".", "_logger", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "\n", "rootdir", "=", "os", ".", "path", ".", "expandvars", "(", "os", ".", "path", ".", "expanduser", "(", "rootdir", ")", ")", "\n", "# To file handler.", "\n", "self", ".", "basepath", "=", "os", ".", "path", ".", "join", "(", "rootdir", ",", "self", ".", "xpid", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "basepath", ")", ":", "\n", "            ", "self", ".", "_logger", ".", "info", "(", "\"Creating log directory: %s\"", ",", "self", ".", "basepath", ")", "\n", "os", ".", "makedirs", "(", "self", ".", "basepath", ",", "exist_ok", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_logger", ".", "info", "(", "\"Found log directory: %s\"", ",", "self", ".", "basepath", ")", "\n", "\n", "", "if", "symlink_to_latest", ":", "\n", "# Add 'latest' as symlink unless it exists and is no symlink.", "\n", "            ", "symlink", "=", "os", ".", "path", ".", "join", "(", "rootdir", ",", "\"latest\"", ")", "\n", "try", ":", "\n", "                ", "if", "os", ".", "path", ".", "islink", "(", "symlink", ")", ":", "\n", "                    ", "os", ".", "remove", "(", "symlink", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "symlink", ")", ":", "\n", "                    ", "os", ".", "symlink", "(", "self", ".", "basepath", ",", "symlink", ")", "\n", "self", ".", "_logger", ".", "info", "(", "\"Symlinked log directory: %s\"", ",", "symlink", ")", "\n", "", "", "except", "OSError", ":", "\n", "# os.remove() or os.symlink() raced. Don't do anything.", "\n", "                ", "pass", "\n", "\n", "", "", "self", ".", "paths", "=", "dict", "(", "\n", "msg", "=", "\"{base}/out.log\"", ".", "format", "(", "base", "=", "self", ".", "basepath", ")", ",", "\n", "logs", "=", "\"{base}/logs.csv\"", ".", "format", "(", "base", "=", "self", ".", "basepath", ")", ",", "\n", "fields", "=", "\"{base}/fields.csv\"", ".", "format", "(", "base", "=", "self", ".", "basepath", ")", ",", "\n", "meta", "=", "\"{base}/meta.json\"", ".", "format", "(", "base", "=", "self", ".", "basepath", ")", ",", "\n", ")", "\n", "\n", "self", ".", "_logger", ".", "info", "(", "\"Saving arguments to %s\"", ",", "self", ".", "paths", "[", "\"meta\"", "]", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "self", ".", "paths", "[", "\"meta\"", "]", ")", ":", "\n", "            ", "self", ".", "_logger", ".", "warning", "(", "\n", "\"Path to meta file already exists. \"", "\"Not overriding meta.\"", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_save_metadata", "(", ")", "\n", "\n", "", "self", ".", "_logger", ".", "info", "(", "\"Saving messages to %s\"", ",", "self", ".", "paths", "[", "\"msg\"", "]", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "self", ".", "paths", "[", "\"msg\"", "]", ")", ":", "\n", "            ", "self", ".", "_logger", ".", "warning", "(", "\n", "\"Path to message file already exists. \"", "\"New data will be appended.\"", "\n", ")", "\n", "\n", "", "fhandle", "=", "logging", ".", "FileHandler", "(", "self", ".", "paths", "[", "\"msg\"", "]", ")", "\n", "fhandle", ".", "setFormatter", "(", "formatter", ")", "\n", "self", ".", "_logger", ".", "addHandler", "(", "fhandle", ")", "\n", "\n", "self", ".", "_logger", ".", "info", "(", "\"Saving logs data to %s\"", ",", "self", ".", "paths", "[", "\"logs\"", "]", ")", "\n", "self", ".", "_logger", ".", "info", "(", "\"Saving logs' fields to %s\"", ",", "self", ".", "paths", "[", "\"fields\"", "]", ")", "\n", "self", ".", "fieldnames", "=", "[", "\"_tick\"", ",", "\"_time\"", "]", "\n", "if", "os", ".", "path", ".", "exists", "(", "self", ".", "paths", "[", "\"logs\"", "]", ")", ":", "\n", "            ", "self", ".", "_logger", ".", "warning", "(", "\n", "\"Path to log file already exists. \"", "\"New data will be appended.\"", "\n", ")", "\n", "# Override default fieldnames.", "\n", "with", "open", "(", "self", ".", "paths", "[", "\"fields\"", "]", ",", "\"r\"", ")", "as", "csvfile", ":", "\n", "                ", "reader", "=", "csv", ".", "reader", "(", "csvfile", ")", "\n", "lines", "=", "list", "(", "reader", ")", "\n", "if", "len", "(", "lines", ")", ">", "0", ":", "\n", "                    ", "self", ".", "fieldnames", "=", "lines", "[", "-", "1", "]", "\n", "# Override default tick: use the last tick from the logs file plus 1.", "\n", "", "", "with", "open", "(", "self", ".", "paths", "[", "\"logs\"", "]", ",", "\"r\"", ")", "as", "csvfile", ":", "\n", "                ", "reader", "=", "csv", ".", "reader", "(", "csvfile", ")", "\n", "lines", "=", "list", "(", "reader", ")", "\n", "# Need at least two lines in order to read the last tick:", "\n", "# the first is the csv header and the second is the first line", "\n", "# of data.", "\n", "if", "len", "(", "lines", ")", ">", "1", ":", "\n", "                    ", "self", ".", "_tick", "=", "int", "(", "lines", "[", "-", "1", "]", "[", "0", "]", ")", "+", "1", "\n", "\n", "", "", "", "self", ".", "_fieldfile", "=", "open", "(", "self", ".", "paths", "[", "\"fields\"", "]", ",", "\"a\"", ")", "\n", "self", ".", "_fieldwriter", "=", "csv", ".", "writer", "(", "self", ".", "_fieldfile", ")", "\n", "self", ".", "_logfile", "=", "open", "(", "self", ".", "paths", "[", "\"logs\"", "]", ",", "\"a\"", ")", "\n", "self", ".", "_logwriter", "=", "csv", ".", "DictWriter", "(", "self", ".", "_logfile", ",", "fieldnames", "=", "self", ".", "fieldnames", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.file_writer.FileWriter.log": [[175, 202], ["time.time", "len", "file_writer.FileWriter._logwriter.writerow", "file_writer.FileWriter._logfile.flush", "len", "file_writer.FileWriter._fieldwriter.writerow", "file_writer.FileWriter._logger.info", "file_writer.FileWriter._logfile.write", "file_writer.FileWriter._logger.info", "file_writer.FileWriter.fieldnames.append", "sorted"], "methods", ["None"], ["", "def", "log", "(", "self", ",", "to_log", ":", "Dict", ",", "tick", ":", "int", "=", "None", ",", "verbose", ":", "bool", "=", "False", ")", "->", "None", ":", "\n", "        ", "if", "tick", "is", "not", "None", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "else", ":", "\n", "            ", "to_log", "[", "\"_tick\"", "]", "=", "self", ".", "_tick", "\n", "self", ".", "_tick", "+=", "1", "\n", "", "to_log", "[", "\"_time\"", "]", "=", "time", ".", "time", "(", ")", "\n", "\n", "old_len", "=", "len", "(", "self", ".", "fieldnames", ")", "\n", "for", "k", "in", "to_log", ":", "\n", "            ", "if", "k", "not", "in", "self", ".", "fieldnames", ":", "\n", "                ", "self", ".", "fieldnames", ".", "append", "(", "k", ")", "\n", "", "", "if", "old_len", "!=", "len", "(", "self", ".", "fieldnames", ")", ":", "\n", "            ", "self", ".", "_fieldwriter", ".", "writerow", "(", "self", ".", "fieldnames", ")", "\n", "self", ".", "_logger", ".", "info", "(", "\"Updated log fields: %s\"", ",", "self", ".", "fieldnames", ")", "\n", "\n", "", "if", "to_log", "[", "\"_tick\"", "]", "==", "0", ":", "\n", "            ", "self", ".", "_logfile", ".", "write", "(", "\"# %s\\n\"", "%", "\",\"", ".", "join", "(", "self", ".", "fieldnames", ")", ")", "\n", "\n", "", "if", "verbose", ":", "\n", "            ", "self", ".", "_logger", ".", "info", "(", "\n", "\"LOG | %s\"", ",", "\n", "\", \"", ".", "join", "(", "[", "\"{}: {}\"", ".", "format", "(", "k", ",", "to_log", "[", "k", "]", ")", "for", "k", "in", "sorted", "(", "to_log", ")", "]", ")", ",", "\n", ")", "\n", "\n", "", "self", ".", "_logwriter", ".", "writerow", "(", "to_log", ")", "\n", "self", ".", "_logfile", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.file_writer.FileWriter.close": [[203, 212], ["datetime.datetime.now().strftime", "file_writer.FileWriter._save_metadata", "f.close", "datetime.datetime.now"], "methods", ["home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.file_writer.FileWriter._save_metadata", "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.file_writer.FileWriter.close"], ["", "def", "close", "(", "self", ",", "successful", ":", "bool", "=", "True", ")", "->", "None", ":", "\n", "        ", "self", ".", "metadata", "[", "\"date_end\"", "]", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\n", "\"%Y-%m-%d %H:%M:%S.%f\"", "\n", ")", "\n", "self", ".", "metadata", "[", "\"successful\"", "]", "=", "successful", "\n", "self", ".", "_save_metadata", "(", ")", "\n", "\n", "for", "f", "in", "[", "self", ".", "_logfile", ",", "self", ".", "_fieldfile", "]", ":", "\n", "            ", "f", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.file_writer.FileWriter._save_metadata": [[213, 216], ["open", "json.dump"], "methods", ["None"], ["", "", "def", "_save_metadata", "(", "self", ")", "->", "None", ":", "\n", "        ", "with", "open", "(", "self", ".", "paths", "[", "\"meta\"", "]", ",", "\"w\"", ")", "as", "jsonfile", ":", "\n", "            ", "json", ".", "dump", "(", "self", ".", "metadata", ",", "jsonfile", ",", "indent", "=", "4", ",", "sort_keys", "=", "True", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.JoeRoussy_adaptive-attention-in-cv.None.file_writer.gather_metadata": [[27, 62], ["datetime.datetime.now().strftime", "dict", "datetime.datetime.now", "git.Repo", "dict", "k.replace().replace().lower", "os.environ.copy", "git.Repo.commit", "k.startswith", "git.Repo.is_dirty", "k.replace().replace", "k.replace"], "function", ["None"], ["def", "gather_metadata", "(", ")", "->", "Dict", ":", "\n", "    ", "date_start", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%Y-%m-%d %H:%M:%S.%f\"", ")", "\n", "# Gathering git metadata.", "\n", "try", ":", "\n", "        ", "import", "git", "\n", "\n", "try", ":", "\n", "            ", "repo", "=", "git", ".", "Repo", "(", "search_parent_directories", "=", "True", ")", "\n", "git_sha", "=", "repo", ".", "commit", "(", ")", ".", "hexsha", "\n", "git_data", "=", "dict", "(", "\n", "commit", "=", "git_sha", ",", "\n", "branch", "=", "None", "if", "repo", ".", "head", ".", "is_detached", "else", "repo", ".", "active_branch", ".", "name", ",", "\n", "is_dirty", "=", "repo", ".", "is_dirty", "(", ")", ",", "\n", "path", "=", "repo", ".", "git_dir", ",", "\n", ")", "\n", "", "except", "git", ".", "InvalidGitRepositoryError", ":", "\n", "            ", "git_data", "=", "None", "\n", "", "", "except", "ImportError", ":", "\n", "        ", "git_data", "=", "None", "\n", "# Gathering slurm metadata.", "\n", "", "if", "\"SLURM_JOB_ID\"", "in", "os", ".", "environ", ":", "\n", "        ", "slurm_env_keys", "=", "[", "k", "for", "k", "in", "os", ".", "environ", "if", "k", ".", "startswith", "(", "\"SLURM\"", ")", "]", "\n", "slurm_data", "=", "{", "}", "\n", "for", "k", "in", "slurm_env_keys", ":", "\n", "            ", "d_key", "=", "k", ".", "replace", "(", "\"SLURM_\"", ",", "\"\"", ")", ".", "replace", "(", "\"SLURMD_\"", ",", "\"\"", ")", ".", "lower", "(", ")", "\n", "slurm_data", "[", "d_key", "]", "=", "os", ".", "environ", "[", "k", "]", "\n", "", "", "else", ":", "\n", "        ", "slurm_data", "=", "None", "\n", "", "return", "dict", "(", "\n", "date_start", "=", "date_start", ",", "\n", "date_end", "=", "None", ",", "\n", "successful", "=", "False", ",", "\n", "git", "=", "git_data", ",", "\n", "slurm", "=", "slurm_data", ",", "\n", "env", "=", "os", ".", "environ", ".", "copy", "(", ")", ",", "\n", ")", "\n"]]}