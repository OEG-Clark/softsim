{"home.repos.pwc.inspect_result.jasonyzhang_ners.None.main.get_parser": [[42, 134], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.set_defaults"], "function", ["None"], ["def", "get_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--instance-dir\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "help", "=", "\"Path to instance directory.\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--output-dir\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "None", ",", "\n", "help", "=", "\"Path to output directory (Defaults to output/<instance directory>).\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--mvmc\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"If set, uses MVMC dataset loader.\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--export-mesh\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"If set, exports textured mesh to an obj file.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--force\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"If set, overwrites existing predictions.\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--predict-illumination\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "dest", "=", "\"predict_illumination\"", ",", "\n", "help", "=", "\"If True, predicts an environment map to model illumination.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--no-predict-illumination\"", ",", "\n", "action", "=", "\"store_false\"", ",", "\n", "dest", "=", "\"predict_illumination\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--symmetrize\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "dest", "=", "\"symmetrize\"", ",", "\n", "help", "=", "\"If set, makes object symmetric about the y-z plane axis\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--no-symmetrize\"", ",", "\n", "action", "=", "\"store_false\"", ",", "\n", "dest", "=", "\"symmetrize\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--num-frames\"", ",", "\n", "default", "=", "360", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Number of frames for video visualization.\"", ",", "\n", ")", "\n", "# Hyperparameters", "\n", "parser", ".", "add_argument", "(", "\n", "\"--num-iterations-camera\"", ",", "\n", "default", "=", "500", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Number of iterations to optimize camera pose.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--num-iterations-shape\"", ",", "\n", "default", "=", "500", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Number of iterations to optimize object shape.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--num-iterations-texture\"", ",", "\n", "default", "=", "3000", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Number of iterations to learn texture network.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--num-iterations-radiance\"", ",", "\n", "default", "=", "500", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Number of iterations to learn illumination.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--fov-init\"", ",", "default", "=", "60.0", ",", "type", "=", "float", ",", "help", "=", "\"Initial field of view.\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--L\"", ",", "type", "=", "int", ",", "default", "=", "10", ",", "help", "=", "\"Number of bases for positiional encoding.\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--num-layers-shape\"", ",", "type", "=", "int", ",", "default", "=", "4", ",", "help", "=", "\"Number of layers in f_shape.\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--num-layers-tex\"", ",", "type", "=", "int", ",", "default", "=", "12", ",", "help", "=", "\"Number of layers in f_tex.\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--num-layers-env\"", ",", "type", "=", "int", ",", "default", "=", "4", ",", "help", "=", "\"Number of layers in f_env.\"", "\n", ")", "\n", "parser", ".", "set_defaults", "(", "predict_illumination", "=", "True", ",", "symmetrize", "=", "True", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.None.main.main": [[136, 234], ["print", "os.makedirs", "os.makedirs", "os.join", "print", "torch.cuda.device_count", "ners.Ners", "os.basename", "ners.Ners.visualize_input_views", "ners.Ners.optimize_camera", "ners.Ners.visualize_input_views", "ners.Ners.optimize_shape", "ners.Ners.visualize_input_views", "ners.Ners.optimize_texture", "ners.Ners.visualize_input_views", "ners.Ners.make_video", "ners.Ners.save_parameters", "os.join", "os.exists", "print", "list", "list", "ners.data.load_car_data", "ners.models.load_car_model", "ners.data.load_data_from_dir", "ners.models.TemplateUV", "ners.models.pretrain_template_uv", "os.join", "ners.Ners.save_obj", "os.join", "torch.cuda.empty_cache", "ners.Ners.optimize_radiance", "ners.Ners.visualize_input_views", "ners.Ners.make_video", "os.basename", "range", "range", "ValueError", "os.join", "os.join", "os.join", "os.join", "os.join", "os.join"], "function", ["home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.visualize_input_views", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.optimize_camera", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.visualize_input_views", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.optimize_shape", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.visualize_input_views", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.optimize_texture", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.visualize_input_views", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.make_video", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.save_parameters", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.data.load_car_data", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.load_car_model", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.data.load_data_from_dir", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.pretrain_template_uv", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.save_obj", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.optimize_radiance", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.visualize_input_views", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.make_video"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "print", "(", "args", ")", "\n", "\n", "instance_dir", "=", "args", ".", "instance_dir", "\n", "output_dir", "=", "args", ".", "output_dir", "\n", "if", "output_dir", "is", "None", ":", "\n", "        ", "output_dir", "=", "osp", ".", "join", "(", "\"output\"", ",", "osp", ".", "basename", "(", "instance_dir", ")", ")", "\n", "", "os", ".", "makedirs", "(", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "weights_path", "=", "osp", ".", "join", "(", "output_dir", ",", "\"weights.pth\"", ")", "\n", "if", "not", "args", ".", "force", "and", "osp", ".", "exists", "(", "weights_path", ")", ":", "\n", "        ", "print", "(", "\n", "\"Weights already exist at {}. Use --force to override.\"", ".", "format", "(", "weights_path", ")", "\n", ")", "\n", "return", "\n", "", "print", "(", "\"Saving weights to {}\"", ".", "format", "(", "weights_path", ")", ")", "\n", "\n", "num_gpus", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "if", "args", ".", "predict_illumination", ":", "\n", "        ", "gpu_ids", "=", "list", "(", "range", "(", "num_gpus", "-", "1", ")", ")", "\n", "gpu_id_illumination", "=", "num_gpus", "-", "1", "\n", "", "else", ":", "\n", "        ", "gpu_ids", "=", "list", "(", "range", "(", "num_gpus", ")", ")", "\n", "gpu_id_illumination", "=", "None", "\n", "\n", "", "if", "args", ".", "mvmc", ":", "\n", "        ", "data", "=", "load_car_data", "(", "instance_dir", ",", "use_optimized_cameras", "=", "True", ",", "image_size", "=", "256", ")", "\n", "f_template", "=", "load_car_model", "(", ")", "\n", "", "else", ":", "\n", "        ", "data", "=", "load_data_from_dir", "(", "instance_dir", ",", "image_size", "=", "256", ")", "\n", "if", "\"extents\"", "not", "in", "data", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"For your own objects, please specify the cuboid extents in \"", "\n", "\"metadata.json.\"", "\n", ")", "\n", "", "f_template", "=", "TemplateUV", "(", "L", "=", "10", ")", "\n", "f_template", "=", "pretrain_template_uv", "(", "f_template", ",", "extents", "=", "data", "[", "\"extents\"", "]", ")", "\n", "", "ners", "=", "Ners", "(", "\n", "images", "=", "data", "[", "\"images\"", "]", ",", "\n", "masks", "=", "data", "[", "\"masks\"", "]", ",", "\n", "masks_dt", "=", "data", "[", "\"masks_dt\"", "]", ",", "\n", "initial_poses", "=", "data", "[", "\"initial_poses\"", "]", ",", "\n", "image_center", "=", "data", "[", "\"image_centers\"", "]", ",", "\n", "crop_scale", "=", "data", "[", "\"crop_scales\"", "]", ",", "\n", "f_template", "=", "f_template", ",", "\n", "fov", "=", "args", ".", "fov_init", ",", "\n", "jitter_uv", "=", "True", ",", "\n", "gpu_ids", "=", "gpu_ids", ",", "\n", "gpu_id_illumination", "=", "gpu_id_illumination", ",", "\n", "L", "=", "args", ".", "L", ",", "\n", "symmetrize", "=", "args", ".", "symmetrize", ",", "\n", "num_layers_shape", "=", "args", ".", "num_layers_shape", ",", "\n", "num_layers_tex", "=", "args", ".", "num_layers_tex", ",", "\n", "num_layers_env", "=", "args", ".", "num_layers_env", ",", "\n", ")", "\n", "name", "=", "osp", ".", "basename", "(", "instance_dir", ")", "\n", "ners", ".", "visualize_input_views", "(", "\n", "filename", "=", "osp", ".", "join", "(", "output_dir", ",", "f\"{name}_1_initial_cameras.jpg\"", ")", ",", "\n", "title", "=", "f\"{name} Initial Cameras\"", ",", "\n", ")", "\n", "ners", ".", "optimize_camera", "(", "num_iterations", "=", "args", ".", "num_iterations_camera", ")", "\n", "ners", ".", "visualize_input_views", "(", "\n", "filename", "=", "osp", ".", "join", "(", "output_dir", ",", "f\"{name}_2_optimized_cameras.jpg\"", ")", ",", "\n", "title", "=", "f\"{name} Optimized Cameras\"", ",", "\n", ")", "\n", "ners", ".", "optimize_shape", "(", "num_iterations", "=", "args", ".", "num_iterations_shape", ")", "\n", "ners", ".", "visualize_input_views", "(", "\n", "filename", "=", "osp", ".", "join", "(", "output_dir", ",", "f\"{name}_3_optimized_shape.jpg\"", ")", ",", "\n", "title", "=", "f\"{name} Optimized Shape\"", ",", "\n", ")", "\n", "ners", ".", "optimize_texture", "(", "num_iterations", "=", "args", ".", "num_iterations_texture", ")", "\n", "ners", ".", "visualize_input_views", "(", "\n", "filename", "=", "osp", ".", "join", "(", "output_dir", ",", "f\"{name}_4_optimized_texture.jpg\"", ")", ",", "\n", "title", "=", "f\"{name} Optimized Texture\"", ",", "\n", ")", "\n", "if", "args", ".", "export_mesh", ":", "\n", "        ", "mesh_name", "=", "osp", ".", "join", "(", "output_dir", ",", "f\"{name}_mesh.obj\"", ")", "\n", "ners", ".", "save_obj", "(", "mesh_name", ")", "\n", "", "ners", ".", "make_video", "(", "\n", "osp", ".", "join", "(", "output_dir", ",", "f\"{name}_video_texture_only\"", ")", ",", "\n", "use_antialiasing", "=", "True", ",", "\n", "visuals", "=", "(", "\"nn\"", ",", "\"albedo\"", ")", ",", "\n", "num_frames", "=", "args", ".", "num_frames", ",", "\n", ")", "\n", "if", "args", ".", "predict_illumination", ":", "\n", "        ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "ners", ".", "optimize_radiance", "(", "num_iterations", "=", "args", ".", "num_iterations_radiance", ")", "\n", "\n", "ners", ".", "visualize_input_views", "(", "\n", "filename", "=", "osp", ".", "join", "(", "output_dir", ",", "f\"{name}_5_optimized_radiance.jpg\"", ")", ",", "\n", "title", "=", "f\"{name} Optimized Radiance\"", ",", "\n", ")", "\n", "ners", ".", "make_video", "(", "\n", "osp", ".", "join", "(", "output_dir", ",", "f\"{name}_video\"", ")", ",", "\n", "use_antialiasing", "=", "True", ",", "\n", "visuals", "=", "(", "\"nn\"", ",", "\"full\"", ",", "\"albedo\"", ",", "\"lighting\"", ")", ",", "\n", "num_frames", "=", "args", ".", "num_frames", ",", "\n", ")", "\n", "", "ners", ".", "save_parameters", "(", "weights_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.HarmonicEmbedding.__init__": [[84, 112], ["super().__init__", "models.HarmonicEmbedding.register_buffer", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.__init__"], ["    ", "def", "__init__", "(", "self", ",", "n_harmonic_functions", "=", "10", ",", "omega0", "=", "0.1", ")", ":", "\n", "        ", "\"\"\"\n        Positional Embedding implementation (adapted from Pytorch3D).\n\n        Given an input tensor `x` of shape [minibatch, ... , dim],\n        the harmonic embedding layer converts each feature\n        in `x` into a series of harmonic features `embedding`\n        as follows:\n            embedding[..., i*dim:(i+1)*dim] = [\n                sin(x[..., i]),\n                sin(2*x[..., i]),\n                sin(4*x[..., i]),\n                ...\n                sin(2**self.n_harmonic_functions * x[..., i]),\n                cos(x[..., i]),\n                cos(2*x[..., i]),\n                cos(4*x[..., i]),\n                ...\n                cos(2**self.n_harmonic_functions * x[..., i])\n            ]\n\n        Note that `x` is also premultiplied by `omega0` before\n        evaluting the harmonic functions.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "register_buffer", "(", "\n", "\"frequencies\"", ",", "\n", "omega0", "*", "(", "2.0", "**", "torch", ".", "arange", "(", "n_harmonic_functions", ")", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.HarmonicEmbedding.forward": [[114, 124], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "embed.sin", "embed.cos"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x: tensor of shape [..., dim]\n        Returns:\n            embedding: a harmonic embedding of `x`\n                of shape [..., n_harmonic_functions * dim * 2]\n        \"\"\"", "\n", "embed", "=", "(", "x", "[", "...", ",", "None", "]", "*", "self", ".", "frequencies", ")", ".", "view", "(", "*", "x", ".", "shape", "[", ":", "-", "1", "]", ",", "-", "1", ")", "\n", "return", "torch", ".", "cat", "(", "(", "embed", ".", "sin", "(", ")", ",", "embed", ".", "cos", "(", ")", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.BaseNetwork.__init__": [[127, 130], ["torch.Module.__init__", "models.HarmonicEmbedding"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.__init__"], ["    ", "def", "__init__", "(", "self", ",", "n_harmonic_functions", "=", "10", ",", "omega0", "=", "0.1", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "positional_encoding", "=", "HarmonicEmbedding", "(", "n_harmonic_functions", ",", "omega0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.BaseNetwork.get_device": [[131, 140], ["next", "models.BaseNetwork.parameters"], "methods", ["None"], ["", "def", "get_device", "(", "self", ",", "default_device", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Returns which device the module is on. If wrapped in DataParallel, will return\n        the default device.\n        \"\"\"", "\n", "try", ":", "\n", "            ", "return", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "device", "\n", "", "except", "StopIteration", ":", "\n", "            ", "return", "default_device", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.TemplateUV.__init__": [[143, 160], ["models.BaseNetwork.__init__", "range", "layers.append", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.zeros_", "torch.init.zeros_", "torch.Sequential", "torch.Sequential", "layers.append", "layers.append", "torch.Linear", "torch.Linear", "layers.append", "layers.append", "torch.LayerNorm", "torch.LayerNorm", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "num_layers", "=", "3", ",", "input_size", "=", "3", ",", "output_size", "=", "3", ",", "hidden_size", "=", "256", ",", "L", "=", "10", "\n", ")", ":", "\n", "        ", "input_size", "=", "L", "*", "2", "*", "input_size", "\n", "super", "(", ")", ".", "__init__", "(", "n_harmonic_functions", "=", "L", ")", "\n", "layers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_layers", "-", "1", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "layers", ".", "append", "(", "nn", ".", "Linear", "(", "input_size", ",", "hidden_size", ")", ")", "\n", "", "else", ":", "\n", "                ", "layers", ".", "append", "(", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ")", ")", "\n", "", "layers", ".", "append", "(", "nn", ".", "LayerNorm", "(", "hidden_size", ")", ")", "\n", "layers", ".", "append", "(", "nn", ".", "LeakyReLU", "(", ")", ")", "\n", "", "layers", ".", "append", "(", "nn", ".", "Linear", "(", "hidden_size", ",", "output_size", ")", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "layers", "[", "-", "1", "]", ".", "weight", ",", "gain", "=", "0.001", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "layers", "[", "-", "1", "]", ".", "bias", ")", "\n", "self", ".", "mlp", "=", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.TemplateUV.forward": [[161, 169], ["x.to.to.to", "models.TemplateUV.positional_encoding", "models.TemplateUV.mlp", "models.TemplateUV.get_device", "x.to.to.norm"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.utils.perceptual_loss.PerceptualLoss.get_device"], ["", "def", "forward", "(", "self", ",", "x", ",", "normalize", "=", "True", ")", ":", "\n", "        ", "temp_device", "=", "x", ".", "device", "\n", "x", "=", "x", ".", "to", "(", "self", ".", "get_device", "(", "temp_device", ")", ")", "\n", "if", "normalize", ":", "\n", "            ", "x", "=", "x", "/", "(", "x", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", ")", "\n", "", "h", "=", "self", ".", "positional_encoding", "(", "x", ")", "\n", "h", "=", "self", ".", "mlp", "(", "h", ")", "\n", "return", "(", "x", "+", "h", ")", ".", "to", "(", "temp_device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.DeltaUV.__init__": [[172, 189], ["models.BaseNetwork.__init__", "range", "layers.append", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.zeros_", "torch.init.zeros_", "torch.Sequential", "torch.Sequential", "layers.append", "layers.append", "torch.Linear", "torch.Linear", "layers.append", "layers.append", "torch.LayerNorm", "torch.LayerNorm", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "num_layers", "=", "3", ",", "input_size", "=", "3", ",", "output_size", "=", "3", ",", "hidden_size", "=", "256", ",", "L", "=", "10", "\n", ")", ":", "\n", "        ", "input_size", "=", "L", "*", "2", "*", "input_size", "\n", "super", "(", ")", ".", "__init__", "(", "n_harmonic_functions", "=", "L", ")", "\n", "layers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_layers", "-", "1", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "layers", ".", "append", "(", "nn", ".", "Linear", "(", "input_size", ",", "hidden_size", ")", ")", "\n", "", "else", ":", "\n", "                ", "layers", ".", "append", "(", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ")", ")", "\n", "", "layers", ".", "append", "(", "nn", ".", "LayerNorm", "(", "hidden_size", ")", ")", "\n", "layers", ".", "append", "(", "nn", ".", "LeakyReLU", "(", ")", ")", "\n", "", "layers", ".", "append", "(", "nn", ".", "Linear", "(", "hidden_size", ",", "output_size", ")", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "layers", "[", "-", "1", "]", ".", "weight", ",", "gain", "=", "0.001", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "layers", "[", "-", "1", "]", ".", "bias", ")", "\n", "self", ".", "mlp", "=", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.DeltaUV.forward": [[190, 196], ["models.DeltaUV.to", "models.DeltaUV.positional_encoding", "models.DeltaUV.mlp", "models.DeltaUV.to", "models.DeltaUV.get_device"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.utils.perceptual_loss.PerceptualLoss.get_device"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "temp_device", "=", "x", ".", "device", "\n", "x", "=", "x", ".", "to", "(", "self", ".", "get_device", "(", "temp_device", ")", ")", "\n", "x", "=", "self", ".", "positional_encoding", "(", "x", ")", "\n", "x", "=", "self", ".", "mlp", "(", "x", ")", "\n", "return", "x", ".", "to", "(", "temp_device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.ImplicitTextureNet.__init__": [[199, 254], ["models.BaseNetwork.__init__", "torch.LayerNorm", "torch.LayerNorm", "range", "layers.append", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.zeros_", "torch.init.zeros_", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.LeakyReLU", "torch.LeakyReLU", "layers.append", "layers.append", "layers.append", "torch.Linear", "torch.Linear", "torch.Sigmoid", "torch.Sigmoid", "torch.Linear", "torch.Linear", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Tanh", "torch.Tanh", "Exception"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "num_layers", "=", "8", ",", "\n", "input_size", "=", "3", ",", "\n", "hidden_size", "=", "256", ",", "\n", "output_size", "=", "3", ",", "\n", "L", "=", "10", ",", "\n", "max_batch_size", "=", "10000", ",", "\n", "output_activation", "=", "\"sigmoid\"", ",", "\n", "gain", "=", "0.01", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Texture prediction network mapping UV to RGB.\n\n        Args:\n            num_layers (int, optional): Number of layers. Defaults to 12.\n            input_size (int, optional): Dimension of input. Defaults to 3.\n            hidden_size (int, optional): Dimension of hidden layers. Defaults to 256.\n            output_size (int, optional): Dimension of output. Defaults to 3.\n            L (int, optional): Number of frequencies for positional encoding. Defaults\n                to 6.\n            max_batch_size (int, optional): Maximum batch size. If over, automatically\n                computes separate batches when using `forward_batched`. Defaults to\n                10000.\n            output_activation (str, optional): Output activation function can be\n                \"sigmoid\" if outputting RGB or \"tanh\" if outputting deltas. Defaults to\n                \"sigmoid\".\n            gain (float, optional): Gain for output activation to initialize near 0.5.\n        \"\"\"", "\n", "input_size", "=", "input_size", "*", "L", "*", "2", "\n", "super", "(", ")", ".", "__init__", "(", "n_harmonic_functions", "=", "L", ",", "omega0", "=", "0.1", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "output_size", "=", "output_size", "\n", "norm", "=", "nn", ".", "LayerNorm", "(", "hidden_size", ")", "\n", "layers", "=", "[", "nn", ".", "Linear", "(", "input_size", ",", "hidden_size", ")", ",", "norm", ",", "nn", ".", "LeakyReLU", "(", ")", "]", "\n", "for", "_", "in", "range", "(", "num_layers", "-", "2", ")", ":", "\n", "            ", "layers", ".", "append", "(", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ")", ")", "\n", "layers", ".", "append", "(", "norm", ")", "\n", "layers", ".", "append", "(", "nn", ".", "LeakyReLU", "(", ")", ")", "\n", "", "layers", ".", "append", "(", "nn", ".", "Linear", "(", "hidden_size", ",", "output_size", ")", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "layers", "[", "-", "1", "]", ".", "weight", ",", "gain", "=", "gain", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "layers", "[", "-", "1", "]", ".", "bias", ")", "\n", "\n", "self", ".", "mlp", "=", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "if", "output_activation", "==", "\"sigmoid\"", ":", "\n", "            ", "self", ".", "final_activation", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "", "elif", "output_activation", "==", "\"tanh\"", ":", "\n", "            ", "self", ".", "final_activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\n", "f\"Final activation must be sigmoid or tanh. Got: {output_activation}.\"", "\n", ")", "\n", "", "self", ".", "max_batch_size", "=", "max_batch_size", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.ImplicitTextureNet.forward": [[255, 284], ["x.to.to.reshape", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "y.reshape.reshape.reshape", "y.reshape.reshape.float", "x.to.to.to", "models.ImplicitTextureNet.positional_encoding", "models.ImplicitTextureNet.mlp", "models.ImplicitTextureNet.final_activation", "x.to.to.to", "len", "models.ImplicitTextureNet.get_device", "x.to.to.norm"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.utils.perceptual_loss.PerceptualLoss.get_device"], ["", "def", "forward", "(", "self", ",", "x", ",", "normalize", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x: (B,3)\n\n        Returns:\n            y: (B,3)\n        \"\"\"", "\n", "shape", "=", "x", ".", "shape", "\n", "x", "=", "x", ".", "reshape", "(", "-", "1", ",", "shape", "[", "-", "1", "]", ")", "\n", "\n", "# The points outside of the mesh also get passed into TexNet, which is a lot of", "\n", "# unnecessary computation. We will skip over those points, which correspond to", "\n", "# (0, 0, 0)", "\n", "mask", "=", "torch", ".", "any", "(", "x", "!=", "0", ",", "dim", "=", "1", ")", "\n", "x", "=", "x", "[", "mask", "]", "\n", "temp_device", "=", "x", ".", "device", "\n", "if", "torch", ".", "any", "(", "mask", ")", ":", "\n", "            ", "x", "=", "x", ".", "to", "(", "self", ".", "get_device", "(", "temp_device", ")", ")", "\n", "if", "normalize", ":", "\n", "                ", "x", "=", "x", "/", "(", "x", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "+", "1e-6", ")", "# Project to sphere.", "\n", "", "x", "=", "self", ".", "positional_encoding", "(", "x", ")", "\n", "x", "=", "self", ".", "mlp", "(", "x", ")", "\n", "x", "=", "self", ".", "final_activation", "(", "x", ")", "\n", "x", "=", "x", ".", "to", "(", "temp_device", ")", "\n", "", "y", "=", "torch", ".", "ones", "(", "len", "(", "mask", ")", ",", "self", ".", "output_size", ",", "device", "=", "temp_device", ")", "\n", "y", "[", "mask", "]", "=", "x", "\n", "y", "=", "y", ".", "reshape", "(", "shape", "[", ":", "-", "1", "]", "+", "(", "-", "1", ",", ")", ")", "\n", "return", "y", ".", "float", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.ImplicitTextureNet.forward_batched": [[285, 305], ["range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "models.ImplicitTextureNet.forward", "y.append"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.rasterizer.MeshRasterizer.forward"], ["", "def", "forward_batched", "(", "self", ",", "x", ",", "batch_size", "=", "None", ",", "normalize", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Computes forward pass using minibatches to reduce memory usage of forward pass.\n\n        Args:\n            x (B,3).\n\n        Returns:\n            y (B,3).\n        \"\"\"", "\n", "n", "=", "x", ".", "shape", "[", "0", "]", "\n", "b", "=", "self", ".", "max_batch_size", "if", "batch_size", "is", "None", "else", "batch_size", "\n", "y", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "n", ",", "b", ")", ":", "\n", "            ", "pred", "=", "self", ".", "forward", "(", "\n", "x", "[", "i", ":", "i", "+", "b", "]", ",", "\n", "normalize", "=", "normalize", ",", "\n", ")", "\n", "y", ".", "append", "(", "pred", ")", "\n", "", "return", "torch", ".", "cat", "(", "y", ",", "dim", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.ImplicitTextureNet.save_model": [[306, 309], ["torch.save", "torch.save", "torch.save", "torch.save", "models.ImplicitTextureNet.state_dict"], "methods", ["None"], ["", "def", "save_model", "(", "self", ",", "name", ")", ":", "\n", "        ", "path", "=", "f\"{name}.pth\"", "\n", "torch", ".", "save", "(", "self", ".", "state_dict", "(", ")", ",", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.ImplicitTextureNet.load_model": [[310, 313], ["models.ImplicitTextureNet.load_state_dict", "torch.load", "torch.load", "torch.load", "torch.load"], "methods", ["None"], ["", "def", "load_model", "(", "self", ",", "name", ")", ":", "\n", "        ", "path", "=", "f\"{name}.pth\"", "\n", "self", ".", "load_state_dict", "(", "torch", ".", "load", "(", "path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.ImplicitTextureNet.unwrap_uv_map": [[314, 341], ["torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "ners.spherical_to_cartesian", "torch.dstack().cuda", "torch.dstack().cuda", "torch.dstack().cuda", "torch.dstack().cuda", "models.ImplicitTextureNet.forward", "pred_texture.repeat.repeat.reshape", "torch.dstack().cuda.reshape", "torch.dstack().cuda.reshape", "pred_texture.repeat.repeat.repeat", "torch.dstack", "torch.dstack", "torch.dstack", "torch.dstack"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.spherical_to_cartesian", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.rasterizer.MeshRasterizer.forward"], ["", "def", "unwrap_uv_map", "(", "self", ",", "height", "=", "256", ",", "width", "=", "256", ",", "margin", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        Unwraps the tex_net into a UV Image.\n\n        Args:\n            tex_net (ImplicitTextureNet): Texture network mapping from spherical coordinates\n                to RGB.\n            height (int, optional): Height of UV image. Defaults to 256.\n            width (int, optional): Width of UV image. Defaults to 256.\n            margin (float, optional): Width of redundancy on right side. Defaults to 0\n                (no margin).\n\n        Returns:\n            tensor: Unwrapped texture (H, W, 3).\n        \"\"\"", "\n", "theta", "=", "torch", ".", "linspace", "(", "0", ",", "np", ".", "pi", ",", "height", ")", "\n", "phi", "=", "torch", ".", "linspace", "(", "-", "np", ".", "pi", ",", "np", ".", "pi", "*", "(", "1", "+", "margin", ")", ",", "width", ")", "\n", "theta", ",", "phi", "=", "torch", ".", "meshgrid", "(", "theta", ",", "phi", ")", "\n", "x", ",", "y", ",", "z", "=", "geom_util", ".", "spherical_to_cartesian", "(", "theta", ",", "phi", ")", "\n", "coords", "=", "torch", ".", "dstack", "(", "(", "x", ",", "y", ",", "z", ")", ")", ".", "cuda", "(", ")", "\n", "shape", "=", "coords", ".", "shape", "[", ":", "2", "]", "+", "(", "3", ",", ")", "\n", "pred_texture", "=", "self", ".", "forward", "(", "coords", ".", "reshape", "(", "-", "1", ",", "3", ")", ")", "\n", "if", "pred_texture", ".", "shape", "[", "1", "]", "==", "1", ":", "\n", "# For single channel environment maps.", "\n", "            ", "pred_texture", "=", "pred_texture", ".", "repeat", "(", "1", ",", "3", ")", "\n", "", "pred_texture", "=", "pred_texture", ".", "reshape", "(", "shape", ")", "\n", "return", "pred_texture", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.ImplicitTextureNet.clone": [[342, 344], ["copy.deepcopy"], "methods", ["None"], ["", "def", "clone", "(", "self", ")", ":", "\n", "        ", "return", "copy", ".", "deepcopy", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.EnvironmentMap.__init__": [[347, 350], ["models.ImplicitTextureNet.__init__"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "use_single_channel", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.EnvironmentMap.forward": [[351, 365], ["x.mean.mean.to", "models.EnvironmentMap.positional_encoding", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "x.mean.mean.to", "models.EnvironmentMap.get_device", "x.mean.mean.mean", "models.EnvironmentMap.mlp", "x.mean.mean.norm"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.utils.perceptual_loss.PerceptualLoss.get_device"], ["", "def", "forward", "(", "self", ",", "x", ",", "normalize", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "        ", "temp_device", "=", "x", ".", "device", "\n", "x", "=", "x", ".", "to", "(", "self", ".", "get_device", "(", "temp_device", ")", ")", "\n", "\n", "if", "normalize", ":", "\n", "            ", "x", "=", "x", "/", "(", "x", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "+", "1e-6", ")", "# Project to sphere.", "\n", "", "x", "=", "self", ".", "positional_encoding", "(", "x", ")", "\n", "# We will let the environment map's lighting be non-negative unbounded,", "\n", "# initialized at 3 (~75% brightness).", "\n", "x", "=", "torch", ".", "relu", "(", "self", ".", "mlp", "(", "x", ")", "+", "3", ")", "\n", "if", "self", ".", "use_single_channel", ":", "\n", "            ", "x", "=", "x", ".", "mean", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "# x = x.repeat((1,) * (x.ndim - 1) + (3,))  # repeat last dimension 3x", "\n", "", "return", "x", ".", "to", "(", "temp_device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.Symmetrize.__init__": [[368, 377], ["models.BaseNetwork.__init__"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.__init__"], ["    ", "def", "__init__", "(", "self", ",", "uv3d", ",", "sym_axis", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            uv3d: module to symmetrize\n            sym_axis: axis of symmetry: 0 -> X, 1 -> Y, 2-> Z\n        \"\"\"", "\n", "super", "(", "Symmetrize", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "uv3d", "=", "uv3d", "\n", "self", ".", "sym_axis", "=", "sym_axis", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.Symmetrize.forward": [[378, 387], ["models.Symmetrize.uv3d", "models.Symmetrize.uv3d"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "uvs", ",", "*", "args", ")", ":", "\n", "        ", "uvs_ref", "=", "uvs", "*", "1", "\n", "\n", "uvs_ref", "[", "...", ",", "[", "self", ".", "sym_axis", "]", "]", "*=", "-", "1", "\n", "pred", "=", "self", ".", "uv3d", "(", "uvs", ",", "*", "args", ")", "\n", "pred_ref", "=", "self", ".", "uv3d", "(", "uvs_ref", ",", "*", "args", ")", "\n", "pred_ref", "[", "...", ",", "[", "self", ".", "sym_axis", "]", "]", "*=", "-", "1", "\n", "\n", "return", "(", "pred", "+", "pred_ref", ")", "*", "0.5", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.pretrain_template_uv": [[16, 68], ["template_uv.to.to", "torch.optim.Adam", "torch.optim.Adam", "ners.create_sphere", "trimesh.creation.box", "torch.tensor().float", "torch.tensor().float", "torch.tensor().long", "torch.tensor().long", "verts.to.to", "faces.to.to", "verts.to.norm", "template_uv.to.parameters", "tqdm.auto.tqdm", "range", "torch.optim.Adam.zero_grad", "ners.utils.sample_consistent_points", "template_uv.to.", "pytorch3d.structures.Meshes", "torch.mean", "torch.mean", "pytorch3d.loss.mesh_laplacian_smoothing", "loss.backward", "torch.optim.Adam.step", "loop.set_description", "range", "uvs.to", "template_uv.to.", "sphere_fs.unsqueeze", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "ners.random_rotation", "targets.to", "loss.item"], "function", ["home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.create_sphere", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.sampling.sample_consistent_points", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.random_rotation"], ["def", "pretrain_template_uv", "(", "\n", "template_uv", ",", "\n", "verts", "=", "None", ",", "\n", "faces", "=", "None", ",", "\n", "extents", "=", "None", ",", "\n", "num_iterations", "=", "1000", ",", "\n", "num_samples", "=", "1000", ",", "\n", "sphere_level", "=", "5", ",", "\n", "device", "=", "\"cuda:0\"", ",", "\n", "pbar", "=", "True", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Pretrains the template UV shape model. Must be initialized either with vertices and\n    faces or with 3D cuboid extents.\n\n    Args:\n        verts (torch.Tensor): (N_v, 3) tensor of vertices.\n        faces (torch.Tensor): (N_f, 3) tensor of faces.\n        extents (list): list of 3D cuboid extents (w, h, d).\n\n    Returns:\n        template_uv (TemplateUV): pretrained template UV shape model mapping from uv\n            coordinates (..., 3) to 3D vertex coordinates (..., 3).\n    \"\"\"", "\n", "template_uv", "=", "template_uv", ".", "to", "(", "device", ")", "\n", "if", "verts", "is", "None", ":", "\n", "        ", "tmesh", "=", "trimesh", ".", "creation", ".", "box", "(", "extents", "=", "extents", ")", "\n", "verts", "=", "torch", ".", "tensor", "(", "tmesh", ".", "vertices", ",", "device", "=", "device", ")", ".", "float", "(", ")", "\n", "faces", "=", "torch", ".", "tensor", "(", "tmesh", ".", "faces", ",", "device", "=", "device", ")", ".", "long", "(", ")", "\n", "", "else", ":", "\n", "        ", "verts", "=", "verts", ".", "to", "(", "device", ")", "\n", "faces", "=", "faces", ".", "to", "(", "device", ")", "\n", "", "verts_sphere", "=", "verts", "/", "verts", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "optim", "=", "torch", ".", "optim", ".", "Adam", "(", "template_uv", ".", "parameters", "(", ")", ",", "lr", "=", "0.001", ")", "\n", "sphere_vs", ",", "sphere_fs", "=", "geom_util", ".", "create_sphere", "(", "level", "=", "sphere_level", ",", "device", "=", "device", ")", "\n", "loop", "=", "tqdm", "(", "range", "(", "num_iterations", ")", ")", "if", "pbar", "else", "range", "(", "num_iterations", ")", "\n", "for", "_", "in", "loop", ":", "\n", "        ", "optim", ".", "zero_grad", "(", ")", "\n", "targets", ",", "uvs", "=", "sample_consistent_points", "(", "\n", "verts", ",", "faces", ",", "[", "verts", ",", "verts_sphere", "]", ",", "num_samples", "\n", ")", "\n", "pred_vs", "=", "template_uv", "(", "uvs", ".", "to", "(", "device", ")", ",", "normalize", "=", "True", ")", "\n", "sv", "=", "(", "sphere_vs", "@", "geom_util", ".", "random_rotation", "(", "device", ")", ")", ".", "unsqueeze", "(", "0", ")", "\n", "meshes", "=", "Meshes", "(", "template_uv", "(", "sv", ",", "normalize", "=", "True", ")", ",", "sphere_fs", ".", "unsqueeze", "(", "0", ")", ")", "\n", "loss_reconstruction", "=", "torch", ".", "mean", "(", "(", "pred_vs", "-", "targets", ".", "to", "(", "device", ")", ")", "**", "2", ")", "\n", "loss_laplacian", "=", "mesh_laplacian_smoothing", "(", "meshes", ")", "\n", "loss", "=", "20", "*", "loss_reconstruction", "+", "loss_laplacian", "\n", "loss", ".", "backward", "(", ")", "\n", "optim", ".", "step", "(", ")", "\n", "loop", ".", "set_description", "(", "f\"Template: {loss.item():.4f}\"", ")", "\n", "\n", "", "return", "template_uv", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.shape_model_to_mesh": [[70, 81], ["shape_model.get_device", "ners.create_sphere", "pytorch3d.structures.Meshes", "pytorch3d.structures.Meshes.to", "pytorch3d.renderer.TexturesVertex", "shape_model", "sphere_vs.unsqueeze"], "function", ["home.repos.pwc.inspect_result.jasonyzhang_ners.utils.perceptual_loss.PerceptualLoss.get_device", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.create_sphere"], ["", "def", "shape_model_to_mesh", "(", "shape_model", ",", "sphere_level", "=", "4", ",", "textures", "=", "None", ")", ":", "\n", "    ", "device", "=", "shape_model", ".", "get_device", "(", "default_device", "=", "\"cuda:0\"", ")", "\n", "sphere_vs", ",", "sphere_fs", "=", "geom_util", ".", "create_sphere", "(", "level", "=", "sphere_level", ",", "device", "=", "device", ")", "\n", "if", "textures", "is", "None", ":", "\n", "        ", "textures", "=", "pytorch3d", ".", "renderer", ".", "TexturesVertex", "(", "(", "sphere_vs", ".", "unsqueeze", "(", "0", ")", "+", "1", ")", "/", "2", ")", "\n", "", "mesh", "=", "Meshes", "(", "\n", "[", "shape_model", "(", "sphere_vs", ")", "]", ",", "\n", "[", "sphere_fs", "]", ",", "\n", "textures", "=", "textures", ",", "\n", ")", "\n", "return", "mesh", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.load_car_model": [[389, 393], ["models.TemplateUV", "TemplateUV.load_state_dict", "torch.load", "torch.load"], "function", ["None"], ["", "", "def", "load_car_model", "(", "path", "=", "\"models/templates/car.pth\"", ")", ":", "\n", "    ", "template", "=", "TemplateUV", "(", "L", "=", "10", ",", "num_layers", "=", "3", ",", "hidden_size", "=", "256", ")", "\n", "template", ".", "load_state_dict", "(", "torch", ".", "load", "(", "path", ")", ")", "\n", "return", "template", "\n", "", ""]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.__init__": [[40, 190], ["torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "len", "torch.device", "torch.device", "torch.device", "torch.device", "numpy.stack", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "all_images_tensor.float().to().contiguous.float().to().contiguous.float().to().contiguous", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "all_images_tensor.float().to().contiguous.float().to().contiguous.clone", "ners.pytorch3d.get_renderers", "ners.pytorch3d.get_renderers", "ners.ImplicitTextureNet", "ners.ImplicitTextureNet", "ners.EnvironmentMap", "ners.EnvironmentMap", "torch.nn.parallel.DataParallel().to", "torch.nn.parallel.DataParallel().to", "torch.nn.parallel.DataParallel().to", "torch.nn.parallel.DataParallel().to", "torch.nn.parallel.DataParallel().to", "torch.nn.parallel.DataParallel().to", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "ners.pytorch3d.PerspectiveCameras", "ners.pytorch3d.PerspectiveCameras", "ners.create_sphere", "ners.create_sphere", "ners.Ners.get_pred_verts", "ners.Ners.get_pred_verts", "pytorch3d.renderer.TexturesVertex", "pytorch3d.structures.Meshes", "torch.MaxPool2d", "torch.MaxPool2d", "ners.utils.PerceptualLoss", "ners.utils.PerceptualLoss", "torch.nn.parallel.DataParallel().to", "torch.nn.parallel.DataParallel().to", "list", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "numpy.stack.transpose", "ners.DeltaUV", "ners.DeltaUV", "torch.nn.parallel.DataParallel().to", "torch.nn.parallel.DataParallel().to", "torch.MaxPool2d.", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "range", "len", "list.pop", "all_images_tensor.float().to().contiguous.float().to().contiguous.float().to", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "ners.Symmetrize", "ners.Symmetrize", "ners.Symmetrize", "ners.Symmetrize", "torch.nn.parallel.DataParallel", "torch.nn.parallel.DataParallel", "torch.nn.parallel.DataParallel", "torch.nn.parallel.DataParallel", "torch.nn.parallel.DataParallel", "torch.nn.parallel.DataParallel", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "numpy.stack", "torch.where", "torch.where", "torch.where", "torch.where", "torch.nn.parallel.DataParallel", "torch.nn.parallel.DataParallel", "ners.Ners.target_masks.unsqueeze().repeat", "ners.Ners.target_masks.unsqueeze().repeat", "torch.nn.parallel.DataParallel", "torch.nn.parallel.DataParallel", "sv.unsqueeze", "all_images_tensor.float().to().contiguous.float().to().contiguous.float", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "numpy.stack", "numpy.stack", "ners.Ners.target_masks.unsqueeze", "ners.Ners.target_masks.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.rendering.get_renderers", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.rendering.get_renderers", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.create_sphere", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.create_sphere", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.get_pred_verts", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.get_pred_verts"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "images", "=", "DEFAULTS", "[", "\"images\"", "]", ",", "\n", "masks", "=", "DEFAULTS", "[", "\"masks\"", "]", ",", "\n", "masks_dt", "=", "DEFAULTS", "[", "\"masks_dt\"", "]", ",", "\n", "initial_poses", "=", "DEFAULTS", "[", "\"initial_poses\"", "]", ",", "\n", "image_center", "=", "DEFAULTS", "[", "\"image_center\"", "]", ",", "\n", "crop_scale", "=", "DEFAULTS", "[", "\"crop_scale\"", "]", ",", "\n", "f_template", "=", "None", ",", "\n", "fov", "=", "60.0", ",", "\n", "jitter_uv", "=", "True", ",", "\n", "gpu_ids", "=", "None", ",", "\n", "gpu_id_illumination", "=", "None", ",", "\n", "use_template_as_shape", "=", "True", ",", "\n", "symmetrize", "=", "False", ",", "\n", "num_layers_shape", "=", "4", ",", "\n", "num_layers_tex", "=", "12", ",", "\n", "num_layers_env", "=", "4", ",", "\n", "L", "=", "6", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Wrapper for training and visualizing a NeRS model.\n\n        Args:\n            images (ndarray): Cropped masked images (N, H, W, 3).\n            masks (ndarray): Masks (N, H, W).\n            masks_dt (ndarray): Distance transforms (N, H, W).\n            initial_poses (ndarray): Initial camera poses (N, 3, 3).\n            image_center (ndarray): Principal points of images (N, 2).\n            crop_scale (ndarray): Crop scale (N,).\n            f_template (nn.Module): Template shape network mapping from UV to XYZ.\n            fov (float, optional): Initial camera field of view. Defaults to 60.0.\n            jitter_uv (bool, optional): If True, jitters the UVs each iteration to\n                improve coverage. Defaults to True.\n            gpu_ids (list, optional): List of GPU ids to run implicit networks on.\n                Defaults to all but one of the available GPUs.\n            gpu_id_illumination (int, optional): GPU id to run illumination computation\n                on. This is separate from gpu_ids because computing the illumination\n                effects is computationally expensive, so it is recommended to do this on\n                a separate GPU. Defaults to the last available GPU.\n            use_template_as_shape (bool, optional): If True, uses the template network\n                as the shape model. Otherwise, uses a separate shape model for\n                instance-specific deformation. Defaults to True.\n            symmetrize (bool, optional): If True, symmetrizes the mesh about the y-z\n                plane. Defaults to False.\n            num_layers_shape (int, optional): Number of layers for f_shape. Defaults\n                to 4.\n            num_layers_tex (int, optional): Number of layers for f_tex. The deeper, the\n                higher the resolution. Defaults to 8.\n            num_layers_env (int, optional): Number of lyaers for f_env. Defaults to 4.\n        \"\"\"", "\n", "num_gpus", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "assert", "num_gpus", ">", "0", ",", "\"No GPUs available.\"", "\n", "if", "not", "gpu_ids", ":", "\n", "            ", "gpu_ids", "=", "list", "(", "range", "(", "num_gpus", ")", ")", "\n", "if", "len", "(", "gpu_ids", ")", ">", "1", ":", "\n", "                ", "gpu_ids", ".", "pop", "(", "-", "1", ")", "\n", "", "", "self", ".", "N", "=", "len", "(", "images", ")", "\n", "device", "=", "torch", ".", "device", "(", "f\"cuda:{gpu_ids[0]}\"", ")", "# Device used for hanging params.", "\n", "self", ".", "device", "=", "device", "\n", "if", "gpu_id_illumination", ":", "\n", "            ", "self", ".", "device_illumination", "=", "torch", ".", "device", "(", "f\"cuda:{gpu_id_illumination}\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "device_illumination", "=", "torch", ".", "device", "(", "f\"cuda:{num_gpus - 1}\"", ")", "\n", "", "self", ".", "use_template_as_shape", "=", "use_template_as_shape", "\n", "\n", "all_images", "=", "np", ".", "stack", "(", "images", ")", "\n", "all_images_tensor", "=", "torch", ".", "from_numpy", "(", "all_images", ".", "transpose", "(", "0", ",", "3", ",", "1", ",", "2", ")", ")", "\n", "all_images_tensor", "=", "all_images_tensor", ".", "float", "(", ")", ".", "to", "(", "device", ")", ".", "contiguous", "(", ")", "\n", "self", ".", "all_images", "=", "all_images", "\n", "self", ".", "all_images_tensor", "=", "all_images_tensor", "\n", "self", ".", "masks", "=", "masks", "\n", "self", ".", "target_masks", "=", "torch", ".", "from_numpy", "(", "np", ".", "stack", "(", "masks", ")", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "target_masks_dt", "=", "torch", ".", "from_numpy", "(", "np", ".", "stack", "(", "masks_dt", ")", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "images_masked", "=", "all_images_tensor", ".", "clone", "(", ")", "\n", "images_masked", "[", "self", ".", "target_masks", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "3", ",", "1", ",", "1", ")", "==", "0", "]", "=", "1", "\n", "self", ".", "images_masked", "=", "images_masked", "\n", "self", ".", "jitter_uv", "=", "jitter_uv", "\n", "self", ".", "mean_texture", "=", "None", "\n", "self", ".", "finetune_camera", "=", "True", "\n", "\n", "self", ".", "renderer_textured", ",", "self", ".", "renderer_silhouette", "=", "get_renderers", "(", "device", "=", "device", ")", "\n", "\n", "# Prepare implicit shape, texture, and environmental illumination networks.", "\n", "if", "not", "self", ".", "use_template_as_shape", ":", "\n", "# To model implicit shape, we can either use a deformation network that", "\n", "# learns offsets from the template network, or directly optimize the", "\n", "# template network itself. The latter is less compute intensive but may have", "\n", "# worse performance.", "\n", "            ", "f_shape", "=", "models", ".", "DeltaUV", "(", "num_layers", "=", "num_layers_shape", ",", "hidden_size", "=", "128", ")", "\n", "if", "symmetrize", ":", "\n", "                ", "f_shape", "=", "models", ".", "Symmetrize", "(", "f_shape", ")", "\n", "", "self", ".", "f_shape", "=", "DataParallel", "(", "f_shape", ",", "device_ids", "=", "gpu_ids", ")", ".", "to", "(", "device", ")", "\n", "", "else", ":", "\n", "            ", "if", "symmetrize", ":", "\n", "                ", "f_template", "=", "models", ".", "Symmetrize", "(", "f_template", ")", "\n", "", "self", ".", "f_shape", "=", "f_template", "\n", "\n", "", "f_tex", "=", "models", ".", "ImplicitTextureNet", "(", "\n", "num_layers", "=", "num_layers_tex", ",", "hidden_size", "=", "256", ",", "L", "=", "L", "\n", ")", "\n", "f_env", "=", "models", ".", "EnvironmentMap", "(", "\n", "num_layers", "=", "num_layers_env", ",", "hidden_size", "=", "128", ",", "L", "=", "L", ",", "gain", "=", "1", "\n", ")", "\n", "self", ".", "f_template", "=", "DataParallel", "(", "f_template", ",", "device_ids", "=", "gpu_ids", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "f_tex", "=", "DataParallel", "(", "f_tex", ",", "device_ids", "=", "gpu_ids", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "f_env", "=", "DataParallel", "(", "f_env", ",", "device_ids", "=", "gpu_ids", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "specularity", "=", "torch", ".", "tensor", "(", "1.0", ",", "device", "=", "device", ",", "requires_grad", "=", "True", ")", ".", "float", "(", ")", "\n", "self", ".", "shininess", "=", "torch", ".", "tensor", "(", "15.0", ",", "device", "=", "device", ",", "requires_grad", "=", "True", ")", ".", "float", "(", ")", "\n", "\n", "# Prepare initial camera poses.", "\n", "self", ".", "fov", "=", "torch", ".", "tensor", "(", "fov", ",", "device", "=", "device", ")", ".", "float", "(", ")", "\n", "self", ".", "cameras_current", "=", "PerspectiveCameras", "(", "\n", "fov", "=", "self", ".", "fov", ",", "\n", "R", "=", "np", ".", "stack", "(", "initial_poses", ")", ",", "\n", "T", "=", "[", "[", "0", ",", "0", ",", "2", "]", "]", "*", "self", ".", "N", ",", "\n", "image_center", "=", "image_center", ",", "\n", "crop_scale", "=", "crop_scale", ",", "\n", "device", "=", "device", ",", "\n", ")", "\n", "\n", "# Prepare initial mesh.", "\n", "self", ".", "sphere_vs", ",", "self", ".", "sphere_fs", "=", "geom_util", ".", "create_sphere", "(", "4", ",", "device", "=", "device", ")", "\n", "pred_vs", ",", "sv", "=", "self", ".", "get_pred_verts", "(", "self", ".", "sphere_vs", ",", "predict_deformation", "=", "False", ")", "\n", "texture_rainbow", "=", "TexturesVertex", "(", "(", "sv", ".", "unsqueeze", "(", "0", ")", "+", "1", ")", "/", "2", ")", "\n", "self", ".", "meshes_current", "=", "Meshes", "(", "\n", "[", "pred_vs", "]", ",", "[", "self", ".", "sphere_fs", "]", ",", "textures", "=", "texture_rainbow", "\n", ")", "\n", "\n", "# Compute edges for chamfer loss.", "\n", "pool", "=", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "masks_pooled", "=", "-", "pool", "(", "-", "self", ".", "target_masks", ")", "# Min pool", "\n", "edges", "=", "self", ".", "target_masks", "-", "masks_pooled", "\n", "# [(E, 2)]", "\n", "self", ".", "edge_pixels", "=", "[", "torch", ".", "stack", "(", "torch", ".", "where", "(", "e", ">", "0.5", ")", ",", "dim", "=", "1", ")", "for", "e", "in", "edges", "]", "\n", "self", ".", "edge_counts", "=", "[", "e", ".", "shape", "[", "0", "]", "for", "e", "in", "self", ".", "edge_pixels", "]", "\n", "\n", "# Prepare perceptual loss.", "\n", "lp", "=", "PerceptualLoss", "(", "net", "=", "\"vgg\"", ")", "\n", "self", ".", "loss_perceptual", "=", "DataParallel", "(", "lp", ",", "device_ids", "=", "gpu_ids", ")", ".", "to", "(", "device", ")", "\n", "\n", "self", ".", "loss_weights", "=", "{", "\n", "\"chamfer\"", ":", "0.02", ",", "\n", "\"silhouette\"", ":", "2.0", ",", "\n", "\"distance_transform\"", ":", "20", ",", "\n", "\"normal\"", ":", "0.1", ",", "\n", "\"laplacian\"", ":", "0.1", ",", "\n", "\"offscreen\"", ":", "1000", ",", "\n", "\"texture\"", ":", "0.5", ",", "\n", "\"texture_mean\"", ":", "0.15", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.get_pred_verts": [[192, 220], ["ners.Ners.f_template", "ners.Ners.f_template", "ners.random_rotation", "ners.random_rotation", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "ners.Ners.f_template", "ners.Ners.f_template", "ners.Ners.f_shape", "ners.Ners.f_shape"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.random_rotation", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.random_rotation"], ["", "def", "get_pred_verts", "(", "self", ",", "sphere_vs", "=", "None", ",", "predict_deformation", "=", "True", ",", "jitter_uv", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Outputs predicted vertex locations from a UV sphere.\n\n        Args:\n            sphere_vs (torch.Tensor): UV coordinates on sphere (V, 3).\n            predict_deformation (bool): If True, also predict using the current\n                deformation model. Otherwise, just outputs shape based on category\n                shape.\n            jitter_uv (bool): If True, jitters the uv values using a random rotation.\n\n        Returns:\n            torch.Tensor: Predicted vertices corresponding to the sphere UVs (V, 3).\n            torch.Tensor: Sphere UVs to predict vertices (V, 3).\n        \"\"\"", "\n", "sv", "=", "self", ".", "sphere_vs", "if", "sphere_vs", "is", "None", "else", "sphere_vs", "\n", "if", "jitter_uv", "is", "None", ":", "\n", "            ", "jitter_uv", "=", "self", ".", "jitter_uv", "\n", "", "if", "jitter_uv", ":", "\n", "            ", "sv", "=", "sv", "@", "geom_util", ".", "random_rotation", "(", "sv", ".", "device", ")", "\n", "", "if", "self", ".", "use_template_as_shape", ":", "\n", "            ", "pred_vs", "=", "self", ".", "f_template", "(", "sv", ")", "\n", "", "else", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "pred_vs", "=", "self", ".", "f_template", "(", "sv", ")", "\n", "", "if", "predict_deformation", ":", "\n", "                ", "pred_vs", "=", "pred_vs", "+", "self", ".", "f_shape", "(", "sv", ")", "\n", "", "", "return", "pred_vs", ",", "sv", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.compute_chamfer_loss": [[221, 251], ["zip", "torch.stack().unsqueeze", "torch.stack().unsqueeze", "torch.stack().unsqueeze", "torch.stack().unsqueeze", "cameras.transform_points_screen", "proj.unsqueeze.unsqueeze.unsqueeze", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "sample.roll.roll.roll", "torch.stack().unsqueeze.append", "torch.stack().unsqueeze.append", "meshes.verts_padded", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "dists.min", "torch.randint", "torch.randint", "torch.randint", "torch.randint"], "methods", ["None"], ["", "def", "compute_chamfer_loss", "(", "self", ",", "meshes", ",", "cameras", ",", "num_samples", "=", "300", ")", ":", "\n", "        ", "\"\"\"\n        Computes the 2D chamfer loss between the edges of the mask and edges of the\n        rendered silhouette.\n\n        Args:\n            meshes: Meshes to compute chamfer loss on.\n            cameras: Cameras to render meshes with.\n            num_samples (int): Number of samples to use for chamfer loss.\n\n        Returns:\n            torch.Tensor: Chamfer loss.\n        \"\"\"", "\n", "edge_pixels_sampled", "=", "[", "]", "# [(num_samples, 2)] * N", "\n", "for", "counts", ",", "pixels", "in", "zip", "(", "self", ".", "edge_counts", ",", "self", ".", "edge_pixels", ")", ":", "\n", "# Sampling with replacement because multinomial doesn't play well with Long.", "\n", "            ", "sample", "=", "pixels", "[", "torch", ".", "randint", "(", "counts", ",", "size", "=", "(", "num_samples", ",", ")", ")", "]", "\n", "sample", "=", "sample", ".", "roll", "(", "1", ",", "dims", "=", "1", ")", "# row, column -> x, y", "\n", "edge_pixels_sampled", ".", "append", "(", "sample", ")", "\n", "# (N, num_samples, 1, 2)", "\n", "", "edge_pixels_sampled", "=", "torch", ".", "stack", "(", "edge_pixels_sampled", ")", ".", "unsqueeze", "(", "2", ")", "\n", "# Projected Points:", "\n", "proj", "=", "cameras", ".", "transform_points_screen", "(", "# (N, V, 2)", "\n", "meshes", ".", "verts_padded", "(", ")", ",", "\n", "image_size", "=", "[", "(", "256", ",", "256", ")", "]", "*", "self", ".", "N", ",", "\n", ")", "\n", "proj", "=", "proj", "[", "...", ",", ":", "2", "]", "# Drop the depth buffer", "\n", "proj", "=", "proj", ".", "unsqueeze", "(", "1", ")", "# (N, 1, V, 2)", "\n", "dists", "=", "(", "proj", "-", "edge_pixels_sampled", ")", ".", "norm", "(", "dim", "=", "-", "1", ")", "\n", "return", "torch", ".", "mean", "(", "dists", ".", "min", "(", "dim", "=", "-", "1", ")", ".", "values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.compute_offscreen_loss": [[252, 267], ["torch.sum", "torch.sum", "torch.sum", "torch.sum", "cameras.transform_points", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu"], "methods", ["None"], ["", "def", "compute_offscreen_loss", "(", "self", ",", "cameras", ",", "verts", ")", ":", "\n", "        ", "\"\"\"\n        Computes a loss for how much vertices go offscreen, ie outside of [-1, 1] NDC\n        coordinates. This helps avoid the degenerate solution of moving objects\n        offscreen to minimize the distance transform/silhouette loss.\n\n        Args:\n            cameras: Cameras to render vertices with.\n            verts: Mesh vertices to compute loss on (N, V, 3).\n\n        Returns:\n            torch.Tensor: Offscreen loss.\n        \"\"\"", "\n", "points", "=", "cameras", ".", "transform_points", "(", "verts", ")", "[", "...", ",", ":", "2", "]", "\n", "return", "torch", ".", "sum", "(", "torch", ".", "relu", "(", "points", "-", "1", ")", "+", "torch", ".", "relu", "(", "-", "1", "-", "points", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.visualize_input_views": [[268, 314], ["len", "matplotlib.subplots", "axs.flatten.flatten.flatten", "range", "ners.Ners.meshes_current.extend", "ners.Ners.meshes_current.extend", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "ners.Ners.renderer_textured", "ners.Ners.renderer_textured", "[].clip", "ners.Ners.renderer_silhouette", "ners.Ners.renderer_silhouette", "[].clip", "axs[].imshow", "axs[].set_title", "rend_image[].clip", "axs[].imshow", "axs[].imshow", "axs[].imshow", "ax.set_xticks", "ax.set_yticks", "matplotlib.suptitle", "matplotlib.savefig", "matplotlib.close", "matplotlib.show", "rend_image.clip", "ners.utils.visualize_masks", "ners.utils.visualize_masks", "ners.Ners.detach().cpu().numpy", "ners.Ners.detach().cpu().numpy", "[].clip.detach().cpu().numpy", "image.copy", "ners.Ners.detach().cpu", "ners.Ners.detach().cpu", "[].clip.detach().cpu", "ners.Ners.detach", "ners.Ners.detach", "[].clip.detach"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.extend", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.extend", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.masks.visualize_masks", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.masks.visualize_masks", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach"], ["", "def", "visualize_input_views", "(", "self", ",", "filename", "=", "None", ",", "title", "=", "\"\"", ",", "meshes", "=", "None", ",", "cameras", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Generates visualizations for each of the training views.\n\n        Args:\n            filename (str): If not None, saves the image to this file.\n            title (str): Title for the figure.\n            meshes (pytorch3d.structures.Meshes): Meshes to visualize. If None, uses\n                currently predicted mesh.\n            cameras: Cameras to visualize. If None, uses currently predicted cameras.\n        \"\"\"", "\n", "if", "meshes", "is", "None", ":", "\n", "            ", "meshes", "=", "self", ".", "meshes_current", ".", "extend", "(", "self", ".", "N", ")", "\n", "", "if", "cameras", "is", "None", ":", "\n", "            ", "cameras", "=", "self", ".", "cameras_current", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "rend", "=", "self", ".", "renderer_textured", "(", "meshes", ",", "cameras", "=", "cameras", ")", "\n", "rend_images", "=", "rend", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", "...", ",", ":", "3", "]", ".", "clip", "(", "0", ",", "1", ")", "\n", "rend_sil", "=", "self", ".", "renderer_silhouette", "(", "meshes", ",", "cameras", "=", "cameras", ")", "\n", "rend_sil", "=", "rend_sil", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", "...", ",", "3", "]", ".", "clip", "(", "0", ",", "1", ")", "\n", "", "n", "=", "len", "(", "rend_images", ")", "\n", "fig", ",", "axs", "=", "plt", ".", "subplots", "(", "n", ",", "4", ",", "figsize", "=", "(", "8", ",", "n", "*", "2", ")", ",", "dpi", "=", "100", ")", "\n", "axs", "=", "axs", ".", "flatten", "(", ")", "\n", "for", "i", "in", "range", "(", "n", ")", ":", "\n", "            ", "image", "=", "self", ".", "all_images", "[", "i", "]", "\n", "axs", "[", "4", "*", "i", "]", ".", "imshow", "(", "image", ")", "\n", "axs", "[", "4", "*", "i", "]", ".", "set_title", "(", "f\"Image {i + 1}\"", ")", "\n", "rend_image", "=", "rend_images", "[", "i", "]", "\n", "mask", "=", "rend_sil", "[", "i", "]", ">", "0.5", "\n", "vis_im", "=", "1", "-", "(", "1", "-", "image", ".", "copy", "(", ")", ")", "*", "0.7", "\n", "vis_im", "[", "mask", "]", "=", "rend_image", "[", "mask", "]", ".", "clip", "(", "0", ",", "1", ")", "\n", "\n", "axs", "[", "4", "*", "i", "+", "1", "]", ".", "imshow", "(", "vis_im", ")", "\n", "axs", "[", "4", "*", "i", "+", "2", "]", ".", "imshow", "(", "rend_image", ".", "clip", "(", "0", ",", "1", ")", ")", "\n", "mask_pred", "=", "self", ".", "masks", "[", "i", "]", ">", "0.5", "\n", "axs", "[", "4", "*", "i", "+", "3", "]", ".", "imshow", "(", "visualize_masks", "(", "mask", ",", "mask_pred", ")", ")", "\n", "", "for", "ax", "in", "axs", ":", "\n", "            ", "ax", ".", "set_xticks", "(", "[", "]", ")", "\n", "ax", ".", "set_yticks", "(", "[", "]", ")", "\n", "", "if", "title", ":", "\n", "            ", "plt", ".", "suptitle", "(", "title", ",", "y", "=", "0.93", ")", "\n", "", "if", "filename", ":", "\n", "            ", "plt", ".", "savefig", "(", "fname", "=", "filename", ",", "format", "=", "\"jpg\"", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "plt", ".", "close", "(", ")", "\n", "", "else", ":", "\n", "            ", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.optimize_camera": [[315, 349], ["cameras.T.detach", "ners.matrix_to_rot6d", "ners.matrix_to_rot6d", "ners.Ners.meshes_current.extend().detach", "ners.Ners.meshes_current.extend().detach", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "cameras.detach", "cameras.R.detach", "tqdm.auto.tqdm.auto.tqdm", "range", "torch.optim.Adam.zero_grad", "torch.optim.Adam.zero_grad", "ners.rot6d_to_matrix", "ners.rot6d_to_matrix", "ners.Ners.compute_chamfer_loss", "ners.Ners.compute_chamfer_loss", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "ners.Ners.compute_offscreen_loss", "ners.Ners.compute_offscreen_loss", "sum", "sum.backward", "torch.optim.Adam.step", "torch.optim.Adam.step", "ners.Ners.meshes_current.extend", "ners.Ners.meshes_current.extend", "range", "ners.Ners.renderer_silhouette", "ners.Ners.renderer_silhouette", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "ners.Ners.verts_padded().detach", "ners.Ners.verts_padded().detach", "loop.set_description", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "ners.Ners.verts_padded", "ners.Ners.verts_padded", "losses.keys", "sum.item"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.matrix_to_rot6d", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.matrix_to_rot6d", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.rot6d_to_matrix", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.rot6d_to_matrix", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.compute_chamfer_loss", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.compute_chamfer_loss", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.compute_offscreen_loss", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.compute_offscreen_loss", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.extend", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.extend", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach"], ["", "", "def", "optimize_camera", "(", "self", ",", "num_iterations", "=", "500", ",", "finetune_rotations", "=", "True", ",", "pbar", "=", "True", ")", ":", "\n", "        ", "cameras", "=", "self", ".", "cameras_current", "\n", "T", "=", "cameras", ".", "T", ".", "detach", "(", ")", "\n", "R", "=", "geom_util", ".", "matrix_to_rot6d", "(", "cameras", ".", "R", ".", "detach", "(", ")", ")", "\n", "T", ".", "requires_grad", "=", "True", "\n", "R", ".", "requires_grad", "=", "False", "\n", "meshes", "=", "self", ".", "meshes_current", ".", "extend", "(", "self", ".", "N", ")", ".", "detach", "(", ")", "\n", "optim", "=", "torch", ".", "optim", ".", "Adam", "(", "[", "T", "]", ",", "lr", "=", "0.01", ")", "\n", "loop", "=", "tqdm", "(", "range", "(", "num_iterations", ")", ")", "if", "pbar", "else", "range", "(", "num_iterations", ")", "\n", "for", "i", "in", "loop", ":", "\n", "            ", "optim", ".", "zero_grad", "(", ")", "\n", "cameras", ".", "T", "=", "T", "\n", "cameras", ".", "R", "=", "geom_util", ".", "rot6d_to_matrix", "(", "R", ")", "\n", "rend", "=", "self", ".", "renderer_silhouette", "(", "meshes", ",", "cameras", "=", "cameras", ")", "[", "...", ",", "3", "]", "\n", "losses", "=", "{", "\"silhouette\"", ":", "torch", ".", "mean", "(", "(", "rend", "-", "self", ".", "target_masks", ")", "**", "2", ")", "}", "\n", "losses", "[", "\"chamfer\"", "]", "=", "self", ".", "compute_chamfer_loss", "(", "\n", "meshes", "=", "meshes", ",", "cameras", "=", "cameras", "\n", ")", "\n", "losses", "[", "\"distance_transform\"", "]", "=", "torch", ".", "mean", "(", "rend", "*", "self", ".", "target_masks_dt", ")", "\n", "losses", "[", "\"offscreen\"", "]", "=", "self", ".", "compute_offscreen_loss", "(", "\n", "cameras", ",", "meshes", ".", "verts_padded", "(", ")", ".", "detach", "(", ")", "\n", ")", "\n", "loss", "=", "sum", "(", "losses", "[", "k", "]", "*", "self", ".", "loss_weights", "[", "k", "]", "for", "k", "in", "losses", ".", "keys", "(", ")", ")", "\n", "if", "pbar", ":", "\n", "                ", "loop", ".", "set_description", "(", "f\"Camera: {loss.item():.4f}\"", ")", "\n", "", "loss", ".", "backward", "(", ")", "\n", "optim", ".", "step", "(", ")", "\n", "if", "i", "==", "num_iterations", "//", "2", "and", "finetune_rotations", ":", "\n", "                ", "R", ".", "requires_grad", "=", "True", "\n", "optim", "=", "torch", ".", "optim", ".", "Adam", "(", "[", "T", ",", "R", "]", ",", "lr", "=", "1e-3", ")", "\n", "", "", "R", ".", "requires_grad", "=", "False", "\n", "T", ".", "requires_grad", "=", "False", "\n", "self", ".", "cameras_current", "=", "cameras", ".", "detach", "(", ")", "\n", "return", "T", ",", "R", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.optimize_shape": [[350, 398], ["ners.matrix_to_rot6d", "ners.matrix_to_rot6d", "ners.Ners.cameras_current.T.detach", "ners.Ners.cameras_current.T.detach", "ners.Ners.fov.clone().detach", "ners.Ners.fov.clone().detach", "parameters.append", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "ners.Ners.clone().detach", "ners.Ners.clone().detach", "ners.pytorch3d.PerspectiveCameras.detach", "ners.pytorch3d.PerspectiveCameras.detach", "pytorch3d.structures.Meshes().detach", "ners.Ners.cameras_current.R.detach", "ners.Ners.cameras_current.R.detach", "tqdm.auto.tqdm.auto.tqdm", "range", "torch.optim.Adam.zero_grad", "torch.optim.Adam.zero_grad", "ners.Ners.get_pred_verts", "ners.Ners.get_pred_verts", "pytorch3d.structures.Meshes().extend", "ners.pytorch3d.PerspectiveCameras", "ners.pytorch3d.PerspectiveCameras", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "ners.Ners.compute_offscreen_loss", "ners.Ners.compute_offscreen_loss", "pytorch3d.loss.mesh_laplacian_smoothing", "pytorch3d.loss.mesh_normal_consistency", "ners.Ners.compute_chamfer_loss", "ners.Ners.compute_chamfer_loss", "sum", "sum.backward", "torch.optim.Adam.step", "torch.optim.Adam.step", "ners.Ners.fov.clone", "ners.Ners.fov.clone", "ners.Ners.f_shape.parameters", "ners.Ners.f_shape.parameters", "range", "ners.Ners.renderer_silhouette", "ners.Ners.renderer_silhouette", "pred_vs.unsqueeze().detach", "loop.set_description", "ners.Ners.clone", "ners.Ners.clone", "pytorch3d.structures.Meshes", "pytorch3d.structures.Meshes", "ners.rot6d_to_matrix", "ners.rot6d_to_matrix", "pred_vs.unsqueeze", "loss_dict.keys", "pytorch3d.renderer.TexturesVertex", "sum.item", "sv.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.matrix_to_rot6d", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.matrix_to_rot6d", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.get_pred_verts", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.get_pred_verts", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.extend", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.compute_offscreen_loss", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.compute_offscreen_loss", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.compute_chamfer_loss", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.compute_chamfer_loss", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.rot6d_to_matrix", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.rot6d_to_matrix"], ["", "def", "optimize_shape", "(", "self", ",", "num_iterations", "=", "500", ",", "pbar", "=", "True", ",", "lr", "=", "1e-4", ")", ":", "\n", "        ", "R", "=", "geom_util", ".", "matrix_to_rot6d", "(", "self", ".", "cameras_current", ".", "R", ".", "detach", "(", ")", ")", "\n", "T", "=", "self", ".", "cameras_current", ".", "T", ".", "detach", "(", ")", "\n", "fov", "=", "self", ".", "fov", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "if", "self", ".", "finetune_camera", ":", "\n", "            ", "params", "=", "[", "R", ",", "T", ",", "fov", "]", "\n", "for", "param", "in", "params", ":", "\n", "                ", "param", ".", "requires_grad", "=", "True", "\n", "", "parameters", "=", "[", "{", "\"params\"", ":", "params", ",", "\"lr\"", ":", "lr", "*", "10", "}", "]", "\n", "", "else", ":", "\n", "            ", "parameters", "=", "[", "]", "\n", "", "parameters", ".", "append", "(", "{", "\"params\"", ":", "self", ".", "f_shape", ".", "parameters", "(", ")", ",", "\"lr\"", ":", "lr", "}", ")", "\n", "\n", "optim", "=", "torch", ".", "optim", ".", "Adam", "(", "parameters", ")", "\n", "loop", "=", "tqdm", "(", "range", "(", "num_iterations", ")", ")", "if", "pbar", "else", "range", "(", "num_iterations", ")", "\n", "for", "_", "in", "loop", ":", "\n", "            ", "optim", ".", "zero_grad", "(", ")", "\n", "pred_vs", ",", "sv", "=", "self", ".", "get_pred_verts", "(", "predict_deformation", "=", "True", ")", "\n", "meshes", "=", "Meshes", "(", "[", "pred_vs", "]", ",", "[", "self", ".", "sphere_fs", "]", ")", ".", "extend", "(", "self", ".", "N", ")", "\n", "cameras", "=", "PerspectiveCameras", "(", "\n", "fov", "=", "fov", ",", "\n", "R", "=", "geom_util", ".", "rot6d_to_matrix", "(", "R", ")", ",", "\n", "T", "=", "T", ",", "\n", "image_center", "=", "self", ".", "cameras_current", ".", "image_center", ",", "\n", "crop_scale", "=", "self", ".", "cameras_current", ".", "crop_scale", ",", "\n", "device", "=", "self", ".", "device", ",", "\n", ")", "\n", "rend", "=", "self", ".", "renderer_silhouette", "(", "meshes", ",", "cameras", "=", "cameras", ")", "[", "...", ",", "3", "]", "\n", "loss_dict", "=", "{", "}", "\n", "loss_dict", "[", "\"silhouette\"", "]", "=", "torch", ".", "mean", "(", "(", "rend", "-", "self", ".", "target_masks", ")", "**", "2", ")", "\n", "loss_dict", "[", "\"distance_transform\"", "]", "=", "torch", ".", "mean", "(", "rend", "*", "self", ".", "target_masks_dt", ")", "\n", "loss_dict", "[", "\"offscreen\"", "]", "=", "self", ".", "compute_offscreen_loss", "(", "\n", "cameras", ",", "pred_vs", ".", "unsqueeze", "(", "0", ")", ".", "detach", "(", ")", "\n", ")", "\n", "loss_dict", "[", "\"laplacian\"", "]", "=", "mesh_laplacian_smoothing", "(", "meshes", ")", "\n", "loss_dict", "[", "\"normal\"", "]", "=", "mesh_normal_consistency", "(", "meshes", ")", "\n", "loss_dict", "[", "\"chamfer\"", "]", "=", "self", ".", "compute_chamfer_loss", "(", "meshes", ",", "cameras", ")", "\n", "loss", "=", "sum", "(", "loss_dict", "[", "k", "]", "*", "self", ".", "loss_weights", "[", "k", "]", "for", "k", "in", "loss_dict", ".", "keys", "(", ")", ")", "\n", "if", "pbar", ":", "\n", "                ", "loop", ".", "set_description", "(", "f\"Shape: {loss.item():.4f}\"", ")", "\n", "", "loss", ".", "backward", "(", ")", "\n", "optim", ".", "step", "(", ")", "\n", "", "self", ".", "fov", "=", "fov", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "self", ".", "cameras_current", "=", "cameras", ".", "detach", "(", ")", "\n", "self", ".", "meshes_current", "=", "Meshes", "(", "\n", "verts", "=", "[", "pred_vs", "]", ",", "\n", "faces", "=", "[", "self", ".", "sphere_fs", "]", ",", "\n", "textures", "=", "TexturesVertex", "(", "(", "sv", ".", "unsqueeze", "(", "0", ")", "+", "1", ")", "/", "2", ")", ",", "\n", ")", ".", "detach", "(", ")", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.optimize_texture": [[400, 462], ["ners.matrix_to_rot6d", "ners.matrix_to_rot6d", "ners.Ners.cameras_current.T.detach", "ners.Ners.cameras_current.T.detach", "ners.Ners.fov.clone().detach", "ners.Ners.fov.clone().detach", "parameters.append", "parameters.append", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "ners.Ners.clone().detach", "ners.Ners.clone().detach", "ners.pytorch3d.PerspectiveCameras.detach", "ners.pytorch3d.PerspectiveCameras.detach", "ners.Ners.cameras_current.R.detach", "ners.Ners.cameras_current.R.detach", "tqdm.auto.tqdm.auto.tqdm", "range", "torch.optim.Adam.zero_grad", "torch.optim.Adam.zero_grad", "ners.pytorch3d.PerspectiveCameras", "ners.pytorch3d.PerspectiveCameras", "ners.Ners.get_pred_verts", "ners.Ners.get_pred_verts", "pytorch3d.structures.Meshes().extend", "ners.pytorch3d.TexturesImplicit", "ners.pytorch3d.TexturesImplicit", "rend_text.clamp.clamp.clamp", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "ners.Ners.compute_offscreen_loss", "ners.Ners.compute_offscreen_loss", "pytorch3d.loss.mesh_laplacian_smoothing", "pytorch3d.loss.mesh_normal_consistency", "ners.Ners.loss_perceptual().mean", "ners.Ners.loss_perceptual().mean", "ners.Ners.compute_chamfer_loss", "ners.Ners.compute_chamfer_loss", "sum", "sum.backward", "torch.optim.Adam.step", "torch.optim.Adam.step", "ners.Ners.fov.clone", "ners.Ners.fov.clone", "ners.Ners.f_shape.parameters", "ners.Ners.f_shape.parameters", "ners.Ners.f_tex.parameters", "ners.Ners.f_tex.parameters", "range", "ners.Ners.renderer_silhouette", "ners.Ners.renderer_silhouette", "ners.Ners.renderer_textured", "ners.Ners.renderer_textured", "pred_vs.unsqueeze().detach", "loop.set_description", "ners.Ners.clone", "ners.Ners.clone", "ners.rot6d_to_matrix", "ners.rot6d_to_matrix", "pytorch3d.structures.Meshes", "ners.Ners.loss_perceptual", "ners.Ners.loss_perceptual", "pred_vs.unsqueeze", "rend_text.clamp.clamp.permute", "sum.item"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.matrix_to_rot6d", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.matrix_to_rot6d", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.get_pred_verts", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.get_pred_verts", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.extend", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.compute_offscreen_loss", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.compute_offscreen_loss", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.compute_chamfer_loss", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.compute_chamfer_loss", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.rot6d_to_matrix", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.rot6d_to_matrix"], ["", "def", "optimize_texture", "(", "self", ",", "num_iterations", "=", "3000", ",", "lr", "=", "1e-4", ",", "pbar", "=", "True", ")", ":", "\n", "        ", "R", "=", "geom_util", ".", "matrix_to_rot6d", "(", "self", ".", "cameras_current", ".", "R", ".", "detach", "(", ")", ")", "\n", "T", "=", "self", ".", "cameras_current", ".", "T", ".", "detach", "(", ")", "\n", "fov", "=", "self", ".", "fov", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "if", "self", ".", "finetune_camera", ":", "\n", "            ", "params", "=", "[", "R", ",", "T", ",", "fov", "]", "\n", "for", "param", "in", "params", ":", "\n", "                ", "param", ".", "requires_grad", "=", "True", "\n", "", "parameters", "=", "[", "{", "\"params\"", ":", "params", ",", "\"lr\"", ":", "lr", "*", "10", "}", "]", "\n", "", "else", ":", "\n", "            ", "parameters", "=", "[", "]", "\n", "", "parameters", ".", "append", "(", "{", "\"params\"", ":", "self", ".", "f_shape", ".", "parameters", "(", ")", ",", "\"lr\"", ":", "lr", "}", ")", "\n", "parameters", ".", "append", "(", "{", "\"params\"", ":", "self", ".", "f_tex", ".", "parameters", "(", ")", ",", "\"lr\"", ":", "lr", "}", ")", "\n", "\n", "optim", "=", "torch", ".", "optim", ".", "Adam", "(", "parameters", ")", "\n", "loop", "=", "tqdm", "(", "range", "(", "num_iterations", ")", ")", "if", "pbar", "else", "range", "(", "num_iterations", ")", "\n", "for", "_", "in", "loop", ":", "\n", "            ", "optim", ".", "zero_grad", "(", ")", "\n", "cameras", "=", "PerspectiveCameras", "(", "\n", "fov", "=", "fov", ",", "\n", "R", "=", "geom_util", ".", "rot6d_to_matrix", "(", "R", ")", ",", "\n", "T", "=", "T", ",", "\n", "image_center", "=", "self", ".", "cameras_current", ".", "image_center", ",", "\n", "crop_scale", "=", "self", ".", "cameras_current", ".", "crop_scale", ",", "\n", "device", "=", "self", ".", "device", ",", "\n", ")", "\n", "pred_vs", ",", "sv", "=", "self", ".", "get_pred_verts", "(", "predict_deformation", "=", "True", ")", "\n", "meshes", "=", "Meshes", "(", "[", "pred_vs", "]", ",", "[", "self", ".", "sphere_fs", "]", ")", ".", "extend", "(", "self", ".", "N", ")", "\n", "pred_textures", "=", "TexturesImplicit", "(", "\n", "texture_predictor", "=", "self", ".", "f_tex", ",", "\n", "faces", "=", "self", ".", "sphere_fs", ",", "\n", "verts_sphere_coords", "=", "sv", ",", "\n", "verts_deformed_coords", "=", "pred_vs", ",", "\n", "predict_radiance", "=", "False", ",", "\n", ")", "\n", "meshes", ".", "textures", "=", "pred_textures", "\n", "rend_sil", "=", "self", ".", "renderer_silhouette", "(", "meshes", ",", "cameras", "=", "cameras", ")", "[", "...", ",", "3", "]", "\n", "rend_text", "=", "self", ".", "renderer_textured", "(", "meshes", ",", "cameras", "=", "cameras", ")", "[", "...", ",", ":", "3", "]", "\n", "rend_text", "=", "rend_text", ".", "clamp", "(", "0", ",", "1", ")", "\n", "loss_dict", "=", "{", "}", "\n", "loss_dict", "[", "\"silhouette\"", "]", "=", "torch", ".", "mean", "(", "(", "rend_sil", "-", "self", ".", "target_masks", ")", "**", "2", ")", "\n", "loss_dict", "[", "\"distance_transform\"", "]", "=", "torch", ".", "mean", "(", "\n", "rend_sil", "*", "self", ".", "target_masks_dt", "\n", ")", "\n", "loss_dict", "[", "\"offscreen\"", "]", "=", "self", ".", "compute_offscreen_loss", "(", "\n", "cameras", ",", "pred_vs", ".", "unsqueeze", "(", "0", ")", ".", "detach", "(", ")", "\n", ")", "\n", "loss_dict", "[", "\"laplacian\"", "]", "=", "mesh_laplacian_smoothing", "(", "meshes", ")", "\n", "loss_dict", "[", "\"normal\"", "]", "=", "mesh_normal_consistency", "(", "meshes", ")", "\n", "loss_dict", "[", "\"texture\"", "]", "=", "self", ".", "loss_perceptual", "(", "\n", "self", ".", "images_masked", ",", "rend_text", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", ")", ".", "mean", "(", ")", "\n", "loss_dict", "[", "\"chamfer\"", "]", "=", "self", ".", "compute_chamfer_loss", "(", "meshes", ",", "cameras", ")", "\n", "loss", "=", "sum", "(", "loss_dict", "[", "k", "]", "*", "self", ".", "loss_weights", "[", "k", "]", "for", "k", "in", "loss_dict", ")", "\n", "if", "pbar", ":", "\n", "                ", "loop", ".", "set_description", "(", "f\"Texture: {loss.item():.4f}\"", ")", "\n", "", "loss", ".", "backward", "(", ")", "\n", "optim", ".", "step", "(", ")", "\n", "", "self", ".", "fov", "=", "fov", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "self", ".", "cameras_current", "=", "cameras", ".", "detach", "(", ")", "\n", "self", ".", "meshes_current", "=", "meshes", "[", "0", "]", "\n", "self", ".", "meshes_current", ".", "textures", "=", "pred_textures", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.optimize_radiance": [[463, 542], ["ners.matrix_to_rot6d", "ners.matrix_to_rot6d", "ners.Ners.cameras_current.T.detach", "ners.Ners.cameras_current.T.detach", "ners.Ners.fov.clone().detach", "ners.Ners.fov.clone().detach", "parameters.append", "parameters.append", "parameters.append", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "ners.Ners.clone().detach", "ners.Ners.clone().detach", "ners.pytorch3d.PerspectiveCameras.detach", "ners.pytorch3d.PerspectiveCameras.detach", "ners.Ners.clone().detach", "ners.Ners.clone().detach", "ners.Ners.cameras_current.R.detach", "ners.Ners.cameras_current.R.detach", "params.extend", "tqdm.auto.tqdm.auto.tqdm", "range", "torch.optim.Adam.zero_grad", "torch.optim.Adam.zero_grad", "ners.pytorch3d.PerspectiveCameras", "ners.pytorch3d.PerspectiveCameras", "ners.Ners.get_pred_verts", "ners.Ners.get_pred_verts", "pytorch3d.structures.Meshes().extend", "ners.pytorch3d.TexturesImplicit", "ners.pytorch3d.TexturesImplicit", "ners.Ners.f_tex().mean().clone().detach", "ners.Ners.f_tex().mean().clone().detach", "rend_text.clamp.clamp.clamp", "ners.pytorch3d.TexturesImplicit.set_texture_color", "ners.pytorch3d.TexturesImplicit.set_texture_color", "ners.Ners.renderer_textured", "ners.Ners.renderer_textured", "rend_text_mean[].clamp", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "ners.Ners.compute_offscreen_loss", "ners.Ners.compute_offscreen_loss", "pytorch3d.loss.mesh_laplacian_smoothing", "pytorch3d.loss.mesh_normal_consistency", "ners.Ners.loss_perceptual().mean", "ners.Ners.loss_perceptual().mean", "ners.Ners.loss_perceptual().mean", "ners.Ners.loss_perceptual().mean", "ners.Ners.compute_chamfer_loss", "ners.Ners.compute_chamfer_loss", "sum", "sum.backward", "torch.optim.Adam.step", "torch.optim.Adam.step", "ners.Ners.fov.clone", "ners.Ners.fov.clone", "ners.Ners.f_shape.parameters", "ners.Ners.f_shape.parameters", "ners.Ners.f_tex.parameters", "ners.Ners.f_tex.parameters", "ners.Ners.f_env.parameters", "ners.Ners.f_env.parameters", "range", "ners.Ners.renderer_silhouette", "ners.Ners.renderer_silhouette", "ners.Ners.renderer_textured", "ners.Ners.renderer_textured", "pred_vs.unsqueeze().detach", "loop.set_description", "ners.Ners.clone", "ners.Ners.clone", "ners.Ners.clone", "ners.Ners.clone", "ners.rot6d_to_matrix", "ners.rot6d_to_matrix", "pytorch3d.structures.Meshes", "ners.Ners.f_tex().mean().clone", "ners.Ners.f_tex().mean().clone", "ners.Ners.loss_perceptual", "ners.Ners.loss_perceptual", "ners.Ners.loss_perceptual", "ners.Ners.loss_perceptual", "pytorch3d.structures.Meshes().extend.verts_normals_padded", "pred_vs.unsqueeze", "rend_text.clamp.clamp.permute", "rend_text_mean[].clamp.permute", "ners.Ners.f_tex().mean", "ners.Ners.f_tex().mean", "sum.item", "ners.Ners.f_tex", "ners.Ners.f_tex"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.matrix_to_rot6d", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.matrix_to_rot6d", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.extend", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.get_pred_verts", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.get_pred_verts", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.extend", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.set_texture_color", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.set_texture_color", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.compute_offscreen_loss", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.compute_offscreen_loss", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.compute_chamfer_loss", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.compute_chamfer_loss", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.rot6d_to_matrix", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.rot6d_to_matrix", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone"], ["", "def", "optimize_radiance", "(", "self", ",", "num_iterations", "=", "500", ",", "pbar", "=", "True", ",", "lr", "=", "1e-4", ")", ":", "\n", "        ", "R", "=", "geom_util", ".", "matrix_to_rot6d", "(", "self", ".", "cameras_current", ".", "R", ".", "detach", "(", ")", ")", "\n", "T", "=", "self", ".", "cameras_current", ".", "T", ".", "detach", "(", ")", "\n", "fov", "=", "self", ".", "fov", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "params", "=", "[", "self", ".", "specularity", ",", "self", ".", "shininess", "]", "\n", "if", "self", ".", "finetune_camera", ":", "\n", "            ", "params", ".", "extend", "(", "[", "R", ",", "T", ",", "fov", "]", ")", "\n", "", "for", "param", "in", "params", ":", "\n", "            ", "param", ".", "requires_grad", "=", "True", "\n", "", "parameters", "=", "[", "{", "\"params\"", ":", "params", ",", "\"lr\"", ":", "lr", "*", "10", "}", "]", "\n", "parameters", ".", "append", "(", "{", "\"params\"", ":", "self", ".", "f_shape", ".", "parameters", "(", ")", ",", "\"lr\"", ":", "lr", "}", ")", "\n", "parameters", ".", "append", "(", "{", "\"params\"", ":", "self", ".", "f_tex", ".", "parameters", "(", ")", ",", "\"lr\"", ":", "lr", "}", ")", "\n", "parameters", ".", "append", "(", "{", "\"params\"", ":", "self", ".", "f_env", ".", "parameters", "(", ")", ",", "\"lr\"", ":", "lr", "}", ")", "\n", "\n", "loop", "=", "tqdm", "(", "range", "(", "num_iterations", ")", ")", "if", "pbar", "else", "range", "(", "num_iterations", ")", "\n", "optim", "=", "torch", ".", "optim", ".", "Adam", "(", "parameters", ")", "\n", "for", "_", "in", "loop", ":", "\n", "            ", "optim", ".", "zero_grad", "(", ")", "\n", "cameras", "=", "PerspectiveCameras", "(", "\n", "fov", "=", "fov", ",", "\n", "R", "=", "geom_util", ".", "rot6d_to_matrix", "(", "R", ")", ",", "\n", "T", "=", "T", ",", "\n", "image_center", "=", "self", ".", "cameras_current", ".", "image_center", ",", "\n", "crop_scale", "=", "self", ".", "cameras_current", ".", "crop_scale", ",", "\n", "device", "=", "self", ".", "device", ",", "\n", ")", "\n", "pred_vs", ",", "sv", "=", "self", ".", "get_pred_verts", "(", "predict_deformation", "=", "True", ")", "\n", "meshes", "=", "Meshes", "(", "[", "pred_vs", "]", ",", "[", "self", ".", "sphere_fs", "]", ")", ".", "extend", "(", "self", ".", "N", ")", "\n", "pred_textures", "=", "TexturesImplicit", "(", "\n", "texture_predictor", "=", "self", ".", "f_tex", ",", "\n", "faces", "=", "self", ".", "sphere_fs", ",", "\n", "verts_sphere_coords", "=", "sv", ",", "\n", "verts_deformed_coords", "=", "pred_vs", ",", "\n", "verts_normals", "=", "meshes", ".", "verts_normals_padded", "(", ")", "[", "0", "]", ",", "\n", "predict_radiance", "=", "True", ",", "\n", "env_map", "=", "self", ".", "f_env", ",", "\n", "specularity", "=", "self", ".", "specularity", ",", "\n", "shininess", "=", "self", ".", "shininess", ",", "\n", "jitter_env_map_rays", "=", "True", ",", "\n", ")", "\n", "pred_textures", ".", "custom_device", "=", "self", ".", "device_illumination", "\n", "mean_texture", "=", "self", ".", "f_tex", "(", "sv", ")", ".", "mean", "(", "dim", "=", "0", ")", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "meshes", ".", "textures", "=", "pred_textures", "\n", "rend_sil", "=", "self", ".", "renderer_silhouette", "(", "meshes", ",", "cameras", "=", "cameras", ")", "[", "...", ",", "3", "]", "\n", "rend_text", "=", "self", ".", "renderer_textured", "(", "meshes", ",", "cameras", "=", "cameras", ")", "[", "...", ",", ":", "3", "]", "\n", "rend_text", "=", "rend_text", ".", "clamp", "(", "0", ",", "1", ")", "\n", "pred_textures", ".", "set_texture_color", "(", "mean_texture", ")", "\n", "rend_text_mean", "=", "self", ".", "renderer_textured", "(", "meshes", ",", "cameras", "=", "cameras", ")", "\n", "rend_text_mean", "=", "rend_text_mean", "[", "...", ",", ":", "3", "]", ".", "clamp", "(", "0", ",", "1", ")", "\n", "pred_textures", ".", "texture_color", "=", "None", "\n", "\n", "loss_dict", "=", "{", "}", "\n", "loss_dict", "[", "\"silhouette\"", "]", "=", "torch", ".", "mean", "(", "(", "rend_sil", "-", "self", ".", "target_masks", ")", "**", "2", ")", "\n", "loss_dict", "[", "\"distance_transform\"", "]", "=", "torch", ".", "mean", "(", "\n", "rend_sil", "*", "self", ".", "target_masks_dt", "\n", ")", "\n", "loss_dict", "[", "\"offscreen\"", "]", "=", "self", ".", "compute_offscreen_loss", "(", "\n", "cameras", ",", "pred_vs", ".", "unsqueeze", "(", "0", ")", ".", "detach", "(", ")", "\n", ")", "\n", "loss_dict", "[", "\"laplacian\"", "]", "=", "mesh_laplacian_smoothing", "(", "meshes", ")", "\n", "loss_dict", "[", "\"normal\"", "]", "=", "mesh_normal_consistency", "(", "meshes", ")", "\n", "loss_dict", "[", "\"texture\"", "]", "=", "self", ".", "loss_perceptual", "(", "\n", "self", ".", "images_masked", ",", "rend_text", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", ")", ".", "mean", "(", ")", "\n", "loss_dict", "[", "\"texture_mean\"", "]", "=", "self", ".", "loss_perceptual", "(", "\n", "self", ".", "images_masked", ",", "rend_text_mean", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", ")", ".", "mean", "(", ")", "\n", "loss_dict", "[", "\"chamfer\"", "]", "=", "self", ".", "compute_chamfer_loss", "(", "meshes", ",", "cameras", ")", "\n", "\n", "loss", "=", "sum", "(", "loss_dict", "[", "k", "]", "*", "self", ".", "loss_weights", "[", "k", "]", "for", "k", "in", "loss_dict", ")", "\n", "if", "pbar", ":", "\n", "                ", "loop", ".", "set_description", "(", "f\"Radiance: {loss.item():.4f}\"", ")", "\n", "", "loss", ".", "backward", "(", ")", "\n", "optim", ".", "step", "(", ")", "\n", "", "self", ".", "fov", "=", "fov", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "self", ".", "cameras_current", "=", "cameras", ".", "detach", "(", ")", "\n", "self", ".", "meshes_current", "=", "meshes", "[", "0", "]", "\n", "self", ".", "meshes_current", ".", "texture", "=", "pred_textures", "\n", "self", ".", "mean_texture", "=", "mean_texture", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.save_parameters": [[543, 555], ["torch.save", "torch.save", "torch.save", "torch.save", "ners.Ners.f_shape.state_dict", "ners.Ners.f_shape.state_dict", "ners.Ners.f_tex.state_dict", "ners.Ners.f_tex.state_dict", "ners.Ners.f_env.state_dict", "ners.Ners.f_env.state_dict", "ners.Ners.f_template.state_dict", "ners.Ners.f_template.state_dict"], "methods", ["None"], ["", "def", "save_parameters", "(", "self", ",", "filename", ")", ":", "\n", "        ", "state_dicts", "=", "{", "\n", "\"cameras\"", ":", "self", ".", "cameras_current", ",", "\n", "\"fov\"", ":", "self", ".", "fov", ",", "\n", "\"shininess\"", ":", "self", ".", "shininess", ",", "\n", "\"specularity\"", ":", "self", ".", "specularity", ",", "\n", "\"f_shape\"", ":", "self", ".", "f_shape", ".", "state_dict", "(", ")", ",", "\n", "\"f_tex\"", ":", "self", ".", "f_tex", ".", "state_dict", "(", ")", ",", "\n", "\"f_env\"", ":", "self", ".", "f_env", ".", "state_dict", "(", ")", ",", "\n", "\"f_template\"", ":", "self", ".", "f_template", ".", "state_dict", "(", ")", ",", "\n", "}", "\n", "torch", ".", "save", "(", "state_dicts", ",", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.load_parameters": [[556, 589], ["torch.load", "torch.load", "torch.load", "torch.load", "ners.Ners.f_shape.load_state_dict", "ners.Ners.f_shape.load_state_dict", "ners.Ners.f_tex.load_state_dict", "ners.Ners.f_tex.load_state_dict", "ners.Ners.f_env.load_state_dict", "ners.Ners.f_env.load_state_dict", "ners.Ners.f_template.load_state_dict", "ners.Ners.f_template.load_state_dict", "ners.Ners.get_pred_verts", "ners.Ners.get_pred_verts", "pytorch3d.structures.Meshes", "ners.pytorch3d.TexturesImplicit", "ners.pytorch3d.TexturesImplicit", "ners.Ners.f_shape.to", "ners.Ners.f_shape.to", "ners.Ners.f_tex.to", "ners.Ners.f_tex.to", "ners.Ners.f_env.to", "ners.Ners.f_env.to", "ners.Ners.f_template.to", "ners.Ners.f_template.to", "pytorch3d.structures.Meshes.verts_normals_padded"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.get_pred_verts", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.get_pred_verts"], ["", "def", "load_parameters", "(", "self", ",", "filename", ",", "device", "=", "None", ")", ":", "\n", "        ", "state_dicts", "=", "torch", ".", "load", "(", "filename", ")", "\n", "self", ".", "cameras_current", "=", "state_dicts", "[", "\"cameras\"", "]", "\n", "self", ".", "fov", "=", "state_dicts", "[", "\"fov\"", "]", "\n", "self", ".", "shininess", "=", "state_dicts", "[", "\"shininess\"", "]", "\n", "self", ".", "specularity", "=", "state_dicts", "[", "\"specularity\"", "]", "\n", "\n", "self", ".", "f_shape", ".", "load_state_dict", "(", "state_dicts", "[", "\"f_shape\"", "]", ")", "\n", "self", ".", "f_tex", ".", "load_state_dict", "(", "state_dicts", "[", "\"f_tex\"", "]", ")", "\n", "self", ".", "f_env", ".", "load_state_dict", "(", "state_dicts", "[", "\"f_env\"", "]", ")", "\n", "self", ".", "f_template", ".", "load_state_dict", "(", "state_dicts", "[", "\"f_template\"", "]", ")", "\n", "pred_vs", ",", "sv", "=", "self", ".", "get_pred_verts", "(", "predict_deformation", "=", "True", ")", "\n", "meshes", "=", "Meshes", "(", "[", "pred_vs", "]", ",", "[", "self", ".", "sphere_fs", "]", ")", "\n", "pred_textures", "=", "TexturesImplicit", "(", "\n", "texture_predictor", "=", "self", ".", "f_tex", ",", "\n", "faces", "=", "self", ".", "sphere_fs", ",", "\n", "verts_sphere_coords", "=", "sv", ",", "\n", "verts_deformed_coords", "=", "pred_vs", ",", "\n", "verts_normals", "=", "meshes", ".", "verts_normals_padded", "(", ")", "[", "0", "]", ",", "\n", "predict_radiance", "=", "True", ",", "\n", "env_map", "=", "self", ".", "f_env", ",", "\n", "specularity", "=", "self", ".", "specularity", ",", "\n", "shininess", "=", "self", ".", "shininess", ",", "\n", "jitter_env_map_rays", "=", "False", ",", "\n", ")", "\n", "meshes", ".", "textures", "=", "pred_textures", "\n", "self", ".", "meshes_current", "=", "meshes", "\n", "if", "device", "is", "not", "None", ":", "\n", "            ", "self", ".", "f_shape", "=", "self", ".", "f_shape", ".", "to", "(", "device", ")", "\n", "self", ".", "f_tex", "=", "self", ".", "f_tex", ".", "to", "(", "device", ")", "\n", "self", ".", "f_env", "=", "self", ".", "f_env", ".", "to", "(", "device", ")", "\n", "self", ".", "f_template", "=", "self", ".", "f_template", ".", "to", "(", "device", ")", "\n", "pred_textures", ".", "custom_device", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.make_video": [[590, 690], ["torch.cuda.empty_cache", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "pytorch3d.renderer.RasterizationSettings", "imageio.get_writer", "imageio.get_writer.close", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "pytorch3d.renderer.cameras.look_at_view_transform", "tqdm.auto.tqdm.auto.tqdm", "range", "ners.pytorch3d.FoVPerspectiveCameras", "ners.pytorch3d.FoVPerspectiveCameras", "imageio.get_writer.append_data", "range", "ners.pytorch3d.TexturesImplicit", "ners.pytorch3d.TexturesImplicit", "ners.Ners.renderer_textured", "ners.Ners.renderer_textured", "images.append", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm.cpu().numpy().argmin", "torch.norm.cpu().numpy().argmin", "images.append", "numpy.clip", "ners.utils.antialias", "ners.utils.antialias", "cv2.resize", "numpy.hstack", "torch.norm.cpu().numpy", "torch.norm.cpu().numpy", "ners.Ners.detach().cpu().numpy", "ners.Ners.detach().cpu().numpy", "ners.Ners.f_tex().mean", "ners.Ners.f_tex().mean", "ners.pytorch3d.TexturesImplicit.set_texture_color", "ners.pytorch3d.TexturesImplicit.set_texture_color", "Exception", "torch.norm.cpu", "torch.norm.cpu", "ners.Ners.detach().cpu", "ners.Ners.detach().cpu", "ners.Ners.f_tex", "ners.Ners.f_tex", "ners.Ners.detach", "ners.Ners.detach"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.utils.image.antialias", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.image.antialias", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.set_texture_color", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.set_texture_color", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach"], ["", "", "def", "make_video", "(", "\n", "self", ",", "\n", "fname", ",", "\n", "num_frames", "=", "360", ",", "\n", "fps", "=", "30", ",", "\n", "image_size", "=", "512", ",", "\n", "use_antialiasing", "=", "True", ",", "\n", "extension", "=", "\"mp4\"", ",", "\n", "visuals", "=", "(", "\"nn\"", ",", "\"full\"", ",", "\"lighting\"", ")", ",", "\n", "pbar", "=", "True", ",", "\n", "elev", "=", "15", ",", "\n", "dist", "=", "2.2", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Generates a video of the results\n\n        Args:\n            fname (str): Filename to write video.\n            num_frames (int, optional): Number of frames. Defaults to 360.\n            fps (int, optional): [description]. Defaults to 30.\n            image_size (int, optional): Size of image. Defaults to 512.\n            use_antialiasing (bool, optional): If True, performs antialiasing. Defaults\n                to True.\n            extension (str, optional): Defaults to \"mp4\".\n        \"\"\"", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "if", "use_antialiasing", ":", "\n", "            ", "image_size", "*=", "2", "\n", "", "raster_settings", "=", "RasterizationSettings", "(", "\n", "image_size", "=", "image_size", ",", "\n", "blur_radius", "=", "0.0", ",", "\n", "faces_per_pixel", "=", "1", ",", "\n", "bin_size", "=", "0", ",", "\n", "perspective_correct", "=", "False", ",", "\n", ")", "\n", "writer", "=", "imageio", ".", "get_writer", "(", "f\"{fname}.{extension}\"", ",", "mode", "=", "\"I\"", ",", "fps", "=", "fps", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "azim", "=", "torch", ".", "linspace", "(", "0", ",", "360", ",", "num_frames", ")", "\n", "R", ",", "T", "=", "pytorch3d", ".", "renderer", ".", "cameras", ".", "look_at_view_transform", "(", "\n", "dist", "=", "dist", ",", "\n", "elev", "=", "elev", ",", "\n", "azim", "=", "azim", ",", "\n", "device", "=", "self", ".", "device", ",", "\n", ")", "\n", "new_R", "=", "R", "\n", "loop", "=", "tqdm", "(", "range", "(", "num_frames", ")", ",", "\"Video\"", ")", "if", "pbar", "else", "range", "(", "num_frames", ")", "\n", "for", "i", "in", "loop", ":", "\n", "                ", "perspective_camera", "=", "FoVPerspectiveCameras", "(", "\n", "device", "=", "self", ".", "device", ",", "R", "=", "new_R", "[", "i", ",", "None", "]", ",", "T", "=", "T", "[", "i", ",", "None", "]", ",", "fov", "=", "self", ".", "fov", "\n", ")", "\n", "images", "=", "[", "]", "\n", "for", "visual_name", "in", "visuals", ":", "\n", "                    ", "textures_og", "=", "self", ".", "meshes_current", ".", "textures", "\n", "textures", "=", "TexturesImplicit", "(", "\n", "texture_predictor", "=", "textures_og", ".", "texture_predictor", ",", "\n", "faces", "=", "textures_og", ".", "faces", ",", "\n", "verts_sphere_coords", "=", "textures_og", ".", "verts_sphere_coords", ",", "\n", "verts_deformed_coords", "=", "textures_og", ".", "verts_deformed_coords", ",", "\n", "verts_normals", "=", "textures_og", ".", "verts_normals", ",", "\n", "predict_radiance", "=", "True", ",", "\n", "env_map", "=", "textures_og", ".", "env_map", ",", "\n", "specularity", "=", "textures_og", ".", "specularity", ",", "\n", "shininess", "=", "textures_og", ".", "shininess", ",", "\n", "jitter_env_map_rays", "=", "False", ",", "\n", ")", "\n", "if", "visual_name", "==", "\"nn\"", ":", "\n", "                        ", "R_dist", "=", "torch", ".", "norm", "(", "\n", "self", ".", "cameras_current", ".", "R", "-", "new_R", "[", "i", ",", "None", "]", ",", "dim", "=", "(", "1", ",", "2", ")", "\n", ")", "\n", "ind", "=", "R_dist", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "argmin", "(", ")", "\n", "images", ".", "append", "(", "\n", "cv2", ".", "resize", "(", "self", ".", "all_images", "[", "ind", "]", ",", "(", "image_size", ",", "image_size", ")", ")", "\n", ")", "\n", "continue", "\n", "", "elif", "visual_name", "==", "\"full\"", ":", "\n", "# Default", "\n", "                        ", "pass", "\n", "", "elif", "visual_name", "==", "\"albedo\"", ":", "\n", "                        ", "textures", ".", "predict_radiance", "=", "False", "\n", "", "elif", "visual_name", "==", "\"lighting\"", ":", "\n", "                        ", "mean_color", "=", "self", ".", "f_tex", "(", "self", ".", "sphere_vs", ")", ".", "mean", "(", "dim", "=", "0", ")", "\n", "textures", ".", "set_texture_color", "(", "mean_color", ")", "\n", "", "else", ":", "\n", "                        ", "raise", "Exception", "(", "\n", "f\"Visualization format: {visual_name} not recognized.\"", "\n", ")", "\n", "", "self", ".", "meshes_current", ".", "textures", "=", "textures", "\n", "rend", "=", "self", ".", "renderer_textured", "(", "\n", "self", ".", "meshes_current", ",", "\n", "cameras", "=", "perspective_camera", ",", "\n", "raster_settings", "=", "raster_settings", ",", "\n", ")", "\n", "images", ".", "append", "(", "\n", "np", ".", "clip", "(", "rend", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", "0", ",", "...", ",", ":", "3", "]", ",", "0", ",", "1", ")", "\n", ")", "\n", "", "if", "use_antialiasing", ":", "\n", "                    ", "images", "=", "[", "antialias", "(", "image", ")", "for", "image", "in", "images", "]", "\n", "", "combined", "=", "(", "np", ".", "hstack", "(", "images", ")", "*", "255", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "writer", ".", "append_data", "(", "combined", ")", "\n", "", "", "writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.save_obj": [[691, 774], ["os.basename", "ners.create_sphere", "ners.create_sphere", "sv.to.to.to", "sf.to().long.to().long.to().long", "ners.cartesian_to_spherical", "ners.cartesian_to_spherical", "torch.logical_and", "torch.logical_and", "torch.logical_and", "torch.logical_and", "[].cpu", "pred_verts.cpu", "sf.to().long.to().long.cpu", "sf.to().long.cpu.clone", "torch.cat.cpu", "torch.cat.cpu", "torch.cat.cpu", "torch.cat.cpu", "matplotlib.imsave", "fname.replace", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "ners.Ners.get_pred_verts", "ners.Ners.get_pred_verts", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "ners.Ners.f_tex.module.unwrap_uv_map", "ners.Ners.f_tex.module.unwrap_uv_map", "open", "f.write", "f.write", "zip", "zip", "open", "f.write", "f.write", "fname.replace", "ners.Ners.cpu().numpy", "ners.Ners.cpu().numpy", "sf.to().long.to().long.to", "v_i.tolist.tolist.tolist", "texture_verts_indices.append", "f.write", "f.write", "f.write", "fname.replace", "torch.where", "torch.where", "torch.where", "torch.where", "texture_verts_indices.append", "ners.Ners.cpu", "ners.Ners.cpu", "len", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.create_sphere", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.create_sphere", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.cartesian_to_spherical", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.cartesian_to_spherical", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.get_pred_verts", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.get_pred_verts", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.unwrap_uv_map", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.unwrap_uv_map"], ["", "def", "save_obj", "(", "\n", "self", ",", "\n", "fname", ",", "\n", "sphere_level", "=", "4", ",", "\n", "margin", "=", "0.25", ",", "\n", "height", "=", "2048", ",", "\n", "width", "=", "2048", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Exports NeRS model to a textured mesh.\n\n        Texture is discretized using a UV image.\n\n        Args:\n            fname (str): Filename\n            sphere_level (int, optional): Level for isosphere. Defaults to 20.\n            height (int): Height of uv image. Defaults to 2048.\n            width (int): Width of uv image. Defaults to 2048.\n        \"\"\"", "\n", "device", "=", "self", ".", "device", "\n", "if", "\".obj\"", "not", "in", "fname", ":", "\n", "            ", "fname", "=", "fname", "+", "\".obj\"", "\n", "", "basename", "=", "osp", ".", "basename", "(", "fname", ".", "replace", "(", "\".obj\"", ",", "\"\"", ")", ")", "\n", "sv", ",", "sf", "=", "geom_util", ".", "create_sphere", "(", "sphere_level", ")", "\n", "sv", "=", "sv", ".", "to", "(", "device", ")", "\n", "sf", "=", "sf", ".", "to", "(", "device", ")", ".", "long", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "pred_verts", ",", "sv", "=", "self", ".", "get_pred_verts", "(", "sv", ",", "jitter_uv", "=", "False", ")", "\n", "\n", "", "theta", ",", "phi", "=", "geom_util", ".", "cartesian_to_spherical", "(", "sv", "[", ":", ",", "0", "]", ",", "sv", "[", ":", ",", "1", "]", ",", "sv", "[", ":", ",", "2", "]", ")", "\n", "# Find faces with vertices on opposite sides of the seam", "\n", "seam_mask", "=", "torch", ".", "logical_and", "(", "\n", "torch", ".", "any", "(", "phi", "[", "sf", "]", ">", "np", ".", "pi", "/", "2", ",", "1", ")", ",", "# Face has points on right edge.", "\n", "torch", ".", "any", "(", "phi", "[", "sf", "]", "<", "-", "np", ".", "pi", "/", "2", ",", "1", ")", ",", "# Face has points on left edge.", "\n", ")", "\n", "seam_indices", "=", "torch", ".", "where", "(", "seam_mask", ")", "[", "0", "]", ".", "cpu", "(", ")", "\n", "verts", "=", "pred_verts", ".", "cpu", "(", ")", "\n", "faces_verts", "=", "sf", ".", "cpu", "(", ")", "\n", "faces_textures", "=", "faces_verts", ".", "clone", "(", ")", "\n", "phi", "=", "phi", ".", "cpu", "(", ")", "\n", "theta", "=", "theta", ".", "cpu", "(", ")", "\n", "texture_vertex_map", "=", "{", "}", "# map from vert indices to texture vert indices.", "\n", "for", "ind", "in", "seam_indices", ":", "\n", "# Make a new set of texture vertices.", "\n", "            ", "verts_indices", "=", "faces_verts", "[", "ind", "]", "\n", "texture_verts_indices", "=", "[", "]", "\n", "for", "v_i", "in", "verts_indices", ":", "\n", "                ", "v_i", "=", "v_i", ".", "tolist", "(", ")", "\n", "if", "v_i", "in", "texture_vertex_map", ":", "\n", "# Already processed this vertex.", "\n", "                    ", "texture_verts_indices", ".", "append", "(", "texture_vertex_map", "[", "v_i", "]", ")", "\n", "continue", "\n", "", "elif", "phi", "[", "v_i", "]", ">", "0", ":", "\n", "# Vertex is already on the right side.", "\n", "                    ", "texture_vertex_map", "[", "v_i", "]", "=", "v_i", "\n", "", "else", ":", "\n", "# Need to construct a new set of texture verts.", "\n", "                    ", "texture_vertex_map", "[", "v_i", "]", "=", "len", "(", "phi", ")", "\n", "new_phi", "=", "phi", "[", "v_i", "]", "+", "2", "*", "np", ".", "pi", "\n", "phi", "=", "torch", ".", "cat", "(", "(", "phi", ",", "torch", ".", "tensor", "(", "[", "new_phi", "]", ")", ")", ")", "\n", "theta", "=", "torch", ".", "cat", "(", "(", "theta", ",", "torch", ".", "tensor", "(", "[", "theta", "[", "v_i", "]", "]", ")", ")", ")", "\n", "", "texture_verts_indices", ".", "append", "(", "texture_vertex_map", "[", "v_i", "]", ")", "\n", "", "faces_textures", "[", "ind", "]", "=", "torch", ".", "tensor", "(", "texture_verts_indices", ")", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "uv_map", "=", "self", ".", "f_tex", ".", "module", ".", "unwrap_uv_map", "(", "height", ",", "width", ",", "margin", "=", "margin", ")", "\n", "", "theta", "=", "1", "-", "theta", "/", "np", ".", "pi", "\n", "phi", "=", "(", "phi", "+", "np", ".", "pi", ")", "/", "(", "(", "2", "+", "margin", ")", "*", "np", ".", "pi", ")", "\n", "\n", "with", "open", "(", "fname", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "f\"mtllib {basename}.mtl\\n\"", ")", "\n", "f", ".", "write", "(", "f\"usemtl {basename}\\n\"", ")", "\n", "for", "vert", "in", "verts", ":", "\n", "                ", "f", ".", "write", "(", "f\"v {vert[0]} {vert[1]} {vert[2]}\\n\"", ")", "\n", "", "for", "p", ",", "t", "in", "zip", "(", "phi", ",", "theta", ")", ":", "\n", "                ", "f", ".", "write", "(", "f\"vt {p} {t}\\n\"", ")", "\n", "", "for", "fv", ",", "ft", "in", "zip", "(", "faces_verts", ",", "faces_textures", ")", ":", "\n", "                ", "fv", "=", "fv", "+", "1", "\n", "ft", "=", "ft", "+", "1", "\n", "f", ".", "write", "(", "f\"f {fv[0]}/{ft[0]} {fv[1]}/{ft[1]} {fv[2]}/{ft[2]}\\n\"", ")", "\n", "", "", "with", "open", "(", "fname", ".", "replace", "(", "\".obj\"", ",", "\".mtl\"", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "f\"newmtl {basename}\\n\"", ")", "\n", "f", ".", "write", "(", "f\"map_Kd {basename}.png\\n\"", ")", "\n", "", "plt", ".", "imsave", "(", "fname", ".", "replace", "(", "\".obj\"", ",", "\".png\"", ")", ",", "uv_map", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.data.get_bbox": [[17, 21], ["numpy.where", "numpy.array", "numpy.min", "numpy.min", "numpy.max", "numpy.max"], "function", ["None"], ["def", "get_bbox", "(", "img", ")", ":", "\n", "    ", "a", "=", "np", ".", "where", "(", "img", "!=", "0", ")", "\n", "bbox", "=", "np", ".", "min", "(", "a", "[", "1", "]", ")", ",", "np", ".", "min", "(", "a", "[", "0", "]", ")", ",", "np", ".", "max", "(", "a", "[", "1", "]", ")", "+", "1", ",", "np", ".", "max", "(", "a", "[", "0", "]", ")", "+", "1", "\n", "return", "np", ".", "array", "(", "bbox", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.data.load_data_from_dir": [[23, 80], ["os.join", "os.join", "enumerate", "data_dict.items", "os.exists", "sorted", "os.basename", "os.join", "PIL.Image.open().convert", "PIL.Image.open().convert", "data.get_bbox", "numpy.concatenate().astype", "ners.crop_image", "ners.crop_image", "numpy.array", "ners.utils.compute_crop_parameters", "data_dict[].append", "data_dict[].append", "data_dict[].append", "data_dict[].append", "data_dict[].append", "data_dict[].append", "data_dict[].append", "os.join", "json.load", "pytorch3d.renderer.look_at_view_transform", "R.tolist", "glob.glob", "osp.basename.replace", "numpy.array", "np.array.resize", "ners.utils.compute_distance_transform", "numpy.stack", "open", "os.join", "PIL.Image.open", "PIL.Image.open", "max", "numpy.concatenate", "image_util.crop_image.resize", "os.join", "numpy.array"], "function", ["home.repos.pwc.inspect_result.jasonyzhang_ners.ners.data.get_bbox", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.image.crop_image", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.image.crop_image", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.cameras.compute_crop_parameters", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.masks.compute_distance_transform"], ["", "def", "load_data_from_dir", "(", "instance_dir", ",", "image_size", "=", "256", ",", "pad_size", "=", "0.1", ",", "skip_indices", "=", "(", ")", ")", ":", "\n", "    ", "\"\"\"\n    Loads NeRS data from a directory. Assumes that a folder containing images and a\n    folder container masks. Mask names should be the same as the images.\n    \"\"\"", "\n", "image_dir", "=", "osp", ".", "join", "(", "instance_dir", ",", "\"images\"", ")", "\n", "mask_dir", "=", "osp", ".", "join", "(", "instance_dir", ",", "\"masks\"", ")", "\n", "data_dict", "=", "{", "\n", "\"images_og\"", ":", "[", "]", ",", "\n", "\"images\"", ":", "[", "]", ",", "\n", "\"masks\"", ":", "[", "]", ",", "\n", "\"masks_dt\"", ":", "[", "]", ",", "\n", "\"bbox\"", ":", "[", "]", ",", "\n", "\"image_centers\"", ":", "[", "]", ",", "\n", "\"crop_scales\"", ":", "[", "]", ",", "\n", "}", "\n", "for", "i", ",", "image_path", "in", "enumerate", "(", "sorted", "(", "glob", "(", "osp", ".", "join", "(", "image_dir", ",", "\"*.jpg\"", ")", ")", ")", ")", ":", "\n", "        ", "if", "i", "in", "skip_indices", ":", "\n", "            ", "continue", "\n", "", "image_name", "=", "osp", ".", "basename", "(", "image_path", ")", "\n", "mask_path", "=", "osp", ".", "join", "(", "mask_dir", ",", "image_name", ".", "replace", "(", "\"jpg\"", ",", "\"png\"", ")", ")", "\n", "image_og", "=", "Image", ".", "open", "(", "image_path", ")", ".", "convert", "(", "\"RGB\"", ")", "\n", "mask", "=", "Image", ".", "open", "(", "mask_path", ")", ".", "convert", "(", "\"L\"", ")", "\n", "bbox", "=", "get_bbox", "(", "np", ".", "array", "(", "mask", ")", "/", "255.0", ">", "0.5", ")", "\n", "center", "=", "(", "bbox", "[", ":", "2", "]", "+", "bbox", "[", "2", ":", "]", ")", "/", "2.0", "\n", "s", "=", "max", "(", "bbox", "[", "2", ":", "]", "-", "bbox", "[", ":", "2", "]", ")", "/", "2.0", "*", "(", "1", "+", "pad_size", ")", "\n", "square_bbox", "=", "np", ".", "concatenate", "(", "[", "center", "-", "s", ",", "center", "+", "s", "]", ")", ".", "astype", "(", "int", ")", "\n", "# Crop image and mask.", "\n", "image", "=", "image_util", ".", "crop_image", "(", "image_og", ",", "square_bbox", ")", "\n", "image", "=", "np", ".", "array", "(", "image", ".", "resize", "(", "(", "image_size", ",", "image_size", ")", ",", "Image", ".", "LANCZOS", ")", ")", "/", "255.0", "\n", "mask", "=", "image_util", ".", "crop_image", "(", "mask", ",", "square_bbox", ")", "\n", "mask", "=", "np", ".", "array", "(", "mask", ".", "resize", "(", "(", "image_size", ",", "image_size", ")", ",", "Image", ".", "BILINEAR", ")", ")", "\n", "mask", "=", "mask", "/", "255.0", ">", "0.5", "\n", "image_center", ",", "crop_scale", "=", "compute_crop_parameters", "(", "image_og", ".", "size", ",", "square_bbox", ")", "\n", "data_dict", "[", "\"bbox\"", "]", ".", "append", "(", "square_bbox", ")", "\n", "data_dict", "[", "\"crop_scales\"", "]", ".", "append", "(", "crop_scale", ")", "\n", "data_dict", "[", "\"image_centers\"", "]", ".", "append", "(", "image_center", ")", "\n", "data_dict", "[", "\"images\"", "]", ".", "append", "(", "image", ")", "\n", "data_dict", "[", "\"images_og\"", "]", ".", "append", "(", "image_og", ")", "\n", "data_dict", "[", "\"masks\"", "]", ".", "append", "(", "mask", ")", "\n", "data_dict", "[", "\"masks_dt\"", "]", ".", "append", "(", "compute_distance_transform", "(", "mask", ")", ")", "\n", "", "for", "k", ",", "v", "in", "data_dict", ".", "items", "(", ")", ":", "\n", "        ", "if", "k", "!=", "\"images_og\"", ":", "# Original images can have any resolution.", "\n", "            ", "data_dict", "[", "k", "]", "=", "np", ".", "stack", "(", "v", ")", "\n", "\n", "", "", "if", "osp", ".", "exists", "(", "osp", ".", "join", "(", "instance_dir", ",", "\"metadata.json\"", ")", ")", ":", "\n", "        ", "metadata", "=", "json", ".", "load", "(", "open", "(", "osp", ".", "join", "(", "instance_dir", ",", "\"metadata.json\"", ")", ")", ")", "\n", "data_dict", "[", "\"extents\"", "]", "=", "metadata", "[", "\"extents\"", "]", "\n", "azimuths", "=", "metadata", "[", "\"azimuths\"", "]", "\n", "elevations", "=", "metadata", "[", "\"elevations\"", "]", "\n", "R", ",", "T", "=", "pytorch3d", ".", "renderer", ".", "look_at_view_transform", "(", "\n", "dist", "=", "2", ",", "\n", "elev", "=", "elevations", ",", "\n", "azim", "=", "azimuths", ",", "\n", ")", "\n", "data_dict", "[", "\"initial_poses\"", "]", "=", "R", ".", "tolist", "(", ")", "\n", "", "return", "data_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.data.load_car_data": [[82, 156], ["os.join", "data_dict.items", "open", "json.load", "os.join", "numpy.array", "numpy.concatenate", "PIL.Image.open().convert", "PIL.Image.fromarray", "ners.crop_image", "ners.crop_image", "ners.utils.compute_crop_parameters", "data_dict[].append", "data_dict[].append", "data_dict[].append", "data_dict[].append", "data_dict[].append", "data_dict[].append", "data_dict[].append", "data_dict[].append", "ners.utils.rle_to_binary_mask", "numpy.array", "numpy.array", "ners.utils.compute_distance_transform", "numpy.stack", "PIL.Image.open", "image_util.crop_image.resize", "image_util.crop_image.resize", "max"], "function", ["home.repos.pwc.inspect_result.jasonyzhang_ners.utils.image.crop_image", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.image.crop_image", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.cameras.compute_crop_parameters", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.masks.rle_to_binary_mask", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.masks.compute_distance_transform"], ["", "def", "load_car_data", "(", "\n", "instance_dir", ",", "\n", "use_optimized_cameras", "=", "True", ",", "\n", "image_size", "=", "256", ",", "\n", "pad_size", "=", "0.1", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Processes instance of car dataset for NeRS optimization.\n\n    Args:\n        instance_dir (str): Path to car instance.\n        use_optimized_cameras (bool, optional): If true, uses optimized pose from NeRS.\n            Otherwise, uses filtered poses from PoseFromShape.\n        image_size (int, optional): Size of image crop.\n        pad_size (float, optional): Amount to pad the bounding box before cropping.\n\n    Returns:\n        dict: Dictionary containing the following keys:\n            \"bbox\": List of bounding boxes (xyxy).\n            \"crop_scales\": List of crop scales.\n            \"image_centers\": List of image centers.\n            \"images\": List of cropped images.\n            \"images_og\": List of original, uncropped images.\n            \"initial_poses\": List of rotation matrices to initialize pose.\n            \"masks\": List of binary masks.\n    \"\"\"", "\n", "annotations_json", "=", "osp", ".", "join", "(", "instance_dir", ",", "\"annotations.json\"", ")", "\n", "with", "open", "(", "annotations_json", ")", "as", "f", ":", "\n", "        ", "annotations", "=", "json", ".", "load", "(", "f", ")", "\n", "", "data_dict", "=", "{", "\n", "\"bbox\"", ":", "[", "]", ",", "# (N, 4).", "\n", "\"crop_scales\"", ":", "[", "]", ",", "# (N,).", "\n", "\"image_centers\"", ":", "[", "]", ",", "# (N, 2).", "\n", "\"images\"", ":", "[", "]", ",", "# (N, 256, 256, 3).", "\n", "\"images_og\"", ":", "[", "]", ",", "# (N, H, W, 3).", "\n", "\"initial_poses\"", ":", "[", "]", ",", "# (N, 3, 3).", "\n", "\"masks\"", ":", "[", "]", ",", "# (N, 256, 256).", "\n", "\"masks_dt\"", ":", "[", "]", ",", "# (N, 256, 256).", "\n", "}", "\n", "for", "annotation", "in", "annotations", "[", "\"annotations\"", "]", ":", "\n", "        ", "filename", "=", "osp", ".", "join", "(", "instance_dir", ",", "\"images\"", ",", "annotation", "[", "\"filename\"", "]", ")", "\n", "\n", "# Make a square bbox.", "\n", "bbox", "=", "np", ".", "array", "(", "annotation", "[", "\"bbox\"", "]", ")", "\n", "center", "=", "(", "(", "bbox", "[", ":", "2", "]", "+", "bbox", "[", "2", ":", "]", ")", "/", "2.0", ")", ".", "astype", "(", "int", ")", "\n", "s", "=", "(", "max", "(", "bbox", "[", "2", ":", "]", "-", "bbox", "[", ":", "2", "]", ")", "/", "2.0", "*", "(", "1", "+", "pad_size", ")", ")", ".", "astype", "(", "int", ")", "\n", "square_bbox", "=", "np", ".", "concatenate", "(", "[", "center", "-", "s", ",", "center", "+", "s", "]", ")", "\n", "\n", "# Load image and mask.", "\n", "image_og", "=", "Image", ".", "open", "(", "filename", ")", ".", "convert", "(", "\"RGB\"", ")", "\n", "mask", "=", "Image", ".", "fromarray", "(", "rle_to_binary_mask", "(", "annotation", "[", "\"mask\"", "]", ")", ")", "\n", "\n", "# Crop image and mask.", "\n", "image", "=", "image_util", ".", "crop_image", "(", "image_og", ",", "square_bbox", ")", "\n", "image", "=", "np", ".", "array", "(", "image", ".", "resize", "(", "(", "image_size", ",", "image_size", ")", ",", "Image", ".", "LANCZOS", ")", ")", "/", "255.0", "\n", "mask", "=", "image_util", ".", "crop_image", "(", "mask", ",", "square_bbox", ")", "\n", "mask", "=", "np", ".", "array", "(", "mask", ".", "resize", "(", "(", "image_size", ",", "image_size", ")", ",", "Image", ".", "BILINEAR", ")", ")", ">", "0.5", "\n", "image_center", ",", "crop_scale", "=", "compute_crop_parameters", "(", "image_og", ".", "size", ",", "square_bbox", ")", "\n", "if", "use_optimized_cameras", ":", "\n", "            ", "initial_pose", "=", "annotation", "[", "\"camera_optimized\"", "]", "[", "\"R\"", "]", "\n", "", "else", ":", "\n", "            ", "initial_pose", "=", "annotation", "[", "\"camera_initial\"", "]", "[", "\"R\"", "]", "\n", "", "data_dict", "[", "\"bbox\"", "]", ".", "append", "(", "square_bbox", ")", "\n", "data_dict", "[", "\"crop_scales\"", "]", ".", "append", "(", "crop_scale", ")", "\n", "data_dict", "[", "\"image_centers\"", "]", ".", "append", "(", "image_center", ")", "\n", "data_dict", "[", "\"images\"", "]", ".", "append", "(", "image", ")", "\n", "data_dict", "[", "\"images_og\"", "]", ".", "append", "(", "image_og", ")", "\n", "data_dict", "[", "\"initial_poses\"", "]", ".", "append", "(", "initial_pose", ")", "\n", "data_dict", "[", "\"masks\"", "]", ".", "append", "(", "mask", ")", "\n", "data_dict", "[", "\"masks_dt\"", "]", ".", "append", "(", "compute_distance_transform", "(", "mask", ")", ")", "\n", "", "for", "k", ",", "v", "in", "data_dict", ".", "items", "(", ")", ":", "\n", "        ", "if", "k", "!=", "\"images_og\"", ":", "# Original images can have any resolution.", "\n", "            ", "data_dict", "[", "k", "]", "=", "np", ".", "stack", "(", "v", ")", "\n", "", "", "return", "data_dict", "\n", "", ""]], "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.perceptual_loss.PerceptualLoss.__init__": [[12, 15], ["torch.Module.__init__", "lpips.LPIPS"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.__init__"], ["    ", "def", "__init__", "(", "self", ",", "net", "=", "\"vgg\"", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "model", "=", "lpips", ".", "LPIPS", "(", "net", "=", "net", ",", "verbose", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.perceptual_loss.PerceptualLoss.get_device": [[16, 24], ["next", "perceptual_loss.PerceptualLoss.parameters"], "methods", ["None"], ["", "def", "get_device", "(", "self", ",", "default_device", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Returns which device module is on, assuming all parameters are on the same GPU.\n        \"\"\"", "\n", "try", ":", "\n", "            ", "return", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "device", "\n", "", "except", "StopIteration", ":", "\n", "            ", "return", "default_device", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.perceptual_loss.PerceptualLoss.__call__": [[25, 43], ["perceptual_loss.PerceptualLoss.get_device", "pred.to.to.to", "target.to.to.to", "perceptual_loss.PerceptualLoss.model.forward", "perceptual_loss.PerceptualLoss.to"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.utils.perceptual_loss.PerceptualLoss.get_device", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.rasterizer.MeshRasterizer.forward"], ["", "", "def", "__call__", "(", "self", ",", "pred", ",", "target", ",", "normalize", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Pred and target are Variables.\n        If normalize is on, scales images between [-1, 1]\n        Assumes the inputs are in range [0, 1].\n        \"\"\"", "\n", "if", "normalize", ":", "\n", "            ", "target", "=", "2", "*", "target", "-", "1", "\n", "pred", "=", "2", "*", "pred", "-", "1", "\n", "\n", "", "temp_device", "=", "pred", ".", "device", "\n", "device", "=", "self", ".", "get_device", "(", "temp_device", ")", "\n", "\n", "pred", "=", "pred", ".", "to", "(", "device", ")", "\n", "target", "=", "target", ".", "to", "(", "device", ")", "\n", "\n", "dist", "=", "self", ".", "model", ".", "forward", "(", "pred", ",", "target", ")", "\n", "return", "dist", ".", "to", "(", "temp_device", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.camera.compute_crop_parameters": [[4, 36], ["numpy.array", "max", "isinstance", "max", "numpy.array"], "function", ["None"], ["def", "compute_crop_parameters", "(", "image_size", ",", "bbox", ",", "image_center", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Computes the principal point and scaling factor for focal length given a square\n    bounding box crop of an image. Note that the image size should be (width, height).\n\n    These intrinsic parameters are used to preserve the original principal point even\n    after cropping the image.\n\n    Args:\n        image_size (int or tuple): Size of image, either length of longer dimension or\n            (W, H).\n        bbox: Square bounding box in xyxy (4,).\n        image_center: Center of projection/principal point (2,).\n\n    Returns:\n        principal_point: Coordinates in NDC using Pytorch3D convention with (1, 1)\n            as upper-left (2,).\n        crop_scale (float): Scaling factor for focal length.\n    \"\"\"", "\n", "bbox", "=", "np", ".", "array", "(", "bbox", ")", "\n", "b", "=", "max", "(", "bbox", "[", "2", ":", "]", "-", "bbox", "[", ":", "2", "]", ")", "\n", "if", "isinstance", "(", "image_size", ",", "int", ")", ":", "\n", "        ", "w", "=", "h", "=", "image_size", "\n", "", "else", ":", "\n", "        ", "w", ",", "h", "=", "image_size", "\n", "image_size", "=", "max", "(", "image_size", ")", "\n", "", "if", "image_center", "is", "None", ":", "\n", "        ", "image_center", "=", "np", ".", "array", "(", "[", "w", "/", "2", ",", "h", "/", "2", "]", ")", "\n", "", "bbox_center", "=", "(", "bbox", "[", ":", "2", "]", "+", "bbox", "[", "2", ":", "]", ")", "/", "2", "\n", "crop_scale", "=", "b", "/", "image_size", "\n", "principal_point", "=", "2", "*", "(", "bbox_center", "-", "image_center", ")", "/", "b", "\n", "return", "principal_point", ",", "crop_scale", "\n", "", ""]], "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.masks.visualize_masks": [[5, 11], ["numpy.ones", "numpy.array", "numpy.array", "numpy.array", "numpy.logical_and", "numpy.logical_and", "numpy.logical_and", "numpy.logical_not", "numpy.logical_not"], "function", ["None"], ["def", "visualize_masks", "(", "mask", ",", "mask_pred", ")", ":", "\n", "    ", "m", "=", "np", ".", "ones", "(", "(", "256", ",", "256", ",", "3", ")", ")", "\n", "m", "[", "np", ".", "logical_and", "(", "mask", ",", "mask_pred", ")", "]", "=", "np", ".", "array", "(", "[", "0.1", ",", "0.5", ",", "0.1", "]", ")", "\n", "m", "[", "np", ".", "logical_and", "(", "mask", ",", "np", ".", "logical_not", "(", "mask_pred", ")", ")", "]", "=", "np", ".", "array", "(", "[", "1", ",", "0", ",", "0", "]", ")", "\n", "m", "[", "np", ".", "logical_and", "(", "np", ".", "logical_not", "(", "mask", ")", ",", "mask_pred", ")", "]", "=", "np", ".", "array", "(", "[", "0", ",", "0", ",", "1", "]", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.masks.compute_distance_transform": [[13, 17], ["scipy.ndimage.distance_transform_edt", "max"], "function", ["None"], ["", "def", "compute_distance_transform", "(", "mask", ")", ":", "\n", "    ", "dist_out", "=", "distance_transform_edt", "(", "1", "-", "mask", ")", "\n", "dist_out", "=", "2", "*", "dist_out", "/", "max", "(", "mask", ".", "shape", ")", "\n", "return", "dist_out", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.masks.rle_to_binary_mask": [[19, 35], ["isinstance", "isinstance", "numpy.zeros", "zip", "np.zeros.reshape", "numpy.stack", "list", "numpy.prod", "map", "masks.rle_to_binary_mask", "list.split"], "function", ["home.repos.pwc.inspect_result.jasonyzhang_ners.utils.masks.rle_to_binary_mask"], ["", "def", "rle_to_binary_mask", "(", "rle", ")", ":", "\n", "    ", "\"\"\"\n    rle should be coco format: {\"counts\": [], \"size\": []}\n    \"\"\"", "\n", "if", "isinstance", "(", "rle", ",", "list", ")", ":", "\n", "        ", "return", "np", ".", "stack", "(", "[", "rle_to_binary_mask", "(", "r", ")", "for", "r", "in", "rle", "]", ")", "\n", "", "counts", "=", "rle", "[", "\"counts\"", "]", "\n", "if", "isinstance", "(", "counts", ",", "str", ")", ":", "\n", "        ", "counts", "=", "list", "(", "map", "(", "int", ",", "counts", ".", "split", "(", "\" \"", ")", ")", ")", "\n", "", "mask", "=", "np", ".", "zeros", "(", "np", ".", "prod", "(", "rle", "[", "\"size\"", "]", ")", ",", "dtype", "=", "bool", ")", "\n", "running_length", "=", "0", "\n", "for", "start", ",", "length", "in", "zip", "(", "counts", "[", ":", ":", "2", "]", ",", "counts", "[", "1", ":", ":", "2", "]", ")", ":", "\n", "        ", "running_length", "+=", "start", "\n", "mask", "[", "running_length", ":", "running_length", "+", "length", "]", "=", "1", "\n", "running_length", "+=", "length", "\n", "", "return", "mask", ".", "reshape", "(", "rle", "[", "\"size\"", "]", ",", "order", "=", "\"F\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.masks.binary_mask_to_rle": [[37, 52], ["binary_mask.ravel", "counts.append", "list", "counts.append", "map"], "function", ["None"], ["", "def", "binary_mask_to_rle", "(", "binary_mask", ")", ":", "\n", "    ", "counts", "=", "[", "]", "\n", "last_elem", "=", "0", "\n", "running_length", "=", "0", "\n", "for", "elem", "in", "binary_mask", ".", "ravel", "(", "order", "=", "\"F\"", ")", ":", "\n", "        ", "if", "elem", "==", "last_elem", ":", "\n", "            ", "pass", "\n", "", "else", ":", "\n", "            ", "counts", ".", "append", "(", "running_length", ")", "\n", "running_length", "=", "0", "\n", "last_elem", "=", "elem", "\n", "", "running_length", "+=", "1", "\n", "", "counts", ".", "append", "(", "running_length", ")", "\n", "rle", "=", "{", "\"counts\"", ":", "\" \"", ".", "join", "(", "map", "(", "str", ",", "counts", ")", ")", ",", "\"size\"", ":", "list", "(", "binary_mask", ".", "shape", ")", "}", "\n", "return", "rle", "\n", "", ""]], "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.random_rotation": [[11, 15], ["torch.randn", "torch.randn", "torch.randn.norm", "pytorch3d.transforms.quaternion_to_matrix"], "function", ["None"], ["def", "random_rotation", "(", "device", "=", "None", ")", ":", "\n", "    ", "quat", "=", "torch", ".", "randn", "(", "4", ",", "device", "=", "device", ")", "\n", "quat", "/=", "quat", ".", "norm", "(", ")", "\n", "return", "pytorch3d", ".", "transforms", ".", "quaternion_to_matrix", "(", "quat", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.rot6d_to_matrix": [[17, 34], ["rot_6d.view.view", "torch.normalize", "torch.normalize", "torch.cross", "torch.cross", "torch.stack", "torch.stack", "torch.einsum().unsqueeze", "torch.einsum().unsqueeze", "torch.einsum", "torch.einsum"], "function", ["home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.normalize", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.normalize"], ["", "def", "rot6d_to_matrix", "(", "rot_6d", ")", ":", "\n", "    ", "\"\"\"\n    Convert 6D rotation representation to 3x3 rotation matrix.\n    Reference: Zhou et al., \"On the Continuity of Rotation Representations in Neural\n    Networks\", CVPR 2019\n    Args:\n        rot_6d (B x 6): Batch of 6D Rotation representation.\n    Returns:\n        Rotation matrices (B x 3 x 3).\n    \"\"\"", "\n", "rot_6d", "=", "rot_6d", ".", "view", "(", "-", "1", ",", "3", ",", "2", ")", "\n", "a1", "=", "rot_6d", "[", ":", ",", ":", ",", "0", "]", "\n", "a2", "=", "rot_6d", "[", ":", ",", ":", ",", "1", "]", "\n", "b1", "=", "F", ".", "normalize", "(", "a1", ")", "\n", "b2", "=", "F", ".", "normalize", "(", "a2", "-", "torch", ".", "einsum", "(", "\"bi,bi->b\"", ",", "b1", ",", "a2", ")", ".", "unsqueeze", "(", "-", "1", ")", "*", "b1", ")", "\n", "b3", "=", "torch", ".", "cross", "(", "b1", ",", "b2", ")", "\n", "return", "torch", ".", "stack", "(", "(", "b1", ",", "b2", ",", "b3", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.matrix_to_rot6d": [[36, 45], ["rotmat.view"], "function", ["None"], ["", "def", "matrix_to_rot6d", "(", "rotmat", ")", ":", "\n", "    ", "\"\"\"\n    Convert rotation matrix to 6D rotation representation.\n    Args:\n        rotmat (B x 3 x 3): Batch of rotation matrices.\n    Returns:\n        6D Rotations (B x 3 x 2).\n    \"\"\"", "\n", "return", "rotmat", ".", "view", "(", "-", "1", ",", "3", ",", "3", ")", "[", ":", ",", ":", ",", ":", "2", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.spherical_to_cartesian": [[47, 66], ["torch.cos", "torch.cos", "torch.sin", "torch.sin", "torch.cos", "torch.cos", "torch.sin", "torch.sin", "torch.sin", "torch.sin"], "function", ["None"], ["", "def", "spherical_to_cartesian", "(", "theta", ",", "phi", ",", "radius", "=", "1.0", ")", ":", "\n", "    ", "\"\"\"\n    Converts from spherical coordinates to cartesian coordinates. Spherical coordinates\n    are defined according to the physics convention (theta elevation, phi azimuth).\n\n    https://en.wikipedia.org/wiki/Spherical_coordinate_system#Cartesian_coordinates\n\n    Args:\n        theta (tensor): elevation.\n        phi (tensor): azimuth.\n        radius (tensor): radius. Defaults to 1.\n\n    Returns:\n        (x, y, z)\n    \"\"\"", "\n", "x", "=", "radius", "*", "torch", ".", "sin", "(", "theta", ")", "*", "torch", ".", "cos", "(", "phi", ")", "\n", "y", "=", "radius", "*", "torch", ".", "sin", "(", "theta", ")", "*", "torch", ".", "sin", "(", "phi", ")", "\n", "z", "=", "radius", "*", "torch", ".", "cos", "(", "theta", ")", "\n", "return", "x", ",", "y", ",", "z", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.cartesian_to_spherical": [[68, 83], ["torch.arccos", "torch.arccos", "torch.atan2", "torch.atan2"], "function", ["None"], ["", "def", "cartesian_to_spherical", "(", "x", ",", "y", ",", "z", ")", ":", "\n", "    ", "\"\"\"\n    Converts spherical coordinates to cartesian coordinates.\n\n    Args:\n        x (tensor).\n        y (tensor).\n        z (tensor).\n\n    Returns:\n       (theta, phi)\n    \"\"\"", "\n", "theta", "=", "torch", ".", "arccos", "(", "z", ")", "\n", "phi", "=", "torch", ".", "atan2", "(", "y", ",", "x", ")", "\n", "return", "theta", ",", "phi", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.create_sphere": [[85, 91], ["pytorch3d.utils.ico_sphere", "pytorch3d.utils.ico_sphere.verts_padded", "pytorch3d.utils.ico_sphere.faces_padded"], "function", ["None"], ["", "def", "create_sphere", "(", "level", "=", "4", ",", "device", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Creates a unit ico-sphere.\n    \"\"\"", "\n", "mesh", "=", "ico_sphere", "(", "level", "=", "level", ",", "device", "=", "device", ")", "\n", "return", "mesh", ".", "verts_padded", "(", ")", "[", "0", "]", ",", "mesh", ".", "faces_padded", "(", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.unwrap_uv_map": [[93, 109], ["torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.meshgrid", "torch.meshgrid", "geometry.spherical_to_cartesian", "torch.dstack", "torch.dstack"], "function", ["home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.spherical_to_cartesian"], ["", "def", "unwrap_uv_map", "(", "height", "=", "256", ",", "width", "=", "256", ")", ":", "\n", "    ", "\"\"\"\n    Samples spherical coordinates to unwrap a UV map.\n\n    Args:\n        height (int).\n        width (int).\n\n    Returns:\n        Spherical coordinates (H,W,3).\n    \"\"\"", "\n", "theta_", "=", "torch", ".", "linspace", "(", "0", ",", "np", ".", "pi", ",", "height", ")", "\n", "phi_", "=", "torch", ".", "linspace", "(", "-", "np", ".", "pi", ",", "np", ".", "pi", ",", "width", ")", "\n", "theta", ",", "phi", "=", "torch", ".", "meshgrid", "(", "theta_", ",", "phi_", ")", "\n", "x", ",", "y", ",", "z", "=", "spherical_to_cartesian", "(", "theta", ",", "phi", ")", "\n", "return", "torch", ".", "dstack", "(", "(", "x", ",", "y", ",", "z", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.image.antialias": [[5, 23], ["isinstance", "Image.fromarray.resize", "PIL.Image.fromarray", "numpy.array", "numpy.array", "Image.fromarray.clip"], "function", ["None"], ["def", "antialias", "(", "image", ")", ":", "\n", "    ", "\"\"\"\n    Performs antialiasing on image by downsampling using Lanczos filter.\n\n    Args:\n        image (PIL.Image or np.ndarray): Image to be antialiased.\n\n    Returns:\n        Image or ndarray: Antialiased image.\n    \"\"\"", "\n", "is_image", "=", "isinstance", "(", "image", ",", "Image", ".", "Image", ")", "\n", "if", "not", "is_image", ":", "\n", "        ", "image", "=", "Image", ".", "fromarray", "(", "(", "image", ".", "clip", "(", "0", ",", "1", ")", "*", "255", ")", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "", "shape", "=", "np", ".", "array", "(", "image", ".", "size", "[", ":", "2", "]", ")", "//", "2", "\n", "image", "=", "image", ".", "resize", "(", "shape", ",", "Image", ".", "LANCZOS", ")", "\n", "if", "not", "is_image", ":", "\n", "        ", "image", "=", "np", ".", "array", "(", "image", ")", "/", "255.0", "\n", "", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.image.crop_image": [[25, 43], ["numpy.array", "PIL.Image.new", "Image.new.paste"], "function", ["None"], ["", "def", "crop_image", "(", "image", ",", "bbox", ")", ":", "\n", "    ", "\"\"\"\n    Crops PIL image using bounding box.\n\n    Args:\n        image (PIL.Image): Image to be cropped.\n        bbox (tuple): Integer bounding box (xyxy).\n    \"\"\"", "\n", "bbox", "=", "np", ".", "array", "(", "bbox", ")", "\n", "if", "image", ".", "mode", "==", "\"RGB\"", ":", "\n", "        ", "default", "=", "(", "255", ",", "255", ",", "255", ")", "\n", "", "elif", "image", ".", "mode", "==", "\"RGBA\"", ":", "\n", "        ", "default", "=", "(", "255", ",", "255", ",", "255", ",", "255", ")", "\n", "", "else", ":", "\n", "        ", "default", "=", "0", "\n", "", "bg", "=", "Image", ".", "new", "(", "image", ".", "mode", ",", "(", "bbox", "[", "2", "]", "-", "bbox", "[", "0", "]", ",", "bbox", "[", "3", "]", "-", "bbox", "[", "1", "]", ")", ",", "default", ")", "\n", "bg", ".", "paste", "(", "image", ",", "(", "-", "bbox", "[", "0", "]", ",", "-", "bbox", "[", "1", "]", ")", ")", "\n", "return", "bg", "\n", "", ""]], "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.sampling.sample_consistent_points": [[15, 26], ["sampling.sample_faces", "sampling._rand_barycentric_coords", "samples.append", "sampling.sample_points_from_faces"], "function", ["home.repos.pwc.inspect_result.jasonyzhang_ners.utils.sampling.sample_faces", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.sampling._rand_barycentric_coords", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.sampling.sample_points_from_faces"], ["def", "sample_consistent_points", "(", "verts", ",", "faces", ",", "trg_verts_list", ",", "num_samples", ":", "int", "=", "10000", ")", ":", "\n", "    ", "sample_face_idxs", "=", "sample_faces", "(", "verts", ",", "faces", ",", "num_samples", ")", "\n", "barycentric_coords", "=", "_rand_barycentric_coords", "(", "\n", "num_samples", ",", "1", ",", "verts", ".", "dtype", ",", "verts", ".", "device", "\n", ")", "\n", "samples", "=", "[", "]", "\n", "for", "vs", "in", "trg_verts_list", ":", "\n", "        ", "samples", ".", "append", "(", "\n", "sample_points_from_faces", "(", "vs", ",", "faces", ",", "sample_face_idxs", ",", "barycentric_coords", ")", "\n", ")", "\n", "", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.sampling.sample_faces": [[28, 33], ["torch.no_grad", "pytorch3d.ops.mesh_face_areas_normals.mesh_face_areas_normals", "areas.multinomial"], "function", ["None"], ["", "def", "sample_faces", "(", "verts", ",", "faces", ",", "num_samples", ":", "int", "=", "10000", ")", ":", "\n", "    ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "areas", ",", "_", "=", "mesh_face_areas_normals", "(", "verts", ",", "faces", ")", "\n", "sample_face_idxs", "=", "areas", ".", "multinomial", "(", "num_samples", ",", "replacement", "=", "True", ")", "\n", "return", "sample_face_idxs", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.sampling.sample_points_from_faces": [[35, 46], ["faces.long"], "function", ["None"], ["", "", "def", "sample_points_from_faces", "(", "verts", ",", "faces", ",", "face_idx", ",", "barycentric_coords", ")", ":", "\n", "    ", "w0", ",", "w1", ",", "w2", "=", "barycentric_coords", "\n", "face_verts", "=", "verts", "[", "faces", ".", "long", "(", ")", "]", "\n", "v0", ",", "v1", ",", "v2", "=", "face_verts", "[", ":", ",", "0", "]", ",", "face_verts", "[", ":", ",", "1", "]", ",", "face_verts", "[", ":", ",", "2", "]", "\n", "\n", "# Use the barycentric coords to get a point on each sampled face.", "\n", "a", "=", "v0", "[", "face_idx", "]", "# (N, num_samples, 3)", "\n", "b", "=", "v1", "[", "face_idx", "]", "\n", "c", "=", "v2", "[", "face_idx", "]", "\n", "samples", "=", "w0", "*", "a", "+", "w1", "*", "b", "+", "w2", "*", "c", "\n", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.sampling._rand_barycentric_coords": [[48, 70], ["torch.rand", "u.sqrt"], "function", ["None"], ["", "def", "_rand_barycentric_coords", "(", "\n", "size1", ",", "size2", ",", "dtype", ",", "device", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "    ", "\"\"\"\n    Helper function to generate random barycentric coordinates which are uniformly\n    distributed over a triangle.\n    Args:\n        size1, size2: The number of coordinates generated will be size1*size2.\n                      Output tensors will each be of shape (size1, size2).\n        dtype: Datatype to generate.\n        device: A torch.device object on which the outputs will be allocated.\n    Returns:\n        w0, w1, w2: Tensors of shape (size1, size2) giving random barycentric\n            coordinates\n    \"\"\"", "\n", "uv", "=", "torch", ".", "rand", "(", "2", ",", "size1", ",", "size2", ",", "dtype", "=", "dtype", ",", "device", "=", "device", ")", "\n", "u", ",", "v", "=", "uv", "[", "0", "]", ",", "uv", "[", "1", "]", "\n", "u_sqrt", "=", "u", ".", "sqrt", "(", ")", "\n", "w0", "=", "1.0", "-", "u_sqrt", "\n", "w1", "=", "u_sqrt", "*", "(", "1.0", "-", "v", ")", "\n", "w2", "=", "u_sqrt", "*", "v", "\n", "return", "w0", ",", "w1", ",", "w2", "\n", "", ""]], "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.rendering.ambient_light": [[10, 18], ["pytorch3d.renderer.DirectionalLights"], "function", ["None"], ["def", "ambient_light", "(", "device", "=", "\"cpu\"", ")", ":", "\n", "    ", "lights", "=", "pytorch3d", ".", "renderer", ".", "DirectionalLights", "(", "\n", "device", "=", "device", ",", "\n", "ambient_color", "=", "(", "(", "1.0", ",", "1.0", ",", "1.0", ")", ",", ")", ",", "\n", "diffuse_color", "=", "(", "(", "0.0", ",", "0.0", ",", "0.0", ")", ",", ")", ",", "\n", "specular_color", "=", "(", "(", "0.0", ",", "0.0", ",", "0.0", ")", ",", ")", ",", "\n", ")", "\n", "return", "lights", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.rendering.get_renderers": [[20, 68], ["pytorch3d.renderer.BlendParams", "pytorch3d.renderer.RasterizationSettings", "pytorch3d.renderer.RasterizationSettings", "ners.pytorch3d.rasterizer.MeshRasterizer", "ners.pytorch3d.rasterizer.MeshRasterizer", "pytorch3d.renderer.SoftPhongShader", "pytorch3d.renderer.SoftSilhouetteShader", "pytorch3d.renderer.MeshRenderer", "pytorch3d.renderer.MeshRenderer", "rendering.ambient_light", "numpy.log"], "function", ["home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.rendering.ambient_light"], ["", "def", "get_renderers", "(", "device", "=", "None", ",", "cameras", "=", "None", ",", "img_size", "=", "256", ")", ":", "\n", "    ", "\"\"\"\n    Returns Pytorch3D MeshRenderers.\n\n    Args:\n        device (torch.device): Device to place renderers on.\n        cameras: Default pytorch3d cameras for renderers.\n        img_size (int): Rendered image size.\n\n    Returns:\n        textured_img_renderer\n        masked_img_renderer\n    \"\"\"", "\n", "blend_params", "=", "pytorch3d", ".", "renderer", ".", "BlendParams", "(", "sigma", "=", "1e-4", ",", "gamma", "=", "1e-4", ")", "\n", "# TODO(@jason): As of v0.4.0, there's a bug in Pytorch3d with the default", "\n", "# perspective_correct. Revisit this for future versions.", "\n", "img_raster_settings", "=", "pytorch3d", ".", "renderer", ".", "RasterizationSettings", "(", "\n", "image_size", "=", "img_size", ",", "\n", "blur_radius", "=", "0.0", ",", "\n", "faces_per_pixel", "=", "1", ",", "\n", "bin_size", "=", "0", ",", "\n", "perspective_correct", "=", "False", ",", "\n", ")", "\n", "mask_raster_settings", "=", "pytorch3d", ".", "renderer", ".", "RasterizationSettings", "(", "\n", "image_size", "=", "img_size", ",", "\n", "blur_radius", "=", "np", ".", "log", "(", "1.0", "/", "1e-4", "-", "1.0", ")", "*", "blend_params", ".", "sigma", ",", "\n", "faces_per_pixel", "=", "50", ",", "\n", "bin_size", "=", "0", ",", "\n", "perspective_correct", "=", "False", ",", "\n", ")", "\n", "img_rasterizer", "=", "MeshRasterizer", "(", "\n", "cameras", "=", "cameras", ",", "raster_settings", "=", "img_raster_settings", "\n", ")", "\n", "mask_rasterizer", "=", "MeshRasterizer", "(", "\n", "cameras", "=", "cameras", ",", "raster_settings", "=", "mask_raster_settings", "\n", ")", "\n", "textured_img_shader", "=", "pytorch3d", ".", "renderer", ".", "SoftPhongShader", "(", "\n", "cameras", "=", "cameras", ",", "\n", "device", "=", "device", ",", "\n", "blend_params", "=", "blend_params", ",", "\n", "lights", "=", "ambient_light", "(", "device", "=", "device", ")", ",", "\n", ")", "\n", "mask_shader", "=", "pytorch3d", ".", "renderer", ".", "SoftSilhouetteShader", "(", "blend_params", "=", "blend_params", ")", "\n", "textured_img_renderer", "=", "pytorch3d", ".", "renderer", ".", "MeshRenderer", "(", "\n", "img_rasterizer", ",", "textured_img_shader", "\n", ")", "\n", "mask_renderer", "=", "pytorch3d", ".", "renderer", ".", "MeshRenderer", "(", "mask_rasterizer", ",", "mask_shader", ")", "\n", "return", "textured_img_renderer", ",", "mask_renderer", "\n", "", ""]], "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.rasterizer.MeshRasterizer.forward": [[22, 67], ["rasterizer.MeshRasterizer.transform", "kwargs.get", "pytorch3d.renderer.mesh.rasterize_meshes", "kwargs.get", "kwargs.get.get_camera_center().repeat", "cameras_centers.transpose.transpose.transpose", "rasterizer.Fragments", "kwargs.get.get_camera_center"], "methods", ["None"], ["    ", "def", "forward", "(", "self", ",", "meshes_world", ",", "**", "kwargs", ")", "->", "Fragments", ":", "\n", "        ", "\"\"\"\n        Args:\n            meshes_world: a Meshes object representing a batch of meshes with\n                          coordinates in world space.\n        Returns:\n            Fragments: Rasterization outputs as a named tuple.\n        \"\"\"", "\n", "meshes_screen", "=", "self", ".", "transform", "(", "meshes_world", ",", "**", "kwargs", ")", "\n", "raster_settings", "=", "kwargs", ".", "get", "(", "\"raster_settings\"", ",", "self", ".", "raster_settings", ")", "\n", "\n", "# By default, turn on clip_barycentric_coords if blur_radius > 0.", "\n", "# When blur_radius > 0, a face can be matched to a pixel that is outside the", "\n", "# face, resulting in negative barycentric coordinates.", "\n", "clip_barycentric_coords", "=", "raster_settings", ".", "clip_barycentric_coords", "\n", "if", "clip_barycentric_coords", "is", "None", ":", "\n", "            ", "clip_barycentric_coords", "=", "raster_settings", ".", "blur_radius", ">", "0.0", "\n", "\n", "", "pix_to_face", ",", "zbuf", ",", "bary_coords", ",", "dists", "=", "rasterize_meshes", "(", "\n", "meshes_screen", ",", "\n", "image_size", "=", "raster_settings", ".", "image_size", ",", "\n", "blur_radius", "=", "raster_settings", ".", "blur_radius", ",", "\n", "faces_per_pixel", "=", "raster_settings", ".", "faces_per_pixel", ",", "\n", "bin_size", "=", "raster_settings", ".", "bin_size", ",", "\n", "max_faces_per_bin", "=", "raster_settings", ".", "max_faces_per_bin", ",", "\n", "perspective_correct", "=", "raster_settings", ".", "perspective_correct", ",", "\n", "clip_barycentric_coords", "=", "clip_barycentric_coords", ",", "\n", "cull_backfaces", "=", "raster_settings", ".", "cull_backfaces", ",", "\n", ")", "\n", "cameras", "=", "kwargs", ".", "get", "(", "\"cameras\"", ",", "self", ".", "cameras", ")", "\n", "cameras_centers", "=", "cameras", ".", "get_camera_center", "(", ")", ".", "repeat", "(", "# (K, H, W, N, 3)", "\n", "raster_settings", ".", "faces_per_pixel", ",", "\n", "raster_settings", ".", "image_size", ",", "\n", "raster_settings", ".", "image_size", ",", "\n", "1", ",", "\n", "1", ",", "\n", ")", "\n", "\n", "cameras_centers", "=", "cameras_centers", ".", "transpose", "(", "0", ",", "3", ")", "# (N, H, W, K, 3)", "\n", "return", "Fragments", "(", "\n", "pix_to_face", "=", "pix_to_face", ",", "\n", "zbuf", "=", "zbuf", ",", "\n", "bary_coords", "=", "bary_coords", ",", "\n", "dists", "=", "dists", ",", "\n", "cameras_centers", "=", "cameras_centers", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.cameras.PerspectiveCameras.__init__": [[48, 103], ["pytorch3d.renderer.FoVPerspectiveCameras.__init__", "torch.is_tensor", "torch.tensor", "len", "image_center.repeat.repeat.repeat", "torch.is_tensor", "torch.tensor", "len", "crop_scale.repeat.repeat.repeat"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "znear", "=", "1.0", ",", "\n", "zfar", "=", "100.0", ",", "\n", "aspect_ratio", "=", "1.0", ",", "\n", "fov", "=", "60.0", ",", "\n", "degrees", ":", "bool", "=", "True", ",", "\n", "R", "=", "_R", ",", "\n", "T", "=", "_T", ",", "\n", "K", "=", "None", ",", "\n", "image_center", "=", "(", "(", "0", ",", "0", ")", ",", ")", ",", "\n", "crop_scale", "=", "(", "1", ",", ")", ",", "\n", "device", "=", "\"cpu\"", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Perspective Cameras with support for cropped images and appropriate scaling of\n        the focal length.\n\n        Args:\n            znear: near clipping plane of the view frustrum.\n            zfar: far clipping plane of the view frustrum.\n            aspect_ratio: aspect ratio of the image pixels.\n                1.0 indicates square pixels.\n            fov: field of view angle of the camera.\n            degrees: bool, set to True if fov is specified in degrees.\n            R: Rotation matrix of shape (N, 3, 3)\n            T: Translation matrix of shape (N, 3)\n            K: (optional) A calibration matrix of shape (N, 4, 4)\n                If provided, don't need znear, zfar, fov, aspect_ratio, degrees\n            image_center: Principal points of camera projection specified in NDC (N, 2).\n            crop_scale: Scaling factor for the focal length of a crop.\n            device: torch.device or string\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "device", "=", "device", ",", "\n", "znear", "=", "znear", ",", "\n", "zfar", "=", "zfar", ",", "\n", "aspect_ratio", "=", "aspect_ratio", ",", "\n", "fov", "=", "fov", ",", "\n", "R", "=", "R", ",", "\n", "T", "=", "T", ",", "\n", "K", "=", "K", ",", "\n", "degrees", "=", "degrees", ",", "\n", ")", "\n", "if", "not", "torch", ".", "is_tensor", "(", "image_center", ")", ":", "\n", "            ", "image_center", "=", "torch", ".", "tensor", "(", "image_center", ",", "device", "=", "self", ".", "device", ")", "\n", "", "if", "len", "(", "image_center", ")", "!=", "self", ".", "_N", ":", "\n", "            ", "image_center", "=", "image_center", ".", "repeat", "(", "self", ".", "_N", ",", "1", ")", "\n", "", "self", ".", "image_center", "=", "image_center", "\n", "\n", "if", "not", "torch", ".", "is_tensor", "(", "crop_scale", ")", ":", "\n", "            ", "crop_scale", "=", "torch", ".", "tensor", "(", "crop_scale", ",", "device", "=", "self", ".", "device", ")", "\n", "", "if", "len", "(", "crop_scale", ")", "!=", "self", ".", "_N", ":", "\n", "            ", "crop_scale", "=", "crop_scale", ".", "repeat", "(", "self", ".", "_N", ")", "\n", "", "self", ".", "crop_scale", "=", "crop_scale", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.cameras.PerspectiveCameras.compute_projection_matrix": [[104, 158], ["torch.zeros", "torch.ones", "torch.is_tensor", "torch.tensor", "torch.tan", "cameras.PerspectiveCameras.image_center.clone"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone"], ["", "def", "compute_projection_matrix", "(", "self", ",", "znear", ",", "zfar", ",", "fov", ",", "aspect_ratio", ",", "degrees", ")", ":", "\n", "        ", "\"\"\"\n        Compute the calibration matrix K of shape (N, 4, 4)\n\n        Args:\n            znear: near clipping plane of the view frustrum.\n            zfar: far clipping plane of the view frustrum.\n            fov: field of view angle of the camera.\n            aspect_ratio: aspect ratio of the image pixels.\n                1.0 indicates square pixels.\n            degrees: bool, set to True if fov is specified in degrees.\n\n        Returns:\n            torch.floatTensor of the calibration matrix with shape (N, 4, 4)\n        \"\"\"", "\n", "K", "=", "torch", ".", "zeros", "(", "(", "self", ".", "_N", ",", "4", ",", "4", ")", ",", "device", "=", "self", ".", "device", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "ones", "=", "torch", ".", "ones", "(", "(", "self", ".", "_N", ")", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "self", ".", "device", ")", "\n", "if", "degrees", ":", "\n", "            ", "fov", "=", "(", "np", ".", "pi", "/", "180", ")", "*", "fov", "\n", "\n", "", "if", "not", "torch", ".", "is_tensor", "(", "fov", ")", ":", "\n", "            ", "fov", "=", "torch", ".", "tensor", "(", "fov", ",", "device", "=", "self", ".", "device", ")", "\n", "", "tanHalfFov", "=", "torch", ".", "tan", "(", "(", "fov", "/", "2", ")", ")", "*", "self", ".", "crop_scale", "\n", "principal_point", "=", "self", ".", "image_center", ".", "clone", "(", ")", "*", "(", "znear", "*", "tanHalfFov", ")", ".", "unsqueeze", "(", "1", ")", "\n", "max_y", "=", "tanHalfFov", "*", "znear", "\n", "min_y", "=", "-", "max_y", "\n", "max_x", "=", "max_y", "*", "aspect_ratio", "\n", "min_x", "=", "-", "max_x", "\n", "max_y", "+=", "principal_point", "[", ":", ",", "1", "]", "\n", "min_y", "+=", "principal_point", "[", ":", ",", "1", "]", "\n", "max_x", "+=", "principal_point", "[", ":", ",", "0", "]", "\n", "min_x", "+=", "principal_point", "[", ":", ",", "0", "]", "\n", "\n", "# NOTE: In OpenGL the projection matrix changes the handedness of the", "\n", "# coordinate frame. i.e the NDC space postive z direction is the", "\n", "# camera space negative z direction. This is because the sign of the z", "\n", "# in the projection matrix is set to -1.0.", "\n", "# In pytorch3d we maintain a right handed coordinate system throughout", "\n", "# so the so the z sign is 1.0.", "\n", "z_sign", "=", "1.0", "\n", "\n", "K", "[", ":", ",", "0", ",", "0", "]", "=", "2.0", "*", "znear", "/", "(", "max_x", "-", "min_x", ")", "\n", "K", "[", ":", ",", "1", ",", "1", "]", "=", "2.0", "*", "znear", "/", "(", "max_y", "-", "min_y", ")", "\n", "K", "[", ":", ",", "0", ",", "2", "]", "=", "(", "max_x", "+", "min_x", ")", "/", "(", "max_x", "-", "min_x", ")", "\n", "K", "[", ":", ",", "1", ",", "2", "]", "=", "(", "max_y", "+", "min_y", ")", "/", "(", "max_y", "-", "min_y", ")", "\n", "K", "[", ":", ",", "3", ",", "2", "]", "=", "z_sign", "*", "ones", "\n", "\n", "# NOTE: This maps the z coordinate from [0, 1] where z = 0 if the point", "\n", "# is at the near clipping plane and z = 1 when the point is at the far", "\n", "# clipping plane.", "\n", "K", "[", ":", ",", "2", ",", "2", "]", "=", "z_sign", "*", "zfar", "/", "(", "zfar", "-", "znear", ")", "\n", "K", "[", ":", ",", "2", ",", "3", "]", "=", "-", "(", "zfar", "*", "znear", ")", "/", "(", "zfar", "-", "znear", ")", "\n", "\n", "return", "K", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.cameras.PerspectiveCameras.__getitem__": [[159, 208], ["isinstance", "cameras.PerspectiveCameras.__class__", "getattr", "attributes.items", "isinstance", "attributes.items", "isinstance", "attributes.items", "isinstance", "attributes.items", "IndexError", "torch.stack", "IndexError", "index.tolist.tolist.nonzero", "index.tolist.tolist.tolist", "index.tolist.tolist.dim", "index.tolist.tolist.squeeze", "torch.stack", "index.tolist.tolist.numel"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "attribute_names", "=", "[", "\n", "\"znear\"", ",", "\n", "\"zfar\"", ",", "\n", "\"aspect_ratio\"", ",", "\n", "\"fov\"", ",", "\n", "\"R\"", ",", "\n", "\"T\"", ",", "\n", "\"K\"", ",", "\n", "\"crop_scale\"", ",", "\n", "\"image_center\"", ",", "\n", "]", "\n", "attributes", "=", "{", "k", ":", "getattr", "(", "self", ",", "k", ")", "for", "k", "in", "attribute_names", "}", "\n", "if", "isinstance", "(", "index", ",", "int", ")", ":", "\n", "            ", "for", "k", ",", "v", "in", "attributes", ".", "items", "(", ")", ":", "\n", "                ", "if", "v", "is", "None", ":", "\n", "                    ", "attributes", "[", "k", "]", "=", "v", "\n", "", "else", ":", "\n", "                    ", "attributes", "[", "k", "]", "=", "v", "[", "index", ":", "index", "+", "1", "]", "\n", "", "", "", "elif", "isinstance", "(", "index", ",", "slice", ")", ":", "\n", "            ", "for", "k", ",", "v", "in", "attributes", ".", "items", "(", ")", ":", "\n", "                ", "if", "v", "is", "None", ":", "\n", "                    ", "attributes", "[", "k", "]", "=", "v", "\n", "", "else", ":", "\n", "                    ", "attributes", "[", "k", "]", "=", "v", "[", "index", "]", "\n", "", "", "", "elif", "isinstance", "(", "index", ",", "list", ")", ":", "\n", "            ", "for", "k", ",", "v", "in", "attributes", ".", "items", "(", ")", ":", "\n", "                ", "if", "v", "is", "None", ":", "\n", "                    ", "attributes", "[", "k", "]", "=", "v", "\n", "", "else", ":", "\n", "                    ", "attributes", "[", "k", "]", "=", "torch", ".", "stack", "(", "[", "v", "[", "i", "]", "for", "i", "in", "index", "]", ",", "0", ")", "\n", "", "", "", "elif", "isinstance", "(", "index", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "if", "index", ".", "dim", "(", ")", "!=", "1", "or", "index", ".", "dtype", ".", "is_floating_point", ":", "\n", "                ", "raise", "IndexError", "(", "index", ")", "\n", "# NOTE consider converting index to cpu for efficiency", "\n", "", "if", "index", ".", "dtype", "==", "torch", ".", "bool", ":", "\n", "# advanced indexing on a single dimension", "\n", "                ", "index", "=", "index", ".", "nonzero", "(", ")", "\n", "index", "=", "index", ".", "squeeze", "(", "1", ")", "if", "index", ".", "numel", "(", ")", ">", "0", "else", "index", "\n", "index", "=", "index", ".", "tolist", "(", ")", "\n", "", "for", "k", ",", "v", "in", "attributes", ".", "items", "(", ")", ":", "\n", "                ", "if", "v", "is", "None", ":", "\n", "                    ", "attributes", "[", "k", "]", "=", "v", "\n", "", "else", ":", "\n", "                    ", "attributes", "[", "k", "]", "=", "torch", ".", "stack", "(", "[", "v", "[", "i", "]", "for", "i", "in", "index", "]", ",", "0", ")", "\n", "", "", "", "else", ":", "\n", "            ", "raise", "IndexError", "(", "index", ")", "\n", "\n", "", "return", "self", ".", "__class__", "(", "device", "=", "self", ".", "device", ",", "degrees", "=", "self", ".", "degrees", ",", "**", "attributes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.cameras.PerspectiveCameras.detach": [[209, 227], ["cameras.PerspectiveCameras.__class__", "getattr", "getattr.detach"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach"], ["", "def", "detach", "(", "self", ")", ":", "\n", "        ", "attribute_names", "=", "[", "\n", "\"znear\"", ",", "\n", "\"zfar\"", ",", "\n", "\"aspect_ratio\"", ",", "\n", "\"fov\"", ",", "\n", "\"R\"", ",", "\n", "\"T\"", ",", "\n", "\"K\"", ",", "\n", "\"crop_scale\"", ",", "\n", "\"image_center\"", ",", "\n", "]", "\n", "attributes", "=", "{", "}", "\n", "for", "k", "in", "attribute_names", ":", "\n", "            ", "v", "=", "getattr", "(", "self", ",", "k", ")", "\n", "if", "v", "is", "not", "None", ":", "\n", "                ", "attributes", "[", "k", "]", "=", "v", ".", "detach", "(", ")", "\n", "", "", "return", "self", ".", "__class__", "(", "device", "=", "self", ".", "device", ",", "degrees", "=", "self", ".", "degrees", ",", "**", "attributes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.cameras.PerspectiveCameras.get_focal_length": [[228, 235], ["torch.abs", "torch.tan"], "methods", ["None"], ["", "def", "get_focal_length", "(", "self", ",", "fov", "=", "None", ")", ":", "\n", "        ", "if", "fov", "is", "None", ":", "\n", "            ", "fov", "=", "self", ".", "fov", "\n", "", "if", "self", ".", "degrees", ":", "\n", "            ", "fov", "=", "(", "np", ".", "pi", "/", "180", ")", "*", "fov", "\n", "# If fov is negative, f should still be positive.", "\n", "", "return", "torch", ".", "abs", "(", "1", "/", "torch", ".", "tan", "(", "fov", "/", "2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.cameras.PerspectiveCameras.extra_repr": [[236, 238], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "return", "f\"N={self._N}\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.cameras.compute_crop_parameters": [[13, 45], ["numpy.array", "max", "isinstance", "max", "numpy.array"], "function", ["None"], ["def", "compute_crop_parameters", "(", "image_size", ",", "bbox", ",", "image_center", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Computes the principal point and scaling factor for focal length given a square\n    bounding box crop of an image.\n\n    These intrinsic parameters are used to preserve the original principal point even\n    after cropping the image.\n\n    Args:\n        image_size (int or array): Size of image, either length of longer dimension or\n            (N, H, C).\n        bbox: Square bounding box in xyxy (4,).\n        image_center: Center of projection/principal point (2,).\n\n    Returns:\n        principal_point: Coordinates in NDC using Pytorch3D convention with (1, 1)\n            as upper-left (2,).\n        crop_scale (float): Scaling factor for focal length.\n    \"\"\"", "\n", "bbox", "=", "np", ".", "array", "(", "bbox", ")", "\n", "b", "=", "max", "(", "bbox", "[", "2", ":", "]", "-", "bbox", "[", ":", "2", "]", ")", "\n", "if", "isinstance", "(", "image_size", ",", "int", ")", ":", "\n", "        ", "h", "=", "w", "=", "image_size", "\n", "", "else", ":", "\n", "        ", "h", ",", "w", ",", "*", "c", "=", "image_size", "\n", "image_size", "=", "max", "(", "image_size", ")", "\n", "", "if", "image_center", "is", "None", ":", "\n", "        ", "image_center", "=", "np", ".", "array", "(", "[", "w", "/", "2", ",", "h", "/", "2", "]", ")", "\n", "", "bbox_center", "=", "(", "bbox", "[", ":", "2", "]", "+", "bbox", "[", "2", ":", "]", ")", "/", "2", "\n", "crop_scale", "=", "b", "/", "image_size", "\n", "principal_point", "=", "2", "*", "(", "bbox_center", "-", "image_center", ")", "/", "b", "\n", "return", "principal_point", ",", "crop_scale", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.__init__": [[107, 165], ["ners.create_sphere", "sphere_vs.to", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.create_sphere"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "texture_predictor", ",", "\n", "faces", ",", "\n", "verts_sphere_coords", ",", "\n", "verts_deformed_coords", "=", "None", ",", "\n", "verts_normals", "=", "None", ",", "\n", "predict_radiance", "=", "False", ",", "\n", "env_map", "=", "None", ",", "\n", "specularity", "=", "None", ",", "\n", "shininess", "=", "None", ",", "\n", "jitter_env_map_rays", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Textures are represented implicitly using the weights of a neural network.\n        NOTE: only ONE texture is supported across all batches. Thus, each batch should\n        correspond to the same mesh.\n\n        Args:\n            texture_predictor (ImplicitTextureNet): Network that maps from uv coordinate\n                to RGB color.\n            faces (F, 3): Faces indices.\n            verts_sphere_coords (V, 3): Coordinates from sphere (UV).\n            verts_deformed_coords (V, 3): Coordinates from deformed mesh. Used to\n                compute illumination.\n            verts_normals (V, 3): Normals at each vertex. Used to compute illumination.\n            predict_radiance (bool): If True, predicts a radiance (illumination).\n            env_map (nn.Module): Environment map mapping (dx, dy, dz) to light\n                intensity.\n            specularity (float, Tensor): Weighting factor of specularity.\n            shininess (float, Tensor): Shininess coefficient.\n            jitter_env_map_rays (bool): If True, jitters rays to environment map.\n\n        Returns:\n            texels (N,H,W,K,3): RGB texels.\n        \"\"\"", "\n", "assert", "len", "(", "faces", ".", "shape", ")", "==", "2", ",", "\"Faces should be (F,3).\"", "\n", "assert", "len", "(", "verts_sphere_coords", ".", "shape", ")", "==", "2", ",", "\"Verts should be (V,3).\"", "\n", "if", "verts_deformed_coords", "is", "not", "None", ":", "\n", "            ", "assert", "len", "(", "verts_deformed_coords", ".", "shape", ")", "==", "2", ",", "\"Verts should be (V,3).\"", "\n", "", "self", ".", "texture_predictor", "=", "texture_predictor", "\n", "self", ".", "device", "=", "faces", ".", "device", "\n", "self", ".", "faces", "=", "faces", "\n", "self", ".", "verts_sphere_coords", "=", "verts_sphere_coords", "\n", "self", ".", "verts_deformed_coords", "=", "verts_deformed_coords", "\n", "self", ".", "verts_normals", "=", "verts_normals", "\n", "self", ".", "predict_radiance", "=", "predict_radiance", "\n", "self", ".", "env_map", "=", "env_map", "\n", "self", ".", "specularity", "=", "specularity", "\n", "self", ".", "shininess", "=", "shininess", "\n", "self", ".", "jitter_env_map_rays", "=", "jitter_env_map_rays", "\n", "\n", "# If set, will use this color instead of querying the texture predictor.", "\n", "self", ".", "texture_color", "=", "None", "\n", "# Set for when illumination computation needs a new GPU.", "\n", "self", ".", "custom_device", "=", "None", "\n", "sphere_vs", ",", "_", "=", "geom_util", ".", "create_sphere", "(", "3", ")", "\n", "self", ".", "env_map_rays", "=", "sphere_vs", ".", "to", "(", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.sample_textures": [[166, 248], ["faces_uv_coords.repeat().reshape.repeat().reshape.repeat().reshape", "pytorch3d.ops.interpolate_face_attributes", "pixel_uvs.reshape.reshape.reshape", "textures.TexturesImplicit.texture_predictor.forward", "torch.ones", "textures.TexturesImplicit.specularity.reshape", "textures.TexturesImplicit.shininess.reshape", "normalize().repeat", "textures.TexturesImplicit.verts_deformed_coords.unsqueeze", "fragments.cameras_centers[].unsqueeze", "textures.normalize", "compute_lighting_diffuse().reshape", "lighting_diffuse[].reshape", "pytorch3d.ops.interpolate_face_attributes().reshape", "compute_lighting_specular().reshape", "lighting_specular[].reshape", "pytorch3d.ops.interpolate_face_attributes().reshape", "textures.TexturesImplicit.faces.long", "faces_uv_coords.repeat().reshape.repeat().reshape.repeat", "pixel_uvs.reshape.reshape.norm", "hasattr", "AttributeError", "ners.random_rotation", "textures.normalize", "textures.compute_lighting_diffuse", "pytorch3d.ops.interpolate_face_attributes", "textures.compute_lighting_specular", "pytorch3d.ops.interpolate_face_attributes", "normalize().repeat.reshape", "sphere_vs.reshape", "normalize().repeat.reshape", "normalize.reshape", "textures.TexturesImplicit.faces.long", "textures.TexturesImplicit.faces.long"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.rasterizer.MeshRasterizer.forward", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.normalize", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.random_rotation", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.normalize", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.compute_lighting_diffuse", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.compute_lighting_specular"], ["", "def", "sample_textures", "(", "self", ",", "fragments", ",", "**", "kwargs", ")", ":", "\n", "        ", "N", ",", "H", ",", "W", ",", "K", "=", "fragments", ".", "pix_to_face", ".", "shape", "\n", "faces_uv_coords", "=", "self", ".", "verts_sphere_coords", "[", "self", ".", "faces", ".", "long", "(", ")", "]", "# (F,3,3)", "\n", "# not in torch 1.7.0 yet", "\n", "# faces_verts_coords = torch.tile(faces_verts_coords, (N, 1, 1))", "\n", "faces_uv_coords", "=", "faces_uv_coords", ".", "repeat", "(", "N", ",", "1", ",", "1", ",", "1", ")", ".", "reshape", "(", "\n", "-", "1", ",", "3", ",", "3", "\n", ")", "# (N*F,3,3)", "\n", "pixel_uvs", "=", "interpolate_face_attributes", "(", "# (N,H,W,K,3)", "\n", "fragments", ".", "pix_to_face", ",", "fragments", ".", "bary_coords", ",", "faces_uv_coords", "\n", ")", "\n", "pixel_uvs", "=", "pixel_uvs", ".", "reshape", "(", "N", "*", "H", "*", "W", "*", "K", ",", "3", ")", "\n", "# Normalize because interpolated points may not be on sphere anymore.", "\n", "pixel_uvs", "=", "pixel_uvs", "/", "(", "pixel_uvs", ".", "norm", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "+", "1e-6", ")", "\n", "if", "self", ".", "texture_color", "is", "None", ":", "\n", "# sampled_texture = self.texture_predictor.forward_batched(pixel_uvs)", "\n", "            ", "sampled_texture", "=", "self", ".", "texture_predictor", ".", "forward", "(", "pixel_uvs", ")", "\n", "", "else", ":", "\n", "# Set textures to be the given color.", "\n", "            ", "sampled_texture", "=", "torch", ".", "ones", "(", "N", "*", "H", "*", "W", "*", "K", ",", "3", ",", "device", "=", "self", ".", "device", ")", "\n", "sampled_texture", "*=", "self", ".", "texture_color", "\n", "\n", "", "if", "self", ".", "predict_radiance", ":", "\n", "            ", "if", "not", "hasattr", "(", "fragments", ",", "\"cameras_centers\"", ")", ":", "\n", "                ", "raise", "AttributeError", "(", "\n", "\"fragments do not have attribute 'cameras_centers'. Make sure you\"", "\n", "\"are using the custom MeshRasterizer.\"", "\n", ")", "\n", "\n", "", "sphere_vs", "=", "self", ".", "verts_sphere_coords", "\n", "specularity", "=", "self", ".", "specularity", ".", "reshape", "(", "-", "1", ",", "1", ")", "# (N1, 1)", "\n", "shininess", "=", "self", ".", "shininess", ".", "reshape", "(", "-", "1", ",", "1", ")", "# (N1, 1)", "\n", "\n", "env_map_rays", "=", "sphere_vs", "if", "self", ".", "env_map_rays", "is", "None", "else", "self", ".", "env_map_rays", "\n", "if", "self", ".", "jitter_env_map_rays", ":", "\n", "                ", "rot", "=", "geom_util", ".", "random_rotation", "(", "self", ".", "device", ")", "\n", "env_map_rays", "=", "env_map_rays", "@", "rot", "\n", "\n", "", "verts_normals", "=", "normalize", "(", "self", ".", "verts_normals", ")", ".", "repeat", "(", "N", ",", "1", ",", "1", ")", "# (N,V,3)", "\n", "verts", "=", "self", ".", "verts_deformed_coords", ".", "unsqueeze", "(", "0", ")", "# (1,V,3)", "\n", "# (N,1,3)", "\n", "camera_centers", "=", "fragments", ".", "cameras_centers", "[", ":", ",", "0", ",", "0", ",", "0", "]", ".", "unsqueeze", "(", "1", ")", "\n", "verts_views", "=", "normalize", "(", "camera_centers", "-", "verts", ")", "# (N,V,3)", "\n", "\n", "lighting_diffuse", "=", "compute_lighting_diffuse", "(", "# (N, V, 3)", "\n", "env_map", "=", "self", ".", "env_map", ",", "\n", "vertex_normals", "=", "verts_normals", ".", "reshape", "(", "-", "1", ",", "3", ")", ",", "\n", "env_map_rays", "=", "sphere_vs", ".", "reshape", "(", "-", "1", ",", "3", ")", ",", "\n", "device", "=", "self", ".", "custom_device", ",", "\n", ")", ".", "reshape", "(", "N", ",", "-", "1", ",", "3", ")", "\n", "\n", "lighting_diffuse", "=", "lighting_diffuse", "[", ":", ",", "self", ".", "faces", ".", "long", "(", ")", "]", ".", "reshape", "(", "-", "1", ",", "3", ",", "3", ")", "\n", "pixel_diffuse", "=", "interpolate_face_attributes", "(", "\n", "fragments", ".", "pix_to_face", ",", "\n", "fragments", ".", "bary_coords", ",", "\n", "lighting_diffuse", ",", "\n", ")", ".", "reshape", "(", "N", "*", "H", "*", "W", "*", "K", ",", "3", ")", "\n", "\n", "lighting_specular", "=", "compute_lighting_specular", "(", "# (N, V, 3)", "\n", "env_map", "=", "self", ".", "env_map", ",", "\n", "vertex_normals", "=", "verts_normals", ".", "reshape", "(", "-", "1", ",", "3", ")", ",", "\n", "view_direction", "=", "verts_views", ".", "reshape", "(", "-", "1", ",", "3", ")", ",", "\n", "env_map_rays", "=", "env_map_rays", ",", "\n", "specularity", "=", "specularity", ",", "\n", "shininess", "=", "shininess", ",", "\n", "device", "=", "self", ".", "custom_device", ",", "\n", "max_batch_size", "=", "5000", ",", "\n", ")", ".", "reshape", "(", "N", ",", "-", "1", ",", "3", ")", "\n", "\n", "lighting_specular", "=", "lighting_specular", "[", ":", ",", "self", ".", "faces", ".", "long", "(", ")", "]", ".", "reshape", "(", "\n", "-", "1", ",", "3", ",", "3", "\n", ")", "\n", "pixel_specular", "=", "interpolate_face_attributes", "(", "\n", "fragments", ".", "pix_to_face", ",", "\n", "fragments", ".", "bary_coords", ",", "\n", "lighting_specular", ",", "\n", ")", ".", "reshape", "(", "N", "*", "H", "*", "W", "*", "K", ",", "3", ")", "\n", "sampled_texture", "=", "sampled_texture", "*", "pixel_diffuse", "\n", "sampled_radiance", "=", "pixel_specular", "\n", "", "else", ":", "\n", "            ", "sampled_radiance", "=", "0", "\n", "", "return", "(", "sampled_texture", "+", "sampled_radiance", ")", ".", "reshape", "(", "N", ",", "H", ",", "W", ",", "K", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.check_shapes": [[249, 256], ["None"], "methods", ["None"], ["", "def", "check_shapes", "(", "\n", "self", ",", "batch_size", ":", "int", ",", "max_num_verts", ":", "int", ",", "max_num_faces", ":", "int", "\n", ")", "->", "bool", ":", "\n", "        ", "\"\"\"\n        Check if the dimensions of the verts features match that of the mesh verts\n        \"\"\"", "\n", "return", "self", ".", "verts_deformed_coords", ".", "shape", "[", "0", "]", "==", "max_num_verts", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.extend": [[257, 266], ["None"], "methods", ["None"], ["", "def", "extend", "(", "self", ",", "N", ")", ":", "\n", "        ", "\"\"\"\n        Repeats batch elements N times. (Not implemented since we only support single\n        texture across batch.)\n\n        Args:\n            N (int).\n        \"\"\"", "\n", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.__getitem__": [[267, 269], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "item", ")", ":", "\n", "        ", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach": [[270, 283], ["textures.TexturesImplicit.__class__", "textures.TexturesImplicit.texture_predictor.clone", "textures.TexturesImplicit.faces.detach", "textures.TexturesImplicit.verts_sphere_coords.detach", "textures.TexturesImplicit.verts_deformed_coords.detach", "textures.TexturesImplicit.radiance_predictor.detach"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach"], ["", "def", "detach", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Note that the texture predictor is NOT detached.\n        \"\"\"", "\n", "tex", "=", "self", ".", "__class__", "(", "\n", "texture_predictor", "=", "self", ".", "texture_predictor", ".", "clone", "(", ")", ",", "\n", "faces", "=", "self", ".", "faces", ".", "detach", "(", ")", ",", "\n", "verts_sphere_coords", "=", "self", ".", "verts_sphere_coords", ".", "detach", "(", ")", ",", "\n", "verts_deformed_coords", "=", "self", ".", "verts_deformed_coords", ".", "detach", "(", ")", ",", "\n", "radiance_predictor", "=", "self", ".", "radiance_predictor", ".", "detach", "(", ")", ",", "\n", "predict_radiance", "=", "self", ".", "predict_radiance", ",", "\n", ")", "\n", "return", "tex", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone": [[284, 294], ["textures.TexturesImplicit.__class__", "textures.TexturesImplicit.texture_predictor.clone", "textures.TexturesImplicit.faces.clone", "textures.TexturesImplicit.verts_sphere_coords.clone", "textures.TexturesImplicit.verts_deformed_coords.clone", "textures.TexturesImplicit.radiance_predictor.clone"], "methods", ["home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clone"], ["", "def", "clone", "(", "self", ")", ":", "\n", "        ", "tex", "=", "self", ".", "__class__", "(", "\n", "texture_predictor", "=", "self", ".", "texture_predictor", ".", "clone", "(", ")", ",", "\n", "faces", "=", "self", ".", "faces", ".", "clone", "(", ")", ",", "\n", "verts_sphere_coords", "=", "self", ".", "verts_sphere_coords", ".", "clone", "(", ")", ",", "\n", "verts_deformed_coords", "=", "self", ".", "verts_deformed_coords", ".", "clone", "(", ")", ",", "\n", "radiance_predictor", "=", "self", ".", "radiance_predictor", ".", "clone", "(", ")", ",", "\n", "predict_radiance", "=", "self", ".", "predict_radiance", ",", "\n", ")", "\n", "return", "tex", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.set_texture_color": [[295, 306], ["isinstance", "torch.tensor"], "methods", ["None"], ["", "def", "set_texture_color", "(", "self", ",", "rgb", "=", "(", "0.5", ",", "0.5", ",", "0.5", ")", ")", ":", "\n", "        ", "\"\"\"\n        Uses a set color for the texture (rather than using the texture network). Use\n        for visualizing the radiances.\n\n        Args:\n            rgb (tuple): Color. Defaults to (0.5, 0.5, 0.5).\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "rgb", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "rgb", "=", "torch", ".", "tensor", "(", "rgb", ",", "device", "=", "self", ".", "device", ")", "\n", "", "self", ".", "texture_color", "=", "rgb", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.clear_texture_color": [[307, 309], ["None"], "methods", ["None"], ["", "def", "clear_texture_color", "(", "self", ")", ":", "\n", "        ", "self", ".", "texture_color", "=", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.dot_product": [[9, 11], ["torch.sum"], "function", ["None"], ["def", "dot_product", "(", "a", ",", "b", ")", ":", "\n", "    ", "return", "torch", ".", "sum", "(", "a", "*", "b", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.normalize": [[13, 15], ["x.norm"], "function", ["None"], ["", "def", "normalize", "(", "x", ",", "eps", "=", "1e-6", ")", ":", "\n", "    ", "return", "x", "/", "(", "x", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "+", "eps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.compute_lighting_specular": [[17, 80], ["shininess.to.unsqueeze", "specularity.to.unsqueeze", "vertex_normals.to.unsqueeze", "view_direction.to.unsqueeze", "env_map_rays.to.unsqueeze", "env_map().unsqueeze().to", "range", "torch.cat", "vertex_normals.to.to", "view_direction.to.to", "env_map_rays.to.to", "specularity.to.to", "shininess.to.to", "slice", "torch.relu", "lights.append", "light.repeat.to", "light.repeat.repeat", "env_map().unsqueeze", "textures.dot_product", "env_map", "textures.dot_product"], "function", ["home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.dot_product", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.dot_product"], ["", "def", "compute_lighting_specular", "(", "\n", "env_map", ",", "\n", "vertex_normals", ",", "\n", "view_direction", ",", "\n", "env_map_rays", ",", "\n", "specularity", ",", "\n", "shininess", ",", "\n", "device", "=", "None", ",", "\n", "max_batch_size", "=", "None", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    env_map (nn.Module): Environment map mapping (dx, dy, dz) to light intensity.\n    vertex_normals (N1,3): Normalized normal vector.\n    view_direction (N1,3): Normalized vector from vertex/face to camera.\n    env_map_rays (N2,3): Discrete sampling of ray directions to environment map.\n    specularity ((N1,) or (1,)): Weighting factor of specularity\n    shininess ((N1,) or (1,)):\n    \"\"\"", "\n", "if", "device", ":", "\n", "        ", "temp_device", "=", "vertex_normals", ".", "device", "\n", "vertex_normals", "=", "vertex_normals", ".", "to", "(", "device", ")", "\n", "view_direction", "=", "view_direction", ".", "to", "(", "device", ")", "\n", "env_map_rays", "=", "env_map_rays", ".", "to", "(", "device", ")", "\n", "specularity", "=", "specularity", ".", "to", "(", "device", ")", "\n", "shininess", "=", "shininess", ".", "to", "(", "device", ")", "\n", "", "num_rays", "=", "env_map_rays", ".", "shape", "[", "0", "]", "\n", "# shininess = shininess.reshape(-1, 1, 1)  # (N1, 1, 3)", "\n", "# specularity = specularity.reshape(-1, 1, 1)  # (N1, 1, 3)", "\n", "shininess", "=", "shininess", ".", "unsqueeze", "(", "1", ")", "\n", "specularity", "=", "specularity", ".", "unsqueeze", "(", "1", ")", "\n", "N", "=", "vertex_normals", ".", "unsqueeze", "(", "1", ")", "# (N1, 1, 3)", "\n", "V", "=", "view_direction", ".", "unsqueeze", "(", "1", ")", "# (N1, 1, 3)", "\n", "\n", "L", "=", "env_map_rays", ".", "unsqueeze", "(", "0", ")", "# (1, N2, 3)", "\n", "E", "=", "env_map", "(", "env_map_rays", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "L", ".", "device", ")", "# (1, N2, 3)", "\n", "\n", "lights", "=", "[", "]", "\n", "\n", "if", "max_batch_size", "is", "None", ":", "\n", "        ", "max_batch_size", "=", "N", ".", "shape", "[", "0", "]", "\n", "\n", "", "for", "i", "in", "range", "(", "0", ",", "N", ".", "shape", "[", "0", "]", ",", "max_batch_size", ")", ":", "\n", "        ", "ind", "=", "slice", "(", "i", ",", "i", "+", "max_batch_size", ")", "\n", "N_", "=", "N", "[", "ind", "]", "\n", "V_", "=", "V", "[", "ind", "]", "\n", "shininess_", "=", "shininess", "if", "shininess", ".", "shape", "[", "0", "]", "==", "1", "else", "shininess", "[", "ind", "]", "\n", "specularity_", "=", "specularity", "if", "specularity", ".", "shape", "[", "0", "]", "==", "1", "else", "specularity", "[", "ind", "]", "\n", "R", "=", "2", "*", "dot_product", "(", "L", ",", "N_", ")", "*", "N_", "-", "L", "# (B, N2, 3)", "\n", "similarity", "=", "torch", ".", "relu", "(", "dot_product", "(", "R", ",", "V_", ")", ")", "# (B, N2, 1)", "\n", "del", "R", "\n", "weight", "=", "specularity_", "*", "(", "shininess_", "+", "1", ")", "/", "(", "2", "*", "np", ".", "pi", ")", "# (B, 1, 1)", "\n", "light", "=", "weight", "*", "similarity", "**", "shininess_", "# (B, 1, 1)", "\n", "del", "similarity", "\n", "light", "=", "(", "light", "*", "E", ")", ".", "sum", "(", "dim", "=", "1", ")", "/", "num_rays", "\n", "lights", ".", "append", "(", "light", ")", "\n", "", "light", "=", "torch", ".", "cat", "(", "lights", ",", "dim", "=", "0", ")", "\n", "\n", "if", "device", ":", "\n", "        ", "light", "=", "light", ".", "to", "(", "temp_device", ")", "\n", "", "if", "light", ".", "shape", "[", "-", "1", "]", "==", "1", ":", "\n", "# Repeat last channel 3x.", "\n", "        ", "light", "=", "light", ".", "repeat", "(", "(", "1", ",", ")", "*", "(", "light", ".", "ndim", "-", "1", ")", "+", "(", "3", ",", ")", ")", "\n", "", "return", "light", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.compute_lighting_diffuse": [[82, 104], ["vertex_normals.to.unsqueeze", "env_map_rays.to.unsqueeze", "env_map().unsqueeze().to", "vertex_normals.to.to", "env_map_rays.to.to", "torch.relu", "light_diffuse.to.sum", "light_diffuse.to.repeat", "light_diffuse.to.to", "env_map().unsqueeze", "textures.dot_product", "env_map"], "function", ["home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.dot_product"], ["", "def", "compute_lighting_diffuse", "(", "env_map", ",", "vertex_normals", ",", "env_map_rays", ",", "device", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    vertex_normals (N1,3): Normalized normal vector.\n    env_map_rays (N2,3): Discrete sampling of ray directions to environment map.\n    \"\"\"", "\n", "num_rays", "=", "env_map_rays", ".", "shape", "[", "0", "]", "\n", "if", "device", ":", "\n", "        ", "temp_device", "=", "vertex_normals", ".", "device", "\n", "vertex_normals", "=", "vertex_normals", ".", "to", "(", "device", ")", "\n", "env_map_rays", "=", "env_map_rays", ".", "to", "(", "device", ")", "\n", "", "N", "=", "vertex_normals", ".", "unsqueeze", "(", "1", ")", "# (N1, 1, 3)", "\n", "L", "=", "env_map_rays", ".", "unsqueeze", "(", "0", ")", "# (1, N2, 3)", "\n", "E", "=", "env_map", "(", "env_map_rays", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "L", ".", "device", ")", "\n", "light_diffuse", "=", "E", "*", "torch", ".", "relu", "(", "dot_product", "(", "N", ",", "L", ")", ")", "# (N1, N2, 3)", "\n", "light_diffuse", "=", "light_diffuse", ".", "sum", "(", "dim", "=", "1", ")", "/", "num_rays", "\n", "\n", "if", "light_diffuse", ".", "shape", "[", "-", "1", "]", "==", "1", ":", "\n", "# Repeat last channel 3x.", "\n", "        ", "light_diffuse", "=", "light_diffuse", ".", "repeat", "(", "(", "1", ",", ")", "*", "(", "light_diffuse", ".", "ndim", "-", "1", ")", "+", "(", "3", ",", ")", ")", "\n", "", "if", "device", ":", "\n", "        ", "light_diffuse", "=", "light_diffuse", ".", "to", "(", "temp_device", ")", "\n", "", "return", "light_diffuse", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.eval_driver.get_parser": [[14, 26], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "get_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--data-dir\"", ",", "default", "=", "\"data/evaluation\"", ",", "help", "=", "\"Path to evaluation set.\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--evaluation-mode\"", ",", "default", "=", "\"fixed\"", ",", "choices", "=", "[", "\"fixed\"", ",", "\"in-the-wild\"", "]", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--force\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"If set, overwrites existing predictions.\"", "\n", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.eval_driver.main": [[28, 43], ["os.listdir", "len", "range", "os.listdir", "print", "subprocess.run", "os.path.join", "str"], "function", ["None"], ["", "def", "main", "(", "data_dir", ",", "evaluation_mode", ",", "force", "=", "False", ")", ":", "\n", "    ", "base_cmd", "=", "[", "\"python\"", ",", "\"-m\"", ",", "\"eval.train_evaluation_model\"", "]", "\n", "if", "evaluation_mode", "==", "\"fixed\"", ":", "\n", "        ", "base_cmd", "+=", "[", "\"--fix-cameras\"", ",", "\"--camera-type\"", ",", "\"camera_optimized\"", "]", "\n", "", "elif", "evaluation_mode", "==", "\"in-the-wild\"", ":", "\n", "        ", "base_cmd", "+=", "[", "\"--camera-type\"", ",", "\"camera_pretrained\"", "]", "\n", "", "if", "force", ":", "\n", "        ", "base_cmd", "+=", "[", "\"--force\"", "]", "\n", "", "instance_ids", "=", "os", ".", "listdir", "(", "data_dir", ")", "\n", "for", "instance_id", "in", "instance_ids", ":", "\n", "        ", "num_images", "=", "len", "(", "os", ".", "listdir", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "instance_id", ",", "\"images\"", ")", ")", ")", "\n", "for", "i", "in", "range", "(", "num_images", ")", ":", "\n", "            ", "cmd", "=", "base_cmd", "+", "[", "\"--instance-id\"", ",", "instance_id", ",", "\"--camera-index\"", ",", "str", "(", "i", ")", "]", "\n", "print", "(", "\"Running:\"", ",", "\" \"", ".", "join", "(", "cmd", ")", ")", "\n", "subprocess", ".", "run", "(", "cmd", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.train_evaluation_model.get_parser": [[39, 68], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "get_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--data_dir\"", ",", "default", "=", "\"data/evaluation\"", ",", "help", "=", "\"Path to evaluation set.\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--instance-id\"", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--camera-index\"", ",", "type", "=", "int", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--fix-cameras\"", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--camera-type\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"camera_optimized\"", ",", "\n", "choices", "=", "[", "\"camera_optimized\"", ",", "\"camera_pretrained\"", "]", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--output-dir\"", ",", "type", "=", "str", ",", "default", "=", "\"output/eval\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--force\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"If set, overwrites existing predictions.\"", "\n", ")", "\n", "\n", "# Hyperparameters", "\n", "parser", ".", "add_argument", "(", "\"--num-iterations-camera\"", ",", "default", "=", "500", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--num-iterations-shape\"", ",", "default", "=", "500", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--num-iterations-texture\"", ",", "default", "=", "3000", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--num-iterations-radiance\"", ",", "default", "=", "500", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--num-layers-tex\"", ",", "default", "=", "12", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--L\"", ",", "default", "=", "10", ",", "type", "=", "int", ",", "help", "=", "\"Number of bases for positional encoding.\"", "\n", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.train_evaluation_model.render_target_view": [[70, 90], ["torch.cuda.empty_cache", "pytorch3d.renderer.RasterizationSettings", "torch.no_grad", "target_camera.to.to", "ners.renderer_textured", "numpy.clip", "ners.utils.image.antialias", "ners.renderer_textured.detach().cpu().numpy", "ners.renderer_textured.detach().cpu", "ners.renderer_textured.detach"], "function", ["home.repos.pwc.inspect_result.jasonyzhang_ners.utils.image.antialias", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach"], ["", "def", "render_target_view", "(", "ners", ",", "target_camera", ",", "image_size", ",", "use_antialiasing", "=", "True", ")", ":", "\n", "    ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "if", "use_antialiasing", ":", "\n", "        ", "image_size", "*=", "2", "\n", "", "raster_settings", "=", "RasterizationSettings", "(", "\n", "image_size", "=", "image_size", ",", "\n", "blur_radius", "=", "0.0", ",", "\n", "faces_per_pixel", "=", "1", ",", "\n", "bin_size", "=", "0", ",", "\n", "perspective_correct", "=", "False", ",", "\n", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "target_camera", "=", "target_camera", ".", "to", "(", "ners", ".", "device", ")", "\n", "rend", "=", "ners", ".", "renderer_textured", "(", "\n", "ners", ".", "meshes_current", ",", "cameras", "=", "target_camera", ",", "raster_settings", "=", "raster_settings", "\n", ")", "\n", "image", "=", "np", ".", "clip", "(", "rend", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", "0", ",", "...", ",", ":", "3", "]", ",", "0", ",", "1", ")", "\n", "if", "use_antialiasing", ":", "\n", "            ", "image", "=", "antialias", "(", "image", ")", "\n", "", "", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.train_evaluation_model.skip_index": [[92, 97], ["isinstance", "numpy.concatenate"], "function", ["None"], ["", "def", "skip_index", "(", "data", ",", "skip_index", ")", ":", "\n", "    ", "if", "isinstance", "(", "data", ",", "list", ")", ":", "\n", "        ", "return", "data", "[", ":", "skip_index", "]", "+", "data", "[", "skip_index", "+", "1", ":", "]", "\n", "", "else", ":", "\n", "        ", "return", "np", ".", "concatenate", "(", "(", "data", "[", ":", "skip_index", "]", ",", "data", "[", "skip_index", "+", "1", ":", "]", ")", ",", "axis", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.train_evaluation_model.load_cameras": [[99, 115], ["os.join", "open", "json.load", "all_cameras[].append", "all_cameras[].append", "all_cameras[].append"], "function", ["None"], ["", "", "def", "load_cameras", "(", "instance_dir", ",", "camera_type", ")", ":", "\n", "    ", "annotations_json", "=", "osp", ".", "join", "(", "instance_dir", ",", "\"annotations.json\"", ")", "\n", "with", "open", "(", "annotations_json", ")", "as", "f", ":", "\n", "        ", "annotations", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "all_cameras", "=", "{", "\n", "\"R\"", ":", "[", "]", ",", "\n", "\"T\"", ":", "[", "]", ",", "\n", "\"fov\"", ":", "[", "]", ",", "\n", "}", "\n", "for", "annotation", "in", "annotations", "[", "\"annotations\"", "]", ":", "\n", "        ", "camera", "=", "annotation", "[", "camera_type", "]", "\n", "all_cameras", "[", "\"R\"", "]", ".", "append", "(", "camera", "[", "\"R\"", "]", ")", "\n", "all_cameras", "[", "\"T\"", "]", ".", "append", "(", "camera", "[", "\"T\"", "]", ")", "\n", "all_cameras", "[", "\"fov\"", "]", ".", "append", "(", "camera", "[", "\"fov\"", "]", ")", "\n", "", "return", "all_cameras", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.train_evaluation_model.main": [[117, 192], ["os.join", "os.join", "os.makedirs", "os.makedirs", "os.join", "os.join", "ners.data.load_car_data", "ners.models.load_car_model", "train_evaluation_model.skip_index", "train_evaluation_model.skip_index", "train_evaluation_model.skip_index", "train_evaluation_model.skip_index", "train_evaluation_model.skip_index", "train_evaluation_model.load_cameras", "ners.pytorch3d.PerspectiveCameras", "ners.pytorch3d.PerspectiveCameras", "ners.Ners", "ners.pytorch3d.PerspectiveCameras.to", "ners.Ners.optimize_shape", "ners.Ners.optimize_texture", "ners.Ners.optimize_radiance", "train_evaluation_model.render_target_view", "matplotlib.imsave", "ners.Ners.save_parameters", "os.exists", "print", "ners.Ners.optimize_camera", "train_evaluation_model.skip_index", "train_evaluation_model.skip_index", "train_evaluation_model.skip_index", "ners.pytorch3d.PerspectiveCameras.R.tolist"], "function", ["home.repos.pwc.inspect_result.jasonyzhang_ners.ners.data.load_car_data", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.models.load_car_model", "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.train_evaluation_model.skip_index", "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.train_evaluation_model.skip_index", "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.train_evaluation_model.skip_index", "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.train_evaluation_model.skip_index", "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.train_evaluation_model.skip_index", "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.train_evaluation_model.load_cameras", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.optimize_shape", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.optimize_texture", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.optimize_radiance", "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.train_evaluation_model.render_target_view", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.save_parameters", "home.repos.pwc.inspect_result.jasonyzhang_ners.ners.ners.Ners.optimize_camera", "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.train_evaluation_model.skip_index", "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.train_evaluation_model.skip_index", "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.train_evaluation_model.skip_index"], ["", "def", "main", "(", "\n", "data_dir", ",", "\n", "instance_id", ",", "\n", "camera_index", ",", "\n", "camera_type", ",", "\n", "output_dir", ",", "\n", "fix_cameras", ",", "\n", "num_iterations_camera", ",", "\n", "num_iterations_shape", ",", "\n", "num_iterations_texture", ",", "\n", "num_iterations_radiance", ",", "\n", "num_layers_tex", ",", "\n", "L", ",", "\n", "force", "=", "False", ",", "\n", ")", ":", "\n", "    ", "instance_dir", "=", "osp", ".", "join", "(", "data_dir", ",", "instance_id", ")", "\n", "name", "=", "f\"{instance_id}_{camera_type}\"", "\n", "if", "fix_cameras", ":", "\n", "        ", "name", "+=", "\"_fix\"", "\n", "", "output_dir", "=", "osp", ".", "join", "(", "output_dir", ",", "name", ")", "\n", "os", ".", "makedirs", "(", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "weights_path", "=", "osp", ".", "join", "(", "output_dir", ",", "f\"weights_{camera_index:02d}.pth\"", ")", "\n", "render_path", "=", "osp", ".", "join", "(", "output_dir", ",", "f\"render_{camera_index:02d}.png\"", ")", "\n", "if", "osp", ".", "exists", "(", "weights_path", ")", "and", "not", "force", ":", "\n", "        ", "print", "(", "f\"Found weights at {weights_path}\"", ")", "\n", "return", "\n", "\n", "", "data", "=", "load_car_data", "(", "instance_dir", ",", "image_size", "=", "256", ")", "\n", "f_template", "=", "load_car_model", "(", ")", "\n", "images", "=", "skip_index", "(", "data", "[", "\"images\"", "]", ",", "camera_index", ")", "\n", "masks", "=", "skip_index", "(", "data", "[", "\"masks\"", "]", ",", "camera_index", ")", "\n", "masks_dt", "=", "skip_index", "(", "data", "[", "\"masks_dt\"", "]", ",", "camera_index", ")", "\n", "image_center", "=", "skip_index", "(", "data", "[", "\"image_centers\"", "]", ",", "camera_index", ")", "\n", "crop_scale", "=", "skip_index", "(", "data", "[", "\"crop_scales\"", "]", ",", "camera_index", ")", "\n", "\n", "all_cameras", "=", "load_cameras", "(", "instance_dir", ",", "camera_type", ")", "\n", "cameras_training", "=", "PerspectiveCameras", "(", "\n", "R", "=", "skip_index", "(", "all_cameras", "[", "\"R\"", "]", ",", "camera_index", ")", ",", "\n", "T", "=", "skip_index", "(", "all_cameras", "[", "\"T\"", "]", ",", "camera_index", ")", ",", "\n", "fov", "=", "skip_index", "(", "all_cameras", "[", "\"fov\"", "]", ",", "camera_index", ")", ",", "\n", "image_center", "=", "image_center", ",", "\n", "crop_scale", "=", "crop_scale", ",", "\n", ")", "\n", "cameras_target", "=", "PerspectiveCameras", "(", "\n", "R", "=", "[", "all_cameras", "[", "\"R\"", "]", "[", "camera_index", "]", "]", ",", "\n", "T", "=", "[", "all_cameras", "[", "\"T\"", "]", "[", "camera_index", "]", "]", ",", "\n", "fov", "=", "[", "all_cameras", "[", "\"fov\"", "]", "[", "camera_index", "]", "]", ",", "\n", "image_center", "=", "[", "data", "[", "\"image_centers\"", "]", "[", "camera_index", "]", "]", ",", "\n", "crop_scale", "=", "[", "data", "[", "\"crop_scales\"", "]", "[", "camera_index", "]", "]", ",", "\n", ")", "\n", "ners", "=", "Ners", "(", "\n", "images", "=", "images", ",", "\n", "masks", "=", "masks", ",", "\n", "masks_dt", "=", "masks_dt", ",", "\n", "initial_poses", "=", "cameras_training", ".", "R", ".", "tolist", "(", ")", ",", "\n", "crop_scale", "=", "crop_scale", ",", "\n", "image_center", "=", "image_center", ",", "\n", "symmetrize", "=", "True", ",", "\n", "num_layers_shape", "=", "4", ",", "\n", "num_layers_tex", "=", "num_layers_tex", ",", "\n", "num_layers_env", "=", "4", ",", "\n", "f_template", "=", "f_template", ",", "\n", "L", "=", "L", ",", "\n", ")", "\n", "ners", ".", "cameras_current", "=", "cameras_training", ".", "to", "(", "ners", ".", "device", ")", "\n", "if", "fix_cameras", ":", "\n", "        ", "ners", ".", "finetune_camera", "=", "False", "\n", "", "else", ":", "\n", "        ", "ners", ".", "optimize_camera", "(", "num_iterations_camera", ")", "\n", "", "ners", ".", "optimize_shape", "(", "num_iterations_shape", ")", "\n", "ners", ".", "optimize_texture", "(", "num_iterations_texture", ")", "\n", "ners", ".", "optimize_radiance", "(", "num_iterations_radiance", ")", "\n", "target_image", "=", "render_target_view", "(", "ners", ",", "cameras_target", ",", "image_size", "=", "256", ")", "\n", "plt", ".", "imsave", "(", "render_path", ",", "target_image", ")", "\n", "ners", ".", "save_parameters", "(", "weights_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.metrics.compute_perceptual": [[14, 29], ["LPIPS_NET", "LPIPS_NET.item", "image_gt.unsqueeze", "image_pred.unsqueeze", "torch.tensor().float", "torch.tensor().float", "torch.tensor", "torch.tensor", "image_gt.transpose", "image_pred.transpose"], "function", ["None"], ["def", "compute_perceptual", "(", "image_gt", ",", "image_pred", ")", ":", "\n", "    ", "\"\"\"\n    Compute the perceptual loss between two images.\n\n    Args:\n        image_gt (np.ndarray): The ground truth image (H, W, C).\n        image_pred (np.ndarray): The predicted image (H, W, C).\n\n    Returns:\n        float: Perceptual error.\n    \"\"\"", "\n", "image_gt", "=", "torch", ".", "tensor", "(", "image_gt", ".", "transpose", "(", "2", ",", "0", ",", "1", ")", ")", ".", "float", "(", ")", "*", "2", "-", "1", "\n", "image_pred", "=", "torch", ".", "tensor", "(", "image_pred", ".", "transpose", "(", "2", ",", "0", ",", "1", ")", ")", ".", "float", "(", ")", "*", "2", "-", "1", "\n", "dist", "=", "LPIPS_NET", "(", "image_gt", ".", "unsqueeze", "(", "0", ")", ",", "image_pred", ".", "unsqueeze", "(", "0", ")", ")", "\n", "return", "dist", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.metrics.compute_psnr": [[31, 46], ["skimage.metrics.peak_signal_noise_ratio"], "function", ["None"], ["", "def", "compute_psnr", "(", "image_gt", ",", "image_pred", ")", ":", "\n", "    ", "\"\"\"\n    Compute the PSNR between two images.\n\n    Args:\n        image_gt (np.ndarray): The ground truth image (H, W, C).\n        image_pred (np.ndarray): The predicted image (H, W, C).\n\n    Returns:\n        float: PSNR.\n    \"\"\"", "\n", "return", "skimage", ".", "metrics", ".", "peak_signal_noise_ratio", "(", "\n", "image_true", "=", "image_gt", ",", "\n", "image_test", "=", "image_pred", ",", "\n", "data_range", "=", "1", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.metrics.compute_ssim": [[49, 65], ["skimage.metrics.structural_similarity"], "function", ["None"], ["", "def", "compute_ssim", "(", "image_gt", ",", "image_pred", ")", ":", "\n", "    ", "\"\"\"\n    Compute the SSIM between two images.\n\n    Args:\n        image_gt (np.ndarray): The ground truth image (H, W, C).\n        image_pred (np.ndarray): The predicted image (H, W, C).\n\n    Returns:\n        float: SSIM.\n    \"\"\"", "\n", "return", "skimage", ".", "metrics", ".", "structural_similarity", "(", "\n", "im1", "=", "image_gt", ",", "\n", "im2", "=", "image_pred", ",", "\n", "data_range", "=", "1", ",", "\n", "channel_axis", "=", "2", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.metrics.compute_mse": [[68, 80], ["numpy.mean"], "function", ["None"], ["", "def", "compute_mse", "(", "image_gt", ",", "image_pred", ")", ":", "\n", "    ", "\"\"\"\n    Compute the MSE between two images.\n\n    Args:\n        image_gt (np.ndarray): The ground truth image (H, W, C).\n        image_pred (np.ndarray): The predicted image (H, W, C).\n\n    Returns:\n        float: MSE.\n    \"\"\"", "\n", "return", "np", ".", "mean", "(", "(", "image_gt", "-", "image_pred", ")", "**", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.metrics.compute_fid": [[82, 105], ["tempfile.TemporaryDirectory", "tempfile.TemporaryDirectory", "enumerate", "enumerate", "cleanfid.fid.compute_fid", "tempfile.TemporaryDirectory.cleanup", "tempfile.TemporaryDirectory.cleanup", "shutil.copy", "shutil.copy", "os.join", "os.join", "os.basename", "os.basename"], "function", ["home.repos.pwc.inspect_result.jasonyzhang_ners.eval.metrics.compute_fid"], ["", "def", "compute_fid", "(", "image_paths_gt", ",", "image_paths_pred", ")", ":", "\n", "    ", "\"\"\"\n    Compute the FID between two directories.\n\n    Args:\n        image_paths_gt (list): List of image paths corresponding to ground truth images.\n        image_paths_pred (list): List of image paths corresponding to predicted images.\n\n    Returns:\n        float: FID.\n    \"\"\"", "\n", "dir_gt", "=", "tempfile", ".", "TemporaryDirectory", "(", ")", "\n", "dir_pred", "=", "tempfile", ".", "TemporaryDirectory", "(", ")", "\n", "for", "i", ",", "image_path", "in", "enumerate", "(", "image_paths_gt", ")", ":", "\n", "        ", "filename", "=", "f\"{i:06d}_{osp.basename(image_path)}\"", "\n", "shutil", ".", "copy", "(", "image_path", ",", "osp", ".", "join", "(", "dir_gt", ".", "name", ",", "filename", ")", ")", "\n", "", "for", "i", ",", "image_path", "in", "enumerate", "(", "image_paths_pred", ")", ":", "\n", "        ", "filename", "=", "f\"{i:06d}_{osp.basename(image_path)}\"", "\n", "shutil", ".", "copy", "(", "image_path", ",", "osp", ".", "join", "(", "dir_pred", ".", "name", ",", "filename", ")", ")", "\n", "", "fid_score", "=", "fid", ".", "compute_fid", "(", "dir_gt", ".", "name", ",", "dir_pred", ".", "name", ",", "verbose", "=", "False", ")", "\n", "dir_gt", ".", "cleanup", "(", ")", "\n", "dir_pred", ".", "cleanup", "(", ")", "\n", "return", "fid_score", "\n", "", ""]], "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.eval.get_parser": [[44, 65], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "get_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--eval-dir\"", ",", "default", "=", "\"data/evaluation\"", ",", "help", "=", "\"Path to evaluation set.\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--gt-name\"", ",", "\n", "default", "=", "\"images_masked\"", ",", "\n", "help", "=", "\"Name of folder containing ground truth images.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--pred-name\"", ",", "\n", "default", "=", "\"ners_fixed\"", ",", "\n", "help", "=", "\"Name of folder containing predicted images.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--print-per-instance\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Print metrics per instance.\"", ",", "\n", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.eval.evaluate": [[67, 117], ["os.listdir", "os.listdir", "print", "eval.metrics.compute_fid", "print", "os.join", "os.join", "sorted", "sorted", "image_paths_gt.extend", "image_paths_pred.extend", "zip", "metrics.items", "glob.glob", "glob.glob", "len", "len", "metrics[].append", "metrics[].append", "metrics[].append", "metrics[].append", "numpy.mean", "all_metrics[].append", "print", "os.isdir", "os.isdir", "os.join", "os.join", "matplotlib.imread", "matplotlib.imread", "eval.metrics.compute_mse", "eval.metrics.compute_psnr", "eval.metrics.compute_ssim", "eval.metrics.compute_perceptual", "numpy.mean", "all_metrics.values", "metrics.values"], "function", ["home.repos.pwc.inspect_result.jasonyzhang_ners.eval.metrics.compute_fid", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.extend", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.extend", "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.metrics.compute_mse", "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.metrics.compute_psnr", "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.metrics.compute_ssim", "home.repos.pwc.inspect_result.jasonyzhang_ners.eval.metrics.compute_perceptual"], ["", "def", "evaluate", "(", "eval_dir", ",", "gt_name", ",", "pred_name", ",", "print_per_instance", "=", "False", ")", ":", "\n", "    ", "instance_ids", "=", "os", ".", "listdir", "(", "eval_dir", ")", "\n", "\n", "print", "(", "\n", "f\"{'Name':12s} {'MSE':>6s} {'PSNR':>6s} {'SSIM':>6s} {'LPIPS':>6s} {'FID':>6s}\"", "\n", ")", "\n", "\n", "all_metrics", "=", "{", "\"mse\"", ":", "[", "]", ",", "\"psnr\"", ":", "[", "]", ",", "\"ssim\"", ":", "[", "]", ",", "\"lpips\"", ":", "[", "]", "}", "\n", "image_paths_gt", "=", "[", "]", "\n", "image_paths_pred", "=", "[", "]", "\n", "for", "instance_id", "in", "instance_ids", ":", "\n", "        ", "gt_dir", "=", "osp", ".", "join", "(", "eval_dir", ",", "instance_id", ",", "gt_name", ")", "\n", "pred_dir", "=", "osp", ".", "join", "(", "eval_dir", ",", "instance_id", ",", "pred_name", ")", "\n", "if", "not", "osp", ".", "isdir", "(", "gt_dir", ")", "or", "not", "osp", ".", "isdir", "(", "pred_dir", ")", ":", "\n", "            ", "continue", "\n", "", "gt_images", "=", "sorted", "(", "glob", "(", "osp", ".", "join", "(", "gt_dir", ",", "\"*.png\"", ")", ")", ")", "\n", "pred_images", "=", "sorted", "(", "glob", "(", "osp", ".", "join", "(", "pred_dir", ",", "\"*.png\"", ")", ")", ")", "\n", "image_paths_gt", ".", "extend", "(", "gt_images", ")", "\n", "image_paths_pred", ".", "extend", "(", "pred_images", ")", "\n", "assert", "len", "(", "gt_images", ")", "==", "len", "(", "pred_images", ")", "\n", "metrics", "=", "{", "\n", "\"mse\"", ":", "[", "]", ",", "\n", "\"psnr\"", ":", "[", "]", ",", "\n", "\"ssim\"", ":", "[", "]", ",", "\n", "\"lpips\"", ":", "[", "]", ",", "\n", "}", "\n", "for", "gt_image_path", ",", "pred_image_path", "in", "zip", "(", "gt_images", ",", "pred_images", ")", ":", "\n", "            ", "gt_image", "=", "plt", ".", "imread", "(", "gt_image_path", ")", "[", "...", ",", ":", "3", "]", "\n", "pred_image", "=", "plt", ".", "imread", "(", "pred_image_path", ")", "[", "...", ",", ":", "3", "]", "\n", "\n", "metrics", "[", "\"mse\"", "]", ".", "append", "(", "compute_mse", "(", "gt_image", ",", "pred_image", ")", ")", "\n", "metrics", "[", "\"psnr\"", "]", ".", "append", "(", "compute_psnr", "(", "gt_image", ",", "pred_image", ")", ")", "\n", "metrics", "[", "\"ssim\"", "]", ".", "append", "(", "compute_ssim", "(", "gt_image", ",", "pred_image", ")", ")", "\n", "metrics", "[", "\"lpips\"", "]", ".", "append", "(", "compute_perceptual", "(", "gt_image", ",", "pred_image", ")", ")", "\n", "\n", "", "for", "metric", ",", "values", "in", "metrics", ".", "items", "(", ")", ":", "\n", "            ", "mean_metric", "=", "np", ".", "mean", "(", "values", ")", "\n", "metrics", "[", "metric", "]", "=", "mean_metric", "\n", "all_metrics", "[", "metric", "]", ".", "append", "(", "mean_metric", ")", "\n", "\n", "", "if", "print_per_instance", ":", "\n", "            ", "metrics_str", "=", "\" \"", ".", "join", "(", "[", "\"{0:>#6.3g}\"", ".", "format", "(", "v", ")", "for", "v", "in", "metrics", ".", "values", "(", ")", "]", ")", "\n", "print", "(", "f\"{instance_id:12s} {metrics_str}\"", ")", "\n", "\n", "", "", "fid", "=", "compute_fid", "(", "image_paths_gt", ",", "image_paths_pred", ")", "\n", "metrics_str", "=", "\" \"", ".", "join", "(", "\n", "[", "\"{0:>#6.3g}\"", ".", "format", "(", "np", ".", "mean", "(", "v", ")", ")", "for", "v", "in", "all_metrics", ".", "values", "(", ")", "]", "\n", "+", "[", "f\"{fid:>#6.3g}\"", "]", "\n", ")", "\n", "print", "(", "f\"{pred_name:12s} {metrics_str}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.scripts.pretrain_shape_template.get_parser": [[40, 72], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "get_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--object-mesh\"", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "default", "=", "\"models/meshes/car.obj\"", ",", "\n", "help", "=", "\"Path to object mesh.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--sphere-mesh\"", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "default", "=", "\"models/meshes/car_sphere.obj\"", ",", "\n", "help", "=", "\"Path to sphere mesh.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--visualize-path\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"\"", ",", "\n", "help", "=", "\"If an obj path is specified, will visualize the implicit template model.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--output-path\"", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "default", "=", "\"models/templates/car.pth\"", ",", "\n", "help", "=", "\"Path to output template weights.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--num-iterations\"", ",", "type", "=", "int", ",", "default", "=", "5000", ")", "\n", "parser", ".", "add_argument", "(", "\"--num-samples\"", ",", "type", "=", "int", ",", "default", "=", "15000", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.scripts.pretrain_shape_template.main": [[74, 137], ["print", "torch.device", "pytorch3d.io.load_obj", "pytorch3d.io.load_obj", "pytorch3d.io.load_obj", "pytorch3d.io.load_obj", "verts_obj.max", "pytorch3d.transforms.euler_angles_to_matrix", "pytorch3d.transforms.euler_angles_to_matrix", "ners.models.TemplateUV().to", "torch.optim.Adam", "ners.utils.create_sphere", "sphere_vs.to.to", "sphere_fs.to.to", "tqdm.auto.tqdm", "template_uv.cpu.cpu", "torch.save", "torch.as_tensor", "template_uv.cpu.parameters", "range", "torch.optim.Adam.zero_grad", "ners.utils.sample_consistent_points", "template_uv.cpu.", "sv.unsqueeze.unsqueeze", "pytorch3d.structures.Meshes", "pytorch3d.structures.Meshes", "torch.mean", "pytorch3d.loss.mesh_laplacian_smoothing", "pytorch3d.loss.mesh_normal_consistency", "loss.backward", "torch.optim.Adam.step", "template_uv.cpu.state_dict", "trimesh.Trimesh", "verts_sphere.norm", "ners.models.TemplateUV", "uvs.to", "ners.utils.random_rotation", "template_uv.cpu.", "sphere_fs.to.unsqueeze", "pytorch3d.loss.chamfer_distance", "open", "trimesh.exchange.export.export_mesh", "verts_obj.max", "verts_obj.min", "template_uv.cpu.", "targets.unsqueeze().to", "template_uv.cpu.detach().cpu().numpy", "sphere_fs.to.cpu().numpy", "targets.to", "targets.unsqueeze", "template_uv.cpu.detach().cpu", "sphere_fs.to.cpu", "sphere_vs.to.detach().cpu().numpy", "template_uv.cpu.detach", "sphere_vs.to.detach().cpu", "template_uv.cpu.", "sphere_vs.to.detach"], "function", ["home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.create_sphere", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.sampling.sample_consistent_points", "home.repos.pwc.inspect_result.jasonyzhang_ners.utils.geometry.random_rotation", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach", "home.repos.pwc.inspect_result.jasonyzhang_ners.pytorch3d.textures.TexturesImplicit.detach"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "print", "(", "args", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda:0\"", ")", "\n", "verts_obj", ",", "faces_obj", ",", "*", "_", "=", "pytorch3d", ".", "io", ".", "load_obj", "(", "args", ".", "object_mesh", ")", "\n", "verts_sphere", ",", "faces_sphere", ",", "*", "_", "=", "pytorch3d", ".", "io", ".", "load_obj", "(", "args", ".", "sphere_mesh", ")", "\n", "# Center and rescale", "\n", "verts_obj", "-=", "(", "verts_obj", ".", "max", "(", "dim", "=", "0", ")", ".", "values", "+", "verts_obj", ".", "min", "(", "dim", "=", "0", ")", ".", "values", ")", "/", "2", "\n", "verts_obj", "/=", "verts_obj", ".", "max", "(", ")", "\n", "verts_sphere", "=", "verts_sphere", "/", "(", "verts_sphere", ".", "norm", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "+", "1e-6", ")", "\n", "\n", "# Need to transform meshes such that identity camera corresponds to back.", "\n", "rot", "=", "pytorch3d", ".", "transforms", ".", "euler_angles_to_matrix", "(", "\n", "torch", ".", "as_tensor", "(", "[", "np", ".", "pi", "/", "2", ",", "np", ".", "pi", "/", "2", ",", "0", "]", ")", ",", "\"YZX\"", "\n", ")", "\n", "verts_sphere", "=", "verts_sphere", "@", "rot", "\n", "verts_obj", "=", "verts_obj", "@", "rot", "\n", "\n", "template_uv", "=", "TemplateUV", "(", ")", ".", "to", "(", "device", ")", "\n", "optim", "=", "torch", ".", "optim", ".", "Adam", "(", "template_uv", ".", "parameters", "(", ")", ",", "lr", "=", "1e-4", ")", "\n", "\n", "num_iterations", "=", "args", ".", "num_iterations", "\n", "num_samples", "=", "args", ".", "num_samples", "\n", "\n", "sphere_vs", ",", "sphere_fs", "=", "create_sphere", "(", "5", ")", "\n", "sphere_vs", "=", "sphere_vs", ".", "to", "(", "device", ")", "\n", "sphere_fs", "=", "sphere_fs", ".", "to", "(", "device", ")", "\n", "\n", "for", "_", "in", "tqdm", "(", "range", "(", "num_iterations", ")", ")", ":", "\n", "        ", "optim", ".", "zero_grad", "(", ")", "\n", "targets", ",", "uvs", "=", "sample_consistent_points", "(", "\n", "verts_obj", ",", "faces_obj", ".", "verts_idx", ",", "[", "verts_obj", ",", "verts_sphere", "]", ",", "num_samples", "\n", ")", "\n", "pred_vs", "=", "template_uv", "(", "uvs", ".", "to", "(", "device", ")", ",", "normalize", "=", "True", ")", "\n", "sv", "=", "sphere_vs", "@", "random_rotation", "(", "sphere_vs", ".", "device", ")", "\n", "sv", "=", "sv", ".", "unsqueeze", "(", "0", ")", "# (1, V, 3)", "\n", "meshes", "=", "pytorch3d", ".", "structures", ".", "Meshes", "(", "\n", "template_uv", "(", "sv", ",", "normalize", "=", "True", ")", ",", "sphere_fs", ".", "unsqueeze", "(", "0", ")", "\n", ")", "\n", "loss_reconstruction", "=", "torch", ".", "mean", "(", "(", "pred_vs", "-", "targets", ".", "to", "(", "device", ")", ")", "**", "2", ")", "\n", "loss_laplacian", "=", "mesh_laplacian_smoothing", "(", "meshes", ")", "\n", "loss_normal", "=", "mesh_normal_consistency", "(", "meshes", ")", "\n", "loss_chamfer", "=", "chamfer_distance", "(", "\n", "template_uv", "(", "sv", ")", ",", "targets", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "device", ")", "\n", ")", "[", "0", "]", "\n", "loss", "=", "(", "\n", "loss_chamfer", "\n", "+", "loss_reconstruction", "\n", "+", "0.005", "*", "loss_laplacian", "\n", "+", "0.005", "*", "loss_normal", "\n", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optim", ".", "step", "(", ")", "\n", "", "template_uv", "=", "template_uv", ".", "cpu", "(", ")", "\n", "torch", ".", "save", "(", "template_uv", ".", "state_dict", "(", ")", ",", "args", ".", "output_path", ")", "\n", "\n", "if", "args", ".", "visualize_path", ":", "\n", "        ", "tmesh", "=", "trimesh", ".", "Trimesh", "(", "\n", "vertices", "=", "template_uv", "(", "sphere_vs", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "\n", "faces", "=", "sphere_fs", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "\n", "vertex_colors", "=", "(", "sphere_vs", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "+", "1", ")", "/", "2", ",", "\n", ")", "\n", "with", "open", "(", "args", ".", "visualize_path", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "trimesh", ".", "exchange", ".", "export", ".", "export_mesh", "(", "tmesh", ",", "f", ",", "file_type", "=", "\"obj\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.scripts.mvmc_driver.get_parser": [[17, 30], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "get_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--mvmc_path\"", ",", "type", "=", "str", ",", "default", "=", "\"data/mvmc\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--index\"", ",", "default", "=", "0", ",", "type", "=", "int", ",", "help", "=", "\"Initial index to start at.\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--skip\"", ",", "default", "=", "1", ",", "type", "=", "int", ",", "help", "=", "\"Number of instances to skip at a time.\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--force\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Re-run even if output exists.\"", "\n", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.jasonyzhang_ners.scripts.mvmc_driver.main": [[32, 52], ["sorted", "tqdm.auto.tqdm", "os.listdir", "base_cmd.append", "print", "subprocess.call", "os.path.join"], "function", ["None"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "instance_ids", "=", "sorted", "(", "os", ".", "listdir", "(", "args", ".", "mvmc_path", ")", ")", "\n", "base_cmd", "=", "[", "\n", "\"python\"", ",", "\n", "\"main.py\"", ",", "\n", "\"--mvmc\"", ",", "\n", "\"--symmetrize\"", ",", "\n", "\"--export-mesh\"", ",", "\n", "\"--predict-illumination\"", ",", "\n", "]", "\n", "if", "args", ".", "force", ":", "\n", "        ", "base_cmd", ".", "append", "(", "\"--force\"", ")", "\n", "\n", "", "for", "instance_id", "in", "tqdm", "(", "instance_ids", "[", "args", ".", "index", ":", ":", "args", ".", "skip", "]", ")", ":", "\n", "        ", "cmd", "=", "base_cmd", "+", "[", "\n", "\"--instance-dir\"", ",", "\n", "os", ".", "path", ".", "join", "(", "args", ".", "mvmc_path", ",", "instance_id", ")", ",", "\n", "]", "\n", "print", "(", "\"Running:\"", ",", "\" \"", ".", "join", "(", "cmd", ")", ")", "\n", "subprocess", ".", "call", "(", "cmd", ")", "\n", "\n"]]}