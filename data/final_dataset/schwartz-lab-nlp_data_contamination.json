{"home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.finetune.SST5Classifier.SST5Classifier.__init__": [[10, 21], ["pytorch_lightning.LightningModule.__init__", "SST5Classifier.SST5Classifier.save_hyperparameters", "transformers.BertModel.from_pretrained", "torch.Linear", "torch.Linear", "torchmetrics.Accuracy", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.Softmax", "torch.Softmax"], "methods", ["home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.finetune.SST5DataModule.SST5DataModule.__init__"], ["    ", "def", "__init__", "(", "self", ",", "bert_path", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "save_hyperparameters", "(", ")", "\n", "self", ".", "bert", "=", "BertModel", ".", "from_pretrained", "(", "bert_path", ",", "return_dict", "=", "True", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "self", ".", "bert", ".", "config", ".", "hidden_size", ",", "5", ")", "# 5 for 5 labels", "\n", "\n", "self", ".", "accuracy", "=", "torchmetrics", ".", "Accuracy", "(", ")", "\n", "self", ".", "loss", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "1", ")", "\n", "\n", "self", ".", "last_test_results_dict", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.finetune.SST5Classifier.SST5Classifier.forward": [[22, 29], ["SST5Classifier.SST5Classifier.bert", "SST5Classifier.SST5Classifier.classifier", "SST5Classifier.SST5Classifier.loss", "label.flatten"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "label", "=", "None", ")", ":", "\n", "        ", "output", "=", "self", ".", "bert", "(", "input_ids", ",", "attention_mask", "=", "attention_mask", ")", "\n", "output", "=", "self", ".", "classifier", "(", "output", ".", "pooler_output", ")", "\n", "loss", "=", "0", "\n", "if", "label", "is", "not", "None", ":", "# label is None in inference mode", "\n", "            ", "loss", "=", "self", ".", "loss", "(", "output", ",", "label", ".", "flatten", "(", ")", ")", "\n", "", "return", "loss", ",", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.finetune.SST5Classifier.SST5Classifier.training_step": [[30, 34], ["SST5Classifier.SST5Classifier.", "SST5Classifier.SST5Classifier.log"], "methods", ["None"], ["", "def", "training_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "loss", ",", "output", "=", "self", "(", "batch", "[", "'input_ids'", "]", ",", "batch", "[", "'attention_mask'", "]", ",", "batch", "[", "'label'", "]", ")", "\n", "self", ".", "log", "(", "'train_loss'", ",", "loss", ",", "prog_bar", "=", "True", ",", "logger", "=", "True", ")", "\n", "return", "{", "'loss'", ":", "loss", ",", "'prediction'", ":", "output", ",", "'label'", ":", "batch", "[", "'label'", "]", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.finetune.SST5Classifier.SST5Classifier.test_step": [[35, 41], ["SST5Classifier.SST5Classifier.", "SST5Classifier.SST5Classifier.log", "SST5Classifier.SST5Classifier.accuracy", "SST5Classifier.SST5Classifier.log", "SST5Classifier.SST5Classifier.softmax", "label.flatten"], "methods", ["None"], ["", "def", "test_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "loss", ",", "output", "=", "self", "(", "batch", "[", "'input_ids'", "]", ",", "batch", "[", "'attention_mask'", "]", ",", "batch", "[", "'label'", "]", ")", "\n", "self", ".", "log", "(", "'test_loss'", ",", "loss", ",", "prog_bar", "=", "True", ",", "logger", "=", "True", ")", "\n", "self", ".", "accuracy", "(", "self", ".", "softmax", "(", "output", ")", ",", "label", ".", "flatten", "(", ")", ")", "\n", "self", ".", "log", "(", "'test_acc_step'", ",", "self", ".", "accuracy", ")", "\n", "return", "{", "'loss'", ":", "loss", ",", "'prediction'", ":", "output", ",", "'review'", ":", "batch", "[", "'review'", "]", ",", "'label'", ":", "batch", "[", "'label'", "]", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.finetune.SST5Classifier.SST5Classifier.test_epoch_end": [[42, 68], ["range", "label_count_dict.keys", "print", "len", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "[].flatten", "range", "print", "SST5Classifier.SST5Classifier.softmax", "torch.max", "torch.max", "torch.max", "torch.max", "len", "test_dict[].append", "test_dict[].append", "test_dict[].append", "test_dict[].append", "test_dict[].append", "sum", "sum", "SST5Classifier.SST5Classifier.softmax", "true_labels[].item", "preds[].item", "confidence_score[].item", "int", "correct_count_dict.values", "label_count_dict.values", "true_labels[].item", "true_labels[].item"], "methods", ["None"], ["", "def", "test_epoch_end", "(", "self", ",", "outputs", ")", ":", "\n", "        ", "\"\"\" This function saves the test's predictions in a dictionary. \"\"\"", "\n", "correct_count_dict", "=", "{", "0", ":", "0", ",", "1", ":", "0", ",", "2", ":", "0", ",", "3", ":", "0", ",", "4", ":", "0", "}", "\n", "label_count_dict", "=", "{", "0", ":", "0", ",", "1", ":", "0", ",", "2", ":", "0", ",", "3", ":", "0", ",", "4", ":", "0", "}", "\n", "test_dict", "=", "{", "'review'", ":", "[", "]", ",", "'label'", ":", "[", "]", ",", "'pred'", ":", "[", "]", ",", "'confidence_score'", ":", "[", "]", ",", "'acc'", ":", "[", "]", "}", "\n", "for", "batch", "in", "range", "(", "len", "(", "outputs", ")", ")", ":", "\n", "            ", "reviews", "=", "outputs", "[", "batch", "]", "[", "'review'", "]", "\n", "preds", "=", "torch", ".", "argmax", "(", "self", ".", "softmax", "(", "outputs", "[", "batch", "]", "[", "'prediction'", "]", ")", ",", "dim", "=", "1", ")", "\n", "confidence_score", "=", "torch", ".", "max", "(", "self", ".", "softmax", "(", "outputs", "[", "batch", "]", "[", "'prediction'", "]", ")", ",", "dim", "=", "1", ")", ".", "values", "\n", "true_labels", "=", "outputs", "[", "batch", "]", "[", "'label'", "]", ".", "flatten", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "true_labels", ")", ")", ":", "\n", "                ", "test_dict", "[", "'review'", "]", ".", "append", "(", "reviews", "[", "i", "]", ")", "\n", "test_dict", "[", "'label'", "]", ".", "append", "(", "true_labels", "[", "i", "]", ".", "item", "(", ")", ")", "\n", "test_dict", "[", "'pred'", "]", ".", "append", "(", "preds", "[", "i", "]", ".", "item", "(", ")", ")", "\n", "test_dict", "[", "'confidence_score'", "]", ".", "append", "(", "confidence_score", "[", "i", "]", ".", "item", "(", ")", ")", "\n", "test_dict", "[", "'acc'", "]", ".", "append", "(", "int", "(", "preds", "[", "i", "]", "==", "true_labels", "[", "i", "]", ")", ")", "\n", "\n", "label_count_dict", "[", "true_labels", "[", "i", "]", ".", "item", "(", ")", "]", "+=", "1", "\n", "if", "preds", "[", "i", "]", "==", "true_labels", "[", "i", "]", ":", "\n", "                    ", "correct_count_dict", "[", "true_labels", "[", "i", "]", ".", "item", "(", ")", "]", "+=", "1", "\n", "\n", "", "", "", "for", "key", "in", "label_count_dict", ".", "keys", "(", ")", ":", "\n", "            ", "print", "(", "f\"acc for {key}: \"", ",", "correct_count_dict", "[", "key", "]", "/", "label_count_dict", "[", "key", "]", ")", "\n", "", "print", "(", "f\"total acc: \"", ",", "sum", "(", "correct_count_dict", ".", "values", "(", ")", ")", "/", "sum", "(", "label_count_dict", ".", "values", "(", ")", ")", ")", "\n", "self", ".", "last_test_results_dict", "=", "test_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.finetune.SST5Classifier.SST5Classifier.configure_optimizers": [[69, 72], ["transformers.AdamW", "SST5Classifier.SST5Classifier.parameters"], "methods", ["None"], ["", "def", "configure_optimizers", "(", "self", ")", ":", "\n", "        ", "optimizer", "=", "AdamW", "(", "self", ".", "parameters", "(", ")", ",", "lr", "=", "2e-5", ")", "\n", "return", "[", "optimizer", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.finetune.results_parser.get_results": [[6, 35], ["open", "open", "print", "print", "print", "print", "print", "print", "re.findall", "train_loss.append", "test_loss.append", "test_acc.append", "round", "round", "round", "round", "re.sub", "float", "float", "float", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.std"], "function", ["None"], ["def", "get_results", "(", "type", ",", "cmpr_file_permission", ")", ":", "\n", "    ", "train_loss", "=", "[", "]", "\n", "test_loss", "=", "[", "]", "\n", "test_acc", "=", "[", "]", "\n", "\n", "if", "type", "==", "\"seen\"", ":", "\n", "        ", "file_name", "=", "args", ".", "seen", "\n", "", "elif", "type", "==", "\"unseen\"", ":", "\n", "        ", "file_name", "=", "args", ".", "unseen", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"non correct type\"", ")", "\n", "\n", "", "pattern", "=", "'[-+]?\\d+\\.\\d*,|[-+]?\\d+\\.\\d*e-?\\d+,'", "\n", "with", "open", "(", "file_name", ",", "\"r\"", ")", "as", "a_file", ":", "\n", "        ", "for", "line", "in", "a_file", ":", "\n", "            ", "tmp", "=", "re", ".", "findall", "(", "pattern", ",", "line", ")", "\n", "tmp", "=", "[", "re", ".", "sub", "(", "r','", ",", "''", ",", "match", ")", "for", "match", "in", "tmp", "]", "\n", "train_loss", ".", "append", "(", "float", "(", "tmp", "[", "0", "]", ")", ")", "\n", "test_loss", ".", "append", "(", "float", "(", "tmp", "[", "1", "]", ")", ")", "\n", "test_acc", ".", "append", "(", "float", "(", "tmp", "[", "2", "]", ")", ")", "\n", "\n", "", "", "with", "open", "(", "cmp_results_file_name", ",", "cmpr_file_permission", ")", "as", "f", ":", "\n", "        ", "print", "(", "f\"{type}_train_loss_avg: \"", ",", "round", "(", "np", ".", "mean", "(", "train_loss", ")", ",", "4", ")", ",", "file", "=", "f", ")", "\n", "print", "(", "f\"{type}_test_loss_avg: \"", ",", "round", "(", "np", ".", "mean", "(", "test_loss", ")", ",", "4", ")", ",", "file", "=", "f", ")", "\n", "print", "(", "f\"{type}_test_acc_avg: \"", ",", "round", "(", "np", ".", "mean", "(", "test_acc", ")", ",", "4", ")", ",", "file", "=", "f", ")", "\n", "print", "(", "f\"{type}_test_acc_std: \"", ",", "round", "(", "np", ".", "std", "(", "test_acc", ")", ",", "4", ")", ",", "file", "=", "f", ")", "\n", "print", "(", "\"\"", ",", "file", "=", "f", ")", "\n", "\n", "", "return", "test_acc", "\n", "\n"]], "home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.finetune.results_parser.get_diff_results": [[36, 46], ["range", "len", "diff.append", "open", "print", "print", "print", "print", "print", "round", "round", "round", "numpy.mean", "numpy.std"], "function", ["None"], ["", "def", "get_diff_results", "(", "seen_acc_lst", ",", "unseen_acc_lst", ")", ":", "\n", "    ", "diff", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "seen_acc_lst", ")", ")", ":", "\n", "        ", "diff", ".", "append", "(", "round", "(", "seen_acc_lst", "[", "i", "]", "-", "unseen_acc_lst", "[", "i", "]", ",", "4", ")", ")", "\n", "", "with", "open", "(", "cmp_results_file_name", ",", "'a'", ")", "as", "f", ":", "\n", "        ", "print", "(", "\"seen:   \"", ",", "seen_acc_lst", ",", "file", "=", "f", ")", "\n", "print", "(", "\"unseen: \"", ",", "unseen_acc_lst", ",", "file", "=", "f", ")", "\n", "print", "(", "\"diff: \"", ",", "diff", ",", "file", "=", "f", ")", "\n", "print", "(", "\"diff_mean (expl): \"", ",", "round", "(", "np", ".", "mean", "(", "diff", ")", ",", "4", ")", ",", "file", "=", "f", ")", "\n", "print", "(", "\"diff_std: \"", ",", "round", "(", "np", ".", "std", "(", "diff", ")", ",", "4", ")", ",", "file", "=", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.finetune.SST5Dataset.SST5Dataset.__init__": [[11, 15], ["None"], "methods", ["None"], ["  ", "def", "__init__", "(", "self", ",", "data", ":", "pd", ".", "DataFrame", ",", "tokenizer", ":", "BertTokenizer", ",", "max_token_len", ":", "int", "=", "128", ")", ":", "\n", "    ", "self", ".", "data", "=", "data", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "max_token_len", "=", "max_token_len", "\n", "\n"]], "home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.finetune.SST5Dataset.SST5Dataset.__len__": [[16, 18], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "    ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.finetune.SST5Dataset.SST5Dataset.__getitem__": [[19, 40], ["SST5Dataset.SST5Dataset.tokenizer.encode_plus", "dict", "encoding[].flatten", "encoding[].flatten", "torch.LongTensor"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", ")", ":", "\n", "    ", "data_row", "=", "self", ".", "data", ".", "iloc", "[", "index", "]", "\n", "review", "=", "data_row", ".", "review", "\n", "label", "=", "data_row", "[", "LABEL_COLUMNS", "]", "\n", "\n", "encoding", "=", "self", ".", "tokenizer", ".", "encode_plus", "(", "\n", "review", ",", "\n", "add_special_tokens", "=", "True", ",", "\n", "max_length", "=", "self", ".", "max_token_len", ",", "\n", "return_token_type_ids", "=", "False", ",", "\n", "padding", "=", "\"max_length\"", ",", "\n", "truncation", "=", "True", ",", "\n", "return_attention_mask", "=", "True", ",", "\n", "return_tensors", "=", "\"pt\"", "# pt = pytorch tensor", "\n", ")", "\n", "\n", "return", "dict", "(", "\n", "review", "=", "review", ",", "\n", "input_ids", "=", "encoding", "[", "'input_ids'", "]", ".", "flatten", "(", ")", ",", "\n", "attention_mask", "=", "encoding", "[", "'attention_mask'", "]", ".", "flatten", "(", ")", ",", "\n", "label", "=", "torch", ".", "LongTensor", "(", "label", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.finetune.SST5DataModule.SST5DataModule.__init__": [[9, 16], ["pytorch_lightning.LightningDataModule.__init__"], "methods", ["home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.finetune.SST5DataModule.SST5DataModule.__init__"], ["    ", "def", "__init__", "(", "self", ",", "train_df", ",", "test_df", ",", "tokenizer", ",", "batch_size", ",", "max_token_len", "=", "128", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "train_df", "=", "train_df", "\n", "self", ".", "test_df", "=", "test_df", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "max_token_len", "=", "max_token_len", "\n", "\n"]], "home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.finetune.SST5DataModule.SST5DataModule.setup": [[17, 25], ["SST5Dataset.SST5Dataset.SST5Dataset", "SST5Dataset.SST5Dataset.SST5Dataset"], "methods", ["None"], ["", "def", "setup", "(", "self", ")", ":", "\n", "        ", "self", ".", "train_dataset", "=", "SST5Dataset", "(", "self", ".", "train_df", ",", "\n", "self", ".", "tokenizer", ",", "\n", "self", ".", "max_token_len", ")", "\n", "\n", "self", ".", "test_dataset", "=", "SST5Dataset", "(", "self", ".", "test_df", ",", "\n", "self", ".", "tokenizer", ",", "\n", "self", ".", "max_token_len", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.finetune.SST5DataModule.SST5DataModule.train_dataloader": [[26, 31], ["torch.utils.data.DataLoader"], "methods", ["None"], ["", "def", "train_dataloader", "(", "self", ")", ":", "\n", "        ", "return", "DataLoader", "(", "self", ".", "train_dataset", ",", "\n", "batch_size", "=", "self", ".", "batch_size", ",", "\n", "shuffle", "=", "True", ",", "\n", "num_workers", "=", "72", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.finetune.SST5DataModule.SST5DataModule.test_dataloader": [[32, 36], ["torch.utils.data.DataLoader"], "methods", ["None"], ["", "def", "test_dataloader", "(", "self", ")", ":", "\n", "        ", "return", "DataLoader", "(", "self", ".", "test_dataset", ",", "\n", "batch_size", "=", "self", ".", "batch_size", ",", "\n", "num_workers", "=", "72", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.finetune.SST5_finetune.fine_tune": [[22, 57], ["pandas.read_csv", "pandas.read_csv", "pandas.read_csv", "transformers.BertTokenizer.from_pretrained", "SST5DataModule.SST5DataModule", "SST5DataModule.SST5DataModule.setup", "SST5Classifier.SST5Classifier", "pytorch_lightning.callbacks.ModelCheckpoint", "pytorch_lightning.Trainer", "pl.Trainer.fit", "pl.Trainer.test", "pandas.DataFrame", "pd.DataFrame.to_csv", "SST5Dataset", "torch.utils.data.DataLoader", "pl.Trainer.test", "pandas.DataFrame", "pd.DataFrame.to_csv", "open", "print", "open", "print"], "function", ["home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.finetune.SST5DataModule.SST5DataModule.setup"], ["def", "fine_tune", "(", ")", ":", "\n", "    ", "train_df", "=", "pd", ".", "read_csv", "(", "f\"datasets/SST5/sst5_train_set.csv\"", ")", "\n", "test_df", "=", "pd", ".", "read_csv", "(", "f\"datasets/SST5/sst5_test_set_1.csv\"", ")", "\n", "test2_df", "=", "pd", ".", "read_csv", "(", "f\"datasets/SST5/sst5_test_set_2.csv\"", ")", "\n", "\n", "# create data module (train and test)", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "tokenizer", ")", "\n", "data_module", "=", "SST5DataModule", "(", "train_df", ",", "test_df", ",", "tokenizer", ",", "batch_size", "=", "BATCH_SIZE", ",", "max_token_len", "=", "128", ")", "\n", "data_module", ".", "setup", "(", ")", "\n", "\n", "# create model", "\n", "model", "=", "SST5Classifier", "(", "args", ".", "bert_model", ")", "\n", "checkpoint_callback", "=", "ModelCheckpoint", "(", "save_last", "=", "True", ")", "\n", "\n", "# train + test (seen test set)", "\n", "trainer", "=", "pl", ".", "Trainer", "(", "checkpoint_callback", "=", "checkpoint_callback", ",", "gpus", "=", "1", ",", "max_epochs", "=", "N_EPOCHS", ")", "\n", "trainer", ".", "fit", "(", "model", ",", "data_module", ")", "\n", "trainer", ".", "test", "(", "model", ",", "ckpt_path", "=", "None", ")", "\n", "\n", "results_dict", "=", "model", ".", "last_test_results_dict", "\n", "results_df", "=", "pd", ".", "DataFrame", "(", "data", "=", "results_dict", ")", "\n", "results_df", ".", "to_csv", "(", "f\"{args.results_path}{args.model_id}_{args.seed}_seen.csv\"", ",", "index", "=", "False", ")", "\n", "with", "open", "(", "f\"{args.results_path}{args.model_id}_metrics_seen.txt\"", ",", "'a'", ")", "as", "f", ":", "\n", "        ", "print", "(", "trainer", ".", "callback_metrics", ",", "file", "=", "f", ")", "\n", "\n", "# test (unseen test set)", "\n", "", "test2_dataset", "=", "SST5Dataset", "(", "test2_df", ",", "tokenizer", ",", "max_token_len", "=", "128", ")", "\n", "test2", "=", "DataLoader", "(", "test2_dataset", ",", "batch_size", "=", "BATCH_SIZE", ",", "num_workers", "=", "72", ")", "\n", "trainer", ".", "test", "(", "test_dataloaders", "=", "test2", ",", "ckpt_path", "=", "None", ")", "\n", "\n", "results_dict", "=", "model", ".", "last_test_results_dict", "\n", "results_df", "=", "pd", ".", "DataFrame", "(", "data", "=", "results_dict", ")", "\n", "results_df", ".", "to_csv", "(", "f\"{args.results_path}{args.model_id}_{args.seed}_unseen.csv\"", ",", "index", "=", "False", ")", "\n", "with", "open", "(", "f\"{args.results_path}{args.model_id}_metrics_unseen.txt\"", ",", "'a'", ")", "as", "f", ":", "\n", "        ", "print", "(", "trainer", ".", "callback_metrics", ",", "file", "=", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.prepare_data.prepare_data_contamination_middle.main": [[5, 36], ["int", "random.seed", "random.shuffle", "random.shuffle", "random.shuffle", "open", "list", "open", "f.read().splitlines", "open", "f.write", "next", "map", "range", "f.read", "s.strip"], "function", ["None"], ["def", "main", "(", ")", ":", "\n", "    ", "contamination_copies", "=", "100", "\n", "wiki_size", "=", "int", "(", "1e6", ")", "\n", "random", ".", "seed", "(", "42", ")", "\n", "\n", "# get wiki data", "\n", "with", "open", "(", "arge", ".", "wiki_path", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "wiki_text", "=", "[", "next", "(", "f", ")", "for", "_", "in", "range", "(", "wiki_size", ")", "]", "\n", "wiki_text", "=", "list", "(", "map", "(", "lambda", "s", ":", "s", ".", "strip", "(", ")", ",", "wiki_text", ")", ")", "# remove '\\n' from end of instance", "\n", "\n", "# get contaminated data", "\n", "", "with", "open", "(", "args", ".", "contamination_path", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "contaminted_text", "=", "f", ".", "read", "(", ")", ".", "splitlines", "(", ")", "\n", "contaminted_text", "=", "contaminted_text", "*", "contamination_copies", "\n", "\n", "", "first_wiki_size", "=", "400000", "\n", "middle_wiki_size", "=", "200000", "\n", "# last_wiki_size = 400000 # this variable is not truly required", "\n", "\n", "first_text", "=", "wiki_text", "[", ":", "first_wiki_size", "]", "\n", "random", ".", "shuffle", "(", "first_text", ")", "\n", "middle_text", "=", "wiki_text", "[", "first_wiki_size", ":", "first_wiki_size", "+", "middle_wiki_size", "]", "+", "contaminted_text", "\n", "random", ".", "shuffle", "(", "middle_text", ")", "\n", "last_text", "=", "wiki_text", "[", "first_wiki_size", "+", "middle_wiki_size", ":", "]", "\n", "random", ".", "shuffle", "(", "last_text", ")", "\n", "\n", "# combine", "\n", "text", "=", "first_text", "+", "middle_text", "+", "last_text", "\n", "\n", "with", "open", "(", "f'{args.path}wiki_{args.wiki_size}_contamination_copies_{sst_size}_middle.txt'", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "'\\n'", ".", "join", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.prepare_data.prepare_data_contamination_first.main": [[5, 36], ["int", "random.seed", "random.shuffle", "random.shuffle", "random.shuffle", "open", "list", "open", "f.read().splitlines", "open", "f.write", "next", "map", "range", "f.read", "s.strip"], "function", ["None"], ["def", "main", "(", ")", ":", "\n", "    ", "contamination_copies", "=", "100", "\n", "wiki_size", "=", "int", "(", "1e6", ")", "\n", "random", ".", "seed", "(", "42", ")", "\n", "\n", "# get wiki data", "\n", "with", "open", "(", "arge", ".", "wiki_path", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "wiki_text", "=", "[", "next", "(", "f", ")", "for", "_", "in", "range", "(", "wiki_size", ")", "]", "\n", "wiki_text", "=", "list", "(", "map", "(", "lambda", "s", ":", "s", ".", "strip", "(", ")", ",", "wiki_text", ")", ")", "# remove '\\n' from end of instance", "\n", "\n", "# get contaminated data", "\n", "", "with", "open", "(", "args", ".", "contamination_path", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "contaminted_text", "=", "f", ".", "read", "(", ")", ".", "splitlines", "(", ")", "\n", "contaminted_text", "=", "contaminted_text", "*", "contamination_copies", "\n", "\n", "", "first_wiki_size", "=", "200000", "\n", "middle_wiki_size", "=", "400000", "\n", "# last_wiki_size = 400000 # this variable is not truly required", "\n", "\n", "first_text", "=", "wiki_text", "[", ":", "first_wiki_size", "]", "+", "contaminted_text", "\n", "random", ".", "shuffle", "(", "first_text", ")", "\n", "middle_text", "=", "wiki_text", "[", "first_wiki_size", ":", "first_wiki_size", "+", "middle_wiki_size", "]", "\n", "random", ".", "shuffle", "(", "middle_text", ")", "\n", "last_text", "=", "wiki_text", "[", "first_wiki_size", "+", "middle_wiki_size", ":", "]", "\n", "random", ".", "shuffle", "(", "last_text", ")", "\n", "\n", "# combine", "\n", "text", "=", "first_text", "+", "middle_text", "+", "last_text", "\n", "\n", "with", "open", "(", "f'{args.path}wiki_{args.wiki_size}_contamination_copies_{sst_size}_first.txt'", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "'\\n'", ".", "join", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.prepare_data.prepare_data.main": [[4, 25], ["int", "int", "random.seed", "random.shuffle", "float", "open", "list", "open", "f.read().splitlines", "open", "f.write", "next", "map", "range", "f.read", "s.strip"], "function", ["None"], ["def", "main", "(", ")", ":", "\n", "    ", "contamination_copies", "=", "int", "(", "args", ".", "contamination_copies", ")", "\n", "wiki_size", "=", "int", "(", "float", "(", "args", ".", "wiki_size", ")", ")", "\n", "random", ".", "seed", "(", "42", ")", "\n", "\n", "# get wiki data", "\n", "with", "open", "(", "arge", ".", "wiki_path", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "wiki_text", "=", "[", "next", "(", "f", ")", "for", "_", "in", "range", "(", "wiki_size", ")", "]", "\n", "wiki_text", "=", "list", "(", "map", "(", "lambda", "s", ":", "s", ".", "strip", "(", ")", ",", "wiki_text", ")", ")", "# remove '\\n' from end of instance", "\n", "\n", "# get contaminated data", "\n", "", "with", "open", "(", "args", ".", "contamination_path", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "contaminted_text", "=", "f", ".", "read", "(", ")", ".", "splitlines", "(", ")", "\n", "contaminted_text", "=", "contaminted_text", "*", "contamination_copies", "\n", "\n", "# combine", "\n", "", "text", "=", "wiki_text", "+", "contaminted_text", "\n", "random", ".", "shuffle", "(", "text", ")", "\n", "\n", "with", "open", "(", "f'{args.path}wiki_{args.wiki_size}_contamination_copies_{contamination_copies}.txt'", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "'\\n'", ".", "join", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.prepare_data.prepare_data_contamination_last.main": [[5, 36], ["int", "random.seed", "random.shuffle", "random.shuffle", "random.shuffle", "open", "list", "open", "f.read().splitlines", "open", "f.write", "next", "map", "range", "f.read", "s.strip"], "function", ["None"], ["def", "main", "(", ")", ":", "\n", "    ", "contamination_copies", "=", "100", "\n", "wiki_size", "=", "int", "(", "1e6", ")", "\n", "random", ".", "seed", "(", "42", ")", "\n", "\n", "# get wiki data", "\n", "with", "open", "(", "arge", ".", "wiki_path", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "wiki_text", "=", "[", "next", "(", "f", ")", "for", "_", "in", "range", "(", "wiki_size", ")", "]", "\n", "wiki_text", "=", "list", "(", "map", "(", "lambda", "s", ":", "s", ".", "strip", "(", ")", ",", "wiki_text", ")", ")", "# remove '\\n' from end of instance", "\n", "\n", "# get contaminated data", "\n", "", "with", "open", "(", "args", ".", "contamination_path", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "contaminted_text", "=", "f", ".", "read", "(", ")", ".", "splitlines", "(", ")", "\n", "contaminted_text", "=", "contaminted_text", "*", "contamination_copies", "\n", "\n", "", "first_wiki_size", "=", "400000", "\n", "middle_wiki_size", "=", "400000", "\n", "# last_wiki_size = 200000 # this variable is not truly required", "\n", "\n", "first_text", "=", "wiki_text", "[", ":", "first_wiki_size", "]", "\n", "random", ".", "shuffle", "(", "first_text", ")", "\n", "middle_text", "=", "wiki_text", "[", "first_wiki_size", ":", "first_wiki_size", "+", "middle_wiki_size", "]", "\n", "random", ".", "shuffle", "(", "middle_text", ")", "\n", "last_text", "=", "wiki_text", "[", "first_wiki_size", "+", "middle_wiki_size", ":", "]", "+", "contaminted_text", "\n", "random", ".", "shuffle", "(", "last_text", ")", "\n", "\n", "# combine", "\n", "text", "=", "first_text", "+", "middle_text", "+", "last_text", "\n", "\n", "with", "open", "(", "f'{args.path}wiki_{args.wiki_size}_contamination_copies_{sst_size}_last.txt'", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "'\\n'", ".", "join", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.pretrain.run_mlm.DataTrainingArguments.__post_init__": [[150, 160], ["ValueError", "run_mlm.DataTrainingArguments.train_file.split", "run_mlm.DataTrainingArguments.validation_file.split"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "dataset_name", "is", "None", "and", "self", ".", "train_file", "is", "None", "and", "self", ".", "validation_file", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Need either a dataset name or a training/validation file.\"", ")", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "train_file", "is", "not", "None", ":", "\n", "                ", "extension", "=", "self", ".", "train_file", ".", "split", "(", "\".\"", ")", "[", "-", "1", "]", "\n", "assert", "extension", "in", "[", "\"csv\"", ",", "\"json\"", ",", "\"txt\"", "]", ",", "\"`train_file` should be a csv, a json or a txt file.\"", "\n", "", "if", "self", ".", "validation_file", "is", "not", "None", ":", "\n", "                ", "extension", "=", "self", ".", "validation_file", ".", "split", "(", "\".\"", ")", "[", "-", "1", "]", "\n", "assert", "extension", "in", "[", "\"csv\"", ",", "\"json\"", ",", "\"txt\"", "]", ",", "\"`validation_file` should be a csv, a json or a txt file.\"", "\n", "\n"]], "home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.pretrain.run_mlm.main": [[162, 434], ["transformers.HfArgumentParser", "logging.basicConfig", "logger.setLevel", "logger.warning", "transformers.trainer_utils.is_main_process", "logger.info", "transformers.set_seed", "AutoModelForMaskedLM.from_config.resize_token_embeddings", "transformers.DataCollatorForLanguageModeling", "transformers.Trainer", "sys.argv[].endswith", "transformers.HfArgumentParser.parse_json_file", "transformers.HfArgumentParser.parse_args_into_dataclasses", "os.path.isdir", "transformers.trainer_utils.get_last_checkpoint", "transformers.utils.logging.set_verbosity_info", "transformers.utils.logging.enable_default_handler", "transformers.utils.logging.enable_explicit_format", "datasets.load_dataset", "datasets.load_dataset", "transformers.AutoConfig.from_pretrained", "transformers.BertTokenizer.from_pretrained", "transformers.AutoModelForMaskedLM.from_pretrained", "logger.info", "transformers.AutoModelForMaskedLM.from_config", "len", "min", "datasets.load_dataset.map", "datasets.load_dataset.map", "tokenized_datasets.map.map", "transformers.Trainer.train", "transformers.Trainer.save_model", "transformers.Trainer.log_metrics", "transformers.Trainer.save_metrics", "transformers.Trainer.save_state", "logger.info", "transformers.Trainer.evaluate", "math.exp", "transformers.Trainer.log_metrics", "transformers.Trainer.save_metrics", "len", "ValueError", "transformers.trainer_utils.is_main_process", "datasets.load_dataset.keys", "datasets.load_dataset", "datasets.load_dataset", "data_args.train_file.split", "transformers.AutoConfig.from_pretrained", "logger.warning", "transformers.AutoTokenizer.from_pretrained", "ValueError", "logger.warn", "logger.warn", "AutoTokenizer.from_pretrained.", "AutoTokenizer.from_pretrained.", "len", "os.path.abspath", "len", "logger.info", "logging.StreamHandler", "bool", "sum", "os.path.isdir", "os.listdir", "bool", "examples.keys", "concatenated_examples.items", "range", "len", "line.isspace", "list", "examples.keys"], "function", ["None"], ["", "", "", "", "def", "main", "(", ")", ":", "\n", "# See all possible arguments in src/transformers/training_args.py", "\n", "# or by passing the --help flag to this script.", "\n", "# We now keep distinct sets of args, for a cleaner separation of concerns.", "\n", "\n", "    ", "parser", "=", "HfArgumentParser", "(", "(", "ModelArguments", ",", "DataTrainingArguments", ",", "TrainingArguments", ")", ")", "\n", "if", "len", "(", "sys", ".", "argv", ")", "==", "2", "and", "sys", ".", "argv", "[", "1", "]", ".", "endswith", "(", "\".json\"", ")", ":", "\n", "# If we pass only one argument to the script and it's the path to a json file,", "\n", "# let's parse it to get our arguments.", "\n", "        ", "model_args", ",", "data_args", ",", "training_args", "=", "parser", ".", "parse_json_file", "(", "json_file", "=", "os", ".", "path", ".", "abspath", "(", "sys", ".", "argv", "[", "1", "]", ")", ")", "\n", "", "else", ":", "\n", "        ", "model_args", ",", "data_args", ",", "training_args", "=", "parser", ".", "parse_args_into_dataclasses", "(", ")", "\n", "\n", "# Detecting last checkpoint.", "\n", "", "last_checkpoint", "=", "None", "\n", "if", "os", ".", "path", ".", "isdir", "(", "training_args", ".", "output_dir", ")", "and", "training_args", ".", "do_train", "and", "not", "training_args", ".", "overwrite_output_dir", ":", "\n", "        ", "last_checkpoint", "=", "get_last_checkpoint", "(", "training_args", ".", "output_dir", ")", "\n", "if", "last_checkpoint", "is", "None", "and", "len", "(", "os", ".", "listdir", "(", "training_args", ".", "output_dir", ")", ")", ">", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"", "\n", "\"Use --overwrite_output_dir to overcome.\"", "\n", ")", "\n", "", "elif", "last_checkpoint", "is", "not", "None", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"", "\n", "\"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"", "\n", ")", "\n", "\n", "# Setup logging", "\n", "", "", "logging", ".", "basicConfig", "(", "\n", "format", "=", "\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\"", ",", "\n", "datefmt", "=", "\"%m/%d/%Y %H:%M:%S\"", ",", "\n", "handlers", "=", "[", "logging", ".", "StreamHandler", "(", "sys", ".", "stdout", ")", "]", ",", "\n", ")", "\n", "logger", ".", "setLevel", "(", "logging", ".", "INFO", "if", "is_main_process", "(", "training_args", ".", "local_rank", ")", "else", "logging", ".", "WARN", ")", "\n", "\n", "# Log on each process the small summary:", "\n", "logger", ".", "warning", "(", "\n", "f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"", "\n", "+", "f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"", "\n", ")", "\n", "# Set the verbosity to info of the Transformers logger (on main process only):", "\n", "if", "is_main_process", "(", "training_args", ".", "local_rank", ")", ":", "\n", "        ", "transformers", ".", "utils", ".", "logging", ".", "set_verbosity_info", "(", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "enable_default_handler", "(", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "enable_explicit_format", "(", ")", "\n", "", "logger", ".", "info", "(", "\"Training/evaluation parameters %s\"", ",", "training_args", ")", "\n", "\n", "# Set seed before initializing model.", "\n", "set_seed", "(", "training_args", ".", "seed", ")", "\n", "\n", "# Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)", "\n", "# or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/", "\n", "# (the dataset will be downloaded automatically from the datasets Hub", "\n", "#", "\n", "# For CSV/JSON files, this script will use the column called 'text' or the first column. You can easily tweak this", "\n", "# behavior (see below)", "\n", "#", "\n", "# In distributed training, the load_dataset function guarantee that only one local process can concurrently", "\n", "# download the dataset.", "\n", "if", "data_args", ".", "dataset_name", "is", "not", "None", ":", "\n", "# Downloading and loading a dataset from the hub.", "\n", "        ", "datasets", "=", "load_dataset", "(", "data_args", ".", "dataset_name", ",", "data_args", ".", "dataset_config_name", ")", "\n", "if", "\"validation\"", "not", "in", "datasets", ".", "keys", "(", ")", ":", "\n", "            ", "datasets", "[", "\"validation\"", "]", "=", "load_dataset", "(", "\n", "data_args", ".", "dataset_name", ",", "\n", "data_args", ".", "dataset_config_name", ",", "\n", "split", "=", "f\"train[:{data_args.validation_split_percentage}%]\"", ",", "\n", ")", "\n", "datasets", "[", "\"train\"", "]", "=", "load_dataset", "(", "\n", "data_args", ".", "dataset_name", ",", "\n", "data_args", ".", "dataset_config_name", ",", "\n", "split", "=", "f\"train[{data_args.validation_split_percentage}%:]\"", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "        ", "data_files", "=", "{", "}", "\n", "if", "data_args", ".", "train_file", "is", "not", "None", ":", "\n", "            ", "data_files", "[", "\"train\"", "]", "=", "data_args", ".", "train_file", "\n", "", "if", "data_args", ".", "validation_file", "is", "not", "None", ":", "\n", "            ", "data_files", "[", "\"validation\"", "]", "=", "data_args", ".", "validation_file", "\n", "", "extension", "=", "data_args", ".", "train_file", ".", "split", "(", "\".\"", ")", "[", "-", "1", "]", "\n", "if", "extension", "==", "\"txt\"", ":", "\n", "            ", "extension", "=", "\"text\"", "\n", "", "datasets", "=", "load_dataset", "(", "extension", ",", "data_files", "=", "data_files", ")", "\n", "# See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at", "\n", "# https://huggingface.co/docs/datasets/loading_datasets.html.", "\n", "\n", "# Load pretrained model and tokenizer", "\n", "#", "\n", "# Distributed training:", "\n", "# The .from_pretrained methods guarantee that only one local process can concurrently", "\n", "# download model & vocab.", "\n", "", "config_kwargs", "=", "{", "\n", "\"cache_dir\"", ":", "model_args", ".", "cache_dir", ",", "\n", "\"revision\"", ":", "model_args", ".", "model_revision", ",", "\n", "\"use_auth_token\"", ":", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", "}", "\n", "if", "model_args", ".", "config_name", ":", "\n", "        ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "model_args", ".", "config_name", ",", "**", "config_kwargs", ")", "\n", "", "elif", "model_args", ".", "model_name_or_path", ":", "\n", "        ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "model_args", ".", "model_name_or_path", ",", "**", "config_kwargs", ")", "\n", "", "else", ":", "\n", "        ", "config", "=", "CONFIG_MAPPING", "[", "model_args", ".", "model_type", "]", "(", ")", "\n", "logger", ".", "warning", "(", "\"You are instantiating a new config instance from scratch.\"", ")", "\n", "\n", "", "tokenizer_kwargs", "=", "{", "\n", "\"cache_dir\"", ":", "model_args", ".", "cache_dir", ",", "\n", "\"use_fast\"", ":", "model_args", ".", "use_fast_tokenizer", ",", "\n", "\"revision\"", ":", "model_args", ".", "model_revision", ",", "\n", "\"use_auth_token\"", ":", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", "}", "\n", "if", "model_args", ".", "tokenizer_name", ":", "\n", "        ", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "model_args", ".", "tokenizer_name", ",", "**", "tokenizer_kwargs", ")", "\n", "", "elif", "model_args", ".", "model_name_or_path", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "model_args", ".", "model_name_or_path", ",", "**", "tokenizer_kwargs", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"", "\n", "\"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"", "\n", ")", "\n", "\n", "", "if", "model_args", ".", "model_name_or_path", ":", "\n", "        ", "model", "=", "AutoModelForMaskedLM", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "\n", "from_tf", "=", "bool", "(", "\".ckpt\"", "in", "model_args", ".", "model_name_or_path", ")", ",", "\n", "config", "=", "config", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", "revision", "=", "model_args", ".", "model_revision", ",", "\n", "use_auth_token", "=", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "\"Training new model from scratch\"", ")", "\n", "model", "=", "AutoModelForMaskedLM", ".", "from_config", "(", "config", ")", "\n", "\n", "", "model", ".", "resize_token_embeddings", "(", "len", "(", "tokenizer", ")", ")", "\n", "\n", "# Preprocessing the datasets.", "\n", "# First we tokenize all the texts.", "\n", "if", "training_args", ".", "do_train", ":", "\n", "        ", "column_names", "=", "datasets", "[", "\"train\"", "]", ".", "column_names", "\n", "", "else", ":", "\n", "        ", "column_names", "=", "datasets", "[", "\"validation\"", "]", ".", "column_names", "\n", "", "text_column_name", "=", "\"text\"", "if", "\"text\"", "in", "column_names", "else", "column_names", "[", "0", "]", "\n", "\n", "if", "data_args", ".", "max_seq_length", "is", "None", ":", "\n", "        ", "max_seq_length", "=", "tokenizer", ".", "model_max_length", "\n", "if", "max_seq_length", ">", "1024", ":", "\n", "            ", "logger", ".", "warn", "(", "\n", "f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"", "\n", "\"Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.\"", "\n", ")", "\n", "max_seq_length", "=", "1024", "\n", "", "", "else", ":", "\n", "        ", "if", "data_args", ".", "max_seq_length", ">", "tokenizer", ".", "model_max_length", ":", "\n", "            ", "logger", ".", "warn", "(", "\n", "f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the\"", "\n", "f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"", "\n", ")", "\n", "", "max_seq_length", "=", "min", "(", "data_args", ".", "max_seq_length", ",", "tokenizer", ".", "model_max_length", ")", "\n", "\n", "", "if", "data_args", ".", "line_by_line", ":", "\n", "# When using line_by_line, we just tokenize each nonempty line.", "\n", "        ", "padding", "=", "\"max_length\"", "if", "data_args", ".", "pad_to_max_length", "else", "False", "\n", "\n", "def", "tokenize_function", "(", "examples", ")", ":", "\n", "# Remove empty lines", "\n", "            ", "examples", "[", "\"text\"", "]", "=", "[", "line", "for", "line", "in", "examples", "[", "\"text\"", "]", "if", "len", "(", "line", ")", ">", "0", "and", "not", "line", ".", "isspace", "(", ")", "]", "\n", "return", "tokenizer", "(", "\n", "examples", "[", "\"text\"", "]", ",", "\n", "padding", "=", "padding", ",", "\n", "truncation", "=", "True", ",", "\n", "max_length", "=", "max_seq_length", ",", "\n", "# We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it", "\n", "# receives the `special_tokens_mask`.", "\n", "return_special_tokens_mask", "=", "True", ",", "\n", ")", "\n", "\n", "", "tokenized_datasets", "=", "datasets", ".", "map", "(", "\n", "tokenize_function", ",", "\n", "batched", "=", "True", ",", "\n", "num_proc", "=", "data_args", ".", "preprocessing_num_workers", ",", "\n", "remove_columns", "=", "[", "text_column_name", "]", ",", "\n", "load_from_cache_file", "=", "not", "data_args", ".", "overwrite_cache", ",", "\n", ")", "\n", "", "else", ":", "\n", "# Otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.", "\n", "# We use `return_special_tokens_mask=True` because DataCollatorForLanguageModeling (see below) is more", "\n", "# efficient when it receives the `special_tokens_mask`.", "\n", "        ", "def", "tokenize_function", "(", "examples", ")", ":", "\n", "            ", "return", "tokenizer", "(", "examples", "[", "text_column_name", "]", ",", "return_special_tokens_mask", "=", "True", ")", "\n", "\n", "", "tokenized_datasets", "=", "datasets", ".", "map", "(", "\n", "tokenize_function", ",", "\n", "batched", "=", "True", ",", "\n", "num_proc", "=", "data_args", ".", "preprocessing_num_workers", ",", "\n", "remove_columns", "=", "column_names", ",", "\n", "load_from_cache_file", "=", "not", "data_args", ".", "overwrite_cache", ",", "\n", ")", "\n", "\n", "# Main data processing function that will concatenate all texts from our dataset and generate chunks of", "\n", "# max_seq_length.", "\n", "def", "group_texts", "(", "examples", ")", ":", "\n", "# Concatenate all texts.", "\n", "            ", "concatenated_examples", "=", "{", "k", ":", "sum", "(", "examples", "[", "k", "]", ",", "[", "]", ")", "for", "k", "in", "examples", ".", "keys", "(", ")", "}", "\n", "total_length", "=", "len", "(", "concatenated_examples", "[", "list", "(", "examples", ".", "keys", "(", ")", ")", "[", "0", "]", "]", ")", "\n", "# We drop the small remainder, we could add padding if the model supported it instead of this drop, you can", "\n", "# customize this part to your needs.", "\n", "total_length", "=", "(", "total_length", "//", "max_seq_length", ")", "*", "max_seq_length", "\n", "# Split by chunks of max_len.", "\n", "result", "=", "{", "\n", "k", ":", "[", "t", "[", "i", ":", "i", "+", "max_seq_length", "]", "for", "i", "in", "range", "(", "0", ",", "total_length", ",", "max_seq_length", ")", "]", "\n", "for", "k", ",", "t", "in", "concatenated_examples", ".", "items", "(", ")", "\n", "}", "\n", "return", "result", "\n", "\n", "# Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a", "\n", "# remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value", "\n", "# might be slower to preprocess.", "\n", "#", "\n", "# To speed up this part, we use multiprocessing. See the documentation of the map method for more information:", "\n", "# https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map", "\n", "", "tokenized_datasets", "=", "tokenized_datasets", ".", "map", "(", "\n", "group_texts", ",", "\n", "batched", "=", "True", ",", "\n", "num_proc", "=", "data_args", ".", "preprocessing_num_workers", ",", "\n", "load_from_cache_file", "=", "not", "data_args", ".", "overwrite_cache", ",", "\n", ")", "\n", "\n", "# Data collator", "\n", "# This one will take care of randomly masking the tokens.", "\n", "", "data_collator", "=", "DataCollatorForLanguageModeling", "(", "tokenizer", "=", "tokenizer", ",", "mlm_probability", "=", "data_args", ".", "mlm_probability", ")", "\n", "\n", "# Initialize our Trainer", "\n", "trainer", "=", "Trainer", "(", "\n", "model", "=", "model", ",", "\n", "args", "=", "training_args", ",", "\n", "train_dataset", "=", "tokenized_datasets", "[", "\"train\"", "]", "if", "training_args", ".", "do_train", "else", "None", ",", "\n", "eval_dataset", "=", "tokenized_datasets", "[", "\"validation\"", "]", "if", "training_args", ".", "do_eval", "else", "None", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "data_collator", "=", "data_collator", ",", "\n", ")", "\n", "\n", "# Training", "\n", "if", "training_args", ".", "do_train", ":", "\n", "        ", "if", "last_checkpoint", "is", "not", "None", ":", "\n", "            ", "checkpoint", "=", "last_checkpoint", "\n", "", "elif", "model_args", ".", "model_name_or_path", "is", "not", "None", "and", "os", ".", "path", ".", "isdir", "(", "model_args", ".", "model_name_or_path", ")", ":", "\n", "            ", "checkpoint", "=", "model_args", ".", "model_name_or_path", "\n", "", "else", ":", "\n", "            ", "checkpoint", "=", "None", "\n", "", "train_result", "=", "trainer", ".", "train", "(", "resume_from_checkpoint", "=", "checkpoint", ")", "\n", "trainer", ".", "save_model", "(", ")", "# Saves the tokenizer too for easy upload", "\n", "metrics", "=", "train_result", ".", "metrics", "\n", "\n", "trainer", ".", "log_metrics", "(", "\"train\"", ",", "metrics", ")", "\n", "trainer", ".", "save_metrics", "(", "\"train\"", ",", "metrics", ")", "\n", "trainer", ".", "save_state", "(", ")", "\n", "\n", "# Evaluation", "\n", "", "results", "=", "{", "}", "\n", "if", "training_args", ".", "do_eval", ":", "\n", "        ", "logger", ".", "info", "(", "\"*** Evaluate ***\"", ")", "\n", "\n", "eval_output", "=", "trainer", ".", "evaluate", "(", ")", "\n", "\n", "perplexity", "=", "math", ".", "exp", "(", "eval_output", "[", "\"eval_loss\"", "]", ")", "\n", "results", "[", "\"perplexity\"", "]", "=", "perplexity", "\n", "\n", "trainer", ".", "log_metrics", "(", "\"eval\"", ",", "results", ")", "\n", "trainer", ".", "save_metrics", "(", "\"eval\"", ",", "results", ")", "\n", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.pretrain.run_mlm._mp_fn": [[436, 439], ["run_mlm.main"], "function", ["home.repos.pwc.inspect_result.schwartz-lab-nlp_data_contamination.pretrain.run_mlm.main"], ["", "def", "_mp_fn", "(", "index", ")", ":", "\n", "# For xla_spawn (TPUs)", "\n", "    ", "main", "(", ")", "\n", "\n"]]}