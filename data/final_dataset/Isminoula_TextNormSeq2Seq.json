{"home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.None.main.train_char_model": [[11, 51], ["logger.info", "copy.deepcopy", "lib.data.create_datasets", "lib.model.create_model", "lib.train.Evaluator", "lib.train.Evaluator", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "raw_input", "lib.data.DataLoader.create_data", "lib.train.Evaluator.eval", "print", "logger.info", "os.path.join", "lib.train.Evaluator.eval", "logger.info", "os.path.join", "lib.train.Evaluator.eval", "lib.train.Trainer", "lib.train.Trainer.train", "logger.info", "os.path.join", "lib.train.Evaluator.eval", "logger.info", "os.path.join", "lib.train.Evaluator.eval", "logger.info", "len", "len", "raw_input.lower", "lib.data.Tweet.Tweet", "raw_input.split", "raw_input.split", "range"], "function", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.create_datasets", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.model.model_factory.create_model", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.create_data", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.evaluator.Evaluator.eval", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.evaluator.Evaluator.eval", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.evaluator.Evaluator.eval", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.trainer.Trainer.train", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.evaluator.Evaluator.eval", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.evaluator.Evaluator.eval"], ["def", "train_char_model", "(", "args", ")", ":", "\n", "    ", "logger", ".", "info", "(", "'*** Character model ***'", ")", "\n", "opt", "=", "copy", ".", "deepcopy", "(", "args", ")", "\n", "opt", ".", "input", "=", "'spelling'", "\n", "train_data", ",", "valid_data", ",", "test_data", ",", "vocab", ",", "mappings", "=", "lib", ".", "data", ".", "create_datasets", "(", "opt", ")", "\n", "char_model", ",", "char_optim", "=", "lib", ".", "model", ".", "create_model", "(", "(", "vocab", "[", "'src'", "]", ",", "vocab", "[", "'tgt'", "]", ")", ",", "opt", ",", "is_char_model", "=", "True", ")", "\n", "char_evaluator", "=", "lib", ".", "train", ".", "Evaluator", "(", "char_model", ",", "opt", ")", "\n", "char_test_evaluator", "=", "lib", ".", "train", ".", "Evaluator", "(", "char_model", ",", "opt", ")", "\n", "logger", ".", "info", "(", "char_model", ".", "opt", ")", "\n", "logger", ".", "info", "(", "'Loading test data for character model from \"%s\"'", "%", "opt", ".", "testdata", ")", "\n", "logger", ".", "info", "(", "'Loading training data for character model from \"%s\"'", "%", "opt", ".", "traindata", ")", "\n", "logger", ".", "info", "(", "' * Character model vocabulary size. source = %d; target = %d'", "%", "(", "len", "(", "vocab", "[", "'src'", "]", ")", ",", "len", "(", "vocab", "[", "'tgt'", "]", ")", ")", ")", "\n", "logger", ".", "info", "(", "' * Character model maximum batch size. %d'", "%", "opt", ".", "batch_size", ")", "\n", "logger", ".", "info", "(", "char_model", ")", "\n", "if", "opt", ".", "interactive", "and", "args", ".", "input", "!=", "'hybrid'", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "var", "=", "raw_input", "(", "\"Please enter a word to be try spelling model (q to quit): \"", ")", "\n", "if", "var", ".", "lower", "(", ")", "==", "'q'", ":", "break", "\n", "tweets", "=", "[", "Tweet", "(", "var", ".", "split", "(", ")", ",", "var", ".", "split", "(", ")", ",", "'1'", ",", "'1'", ")", "for", "i", "in", "range", "(", "2", ")", "]", "# suboptimal but works with minimal changes", "\n", "test_data", ",", "test_vocab", ",", "mappings", "=", "create_data", "(", "tweets", ",", "opt", "=", "opt", ",", "vocab", "=", "vocab", ",", "mappings", "=", "mappings", ")", "\n", "prediction", "=", "char_test_evaluator", ".", "eval", "(", "test_data", ")", "\n", "print", "(", "'Prediction is: {}'", ".", "format", "(", "''", ".", "join", "(", "prediction", ")", ")", ")", "\n", "", "", "elif", "opt", ".", "eval", ":", "# Evaluation only", "\n", "        ", "logger", ".", "info", "(", "\"=======Char eval on test set=============\"", ")", "\n", "pred_file", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "save_dir", ",", "'test.pred.char'", ")", "\n", "char_test_evaluator", ".", "eval", "(", "test_data", ",", "pred_file", "=", "pred_file", ")", "\n", "logger", ".", "info", "(", "\"=======Char eval on validation set=============\"", ")", "\n", "pred_file", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "save_dir", ",", "'valid.pred.char'", ")", "\n", "char_evaluator", ".", "eval", "(", "valid_data", ",", "pred_file", "=", "pred_file", ")", "\n", "", "else", ":", "# Training", "\n", "        ", "char_trainer", "=", "lib", ".", "train", ".", "Trainer", "(", "char_model", ",", "char_evaluator", ",", "train_data", ",", "valid_data", ",", "char_optim", ",", "opt", ")", "\n", "char_trainer", ".", "train", "(", "opt", ".", "start_epoch", ",", "opt", ".", "end_epoch", ")", "\n", "logger", ".", "info", "(", "\"=======Eval on test set=============\"", ")", "\n", "pred_file", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "save_dir", ",", "'test.pred.char'", ")", "\n", "char_test_evaluator", ".", "eval", "(", "test_data", ",", "pred_file", "=", "pred_file", ")", "\n", "logger", ".", "info", "(", "\"=======Eval on validation set=============\"", ")", "\n", "pred_file", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "save_dir", ",", "'valid.pred.char'", ")", "\n", "char_evaluator", ".", "eval", "(", "valid_data", ",", "pred_file", "=", "pred_file", ")", "\n", "logger", ".", "info", "(", "'*** Finished Character model ***\\n'", ")", "\n", "", "return", "char_model", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.None.main.main": [[53, 93], ["parameters.parser.parse_args", "parameters.change_args", "logging.basicConfig", "lib.data.create_datasets", "lib.model.create_model", "lib.train.Evaluator", "lib.train.Evaluator", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "main.train_char_model", "exit", "raw_input", "lib.data.DataLoader.create_data", "lib.train.Evaluator.eval", "print", "logger.info", "os.path.join", "lib.train.Evaluator.eval", "logger.info", "os.path.join", "lib.train.Evaluator.eval", "lib.train.Trainer", "lib.train.Trainer.train", "logger.info", "os.path.join", "lib.train.Evaluator.eval", "logger.info", "os.path.join", "lib.train.Evaluator.eval", "os.path.join", "len", "len", "raw_input.lower", "lib.data.Tweet.Tweet", "raw_input.split", "raw_input.split", "range"], "function", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.None.parameters.change_args", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.create_datasets", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.model.model_factory.create_model", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.None.main.train_char_model", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.create_data", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.evaluator.Evaluator.eval", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.evaluator.Evaluator.eval", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.evaluator.Evaluator.eval", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.trainer.Trainer.train", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.evaluator.Evaluator.eval", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.evaluator.Evaluator.eval"], ["", "def", "main", "(", ")", ":", "\n", "    ", "opt", "=", "parser", ".", "parse_args", "(", ")", "\n", "opt", "=", "change_args", "(", "opt", ")", "\n", "logging", ".", "basicConfig", "(", "filename", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "save_dir", ",", "'output.log'", ")", "if", "opt", ".", "logfolder", "else", "None", ",", "level", "=", "logging", ".", "INFO", ")", "\n", "unk_model", "=", "train_char_model", "(", "opt", ")", "if", "(", "opt", ".", "input", "in", "[", "'hybrid'", ",", "'spelling'", "]", ")", "else", "None", "\n", "if", "(", "opt", ".", "input", "==", "'spelling'", ")", ":", "exit", "(", ")", "\n", "train_data", ",", "valid_data", ",", "test_data", ",", "vocab", ",", "mappings", "=", "lib", ".", "data", ".", "create_datasets", "(", "opt", ")", "\n", "model", ",", "optim", "=", "lib", ".", "model", ".", "create_model", "(", "(", "vocab", "[", "'src'", "]", ",", "vocab", "[", "'tgt'", "]", ")", ",", "opt", ")", "\n", "evaluator", "=", "lib", ".", "train", ".", "Evaluator", "(", "model", ",", "opt", ",", "unk_model", ")", "\n", "test_evaluator", "=", "lib", ".", "train", ".", "Evaluator", "(", "model", ",", "opt", ",", "unk_model", ")", "\n", "logger", ".", "info", "(", "model", ".", "opt", ")", "\n", "logger", ".", "info", "(", "'Loading test data from \"%s\"'", "%", "opt", ".", "testdata", ")", "\n", "logger", ".", "info", "(", "'Loading training data from \"%s\"'", "%", "opt", ".", "traindata", ")", "\n", "logger", ".", "info", "(", "' * Vocabulary size. source = %d; target = %d'", "%", "(", "len", "(", "vocab", "[", "'src'", "]", ")", ",", "len", "(", "vocab", "[", "'tgt'", "]", ")", ")", ")", "\n", "logger", ".", "info", "(", "' * Maximum batch size. %d'", "%", "opt", ".", "batch_size", ")", "\n", "logger", ".", "info", "(", "model", ")", "\n", "if", "opt", ".", "interactive", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "var", "=", "raw_input", "(", "\"Please enter the text to be normalized (q to quit): \"", ")", "\n", "if", "var", ".", "lower", "(", ")", "==", "'q'", ":", "break", "\n", "tweets", "=", "[", "Tweet", "(", "var", ".", "split", "(", ")", ",", "var", ".", "split", "(", ")", ",", "'1'", ",", "'1'", ")", "for", "i", "in", "range", "(", "2", ")", "]", "#suboptimal but works with minimal changes", "\n", "test_data", ",", "test_vocab", ",", "mappings", "=", "create_data", "(", "tweets", ",", "opt", "=", "opt", ",", "vocab", "=", "vocab", ",", "mappings", "=", "mappings", ")", "\n", "prediction", "=", "test_evaluator", ".", "eval", "(", "test_data", ")", "\n", "print", "(", "'Prediction is: {}'", ".", "format", "(", "' '", ".", "join", "(", "prediction", ")", ")", ")", "\n", "", "", "elif", "opt", ".", "eval", ":", "# Evaluation only", "\n", "        ", "logger", ".", "info", "(", "\"=======Eval on test set=============\"", ")", "\n", "pred_file", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "save_dir", ",", "'test.pred'", ")", "\n", "test_evaluator", ".", "eval", "(", "test_data", ",", "pred_file", "=", "pred_file", ")", "\n", "logger", ".", "info", "(", "\"=======Eval on validation set=============\"", ")", "\n", "pred_file", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "save_dir", ",", "'valid.pred'", ")", "\n", "evaluator", ".", "eval", "(", "valid_data", ",", "pred_file", "=", "pred_file", ")", "\n", "", "else", ":", "# Training", "\n", "        ", "trainer", "=", "lib", ".", "train", ".", "Trainer", "(", "model", ",", "evaluator", ",", "train_data", ",", "valid_data", ",", "optim", ",", "opt", ")", "\n", "trainer", ".", "train", "(", "opt", ".", "start_epoch", ",", "opt", ".", "end_epoch", ")", "\n", "logger", ".", "info", "(", "\"=======Eval on test set=============\"", ")", "\n", "pred_file", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "save_dir", ",", "'test.pred'", ")", "\n", "test_evaluator", ".", "eval", "(", "test_data", ",", "pred_file", "=", "pred_file", ")", "\n", "logger", ".", "info", "(", "\"=======Eval on validation set=============\"", ")", "\n", "pred_file", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "save_dir", ",", "'valid.pred'", ")", "\n", "evaluator", ".", "eval", "(", "valid_data", ",", "pred_file", "=", "pred_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.None.parameters.change_args": [[69, 91], ["random.seed", "torch.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed_all", "numpy.random.seed", "logging.basicConfig", "os.makedirs", "torch.cuda.is_available", "logger.warning", "torch.cuda.set_device", "os.path.exists", "logger.warning", "os.path.join"], "function", ["None"], ["def", "change_args", "(", "opt", ")", ":", "\n", "    ", "torch", ".", "backends", ".", "cudnn", ".", "enabled", "=", "False", "\n", "cudnn", ".", "benchmark", "=", "False", "\n", "cudnn", ".", "deterministic", "=", "True", "\n", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", "=", "True", "\n", "random", ".", "seed", "(", "opt", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "opt", ".", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed", "(", "opt", ".", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "opt", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "opt", ".", "seed", ")", "\n", "if", "opt", ".", "save_dir", "and", "not", "os", ".", "path", ".", "exists", "(", "opt", ".", "save_dir", ")", ":", "os", ".", "makedirs", "(", "opt", ".", "save_dir", ")", "\n", "logging", ".", "basicConfig", "(", "filename", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "save_dir", ",", "'output.log'", ")", "if", "opt", ".", "logfolder", "else", "None", ",", "level", "=", "logging", ".", "INFO", ")", "\n", "if", "opt", ".", "self_tok", ":", "opt", ".", "self_tok", "=", "lib", ".", "constants", ".", "SELF", "\n", "opt", ".", "cuda", "=", "(", "opt", ".", "gpu", "!=", "-", "1", ")", "# Set cuda", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "opt", ".", "cuda", ":", "\n", "        ", "logger", ".", "warning", "(", "\"WARNING: You have a CUDA device, so you should probably run with -gpu 1\"", ")", "\n", "", "if", "opt", ".", "cuda", ":", "cuda", ".", "set_device", "(", "opt", ".", "gpu", ")", "\n", "if", "opt", ".", "share_embeddings", ":", "\n", "        ", "if", "not", "opt", ".", "share_vocab", ":", "\n", "            ", "logger", ".", "warning", "(", "'src/tgt vocab should be the same if you use share_embeddings! Changing share_vocab to True.'", ")", "\n", "opt", ".", "share_vocab", "=", "True", "\n", "", "", "return", "opt", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.model.model_factory.build_model": [[8, 15], ["lib.model.EncoderRNN", "lib.model.LuongAttnDecoderRNN", "lib.model.Seq2Seq", "model_factory.create_optim"], "function", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.model.model_factory.create_optim"], ["def", "build_model", "(", "vocabs", ",", "opt", ")", ":", "\n", "    ", "src_vocab", ",", "tgt_vocab", "=", "vocabs", "\n", "encoder", "=", "lib", ".", "model", ".", "EncoderRNN", "(", "opt", ",", "src_vocab", ")", "\n", "decoder", "=", "lib", ".", "model", ".", "LuongAttnDecoderRNN", "(", "opt", ",", "tgt_vocab", ")", "\n", "s2smodel", "=", "lib", ".", "model", ".", "Seq2Seq", "(", "encoder", ",", "decoder", ",", "opt", ")", "\n", "optim", "=", "create_optim", "(", "s2smodel", ",", "opt", ")", "\n", "return", "s2smodel", ",", "optim", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.model.model_factory.create_optim": [[17, 21], ["filter", "lib.train.Optim", "model.parameters"], "function", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Preprocessor.filter"], ["", "def", "create_optim", "(", "model", ",", "opt", ")", ":", "\n", "    ", "trained_params", "=", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "model", ".", "parameters", "(", ")", ")", "\n", "return", "lib", ".", "train", ".", "Optim", "(", "trained_params", ",", "opt", ".", "optim", ",", "opt", ".", "lr", ",", "opt", ".", "max_grad_norm", ",", "\n", "lr_decay", "=", "opt", ".", "learning_rate_decay", ",", "start_decay_after", "=", "opt", ".", "start_decay_after", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.model.model_factory.create_model": [[23, 47], ["sum", "logger.info", "logger.info", "torch.load", "model_factory.build_model", "model.load_state_dict", "optim.load_state_dict", "logger.info", "model_factory.build_model", "model.cuda", "p.nelement", "storage.cuda", "model.parameters"], "function", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.model.model_factory.build_model", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.optim.Optim.load_state_dict", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.optim.Optim.load_state_dict", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.model.model_factory.build_model"], ["", "def", "create_model", "(", "vocabs", ",", "opt", ",", "is_char_model", "=", "False", ")", ":", "\n", "    ", "model_state", "=", "'model_state_dict'", "\n", "optim_state", "=", "'optim_state_dict'", "\n", "if", "opt", ".", "load_from", "is", "not", "None", "or", "(", "opt", ".", "char_model", "!=", "None", "and", "is_char_model", ")", ":", "\n", "        ", "load_loc", "=", "opt", ".", "load_from", "if", "not", "is_char_model", "else", "opt", ".", "char_model", "\n", "logger", ".", "info", "(", "'Loading model from checkpoint at {}'", ".", "format", "(", "load_loc", ")", ")", "\n", "if", "opt", ".", "cuda", ":", "\n", "            ", "location", "=", "lambda", "storage", ",", "loc", ":", "storage", ".", "cuda", "(", "opt", ".", "gpu", ")", "\n", "", "else", ":", "\n", "            ", "location", "=", "lambda", "storage", ",", "loc", ":", "storage", "\n", "", "checkpoint", "=", "torch", ".", "load", "(", "load_loc", ",", "map_location", "=", "location", ")", "\n", "checkpoint", "[", "'opt'", "]", ".", "cuda", "=", "opt", ".", "cuda", "\n", "model", ",", "optim", "=", "build_model", "(", "vocabs", ",", "checkpoint", "[", "'opt'", "]", ")", "\n", "model", ".", "load_state_dict", "(", "checkpoint", "[", "model_state", "]", ")", "\n", "optim", ".", "load_state_dict", "(", "checkpoint", "[", "optim_state", "]", ")", "\n", "opt", ".", "start_epoch", "=", "checkpoint", "[", "'epoch'", "]", "+", "1", "\n", "opt", ".", "batch_size", "=", "checkpoint", "[", "'opt'", "]", ".", "batch_size", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "'Building Model'", ")", "\n", "model", ",", "optim", "=", "build_model", "(", "vocabs", ",", "opt", ")", "\n", "", "if", "opt", ".", "cuda", ":", "model", ".", "cuda", "(", ")", "# GPU.", "\n", "nParams", "=", "sum", "(", "[", "p", ".", "nelement", "(", ")", "for", "p", "in", "model", ".", "parameters", "(", ")", "]", ")", "\n", "logger", ".", "info", "(", "'* number of parameters: %d'", "%", "nParams", ")", "\n", "return", "model", ",", "optim", "\n", "", ""]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.model.model.EncoderRNN.__init__": [[11, 24], ["torch.Module.__init__", "len", "torch.Embedding", "torch.Embedding", "torch.Embedding", "getattr"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Preprocessor.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ",", "vocab", ")", ":", "\n", "        ", "super", "(", "EncoderRNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "opt", "=", "opt", "\n", "self", ".", "vocab_size", "=", "len", "(", "self", ".", "vocab", ")", "\n", "self", ".", "num_directions", "=", "2", "if", "self", ".", "opt", ".", "brnn", "else", "1", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "self", ".", "vocab_size", ",", "opt", ".", "emb_size", ",", "padding_idx", "=", "lib", ".", "constants", ".", "PAD", ")", "\n", "self", ".", "rnn", "=", "getattr", "(", "nn", ",", "self", ".", "opt", ".", "rnn_type", ")", "(", "\n", "input_size", "=", "self", ".", "opt", ".", "emb_size", ",", "\n", "hidden_size", "=", "opt", ".", "rnn_size", "//", "self", ".", "num_directions", ",", "\n", "num_layers", "=", "self", ".", "opt", ".", "layers", ",", "\n", "dropout", "=", "self", ".", "opt", ".", "dropout", ",", "\n", "bidirectional", "=", "self", ".", "opt", ".", "brnn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.model.model.EncoderRNN.forward": [[25, 32], ["model.EncoderRNN.embedding", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "model.EncoderRNN.rnn", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "model.EncoderRNN._cat_directions"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.model.model.EncoderRNN._cat_directions"], ["", "def", "forward", "(", "self", ",", "src", ",", "src_lens", ",", "hidden", "=", "None", ")", ":", "\n", "        ", "emb", "=", "self", ".", "embedding", "(", "src", ")", "\n", "packed_emb", "=", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "emb", ",", "src_lens", ")", "\n", "packed_outputs", ",", "self", ".", "hidden", "=", "self", ".", "rnn", "(", "packed_emb", ",", "hidden", ")", "\n", "outputs", ",", "output_lens", "=", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "packed_outputs", ")", "\n", "if", "self", ".", "opt", ".", "brnn", ":", "self", ".", "hidden", "=", "self", ".", "_cat_directions", "(", "self", ".", "hidden", ")", "\n", "return", "outputs", ",", "self", ".", "hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.model.model.EncoderRNN._cat_directions": [[33, 41], ["isinstance", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "tuple", "model.EncoderRNN._cat_directions._cat"], "methods", ["None"], ["", "def", "_cat_directions", "(", "self", ",", "hidden", ")", ":", "\n", "        ", "def", "_cat", "(", "h", ")", ":", "\n", "            ", "return", "torch", ".", "cat", "(", "[", "h", "[", "0", ":", "h", ".", "size", "(", "0", ")", ":", "2", "]", ",", "h", "[", "1", ":", "h", ".", "size", "(", "0", ")", ":", "2", "]", "]", ",", "2", ")", "\n", "", "if", "isinstance", "(", "hidden", ",", "tuple", ")", ":", "#LSTM", "\n", "            ", "hidden", "=", "tuple", "(", "[", "_cat", "(", "h", ")", "for", "h", "in", "hidden", "]", ")", "\n", "", "else", ":", "#GRU", "\n", "            ", "hidden", "=", "_cat", "(", "hidden", ")", "\n", "", "return", "hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.model.model.LuongAttnDecoderRNN.__init__": [[44, 67], ["torch.Module.__init__", "len", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Embedding", "torch.Embedding", "torch.Embedding", "getattr", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Preprocessor.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ",", "vocab", ")", ":", "\n", "        ", "super", "(", "LuongAttnDecoderRNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "opt", "=", "opt", "\n", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "vocab_size", "=", "len", "(", "self", ".", "vocab", ")", "\n", "self", ".", "tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "self", ".", "vocab_size", ",", "opt", ".", "emb_size", ",", "padding_idx", "=", "lib", ".", "constants", ".", "PAD", ")", "\n", "self", ".", "rnn", "=", "getattr", "(", "nn", ",", "self", ".", "opt", ".", "rnn_type", ")", "(", "\n", "input_size", "=", "self", ".", "opt", ".", "emb_size", ",", "\n", "hidden_size", "=", "self", ".", "opt", ".", "rnn_size", ",", "\n", "num_layers", "=", "self", ".", "opt", ".", "layers", ",", "\n", "dropout", "=", "self", ".", "opt", ".", "dropout", ")", "\n", "\n", "if", "self", ".", "opt", ".", "attention", ":", "\n", "            ", "self", ".", "W_a", "=", "nn", ".", "Linear", "(", "self", ".", "opt", ".", "rnn_size", ",", "self", ".", "opt", ".", "rnn_size", ",", "bias", "=", "opt", ".", "bias", ")", "\n", "self", ".", "W_c", "=", "nn", ".", "Linear", "(", "self", ".", "opt", ".", "rnn_size", "+", "self", ".", "opt", ".", "rnn_size", ",", "self", ".", "opt", ".", "rnn_size", ",", "bias", "=", "opt", ".", "bias", ")", "\n", "\n", "", "if", "self", ".", "opt", ".", "tie_decoder_embeddings", "and", "self", ".", "vocab_size", "!=", "1", ":", "\n", "            ", "self", ".", "W_proj", "=", "nn", ".", "Linear", "(", "self", ".", "opt", ".", "rnn_size", ",", "self", ".", "opt", ".", "emb_size", ",", "bias", "=", "opt", ".", "bias", ")", "\n", "self", ".", "W_s", "=", "nn", ".", "Linear", "(", "self", ".", "opt", ".", "emb_size", ",", "self", ".", "vocab_size", ",", "bias", "=", "opt", ".", "bias", ")", "\n", "self", ".", "W_s", ".", "weight", "=", "self", ".", "embedding", ".", "weight", "\n", "", "else", ":", "\n", "            ", "self", ".", "W_s", "=", "nn", ".", "Linear", "(", "self", ".", "opt", ".", "rnn_size", ",", "self", ".", "vocab_size", ",", "bias", "=", "opt", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.model.model.LuongAttnDecoderRNN.forward": [[68, 91], ["model.LuongAttnDecoderRNN.embedding", "model.LuongAttnDecoderRNN.rnn", "decoder_output.transpose.transpose.transpose", "model.LuongAttnDecoderRNN.squeeze", "src.unsqueeze", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "lib.metric.sequence_mask().unsqueeze", "torch.bmm.data.masked_fill_", "torch.bmm.data.masked_fill_", "torch.bmm.data.masked_fill_", "torch.softmax().unsqueeze", "torch.softmax().unsqueeze", "torch.softmax().unsqueeze", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.LuongAttnDecoderRNN.tanh", "attention_weights.squeeze.squeeze.squeeze", "model.LuongAttnDecoderRNN.W_s", "model.LuongAttnDecoderRNN.W_s", "model.LuongAttnDecoderRNN.W_a().transpose().transpose", "encoder_outputs.transpose", "model.LuongAttnDecoderRNN.W_c", "model.LuongAttnDecoderRNN.W_proj", "lib.metric.sequence_mask", "float", "torch.softmax", "torch.softmax", "torch.softmax", "model.LuongAttnDecoderRNN.W_a().transpose", "torch.bmm.squeeze", "torch.bmm.squeeze", "torch.bmm.squeeze", "model.LuongAttnDecoderRNN.W_a"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.loss.sequence_mask"], ["", "", "def", "forward", "(", "self", ",", "src", ",", "src_lens", ",", "encoder_outputs", ",", "decoder_hidden", ")", ":", "\n", "        ", "emb", "=", "self", ".", "embedding", "(", "src", ".", "unsqueeze", "(", "0", ")", ")", "\n", "decoder_output", ",", "self", ".", "decoder_hidden", "=", "self", ".", "rnn", "(", "emb", ",", "decoder_hidden", ")", "\n", "decoder_output", "=", "decoder_output", ".", "transpose", "(", "0", ",", "1", ")", "\n", "if", "self", ".", "opt", ".", "attention", ":", "\n", "            ", "attention_scores", "=", "torch", ".", "bmm", "(", "decoder_output", ",", "self", ".", "W_a", "(", "encoder_outputs", ")", ".", "transpose", "(", "0", ",", "1", ")", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "attention_mask", "=", "lib", ".", "metric", ".", "sequence_mask", "(", "src_lens", ")", ".", "unsqueeze", "(", "1", ")", "\n", "attention_scores", ".", "data", ".", "masked_fill_", "(", "1", "-", "attention_mask", ".", "data", ",", "-", "float", "(", "'inf'", ")", ")", "\n", "attention_weights", "=", "F", ".", "softmax", "(", "attention_scores", ".", "squeeze", "(", "1", ")", ",", "dim", "=", "1", ")", ".", "unsqueeze", "(", "1", ")", "\n", "context_vector", "=", "torch", ".", "bmm", "(", "attention_weights", ",", "encoder_outputs", ".", "transpose", "(", "0", ",", "1", ")", ")", "\n", "concat_input", "=", "torch", ".", "cat", "(", "[", "context_vector", ",", "decoder_output", "]", ",", "-", "1", ")", "\n", "concat_output", "=", "self", ".", "tanh", "(", "self", ".", "W_c", "(", "concat_input", ")", ")", "\n", "attention_weights", "=", "attention_weights", ".", "squeeze", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "attention_weights", "=", "None", "\n", "concat_output", "=", "decoder_output", "\n", "", "if", "self", ".", "opt", ".", "tie_decoder_embeddings", "and", "self", ".", "vocab_size", "!=", "1", ":", "\n", "            ", "output", "=", "self", ".", "W_s", "(", "self", ".", "W_proj", "(", "concat_output", ")", ")", "\n", "", "else", ":", "\n", "            ", "output", "=", "self", ".", "W_s", "(", "concat_output", ")", "\n", "", "output", "=", "output", ".", "squeeze", "(", "1", ")", "\n", "del", "src_lens", "\n", "return", "output", ",", "self", ".", "decoder_hidden", ",", "attention_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.model.model.Seq2Seq.__init__": [[94, 102], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Preprocessor.__init__"], ["    ", "def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ",", "opt", ")", ":", "\n", "        ", "super", "(", "Seq2Seq", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "torch", "=", "torch", ".", "cuda", "if", "opt", ".", "cuda", "else", "torch", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "decoder", "=", "decoder", "\n", "self", ".", "opt", "=", "opt", "\n", "if", "opt", ".", "share_embeddings", ":", "\n", "            ", "self", ".", "encoder", ".", "embedding", ".", "weight", "=", "self", ".", "decoder", ".", "embedding", ".", "weight", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.model.model.Seq2Seq.forward": [[103, 124], ["src.size", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "model.Seq2Seq.encoder", "range", "tgt.size", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "tgt.size", "src_lens.data.tolist", "model.Seq2Seq.decoder", "topi.squeeze.cuda", "torch.autograd.Variable.cuda", "torch.autograd.Variable.cuda", "torch.autograd.Variable.cuda", "random.random", "decoder_output.topk", "topi.squeeze"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.size", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.size", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.size"], ["", "", "def", "forward", "(", "self", ",", "batch", ",", "eval", "=", "False", ")", ":", "\n", "        ", "tgt", ",", "tgt_lens", "=", "batch", "[", "'tgt'", "]", "\n", "src", ",", "src_lens", "=", "batch", "[", "'src'", "]", "\n", "batch_size", "=", "src", ".", "size", "(", "1", ")", "\n", "assert", "(", "batch_size", "==", "tgt", ".", "size", "(", "1", ")", ")", "\n", "input_seq", "=", "Variable", "(", "torch", ".", "LongTensor", "(", "[", "lib", ".", "constants", ".", "BOS", "]", "*", "batch_size", ")", ")", "\n", "decoder_outputs", "=", "Variable", "(", "torch", ".", "zeros", "(", "self", ".", "opt", ".", "max_train_decode_len", ",", "batch_size", ",", "self", ".", "decoder", ".", "vocab_size", ")", ")", "\n", "if", "self", ".", "opt", ".", "cuda", ":", "input_seq", ",", "decoder_outputs", "=", "input_seq", ".", "cuda", "(", ")", ",", "decoder_outputs", ".", "cuda", "(", ")", "\n", "max_tgt_len", "=", "tgt", ".", "size", "(", ")", "[", "0", "]", "\n", "encoder_outputs", ",", "encoder_hidden", "=", "self", ".", "encoder", "(", "src", ",", "src_lens", ".", "data", ".", "tolist", "(", ")", ")", "\n", "decoder_hidden", "=", "encoder_hidden", "\n", "use_teacher_forcing", "=", "False", "if", "eval", "else", "random", ".", "random", "(", ")", "<", "self", ".", "opt", ".", "teacher_forcing_ratio", "\n", "for", "t", "in", "range", "(", "max_tgt_len", ")", ":", "\n", "            ", "decoder_output", ",", "decoder_hidden", ",", "attention_weights", "=", "self", ".", "decoder", "(", "input_seq", ",", "src_lens", ",", "encoder_outputs", ",", "decoder_hidden", ")", "\n", "decoder_outputs", "[", "t", "]", "=", "decoder_output", "\n", "if", "use_teacher_forcing", ":", "\n", "                ", "input_seq", "=", "tgt", "[", "t", "]", "\n", "", "else", ":", "\n", "                ", "topv", ",", "topi", "=", "decoder_output", ".", "topk", "(", "1", ")", "\n", "input_seq", "=", "topi", ".", "squeeze", "(", ")", "\n", "", "", "return", "decoder_outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.model.model.Seq2Seq.backward": [[125, 131], ["criterion", "tgt_seqs.size", "loss.backward", "loss.item"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.size", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.model.model.Seq2Seq.backward"], ["", "def", "backward", "(", "self", ",", "outputs", ",", "tgt_seqs", ",", "mask", ",", "criterion", ",", "eval", "=", "False", ",", "normalize", "=", "True", ")", ":", "\n", "        ", "max_tgt_len", "=", "tgt_seqs", ".", "size", "(", ")", "[", "0", "]", "\n", "logits", "=", "outputs", "[", ":", "max_tgt_len", "]", "\n", "loss", ",", "num_corrects", "=", "criterion", "(", "logits", ",", "tgt_seqs", ",", "mask", ",", "normalize", "=", "normalize", ")", "\n", "if", "(", "not", "eval", ")", ":", "loss", ".", "backward", "(", ")", "\n", "return", "loss", ".", "item", "(", ")", ",", "num_corrects", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.model.model.Seq2Seq.translate": [[132, 160], ["src.size", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "model.Seq2Seq.encoder", "numpy.array", "range", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "tgt.size", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "src_lens.data.tolist", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "numpy.ones", "numpy.ones", "model.Seq2Seq.decoder", "decoder_output.data.topk", "token_ids.squeeze.squeeze.squeeze", "prob.squeeze.squeeze.squeeze", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable.cuda", "torch.autograd.Variable.cuda", "torch.autograd.Variable.cuda", "torch.autograd.Variable.cuda", "torch.autograd.Variable.cuda", "torch.autograd.Variable.cuda", "src.size", "len", "len", "numpy.sum", "len", "len", "len", "attention_weights.cpu", "numpy.equal", "token_ids.squeeze.squeeze.cpu().numpy", "token_ids.squeeze.squeeze.cpu"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.size", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.size", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.size"], ["", "def", "translate", "(", "self", ",", "batch", ")", ":", "\n", "        ", "tgt", ",", "tgt_lens", "=", "batch", "[", "'tgt'", "]", "\n", "src", ",", "src_lens", "=", "batch", "[", "'src'", "]", "\n", "batch_size", "=", "src", ".", "size", "(", "1", ")", "\n", "assert", "(", "batch_size", "==", "tgt", ".", "size", "(", "1", ")", ")", "\n", "input_seq", "=", "Variable", "(", "torch", ".", "LongTensor", "(", "[", "lib", ".", "constants", ".", "BOS", "]", "*", "batch_size", ")", ")", "\n", "decoder_outputs", "=", "Variable", "(", "torch", ".", "zeros", "(", "self", ".", "opt", ".", "max_train_decode_len", ",", "batch_size", ",", "self", ".", "decoder", ".", "vocab_size", ")", ")", "\n", "if", "self", ".", "opt", ".", "cuda", ":", "input_seq", ",", "decoder_outputs", "=", "input_seq", ".", "cuda", "(", ")", ",", "decoder_outputs", ".", "cuda", "(", ")", "\n", "encoder_outputs", ",", "encoder_hidden", "=", "self", ".", "encoder", "(", "src", ",", "src_lens", ".", "data", ".", "tolist", "(", ")", ")", "\n", "decoder_hidden", "=", "encoder_hidden", "\n", "if", "self", ".", "opt", ".", "attention", ":", "all_attention_weights", "=", "torch", ".", "zeros", "(", "self", ".", "opt", ".", "max_train_decode_len", ",", "src", ".", "size", "(", "1", ")", ",", "len", "(", "src", ")", ")", "\n", "end_of_batch_pred", "=", "np", ".", "array", "(", "[", "lib", ".", "constants", ".", "EOS", "]", "*", "len", "(", "src_lens", ")", ")", "\n", "preds", "=", "np", ".", "ones", "(", "(", "self", ".", "opt", ".", "max_train_decode_len", ",", "len", "(", "src_lens", ")", ")", ")", "*", "2", "\n", "probs", "=", "np", ".", "ones", "(", "(", "self", ".", "opt", ".", "max_train_decode_len", ",", "len", "(", "src_lens", ")", ")", ")", "*", "2", "\n", "for", "t", "in", "range", "(", "self", ".", "opt", ".", "max_train_decode_len", ")", ":", "\n", "            ", "decoder_output", ",", "decoder_hidden", ",", "attention_weights", "=", "self", ".", "decoder", "(", "input_seq", ",", "src_lens", ",", "encoder_outputs", ",", "decoder_hidden", ")", "\n", "if", "self", ".", "opt", ".", "attention", ":", "\n", "                ", "all_attention_weights", "[", "t", "]", "=", "attention_weights", ".", "cpu", "(", ")", ".", "data", "\n", "", "prob", ",", "token_ids", "=", "decoder_output", ".", "data", ".", "topk", "(", "1", ")", "\n", "token_ids", "=", "token_ids", ".", "squeeze", "(", ")", "\n", "prob", "=", "prob", ".", "squeeze", "(", ")", "\n", "preds", "[", "t", ",", ":", "]", "=", "token_ids", "\n", "probs", "[", "t", ",", ":", "]", "=", "prob", "\n", "input_seq", "=", "Variable", "(", "token_ids", ")", "\n", "if", "np", ".", "sum", "(", "np", ".", "equal", "(", "token_ids", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "end_of_batch_pred", ")", ")", "==", "len", "(", "src", ")", ":", "\n", "                ", "break", "\n", "", "", "preds", "=", "torch", ".", "LongTensor", "(", "preds", ")", "\n", "return", "probs", ",", "preds", "", "", "", ""]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.metrics.f1": [[5, 44], ["len", "len", "len", "zip", "zip", "min", "range", "oracle_token.strip", "oracle_token.strip", "pred_token.strip", "len", "len", "len", "pred_tokens.append", "pred_token.lower", "input_token.lower", "oracle_token.lower", "pred_token.lower", "oracle_token.lower", "input_token.lower", "pred_token.lower", "input_token.lower", "oracle_token.strip", "oracle_token.strip", "pred_token.strip", "pred_token.lower", "input_token.lower", "oracle_token.lower", "pred_token.lower", "oracle_token.lower", "input_token.lower", "pred_token.lower", "input_token.lower"], "function", ["None"], ["def", "f1", "(", "inputs", ",", "preds", ",", "golds", ",", "spelling", "=", "False", ")", ":", "\n", "    ", "assert", "len", "(", "preds", ")", "==", "len", "(", "golds", ")", "==", "len", "(", "inputs", ")", "\n", "correct_norm", ",", "total_norm", ",", "total_nsw", "=", "0.0", ",", "0.0", ",", "0.0", "\n", "if", "(", "spelling", ")", ":", "#for spelling we got only one word each time", "\n", "        ", "for", "input_token", ",", "pred_token", ",", "oracle_token", "in", "zip", "(", "inputs", ",", "preds", ",", "golds", ")", ":", "\n", "            ", "pred_token", "=", "''", ".", "join", "(", "pred_token", ")", "\n", "if", "pred_token", ".", "lower", "(", ")", "!=", "input_token", ".", "lower", "(", ")", "and", "oracle_token", ".", "lower", "(", ")", "==", "pred_token", ".", "lower", "(", ")", "and", "oracle_token", ".", "strip", "(", ")", ":", "\n", "                ", "correct_norm", "+=", "1", "\n", "", "if", "oracle_token", ".", "lower", "(", ")", "!=", "input_token", ".", "lower", "(", ")", "and", "oracle_token", ".", "strip", "(", ")", ":", "\n", "                ", "total_nsw", "+=", "1", "\n", "", "if", "pred_token", ".", "lower", "(", ")", "!=", "input_token", ".", "lower", "(", ")", "and", "pred_token", ".", "strip", "(", ")", ":", "\n", "                ", "total_norm", "+=", "1", "\n", "", "", "", "else", ":", "\n", "        ", "for", "input_tokens", ",", "pred_tokens", ",", "oracle_tokens", "in", "zip", "(", "inputs", ",", "preds", ",", "golds", ")", ":", "\n", "            ", "sent_length", "=", "min", "(", "len", "(", "input_tokens", ")", ",", "len", "(", "oracle_tokens", ")", ")", "\n", "while", "len", "(", "pred_tokens", ")", "<", "sent_length", ":", "pred_tokens", ".", "append", "(", "lib", ".", "constants", ".", "PAD_WORD", ")", "\n", "for", "i", "in", "range", "(", "sent_length", ")", ":", "\n", "                ", "pred_token", "=", "pred_tokens", "[", "i", "]", "\n", "oracle_token", "=", "oracle_tokens", "[", "i", "]", "\n", "input_token", "=", "input_tokens", "[", "i", "]", "\n", "if", "pred_token", ".", "lower", "(", ")", "!=", "input_token", ".", "lower", "(", ")", "and", "oracle_token", ".", "lower", "(", ")", "==", "pred_token", ".", "lower", "(", ")", "and", "oracle_token", ".", "strip", "(", ")", ":", "\n", "                    ", "correct_norm", "+=", "1", "\n", "", "if", "oracle_token", ".", "lower", "(", ")", "!=", "input_token", ".", "lower", "(", ")", "and", "oracle_token", ".", "strip", "(", ")", ":", "\n", "                    ", "total_nsw", "+=", "1", "\n", "", "if", "pred_token", ".", "lower", "(", ")", "!=", "input_token", ".", "lower", "(", ")", "and", "pred_token", ".", "strip", "(", ")", ":", "\n", "                    ", "total_norm", "+=", "1", "\n", "# calc p, r, f", "\n", "", "", "", "", "p", "=", "r", "=", "f1", "=", "0.0", "\n", "if", "(", "total_norm", "!=", "0", "and", "correct_norm", "!=", "0", ")", ":", "p", "=", "correct_norm", "/", "total_norm", "\n", "if", "(", "total_norm", "!=", "0", "and", "total_nsw", "!=", "0", ")", ":", "r", "=", "correct_norm", "/", "total_nsw", "\n", "if", "p", "!=", "0", "and", "r", "!=", "0", ":", "f1", "=", "(", "2", "*", "p", "*", "r", ")", "/", "(", "p", "+", "r", ")", "\n", "results", "=", "{", "}", "\n", "results", "[", "\"correct_norm\"", "]", "=", "correct_norm", "\n", "results", "[", "\"total_norm\"", "]", "=", "total_norm", "\n", "results", "[", "\"total_nsw\"", "]", "=", "total_nsw", "\n", "results", "[", "\"precision\"", "]", "=", "p", "\n", "results", "[", "\"recall\"", "]", "=", "r", "\n", "results", "[", "\"f1\"", "]", "=", "f1", "\n", "return", "results", "", "", ""]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.loss.weighted_xent_loss": [[6, 19], ["logits.contiguous().view", "targets.contiguous().view", "torch.log_softmax", "losses.view.view", "losses.view.sum", "logits.size", "F.log_softmax.gather().squeeze", "mask.float", "F.log_softmax.max", "int", "int", "logits.contiguous", "targets.contiguous", "targets.size", "mask.float().sum", "pred_flat.eq().masked_select().float().data.sum", "pred_flat.eq().float().data.sum", "F.log_softmax.gather", "targets.contiguous().view.unsqueeze", "mask.float", "pred_flat.eq().masked_select().float", "pred_flat.eq().float", "pred_flat.eq().masked_select", "pred_flat.eq", "mask.contiguous().view", "pred_flat.eq", "mask.contiguous"], "function", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.size", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.size"], ["def", "weighted_xent_loss", "(", "logits", ",", "targets", ",", "mask", ",", "normalize", "=", "True", ")", ":", "\n", "    ", "logits_flat", "=", "logits", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "logits", ".", "size", "(", "-", "1", ")", ")", "\n", "targets_flat", "=", "targets", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", ")", "\n", "log_dist", "=", "F", ".", "log_softmax", "(", "logits_flat", ",", "dim", "=", "-", "1", ")", "\n", "losses", "=", "-", "log_dist", ".", "gather", "(", "1", ",", "targets_flat", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "losses", "=", "losses", ".", "view", "(", "*", "targets", ".", "size", "(", ")", ")", "\n", "losses", "=", "losses", "*", "mask", ".", "float", "(", ")", "\n", "loss", "=", "losses", ".", "sum", "(", ")", "\n", "loss", "=", "loss", "/", "mask", ".", "float", "(", ")", ".", "sum", "(", ")", "if", "normalize", "else", "loss", "\n", "pred_flat", "=", "log_dist", ".", "max", "(", "1", ")", "[", "1", "]", "\n", "num_corrects", "=", "int", "(", "pred_flat", ".", "eq", "(", "targets_flat", ")", ".", "masked_select", "(", "mask", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", ")", ".", "float", "(", ")", ".", "data", ".", "sum", "(", ")", ")", "if", "normalize", "else", "int", "(", "pred_flat", ".", "eq", "(", "targets_flat", ")", ".", "float", "(", ")", ".", "data", ".", "sum", "(", ")", ")", "\n", "return", "loss", ",", "num_corrects", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.loss.sequence_mask": [[21, 32], ["sequence_length.size", "torch.arange().long", "torch.arange().long", "torch.arange().long.unsqueeze().expand", "torch.autograd.Variable", "sequence_length.unsqueeze().expand_as", "sequence_length.data.max", "seq_range_expand.cuda.cuda", "torch.arange", "torch.arange", "torch.arange().long.unsqueeze", "sequence_length.unsqueeze"], "function", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.size"], ["", "def", "sequence_mask", "(", "sequence_length", ",", "max_len", "=", "None", ")", ":", "\n", "    ", "if", "max_len", "is", "None", ":", "\n", "        ", "max_len", "=", "sequence_length", ".", "data", ".", "max", "(", ")", "\n", "", "batch_size", "=", "sequence_length", ".", "size", "(", "0", ")", "\n", "seq_range", "=", "torch", ".", "arange", "(", "0", ",", "max_len", ")", ".", "long", "(", ")", "\n", "seq_range_expand", "=", "seq_range", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "batch_size", ",", "max_len", ")", "\n", "seq_range_expand", "=", "Variable", "(", "seq_range_expand", ")", "\n", "if", "sequence_length", ".", "is_cuda", ":", "seq_range_expand", "=", "seq_range_expand", ".", "cuda", "(", ")", "\n", "seq_length_expand", "=", "(", "sequence_length", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "seq_range_expand", ")", ")", "\n", "mask", "=", "seq_range_expand", "<", "seq_length_expand", "\n", "return", "mask", "\n", "", ""]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.utils.clean_sentence": [[11, 23], ["filter", "len", "len", "filter.index"], "function", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Preprocessor.filter"], ["def", "clean_sentence", "(", "sent", ",", "remove_unk", "=", "False", ",", "remove_eos", "=", "True", ",", "remove_bos", "=", "True", ")", ":", "\n", "    ", "if", "lib", ".", "constants", ".", "EOS_WORD", "in", "sent", ":", "\n", "        ", "sent", "=", "sent", "[", ":", "sent", ".", "index", "(", "lib", ".", "constants", ".", "EOS_WORD", ")", "+", "1", "]", "\n", "", "if", "remove_unk", ":", "\n", "        ", "sent", "=", "filter", "(", "lambda", "x", ":", "x", "!=", "lib", ".", "constants", ".", "UNK_WORD", ",", "sent", ")", "\n", "", "if", "remove_eos", ":", "\n", "        ", "if", "len", "(", "sent", ")", ">", "0", "and", "sent", "[", "-", "1", "]", "==", "lib", ".", "constants", ".", "EOS_WORD", ":", "\n", "            ", "sent", "=", "sent", "[", ":", "-", "1", "]", "\n", "", "", "if", "remove_bos", ":", "\n", "        ", "if", "len", "(", "sent", ")", ">", "0", "and", "sent", "[", "0", "]", "==", "lib", ".", "constants", ".", "BOS_WORD", ":", "\n", "            ", "sent", "=", "sent", "[", "1", ":", "]", "\n", "", "", "return", "sent", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.utils.handle_tags": [[25, 36], ["zip", "len", "len", "ret.append", "min", "range", "len", "len"], "function", ["None"], ["", "def", "handle_tags", "(", "input_words", ",", "pred_words", ")", ":", "\n", "    ", "assert", "len", "(", "input_words", ")", "==", "len", "(", "pred_words", ")", "\n", "ret", "=", "[", "]", "\n", "for", "input_tokens", ",", "pred_tokens", "in", "zip", "(", "input_words", ",", "pred_words", ")", ":", "\n", "        ", "if", "lib", ".", "constants", ".", "URL", "in", "pred_tokens", "or", "lib", ".", "constants", ".", "HASH", "in", "pred_tokens", "or", "lib", ".", "constants", ".", "MENTION", "in", "pred_tokens", ":", "\n", "            ", "sent_length", "=", "min", "(", "len", "(", "input_tokens", ")", ",", "len", "(", "pred_tokens", ")", ")", "\n", "for", "i", "in", "range", "(", "sent_length", ")", ":", "\n", "                ", "if", "(", "pred_tokens", "[", "i", "]", "==", "lib", ".", "constants", ".", "URL", "or", "pred_tokens", "[", "i", "]", "==", "lib", ".", "constants", ".", "HASH", "or", "pred_tokens", "[", "i", "]", "==", "lib", ".", "constants", ".", "MENTION", ")", ":", "\n", "                    ", "pred_tokens", "[", "i", "]", "=", "input_tokens", "[", "i", "]", "\n", "", "", "", "ret", ".", "append", "(", "pred_tokens", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.utils.handle_numbers": [[38, 48], ["zip", "len", "len", "min", "range", "ret.append", "len", "len", "any", "char.isdigit"], "function", ["None"], ["", "def", "handle_numbers", "(", "input_words", ",", "pred_words", ")", ":", "\n", "    ", "assert", "len", "(", "input_words", ")", "==", "len", "(", "pred_words", ")", "\n", "ret", "=", "[", "]", "\n", "for", "input_tokens", ",", "pred_tokens", "in", "zip", "(", "input_words", ",", "pred_words", ")", ":", "\n", "        ", "sent_length", "=", "min", "(", "len", "(", "input_tokens", ")", ",", "len", "(", "pred_tokens", ")", ")", "\n", "for", "i", "in", "range", "(", "sent_length", ")", ":", "\n", "            ", "if", "(", "any", "(", "char", ".", "isdigit", "(", ")", "for", "char", "in", "pred_tokens", "[", "i", "]", ")", ")", ":", "\n", "                ", "pred_tokens", "[", "i", "]", "=", "input_tokens", "[", "i", "]", "\n", "", "", "ret", ".", "append", "(", "pred_tokens", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.utils.handle_unk": [[50, 84], ["zip", "utils.copy_unks", "len", "len", "copy_unks.append", "min", "range", "len", "len", "unk_model.encoder.vocab.to_indices().view", "torch.cat", "torch.cat", "torch.autograd.Variable", "torch.autograd.Variable", "unk_src.cuda.t", "unk_model.translate", "[].max", "translation.t().tolist.t().tolist", "lib.metric.to_words", "torch.LongTensor", "torch.LongTensor", "unk_src.cuda.cuda", "src_lens.cuda.cuda", "unkowns_file.writerow", "logger.info", "unk_model.encoder.vocab.to_indices", "translation.t().tolist.t", "input_words_tokens[].isalpha", "len", "probs.transpose"], "function", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.utils.copy_unks", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.model.model.Seq2Seq.translate", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.utils.to_words", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.to_indices"], ["", "def", "handle_unk", "(", "input", ",", "input_words", ",", "pred_words", ",", "unk_model", ",", "unkowns_file", "=", "None", ")", ":", "\n", "    ", "if", "(", "unk_model", ")", ":", "\n", "        ", "assert", "len", "(", "input", ")", "==", "len", "(", "pred_words", ")", "\n", "ret", "=", "[", "]", "\n", "for", "input_tokens", ",", "input_words_tokens", ",", "pred_tokens", "in", "zip", "(", "input", ",", "input_words", ",", "pred_words", ")", ":", "\n", "            ", "if", "lib", ".", "constants", ".", "UNK_WORD", "in", "input_tokens", ":", "\n", "                ", "sent_length", "=", "min", "(", "len", "(", "input_tokens", ")", ",", "len", "(", "pred_tokens", ")", ")", "\n", "for", "i", "in", "range", "(", "sent_length", ")", ":", "\n", "                    ", "if", "(", "input_tokens", "[", "i", "]", "==", "lib", ".", "constants", ".", "UNK_WORD", ")", ":", "\n", "                        ", "unk_src", "=", "unk_model", ".", "encoder", ".", "vocab", ".", "to_indices", "(", "input_words_tokens", "[", "i", "]", ",", "\n", "eosWord", "=", "unk_model", ".", "opt", ".", "eos", ",", "bosWord", "=", "unk_model", ".", "opt", ".", "bos", ")", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "#Repeat as many times as the batch size, awful but works", "\n", "unk_src", "=", "torch", ".", "cat", "(", "[", "unk_src", "]", "*", "unk_model", ".", "opt", ".", "batch_size", ")", "\n", "unk_src", "=", "Variable", "(", "unk_src", ")", "\n", "if", "input_words_tokens", "[", "i", "]", "==", "''", "or", "input_words_tokens", "[", "i", "]", "==", "' '", ":", "\n", "                            ", "continue", "\n", "", "src_lens", "=", "Variable", "(", "torch", ".", "LongTensor", "(", "[", "len", "(", "p", ")", "for", "p", "in", "unk_src", "]", ")", ")", "\n", "if", "unk_model", ".", "opt", ".", "cuda", ":", "unk_src", "=", "unk_src", ".", "cuda", "(", ")", "\n", "if", "unk_model", ".", "opt", ".", "cuda", ":", "src_lens", "=", "src_lens", ".", "cuda", "(", ")", "\n", "unk_src", "=", "unk_src", ".", "t", "(", ")", "\n", "batch", "=", "{", "}", "\n", "batch", "[", "'src'", "]", "=", "unk_src", ",", "src_lens", "\n", "batch", "[", "'tgt'", "]", "=", "unk_src", ",", "src_lens", "\n", "probs", ",", "translation", "=", "unk_model", ".", "translate", "(", "batch", ")", "\n", "confidence", "=", "probs", ".", "transpose", "(", ")", "[", "0", "]", ".", "max", "(", ")", "\n", "translation", "=", "translation", ".", "t", "(", ")", ".", "tolist", "(", ")", "\n", "trsl2wrds", "=", "lib", ".", "metric", ".", "to_words", "(", "translation", ",", "unk_model", ".", "encoder", ".", "vocab", ")", "\n", "if", "unkowns_file", ":", "unkowns_file", ".", "writerow", "(", "[", "input_words_tokens", "[", "i", "]", ",", "''", ".", "join", "(", "trsl2wrds", "[", "0", "]", ")", ",", "confidence", "]", ")", "\n", "pred_tokens", "[", "i", "]", "=", "''", ".", "join", "(", "trsl2wrds", "[", "0", "]", ")", "if", "confidence", ">", "50.0", "and", "input_words_tokens", "[", "i", "]", ".", "isalpha", "(", ")", "else", "input_words_tokens", "[", "i", "]", "\n", "if", "input_words_tokens", "[", "i", "]", "!=", "pred_tokens", "[", "i", "]", ":", "logger", ".", "info", "(", "'secondary model confidence:{}, unk_word:{}, prediction:{}'", ".", "format", "(", "confidence", ",", "input_words_tokens", "[", "i", "]", ",", "pred_tokens", "[", "i", "]", ")", ")", "\n", "", "", "", "ret", ".", "append", "(", "pred_tokens", ")", "\n", "", "", "else", ":", "\n", "        ", "ret", "=", "copy_unks", "(", "input", ",", "input_words", ",", "pred_words", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.utils.copy_unks": [[86, 97], ["zip", "len", "len", "ret.append", "min", "range", "len", "len"], "function", ["None"], ["", "def", "copy_unks", "(", "input", ",", "input_words", ",", "pred_words", ")", ":", "\n", "    ", "assert", "len", "(", "input", ")", "==", "len", "(", "pred_words", ")", "\n", "ret", "=", "[", "]", "\n", "for", "input_tokens", ",", "input_words_tokens", ",", "pred_tokens", "in", "zip", "(", "input", ",", "input_words", ",", "pred_words", ")", ":", "\n", "        ", "if", "lib", ".", "constants", ".", "UNK_WORD", "in", "input_tokens", "or", "lib", ".", "constants", ".", "UNK_WORD", "in", "pred_tokens", ":", "\n", "            ", "sent_length", "=", "min", "(", "len", "(", "input_tokens", ")", ",", "len", "(", "pred_tokens", ")", ")", "\n", "for", "i", "in", "range", "(", "sent_length", ")", ":", "\n", "                ", "if", "(", "input_tokens", "[", "i", "]", "==", "lib", ".", "constants", ".", "UNK_WORD", "or", "pred_tokens", "[", "i", "]", "==", "lib", ".", "constants", ".", "UNK_WORD", ")", ":", "\n", "                    ", "pred_tokens", "[", "i", "]", "=", "input_words_tokens", "[", "i", "]", "\n", "", "", "", "ret", ".", "append", "(", "pred_tokens", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.utils.clean_self_toks": [[99, 109], ["zip", "ret_preds.append", "min", "range", "len", "len"], "function", ["None"], ["", "def", "clean_self_toks", "(", "inputs", ",", "preds", ",", "token", ")", ":", "\n", "    ", "ret_preds", "=", "[", "]", "\n", "for", "input_tokens", ",", "pred_tokens", "in", "zip", "(", "inputs", ",", "preds", ")", ":", "\n", "        ", "if", "token", "in", "pred_tokens", ":", "\n", "            ", "length", "=", "min", "(", "len", "(", "input_tokens", ")", ",", "len", "(", "pred_tokens", ")", ")", "\n", "for", "i", "in", "range", "(", "length", ")", ":", "\n", "                ", "if", "pred_tokens", "[", "i", "]", "==", "token", ":", "\n", "                    ", "pred_tokens", "[", "i", "]", "=", "input_tokens", "[", "i", "]", "\n", "", "", "", "ret_preds", ".", "append", "(", "pred_tokens", ")", "\n", "", "return", "ret_preds", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.utils.to_words": [[111, 118], ["utils.clean_sentence", "ret.append", "dict.itos"], "function", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.utils.clean_sentence", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.itos"], ["", "def", "to_words", "(", "sents", ",", "dict", ")", ":", "\n", "    ", "ret", "=", "[", "]", "\n", "for", "sent", "in", "sents", ":", "\n", "        ", "sent", "=", "[", "dict", ".", "itos", "(", "id", ")", "for", "id", "in", "sent", "]", "\n", "sent", "=", "clean_sentence", "(", "sent", ",", "remove_unk", "=", "False", ")", "\n", "ret", ".", "append", "(", "sent", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.utils.char_to_words": [[120, 126], ["ret.append"], "function", ["None"], ["", "def", "char_to_words", "(", "sents", ")", ":", "\n", "    ", "ret", "=", "[", "]", "\n", "for", "sent", "in", "sents", ":", "\n", "        ", "sent", "=", "''", ".", "join", "(", "sent", ")", ".", "split", "(", "'#'", ")", "\n", "ret", ".", "append", "(", "sent", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.utils.compute_single": [[128, 135], ["len", "metric_fn"], "function", ["None"], ["", "def", "compute_single", "(", "pair", ",", "metric_fn", "=", "None", ")", ":", "\n", "    ", "input", ",", "pred", ",", "gold", "=", "pair", "\n", "if", "len", "(", "pred", ")", "==", "0", ":", "\n", "        ", "score", "=", "0.", "\n", "", "else", ":", "\n", "        ", "score", "=", "metric_fn", "(", "input", ",", "pred", ",", "gold", ")", "[", "'f1'", "]", "\n", "", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.utils.compute_batch": [[137, 141], ["functools.partial", "map", "list", "zip"], "function", ["None"], ["", "def", "compute_batch", "(", "inputs", ",", "preds", ",", "golds", ",", "metric_fn", ")", ":", "\n", "    ", "compute_single_with_metric", "=", "functools", ".", "partial", "(", "compute_single", ",", "metric_fn", "=", "metric_fn", ")", "\n", "scores", "=", "map", "(", "compute_single_with_metric", ",", "zip", "(", "inputs", ",", "preds", ",", "golds", ")", ")", "\n", "return", "list", "(", "scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.utils.compute_numcorrects": [[143, 149], ["torch.log_softmax", "F.log_softmax.max", "int", "int", "pred_flat.eq().masked_select().float().data.sum", "pred_flat.eq().float().data.sum", "pred_flat.eq().masked_select().float", "pred_flat.eq().float", "pred_flat.eq().masked_select", "pred_flat.eq", "pred_flat.eq"], "function", ["None"], ["", "def", "compute_numcorrects", "(", "dec_logits", ",", "targets", ",", "pad_masks", "=", "None", ")", ":", "\n", "    ", "log_dist", "=", "F", ".", "log_softmax", "(", "dec_logits", ",", "dim", "=", "-", "1", ")", "\n", "pred_flat", "=", "log_dist", ".", "max", "(", "-", "1", ")", "[", "1", "]", "\n", "num_corrects", "=", "int", "(", "pred_flat", ".", "eq", "(", "targets", ")", ".", "masked_select", "(", "pad_masks", ")", ".", "float", "(", ")", ".", "data", ".", "sum", "(", ")", ")", "if", "pad_masks", "is", "not", "None", "else", "int", "(", "pred_flat", ".", "eq", "(", "targets", ")", ".", "float", "(", ")", ".", "data", ".", "sum", "(", ")", ")", "\n", "return", "num_corrects", "\n", "", ""]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.trainer.Trainer.__init__": [[11, 20], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "model", ",", "evaluator", ",", "train_data", ",", "eval_data", ",", "optim", ",", "opt", ",", "test_eval", "=", "None", ")", ":", "\n", "        ", "self", ".", "model", "=", "model", "\n", "self", ".", "evaluator", "=", "evaluator", "\n", "self", ".", "train_data", "=", "train_data", "\n", "self", ".", "eval_data", "=", "eval_data", "\n", "self", ".", "optim", "=", "optim", "\n", "self", ".", "opt", "=", "opt", "\n", "self", ".", "test_eval", "=", "test_eval", "\n", "self", ".", "criterion", "=", "lib", ".", "metric", ".", "weighted_xent_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.trainer.Trainer.train": [[21, 42], ["range", "logger.info", "logger.info", "trainer.Trainer.train_epoch", "logger.info", "logger.info", "trainer.Trainer.evaluator.eval", "trainer.Trainer.optim.update_lr", "os.path.join", "torch.save", "logger.info", "trainer.Trainer.model.state_dict", "trainer.Trainer.optim.state_dict"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.trainer.Trainer.train_epoch", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.evaluator.Evaluator.eval", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.optim.Optim.update_lr", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.optim.Optim.state_dict", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.optim.Optim.state_dict"], ["", "def", "train", "(", "self", ",", "start_epoch", ",", "end_epoch", ")", ":", "\n", "        ", "if", "(", "self", ".", "opt", ".", "save_interval", "==", "-", "1", ")", ":", "self", ".", "opt", ".", "save_interval", "=", "end_epoch", "+", "1", "\n", "for", "epoch", "in", "range", "(", "start_epoch", ",", "end_epoch", "+", "1", ")", ":", "\n", "            ", "logger", ".", "info", "(", "'\\n* TextNorm epoch *'", ")", "\n", "logger", ".", "info", "(", "'Model optim lr: %g'", "%", "self", ".", "optim", ".", "lr", ")", "\n", "total_loss", ",", "total_accuracy", "=", "self", ".", "train_epoch", "(", "epoch", ")", "\n", "logger", ".", "info", "(", "'Train loss: %.2f'", "%", "total_loss", ")", "\n", "logger", ".", "info", "(", "'Train total_accuracy: %.2f'", "%", "total_accuracy", ")", "\n", "valid_loss", ",", "valid_f1", "=", "self", ".", "evaluator", ".", "eval", "(", "self", ".", "eval_data", ")", "\n", "self", ".", "optim", ".", "update_lr", "(", "valid_loss", ",", "epoch", ")", "\n", "if", "epoch", "%", "self", ".", "opt", ".", "save_interval", "==", "0", "or", "epoch", "==", "end_epoch", ":", "\n", "                ", "checkpoint", "=", "{", "\n", "'model_state_dict'", ":", "self", ".", "model", ".", "state_dict", "(", ")", ",", "\n", "'optim_state_dict'", ":", "self", ".", "optim", ".", "state_dict", "(", ")", ",", "\n", "'opt'", ":", "self", ".", "opt", ",", "\n", "'epoch'", ":", "epoch", ",", "\n", "}", "\n", "model_name", "=", "os", ".", "path", ".", "join", "(", "self", ".", "opt", ".", "save_dir", ",", "\"model_%d\"", "%", "epoch", ")", "\n", "model_name", "+=", "\"_\"", "+", "self", ".", "opt", ".", "input", "+", "\".pt\"", "\n", "torch", ".", "save", "(", "checkpoint", ",", "model_name", ")", "\n", "logger", ".", "info", "(", "'Save model as %s'", "%", "model_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.trainer.Trainer.train_epoch": [[43, 70], ["trainer.Trainer.model.train", "time.time", "lib.data.Dataset", "lib.data.Dataset.batches", "enumerate", "trainer.Trainer.model.train", "trainer.Trainer.model", "trainer.Trainer.model.zero_grad", "lib.metric.sequence_mask().transpose", "trainer.Trainer.model.backward", "tgt_lens.data.sum().item", "trainer.Trainer.optim.step", "int", "logger.info", "float", "lib.metric.sequence_mask", "tgt_lens.data.sum", "float", "tgt.data.ne().sum", "src.data.ne().sum", "float", "tgt.size", "time.time", "tgt.data.ne", "src.data.ne"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.trainer.Trainer.train", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dataset.Dataset.batches", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.trainer.Trainer.train", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.model.model.Seq2Seq.backward", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.optim.Optim.step", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.loss.sequence_mask", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.size"], ["", "", "", "def", "train_epoch", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "self", ".", "model", ".", "train", "(", ")", "\n", "epoch_time", "=", "time", ".", "time", "(", ")", "\n", "train_data", "=", "lib", ".", "data", ".", "Dataset", "(", "self", ".", "train_data", ",", "self", ".", "opt", ")", "\n", "num_batches", "=", "train_data", ".", "num_batches", "\n", "train_iter", "=", "train_data", ".", "batches", "(", ")", "\n", "total_loss", ",", "total_corrects", ",", "total_tgts", "=", "0", ",", "0", ",", "0", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "train_iter", ")", ":", "\n", "            ", "self", ".", "model", ".", "train", "(", ")", "\n", "tgt", ",", "tgt_lens", "=", "batch", "[", "'tgt'", "]", "\n", "src", ",", "src_lens", "=", "batch", "[", "'src'", "]", "\n", "outputs", "=", "self", ".", "model", "(", "batch", ")", "\n", "self", ".", "model", ".", "zero_grad", "(", ")", "\n", "pad_masks", "=", "lib", ".", "metric", ".", "sequence_mask", "(", "sequence_length", "=", "tgt_lens", ",", "max_len", "=", "tgt", ".", "size", "(", "0", ")", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "loss", ",", "num_corrects", "=", "self", ".", "model", ".", "backward", "(", "outputs", ",", "tgt", ",", "pad_masks", ",", "criterion", "=", "self", ".", "criterion", ")", "\n", "num_words", "=", "(", "tgt", ".", "data", ".", "ne", "(", "lib", ".", "constants", ".", "PAD", ")", ".", "sum", "(", ")", "+", "src", ".", "data", ".", "ne", "(", "lib", ".", "constants", ".", "PAD", ")", ".", "sum", "(", ")", ")", ".", "item", "(", ")", "\n", "num_tgts", "=", "tgt_lens", ".", "data", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "total_loss", "+=", "loss", "\n", "total_corrects", "+=", "num_corrects", "\n", "total_tgts", "+=", "num_tgts", "\n", "self", ".", "optim", ".", "step", "(", ")", "\n", "if", "(", "i", "+", "1", ")", "%", "self", ".", "opt", ".", "log_interval", "==", "0", ":", "\n", "                ", "words_pers", "=", "int", "(", "num_words", "/", "(", "time", ".", "time", "(", ")", "-", "epoch_time", ")", ")", "\n", "accuracy", "=", "100", "*", "(", "num_corrects", "/", "float", "(", "num_tgts", ")", ")", "\n", "logger", ".", "info", "(", "'Epoch %3d,  %6d/%d batches  loss:%f,  num_words:%d,  accuracy:%f'", "%", "\n", "(", "epoch", ",", "i", "+", "1", ",", "num_batches", ",", "loss", ",", "words_pers", ",", "accuracy", ")", ")", "\n", "", "", "return", "total_loss", "/", "float", "(", "num_batches", ")", ",", "100", "*", "(", "total_corrects", "/", "float", "(", "total_tgts", ")", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.optim.Optim._makeOptimizer": [[9, 20], ["torch.SGD", "torch.Adagrad", "torch.Adadelta", "torch.Adam", "RuntimeError"], "methods", ["None"], ["    ", "def", "_makeOptimizer", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "method", "==", "'sgd'", ":", "\n", "            ", "self", ".", "optimizer", "=", "optim", ".", "SGD", "(", "self", ".", "params", ",", "lr", "=", "self", ".", "lr", ")", "\n", "", "elif", "self", ".", "method", "==", "'adagrad'", ":", "\n", "            ", "self", ".", "optimizer", "=", "optim", ".", "Adagrad", "(", "self", ".", "params", ",", "lr", "=", "self", ".", "lr", ")", "\n", "", "elif", "self", ".", "method", "==", "'adadelta'", ":", "\n", "            ", "self", ".", "optimizer", "=", "optim", ".", "Adadelta", "(", "self", ".", "params", ",", "lr", "=", "self", ".", "lr", ")", "\n", "", "elif", "self", ".", "method", "==", "'adam'", ":", "\n", "            ", "self", ".", "optimizer", "=", "optim", ".", "Adam", "(", "self", ".", "params", ",", "lr", "=", "self", ".", "lr", ")", "\n", "", "else", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Invalid optim method: \"", "+", "self", ".", "method", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.optim.Optim.__init__": [[21, 30], ["list", "torch.Optim._makeOptimizer"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.optim.Optim._makeOptimizer"], ["", "", "def", "__init__", "(", "self", ",", "params", ",", "method", ",", "lr", ",", "max_grad_norm", ",", "lr_decay", "=", "1", ",", "start_decay_after", "=", "None", ")", ":", "\n", "        ", "self", ".", "params", "=", "list", "(", "params", ")", "# careful: params may be a generator", "\n", "self", ".", "last_loss", "=", "None", "\n", "self", ".", "lr", "=", "lr", "\n", "self", ".", "max_grad_norm", "=", "max_grad_norm", "\n", "self", ".", "method", "=", "method", "\n", "self", ".", "lr_decay", "=", "lr_decay", "\n", "self", ".", "start_decay_after", "=", "start_decay_after", "\n", "self", ".", "_makeOptimizer", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.optim.Optim.step": [[31, 34], ["torch.nn.utils.clip_grad_norm_", "torch.Optim.optimizer.step"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.optim.Optim.step"], ["", "def", "step", "(", "self", ")", ":", "\n", "        ", "clip_grad_norm_", "(", "self", ".", "params", ",", "self", ".", "max_grad_norm", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.optim.Optim.set_lr": [[35, 38], ["None"], "methods", ["None"], ["", "def", "set_lr", "(", "self", ",", "lr", ")", ":", "\n", "        ", "self", ".", "lr", "=", "lr", "\n", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "\"lr\"", "]", "=", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.optim.Optim.state_dict": [[39, 41], ["torch.Optim.optimizer.state_dict"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.optim.Optim.state_dict"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "optimizer", ".", "state_dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.optim.Optim.load_state_dict": [[42, 44], ["torch.Optim.optimizer.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.optim.Optim.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "d", ")", ":", "\n", "        ", "return", "self", ".", "optimizer", ".", "load_state_dict", "(", "d", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.optim.Optim.update_lr": [[45, 51], ["logging.info", "torch.Optim.set_lr"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.optim.Optim.set_lr"], ["", "def", "update_lr", "(", "self", ",", "loss", ",", "epoch", ")", ":", "\n", "        ", "if", "self", ".", "start_decay_after", "is", "not", "None", "and", "epoch", ">=", "self", ".", "start_decay_after", ":", "\n", "            ", "if", "self", ".", "last_loss", "is", "not", "None", "and", "loss", ">", "self", ".", "last_loss", ":", "\n", "                ", "logging", ".", "info", "(", "\"Decaying learning rate from {} to {}\"", ".", "format", "(", "self", ".", "lr", ",", "self", ".", "lr", "*", "self", ".", "lr_decay", ")", ")", "\n", "self", ".", "set_lr", "(", "self", ".", "lr", "*", "self", ".", "lr_decay", ")", "\n", "", "", "self", ".", "last_loss", "=", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.evaluator.Evaluator.__init__": [[11, 16], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "model", ",", "opt", ",", "unk_model", "=", "None", ")", ":", "\n", "        ", "self", ".", "opt", "=", "opt", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "unk_model", "=", "unk_model", "\n", "self", ".", "criterion", "=", "lib", ".", "metric", ".", "weighted_xent_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.evaluator.Evaluator.eval": [[17, 73], ["evaluator.Evaluator.model.eval", "lib.data.Dataset", "lib.data.Dataset.batches", "enumerate", "lib.metrics.f1", "evaluator.Evaluator.model", "lib.metric.sequence_mask().transpose", "evaluator.Evaluator.model.backward", "evaluator.Evaluator.model.translate", "predictions.t().tolist.t().tolist.t().tolist", "lib.metric.to_words", "lib.metric.to_words", "lib.metric.handle_tags", "lib.metric.handle_unk", "all_inputs.extend", "all_preds.extend", "all_targets.extend", "all_others.extend", "float", "logger.info", "logger.info", "evaluator.Evaluator._report", "isinstance", "lib.metric.to_words.data.t().tolist", "tgt.data.t().tolist", "lib.metric.char_to_words", "lib.metric.char_to_words", "lib.metric.char_to_words", "csv.writer", "lib.metric.clean_self_toks", "lib.metric.clean_self_toks", "evaluator.Evaluator.save_json", "logger.info", "logger.info", "lib.metric.sequence_mask", "predictions.t().tolist.t().tolist.t", "open", "lib.metric.f1", "zip", "lib.metric.to_words.data.t", "tgt.data.t", "os.path.join", "zip", "tgt.size"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.evaluator.Evaluator.eval", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dataset.Dataset.batches", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.metrics.f1", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.model.model.Seq2Seq.backward", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.model.model.Seq2Seq.translate", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.utils.to_words", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.utils.to_words", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.utils.handle_tags", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.utils.handle_unk", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.evaluator.Evaluator._report", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.utils.char_to_words", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.utils.char_to_words", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.utils.char_to_words", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.utils.clean_self_toks", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.utils.clean_self_toks", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.evaluator.Evaluator.save_json", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.loss.sequence_mask", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.metric.metrics.f1", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.size"], ["", "def", "eval", "(", "self", ",", "data_iter", ",", "pred_file", "=", "None", ")", ":", "\n", "        ", "self", ".", "model", ".", "eval", "(", ")", "\n", "valid_data", "=", "lib", ".", "data", ".", "Dataset", "(", "data_iter", ",", "self", ".", "opt", ")", "\n", "num_batches", "=", "valid_data", ".", "num_batches", "\n", "val_iter", "=", "valid_data", ".", "batches", "(", ")", "\n", "all_inputs", ",", "all_preds", ",", "all_targets", ",", "all_others", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "total_loss", "=", "0", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "val_iter", ")", ":", "\n", "            ", "tgt", ",", "tgt_lens", "=", "batch", "[", "'tgt'", "]", "\n", "src", ",", "src_lens", "=", "batch", "[", "'src'", "]", "\n", "tids", "=", "batch", "[", "'tid'", "]", "\n", "indices", "=", "batch", "[", "'index'", "]", "\n", "outputs", "=", "self", ".", "model", "(", "batch", ",", "eval", "=", "True", ")", "\n", "mask", "=", "lib", ".", "metric", ".", "sequence_mask", "(", "sequence_length", "=", "tgt_lens", ",", "max_len", "=", "tgt", ".", "size", "(", "0", ")", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "loss", ",", "_", "=", "self", ".", "model", ".", "backward", "(", "outputs", ",", "tgt", ",", "mask", ",", "criterion", "=", "self", ".", "criterion", ",", "eval", "=", "True", ",", "normalize", "=", "True", ")", "\n", "probs", ",", "predictions", "=", "self", ".", "model", ".", "translate", "(", "batch", ")", "\n", "predictions", "=", "predictions", ".", "t", "(", ")", ".", "tolist", "(", ")", "\n", "src", ",", "tgt", "=", "src", ".", "data", ".", "t", "(", ")", ".", "tolist", "(", ")", ",", "tgt", ".", "data", ".", "t", "(", ")", ".", "tolist", "(", ")", "\n", "src", "=", "lib", ".", "metric", ".", "to_words", "(", "src", ",", "self", ".", "model", ".", "encoder", ".", "vocab", ")", "\n", "preds", "=", "lib", ".", "metric", ".", "to_words", "(", "predictions", ",", "self", ".", "model", ".", "decoder", ".", "vocab", ")", "\n", "tgt_sent_words", "=", "batch", "[", "'tgt_sent_words'", "]", "\n", "src_sent_words", "=", "batch", "[", "'src_sent_words'", "]", "\n", "\n", "if", "(", "self", ".", "opt", ".", "input", "==", "'char'", ")", ":", "\n", "                ", "tgt_sent_words", "=", "lib", ".", "metric", ".", "char_to_words", "(", "tgt_sent_words", ")", "\n", "src_sent_words", "=", "lib", ".", "metric", ".", "char_to_words", "(", "src_sent_words", ")", "\n", "preds", "=", "lib", ".", "metric", ".", "char_to_words", "(", "preds", ")", "\n", "\n", "#write predictions from secondary char model to file", "\n", "", "unk_file", "=", "csv", ".", "writer", "(", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "opt", ".", "save_dir", ",", "\"unkowns.csv\"", ")", ",", "\"a\"", ")", ",", "delimiter", "=", "'\\t'", ")", "if", "self", ".", "unk_model", "and", "not", "self", ".", "opt", ".", "interactive", "else", "None", "\n", "preds", "=", "lib", ".", "metric", ".", "handle_tags", "(", "src_sent_words", ",", "preds", ")", "\n", "preds", "=", "lib", ".", "metric", ".", "handle_unk", "(", "src", ",", "src_sent_words", ",", "preds", ",", "self", ".", "unk_model", ",", "unk_file", ")", "\n", "if", "(", "self", ".", "opt", ".", "self_tok", ")", ":", "\n", "                ", "preds", "=", "lib", ".", "metric", ".", "clean_self_toks", "(", "src_sent_words", ",", "preds", ",", "self", ".", "opt", ".", "self_tok", ")", "\n", "tgt_sent_words", "=", "lib", ".", "metric", ".", "clean_self_toks", "(", "src_sent_words", ",", "tgt_sent_words", ",", "self", ".", "opt", ".", "self_tok", ")", "\n", "\n", "", "sent_f1", "=", "[", "lib", ".", "metric", ".", "f1", "(", "[", "s", "]", ",", "[", "p", "]", ",", "[", "t", "]", ",", "spelling", "=", "(", "self", ".", "opt", ".", "input", "==", "'spelling'", ")", ")", "[", "'f1'", "]", "for", "s", ",", "p", ",", "t", "in", "zip", "(", "src_sent_words", ",", "preds", ",", "tgt_sent_words", ")", "]", "\n", "all_inputs", ".", "extend", "(", "src_sent_words", ")", "\n", "all_preds", ".", "extend", "(", "preds", ")", "\n", "all_targets", ".", "extend", "(", "tgt_sent_words", ")", "\n", "all_others", ".", "extend", "(", "[", "x", "for", "x", "in", "zip", "(", "tids", ",", "indices", ",", "sent_f1", ")", "]", ")", "\n", "total_loss", "+=", "loss", "\n", "\n", "", "valid_loss", "=", "total_loss", "/", "float", "(", "num_batches", ")", "\n", "results", "=", "lib", ".", "metrics", ".", "f1", "(", "all_inputs", ",", "all_preds", ",", "all_targets", ",", "spelling", "=", "(", "self", ".", "opt", ".", "input", "==", "'spelling'", ")", ")", "\n", "if", "self", ".", "opt", ".", "interactive", ":", "\n", "            ", "return", "all_preds", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"correct_norm:{}, total_norm:{}, total_nsw:{}\"", ".", "format", "(", "results", "[", "\"correct_norm\"", "]", ",", "results", "[", "\"total_norm\"", "]", ",", "results", "[", "\"total_nsw\"", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"precision:{}, recall:{}, f1:{}\\n\"", ".", "format", "(", "results", "[", "\"precision\"", "]", ",", "results", "[", "\"recall\"", "]", ",", "results", "[", "\"f1\"", "]", ")", ")", "\n", "self", ".", "_report", "(", "all_inputs", "[", "0", ":", "5", "]", ",", "all_preds", "[", "0", ":", "5", "]", ",", "all_targets", "[", "0", ":", "5", "]", ",", "all_others", "[", "0", ":", "5", "]", ")", "\n", "if", "isinstance", "(", "pred_file", ",", "str", ")", ":", "\n", "                ", "self", ".", "save_json", "(", "all_inputs", ",", "all_preds", ",", "all_targets", ",", "all_others", ",", "pred_file", ")", "\n", "logger", ".", "info", "(", "\"Corpus F1 score: %.2f\"", "%", "(", "results", "[", "\"f1\"", "]", "*", "100", ")", ")", "\n", "logger", ".", "info", "(", "\"Predictions saved to %s\"", "%", "pred_file", ")", "\n", "", "return", "valid_loss", ",", "results", "[", "'f1'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.evaluator.Evaluator._report": [[74, 80], ["zip", "logger.info", "token.join", "token.join", "token.join"], "methods", ["None"], ["", "", "def", "_report", "(", "self", ",", "inputs", ",", "preds", ",", "targets", ",", "others", ")", ":", "\n", "        ", "for", "input", ",", "pred", ",", "target", ",", "other", "in", "zip", "(", "inputs", ",", "preds", ",", "targets", ",", "others", ")", ":", "\n", "            ", "tid", ",", "ind", ",", "score", "=", "other", "\n", "token", "=", "''", "if", "self", ".", "opt", ".", "input", "==", "'spelling'", "else", "' '", "\n", "logger", ".", "info", "(", "'ind:{} tid:{} \\ninput:{}\\ntarget:{}\\nprediction:{}\\n'", ".", "format", "(", "\n", "ind", ",", "tid", ",", "token", ".", "join", "(", "input", ")", ",", "token", ".", "join", "(", "target", ")", ",", "token", ".", "join", "(", "pred", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.train.evaluator.Evaluator.save_json": [[81, 93], ["zip", "json_entries.append", "open", "f.write", "open", "csv.writer", "csv.writer.writerow", "json.dumps", "csv.writer.writerow"], "methods", ["None"], ["", "", "def", "save_json", "(", "self", ",", "inputs", ",", "preds", ",", "targets", ",", "others", ",", "pred_file", ")", ":", "#And csv!", "\n", "        ", "json_entries", "=", "[", "]", "\n", "for", "input", ",", "pred", ",", "target", ",", "other", "in", "zip", "(", "inputs", ",", "preds", ",", "targets", ",", "others", ")", ":", "\n", "            ", "tid", ",", "ind", ",", "sent_f1", "=", "other", "\n", "json_entries", ".", "append", "(", "{", "\"tid\"", ":", "tid", ",", "\"index\"", ":", "ind", ",", "\"output\"", ":", "pred", ",", "\"input\"", ":", "input", ",", "\"target\"", ":", "target", ",", "\"score\"", ":", "sent_f1", "}", ")", "\n", "", "with", "open", "(", "pred_file", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "json", ".", "dumps", "(", "json_entries", ",", "ensure_ascii", "=", "False", ")", ")", "\n", "", "with", "open", "(", "pred_file", "+", "\".csv\"", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "tsvfile", "=", "csv", ".", "writer", "(", "f", ",", "delimiter", "=", "'\\t'", ")", "\n", "tsvfile", ".", "writerow", "(", "[", "\"ixdex\"", ",", "\"score\"", ",", "\"output\"", ",", "\"input\"", ",", "\"target\"", "]", ")", "\n", "for", "x", "in", "json_entries", ":", "\n", "                ", "tsvfile", ".", "writerow", "(", "[", "x", "[", "\"index\"", "]", ",", "x", "[", "\"score\"", "]", ",", "x", "[", "\"output\"", "]", ",", "x", "[", "\"input\"", "]", ",", "x", "[", "\"target\"", "]", "]", ")", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.DataLoader.__init__": [[10, 19], ["DataLoader.DataLoader.get_prox_keys", "DataLoader.DataLoader.load_data", "DataLoader.DataLoader.encode_tweets"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.DataLoader.get_prox_keys", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.DataLoader.load_data", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.DataLoader.encode_tweets"], ["    ", "def", "__init__", "(", "self", ",", "tweets", ",", "vocab", ",", "mappings", ",", "opt", ")", ":", "\n", "        ", "self", ".", "opt", "=", "opt", "\n", "self", ".", "prox_arr", "=", "self", ".", "get_prox_keys", "(", ")", "\n", "self", ".", "mappings", "=", "mappings", "if", "mappings", "else", "{", "}", "\n", "self", ".", "tweets", ",", "self", ".", "source_vocab", ",", "self", ".", "target_vocab", "=", "self", ".", "load_data", "(", "tweets", ")", "\n", "if", "(", "vocab", ")", ":", "\n", "            ", "self", ".", "source_vocab", "=", "vocab", "[", "\"src\"", "]", "\n", "self", ".", "target_vocab", "=", "vocab", "[", "\"tgt\"", "]", "\n", "", "self", ".", "ret", "=", "self", ".", "encode_tweets", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.DataLoader.tweets_toIdx": [[20, 30], ["copy.deepcopy", "tweet.set_inputidx", "tweet.set_outputidx", "enumerate", "DataLoader.DataLoader.source_vocab.to_indices", "DataLoader.DataLoader.target_vocab.to_indices", "DataLoader.DataLoader.mappings.get", "len", "list", "list"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Tweet.set_inputidx", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Tweet.set_outputidx", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.to_indices", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.to_indices"], ["", "def", "tweets_toIdx", "(", "self", ")", ":", "\n", "        ", "for", "tweet", "in", "self", ".", "tweets", ":", "\n", "            ", "input", "=", "copy", ".", "deepcopy", "(", "tweet", ".", "input", ")", "\n", "if", "(", "self", ".", "opt", ".", "correct_unique_mappings", ")", ":", "\n", "                ", "for", "index", ",", "wi", "in", "enumerate", "(", "input", ")", ":", "\n", "                    ", "mapping", "=", "self", ".", "mappings", ".", "get", "(", "wi", ")", "\n", "if", "mapping", "and", "len", "(", "mapping", ")", "==", "1", "and", "list", "(", "mapping", ")", "[", "0", "]", "!=", "wi", ":", "\n", "                        ", "input", "[", "index", "]", "=", "list", "(", "mapping", ")", "[", "0", "]", "\n", "", "", "", "tweet", ".", "set_inputidx", "(", "self", ".", "source_vocab", ".", "to_indices", "(", "input", ",", "bosWord", "=", "self", ".", "opt", ".", "bos", ",", "eosWord", "=", "self", ".", "opt", ".", "eos", ")", ")", "\n", "tweet", ".", "set_outputidx", "(", "self", ".", "target_vocab", ".", "to_indices", "(", "tweet", ".", "output", ",", "bosWord", "=", "self", ".", "opt", ".", "bos", ",", "eosWord", "=", "self", ".", "opt", ".", "eos", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.DataLoader.encode_tweets": [[31, 51], ["DataLoader.DataLoader.tweets_toIdx", "src_sents.append", "tgt_sents.append", "src_sents_words.append", "tgt_sents_words.append", "indices.append", "tids.append", "range", "len"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.DataLoader.tweets_toIdx"], ["", "", "def", "encode_tweets", "(", "self", ")", ":", "\n", "        ", "self", ".", "tweets_toIdx", "(", ")", "\n", "src_sents", ",", "tgt_sents", ",", "tgt_sents_words", ",", "src_sents_words", ",", "indices", ",", "tids", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "tweet", "in", "self", ".", "tweets", ":", "\n", "            ", "src_sents", ".", "append", "(", "tweet", ".", "inputidx", ")", "\n", "tgt_sents", ".", "append", "(", "tweet", ".", "outputidx", ")", "\n", "src_sents_words", ".", "append", "(", "tweet", ".", "input", ")", "\n", "tgt_sents_words", ".", "append", "(", "tweet", ".", "output", ")", "\n", "indices", ".", "append", "(", "tweet", ".", "ind", ")", "\n", "tids", ".", "append", "(", "tweet", ".", "tid", ")", "\n", "\n", "", "ret", "=", "{", "'src'", ":", "src_sents", ",", "\n", "'src_sent_words'", ":", "src_sents_words", ",", "\n", "'tgt'", ":", "tgt_sents", ",", "\n", "'tgt_sent_words'", ":", "tgt_sents_words", ",", "\n", "'pos'", ":", "range", "(", "len", "(", "src_sents", ")", ")", ",", "\n", "'index'", ":", "indices", ",", "\n", "'tid'", ":", "tids", "}", "\n", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.DataLoader.vector_repr": [[52, 71], ["range", "range", "list", "list", "len", "len", "DataLoader.DataLoader.mappings[].add", "inp_i[].lower", "inp_o[].lower", "inp_o[].lower", "set", "DataLoader.DataLoader.mappings[].add", "inp_o[].lower", "inp_i[].lower", "inp_i[].lower", "inp_i[].lower"], "methods", ["None"], ["", "def", "vector_repr", "(", "self", ",", "inp_i", ",", "inp_o", ",", "update_mappings", ")", ":", "\n", "        ", "if", "update_mappings", ":", "\n", "            ", "for", "k", "in", "range", "(", "len", "(", "inp_i", ")", ")", ":", "\n", "                ", "try", ":", "\n", "                    ", "self", ".", "mappings", "[", "inp_i", "[", "k", "]", ".", "lower", "(", ")", "]", ".", "add", "(", "inp_o", "[", "k", "]", ".", "lower", "(", ")", ")", "\n", "", "except", "KeyError", ":", "\n", "                    ", "self", ".", "mappings", "[", "inp_i", "[", "k", "]", ".", "lower", "(", ")", "]", "=", "set", "(", ")", "\n", "self", ".", "mappings", "[", "inp_i", "[", "k", "]", ".", "lower", "(", ")", "]", ".", "add", "(", "inp_o", "[", "k", "]", ".", "lower", "(", ")", ")", "\n", "\n", "", "", "", "if", "(", "self", ".", "opt", ".", "self_tok", "==", "lib", ".", "constants", ".", "SELF", ")", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "inp_i", ")", ")", ":", "\n", "                ", "if", "(", "inp_i", "[", "i", "]", ".", "lower", "(", ")", "==", "inp_o", "[", "i", "]", ".", "lower", "(", ")", ")", ":", "\n", "                    ", "inp_o", "[", "i", "]", "=", "self", ".", "opt", ".", "self_tok", "\n", "\n", "", "", "", "if", "(", "self", ".", "opt", ".", "input", "==", "'char'", ")", ":", "\n", "            ", "inp_i", "=", "list", "(", "'#'", ".", "join", "(", "inp_i", ")", ")", "\n", "inp_o", "=", "list", "(", "'#'", ".", "join", "(", "inp_o", ")", ")", "\n", "\n", "", "return", "inp_i", ",", "inp_o", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.DataLoader.load_data": [[73, 135], ["Dict.Dict.Dict", "Dict.Dict.Dict", "Tweet.Tweet.Preprocessor", "Dict.Dict.Dict.makeVocabulary", "Dict.Dict.Dict.makeLabelToIdx", "Dict.Dict.Dict.makeVocabulary", "Dict.Dict.Dict.makeLabelToIdx", "Tweet.Tweet.Preprocessor.run", "Tweet.Tweet.Preprocessor.run", "zip", "DataLoader.DataLoader.vector_repr", "Dict.Dict.Dict.add_words", "Dict.Dict.Dict.add_words", "tweet.set_input", "tweet.set_output", "word_tweets.append", "copy.deepcopy", "same_tw.append", "diff_tw.append", "DataLoader.DataLoader.isalnum", "oword.isalnum", "DataLoader.DataLoader.vector_repr", "Dict.Dict.Dict.add_words", "Dict.Dict.Dict.add_words", "tweet.set_input", "tweet.set_output", "word_tweets.append", "copy.deepcopy", "len", "len", "any", "any", "random.random", "random.random", "DataLoader.DataLoader.add_noise", "DataLoader.DataLoader.vector_repr", "Dict.Dict.Dict.add_words", "Dict.Dict.Dict.add_words", "tweet.set_input", "tweet.set_output", "word_tweets.append", "c.isdigit", "c.isdigit", "len", "len", "any", "any", "copy.deepcopy", "c.isdigit", "c.isdigit"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.makeVocabulary", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.makeLabelToIdx", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.makeVocabulary", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.makeLabelToIdx", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Preprocessor.run", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Preprocessor.run", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.DataLoader.vector_repr", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.add_words", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.add_words", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Tweet.set_input", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Tweet.set_output", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.DataLoader.vector_repr", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.add_words", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.add_words", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Tweet.set_input", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Tweet.set_output", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.DataLoader.add_noise", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.DataLoader.vector_repr", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.add_words", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.add_words", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Tweet.set_input", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Tweet.set_output"], ["", "def", "load_data", "(", "self", ",", "tweets", ")", ":", "\n", "        ", "source_vocab", "=", "Dict", "(", "vocab_size", "=", "self", ".", "opt", ".", "vocab_size", ",", "bosWord", "=", "self", ".", "opt", ".", "bos", ",", "eosWord", "=", "self", ".", "opt", ".", "eos", ")", "\n", "target_vocab", "=", "Dict", "(", "vocab_size", "=", "self", ".", "opt", ".", "vocab_size", ",", "bosWord", "=", "self", ".", "opt", ".", "bos", ",", "eosWord", "=", "self", ".", "opt", ".", "eos", ")", "\n", "if", "(", "self", ".", "opt", ".", "share_vocab", ")", ":", "\n", "            ", "target_vocab", "=", "source_vocab", "\n", "", "processor", "=", "Preprocessor", "(", ")", "\n", "#for test the mappings are predefined and for all other inputs except word level we dont need them, so no updates", "\n", "update_mappings", "=", "not", "self", ".", "mappings", "and", "self", ".", "opt", ".", "input", "==", "'word'", "\n", "word_tweets", "=", "[", "]", "\n", "for", "tweet", "in", "tweets", ":", "\n", "            ", "inp_i", ",", "pos_i", "=", "processor", ".", "run", "(", "tweet", ".", "input", ",", "self", ".", "opt", ".", "lowercase", ")", "\n", "inp_o", ",", "pos_o", "=", "processor", ".", "run", "(", "tweet", ".", "output", ",", "self", ".", "opt", ".", "lowercase", ")", "\n", "\n", "if", "(", "self", ".", "opt", ".", "input", "==", "'spelling'", ")", ":", "#character model word2word corrections", "\n", "                ", "for", "iword", ",", "oword", "in", "zip", "(", "inp_i", ",", "inp_o", ")", ":", "\n", "                    ", "if", "iword", "and", "oword", "and", "iword", ".", "isalnum", "(", ")", "and", "oword", ".", "isalnum", "(", ")", ":", "\n", "                        ", "if", "iword", "==", "oword", "and", "len", "(", "iword", ")", ">", "1", "and", "len", "(", "oword", ")", ">", "1", "and", "not", "any", "(", "c", ".", "isdigit", "(", ")", "for", "c", "in", "iword", ")", "and", "not", "any", "(", "c", ".", "isdigit", "(", ")", "for", "c", "in", "oword", ")", ":", "\n", "                            ", "if", "random", ".", "random", "(", ")", ">", "0.9", "and", "not", "self", ".", "opt", ".", "data_augm", ":", "\n", "                                ", "continue", "\n", "", "", "iwordv", ",", "owordv", "=", "self", ".", "vector_repr", "(", "iword", ",", "oword", ",", "update_mappings", ")", "\n", "source_vocab", ".", "add_words", "(", "iwordv", ")", "\n", "target_vocab", ".", "add_words", "(", "owordv", ")", "\n", "tweet", ".", "set_input", "(", "iwordv", ")", "\n", "tweet", ".", "set_output", "(", "owordv", ")", "\n", "word_tweets", ".", "append", "(", "copy", ".", "deepcopy", "(", "tweet", ")", ")", "\n", "if", "(", "self", ".", "opt", ".", "data_augm", ")", ":", "\n", "                            ", "if", "random", ".", "random", "(", ")", ">", "(", "1", "-", "self", ".", "opt", ".", "noise_ratio", ")", ":", "\n", "                                ", "if", "iword", "==", "oword", "and", "len", "(", "iword", ")", ">", "1", "and", "len", "(", "oword", ")", ">", "1", "and", "not", "any", "(", "c", ".", "isdigit", "(", ")", "for", "c", "in", "iword", ")", "and", "not", "any", "(", "c", ".", "isdigit", "(", ")", "for", "c", "in", "oword", ")", ":", "\n", "                                    ", "iword", "=", "self", ".", "add_noise", "(", "iword", ")", "\n", "if", "(", "iword", "==", "''", "or", "iword", "==", "' '", ")", ":", "\n", "                                        ", "continue", "\n", "", "iwordv", ",", "owordv", "=", "self", ".", "vector_repr", "(", "iword", ",", "oword", ",", "update_mappings", ")", "\n", "source_vocab", ".", "add_words", "(", "iwordv", ")", "\n", "target_vocab", ".", "add_words", "(", "owordv", ")", "\n", "tweet", ".", "set_input", "(", "iwordv", ")", "\n", "tweet", ".", "set_output", "(", "owordv", ")", "\n", "word_tweets", ".", "append", "(", "copy", ".", "deepcopy", "(", "tweet", ")", ")", "\n", "", "", "", "", "", "", "else", ":", "\n", "                ", "inp_i", ",", "inp_o", "=", "self", ".", "vector_repr", "(", "inp_i", ",", "inp_o", ",", "update_mappings", ")", "\n", "source_vocab", ".", "add_words", "(", "inp_i", ")", "\n", "target_vocab", ".", "add_words", "(", "inp_o", ")", "\n", "tweet", ".", "set_input", "(", "inp_i", ")", "\n", "tweet", ".", "set_output", "(", "inp_o", ")", "\n", "word_tweets", ".", "append", "(", "copy", ".", "deepcopy", "(", "tweet", ")", ")", "\n", "\n", "\n", "", "", "tweets", "=", "word_tweets", "\n", "if", "(", "self", ".", "opt", ".", "input", "==", "'spelling'", ")", ":", "\n", "            ", "same_tw", ",", "diff_tw", "=", "[", "]", ",", "[", "]", "\n", "for", "tweet", "in", "tweets", ":", "\n", "                ", "if", "tweet", ".", "input", "==", "tweet", ".", "output", ":", "\n", "                    ", "same_tw", ".", "append", "(", "tweet", ")", "\n", "", "else", ":", "\n", "                    ", "diff_tw", ".", "append", "(", "tweet", ")", "\n", "\n", "", "", "", "source_vocab", ".", "makeVocabulary", "(", "self", ".", "opt", ".", "vocab_size", ")", "\n", "source_vocab", ".", "makeLabelToIdx", "(", ")", "\n", "target_vocab", ".", "makeVocabulary", "(", "self", ".", "opt", ".", "vocab_size", ")", "\n", "target_vocab", ".", "makeLabelToIdx", "(", ")", "\n", "if", "(", "self", ".", "opt", ".", "share_vocab", ")", ":", "\n", "            ", "assert", "source_vocab", ".", "idx_to_label", "==", "target_vocab", ".", "idx_to_label", "\n", "", "return", "tweets", ",", "source_vocab", ",", "target_vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.DataLoader.add_noise": [[137, 180], ["random.randint", "random.randint", "word.find", "word.find", "word.find", "word.find", "word.find", "max", "word.find", "word.find", "word.find", "len", "random.choice", "random.randint", "random.randint"], "methods", ["None"], ["", "def", "add_noise", "(", "self", ",", "word", ")", ":", "\n", "        ", "\"\"\"\n            There are 7 kinds of errors we can introduce for data aug:\n            0) forget to \"type\" a char\n            1) swap the placement of two chars\n            2) if the word ends in u, y, s, r, extend the last char\n            3) if vowel in sentence extend vowel (o, u, e, a, i)\n            4-6) misplaced or missing \" ' \"\n            7-10) keyboard errors\n        \"\"\"", "\n", "i", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "word", ")", "-", "1", ")", "\n", "op", "=", "random", ".", "randint", "(", "0", ",", "10", ")", "\n", "if", "op", "==", "0", ":", "\n", "            ", "return", "word", "[", ":", "i", "]", "+", "word", "[", "i", "+", "1", ":", "]", "\n", "", "if", "op", "==", "1", ":", "\n", "            ", "i", "+=", "1", "\n", "return", "word", "[", ":", "i", "-", "1", "]", "+", "word", "[", "i", ":", "i", "+", "1", "]", "+", "word", "[", "i", "-", "1", ":", "i", "]", "+", "word", "[", "i", "+", "1", ":", "]", "\n", "", "if", "op", "==", "2", ":", "\n", "            ", "l", "=", "word", "[", ":", "-", "1", "]", "\n", "if", "l", "==", "'u'", "or", "l", "==", "'y'", "or", "l", "==", "'s'", "or", "l", "==", "'r'", "or", "l", "==", "'a'", "or", "l", "==", "'o'", "or", "l", "==", "'i'", ":", "\n", "                ", "return", "word", "+", "random", ".", "randint", "(", "1", ",", "5", ")", "*", "l", "\n", "", "", "if", "op", "==", "3", ":", "\n", "            ", "a", "=", "word", ".", "find", "(", "'a'", ")", "\n", "e", "=", "word", ".", "find", "(", "'e'", ")", "\n", "i", "=", "word", ".", "find", "(", "'i'", ")", "\n", "o", "=", "word", ".", "find", "(", "'o'", ")", "\n", "u", "=", "word", ".", "find", "(", "'u'", ")", "\n", "idx", "=", "max", "(", "[", "a", ",", "e", ",", "i", ",", "o", ",", "u", "]", ")", "\n", "if", "idx", "!=", "-", "1", ":", "\n", "                ", "return", "word", "[", ":", "idx", "]", "+", "random", ".", "randint", "(", "1", ",", "5", ")", "*", "word", "[", "idx", "]", "+", "word", "[", "idx", ":", "]", "\n", "", "", "if", "op", "==", "4", ":", "\n", "            ", "idx", "=", "word", ".", "find", "(", "\"'\"", ")", "\n", "if", "idx", "!=", "-", "1", ":", "\n", "                ", "return", "word", "[", ":", "idx", "]", "+", "word", "[", "idx", "+", "1", ":", "]", "+", "word", "[", "idx", "]", "\n", "", "", "if", "op", "==", "5", ":", "\n", "            ", "idx", "=", "word", ".", "find", "(", "\"'\"", ")", "\n", "if", "idx", "!=", "-", "1", ":", "\n", "                ", "return", "word", "[", ":", "idx", "-", "1", "]", "+", "word", "[", "idx", ":", "idx", "+", "1", "]", "+", "word", "[", "idx", "-", "1", ":", "idx", "]", "+", "word", "[", "idx", "+", "1", ":", "]", "\n", "", "", "if", "op", "==", "6", ":", "\n", "            ", "idx", "=", "word", ".", "find", "(", "\"'\"", ")", "\n", "if", "idx", "!=", "-", "1", ":", "\n", "                ", "return", "word", "[", ":", "idx", "]", "+", "word", "[", "idx", "+", "1", ":", "]", "\n", "", "", "return", "word", "[", ":", "i", "]", "+", "random", ".", "choice", "(", "self", ".", "prox_arr", "[", "word", "[", "i", "]", "]", ")", "+", "word", "[", "i", "+", "1", ":", "]", "#default is keyboard errors", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.DataLoader.get_prox_keys": [[181, 220], ["None"], "methods", ["None"], ["", "def", "get_prox_keys", "(", "self", ")", ":", "\n", "        ", "array_prox", "=", "{", "}", "\n", "array_prox", "[", "'a'", "]", "=", "[", "'q'", ",", "'w'", ",", "'z'", ",", "'x'", ",", "'s'", "]", "\n", "array_prox", "[", "'b'", "]", "=", "[", "'v'", ",", "'f'", ",", "'g'", ",", "'h'", ",", "'n'", ",", "' '", "]", "\n", "array_prox", "[", "'c'", "]", "=", "[", "'x'", ",", "'s'", ",", "'d'", ",", "'f'", ",", "'v'", "]", "\n", "array_prox", "[", "'d'", "]", "=", "[", "'x'", ",", "'s'", ",", "'w'", ",", "'e'", ",", "'r'", ",", "'f'", ",", "'v'", ",", "'c'", "]", "\n", "array_prox", "[", "'e'", "]", "=", "[", "'w'", ",", "'s'", ",", "'d'", ",", "'f'", ",", "'r'", "]", "\n", "array_prox", "[", "'f'", "]", "=", "[", "'c'", ",", "'d'", ",", "'e'", ",", "'r'", ",", "'t'", ",", "'g'", ",", "'b'", ",", "'v'", "]", "\n", "array_prox", "[", "'g'", "]", "=", "[", "'r'", ",", "'f'", ",", "'v'", ",", "'t'", ",", "'b'", ",", "'y'", ",", "'h'", ",", "'n'", "]", "\n", "array_prox", "[", "'h'", "]", "=", "[", "'b'", ",", "'g'", ",", "'t'", ",", "'y'", ",", "'u'", ",", "'j'", ",", "'m'", ",", "'n'", "]", "\n", "array_prox", "[", "'i'", "]", "=", "[", "'u'", ",", "'j'", ",", "'k'", ",", "'l'", ",", "'o'", "]", "\n", "array_prox", "[", "'j'", "]", "=", "[", "'n'", ",", "'h'", ",", "'y'", ",", "'u'", ",", "'i'", ",", "'k'", ",", "'m'", "]", "\n", "array_prox", "[", "'k'", "]", "=", "[", "'u'", ",", "'j'", ",", "'m'", ",", "'l'", ",", "'o'", "]", "\n", "array_prox", "[", "'l'", "]", "=", "[", "'p'", ",", "'o'", ",", "'i'", ",", "'k'", ",", "'m'", "]", "\n", "array_prox", "[", "'m'", "]", "=", "[", "'n'", ",", "'h'", ",", "'j'", ",", "'k'", ",", "'l'", "]", "\n", "array_prox", "[", "'n'", "]", "=", "[", "'b'", ",", "'g'", ",", "'h'", ",", "'j'", ",", "'m'", "]", "\n", "array_prox", "[", "'o'", "]", "=", "[", "'i'", ",", "'k'", ",", "'l'", ",", "'p'", "]", "\n", "array_prox", "[", "'p'", "]", "=", "[", "'o'", ",", "'l'", "]", "\n", "array_prox", "[", "'q'", "]", "=", "[", "'w'", ",", "'a'", "]", "\n", "array_prox", "[", "'r'", "]", "=", "[", "'e'", ",", "'d'", ",", "'f'", ",", "'g'", ",", "'t'", "]", "\n", "array_prox", "[", "'s'", "]", "=", "[", "'q'", ",", "'w'", ",", "'e'", ",", "'z'", ",", "'x'", ",", "'c'", "]", "\n", "array_prox", "[", "'t'", "]", "=", "[", "'r'", ",", "'f'", ",", "'g'", ",", "'h'", ",", "'y'", "]", "\n", "array_prox", "[", "'u'", "]", "=", "[", "'y'", ",", "'h'", ",", "'j'", ",", "'k'", ",", "'i'", "]", "\n", "array_prox", "[", "'v'", "]", "=", "[", "''", ",", "'c'", ",", "'d'", ",", "'f'", ",", "'g'", ",", "'b'", "]", "\n", "array_prox", "[", "'w'", "]", "=", "[", "'q'", ",", "'a'", ",", "'s'", ",", "'d'", ",", "'e'", "]", "\n", "array_prox", "[", "'x'", "]", "=", "[", "'z'", ",", "'a'", ",", "'s'", ",", "'d'", ",", "'c'", "]", "\n", "array_prox", "[", "'y'", "]", "=", "[", "'t'", ",", "'g'", ",", "'h'", ",", "'j'", ",", "'u'", "]", "\n", "array_prox", "[", "'z'", "]", "=", "[", "'x'", ",", "'s'", ",", "'a'", "]", "\n", "array_prox", "[", "'1'", "]", "=", "[", "'q'", ",", "'w'", "]", "\n", "array_prox", "[", "'2'", "]", "=", "[", "'q'", ",", "'w'", ",", "'e'", "]", "\n", "array_prox", "[", "'3'", "]", "=", "[", "'w'", ",", "'e'", ",", "'r'", "]", "\n", "array_prox", "[", "'4'", "]", "=", "[", "'e'", ",", "'r'", ",", "'t'", "]", "\n", "array_prox", "[", "'5'", "]", "=", "[", "'r'", ",", "'t'", ",", "'y'", "]", "\n", "array_prox", "[", "'6'", "]", "=", "[", "'t'", ",", "'y'", ",", "'u'", "]", "\n", "array_prox", "[", "'7'", "]", "=", "[", "'y'", ",", "'u'", ",", "'i'", "]", "\n", "array_prox", "[", "'8'", "]", "=", "[", "'u'", ",", "'i'", ",", "'o'", "]", "\n", "array_prox", "[", "'9'", "]", "=", "[", "'i'", ",", "'o'", ",", "'p'", "]", "\n", "array_prox", "[", "'0'", "]", "=", "[", "'o'", ",", "'p'", "]", "\n", "return", "array_prox", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.create_data": [[223, 229], ["DataLoader.DataLoader"], "function", ["None"], ["", "", "def", "create_data", "(", "data", ",", "opt", ",", "vocab", "=", "None", ",", "mappings", "=", "None", ")", ":", "\n", "    ", "dataload", "=", "DataLoader", "(", "data", ",", "vocab", "=", "vocab", ",", "mappings", "=", "mappings", ",", "opt", "=", "opt", ")", "\n", "vocab", "=", "{", "}", "\n", "vocab", "[", "'src'", "]", "=", "dataload", ".", "source_vocab", "\n", "vocab", "[", "'tgt'", "]", "=", "dataload", ".", "target_vocab", "\n", "return", "dataload", ".", "ret", ",", "vocab", ",", "dataload", ".", "mappings", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.create_datasets": [[231, 239], ["DataLoader.read_file", "DataLoader.read_file", "DataLoader.create_data", "DataLoader.create_data", "DataLoader.create_data"], "function", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.read_file", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.read_file", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.create_data", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.create_data", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.create_data"], ["", "def", "create_datasets", "(", "opt", ")", ":", "\n", "    ", "train", ",", "val", "=", "read_file", "(", "opt", ".", "traindata", ",", "opt", ".", "valsplit", ")", "\n", "test", ",", "_", "=", "read_file", "(", "opt", ".", "testdata", ")", "\n", "train_data", ",", "vocab", ",", "mappings", "=", "create_data", "(", "train", ",", "opt", "=", "opt", ")", "\n", "if", "val", ":", "val_data", ",", "val_vocab", ",", "mappings", "=", "create_data", "(", "val", ",", "opt", "=", "opt", ",", "vocab", "=", "vocab", ",", "mappings", "=", "mappings", ")", "\n", "else", ":", "val_data", ",", "val_vocab", ",", "mappings", "=", "train_data", ",", "vocab", ",", "mappings", "\n", "test_data", ",", "test_vocab", ",", "mappings", "=", "create_data", "(", "test", ",", "opt", "=", "opt", ",", "vocab", "=", "vocab", ",", "mappings", "=", "mappings", ")", "\n", "return", "train_data", ",", "val_data", ",", "test_data", ",", "vocab", ",", "mappings", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.DataLoader.read_file": [[241, 257], ["open", "json.load", "tweets.append", "random.shuffle", "Tweet.Tweet"], "function", ["None"], ["", "def", "read_file", "(", "fn", ",", "valsplit", "=", "None", ")", ":", "\n", "    ", "tweets", "=", "[", "]", "\n", "with", "open", "(", "fn", ",", "'r'", ")", "as", "json_data", ":", "\n", "        ", "data", "=", "json", ".", "load", "(", "json_data", ")", "\n", "", "for", "tweet", "in", "data", ":", "\n", "        ", "src_tweet", "=", "tweet", "[", "'input'", "]", "\n", "tgt_tweet", "=", "tweet", "[", "'output'", "]", "\n", "ind", "=", "tweet", "[", "'index'", "]", "\n", "tid", "=", "tweet", "[", "'tid'", "]", "\n", "tweets", ".", "append", "(", "Tweet", "(", "src_tweet", ",", "tgt_tweet", ",", "tid", ",", "ind", ")", ")", "\n", "", "if", "(", "valsplit", ")", ":", "\n", "        ", "random", ".", "shuffle", "(", "tweets", ")", "\n", "val", "=", "tweets", "[", ":", "valsplit", "]", "\n", "train", "=", "tweets", "[", "valsplit", ":", "]", "\n", "return", "train", ",", "val", "\n", "", "return", "tweets", ",", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.__init__": [[6, 13], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "vocab_size", ",", "bosWord", "=", "None", ",", "eosWord", "=", "None", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "[", "]", "\n", "self", ".", "vocab_counts", "=", "None", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "bosWord", "=", "bosWord", "\n", "self", ".", "eosWord", "=", "eosWord", "\n", "self", ".", "unkown_words", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.size": [[14, 17], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "size", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "label_to_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.__len__": [[18, 20], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "label_to_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.add_words": [[21, 24], ["Dict.Dict.vocab.append"], "methods", ["None"], ["", "def", "add_words", "(", "self", ",", "sequence", ")", ":", "\n", "        ", "for", "word", "in", "sequence", ":", "\n", "            ", "self", ".", "vocab", ".", "append", "(", "word", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.makeVocabulary": [[25, 33], ["collections.Counter", "collections.Counter", "Dict.Dict.prune", "Dict.Dict.vocab.append", "Dict.Dict.vocab.append", "Dict.Dict.vocab.append", "Dict.Dict.vocab.append"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.prune"], ["", "", "def", "makeVocabulary", "(", "self", ",", "vocab_size", "=", "None", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "Counter", "(", "self", ".", "vocab", ")", "\n", "self", ".", "vocab_counts", "=", "Counter", "(", "self", ".", "vocab", ")", "\n", "self", ".", "vocab", "=", "self", ".", "prune", "(", "vocab_size", ")", "\n", "self", ".", "vocab", ".", "append", "(", "PAD_WORD", ")", "\n", "self", ".", "vocab", ".", "append", "(", "UNK_WORD", ")", "\n", "if", "(", "self", ".", "bosWord", ")", ":", "self", ".", "vocab", ".", "append", "(", "BOS_WORD", ")", "\n", "if", "(", "self", ".", "eosWord", ")", ":", "self", ".", "vocab", ".", "append", "(", "EOS_WORD", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.makeLabelToIdx": [[35, 52], ["len", "len"], "methods", ["None"], ["", "def", "makeLabelToIdx", "(", "self", ")", ":", "\n", "        ", "self", ".", "label_to_idx", "=", "{", "PAD_WORD", ":", "PAD", ",", "UNK_WORD", ":", "UNK", "}", "\n", "self", ".", "idx_to_label", "=", "{", "PAD", ":", "PAD_WORD", ",", "UNK", ":", "UNK_WORD", "}", "\n", "if", "(", "self", ".", "bosWord", ")", ":", "\n", "            ", "self", ".", "bosWord", "=", "BOS_WORD", "\n", "self", ".", "label_to_idx", "[", "BOS_WORD", "]", "=", "BOS", "\n", "self", ".", "idx_to_label", "[", "BOS", "]", "=", "BOS_WORD", "\n", "", "if", "(", "self", ".", "eosWord", ")", ":", "\n", "            ", "self", ".", "eosWord", "=", "EOS_WORD", "\n", "self", ".", "label_to_idx", "[", "EOS_WORD", "]", "=", "EOS", "\n", "self", ".", "idx_to_label", "[", "EOS", "]", "=", "EOS_WORD", "\n", "", "for", "item", "in", "self", ".", "vocab", ":", "\n", "            ", "if", "(", "item", "not", "in", "self", ".", "label_to_idx", ")", ":", "\n", "                ", "self", ".", "label_to_idx", "[", "item", "]", "=", "len", "(", "self", ".", "label_to_idx", ")", "\n", "self", ".", "idx_to_label", "[", "len", "(", "self", ".", "idx_to_label", ")", "]", "=", "item", "\n", "#TODO: bug when EOS is used and BOS is not used!", "\n", "assert", "item", "==", "self", ".", "idx_to_label", "[", "self", ".", "label_to_idx", "[", "item", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.prune": [[54, 62], ["Dict.Dict.vocab.most_common", "Dict.Dict.vocab_counts.most_common", "sorted", "sorted", "len"], "methods", ["None"], ["", "", "", "def", "prune", "(", "self", ",", "vocab_size", "=", "None", ")", ":", "\n", "        ", "if", "(", "vocab_size", "is", "None", ")", ":", "vocab_size", "=", "-", "1", "\n", "if", "vocab_size", ">=", "len", "(", "self", ".", "vocab", ")", "or", "(", "vocab_size", "==", "-", "1", ")", ":", "\n", "            ", "return", "sorted", "(", "self", ".", "vocab", ",", "key", "=", "self", ".", "vocab", ".", "get", ",", "reverse", "=", "True", ")", "\n", "", "newvocab", "=", "self", ".", "vocab", ".", "most_common", "(", "vocab_size", ")", "\n", "self", ".", "vocab_counts", "=", "self", ".", "vocab_counts", ".", "most_common", "(", "vocab_size", ")", "\n", "# Only keep the `size` most frequent entries.", "\n", "return", "sorted", "(", "newvocab", ",", "key", "=", "newvocab", ".", "get", ",", "reverse", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.stoi": [[63, 69], ["Dict.Dict.unkown_words.append"], "methods", ["None"], ["", "def", "stoi", "(", "self", ",", "label", ",", "default", "=", "None", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "self", ".", "label_to_idx", "[", "label", "]", "\n", "", "except", "KeyError", ":", "\n", "            ", "self", ".", "unkown_words", ".", "append", "(", "label", ")", "\n", "return", "default", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.itos": [[70, 75], ["None"], "methods", ["None"], ["", "", "def", "itos", "(", "self", ",", "idx", ",", "default", "=", "None", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "self", ".", "idx_to_label", "[", "idx", "]", "\n", "", "except", "KeyError", ":", "\n", "            ", "return", "default", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.to_indices": [[76, 85], ["Dict.Dict.stoi", "torch.LongTensor", "Dict.Dict.stoi", "Dict.Dict.stoi", "Dict.Dict.stoi"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.stoi", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.stoi", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.stoi", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.stoi"], ["", "", "def", "to_indices", "(", "self", ",", "labels", ",", "bosWord", "=", "False", ",", "eosWord", "=", "False", ")", ":", "\n", "        ", "vec", "=", "[", "]", "\n", "if", "bosWord", ":", "\n", "            ", "vec", "+=", "[", "self", ".", "stoi", "(", "BOS_WORD", ")", "]", "\n", "", "unk", "=", "self", ".", "stoi", "(", "UNK_WORD", ")", "\n", "vec", "+=", "[", "self", ".", "stoi", "(", "label", ",", "default", "=", "unk", ")", "for", "label", "in", "labels", "]", "\n", "if", "eosWord", ":", "\n", "            ", "vec", "+=", "[", "self", ".", "stoi", "(", "EOS_WORD", ")", "]", "\n", "", "return", "torch", ".", "LongTensor", "(", "vec", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.to_labels": [[86, 93], ["Dict.Dict.itos"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.itos"], ["", "def", "to_labels", "(", "self", ",", "idx", ",", "stop", ")", ":", "\n", "        ", "labels", "=", "[", "]", "\n", "for", "i", "in", "idx", ":", "\n", "            ", "labels", "+=", "[", "self", ".", "itos", "(", "i", ")", "]", "\n", "if", "i", "==", "stop", ":", "\n", "                ", "break", "\n", "", "", "return", "labels", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dataset.Dataset.__init__": [[6, 14], ["data.keys", "len", "setattr"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "data", ",", "opt", ")", ":", "\n", "        ", "self", ".", "DATA_KEYS", "=", "data", ".", "keys", "(", ")", "\n", "self", ".", "TENSOR_KEYS", "=", "[", "'src'", ",", "'tgt'", "]", "\n", "for", "key", "in", "self", ".", "DATA_KEYS", ":", "\n", "            ", "setattr", "(", "self", ",", "key", ",", "data", "[", "key", "]", ")", "\n", "", "self", ".", "opt", "=", "opt", "\n", "self", ".", "size", "=", "len", "(", "self", ".", "src", ")", "\n", "self", ".", "num_batches", "=", "(", "self", ".", "size", "+", "self", ".", "opt", ".", "batch_size", "-", "1", ")", "//", "self", ".", "opt", ".", "batch_size", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dataset.Dataset.__len__": [[15, 17], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num_batches", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dataset.Dataset._to_tensor": [[18, 31], ["max", "data[].new().fill_", "range", "out.cuda.cuda.t_().contiguous", "torch.autograd.Variable", "torch.autograd.Variable", "x.size", "len", "data[].size", "out[].narrow().copy_", "out.cuda.cuda.cuda", "torch.LongTensor", "lens.cuda.cuda.cuda", "data[].new", "out.cuda.cuda.t_", "len", "out[].narrow"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.size", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.size"], ["", "def", "_to_tensor", "(", "self", ",", "data", ",", "return_lens", ")", ":", "\n", "        ", "lens", "=", "[", "x", ".", "size", "(", "0", ")", "for", "x", "in", "data", "]", "\n", "max_length", "=", "max", "(", "lens", ")", "\n", "out", "=", "data", "[", "0", "]", ".", "new", "(", "len", "(", "data", ")", ",", "max_length", ")", ".", "fill_", "(", "lib", ".", "constants", ".", "PAD", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "            ", "data_length", "=", "data", "[", "i", "]", ".", "size", "(", "0", ")", "\n", "out", "[", "i", "]", ".", "narrow", "(", "0", ",", "0", ",", "data_length", ")", ".", "copy_", "(", "data", "[", "i", "]", ")", "\n", "", "out", "=", "out", ".", "t_", "(", ")", ".", "contiguous", "(", ")", "\n", "if", "self", ".", "opt", ".", "cuda", ":", "out", "=", "out", ".", "cuda", "(", ")", "\n", "v", "=", "Variable", "(", "out", ")", "\n", "lens", "=", "Variable", "(", "torch", ".", "LongTensor", "(", "lens", ")", ")", "\n", "if", "self", ".", "opt", ".", "cuda", ":", "lens", "=", "lens", ".", "cuda", "(", ")", "\n", "return", "(", "v", ",", "lens", ")", "if", "return_lens", "else", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dataset.Dataset.batches": [[32, 48], ["range", "list().index", "zip", "list", "zip", "len", "list", "getattr", "sorted", "Dataset.Dataset._to_tensor", "list", "zip", "x[].size"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dataset.Dataset._to_tensor", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Dict.Dict.size"], ["", "def", "batches", "(", "self", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "self", ".", "num_batches", ")", ":", "\n", "            ", "s_idx", "=", "i", "*", "self", ".", "opt", ".", "batch_size", "\n", "e_idx", "=", "(", "i", "+", "1", ")", "*", "self", ".", "opt", ".", "batch_size", "\n", "src_idx_in_data_keys", "=", "list", "(", "self", ".", "DATA_KEYS", ")", ".", "index", "(", "'src'", ")", "\n", "value_lists", "=", "[", "getattr", "(", "self", ",", "key", ")", "[", "s_idx", ":", "e_idx", "]", "for", "key", "in", "self", ".", "DATA_KEYS", "]", "\n", "sorted_value_lists", "=", "zip", "(", "*", "sorted", "(", "list", "(", "zip", "(", "*", "value_lists", ")", ")", ",", "\n", "key", "=", "lambda", "x", ":", "-", "x", "[", "src_idx_in_data_keys", "]", ".", "size", "(", "0", ")", ")", ")", "\n", "sorted_value_lists", "=", "list", "(", "sorted_value_lists", ")", "\n", "batch", "=", "{", "}", "\n", "for", "key", ",", "value", "in", "zip", "(", "self", ".", "DATA_KEYS", ",", "sorted_value_lists", ")", ":", "\n", "                ", "batch", "[", "key", "]", "=", "value", "\n", "if", "key", "in", "self", ".", "TENSOR_KEYS", ":", "\n", "                    ", "batch", "[", "key", "]", "=", "self", ".", "_to_tensor", "(", "value", ",", "return_lens", "=", "True", ")", "\n", "", "", "batch", "[", "'size'", "]", "=", "len", "(", "batch", "[", "'pos'", "]", ")", "\n", "yield", "batch", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Tweet.__init__": [[5, 12], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "input", ",", "output", ",", "tid", ",", "ind", ",", "inputidx", "=", "None", ",", "outputidx", "=", "None", ")", ":", "\n", "        ", "self", ".", "input", "=", "input", "\n", "self", ".", "output", "=", "output", "\n", "self", ".", "tid", "=", "tid", "\n", "self", ".", "ind", "=", "ind", "\n", "self", ".", "inputidx", "=", "inputidx", "\n", "self", ".", "outputidx", "=", "outputidx", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Tweet.__repr__": [[13, 15], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "\"{}/{}:{}->{}\"", ".", "format", "(", "self", ".", "ind", ",", "self", ".", "tid", ",", "self", ".", "input", ",", "self", ".", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Tweet.set_inputidx": [[16, 18], ["None"], "methods", ["None"], ["", "def", "set_inputidx", "(", "self", ",", "inputidx", ")", ":", "\n", "        ", "self", ".", "inputidx", "=", "inputidx", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Tweet.set_outputidx": [[19, 21], ["None"], "methods", ["None"], ["", "def", "set_outputidx", "(", "self", ",", "outputidx", ")", ":", "\n", "        ", "self", ".", "outputidx", "=", "outputidx", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Tweet.set_input": [[22, 24], ["None"], "methods", ["None"], ["", "def", "set_input", "(", "self", ",", "input", ")", ":", "\n", "        ", "self", ".", "input", "=", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Tweet.set_output": [[25, 27], ["None"], "methods", ["None"], ["", "def", "set_output", "(", "self", ",", "output", ")", ":", "\n", "        ", "self", ".", "output", "=", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Preprocessor.__init__": [[29, 32], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "tokens", "=", "[", "]", "\n", "self", ".", "positions", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Preprocessor.lowercase": [[33, 36], ["x.lower"], "methods", ["None"], ["", "def", "lowercase", "(", "self", ")", ":", "\n", "        ", "self", ".", "tokens", "=", "[", "x", ".", "lower", "(", ")", "for", "x", "in", "self", ".", "tokens", "]", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Preprocessor.isUrl": [[37, 50], ["re.compile", "re.match"], "methods", ["None"], ["", "def", "isUrl", "(", "self", ",", "token", ")", ":", "\n", "        ", "regex", "=", "re", ".", "compile", "(", "\n", "r'^(?:http|ftp)s?://'", "# http:// or https://", "\n", "r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|'", "#domain...", "\n", "r'localhost|'", "#localhost...", "\n", "r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})'", "# ...or ip", "\n", "r'(?::\\d+)?'", "# optional port", "\n", "r'(?:/?|[/?]\\S+)$'", ",", "re", ".", "IGNORECASE", ")", "\n", "match", "=", "re", ".", "match", "(", "regex", ",", "token", ")", "\n", "if", "match", "is", "None", ":", "\n", "            ", "return", "False", "\n", "", "else", ":", "\n", "            ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Preprocessor.filter": [[51, 65], ["enumerate", "Tweet.Preprocessor.isUrl", "filtered.append", "token.startswith", "filtered.append", "token.startswith", "filtered.append", "filtered.append", "Tweet.Preprocessor.positions.append"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Preprocessor.isUrl"], ["", "", "def", "filter", "(", "self", ")", ":", "\n", "        ", "filtered", "=", "[", "]", "\n", "for", "pos", ",", "token", "in", "enumerate", "(", "self", ".", "tokens", ")", ":", "\n", "            ", "if", "self", ".", "isUrl", "(", "token", ")", ":", "\n", "                ", "filtered", ".", "append", "(", "lib", ".", "constants", ".", "URL", ")", "\n", "", "elif", "token", ".", "startswith", "(", "'#'", ")", ":", "\n", "                ", "filtered", ".", "append", "(", "lib", ".", "constants", ".", "HASH", ")", "\n", "", "elif", "token", ".", "startswith", "(", "'@'", ")", ":", "\n", "                ", "filtered", ".", "append", "(", "lib", ".", "constants", ".", "MENTION", ")", "\n", "", "else", ":", "\n", "                ", "filtered", ".", "append", "(", "token", ")", "\n", "self", ".", "positions", ".", "append", "(", "pos", ")", "\n", "", "", "self", ".", "tokens", "=", "filtered", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Preprocessor.run": [[66, 73], ["Tweet.Preprocessor.filter", "Tweet.Preprocessor.lowercase"], "methods", ["home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Preprocessor.filter", "home.repos.pwc.inspect_result.Isminoula_TextNormSeq2Seq.data.Tweet.Preprocessor.lowercase"], ["", "def", "run", "(", "self", ",", "tokens", ",", "lowercase", "=", "False", ")", ":", "\n", "        ", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "positions", "=", "[", "]", "\n", "if", "(", "lowercase", ")", ":", "\n", "            ", "self", ".", "lowercase", "(", ")", "\n", "", "self", ".", "filter", "(", ")", "\n", "return", "self", ".", "tokens", ",", "self", ".", "positions", "", "", "", ""]]}