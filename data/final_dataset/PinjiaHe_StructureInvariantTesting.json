{"home.repos.pwc.inspect_result.PinjiaHe_StructureInvariantTesting.None.SIT.bingtranslate": [[18, 41], ["requests.post", "requests.post.json", "str", "type", "uuid.uuid4"], "function", ["None"], ["def", "bingtranslate", "(", "api_key", ",", "text", ",", "language_from", ",", "language_to", ")", ":", "\n", "# If you encounter any issues with the base_url or path, make sure", "\n", "# that you are using the latest endpoint: https://docs.microsoft.com/azure/cognitive-services/translator/reference/v3-0-translate", "\n", "\t", "base_url", "=", "'https://api.cognitive.microsofttranslator.com'", "\n", "path", "=", "'/translate?api-version=3.0'", "\n", "params", "=", "'&language='", "+", "language_from", "+", "'&to='", "+", "language_to", "\n", "constructed_url", "=", "base_url", "+", "path", "+", "params", "\n", "\n", "headers", "=", "{", "\n", "'Ocp-Apim-Subscription-Key'", ":", "api_key", ",", "\n", "'Content-type'", ":", "'application/json'", ",", "\n", "'X-ClientTraceId'", ":", "str", "(", "uuid", ".", "uuid4", "(", ")", ")", "\n", "}", "\n", "if", "type", "(", "text", ")", "is", "str", ":", "\n", "\t\t", "text", "=", "[", "text", "]", "\n", "\n", "", "body", "=", "[", "{", "'text'", ":", "x", "}", "for", "x", "in", "text", "]", "\n", "# You can pass more than one object in body.", "\n", "\n", "request", "=", "requests", ".", "post", "(", "constructed_url", ",", "headers", "=", "headers", ",", "json", "=", "body", ")", "\n", "response", "=", "request", ".", "json", "(", ")", "\n", "\n", "return", "[", "i", "[", "\"translations\"", "]", "[", "0", "]", "[", "\"text\"", "]", "for", "i", "in", "response", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.PinjiaHe_StructureInvariantTesting.None.SIT.perturb": [[44, 68], ["tokenizer.tokenize", "nltk.tag.pos_tag", "list", "enumerate", "list", "SIT.perturbBert", "tag.startswith", "tag.startswith", "list.append", "len"], "function", ["home.repos.pwc.inspect_result.PinjiaHe_StructureInvariantTesting.None.SIT.perturbBert"], ["", "def", "perturb", "(", "sent", ",", "bertmodel", ",", "num", ")", ":", "\n", "\t", "tokens", "=", "tokenizer", ".", "tokenize", "(", "sent", ")", "\n", "pos_inf", "=", "nltk", ".", "tag", ".", "pos_tag", "(", "tokens", ")", "\n", "\n", "# the elements in the lists are tuples <index of token, pos tag of token>", "\n", "bert_masked_indexL", "=", "list", "(", ")", "\n", "\n", "# collect the token index for substitution", "\n", "for", "idx", ",", "(", "word", ",", "tag", ")", "in", "enumerate", "(", "pos_inf", ")", ":", "\n", "# substitute the nouns and adjectives; you could easily substitue more words by modifying the code here", "\n", "\t\t", "if", "(", "tag", ".", "startswith", "(", "'NN'", ")", "or", "tag", ".", "startswith", "(", "'JJ'", ")", ")", ":", "\n", "\t\t\t", "tagFlag", "=", "tag", "[", ":", "2", "]", "\n", "\n", "# we do not perturb the first and the last token because BERT's performance drops on for those positions", "\n", "if", "(", "idx", "!=", "0", "and", "idx", "!=", "len", "(", "tokens", ")", "-", "1", ")", ":", "\n", "\t\t\t\t", "bert_masked_indexL", ".", "append", "(", "(", "idx", ",", "tagFlag", ")", ")", "\n", "\n", "", "", "", "bert_new_sentences", "=", "list", "(", ")", "\n", "\n", "# generate similar setences using Bert", "\n", "if", "bert_masked_indexL", ":", "\n", "\t\t", "bert_new_sentences", "=", "perturbBert", "(", "sent", ",", "bertmodel", ",", "num", ",", "bert_masked_indexL", ")", "\n", "\n", "", "return", "bert_new_sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.PinjiaHe_StructureInvariantTesting.None.SIT.perturbBert": [[70, 118], ["list", "tokenizer.tokenize", "set", "[].tolist", "berttokenizer.convert_ids_to_tokens", "list", "x.lower", "berttokenizer.convert_tokens_to_ids", "torch.tensor", "bertmodel", "filter", "any", "nltk.tag.pos_tag", "[].startswith", "print", "detokenizer.detokenize", "list.append", "torch.topk", "len"], "function", ["None"], ["", "def", "perturbBert", "(", "sent", ",", "bertmodel", ",", "num", ",", "masked_indexL", ")", ":", "\n", "\t", "new_sentences", "=", "list", "(", ")", "\n", "tokens", "=", "tokenizer", ".", "tokenize", "(", "sent", ")", "\n", "\n", "invalidChars", "=", "set", "(", "string", ".", "punctuation", ")", "\n", "\n", "# for each idx, use Bert to generate k (i.e., num) candidate tokens", "\n", "for", "(", "masked_index", ",", "tagFlag", ")", "in", "masked_indexL", ":", "\n", "\t\t", "original_word", "=", "tokens", "[", "masked_index", "]", "\n", "\n", "low_tokens", "=", "[", "x", ".", "lower", "(", ")", "for", "x", "in", "tokens", "]", "\n", "low_tokens", "[", "masked_index", "]", "=", "'[MASK]'", "\n", "\n", "# try whether all the tokens are in the vocabulary", "\n", "try", ":", "\n", "\t\t\t", "indexed_tokens", "=", "berttokenizer", ".", "convert_tokens_to_ids", "(", "low_tokens", ")", "\n", "tokens_tensor", "=", "torch", ".", "tensor", "(", "[", "indexed_tokens", "]", ")", "\n", "prediction", "=", "bertmodel", "(", "tokens_tensor", ")", "\n", "\n", "# skip the sentences that contain unknown words", "\n", "# another option is to mark the unknow words as [MASK]; we skip sentences to reduce fp caused by BERT", "\n", "", "except", "KeyError", "as", "error", ":", "\n", "\t\t\t", "print", "(", "'skip a sentence. unknown token is %s'", "%", "error", ")", "\n", "break", "\n", "\n", "# get the similar words", "\n", "", "topk_Idx", "=", "torch", ".", "topk", "(", "prediction", "[", "0", ",", "masked_index", "]", ",", "num", ")", "[", "1", "]", ".", "tolist", "(", ")", "\n", "topk_tokens", "=", "berttokenizer", ".", "convert_ids_to_tokens", "(", "topk_Idx", ")", "\n", "\n", "# remove the tokens that only contains 0 or 1 char (e.g., i, a, s)", "\n", "# this step could be further optimized by filtering more tokens (e.g., non-english tokens)", "\n", "topk_tokens", "=", "list", "(", "filter", "(", "lambda", "x", ":", "len", "(", "x", ")", ">", "1", ",", "topk_tokens", ")", ")", "\n", "\n", "# generate similar sentences", "\n", "for", "t", "in", "topk_tokens", ":", "\n", "\t\t\t", "if", "any", "(", "char", "in", "invalidChars", "for", "char", "in", "t", ")", ":", "\n", "\t\t\t\t", "continue", "\n", "", "tokens", "[", "masked_index", "]", "=", "t", "\n", "new_pos_inf", "=", "nltk", ".", "tag", ".", "pos_tag", "(", "tokens", ")", "\n", "\n", "# only use the similar sentences whose similar token's tag is still NN or JJ", "\n", "if", "(", "new_pos_inf", "[", "masked_index", "]", "[", "1", "]", ".", "startswith", "(", "tagFlag", ")", ")", ":", "\n", "\t\t\t\t", "new_sentence", "=", "detokenizer", ".", "detokenize", "(", "tokens", ")", "\n", "new_sentences", ".", "append", "(", "new_sentence", ")", "\n", "\n", "", "", "tokens", "[", "masked_index", "]", "=", "original_word", "\n", "\n", "", "return", "new_sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.PinjiaHe_StructureInvariantTesting.None.SIT.depDistance": [[120, 136], ["dict", "dict", "set", "abs", "dict.get", "dict.get", "list", "list", "dict.keys", "dict.keys", "dict.get", "dict.get"], "function", ["None"], ["", "def", "depDistance", "(", "graph1", ",", "graph2", ")", ":", "\n", "\n", "# count occurences of each type of relationship", "\n", "\t", "counts1", "=", "dict", "(", ")", "\n", "for", "i", "in", "graph1", ":", "\n", "\t  ", "counts1", "[", "i", "[", "1", "]", "]", "=", "counts1", ".", "get", "(", "i", "[", "1", "]", ",", "0", ")", "+", "1", "\n", "\n", "", "counts2", "=", "dict", "(", ")", "\n", "for", "i", "in", "graph2", ":", "\n", "\t  ", "counts2", "[", "i", "[", "1", "]", "]", "=", "counts2", ".", "get", "(", "i", "[", "1", "]", ",", "0", ")", "+", "1", "\n", "\n", "", "all_deps", "=", "set", "(", "list", "(", "counts1", ".", "keys", "(", ")", ")", "+", "list", "(", "counts2", ".", "keys", "(", ")", ")", ")", "\n", "diffs", "=", "0", "\n", "for", "dep", "in", "all_deps", ":", "\n", "\t\t", "diffs", "+=", "abs", "(", "counts1", ".", "get", "(", "dep", ",", "0", ")", "-", "counts2", ".", "get", "(", "dep", ",", "0", ")", ")", "\n", "", "return", "diffs", "\n", "\n"]]}