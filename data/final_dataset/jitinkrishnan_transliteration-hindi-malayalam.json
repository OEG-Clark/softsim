{"home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.joint_st.mlm_joint_ts_movie.mBERT.__init__": [[27, 35], ["torch.Module.__init__", "torch.Sigmoid", "torch.Sigmoid", "torch.CosineSimilarity", "torch.CosineSimilarity"], "methods", ["home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.mBERT.__init__"], ["    ", "def", "__init__", "(", "self", ",", "bert_teacher", ",", "bert_student", ",", "num_classes", ",", "hidden_layers", ",", "drop", "=", "0.1", ")", ":", "\n", "\n", "      ", "super", "(", "mBERT", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "bert_teacher", "=", "bert_teacher", "\n", "self", ".", "bert_student", "=", "bert_student", "\n", "self", ".", "sigmoid", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "self", ".", "cosine", "=", "nn", ".", "CosineSimilarity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.joint_st.mlm_joint_ts_movie.mBERT.forward": [[37, 70], ["mlm_joint_ts_movie.mBERT.bert_student", "mlm_joint_ts_movie.mBERT.bert_teacher", "mlm_joint_ts_movie.mBERT.bert_student", "mlm_joint_ts_movie.mBERT.bert_student", "mlm_joint_ts_movie.mBERT.bert_student", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "mlm_joint_ts_movie.mBERT.bert_student", "mlm_joint_ts_movie.mBERT.bert_student", "mlm_joint_ts_movie.mBERT.bert_student", "mlm_joint_ts_movie.mBERT.bert_student", "mlm_joint_ts_movie.mBERT.bert_student", "mlm_joint_ts_movie.mBERT.bert_student", "mlm_joint_ts_movie.mBERT.cosine", "mlm_joint_ts_movie.mBERT.cosine", "mlm_joint_ts_movie.mBERT.cosine"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sent_id_1", ",", "mask_1", ",", "sent_id_2", ",", "mask_2", ",", "sent_id_3", ",", "mask_3", ",", "labels", "=", "None", ",", "TEST", "=", "False", ")", ":", "\n", "        ", "global", "semi_sup", "\n", "global", "gamma", "\n", "\n", "if", "TEST", ":", "\n", "            ", "out", "=", "self", ".", "bert_student", "(", "sent_id_1", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "mask_1", ")", "\n", "return", "out", "[", "0", "]", "\n", "\n", "", "res", "=", "0", "\n", "if", "semi_sup", ":", "\n", "            ", "out_teacher", "=", "self", ".", "bert_teacher", "(", "sent_id_1", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "mask_1", ")", "\n", "out1", "=", "self", ".", "bert_student", "(", "sent_id_1", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "mask_1", ")", "\n", "out2", "=", "self", ".", "bert_student", "(", "sent_id_2", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "mask_2", ")", "\n", "out3", "=", "self", ".", "bert_student", "(", "sent_id_3", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "mask_3", ")", "\n", "ot", "=", "out_teacher", ".", "hidden_states", "[", "-", "1", "]", "[", ":", ",", "0", ",", ":", "]", "\n", "a", "=", "out1", ".", "hidden_states", "[", "-", "1", "]", "[", ":", ",", "0", ",", ":", "]", "\n", "b", "=", "out2", ".", "hidden_states", "[", "-", "1", "]", "[", ":", ",", "0", ",", ":", "]", "\n", "c", "=", "out3", ".", "hidden_states", "[", "-", "1", "]", "[", ":", ",", "0", ",", ":", "]", "\n", "output", "=", "1", "-", "self", ".", "cosine", "(", "ot", ",", "a", ")", "+", "1", "-", "self", ".", "cosine", "(", "ot", ",", "b", ")", "+", "1", "-", "self", ".", "cosine", "(", "ot", ",", "c", ")", "\n", "res", "=", "torch", ".", "mean", "(", "output", ")", "\n", "\n", "", "if", "labels", "is", "None", ":", "\n", "            ", "out1", "=", "self", ".", "bert_student", "(", "sent_id_1", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "mask_1", ")", "\n", "out2", "=", "self", ".", "bert_student", "(", "sent_id_2", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "mask_2", ")", "\n", "out3", "=", "self", ".", "bert_student", "(", "sent_id_3", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "mask_3", ")", "\n", "return", "res", ",", "out1", "[", "0", "]", ",", "out2", "[", "0", "]", ",", "out3", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "out1", "=", "self", ".", "bert_student", "(", "sent_id_1", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "mask_1", ",", "labels", "=", "labels", ")", "\n", "out2", "=", "self", ".", "bert_student", "(", "sent_id_2", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "mask_2", ",", "labels", "=", "labels", ")", "\n", "out3", "=", "self", ".", "bert_student", "(", "sent_id_3", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "mask_3", ",", "labels", "=", "labels", ")", "\n", "return", "gamma", "*", "res", "+", "out1", "[", "0", "]", "+", "out2", "[", "0", "]", "+", "out3", "[", "0", "]", "\n", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.joint_st.mlm_joint_ts_movie.is_changing": [[71, 92], ["len", "set", "set.remove", "max", "len", "max", "abs"], "function", ["None"], ["", "", "def", "is_changing", "(", "arr", ",", "delta", ")", ":", "\n", "    ", "curr", "=", "arr", "[", "0", "]", "\n", "count", "=", "len", "(", "arr", ")", "\n", "for", "x", "in", "arr", "[", "1", ":", "]", ":", "\n", "        ", "if", "x", ">", "curr", ":", "\n", "            ", "count", "-=", "1", "\n", "curr", "=", "x", "\n", "", "", "if", "count", "<", "3", ":", "\n", "        ", "return", "False", "\n", "\n", "", "new_list", "=", "set", "(", "arr", ")", "\n", "new_list", ".", "remove", "(", "max", "(", "new_list", ")", ")", "\n", "curr", "=", "max", "(", "new_list", ")", "\n", "count", "=", "len", "(", "arr", ")", "\n", "for", "x", "in", "arr", ":", "\n", "        ", "if", "abs", "(", "x", "-", "curr", ")", "<", "delta", ":", "\n", "          ", "count", "-=", "1", "\n", "", "", "if", "count", "<", "2", ":", "\n", "        ", "return", "False", "\n", "\n", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.joint_st.mlm_joint_ts_movie.tokenize_and_preserve_labels": [[93, 99], ["mlm_joint_ts_movie.tokenize", "mlm_joint_ts_movie.tokenize", "mlm_joint_ts_movie.tokenize"], "function", ["home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.tokenize", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.tokenize", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.tokenize"], ["", "def", "tokenize_and_preserve_labels", "(", "sentence1", ",", "sent_labels1", ",", "sentence2", ",", "sent_labels2", ",", "sentence3", ",", "sent_labels3", ")", ":", "\n", "    ", "encoded1", ",", "tokens1", "=", "tokenize", "(", "sentence1", ")", "\n", "encoded2", ",", "tokens2", "=", "tokenize", "(", "sentence2", ")", "\n", "encoded3", ",", "tokens3", "=", "tokenize", "(", "sentence3", ")", "\n", "\n", "return", "encoded1", ",", "sent_labels1", ",", "tokens1", ",", "encoded2", ",", "sent_labels2", ",", "tokens2", ",", "encoded3", ",", "sent_labels3", ",", "tokens3", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.joint_st.mlm_joint_ts_movie.tokenize": [[100, 116], ["tokenizer.encode_plus", "tokenizer.convert_ids_to_tokens", "encoded[].squeeze", "x.isdigit"], "function", ["None"], ["", "def", "tokenize", "(", "sentence", ")", ":", "\n", "    ", "sentence", "=", "[", "x", "for", "x", "in", "sentence", "if", "not", "x", ".", "isdigit", "(", ")", "]", "\n", "sentence", "=", "sentence", "[", ":", "500", "]", "\n", "\n", "full_sentence", "=", "' '", ".", "join", "(", "sentence", ")", "\n", "encoded", "=", "tokenizer", ".", "encode_plus", "(", "\n", "text", "=", "full_sentence", ",", "# the sentence to be encoded", "\n", "add_special_tokens", "=", "True", ",", "# Add [CLS] and [SEP]", "\n", "max_length", "=", "100", ",", "# maximum length of a sentence", "\n", "padding", "=", "True", ",", "# Add [PAD]s", "\n", "return_attention_mask", "=", "True", ",", "# Generate the attention mask", "\n", "return_tensors", "=", "'pt'", ",", "# ask the function to return PyTorch tensors", "\n", ")", "\n", "tokens", "=", "tokenizer", ".", "convert_ids_to_tokens", "(", "encoded", "[", "'input_ids'", "]", ".", "squeeze", "(", ")", ")", "\n", "\n", "return", "encoded", ",", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.joint_st.mlm_joint_ts_movie.tokenize_and_encode": [[118, 124], ["tokenizer.batch_encode_plus", "split.tolist"], "function", ["None"], ["", "def", "tokenize_and_encode", "(", "split", ",", "tokenizer", ",", "max_length", ")", ":", "\n", "    ", "return", "tokenizer", ".", "batch_encode_plus", "(", "\n", "split", ".", "tolist", "(", ")", ",", "\n", "max_length", "=", "max_length", ",", "\n", "pad_to_max_length", "=", "True", ",", "\n", "truncation", "=", "True", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.joint_st.mlm_joint_ts_movie.getData4Bert": [[126, 216], ["open", "open.readlines", "open.close", "open", "open.readlines", "open.close", "open", "open.readlines", "open.close", "open", "open.readlines", "open.close", "open", "open.readlines", "open.close", "open", "open.readlines", "open.close", "keras.preprocessing.sequence.pad_sequences", "keras.preprocessing.sequence.pad_sequences", "keras.preprocessing.sequence.pad_sequences", "print", "mlm_joint_ts_movie.tokenize_and_preserve_labels", "[].squeeze", "[].squeeze", "[].squeeze", "len", "len", "len", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "x.strip().split", "x.strip().split", "len", "len", "x.strip().split", "x.strip().split", "len", "len", "x.strip().split", "x.strip().split", "len", "len", "zip", "float", "float", "float", "torch.tensor", "torch.tensor", "x.strip", "x.strip", "x.strip", "x.strip", "x.strip", "x.strip"], "function", ["home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.tokenize_and_preserve_labels"], ["", "def", "getData4Bert", "(", "FNAME1", ",", "FNAME2", ",", "FNAME3", ",", "FNAME4", ",", "FNAME5", ",", "FNAME6", ",", "tokenizer", ",", "MAX_LEN", "=", "None", ")", ":", "\n", "    ", "global", "target", "\n", "if", "target", "==", "'ml'", ":", "\n", "        ", "maxl", "=", "30", "\n", "", "if", "target", "==", "'hi'", ":", "\n", "        ", "maxl", "=", "200", "\n", "\n", "", "POS_LINES", "=", "[", "]", "\n", "f", "=", "open", "(", "FNAME1", ")", "\n", "POS_LINES", "=", "f", ".", "readlines", "(", ")", "\n", "f", ".", "close", "(", ")", "\n", "POS_LINES", "=", "[", "x", ".", "strip", "(", ")", ".", "split", "(", ")", "[", ":", "maxl", "]", "for", "x", "in", "POS_LINES", "]", "\n", "\n", "NEG_LINES", "=", "[", "]", "\n", "f", "=", "open", "(", "FNAME2", ")", "\n", "NEG_LINES", "=", "f", ".", "readlines", "(", ")", "\n", "f", ".", "close", "(", ")", "\n", "NEG_LINES", "=", "[", "x", ".", "strip", "(", ")", ".", "split", "(", ")", "[", ":", "maxl", "]", "for", "x", "in", "NEG_LINES", "]", "\n", "\n", "sentences1", "=", "POS_LINES", "+", "NEG_LINES", "\n", "labels1", "=", "[", "1", "]", "*", "len", "(", "POS_LINES", ")", "+", "[", "0", "]", "*", "len", "(", "NEG_LINES", ")", "\n", "\n", "POS_LINES", "=", "[", "]", "\n", "f", "=", "open", "(", "FNAME3", ")", "\n", "POS_LINES", "=", "f", ".", "readlines", "(", ")", "\n", "f", ".", "close", "(", ")", "\n", "POS_LINES", "=", "[", "x", ".", "strip", "(", ")", ".", "split", "(", ")", "[", ":", "maxl", "]", "for", "x", "in", "POS_LINES", "]", "\n", "\n", "NEG_LINES", "=", "[", "]", "\n", "f", "=", "open", "(", "FNAME4", ")", "\n", "NEG_LINES", "=", "f", ".", "readlines", "(", ")", "\n", "f", ".", "close", "(", ")", "\n", "NEG_LINES", "=", "[", "x", ".", "strip", "(", ")", ".", "split", "(", ")", "[", ":", "maxl", "]", "for", "x", "in", "NEG_LINES", "]", "\n", "\n", "sentences2", "=", "POS_LINES", "+", "NEG_LINES", "\n", "labels2", "=", "[", "1", "]", "*", "len", "(", "POS_LINES", ")", "+", "[", "0", "]", "*", "len", "(", "NEG_LINES", ")", "\n", "\n", "POS_LINES", "=", "[", "]", "\n", "f", "=", "open", "(", "FNAME5", ")", "\n", "POS_LINES", "=", "f", ".", "readlines", "(", ")", "\n", "f", ".", "close", "(", ")", "\n", "POS_LINES", "=", "[", "x", ".", "strip", "(", ")", ".", "split", "(", ")", "[", ":", "maxl", "]", "for", "x", "in", "POS_LINES", "]", "\n", "\n", "NEG_LINES", "=", "[", "]", "\n", "f", "=", "open", "(", "FNAME6", ")", "\n", "NEG_LINES", "=", "f", ".", "readlines", "(", ")", "\n", "f", ".", "close", "(", ")", "\n", "NEG_LINES", "=", "[", "x", ".", "strip", "(", ")", ".", "split", "(", ")", "[", ":", "maxl", "]", "for", "x", "in", "NEG_LINES", "]", "\n", "\n", "sentences3", "=", "POS_LINES", "+", "NEG_LINES", "\n", "labels3", "=", "[", "1", "]", "*", "len", "(", "POS_LINES", ")", "+", "[", "0", "]", "*", "len", "(", "NEG_LINES", ")", "\n", "\n", "\n", "tokenized_texts_and_labels", "=", "[", "\n", "tokenize_and_preserve_labels", "(", "sent1", ",", "slabs1", ",", "sent2", ",", "slabs2", ",", "sent3", ",", "slabs3", ")", "\n", "for", "sent1", ",", "slabs1", ",", "sent2", ",", "slabs2", ",", "sent3", ",", "slabs3", "in", "zip", "(", "sentences1", ",", "labels1", ",", "sentences2", ",", "labels2", ",", "sentences3", ",", "labels3", ")", "\n", "]", "\n", "\n", "tokenized_texts1", "=", "[", "token_label_pair", "[", "2", "]", "for", "token_label_pair", "in", "tokenized_texts_and_labels", "]", "\n", "sent_labels1", "=", "[", "token_label_pair", "[", "1", "]", "for", "token_label_pair", "in", "tokenized_texts_and_labels", "]", "\n", "tokenized_texts2", "=", "[", "token_label_pair", "[", "5", "]", "for", "token_label_pair", "in", "tokenized_texts_and_labels", "]", "\n", "sent_labels2", "=", "[", "token_label_pair", "[", "4", "]", "for", "token_label_pair", "in", "tokenized_texts_and_labels", "]", "\n", "tokenized_texts3", "=", "[", "token_label_pair", "[", "8", "]", "for", "token_label_pair", "in", "tokenized_texts_and_labels", "]", "\n", "sent_labels3", "=", "[", "token_label_pair", "[", "7", "]", "for", "token_label_pair", "in", "tokenized_texts_and_labels", "]", "\n", "\n", "input_ids1", "=", "[", "token_label_pair", "[", "0", "]", "[", "'input_ids'", "]", ".", "squeeze", "(", ")", "for", "token_label_pair", "in", "tokenized_texts_and_labels", "]", "\n", "input_ids2", "=", "[", "token_label_pair", "[", "3", "]", "[", "'input_ids'", "]", ".", "squeeze", "(", ")", "for", "token_label_pair", "in", "tokenized_texts_and_labels", "]", "\n", "input_ids3", "=", "[", "token_label_pair", "[", "6", "]", "[", "'input_ids'", "]", ".", "squeeze", "(", ")", "for", "token_label_pair", "in", "tokenized_texts_and_labels", "]", "\n", "\n", "input_ids1", "=", "pad_sequences", "(", "[", "input_id", "for", "input_id", "in", "input_ids1", "]", ",", "\n", "maxlen", "=", "MAX_LEN", ",", "dtype", "=", "\"long\"", ",", "value", "=", "0.0", ",", "\n", "truncating", "=", "\"post\"", ",", "padding", "=", "\"post\"", ")", "\n", "\n", "input_ids2", "=", "pad_sequences", "(", "[", "input_id", "for", "input_id", "in", "input_ids2", "]", ",", "\n", "maxlen", "=", "MAX_LEN", ",", "dtype", "=", "\"long\"", ",", "value", "=", "0.0", ",", "\n", "truncating", "=", "\"post\"", ",", "padding", "=", "\"post\"", ")", "\n", "\n", "input_ids3", "=", "pad_sequences", "(", "[", "input_id", "for", "input_id", "in", "input_ids3", "]", ",", "\n", "maxlen", "=", "MAX_LEN", ",", "dtype", "=", "\"long\"", ",", "value", "=", "0.0", ",", "\n", "truncating", "=", "\"post\"", ",", "padding", "=", "\"post\"", ")", "\n", "\n", "attention_masks1", "=", "[", "[", "float", "(", "i", "!=", "0.0", ")", "for", "i", "in", "ii", "]", "for", "ii", "in", "input_ids1", "]", "\n", "attention_masks2", "=", "[", "[", "float", "(", "i", "!=", "0.0", ")", "for", "i", "in", "ii", "]", "for", "ii", "in", "input_ids2", "]", "\n", "attention_masks3", "=", "[", "[", "float", "(", "i", "!=", "0.0", ")", "for", "i", "in", "ii", "]", "for", "ii", "in", "input_ids3", "]", "\n", "\n", "assert", "(", "len", "(", "labels1", ")", "==", "len", "(", "labels2", ")", "==", "len", "(", "labels3", ")", ")", "\n", "\n", "print", "(", "\"SHAPE= \"", ",", "torch", ".", "tensor", "(", "input_ids1", ")", ".", "shape", ")", "\n", "\n", "return", "torch", ".", "tensor", "(", "input_ids1", ")", ",", "torch", ".", "tensor", "(", "attention_masks1", ")", ",", "torch", ".", "tensor", "(", "input_ids2", ")", ",", "torch", ".", "tensor", "(", "attention_masks2", ")", ",", "torch", ".", "tensor", "(", "input_ids3", ")", ",", "torch", ".", "tensor", "(", "attention_masks3", ")", ",", "torch", ".", "tensor", "(", "labels3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.joint_st.mlm_joint_ts_movie.create_loaders": [[218, 226], ["torch.utils.data.TensorDataset", "torch.utils.data.RandomSampler", "torch.utils.data.DataLoader"], "function", ["None"], ["", "def", "create_loaders", "(", "seq1", ",", "mask1", ",", "seq2", ",", "mask2", ",", "seq3", ",", "mask3", ",", "y", ")", ":", "\n", "# wrap tensors", "\n", "  ", "data", "=", "TensorDataset", "(", "seq1", ",", "mask1", ",", "seq2", ",", "mask2", ",", "seq3", ",", "mask3", ",", "y", ")", "\n", "# sampler for sampling the data during training", "\n", "sampler", "=", "RandomSampler", "(", "data", ")", "\n", "# dataLoader for train set", "\n", "dataloader", "=", "DataLoader", "(", "data", ",", "sampler", "=", "sampler", ",", "batch_size", "=", "batch_size", ")", "\n", "return", "dataloader", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.joint_st.mlm_joint_ts_movie.train": [[230, 299], ["model.train", "enumerate", "range", "steps.append", "batches.append", "len", "model.zero_grad", "model", "lossA.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "optimizer.step", "schedular.step", "predsA.detach().cpu().numpy.detach().cpu().numpy", "len", "r.to", "lossA.item", "model.parameters", "predsA.detach().cpu().numpy.detach().cpu", "predsA.detach().cpu().numpy.detach"], "function", ["home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.train"], ["", "def", "train", "(", "train_dataloader", ",", "optimizer", ",", "schedular", ",", "alpha", "=", "1.0", ",", "beta", "=", "1.0", ")", ":", "\n", "    ", "global", "model", "\n", "model", ".", "train", "(", ")", "\n", "\n", "total_loss", ",", "total_accuracy", "=", "0", ",", "0", "\n", "\n", "# empty list to save model predictions", "\n", "total_preds", "=", "[", "]", "\n", "\n", "# iterate over batches", "\n", "steps", "=", "[", "]", "\n", "batches", "=", "[", "]", "\n", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "train_dataloader", ")", ":", "\n", "        ", "steps", ".", "append", "(", "step", ")", "\n", "batches", ".", "append", "(", "batch", ")", "\n", "\n", "", "for", "index", "in", "range", "(", "len", "(", "steps", ")", ")", ":", "\n", "        ", "step", "=", "steps", "[", "index", "]", "\n", "batch", "=", "batches", "[", "index", "]", "\n", "\n", "# progress update after every 50 batches.", "\n", "#if step % 50 == 0 and not step == 0:", "\n", "#print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))", "\n", "\n", "# push the batch to gpu", "\n", "batch", "=", "[", "r", ".", "to", "(", "device", ")", "for", "r", "in", "batch", "]", "\n", "#batch = [r for r in batch]", "\n", "\n", "sent_id1", ",", "mask1", ",", "sent_id2", ",", "mask2", ",", "sent_id3", ",", "mask3", ",", "labels", "=", "batch", "\n", "\n", "# clear previously calculated gradients ", "\n", "model", ".", "zero_grad", "(", ")", "\n", "\n", "# get model predictions for the current batch", "\n", "predsA", "=", "model", "(", "sent_id1", ",", "mask1", ",", "sent_id2", ",", "mask2", ",", "sent_id3", ",", "mask3", ",", "labels", ")", "\n", "\n", "# compute the loss between actual and predicted values", "\n", "lossA", "=", "alpha", "*", "predsA", "#cross_entropy(predsA, labels)", "\n", "total_loss", "=", "total_loss", "+", "lossA", ".", "item", "(", ")", "\n", "\n", "# backward pass to calculate the gradients", "\n", "lossA", ".", "backward", "(", ")", "#retain_graph=True)", "\n", "\n", "# clip the the gradients to 1.0. It helps in preventing the exploding gradient problem", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "1.0", ")", "\n", "\n", "# update parameters", "\n", "optimizer", ".", "step", "(", ")", "\n", "schedular", ".", "step", "(", ")", "\n", "\n", "# model predictions are stored on GPU. So, push it to CPU", "\n", "predsA", "=", "predsA", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "'''\n        for index in range(len(slots_preds)):\n            slots_preds[index]=slots_preds[index].detach().cpu().numpy()\n        '''", "\n", "# append the model predictions", "\n", "#total_preds.append(predsA)", "\n", "\n", "# compute the training loss of the epoch", "\n", "", "avg_loss", "=", "total_loss", "/", "len", "(", "train_dataloader", ")", "\n", "\n", "# predictions are in the form of (no. of batches, size of batch, no. of classes).", "\n", "# reshape the predictions in form of (number of samples, no. of classes)", "\n", "#total_preds  = np.concatenate(total_preds, axis=0)", "\n", "\n", "#returns the loss and predictions", "\n", "return", "avg_loss", ",", "total_preds", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.joint_st.mlm_joint_ts_movie.evaluate": [[303, 363], ["model.eval", "enumerate", "len", "t.to", "torch.no_grad", "torch.no_grad", "model", "predsA.detach().cpu().numpy.detach().cpu().numpy", "predsB.detach().cpu().numpy.detach().cpu().numpy", "predsC.detach().cpu().numpy.detach().cpu().numpy", "type", "semi_loss.detach().cpu().numpy.detach().cpu().numpy", "cross_entropy", "predsA.detach().cpu().numpy.detach().cpu", "predsB.detach().cpu().numpy.detach().cpu", "predsC.detach().cpu().numpy.detach().cpu", "cross_entropy", "cross_entropy", "lossA.item", "lossA.item", "semi_loss.detach().cpu().numpy.item", "semi_loss.detach().cpu().numpy.detach().cpu", "predsA.detach().cpu().numpy.detach", "predsB.detach().cpu().numpy.detach", "predsC.detach().cpu().numpy.detach", "semi_loss.detach().cpu().numpy.detach"], "function", ["None"], ["", "def", "evaluate", "(", "val_dataloader", ",", "alpha", "=", "1.0", ",", "beta", "=", "1.0", ")", ":", "\n", "    ", "global", "model", "\n", "global", "gamma", "\n", "#print(\"\\nEvaluating...\")", "\n", "\n", "# deactivate dropout layers", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "total_loss", ",", "total_accuracy", "=", "0", ",", "0", "\n", "\n", "# empty list to save the model predictions", "\n", "total_preds", "=", "[", "]", "\n", "\n", "# iterate over batches", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "val_dataloader", ")", ":", "\n", "\n", "# Progress update every 50 batches.", "\n", "#if step % 500 == 0 and not step == 0:", "\n", "# Calculate elapsed time in minutes.", "\n", "#elapsed = format_time(time.time() - t0)", "\n", "\n", "# Report progress.", "\n", "#print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))", "\n", "\n", "# push the batch to gpu", "\n", "        ", "batch", "=", "[", "t", ".", "to", "(", "device", ")", "for", "t", "in", "batch", "]", "\n", "#batch = [t for t in batch]", "\n", "\n", "sent_id1", ",", "mask1", ",", "sent_id2", ",", "mask2", ",", "sent_id3", ",", "mask3", ",", "labels", "=", "batch", "\n", "\n", "# deactivate autograd", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# model predictions", "\n", "            ", "semi_loss", ",", "predsA", ",", "predsB", ",", "predsC", "=", "model", "(", "sent_id1", ",", "mask1", ",", "sent_id2", ",", "mask2", ",", "sent_id3", ",", "mask3", ")", "\n", "\n", "#preds = preds.squeeze()", "\n", "#print(\"preds shape: \", preds.shape)", "\n", "\n", "# compute the validation loss between actual and predicted values", "\n", "\n", "lossA", "=", "alpha", "*", "(", "cross_entropy", "(", "predsA", ",", "labels", ")", "+", "cross_entropy", "(", "predsB", ",", "labels", ")", "+", "cross_entropy", "(", "predsC", ",", "labels", ")", ")", "\n", "\n", "if", "type", "(", "semi_loss", ")", "==", "int", ":", "\n", "                ", "total_loss", "=", "total_loss", "+", "lossA", ".", "item", "(", ")", "+", "gamma", "*", "semi_loss", "\n", "", "else", ":", "\n", "                ", "total_loss", "=", "total_loss", "+", "lossA", ".", "item", "(", ")", "+", "gamma", "*", "semi_loss", ".", "item", "(", ")", "\n", "semi_loss", "=", "semi_loss", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "predsA", "=", "predsA", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "predsB", "=", "predsB", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "predsC", "=", "predsC", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "#total_preds.append(predsA)", "\n", "#outputs=outputs.detach().cpu().numpy()", "\n", "\n", "# compute the validation loss of the epoch", "\n", "", "", "avg_loss", "=", "total_loss", "/", "len", "(", "val_dataloader", ")", "\n", "\n", "# reshape the predictions in form of (number of samples, no. of classes)", "\n", "#total_preds  = np.concatenate(total_preds, axis=0)", "\n", "\n", "return", "avg_loss", ",", "total_preds", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.joint_st.mlm_joint_ts_movie.final_model": [[365, 471], ["mlm_joint_ts_movie.getData4Bert", "mlm_joint_ts_movie.getData4Bert", "mlm_joint_ts_movie.create_loaders", "mlm_joint_ts_movie.create_loaders", "model.to.to", "transformers.AdamW", "transformers.get_linear_schedule_with_warmup", "float", "time.time", "range", "print", "range", "mlm_joint_ts_movie.getData4Bert", "TEST_DATA.append", "mlm_joint_ts_movie.getData4Bert", "TEST_DATA.append", "model.to.parameters", "mlm_joint_ts_movie.train", "mlm_joint_ts_movie.evaluate", "train_losses.append", "valid_losses.append", "numpy.argmax", "sklearn.metrics.accuracy_score", "sklearn.metrics.f1_score", "sklearn.metrics.f1_score", "sklearn.metrics.f1_score", "print", "print", "print", "bert.roberta.embeddings.parameters", "range", "bert.bert.embeddings.parameters", "range", "torch.no_grad", "torch.no_grad", "model.to.", "preds.detach().cpu().numpy.detach().cpu().numpy", "len", "len", "len", "mlm_joint_ts_movie.is_changing", "time.time", "test_seq.to", "test_mask.to", "test_seq.to", "test_mask.to", "test_seq.to", "test_mask.to", "str", "str", "bert.roberta.encoder.layer[].parameters", "bert.bert.encoder.layer[].parameters", "preds.detach().cpu().numpy.detach().cpu", "round", "round", "preds.detach().cpu().numpy.detach"], "function", ["home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.getData4Bert", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.getData4Bert", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.create_loaders", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.create_loaders", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.getData4Bert", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.getData4Bert", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.train", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.evaluate", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.is_changing"], ["", "def", "final_model", "(", "train_FNAME1", ",", "train_FNAME2", ",", "train_FNAME3", ",", "train_FNAME4", ",", "train_FNAME5", ",", "train_FNAME6", ",", "val_FNAME1", ",", "val_FNAME2", ",", "val_FNAME3", ",", "val_FNAME4", ",", "val_FNAME5", ",", "val_FNAME6", ",", "mode", "=", "'ml'", ")", ":", "\n", "    ", "global", "model", ",", "base_model_name", "\n", "FREEZE", "=", "8", "\n", "############### FREEZE ##########################", "\n", "if", "FREEZE", "==", "0", ":", "\n", "        ", "pass", "\n", "", "else", ":", "\n", "        ", "if", "base_model_name", "==", "\"xlm-roberta-base\"", ":", "\n", "            ", "for", "param", "in", "bert", ".", "roberta", ".", "embeddings", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "", "for", "index", "in", "range", "(", "len", "(", "bert", ".", "roberta", ".", "encoder", ".", "layer", ")", ")", ":", "\n", "                ", "if", "index", "<", "FREEZE", ":", "\n", "                    ", "for", "param", "in", "bert", ".", "roberta", ".", "encoder", ".", "layer", "[", "index", "]", ".", "parameters", "(", ")", ":", "\n", "                        ", "param", ".", "requires_grad", "=", "False", "\n", "", "", "", "", "else", ":", "\n", "            ", "for", "param", "in", "bert", ".", "bert", ".", "embeddings", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "", "for", "index", "in", "range", "(", "len", "(", "bert", ".", "bert", ".", "encoder", ".", "layer", ")", ")", ":", "\n", "                ", "if", "index", "<", "FREEZE", ":", "\n", "                    ", "for", "param", "in", "bert", ".", "bert", ".", "encoder", ".", "layer", "[", "index", "]", ".", "parameters", "(", ")", ":", "\n", "                        ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "############### LOAD DATASET ##########################", "\n", "", "", "", "", "", "MAX_LEN", "=", "100", "\n", "train_seq1", ",", "train_mask1", ",", "train_seq2", ",", "train_mask2", ",", "train_seq3", ",", "train_mask3", ",", "train_y", "=", "getData4Bert", "(", "train_FNAME1", ",", "train_FNAME2", ",", "train_FNAME3", ",", "train_FNAME4", ",", "train_FNAME5", ",", "train_FNAME6", ",", "tokenizer", ",", "MAX_LEN", "=", "MAX_LEN", ")", "\n", "val_seq1", ",", "val_mask1", ",", "val_seq2", ",", "val_mask2", ",", "val_seq3", ",", "val_mask3", ",", "val_y", "=", "getData4Bert", "(", "val_FNAME1", ",", "val_FNAME2", ",", "val_FNAME3", ",", "val_FNAME4", ",", "val_FNAME5", ",", "val_FNAME6", ",", "tokenizer", ",", "MAX_LEN", "=", "MAX_LEN", ")", "\n", "\n", "TEST_DATA", "=", "[", "]", "\n", "if", "mode", "==", "'ml'", ":", "\n", "        ", "td", "=", "getData4Bert", "(", "'data/ml_pos_ro'", ",", "'data/ml_neg_ro'", ",", "'data/ml_pos_ro'", ",", "'data/ml_neg_ro'", ",", "'data/ml_pos_ro'", ",", "'data/ml_neg_ro'", ",", "tokenizer", ",", "MAX_LEN", ")", "\n", "TEST_DATA", ".", "append", "(", "td", "[", "4", ":", "]", ")", "\n", "", "if", "mode", "==", "'hi'", ":", "\n", "        ", "td", "=", "getData4Bert", "(", "'data/hi_pos_ro'", ",", "'data/hi_neg_ro'", ",", "'data/hi_pos_ro'", ",", "'data/hi_neg_ro'", ",", "'data/hi_pos_ro'", ",", "'data/hi_neg_ro'", ",", "tokenizer", ",", "MAX_LEN", ")", "\n", "TEST_DATA", ".", "append", "(", "td", "[", "4", ":", "]", ")", "\n", "\n", "", "train_dataloader", "=", "create_loaders", "(", "train_seq1", ",", "train_mask1", ",", "train_seq2", ",", "train_mask2", ",", "train_seq3", ",", "train_mask3", ",", "train_y", ")", "\n", "val_dataloader", "=", "create_loaders", "(", "val_seq1", ",", "val_mask1", ",", "val_seq2", ",", "val_mask2", ",", "val_seq3", ",", "val_mask3", ",", "val_y", ")", "\n", "\n", "# push the model to GPU", "\n", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "\n", "# define the optimizer", "\n", "optimizer", "=", "AdamW", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", "# learning rate", "\n", "\n", "schedular", "=", "get_linear_schedule_with_warmup", "(", "\n", "optimizer", ",", "\n", "num_warmup_steps", "=", "0", ",", "\n", "num_training_steps", "=", "len", "(", "train_dataloader", ")", "*", "epochs", ",", "\n", ")", "\n", "\n", "# set initial loss to infinite", "\n", "best_valid_loss", "=", "float", "(", "'inf'", ")", "\n", "\n", "# empty lists to store training and validation loss of each epoch", "\n", "train_losses", "=", "[", "]", "\n", "valid_losses", "=", "[", "]", "\n", "best_epoch", "=", "0", "\n", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "#for each epoch", "\n", "for", "epoch", "in", "range", "(", "epochs", ")", ":", "\n", "\n", "#print('\\nEpoch {:} / {:}'.format(epoch + 1, epochs), end=\" \")", "\n", "\n", "#train model", "\n", "        ", "train_loss", ",", "_", "=", "train", "(", "train_dataloader", ",", "optimizer", ",", "schedular", ")", "\n", "\n", "#evaluate model", "\n", "valid_loss", ",", "_", "=", "evaluate", "(", "val_dataloader", ")", "\n", "\n", "#save the best model", "\n", "if", "valid_loss", "<", "best_valid_loss", "and", "epoch", ">", "5", ":", "\n", "            ", "best_valid_loss", "=", "valid_loss", "\n", "#torch.save(model.state_dict(), SCRATCH_FNAME)", "\n", "best_epoch", "=", "epoch", "+", "1", "\n", "\n", "# append training and validation loss", "\n", "", "train_losses", ".", "append", "(", "train_loss", ")", "\n", "valid_losses", ".", "append", "(", "valid_loss", ")", "\n", "\n", "#print(f'Training Loss: {train_loss:.5f} Validation Loss: {valid_loss:.5f}')", "\n", "\n", "if", "epoch", ">", "patience", ":", "\n", "            ", "if", "not", "is_changing", "(", "valid_losses", "[", "-", "1", "*", "patience", ":", "]", ",", "delta", ")", ":", "\n", "#print(\"STOPPING - NO CHANGE\")", "\n", "                ", "break", "\n", "\n", "#print('BEST EPOCH= ', best_epoch)", "\n", "", "", "", "print", "(", "\"--- %s seconds ---\"", "%", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "\n", "############### predict ########################## ", "\n", "\n", "for", "index", "in", "range", "(", "1", ")", ":", "\n", "        ", "test_seq", ",", "test_mask", ",", "test_y", "=", "TEST_DATA", "[", "index", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "preds", "=", "model", "(", "test_seq", ".", "to", "(", "device", ")", ",", "test_mask", ".", "to", "(", "device", ")", ",", "test_seq", ".", "to", "(", "device", ")", ",", "test_mask", ".", "to", "(", "device", ")", ",", "test_seq", ".", "to", "(", "device", ")", ",", "test_mask", ".", "to", "(", "device", ")", ",", "TEST", "=", "True", ")", "\n", "preds", "=", "preds", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "", "preds", "=", "np", ".", "argmax", "(", "preds", ",", "axis", "=", "1", ")", "\n", "#print(classification_report(test_y, preds, digits=4))", "\n", "acc", "=", "accuracy_score", "(", "test_y", ",", "preds", ")", "\n", "f1micro", "=", "f1_score", "(", "test_y", ",", "preds", ",", "average", "=", "'micro'", ")", "\n", "f1macro", "=", "f1_score", "(", "test_y", ",", "preds", ",", "average", "=", "'macro'", ")", "\n", "f1weighted", "=", "f1_score", "(", "test_y", ",", "preds", ",", "average", "=", "'weighted'", ")", "\n", "print", "(", "\"Acc: \"", "+", "str", "(", "round", "(", "acc", "*", "100", ",", "4", ")", ")", ")", "\n", "print", "(", "\"F1: \"", "+", "str", "(", "round", "(", "f1weighted", "*", "100", ",", "4", ")", ")", ")", "\n", "print", "(", "\"--------------\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.joint_st.mlm_joint_ts_floods.mBERT.__init__": [[27, 35], ["torch.Module.__init__", "torch.Sigmoid", "torch.Sigmoid", "torch.CosineSimilarity", "torch.CosineSimilarity"], "methods", ["home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.mBERT.__init__"], ["    ", "def", "__init__", "(", "self", ",", "bert_teacher", ",", "bert_student", ",", "num_classes", ",", "hidden_layers", ",", "drop", "=", "0.1", ")", ":", "\n", "\n", "      ", "super", "(", "mBERT", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "bert_teacher", "=", "bert_teacher", "\n", "self", ".", "bert_student", "=", "bert_student", "\n", "self", ".", "sigmoid", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "self", ".", "cosine", "=", "nn", ".", "CosineSimilarity", "(", ")", "\n", "#self.relu =  nn.ReLU()", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.joint_st.mlm_joint_ts_floods.mBERT.forward": [[39, 72], ["mlm_joint_ts_floods.mBERT.bert_student", "mlm_joint_ts_floods.mBERT.bert_teacher", "mlm_joint_ts_floods.mBERT.bert_student", "mlm_joint_ts_floods.mBERT.bert_student", "mlm_joint_ts_floods.mBERT.bert_student", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "mlm_joint_ts_floods.mBERT.bert_student", "mlm_joint_ts_floods.mBERT.bert_student", "mlm_joint_ts_floods.mBERT.bert_student", "mlm_joint_ts_floods.mBERT.bert_student", "mlm_joint_ts_floods.mBERT.bert_student", "mlm_joint_ts_floods.mBERT.bert_student", "mlm_joint_ts_floods.mBERT.cosine", "mlm_joint_ts_floods.mBERT.cosine", "mlm_joint_ts_floods.mBERT.cosine"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sent_id_1", ",", "mask_1", ",", "sent_id_2", ",", "mask_2", ",", "sent_id_3", ",", "mask_3", ",", "labels", "=", "None", ",", "TEST", "=", "False", ")", ":", "\n", "        ", "global", "semi_sup", "\n", "global", "gamma", "\n", "\n", "if", "TEST", ":", "\n", "            ", "out", "=", "self", ".", "bert_student", "(", "sent_id_1", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "mask_1", ")", "\n", "return", "out", "[", "0", "]", "\n", "\n", "", "res", "=", "0", "\n", "if", "semi_sup", ":", "\n", "            ", "out_teacher", "=", "self", ".", "bert_teacher", "(", "sent_id_1", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "mask_1", ")", "\n", "out1", "=", "self", ".", "bert_student", "(", "sent_id_1", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "mask_1", ")", "\n", "out2", "=", "self", ".", "bert_student", "(", "sent_id_2", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "mask_2", ")", "\n", "out3", "=", "self", ".", "bert_student", "(", "sent_id_3", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "mask_3", ")", "\n", "ot", "=", "out_teacher", ".", "hidden_states", "[", "-", "1", "]", "[", ":", ",", "0", ",", ":", "]", "\n", "a", "=", "out1", ".", "hidden_states", "[", "-", "1", "]", "[", ":", ",", "0", ",", ":", "]", "\n", "b", "=", "out2", ".", "hidden_states", "[", "-", "1", "]", "[", ":", ",", "0", ",", ":", "]", "\n", "c", "=", "out3", ".", "hidden_states", "[", "-", "1", "]", "[", ":", ",", "0", ",", ":", "]", "\n", "output", "=", "1", "-", "self", ".", "cosine", "(", "ot", ",", "a", ")", "+", "1", "-", "self", ".", "cosine", "(", "ot", ",", "b", ")", "+", "1", "-", "self", ".", "cosine", "(", "ot", ",", "c", ")", "\n", "res", "=", "torch", ".", "mean", "(", "output", ")", "\n", "\n", "", "if", "labels", "is", "None", ":", "\n", "            ", "out1", "=", "self", ".", "bert_student", "(", "sent_id_1", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "mask_1", ")", "\n", "out2", "=", "self", ".", "bert_student", "(", "sent_id_2", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "mask_2", ")", "\n", "out3", "=", "self", ".", "bert_student", "(", "sent_id_3", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "mask_3", ")", "\n", "return", "res", ",", "out1", "[", "0", "]", ",", "out2", "[", "0", "]", ",", "out3", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "out1", "=", "self", ".", "bert_student", "(", "sent_id_1", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "mask_1", ",", "labels", "=", "labels", ")", "\n", "out2", "=", "self", ".", "bert_student", "(", "sent_id_2", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "mask_2", ",", "labels", "=", "labels", ")", "\n", "out3", "=", "self", ".", "bert_student", "(", "sent_id_3", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "mask_3", ",", "labels", "=", "labels", ")", "\n", "return", "gamma", "*", "res", "+", "out1", "[", "0", "]", "+", "out2", "[", "0", "]", "+", "out3", "[", "0", "]", "\n", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.joint_st.mlm_joint_ts_floods.is_changing": [[73, 94], ["len", "set", "set.remove", "max", "len", "max", "abs"], "function", ["None"], ["", "", "def", "is_changing", "(", "arr", ",", "delta", ")", ":", "\n", "    ", "curr", "=", "arr", "[", "0", "]", "\n", "count", "=", "len", "(", "arr", ")", "\n", "for", "x", "in", "arr", "[", "1", ":", "]", ":", "\n", "        ", "if", "x", ">", "curr", ":", "\n", "            ", "count", "-=", "1", "\n", "curr", "=", "x", "\n", "", "", "if", "count", "<", "3", ":", "\n", "        ", "return", "False", "\n", "\n", "", "new_list", "=", "set", "(", "arr", ")", "\n", "new_list", ".", "remove", "(", "max", "(", "new_list", ")", ")", "\n", "curr", "=", "max", "(", "new_list", ")", "\n", "count", "=", "len", "(", "arr", ")", "\n", "for", "x", "in", "arr", ":", "\n", "        ", "if", "abs", "(", "x", "-", "curr", ")", "<", "delta", ":", "\n", "          ", "count", "-=", "1", "\n", "", "", "if", "count", "<", "2", ":", "\n", "        ", "return", "False", "\n", "\n", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.joint_st.mlm_joint_ts_floods.tokenize_and_preserve_labels": [[95, 101], ["mlm_joint_ts_floods.tokenize", "mlm_joint_ts_floods.tokenize", "mlm_joint_ts_floods.tokenize"], "function", ["home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.tokenize", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.tokenize", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.tokenize"], ["", "def", "tokenize_and_preserve_labels", "(", "sentence1", ",", "sent_labels1", ",", "sentence2", ",", "sent_labels2", ",", "sentence3", ",", "sent_labels3", ")", ":", "\n", "    ", "encoded1", ",", "tokens1", "=", "tokenize", "(", "sentence1", ")", "\n", "encoded2", ",", "tokens2", "=", "tokenize", "(", "sentence2", ")", "\n", "encoded3", ",", "tokens3", "=", "tokenize", "(", "sentence3", ")", "\n", "\n", "return", "encoded1", ",", "sent_labels1", ",", "tokens1", ",", "encoded2", ",", "sent_labels2", ",", "tokens2", ",", "encoded3", ",", "sent_labels3", ",", "tokens3", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.joint_st.mlm_joint_ts_floods.tokenize": [[102, 118], ["tokenizer.encode_plus", "tokenizer.convert_ids_to_tokens", "encoded[].squeeze", "x.isdigit"], "function", ["None"], ["", "def", "tokenize", "(", "sentence", ")", ":", "\n", "    ", "sentence", "=", "[", "x", "for", "x", "in", "sentence", "if", "not", "x", ".", "isdigit", "(", ")", "]", "\n", "sentence", "=", "sentence", "[", ":", "500", "]", "\n", "\n", "full_sentence", "=", "' '", ".", "join", "(", "sentence", ")", "\n", "encoded", "=", "tokenizer", ".", "encode_plus", "(", "\n", "text", "=", "full_sentence", ",", "# the sentence to be encoded", "\n", "add_special_tokens", "=", "True", ",", "# Add [CLS] and [SEP]", "\n", "max_length", "=", "45", ",", "# maximum length of a sentence", "\n", "padding", "=", "True", ",", "# Add [PAD]s", "\n", "return_attention_mask", "=", "True", ",", "# Generate the attention mask", "\n", "return_tensors", "=", "'pt'", ",", "# ask the function to return PyTorch tensors", "\n", ")", "\n", "tokens", "=", "tokenizer", ".", "convert_ids_to_tokens", "(", "encoded", "[", "'input_ids'", "]", ".", "squeeze", "(", ")", ")", "\n", "\n", "return", "encoded", ",", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.joint_st.mlm_joint_ts_floods.tokenize_and_encode": [[120, 126], ["tokenizer.batch_encode_plus", "split.tolist"], "function", ["None"], ["", "def", "tokenize_and_encode", "(", "split", ",", "tokenizer", ",", "max_length", ")", ":", "\n", "    ", "return", "tokenizer", ".", "batch_encode_plus", "(", "\n", "split", ".", "tolist", "(", ")", ",", "\n", "max_length", "=", "max_length", ",", "\n", "pad_to_max_length", "=", "True", ",", "\n", "truncation", "=", "True", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.joint_st.mlm_joint_ts_floods.getData4Bert": [[128, 218], ["open", "open.readlines", "open.close", "open", "open.readlines", "open.close", "open", "open.readlines", "open.close", "open", "open.readlines", "open.close", "open", "open.readlines", "open.close", "open", "open.readlines", "open.close", "keras.preprocessing.sequence.pad_sequences", "keras.preprocessing.sequence.pad_sequences", "keras.preprocessing.sequence.pad_sequences", "print", "mlm_joint_ts_floods.tokenize_and_preserve_labels", "[].squeeze", "[].squeeze", "[].squeeze", "len", "len", "len", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "x.strip().split", "x.strip().split", "len", "len", "x.strip().split", "x.strip().split", "len", "len", "x.strip().split", "x.strip().split", "len", "len", "zip", "float", "float", "float", "torch.tensor", "torch.tensor", "x.strip", "x.strip", "x.strip", "x.strip", "x.strip", "x.strip"], "function", ["home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.tokenize_and_preserve_labels"], ["", "def", "getData4Bert", "(", "FNAME1", ",", "FNAME2", ",", "FNAME3", ",", "FNAME4", ",", "FNAME5", ",", "FNAME6", ",", "tokenizer", ",", "MAX_LEN", "=", "None", ")", ":", "\n", "    ", "global", "target", "\n", "if", "target", "==", "'ml'", ":", "\n", "        ", "maxl", "=", "45", "\n", "", "if", "target", "==", "'hi'", ":", "\n", "        ", "maxl", "=", "45", "\n", "\n", "", "POS_LINES", "=", "[", "]", "\n", "f", "=", "open", "(", "FNAME1", ")", "\n", "POS_LINES", "=", "f", ".", "readlines", "(", ")", "\n", "f", ".", "close", "(", ")", "\n", "POS_LINES", "=", "[", "x", ".", "strip", "(", ")", ".", "split", "(", ")", "[", ":", "maxl", "]", "for", "x", "in", "POS_LINES", "]", "\n", "\n", "NEG_LINES", "=", "[", "]", "\n", "f", "=", "open", "(", "FNAME2", ")", "\n", "NEG_LINES", "=", "f", ".", "readlines", "(", ")", "\n", "f", ".", "close", "(", ")", "\n", "NEG_LINES", "=", "[", "x", ".", "strip", "(", ")", ".", "split", "(", ")", "[", ":", "maxl", "]", "for", "x", "in", "NEG_LINES", "]", "\n", "\n", "sentences1", "=", "POS_LINES", "+", "NEG_LINES", "\n", "labels1", "=", "[", "1", "]", "*", "len", "(", "POS_LINES", ")", "+", "[", "0", "]", "*", "len", "(", "NEG_LINES", ")", "\n", "\n", "POS_LINES", "=", "[", "]", "\n", "f", "=", "open", "(", "FNAME3", ")", "\n", "POS_LINES", "=", "f", ".", "readlines", "(", ")", "\n", "f", ".", "close", "(", ")", "\n", "POS_LINES", "=", "[", "x", ".", "strip", "(", ")", ".", "split", "(", ")", "[", ":", "maxl", "]", "for", "x", "in", "POS_LINES", "]", "\n", "\n", "NEG_LINES", "=", "[", "]", "\n", "f", "=", "open", "(", "FNAME4", ")", "\n", "NEG_LINES", "=", "f", ".", "readlines", "(", ")", "\n", "f", ".", "close", "(", ")", "\n", "NEG_LINES", "=", "[", "x", ".", "strip", "(", ")", ".", "split", "(", ")", "[", ":", "maxl", "]", "for", "x", "in", "NEG_LINES", "]", "\n", "\n", "sentences2", "=", "POS_LINES", "+", "NEG_LINES", "\n", "labels2", "=", "[", "1", "]", "*", "len", "(", "POS_LINES", ")", "+", "[", "0", "]", "*", "len", "(", "NEG_LINES", ")", "\n", "\n", "POS_LINES", "=", "[", "]", "\n", "f", "=", "open", "(", "FNAME5", ")", "\n", "POS_LINES", "=", "f", ".", "readlines", "(", ")", "\n", "f", ".", "close", "(", ")", "\n", "POS_LINES", "=", "[", "x", ".", "strip", "(", ")", ".", "split", "(", ")", "[", ":", "maxl", "]", "for", "x", "in", "POS_LINES", "]", "\n", "\n", "NEG_LINES", "=", "[", "]", "\n", "f", "=", "open", "(", "FNAME6", ")", "\n", "NEG_LINES", "=", "f", ".", "readlines", "(", ")", "\n", "f", ".", "close", "(", ")", "\n", "NEG_LINES", "=", "[", "x", ".", "strip", "(", ")", ".", "split", "(", ")", "[", ":", "maxl", "]", "for", "x", "in", "NEG_LINES", "]", "\n", "\n", "sentences3", "=", "POS_LINES", "+", "NEG_LINES", "\n", "labels3", "=", "[", "1", "]", "*", "len", "(", "POS_LINES", ")", "+", "[", "0", "]", "*", "len", "(", "NEG_LINES", ")", "\n", "\n", "\n", "tokenized_texts_and_labels", "=", "[", "\n", "tokenize_and_preserve_labels", "(", "sent1", ",", "slabs1", ",", "sent2", ",", "slabs2", ",", "sent3", ",", "slabs3", ")", "\n", "for", "sent1", ",", "slabs1", ",", "sent2", ",", "slabs2", ",", "sent3", ",", "slabs3", "in", "zip", "(", "sentences1", ",", "labels1", ",", "sentences2", ",", "labels2", ",", "sentences3", ",", "labels3", ")", "\n", "]", "\n", "\n", "tokenized_texts1", "=", "[", "token_label_pair", "[", "2", "]", "for", "token_label_pair", "in", "tokenized_texts_and_labels", "]", "\n", "sent_labels1", "=", "[", "token_label_pair", "[", "1", "]", "for", "token_label_pair", "in", "tokenized_texts_and_labels", "]", "\n", "tokenized_texts2", "=", "[", "token_label_pair", "[", "5", "]", "for", "token_label_pair", "in", "tokenized_texts_and_labels", "]", "\n", "sent_labels2", "=", "[", "token_label_pair", "[", "4", "]", "for", "token_label_pair", "in", "tokenized_texts_and_labels", "]", "\n", "tokenized_texts3", "=", "[", "token_label_pair", "[", "8", "]", "for", "token_label_pair", "in", "tokenized_texts_and_labels", "]", "\n", "sent_labels3", "=", "[", "token_label_pair", "[", "7", "]", "for", "token_label_pair", "in", "tokenized_texts_and_labels", "]", "\n", "\n", "input_ids1", "=", "[", "token_label_pair", "[", "0", "]", "[", "'input_ids'", "]", ".", "squeeze", "(", ")", "for", "token_label_pair", "in", "tokenized_texts_and_labels", "]", "\n", "input_ids2", "=", "[", "token_label_pair", "[", "3", "]", "[", "'input_ids'", "]", ".", "squeeze", "(", ")", "for", "token_label_pair", "in", "tokenized_texts_and_labels", "]", "\n", "input_ids3", "=", "[", "token_label_pair", "[", "6", "]", "[", "'input_ids'", "]", ".", "squeeze", "(", ")", "for", "token_label_pair", "in", "tokenized_texts_and_labels", "]", "\n", "\n", "input_ids1", "=", "pad_sequences", "(", "[", "input_id", "for", "input_id", "in", "input_ids1", "]", ",", "\n", "maxlen", "=", "MAX_LEN", ",", "dtype", "=", "\"long\"", ",", "value", "=", "0.0", ",", "\n", "truncating", "=", "\"post\"", ",", "padding", "=", "\"post\"", ")", "\n", "\n", "input_ids2", "=", "pad_sequences", "(", "[", "input_id", "for", "input_id", "in", "input_ids2", "]", ",", "\n", "maxlen", "=", "MAX_LEN", ",", "dtype", "=", "\"long\"", ",", "value", "=", "0.0", ",", "\n", "truncating", "=", "\"post\"", ",", "padding", "=", "\"post\"", ")", "\n", "\n", "input_ids3", "=", "pad_sequences", "(", "[", "input_id", "for", "input_id", "in", "input_ids3", "]", ",", "\n", "maxlen", "=", "MAX_LEN", ",", "dtype", "=", "\"long\"", ",", "value", "=", "0.0", ",", "\n", "truncating", "=", "\"post\"", ",", "padding", "=", "\"post\"", ")", "\n", "\n", "attention_masks1", "=", "[", "[", "float", "(", "i", "!=", "0.0", ")", "for", "i", "in", "ii", "]", "for", "ii", "in", "input_ids1", "]", "\n", "attention_masks2", "=", "[", "[", "float", "(", "i", "!=", "0.0", ")", "for", "i", "in", "ii", "]", "for", "ii", "in", "input_ids2", "]", "\n", "attention_masks3", "=", "[", "[", "float", "(", "i", "!=", "0.0", ")", "for", "i", "in", "ii", "]", "for", "ii", "in", "input_ids3", "]", "\n", "\n", "assert", "(", "len", "(", "labels1", ")", "==", "len", "(", "labels2", ")", "==", "len", "(", "labels3", ")", ")", "\n", "\n", "print", "(", "\"SHAPE= \"", ",", "torch", ".", "tensor", "(", "input_ids1", ")", ".", "shape", ")", "\n", "\n", "return", "torch", ".", "tensor", "(", "input_ids1", ")", ",", "torch", ".", "tensor", "(", "attention_masks1", ")", ",", "torch", ".", "tensor", "(", "input_ids2", ")", ",", "torch", ".", "tensor", "(", "attention_masks2", ")", ",", "torch", ".", "tensor", "(", "input_ids3", ")", ",", "torch", ".", "tensor", "(", "attention_masks3", ")", ",", "torch", ".", "tensor", "(", "labels3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.joint_st.mlm_joint_ts_floods.create_loaders": [[220, 228], ["torch.utils.data.TensorDataset", "torch.utils.data.RandomSampler", "torch.utils.data.DataLoader"], "function", ["None"], ["", "def", "create_loaders", "(", "seq1", ",", "mask1", ",", "seq2", ",", "mask2", ",", "seq3", ",", "mask3", ",", "y", ")", ":", "\n", "# wrap tensors", "\n", "  ", "data", "=", "TensorDataset", "(", "seq1", ",", "mask1", ",", "seq2", ",", "mask2", ",", "seq3", ",", "mask3", ",", "y", ")", "\n", "# sampler for sampling the data during training", "\n", "sampler", "=", "RandomSampler", "(", "data", ")", "\n", "# dataLoader for train set", "\n", "dataloader", "=", "DataLoader", "(", "data", ",", "sampler", "=", "sampler", ",", "batch_size", "=", "batch_size", ")", "\n", "return", "dataloader", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.joint_st.mlm_joint_ts_floods.train": [[232, 301], ["model.train", "enumerate", "range", "steps.append", "batches.append", "len", "model.zero_grad", "model", "lossA.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "optimizer.step", "schedular.step", "predsA.detach().cpu().numpy.detach().cpu().numpy", "len", "r.to", "lossA.item", "model.parameters", "predsA.detach().cpu().numpy.detach().cpu", "predsA.detach().cpu().numpy.detach"], "function", ["home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.train"], ["", "def", "train", "(", "train_dataloader", ",", "optimizer", ",", "schedular", ",", "alpha", "=", "1.0", ",", "beta", "=", "1.0", ")", ":", "\n", "    ", "global", "model", "\n", "model", ".", "train", "(", ")", "\n", "\n", "total_loss", ",", "total_accuracy", "=", "0", ",", "0", "\n", "\n", "# empty list to save model predictions", "\n", "total_preds", "=", "[", "]", "\n", "\n", "# iterate over batches", "\n", "steps", "=", "[", "]", "\n", "batches", "=", "[", "]", "\n", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "train_dataloader", ")", ":", "\n", "        ", "steps", ".", "append", "(", "step", ")", "\n", "batches", ".", "append", "(", "batch", ")", "\n", "\n", "", "for", "index", "in", "range", "(", "len", "(", "steps", ")", ")", ":", "\n", "        ", "step", "=", "steps", "[", "index", "]", "\n", "batch", "=", "batches", "[", "index", "]", "\n", "\n", "# progress update after every 50 batches.", "\n", "#if step % 50 == 0 and not step == 0:", "\n", "#print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))", "\n", "\n", "# push the batch to gpu", "\n", "batch", "=", "[", "r", ".", "to", "(", "device", ")", "for", "r", "in", "batch", "]", "\n", "#batch = [r for r in batch]", "\n", "\n", "sent_id1", ",", "mask1", ",", "sent_id2", ",", "mask2", ",", "sent_id3", ",", "mask3", ",", "labels", "=", "batch", "\n", "\n", "# clear previously calculated gradients ", "\n", "model", ".", "zero_grad", "(", ")", "\n", "\n", "# get model predictions for the current batch", "\n", "predsA", "=", "model", "(", "sent_id1", ",", "mask1", ",", "sent_id2", ",", "mask2", ",", "sent_id3", ",", "mask3", ",", "labels", ")", "\n", "\n", "# compute the loss between actual and predicted values", "\n", "lossA", "=", "alpha", "*", "predsA", "#cross_entropy(predsA, labels)", "\n", "total_loss", "=", "total_loss", "+", "lossA", ".", "item", "(", ")", "\n", "\n", "# backward pass to calculate the gradients", "\n", "lossA", ".", "backward", "(", ")", "#retain_graph=True)", "\n", "\n", "# clip the the gradients to 1.0. It helps in preventing the exploding gradient problem", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "1.0", ")", "\n", "\n", "# update parameters", "\n", "optimizer", ".", "step", "(", ")", "\n", "schedular", ".", "step", "(", ")", "\n", "\n", "# model predictions are stored on GPU. So, push it to CPU", "\n", "predsA", "=", "predsA", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "'''\n        for index in range(len(slots_preds)):\n            slots_preds[index]=slots_preds[index].detach().cpu().numpy()\n        '''", "\n", "# append the model predictions", "\n", "#total_preds.append(predsA)", "\n", "\n", "# compute the training loss of the epoch", "\n", "", "avg_loss", "=", "total_loss", "/", "len", "(", "train_dataloader", ")", "\n", "\n", "# predictions are in the form of (no. of batches, size of batch, no. of classes).", "\n", "# reshape the predictions in form of (number of samples, no. of classes)", "\n", "#total_preds  = np.concatenate(total_preds, axis=0)", "\n", "\n", "#returns the loss and predictions", "\n", "return", "avg_loss", ",", "total_preds", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.joint_st.mlm_joint_ts_floods.evaluate": [[305, 365], ["model.eval", "enumerate", "len", "t.to", "torch.no_grad", "torch.no_grad", "model", "predsA.detach().cpu().numpy.detach().cpu().numpy", "predsB.detach().cpu().numpy.detach().cpu().numpy", "predsC.detach().cpu().numpy.detach().cpu().numpy", "type", "semi_loss.detach().cpu().numpy.detach().cpu().numpy", "cross_entropy", "predsA.detach().cpu().numpy.detach().cpu", "predsB.detach().cpu().numpy.detach().cpu", "predsC.detach().cpu().numpy.detach().cpu", "cross_entropy", "cross_entropy", "lossA.item", "lossA.item", "semi_loss.detach().cpu().numpy.item", "semi_loss.detach().cpu().numpy.detach().cpu", "predsA.detach().cpu().numpy.detach", "predsB.detach().cpu().numpy.detach", "predsC.detach().cpu().numpy.detach", "semi_loss.detach().cpu().numpy.detach"], "function", ["None"], ["", "def", "evaluate", "(", "val_dataloader", ",", "alpha", "=", "1.0", ",", "beta", "=", "1.0", ")", ":", "\n", "    ", "global", "model", "\n", "global", "gamma", "\n", "#print(\"\\nEvaluating...\")", "\n", "\n", "# deactivate dropout layers", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "total_loss", ",", "total_accuracy", "=", "0", ",", "0", "\n", "\n", "# empty list to save the model predictions", "\n", "total_preds", "=", "[", "]", "\n", "\n", "# iterate over batches", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "val_dataloader", ")", ":", "\n", "\n", "# Progress update every 50 batches.", "\n", "#if step % 500 == 0 and not step == 0:", "\n", "# Calculate elapsed time in minutes.", "\n", "#elapsed = format_time(time.time() - t0)", "\n", "\n", "# Report progress.", "\n", "#print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))", "\n", "\n", "# push the batch to gpu", "\n", "        ", "batch", "=", "[", "t", ".", "to", "(", "device", ")", "for", "t", "in", "batch", "]", "\n", "#batch = [t for t in batch]", "\n", "\n", "sent_id1", ",", "mask1", ",", "sent_id2", ",", "mask2", ",", "sent_id3", ",", "mask3", ",", "labels", "=", "batch", "\n", "\n", "# deactivate autograd", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# model predictions", "\n", "            ", "semi_loss", ",", "predsA", ",", "predsB", ",", "predsC", "=", "model", "(", "sent_id1", ",", "mask1", ",", "sent_id2", ",", "mask2", ",", "sent_id3", ",", "mask3", ")", "\n", "\n", "#preds = preds.squeeze()", "\n", "#print(\"preds shape: \", preds.shape)", "\n", "\n", "# compute the validation loss between actual and predicted values", "\n", "\n", "lossA", "=", "alpha", "*", "(", "cross_entropy", "(", "predsA", ",", "labels", ")", "+", "cross_entropy", "(", "predsB", ",", "labels", ")", "+", "cross_entropy", "(", "predsC", ",", "labels", ")", ")", "\n", "\n", "if", "type", "(", "semi_loss", ")", "==", "int", ":", "\n", "                ", "total_loss", "=", "total_loss", "+", "lossA", ".", "item", "(", ")", "+", "gamma", "*", "semi_loss", "\n", "", "else", ":", "\n", "                ", "total_loss", "=", "total_loss", "+", "lossA", ".", "item", "(", ")", "+", "gamma", "*", "semi_loss", ".", "item", "(", ")", "\n", "semi_loss", "=", "semi_loss", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "predsA", "=", "predsA", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "predsB", "=", "predsB", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "predsC", "=", "predsC", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "#total_preds.append(predsA)", "\n", "#outputs=outputs.detach().cpu().numpy()", "\n", "\n", "# compute the validation loss of the epoch", "\n", "", "", "avg_loss", "=", "total_loss", "/", "len", "(", "val_dataloader", ")", "\n", "\n", "# reshape the predictions in form of (number of samples, no. of classes)", "\n", "#total_preds  = np.concatenate(total_preds, axis=0)", "\n", "\n", "return", "avg_loss", ",", "total_preds", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.joint_st.mlm_joint_ts_floods.final_model": [[367, 473], ["mlm_joint_ts_floods.getData4Bert", "mlm_joint_ts_floods.getData4Bert", "mlm_joint_ts_floods.create_loaders", "mlm_joint_ts_floods.create_loaders", "model.to.to", "transformers.AdamW", "transformers.get_linear_schedule_with_warmup", "float", "time.time", "range", "print", "range", "mlm_joint_ts_floods.getData4Bert", "TEST_DATA.append", "mlm_joint_ts_floods.getData4Bert", "TEST_DATA.append", "model.to.parameters", "mlm_joint_ts_floods.train", "mlm_joint_ts_floods.evaluate", "train_losses.append", "valid_losses.append", "numpy.argmax", "sklearn.metrics.accuracy_score", "sklearn.metrics.f1_score", "sklearn.metrics.f1_score", "sklearn.metrics.f1_score", "print", "print", "print", "bert.roberta.embeddings.parameters", "range", "bert.bert.embeddings.parameters", "range", "torch.no_grad", "torch.no_grad", "model.to.", "preds.detach().cpu().numpy.detach().cpu().numpy", "len", "len", "len", "mlm_joint_ts_floods.is_changing", "time.time", "test_seq.to", "test_mask.to", "test_seq.to", "test_mask.to", "test_seq.to", "test_mask.to", "str", "str", "bert.roberta.encoder.layer[].parameters", "bert.bert.encoder.layer[].parameters", "preds.detach().cpu().numpy.detach().cpu", "round", "round", "preds.detach().cpu().numpy.detach"], "function", ["home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.getData4Bert", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.getData4Bert", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.create_loaders", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.create_loaders", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.getData4Bert", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.getData4Bert", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.train", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.evaluate", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.is_changing"], ["", "def", "final_model", "(", "train_FNAME1", ",", "train_FNAME2", ",", "train_FNAME3", ",", "train_FNAME4", ",", "train_FNAME5", ",", "train_FNAME6", ",", "val_FNAME1", ",", "val_FNAME2", ",", "val_FNAME3", ",", "val_FNAME4", ",", "val_FNAME5", ",", "val_FNAME6", ",", "mode", "=", "'ml'", ")", ":", "\n", "    ", "global", "model", ",", "base_model_name", "\n", "FREEZE", "=", "8", "\n", "############### FREEZE ##########################", "\n", "if", "FREEZE", "==", "0", ":", "\n", "        ", "pass", "\n", "", "else", ":", "\n", "        ", "if", "base_model_name", "==", "\"xlm-roberta-base\"", ":", "\n", "            ", "for", "param", "in", "bert", ".", "roberta", ".", "embeddings", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "", "for", "index", "in", "range", "(", "len", "(", "bert", ".", "roberta", ".", "encoder", ".", "layer", ")", ")", ":", "\n", "                ", "if", "index", "<", "FREEZE", ":", "\n", "                    ", "for", "param", "in", "bert", ".", "roberta", ".", "encoder", ".", "layer", "[", "index", "]", ".", "parameters", "(", ")", ":", "\n", "                        ", "param", ".", "requires_grad", "=", "False", "\n", "", "", "", "", "else", ":", "\n", "            ", "for", "param", "in", "bert", ".", "bert", ".", "embeddings", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "", "for", "index", "in", "range", "(", "len", "(", "bert", ".", "bert", ".", "encoder", ".", "layer", ")", ")", ":", "\n", "                ", "if", "index", "<", "FREEZE", ":", "\n", "                    ", "for", "param", "in", "bert", ".", "bert", ".", "encoder", ".", "layer", "[", "index", "]", ".", "parameters", "(", ")", ":", "\n", "                        ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "############### LOAD DATASET ##########################", "\n", "", "", "", "", "", "MAX_LEN", "=", "45", "\n", "train_seq1", ",", "train_mask1", ",", "train_seq2", ",", "train_mask2", ",", "train_seq3", ",", "train_mask3", ",", "train_y", "=", "getData4Bert", "(", "train_FNAME1", ",", "train_FNAME2", ",", "train_FNAME3", ",", "train_FNAME4", ",", "train_FNAME5", ",", "train_FNAME6", ",", "tokenizer", ",", "MAX_LEN", "=", "MAX_LEN", ")", "\n", "val_seq1", ",", "val_mask1", ",", "val_seq2", ",", "val_mask2", ",", "val_seq3", ",", "val_mask3", ",", "val_y", "=", "getData4Bert", "(", "val_FNAME1", ",", "val_FNAME2", ",", "val_FNAME3", ",", "val_FNAME4", ",", "val_FNAME5", ",", "val_FNAME6", ",", "tokenizer", ",", "MAX_LEN", "=", "MAX_LEN", ")", "\n", "\n", "TEST_DATA", "=", "[", "]", "\n", "if", "mode", "==", "'ml'", ":", "\n", "        ", "td", "=", "getData4Bert", "(", "'data/appen/kf_pos'", ",", "'data/appen/kf_neg'", ",", "'data/appen/kf_pos'", ",", "'data/appen/kf_neg'", ",", "'data/appen/kf_pos'", ",", "'data/appen/kf_neg'", ",", "tokenizer", ",", "MAX_LEN", ")", "\n", "TEST_DATA", ".", "append", "(", "td", "[", "4", ":", "]", ")", "\n", "", "if", "mode", "==", "'hi'", ":", "\n", "        ", "td", "=", "getData4Bert", "(", "'data/appen/ni_pos'", ",", "'data/appen/ni_neg'", ",", "'data/appen/ni_pos'", ",", "'data/appen/ni_neg'", ",", "'data/appen/ni_pos'", ",", "'data/appen/ni_neg'", ",", "tokenizer", ",", "MAX_LEN", ")", "\n", "TEST_DATA", ".", "append", "(", "td", "[", "4", ":", "]", ")", "\n", "\n", "", "train_dataloader", "=", "create_loaders", "(", "train_seq1", ",", "train_mask1", ",", "train_seq2", ",", "train_mask2", ",", "train_seq3", ",", "train_mask3", ",", "train_y", ")", "\n", "val_dataloader", "=", "create_loaders", "(", "val_seq1", ",", "val_mask1", ",", "val_seq2", ",", "val_mask2", ",", "val_seq3", ",", "val_mask3", ",", "val_y", ")", "\n", "\n", "# push the model to GPU", "\n", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "\n", "# define the optimizer", "\n", "optimizer", "=", "AdamW", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", "# learning rate", "\n", "\n", "schedular", "=", "get_linear_schedule_with_warmup", "(", "\n", "optimizer", ",", "\n", "num_warmup_steps", "=", "0", ",", "\n", "num_training_steps", "=", "len", "(", "train_dataloader", ")", "*", "epochs", ",", "\n", ")", "\n", "\n", "# set initial loss to infinite", "\n", "best_valid_loss", "=", "float", "(", "'inf'", ")", "\n", "\n", "# empty lists to store training and validation loss of each epoch", "\n", "train_losses", "=", "[", "]", "\n", "valid_losses", "=", "[", "]", "\n", "best_epoch", "=", "0", "\n", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "#for each epoch", "\n", "for", "epoch", "in", "range", "(", "epochs", ")", ":", "\n", "\n", "#print('\\nEpoch {:} / {:}'.format(epoch + 1, epochs), end=\" \")", "\n", "\n", "#train model", "\n", "        ", "train_loss", ",", "_", "=", "train", "(", "train_dataloader", ",", "optimizer", ",", "schedular", ")", "\n", "\n", "#evaluate model", "\n", "valid_loss", ",", "_", "=", "evaluate", "(", "val_dataloader", ")", "\n", "\n", "#save the best model", "\n", "if", "valid_loss", "<", "best_valid_loss", "and", "epoch", ">", "5", ":", "\n", "            ", "best_valid_loss", "=", "valid_loss", "\n", "#torch.save(model.state_dict(), SCRATCH_FNAME)", "\n", "best_epoch", "=", "epoch", "+", "1", "\n", "\n", "# append training and validation loss", "\n", "", "train_losses", ".", "append", "(", "train_loss", ")", "\n", "valid_losses", ".", "append", "(", "valid_loss", ")", "\n", "\n", "#print(f'Training Loss: {train_loss:.5f} Validation Loss: {valid_loss:.5f}')", "\n", "\n", "if", "epoch", ">", "patience", ":", "\n", "            ", "if", "not", "is_changing", "(", "valid_losses", "[", "-", "1", "*", "patience", ":", "]", ",", "delta", ")", ":", "\n", "#print(\"STOPPING - NO CHANGE\")", "\n", "                ", "break", "\n", "\n", "#print('BEST EPOCH= ', best_epoch)", "\n", "", "", "", "print", "(", "\"--- %s seconds ---\"", "%", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "\n", "############### predict ########################## ", "\n", "\n", "for", "index", "in", "range", "(", "1", ")", ":", "\n", "        ", "test_seq", ",", "test_mask", ",", "test_y", "=", "TEST_DATA", "[", "index", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "preds", "=", "model", "(", "test_seq", ".", "to", "(", "device", ")", ",", "test_mask", ".", "to", "(", "device", ")", ",", "test_seq", ".", "to", "(", "device", ")", ",", "test_mask", ".", "to", "(", "device", ")", ",", "test_seq", ".", "to", "(", "device", ")", ",", "test_mask", ".", "to", "(", "device", ")", ",", "TEST", "=", "True", ")", "\n", "preds", "=", "preds", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "", "preds", "=", "np", ".", "argmax", "(", "preds", ",", "axis", "=", "1", ")", "\n", "#print(classification_report(test_y, preds, digits=4))", "\n", "acc", "=", "accuracy_score", "(", "test_y", ",", "preds", ")", "\n", "f1micro", "=", "f1_score", "(", "test_y", ",", "preds", ",", "average", "=", "'micro'", ")", "\n", "f1macro", "=", "f1_score", "(", "test_y", ",", "preds", ",", "average", "=", "'macro'", ")", "\n", "f1weighted", "=", "f1_score", "(", "test_y", ",", "preds", ",", "average", "=", "'weighted'", ")", "\n", "print", "(", "\"Acc: \"", "+", "str", "(", "round", "(", "acc", "*", "100", ",", "4", ")", ")", ")", "\n", "print", "(", "\"F1: \"", "+", "str", "(", "round", "(", "f1weighted", "*", "100", ",", "4", ")", ")", ")", "\n", "print", "(", "\"--------------\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.mBERT.__init__": [[28, 33], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.mBERT.__init__"], ["    ", "def", "__init__", "(", "self", ",", "bert", ",", "max_length", ",", "num_classes", ",", "hidden_layers", ",", "drop", "=", "0.1", ")", ":", "\n", "\n", "      ", "super", "(", "mBERT", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "bert", "=", "bert", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.mBERT.forward": [[35, 48], ["mlm_movie.mBERT.bert", "mlm_movie.mBERT.bert"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sent_id_1", ",", "mask_1", ",", "labels", "=", "None", ")", ":", "\n", "\n", "      ", "if", "labels", "is", "None", ":", "\n", "          ", "out", "=", "self", ".", "bert", "(", "sent_id_1", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "mask_1", ")", "\n", "", "else", ":", "\n", "          ", "out", "=", "self", ".", "bert", "(", "sent_id_1", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "mask_1", ",", "\n", "labels", "=", "labels", ")", "\n", "\n", "", "return", "out", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.is_changing": [[50, 71], ["len", "set", "set.remove", "max", "len", "max", "abs"], "function", ["None"], ["", "", "def", "is_changing", "(", "arr", ",", "delta", ")", ":", "\n", "    ", "curr", "=", "arr", "[", "0", "]", "\n", "count", "=", "len", "(", "arr", ")", "\n", "for", "x", "in", "arr", "[", "1", ":", "]", ":", "\n", "        ", "if", "x", ">", "curr", ":", "\n", "            ", "count", "-=", "1", "\n", "curr", "=", "x", "\n", "", "", "if", "count", "<", "3", ":", "\n", "        ", "return", "False", "\n", "\n", "", "new_list", "=", "set", "(", "arr", ")", "\n", "new_list", ".", "remove", "(", "max", "(", "new_list", ")", ")", "\n", "curr", "=", "max", "(", "new_list", ")", "\n", "count", "=", "len", "(", "arr", ")", "\n", "for", "x", "in", "arr", ":", "\n", "        ", "if", "abs", "(", "x", "-", "curr", ")", "<", "delta", ":", "\n", "          ", "count", "-=", "1", "\n", "", "", "if", "count", "<", "2", ":", "\n", "        ", "return", "False", "\n", "\n", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.tokenize_and_preserve_labels": [[72, 91], ["tokenizer.tokenize", "tokenized_sentence.extend", "x.isdigit"], "function", ["home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.tokenize"], ["", "def", "tokenize_and_preserve_labels", "(", "sentence", ",", "sent_labels", ")", ":", "\n", "    ", "tokenized_sentence", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "#print(sentence)", "\n", "#print(sent_labels)", "\n", "\n", "for", "word", "in", "sentence", ":", "\n", "# Tokenize the word and count # of subwords the word is broken into", "\n", "#splits = word.split()", "\n", "#splits = [x for x in splits if not x.isdigit()]", "\n", "#splits = splits[:500]", "\n", "#word = \" \".join(splits)", "\n", "        ", "tokenized_word", "=", "tokenizer", ".", "tokenize", "(", "word", ")", "\n", "# Add the tokenized word to the final tokenized word list", "\n", "tokenized_sentence", ".", "extend", "(", "tokenized_word", ")", "\n", "\n", "", "tokenized_sentence", "=", "[", "x", "for", "x", "in", "tokenized_sentence", "if", "not", "x", ".", "isdigit", "(", ")", "]", "\n", "\n", "return", "tokenized_sentence", ",", "sent_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.tokenize": [[92, 104], ["tokenizer.tokenize", "tokenized_sentence.extend"], "function", ["home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.tokenize"], ["", "def", "tokenize", "(", "sentence", ")", ":", "\n", "    ", "tokenized_sentence", "=", "[", "]", "\n", "\n", "for", "word", "in", "sentence", ":", "\n", "\n", "# Tokenize the word and count # of subwords the word is broken into", "\n", "        ", "tokenized_word", "=", "tokenizer", ".", "tokenize", "(", "word", ")", "\n", "\n", "# Add the tokenized word to the final tokenized word list", "\n", "tokenized_sentence", ".", "extend", "(", "tokenized_word", ")", "\n", "\n", "", "return", "tokenized_sentence", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.tokenize_and_encode": [[106, 112], ["tokenizer.batch_encode_plus", "split.tolist"], "function", ["None"], ["", "def", "tokenize_and_encode", "(", "split", ",", "tokenizer", ",", "max_length", ")", ":", "\n", "    ", "return", "tokenizer", ".", "batch_encode_plus", "(", "\n", "split", ".", "tolist", "(", ")", ",", "\n", "max_length", "=", "max_length", ",", "\n", "pad_to_max_length", "=", "True", ",", "\n", "truncation", "=", "True", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.getData4Bert": [[114, 155], ["open", "open.readlines", "open.close", "open", "open.readlines", "open.close", "keras.preprocessing.sequence.pad_sequences", "len", "int", "mlm_movie.tokenize_and_preserve_labels", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "x.strip().split", "x.strip().split", "len", "len", "max", "zip", "tokenizer.convert_tokens_to_ids", "float", "x.strip", "x.strip"], "function", ["home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.tokenize_and_preserve_labels"], ["", "def", "getData4Bert", "(", "FNAME1", ",", "FNAME2", ",", "tokenizer", ",", "MAX_LEN", "=", "None", ")", ":", "\n", "\n", "    ", "POS_LINES", "=", "[", "]", "\n", "f", "=", "open", "(", "FNAME1", ")", "\n", "POS_LINES", "=", "f", ".", "readlines", "(", ")", "\n", "f", ".", "close", "(", ")", "\n", "POS_LINES", "=", "[", "x", ".", "strip", "(", ")", ".", "split", "(", ")", "[", ":", "200", "]", "for", "x", "in", "POS_LINES", "]", "\n", "\n", "NEG_LINES", "=", "[", "]", "\n", "f", "=", "open", "(", "FNAME2", ")", "\n", "NEG_LINES", "=", "f", ".", "readlines", "(", ")", "\n", "f", ".", "close", "(", ")", "\n", "NEG_LINES", "=", "[", "x", ".", "strip", "(", ")", ".", "split", "(", ")", "[", ":", "200", "]", "for", "x", "in", "NEG_LINES", "]", "\n", "\n", "sentences", "=", "POS_LINES", "+", "NEG_LINES", "\n", "labels", "=", "[", "1", "]", "*", "len", "(", "POS_LINES", ")", "+", "[", "0", "]", "*", "len", "(", "NEG_LINES", ")", "\n", "\n", "seq_len", "=", "[", "len", "(", "sentence", ")", "for", "sentence", "in", "sentences", "]", "\n", "\n", "if", "MAX_LEN", "is", "None", ":", "\n", "        ", "MAX_LEN", "=", "int", "(", "max", "(", "seq_len", ")", ")", "\n", "\n", "\n", "", "tokenized_texts_and_labels", "=", "[", "\n", "tokenize_and_preserve_labels", "(", "sent", ",", "slabs", ")", "\n", "for", "sent", ",", "slabs", "in", "zip", "(", "sentences", ",", "labels", ")", "\n", "]", "\n", "\n", "sent_labels", "=", "labels", "\n", "\n", "tokenized_texts", "=", "[", "token_label_pair", "[", "0", "]", "for", "token_label_pair", "in", "tokenized_texts_and_labels", "]", "\n", "sent_labels", "=", "[", "token_label_pair", "[", "1", "]", "for", "token_label_pair", "in", "tokenized_texts_and_labels", "]", "\n", "\n", "input_ids", "=", "pad_sequences", "(", "[", "tokenizer", ".", "convert_tokens_to_ids", "(", "txt", ")", "for", "txt", "in", "tokenized_texts", "]", ",", "\n", "maxlen", "=", "MAX_LEN", ",", "dtype", "=", "\"long\"", ",", "value", "=", "0.0", ",", "\n", "truncating", "=", "\"post\"", ",", "padding", "=", "\"post\"", ")", "\n", "\n", "attention_masks", "=", "[", "[", "float", "(", "i", "!=", "0.0", ")", "for", "i", "in", "ii", "]", "for", "ii", "in", "input_ids", "]", "\n", "\n", "\n", "return", "torch", ".", "tensor", "(", "input_ids", ")", ",", "torch", ".", "tensor", "(", "attention_masks", ")", ",", "torch", ".", "tensor", "(", "sent_labels", ")", ",", "MAX_LEN", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.create_loaders": [[157, 165], ["torch.utils.data.TensorDataset", "torch.utils.data.RandomSampler", "torch.utils.data.DataLoader"], "function", ["None"], ["", "def", "create_loaders", "(", "seq", ",", "mask", ",", "y", ")", ":", "\n", "# wrap tensors", "\n", "  ", "data", "=", "TensorDataset", "(", "seq", ",", "mask", ",", "y", ")", "\n", "# sampler for sampling the data during training", "\n", "sampler", "=", "RandomSampler", "(", "data", ")", "\n", "# dataLoader for train set", "\n", "dataloader", "=", "DataLoader", "(", "data", ",", "sampler", "=", "sampler", ",", "batch_size", "=", "batch_size", ")", "\n", "return", "dataloader", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.train": [[169, 238], ["model.train", "enumerate", "range", "steps.append", "batches.append", "len", "model.zero_grad", "model", "lossA.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "optimizer.step", "schedular.step", "predsA.detach().cpu().numpy.detach().cpu().numpy", "len", "r.to", "lossA.item", "model.parameters", "predsA.detach().cpu().numpy.detach().cpu", "predsA.detach().cpu().numpy.detach"], "function", ["home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.train"], ["", "def", "train", "(", "model", ",", "train_dataloader", ",", "optimizer", ",", "schedular", ",", "alpha", "=", "1.0", ",", "beta", "=", "1.0", ")", ":", "\n", "\n", "    ", "model", ".", "train", "(", ")", "\n", "\n", "total_loss", ",", "total_accuracy", "=", "0", ",", "0", "\n", "\n", "# empty list to save model predictions", "\n", "total_preds", "=", "[", "]", "\n", "\n", "# iterate over batches", "\n", "steps", "=", "[", "]", "\n", "batches", "=", "[", "]", "\n", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "train_dataloader", ")", ":", "\n", "        ", "steps", ".", "append", "(", "step", ")", "\n", "batches", ".", "append", "(", "batch", ")", "\n", "\n", "", "for", "index", "in", "range", "(", "len", "(", "steps", ")", ")", ":", "\n", "        ", "step", "=", "steps", "[", "index", "]", "\n", "batch", "=", "batches", "[", "index", "]", "\n", "\n", "# progress update after every 50 batches.", "\n", "#if step % 50 == 0 and not step == 0:", "\n", "#print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))", "\n", "\n", "# push the batch to gpu", "\n", "batch", "=", "[", "r", ".", "to", "(", "device", ")", "for", "r", "in", "batch", "]", "\n", "#batch = [r for r in batch]", "\n", "\n", "sent_id", ",", "mask", ",", "labels", "=", "batch", "\n", "\n", "# clear previously calculated gradients ", "\n", "model", ".", "zero_grad", "(", ")", "\n", "\n", "# get model predictions for the current batch", "\n", "predsA", "=", "model", "(", "sent_id", ",", "mask", ",", "labels", ")", "\n", "\n", "# compute the loss between actual and predicted values", "\n", "lossA", "=", "alpha", "*", "predsA", "#cross_entropy(predsA, labels)", "\n", "total_loss", "=", "total_loss", "+", "lossA", ".", "item", "(", ")", "\n", "\n", "# backward pass to calculate the gradients", "\n", "lossA", ".", "backward", "(", ")", "#retain_graph=True)", "\n", "\n", "# clip the the gradients to 1.0. It helps in preventing the exploding gradient problem", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "1.0", ")", "\n", "\n", "# update parameters", "\n", "optimizer", ".", "step", "(", ")", "\n", "schedular", ".", "step", "(", ")", "\n", "\n", "# model predictions are stored on GPU. So, push it to CPU", "\n", "predsA", "=", "predsA", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "'''\n        for index in range(len(slots_preds)):\n            slots_preds[index]=slots_preds[index].detach().cpu().numpy()\n        '''", "\n", "# append the model predictions", "\n", "#total_preds.append(predsA)", "\n", "\n", "# compute the training loss of the epoch", "\n", "", "avg_loss", "=", "total_loss", "/", "len", "(", "train_dataloader", ")", "\n", "\n", "# predictions are in the form of (no. of batches, size of batch, no. of classes).", "\n", "# reshape the predictions in form of (number of samples, no. of classes)", "\n", "#total_preds  = np.concatenate(total_preds, axis=0)", "\n", "\n", "#returns the loss and predictions", "\n", "return", "model", ",", "avg_loss", ",", "total_preds", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.evaluate": [[242, 295], ["model.eval", "enumerate", "len", "t.to", "torch.no_grad", "torch.no_grad", "model", "predsA.detach().cpu().numpy.detach().cpu().numpy", "cross_entropy", "lossA.item", "predsA.detach().cpu().numpy.detach().cpu", "predsA.detach().cpu().numpy.detach"], "function", ["None"], ["", "def", "evaluate", "(", "model", ",", "val_dataloader", ",", "alpha", "=", "1.0", ",", "beta", "=", "1.0", ")", ":", "\n", "\n", "#print(\"\\nEvaluating...\")", "\n", "\n", "# deactivate dropout layers", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "\n", "total_loss", ",", "total_accuracy", "=", "0", ",", "0", "\n", "\n", "# empty list to save the model predictions", "\n", "total_preds", "=", "[", "]", "\n", "\n", "# iterate over batches", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "val_dataloader", ")", ":", "\n", "\n", "# Progress update every 50 batches.", "\n", "#if step % 500 == 0 and not step == 0:", "\n", "# Calculate elapsed time in minutes.", "\n", "#elapsed = format_time(time.time() - t0)", "\n", "\n", "# Report progress.", "\n", "#print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))", "\n", "\n", "# push the batch to gpu", "\n", "        ", "batch", "=", "[", "t", ".", "to", "(", "device", ")", "for", "t", "in", "batch", "]", "\n", "#batch = [t for t in batch]", "\n", "\n", "sent_id", ",", "mask", ",", "labels", "=", "batch", "\n", "\n", "# deactivate autograd", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# model predictions", "\n", "            ", "predsA", "=", "model", "(", "sent_id", ",", "mask", ")", "\n", "\n", "#preds = preds.squeeze()", "\n", "#print(\"preds shape: \", preds.shape)", "\n", "\n", "# compute the validation loss between actual and predicted values", "\n", "\n", "lossA", "=", "alpha", "*", "cross_entropy", "(", "predsA", ",", "labels", ")", "\n", "total_loss", "=", "total_loss", "+", "lossA", ".", "item", "(", ")", "\n", "\n", "predsA", "=", "predsA", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "#total_preds.append(predsA)", "\n", "#outputs=outputs.detach().cpu().numpy()", "\n", "\n", "# compute the validation loss of the epoch", "\n", "", "", "avg_loss", "=", "total_loss", "/", "len", "(", "val_dataloader", ")", "\n", "\n", "# reshape the predictions in form of (number of samples, no. of classes)", "\n", "#total_preds  = np.concatenate(total_preds, axis=0)", "\n", "\n", "return", "model", ",", "avg_loss", ",", "total_preds", "\n", "\n"]], "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.final_model": [[296, 433], ["mlm_movie.getData4Bert", "mlm_movie.getData4Bert", "mlm_movie.create_loaders", "mlm_movie.create_loaders", "mlm_movie.mBERT", "model.to.to", "transformers.AdamW", "transformers.get_linear_schedule_with_warmup", "float", "range", "range", "mlm_movie.getData4Bert", "TEST_DATA.append", "mlm_movie.getData4Bert", "TEST_DATA.append", "mlm_movie.getData4Bert", "TEST_DATA.append", "mlm_movie.getData4Bert", "TEST_DATA.append", "mlm_movie.getData4Bert", "TEST_DATA.append", "mlm_movie.getData4Bert", "TEST_DATA.append", "mlm_movie.getData4Bert", "TEST_DATA.append", "mlm_movie.getData4Bert", "TEST_DATA.append", "transformers.RobertaForSequenceClassification.from_pretrained", "transformers.BertForSequenceClassification.from_pretrained", "model.to.parameters", "mlm_movie.train", "mlm_movie.evaluate", "train_losses.append", "valid_losses.append", "numpy.argmax", "sklearn.metrics.accuracy_score", "sklearn.metrics.f1_score", "sklearn.metrics.f1_score", "sklearn.metrics.f1_score", "round", "round", "accuracy_scores.append", "f1_scores.append", "BertForSequenceClassification.from_pretrained.roberta.embeddings.parameters", "range", "BertForSequenceClassification.from_pretrained.bert.embeddings.parameters", "range", "torch.no_grad", "torch.no_grad", "model.to.", "preds.detach().cpu().numpy.detach().cpu().numpy", "len", "len", "len", "mlm_movie.is_changing", "test_seq.to", "test_mask.to", "BertForSequenceClassification.from_pretrained.roberta.encoder.layer[].parameters", "BertForSequenceClassification.from_pretrained.bert.encoder.layer[].parameters", "preds.detach().cpu().numpy.detach().cpu", "preds.detach().cpu().numpy.detach"], "function", ["home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.getData4Bert", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.getData4Bert", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.create_loaders", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.create_loaders", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.getData4Bert", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.getData4Bert", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.getData4Bert", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.getData4Bert", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.getData4Bert", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.getData4Bert", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.getData4Bert", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.getData4Bert", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.train", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.evaluate", "home.repos.pwc.inspect_result.jitinkrishnan_transliteration-hindi-malayalam.baselines.mlm_movie.is_changing"], ["", "def", "final_model", "(", "train_FNAME1", ",", "train_FNAME2", ",", "val_FNAME1", ",", "val_FNAME2", ",", "mode", "=", "'ml'", ")", ":", "\n", "    ", "global", "MAX_LEN", ",", "base_model_name", ",", "FREEZE", ",", "SAVE", "\n", "\n", "############### LOAD DATASET ##########################", "\n", "train_seq", ",", "train_mask", ",", "train_y", ",", "MAX_LEN", "=", "getData4Bert", "(", "train_FNAME1", ",", "train_FNAME2", ",", "tokenizer", ",", "MAX_LEN", "=", "MAX_LEN", ")", "\n", "val_seq", ",", "val_mask", ",", "val_y", ",", "_", "=", "getData4Bert", "(", "val_FNAME1", ",", "val_FNAME2", ",", "tokenizer", ",", "MAX_LEN", ")", "\n", "\n", "TEST_DATA", "=", "[", "]", "\n", "if", "mode", "==", "'ml'", ":", "\n", "        ", "td", "=", "getData4Bert", "(", "'data/ml_pos'", ",", "'data/ml_neg'", ",", "tokenizer", ",", "MAX_LEN", ")", "\n", "TEST_DATA", ".", "append", "(", "td", "[", ":", "3", "]", ")", "\n", "td", "=", "getData4Bert", "(", "'data/ml_pos_ro'", ",", "'data/ml_neg_ro'", ",", "tokenizer", ",", "MAX_LEN", ")", "\n", "TEST_DATA", ".", "append", "(", "td", "[", ":", "3", "]", ")", "\n", "td", "=", "getData4Bert", "(", "'data/imdb_test_pos'", ",", "'data/imdb_test_neg'", ",", "tokenizer", ",", "MAX_LEN", ")", "\n", "TEST_DATA", ".", "append", "(", "td", "[", ":", "3", "]", ")", "\n", "td", "=", "getData4Bert", "(", "'data/imdb_sent_pos'", ",", "'data/imdb_sent_neg'", ",", "tokenizer", ",", "MAX_LEN", ")", "\n", "TEST_DATA", ".", "append", "(", "td", "[", ":", "3", "]", ")", "\n", "", "if", "mode", "==", "'hi'", ":", "\n", "        ", "td", "=", "getData4Bert", "(", "'data/hi_pos'", ",", "'data/hi_neg'", ",", "tokenizer", ",", "MAX_LEN", ")", "\n", "TEST_DATA", ".", "append", "(", "td", "[", ":", "3", "]", ")", "\n", "td", "=", "getData4Bert", "(", "'data/hi_pos_ro'", ",", "'data/hi_neg_ro'", ",", "tokenizer", ",", "MAX_LEN", ")", "\n", "TEST_DATA", ".", "append", "(", "td", "[", ":", "3", "]", ")", "\n", "td", "=", "getData4Bert", "(", "'data/imdb_test_pos'", ",", "'data/imdb_test_neg'", ",", "tokenizer", ",", "MAX_LEN", ")", "\n", "TEST_DATA", ".", "append", "(", "td", "[", ":", "3", "]", ")", "\n", "td", "=", "getData4Bert", "(", "'data/imdb_sent_pos'", ",", "'data/imdb_sent_neg'", ",", "tokenizer", ",", "MAX_LEN", ")", "\n", "TEST_DATA", ".", "append", "(", "td", "[", ":", "3", "]", ")", "\n", "\n", "", "train_dataloader", "=", "create_loaders", "(", "train_seq", ",", "train_mask", ",", "train_y", ")", "\n", "val_dataloader", "=", "create_loaders", "(", "val_seq", ",", "val_mask", ",", "val_y", ")", "\n", "\n", "\n", "############### LOAD BERT ##########################", "\n", "bert", "=", "None", "\n", "if", "base_model_name", "==", "\"xlm-roberta-base\"", ":", "\n", "        ", "bert", "=", "RobertaForSequenceClassification", ".", "from_pretrained", "(", "base_model_name", ",", "num_labels", "=", "2", ",", "output_attentions", "=", "False", ",", "output_hidden_states", "=", "True", ",", "return_dict", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "bert", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "base_model_name", ",", "num_labels", "=", "2", ",", "output_attentions", "=", "False", ",", "output_hidden_states", "=", "True", ",", "return_dict", "=", "True", ")", "\n", "\n", "############### FREEZE ##########################", "\n", "", "if", "FREEZE", "==", "0", ":", "\n", "        ", "pass", "\n", "", "else", ":", "\n", "        ", "if", "base_model_name", "==", "\"xlm-roberta-base\"", ":", "\n", "            ", "for", "param", "in", "bert", ".", "roberta", ".", "embeddings", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "", "for", "index", "in", "range", "(", "len", "(", "bert", ".", "roberta", ".", "encoder", ".", "layer", ")", ")", ":", "\n", "                ", "if", "index", "<", "FREEZE", ":", "\n", "                    ", "for", "param", "in", "bert", ".", "roberta", ".", "encoder", ".", "layer", "[", "index", "]", ".", "parameters", "(", ")", ":", "\n", "                        ", "param", ".", "requires_grad", "=", "False", "\n", "", "", "", "", "else", ":", "\n", "            ", "for", "param", "in", "bert", ".", "bert", ".", "embeddings", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "", "for", "index", "in", "range", "(", "len", "(", "bert", ".", "bert", ".", "encoder", ".", "layer", ")", ")", ":", "\n", "                ", "if", "index", "<", "FREEZE", ":", "\n", "                    ", "for", "param", "in", "bert", ".", "bert", ".", "encoder", ".", "layer", "[", "index", "]", ".", "parameters", "(", ")", ":", "\n", "                        ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "\n", "############### LOAD BERT ##########################", "\n", "", "", "", "", "", "model", "=", "mBERT", "(", "bert", ",", "MAX_LEN", ",", "2", ",", "hidden_layers", ")", "\n", "\n", "# push the model to GPU", "\n", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "\n", "# define the optimizer", "\n", "optimizer", "=", "AdamW", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", "# learning rate", "\n", "\n", "schedular", "=", "get_linear_schedule_with_warmup", "(", "\n", "optimizer", ",", "\n", "num_warmup_steps", "=", "0", ",", "\n", "num_training_steps", "=", "len", "(", "train_dataloader", ")", "*", "epochs", ",", "\n", ")", "\n", "\n", "# set initial loss to infinite", "\n", "best_valid_loss", "=", "float", "(", "'inf'", ")", "\n", "\n", "# empty lists to store training and validation loss of each epoch", "\n", "train_losses", "=", "[", "]", "\n", "valid_losses", "=", "[", "]", "\n", "best_epoch", "=", "0", "\n", "\n", "#for each epoch", "\n", "for", "epoch", "in", "range", "(", "epochs", ")", ":", "\n", "\n", "#print('\\nEpoch {:} / {:}'.format(epoch + 1, epochs), end=\" \")", "\n", "\n", "#train model", "\n", "        ", "model", ",", "train_loss", ",", "_", "=", "train", "(", "model", ",", "train_dataloader", ",", "optimizer", ",", "schedular", ")", "\n", "\n", "#evaluate model", "\n", "model", ",", "valid_loss", ",", "_", "=", "evaluate", "(", "model", ",", "val_dataloader", ")", "\n", "\n", "#save the best model", "\n", "if", "valid_loss", "<", "best_valid_loss", "and", "epoch", ">", "5", ":", "\n", "            ", "best_valid_loss", "=", "valid_loss", "\n", "best_epoch", "=", "epoch", "+", "1", "\n", "\n", "# append training and validation loss", "\n", "", "train_losses", ".", "append", "(", "train_loss", ")", "\n", "valid_losses", ".", "append", "(", "valid_loss", ")", "\n", "\n", "#print(f'Training Loss: {train_loss:.5f} Validation Loss: {valid_loss:.5f}')", "\n", "\n", "if", "epoch", ">", "patience", ":", "\n", "            ", "if", "not", "is_changing", "(", "valid_losses", "[", "-", "1", "*", "patience", ":", "]", ",", "delta", ")", ":", "\n", "#print(\"STOPPING - NO CHANGE\")", "\n", "                ", "break", "\n", "\n", "#print('BEST EPOCH= ', best_epoch)", "\n", "\n", "############### predict ########################## ", "\n", "\n", "", "", "", "accuracy_scores", "=", "[", "]", "\n", "f1_scores", "=", "[", "]", "\n", "\n", "for", "index", "in", "range", "(", "3", ")", ":", "\n", "        ", "test_seq", ",", "test_mask", ",", "test_y", "=", "TEST_DATA", "[", "index", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "preds", "=", "model", "(", "test_seq", ".", "to", "(", "device", ")", ",", "test_mask", ".", "to", "(", "device", ")", ")", "\n", "preds", "=", "preds", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "", "preds", "=", "np", ".", "argmax", "(", "preds", ",", "axis", "=", "1", ")", "\n", "#print(classification_report(test_y, preds, digits=4))", "\n", "acc", "=", "accuracy_score", "(", "test_y", ",", "preds", ")", "\n", "f1micro", "=", "f1_score", "(", "test_y", ",", "preds", ",", "average", "=", "'micro'", ")", "\n", "f1macro", "=", "f1_score", "(", "test_y", ",", "preds", ",", "average", "=", "'macro'", ")", "\n", "f1weighted", "=", "f1_score", "(", "test_y", ",", "preds", ",", "average", "=", "'weighted'", ")", "\n", "acc", "=", "round", "(", "acc", "*", "100", ",", "2", ")", "\n", "f1", "=", "round", "(", "f1weighted", "*", "100", ",", "2", ")", "\n", "#print(\"Intent Acc.: \"+str(acc))", "\n", "#print(\"F1 micro.: \"+str(round(f1micro*100,4)))", "\n", "#print(\"F1 macro.: \"+str(round(f1macro*100,4)))", "\n", "#print(\"F1 weighted.: \"+str(f1))", "\n", "accuracy_scores", ".", "append", "(", "acc", ")", "\n", "f1_scores", ".", "append", "(", "f1", ")", "\n", "\n", "", "return", "accuracy_scores", ",", "f1_scores", "\n", "\n"]]}