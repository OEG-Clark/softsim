{"home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.None.train_demo.main": [[29, 363], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "datetime.datetime.now", "print", "print", "print", "print", "print", "torch.cuda.is_available", "fewshot_re_kit.framework.FewShotREFramework.eval", "print", "datetime.datetime.now", "print", "print", "fewshot_re_kit.sentence_encoder.CNNSentenceEncoder", "models.d.Discriminator", "fewshot_re_kit.framework.FewShotREFramework", "fewshot_re_kit.framework.FewShotREFramework", "models.proto.Proto", "os.path.exists", "os.mkdir", "print", "models.pair.Pair.cuda", "torch.device", "fewshot_re_kit.framework.FewShotREFramework.train", "numpy.load", "json.load", "fewshot_re_kit.conceptgraph_utils.loadingInstance2concept", "fewshot_re_kit.conceptgraph_utils.loadingConceptGraphEntity2ID", "fewshot_re_kit.data_kg_loader.get_concept_loader_pair", "fewshot_re_kit.data_kg_loader.get_concept_loader_pair", "fewshot_re_kit.data_kg_loader.get_concept_loader_pair", "fewshot_re_kit.conceptgraph_utils.loadingInstance2concept", "fewshot_re_kit.data_loader.get_loader_pair", "fewshot_re_kit.data_loader.get_loader_pair", "fewshot_re_kit.data_loader.get_loader_pair", "fewshot_re_kit.data_kg_loader.get_concept_loader", "fewshot_re_kit.data_kg_loader.get_concept_loader", "fewshot_re_kit.data_kg_loader.get_concept_loader", "fewshot_re_kit.data_loader.get_loader", "fewshot_re_kit.data_loader.get_loader", "fewshot_re_kit.data_loader.get_loader", "fewshot_re_kit.data_loader.get_loader_unsupervised", "str", "str", "models.gnn.GNN", "models.pair.Pair.cuda", "print", "open", "Exception", "open", "json.load", "open", "json.load", "models.snail.SNAIL", "torch.cuda.is_available", "numpy.loadtxt", "torch.from_numpy", "fewshot_re_kit.sentence_encoder.BERTPAIRConceptSentenceEncoder", "print", "fewshot_re_kit.sentence_encoder.BERTPAIRSentenceEncoder", "fewshot_re_kit.sentence_encoder.BERTConceptSentenceEncoder", "fewshot_re_kit.sentence_encoder.BERTSentenceEncoder", "fewshot_re_kit.sentence_encoder.RobertaPAIRSentenceEncoder", "fewshot_re_kit.sentence_encoder.RobertaSentenceEncoder", "models.metanet.MetaNet", "open", "json.load", "models.siamese.Siamese", "models.pair.Pair"], "function", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREFramework.eval", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREFramework.train", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.loadingInstance2concept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.loadingConceptGraphEntity2ID", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.get_concept_loader_pair", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.get_concept_loader_pair", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.get_concept_loader_pair", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.loadingInstance2concept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.get_loader_pair", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.get_loader_pair", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.get_loader_pair", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.get_concept_loader", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.get_concept_loader", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.get_concept_loader", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.get_loader", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.get_loader", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.get_loader", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.get_loader_unsupervised", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--train'", ",", "default", "=", "'train'", ",", "\n", "help", "=", "'train file'", ")", "\n", "parser", ".", "add_argument", "(", "'--val'", ",", "default", "=", "'val'", ",", "\n", "help", "=", "'val file'", ")", "\n", "parser", ".", "add_argument", "(", "'--test'", ",", "default", "=", "'test_wiki'", ",", "\n", "help", "=", "'test file'", ")", "\n", "parser", ".", "add_argument", "(", "'--adv'", ",", "default", "=", "None", ",", "\n", "help", "=", "'adv file'", ")", "\n", "parser", ".", "add_argument", "(", "'--trainN'", ",", "default", "=", "10", ",", "type", "=", "int", ",", "\n", "help", "=", "'N in train'", ")", "\n", "parser", ".", "add_argument", "(", "'--N'", ",", "default", "=", "5", ",", "type", "=", "int", ",", "\n", "help", "=", "'N way'", ")", "\n", "parser", ".", "add_argument", "(", "'--K'", ",", "default", "=", "5", ",", "type", "=", "int", ",", "\n", "help", "=", "'K shot'", ")", "\n", "parser", ".", "add_argument", "(", "'--Q'", ",", "default", "=", "5", ",", "type", "=", "int", ",", "\n", "help", "=", "'Num of query per class'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "default", "=", "4", ",", "type", "=", "int", ",", "\n", "help", "=", "'batch size'", ")", "\n", "parser", ".", "add_argument", "(", "'--train_iter'", ",", "default", "=", "30000", ",", "type", "=", "int", ",", "\n", "help", "=", "'num of iters in training'", ")", "\n", "parser", ".", "add_argument", "(", "'--val_iter'", ",", "default", "=", "1000", ",", "type", "=", "int", ",", "\n", "help", "=", "'num of iters in validation'", ")", "\n", "parser", ".", "add_argument", "(", "'--test_iter'", ",", "default", "=", "10000", ",", "type", "=", "int", ",", "\n", "help", "=", "'num of iters in testing'", ")", "\n", "parser", ".", "add_argument", "(", "'--val_step'", ",", "default", "=", "2000", ",", "type", "=", "int", ",", "\n", "help", "=", "'val after training how many iters'", ")", "\n", "parser", ".", "add_argument", "(", "'--model'", ",", "default", "=", "'proto'", ",", "\n", "help", "=", "'model name'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder'", ",", "default", "=", "'cnn'", ",", "\n", "help", "=", "'encoder: cnn or bert or roberta'", ")", "\n", "parser", ".", "add_argument", "(", "'--max_length'", ",", "default", "=", "128", ",", "type", "=", "int", ",", "\n", "help", "=", "'max length'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "default", "=", "1e-1", ",", "type", "=", "float", ",", "\n", "help", "=", "'learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight_decay'", ",", "default", "=", "1e-5", ",", "type", "=", "float", ",", "\n", "help", "=", "'weight decay'", ")", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "default", "=", "0.0", ",", "type", "=", "float", ",", "\n", "help", "=", "'dropout rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--na_rate'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "'NA rate (NA = Q * na_rate)'", ")", "\n", "parser", ".", "add_argument", "(", "'--grad_iter'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "'accumulate gradient every x iterations'", ")", "\n", "parser", ".", "add_argument", "(", "'--optim'", ",", "default", "=", "'sgd'", ",", "\n", "help", "=", "'sgd / adam / adamw'", ")", "\n", "parser", ".", "add_argument", "(", "'--hidden_size'", ",", "default", "=", "230", ",", "type", "=", "int", ",", "\n", "help", "=", "'hidden size'", ")", "\n", "parser", ".", "add_argument", "(", "'--load_ckpt'", ",", "default", "=", "None", ",", "\n", "help", "=", "'load ckpt'", ")", "\n", "parser", ".", "add_argument", "(", "'--save_ckpt'", ",", "default", "=", "'checkpoint/10way1shot.ConceptFere.pth.tar'", ",", "\n", "help", "=", "'save ckpt'", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use nvidia apex fp16'", ")", "\n", "parser", ".", "add_argument", "(", "'--only_test'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'only test'", ")", "\n", "\n", "# only for bert / roberta", "\n", "parser", ".", "add_argument", "(", "'--pair'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use pair model'", ")", "\n", "parser", ".", "add_argument", "(", "'--pretrain_ckpt'", ",", "default", "=", "None", ",", "\n", "help", "=", "'bert / roberta pre-trained checkpoint'", ")", "\n", "parser", ".", "add_argument", "(", "'--cat_entity_rep'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'concatenate entity representation as sentence rep'", ")", "\n", "\n", "# only for prototypical networks", "\n", "parser", ".", "add_argument", "(", "'--dot'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use dot instead of L2 distance for proto'", ")", "\n", "\n", "# experiment", "\n", "parser", ".", "add_argument", "(", "'--mask_entity'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'mask entity names'", ")", "\n", "# concept", "\n", "parser", ".", "add_argument", "(", "'--ins2cpt'", ",", "default", "=", "'conceptgraph/instance2concept'", ",", "\n", "help", "=", "'instance2concept in conceptgraph file'", ")", "\n", "\n", "# BeyondWordEmbedding", "\n", "parser", ".", "add_argument", "(", "'--normalize'", ",", "dest", "=", "'normalize'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--model_format'", ",", "dest", "=", "'model_format'", ",", "default", "=", "'bin'", ",", "nargs", "=", "'?'", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "'--model_file'", ",", "dest", "=", "'model_file'", ",", "default", "=", "'/home/LAB/zhaoqh/yangshan/kg/pretrainingConceptGraph/models/cme.bin'", ",", "nargs", "=", "'?'", ",", "\n", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "'--id_from'", ",", "default", "=", "''", ",", "\n", "help", "=", "'BeyondWordEmbedding Or keEmbedding Or MultiHeadAttentionAndBeyondWordEmbedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--concept'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use concept in kg(ConceptGraph)'", ")", "\n", "parser", ".", "add_argument", "(", "'--entity2id'", ",", "default", "=", "'conceptgraphEmbedding/TransE_l2_concetgraph_2/entities2id'", ",", "\n", "help", "=", "'entity2id in conceptgraph file path'", ")", "\n", "parser", ".", "add_argument", "(", "'--word2id'", ",", "default", "=", "'BeyondWordEmbedding/word2id'", ",", "help", "=", "'word2id file path'", ")", "\n", "parser", ".", "add_argument", "(", "'--title2id'", ",", "default", "=", "'BeyondWordEmbedding/all_titles2id'", ",", "help", "=", "'title2id file path'", ")", "\n", "\n", "# kg embedding", "\n", "parser", ".", "add_argument", "(", "'--id2embeddingID'", ",", "default", "=", "'BeyondWordEmbedding/id2embeddingID'", ",", "help", "=", "'file path'", ")", "\n", "parser", ".", "add_argument", "(", "'--BeyondWordEmbedding'", ",", "default", "=", "'BeyondWordEmbedding/partOfBeyondWordEmbedding'", ",", "\n", "help", "=", "'file path'", ")", "\n", "parser", ".", "add_argument", "(", "'--conceptEmbedding'", ",", "\n", "default", "=", "'conceptgraphEmbedding/TransE_l2_concetgraph_2/concetgraph_TransE_l2_entity'", ",", "\n", "help", "=", "'file path'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--sentenceORword'", ",", "default", "=", "'sentence'", ",", "help", "=", "'select bert output'", ")", "\n", "\n", "opt", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "trainN", "=", "opt", ".", "trainN", "\n", "N", "=", "opt", ".", "N", "\n", "K", "=", "opt", ".", "K", "\n", "Q", "=", "opt", ".", "Q", "\n", "batch_size", "=", "opt", ".", "batch_size", "\n", "model_name", "=", "opt", ".", "model", "\n", "encoder_name", "=", "opt", ".", "encoder", "\n", "max_length", "=", "opt", ".", "max_length", "\n", "starting_time", "=", "datetime", ".", "now", "(", ")", "\n", "print", "(", "'starting time'", ",", "starting_time", ")", "\n", "print", "(", "\"{}-way-{}-shot Few-Shot Relation Classification\"", ".", "format", "(", "N", ",", "K", ")", ")", "\n", "print", "(", "\"model: {}\"", ".", "format", "(", "model_name", ")", ")", "\n", "print", "(", "\"encoder: {}\"", ".", "format", "(", "encoder_name", ")", ")", "\n", "print", "(", "\"max_length: {}\"", ".", "format", "(", "max_length", ")", ")", "\n", "\n", "if", "encoder_name", "==", "'cnn'", ":", "\n", "        ", "try", ":", "\n", "            ", "glove_mat", "=", "np", ".", "load", "(", "'./pretrain/glove/glove_mat.npy'", ")", "\n", "glove_word2id", "=", "json", ".", "load", "(", "open", "(", "'./pretrain/glove/glove_word2id.json'", ")", ")", "\n", "", "except", ":", "\n", "            ", "raise", "Exception", "(", "\"Cannot find glove files. Run glove/download_glove.sh to download glove files.\"", ")", "\n", "", "sentence_encoder", "=", "CNNSentenceEncoder", "(", "\n", "glove_mat", ",", "\n", "glove_word2id", ",", "\n", "max_length", ")", "\n", "", "elif", "encoder_name", "==", "'bert'", ":", "\n", "        ", "pretrain_ckpt", "=", "opt", ".", "pretrain_ckpt", "or", "'bert-base-uncased'", "\n", "if", "opt", ".", "pair", ":", "\n", "\n", "# sentence_encoder = BERTPAIRSentenceEncoder(", "\n", "#     pretrain_ckpt,", "\n", "#     max_length)", "\n", "# titles, redirects, vector_size, W, id2word, word2id, all_titles = load(model_path=opt.model_file,", "\n", "#                                                                        format=opt.model_format,", "\n", "#                                                                        load_concepts=True,", "\n", "#                                                                        normalize=opt.normalize,", "\n", "#                                                                        log_every=1000000)", "\n", "\n", "            ", "if", "(", "opt", ".", "id_from", "==", "'BeyondWordEmbedding'", ")", "|", "(", "opt", ".", "id_from", "==", "'MultiHeadAttentionAndBeyondWordEmbedding'", ")", ":", "\n", "                ", "with", "open", "(", "'./data/BeyondWordEmbedding/id2embeddingID.json'", ",", "mode", "=", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "fr", ":", "\n", "                    ", "id2embeddingID", "=", "json", ".", "load", "(", "fr", ")", "\n", "", "BeyondWordEmbedding", "=", "np", ".", "loadtxt", "(", "'./data/BeyondWordEmbedding/partOfBeyondWordEmbedding.npy'", ")", "\n", "BeyondWordEmbedding", "=", "torch", ".", "from_numpy", "(", "BeyondWordEmbedding", ")", "\n", "\n", "# print('loading conceptEmbedding')", "\n", "# path = './data/conceptgraphEmbedding/TransE_l2_concetgraph_2/concetgraph_TransE_l2_entity.npy'", "\n", "# conceptEmbedding = np.load(path)", "\n", "# conceptEmbedding = torch.from_numpy(conceptEmbedding)", "\n", "conceptEmbedding", "=", "{", "}", "\n", "sentence_encoder", "=", "BERTPAIRConceptSentenceEncoder", "(", "\n", "pretrain_ckpt", ",", "\n", "max_length", ",", "conceptEmbedding", ",", "BeyondWordEmbedding", ",", "id2embeddingID", ",", "\n", "id_from", "=", "opt", ".", "id_from", ")", "\n", "", "else", ":", "\n", "                ", "print", "(", "'init BERTPAIRSentenceEncoder'", ")", "\n", "sentence_encoder", "=", "BERTPAIRSentenceEncoder", "(", "\n", "pretrain_ckpt", ",", "\n", "max_length", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "opt", ".", "concept", "|", "(", "opt", ".", "id_from", "==", "'BeyondWordEmbedding'", ")", "|", "(", "\n", "opt", ".", "id_from", "==", "'MultiHeadAttentionAndBeyondWordEmbedding'", ")", ":", "\n", "# print('concept: use')", "\n", "                ", "sentence_encoder", "=", "BERTConceptSentenceEncoder", "(", "pretrain_ckpt", ",", "\n", "max_length", ",", "sentenceORword", "=", "opt", ".", "sentenceORword", ",", "\n", "cat_entity_rep", "=", "opt", ".", "cat_entity_rep", ",", "\n", "mask_entity", "=", "opt", ".", "mask_entity", ")", "\n", "", "else", ":", "\n", "                ", "sentence_encoder", "=", "BERTSentenceEncoder", "(", "\n", "pretrain_ckpt", ",", "\n", "max_length", ",", "\n", "cat_entity_rep", "=", "opt", ".", "cat_entity_rep", ",", "\n", "mask_entity", "=", "opt", ".", "mask_entity", ")", "\n", "", "", "", "elif", "encoder_name", "==", "'roberta'", ":", "\n", "        ", "pretrain_ckpt", "=", "opt", ".", "pretrain_ckpt", "or", "'roberta-base'", "\n", "if", "opt", ".", "pair", ":", "\n", "            ", "sentence_encoder", "=", "RobertaPAIRSentenceEncoder", "(", "\n", "pretrain_ckpt", ",", "\n", "max_length", ")", "\n", "", "else", ":", "\n", "            ", "sentence_encoder", "=", "RobertaSentenceEncoder", "(", "\n", "pretrain_ckpt", ",", "\n", "max_length", ",", "\n", "cat_entity_rep", "=", "opt", ".", "cat_entity_rep", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n", "", "if", "opt", ".", "pair", ":", "\n", "\n", "        ", "if", "(", "opt", ".", "id_from", "==", "'keEmbedding'", ")", "|", "(", "\n", "opt", ".", "id_from", "==", "'MultiHeadAttentionAndBeyondWordEmbedding'", ")", "|", "(", "opt", ".", "id_from", "==", "'BeyondWordEmbedding'", ")", ":", "\n", "            ", "ins2cpt", "=", "loadingInstance2concept", "(", "path", "=", "'./data/conceptgraph/instance2concept.pickle'", ")", "\n", "entity2id", "=", "loadingConceptGraphEntity2ID", "(", "root", "=", "'./data/'", ")", "\n", "path1", "=", "'./data/BeyondWordEmbedding/word2id.json'", "\n", "with", "open", "(", "path1", ",", "mode", "=", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f1", ":", "\n", "                ", "word2id", "=", "json", ".", "load", "(", "f1", ")", "\n", "\n", "", "path2", "=", "'./data/BeyondWordEmbedding/all_titles2id.json'", "\n", "with", "open", "(", "path2", ",", "mode", "=", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f2", ":", "\n", "                ", "title2id", "=", "json", ".", "load", "(", "f2", ")", "\n", "\n", "", "train_data_loader", "=", "get_concept_loader_pair", "(", "opt", ".", "train", ",", "ins2cpt", ",", "entity2id", ",", "title2id", ",", "word2id", ",", "\n", "sentence_encoder", ",", "\n", "nWay", "=", "trainN", ",", "K", "=", "K", ",", "Q", "=", "Q", ",", "batch_size", "=", "batch_size", ",", "num_workers", "=", "0", ",", "\n", "na_rate", "=", "opt", ".", "na_rate", ",", "encoder_name", "=", "encoder_name", ",", "\n", "id_from", "=", "opt", ".", "id_from", ")", "\n", "val_data_loader", "=", "get_concept_loader_pair", "(", "opt", ".", "val", ",", "ins2cpt", ",", "entity2id", ",", "title2id", ",", "word2id", ",", "sentence_encoder", ",", "\n", "nWay", "=", "N", ",", "K", "=", "K", ",", "Q", "=", "Q", ",", "na_rate", "=", "opt", ".", "na_rate", ",", "batch_size", "=", "batch_size", ",", "\n", "num_workers", "=", "0", ",", "\n", "encoder_name", "=", "encoder_name", ",", "id_from", "=", "opt", ".", "id_from", ")", "\n", "test_data_loader", "=", "get_concept_loader_pair", "(", "opt", ".", "test", ",", "ins2cpt", ",", "entity2id", ",", "title2id", ",", "word2id", ",", "\n", "sentence_encoder", ",", "\n", "num_workers", "=", "0", ",", "\n", "nWay", "=", "N", ",", "K", "=", "K", ",", "Q", "=", "Q", ",", "na_rate", "=", "opt", ".", "na_rate", ",", "batch_size", "=", "batch_size", ",", "\n", "encoder_name", "=", "encoder_name", ",", "id_from", "=", "opt", ".", "id_from", ")", "\n", "", "else", ":", "\n", "            ", "ins2cpt", "=", "loadingInstance2concept", "(", "path", "=", "'./data/conceptgraph/instance2concept.pickle'", ")", "\n", "train_data_loader", "=", "get_loader_pair", "(", "opt", ".", "train", ",", "ins2cpt", ",", "\n", "sentence_encoder", ",", "\n", "nWay", "=", "trainN", ",", "K", "=", "K", ",", "Q", "=", "Q", ",", "batch_size", "=", "batch_size", ",", "num_workers", "=", "8", ",", "\n", "na_rate", "=", "opt", ".", "na_rate", ",", "encoder_name", "=", "encoder_name", ")", "\n", "val_data_loader", "=", "get_loader_pair", "(", "opt", ".", "val", ",", "ins2cpt", ",", "sentence_encoder", ",", "\n", "nWay", "=", "N", ",", "K", "=", "K", ",", "Q", "=", "Q", ",", "na_rate", "=", "opt", ".", "na_rate", ",", "batch_size", "=", "batch_size", ",", "\n", "num_workers", "=", "8", ",", "\n", "encoder_name", "=", "encoder_name", ")", "\n", "test_data_loader", "=", "get_loader_pair", "(", "opt", ".", "test", ",", "ins2cpt", ",", "\n", "sentence_encoder", ",", "\n", "num_workers", "=", "8", ",", "\n", "nWay", "=", "N", ",", "K", "=", "K", ",", "Q", "=", "Q", ",", "na_rate", "=", "opt", ".", "na_rate", ",", "batch_size", "=", "batch_size", ",", "\n", "encoder_name", "=", "encoder_name", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "opt", ".", "concept", "|", "(", "opt", ".", "id_from", "==", "'keEmbedding'", ")", "|", "(", "\n", "opt", ".", "id_from", "==", "'MultiHeadAttentionAndBeyondWordEmbedding'", ")", "|", "(", "opt", ".", "id_from", "==", "'BeyondWordEmbedding'", ")", ":", "\n", "            ", "train_data_loader", "=", "get_concept_loader", "(", "opt", ".", "train", ",", "sentence_encoder", ",", "\n", "nWay", "=", "trainN", ",", "K", "=", "K", ",", "Q", "=", "Q", ",", "na_rate", "=", "opt", ".", "na_rate", ",", "batch_size", "=", "batch_size", ",", "\n", "ins2cpt", "=", "opt", ".", "ins2cpt", ",", "concept", "=", "opt", ".", "concept", ",", "id_from", "=", "opt", ".", "id_from", ",", "\n", "entity2id", "=", "opt", ".", "entity2id", ",", "title2id", "=", "opt", ".", "title2id", ",", "word2id", "=", "opt", ".", "word2id", ")", "\n", "val_data_loader", "=", "get_concept_loader", "(", "opt", ".", "val", ",", "sentence_encoder", ",", "\n", "nWay", "=", "N", ",", "K", "=", "K", ",", "Q", "=", "Q", ",", "na_rate", "=", "opt", ".", "na_rate", ",", "batch_size", "=", "batch_size", ",", "\n", "ins2cpt", "=", "opt", ".", "ins2cpt", ",", "concept", "=", "opt", ".", "concept", ",", "id_from", "=", "opt", ".", "id_from", ",", "\n", "entity2id", "=", "opt", ".", "entity2id", ",", "title2id", "=", "opt", ".", "title2id", ",", "word2id", "=", "opt", ".", "word2id", ")", "\n", "test_data_loader", "=", "get_concept_loader", "(", "opt", ".", "test", ",", "sentence_encoder", ",", "\n", "nWay", "=", "N", ",", "K", "=", "K", ",", "Q", "=", "Q", ",", "na_rate", "=", "opt", ".", "na_rate", ",", "batch_size", "=", "batch_size", ",", "\n", "ins2cpt", "=", "opt", ".", "ins2cpt", ",", "concept", "=", "opt", ".", "concept", ",", "id_from", "=", "opt", ".", "id_from", ",", "\n", "entity2id", "=", "opt", ".", "entity2id", ",", "title2id", "=", "opt", ".", "title2id", ",", "word2id", "=", "opt", ".", "word2id", ")", "\n", "", "else", ":", "\n", "            ", "train_data_loader", "=", "get_loader", "(", "opt", ".", "train", ",", "sentence_encoder", ",", "\n", "N", "=", "trainN", ",", "K", "=", "K", ",", "Q", "=", "Q", ",", "na_rate", "=", "opt", ".", "na_rate", ",", "batch_size", "=", "batch_size", ")", "\n", "val_data_loader", "=", "get_loader", "(", "opt", ".", "val", ",", "sentence_encoder", ",", "\n", "N", "=", "N", ",", "K", "=", "K", ",", "Q", "=", "Q", ",", "na_rate", "=", "opt", ".", "na_rate", ",", "batch_size", "=", "batch_size", ")", "\n", "test_data_loader", "=", "get_loader", "(", "opt", ".", "test", ",", "sentence_encoder", ",", "\n", "N", "=", "N", ",", "K", "=", "K", ",", "Q", "=", "Q", ",", "na_rate", "=", "opt", ".", "na_rate", ",", "batch_size", "=", "batch_size", ")", "\n", "", "if", "opt", ".", "adv", ":", "\n", "            ", "adv_data_loader", "=", "get_loader_unsupervised", "(", "opt", ".", "adv", ",", "sentence_encoder", ",", "\n", "N", "=", "trainN", ",", "K", "=", "K", ",", "Q", "=", "Q", ",", "na_rate", "=", "opt", ".", "na_rate", ",", "batch_size", "=", "batch_size", ")", "\n", "\n", "", "", "if", "opt", ".", "optim", "==", "'sgd'", ":", "\n", "        ", "pytorch_optim", "=", "optim", ".", "SGD", "\n", "", "elif", "opt", ".", "optim", "==", "'adam'", ":", "\n", "        ", "pytorch_optim", "=", "optim", ".", "Adam", "\n", "", "elif", "opt", ".", "optim", "==", "'adamw'", ":", "\n", "        ", "from", "transformers", "import", "AdamW", "\n", "pytorch_optim", "=", "AdamW", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "", "if", "opt", ".", "adv", ":", "\n", "        ", "d", "=", "Discriminator", "(", "opt", ".", "hidden_size", ")", "\n", "framework", "=", "FewShotREFramework", "(", "train_data_loader", ",", "val_data_loader", ",", "test_data_loader", ",", "adv_data_loader", ",", "\n", "adv", "=", "opt", ".", "adv", ",", "d", "=", "d", ")", "\n", "", "else", ":", "\n", "        ", "framework", "=", "FewShotREFramework", "(", "train_data_loader", ",", "val_data_loader", ",", "test_data_loader", ")", "\n", "\n", "", "prefix", "=", "'-'", ".", "join", "(", "[", "model_name", ",", "encoder_name", ",", "opt", ".", "train", ",", "opt", ".", "val", ",", "str", "(", "N", ")", ",", "str", "(", "K", ")", "]", ")", "\n", "if", "opt", ".", "adv", "is", "not", "None", ":", "\n", "        ", "prefix", "+=", "'-adv_'", "+", "opt", ".", "adv", "\n", "", "if", "opt", ".", "na_rate", "!=", "0", ":", "\n", "        ", "prefix", "+=", "'-na{}'", ".", "format", "(", "opt", ".", "na_rate", ")", "\n", "", "if", "opt", ".", "dot", ":", "\n", "        ", "prefix", "+=", "'-dot'", "\n", "", "if", "opt", ".", "cat_entity_rep", ":", "\n", "        ", "prefix", "+=", "'-catentity'", "\n", "\n", "", "if", "model_name", "==", "'proto'", ":", "\n", "\n", "        ", "model", "=", "Proto", "(", "sentence_encoder", ",", "dot", "=", "opt", ".", "dot", ")", "\n", "", "elif", "model_name", "==", "'gnn'", ":", "\n", "        ", "model", "=", "GNN", "(", "sentence_encoder", ",", "N", ",", "hidden_size", "=", "opt", ".", "hidden_size", ")", "\n", "", "elif", "model_name", "==", "'snail'", ":", "\n", "        ", "model", "=", "SNAIL", "(", "sentence_encoder", ",", "N", ",", "K", ",", "hidden_size", "=", "opt", ".", "hidden_size", ")", "\n", "", "elif", "model_name", "==", "'metanet'", ":", "\n", "        ", "model", "=", "MetaNet", "(", "N", ",", "K", ",", "sentence_encoder", ".", "embedding", ",", "max_length", ")", "\n", "", "elif", "model_name", "==", "'siamese'", ":", "\n", "        ", "model", "=", "Siamese", "(", "sentence_encoder", ",", "hidden_size", "=", "opt", ".", "hidden_size", ",", "dropout", "=", "opt", ".", "dropout", ")", "\n", "", "elif", "model_name", "==", "'pair'", ":", "\n", "        ", "model", "=", "Pair", "(", "sentence_encoder", ",", "hidden_size", "=", "opt", ".", "hidden_size", ")", "\n", "\n", "# elif model_name=='concept':", "\n", "#     model=Pair(sentence_encoder,hidden_size=opt.hidden_sieze)", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "'checkpoint'", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "'checkpoint'", ")", "\n", "", "ckpt", "=", "'checkpoint/{}.pth.tar'", ".", "format", "(", "prefix", ")", "\n", "if", "opt", ".", "save_ckpt", ":", "\n", "        ", "ckpt", "=", "opt", ".", "save_ckpt", "\n", "\n", "", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "print", "(", "'-------------------------------------model.cuda()-------------------------'", ",", "model", ".", "cuda", "(", ")", ")", "\n", "model", ".", "cuda", "(", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "\n", "", "if", "not", "opt", ".", "only_test", ":", "\n", "        ", "if", "encoder_name", "in", "[", "'bert'", ",", "'roberta'", "]", ":", "\n", "            ", "bert_optim", "=", "True", "\n", "", "else", ":", "\n", "            ", "bert_optim", "=", "False", "\n", "\n", "", "framework", ".", "train", "(", "device", ",", "model", ",", "prefix", ",", "batch_size", ",", "trainN", ",", "N", ",", "K", ",", "Q", ",", "\n", "pytorch_optim", "=", "pytorch_optim", ",", "load_ckpt", "=", "opt", ".", "load_ckpt", ",", "save_ckpt", "=", "ckpt", ",", "\n", "na_rate", "=", "opt", ".", "na_rate", ",", "val_step", "=", "opt", ".", "val_step", ",", "fp16", "=", "opt", ".", "fp16", ",", "pair", "=", "opt", ".", "pair", ",", "\n", "train_iter", "=", "opt", ".", "train_iter", ",", "val_iter", "=", "opt", ".", "val_iter", ",", "bert_optim", "=", "bert_optim", ")", "\n", "", "else", ":", "\n", "        ", "ckpt", "=", "opt", ".", "load_ckpt", "\n", "if", "ckpt", "is", "None", ":", "\n", "            ", "print", "(", "\"Warning: --load_ckpt is not specified. Will load Hugginface pre-trained checkpoint.\"", ")", "\n", "ckpt", "=", "'none'", "\n", "\n", "", "", "acc", "=", "framework", ".", "eval", "(", "device", ",", "model", ",", "batch_size", ",", "N", ",", "K", ",", "Q", ",", "opt", ".", "test_iter", ",", "na_rate", "=", "opt", ".", "na_rate", ",", "ckpt", "=", "ckpt", ",", "\n", "pair", "=", "opt", ".", "pair", ")", "\n", "print", "(", "\"RESULT: %.2f\"", "%", "(", "acc", "*", "100", ")", ")", "\n", "ending_time", "=", "datetime", ".", "now", "(", ")", "\n", "print", "(", "'ending time'", ",", "ending_time", ")", "\n", "print", "(", "'training takes time'", ",", "ending_time", "-", "starting_time", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.metanet.LearnerForAttention.__init__": [[23, 29], ["torch.nn.Module.__init__", "torch.nn.LSTM", "torch.nn.Linear", "torch.nn.LSTM", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "self", ".", "conv_lstm", "=", "nn", ".", "LSTM", "(", "2", ",", "20", ",", "batch_first", "=", "True", ")", "\n", "self", ".", "conv_fc", "=", "nn", ".", "Linear", "(", "20", ",", "1", ")", "\n", "self", ".", "fc_lstm", "=", "nn", ".", "LSTM", "(", "2", ",", "20", ",", "batch_first", "=", "True", ")", "\n", "self", ".", "fc_fc", "=", "nn", ".", "Linear", "(", "20", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.metanet.LearnerForAttention.forward": [[30, 48], ["inputs.size", "inputs.view", "metanet.log_and_sign", "torch.autograd.Variable().unsqueeze", "metanet.LearnerForAttention.view", "metanet.LearnerForAttention.conv_lstm", "metanet.LearnerForAttention.squeeze", "metanet.LearnerForAttention.conv_fc", "metanet.LearnerForAttention.fc_lstm", "metanet.LearnerForAttention.squeeze", "metanet.LearnerForAttention.fc_fc", "torch.autograd.Variable"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.metanet.log_and_sign"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "is_conv", ")", ":", "\n", "        ", "size", "=", "inputs", ".", "size", "(", ")", "\n", "x", "=", "inputs", ".", "view", "(", "(", "-", "1", ",", "1", ")", ")", "\n", "x", "=", "log_and_sign", "(", "x", ")", "# (-1, 2)", "\n", "\n", "#### NO BACKPROP", "\n", "x", "=", "Variable", "(", "x", ",", "requires_grad", "=", "False", ")", ".", "unsqueeze", "(", "0", ")", "# (1, param_size, 2)", "\n", "#### ", "\n", "\n", "if", "is_conv", ":", "\n", "            ", "x", ",", "_", "=", "self", ".", "conv_lstm", "(", "x", ")", "# (1, param_size, 1)", "\n", "x", "=", "x", ".", "squeeze", "(", ")", "\n", "x", "=", "self", ".", "conv_fc", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", ",", "_", "=", "self", ".", "fc_lstm", "(", "x", ")", "# (1, param_size, 1)", "\n", "x", "=", "x", ".", "squeeze", "(", ")", "\n", "x", "=", "self", ".", "fc_fc", "(", "x", ")", "\n", "", "return", "x", ".", "view", "(", "size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.metanet.LearnerForBasic.__init__": [[51, 59], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "self", ".", "conv_fc1", "=", "nn", ".", "Linear", "(", "2", ",", "20", ")", "\n", "self", ".", "conv_fc2", "=", "nn", ".", "Linear", "(", "20", ",", "20", ")", "\n", "self", ".", "conv_fc3", "=", "nn", ".", "Linear", "(", "20", ",", "1", ")", "\n", "self", ".", "fc_fc1", "=", "nn", ".", "Linear", "(", "2", ",", "20", ")", "\n", "self", ".", "fc_fc2", "=", "nn", ".", "Linear", "(", "20", ",", "20", ")", "\n", "self", ".", "fc_fc3", "=", "nn", ".", "Linear", "(", "20", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.metanet.LearnerForBasic.forward": [[61, 79], ["inputs.size", "inputs.view", "metanet.log_and_sign", "torch.autograd.Variable", "metanet.LearnerForBasic.view", "torch.nn.functional.relu", "torch.nn.functional.relu", "metanet.LearnerForBasic.conv_fc3", "torch.nn.functional.relu", "torch.nn.functional.relu", "metanet.LearnerForBasic.fc_fc3", "metanet.LearnerForBasic.conv_fc1", "metanet.LearnerForBasic.conv_fc2", "metanet.LearnerForBasic.fc_fc1", "metanet.LearnerForBasic.fc_fc2"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.metanet.log_and_sign"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "is_conv", ")", ":", "\n", "        ", "size", "=", "inputs", ".", "size", "(", ")", "\n", "x", "=", "inputs", ".", "view", "(", "(", "-", "1", ",", "1", ")", ")", "\n", "x", "=", "log_and_sign", "(", "x", ")", "# (-1, 2)", "\n", "\n", "#### NO BACKPROP", "\n", "x", "=", "Variable", "(", "x", ",", "requires_grad", "=", "False", ")", "\n", "####", "\n", "\n", "if", "is_conv", ":", "\n", "            ", "x", "=", "F", ".", "relu", "(", "self", ".", "conv_fc1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "conv_fc2", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "conv_fc3", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "F", ".", "relu", "(", "self", ".", "fc_fc1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc_fc2", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "fc_fc3", "(", "x", ")", "\n", "", "return", "x", ".", "view", "(", "size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.metanet.MetaNet.__init__": [[82, 111], ["fewshot_re_kit.framework.FewShotREModel.__init__", "fewshot_re_kit.network.encoder.Encoder", "fewshot_re_kit.network.encoder.Encoder", "torch.nn.Linear", "torch.nn.Linear", "metanet.LearnerForBasic", "metanet.LearnerForAttention"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "N", ",", "K", ",", "embedding", ",", "max_length", ",", "hidden_size", "=", "230", ")", ":", "\n", "        ", "'''\n        N: num of classes\n        K: num of instances for each class\n        word_vec_mat, max_length, hidden_size: same as sentence_encoder\n        '''", "\n", "fewshot_re_kit", ".", "framework", ".", "FewShotREModel", ".", "__init__", "(", "self", ",", "None", ")", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "N", "=", "N", "\n", "self", ".", "K", "=", "K", "\n", "\n", "# self.embedding = Embedding(word_vec_mat, max_length, word_embedding_dim=50, pos_embedding_dim=5)", "\n", "self", ".", "embedding", "=", "embedding", "\n", "\n", "self", ".", "basic_encoder", "=", "Encoder", "(", "max_length", ",", "word_embedding_dim", "=", "50", ",", "pos_embedding_dim", "=", "5", ",", "hidden_size", "=", "hidden_size", ")", "\n", "self", ".", "attention_encoder", "=", "Encoder", "(", "max_length", ",", "word_embedding_dim", "=", "50", ",", "pos_embedding_dim", "=", "5", ",", "hidden_size", "=", "hidden_size", ")", "\n", "\n", "self", ".", "basic_fast_conv_W", "=", "None", "\n", "self", ".", "attention_fast_conv_W", "=", "None", "\n", "\n", "self", ".", "basic_fc", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "N", ",", "bias", "=", "False", ")", "\n", "self", ".", "attention_fc", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "N", ",", "bias", "=", "False", ")", "\n", "\n", "self", ".", "basic_fast_fc_W", "=", "None", "\n", "self", ".", "attention_fast_fc_W", "=", "None", "\n", "\n", "self", ".", "learner_basic", "=", "LearnerForBasic", "(", ")", "\n", "self", ".", "learner_attention", "=", "LearnerForAttention", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.metanet.MetaNet.basic_emb": [[112, 118], ["metanet.MetaNet.embedding", "metanet.MetaNet.basic_encoder", "metanet.MetaNet.view", "torch.nn.functional.relu().max", "torch.nn.functional.relu", "torch.nn.functional.conv1d", "metanet.MetaNet.transpose"], "methods", ["None"], ["", "def", "basic_emb", "(", "self", ",", "inputs", ",", "size", ",", "use_fast", "=", "False", ")", ":", "\n", "        ", "x", "=", "self", ".", "embedding", "(", "inputs", ")", "\n", "output", "=", "self", ".", "basic_encoder", "(", "x", ")", "\n", "if", "use_fast", ":", "\n", "            ", "output", "+=", "F", ".", "relu", "(", "F", ".", "conv1d", "(", "x", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ",", "self", ".", "basic_fast_conv_W", ",", "padding", "=", "1", ")", ")", ".", "max", "(", "-", "1", ")", "[", "0", "]", "\n", "", "return", "output", ".", "view", "(", "size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.metanet.MetaNet.attention_emb": [[119, 125], ["metanet.MetaNet.embedding", "metanet.MetaNet.attention_encoder", "metanet.MetaNet.view", "torch.nn.functional.relu().max", "torch.nn.functional.relu", "torch.nn.functional.conv1d", "metanet.MetaNet.transpose"], "methods", ["None"], ["", "def", "attention_emb", "(", "self", ",", "inputs", ",", "size", ",", "use_fast", "=", "False", ")", ":", "\n", "        ", "x", "=", "self", ".", "embedding", "(", "inputs", ")", "\n", "output", "=", "self", ".", "attention_encoder", "(", "x", ")", "\n", "if", "use_fast", ":", "\n", "            ", "output", "+=", "F", ".", "relu", "(", "F", ".", "conv1d", "(", "x", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ",", "self", ".", "attention_fast_conv_W", ",", "padding", "=", "1", ")", ")", ".", "max", "(", "-", "1", ")", "[", "0", "]", "\n", "", "return", "output", ".", "view", "(", "size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.metanet.MetaNet.attention_score": [[126, 137], ["s_att.unsqueeze.unsqueeze.view", "s_att.unsqueeze.unsqueeze.unsqueeze", "q_att.unsqueeze.unsqueeze.unsqueeze", "torch.nn.functional.cosine_similarity", "torch.nn.functional.softmax", "s_att.unsqueeze.unsqueeze.size", "s_att.unsqueeze.unsqueeze.size", "s_att.unsqueeze.unsqueeze.size", "s_att.unsqueeze.unsqueeze.size"], "methods", ["None"], ["", "def", "attention_score", "(", "self", ",", "s_att", ",", "q_att", ")", ":", "\n", "        ", "'''\n        s_att: (B, N, K, D)\n        q_att: (B, NQ, D)\n        '''", "\n", "s_att", "=", "s_att", ".", "view", "(", "s_att", ".", "size", "(", "0", ")", ",", "s_att", ".", "size", "(", "1", ")", "*", "s_att", ".", "size", "(", "2", ")", ",", "s_att", ".", "size", "(", "3", ")", ")", "# (B, N * K, D)", "\n", "s_att", "=", "s_att", ".", "unsqueeze", "(", "1", ")", "# (B, 1, N * K, D)", "\n", "q_att", "=", "q_att", ".", "unsqueeze", "(", "2", ")", "# (B, NQ, 1, D)", "\n", "cos", "=", "F", ".", "cosine_similarity", "(", "s_att", ",", "q_att", ",", "dim", "=", "-", "1", ")", "# (B, NQ, N * K)", "\n", "score", "=", "F", ".", "softmax", "(", "cos", ",", "-", "1", ")", "# (B, NQ, N * K)", "\n", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.metanet.MetaNet.forward": [[138, 207], ["metanet.MetaNet.attention_emb", "metanet.MetaNet.attention_fc", "metanet.MetaNet.size", "metanet.MetaNet.zero_grad", "torch.autograd.Variable", "metanet.MetaNet.cost", "metanet.MetaNet.backward", "metanet.MetaNet.learner_attention", "metanet.MetaNet.learner_attention", "metanet.MetaNet.basic_emb", "metanet.MetaNet.basic_fc", "range", "torch.stack", "torch.stack", "metanet.MetaNet.zero_grad", "metanet.MetaNet.attention_emb", "metanet.MetaNet.attention_emb", "metanet.MetaNet.attention_score().squeeze", "torch.matmul", "torch.matmul", "range", "torch.stack", "torch.max", "torch.tensor().cuda", "torch.stack.view", "torch.autograd.Variable.view", "range", "torch.stack.size", "torch.stack.size", "torch.stack.view", "torch.stack.view", "final_fast_conv_param[].view", "final_fast_fc_param[].view", "metanet.MetaNet.basic_emb", "stack_logits.append", "torch.stack.view", "metanet.MetaNet.zero_grad", "torch.autograd.Variable", "metanet.MetaNet.cost", "metanet.MetaNet.backward", "torch.stack.append", "torch.stack.append", "metanet.MetaNet.attention_score", "metanet.MetaNet.basic_fc", "torch.nn.functional.linear", "torch.tensor", "torch.tensor().cuda", "logits[].view", "torch.autograd.Variable.view", "metanet.MetaNet.learner_basic", "metanet.MetaNet.learner_basic", "torch.tensor", "range"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.metanet.MetaNet.attention_emb", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.metanet.MetaNet.basic_emb", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.metanet.MetaNet.attention_emb", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.metanet.MetaNet.attention_emb", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.metanet.MetaNet.basic_emb", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.metanet.MetaNet.attention_score"], ["", "def", "forward", "(", "self", ",", "support", ",", "query", ",", "N", ",", "K", ",", "Q", ")", ":", "\n", "        ", "'''\n        support: Inputs of the support set.\n        query: Inputs of the query set.\n        N: Num of classes\n        K: Num of instances for each class in the support set\n        Q: Num of instances for each class in the query set\n        '''", "\n", "\n", "# learn fast parameters for attention encoder", "\n", "s", "=", "self", ".", "attention_emb", "(", "support", ",", "(", "-", "1", ",", "N", ",", "K", ",", "self", ".", "hidden_size", ")", ")", "\n", "logits", "=", "self", ".", "attention_fc", "(", "s", ")", "# (B, N, K, N)", "\n", "\n", "B", "=", "s", ".", "size", "(", "0", ")", "\n", "NQ", "=", "N", "*", "Q", "\n", "assert", "(", "B", "==", "1", ")", "\n", "\n", "self", ".", "zero_grad", "(", ")", "\n", "tmp_label", "=", "Variable", "(", "torch", ".", "tensor", "(", "[", "[", "x", "]", "*", "K", "for", "x", "in", "range", "(", "N", ")", "]", "*", "B", ",", "dtype", "=", "torch", ".", "long", ")", ".", "cuda", "(", ")", ")", "\n", "loss", "=", "self", ".", "cost", "(", "logits", ".", "view", "(", "-", "1", ",", "N", ")", ",", "tmp_label", ".", "view", "(", "-", "1", ")", ")", "\n", "loss", ".", "backward", "(", "retain_graph", "=", "True", ")", "\n", "\n", "grad_conv", "=", "self", ".", "attention_encoder", ".", "conv", ".", "weight", ".", "grad", "\n", "grad_fc", "=", "self", ".", "attention_fc", ".", "weight", ".", "grad", "\n", "\n", "self", ".", "attention_fast_conv_W", "=", "self", ".", "learner_attention", "(", "grad_conv", ",", "is_conv", "=", "True", ")", "\n", "self", ".", "attention_fast_fc_W", "=", "self", ".", "learner_attention", "(", "grad_fc", ",", "is_conv", "=", "False", ")", "\n", "\n", "# learn fast parameters for basic encoder (each class)", "\n", "s", "=", "self", ".", "basic_emb", "(", "support", ",", "(", "-", "1", ",", "N", ",", "K", ",", "self", ".", "hidden_size", ")", ")", "\n", "logits", "=", "self", ".", "basic_fc", "(", "s", ")", "# (B, N, K, N)", "\n", "\n", "basic_fast_conv_params", "=", "[", "]", "\n", "basic_fast_fc_params", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "N", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "K", ")", ":", "\n", "                ", "self", ".", "zero_grad", "(", ")", "\n", "tmp_label", "=", "Variable", "(", "torch", ".", "tensor", "(", "[", "i", "]", ",", "dtype", "=", "torch", ".", "long", ")", ".", "cuda", "(", ")", ")", "\n", "loss", "=", "self", ".", "cost", "(", "logits", "[", ":", ",", "i", ",", "j", "]", ".", "view", "(", "-", "1", ",", "N", ")", ",", "tmp_label", ".", "view", "(", "-", "1", ")", ")", "\n", "loss", ".", "backward", "(", "retain_graph", "=", "True", ")", "\n", "\n", "grad_conv", "=", "self", ".", "basic_encoder", ".", "conv", ".", "weight", ".", "grad", "\n", "grad_fc", "=", "self", ".", "basic_fc", ".", "weight", ".", "grad", "\n", "\n", "basic_fast_conv_params", ".", "append", "(", "self", ".", "learner_basic", "(", "grad_conv", ",", "is_conv", "=", "True", ")", ")", "\n", "basic_fast_fc_params", ".", "append", "(", "self", ".", "learner_basic", "(", "grad_fc", ",", "is_conv", "=", "False", ")", ")", "\n", "", "", "basic_fast_conv_params", "=", "torch", ".", "stack", "(", "basic_fast_conv_params", ",", "0", ")", "# (N * K, conv_weight_size)", "\n", "basic_fast_fc_params", "=", "torch", ".", "stack", "(", "basic_fast_fc_params", ",", "0", ")", "# (N * K, fc_weight_size)", "\n", "\n", "# final", "\n", "self", ".", "zero_grad", "(", ")", "\n", "s_att", "=", "self", ".", "attention_emb", "(", "support", ",", "(", "-", "1", ",", "N", ",", "K", ",", "self", ".", "hidden_size", ")", ",", "use_fast", "=", "True", ")", "\n", "q_att", "=", "self", ".", "attention_emb", "(", "query", ",", "(", "-", "1", ",", "NQ", ",", "self", ".", "hidden_size", ")", ",", "use_fast", "=", "True", ")", "\n", "score", "=", "self", ".", "attention_score", "(", "s_att", ",", "q_att", ")", ".", "squeeze", "(", "0", ")", "# assume B = 1, (NQ, N * K)", "\n", "size_conv_param", "=", "basic_fast_conv_params", ".", "size", "(", ")", "[", "1", ":", "]", "\n", "size_fc_param", "=", "basic_fast_fc_params", ".", "size", "(", ")", "[", "1", ":", "]", "\n", "final_fast_conv_param", "=", "torch", ".", "matmul", "(", "score", ",", "basic_fast_conv_params", ".", "view", "(", "N", "*", "K", ",", "-", "1", ")", ")", "# (NQ, conv_weight_size)", "\n", "final_fast_fc_param", "=", "torch", ".", "matmul", "(", "score", ",", "basic_fast_fc_params", ".", "view", "(", "N", "*", "K", ",", "-", "1", ")", ")", "# (NQ, fc_weight_size)", "\n", "stack_logits", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "NQ", ")", ":", "\n", "            ", "self", ".", "basic_fast_conv_W", "=", "final_fast_conv_param", "[", "i", "]", ".", "view", "(", "size_conv_param", ")", "\n", "self", ".", "basic_fast_fc_W", "=", "final_fast_fc_param", "[", "i", "]", ".", "view", "(", "size_fc_param", ")", "\n", "q", "=", "self", ".", "basic_emb", "(", "{", "'word'", ":", "query", "[", "'word'", "]", "[", "i", ":", "i", "+", "1", "]", ",", "'pos1'", ":", "query", "[", "'pos1'", "]", "[", "i", ":", "i", "+", "1", "]", ",", "'pos2'", ":", "query", "[", "'pos2'", "]", "[", "i", ":", "i", "+", "1", "]", ",", "'mask'", ":", "query", "[", "'mask'", "]", "[", "i", ":", "i", "+", "1", "]", "}", ",", "(", "self", ".", "hidden_size", ")", ",", "use_fast", "=", "True", ")", "\n", "logits", "=", "self", ".", "basic_fc", "(", "q", ")", "+", "F", ".", "linear", "(", "q", ",", "self", ".", "basic_fast_fc_W", ")", "\n", "stack_logits", ".", "append", "(", "logits", ")", "\n", "", "logits", "=", "torch", ".", "stack", "(", "stack_logits", ",", "0", ")", "\n", "\n", "_", ",", "pred", "=", "torch", ".", "max", "(", "logits", ".", "view", "(", "-", "1", ",", "N", ")", ",", "1", ")", "\n", "return", "logits", ",", "pred", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.metanet.log_and_sign": [[12, 20], ["torch.cat", "torch.log", "numpy.exp", "torch.abs"], "function", ["None"], ["def", "log_and_sign", "(", "inputs", ",", "k", "=", "7", ")", ":", "\n", "    ", "eps", "=", "1e-7", "\n", "log", "=", "torch", ".", "log", "(", "torch", ".", "abs", "(", "inputs", ")", "+", "eps", ")", "/", "k", "\n", "log", "[", "log", "<", "-", "1.0", "]", "=", "-", "1.0", "\n", "sign", "=", "log", "*", "np", ".", "exp", "(", "k", ")", "\n", "sign", "[", "sign", "<", "-", "1.0", "]", "=", "-", "1.0", "\n", "sign", "[", "sign", ">", "1.0", "]", "=", "1.0", "\n", "return", "torch", ".", "cat", "(", "[", "log", ",", "sign", "]", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.gnn_iclr.Gconv.__init__": [[39, 49], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.BatchNorm1d"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "nf_input", ",", "nf_output", ",", "J", ",", "bn_bool", "=", "True", ")", ":", "\n", "        ", "super", "(", "Gconv", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "J", "=", "J", "\n", "self", ".", "num_inputs", "=", "J", "*", "nf_input", "\n", "self", ".", "num_outputs", "=", "nf_output", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "self", ".", "num_inputs", ",", "self", ".", "num_outputs", ")", "\n", "\n", "self", ".", "bn_bool", "=", "bn_bool", "\n", "if", "self", ".", "bn_bool", ":", "\n", "            ", "self", ".", "bn", "=", "nn", ".", "BatchNorm1d", "(", "self", ".", "num_outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.gnn_iclr.Gconv.forward": [[50, 65], ["gnn_iclr.gmul", "gnn_iclr.Gconv.size", "gnn_iclr.Gconv.contiguous", "gnn_iclr.Gconv.view", "gnn_iclr.Gconv.fc", "gnn_iclr.Gconv.view", "gnn_iclr.Gconv.bn"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.gnn_iclr.gmul"], ["", "", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "W", "=", "input", "[", "0", "]", "\n", "x", "=", "gmul", "(", "input", ")", "# out has size (bs, N, num_inputs)", "\n", "#if self.J == 1:", "\n", "#    x = torch.abs(x)", "\n", "x_size", "=", "x", ".", "size", "(", ")", "\n", "x", "=", "x", ".", "contiguous", "(", ")", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "self", ".", "num_inputs", ")", "\n", "x", "=", "self", ".", "fc", "(", "x", ")", "# has size (bs*N, num_outputs)", "\n", "\n", "if", "self", ".", "bn_bool", ":", "\n", "            ", "x", "=", "self", ".", "bn", "(", "x", ")", "\n", "\n", "", "x", "=", "x", ".", "view", "(", "x_size", "[", "0", "]", ",", "x_size", "[", "1", "]", ",", "self", ".", "num_outputs", ")", "\n", "return", "W", ",", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.gnn_iclr.Wcompute.__init__": [[68, 85], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "int", "int", "torch.Dropout", "torch.Dropout", "torch.Dropout", "int", "int", "int", "int"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_features", ",", "nf", ",", "operator", "=", "'J2'", ",", "activation", "=", "'softmax'", ",", "ratio", "=", "[", "2", ",", "2", ",", "1", ",", "1", "]", ",", "num_operators", "=", "1", ",", "drop", "=", "False", ")", ":", "\n", "        ", "super", "(", "Wcompute", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_features", "=", "nf", "\n", "self", ".", "operator", "=", "operator", "\n", "self", ".", "conv2d_1", "=", "nn", ".", "Conv2d", "(", "input_features", ",", "int", "(", "nf", "*", "ratio", "[", "0", "]", ")", ",", "1", ",", "stride", "=", "1", ")", "\n", "self", ".", "bn_1", "=", "nn", ".", "BatchNorm2d", "(", "int", "(", "nf", "*", "ratio", "[", "0", "]", ")", ")", "\n", "self", ".", "drop", "=", "drop", "\n", "if", "self", ".", "drop", ":", "\n", "            ", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "0.3", ")", "\n", "", "self", ".", "conv2d_2", "=", "nn", ".", "Conv2d", "(", "int", "(", "nf", "*", "ratio", "[", "0", "]", ")", ",", "int", "(", "nf", "*", "ratio", "[", "1", "]", ")", ",", "1", ",", "stride", "=", "1", ")", "\n", "self", ".", "bn_2", "=", "nn", ".", "BatchNorm2d", "(", "int", "(", "nf", "*", "ratio", "[", "1", "]", ")", ")", "\n", "self", ".", "conv2d_3", "=", "nn", ".", "Conv2d", "(", "int", "(", "nf", "*", "ratio", "[", "1", "]", ")", ",", "nf", "*", "ratio", "[", "2", "]", ",", "1", ",", "stride", "=", "1", ")", "\n", "self", ".", "bn_3", "=", "nn", ".", "BatchNorm2d", "(", "nf", "*", "ratio", "[", "2", "]", ")", "\n", "self", ".", "conv2d_4", "=", "nn", ".", "Conv2d", "(", "nf", "*", "ratio", "[", "2", "]", ",", "nf", "*", "ratio", "[", "3", "]", ",", "1", ",", "stride", "=", "1", ")", "\n", "self", ".", "bn_4", "=", "nn", ".", "BatchNorm2d", "(", "nf", "*", "ratio", "[", "3", "]", ")", "\n", "self", ".", "conv2d_last", "=", "nn", ".", "Conv2d", "(", "nf", ",", "num_operators", ",", "1", ",", "stride", "=", "1", ")", "\n", "self", ".", "activation", "=", "activation", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.gnn_iclr.Wcompute.forward": [[86, 141], ["x.unsqueeze", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "gnn_iclr.Wcompute.conv2d_1", "gnn_iclr.Wcompute.bn_1", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "gnn_iclr.Wcompute.conv2d_2", "gnn_iclr.Wcompute.bn_2", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "gnn_iclr.Wcompute.conv2d_3", "gnn_iclr.Wcompute.bn_3", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "gnn_iclr.Wcompute.conv2d_4", "gnn_iclr.Wcompute.bn_4", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "gnn_iclr.Wcompute.conv2d_last", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "gnn_iclr.Wcompute.dropout", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.cat.contiguous", "torch.cat.contiguous", "torch.cat.contiguous", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.view", "torch.cat.view", "torch.cat.view", "torch.softmax", "torch.softmax", "torch.softmax", "torch.cat.view", "torch.cat.view", "torch.cat.view", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "W_id.expand_as"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "W_id", ")", ":", "\n", "        ", "W1", "=", "x", ".", "unsqueeze", "(", "2", ")", "\n", "W2", "=", "torch", ".", "transpose", "(", "W1", ",", "1", ",", "2", ")", "#size: bs x N x N x num_features", "\n", "W_new", "=", "torch", ".", "abs", "(", "W1", "-", "W2", ")", "#size: bs x N x N x num_features", "\n", "W_new", "=", "torch", ".", "transpose", "(", "W_new", ",", "1", ",", "3", ")", "#size: bs x num_features x N x N", "\n", "\n", "W_new", "=", "self", ".", "conv2d_1", "(", "W_new", ")", "\n", "W_new", "=", "self", ".", "bn_1", "(", "W_new", ")", "\n", "W_new", "=", "F", ".", "leaky_relu", "(", "W_new", ")", "\n", "if", "self", ".", "drop", ":", "\n", "            ", "W_new", "=", "self", ".", "dropout", "(", "W_new", ")", "\n", "\n", "", "W_new", "=", "self", ".", "conv2d_2", "(", "W_new", ")", "\n", "W_new", "=", "self", ".", "bn_2", "(", "W_new", ")", "\n", "W_new", "=", "F", ".", "leaky_relu", "(", "W_new", ")", "\n", "\n", "W_new", "=", "self", ".", "conv2d_3", "(", "W_new", ")", "\n", "W_new", "=", "self", ".", "bn_3", "(", "W_new", ")", "\n", "W_new", "=", "F", ".", "leaky_relu", "(", "W_new", ")", "\n", "\n", "W_new", "=", "self", ".", "conv2d_4", "(", "W_new", ")", "\n", "W_new", "=", "self", ".", "bn_4", "(", "W_new", ")", "\n", "W_new", "=", "F", ".", "leaky_relu", "(", "W_new", ")", "\n", "\n", "W_new", "=", "self", ".", "conv2d_last", "(", "W_new", ")", "\n", "W_new", "=", "torch", ".", "transpose", "(", "W_new", ",", "1", ",", "3", ")", "#size: bs x N x N x 1", "\n", "\n", "if", "self", ".", "activation", "==", "'softmax'", ":", "\n", "            ", "W_new", "=", "W_new", "-", "W_id", ".", "expand_as", "(", "W_new", ")", "*", "1e8", "\n", "W_new", "=", "torch", ".", "transpose", "(", "W_new", ",", "2", ",", "3", ")", "\n", "# Applying Softmax", "\n", "W_new", "=", "W_new", ".", "contiguous", "(", ")", "\n", "W_new_size", "=", "W_new", ".", "size", "(", ")", "\n", "W_new", "=", "W_new", ".", "view", "(", "-", "1", ",", "W_new", ".", "size", "(", "3", ")", ")", "\n", "W_new", "=", "F", ".", "softmax", "(", "W_new", ")", "\n", "W_new", "=", "W_new", ".", "view", "(", "W_new_size", ")", "\n", "# Softmax applied", "\n", "W_new", "=", "torch", ".", "transpose", "(", "W_new", ",", "2", ",", "3", ")", "\n", "\n", "", "elif", "self", ".", "activation", "==", "'sigmoid'", ":", "\n", "            ", "W_new", "=", "F", ".", "sigmoid", "(", "W_new", ")", "\n", "W_new", "*=", "(", "1", "-", "W_id", ")", "\n", "", "elif", "self", ".", "activation", "==", "'none'", ":", "\n", "            ", "W_new", "*=", "(", "1", "-", "W_id", ")", "\n", "", "else", ":", "\n", "            ", "raise", "(", "NotImplementedError", ")", "\n", "\n", "", "if", "self", ".", "operator", "==", "'laplace'", ":", "\n", "            ", "W_new", "=", "W_id", "-", "W_new", "\n", "", "elif", "self", ".", "operator", "==", "'J2'", ":", "\n", "            ", "W_new", "=", "torch", ".", "cat", "(", "[", "W_id", ",", "W_new", "]", ",", "3", ")", "\n", "", "else", ":", "\n", "            ", "raise", "(", "NotImplementedError", ")", "\n", "\n", "", "return", "W_new", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.gnn_iclr.GNN_nl_omniglot.__init__": [[144, 164], ["torch.Module.__init__", "range", "gnn_iclr.Wcompute", "gnn_iclr.Gconv", "gnn_iclr.Wcompute", "gnn_iclr.Gconv", "gnn_iclr.GNN_nl_omniglot.add_module", "gnn_iclr.GNN_nl_omniglot.add_module", "int", "int", "int", "int", "int", "int", "int"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "input_features", ",", "nf", ",", "J", ")", ":", "\n", "        ", "super", "(", "GNN_nl_omniglot", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "input_features", "=", "input_features", "\n", "self", ".", "nf", "=", "nf", "\n", "self", ".", "J", "=", "J", "\n", "\n", "self", ".", "num_layers", "=", "2", "\n", "for", "i", "in", "range", "(", "self", ".", "num_layers", ")", ":", "\n", "            ", "module_w", "=", "Wcompute", "(", "self", ".", "input_features", "+", "int", "(", "nf", "/", "2", ")", "*", "i", ",", "\n", "self", ".", "input_features", "+", "int", "(", "nf", "/", "2", ")", "*", "i", ",", "\n", "operator", "=", "'J2'", ",", "activation", "=", "'softmax'", ",", "ratio", "=", "[", "2", ",", "1.5", ",", "1", ",", "1", "]", ",", "drop", "=", "False", ")", "\n", "module_l", "=", "Gconv", "(", "self", ".", "input_features", "+", "int", "(", "nf", "/", "2", ")", "*", "i", ",", "int", "(", "nf", "/", "2", ")", ",", "2", ")", "\n", "self", ".", "add_module", "(", "'layer_w{}'", ".", "format", "(", "i", ")", ",", "module_w", ")", "\n", "self", ".", "add_module", "(", "'layer_l{}'", ".", "format", "(", "i", ")", ",", "module_l", ")", "\n", "\n", "", "self", ".", "w_comp_last", "=", "Wcompute", "(", "self", ".", "input_features", "+", "int", "(", "self", ".", "nf", "/", "2", ")", "*", "self", ".", "num_layers", ",", "\n", "self", ".", "input_features", "+", "int", "(", "self", ".", "nf", "/", "2", ")", "*", "(", "self", ".", "num_layers", "-", "1", ")", ",", "\n", "operator", "=", "'J2'", ",", "activation", "=", "'softmax'", ",", "ratio", "=", "[", "2", ",", "1.5", ",", "1", ",", "1", "]", ",", "drop", "=", "True", ")", "\n", "self", ".", "layer_last", "=", "Gconv", "(", "self", ".", "input_features", "+", "int", "(", "self", ".", "nf", "/", "2", ")", "*", "self", ".", "num_layers", ",", "args", ".", "train_N_way", ",", "2", ",", "bn_bool", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.gnn_iclr.GNN_nl_omniglot.forward": [[165, 180], ["torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "range", "gnn_iclr.GNN_nl_omniglot.w_comp_last", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.eye().unsqueeze().repeat().unsqueeze", "W_init.cuda.cuda.cuda", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "gnn_iclr.GNN_nl_omniglot.layer_last", "torch.eye().unsqueeze().repeat", "torch.eye().unsqueeze().repeat", "torch.eye().unsqueeze().repeat", "torch.eye().unsqueeze().repeat", "torch.eye().unsqueeze().repeat", "torch.eye().unsqueeze().repeat", "torch.eye().unsqueeze().repeat", "torch.eye().unsqueeze().repeat", "torch.eye().unsqueeze().repeat", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.cat.size", "torch.cat.size", "torch.cat.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "W_init", "=", "Variable", "(", "torch", ".", "eye", "(", "x", ".", "size", "(", "1", ")", ")", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "x", ".", "size", "(", "0", ")", ",", "1", ",", "1", ")", ".", "unsqueeze", "(", "3", ")", ")", "\n", "if", "self", ".", "args", ".", "cuda", ":", "\n", "            ", "W_init", "=", "W_init", ".", "cuda", "(", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "self", ".", "num_layers", ")", ":", "\n", "            ", "Wi", "=", "self", ".", "_modules", "[", "'layer_w{}'", ".", "format", "(", "i", ")", "]", "(", "x", ",", "W_init", ")", "\n", "\n", "x_new", "=", "F", ".", "leaky_relu", "(", "self", ".", "_modules", "[", "'layer_l{}'", ".", "format", "(", "i", ")", "]", "(", "[", "Wi", ",", "x", "]", ")", "[", "1", "]", ")", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x", ",", "x_new", "]", ",", "2", ")", "\n", "\n", "", "Wl", "=", "self", ".", "w_comp_last", "(", "x", ",", "W_init", ")", "\n", "out", "=", "self", ".", "layer_last", "(", "[", "Wl", ",", "x", "]", ")", "[", "1", "]", "\n", "\n", "return", "out", "[", ":", ",", "0", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.gnn_iclr.GNN_nl.__init__": [[183, 204], ["torch.Module.__init__", "range", "gnn_iclr.Wcompute", "gnn_iclr.Gconv", "gnn_iclr.GNN_nl.add_module", "gnn_iclr.GNN_nl.add_module", "gnn_iclr.Wcompute", "gnn_iclr.Gconv", "gnn_iclr.Wcompute", "gnn_iclr.Gconv", "int", "int", "int", "int", "int", "int"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "N", ",", "input_features", ",", "nf", ",", "J", ")", ":", "\n", "        ", "super", "(", "GNN_nl", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# self.args = args", "\n", "self", ".", "input_features", "=", "input_features", "\n", "self", ".", "nf", "=", "nf", "\n", "self", ".", "J", "=", "J", "\n", "\n", "self", ".", "num_layers", "=", "2", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "num_layers", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "module_w", "=", "Wcompute", "(", "self", ".", "input_features", ",", "nf", ",", "operator", "=", "'J2'", ",", "activation", "=", "'softmax'", ",", "ratio", "=", "[", "2", ",", "2", ",", "1", ",", "1", "]", ")", "\n", "module_l", "=", "Gconv", "(", "self", ".", "input_features", ",", "int", "(", "nf", "/", "2", ")", ",", "2", ")", "\n", "", "else", ":", "\n", "                ", "module_w", "=", "Wcompute", "(", "self", ".", "input_features", "+", "int", "(", "nf", "/", "2", ")", "*", "i", ",", "nf", ",", "operator", "=", "'J2'", ",", "activation", "=", "'softmax'", ",", "ratio", "=", "[", "2", ",", "2", ",", "1", ",", "1", "]", ")", "\n", "module_l", "=", "Gconv", "(", "self", ".", "input_features", "+", "int", "(", "nf", "/", "2", ")", "*", "i", ",", "int", "(", "nf", "/", "2", ")", ",", "2", ")", "\n", "", "self", ".", "add_module", "(", "'layer_w{}'", ".", "format", "(", "i", ")", ",", "module_w", ")", "\n", "self", ".", "add_module", "(", "'layer_l{}'", ".", "format", "(", "i", ")", ",", "module_l", ")", "\n", "\n", "", "self", ".", "w_comp_last", "=", "Wcompute", "(", "self", ".", "input_features", "+", "int", "(", "self", ".", "nf", "/", "2", ")", "*", "self", ".", "num_layers", ",", "nf", ",", "operator", "=", "'J2'", ",", "activation", "=", "'softmax'", ",", "ratio", "=", "[", "2", ",", "2", ",", "1", ",", "1", "]", ")", "\n", "self", ".", "layer_last", "=", "Gconv", "(", "self", ".", "input_features", "+", "int", "(", "self", ".", "nf", "/", "2", ")", "*", "self", ".", "num_layers", ",", "N", ",", "2", ",", "bn_bool", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.gnn_iclr.GNN_nl.forward": [[205, 219], ["torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "W_init.cuda.cuda.cuda", "range", "gnn_iclr.GNN_nl.w_comp_last", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "gnn_iclr.GNN_nl.layer_last", "torch.eye().unsqueeze().repeat", "torch.eye().unsqueeze().repeat", "torch.eye().unsqueeze().repeat", "torch.eye().unsqueeze().repeat", "torch.eye().unsqueeze().repeat", "torch.eye().unsqueeze().repeat", "torch.eye().unsqueeze().repeat", "torch.eye().unsqueeze().repeat", "torch.eye().unsqueeze().repeat", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.cat.size", "torch.cat.size", "torch.cat.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "W_init", "=", "Variable", "(", "torch", ".", "eye", "(", "x", ".", "size", "(", "1", ")", ")", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "x", ".", "size", "(", "0", ")", ",", "1", ",", "1", ")", ".", "unsqueeze", "(", "3", ")", ")", "\n", "W_init", "=", "W_init", ".", "cuda", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "num_layers", ")", ":", "\n", "            ", "Wi", "=", "self", ".", "_modules", "[", "'layer_w{}'", ".", "format", "(", "i", ")", "]", "(", "x", ",", "W_init", ")", "\n", "\n", "x_new", "=", "F", ".", "leaky_relu", "(", "self", ".", "_modules", "[", "'layer_l{}'", ".", "format", "(", "i", ")", "]", "(", "[", "Wi", ",", "x", "]", ")", "[", "1", "]", ")", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x", ",", "x_new", "]", ",", "2", ")", "\n", "\n", "", "Wl", "=", "self", ".", "w_comp_last", "(", "x", ",", "W_init", ")", "\n", "out", "=", "self", ".", "layer_last", "(", "[", "Wl", ",", "x", "]", ")", "[", "1", "]", "\n", "\n", "return", "out", "[", ":", ",", "0", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.gnn_iclr.GNN_active.__init__": [[221, 256], ["torch.Module.__init__", "range", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "range", "gnn_iclr.Wcompute", "gnn_iclr.Gconv", "gnn_iclr.GNN_active.add_module", "gnn_iclr.GNN_active.add_module", "int", "gnn_iclr.GNN_active.add_module", "gnn_iclr.GNN_active.add_module", "gnn_iclr.Wcompute", "gnn_iclr.Gconv", "gnn_iclr.Wcompute", "gnn_iclr.Gconv", "gnn_iclr.Wcompute", "gnn_iclr.Gconv", "gnn_iclr.Wcompute", "gnn_iclr.Gconv", "int", "int", "int", "int", "int", "int", "int", "int", "int", "int", "int", "int", "int", "int"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "input_features", ",", "nf", ",", "J", ")", ":", "\n", "        ", "super", "(", "GNN_active", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "input_features", "=", "input_features", "\n", "self", ".", "nf", "=", "nf", "\n", "self", ".", "J", "=", "J", "\n", "\n", "self", ".", "num_layers", "=", "2", "\n", "for", "i", "in", "range", "(", "self", ".", "num_layers", "//", "2", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "module_w", "=", "Wcompute", "(", "self", ".", "input_features", ",", "nf", ",", "operator", "=", "'J2'", ",", "activation", "=", "'softmax'", ",", "ratio", "=", "[", "2", ",", "2", ",", "1", ",", "1", "]", ")", "\n", "module_l", "=", "Gconv", "(", "self", ".", "input_features", ",", "int", "(", "nf", "/", "2", ")", ",", "2", ")", "\n", "", "else", ":", "\n", "                ", "module_w", "=", "Wcompute", "(", "self", ".", "input_features", "+", "int", "(", "nf", "/", "2", ")", "*", "i", ",", "nf", ",", "operator", "=", "'J2'", ",", "activation", "=", "'softmax'", ",", "ratio", "=", "[", "2", ",", "2", ",", "1", ",", "1", "]", ")", "\n", "module_l", "=", "Gconv", "(", "self", ".", "input_features", "+", "int", "(", "nf", "/", "2", ")", "*", "i", ",", "int", "(", "nf", "/", "2", ")", ",", "2", ")", "\n", "\n", "", "self", ".", "add_module", "(", "'layer_w{}'", ".", "format", "(", "i", ")", ",", "module_w", ")", "\n", "self", ".", "add_module", "(", "'layer_l{}'", ".", "format", "(", "i", ")", ",", "module_l", ")", "\n", "\n", "", "self", ".", "conv_active_1", "=", "nn", ".", "Conv1d", "(", "self", ".", "input_features", "+", "int", "(", "nf", "/", "2", ")", "*", "1", ",", "self", ".", "input_features", "+", "int", "(", "nf", "/", "2", ")", "*", "1", ",", "1", ")", "\n", "self", ".", "bn_active", "=", "nn", ".", "BatchNorm1d", "(", "self", ".", "input_features", "+", "int", "(", "nf", "/", "2", ")", "*", "1", ")", "\n", "self", ".", "conv_active_2", "=", "nn", ".", "Conv1d", "(", "self", ".", "input_features", "+", "int", "(", "nf", "/", "2", ")", "*", "1", ",", "1", ",", "1", ")", "\n", "\n", "for", "i", "in", "range", "(", "int", "(", "self", ".", "num_layers", "/", "2", ")", ",", "self", ".", "num_layers", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "module_w", "=", "Wcompute", "(", "self", ".", "input_features", ",", "nf", ",", "operator", "=", "'J2'", ",", "activation", "=", "'softmax'", ",", "ratio", "=", "[", "2", ",", "2", ",", "1", ",", "1", "]", ")", "\n", "module_l", "=", "Gconv", "(", "self", ".", "input_features", ",", "int", "(", "nf", "/", "2", ")", ",", "2", ")", "\n", "", "else", ":", "\n", "                ", "module_w", "=", "Wcompute", "(", "self", ".", "input_features", "+", "int", "(", "nf", "/", "2", ")", "*", "i", ",", "nf", ",", "operator", "=", "'J2'", ",", "activation", "=", "'softmax'", ",", "ratio", "=", "[", "2", ",", "2", ",", "1", ",", "1", "]", ")", "\n", "module_l", "=", "Gconv", "(", "self", ".", "input_features", "+", "int", "(", "nf", "/", "2", ")", "*", "i", ",", "int", "(", "nf", "/", "2", ")", ",", "2", ")", "\n", "", "self", ".", "add_module", "(", "'layer_w{}'", ".", "format", "(", "i", ")", ",", "module_w", ")", "\n", "self", ".", "add_module", "(", "'layer_l{}'", ".", "format", "(", "i", ")", ",", "module_l", ")", "\n", "\n", "", "self", ".", "w_comp_last", "=", "Wcompute", "(", "self", ".", "input_features", "+", "int", "(", "self", ".", "nf", "/", "2", ")", "*", "self", ".", "num_layers", ",", "nf", ",", "operator", "=", "'J2'", ",", "activation", "=", "'softmax'", ",", "ratio", "=", "[", "2", ",", "2", ",", "1", ",", "1", "]", ")", "\n", "self", ".", "layer_last", "=", "Gconv", "(", "self", ".", "input_features", "+", "int", "(", "self", ".", "nf", "/", "2", ")", "*", "self", ".", "num_layers", ",", "args", ".", "train_N_way", ",", "2", ",", "bn_bool", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.gnn_iclr.GNN_active.active": [[257, 301], ["torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "gnn_iclr.GNN_active.conv_active_1", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "gnn_iclr.GNN_active.conv_active_2", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "x_active.detach.detach.squeeze", "torch.softmax", "torch.softmax", "torch.softmax", "decision.unsqueeze.unsqueeze.detach", "torch.FloatTensor().zero_", "torch.FloatTensor().zero_", "torch.FloatTensor().zero_", "torch.FloatTensor().zero_", "torch.FloatTensor().zero_", "torch.FloatTensor().zero_", "torch.FloatTensor().zero_", "torch.FloatTensor().zero_", "torch.FloatTensor().zero_", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "mapping.cuda.cuda.scatter_", "mapping_bp.expand_as.expand_as.expand_as", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.autograd.Variable().detach", "torch.autograd.Variable().detach", "torch.autograd.Variable().detach", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "gnn_iclr.GNN_active.bn_active", "x_active.detach.detach.data.fill_", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "x_active.detach.detach.detach", "mapping.cuda.cuda.cuda", "x.size", "x.size", "padd.cuda.cuda.cuda", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "decision.unsqueeze.unsqueeze.unsqueeze", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "x.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "x_active.detach.detach.size", "decision.unsqueeze.unsqueeze.size", "x_active.detach.detach.size"], "methods", ["None"], ["", "def", "active", "(", "self", ",", "x", ",", "oracles_yi", ",", "hidden_labels", ")", ":", "\n", "        ", "x_active", "=", "torch", ".", "transpose", "(", "x", ",", "1", ",", "2", ")", "\n", "x_active", "=", "self", ".", "conv_active_1", "(", "x_active", ")", "\n", "x_active", "=", "F", ".", "leaky_relu", "(", "self", ".", "bn_active", "(", "x_active", ")", ")", "\n", "x_active", "=", "self", ".", "conv_active_2", "(", "x_active", ")", "\n", "x_active", "=", "torch", ".", "transpose", "(", "x_active", ",", "1", ",", "2", ")", "\n", "\n", "x_active", "=", "x_active", ".", "squeeze", "(", "-", "1", ")", "\n", "x_active", "=", "x_active", "-", "(", "1", "-", "hidden_labels", ")", "*", "1e8", "\n", "x_active", "=", "F", ".", "softmax", "(", "x_active", ")", "\n", "x_active", "=", "x_active", "*", "hidden_labels", "\n", "\n", "if", "self", ".", "args", ".", "active_random", "==", "1", ":", "\n", "#print('random active')", "\n", "            ", "x_active", ".", "data", ".", "fill_", "(", "1.", "/", "x_active", ".", "size", "(", "1", ")", ")", "\n", "decision", "=", "torch", ".", "multinomial", "(", "x_active", ")", "\n", "x_active", "=", "x_active", ".", "detach", "(", ")", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "training", ":", "\n", "                ", "decision", "=", "torch", ".", "multinomial", "(", "x_active", ")", "\n", "", "else", ":", "\n", "                ", "_", ",", "decision", "=", "torch", ".", "max", "(", "x_active", ",", "1", ")", "\n", "decision", "=", "decision", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "", "", "decision", "=", "decision", ".", "detach", "(", ")", "\n", "\n", "mapping", "=", "torch", ".", "FloatTensor", "(", "decision", ".", "size", "(", "0", ")", ",", "x_active", ".", "size", "(", "1", ")", ")", ".", "zero_", "(", ")", "\n", "mapping", "=", "Variable", "(", "mapping", ")", "\n", "if", "self", ".", "args", ".", "cuda", ":", "\n", "            ", "mapping", "=", "mapping", ".", "cuda", "(", ")", "\n", "", "mapping", ".", "scatter_", "(", "1", ",", "decision", ",", "1", ")", "\n", "\n", "mapping_bp", "=", "(", "x_active", "*", "mapping", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "mapping_bp", "=", "mapping_bp", ".", "expand_as", "(", "oracles_yi", ")", "\n", "\n", "label2add", "=", "mapping_bp", "*", "oracles_yi", "#bsxNodesxN_way", "\n", "padd", "=", "torch", ".", "zeros", "(", "x", ".", "size", "(", "0", ")", ",", "x", ".", "size", "(", "1", ")", ",", "x", ".", "size", "(", "2", ")", "-", "label2add", ".", "size", "(", "2", ")", ")", "\n", "padd", "=", "Variable", "(", "padd", ")", ".", "detach", "(", ")", "\n", "if", "self", ".", "args", ".", "cuda", ":", "\n", "            ", "padd", "=", "padd", ".", "cuda", "(", ")", "\n", "", "label2add", "=", "torch", ".", "cat", "(", "[", "label2add", ",", "padd", "]", ",", "2", ")", "\n", "\n", "x", "=", "x", "+", "label2add", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.gnn_iclr.GNN_active.forward": [[303, 325], ["torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "range", "gnn_iclr.GNN_active.active", "range", "gnn_iclr.GNN_active.w_comp_last", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.eye().unsqueeze().repeat().unsqueeze", "torch.eye().unsqueeze().repeat().unsqueeze", "W_init.cuda.cuda.cuda", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "int", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "gnn_iclr.GNN_active.layer_last", "torch.eye().unsqueeze().repeat", "torch.eye().unsqueeze().repeat", "torch.eye().unsqueeze().repeat", "torch.eye().unsqueeze().repeat", "torch.eye().unsqueeze().repeat", "torch.eye().unsqueeze().repeat", "torch.eye().unsqueeze().repeat", "torch.eye().unsqueeze().repeat", "torch.eye().unsqueeze().repeat", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.cat.size", "torch.cat.size", "torch.cat.size"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.gnn_iclr.GNN_active.active"], ["", "def", "forward", "(", "self", ",", "x", ",", "oracles_yi", ",", "hidden_labels", ")", ":", "\n", "        ", "W_init", "=", "Variable", "(", "torch", ".", "eye", "(", "x", ".", "size", "(", "1", ")", ")", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "x", ".", "size", "(", "0", ")", ",", "1", ",", "1", ")", ".", "unsqueeze", "(", "3", ")", ")", "\n", "if", "self", ".", "args", ".", "cuda", ":", "\n", "            ", "W_init", "=", "W_init", ".", "cuda", "(", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "self", ".", "num_layers", "//", "2", ")", ":", "\n", "            ", "Wi", "=", "self", ".", "_modules", "[", "'layer_w{}'", ".", "format", "(", "i", ")", "]", "(", "x", ",", "W_init", ")", "\n", "x_new", "=", "F", ".", "leaky_relu", "(", "self", ".", "_modules", "[", "'layer_l{}'", ".", "format", "(", "i", ")", "]", "(", "[", "Wi", ",", "x", "]", ")", "[", "1", "]", ")", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x", ",", "x_new", "]", ",", "2", ")", "\n", "\n", "", "x", "=", "self", ".", "active", "(", "x", ",", "oracles_yi", ",", "hidden_labels", ")", "\n", "\n", "for", "i", "in", "range", "(", "int", "(", "self", ".", "num_layers", "/", "2", ")", ",", "self", ".", "num_layers", ")", ":", "\n", "            ", "Wi", "=", "self", ".", "_modules", "[", "'layer_w{}'", ".", "format", "(", "i", ")", "]", "(", "x", ",", "W_init", ")", "\n", "x_new", "=", "F", ".", "leaky_relu", "(", "self", ".", "_modules", "[", "'layer_l{}'", ".", "format", "(", "i", ")", "]", "(", "[", "Wi", ",", "x", "]", ")", "[", "1", "]", ")", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x", ",", "x_new", "]", ",", "2", ")", "\n", "\n", "\n", "", "Wl", "=", "self", ".", "w_comp_last", "(", "x", ",", "W_init", ")", "\n", "out", "=", "self", ".", "layer_last", "(", "[", "Wl", ",", "x", "]", ")", "[", "1", "]", "\n", "\n", "return", "out", "[", ":", ",", "0", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.gnn_iclr.gmul": [[23, 36], ["x.size", "torch.cat().squeeze.size", "torch.cat().squeeze.split", "torch.cat().squeeze", "torch.cat().squeeze", "torch.cat().squeeze", "torch.bmm", "torch.bmm", "torch.bmm", "torch.cat.split", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "function", ["None"], ["", "def", "gmul", "(", "input", ")", ":", "\n", "    ", "W", ",", "x", "=", "input", "\n", "# x is a tensor of size (bs, N, num_features)", "\n", "# W is a tensor of size (bs, N, N, J)", "\n", "x_size", "=", "x", ".", "size", "(", ")", "\n", "W_size", "=", "W", ".", "size", "(", ")", "\n", "N", "=", "W_size", "[", "-", "2", "]", "\n", "W", "=", "W", ".", "split", "(", "1", ",", "3", ")", "\n", "W", "=", "torch", ".", "cat", "(", "W", ",", "1", ")", ".", "squeeze", "(", "3", ")", "# W is now a tensor of size (bs, J*N, N)", "\n", "output", "=", "torch", ".", "bmm", "(", "W", ",", "x", ")", "# output has size (bs, J*N, num_features)", "\n", "output", "=", "output", ".", "split", "(", "N", ",", "1", ")", "\n", "output", "=", "torch", ".", "cat", "(", "output", ",", "2", ")", "# output has size (bs, N, J*num_features)", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.d.Discriminator.__init__": [[11, 19], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", "=", "230", ",", "num_labels", "=", "2", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ")", "\n", "self", ".", "relu1", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.d.Discriminator.forward": [[20, 26], ["d.Discriminator.fc1", "d.Discriminator.relu1", "d.Discriminator.drop", "d.Discriminator.fc2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "x", "=", "self", ".", "relu1", "(", "x", ")", "\n", "x", "=", "self", ".", "drop", "(", "x", ")", "\n", "logits", "=", "self", ".", "fc2", "(", "x", ")", "\n", "return", "logits", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.proto_norm.ProtoNorm.__init__": [[16, 21], ["fewshot_re_kit.framework.FewShotREModel.__init__", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "sentence_encoder", ",", "hidden_size", "=", "230", ")", ":", "\n", "        ", "fewshot_re_kit", ".", "framework", ".", "FewShotREModel", ".", "__init__", "(", "self", ",", "sentence_encoder", ")", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "# self.fc = nn.Linear(hidden_size, hidden_size)", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.proto_norm.ProtoNorm.__dist__": [[22, 24], ["torch.pow().sum", "torch.pow"], "methods", ["None"], ["", "def", "__dist__", "(", "self", ",", "x", ",", "y", ",", "dim", ")", ":", "\n", "        ", "return", "(", "torch", ".", "pow", "(", "x", "-", "y", ",", "2", ")", ")", ".", "sum", "(", "dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.proto_norm.ProtoNorm.__batch_dist__": [[25, 27], ["proto_norm.ProtoNorm.__dist__", "S.unsqueeze", "Q.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.proto.Proto.__dist__"], ["", "def", "__batch_dist__", "(", "self", ",", "S", ",", "Q", ")", ":", "\n", "        ", "return", "self", ".", "__dist__", "(", "S", ".", "unsqueeze", "(", "1", ")", ",", "Q", ".", "unsqueeze", "(", "2", ")", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.proto_norm.ProtoNorm.forward": [[28, 55], ["proto_norm.ProtoNorm.sentence_encoder", "proto_norm.ProtoNorm.sentence_encoder", "proto_norm.l2norm", "proto_norm.l2norm", "proto_norm.ProtoNorm.drop", "proto_norm.ProtoNorm.drop", "torch.mean.view", "query.view.view.view", "torch.mean.size", "torch.mean", "torch.cat.min", "torch.cat", "torch.max", "proto_norm.ProtoNorm.__batch_dist__", "torch.cat.view", "minn.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.proto_norm.l2norm", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.proto_norm.l2norm", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.proto.Proto.__batch_dist__"], ["", "def", "forward", "(", "self", ",", "support", ",", "query", ",", "N", ",", "K", ",", "total_Q", ")", ":", "\n", "        ", "'''\n        support: Inputs of the support set.\n        query: Inputs of the query set.\n        N: Num of classes\n        K: Num of instances for each class in the support set\n        Q: Num of instances in the query set\n        '''", "\n", "support", "=", "self", ".", "sentence_encoder", "(", "support", ")", "# (B * N * K, D), where D is the hidden size", "\n", "query", "=", "self", ".", "sentence_encoder", "(", "query", ")", "# (B * total_Q, D)", "\n", "support", "=", "l2norm", "(", "support", ")", "\n", "query", "=", "l2norm", "(", "query", ")", "\n", "support", "=", "self", ".", "drop", "(", "support", ")", "\n", "query", "=", "self", ".", "drop", "(", "query", ")", "\n", "support", "=", "support", ".", "view", "(", "-", "1", ",", "N", ",", "K", ",", "self", ".", "hidden_size", ")", "# (B, N, K, D)", "\n", "query", "=", "query", ".", "view", "(", "-", "1", ",", "total_Q", ",", "self", ".", "hidden_size", ")", "# (B, total_Q, D)", "\n", "\n", "B", "=", "support", ".", "size", "(", "0", ")", "# Batch size", "\n", "\n", "# Prototypical Networks ", "\n", "# Ignore NA policy", "\n", "support", "=", "torch", ".", "mean", "(", "support", ",", "2", ")", "# Calculate prototype for each class", "\n", "logits", "=", "-", "self", ".", "__batch_dist__", "(", "support", ",", "query", ")", "# (B, total_Q, N)", "\n", "minn", ",", "_", "=", "logits", ".", "min", "(", "-", "1", ")", "\n", "logits", "=", "torch", ".", "cat", "(", "[", "logits", ",", "minn", ".", "unsqueeze", "(", "2", ")", "-", "1", "]", ",", "2", ")", "# (B, total_Q, N + 1)", "\n", "_", ",", "pred", "=", "torch", ".", "max", "(", "logits", ".", "view", "(", "-", "1", ",", "N", "+", "1", ")", ",", "1", ")", "\n", "return", "logits", ",", "pred", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.proto_norm.l2norm": [[9, 13], ["torch.pow().sum().sqrt", "torch.div", "torch.pow().sum", "torch.pow"], "function", ["None"], ["def", "l2norm", "(", "X", ")", ":", "\n", "    ", "norm", "=", "torch", ".", "pow", "(", "X", ",", "2", ")", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", ".", "sqrt", "(", ")", "\n", "X", "=", "torch", ".", "div", "(", "X", ",", "norm", ")", "\n", "return", "X", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.siamese.Siamese.__init__": [[11, 16], ["fewshot_re_kit.framework.FewShotREModel.__init__", "torch.nn.LayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "sentence_encoder", ",", "device", ",", "hidden_size", "=", "230", ",", "dropout", "=", "0", ")", ":", "\n", "        ", "fewshot_re_kit", ".", "framework", ".", "FewShotREModel", ".", "__init__", "(", "self", ",", "sentence_encoder", ",", "device", ")", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "normalize", "=", "nn", ".", "LayerNorm", "(", "normalized_shape", "=", "hidden_size", ")", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.siamese.Siamese.forward": [[17, 56], ["siamese.Siamese.sentence_encoder", "siamese.Siamese.sentence_encoder", "siamese.Siamese.normalize", "siamese.Siamese.normalize", "siamese.Siamese.drop", "siamese.Siamese.drop", "support.unsqueeze.unsqueeze.view", "query.unsqueeze.unsqueeze.view", "support.unsqueeze.unsqueeze.size", "support.unsqueeze.unsqueeze.unsqueeze", "query.unsqueeze.unsqueeze.unsqueeze", "z.view.view.view", "torch.cat.min", "torch.cat", "torch.max", "z.view.view.max", "torch.cat.view", "minn.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "support", ",", "query", ",", "N", ",", "K", ",", "total_Q", ")", ":", "\n", "        ", "'''\n        support: Inputs of the support set.\n        query: Inputs of the query set.\n        N: Num of classes\n        K: Num of instances for each class in the support set\n        Q: Num of instances in the query set\n        '''", "\n", "\n", "support", "=", "self", ".", "sentence_encoder", "(", "support", ")", "# (B * N * K, D), where D is the hidden size", "\n", "query", "=", "self", ".", "sentence_encoder", "(", "query", ")", "# (B * total_Q, D)", "\n", "\n", "# Layer Norm", "\n", "support", "=", "self", ".", "normalize", "(", "support", ")", "\n", "query", "=", "self", ".", "normalize", "(", "query", ")", "\n", "\n", "# Dropout ?", "\n", "support", "=", "self", ".", "drop", "(", "support", ")", "\n", "query", "=", "self", ".", "drop", "(", "query", ")", "\n", "\n", "support", "=", "support", ".", "view", "(", "-", "1", ",", "N", "*", "K", ",", "self", ".", "hidden_size", ")", "# (B, N * K, D)", "\n", "query", "=", "query", ".", "view", "(", "-", "1", ",", "total_Q", ",", "self", ".", "hidden_size", ")", "# (B, total_Q, D)", "\n", "B", "=", "support", ".", "size", "(", "0", ")", "# Batch size", "\n", "support", "=", "support", ".", "unsqueeze", "(", "1", ")", "# (B, 1, N * K, D)", "\n", "query", "=", "query", ".", "unsqueeze", "(", "2", ")", "# (B, total_Q, 1, D)", "\n", "\n", "#  Dot production", "\n", "z", "=", "(", "support", "*", "query", ")", ".", "sum", "(", "-", "1", ")", "# (B, total_Q, N * K)", "\n", "z", "=", "z", ".", "view", "(", "-", "1", ",", "total_Q", ",", "N", ",", "K", ")", "# (B, total_Q, N, K)", "\n", "\n", "# Max combination", "\n", "logits", "=", "z", ".", "max", "(", "-", "1", ")", "[", "0", "]", "# (B, total_Q, N)", "\n", "\n", "# NA", "\n", "minn", ",", "_", "=", "logits", ".", "min", "(", "-", "1", ")", "\n", "logits", "=", "torch", ".", "cat", "(", "[", "logits", ",", "minn", ".", "unsqueeze", "(", "2", ")", "-", "1", "]", ",", "2", ")", "# (B, total_Q, N + 1)", "\n", "\n", "_", ",", "pred", "=", "torch", ".", "max", "(", "logits", ".", "view", "(", "-", "1", ",", "N", "+", "1", ")", ",", "1", ")", "\n", "return", "logits", ",", "pred", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.proto.Proto.__init__": [[13, 18], ["fewshot_re_kit.framework.FewShotREModel.__init__", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "sentence_encoder", ",", "dot", "=", "False", ")", ":", "\n", "        ", "fewshot_re_kit", ".", "framework", ".", "FewShotREModel", ".", "__init__", "(", "self", ",", "sentence_encoder", ")", "\n", "# self.fc = nn.Linear(hidden_size, hidden_size)", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", ")", "\n", "self", ".", "dot", "=", "dot", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.proto.Proto.__dist__": [[19, 24], ["torch.pow().sum", "torch.pow"], "methods", ["None"], ["", "def", "__dist__", "(", "self", ",", "x", ",", "y", ",", "dim", ")", ":", "\n", "        ", "if", "self", ".", "dot", ":", "\n", "            ", "return", "(", "x", "*", "y", ")", ".", "sum", "(", "dim", ")", "\n", "", "else", ":", "\n", "            ", "return", "-", "(", "torch", ".", "pow", "(", "x", "-", "y", ",", "2", ")", ")", ".", "sum", "(", "dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.proto.Proto.__batch_dist__": [[25, 27], ["proto.Proto.__dist__", "S.unsqueeze", "Q.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.proto.Proto.__dist__"], ["", "", "def", "__batch_dist__", "(", "self", ",", "S", ",", "Q", ")", ":", "\n", "        ", "return", "self", ".", "__dist__", "(", "S", ".", "unsqueeze", "(", "1", ")", ",", "Q", ".", "unsqueeze", "(", "2", ")", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.proto.Proto.forward": [[28, 54], ["proto.Proto.sentence_encoder", "proto.Proto.sentence_encoder", "proto.Proto.size", "proto.Proto.drop", "proto.Proto.drop", "torch.mean.view", "query.view.view.view", "torch.mean.size", "torch.mean", "proto.Proto.__batch_dist__", "torch.cat.min", "torch.cat", "torch.max", "torch.cat.view", "minn.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.proto.Proto.__batch_dist__"], ["", "def", "forward", "(", "self", ",", "support", ",", "query", ",", "N", ",", "K", ",", "total_Q", ")", ":", "\n", "        ", "'''\n        support: Inputs of the support set.\n        query: Inputs of the query set.\n        N: Num of classes\n        K: Num of instances for each class in the support set\n        Q: Num of instances in the query set\n        '''", "\n", "support_emb", "=", "self", ".", "sentence_encoder", "(", "support", ")", "# (B * N * K, D), where D is the hidden size", "\n", "query_emb", "=", "self", ".", "sentence_encoder", "(", "query", ")", "# (B * total_Q, D)", "\n", "hidden_size", "=", "support_emb", ".", "size", "(", "-", "1", ")", "\n", "support", "=", "self", ".", "drop", "(", "support_emb", ")", "\n", "query", "=", "self", ".", "drop", "(", "query_emb", ")", "\n", "support", "=", "support", ".", "view", "(", "-", "1", ",", "N", ",", "K", ",", "hidden_size", ")", "# (B, N, K, D)", "\n", "query", "=", "query", ".", "view", "(", "-", "1", ",", "total_Q", ",", "hidden_size", ")", "# (B, total_Q, D)", "\n", "\n", "B", "=", "support", ".", "size", "(", "0", ")", "# Batch size", "\n", "\n", "# Prototypical Networks ", "\n", "# Ignore NA policy", "\n", "support", "=", "torch", ".", "mean", "(", "support", ",", "2", ")", "# Calculate prototype for each class", "\n", "logits", "=", "self", ".", "__batch_dist__", "(", "support", ",", "query", ")", "# (B, total_Q, N)", "\n", "minn", ",", "_", "=", "logits", ".", "min", "(", "-", "1", ")", "\n", "logits", "=", "torch", ".", "cat", "(", "[", "logits", ",", "minn", ".", "unsqueeze", "(", "2", ")", "-", "1", "]", ",", "2", ")", "# (B, total_Q, N + 1)", "\n", "_", ",", "pred", "=", "torch", ".", "max", "(", "logits", ".", "view", "(", "-", "1", ",", "N", "+", "1", ")", ",", "1", ")", "\n", "return", "logits", ",", "pred", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.snail.CausalConv1d.__init__": [[12, 21], ["torch.nn.Module.__init__", "torch.nn.Conv1d"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "2", ",", "dilation", "=", "2", ")", ":", "\n", "        ", "super", "(", "CausalConv1d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "padding", "=", "dilation", "\n", "self", ".", "causal_conv", "=", "nn", ".", "Conv1d", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", ",", "\n", "padding", "=", "self", ".", "padding", ",", "\n", "dilation", "=", "dilation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.snail.CausalConv1d.forward": [[22, 24], ["snail.CausalConv1d.causal_conv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "minibatch", ")", ":", "\n", "        ", "return", "self", ".", "causal_conv", "(", "minibatch", ")", "[", ":", ",", ":", ",", ":", "-", "self", ".", "padding", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.snail.DenseBlock.__init__": [[28, 38], ["torch.nn.Module.__init__", "snail.CausalConv1d", "snail.CausalConv1d"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "filters", ",", "dilation", "=", "2", ")", ":", "\n", "        ", "super", "(", "DenseBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "causal_conv1", "=", "CausalConv1d", "(", "\n", "in_channels", ",", "\n", "filters", ",", "\n", "dilation", "=", "dilation", ")", "\n", "self", ".", "causal_conv2", "=", "CausalConv1d", "(", "\n", "in_channels", ",", "\n", "filters", ",", "\n", "dilation", "=", "dilation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.snail.DenseBlock.forward": [[39, 44], ["torch.nn.functional.tanh", "torch.nn.functional.sigmoid", "torch.cat", "snail.DenseBlock.causal_conv1", "snail.DenseBlock.causal_conv2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "minibatch", ")", ":", "\n", "        ", "tanh", "=", "F", ".", "tanh", "(", "self", ".", "causal_conv1", "(", "minibatch", ")", ")", "\n", "sig", "=", "F", ".", "sigmoid", "(", "self", ".", "causal_conv2", "(", "minibatch", ")", ")", "\n", "out", "=", "torch", ".", "cat", "(", "[", "minibatch", ",", "tanh", "*", "sig", "]", ",", "dim", "=", "1", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.snail.TCBlock.__init__": [[47, 58], ["torch.nn.Module.__init__", "numpy.ceil().astype", "range", "torch.nn.Sequential", "snail.DenseBlock", "blocks.append", "numpy.ceil", "numpy.log2"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "filters", ",", "seq_len", ")", ":", "\n", "        ", "super", "(", "TCBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "layer_count", "=", "np", ".", "ceil", "(", "np", ".", "log2", "(", "seq_len", ")", ")", ".", "astype", "(", "np", ".", "int32", ")", "\n", "blocks", "=", "[", "]", "\n", "channel_count", "=", "in_channels", "\n", "for", "layer", "in", "range", "(", "layer_count", ")", ":", "\n", "            ", "block", "=", "DenseBlock", "(", "channel_count", ",", "filters", ",", "dilation", "=", "2", "**", "layer", ")", "\n", "blocks", ".", "append", "(", "block", ")", "\n", "channel_count", "+=", "filters", "\n", "", "self", ".", "tcblock", "=", "nn", ".", "Sequential", "(", "*", "blocks", ")", "\n", "self", ".", "_dim", "=", "channel_count", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.snail.TCBlock.forward": [[59, 61], ["snail.TCBlock.tcblock"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "minibatch", ")", ":", "\n", "        ", "return", "self", ".", "tcblock", "(", "minibatch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.snail.TCBlock.dim": [[62, 65], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "dim", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.snail.AttentionBlock.__init__": [[67, 78], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "numpy.sqrt", "numpy.tril().astype", "torch.nn.Parameter", "torch.from_numpy", "numpy.tril", "numpy.ones"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dims", ",", "k_size", ",", "v_size", ",", "seq_len", ")", ":", "\n", "\n", "        ", "super", "(", "AttentionBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "key_layer", "=", "nn", ".", "Linear", "(", "dims", ",", "k_size", ")", "\n", "self", ".", "query_layer", "=", "nn", ".", "Linear", "(", "dims", ",", "k_size", ")", "\n", "self", ".", "value_layer", "=", "nn", ".", "Linear", "(", "dims", ",", "v_size", ")", "\n", "self", ".", "sqrt_k", "=", "np", ".", "sqrt", "(", "k_size", ")", "\n", "mask", "=", "np", ".", "tril", "(", "np", ".", "ones", "(", "(", "seq_len", ",", "seq_len", ")", ")", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "self", ".", "mask", "=", "nn", ".", "Parameter", "(", "torch", ".", "from_numpy", "(", "mask", ")", ",", "requires_grad", "=", "False", ")", "\n", "self", ".", "minus", "=", "-", "100.", "\n", "self", ".", "_dim", "=", "dims", "+", "v_size", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.snail.AttentionBlock.forward": [[79, 89], ["snail.AttentionBlock.key_layer", "snail.AttentionBlock.value_layer", "torch.nn.functional.softmax", "torch.bmm", "torch.cat", "torch.div", "torch.bmm", "snail.AttentionBlock.transpose"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "minibatch", ",", "current_seq_len", ")", ":", "\n", "        ", "keys", "=", "self", ".", "key_layer", "(", "minibatch", ")", "\n", "#queries = self.query_layer(minibatch)", "\n", "queries", "=", "keys", "\n", "values", "=", "self", ".", "value_layer", "(", "minibatch", ")", "\n", "current_mask", "=", "self", ".", "mask", "[", ":", "current_seq_len", ",", ":", "current_seq_len", "]", "\n", "logits", "=", "current_mask", "*", "torch", ".", "div", "(", "torch", ".", "bmm", "(", "queries", ",", "keys", ".", "transpose", "(", "2", ",", "1", ")", ")", ",", "self", ".", "sqrt_k", ")", "+", "self", ".", "minus", "*", "(", "1.", "-", "current_mask", ")", "\n", "probs", "=", "F", ".", "softmax", "(", "logits", ",", "2", ")", "\n", "read", "=", "torch", ".", "bmm", "(", "probs", ",", "values", ")", "\n", "return", "torch", ".", "cat", "(", "[", "minibatch", ",", "read", "]", ",", "dim", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.snail.AttentionBlock.dim": [[90, 93], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "dim", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.snail.SNAIL.__init__": [[96, 113], ["fewshot_re_kit.framework.FewShotREModel.__init__", "torch.nn.Dropout", "snail.AttentionBlock", "snail.TCBlock", "snail.AttentionBlock", "snail.TCBlock", "snail.AttentionBlock", "torch.nn.Linear", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "sentence_encoder", ",", "N", ",", "K", ",", "hidden_size", "=", "230", ")", ":", "\n", "        ", "'''\n        N: num of classes\n        K: num of instances for each class in the support set\n        '''", "\n", "fewshot_re_kit", ".", "framework", ".", "FewShotREModel", ".", "__init__", "(", "self", ",", "sentence_encoder", ")", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", ")", "\n", "self", ".", "seq_len", "=", "N", "*", "K", "+", "1", "\n", "self", ".", "att0", "=", "AttentionBlock", "(", "hidden_size", "+", "N", ",", "64", ",", "32", ",", "self", ".", "seq_len", ")", "\n", "self", ".", "tc1", "=", "TCBlock", "(", "self", ".", "att0", ".", "dim", ",", "128", ",", "self", ".", "seq_len", ")", "\n", "self", ".", "att1", "=", "AttentionBlock", "(", "self", ".", "tc1", ".", "dim", ",", "256", ",", "128", ",", "self", ".", "seq_len", ")", "\n", "self", ".", "tc2", "=", "TCBlock", "(", "self", ".", "att1", ".", "dim", ",", "128", ",", "self", ".", "seq_len", ")", "\n", "self", ".", "att2", "=", "AttentionBlock", "(", "self", ".", "tc2", ".", "dim", ",", "512", ",", "256", ",", "self", ".", "seq_len", ")", "\n", "self", ".", "disc", "=", "nn", ".", "Linear", "(", "self", ".", "att2", ".", "dim", ",", "N", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn1", "=", "nn", ".", "BatchNorm1d", "(", "self", ".", "tc1", ".", "dim", ")", "\n", "self", ".", "bn2", "=", "nn", ".", "BatchNorm1d", "(", "self", ".", "tc2", ".", "dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.snail.SNAIL.forward": [[114, 144], ["snail.SNAIL.sentence_encoder", "snail.SNAIL.sentence_encoder", "support.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.view", "query.view.view.view", "support.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.size", "support.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "query.view.view.view", "torch.cat", "torch.zeros().float().cuda", "torch.cat", "range", "snail.SNAIL.att0().transpose", "snail.SNAIL.bn1().transpose", "snail.SNAIL.att1().transpose", "snail.SNAIL.bn2().transpose", "snail.SNAIL.att2", "snail.SNAIL.disc", "torch.max", "range", "support.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "torch.zeros().float", "snail.SNAIL.att0", "snail.SNAIL.bn1", "snail.SNAIL.att1", "snail.SNAIL.bn2", "snail.SNAIL.tc1", "snail.SNAIL.tc2", "support.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand", "torch.zeros", "support.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "support", ",", "query", ",", "N", ",", "K", ",", "NQ", ")", ":", "\n", "        ", "support", "=", "self", ".", "sentence_encoder", "(", "support", ")", "# (B * N * K, D), where D is the hidden size", "\n", "query", "=", "self", ".", "sentence_encoder", "(", "query", ")", "# (B * N * Q, D)", "\n", "# support = self.drop(support)", "\n", "# query = self.drop(query)", "\n", "support", "=", "support", ".", "view", "(", "-", "1", ",", "N", ",", "K", ",", "self", ".", "hidden_size", ")", "# (B, N, K, D)", "\n", "query", "=", "query", ".", "view", "(", "-", "1", ",", "NQ", ",", "self", ".", "hidden_size", ")", "# (B, N * Q, D)", "\n", "B", "=", "support", ".", "size", "(", "0", ")", "# Batch size", "\n", "\n", "support", "=", "support", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "-", "1", ",", "NQ", ",", "-", "1", ",", "-", "1", ",", "-", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "N", "*", "K", ",", "self", ".", "hidden_size", ")", "# (B * NQ, N * K, D)", "\n", "query", "=", "query", ".", "view", "(", "-", "1", ",", "1", ",", "self", ".", "hidden_size", ")", "# (B * NQ, 1, D)", "\n", "minibatch", "=", "torch", ".", "cat", "(", "[", "support", ",", "query", "]", ",", "1", ")", "\n", "labels", "=", "torch", ".", "zeros", "(", "(", "B", "*", "NQ", ",", "N", "*", "K", "+", "1", ",", "N", ")", ")", ".", "float", "(", ")", ".", "cuda", "(", ")", "\n", "minibatch", "=", "torch", ".", "cat", "(", "(", "minibatch", ",", "labels", ")", ",", "2", ")", "\n", "for", "i", "in", "range", "(", "N", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "K", ")", ":", "\n", "                ", "minibatch", "[", ":", ",", "i", "*", "K", "+", "j", ",", "i", "]", "=", "1", "\n", "\n", "", "", "x", "=", "self", ".", "att0", "(", "minibatch", ",", "self", ".", "seq_len", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "#x = self.bn1(x).transpose(1, 2)", "\n", "x", "=", "self", ".", "bn1", "(", "self", ".", "tc1", "(", "x", ")", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "#x = self.tc1(x).transpose(1, 2)", "\n", "x", "=", "self", ".", "att1", "(", "x", ",", "self", ".", "seq_len", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "x", "=", "self", ".", "bn2", "(", "self", ".", "tc2", "(", "x", ")", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "#x = self.tc2(x).transpose(1, 2)", "\n", "x", "=", "self", ".", "att2", "(", "x", ",", "self", ".", "seq_len", ")", "\n", "x", "=", "x", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "logits", "=", "self", ".", "disc", "(", "x", ")", "\n", "_", ",", "pred", "=", "torch", ".", "max", "(", "logits", ",", "-", "1", ")", "\n", "return", "logits", ",", "pred", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.pair.Pair.__init__": [[11, 16], ["fewshot_re_kit.framework.FewShotREModel.__init__", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "sentence_encoder", ",", "hidden_size", "=", "230", ")", ":", "\n", "        ", "fewshot_re_kit", ".", "framework", ".", "FewShotREModel", ".", "__init__", "(", "self", ",", "sentence_encoder", ")", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "# self.fc = nn.Linear(hidden_size, hidden_size)", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.pair.Pair.forward": [[17, 35], ["pair.Pair.sentence_encoder", "torch.cat.view", "torch.cat.mean", "logits[].min", "torch.cat", "torch.max", "torch.cat.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "batch", ",", "N", ",", "K", ",", "total_Q", ",", "device", ")", ":", "\n", "        ", "'''\n        support: Inputs of the support set.\n        query: Inputs of the query set.\n        N: Num of classes\n        K: Num of instances for each class in the support set\n        Q: Num of instances in the query set\n        '''", "\n", "# logits = self.sentence_encoder(batch,device)", "\n", "logits", "=", "self", ".", "sentence_encoder", "(", "batch", ")", "\n", "\n", "logits", "=", "logits", ".", "view", "(", "-", "1", ",", "total_Q", ",", "N", ",", "K", ",", "2", ")", "\n", "logits", "=", "logits", ".", "mean", "(", "3", ")", "# (-1, total_Q, N, 2)", "\n", "logits_na", ",", "_", "=", "logits", "[", ":", ",", ":", ",", ":", ",", "0", "]", ".", "min", "(", "2", ",", "keepdim", "=", "True", ")", "# (-1, totalQ, 1)", "\n", "logits", "=", "logits", "[", ":", ",", ":", ",", ":", ",", "1", "]", "# (-1, total_Q, N)", "\n", "logits", "=", "torch", ".", "cat", "(", "[", "logits", ",", "logits_na", "]", ",", "2", ")", "# (B, total_Q, N + 1)", "\n", "_", ",", "pred", "=", "torch", ".", "max", "(", "logits", ".", "view", "(", "-", "1", ",", "N", "+", "1", ")", ",", "1", ")", "\n", "return", "logits", ",", "pred", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.gnn.GNN.__init__": [[12, 20], ["fewshot_re_kit.framework.FewShotREModel.__init__", "gnn_iclr.GNN_nl"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "sentence_encoder", ",", "N", ",", "hidden_size", "=", "230", ")", ":", "\n", "        ", "'''\n        N: Num of classes\n        '''", "\n", "fewshot_re_kit", ".", "framework", ".", "FewShotREModel", ".", "__init__", "(", "self", ",", "sentence_encoder", ")", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "node_dim", "=", "hidden_size", "+", "N", "\n", "self", ".", "gnn_obj", "=", "gnn_iclr", ".", "GNN_nl", "(", "N", ",", "self", ".", "node_dim", ",", "nf", "=", "96", ",", "J", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.models.gnn.GNN.forward": [[21, 49], ["gnn.GNN.sentence_encoder", "gnn.GNN.sentence_encoder", "support.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.view", "query.view.view.view", "support.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.size", "support.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "query.view.view.view", "torch.autograd.Variable().cuda", "range", "torch.cat", "gnn.GNN.gnn_obj", "torch.max", "range", "support.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "torch.autograd.Variable", "range", "torch.cat", "torch.zeros", "support.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand", "support.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "support", ",", "query", ",", "N", ",", "K", ",", "NQ", ")", ":", "\n", "        ", "'''\n        support: Inputs of the support set.\n        query: Inputs of the query set.\n        N: Num of classes\n        K: Num of instances for each class in the support set\n        Q: Num of instances for each class in the query set\n        '''", "\n", "support", "=", "self", ".", "sentence_encoder", "(", "support", ")", "\n", "query", "=", "self", ".", "sentence_encoder", "(", "query", ")", "\n", "support", "=", "support", ".", "view", "(", "-", "1", ",", "N", ",", "K", ",", "self", ".", "hidden_size", ")", "\n", "query", "=", "query", ".", "view", "(", "-", "1", ",", "NQ", ",", "self", ".", "hidden_size", ")", "\n", "\n", "B", "=", "support", ".", "size", "(", "0", ")", "\n", "D", "=", "self", ".", "hidden_size", "\n", "\n", "support", "=", "support", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "-", "1", ",", "NQ", ",", "-", "1", ",", "-", "1", ",", "-", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "N", "*", "K", ",", "D", ")", "# (B * NQ, N * K, D)", "\n", "query", "=", "query", ".", "view", "(", "-", "1", ",", "1", ",", "D", ")", "# (B * NQ, 1, D)", "\n", "labels", "=", "Variable", "(", "torch", ".", "zeros", "(", "(", "B", "*", "NQ", ",", "1", "+", "N", "*", "K", ",", "N", ")", ",", "dtype", "=", "torch", ".", "float", ")", ")", ".", "cuda", "(", ")", "\n", "for", "b", "in", "range", "(", "B", "*", "NQ", ")", ":", "\n", "            ", "for", "i", "in", "range", "(", "N", ")", ":", "\n", "                ", "for", "k", "in", "range", "(", "K", ")", ":", "\n", "                    ", "labels", "[", "b", "]", "[", "1", "+", "i", "*", "K", "+", "k", "]", "[", "i", "]", "=", "1", "\n", "", "", "", "nodes", "=", "torch", ".", "cat", "(", "[", "torch", ".", "cat", "(", "[", "query", ",", "support", "]", ",", "1", ")", ",", "labels", "]", ",", "-", "1", ")", "# (B * NQ, 1 + N * K, D + N)", "\n", "\n", "logits", "=", "self", ".", "gnn_obj", "(", "nodes", ")", "# (B * NQ, N)", "\n", "_", ",", "pred", "=", "torch", ".", "max", "(", "logits", ",", "1", ")", "\n", "return", "logits", ",", "pred", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.CNNSentenceEncoder.__init__": [[16, 26], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "network.embedding.Embedding", "network.encoder.Encoder"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "word_vec_mat", ",", "word2id", ",", "max_length", ",", "word_embedding_dim", "=", "50", ",", "\n", "pos_embedding_dim", "=", "5", ",", "hidden_size", "=", "230", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "embedding", "=", "network", ".", "embedding", ".", "Embedding", "(", "word_vec_mat", ",", "max_length", ",", "\n", "word_embedding_dim", ",", "pos_embedding_dim", ")", "\n", "self", ".", "encoder", "=", "network", ".", "encoder", ".", "Encoder", "(", "max_length", ",", "word_embedding_dim", ",", "\n", "pos_embedding_dim", ",", "hidden_size", ")", "\n", "self", ".", "word2id", "=", "word2id", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.CNNSentenceEncoder.forward": [[27, 31], ["sentence_encoder.CNNSentenceEncoder.embedding", "sentence_encoder.CNNSentenceEncoder.encoder"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "x", "=", "self", ".", "embedding", "(", "inputs", ")", "\n", "x", "=", "self", ".", "encoder", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.CNNSentenceEncoder.tokenize": [[32, 61], ["numpy.zeros", "numpy.zeros", "min", "min", "range", "numpy.zeros", "token.lower.lower.lower", "len", "indexed_tokens.append", "indexed_tokens.append", "indexed_tokens.append", "len"], "methods", ["None"], ["", "def", "tokenize", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ")", ":", "\n", "# token -> index", "\n", "        ", "indexed_tokens", "=", "[", "]", "\n", "for", "token", "in", "raw_tokens", ":", "\n", "            ", "token", "=", "token", ".", "lower", "(", ")", "\n", "if", "token", "in", "self", ".", "word2id", ":", "\n", "                ", "indexed_tokens", ".", "append", "(", "self", ".", "word2id", "[", "token", "]", ")", "\n", "", "else", ":", "\n", "                ", "indexed_tokens", ".", "append", "(", "self", ".", "word2id", "[", "'[UNK]'", "]", ")", "\n", "\n", "# padding", "\n", "", "", "while", "len", "(", "indexed_tokens", ")", "<", "self", ".", "max_length", ":", "\n", "            ", "indexed_tokens", ".", "append", "(", "self", ".", "word2id", "[", "'[PAD]'", "]", ")", "\n", "", "indexed_tokens", "=", "indexed_tokens", "[", ":", "self", ".", "max_length", "]", "\n", "\n", "# pos", "\n", "pos1", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "pos2", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "pos1_in_index", "=", "min", "(", "self", ".", "max_length", ",", "pos_head", "[", "0", "]", ")", "\n", "pos2_in_index", "=", "min", "(", "self", ".", "max_length", ",", "pos_tail", "[", "0", "]", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "max_length", ")", ":", "\n", "            ", "pos1", "[", "i", "]", "=", "i", "-", "pos1_in_index", "+", "self", ".", "max_length", "\n", "pos2", "[", "i", "]", "=", "i", "-", "pos2_in_index", "+", "self", ".", "max_length", "\n", "\n", "# mask", "\n", "", "mask", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "mask", "[", ":", "len", "(", "indexed_tokens", ")", "]", "=", "1", "\n", "\n", "return", "indexed_tokens", ",", "pos1", ",", "pos2", ",", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.BERTSentenceEncoder.__init__": [[65, 72], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "transformers.BertModel.from_pretrained", "transformers.BertTokenizer.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "pretrain_path", ",", "max_length", ",", "cat_entity_rep", "=", "False", ",", "mask_entity", "=", "False", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "self", ".", "bert", "=", "BertModel", ".", "from_pretrained", "(", "pretrain_path", ")", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", "\n", "self", ".", "cat_entity_rep", "=", "cat_entity_rep", "\n", "self", ".", "mask_entity", "=", "mask_entity", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.BERTSentenceEncoder.forward": [[73, 93], ["sentence_encoder.BERTSentenceEncoder.bert", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "sentence_encoder.BERTSentenceEncoder.bert", "inputs[].size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "if", "not", "self", ".", "cat_entity_rep", ":", "\n", "# _, x = self.bert(inputs['word'], attention_mask=inputs['mask'])", "\n", "            ", "x", "=", "self", ".", "bert", "(", "inputs", "[", "'word'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "[", "1", "]", "\n", "\n", "return", "x", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "self", ".", "bert", "(", "inputs", "[", "'word'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "\n", "tensor_range", "=", "torch", ".", "arange", "(", "inputs", "[", "'word'", "]", ".", "size", "(", ")", "[", "0", "]", ")", "\n", "# print('*' * 100)", "\n", "# print('inputs[word].size()', inputs['word'].size())", "\n", "# print('inputs[word].size()[0]', inputs['word'].size()[0])", "\n", "# print('tensor_range', tensor_range)", "\n", "# print('inputs[\"pos1\"]', inputs[\"pos1\"])", "\n", "# print('*' * 100)", "\n", "\n", "h_state", "=", "outputs", "[", "0", "]", "[", "tensor_range", ",", "inputs", "[", "\"pos1\"", "]", "]", "\n", "t_state", "=", "outputs", "[", "0", "]", "[", "tensor_range", ",", "inputs", "[", "\"pos2\"", "]", "]", "\n", "state", "=", "torch", ".", "cat", "(", "(", "h_state", ",", "t_state", ")", ",", "-", "1", ")", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.BERTSentenceEncoder.tokenize": [[94, 140], ["sentence_encoder.BERTSentenceEncoder.tokenizer.convert_tokens_to_ids", "numpy.zeros", "numpy.zeros", "range", "numpy.zeros", "min", "min", "token.lower.lower.lower", "len", "sentence_encoder.BERTSentenceEncoder.append", "tokens.append", "len", "tokens.append", "len", "sentence_encoder.BERTSentenceEncoder.tokenizer.tokenize", "tokens.append", "tokens.append", "len"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "", "def", "tokenize", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ")", ":", "\n", "# token -> index", "\n", "        ", "tokens", "=", "[", "'[CLS]'", "]", "\n", "cur_pos", "=", "0", "\n", "pos1_in_index", "=", "1", "\n", "pos2_in_index", "=", "1", "\n", "for", "token", "in", "raw_tokens", ":", "\n", "            ", "token", "=", "token", ".", "lower", "(", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused0]'", ")", "\n", "pos1_in_index", "=", "len", "(", "tokens", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused1]'", ")", "\n", "pos2_in_index", "=", "len", "(", "tokens", ")", "\n", "", "if", "self", ".", "mask_entity", "and", "(", "(", "pos_head", "[", "0", "]", "<=", "cur_pos", "and", "cur_pos", "<=", "pos_head", "[", "-", "1", "]", ")", "or", "(", "\n", "pos_tail", "[", "0", "]", "<=", "cur_pos", "and", "cur_pos", "<=", "pos_tail", "[", "-", "1", "]", ")", ")", ":", "\n", "                ", "tokens", "+=", "[", "'[unused4]'", "]", "\n", "", "else", ":", "\n", "                ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "", "if", "cur_pos", "==", "pos_head", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused2]'", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused3]'", ")", "\n", "", "cur_pos", "+=", "1", "\n", "", "indexed_tokens", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# padding", "\n", "while", "len", "(", "indexed_tokens", ")", "<", "self", ".", "max_length", ":", "\n", "            ", "indexed_tokens", ".", "append", "(", "0", ")", "\n", "", "indexed_tokens", "=", "indexed_tokens", "[", ":", "self", ".", "max_length", "]", "\n", "\n", "# pos", "\n", "pos1", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "pos2", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "max_length", ")", ":", "\n", "            ", "pos1", "[", "i", "]", "=", "i", "-", "pos1_in_index", "+", "self", ".", "max_length", "\n", "pos2", "[", "i", "]", "=", "i", "-", "pos2_in_index", "+", "self", ".", "max_length", "\n", "\n", "# mask", "\n", "", "mask", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "mask", "[", ":", "len", "(", "tokens", ")", "]", "=", "1", "\n", "\n", "pos1_in_index", "=", "min", "(", "self", ".", "max_length", ",", "pos1_in_index", ")", "\n", "pos2_in_index", "=", "min", "(", "self", ".", "max_length", ",", "pos2_in_index", ")", "\n", "\n", "return", "indexed_tokens", ",", "pos1_in_index", "-", "1", ",", "pos2_in_index", "-", "1", ",", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.BERTConceptSentenceEncoder.__init__": [[144, 152], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "transformers.BertModel.from_pretrained", "transformers.BertTokenizer.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "pretrain_path", ",", "max_length", ",", "sentenceORword", ",", "cat_entity_rep", "=", "False", ",", "mask_entity", "=", "False", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "self", ".", "bert", "=", "BertModel", ".", "from_pretrained", "(", "pretrain_path", ")", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", "\n", "self", ".", "cat_entity_rep", "=", "cat_entity_rep", "\n", "self", ".", "mask_entity", "=", "mask_entity", "\n", "self", ".", "sentenceORword", "=", "sentenceORword", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.BERTConceptSentenceEncoder.forward": [[153, 171], ["sentence_encoder.BERTConceptSentenceEncoder.bert", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "sentence_encoder.BERTConceptSentenceEncoder.bert", "inputs[].size", "sentence_encoder.BERTConceptSentenceEncoder.bert", "sentence_encoder.BERTConceptSentenceEncoder.bert"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "if", "not", "self", ".", "cat_entity_rep", ":", "\n", "            ", "if", "self", ".", "sentenceORword", "==", "'sentence'", ":", "\n", "                ", "_", ",", "x", "=", "self", ".", "bert", "(", "inputs", "[", "'word'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "\n", "return", "x", "\n", "", "elif", "self", ".", "sentenceORword", "==", "'word'", ":", "\n", "\n", "                ", "wordEmbedding", "=", "self", ".", "bert", "(", "inputs", "[", "'word'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "[", "0", "]", "\n", "sentenceEmbedding", "=", "self", ".", "bert", "(", "inputs", "[", "'word'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "[", "1", "]", "\n", "\n", "return", "wordEmbedding", ",", "sentenceEmbedding", "\n", "", "", "else", ":", "\n", "            ", "outputs", "=", "self", ".", "bert", "(", "inputs", "[", "'word'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "\n", "tensor_range", "=", "torch", ".", "arange", "(", "inputs", "[", "'word'", "]", ".", "size", "(", ")", "[", "0", "]", ")", "\n", "h_state", "=", "outputs", "[", "0", "]", "[", "tensor_range", ",", "inputs", "[", "\"pos1\"", "]", "]", "\n", "t_state", "=", "outputs", "[", "0", "]", "[", "tensor_range", ",", "inputs", "[", "\"pos2\"", "]", "]", "\n", "state", "=", "torch", ".", "cat", "(", "(", "h_state", ",", "t_state", ")", ",", "-", "1", ")", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.BERTConceptSentenceEncoder.tokenize": [[172, 218], ["sentence_encoder.BERTConceptSentenceEncoder.tokenizer.convert_tokens_to_ids", "numpy.zeros", "numpy.zeros", "range", "numpy.zeros", "min", "min", "token.lower.lower.lower", "len", "sentence_encoder.BERTConceptSentenceEncoder.append", "tokens.append", "len", "tokens.append", "len", "sentence_encoder.BERTConceptSentenceEncoder.tokenizer.tokenize", "tokens.append", "tokens.append", "len"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "", "def", "tokenize", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ")", ":", "\n", "# token -> index", "\n", "        ", "tokens", "=", "[", "'[CLS]'", "]", "\n", "cur_pos", "=", "0", "\n", "pos1_in_index", "=", "1", "\n", "pos2_in_index", "=", "1", "\n", "for", "token", "in", "raw_tokens", ":", "\n", "            ", "token", "=", "token", ".", "lower", "(", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused0]'", ")", "\n", "pos1_in_index", "=", "len", "(", "tokens", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused1]'", ")", "\n", "pos2_in_index", "=", "len", "(", "tokens", ")", "\n", "", "if", "self", ".", "mask_entity", "and", "(", "(", "pos_head", "[", "0", "]", "<=", "cur_pos", "and", "cur_pos", "<=", "pos_head", "[", "-", "1", "]", ")", "or", "(", "\n", "pos_tail", "[", "0", "]", "<=", "cur_pos", "and", "cur_pos", "<=", "pos_tail", "[", "-", "1", "]", ")", ")", ":", "\n", "                ", "tokens", "+=", "[", "'[unused4]'", "]", "\n", "", "else", ":", "\n", "                ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "", "if", "cur_pos", "==", "pos_head", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused2]'", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused3]'", ")", "\n", "", "cur_pos", "+=", "1", "\n", "", "indexed_tokens", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# padding", "\n", "while", "len", "(", "indexed_tokens", ")", "<", "self", ".", "max_length", ":", "\n", "            ", "indexed_tokens", ".", "append", "(", "0", ")", "\n", "", "indexed_tokens", "=", "indexed_tokens", "[", ":", "self", ".", "max_length", "]", "\n", "\n", "# pos", "\n", "pos1", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "pos2", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "max_length", ")", ":", "\n", "            ", "pos1", "[", "i", "]", "=", "i", "-", "pos1_in_index", "+", "self", ".", "max_length", "\n", "pos2", "[", "i", "]", "=", "i", "-", "pos2_in_index", "+", "self", ".", "max_length", "\n", "\n", "# mask", "\n", "", "mask", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "mask", "[", ":", "len", "(", "tokens", ")", "]", "=", "1", "\n", "\n", "pos1_in_index", "=", "min", "(", "self", ".", "max_length", ",", "pos1_in_index", ")", "\n", "pos2_in_index", "=", "min", "(", "self", ".", "max_length", ",", "pos2_in_index", ")", "\n", "\n", "return", "indexed_tokens", ",", "pos1_in_index", "-", "1", ",", "pos2_in_index", "-", "1", ",", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.BERTConceptSentenceEncoder.tokenize_concept": [[219, 299], ["h.lower.lower.lower", "fewshot_re_kit.conceptgraph_utils.instance2conept", "h2concept[].lower", "h2concept[].lower", "t.lower.lower.lower", "fewshot_re_kit.conceptgraph_utils.instance2conept", "t2concept[].lower", "t2concept[].lower", "tokens.append", "tokens.append", "tokens.append", "tokens.append", "sentence_encoder.BERTConceptSentenceEncoder.tokenizer.convert_tokens_to_ids", "numpy.zeros", "numpy.zeros", "range", "numpy.zeros", "min", "min", "token.lower.lower.lower", "tokens.append", "sentence_encoder.BERTConceptSentenceEncoder.tokenizer.tokenize", "tokens.append", "sentence_encoder.BERTConceptSentenceEncoder.tokenizer.tokenize", "tokens.append", "sentence_encoder.BERTConceptSentenceEncoder.tokenizer.tokenize", "tokens.append", "sentence_encoder.BERTConceptSentenceEncoder.tokenizer.tokenize", "len", "sentence_encoder.BERTConceptSentenceEncoder.append", "tokens.append", "len", "tokens.append", "len", "sentence_encoder.BERTConceptSentenceEncoder.tokenizer.tokenize", "tokens.append", "tokens.append", "len"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.instance2conept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.instance2conept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "def", "tokenize_concept", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ",", "h", ",", "t", ",", "ins2cpt", ")", ":", "\n", "# token -> index", "\n", "        ", "tokens", "=", "[", "'[CLS]'", "]", "\n", "cur_pos", "=", "0", "\n", "pos1_in_index", "=", "1", "\n", "pos2_in_index", "=", "1", "\n", "for", "token", "in", "raw_tokens", ":", "\n", "            ", "token", "=", "token", ".", "lower", "(", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused0]'", ")", "\n", "pos1_in_index", "=", "len", "(", "tokens", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused1]'", ")", "\n", "pos2_in_index", "=", "len", "(", "tokens", ")", "\n", "", "if", "self", ".", "mask_entity", "and", "(", "(", "pos_head", "[", "0", "]", "<=", "cur_pos", "and", "cur_pos", "<=", "pos_head", "[", "-", "1", "]", ")", "or", "(", "\n", "pos_tail", "[", "0", "]", "<=", "cur_pos", "and", "cur_pos", "<=", "pos_tail", "[", "-", "1", "]", ")", ")", ":", "\n", "                ", "tokens", "+=", "[", "'[unused4]'", "]", "\n", "", "else", ":", "\n", "                ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "", "if", "cur_pos", "==", "pos_head", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused2]'", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused3]'", ")", "\n", "", "cur_pos", "+=", "1", "\n", "", "'''\u6dfb\u52a0\u5b9e\u4f53\u7684\u6982\u5ff5\u5230tokens\u4e2d'''", "\n", "h", "=", "h", ".", "lower", "(", ")", "\n", "h2concept", "=", "instance2conept", "(", "ins2cpt", ",", "h", ")", "\n", "h2concept1", "=", "h2concept", "[", "0", "]", ".", "lower", "(", ")", "\n", "h2concept2", "=", "h2concept", "[", "1", "]", ".", "lower", "(", ")", "\n", "t", "=", "t", ".", "lower", "(", ")", "\n", "t2concept", "=", "instance2conept", "(", "ins2cpt", ",", "t", ")", "\n", "t2concept1", "=", "t2concept", "[", "0", "]", ".", "lower", "(", ")", "\n", "t2concept2", "=", "t2concept", "[", "1", "]", ".", "lower", "(", ")", "\n", "\n", "tokens", ".", "append", "(", "'[unused4]'", ")", "\n", "if", "(", "h2concept1", "==", "'unknowconcept1'", ")", "or", "(", "h2concept1", "==", "'unknowconcept2'", ")", ":", "\n", "            ", "tokens", ".", "append", "(", "h2concept1", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "h2concept1", ")", "\n", "\n", "", "tokens", ".", "append", "(", "'[unused5]'", ")", "\n", "if", "(", "h2concept2", "==", "'unknowconcept1'", ")", "or", "(", "h2concept2", "==", "'unknowconcept2'", ")", ":", "\n", "            ", "tokens", ".", "append", "(", "h2concept2", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "h2concept2", ")", "\n", "\n", "", "tokens", ".", "append", "(", "'[unused6]'", ")", "\n", "if", "(", "t2concept1", "==", "'unknowconcept1'", ")", "or", "(", "t2concept1", "==", "'unknowconcept2'", ")", ":", "\n", "            ", "tokens", ".", "append", "(", "t2concept1", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "t2concept1", ")", "\n", "\n", "", "tokens", ".", "append", "(", "'[unused7]'", ")", "\n", "if", "(", "t2concept2", "==", "'unknowconcept1'", ")", "or", "(", "t2concept2", "==", "'unknowconcept2'", ")", ":", "\n", "            ", "tokens", ".", "append", "(", "t2concept2", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "t2concept2", ")", "\n", "\n", "", "indexed_tokens", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# padding", "\n", "while", "len", "(", "indexed_tokens", ")", "<", "self", ".", "max_length", ":", "\n", "            ", "indexed_tokens", ".", "append", "(", "0", ")", "\n", "", "indexed_tokens", "=", "indexed_tokens", "[", ":", "self", ".", "max_length", "]", "\n", "\n", "# pos", "\n", "pos1", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "pos2", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "max_length", ")", ":", "\n", "            ", "pos1", "[", "i", "]", "=", "i", "-", "pos1_in_index", "+", "self", ".", "max_length", "\n", "pos2", "[", "i", "]", "=", "i", "-", "pos2_in_index", "+", "self", ".", "max_length", "\n", "\n", "# mask", "\n", "", "mask", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "mask", "[", ":", "len", "(", "tokens", ")", "]", "=", "1", "\n", "\n", "pos1_in_index", "=", "min", "(", "self", ".", "max_length", ",", "pos1_in_index", ")", "\n", "pos2_in_index", "=", "min", "(", "self", ".", "max_length", ",", "pos2_in_index", ")", "\n", "\n", "return", "indexed_tokens", ",", "pos1_in_index", "-", "1", ",", "pos2_in_index", "-", "1", ",", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.BERTPAIRSentenceEncoder.__init__": [[303, 310], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "transformers.BertForSequenceClassification.from_pretrained", "transformers.BertTokenizer.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "pretrain_path", ",", "max_length", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "self", ".", "bert", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "\n", "pretrain_path", ",", "\n", "num_labels", "=", "2", ")", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.BERTPAIRSentenceEncoder.forward": [[311, 315], ["sentence_encoder.BERTPAIRSentenceEncoder.bert"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "\n", "        ", "x", "=", "self", ".", "bert", "(", "inputs", "[", "'word'", "]", ",", "token_type_ids", "=", "inputs", "[", "'seg'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "[", "0", "]", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.BERTPAIRSentenceEncoder.tokenize": [[316, 341], ["sentence_encoder.BERTPAIRSentenceEncoder.tokenizer.convert_tokens_to_ids", "token.lower.lower.lower", "sentence_encoder.BERTPAIRSentenceEncoder.tokenizer.tokenize", "tokens.append", "len", "tokens.append", "len", "tokens.append", "tokens.append"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "def", "tokenize", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ")", ":", "\n", "# token -> index", "\n", "# tokens = ['[CLS]']", "\n", "        ", "tokens", "=", "[", "]", "\n", "cur_pos", "=", "0", "\n", "pos1_in_index", "=", "0", "\n", "pos2_in_index", "=", "0", "\n", "for", "token", "in", "raw_tokens", ":", "\n", "            ", "token", "=", "token", ".", "lower", "(", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused0]'", ")", "\n", "pos1_in_index", "=", "len", "(", "tokens", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused1]'", ")", "\n", "pos2_in_index", "=", "len", "(", "tokens", ")", "\n", "", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused2]'", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused3]'", ")", "\n", "", "cur_pos", "+=", "1", "\n", "\n", "", "indexed_tokens", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "return", "indexed_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.BERTPAIRSentenceEncoder.tokenize_concept": [[342, 399], ["h.lower.lower.lower", "fewshot_re_kit.conceptgraph_utils.instance2conept", "h2concept[].lower", "h2concept[].lower", "t.lower.lower.lower", "fewshot_re_kit.conceptgraph_utils.instance2conept", "t2concept[].lower", "t2concept[].lower", "tokens.append", "tokens.append", "tokens.append", "tokens.append", "sentence_encoder.BERTPAIRSentenceEncoder.tokenizer.convert_tokens_to_ids", "token.lower.lower.lower", "sentence_encoder.BERTPAIRSentenceEncoder.tokenizer.tokenize", "tokens.append", "sentence_encoder.BERTPAIRSentenceEncoder.tokenizer.tokenize", "tokens.append", "sentence_encoder.BERTPAIRSentenceEncoder.tokenizer.tokenize", "tokens.append", "sentence_encoder.BERTPAIRSentenceEncoder.tokenizer.tokenize", "tokens.append", "sentence_encoder.BERTPAIRSentenceEncoder.tokenizer.tokenize", "tokens.append", "len", "tokens.append", "len", "tokens.append", "tokens.append"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.instance2conept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.instance2conept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "def", "tokenize_concept", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ",", "h", ",", "t", ",", "ins2cpt", ")", ":", "\n", "# token -> index", "\n", "# tokens = ['[CLS]']", "\n", "        ", "tokens", "=", "[", "]", "\n", "cur_pos", "=", "0", "\n", "pos1_in_index", "=", "0", "\n", "pos2_in_index", "=", "0", "\n", "for", "token", "in", "raw_tokens", ":", "\n", "            ", "token", "=", "token", ".", "lower", "(", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused0]'", ")", "\n", "pos1_in_index", "=", "len", "(", "tokens", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused1]'", ")", "\n", "pos2_in_index", "=", "len", "(", "tokens", ")", "\n", "", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused2]'", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused3]'", ")", "\n", "", "cur_pos", "+=", "1", "\n", "", "'''\u6dfb\u52a0\u5b9e\u4f53\u7684\u6982\u5ff5\u5230tokens\u4e2d'''", "\n", "h", "=", "h", ".", "lower", "(", ")", "\n", "h2concept", "=", "instance2conept", "(", "ins2cpt", ",", "h", ")", "\n", "h2concept1", "=", "h2concept", "[", "0", "]", ".", "lower", "(", ")", "\n", "h2concept2", "=", "h2concept", "[", "1", "]", ".", "lower", "(", ")", "\n", "t", "=", "t", ".", "lower", "(", ")", "\n", "t2concept", "=", "instance2conept", "(", "ins2cpt", ",", "t", ")", "\n", "t2concept1", "=", "t2concept", "[", "0", "]", ".", "lower", "(", ")", "\n", "t2concept2", "=", "t2concept", "[", "1", "]", ".", "lower", "(", ")", "\n", "\n", "tokens", ".", "append", "(", "'[unused4]'", ")", "\n", "if", "(", "h2concept1", "==", "'unknowconcept1'", ")", "or", "(", "h2concept1", "==", "'unknowconcept2'", ")", ":", "\n", "# print('-----------I am running-----------------------')", "\n", "            ", "tokens", ".", "append", "(", "h2concept1", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "h2concept1", ")", "\n", "\n", "", "tokens", ".", "append", "(", "'[unused5]'", ")", "\n", "if", "(", "h2concept2", "==", "'unknowconcept1'", ")", "or", "(", "h2concept2", "==", "'unknowconcept2'", ")", ":", "\n", "            ", "tokens", ".", "append", "(", "h2concept2", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "h2concept2", ")", "\n", "\n", "", "tokens", ".", "append", "(", "'[unused6]'", ")", "\n", "if", "(", "t2concept1", "==", "'unknowconcept1'", ")", "or", "(", "t2concept1", "==", "'unknowconcept2'", ")", ":", "\n", "            ", "tokens", ".", "append", "(", "t2concept1", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "t2concept1", ")", "\n", "\n", "", "tokens", ".", "append", "(", "'[unused7]'", ")", "\n", "if", "(", "t2concept2", "==", "'unknowconcept1'", ")", "or", "(", "t2concept2", "==", "'unknowconcept2'", ")", ":", "\n", "            ", "tokens", ".", "append", "(", "t2concept2", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "t2concept2", ")", "\n", "", "indexed_tokens", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "return", "indexed_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.BERTPAIRSentenceEncoder.tokenize_concept_plus": [[400, 443], ["h.lower.lower.lower", "fewshot_re_kit.conceptgraph_utils.instance2coneptPlus", "t.lower.lower.lower", "fewshot_re_kit.conceptgraph_utils.instance2coneptPlus", "tokens.append", "tokens.append", "sentence_encoder.BERTPAIRSentenceEncoder.tokenizer.convert_tokens_to_ids", "token.lower.lower.lower", "sentence_encoder.BERTPAIRSentenceEncoder.tokenizer.tokenize", "tokens.append", "len", "tokens.append", "len", "tokens.append", "tokens.append", "tokens.append", "sentence_encoder.BERTPAIRSentenceEncoder.tokenizer.tokenize", "tokens.append", "sentence_encoder.BERTPAIRSentenceEncoder.tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.instance2coneptPlus", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.instance2coneptPlus", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "def", "tokenize_concept_plus", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ",", "h", ",", "t", ",", "ins2cpt", ")", ":", "\n", "# token -> index", "\n", "# tokens = ['[CLS]']", "\n", "\n", "        ", "tokens", "=", "[", "]", "\n", "cur_pos", "=", "0", "\n", "pos1_in_index", "=", "0", "\n", "pos2_in_index", "=", "0", "\n", "for", "token", "in", "raw_tokens", ":", "\n", "            ", "token", "=", "token", ".", "lower", "(", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused0]'", ")", "\n", "pos1_in_index", "=", "len", "(", "tokens", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused1]'", ")", "\n", "pos2_in_index", "=", "len", "(", "tokens", ")", "\n", "", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused2]'", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused3]'", ")", "\n", "", "cur_pos", "+=", "1", "\n", "", "'''\u6dfb\u52a0\u5b9e\u4f53\u7684\u6982\u5ff5\u5230tokens\u4e2d'''", "\n", "h", "=", "h", ".", "lower", "(", ")", "\n", "h2concept", "=", "instance2coneptPlus", "(", "ins2cpt", ",", "h", ")", "\n", "t", "=", "t", ".", "lower", "(", ")", "\n", "t2concept", "=", "instance2coneptPlus", "(", "ins2cpt", ",", "t", ")", "\n", "\n", "tokens", ".", "append", "(", "'[unused4]'", ")", "\n", "for", "cpt", "in", "h2concept", ":", "\n", "            ", "if", "cpt", "==", "'unknowConcept'", ":", "\n", "                ", "tokens", ".", "append", "(", "cpt", ")", "\n", "", "else", ":", "\n", "                ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "cpt", ")", "\n", "\n", "", "", "tokens", ".", "append", "(", "'[unused5]'", ")", "\n", "for", "cpt", "in", "t2concept", ":", "\n", "            ", "if", "cpt", "==", "'unknowConcept'", ":", "\n", "                ", "tokens", ".", "append", "(", "cpt", ")", "\n", "", "else", ":", "\n", "                ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "cpt", ")", "\n", "", "", "indexed_tokens", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "return", "indexed_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.BERTPAIRConceptSentenceEncoder.__init__": [[447, 483], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "print", "transformers.BertModel.from_pretrained", "transformers.BertTokenizer.from_pretrained", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "pretrain_path", ",", "max_length", ",", "conceptEmbedding", ",", "BeyondWordEmbedding", ",", "id2embeddingID", ",", "\n", "id_from", "=", "'kgEmbeddingOrBeyondWordEmbedding'", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "print", "(", "'----------------------BERTPAIRConceptSentenceEncoder initializing----------------------------------'", ")", "\n", "self", ".", "bert", "=", "BertModel", ".", "from_pretrained", "(", "\n", "pretrain_path", ")", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", "\n", "self", ".", "conceptEmbedding", "=", "conceptEmbedding", "\n", "self", ".", "beyondWordEmbedding", "=", "BeyondWordEmbedding", "# \u8bcd\u5411\u91cf500\u7ef4", "\n", "self", ".", "id2embeddingID", "=", "id2embeddingID", "\n", "self", ".", "id_from", "=", "id_from", "\n", "\n", "if", "self", ".", "id_from", "==", "'keEmbedding'", ":", "\n", "            ", "self", ".", "projector1", "=", "torch", ".", "nn", ".", "Linear", "(", "256", ",", "768", ")", "\n", "self", ".", "projector2", "=", "torch", ".", "nn", ".", "Linear", "(", "768", ",", "768", ")", "\n", "self", ".", "fusionLayer", "=", "torch", ".", "nn", ".", "MultiheadAttention", "(", "embed_dim", "=", "768", ",", "num_heads", "=", "12", ",", "dropout", "=", "0.1", ")", "\n", "self", ".", "classifier", "=", "torch", ".", "nn", ".", "Linear", "(", "136", "*", "768", ",", "2", ")", "\n", "\n", "", "elif", "self", ".", "id_from", "==", "'BeyondWordEmbedding'", ":", "\n", "            ", "self", ".", "projector1", "=", "torch", ".", "nn", ".", "Linear", "(", "500", ",", "128", ")", "\n", "self", ".", "projector2", "=", "torch", ".", "nn", ".", "Linear", "(", "768", ",", "128", ")", "\n", "\n", "self", ".", "classifier", "=", "torch", ".", "nn", ".", "Linear", "(", "1792", ",", "2", ")", "\n", "", "elif", "self", ".", "id_from", "==", "'MultiHeadAttentionAndBeyondWordEmbedding'", ":", "\n", "            ", "word_dim", "=", "768", "\n", "self", ".", "concept_project", "=", "True", "\n", "# word_dim = 500", "\n", "self", ".", "projector1", "=", "torch", ".", "nn", ".", "Linear", "(", "500", ",", "word_dim", ")", "# \u5bf9\u6982\u5ff5\u5411\u91cf\u6295\u5f71", "\n", "self", ".", "projector2", "=", "torch", ".", "nn", ".", "Linear", "(", "768", ",", "word_dim", ")", "# \u5bf9\u53e5\u5b50\u5411\u91cf\u6295\u5f71", "\n", "# self.projector3 = torch.nn.Linear(768, 120)", "\n", "\n", "self", ".", "fusionLayer", "=", "torch", ".", "nn", ".", "MultiheadAttention", "(", "embed_dim", "=", "word_dim", ",", "num_heads", "=", "12", ",", "dropout", "=", "0.1", ")", "\n", "self", ".", "classifier", "=", "torch", ".", "nn", ".", "Linear", "(", "(", "self", ".", "max_length", "+", "8", ")", "*", "word_dim", ",", "2", ")", "\n", "", "else", ":", "\n", "            ", "assert", "(", "'please input right id source'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.BERTPAIRConceptSentenceEncoder.forward": [[484, 528], ["sentence_encoder.BERTPAIRConceptSentenceEncoder.sentence_have_concept", "sentence_encoder.BERTPAIRConceptSentenceEncoder.sentence_have_concept", "sentence_encoder.BERTPAIRConceptSentenceEncoder.bert", "sentence_encoder.BERTPAIRConceptSentenceEncoder.bert", "sentence_encoder.BERTPAIRConceptSentenceEncoder.sen_pair_cat_cpt", "sentence_encoder.BERTPAIRConceptSentenceEncoder.fusionLayer", "sentence_encoder.BERTPAIRConceptSentenceEncoder.classifier", "sentence_encoder.BERTPAIRConceptSentenceEncoder.bert", "sentence_encoder.BERTPAIRConceptSentenceEncoder.sen_pair_cat_cpt", "sentence_encoder.BERTPAIRConceptSentenceEncoder.classifier", "sentence_encoder.BERTPAIRConceptSentenceEncoder.bert", "sentence_encoder.BERTPAIRConceptSentenceEncoder.pair_projecter", "sentence_encoder.BERTPAIRConceptSentenceEncoder.sen_pair_cat_cpt", "sentence_encoder.BERTPAIRConceptSentenceEncoder.fusionLayer", "sen_cpt_vec.reshape.reshape.reshape", "sentence_encoder.BERTPAIRConceptSentenceEncoder.classifier", "sentence_encoder.BERTPAIRConceptSentenceEncoder.bert"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.sentence_have_concept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.sentence_have_concept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.sen_pair_cat_cpt", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.sen_pair_cat_cpt", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.pair_projecter", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.sen_pair_cat_cpt"], ["", "", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "\n", "        ", "'''\u8ba1\u7b97query\u53e5\u5b50\u4e0e\u5176\u5b9e\u4f53\u5bf9\u5e94\u7684\u6982\u5ff5\u76f8\u4f3c\u5ea6'''", "\n", "query_sen", "=", "self", ".", "bert", "(", "inputs", "[", "'query_sen'", "]", ",", "attention_mask", "=", "inputs", "[", "'query_mask'", "]", ")", "[", "1", "]", "\n", "queryConceptID", "=", "inputs", "[", "'queryConceptID'", "]", "\n", "quer_sen_hava_cpt", "=", "self", ".", "sentence_have_concept", "(", "query_sen", ",", "queryConceptID", ")", "\n", "'''\u8ba1\u7b97support\u53e5\u5b50\u4e0e\u5176\u5b9e\u4f53\u5bf9\u5e94\u7684\u6982\u5ff5\u76f8\u4f3c\u5ea6'''", "\n", "support_sen", "=", "self", ".", "bert", "(", "inputs", "[", "'support_sen'", "]", ",", "attention_mask", "=", "inputs", "[", "'support_mask'", "]", ")", "[", "1", "]", "\n", "supportConceptID", "=", "inputs", "[", "'supportConceptID'", "]", "\n", "support_sen_have_cpt", "=", "self", ".", "sentence_have_concept", "(", "support_sen", ",", "supportConceptID", ")", "\n", "\n", "'''\u53e5\u5b50\u548c\u6982\u5ff5embedding\u62fc\u63a5'''", "\n", "if", "self", ".", "id_from", "==", "'keEmbedding'", ":", "\n", "            ", "x", "=", "self", ".", "bert", "(", "inputs", "[", "'word'", "]", ",", "token_type_ids", "=", "inputs", "[", "'seg'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "[", "1", "]", "\n", "sen_cpt_vec", "=", "self", ".", "sen_pair_cat_cpt", "(", "x", ",", "quer_sen_hava_cpt", ",", "\n", "support_sen_have_cpt", ")", "# sen_cpt_vec shape:(-1,768+128*8=1792)", "\n", "sen_cpt_vec", "=", "self", ".", "fusionLayer", "(", "sen_cpt_vec", ")", "\n", "x", "=", "self", ".", "classifier", "(", "sen_cpt_vec", ")", "\n", "", "elif", "self", ".", "id_from", "==", "'BeyondWordEmbedding'", ":", "\n", "            ", "x", "=", "self", ".", "bert", "(", "inputs", "[", "'word'", "]", ",", "token_type_ids", "=", "inputs", "[", "'seg'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "[", "1", "]", "\n", "sen_cpt_vec", "=", "self", ".", "sen_pair_cat_cpt", "(", "x", ",", "quer_sen_hava_cpt", ",", "\n", "support_sen_have_cpt", ")", "# sen_cpt_vec shape:(-1,768+500*8=4768)", "\n", "# sen_cpt_vec = self.fusionLayer(sen_cpt_vec)", "\n", "x", "=", "self", ".", "classifier", "(", "sen_cpt_vec", ")", "\n", "", "elif", "self", ".", "id_from", "==", "'MultiHeadAttentionAndBeyondWordEmbedding'", ":", "\n", "            ", "x", "=", "self", ".", "bert", "(", "inputs", "[", "'word'", "]", ",", "token_type_ids", "=", "inputs", "[", "'seg'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "[", "\n", "0", "]", "# \u53d6\u51fa\u8bcd\u5411\u91cf\u77e9\u9635[-1,\u5355\u8bcd\u4e2a\u6570\uff08\u53e5\u5b50\u957f\u5ea6\uff09\uff0c\u8bcd\u5411\u91cf\u7684\u7ef4\u5ea6]", "\n", "# word_num = x.shape[1]", "\n", "x", "=", "self", ".", "pair_projecter", "(", "x", ")", "\n", "sen_cpt_vec", "=", "self", ".", "sen_pair_cat_cpt", "(", "x", ",", "quer_sen_hava_cpt", ",", "\n", "support_sen_have_cpt", ")", "\n", "\n", "sen_cpt_vec", ",", "_", "=", "self", ".", "fusionLayer", "(", "sen_cpt_vec", ",", "sen_cpt_vec", ",", "sen_cpt_vec", ")", "\n", "\n", "# print('----------------------------sen_cpt_vec.shape----------------------')", "\n", "# print(sen_cpt_vec.shape)", "\n", "# sen_cpt_vec = self.projector3(sen_cpt_vec)", "\n", "# sen_cpt_vec = sen_cpt_vec.reshape(-1, (word_num + 8) * 384)", "\n", "sen_cpt_vec", "=", "sen_cpt_vec", ".", "reshape", "(", "-", "1", ",", "sen_cpt_vec", ".", "shape", "[", "1", "]", "*", "sen_cpt_vec", ".", "shape", "[", "2", "]", ")", "\n", "\n", "# print('---------------------sen_cpt_vec.shape-----------------',sen_cpt_vec.shape)", "\n", "x", "=", "self", ".", "classifier", "(", "sen_cpt_vec", ")", "\n", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.BERTPAIRConceptSentenceEncoder.pair_projecter": [[529, 542], ["torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "range", "sentence_encoder.BERTPAIRConceptSentenceEncoder.projector2", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "pair_projecter", "(", "self", ",", "wordVec", ")", ":", "\n", "\n", "        ", "sample_num", "=", "wordVec", ".", "shape", "[", "0", "]", "\n", "word_num", "=", "wordVec", ".", "shape", "[", "1", "]", "\n", "sen_len", "=", "self", ".", "projector2", ".", "bias", ".", "shape", "[", "0", "]", "\n", "\n", "projected_wordVec", "=", "torch", ".", "zeros", "(", "[", "sample_num", ",", "word_num", ",", "sen_len", "]", ")", ".", "cuda", "(", ")", "\n", "for", "i", "in", "range", "(", "sample_num", ")", ":", "\n", "            ", "wv", "=", "wordVec", "[", "i", ",", ":", ",", ":", "]", "\n", "wv", "=", "self", ".", "projector2", "(", "wv", ")", "\n", "projected_wordVec", "[", "i", ",", ":", ",", ":", "]", "=", "wv", "\n", "# print('projected_wordVec.shape', projected_wordVec.shape)", "\n", "", "return", "projected_wordVec", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.BERTPAIRConceptSentenceEncoder.sentence_have_concept": [[543, 621], ["range", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "sen_have_cpt.append", "sentence_encoder.BERTPAIRConceptSentenceEncoder.id2embedding().cuda", "sentence_encoder.BERTPAIRConceptSentenceEncoder.id2embedding().cuda", "sentence_encoder.BERTPAIRConceptSentenceEncoder.id2embedding().cuda", "sentence_encoder.BERTPAIRConceptSentenceEncoder.id2embedding().cuda", "sentence_encoder.BERTPAIRConceptSentenceEncoder.projector1", "sentence_encoder.BERTPAIRConceptSentenceEncoder.projector1", "sentence_encoder.BERTPAIRConceptSentenceEncoder.projector1", "sentence_encoder.BERTPAIRConceptSentenceEncoder.projector1", "sentence_encoder.BERTPAIRConceptSentenceEncoder.projector2", "sentence_encoder.BERTPAIRConceptSentenceEncoder.id2BeyondWordEmbedding", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "sentence_encoder.BERTPAIRConceptSentenceEncoder.id2embedding", "sentence_encoder.BERTPAIRConceptSentenceEncoder.id2embedding", "sentence_encoder.BERTPAIRConceptSentenceEncoder.id2embedding", "sentence_encoder.BERTPAIRConceptSentenceEncoder.id2embedding", "sentence_encoder.BERTPAIRConceptSentenceEncoder.projector1", "sentence_encoder.BERTPAIRConceptSentenceEncoder.projector1", "sentence_encoder.BERTPAIRConceptSentenceEncoder.projector1", "sentence_encoder.BERTPAIRConceptSentenceEncoder.projector1", "sentence_encoder.BERTPAIRConceptSentenceEncoder.projector2", "sentence_encoder.BERTPAIRConceptSentenceEncoder.t", "sentence_encoder.BERTPAIRConceptSentenceEncoder.t", "sentence_encoder.BERTPAIRConceptSentenceEncoder.t", "sentence_encoder.BERTPAIRConceptSentenceEncoder.t"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.id2BeyondWordEmbedding", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.id2embedding", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.id2embedding", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.id2embedding", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.id2embedding"], ["", "def", "sentence_have_concept", "(", "self", ",", "sen", ",", "conceptID", ")", ":", "\n", "        ", "'''\n        \u8ba1\u7b97\u53e5\u5b50\u4e0econcept\u7684\u76f8\u4f3c\u5ea6\uff0c\u503c\u4e3a0\u62161,\u8fd4\u56devalue\u4e3a1\u7684concept embedding\n        '''", "\n", "\n", "sen_have_cpt", "=", "[", "]", "# \u7528\u4e8e\u5b58\u50a8\u6839\u636e\u53e5\u610f\u9009\u62e9\u7684\u5b9e\u4f53", "\n", "sen_num", "=", "sen", ".", "shape", "[", "0", "]", "\n", "sen_len", "=", "sen", ".", "shape", "[", "1", "]", "# 768", "\n", "\n", "for", "i", "in", "range", "(", "sen_num", ")", ":", "\n", "            ", "if", "self", ".", "id_from", "==", "'kgEmbedding'", ":", "\n", "                ", "'''\u83b7\u53d6\u6982\u5ff5\u5411\u91cf'''", "\n", "cptID", "=", "conceptID", "[", "i", "]", "\n", "h_cpt1ID", "=", "cptID", "[", "0", "]", "# TOdo \u8f6c\u6362\u6210int", "\n", "h_cpt2ID", "=", "cptID", "[", "1", "]", "\n", "t_cpt1ID", "=", "cptID", "[", "2", "]", "\n", "t_cpt2ID", "=", "cptID", "[", "3", "]", "\n", "\n", "h_cpt1_vec", "=", "self", ".", "id2embedding", "(", "h_cpt1ID", ",", "self", ".", "conceptEmbedding", ")", ".", "cuda", "(", ")", "\n", "h_cpt2_vec", "=", "self", ".", "id2embedding", "(", "h_cpt2ID", ",", "self", ".", "conceptEmbedding", ")", ".", "cuda", "(", ")", "\n", "t_cpt1_vec", "=", "self", ".", "id2embedding", "(", "t_cpt1ID", ",", "self", ".", "conceptEmbedding", ")", ".", "cuda", "(", ")", "\n", "t_cpt2_vec", "=", "self", ".", "id2embedding", "(", "t_cpt2ID", ",", "self", ".", "conceptEmbedding", ")", ".", "cuda", "(", ")", "\n", "'''\u83b7\u53d6\u53e5\u5b50\u5411\u91cf'''", "\n", "sen_vec", "=", "sen", "[", "i", ",", ":", "]", "\n", "'''\u6982\u5ff5\u5411\u91cf\u6295\u5f71'''", "\n", "h_cpt1_vec", "=", "self", ".", "projector1", "(", "h_cpt1_vec", ")", "# size \u7531(1,256)\u53d8\u6210(1\uff0c128)", "\n", "h_cpt2_vec", "=", "self", ".", "projector1", "(", "h_cpt2_vec", ")", "\n", "t_cpt1_vec", "=", "self", ".", "projector1", "(", "t_cpt1_vec", ")", "\n", "t_cpt2_vec", "=", "self", ".", "projector1", "(", "t_cpt2_vec", ")", "\n", "'''\u53e5\u5b50\u5411\u91cf\u6295\u5f71'''", "\n", "sen_vec", "=", "self", ".", "projector2", "(", "sen_vec", ")", "\n", "", "elif", "(", "self", ".", "id_from", "==", "'BeyondWordEmbedding'", ")", "|", "(", "self", ".", "id_from", "==", "'MultiHeadAttentionAndBeyondWordEmbedding'", ")", ":", "\n", "                ", "'''\u83b7\u53d6\u6982\u5ff5\u5411\u91cf'''", "\n", "cptID", "=", "conceptID", "[", "i", "]", "\n", "all_cpt_vec", "=", "self", ".", "id2BeyondWordEmbedding", "(", "cptID", ")", "\n", "h_cpt1_vec", "=", "all_cpt_vec", "[", "0", "]", "\n", "h_cpt2_vec", "=", "all_cpt_vec", "[", "1", "]", "\n", "t_cpt1_vec", "=", "all_cpt_vec", "[", "2", "]", "\n", "t_cpt2_vec", "=", "all_cpt_vec", "[", "3", "]", "\n", "'''\u83b7\u53d6\u53e5\u5b50\u5411\u91cf'''", "\n", "sen_vec", "=", "sen", "[", "i", ",", ":", "]", "\n", "\n", "# '''\u53e5\u5b50\u5411\u91cf\u6295\u5f71'''", "\n", "# sen_vec = self.projector2(sen_vec)", "\n", "'''\u6982\u5ff5\u5411\u91cf\u6295\u5f71'''", "\n", "if", "self", ".", "concept_project", ":", "\n", "                    ", "h_cpt1_vec", "=", "self", ".", "projector1", "(", "h_cpt1_vec", ")", "# size \u7531(1,500)\u53d8\u6210(1\uff0c768)", "\n", "h_cpt2_vec", "=", "self", ".", "projector1", "(", "h_cpt2_vec", ")", "\n", "t_cpt1_vec", "=", "self", ".", "projector1", "(", "t_cpt1_vec", ")", "\n", "t_cpt2_vec", "=", "self", ".", "projector1", "(", "t_cpt2_vec", ")", "\n", "", "else", ":", "\n", "                    ", "'''\u53e5\u5b50\u5411\u91cf\u6295\u5f71'''", "\n", "sen_vec", "=", "self", ".", "projector2", "(", "sen_vec", ")", "\n", "\n", "", "", "'''\u8ba1\u7b97\u53e5\u5b50\u548c\u6982\u5ff5\u7684\u76f8\u4f3c\u5ea6'''", "\n", "h_cpt1_sen_sim", "=", "torch", ".", "matmul", "(", "sen_vec", ",", "h_cpt1_vec", ".", "t", "(", ")", ")", ".", "float", "(", ")", "# size(1),\u8ba1\u7b97\u7ed3\u679c\u4e3a\u6807\u91cf", "\n", "h_cpt2_sen_sim", "=", "torch", ".", "matmul", "(", "sen_vec", ",", "h_cpt2_vec", ".", "t", "(", ")", ")", ".", "float", "(", ")", "\n", "t_cpt1_sen_sim", "=", "torch", ".", "matmul", "(", "sen_vec", ",", "t_cpt1_vec", ".", "t", "(", ")", ")", ".", "float", "(", ")", "\n", "t_cpt2_sen_sim", "=", "torch", ".", "matmul", "(", "sen_vec", ",", "t_cpt2_vec", ".", "t", "(", ")", ")", ".", "float", "(", ")", "\n", "'''\u76f8\u4f3c\u5ea601\u5316,\u76f8\u4f3c\u5ea6\u7684\u503c\u53ea\u53d60\u62161'''", "\n", "# softmax", "\n", "sim", "=", "torch", ".", "tensor", "(", "[", "h_cpt1_sen_sim", ",", "h_cpt2_sen_sim", ",", "t_cpt1_sen_sim", ",", "t_cpt2_sen_sim", "]", ")", "\n", "sim", "=", "torch", ".", "softmax", "(", "sim", ",", "dim", "=", "0", ")", "\n", "[", "h_cpt1_sen_sim", ",", "h_cpt2_sen_sim", ",", "t_cpt1_sen_sim", ",", "t_cpt2_sen_sim", "]", "=", "sim", "\n", "'''O1-GATE threshold, alpha: 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1'''", "\n", "threshold_alpha", "=", "0.3", "\n", "h_cpt1_sen_sim", "=", "1", "if", "h_cpt1_sen_sim", ">=", "threshold_alpha", "else", "0", "\n", "h_cpt2_sen_sim", "=", "1", "if", "h_cpt2_sen_sim", ">=", "threshold_alpha", "else", "0", "\n", "t_cpt1_sen_sim", "=", "1", "if", "t_cpt1_sen_sim", ">=", "threshold_alpha", "else", "0", "\n", "t_cpt2_sen_sim", "=", "1", "if", "t_cpt2_sen_sim", ">=", "threshold_alpha", "else", "0", "\n", "\n", "h_cpt1_vec", "=", "h_cpt1_vec", "*", "h_cpt1_sen_sim", "\n", "h_cpt2_vec", "=", "h_cpt2_vec", "*", "h_cpt2_sen_sim", "\n", "t_cpt1_vec", "=", "t_cpt1_vec", "*", "t_cpt1_sen_sim", "\n", "t_cpt2_vec", "=", "t_cpt2_vec", "*", "t_cpt2_sen_sim", "\n", "sen_have_cpt", ".", "append", "(", "[", "h_cpt1_vec", ",", "h_cpt2_vec", ",", "t_cpt1_vec", ",", "t_cpt2_vec", "]", ")", "\n", "\n", "", "return", "sen_have_cpt", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.BERTPAIRConceptSentenceEncoder.id2BeyondWordEmbedding": [[622, 679], ["range", "range", "all_cpt_id.append", "len", "all_cpt_vec.append", "cpt_id.append", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "all_cpt_vec.append", "str.cpu().numpy", "str", "sentence_encoder.BERTPAIRConceptSentenceEncoder.beyondWordEmbedding[].float().cuda", "cpt_vec.view.view.view", "all_cpt_vec.append", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "str.split", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "str.cpu().numpy", "str", "sentence_encoder.BERTPAIRConceptSentenceEncoder.beyondWordEmbedding[].float().cuda", "cpt_vec.view.view.view", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "str.cpu", "sentence_encoder.BERTPAIRConceptSentenceEncoder.beyondWordEmbedding[].float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "str.split", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "str.cpu", "sentence_encoder.BERTPAIRConceptSentenceEncoder.beyondWordEmbedding[].float", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "id2BeyondWordEmbedding", "(", "self", ",", "cptID", ")", ":", "\n", "        ", "'''\n        cptID = torch.tensor([[9.6963e+06, -2.0000e+00, -2.0000e+00, -2.0000e+00, -2.0000e+00],\n                              [-1.0000e+00, -2.0000e+00, -2.0000e+00, -2.0000e+00, -2.0000e+00],\n                              [-1.0000e+00, -2.0000e+00, -2.0000e+00, -2.0000e+00, -2.0000e+00],\n                              [-1.0000e+00, -2.0000e+00, -2.0000e+00, -2.0000e+00, -2.0000e+00]])\n        '''", "\n", "cpt_num", "=", "cptID", ".", "shape", "[", "0", "]", "\n", "all_cpt_id", "=", "[", "]", "\n", "all_cpt_vec", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "cpt_num", ")", ":", "\n", "            ", "wordsID", "=", "cptID", "[", "i", "]", "\n", "cpt_id", "=", "[", "]", "\n", "for", "id", "in", "wordsID", ":", "\n", "                ", "if", "id", "==", "-", "2", ":", "# -2\u4e3a\u65e0\u6548ID\uff0c\u586b\u5145\u5411\u91cf\u7528\u7684", "\n", "                    ", "continue", "\n", "", "else", ":", "\n", "                    ", "cpt_id", ".", "append", "(", "id", ")", "\n", "", "", "all_cpt_id", ".", "append", "(", "cpt_id", ")", "\n", "", "for", "j", "in", "range", "(", "cpt_num", ")", ":", "# \u6982\u5ff5\u53ea\u6709\u4e00\u4e2a\u8bcd\u7ec4\u6210", "\n", "            ", "j_cpt_id", "=", "all_cpt_id", "[", "j", "]", "\n", "if", "len", "(", "j_cpt_id", ")", "==", "1", ":", "\n", "                ", "j_id", "=", "j_cpt_id", "[", "0", "]", "\n", "if", "j_id", "==", "-", "1", ":", "\n", "                    ", "cpt_vec", "=", "torch", ".", "zeros", "(", "(", "1", ",", "500", ")", ")", ".", "float", "(", ")", ".", "cuda", "(", ")", "\n", "all_cpt_vec", ".", "append", "(", "cpt_vec", ")", "\n", "", "else", ":", "\n", "# j_id = j_id.long()", "\n", "\n", "                    ", "j_id", "=", "j_id", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "j_id", "=", "str", "(", "j_id", ")", "\n", "j_id", "=", "j_id", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "j_id", "=", "self", ".", "id2embeddingID", "[", "j_id", "]", "\n", "\n", "cpt_vec", "=", "self", ".", "beyondWordEmbedding", "[", "j_id", ",", ":", "]", ".", "float", "(", ")", ".", "cuda", "(", ")", "\n", "cpt_vec", "=", "cpt_vec", ".", "view", "(", "[", "1", ",", "500", "]", ")", "\n", "all_cpt_vec", ".", "append", "(", "cpt_vec", ")", "\n", "", "", "else", ":", "\n", "                ", "for", "id", "in", "j_cpt_id", ":", "# \u6982\u5ff5\u7531\u591a\u4e2a\u8bcd\u7ec4\u6210\uff0c\u628a\u8bcd\u7684\u5411\u91cf\u53e0\u52a0\uff0c\u4f5c\u4e3a\u6982\u5ff5\u7684\u5411\u91cf", "\n", "                    ", "word_vec", "=", "torch", ".", "zeros", "(", "(", "1", ",", "500", ")", ")", ".", "float", "(", ")", ".", "cuda", "(", ")", "\n", "if", "id", "==", "-", "1", ":", "\n", "                        ", "cpt_vec", "=", "torch", ".", "zeros", "(", "(", "1", ",", "500", ")", ")", ".", "float", "(", ")", ".", "cuda", "(", ")", "\n", "word_vec", "=", "word_vec", "+", "cpt_vec", "\n", "", "else", ":", "\n", "# id = id.long()", "\n", "\n", "                        ", "id", "=", "id", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "id", "=", "str", "(", "id", ")", "\n", "id", "=", "id", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "id", "=", "self", ".", "id2embeddingID", "[", "id", "]", "\n", "\n", "cpt_vec", "=", "self", ".", "beyondWordEmbedding", "[", "id", ",", ":", "]", ".", "float", "(", ")", ".", "cuda", "(", ")", "\n", "cpt_vec", "=", "cpt_vec", ".", "view", "(", "[", "1", ",", "500", "]", ")", "\n", "word_vec", "=", "word_vec", "+", "cpt_vec", "\n", "# all_cpt_vec.append(word_vec)", "\n", "", "", "all_cpt_vec", ".", "append", "(", "word_vec", ")", "\n", "", "", "return", "all_cpt_vec", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.BERTPAIRConceptSentenceEncoder.id2embedding": [[680, 688], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "cpt_vec.view.view.view"], "methods", ["None"], ["", "def", "id2embedding", "(", "self", ",", "id", ",", "conceptEmbedding", ")", ":", "\n", "        ", "if", "id", "==", "-", "1", ":", "\n", "            ", "cpt_vec", "=", "torch", ".", "zeros", "(", "(", "1", ",", "256", ")", ")", "\n", "", "else", ":", "\n", "            ", "cpt_vec", "=", "conceptEmbedding", "[", "id", ",", ":", "]", "\n", "cpt_vec", "=", "cpt_vec", ".", "view", "(", "[", "1", ",", "256", "]", ")", "\n", "\n", "", "return", "cpt_vec", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.BERTPAIRConceptSentenceEncoder.sen_pair_cat_cpt": [[689, 744], ["torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "range", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.cuda", "torch.cat.cuda", "torch.cat.cuda", "sen_vec.view.view.view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "sen_pair_cat_cpt", "(", "self", ",", "sen_pair", ",", "query_sen_hava_cpt", ",", "support_sen_have_cpt", ")", ":", "\n", "        ", "if", "self", ".", "id_from", "==", "'MultiHeadAttentionAndBeyondWordEmbedding'", ":", "\n", "\n", "            ", "sen_num", "=", "sen_pair", ".", "shape", "[", "0", "]", "\n", "word_num", "=", "sen_pair", ".", "shape", "[", "1", "]", "\n", "sen_len", "=", "sen_pair", ".", "shape", "[", "2", "]", "\n", "cpt_num", "=", "8", "\n", "sen_cpt_vec", "=", "torch", ".", "zeros", "(", "[", "sen_num", ",", "word_num", "+", "cpt_num", ",", "sen_len", "]", ")", ".", "cuda", "(", ")", "\n", "\n", "# sen_shape = [sen_pair.shape[1], sen_pair.shape[2]]", "\n", "for", "i", "in", "range", "(", "sen_num", ")", ":", "\n", "                ", "sen_vec", "=", "sen_pair", "[", "i", ",", ":", ",", ":", "]", "\n", "# sen_vec = sen_vec.view(sen_shape)", "\n", "query_cpt_vec", "=", "query_sen_hava_cpt", "[", "i", "]", "\n", "query_h_cpt1_vec", "=", "query_cpt_vec", "[", "0", "]", "\n", "query_h_cpt2_vec", "=", "query_cpt_vec", "[", "1", "]", "\n", "query_t_cpt1_vec", "=", "query_cpt_vec", "[", "2", "]", "\n", "query_t_cpt2_vec", "=", "query_cpt_vec", "[", "3", "]", "\n", "support_cpt_vec", "=", "support_sen_have_cpt", "[", "i", "]", "\n", "support_h_cpt1_vec", "=", "support_cpt_vec", "[", "0", "]", "\n", "support_h_cpt2_vec", "=", "support_cpt_vec", "[", "1", "]", "\n", "support_t_cpt1_vec", "=", "support_cpt_vec", "[", "2", "]", "\n", "support_t_cpt2_vec", "=", "support_cpt_vec", "[", "3", "]", "\n", "i_sen_cpt_vec", "=", "torch", ".", "cat", "(", "\n", "(", "sen_vec", ",", "query_h_cpt1_vec", ",", "query_h_cpt2_vec", ",", "query_t_cpt1_vec", ",", "query_t_cpt2_vec", ",", "\n", "support_h_cpt1_vec", ",", "support_h_cpt2_vec", ",", "support_t_cpt1_vec", ",", "support_t_cpt2_vec", ")", ",", "\n", "0", ")", "\n", "\n", "sen_cpt_vec", "[", "i", ",", ":", ",", ":", "]", "=", "i_sen_cpt_vec", ".", "cuda", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "sen_cpt_vec", "=", "[", "]", "\n", "# print(sen_cpt_vec)", "\n", "sen_num", "=", "sen_pair", ".", "shape", "[", "0", "]", "\n", "sen_len", "=", "sen_pair", ".", "shape", "[", "1", "]", "\n", "for", "i", "in", "range", "(", "sen_num", ")", ":", "\n", "                ", "sen_vec", "=", "sen_pair", "[", "i", ",", ":", "]", "\n", "sen_vec", "=", "sen_vec", ".", "view", "(", "[", "1", ",", "sen_len", "]", ")", "\n", "query_cpt_vec", "=", "query_sen_hava_cpt", "[", "i", "]", "\n", "query_h_cpt1_vec", "=", "query_cpt_vec", "[", "0", "]", "\n", "query_h_cpt2_vec", "=", "query_cpt_vec", "[", "1", "]", "\n", "query_t_cpt1_vec", "=", "query_cpt_vec", "[", "2", "]", "\n", "query_t_cpt2_vec", "=", "query_cpt_vec", "[", "3", "]", "\n", "support_cpt_vec", "=", "support_sen_have_cpt", "[", "i", "]", "\n", "support_h_cpt1_vec", "=", "support_cpt_vec", "[", "0", "]", "\n", "support_h_cpt2_vec", "=", "support_cpt_vec", "[", "1", "]", "\n", "support_t_cpt1_vec", "=", "support_cpt_vec", "[", "2", "]", "\n", "support_t_cpt2_vec", "=", "support_cpt_vec", "[", "3", "]", "\n", "i_sen_cpt_vec", "=", "torch", ".", "cat", "(", "\n", "(", "sen_vec", ",", "query_h_cpt1_vec", ",", "query_h_cpt2_vec", ",", "query_t_cpt1_vec", ",", "query_t_cpt2_vec", ",", "\n", "support_h_cpt1_vec", ",", "support_h_cpt2_vec", ",", "support_t_cpt1_vec", ",", "support_t_cpt2_vec", ")", ",", "\n", "1", ")", "\n", "sen_cpt_vec", ".", "append", "(", "i_sen_cpt_vec", ")", "\n", "", "sen_cpt_vec", "=", "torch", ".", "cat", "(", "sen_cpt_vec", ",", "0", ")", "\n", "\n", "", "return", "sen_cpt_vec", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.BERTPAIRConceptSentenceEncoder.tokenize": [[745, 770], ["sentence_encoder.BERTPAIRConceptSentenceEncoder.tokenizer.convert_tokens_to_ids", "token.lower.lower.lower", "sentence_encoder.BERTPAIRConceptSentenceEncoder.tokenizer.tokenize", "tokens.append", "len", "tokens.append", "len", "tokens.append", "tokens.append"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "def", "tokenize", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ")", ":", "\n", "# token -> index", "\n", "# tokens = ['[CLS]']", "\n", "        ", "tokens", "=", "[", "]", "\n", "cur_pos", "=", "0", "\n", "pos1_in_index", "=", "0", "\n", "pos2_in_index", "=", "0", "\n", "for", "token", "in", "raw_tokens", ":", "\n", "            ", "token", "=", "token", ".", "lower", "(", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused0]'", ")", "\n", "pos1_in_index", "=", "len", "(", "tokens", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused1]'", ")", "\n", "pos2_in_index", "=", "len", "(", "tokens", ")", "\n", "", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused2]'", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused3]'", ")", "\n", "", "cur_pos", "+=", "1", "\n", "\n", "", "indexed_tokens", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "return", "indexed_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.BERTPAIRConceptSentenceEncoder.tokenize_concept": [[771, 827], ["h.lower.lower.lower", "fewshot_re_kit.conceptgraph_utils.instance2conept", "h2concept[].lower", "h2concept[].lower", "t.lower.lower.lower", "fewshot_re_kit.conceptgraph_utils.instance2conept", "t2concept[].lower", "t2concept[].lower", "tokens.append", "tokens.append", "tokens.append", "tokens.append", "sentence_encoder.BERTPAIRConceptSentenceEncoder.tokenizer.convert_tokens_to_ids", "token.lower.lower.lower", "sentence_encoder.BERTPAIRConceptSentenceEncoder.tokenizer.tokenize", "tokens.append", "sentence_encoder.BERTPAIRConceptSentenceEncoder.tokenizer.tokenize", "tokens.append", "sentence_encoder.BERTPAIRConceptSentenceEncoder.tokenizer.tokenize", "tokens.append", "sentence_encoder.BERTPAIRConceptSentenceEncoder.tokenizer.tokenize", "tokens.append", "sentence_encoder.BERTPAIRConceptSentenceEncoder.tokenizer.tokenize", "tokens.append", "len", "tokens.append", "len", "tokens.append", "tokens.append"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.instance2conept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.instance2conept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "def", "tokenize_concept", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ",", "h", ",", "t", ",", "ins2cpt", ")", ":", "\n", "# token -> index", "\n", "# tokens = ['[CLS]']", "\n", "        ", "tokens", "=", "[", "]", "\n", "cur_pos", "=", "0", "\n", "pos1_in_index", "=", "0", "\n", "pos2_in_index", "=", "0", "\n", "for", "token", "in", "raw_tokens", ":", "\n", "            ", "token", "=", "token", ".", "lower", "(", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused0]'", ")", "\n", "pos1_in_index", "=", "len", "(", "tokens", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused1]'", ")", "\n", "pos2_in_index", "=", "len", "(", "tokens", ")", "\n", "", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused2]'", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused3]'", ")", "\n", "", "cur_pos", "+=", "1", "\n", "", "'''\u6dfb\u52a0\u5b9e\u4f53\u7684\u6982\u5ff5\u5230tokens\u4e2d'''", "\n", "h", "=", "h", ".", "lower", "(", ")", "\n", "h2concept", "=", "instance2conept", "(", "ins2cpt", ",", "h", ")", "\n", "h2concept1", "=", "h2concept", "[", "0", "]", ".", "lower", "(", ")", "\n", "h2concept2", "=", "h2concept", "[", "1", "]", ".", "lower", "(", ")", "\n", "t", "=", "t", ".", "lower", "(", ")", "\n", "t2concept", "=", "instance2conept", "(", "ins2cpt", ",", "t", ")", "\n", "t2concept1", "=", "t2concept", "[", "0", "]", ".", "lower", "(", ")", "\n", "t2concept2", "=", "t2concept", "[", "1", "]", ".", "lower", "(", ")", "\n", "\n", "tokens", ".", "append", "(", "'[unused4]'", ")", "\n", "if", "(", "h2concept1", "==", "'unknowconcept1'", ")", "or", "(", "h2concept1", "==", "'unknowconcept2'", ")", ":", "\n", "            ", "tokens", ".", "append", "(", "h2concept1", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "h2concept1", ")", "\n", "\n", "", "tokens", ".", "append", "(", "'[unused5]'", ")", "\n", "if", "(", "h2concept2", "==", "'unknowconcept1'", ")", "or", "(", "h2concept2", "==", "'unknowconcept2'", ")", ":", "\n", "            ", "tokens", ".", "append", "(", "h2concept2", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "h2concept2", ")", "\n", "\n", "", "tokens", ".", "append", "(", "'[unused6]'", ")", "\n", "if", "(", "t2concept1", "==", "'unknowconcept1'", ")", "or", "(", "t2concept1", "==", "'unknowconcept2'", ")", ":", "\n", "            ", "tokens", ".", "append", "(", "t2concept1", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "t2concept1", ")", "\n", "\n", "", "tokens", ".", "append", "(", "'[unused7]'", ")", "\n", "if", "(", "t2concept2", "==", "'unknowconcept1'", ")", "or", "(", "t2concept2", "==", "'unknowconcept2'", ")", ":", "\n", "            ", "tokens", ".", "append", "(", "t2concept2", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "t2concept2", ")", "\n", "", "indexed_tokens", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "return", "indexed_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.RobertaSentenceEncoder.__init__": [[831, 837], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "transformers.RobertaModel.from_pretrained", "transformers.RobertaTokenizer.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "pretrain_path", ",", "max_length", ",", "cat_entity_rep", "=", "False", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "self", ".", "roberta", "=", "RobertaModel", ".", "from_pretrained", "(", "pretrain_path", ")", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "tokenizer", "=", "RobertaTokenizer", ".", "from_pretrained", "(", "'roberta-base'", ")", "\n", "self", ".", "cat_entity_rep", "=", "cat_entity_rep", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.RobertaSentenceEncoder.forward": [[838, 849], ["sentence_encoder.RobertaSentenceEncoder.roberta", "sentence_encoder.RobertaSentenceEncoder.roberta", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "inputs[].size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "if", "not", "self", ".", "cat_entity_rep", ":", "\n", "            ", "_", ",", "x", "=", "self", ".", "roberta", "(", "inputs", "[", "'word'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "\n", "return", "x", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "self", ".", "roberta", "(", "inputs", "[", "'word'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "\n", "tensor_range", "=", "torch", ".", "arange", "(", "inputs", "[", "'word'", "]", ".", "size", "(", ")", "[", "0", "]", ")", "\n", "h_state", "=", "outputs", "[", "0", "]", "[", "tensor_range", ",", "inputs", "[", "\"pos1\"", "]", "]", "\n", "t_state", "=", "outputs", "[", "0", "]", "[", "tensor_range", ",", "inputs", "[", "\"pos2\"", "]", "]", "\n", "state", "=", "torch", ".", "cat", "(", "(", "h_state", ",", "t_state", ")", ",", "-", "1", ")", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.RobertaSentenceEncoder.tokenize": [[850, 923], ["sentence_encoder.RobertaSentenceEncoder.tokenizer.tokenize", "sentence_encoder.RobertaSentenceEncoder.tokenize.getIns"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "", "def", "tokenize", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ")", ":", "\n", "        ", "def", "getIns", "(", "bped", ",", "bpeTokens", ",", "tokens", ",", "L", ")", ":", "\n", "            ", "resL", "=", "0", "\n", "tkL", "=", "\" \"", ".", "join", "(", "tokens", "[", ":", "L", "]", ")", "\n", "bped_tkL", "=", "\" \"", ".", "join", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "tkL", ")", ")", "\n", "if", "bped", ".", "find", "(", "bped_tkL", ")", "==", "0", ":", "\n", "                ", "resL", "=", "len", "(", "bped_tkL", ".", "split", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "tkL", "+=", "\" \"", "\n", "bped_tkL", "=", "\" \"", ".", "join", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "tkL", ")", ")", "\n", "if", "bped", ".", "find", "(", "bped_tkL", ")", "==", "0", ":", "\n", "                    ", "resL", "=", "len", "(", "bped_tkL", ".", "split", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "Exception", "(", "\"Cannot locate the position\"", ")", "\n", "", "", "return", "resL", "\n", "\n", "", "s", "=", "\" \"", ".", "join", "(", "raw_tokens", ")", "\n", "sst", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "s", ")", "\n", "headL", "=", "pos_head", "[", "0", "]", "\n", "headR", "=", "pos_head", "[", "-", "1", "]", "+", "1", "\n", "hiL", "=", "getIns", "(", "\" \"", ".", "join", "(", "sst", ")", ",", "sst", ",", "raw_tokens", ",", "headL", ")", "\n", "hiR", "=", "getIns", "(", "\" \"", ".", "join", "(", "sst", ")", ",", "sst", ",", "raw_tokens", ",", "headR", ")", "\n", "\n", "tailL", "=", "pos_tail", "[", "0", "]", "\n", "tailR", "=", "pos_tail", "[", "-", "1", "]", "+", "1", "\n", "tiL", "=", "getIns", "(", "\" \"", ".", "join", "(", "sst", ")", ",", "sst", ",", "raw_tokens", ",", "tailL", ")", "\n", "tiR", "=", "getIns", "(", "\" \"", ".", "join", "(", "sst", ")", ",", "sst", ",", "raw_tokens", ",", "tailR", ")", "\n", "\n", "E1b", "=", "'madeupword0000'", "\n", "E1e", "=", "'madeupword0001'", "\n", "E2b", "=", "'madeupword0002'", "\n", "E2e", "=", "'madeupword0003'", "\n", "ins", "=", "[", "(", "hiL", ",", "E1b", ")", ",", "(", "hiR", ",", "E1e", ")", ",", "(", "tiL", ",", "E2b", ")", ",", "(", "tiR", ",", "E2e", ")", "]", "\n", "ins", "=", "sorted", "(", "ins", ")", "\n", "pE1", "=", "0", "\n", "pE2", "=", "0", "\n", "pE1_", "=", "0", "\n", "pE2_", "=", "0", "\n", "for", "i", "in", "range", "(", "0", ",", "4", ")", ":", "\n", "            ", "sst", ".", "insert", "(", "ins", "[", "i", "]", "[", "0", "]", "+", "i", ",", "ins", "[", "i", "]", "[", "1", "]", ")", "\n", "if", "ins", "[", "i", "]", "[", "1", "]", "==", "E1b", ":", "\n", "                ", "pE1", "=", "ins", "[", "i", "]", "[", "0", "]", "+", "i", "\n", "", "elif", "ins", "[", "i", "]", "[", "1", "]", "==", "E2b", ":", "\n", "                ", "pE2", "=", "ins", "[", "i", "]", "[", "0", "]", "+", "i", "\n", "", "elif", "ins", "[", "i", "]", "[", "1", "]", "==", "E1e", ":", "\n", "                ", "pE1_", "=", "ins", "[", "i", "]", "[", "0", "]", "+", "i", "\n", "", "else", ":", "\n", "                ", "pE2_", "=", "ins", "[", "i", "]", "[", "0", "]", "+", "i", "\n", "", "", "pos1_in_index", "=", "pE1", "+", "1", "\n", "pos2_in_index", "=", "pE2", "+", "1", "\n", "sst", "=", "[", "'<s>'", "]", "+", "sst", "\n", "indexed_tokens", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "sst", ")", "\n", "\n", "# padding", "\n", "while", "len", "(", "indexed_tokens", ")", "<", "self", ".", "max_length", ":", "\n", "            ", "indexed_tokens", ".", "append", "(", "1", ")", "\n", "", "indexed_tokens", "=", "indexed_tokens", "[", ":", "self", ".", "max_length", "]", "\n", "\n", "# pos", "\n", "pos1", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "pos2", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "max_length", ")", ":", "\n", "            ", "pos1", "[", "i", "]", "=", "i", "-", "pos1_in_index", "+", "self", ".", "max_length", "\n", "pos2", "[", "i", "]", "=", "i", "-", "pos2_in_index", "+", "self", ".", "max_length", "\n", "\n", "# mask", "\n", "", "mask", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "mask", "[", ":", "len", "(", "sst", ")", "]", "=", "1", "\n", "\n", "pos1_in_index", "=", "min", "(", "self", ".", "max_length", ",", "pos1_in_index", ")", "\n", "pos2_in_index", "=", "min", "(", "self", ".", "max_length", ",", "pos2_in_index", ")", "\n", "\n", "return", "indexed_tokens", ",", "pos1_in_index", "-", "1", ",", "pos2_in_index", "-", "1", ",", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.RobertaPAIRSentenceEncoder.__init__": [[927, 934], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "transformers.RobertaForSequenceClassification.from_pretrained", "transformers.RobertaTokenizer.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "pretrain_path", ",", "max_length", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "self", ".", "roberta", "=", "RobertaForSequenceClassification", ".", "from_pretrained", "(", "\n", "pretrain_path", ",", "\n", "num_labels", "=", "2", ")", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "tokenizer", "=", "RobertaTokenizer", ".", "from_pretrained", "(", "'roberta-base'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.RobertaPAIRSentenceEncoder.forward": [[935, 938], ["sentence_encoder.RobertaPAIRSentenceEncoder.roberta"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "x", "=", "self", ".", "roberta", "(", "inputs", "[", "'word'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "[", "0", "]", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.sentence_encoder.RobertaPAIRSentenceEncoder.tokenize": [[939, 977], ["sentence_encoder.RobertaPAIRSentenceEncoder.tokenizer.tokenize", "sentence_encoder.RobertaPAIRSentenceEncoder.tokenize.getIns"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "def", "tokenize", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ")", ":", "\n", "        ", "def", "getIns", "(", "bped", ",", "bpeTokens", ",", "tokens", ",", "L", ")", ":", "\n", "            ", "resL", "=", "0", "\n", "tkL", "=", "\" \"", ".", "join", "(", "tokens", "[", ":", "L", "]", ")", "\n", "bped_tkL", "=", "\" \"", ".", "join", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "tkL", ")", ")", "\n", "if", "bped", ".", "find", "(", "bped_tkL", ")", "==", "0", ":", "\n", "                ", "resL", "=", "len", "(", "bped_tkL", ".", "split", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "tkL", "+=", "\" \"", "\n", "bped_tkL", "=", "\" \"", ".", "join", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "tkL", ")", ")", "\n", "if", "bped", ".", "find", "(", "bped_tkL", ")", "==", "0", ":", "\n", "                    ", "resL", "=", "len", "(", "bped_tkL", ".", "split", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "Exception", "(", "\"Cannot locate the position\"", ")", "\n", "", "", "return", "resL", "\n", "\n", "", "s", "=", "\" \"", ".", "join", "(", "raw_tokens", ")", "\n", "sst", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "s", ")", "\n", "headL", "=", "pos_head", "[", "0", "]", "\n", "headR", "=", "pos_head", "[", "-", "1", "]", "+", "1", "\n", "hiL", "=", "getIns", "(", "\" \"", ".", "join", "(", "sst", ")", ",", "sst", ",", "raw_tokens", ",", "headL", ")", "\n", "hiR", "=", "getIns", "(", "\" \"", ".", "join", "(", "sst", ")", ",", "sst", ",", "raw_tokens", ",", "headR", ")", "\n", "\n", "tailL", "=", "pos_tail", "[", "0", "]", "\n", "tailR", "=", "pos_tail", "[", "-", "1", "]", "+", "1", "\n", "tiL", "=", "getIns", "(", "\" \"", ".", "join", "(", "sst", ")", ",", "sst", ",", "raw_tokens", ",", "tailL", ")", "\n", "tiR", "=", "getIns", "(", "\" \"", ".", "join", "(", "sst", ")", ",", "sst", ",", "raw_tokens", ",", "tailR", ")", "\n", "\n", "E1b", "=", "'madeupword0000'", "\n", "E1e", "=", "'madeupword0001'", "\n", "E2b", "=", "'madeupword0002'", "\n", "E2e", "=", "'madeupword0003'", "\n", "ins", "=", "[", "(", "hiL", ",", "E1b", ")", ",", "(", "hiR", ",", "E1e", ")", ",", "(", "tiL", ",", "E2b", ")", ",", "(", "tiR", ",", "E2e", ")", "]", "\n", "ins", "=", "sorted", "(", "ins", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "4", ")", ":", "\n", "            ", "sst", ".", "insert", "(", "ins", "[", "i", "]", "[", "0", "]", "+", "i", ",", "ins", "[", "i", "]", "[", "1", "]", ")", "\n", "", "indexed_tokens", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "sst", ")", "\n", "return", "indexed_tokens", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREModel.__init__": [[27, 36], ["torch.nn.Module.__init__", "torch.nn.DataParallel", "torch.nn.CrossEntropyLoss"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "sentence_encoder", ")", ":", "\n", "        ", "'''\n        sentence_encoder: Sentence encoder\n        \n        You need to set self.cost as your own loss function.\n        '''", "\n", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "self", ".", "sentence_encoder", "=", "nn", ".", "DataParallel", "(", "sentence_encoder", ")", "\n", "self", ".", "cost", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREModel.forward": [[37, 47], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "support", ",", "query", ",", "N", ",", "K", ",", "Q", ")", ":", "\n", "        ", "'''\n        support: Inputs of the support set.\n        query: Inputs of the query set.\n        N: Num of classes\n        K: Num of instances for each class in the support set\n        Q: Num of instances for each class in the query set\n        return: logits, pred\n        '''", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREModel.loss": [[48, 56], ["logits.size", "framework.FewShotREModel.cost", "logits.view", "label.view"], "methods", ["None"], ["", "def", "loss", "(", "self", ",", "logits", ",", "label", ")", ":", "\n", "        ", "'''\n        logits: Logits with the size (..., class_num)\n        label: Label with whatever size. \n        return: [Loss] (A single value)\n        '''", "\n", "N", "=", "logits", ".", "size", "(", "-", "1", ")", "\n", "return", "self", ".", "cost", "(", "logits", ".", "view", "(", "-", "1", ",", "N", ")", ",", "label", ".", "view", "(", "-", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREModel.accuracy": [[57, 64], ["torch.mean", "pred.view", "label.view"], "methods", ["None"], ["", "def", "accuracy", "(", "self", ",", "pred", ",", "label", ")", ":", "\n", "        ", "'''\n        pred: Prediction results with whatever size\n        label: Label with whatever size\n        return: [Accuracy] (A single value)\n        '''", "\n", "return", "torch", ".", "mean", "(", "(", "pred", ".", "view", "(", "-", "1", ")", "==", "label", ".", "view", "(", "-", "1", ")", ")", ".", "type", "(", "torch", ".", "FloatTensor", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREFramework.__init__": [[68, 83], ["torch.nn.CrossEntropyLoss", "framework.FewShotREFramework.d.cuda"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "train_data_loader", ",", "val_data_loader", ",", "test_data_loader", ",", "adv_data_loader", "=", "None", ",", "adv", "=", "False", ",", "d", "=", "None", ")", ":", "\n", "        ", "'''\n        train_data_loader: DataLoader for training.\n        val_data_loader: DataLoader for validating.\n        test_data_loader: DataLoader for testing.\n        '''", "\n", "self", ".", "train_data_loader", "=", "train_data_loader", "\n", "self", ".", "val_data_loader", "=", "val_data_loader", "\n", "self", ".", "test_data_loader", "=", "test_data_loader", "\n", "self", ".", "adv_data_loader", "=", "adv_data_loader", "\n", "self", ".", "adv", "=", "adv", "\n", "if", "adv", ":", "\n", "            ", "self", ".", "adv_cost", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "self", ".", "d", "=", "d", "\n", "self", ".", "d", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREFramework.__load_model__": [[84, 95], ["os.path.isfile", "torch.load", "print", "Exception"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load"], ["", "", "def", "__load_model__", "(", "self", ",", "ckpt", ")", ":", "\n", "        ", "'''\n        ckpt: Path of the checkpoint\n        return: Checkpoint dict\n        '''", "\n", "if", "os", ".", "path", ".", "isfile", "(", "ckpt", ")", ":", "\n", "            ", "checkpoint", "=", "torch", ".", "load", "(", "ckpt", ")", "\n", "print", "(", "\"Successfully loaded checkpoint '%s'\"", "%", "ckpt", ")", "\n", "return", "checkpoint", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"No checkpoint found at '%s'\"", "%", "ckpt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREFramework.item": [[96, 105], ["torch.__version__.split", "x.item", "int", "int"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREFramework.item"], ["", "", "def", "item", "(", "self", ",", "x", ")", ":", "\n", "        ", "'''\n        PyTorch before and after 0.4\n        '''", "\n", "torch_version", "=", "torch", ".", "__version__", ".", "split", "(", "'.'", ")", "\n", "if", "int", "(", "torch_version", "[", "0", "]", ")", "==", "0", "and", "int", "(", "torch_version", "[", "1", "]", ")", "<", "4", ":", "\n", "            ", "return", "x", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "return", "x", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREFramework.train": [[106, 305], ["print", "model.train", "range", "print", "print", "print", "list", "transformers.AdamW", "transformers.get_linear_schedule_with_warmup", "pytorch_optim", "torch.optim.lr_scheduler.StepLR", "pytorch_optim", "model.state_dict", "state_dict.items", "amp.initialize", "framework.FewShotREFramework.d.train", "model.accuracy", "framework.FewShotREFramework.item", "framework.FewShotREFramework.item", "sys.stdout.flush", "model.named_parameters", "transformers.AdamW", "model.parameters", "pytorch_optim", "framework.FewShotREFramework.d.parameters", "framework.FewShotREFramework.__load_model__", "own_state[].copy_", "next", "torch.cuda.is_available", "model", "next", "torch.cuda.is_available", "model", "model.loss", "float", "loss.backward", "pytorch_optim.step", "torch.optim.lr_scheduler.StepLR.step", "pytorch_optim.zero_grad", "next", "torch.cuda.is_available", "model.sentence_encoder", "model.sentence_encoder", "torch.cat", "torch.cat.size", "torch.cat", "framework.FewShotREFramework.d", "framework.FewShotREFramework.adv_cost", "framework.FewShotREFramework.max", "framework.FewShotREFramework.backward", "pytorch_optim.step", "pytorch_optim.zero_grad", "pytorch_optim.zero_grad", "framework.FewShotREFramework.adv_cost", "framework.FewShotREFramework.backward", "pytorch_optim.step", "pytorch_optim.zero_grad", "pytorch_optim.zero_grad", "framework.FewShotREFramework.item", "sys.stdout.write", "sys.stdout.write", "framework.FewShotREFramework.eval", "model.train", "model.parameters", "label.cuda.cuda.cuda", "label.cuda.cuda.cuda", "amp.scale_loss", "scaled_loss.backward", "float", "float", "print", "torch.save", "batch[].cuda", "support[].cuda", "query[].cuda", "support_adv[].cuda", "torch.zeros().long().cuda", "torch.ones().long().cuda", "any", "model.state_dict", "any", "torch.zeros().long", "torch.ones().long", "torch.zeros", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREFramework.train", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREFramework.train", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREModel.accuracy", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREFramework.item", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREFramework.item", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREFramework.__load_model__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREModel.loss", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREFramework.item", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREFramework.eval", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREFramework.train"], ["", "", "def", "train", "(", "self", ",", "\n", "device", ",", "\n", "model", ",", "\n", "model_name", ",", "\n", "B", ",", "N_for_train", ",", "N_for_eval", ",", "K", ",", "Q", ",", "\n", "na_rate", "=", "0", ",", "\n", "learning_rate", "=", "1e-1", ",", "\n", "lr_step_size", "=", "20000", ",", "\n", "weight_decay", "=", "1e-5", ",", "\n", "train_iter", "=", "30000", ",", "\n", "val_iter", "=", "1000", ",", "\n", "val_step", "=", "2000", ",", "\n", "test_iter", "=", "3000", ",", "\n", "load_ckpt", "=", "None", ",", "\n", "save_ckpt", "=", "None", ",", "\n", "pytorch_optim", "=", "optim", ".", "SGD", ",", "\n", "bert_optim", "=", "False", ",", "\n", "warmup", "=", "True", ",", "\n", "warmup_step", "=", "300", ",", "\n", "grad_iter", "=", "1", ",", "\n", "fp16", "=", "False", ",", "\n", "pair", "=", "False", ",", "\n", "adv_dis_lr", "=", "1e-1", ",", "\n", "adv_enc_lr", "=", "1e-1", ",", "\n", ")", ":", "\n", "        ", "'''\n        model: a FewShotREModel instance\n        model_name: Name of the model\n        B: Batch size\n        N: Num of classes for each batch\n        K: Num of instances for each class in the support set\n        Q: Num of instances for each class in the query set\n        ckpt_dir: Directory of checkpoints\n        learning_rate: Initial learning rate\n        lr_step_size: Decay learning rate every lr_step_size steps\n        weight_decay: Rate of decaying weight\n        train_iter: Num of iterations of training\n        val_iter: Num of iterations of validating\n        val_step: Validate every val_step steps\n        test_iter: Num of iterations of testing\n        '''", "\n", "print", "(", "\"Start training...\"", ")", "\n", "\n", "# Init", "\n", "if", "bert_optim", ":", "\n", "            ", "print", "(", "'Use bert optim!'", ")", "\n", "parameters_to_optimize", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "parameters_to_optimize", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "parameters_to_optimize", "\n", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "parameters_to_optimize", "\n", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "optimizer", "=", "AdamW", "(", "parameters_to_optimize", ",", "lr", "=", "2e-5", ",", "correct_bias", "=", "False", ")", "\n", "if", "self", ".", "adv", ":", "\n", "                ", "optimizer_encoder", "=", "AdamW", "(", "parameters_to_optimize", ",", "lr", "=", "1e-5", ",", "correct_bias", "=", "False", ")", "\n", "", "scheduler", "=", "get_linear_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", "=", "warmup_step", ",", "\n", "num_training_steps", "=", "train_iter", ")", "\n", "# scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_step, t_total=train_iter)", "\n", "\n", "\n", "", "else", ":", "\n", "            ", "optimizer", "=", "pytorch_optim", "(", "model", ".", "parameters", "(", ")", ",", "\n", "learning_rate", ",", "weight_decay", "=", "weight_decay", ")", "\n", "if", "self", ".", "adv", ":", "\n", "                ", "optimizer_encoder", "=", "pytorch_optim", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "adv_enc_lr", ")", "\n", "", "scheduler", "=", "optim", ".", "lr_scheduler", ".", "StepLR", "(", "optimizer", ",", "step_size", "=", "lr_step_size", ")", "\n", "\n", "", "if", "self", ".", "adv", ":", "\n", "            ", "optimizer_dis", "=", "pytorch_optim", "(", "self", ".", "d", ".", "parameters", "(", ")", ",", "lr", "=", "adv_dis_lr", ")", "\n", "\n", "", "if", "load_ckpt", ":", "\n", "            ", "state_dict", "=", "self", ".", "__load_model__", "(", "load_ckpt", ")", "[", "'state_dict'", "]", "\n", "own_state", "=", "model", ".", "state_dict", "(", ")", "\n", "for", "name", ",", "param", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "                ", "if", "name", "not", "in", "own_state", ":", "\n", "                    ", "continue", "\n", "", "own_state", "[", "name", "]", ".", "copy_", "(", "param", ")", "\n", "", "start_iter", "=", "0", "\n", "", "else", ":", "\n", "            ", "start_iter", "=", "0", "\n", "\n", "", "if", "fp16", ":", "\n", "            ", "from", "apex", "import", "amp", "\n", "model", ",", "optimizer", "=", "amp", ".", "initialize", "(", "model", ",", "optimizer", ",", "opt_level", "=", "'O1'", ")", "\n", "", "model", ".", "train", "(", ")", "\n", "\n", "if", "self", ".", "adv", ":", "\n", "            ", "self", ".", "d", ".", "train", "(", ")", "\n", "\n", "# Training", "\n", "", "best_acc", "=", "0", "\n", "not_best_count", "=", "0", "# Stop training after several epochs without improvement.", "\n", "iter_loss", "=", "0.0", "\n", "iter_loss_dis", "=", "0.0", "\n", "iter_right", "=", "0.0", "\n", "iter_right_dis", "=", "0.0", "\n", "iter_sample", "=", "0.0", "\n", "for", "it", "in", "range", "(", "start_iter", ",", "start_iter", "+", "train_iter", ")", ":", "\n", "            ", "if", "pair", ":", "\n", "                ", "batch", ",", "label", "=", "next", "(", "self", ".", "train_data_loader", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                    ", "for", "k", "in", "batch", ":", "\n", "                        ", "batch", "[", "k", "]", "=", "batch", "[", "k", "]", ".", "cuda", "(", ")", "\n", "", "label", "=", "label", ".", "cuda", "(", ")", "\n", "", "logits", ",", "pred", "=", "model", "(", "batch", ",", "N_for_train", ",", "K", ",", "\n", "Q", "*", "N_for_train", "+", "na_rate", "*", "Q", ",", "device", ")", "\n", "", "else", ":", "\n", "                ", "support", ",", "query", ",", "label", "=", "next", "(", "self", ".", "train_data_loader", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                    ", "for", "k", "in", "support", ":", "\n", "                        ", "support", "[", "k", "]", "=", "support", "[", "k", "]", ".", "cuda", "(", ")", "\n", "", "for", "k", "in", "query", ":", "\n", "                        ", "query", "[", "k", "]", "=", "query", "[", "k", "]", ".", "cuda", "(", ")", "\n", "", "label", "=", "label", ".", "cuda", "(", ")", "\n", "\n", "", "logits", ",", "pred", "=", "model", "(", "support", ",", "query", ",", "\n", "N_for_train", ",", "K", ",", "Q", "*", "N_for_train", "+", "na_rate", "*", "Q", ")", "\n", "", "loss", "=", "model", ".", "loss", "(", "logits", ",", "label", ")", "/", "float", "(", "grad_iter", ")", "\n", "right", "=", "model", ".", "accuracy", "(", "pred", ",", "label", ")", "\n", "if", "fp16", ":", "\n", "                ", "with", "amp", ".", "scale_loss", "(", "loss", ",", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                    ", "scaled_loss", ".", "backward", "(", ")", "\n", "# torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), 10)", "\n", "", "", "else", ":", "\n", "                ", "loss", ".", "backward", "(", ")", "\n", "# torch.nn.utils.clip_grad_norm_(model.parameters(), 10)", "\n", "\n", "", "if", "it", "%", "grad_iter", "==", "0", ":", "\n", "                ", "optimizer", ".", "step", "(", ")", "\n", "scheduler", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "# Adv part", "\n", "", "if", "self", ".", "adv", ":", "\n", "                ", "support_adv", "=", "next", "(", "self", ".", "adv_data_loader", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                    ", "for", "k", "in", "support_adv", ":", "\n", "                        ", "support_adv", "[", "k", "]", "=", "support_adv", "[", "k", "]", ".", "cuda", "(", ")", "\n", "\n", "", "", "features_ori", "=", "model", ".", "sentence_encoder", "(", "support", ")", "\n", "features_adv", "=", "model", ".", "sentence_encoder", "(", "support_adv", ")", "\n", "features", "=", "torch", ".", "cat", "(", "[", "features_ori", ",", "features_adv", "]", ",", "0", ")", "\n", "total", "=", "features", ".", "size", "(", "0", ")", "\n", "dis_labels", "=", "torch", ".", "cat", "(", "[", "torch", ".", "zeros", "(", "(", "total", "//", "2", ")", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", ",", "\n", "torch", ".", "ones", "(", "(", "total", "//", "2", ")", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", "]", ",", "0", ")", "\n", "dis_logits", "=", "self", ".", "d", "(", "features", ")", "\n", "loss_dis", "=", "self", ".", "adv_cost", "(", "dis_logits", ",", "dis_labels", ")", "\n", "_", ",", "pred", "=", "dis_logits", ".", "max", "(", "-", "1", ")", "\n", "right_dis", "=", "float", "(", "(", "pred", "==", "dis_labels", ")", ".", "long", "(", ")", ".", "sum", "(", ")", ")", "/", "float", "(", "total", ")", "\n", "\n", "loss_dis", ".", "backward", "(", "retain_graph", "=", "True", ")", "\n", "optimizer_dis", ".", "step", "(", ")", "\n", "optimizer_dis", ".", "zero_grad", "(", ")", "\n", "optimizer_encoder", ".", "zero_grad", "(", ")", "\n", "\n", "loss_encoder", "=", "self", ".", "adv_cost", "(", "dis_logits", ",", "1", "-", "dis_labels", ")", "\n", "\n", "loss_encoder", ".", "backward", "(", "retain_graph", "=", "True", ")", "\n", "optimizer_encoder", ".", "step", "(", ")", "\n", "optimizer_dis", ".", "zero_grad", "(", ")", "\n", "optimizer_encoder", ".", "zero_grad", "(", ")", "\n", "\n", "iter_loss_dis", "+=", "self", ".", "item", "(", "loss_dis", ".", "data", ")", "\n", "iter_right_dis", "+=", "right_dis", "\n", "\n", "", "iter_loss", "+=", "self", ".", "item", "(", "loss", ".", "data", ")", "\n", "iter_right", "+=", "self", ".", "item", "(", "right", ".", "data", ")", "\n", "iter_sample", "+=", "1", "\n", "if", "self", ".", "adv", ":", "\n", "                ", "sys", ".", "stdout", ".", "write", "(", "\n", "'step: {0:4} | loss: {1:2.6f}, accuracy: {2:3.2f}%, dis_loss: {3:2.6f}, dis_acc: {4:2.6f}'", "\n", ".", "format", "(", "it", "+", "1", ",", "iter_loss", "/", "iter_sample", ",", "\n", "100", "*", "iter_right", "/", "iter_sample", ",", "\n", "iter_loss_dis", "/", "iter_sample", ",", "\n", "100", "*", "iter_right_dis", "/", "iter_sample", ")", "+", "'\\r'", ")", "\n", "", "else", ":", "\n", "                ", "sys", ".", "stdout", ".", "write", "(", "\n", "'step: {0:4} | loss: {1:2.6f}, accuracy: {2:3.2f}%'", ".", "format", "(", "it", "+", "1", ",", "iter_loss", "/", "iter_sample", ",", "\n", "100", "*", "iter_right", "/", "iter_sample", ")", "+", "'\\r'", ")", "\n", "", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "if", "(", "it", "+", "1", ")", "%", "val_step", "==", "0", ":", "\n", "                ", "acc", "=", "self", ".", "eval", "(", "device", ",", "model", ",", "B", ",", "N_for_eval", ",", "K", ",", "Q", ",", "val_iter", ",", "\n", "na_rate", "=", "na_rate", ",", "pair", "=", "pair", ")", "\n", "model", ".", "train", "(", ")", "\n", "if", "acc", ">", "best_acc", ":", "\n", "                    ", "print", "(", "'Best checkpoint'", ")", "\n", "torch", ".", "save", "(", "{", "'state_dict'", ":", "model", ".", "state_dict", "(", ")", "}", ",", "save_ckpt", ")", "\n", "best_acc", "=", "acc", "\n", "", "iter_loss", "=", "0.", "\n", "iter_loss_dis", "=", "0.", "\n", "iter_right", "=", "0.", "\n", "iter_right_dis", "=", "0.", "\n", "iter_sample", "=", "0.", "\n", "\n", "", "", "print", "(", "\"\\n####################\\n\"", ")", "\n", "print", "(", "\"Finish training \"", "+", "model_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREFramework.eval": [[306, 370], ["print", "model.eval", "print", "print", "torch.no_grad", "range", "print", "model.state_dict", "state_dict.items", "model.accuracy", "framework.FewShotREFramework.item", "sys.stdout.write", "sys.stdout.flush", "framework.FewShotREFramework.__load_model__", "own_state[].copy_", "next", "torch.cuda.is_available", "model", "next", "torch.cuda.is_available", "model", "label.cuda.cuda.cuda", "label.cuda.cuda.cuda", "batch[].cuda", "support[].cuda", "query[].cuda"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREFramework.eval", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREModel.accuracy", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREFramework.item", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.FewShotREFramework.__load_model__"], ["", "def", "eval", "(", "self", ",", "device", ",", "\n", "model", ",", "\n", "B", ",", "N", ",", "K", ",", "Q", ",", "\n", "eval_iter", ",", "\n", "na_rate", "=", "0", ",", "\n", "pair", "=", "False", ",", "\n", "ckpt", "=", "None", ")", ":", "\n", "        ", "'''\n        model: a FewShotREModel instance\n        B: Batch size\n        N: Num of classes for each batch\n        K: Num of instances for each class in the support set\n        Q: Num of instances for each class in the query set\n        eval_iter: Num of iterations\n        ckpt: Checkpoint path. Set as None if using current model parameters.\n        return: Accuracy\n        '''", "\n", "print", "(", "\"\"", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "if", "ckpt", "is", "None", ":", "\n", "            ", "print", "(", "\"Use val dataset\"", ")", "\n", "eval_dataset", "=", "self", ".", "val_data_loader", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Use test dataset\"", ")", "\n", "if", "ckpt", "!=", "'none'", ":", "\n", "                ", "state_dict", "=", "self", ".", "__load_model__", "(", "ckpt", ")", "[", "'state_dict'", "]", "\n", "own_state", "=", "model", ".", "state_dict", "(", ")", "\n", "for", "name", ",", "param", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "                    ", "if", "name", "not", "in", "own_state", ":", "\n", "                        ", "continue", "\n", "", "own_state", "[", "name", "]", ".", "copy_", "(", "param", ")", "\n", "", "", "eval_dataset", "=", "self", ".", "test_data_loader", "\n", "\n", "", "iter_right", "=", "0.0", "\n", "iter_sample", "=", "0.0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "it", "in", "range", "(", "eval_iter", ")", ":", "\n", "                ", "if", "pair", ":", "\n", "                    ", "batch", ",", "label", "=", "next", "(", "eval_dataset", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                        ", "for", "k", "in", "batch", ":", "\n", "                            ", "batch", "[", "k", "]", "=", "batch", "[", "k", "]", ".", "cuda", "(", ")", "\n", "", "label", "=", "label", ".", "cuda", "(", ")", "\n", "", "logits", ",", "pred", "=", "model", "(", "batch", ",", "N", ",", "K", ",", "Q", "*", "N", "+", "Q", "*", "na_rate", ",", "device", ")", "\n", "", "else", ":", "\n", "                    ", "support", ",", "query", ",", "label", "=", "next", "(", "eval_dataset", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                        ", "for", "k", "in", "support", ":", "\n", "                            ", "support", "[", "k", "]", "=", "support", "[", "k", "]", ".", "cuda", "(", ")", "\n", "", "for", "k", "in", "query", ":", "\n", "                            ", "query", "[", "k", "]", "=", "query", "[", "k", "]", ".", "cuda", "(", ")", "\n", "", "label", "=", "label", ".", "cuda", "(", ")", "\n", "", "logits", ",", "pred", "=", "model", "(", "support", ",", "query", ",", "N", ",", "K", ",", "Q", "*", "N", "+", "Q", "*", "na_rate", ")", "\n", "\n", "", "right", "=", "model", ".", "accuracy", "(", "pred", ",", "label", ")", "\n", "iter_right", "+=", "self", ".", "item", "(", "right", ".", "data", ")", "\n", "iter_sample", "+=", "1", "\n", "\n", "sys", ".", "stdout", ".", "write", "(", "\n", "'[EVAL] step: {0:4} | accuracy: {1:3.2f}%'", ".", "format", "(", "it", "+", "1", ",", "100", "*", "iter_right", "/", "iter_sample", ")", "+", "'\\r'", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "", "print", "(", "\"\"", ")", "\n", "", "return", "iter_right", "/", "iter_sample", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.framework.warmup_linear": [[19, 24], ["None"], "function", ["None"], ["def", "warmup_linear", "(", "global_step", ",", "warmup_step", ")", ":", "\n", "    ", "if", "global_step", "<", "warmup_step", ":", "\n", "        ", "return", "global_step", "/", "warmup_step", "\n", "", "else", ":", "\n", "        ", "return", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.conceptgraph2id": [[10, 52], ["print", "os.path.join", "os.path.join", "print", "datetime.datetime.now", "datetime.datetime.now", "os.path.exists", "print", "print", "open", "tqdm.tqdm", "open", "pickle.dump", "line.split", "entity.strip.strip", "int"], "function", ["None"], ["def", "conceptgraph2id", "(", "\n", "root", "=", "'../data/conceptgraphEmbedding/TransE_l2_concetgraph_2/'", ",", "\n", "name1", "=", "'entities'", ",", "name2", "=", "'relations'", ",", "format", "=", "\".tsv\"", ")", ":", "\n", "    ", "'''\n    \u52a0\u8f7d conceptgraph\u4e2d\u5b9e\u4f53\u548c\u5173\u7cfb\u53ca\u5176\u5bf9\u5e94\u7684id\n    '''", "\n", "\n", "print", "(", "'starting:'", ",", "datetime", ".", "now", "(", ")", ")", "\n", "entity2id", "=", "{", "}", "\n", "relation2id", "=", "{", "}", "\n", "path1", "=", "os", ".", "path", ".", "join", "(", "root", ",", "name1", "+", "format", ")", "\n", "path2", "=", "os", ".", "path", ".", "join", "(", "root", ",", "name2", "+", "format", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path1", ")", ":", "\n", "        ", "print", "(", "path1", ")", "\n", "print", "(", "\"[ERROR] {} file does not exist!\"", ".", "format", "(", "name1", ")", ")", "\n", "assert", "(", "0", ")", "\n", "# if not os.path.exists(path2):", "\n", "#     print(\"[ERROR] {} file does not exist!\".format(name2))", "\n", "#     assert (0)", "\n", "", "with", "open", "(", "path1", ",", "mode", "=", "'r'", ")", "as", "f1", ":", "\n", "        ", "for", "line", "in", "tqdm", "(", "f1", ")", ":", "\n", "            ", "id1", ",", "entity", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "entity", "=", "entity", ".", "strip", "(", "'\\n'", ")", "\n", "entity2id", "[", "entity", "]", "=", "int", "(", "id1", ")", "\n", "# with open(path2, mode='r') as f2:", "\n", "#     for line in tqdm(f2):", "\n", "#         id2, relaiton = line.split('\\t')", "\n", "#         relaiton = relaiton.strip('\\n')", "\n", "#         relation2id[relaiton] = int(id2)", "\n", "", "", "print", "(", "'store entity2id'", ")", "\n", "with", "open", "(", "root", "+", "'entities2id.pickle'", ",", "mode", "=", "'wb'", ")", "as", "f3", ":", "\n", "        ", "pickle", ".", "dump", "(", "entity2id", ",", "f3", ")", "\n", "# print('store relaiont2id')", "\n", "# with open(root + 'relations2id.pickle', mode='wb') as f4:", "\n", "#     pickle.dump(relation2id, f4)", "\n", "# with open(root + 'entities2id.json', mode='w') as f3:", "\n", "#     json.dump(entity2id, f3)", "\n", "# print('store relaiont2id')", "\n", "# with open(root + 'relations2id.json', mode='w') as f4:", "\n", "#     json.dump(relation2id, f4)", "\n", "\n", "", "loadingtime", "=", "datetime", ".", "now", "(", ")", "\n", "# with open(root + 'entities2id.pickle', mode='rb') as f5:", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.entity2vec": [[69, 74], ["conceptgraph_utils.entity2id"], "function", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.entity2id"], ["", "def", "entity2vec", "(", "entity", ":", "str", ",", "entity2id", ",", "entityEmbedding", ")", ":", "\n", "    ", "entityID", "=", "entity2id", "[", "entity", "]", "\n", "entityVec", "=", "entityEmbedding", "[", "entityID", ",", ":", "]", "\n", "\n", "return", "entityVec", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.relation2vec": [[76, 80], ["None"], "function", ["None"], ["", "def", "relation2vec", "(", "relation", ":", "str", ",", "relation2id", ",", "relationEmbedding", ")", ":", "\n", "    ", "relaiontID", "=", "relation2id", "[", "relation", "]", "\n", "relationVec", "=", "relationEmbedding", "[", "relaiontID", ",", ":", "]", "\n", "return", "relationVec", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.conceptgraphInitial": [[82, 103], ["datetime.datetime.now", "datetime.datetime.now", "print", "tqdm.tqdm", "pbar.update", "pbar.update", "numpy.load", "pbar.update", "numpy.load", "pbar.update", "open", "pickle.load", "open", "pickle.load"], "function", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load"], ["", "def", "conceptgraphInitial", "(", "root", "=", "'../data/conceptgraphEmbedding/TransE_l2_concetgraph_2/'", ")", ":", "\n", "    ", "'''\u52a0\u8f7dentity2id\uff0c relaion2id\uff0centityEmbedding\uff0crelationEmbedding \u6587\u4ef6'''", "\n", "loadingtime", "=", "datetime", ".", "now", "(", ")", "\n", "\n", "with", "tqdm", "(", "total", "=", "4", ",", "desc", "=", "f'loading entity2id\uff0crelaion2id\uff0centityEmbedding\uff0crelationEmbedding file'", ")", "as", "pbar", ":", "\n", "        ", "with", "open", "(", "root", "+", "'entities2id.pickle'", ",", "mode", "=", "'rb'", ")", "as", "f5", ":", "\n", "            ", "entity2id", "=", "pickle", ".", "load", "(", "f5", ")", "\n", "", "pbar", ".", "update", "(", "1", ")", "\n", "\n", "with", "open", "(", "root", "+", "'relations2id.pickle'", ",", "mode", "=", "'rb'", ")", "as", "f6", ":", "\n", "            ", "relation2id", "=", "pickle", ".", "load", "(", "f6", ")", "\n", "", "pbar", ".", "update", "(", "1", ")", "\n", "\n", "entityEmbedding", "=", "np", ".", "load", "(", "root", "+", "'concetgraph_TransE_l2_entity.npy'", ")", "\n", "pbar", ".", "update", "(", "1", ")", "\n", "\n", "relaitonEmbedding", "=", "np", ".", "load", "(", "root", "+", "'concetgraph_TransE_l2_relation.npy'", ")", "\n", "pbar", ".", "update", "(", "1", ")", "\n", "", "donetime", "=", "datetime", ".", "now", "(", ")", "\n", "print", "(", "'initializing time'", ",", "donetime", "-", "loadingtime", ")", "\n", "return", "entity2id", ",", "relation2id", ",", "entityEmbedding", ",", "relaitonEmbedding", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.loadingConceptGraphEntity": [[105, 116], ["tqdm.tqdm", "pbar.update", "numpy.load", "pbar.update", "open", "pickle.load"], "function", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load"], ["", "def", "loadingConceptGraphEntity", "(", "root", "=", "'../data/conceptgraphEmbedding/TransE_l2_concetgraph_2/'", ")", ":", "\n", "    ", "'''\u52a0\u8f7dconceptgraph\u4e2d\u5b9e\u4f53\u4ee5\u53caembedding'''", "\n", "\n", "with", "tqdm", "(", "total", "=", "2", ",", "desc", "=", "f'loading entity2id, entityEmbeddingfile'", ")", "as", "pbar", ":", "\n", "        ", "with", "open", "(", "root", "+", "'entities2id.pickle'", ",", "mode", "=", "'rb'", ")", "as", "f5", ":", "\n", "            ", "entity2id", "=", "pickle", ".", "load", "(", "f5", ")", "\n", "", "pbar", ".", "update", "(", "1", ")", "\n", "\n", "entityEmbedding", "=", "np", ".", "load", "(", "root", "+", "'concetgraph_TransE_l2_entity.npy'", ")", "\n", "pbar", ".", "update", "(", "1", ")", "\n", "", "return", "entity2id", ",", "entityEmbedding", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.loadingConceptGraphEntity2ID": [[118, 125], ["tqdm.tqdm", "pbar.update", "open", "pickle.load"], "function", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load"], ["", "def", "loadingConceptGraphEntity2ID", "(", "root", ",", "path", "=", "'conceptgraphEmbedding/TransE_l2_concetgraph_2/'", ")", ":", "\n", "    ", "file", "=", "root", "+", "path", "+", "'entities2id.pickle'", "\n", "with", "tqdm", "(", "total", "=", "1", ",", "desc", "=", "f'loading entity2id in conceptgraph'", ")", "as", "pbar", ":", "\n", "        ", "with", "open", "(", "file", ",", "mode", "=", "'rb'", ")", "as", "f5", ":", "\n", "            ", "entity2id", "=", "pickle", ".", "load", "(", "f5", ")", "\n", "", "pbar", ".", "update", "(", "1", ")", "\n", "", "return", "entity2id", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.loadingInstance2concept": [[127, 133], ["tqdm.tqdm", "pbar.update", "open", "pickle.load"], "function", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load"], ["", "def", "loadingInstance2concept", "(", "path", "=", "'../data/conceptgraph/instance2concept.pickle'", ")", ":", "\n", "    ", "with", "tqdm", "(", "total", "=", "1", ",", "desc", "=", "f'loading Instance2concept file'", ")", "as", "pbar", ":", "\n", "        ", "with", "open", "(", "path", ",", "mode", "=", "'rb'", ")", "as", "f", ":", "\n", "            ", "instance2concept", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "pbar", ".", "update", "(", "1", ")", "\n", "", "return", "instance2concept", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.instance2conept": [[135, 145], ["ins2cpt.get", "len", "ins2cpt.get.append"], "function", ["None"], ["", "def", "instance2conept", "(", "ins2cpt", ":", "dict", ",", "instance", ":", "str", ",", "top", "=", "2", ")", "->", "list", ":", "\n", "    ", "'''\u7ed9\u5b9a\u5b9e\u4f8b\uff0c\u8fd4\u56de\u5176\u5bf9\u5e94\u7684\u6982\u5ff5\uff0c\u6700\u591a\u4e24\u4e2a'''", "\n", "concept", "=", "ins2cpt", ".", "get", "(", "instance", ")", "\n", "if", "concept", "==", "None", ":", "\n", "        ", "concept", "=", "[", "'unknowConcept1'", ",", "'unknowConcept2'", "]", "\n", "", "elif", "len", "(", "concept", ")", "==", "1", ":", "\n", "        ", "concept", ".", "append", "(", "'unknowConcept1'", ")", "\n", "", "else", ":", "\n", "        ", "concept", "=", "concept", "[", ":", "top", "]", "\n", "", "return", "concept", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.instance2coneptPlus": [[147, 162], ["ins2cpt.get", "conceptgraph_utils.word2concept", "len"], "function", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.word2concept"], ["", "def", "instance2coneptPlus", "(", "ins2cpt", ":", "dict", ",", "instance", ":", "str", ",", "top", "=", "2", ")", "->", "list", ":", "\n", "    ", "'''\u7ed9\u5b9a\u5b9e\u4f8b\uff0c\u8fd4\u56de\u5176\u5bf9\u5e94\u7684\u6982\u5ff5\uff0c\u6700\u591atop\u4e2a'''", "\n", "concept", "=", "ins2cpt", ".", "get", "(", "instance", ")", "\n", "if", "concept", "==", "None", ":", "\n", "        ", "'''\u67e5\u627e\u5b9e\u4f53\u4e2d\u7684\u8bcd\u5728\u77e5\u8bc6\u5e93\u4e2d\u7684\u6982\u5ff5'''", "\n", "cpt_list", "=", "word2concept", "(", "ins2cpt", ",", "instance", ",", "top", "=", "top", ")", "\n", "if", "len", "(", "cpt_list", ")", "==", "0", ":", "\n", "            ", "concept", "=", "[", "'unknowConcept'", "]", "\n", "", "else", ":", "\n", "# print('zhao dao la')", "\n", "# print(instance,cpt_list)", "\n", "            ", "concept", "=", "cpt_list", "\n", "", "", "else", ":", "\n", "        ", "concept", "=", "concept", "[", ":", "top", "]", "\n", "", "return", "concept", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.entity2id": [[164, 180], ["os.path.join", "os.path.exists", "print", "print", "open", "line.split", "int", "entity.strip.strip"], "function", ["None"], ["", "def", "entity2id", "(", "root", "=", "'../data/conceptgraphEmbedding/TransE_l2_concetgraph_2'", ",", "name", "=", "'entities'", ")", ":", "\n", "    ", "path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "name", "+", "\".tsv\"", ")", "\n", "e2id", "=", "{", "}", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "        ", "print", "(", "'file path'", ",", "path", ")", "\n", "print", "(", "\"[ERROR] Data file does not exist!\"", ")", "\n", "assert", "(", "0", ")", "\n", "\n", "", "with", "open", "(", "path", ",", "mode", "=", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "entityID", ",", "entity", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "entityID", "=", "int", "(", "entityID", ")", "\n", "entity", "=", "entity", ".", "strip", "(", "'\\n'", ")", "\n", "e2id", "[", "entity", "]", "=", "entityID", "\n", "\n", "", "", "return", "e2id", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.generateZeroVec": [[182, 185], ["numpy.zeros"], "function", ["None"], ["", "def", "generateZeroVec", "(", "shape", "=", "(", "256", ",", ")", ",", "dtype", "=", "\"float32\"", ")", ":", "\n", "    ", "zeroVec", "=", "np", ".", "zeros", "(", "shape", ",", "dtype", ")", "\n", "return", "zeroVec", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.concept2vec": [[187, 195], ["conceptgraph_utils.generateZeroVec", "conceptgraph_utils.entity2vec", "conceptgraph_utils.entity2id", "conceptgraph_utils.entity2id", "conceptgraph_utils.entity2id", "conceptgraph_utils.entity2id"], "function", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.generateZeroVec", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.entity2vec", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.entity2id", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.entity2id", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.entity2id", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.entity2id"], ["", "def", "concept2vec", "(", "cpt", ":", "list", ",", "entity2id", ",", "entityEmbedding", ")", ":", "\n", "    ", "cpt2vec", "=", "{", "}", "\n", "for", "cpt", "in", "cpt", ":", "\n", "        ", "if", "(", "cpt", "==", "'unknowConcept1'", ")", "or", "(", "cpt", "==", "'unknowConcept2'", ")", ":", "\n", "            ", "cpt2vec", "[", "cpt", "]", "=", "generateZeroVec", "(", ")", "\n", "", "else", ":", "\n", "            ", "cpt2vec", "[", "cpt", "]", "=", "entity2vec", "(", "cpt", ",", "entity2id", ",", "entityEmbedding", ")", "\n", "", "", "return", "cpt2vec", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.getConceptVec": [[197, 230], ["conceptgraph_utils.instance2conept", "conceptgraph_utils.instance2conept", "conceptgraph_utils.instance2conept", "conceptgraph_utils.instance2conept", "conceptgraph_utils.concept2vec", "conceptgraph_utils.concept2vec", "conceptgraph_utils.concept2vec", "conceptgraph_utils.concept2vec", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "np.array.values", "np.array.values", "np.array.values", "np.array.values"], "function", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.instance2conept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.instance2conept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.instance2conept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.instance2conept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.concept2vec", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.concept2vec", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.concept2vec", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.concept2vec"], ["", "def", "getConceptVec", "(", "entities", ":", "list", ",", "ins2cpt", ":", "dict", ",", "entity2id", ":", "dict", ",", "entityEmbedding", ")", ":", "\n", "    ", "'''\n    \u4f20\u5165\u5b9e\u4f53\uff0c\u67e5\u627e\u5b9e\u4f53\u5bf9\u5e94\u7684\u6982\u5ff5\uff0c\u7136\u540e\u8fd4\u56de\u6982\u5ff5\u5bf9\u5e94\u7684concept embedding\n    entities:((h1,r1),(h2,r2))\n    entity2id\u548centityEmbedding\u4e3a\u8bad\u7ec3conceptgraph pre-training kg embedding\u4ea7\u751f\u7684\u6587\u4ef6\n    '''", "\n", "h1", "=", "entities", "[", "0", "]", "[", "0", "]", "\n", "r1", "=", "entities", "[", "0", "]", "[", "1", "]", "\n", "h2", "=", "entities", "[", "1", "]", "[", "0", "]", "\n", "r2", "=", "entities", "[", "1", "]", "[", "1", "]", "\n", "\n", "h1_cpt", "=", "instance2conept", "(", "ins2cpt", ",", "h1", ",", "top", "=", "2", ")", "\n", "r1_cpt", "=", "instance2conept", "(", "ins2cpt", ",", "r1", ",", "top", "=", "2", ")", "\n", "h2_cpt", "=", "instance2conept", "(", "ins2cpt", ",", "h2", ",", "top", "=", "2", ")", "\n", "r2_cpt", "=", "instance2conept", "(", "ins2cpt", ",", "r2", ",", "top", "=", "2", ")", "\n", "\n", "h1_cpt2vec", "=", "concept2vec", "(", "h1_cpt", ",", "entity2id", ",", "entityEmbedding", ")", "\n", "r1_cpt2vec", "=", "concept2vec", "(", "r1_cpt", ",", "entity2id", ",", "entityEmbedding", ")", "\n", "h2_cpt2vec", "=", "concept2vec", "(", "h2_cpt", ",", "entity2id", ",", "entityEmbedding", ")", "\n", "r2_cpt2vec", "=", "concept2vec", "(", "r2_cpt", ",", "entity2id", ",", "entityEmbedding", ")", "\n", "\n", "'''\u53d6\u51fa\u5411\u91cfconcept embedding\uff0c\u53bb\u9664concept name'''", "\n", "h1_cpt2vec", "=", "[", "vec", "for", "vec", "in", "h1_cpt2vec", ".", "values", "(", ")", "]", "\n", "r1_cpt2vec", "=", "[", "vec", "for", "vec", "in", "r1_cpt2vec", ".", "values", "(", ")", "]", "\n", "h2_cpt2vec", "=", "[", "vec", "for", "vec", "in", "h2_cpt2vec", ".", "values", "(", ")", "]", "\n", "r2_cpt2vec", "=", "[", "vec", "for", "vec", "in", "r2_cpt2vec", ".", "values", "(", ")", "]", "\n", "\n", "h1_cpt2vec", "=", "np", ".", "array", "(", "h1_cpt2vec", ")", "\n", "r1_cpt2vec", "=", "np", ".", "array", "(", "r1_cpt2vec", ")", "\n", "h2_cpt2vec", "=", "np", ".", "array", "(", "h2_cpt2vec", ")", "\n", "r2_cpt2vec", "=", "np", ".", "array", "(", "r2_cpt2vec", ")", "\n", "\n", "return", "(", "h1_cpt2vec", ",", "r1_cpt2vec", ",", "h2_cpt2vec", ",", "r2_cpt2vec", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.getBatchConceptVec": [[232, 242], ["numpy.array", "conceptgraph_utils.getConceptVec", "np.array.append"], "function", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.getConceptVec"], ["", "def", "getBatchConceptVec", "(", "batchEntities", ":", "list", ",", "ins2cpt", ":", "dict", ",", "e2id", ":", "dict", ",", "entityEmbedding", ")", ":", "\n", "    ", "'''\n    \u83b7\u53d6\u4e00\u4e2aBatch\u91cc\u53e5\u5b50\u7684\u5934\u5c3e\u5b9e\u4f53\u7684\u5bf9\u5e94\u7684concept\u7684kg embedding\n    '''", "\n", "batch_h_r2vec", "=", "[", "]", "\n", "for", "entities", "in", "batchEntities", "[", "0", "]", ":", "\n", "        ", "(", "h1_cpt2vec", ",", "r1_cpt2vec", ",", "h2_cpt2vec", ",", "r2_cpt2vec", ")", "=", "getConceptVec", "(", "entities", ",", "ins2cpt", ",", "e2id", ",", "entityEmbedding", ")", "\n", "batch_h_r2vec", ".", "append", "(", "(", "h1_cpt2vec", ",", "r1_cpt2vec", ",", "h2_cpt2vec", ",", "r2_cpt2vec", ")", ")", "\n", "", "batch_h_r2vec", "=", "np", ".", "array", "(", "batch_h_r2vec", ")", "\n", "return", "batch_h_r2vec", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.word2concept": [[244, 256], ["word.split.split", "instance2concept.get", "concept.append"], "function", ["None"], ["", "def", "word2concept", "(", "instance2concept", ",", "word", ",", "top", "=", "2", ")", ":", "\n", "    ", "'''\u7ed9\u5b9a\u4e00\u4e2a\u8bcd\u67e5\u627e\u5176\u5728conceptgrap\u4e2d\u7684concept'''", "\n", "word", "=", "word", ".", "split", "(", "' '", ")", "\n", "concept", "=", "[", "]", "\n", "for", "w", "in", "word", ":", "\n", "        ", "cpt", "=", "instance2concept", ".", "get", "(", "w", ")", "\n", "if", "cpt", "==", "None", ":", "\n", "            ", "continue", "\n", "", "else", ":", "\n", "            ", "for", "c", "in", "cpt", "[", ":", "top", "]", ":", "\n", "                ", "concept", ".", "append", "(", "c", ")", "\n", "", "", "", "return", "concept", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load": [[258, 272], ["conceptgraph_utils.load_text", "conceptgraph_utils.load_binary", "print", "conceptgraph_utils.get_all_titles"], "function", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load_text", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load_binary", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.get_all_titles"], ["", "def", "load", "(", "model_path", "=", "None", ",", "first", "=", "0", ",", "normalize", "=", "False", ",", "log_every", "=", "0", ",", "load_concepts", "=", "True", ",", "format", "=", "'bin'", ",", "\n", "concepts_pattern", "=", "'id[0-9]+di'", ")", ":", "\n", "    ", "\"\"\"\n    load word2vec vocabulary vectors from binary/text file\n    \"\"\"", "\n", "if", "format", "==", "'txt'", ":", "\n", "        ", "return", "load_text", "(", "model_path", ",", "first", ",", "load_concepts", ",", "normalize", ",", "log_every", ",", "concepts_pattern", ")", "\n", "", "else", ":", "\n", "        ", "return", "load_binary", "(", "model_path", ",", "first", ",", "load_concepts", ",", "normalize", ",", "log_every", ",", "concepts_pattern", ")", "\n", "\n", "", "if", "log_every", ">", "0", ":", "\n", "        ", "print", "(", "'done loading!'", ")", "\n", "\n", "", "return", "titles", ",", "redirects", ",", "vector_size", ",", "W", ",", "id2word", ",", "word2id", ",", "get_all_titles", "(", "W", ",", "titles", ",", "redirects", ",", "word2id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load_text": [[274, 276], ["None"], "function", ["None"], ["", "def", "load_text", "(", ")", ":", "\n", "    ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load_binary": [[278, 342], ["re.compile", "open", "pickle.load", "pickle.load", "len", "numpy.zeros", "range", "print", "print", "print", "inp.tell", "vectors_pairs.extend", "len", "print", "id2word.append", "print", "conceptgraph_utils.get_all_titles", "os.fstat", "pickle.load", "print", "len", "len", "inp.fileno", "len", "re.compile.match", "numpy.linalg.norm"], "function", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.get_all_titles", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load"], ["", "def", "load_binary", "(", "model_path", "=", "None", ",", "first", "=", "0", ",", "load_concepts", "=", "True", ",", "normalize", "=", "False", ",", "log_every", "=", "0", ",", "\n", "concepts_pattern", "=", "'id[0-9]+di'", ")", ":", "\n", "    ", "\"\"\"\n    load word2vec vocabulary vectors from binary file\n    \u8fd9\u90e8\u5206\u4ee3\u7801\u6e90\u4e8e\u8bba\u6587Beyond Word Embeddings: Learning Entity and Concept Representations from Large Scale Knowledge Bases\u7684\u5f00\u6e90\u4ee3\u7801\n    \"\"\"", "\n", "import", "pickle", "\n", "import", "numpy", "as", "np", "\n", "import", "re", "\n", "import", "os", "\n", "\n", "if", "load_concepts", "==", "False", ":", "\n", "        ", "concepts_re", "=", "re", ".", "compile", "(", "concepts_pattern", ")", "\n", "\n", "", "with", "open", "(", "model_path", ",", "'rb'", ")", "as", "inp", ":", "\n", "        ", "if", "log_every", ">", "0", ":", "\n", "            ", "print", "(", "'start loading!'", ")", "\n", "\n", "# read titles meta", "\n", "", "titles", "=", "pickle", ".", "load", "(", "inp", ")", "\n", "if", "log_every", ">", "0", ":", "\n", "            ", "print", "(", "'loaded ({0}) titles'", ".", "format", "(", "len", "(", "titles", ")", ")", ")", "\n", "# read redirects meta", "\n", "", "redirects", "=", "pickle", ".", "load", "(", "inp", ")", "\n", "if", "log_every", ">", "0", ":", "\n", "            ", "print", "(", "'loaded ({0}) redirects'", ".", "format", "(", "len", "(", "redirects", ")", ")", ")", "\n", "# read vectors", "\n", "", "vectors_pairs", "=", "[", "]", "\n", "while", "inp", ".", "tell", "(", ")", "<", "os", ".", "fstat", "(", "inp", ".", "fileno", "(", ")", ")", ".", "st_size", ":", "\n", "            ", "vectors_pairs", ".", "extend", "(", "pickle", ".", "load", "(", "inp", ")", ")", "\n", "", "num", "=", "len", "(", "vectors_pairs", ")", "\n", "if", "num", ">", "0", ":", "\n", "            ", "vector_size", "=", "len", "(", "vectors_pairs", "[", "0", "]", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "vector_size", "=", "0", "\n", "", "if", "log_every", ">", "0", ":", "\n", "            ", "print", "(", "'loading ({0}) vectors of size ({1})'", ".", "format", "(", "len", "(", "vectors_pairs", ")", ",", "vector_size", ")", ")", "\n", "", "W", "=", "np", ".", "zeros", "(", "(", "num", ",", "vector_size", ")", ")", "\n", "id2word", "=", "[", "]", "\n", "word2id", "=", "{", "}", "\n", "total", "=", "0", "\n", "for", "i", "in", "range", "(", "num", ")", ":", "\n", "            ", "term", "=", "vectors_pairs", "[", "i", "]", "[", "0", "]", "\n", "if", "load_concepts", "==", "False", ":", "\n", "                ", "if", "concepts_re", ".", "match", "(", "term", ")", "!=", "None", ":", "\n", "                    ", "continue", "\n", "", "", "vec", "=", "vectors_pairs", "[", "i", "]", "[", "1", "]", "\n", "W", "[", "total", "]", "=", "vec", "\n", "id2word", ".", "append", "(", "term", ")", "\n", "word2id", "[", "term", "]", "=", "total", "\n", "total", "+=", "1", "\n", "if", "first", ">", "0", "and", "total", ">=", "first", ":", "\n", "                ", "break", "\n", "", "if", "log_every", ">", "0", "and", "total", ">", "0", "and", "total", "%", "log_every", "==", "0", ":", "\n", "                ", "print", "(", "'loaded ({0}) vectors'", ".", "format", "(", "total", ")", ")", "\n", "", "", "if", "load_concepts", "==", "False", ":", "\n", "            ", "W", "=", "W", "[", ":", "total", ",", "]", "# take only loaded vectors", "\n", "\n", "", "if", "normalize", "==", "True", ":", "\n", "            ", "W", "=", "(", "W", ".", "T", "/", "(", "np", ".", "linalg", ".", "norm", "(", "W", ",", "axis", "=", "1", ")", ")", ")", ".", "T", "\n", "\n", "", "if", "log_every", ">", "0", ":", "\n", "            ", "print", "(", "'done loading ({0}) vectors!'", ".", "format", "(", "total", ")", ")", "\n", "", "return", "titles", ",", "redirects", ",", "vector_size", ",", "W", ",", "id2word", ",", "word2id", ",", "get_all_titles", "(", "W", ",", "titles", ",", "redirects", ",", "word2id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.get_all_titles": [[344, 370], ["sorted", "sorted", "titles.items", "all_pairs.append", "redirects.items", "all_pairs.append", "i.lower", "all_titles.setdefault", "all_titles.setdefault", "i.isupper"], "function", ["None"], ["", "", "def", "get_all_titles", "(", "model", ",", "titles", ",", "redirects", ",", "word2id", ",", "orig_titles", "=", "True", ",", "lower", "=", "True", ",", "prefix", "=", "''", ",", "postfix", "=", "''", ")", ":", "\n", "    ", "\"\"\"\n    return a map of all wikipedia titles and redirects existing in the model\n    as keys and article id as values\n    \"\"\"", "\n", "all_pairs", "=", "[", "]", "\n", "all_titles", "=", "{", "}", "\n", "for", "i", ",", "j", "in", "sorted", "(", "titles", ".", "items", "(", ")", ")", ":", "\n", "        ", "all_pairs", ".", "append", "(", "(", "i", ",", "prefix", "+", "j", "+", "postfix", ",", "i", ")", ")", "\n", "", "for", "i", ",", "j", "in", "sorted", "(", "redirects", ".", "items", "(", ")", ")", ":", "\n", "        ", "all_pairs", ".", "append", "(", "(", "i", ",", "prefix", "+", "titles", "[", "j", "]", "+", "postfix", ",", "j", ")", ")", "\n", "", "for", "i", ",", "id", ",", "j", "in", "all_pairs", ":", "\n", "        ", "if", "model", "is", "None", "or", "id", "in", "word2id", ":", "\n", "            ", "if", "lower", "==", "True", ":", "\n", "                ", "newi", "=", "i", ".", "lower", "(", ")", "\n", "", "if", "orig_titles", "==", "True", ":", "\n", "                ", "oldval", "=", "all_titles", ".", "setdefault", "(", "newi", ",", "(", "id", ",", "j", ")", ")", "\n", "if", "oldval", "!=", "(", "id", ",", "j", ")", ":", "# this is a duplicate", "\n", "                    ", "if", "i", ".", "isupper", "(", ")", "==", "False", ":", "# keep the lower version Iowa vs. IOWA and America vs. AMERICA", "\n", "                        ", "all_titles", "[", "newi", "]", "=", "(", "id", ",", "j", ")", "\n", "#    print('unexpected duplicate title ({0}) for orginal title ({1}) where old title ({2})'.format(i,j,oldval[1]))", "\n", "", "", "", "else", ":", "\n", "                ", "oldval", "=", "all_titles", ".", "setdefault", "(", "i", ",", "(", "id", ",", ")", ")", "\n", "# if oldval!= (id,):", "\n", "#    print('unexpected duplicate title ({0}) for orginal title ({1})'.format(i,j))", "\n", "", "", "", "return", "all_titles", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.loadJson": [[372, 382], ["os.path.join", "os.path.exists", "print", "tqdm.tqdm", "pbar.update", "open", "json.load"], "function", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load"], ["", "def", "loadJson", "(", "root", ",", "name", ")", ":", "\n", "    ", "path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "name", "+", "\".json\"", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "        ", "print", "(", "\"[ERROR] Data file does not exist!\"", ",", "path", ")", "\n", "assert", "(", "0", ")", "\n", "", "with", "tqdm", "(", "total", "=", "1", ",", "desc", "=", "f'loading'", "+", "path", ")", "as", "pbar", ":", "\n", "        ", "with", "open", "(", "path", ",", "mode", "=", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "fr", ":", "\n", "            ", "data", "=", "json", ".", "load", "(", "fr", ")", "\n", "", "pbar", ".", "update", "(", "1", ")", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load_numpy_file_to_tensor": [[384, 395], ["os.path.join", "os.path.exists", "print", "tqdm.tqdm", "numpy.load", "torch.from_numpy", "pbar.update"], "function", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load"], ["", "def", "load_numpy_file_to_tensor", "(", "root", ",", "name", ")", ":", "\n", "    ", "path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "name", "+", "\".npy\"", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "        ", "print", "(", "\"[ERROR] Data file does not exist!\"", ",", "path", ")", "\n", "assert", "(", "0", ")", "\n", "", "with", "tqdm", "(", "total", "=", "1", ",", "desc", "=", "f'loading'", "+", "path", ")", "as", "pbar", ":", "\n", "        ", "matrix", "=", "np", ".", "load", "(", "path", ",", "allow_pickle", "=", "True", ")", "\n", "matrix", "=", "torch", ".", "from_numpy", "(", "matrix", ")", "\n", "pbar", ".", "update", "(", "1", ")", "\n", "\n", "", "return", "matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.FewRelDataset.__init__": [[24, 38], ["os.path.join", "os.path.join", "os.path.join", "os.path.join", "json.load", "json.load", "json.load", "json.load", "list", "os.path.exists", "os.path.exists", "os.path.exists", "os.path.exists", "print", "open", "data_loader.FewRelDataset.json_data.keys"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load"], ["def", "__init__", "(", "self", ",", "name", ",", "encoder", ",", "N", ",", "K", ",", "Q", ",", "na_rate", ",", "root", ")", ":", "\n", "        ", "self", ".", "root", "=", "root", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "name", "+", "\".json\"", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "            ", "print", "(", "\"[ERROR] Data file does not exist!\"", ")", "\n", "assert", "(", "0", ")", "\n", "", "self", ".", "json_data", "=", "json", ".", "load", "(", "open", "(", "path", ")", ")", "\n", "self", ".", "classes", "=", "list", "(", "self", ".", "json_data", ".", "keys", "(", ")", ")", "\n", "self", ".", "N", "=", "N", "\n", "self", ".", "K", "=", "K", "\n", "self", ".", "Q", "=", "Q", "\n", "self", ".", "na_rate", "=", "na_rate", "\n", "self", ".", "encoder", "=", "encoder", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.FewRelDataset.__getraw__": [[39, 44], ["data_loader.FewRelDataset.encoder.tokenize"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "def", "__getraw__", "(", "self", ",", "item", ")", ":", "\n", "        ", "word", ",", "pos1", ",", "pos2", ",", "mask", "=", "self", ".", "encoder", ".", "tokenize", "(", "item", "[", "'tokens'", "]", ",", "\n", "item", "[", "'h'", "]", "[", "2", "]", "[", "0", "]", ",", "\n", "item", "[", "'t'", "]", "[", "2", "]", "[", "0", "]", ")", "\n", "return", "word", ",", "pos1", ",", "pos2", ",", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.FewRelDataset.__additem__": [[45, 50], ["d[].append", "d[].append", "d[].append", "d[].append"], "methods", ["None"], ["", "def", "__additem__", "(", "self", ",", "d", ",", "word", ",", "pos1", ",", "pos2", ",", "mask", ")", ":", "\n", "        ", "d", "[", "'word'", "]", ".", "append", "(", "word", ")", "\n", "d", "[", "'pos1'", "]", ".", "append", "(", "pos1", ")", "\n", "d", "[", "'pos2'", "]", ".", "append", "(", "pos2", ")", "\n", "d", "[", "'mask'", "]", ".", "append", "(", "mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.FewRelDataset.__getitem__": [[51, 96], ["random.sample", "random.sample", "random.sample", "random.sample", "int", "list", "enumerate", "range", "filter", "numpy.random.choice", "numpy.random.choice", "data_loader.FewRelDataset.__getraw__", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "data_loader.FewRelDataset.__additem__", "list", "data_loader.FewRelDataset.__getraw__", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "numpy.random.choice", "numpy.random.choice", "numpy.random.choice", "numpy.random.choice", "range", "data_loader.FewRelDataset.__additem__", "data_loader.FewRelDataset.__additem__", "list", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "range", "len"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.__getraw__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.__additem__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.__getraw__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.__additem__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.__additem__"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "target_classes", "=", "random", ".", "sample", "(", "self", ".", "classes", ",", "self", ".", "N", ")", "\n", "support_set", "=", "{", "'word'", ":", "[", "]", ",", "'pos1'", ":", "[", "]", ",", "'pos2'", ":", "[", "]", ",", "'mask'", ":", "[", "]", "}", "\n", "query_set", "=", "{", "'word'", ":", "[", "]", ",", "'pos1'", ":", "[", "]", ",", "'pos2'", ":", "[", "]", ",", "'mask'", ":", "[", "]", "}", "\n", "query_label", "=", "[", "]", "\n", "Q_na", "=", "int", "(", "self", ".", "na_rate", "*", "self", ".", "Q", ")", "\n", "na_classes", "=", "list", "(", "filter", "(", "lambda", "x", ":", "x", "not", "in", "target_classes", ",", "\n", "self", ".", "classes", ")", ")", "\n", "\n", "for", "i", ",", "class_name", "in", "enumerate", "(", "target_classes", ")", ":", "\n", "            ", "indices", "=", "np", ".", "random", ".", "choice", "(", "\n", "list", "(", "range", "(", "len", "(", "self", ".", "json_data", "[", "class_name", "]", ")", ")", ")", ",", "\n", "self", ".", "K", "+", "self", ".", "Q", ",", "False", ")", "\n", "count", "=", "0", "\n", "for", "j", "in", "indices", ":", "\n", "                ", "word", ",", "pos1", ",", "pos2", ",", "mask", "=", "self", ".", "__getraw__", "(", "\n", "self", ".", "json_data", "[", "class_name", "]", "[", "j", "]", ")", "\n", "word", "=", "torch", ".", "tensor", "(", "word", ")", ".", "long", "(", ")", "\n", "pos1", "=", "torch", ".", "tensor", "(", "pos1", ")", ".", "long", "(", ")", "\n", "pos2", "=", "torch", ".", "tensor", "(", "pos2", ")", ".", "long", "(", ")", "\n", "mask", "=", "torch", ".", "tensor", "(", "mask", ")", ".", "long", "(", ")", "\n", "if", "count", "<", "self", ".", "K", ":", "\n", "                    ", "self", ".", "__additem__", "(", "support_set", ",", "word", ",", "pos1", ",", "pos2", ",", "mask", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "__additem__", "(", "query_set", ",", "word", ",", "pos1", ",", "pos2", ",", "mask", ")", "\n", "", "count", "+=", "1", "\n", "\n", "", "query_label", "+=", "[", "i", "]", "*", "self", ".", "Q", "\n", "\n", "# NA", "\n", "", "for", "j", "in", "range", "(", "Q_na", ")", ":", "\n", "            ", "cur_class", "=", "np", ".", "random", ".", "choice", "(", "na_classes", ",", "1", ",", "False", ")", "[", "0", "]", "\n", "index", "=", "np", ".", "random", ".", "choice", "(", "\n", "list", "(", "range", "(", "len", "(", "self", ".", "json_data", "[", "cur_class", "]", ")", ")", ")", ",", "\n", "1", ",", "False", ")", "[", "0", "]", "\n", "word", ",", "pos1", ",", "pos2", ",", "mask", "=", "self", ".", "__getraw__", "(", "\n", "self", ".", "json_data", "[", "cur_class", "]", "[", "index", "]", ")", "\n", "word", "=", "torch", ".", "tensor", "(", "word", ")", ".", "long", "(", ")", "\n", "pos1", "=", "torch", ".", "tensor", "(", "pos1", ")", ".", "long", "(", ")", "\n", "pos2", "=", "torch", ".", "tensor", "(", "pos2", ")", ".", "long", "(", ")", "\n", "mask", "=", "torch", ".", "tensor", "(", "mask", ")", ".", "long", "(", ")", "\n", "self", ".", "__additem__", "(", "query_set", ",", "word", ",", "pos1", ",", "pos2", ",", "mask", ")", "\n", "", "query_label", "+=", "[", "self", ".", "N", "]", "*", "Q_na", "\n", "\n", "return", "support_set", ",", "query_set", ",", "query_label", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.FewRelDataset.__len__": [[97, 99], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "1000000000", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.FewRelDatasetPair.__init__": [[137, 156], ["os.path.join", "os.path.join", "os.path.join", "os.path.join", "json.load", "json.load", "json.load", "json.load", "list", "os.path.exists", "os.path.exists", "os.path.exists", "os.path.exists", "print", "print", "open", "data_loader.FewRelDatasetPair.json_data.keys"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load"], ["def", "__init__", "(", "self", ",", "name", ",", "encoder", ",", "N", ",", "K", ",", "Q", ",", "na_rate", ",", "root", ",", "encoder_name", ",", "ins2cpt", ")", ":", "\n", "        ", "self", ".", "root", "=", "root", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "name", "+", "\".json\"", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "            ", "print", "(", "'path:'", ",", "path", ")", "\n", "print", "(", "\"[ERROR] Data file does not exist!\"", ")", "\n", "assert", "(", "0", ")", "\n", "", "self", ".", "json_data", "=", "json", ".", "load", "(", "open", "(", "path", ")", ")", "\n", "self", ".", "classes", "=", "list", "(", "self", ".", "json_data", ".", "keys", "(", ")", ")", "\n", "self", ".", "N", "=", "N", "\n", "self", ".", "K", "=", "K", "\n", "self", ".", "Q", "=", "Q", "\n", "self", ".", "na_rate", "=", "na_rate", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "encoder_name", "=", "encoder_name", "\n", "self", ".", "max_length", "=", "encoder", ".", "max_length", "\n", "\n", "'''\u4fee\u6539\u90e8\u5206'''", "\n", "self", ".", "ins2cpt", "=", "ins2cpt", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.FewRelDatasetPair.__getraw__": [[157, 170], ["data_loader.FewRelDatasetPair.encoder.tokenize_concept"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.tokenize_concept"], ["", "def", "__getraw__", "(", "self", ",", "item", ",", "ins2cpt", ")", ":", "\n", "# word = self.encoder.tokenize(item['tokens'],", "\n", "#                              item['h'][2][0],", "\n", "#                              item['t'][2][0])", "\n", "\n", "        ", "'''\u4fee\u6539\u90e8\u5206'''", "\n", "word", "=", "self", ".", "encoder", ".", "tokenize_concept", "(", "item", "[", "'tokens'", "]", ",", "\n", "item", "[", "'h'", "]", "[", "2", "]", "[", "0", "]", ",", "\n", "item", "[", "'t'", "]", "[", "2", "]", "[", "0", "]", ",", "\n", "item", "[", "'h'", "]", "[", "0", "]", ",", "\n", "item", "[", "'t'", "]", "[", "0", "]", ",", "\n", "ins2cpt", ")", "\n", "return", "word", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.FewRelDatasetPair.__additem__": [[171, 176], ["d[].append", "d[].append", "d[].append", "d[].append"], "methods", ["None"], ["", "def", "__additem__", "(", "self", ",", "d", ",", "word", ",", "pos1", ",", "pos2", ",", "mask", ")", ":", "\n", "        ", "d", "[", "'word'", "]", ".", "append", "(", "word", ")", "\n", "d", "[", "'pos1'", "]", ".", "append", "(", "pos1", ")", "\n", "d", "[", "'pos2'", "]", ".", "append", "(", "pos2", ")", "\n", "d", "[", "'mask'", "]", ".", "append", "(", "mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.FewRelDatasetPair.__getitem__": [[177, 236], ["random.sample", "random.sample", "random.sample", "random.sample", "int", "list", "enumerate", "range", "filter", "numpy.random.choice", "numpy.random.choice", "data_loader.FewRelDatasetPair.__getraw__", "query.append", "list", "data_loader.FewRelDatasetPair.__getraw__", "numpy.random.choice", "numpy.random.choice", "numpy.random.choice", "numpy.random.choice", "range", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "fusion_set[].append", "fusion_set[].append", "fusion_set[].append", "range", "support.append", "query.append", "list", "data_loader.FewRelDatasetPair.encoder.tokenizer.convert_tokens_to_ids", "data_loader.FewRelDatasetPair.encoder.tokenizer.convert_tokens_to_ids", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "data_loader.FewRelDatasetPair.encoder.tokenizer.convert_tokens_to_ids", "data_loader.FewRelDatasetPair.encoder.tokenizer.convert_tokens_to_ids", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "min", "len", "range", "len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "min", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "min", "len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "len", "len"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.__getraw__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.__getraw__"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "target_classes", "=", "random", ".", "sample", "(", "self", ".", "classes", ",", "self", ".", "N", ")", "\n", "support", "=", "[", "]", "\n", "query", "=", "[", "]", "\n", "fusion_set", "=", "{", "'word'", ":", "[", "]", ",", "'mask'", ":", "[", "]", ",", "'seg'", ":", "[", "]", "}", "\n", "query_label", "=", "[", "]", "\n", "Q_na", "=", "int", "(", "self", ".", "na_rate", "*", "self", ".", "Q", ")", "\n", "na_classes", "=", "list", "(", "filter", "(", "lambda", "x", ":", "x", "not", "in", "target_classes", ",", "\n", "self", ".", "classes", ")", ")", "\n", "\n", "for", "i", ",", "class_name", "in", "enumerate", "(", "target_classes", ")", ":", "\n", "            ", "indices", "=", "np", ".", "random", ".", "choice", "(", "\n", "list", "(", "range", "(", "len", "(", "self", ".", "json_data", "[", "class_name", "]", ")", ")", ")", ",", "\n", "self", ".", "K", "+", "self", ".", "Q", ",", "False", ")", "\n", "count", "=", "0", "\n", "for", "j", "in", "indices", ":", "\n", "                ", "word", "=", "self", ".", "__getraw__", "(", "\n", "self", ".", "json_data", "[", "class_name", "]", "[", "j", "]", ",", "self", ".", "ins2cpt", ")", "\n", "if", "count", "<", "self", ".", "K", ":", "\n", "                    ", "support", ".", "append", "(", "word", ")", "\n", "", "else", ":", "\n", "                    ", "query", ".", "append", "(", "word", ")", "\n", "", "count", "+=", "1", "\n", "\n", "", "query_label", "+=", "[", "i", "]", "*", "self", ".", "Q", "\n", "\n", "# NA", "\n", "", "for", "j", "in", "range", "(", "Q_na", ")", ":", "\n", "            ", "cur_class", "=", "np", ".", "random", ".", "choice", "(", "na_classes", ",", "1", ",", "False", ")", "[", "0", "]", "\n", "index", "=", "np", ".", "random", ".", "choice", "(", "\n", "list", "(", "range", "(", "len", "(", "self", ".", "json_data", "[", "cur_class", "]", ")", ")", ")", ",", "\n", "1", ",", "False", ")", "[", "0", "]", "\n", "word", "=", "self", ".", "__getraw__", "(", "\n", "self", ".", "json_data", "[", "cur_class", "]", "[", "index", "]", ",", "self", ".", "ins2cpt", ")", "\n", "query", ".", "append", "(", "word", ")", "\n", "", "query_label", "+=", "[", "self", ".", "N", "]", "*", "Q_na", "\n", "\n", "for", "word_query", "in", "query", ":", "\n", "            ", "for", "word_support", "in", "support", ":", "\n", "                ", "if", "self", ".", "encoder_name", "==", "'bert'", ":", "\n", "                    ", "SEP", "=", "self", ".", "encoder", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "'[SEP]'", "]", ")", "\n", "CLS", "=", "self", ".", "encoder", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "'[CLS]'", "]", ")", "\n", "word_tensor", "=", "torch", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ")", ".", "long", "(", ")", "\n", "", "else", ":", "\n", "                    ", "SEP", "=", "self", ".", "encoder", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "'</s>'", "]", ")", "\n", "CLS", "=", "self", ".", "encoder", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "'<s>'", "]", ")", "\n", "word_tensor", "=", "torch", ".", "ones", "(", "(", "self", ".", "max_length", ")", ")", ".", "long", "(", ")", "\n", "", "new_word", "=", "CLS", "+", "word_support", "+", "SEP", "+", "word_query", "+", "SEP", "\n", "for", "i", "in", "range", "(", "min", "(", "self", ".", "max_length", ",", "len", "(", "new_word", ")", ")", ")", ":", "\n", "                    ", "word_tensor", "[", "i", "]", "=", "new_word", "[", "i", "]", "\n", "", "mask_tensor", "=", "torch", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ")", ".", "long", "(", ")", "\n", "mask_tensor", "[", ":", "min", "(", "self", ".", "max_length", ",", "len", "(", "new_word", ")", ")", "]", "=", "1", "\n", "seg_tensor", "=", "torch", ".", "ones", "(", "(", "self", ".", "max_length", ")", ")", ".", "long", "(", ")", "\n", "seg_tensor", "[", ":", "min", "(", "self", ".", "max_length", ",", "len", "(", "word_support", ")", "+", "1", ")", "]", "=", "0", "\n", "fusion_set", "[", "'word'", "]", ".", "append", "(", "word_tensor", ")", "\n", "fusion_set", "[", "'mask'", "]", ".", "append", "(", "mask_tensor", ")", "\n", "fusion_set", "[", "'seg'", "]", ".", "append", "(", "seg_tensor", ")", "\n", "\n", "", "", "return", "fusion_set", ",", "query_label", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.FewRelDatasetPair.__len__": [[237, 239], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "1000000000", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.FewRelUnsupervisedDataset.__init__": [[272, 284], ["os.path.join", "os.path.join", "os.path.join", "os.path.join", "json.load", "json.load", "json.load", "json.load", "os.path.exists", "os.path.exists", "os.path.exists", "os.path.exists", "print", "open"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load"], ["def", "__init__", "(", "self", ",", "name", ",", "encoder", ",", "N", ",", "K", ",", "Q", ",", "na_rate", ",", "root", ")", ":", "\n", "        ", "self", ".", "root", "=", "root", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "name", "+", "\".json\"", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "            ", "print", "(", "\"[ERROR] Data file does not exist!\"", ")", "\n", "assert", "(", "0", ")", "\n", "", "self", ".", "json_data", "=", "json", ".", "load", "(", "open", "(", "path", ")", ")", "\n", "self", ".", "N", "=", "N", "\n", "self", ".", "K", "=", "K", "\n", "self", ".", "Q", "=", "Q", "\n", "self", ".", "na_rate", "=", "na_rate", "\n", "self", ".", "encoder", "=", "encoder", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.FewRelUnsupervisedDataset.__getraw__": [[285, 290], ["data_loader.FewRelUnsupervisedDataset.encoder.tokenize"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "def", "__getraw__", "(", "self", ",", "item", ")", ":", "\n", "        ", "word", ",", "pos1", ",", "pos2", ",", "mask", "=", "self", ".", "encoder", ".", "tokenize", "(", "item", "[", "'tokens'", "]", ",", "\n", "item", "[", "'h'", "]", "[", "2", "]", "[", "0", "]", ",", "\n", "item", "[", "'t'", "]", "[", "2", "]", "[", "0", "]", ")", "\n", "return", "word", ",", "pos1", ",", "pos2", ",", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.FewRelUnsupervisedDataset.__additem__": [[291, 296], ["d[].append", "d[].append", "d[].append", "d[].append"], "methods", ["None"], ["", "def", "__additem__", "(", "self", ",", "d", ",", "word", ",", "pos1", ",", "pos2", ",", "mask", ")", ":", "\n", "        ", "d", "[", "'word'", "]", ".", "append", "(", "word", ")", "\n", "d", "[", "'pos1'", "]", ".", "append", "(", "pos1", ")", "\n", "d", "[", "'pos2'", "]", ".", "append", "(", "pos2", ")", "\n", "d", "[", "'mask'", "]", ".", "append", "(", "mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.FewRelUnsupervisedDataset.__getitem__": [[297, 312], ["numpy.random.choice", "numpy.random.choice", "list", "data_loader.FewRelUnsupervisedDataset.__getraw__", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "data_loader.FewRelUnsupervisedDataset.__additem__", "range", "len", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.__getraw__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.__additem__"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "total", "=", "self", ".", "N", "*", "self", ".", "K", "\n", "support_set", "=", "{", "'word'", ":", "[", "]", ",", "'pos1'", ":", "[", "]", ",", "'pos2'", ":", "[", "]", ",", "'mask'", ":", "[", "]", "}", "\n", "\n", "indices", "=", "np", ".", "random", ".", "choice", "(", "list", "(", "range", "(", "len", "(", "self", ".", "json_data", ")", ")", ")", ",", "total", ",", "False", ")", "\n", "for", "j", "in", "indices", ":", "\n", "            ", "word", ",", "pos1", ",", "pos2", ",", "mask", "=", "self", ".", "__getraw__", "(", "\n", "self", ".", "json_data", "[", "j", "]", ")", "\n", "word", "=", "torch", ".", "tensor", "(", "word", ")", ".", "long", "(", ")", "\n", "pos1", "=", "torch", ".", "tensor", "(", "pos1", ")", ".", "long", "(", ")", "\n", "pos2", "=", "torch", ".", "tensor", "(", "pos2", ")", ".", "long", "(", ")", "\n", "mask", "=", "torch", ".", "tensor", "(", "mask", ")", ".", "long", "(", ")", "\n", "self", ".", "__additem__", "(", "support_set", ",", "word", ",", "pos1", ",", "pos2", ",", "mask", ")", "\n", "\n", "", "return", "support_set", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.FewRelUnsupervisedDataset.__len__": [[313, 315], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "1000000000", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.collate_fn": [[101, 118], ["zip", "range", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "function", ["None"], ["", "", "def", "collate_fn", "(", "data", ")", ":", "\n", "    ", "batch_support", "=", "{", "'word'", ":", "[", "]", ",", "'pos1'", ":", "[", "]", ",", "'pos2'", ":", "[", "]", ",", "'mask'", ":", "[", "]", "}", "\n", "batch_query", "=", "{", "'word'", ":", "[", "]", ",", "'pos1'", ":", "[", "]", ",", "'pos2'", ":", "[", "]", ",", "'mask'", ":", "[", "]", "}", "\n", "batch_label", "=", "[", "]", "\n", "support_sets", ",", "query_sets", ",", "query_labels", "=", "zip", "(", "*", "data", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "support_sets", ")", ")", ":", "\n", "        ", "for", "k", "in", "support_sets", "[", "i", "]", ":", "\n", "            ", "batch_support", "[", "k", "]", "+=", "support_sets", "[", "i", "]", "[", "k", "]", "\n", "", "for", "k", "in", "query_sets", "[", "i", "]", ":", "\n", "            ", "batch_query", "[", "k", "]", "+=", "query_sets", "[", "i", "]", "[", "k", "]", "\n", "", "batch_label", "+=", "query_labels", "[", "i", "]", "\n", "", "for", "k", "in", "batch_support", ":", "\n", "        ", "batch_support", "[", "k", "]", "=", "torch", ".", "stack", "(", "batch_support", "[", "k", "]", ",", "0", ")", "\n", "", "for", "k", "in", "batch_query", ":", "\n", "        ", "batch_query", "[", "k", "]", "=", "torch", ".", "stack", "(", "batch_query", "[", "k", "]", ",", "0", ")", "\n", "", "batch_label", "=", "torch", ".", "tensor", "(", "batch_label", ")", "\n", "return", "batch_support", ",", "batch_query", ",", "batch_label", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.get_loader": [[120, 130], ["data_loader.FewRelDataset", "torch.DataLoader", "iter"], "function", ["None"], ["", "def", "get_loader", "(", "name", ",", "encoder", ",", "N", ",", "K", ",", "Q", ",", "batch_size", ",", "\n", "num_workers", "=", "8", ",", "collate_fn", "=", "collate_fn", ",", "na_rate", "=", "0", ",", "root", "=", "'./data'", ")", ":", "\n", "    ", "dataset", "=", "FewRelDataset", "(", "name", ",", "encoder", ",", "N", ",", "K", ",", "Q", ",", "na_rate", ",", "root", ")", "\n", "data_loader", "=", "data", ".", "DataLoader", "(", "dataset", "=", "dataset", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "shuffle", "=", "False", ",", "\n", "pin_memory", "=", "True", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", "collate_fn", "=", "collate_fn", ")", "\n", "return", "iter", "(", "data_loader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.collate_fn_pair": [[241, 253], ["zip", "range", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "function", ["None"], ["", "", "def", "collate_fn_pair", "(", "data", ")", ":", "\n", "    ", "batch_set", "=", "{", "'word'", ":", "[", "]", ",", "'seg'", ":", "[", "]", ",", "'mask'", ":", "[", "]", "}", "\n", "batch_label", "=", "[", "]", "\n", "fusion_sets", ",", "query_labels", "=", "zip", "(", "*", "data", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "fusion_sets", ")", ")", ":", "\n", "        ", "for", "k", "in", "fusion_sets", "[", "i", "]", ":", "\n", "            ", "batch_set", "[", "k", "]", "+=", "fusion_sets", "[", "i", "]", "[", "k", "]", "\n", "", "batch_label", "+=", "query_labels", "[", "i", "]", "\n", "", "for", "k", "in", "batch_set", ":", "\n", "        ", "batch_set", "[", "k", "]", "=", "torch", ".", "stack", "(", "batch_set", "[", "k", "]", ",", "0", ")", "\n", "", "batch_label", "=", "torch", ".", "tensor", "(", "batch_label", ")", "\n", "return", "batch_set", ",", "batch_label", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.get_loader_pair": [[255, 265], ["data_loader.FewRelDatasetPair", "torch.DataLoader", "iter"], "function", ["None"], ["", "def", "get_loader_pair", "(", "name", ",", "ins2cpt", ",", "encoder", ",", "nWay", ",", "K", ",", "Q", ",", "batch_size", ",", "\n", "num_workers", "=", "8", ",", "collate_fn", "=", "collate_fn_pair", ",", "na_rate", "=", "0", ",", "root", "=", "'./data'", ",", "encoder_name", "=", "'bert'", ")", ":", "\n", "    ", "dataset", "=", "FewRelDatasetPair", "(", "name", ",", "encoder", ",", "nWay", ",", "K", ",", "Q", ",", "na_rate", ",", "root", ",", "encoder_name", ",", "ins2cpt", ")", "\n", "data_loader", "=", "data", ".", "DataLoader", "(", "dataset", "=", "dataset", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "shuffle", "=", "False", ",", "\n", "pin_memory", "=", "True", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", "collate_fn", "=", "collate_fn", ")", "\n", "return", "iter", "(", "data_loader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.collate_fn_unsupervised": [[317, 326], ["range", "len", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "function", ["None"], ["", "", "def", "collate_fn_unsupervised", "(", "data", ")", ":", "\n", "    ", "batch_support", "=", "{", "'word'", ":", "[", "]", ",", "'pos1'", ":", "[", "]", ",", "'pos2'", ":", "[", "]", ",", "'mask'", ":", "[", "]", "}", "\n", "support_sets", "=", "data", "\n", "for", "i", "in", "range", "(", "len", "(", "support_sets", ")", ")", ":", "\n", "        ", "for", "k", "in", "support_sets", "[", "i", "]", ":", "\n", "            ", "batch_support", "[", "k", "]", "+=", "support_sets", "[", "i", "]", "[", "k", "]", "\n", "", "", "for", "k", "in", "batch_support", ":", "\n", "        ", "batch_support", "[", "k", "]", "=", "torch", ".", "stack", "(", "batch_support", "[", "k", "]", ",", "0", ")", "\n", "", "return", "batch_support", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_loader.get_loader_unsupervised": [[328, 338], ["data_loader.FewRelUnsupervisedDataset", "torch.DataLoader", "iter"], "function", ["None"], ["", "def", "get_loader_unsupervised", "(", "name", ",", "encoder", ",", "N", ",", "K", ",", "Q", ",", "batch_size", ",", "\n", "num_workers", "=", "8", ",", "collate_fn", "=", "collate_fn_unsupervised", ",", "na_rate", "=", "0", ",", "root", "=", "'./data'", ")", ":", "\n", "    ", "dataset", "=", "FewRelUnsupervisedDataset", "(", "name", ",", "encoder", ",", "N", ",", "K", ",", "Q", ",", "na_rate", ",", "root", ")", "\n", "data_loader", "=", "data", ".", "DataLoader", "(", "dataset", "=", "dataset", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "shuffle", "=", "False", ",", "\n", "pin_memory", "=", "True", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", "collate_fn", "=", "collate_fn", ")", "\n", "return", "iter", "(", "data_loader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.ssss.CNNSentenceEncoder.__init__": [[15, 25], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "network.embedding.Embedding", "network.encoder.Encoder"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "word_vec_mat", ",", "word2id", ",", "max_length", ",", "word_embedding_dim", "=", "50", ",", "\n", "pos_embedding_dim", "=", "5", ",", "hidden_size", "=", "230", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "embedding", "=", "network", ".", "embedding", ".", "Embedding", "(", "word_vec_mat", ",", "max_length", ",", "\n", "word_embedding_dim", ",", "pos_embedding_dim", ")", "\n", "self", ".", "encoder", "=", "network", ".", "encoder", ".", "Encoder", "(", "max_length", ",", "word_embedding_dim", ",", "\n", "pos_embedding_dim", ",", "hidden_size", ")", "\n", "self", ".", "word2id", "=", "word2id", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.ssss.CNNSentenceEncoder.forward": [[26, 30], ["ssss.CNNSentenceEncoder.embedding", "ssss.CNNSentenceEncoder.encoder"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "x", "=", "self", ".", "embedding", "(", "inputs", ")", "\n", "x", "=", "self", ".", "encoder", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.ssss.CNNSentenceEncoder.tokenize": [[31, 60], ["numpy.zeros", "numpy.zeros", "min", "min", "range", "numpy.zeros", "token.lower.lower.lower", "len", "indexed_tokens.append", "indexed_tokens.append", "indexed_tokens.append", "len"], "methods", ["None"], ["", "def", "tokenize", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ")", ":", "\n", "# token -> index", "\n", "        ", "indexed_tokens", "=", "[", "]", "\n", "for", "token", "in", "raw_tokens", ":", "\n", "            ", "token", "=", "token", ".", "lower", "(", ")", "\n", "if", "token", "in", "self", ".", "word2id", ":", "\n", "                ", "indexed_tokens", ".", "append", "(", "self", ".", "word2id", "[", "token", "]", ")", "\n", "", "else", ":", "\n", "                ", "indexed_tokens", ".", "append", "(", "self", ".", "word2id", "[", "'[UNK]'", "]", ")", "\n", "\n", "# padding", "\n", "", "", "while", "len", "(", "indexed_tokens", ")", "<", "self", ".", "max_length", ":", "\n", "            ", "indexed_tokens", ".", "append", "(", "self", ".", "word2id", "[", "'[PAD]'", "]", ")", "\n", "", "indexed_tokens", "=", "indexed_tokens", "[", ":", "self", ".", "max_length", "]", "\n", "\n", "# pos", "\n", "pos1", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "pos2", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "pos1_in_index", "=", "min", "(", "self", ".", "max_length", ",", "pos_head", "[", "0", "]", ")", "\n", "pos2_in_index", "=", "min", "(", "self", ".", "max_length", ",", "pos_tail", "[", "0", "]", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "max_length", ")", ":", "\n", "            ", "pos1", "[", "i", "]", "=", "i", "-", "pos1_in_index", "+", "self", ".", "max_length", "\n", "pos2", "[", "i", "]", "=", "i", "-", "pos2_in_index", "+", "self", ".", "max_length", "\n", "\n", "# mask", "\n", "", "mask", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "mask", "[", ":", "len", "(", "indexed_tokens", ")", "]", "=", "1", "\n", "\n", "return", "indexed_tokens", ",", "pos1", ",", "pos2", ",", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.ssss.BERTSentenceEncoder.__init__": [[64, 74], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "pytorch_transformers.BertForSequenceClassification.from_pretrained", "pytorch_transformers.BertTokenizer.from_pretrained", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "pretrain_path", ",", "max_length", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "# self.bert = BertModel.from_pretrained(pretrain_path)", "\n", "self", ".", "bert", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "\n", "pretrain_path", ",", "\n", "num_labels", "=", "2", ")", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "os", ".", "path", ".", "join", "(", "\n", "pretrain_path", ",", "'bert_vocab.txt'", ")", ")", "\n", "self", ".", "modelName", "=", "'Bert'", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.ssss.BERTSentenceEncoder.forward": [[75, 78], ["ssss.BERTSentenceEncoder.bert"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "x", "=", "self", ".", "bert", "(", "inputs", "[", "'word'", "]", ",", "inputs", "[", "'seg'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "[", "0", "]", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.ssss.BERTSentenceEncoder.tokenize": [[79, 121], ["ssss.BERTSentenceEncoder.tokenizer.convert_tokens_to_ids", "token.lower.lower.lower", "ssss.BERTSentenceEncoder.tokenizer.tokenize", "tokens.append", "len", "tokens.append", "len", "tokens.append", "tokens.append"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "def", "tokenize", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ")", ":", "\n", "# token -> index", "\n", "# tokens = ['[CLS]']", "\n", "        ", "tokens", "=", "[", "]", "\n", "cur_pos", "=", "0", "\n", "pos1_in_index", "=", "0", "\n", "pos2_in_index", "=", "0", "\n", "for", "token", "in", "raw_tokens", ":", "\n", "            ", "token", "=", "token", ".", "lower", "(", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused0]'", ")", "\n", "pos1_in_index", "=", "len", "(", "tokens", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused1]'", ")", "\n", "pos2_in_index", "=", "len", "(", "tokens", ")", "\n", "", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused2]'", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused3]'", ")", "\n", "", "cur_pos", "+=", "1", "\n", "", "indexed_tokens", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# padding", "\n", "'''\n        while len(indexed_tokens) < self.max_length:\n            indexed_tokens.append(0)\n        indexed_tokens = indexed_tokens[:self.max_length]\n\n        # pos\n        pos1 = np.zeros((self.max_length), dtype=np.int32)\n        pos2 = np.zeros((self.max_length), dtype=np.int32)\n        for i in range(self.max_length):\n            pos1[i] = i - pos1_in_index + self.max_length\n            pos2[i] = i - pos2_in_index + self.max_length\n\n        # mask\n        mask = np.zeros((self.max_length), dtype=np.int32)\n        mask[:len(indexed_tokens)] = 1\n        '''", "\n", "\n", "return", "indexed_tokens", "# , pos1, pos2, mask", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.ssss.RobertaSentenceEncoder.__init__": [[124, 131], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "pytorch_transformers.RobertaForSequenceClassification.from_pretrained", "pytorch_transformers.RobertaTokenizer.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "pretrain_path", ",", "max_length", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "self", ".", "bert", "=", "RobertaForSequenceClassification", ".", "from_pretrained", "(", "pretrain_path", ",", "num_labels", "=", "2", ")", "\n", "#self.bert = RobertaModel.from_pretrained(pretrain_path)", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "tokenizer", "=", "RobertaTokenizer", ".", "from_pretrained", "(", "'roberta-base'", ")", "\n", "self", ".", "modelName", "=", "'Roberta'", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.ssss.RobertaSentenceEncoder.forward": [[132, 135], ["ssss.RobertaSentenceEncoder.bert"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "x", "=", "self", ".", "bert", "(", "inputs", "[", "'word'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "[", "0", "]", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.ssss.RobertaSentenceEncoder.tokenize": [[136, 219], ["ssss.RobertaSentenceEncoder.tokenizer.tokenize", "ssss.RobertaSentenceEncoder.tokenize.getIns"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "def", "tokenize", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ")", ":", "\n", "# token -> index", "\n", "        ", "'''\n        tokens = ['[CLS]']\n        cur_pos = 0\n        pos1_in_index = 0\n        pos2_in_index = 0\n        for token in raw_tokens:\n            token = token.lower()\n            if cur_pos == pos_head[0]:\n                tokens.append('[HEADSTART]')\n                pos1_in_index = len(tokens)\n            if cur_pos == pos_tail[0]:\n                tokens.append('[TAILSTART]')\n                pos2_in_index = len(tokens)\n            tokens += self.tokenizer.tokenize(token)\n            if cur_pos == pos_head[-1]:\n                tokens.append('[HEADEND]')\n            if cur_pos == pos_tail[-1]:\n                tokens.append('[TAILEND]')\n            cur_pos += 1\n        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokens)\n        '''", "\n", "def", "getIns", "(", "bped", ",", "bpeTokens", ",", "tokens", ",", "L", ",", "R", ")", ":", "\n", "            ", "resL", "=", "0", "\n", "tkL", "=", "\" \"", ".", "join", "(", "tokens", "[", ":", "L", "]", ")", "\n", "bped_tkL", "=", "\" \"", ".", "join", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "tkL", ")", ")", "\n", "if", "bped", ".", "find", "(", "bped_tkL", ")", "==", "0", ":", "\n", "                ", "resL", "=", "len", "(", "bped_tkL", ".", "split", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "tkL", "+=", "\" \"", "\n", "bped_tkL", "=", "\" \"", ".", "join", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "tkL", ")", ")", "\n", "if", "bped", ".", "find", "(", "bped_tkL", ")", "==", "0", ":", "\n", "                    ", "resL", "=", "len", "(", "bped_tkL", ".", "split", "(", ")", ")", "\n", "", "", "resR", "=", "0", "\n", "tkR", "=", "\" \"", ".", "join", "(", "tokens", "[", "R", ":", "]", ")", "\n", "bped_tkR", "=", "\" \"", ".", "join", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "tkR", ")", ")", "\n", "if", "bped", ".", "rfind", "(", "bped_tkR", ")", "+", "len", "(", "bped_tkR", ")", "==", "len", "(", "bped", ")", ":", "\n", "                ", "resR", "=", "len", "(", "bpeTokens", ")", "-", "len", "(", "bped_tkR", ".", "split", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "tkR", "=", "\" \"", "+", "tkR", "\n", "bped_tkR", "=", "\" \"", ".", "join", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "tkR", ")", ")", "\n", "if", "bped", ".", "rfind", "(", "bped_tkR", ")", "+", "len", "(", "bped_tkR", ")", "==", "len", "(", "bped", ")", ":", "\n", "                    ", "resR", "=", "len", "(", "bpeTokens", ")", "-", "len", "(", "bped_tkR", ".", "split", "(", ")", ")", "\n", "", "", "return", "resL", ",", "resR", "\n", "\n", "", "s", "=", "\" \"", ".", "join", "(", "raw_tokens", ")", "\n", "sst", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "s", ")", "\n", "headL", "=", "pos_head", "[", "0", "]", "\n", "headR", "=", "pos_head", "[", "-", "1", "]", "\n", "hiL", ",", "hiR", "=", "getIns", "(", "\" \"", ".", "join", "(", "sst", ")", ",", "sst", ",", "raw_tokens", ",", "headL", ",", "headR", ")", "\n", "tailL", "=", "pos_tail", "[", "0", "]", "\n", "tailR", "=", "pos_tail", "[", "-", "1", "]", "\n", "tiL", ",", "tiR", "=", "getIns", "(", "\" \"", ".", "join", "(", "sst", ")", ",", "sst", ",", "raw_tokens", ",", "tailL", ",", "tailR", ")", "\n", "E1b", "=", "'madeupword0000'", "\n", "E1e", "=", "'madeupword0001'", "\n", "E2b", "=", "'madeupword0002'", "\n", "E2e", "=", "'madeupword0003'", "\n", "ins", "=", "[", "(", "hiL", ",", "E1b", ")", ",", "(", "hiR", ",", "E1e", ")", ",", "(", "tiL", ",", "E2b", ")", ",", "(", "tiR", ",", "E2e", ")", "]", "\n", "ins", "=", "sorted", "(", "ins", ")", "\n", "pE1", "=", "0", "\n", "pE2", "=", "0", "\n", "pE1_", "=", "0", "\n", "pE2_", "=", "0", "\n", "for", "i", "in", "range", "(", "0", ",", "4", ")", ":", "\n", "            ", "sst", ".", "insert", "(", "ins", "[", "i", "]", "[", "0", "]", "+", "i", ",", "ins", "[", "i", "]", "[", "1", "]", ")", "\n", "if", "ins", "[", "i", "]", "[", "1", "]", "==", "E1b", ":", "\n", "                ", "pE1", "=", "ins", "[", "i", "]", "[", "0", "]", "+", "i", "\n", "", "elif", "ins", "[", "i", "]", "[", "1", "]", "==", "E2b", ":", "\n", "                ", "pE2", "=", "ins", "[", "i", "]", "[", "0", "]", "+", "i", "\n", "", "elif", "ins", "[", "i", "]", "[", "1", "]", "==", "E1e", ":", "\n", "                ", "pE1_", "=", "ins", "[", "i", "]", "[", "0", "]", "+", "i", "\n", "", "else", ":", "\n", "                ", "pE2_", "=", "ins", "[", "i", "]", "[", "0", "]", "+", "i", "\n", "", "", "pos1_in_index", "=", "pE1", "+", "1", "\n", "pos2_in_index", "=", "pE2", "+", "1", "\n", "indexed_tokens", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "sst", ")", "\n", "#indexed_tokens=self.tokenizer.add_special_tokens_single_sentence(indexed_tokens)", "\n", "# padding", "\n", "#while len(indexed_tokens) < self.max_length:", "\n", "#    indexed_tokens.append(0)", "\n", "#indexed_tokens = indexed_tokens[:self.max_length]", "\n", "return", "indexed_tokens", "#, pos1, pos2, mask", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.old_data_loader.FileDataLoader.next_batch": [[10, 19], ["None"], "methods", ["None"], ["    ", "def", "next_batch", "(", "self", ",", "B", ",", "N", ",", "K", ",", "Q", ")", ":", "\n", "        ", "'''\n        B: batch size.\n        N: the number of relations for each batch\n        K: the number of support instances for each relation\n        Q: the number of query instances for each relation\n        return: support_set, query_set, query_label\n        '''", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.old_data_loader.JSONFileDataLoader._load_preprocessed_file": [[21, 58], ["os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "print", "numpy.load", "numpy.load", "numpy.load", "numpy.load", "numpy.load", "json.load", "numpy.load", "json.load", "print", "os.path.isdir", "open", "open", "print", "[].split", "[].split", "os.path.exists", "os.path.exists", "os.path.exists", "os.path.exists", "os.path.exists", "os.path.exists", "os.path.exists", "os.path.exists", "old_data_loader.JSONFileDataLoader.file_name.split", "old_data_loader.JSONFileDataLoader.word_vec_file_name.split"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load"], ["    ", "def", "_load_preprocessed_file", "(", "self", ")", ":", "\n", "        ", "name_prefix", "=", "'.'", ".", "join", "(", "self", ".", "file_name", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", ".", "split", "(", "'.'", ")", "[", ":", "-", "1", "]", ")", "\n", "word_vec_name_prefix", "=", "'.'", ".", "join", "(", "self", ".", "word_vec_file_name", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", ".", "split", "(", "'.'", ")", "[", ":", "-", "1", "]", ")", "\n", "processed_data_dir", "=", "'_processed_data'", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "processed_data_dir", ")", ":", "\n", "            ", "return", "False", "\n", "", "word_npy_file_name", "=", "os", ".", "path", ".", "join", "(", "processed_data_dir", ",", "name_prefix", "+", "'_word.npy'", ")", "\n", "pos1_npy_file_name", "=", "os", ".", "path", ".", "join", "(", "processed_data_dir", ",", "name_prefix", "+", "'_pos1.npy'", ")", "\n", "pos2_npy_file_name", "=", "os", ".", "path", ".", "join", "(", "processed_data_dir", ",", "name_prefix", "+", "'_pos2.npy'", ")", "\n", "mask_npy_file_name", "=", "os", ".", "path", ".", "join", "(", "processed_data_dir", ",", "name_prefix", "+", "'_mask.npy'", ")", "\n", "length_npy_file_name", "=", "os", ".", "path", ".", "join", "(", "processed_data_dir", ",", "name_prefix", "+", "'_length.npy'", ")", "\n", "rel2scope_file_name", "=", "os", ".", "path", ".", "join", "(", "processed_data_dir", ",", "name_prefix", "+", "'_rel2scope.json'", ")", "\n", "word_vec_mat_file_name", "=", "os", ".", "path", ".", "join", "(", "processed_data_dir", ",", "word_vec_name_prefix", "+", "'_mat.npy'", ")", "\n", "word2id_file_name", "=", "os", ".", "path", ".", "join", "(", "processed_data_dir", ",", "word_vec_name_prefix", "+", "'_word2id.json'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "word_npy_file_name", ")", "or", "not", "os", ".", "path", ".", "exists", "(", "pos1_npy_file_name", ")", "or", "not", "os", ".", "path", ".", "exists", "(", "pos2_npy_file_name", ")", "or", "not", "os", ".", "path", ".", "exists", "(", "mask_npy_file_name", ")", "or", "not", "os", ".", "path", ".", "exists", "(", "length_npy_file_name", ")", "or", "not", "os", ".", "path", ".", "exists", "(", "rel2scope_file_name", ")", "or", "not", "os", ".", "path", ".", "exists", "(", "word_vec_mat_file_name", ")", "or", "not", "os", ".", "path", ".", "exists", "(", "word2id_file_name", ")", ":", "\n", "            ", "return", "False", "\n", "", "print", "(", "\"Pre-processed files exist. Loading them...\"", ")", "\n", "self", ".", "data_word", "=", "np", ".", "load", "(", "word_npy_file_name", ")", "\n", "self", ".", "data_pos1", "=", "np", ".", "load", "(", "pos1_npy_file_name", ")", "\n", "self", ".", "data_pos2", "=", "np", ".", "load", "(", "pos2_npy_file_name", ")", "\n", "self", ".", "data_mask", "=", "np", ".", "load", "(", "mask_npy_file_name", ")", "\n", "self", ".", "data_length", "=", "np", ".", "load", "(", "length_npy_file_name", ")", "\n", "self", ".", "rel2scope", "=", "json", ".", "load", "(", "open", "(", "rel2scope_file_name", ")", ")", "\n", "self", ".", "word_vec_mat", "=", "np", ".", "load", "(", "word_vec_mat_file_name", ")", "\n", "self", ".", "word2id", "=", "json", ".", "load", "(", "open", "(", "word2id_file_name", ")", ")", "\n", "if", "self", ".", "data_word", ".", "shape", "[", "1", "]", "!=", "self", ".", "max_length", ":", "\n", "            ", "print", "(", "\"Pre-processed files don't match current settings. Reprocessing...\"", ")", "\n", "return", "False", "\n", "", "print", "(", "\"Finish loading\"", ")", "\n", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.old_data_loader.JSONFileDataLoader.__init__": [[59, 209], ["print", "json.load", "print", "print", "json.load", "print", "len", "len", "print", "print", "numpy.zeros", "enumerate", "print", "print", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "print", "print", "numpy.save", "numpy.save", "numpy.save", "numpy.save", "numpy.save", "json.dump", "numpy.save", "json.dump", "print", "old_data_loader.JSONFileDataLoader._load_preprocessed_file", "Exception", "Exception", "open", "open", "print", "print", "len", "os.path.isdir", "os.mkdir", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "open", "os.path.join", "open", "os.path.isfile", "os.path.isfile", "w.lower.lower.lower", "numpy.sqrt", "enumerate", "range", "len", "min", "max", "range", "[].split", "[].split", "os.path.join", "os.path.join", "range", "numpy.sum", "len", "len", "[].lower", "file_name.split", "word_vec_file_name.split"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.old_data_loader.JSONFileDataLoader._load_preprocessed_file"], ["", "def", "__init__", "(", "self", ",", "file_name", ",", "word_vec_file_name", ",", "max_length", "=", "40", ",", "case_sensitive", "=", "False", ",", "reprocess", "=", "False", ",", "cuda", "=", "True", ")", ":", "\n", "        ", "'''\n        file_name: Json file storing the data in the following format\n            {\n                \"P155\": # relation id\n                    [\n                        {\n                            \"h\": [\"song for a future generation\", \"Q7561099\", [[16, 17, ...]]], # head entity [word, id, location]\n                            \"t\": [\"whammy kiss\", \"Q7990594\", [[11, 12]]], # tail entity [word, id, location]\n                            \"token\": [\"Hot\", \"Dance\", \"Club\", ...], # sentence\n                        },\n                        ...\n                    ],\n                \"P177\": \n                    [\n                        ...\n                    ]\n                ...\n            }\n        word_vec_file_name: Json file storing word vectors in the following format\n            [\n                {'word': 'the', 'vec': [0.418, 0.24968, ...]},\n                {'word': ',', 'vec': [0.013441, 0.23682, ...]},\n                ...\n            ]\n        max_length: The length that all the sentences need to be extend to.\n        case_sensitive: Whether the data processing is case-sensitive, default as False.\n        reprocess: Do the pre-processing whether there exist pre-processed files, default as False.\n        cuda: Use cuda or not, default as True.\n        '''", "\n", "self", ".", "file_name", "=", "file_name", "\n", "self", ".", "word_vec_file_name", "=", "word_vec_file_name", "\n", "self", ".", "case_sensitive", "=", "case_sensitive", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "cuda", "=", "cuda", "\n", "\n", "if", "reprocess", "or", "not", "self", ".", "_load_preprocessed_file", "(", ")", ":", "# Try to load pre-processed files:", "\n", "# Check files", "\n", "            ", "if", "file_name", "is", "None", "or", "not", "os", ".", "path", ".", "isfile", "(", "file_name", ")", ":", "\n", "                ", "raise", "Exception", "(", "\"[ERROR] Data file doesn't exist\"", ")", "\n", "", "if", "word_vec_file_name", "is", "None", "or", "not", "os", ".", "path", ".", "isfile", "(", "word_vec_file_name", ")", ":", "\n", "                ", "raise", "Exception", "(", "\"[ERROR] Word vector file doesn't exist\"", ")", "\n", "\n", "# Load files", "\n", "", "print", "(", "\"Loading data file...\"", ")", "\n", "self", ".", "ori_data", "=", "json", ".", "load", "(", "open", "(", "self", ".", "file_name", ",", "\"r\"", ")", ")", "\n", "print", "(", "\"Finish loading\"", ")", "\n", "print", "(", "\"Loading word vector file...\"", ")", "\n", "self", ".", "ori_word_vec", "=", "json", ".", "load", "(", "open", "(", "self", ".", "word_vec_file_name", ",", "\"r\"", ")", ")", "\n", "print", "(", "\"Finish loading\"", ")", "\n", "\n", "# Eliminate case sensitive", "\n", "if", "not", "case_sensitive", ":", "\n", "                ", "print", "(", "\"Elimiating case sensitive problem...\"", ")", "\n", "for", "relation", "in", "self", ".", "ori_data", ":", "\n", "                    ", "for", "ins", "in", "self", ".", "ori_data", "[", "relation", "]", ":", "\n", "                        ", "for", "i", "in", "range", "(", "len", "(", "ins", "[", "'tokens'", "]", ")", ")", ":", "\n", "                            ", "ins", "[", "'tokens'", "]", "[", "i", "]", "=", "ins", "[", "'tokens'", "]", "[", "i", "]", ".", "lower", "(", ")", "\n", "", "", "", "print", "(", "\"Finish eliminating\"", ")", "\n", "\n", "\n", "# Pre-process word vec", "\n", "", "self", ".", "word2id", "=", "{", "}", "\n", "self", ".", "word_vec_tot", "=", "len", "(", "self", ".", "ori_word_vec", ")", "\n", "UNK", "=", "self", ".", "word_vec_tot", "\n", "BLANK", "=", "self", ".", "word_vec_tot", "+", "1", "\n", "self", ".", "word_vec_dim", "=", "len", "(", "self", ".", "ori_word_vec", "[", "0", "]", "[", "'vec'", "]", ")", "\n", "print", "(", "\"Got {} words of {} dims\"", ".", "format", "(", "self", ".", "word_vec_tot", ",", "self", ".", "word_vec_dim", ")", ")", "\n", "print", "(", "\"Building word vector matrix and mapping...\"", ")", "\n", "self", ".", "word_vec_mat", "=", "np", ".", "zeros", "(", "(", "self", ".", "word_vec_tot", ",", "self", ".", "word_vec_dim", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "for", "cur_id", ",", "word", "in", "enumerate", "(", "self", ".", "ori_word_vec", ")", ":", "\n", "                ", "w", "=", "word", "[", "'word'", "]", "\n", "if", "not", "case_sensitive", ":", "\n", "                    ", "w", "=", "w", ".", "lower", "(", ")", "\n", "", "self", ".", "word2id", "[", "w", "]", "=", "cur_id", "\n", "self", ".", "word_vec_mat", "[", "cur_id", ",", ":", "]", "=", "word", "[", "'vec'", "]", "\n", "self", ".", "word_vec_mat", "[", "cur_id", "]", "=", "self", ".", "word_vec_mat", "[", "cur_id", "]", "/", "np", ".", "sqrt", "(", "np", ".", "sum", "(", "self", ".", "word_vec_mat", "[", "cur_id", "]", "**", "2", ")", ")", "\n", "", "self", ".", "word2id", "[", "'UNK'", "]", "=", "UNK", "\n", "self", ".", "word2id", "[", "'BLANK'", "]", "=", "BLANK", "\n", "print", "(", "\"Finish building\"", ")", "\n", "\n", "# Pre-process data", "\n", "print", "(", "\"Pre-processing data...\"", ")", "\n", "self", ".", "instance_tot", "=", "0", "\n", "for", "relation", "in", "self", ".", "ori_data", ":", "\n", "                ", "self", ".", "instance_tot", "+=", "len", "(", "self", ".", "ori_data", "[", "relation", "]", ")", "\n", "", "self", ".", "data_word", "=", "np", ".", "zeros", "(", "(", "self", ".", "instance_tot", ",", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "self", ".", "data_pos1", "=", "np", ".", "zeros", "(", "(", "self", ".", "instance_tot", ",", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "self", ".", "data_pos2", "=", "np", ".", "zeros", "(", "(", "self", ".", "instance_tot", ",", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "self", ".", "data_mask", "=", "np", ".", "zeros", "(", "(", "self", ".", "instance_tot", ",", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "self", ".", "data_length", "=", "np", ".", "zeros", "(", "(", "self", ".", "instance_tot", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "self", ".", "rel2scope", "=", "{", "}", "# left close right open", "\n", "i", "=", "0", "\n", "for", "relation", "in", "self", ".", "ori_data", ":", "\n", "                ", "self", ".", "rel2scope", "[", "relation", "]", "=", "[", "i", ",", "i", "]", "\n", "for", "ins", "in", "self", ".", "ori_data", "[", "relation", "]", ":", "\n", "                    ", "head", "=", "ins", "[", "'h'", "]", "[", "0", "]", "\n", "tail", "=", "ins", "[", "'t'", "]", "[", "0", "]", "\n", "pos1", "=", "ins", "[", "'h'", "]", "[", "2", "]", "[", "0", "]", "[", "0", "]", "\n", "pos2", "=", "ins", "[", "'t'", "]", "[", "2", "]", "[", "0", "]", "[", "0", "]", "\n", "words", "=", "ins", "[", "'tokens'", "]", "\n", "cur_ref_data_word", "=", "self", ".", "data_word", "[", "i", "]", "\n", "for", "j", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "                        ", "if", "j", "<", "max_length", ":", "\n", "                            ", "if", "word", "in", "self", ".", "word2id", ":", "\n", "                                ", "cur_ref_data_word", "[", "j", "]", "=", "self", ".", "word2id", "[", "word", "]", "\n", "", "else", ":", "\n", "                                ", "cur_ref_data_word", "[", "j", "]", "=", "UNK", "\n", "", "", "", "for", "j", "in", "range", "(", "j", "+", "1", ",", "max_length", ")", ":", "\n", "                        ", "cur_ref_data_word", "[", "j", "]", "=", "BLANK", "\n", "", "self", ".", "data_length", "[", "i", "]", "=", "len", "(", "words", ")", "\n", "if", "len", "(", "words", ")", ">", "max_length", ":", "\n", "                        ", "self", ".", "data_length", "[", "i", "]", "=", "max_length", "\n", "", "if", "pos1", ">=", "max_length", ":", "\n", "                        ", "pos1", "=", "max_length", "-", "1", "\n", "", "if", "pos2", ">=", "max_length", ":", "\n", "                        ", "pos2", "=", "max_length", "-", "1", "\n", "", "pos_min", "=", "min", "(", "pos1", ",", "pos2", ")", "\n", "pos_max", "=", "max", "(", "pos1", ",", "pos2", ")", "\n", "for", "j", "in", "range", "(", "max_length", ")", ":", "\n", "                        ", "self", ".", "data_pos1", "[", "i", "]", "[", "j", "]", "=", "j", "-", "pos1", "+", "max_length", "\n", "self", ".", "data_pos2", "[", "i", "]", "[", "j", "]", "=", "j", "-", "pos2", "+", "max_length", "\n", "if", "j", ">=", "self", ".", "data_length", "[", "i", "]", ":", "\n", "                            ", "self", ".", "data_mask", "[", "i", "]", "[", "j", "]", "=", "0", "\n", "", "elif", "j", "<=", "pos_min", ":", "\n", "                            ", "self", ".", "data_mask", "[", "i", "]", "[", "j", "]", "=", "1", "\n", "", "elif", "j", "<=", "pos_max", ":", "\n", "                            ", "self", ".", "data_mask", "[", "i", "]", "[", "j", "]", "=", "2", "\n", "", "else", ":", "\n", "                            ", "self", ".", "data_mask", "[", "i", "]", "[", "j", "]", "=", "3", "\n", "", "", "i", "+=", "1", "\n", "", "self", ".", "rel2scope", "[", "relation", "]", "[", "1", "]", "=", "i", "\n", "\n", "", "print", "(", "\"Finish pre-processing\"", ")", "\n", "\n", "print", "(", "\"Storing processed files...\"", ")", "\n", "name_prefix", "=", "'.'", ".", "join", "(", "file_name", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", ".", "split", "(", "'.'", ")", "[", ":", "-", "1", "]", ")", "\n", "word_vec_name_prefix", "=", "'.'", ".", "join", "(", "word_vec_file_name", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", ".", "split", "(", "'.'", ")", "[", ":", "-", "1", "]", ")", "\n", "processed_data_dir", "=", "'_processed_data'", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "processed_data_dir", ")", ":", "\n", "                ", "os", ".", "mkdir", "(", "processed_data_dir", ")", "\n", "", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "processed_data_dir", ",", "name_prefix", "+", "'_word.npy'", ")", ",", "self", ".", "data_word", ")", "\n", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "processed_data_dir", ",", "name_prefix", "+", "'_pos1.npy'", ")", ",", "self", ".", "data_pos1", ")", "\n", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "processed_data_dir", ",", "name_prefix", "+", "'_pos2.npy'", ")", ",", "self", ".", "data_pos2", ")", "\n", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "processed_data_dir", ",", "name_prefix", "+", "'_mask.npy'", ")", ",", "self", ".", "data_mask", ")", "\n", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "processed_data_dir", ",", "name_prefix", "+", "'_length.npy'", ")", ",", "self", ".", "data_length", ")", "\n", "json", ".", "dump", "(", "self", ".", "rel2scope", ",", "open", "(", "os", ".", "path", ".", "join", "(", "processed_data_dir", ",", "name_prefix", "+", "'_rel2scope.json'", ")", ",", "'w'", ")", ")", "\n", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "processed_data_dir", ",", "word_vec_name_prefix", "+", "'_mat.npy'", ")", ",", "self", ".", "word_vec_mat", ")", "\n", "json", ".", "dump", "(", "self", ".", "word2id", ",", "open", "(", "os", ".", "path", ".", "join", "(", "processed_data_dir", ",", "word_vec_name_prefix", "+", "'_word2id.json'", ")", ",", "'w'", ")", ")", "\n", "print", "(", "\"Finish storing\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.old_data_loader.JSONFileDataLoader.next_one": [[210, 255], ["random.sample", "enumerate", "numpy.stack", "numpy.stack", "numpy.stack", "numpy.stack", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "numpy.array", "numpy.random.permutation", "old_data_loader.JSONFileDataLoader.rel2scope.keys", "numpy.random.choice", "numpy.split", "numpy.split", "numpy.split", "numpy.split", "support_set[].append", "support_set[].append", "support_set[].append", "support_set[].append", "query_set[].append", "query_set[].append", "query_set[].append", "query_set[].append", "list", "range"], "methods", ["None"], ["", "", "def", "next_one", "(", "self", ",", "N", ",", "K", ",", "Q", ")", ":", "\n", "        ", "target_classes", "=", "random", ".", "sample", "(", "self", ".", "rel2scope", ".", "keys", "(", ")", ",", "N", ")", "\n", "support_set", "=", "{", "'word'", ":", "[", "]", ",", "'pos1'", ":", "[", "]", ",", "'pos2'", ":", "[", "]", ",", "'mask'", ":", "[", "]", "}", "\n", "query_set", "=", "{", "'word'", ":", "[", "]", ",", "'pos1'", ":", "[", "]", ",", "'pos2'", ":", "[", "]", ",", "'mask'", ":", "[", "]", "}", "\n", "query_label", "=", "[", "]", "\n", "\n", "for", "i", ",", "class_name", "in", "enumerate", "(", "target_classes", ")", ":", "\n", "            ", "scope", "=", "self", ".", "rel2scope", "[", "class_name", "]", "\n", "indices", "=", "np", ".", "random", ".", "choice", "(", "list", "(", "range", "(", "scope", "[", "0", "]", ",", "scope", "[", "1", "]", ")", ")", ",", "K", "+", "Q", ",", "False", ")", "\n", "word", "=", "self", ".", "data_word", "[", "indices", "]", "\n", "pos1", "=", "self", ".", "data_pos1", "[", "indices", "]", "\n", "pos2", "=", "self", ".", "data_pos2", "[", "indices", "]", "\n", "mask", "=", "self", ".", "data_mask", "[", "indices", "]", "\n", "support_word", ",", "query_word", ",", "_", "=", "np", ".", "split", "(", "word", ",", "[", "K", ",", "K", "+", "Q", "]", ")", "\n", "support_pos1", ",", "query_pos1", ",", "_", "=", "np", ".", "split", "(", "pos1", ",", "[", "K", ",", "K", "+", "Q", "]", ")", "\n", "support_pos2", ",", "query_pos2", ",", "_", "=", "np", ".", "split", "(", "pos2", ",", "[", "K", ",", "K", "+", "Q", "]", ")", "\n", "support_mask", ",", "query_mask", ",", "_", "=", "np", ".", "split", "(", "mask", ",", "[", "K", ",", "K", "+", "Q", "]", ")", "\n", "support_set", "[", "'word'", "]", ".", "append", "(", "support_word", ")", "\n", "support_set", "[", "'pos1'", "]", ".", "append", "(", "support_pos1", ")", "\n", "support_set", "[", "'pos2'", "]", ".", "append", "(", "support_pos2", ")", "\n", "support_set", "[", "'mask'", "]", ".", "append", "(", "support_mask", ")", "\n", "query_set", "[", "'word'", "]", ".", "append", "(", "query_word", ")", "\n", "query_set", "[", "'pos1'", "]", ".", "append", "(", "query_pos1", ")", "\n", "query_set", "[", "'pos2'", "]", ".", "append", "(", "query_pos2", ")", "\n", "query_set", "[", "'mask'", "]", ".", "append", "(", "query_mask", ")", "\n", "query_label", "+=", "[", "i", "]", "*", "Q", "\n", "\n", "", "support_set", "[", "'word'", "]", "=", "np", ".", "stack", "(", "support_set", "[", "'word'", "]", ",", "0", ")", "\n", "support_set", "[", "'pos1'", "]", "=", "np", ".", "stack", "(", "support_set", "[", "'pos1'", "]", ",", "0", ")", "\n", "support_set", "[", "'pos2'", "]", "=", "np", ".", "stack", "(", "support_set", "[", "'pos2'", "]", ",", "0", ")", "\n", "support_set", "[", "'mask'", "]", "=", "np", ".", "stack", "(", "support_set", "[", "'mask'", "]", ",", "0", ")", "\n", "query_set", "[", "'word'", "]", "=", "np", ".", "concatenate", "(", "query_set", "[", "'word'", "]", ",", "0", ")", "\n", "query_set", "[", "'pos1'", "]", "=", "np", ".", "concatenate", "(", "query_set", "[", "'pos1'", "]", ",", "0", ")", "\n", "query_set", "[", "'pos2'", "]", "=", "np", ".", "concatenate", "(", "query_set", "[", "'pos2'", "]", ",", "0", ")", "\n", "query_set", "[", "'mask'", "]", "=", "np", ".", "concatenate", "(", "query_set", "[", "'mask'", "]", ",", "0", ")", "\n", "query_label", "=", "np", ".", "array", "(", "query_label", ")", "\n", "\n", "perm", "=", "np", ".", "random", ".", "permutation", "(", "N", "*", "Q", ")", "\n", "query_set", "[", "'word'", "]", "=", "query_set", "[", "'word'", "]", "[", "perm", "]", "\n", "query_set", "[", "'pos1'", "]", "=", "query_set", "[", "'pos1'", "]", "[", "perm", "]", "\n", "query_set", "[", "'pos2'", "]", "=", "query_set", "[", "'pos2'", "]", "[", "perm", "]", "\n", "query_set", "[", "'mask'", "]", "=", "query_set", "[", "'mask'", "]", "[", "perm", "]", "\n", "query_label", "=", "query_label", "[", "perm", "]", "\n", "\n", "return", "support_set", ",", "query_set", ",", "query_label", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.old_data_loader.JSONFileDataLoader.next_batch": [[256, 290], ["range", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "old_data_loader.JSONFileDataLoader.next_one", "support[].append", "support[].append", "support[].append", "support[].append", "query[].append", "query[].append", "query[].append", "query[].append", "label.cuda.cuda.append", "torch.from_numpy().long().view", "torch.from_numpy().long().view", "torch.from_numpy().long().view", "torch.from_numpy().long().view", "torch.from_numpy().long().view", "torch.from_numpy().long().view", "torch.from_numpy().long().view", "torch.from_numpy().long().view", "torch.from_numpy().long", "label.cuda.cuda.cuda", "support[].cuda", "query[].cuda", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy", "numpy.stack().astype", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "numpy.stack", "numpy.stack", "numpy.stack", "numpy.stack", "numpy.stack", "numpy.stack", "numpy.stack", "numpy.stack", "numpy.stack"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.old_data_loader.JSONFileDataLoader.next_one"], ["", "def", "next_batch", "(", "self", ",", "B", ",", "N", ",", "K", ",", "Q", ")", ":", "\n", "        ", "support", "=", "{", "'word'", ":", "[", "]", ",", "'pos1'", ":", "[", "]", ",", "'pos2'", ":", "[", "]", ",", "'mask'", ":", "[", "]", "}", "\n", "query", "=", "{", "'word'", ":", "[", "]", ",", "'pos1'", ":", "[", "]", ",", "'pos2'", ":", "[", "]", ",", "'mask'", ":", "[", "]", "}", "\n", "label", "=", "[", "]", "\n", "for", "one_sample", "in", "range", "(", "B", ")", ":", "\n", "            ", "current_support", ",", "current_query", ",", "current_label", "=", "self", ".", "next_one", "(", "N", ",", "K", ",", "Q", ")", "\n", "support", "[", "'word'", "]", ".", "append", "(", "current_support", "[", "'word'", "]", ")", "\n", "support", "[", "'pos1'", "]", ".", "append", "(", "current_support", "[", "'pos1'", "]", ")", "\n", "support", "[", "'pos2'", "]", ".", "append", "(", "current_support", "[", "'pos2'", "]", ")", "\n", "support", "[", "'mask'", "]", ".", "append", "(", "current_support", "[", "'mask'", "]", ")", "\n", "query", "[", "'word'", "]", ".", "append", "(", "current_query", "[", "'word'", "]", ")", "\n", "query", "[", "'pos1'", "]", ".", "append", "(", "current_query", "[", "'pos1'", "]", ")", "\n", "query", "[", "'pos2'", "]", ".", "append", "(", "current_query", "[", "'pos2'", "]", ")", "\n", "query", "[", "'mask'", "]", ".", "append", "(", "current_query", "[", "'mask'", "]", ")", "\n", "label", ".", "append", "(", "current_label", ")", "\n", "", "support", "[", "'word'", "]", "=", "Variable", "(", "torch", ".", "from_numpy", "(", "np", ".", "stack", "(", "support", "[", "'word'", "]", ",", "0", ")", ")", ".", "long", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "max_length", ")", ")", "\n", "support", "[", "'pos1'", "]", "=", "Variable", "(", "torch", ".", "from_numpy", "(", "np", ".", "stack", "(", "support", "[", "'pos1'", "]", ",", "0", ")", ")", ".", "long", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "max_length", ")", ")", "\n", "support", "[", "'pos2'", "]", "=", "Variable", "(", "torch", ".", "from_numpy", "(", "np", ".", "stack", "(", "support", "[", "'pos2'", "]", ",", "0", ")", ")", ".", "long", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "max_length", ")", ")", "\n", "support", "[", "'mask'", "]", "=", "Variable", "(", "torch", ".", "from_numpy", "(", "np", ".", "stack", "(", "support", "[", "'mask'", "]", ",", "0", ")", ")", ".", "long", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "max_length", ")", ")", "\n", "query", "[", "'word'", "]", "=", "Variable", "(", "torch", ".", "from_numpy", "(", "np", ".", "stack", "(", "query", "[", "'word'", "]", ",", "0", ")", ")", ".", "long", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "max_length", ")", ")", "\n", "query", "[", "'pos1'", "]", "=", "Variable", "(", "torch", ".", "from_numpy", "(", "np", ".", "stack", "(", "query", "[", "'pos1'", "]", ",", "0", ")", ")", ".", "long", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "max_length", ")", ")", "\n", "query", "[", "'pos2'", "]", "=", "Variable", "(", "torch", ".", "from_numpy", "(", "np", ".", "stack", "(", "query", "[", "'pos2'", "]", ",", "0", ")", ")", ".", "long", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "max_length", ")", ")", "\n", "query", "[", "'mask'", "]", "=", "Variable", "(", "torch", ".", "from_numpy", "(", "np", ".", "stack", "(", "query", "[", "'mask'", "]", ",", "0", ")", ")", ".", "long", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "max_length", ")", ")", "\n", "label", "=", "Variable", "(", "torch", ".", "from_numpy", "(", "np", ".", "stack", "(", "label", ",", "0", ")", ".", "astype", "(", "np", ".", "int64", ")", ")", ".", "long", "(", ")", ")", "\n", "\n", "# To cuda", "\n", "if", "self", ".", "cuda", ":", "\n", "            ", "for", "key", "in", "support", ":", "\n", "                ", "support", "[", "key", "]", "=", "support", "[", "key", "]", ".", "cuda", "(", ")", "\n", "", "for", "key", "in", "query", ":", "\n", "                ", "query", "[", "key", "]", "=", "query", "[", "key", "]", ".", "cuda", "(", ")", "\n", "", "label", "=", "label", ".", "cuda", "(", ")", "\n", "\n", "", "return", "support", ",", "query", ",", "label", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.CNNSentenceEncoder.__init__": [[16, 26], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "network.embedding.Embedding", "network.encoder.Encoder"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "word_vec_mat", ",", "word2id", ",", "max_length", ",", "word_embedding_dim", "=", "50", ",", "\n", "pos_embedding_dim", "=", "5", ",", "hidden_size", "=", "230", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "embedding", "=", "network", ".", "embedding", ".", "Embedding", "(", "word_vec_mat", ",", "max_length", ",", "\n", "word_embedding_dim", ",", "pos_embedding_dim", ")", "\n", "self", ".", "encoder", "=", "network", ".", "encoder", ".", "Encoder", "(", "max_length", ",", "word_embedding_dim", ",", "\n", "pos_embedding_dim", ",", "hidden_size", ")", "\n", "self", ".", "word2id", "=", "word2id", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.CNNSentenceEncoder.forward": [[27, 31], ["test.CNNSentenceEncoder.embedding", "test.CNNSentenceEncoder.encoder"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "x", "=", "self", ".", "embedding", "(", "inputs", ")", "\n", "x", "=", "self", ".", "encoder", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.CNNSentenceEncoder.tokenize": [[32, 61], ["numpy.zeros", "numpy.zeros", "min", "min", "range", "numpy.zeros", "token.lower.lower.lower", "len", "indexed_tokens.append", "indexed_tokens.append", "indexed_tokens.append", "len"], "methods", ["None"], ["", "def", "tokenize", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ")", ":", "\n", "# token -> index", "\n", "        ", "indexed_tokens", "=", "[", "]", "\n", "for", "token", "in", "raw_tokens", ":", "\n", "            ", "token", "=", "token", ".", "lower", "(", ")", "\n", "if", "token", "in", "self", ".", "word2id", ":", "\n", "                ", "indexed_tokens", ".", "append", "(", "self", ".", "word2id", "[", "token", "]", ")", "\n", "", "else", ":", "\n", "                ", "indexed_tokens", ".", "append", "(", "self", ".", "word2id", "[", "'[UNK]'", "]", ")", "\n", "\n", "# padding", "\n", "", "", "while", "len", "(", "indexed_tokens", ")", "<", "self", ".", "max_length", ":", "\n", "            ", "indexed_tokens", ".", "append", "(", "self", ".", "word2id", "[", "'[PAD]'", "]", ")", "\n", "", "indexed_tokens", "=", "indexed_tokens", "[", ":", "self", ".", "max_length", "]", "\n", "\n", "# pos", "\n", "pos1", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "pos2", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "pos1_in_index", "=", "min", "(", "self", ".", "max_length", ",", "pos_head", "[", "0", "]", ")", "\n", "pos2_in_index", "=", "min", "(", "self", ".", "max_length", ",", "pos_tail", "[", "0", "]", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "max_length", ")", ":", "\n", "            ", "pos1", "[", "i", "]", "=", "i", "-", "pos1_in_index", "+", "self", ".", "max_length", "\n", "pos2", "[", "i", "]", "=", "i", "-", "pos2_in_index", "+", "self", ".", "max_length", "\n", "\n", "# mask", "\n", "", "mask", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "mask", "[", ":", "len", "(", "indexed_tokens", ")", "]", "=", "1", "\n", "\n", "return", "indexed_tokens", ",", "pos1", ",", "pos2", ",", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTSentenceEncoder.__init__": [[65, 72], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "transformers.BertModel.from_pretrained", "transformers.BertTokenizer.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "pretrain_path", ",", "max_length", ",", "cat_entity_rep", "=", "False", ",", "mask_entity", "=", "False", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "self", ".", "bert", "=", "BertModel", ".", "from_pretrained", "(", "pretrain_path", ")", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", "\n", "self", ".", "cat_entity_rep", "=", "cat_entity_rep", "\n", "self", ".", "mask_entity", "=", "mask_entity", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTSentenceEncoder.forward": [[73, 86], ["test.BERTSentenceEncoder.bert", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "test.BERTSentenceEncoder.bert", "inputs[].size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "if", "not", "self", ".", "cat_entity_rep", ":", "\n", "# _, x = self.bert(inputs['word'], attention_mask=inputs['mask'])", "\n", "            ", "x", "=", "self", ".", "bert", "(", "inputs", "[", "'word'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "[", "1", "]", "\n", "\n", "return", "x", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "self", ".", "bert", "(", "inputs", "[", "'word'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "\n", "tensor_range", "=", "torch", ".", "arange", "(", "inputs", "[", "'word'", "]", ".", "size", "(", ")", "[", "0", "]", ")", "\n", "h_state", "=", "outputs", "[", "0", "]", "[", "tensor_range", ",", "inputs", "[", "\"pos1\"", "]", "]", "\n", "t_state", "=", "outputs", "[", "0", "]", "[", "tensor_range", ",", "inputs", "[", "\"pos2\"", "]", "]", "\n", "state", "=", "torch", ".", "cat", "(", "(", "h_state", ",", "t_state", ")", ",", "-", "1", ")", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTSentenceEncoder.tokenize": [[87, 133], ["test.BERTSentenceEncoder.tokenizer.convert_tokens_to_ids", "numpy.zeros", "numpy.zeros", "range", "numpy.zeros", "min", "min", "token.lower.lower.lower", "len", "test.BERTSentenceEncoder.append", "tokens.append", "len", "tokens.append", "len", "test.BERTSentenceEncoder.tokenizer.tokenize", "tokens.append", "tokens.append", "len"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "", "def", "tokenize", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ")", ":", "\n", "# token -> index", "\n", "        ", "tokens", "=", "[", "'[CLS]'", "]", "\n", "cur_pos", "=", "0", "\n", "pos1_in_index", "=", "1", "\n", "pos2_in_index", "=", "1", "\n", "for", "token", "in", "raw_tokens", ":", "\n", "            ", "token", "=", "token", ".", "lower", "(", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused0]'", ")", "\n", "pos1_in_index", "=", "len", "(", "tokens", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused1]'", ")", "\n", "pos2_in_index", "=", "len", "(", "tokens", ")", "\n", "", "if", "self", ".", "mask_entity", "and", "(", "(", "pos_head", "[", "0", "]", "<=", "cur_pos", "and", "cur_pos", "<=", "pos_head", "[", "-", "1", "]", ")", "or", "(", "\n", "pos_tail", "[", "0", "]", "<=", "cur_pos", "and", "cur_pos", "<=", "pos_tail", "[", "-", "1", "]", ")", ")", ":", "\n", "                ", "tokens", "+=", "[", "'[unused4]'", "]", "\n", "", "else", ":", "\n", "                ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "", "if", "cur_pos", "==", "pos_head", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused2]'", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused3]'", ")", "\n", "", "cur_pos", "+=", "1", "\n", "", "indexed_tokens", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# padding", "\n", "while", "len", "(", "indexed_tokens", ")", "<", "self", ".", "max_length", ":", "\n", "            ", "indexed_tokens", ".", "append", "(", "0", ")", "\n", "", "indexed_tokens", "=", "indexed_tokens", "[", ":", "self", ".", "max_length", "]", "\n", "\n", "# pos", "\n", "pos1", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "pos2", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "max_length", ")", ":", "\n", "            ", "pos1", "[", "i", "]", "=", "i", "-", "pos1_in_index", "+", "self", ".", "max_length", "\n", "pos2", "[", "i", "]", "=", "i", "-", "pos2_in_index", "+", "self", ".", "max_length", "\n", "\n", "# mask", "\n", "", "mask", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "mask", "[", ":", "len", "(", "tokens", ")", "]", "=", "1", "\n", "\n", "pos1_in_index", "=", "min", "(", "self", ".", "max_length", ",", "pos1_in_index", ")", "\n", "pos2_in_index", "=", "min", "(", "self", ".", "max_length", ",", "pos2_in_index", ")", "\n", "\n", "return", "indexed_tokens", ",", "pos1_in_index", "-", "1", ",", "pos2_in_index", "-", "1", ",", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTConceptSentenceEncoder.__init__": [[137, 145], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "transformers.BertModel.from_pretrained", "transformers.BertTokenizer.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "pretrain_path", ",", "max_length", ",", "sentenceORword", ",", "cat_entity_rep", "=", "False", ",", "mask_entity", "=", "False", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "self", ".", "bert", "=", "BertModel", ".", "from_pretrained", "(", "pretrain_path", ")", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", "\n", "self", ".", "cat_entity_rep", "=", "cat_entity_rep", "\n", "self", ".", "mask_entity", "=", "mask_entity", "\n", "self", ".", "sentenceORword", "=", "sentenceORword", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTConceptSentenceEncoder.forward": [[146, 164], ["test.BERTConceptSentenceEncoder.bert", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "test.BERTConceptSentenceEncoder.bert", "inputs[].size", "test.BERTConceptSentenceEncoder.bert", "test.BERTConceptSentenceEncoder.bert"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "if", "not", "self", ".", "cat_entity_rep", ":", "\n", "            ", "if", "self", ".", "sentenceORword", "==", "'sentence'", ":", "\n", "                ", "_", ",", "x", "=", "self", ".", "bert", "(", "inputs", "[", "'word'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "\n", "return", "x", "\n", "", "elif", "self", ".", "sentenceORword", "==", "'word'", ":", "\n", "\n", "                ", "wordEmbedding", "=", "self", ".", "bert", "(", "inputs", "[", "'word'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "[", "0", "]", "\n", "sentenceEmbedding", "=", "self", ".", "bert", "(", "inputs", "[", "'word'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "[", "1", "]", "\n", "\n", "return", "wordEmbedding", ",", "sentenceEmbedding", "\n", "", "", "else", ":", "\n", "            ", "outputs", "=", "self", ".", "bert", "(", "inputs", "[", "'word'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "\n", "tensor_range", "=", "torch", ".", "arange", "(", "inputs", "[", "'word'", "]", ".", "size", "(", ")", "[", "0", "]", ")", "\n", "h_state", "=", "outputs", "[", "0", "]", "[", "tensor_range", ",", "inputs", "[", "\"pos1\"", "]", "]", "\n", "t_state", "=", "outputs", "[", "0", "]", "[", "tensor_range", ",", "inputs", "[", "\"pos2\"", "]", "]", "\n", "state", "=", "torch", ".", "cat", "(", "(", "h_state", ",", "t_state", ")", ",", "-", "1", ")", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTConceptSentenceEncoder.tokenize": [[165, 211], ["test.BERTConceptSentenceEncoder.tokenizer.convert_tokens_to_ids", "numpy.zeros", "numpy.zeros", "range", "numpy.zeros", "min", "min", "token.lower.lower.lower", "len", "test.BERTConceptSentenceEncoder.append", "tokens.append", "len", "tokens.append", "len", "test.BERTConceptSentenceEncoder.tokenizer.tokenize", "tokens.append", "tokens.append", "len"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "", "def", "tokenize", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ")", ":", "\n", "# token -> index", "\n", "        ", "tokens", "=", "[", "'[CLS]'", "]", "\n", "cur_pos", "=", "0", "\n", "pos1_in_index", "=", "1", "\n", "pos2_in_index", "=", "1", "\n", "for", "token", "in", "raw_tokens", ":", "\n", "            ", "token", "=", "token", ".", "lower", "(", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused0]'", ")", "\n", "pos1_in_index", "=", "len", "(", "tokens", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused1]'", ")", "\n", "pos2_in_index", "=", "len", "(", "tokens", ")", "\n", "", "if", "self", ".", "mask_entity", "and", "(", "(", "pos_head", "[", "0", "]", "<=", "cur_pos", "and", "cur_pos", "<=", "pos_head", "[", "-", "1", "]", ")", "or", "(", "\n", "pos_tail", "[", "0", "]", "<=", "cur_pos", "and", "cur_pos", "<=", "pos_tail", "[", "-", "1", "]", ")", ")", ":", "\n", "                ", "tokens", "+=", "[", "'[unused4]'", "]", "\n", "", "else", ":", "\n", "                ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "", "if", "cur_pos", "==", "pos_head", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused2]'", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused3]'", ")", "\n", "", "cur_pos", "+=", "1", "\n", "", "indexed_tokens", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# padding", "\n", "while", "len", "(", "indexed_tokens", ")", "<", "self", ".", "max_length", ":", "\n", "            ", "indexed_tokens", ".", "append", "(", "0", ")", "\n", "", "indexed_tokens", "=", "indexed_tokens", "[", ":", "self", ".", "max_length", "]", "\n", "\n", "# pos", "\n", "pos1", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "pos2", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "max_length", ")", ":", "\n", "            ", "pos1", "[", "i", "]", "=", "i", "-", "pos1_in_index", "+", "self", ".", "max_length", "\n", "pos2", "[", "i", "]", "=", "i", "-", "pos2_in_index", "+", "self", ".", "max_length", "\n", "\n", "# mask", "\n", "", "mask", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "mask", "[", ":", "len", "(", "tokens", ")", "]", "=", "1", "\n", "\n", "pos1_in_index", "=", "min", "(", "self", ".", "max_length", ",", "pos1_in_index", ")", "\n", "pos2_in_index", "=", "min", "(", "self", ".", "max_length", ",", "pos2_in_index", ")", "\n", "\n", "return", "indexed_tokens", ",", "pos1_in_index", "-", "1", ",", "pos2_in_index", "-", "1", ",", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTConceptSentenceEncoder.tokenize_concept": [[212, 297], ["h.lower.lower.lower", "fewshot_re_kit.conceptgraph_utils.instance2conept", "h2concept[].lower", "h2concept[].lower", "t.lower.lower.lower", "fewshot_re_kit.conceptgraph_utils.instance2conept", "t2concept[].lower", "t2concept[].lower", "tokens.append", "tokens.append", "tokens.append", "tokens.append", "test.BERTConceptSentenceEncoder.tokenizer.convert_tokens_to_ids", "numpy.zeros", "numpy.zeros", "range", "numpy.zeros", "min", "min", "token.lower.lower.lower", "tokens.append", "test.BERTConceptSentenceEncoder.tokenizer.tokenize", "tokens.append", "test.BERTConceptSentenceEncoder.tokenizer.tokenize", "tokens.append", "test.BERTConceptSentenceEncoder.tokenizer.tokenize", "tokens.append", "test.BERTConceptSentenceEncoder.tokenizer.tokenize", "len", "test.BERTConceptSentenceEncoder.append", "tokens.append", "len", "tokens.append", "len", "test.BERTConceptSentenceEncoder.tokenizer.tokenize", "tokens.append", "tokens.append", "len"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.instance2conept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.instance2conept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "def", "tokenize_concept", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ",", "h", ",", "t", ",", "ins2cpt", ")", ":", "\n", "# token -> index", "\n", "        ", "tokens", "=", "[", "'[CLS]'", "]", "\n", "cur_pos", "=", "0", "\n", "pos1_in_index", "=", "1", "\n", "pos2_in_index", "=", "1", "\n", "for", "token", "in", "raw_tokens", ":", "\n", "            ", "token", "=", "token", ".", "lower", "(", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused0]'", ")", "\n", "pos1_in_index", "=", "len", "(", "tokens", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused1]'", ")", "\n", "pos2_in_index", "=", "len", "(", "tokens", ")", "\n", "", "if", "self", ".", "mask_entity", "and", "(", "(", "pos_head", "[", "0", "]", "<=", "cur_pos", "and", "cur_pos", "<=", "pos_head", "[", "-", "1", "]", ")", "or", "(", "\n", "pos_tail", "[", "0", "]", "<=", "cur_pos", "and", "cur_pos", "<=", "pos_tail", "[", "-", "1", "]", ")", ")", ":", "\n", "                ", "tokens", "+=", "[", "'[unused4]'", "]", "\n", "", "else", ":", "\n", "                ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "", "if", "cur_pos", "==", "pos_head", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused2]'", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused3]'", ")", "\n", "", "cur_pos", "+=", "1", "\n", "", "'''\u6dfb\u52a0\u5b9e\u4f53\u7684\u6982\u5ff5\u5230tokens\u4e2d'''", "\n", "h", "=", "h", ".", "lower", "(", ")", "\n", "h2concept", "=", "instance2conept", "(", "ins2cpt", ",", "h", ")", "\n", "h2concept1", "=", "h2concept", "[", "0", "]", ".", "lower", "(", ")", "\n", "h2concept2", "=", "h2concept", "[", "1", "]", ".", "lower", "(", ")", "\n", "t", "=", "t", ".", "lower", "(", ")", "\n", "t2concept", "=", "instance2conept", "(", "ins2cpt", ",", "t", ")", "\n", "t2concept1", "=", "t2concept", "[", "0", "]", ".", "lower", "(", ")", "\n", "t2concept2", "=", "t2concept", "[", "1", "]", ".", "lower", "(", ")", "\n", "\n", "tokens", ".", "append", "(", "'[unused4]'", ")", "\n", "if", "(", "h2concept1", "==", "'unknowconcept1'", ")", "or", "(", "h2concept1", "==", "'unknowconcept2'", ")", ":", "\n", "# print('-----------I am running-----------------------')", "\n", "            ", "tokens", ".", "append", "(", "h2concept1", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "h2concept1", ")", "\n", "\n", "", "tokens", ".", "append", "(", "'[unused5]'", ")", "\n", "if", "(", "h2concept2", "==", "'unknowconcept1'", ")", "or", "(", "h2concept2", "==", "'unknowconcept2'", ")", ":", "\n", "            ", "tokens", ".", "append", "(", "h2concept2", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "h2concept2", ")", "\n", "\n", "", "tokens", ".", "append", "(", "'[unused6]'", ")", "\n", "if", "(", "t2concept1", "==", "'unknowconcept1'", ")", "or", "(", "t2concept1", "==", "'unknowconcept2'", ")", ":", "\n", "# print('-----------I am running-----------------------')", "\n", "\n", "            ", "tokens", ".", "append", "(", "t2concept1", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "t2concept1", ")", "\n", "\n", "", "tokens", ".", "append", "(", "'[unused7]'", ")", "\n", "if", "(", "t2concept2", "==", "'unknowconcept1'", ")", "or", "(", "t2concept2", "==", "'unknowconcept2'", ")", ":", "\n", "            ", "tokens", ".", "append", "(", "t2concept2", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "t2concept2", ")", "\n", "# print('------------------tokens-----------------------')", "\n", "# print(tokens)", "\n", "\n", "", "indexed_tokens", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# padding", "\n", "while", "len", "(", "indexed_tokens", ")", "<", "self", ".", "max_length", ":", "\n", "            ", "indexed_tokens", ".", "append", "(", "0", ")", "\n", "", "indexed_tokens", "=", "indexed_tokens", "[", ":", "self", ".", "max_length", "]", "\n", "\n", "# pos", "\n", "pos1", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "pos2", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "max_length", ")", ":", "\n", "            ", "pos1", "[", "i", "]", "=", "i", "-", "pos1_in_index", "+", "self", ".", "max_length", "\n", "pos2", "[", "i", "]", "=", "i", "-", "pos2_in_index", "+", "self", ".", "max_length", "\n", "\n", "# mask", "\n", "", "mask", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "mask", "[", ":", "len", "(", "tokens", ")", "]", "=", "1", "\n", "\n", "pos1_in_index", "=", "min", "(", "self", ".", "max_length", ",", "pos1_in_index", ")", "\n", "pos2_in_index", "=", "min", "(", "self", ".", "max_length", ",", "pos2_in_index", ")", "\n", "\n", "return", "indexed_tokens", ",", "pos1_in_index", "-", "1", ",", "pos2_in_index", "-", "1", ",", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRSentenceEncoder.__init__": [[301, 308], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "transformers.BertForSequenceClassification.from_pretrained", "transformers.BertTokenizer.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "pretrain_path", ",", "max_length", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "self", ".", "bert", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "\n", "pretrain_path", ",", "\n", "num_labels", "=", "2", ")", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRSentenceEncoder.forward": [[309, 343], ["test.BERTPAIRSentenceEncoder.bert"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "\n", "        ", "x", "=", "self", ".", "bert", "(", "inputs", "[", "'word'", "]", ",", "token_type_ids", "=", "inputs", "[", "'seg'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "[", "0", "]", "\n", "# '''\u4fee\u6539\u90e8\u5206'''", "\n", "# print('------------------------------inputs[word]-----------------------------')", "\n", "# print(inputs['word'])", "\n", "# print(inputs['word'].shape)", "\n", "#", "\n", "# print('------------------------------inputs[query_sen]-----------------------------')", "\n", "# print(inputs['query_sen'])", "\n", "# print(inputs['query_sen'].shape)", "\n", "#", "\n", "# print('------------------------------inputs[support_sen]-----------------------------')", "\n", "# print(inputs['support_sen'])", "\n", "# print(inputs['support_sen'].shape)", "\n", "#", "\n", "# print('------------------------------inputs[support_mask]-----------------------------')", "\n", "# print(inputs['support_mask'])", "\n", "# print(inputs['support_mask'].shape)", "\n", "#", "\n", "# print('------------------------------inputs[queryConceptID]-----------------------------')", "\n", "# print(inputs['queryConceptID'])", "\n", "# print(inputs['queryConceptID'].shape)", "\n", "#", "\n", "# print('------------------------------inputs[supportConceptID]-----------------------------')", "\n", "# print(inputs['supportConceptID'])", "\n", "# print(inputs['supportConceptID'].shape)", "\n", "#", "\n", "# '''\u67e5\u770bx\u7684\u4fe1\u606f'''", "\n", "# print('------------------------------\u67e5\u770bx\u7684\u4fe1\u606f-----------------------------')", "\n", "# print(x.shape)", "\n", "# print(x)", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRSentenceEncoder.tokenize": [[344, 369], ["test.BERTPAIRSentenceEncoder.tokenizer.convert_tokens_to_ids", "token.lower.lower.lower", "test.BERTPAIRSentenceEncoder.tokenizer.tokenize", "tokens.append", "len", "tokens.append", "len", "tokens.append", "tokens.append"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "def", "tokenize", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ")", ":", "\n", "# token -> index", "\n", "# tokens = ['[CLS]']", "\n", "        ", "tokens", "=", "[", "]", "\n", "cur_pos", "=", "0", "\n", "pos1_in_index", "=", "0", "\n", "pos2_in_index", "=", "0", "\n", "for", "token", "in", "raw_tokens", ":", "\n", "            ", "token", "=", "token", ".", "lower", "(", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused0]'", ")", "\n", "pos1_in_index", "=", "len", "(", "tokens", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused1]'", ")", "\n", "pos2_in_index", "=", "len", "(", "tokens", ")", "\n", "", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused2]'", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused3]'", ")", "\n", "", "cur_pos", "+=", "1", "\n", "\n", "", "indexed_tokens", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "return", "indexed_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRSentenceEncoder.tokenize_concept": [[370, 431], ["h.lower.lower.lower", "fewshot_re_kit.conceptgraph_utils.instance2conept", "h2concept[].lower", "h2concept[].lower", "t.lower.lower.lower", "fewshot_re_kit.conceptgraph_utils.instance2conept", "t2concept[].lower", "t2concept[].lower", "tokens.append", "tokens.append", "tokens.append", "tokens.append", "test.BERTPAIRSentenceEncoder.tokenizer.convert_tokens_to_ids", "token.lower.lower.lower", "test.BERTPAIRSentenceEncoder.tokenizer.tokenize", "tokens.append", "test.BERTPAIRSentenceEncoder.tokenizer.tokenize", "tokens.append", "test.BERTPAIRSentenceEncoder.tokenizer.tokenize", "tokens.append", "test.BERTPAIRSentenceEncoder.tokenizer.tokenize", "tokens.append", "test.BERTPAIRSentenceEncoder.tokenizer.tokenize", "tokens.append", "len", "tokens.append", "len", "tokens.append", "tokens.append"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.instance2conept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.instance2conept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "def", "tokenize_concept", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ",", "h", ",", "t", ",", "ins2cpt", ")", ":", "\n", "# token -> index", "\n", "# tokens = ['[CLS]']", "\n", "        ", "tokens", "=", "[", "]", "\n", "cur_pos", "=", "0", "\n", "pos1_in_index", "=", "0", "\n", "pos2_in_index", "=", "0", "\n", "for", "token", "in", "raw_tokens", ":", "\n", "            ", "token", "=", "token", ".", "lower", "(", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused0]'", ")", "\n", "pos1_in_index", "=", "len", "(", "tokens", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused1]'", ")", "\n", "pos2_in_index", "=", "len", "(", "tokens", ")", "\n", "", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused2]'", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused3]'", ")", "\n", "", "cur_pos", "+=", "1", "\n", "", "'''\u6dfb\u52a0\u5b9e\u4f53\u7684\u6982\u5ff5\u5230tokens\u4e2d'''", "\n", "h", "=", "h", ".", "lower", "(", ")", "\n", "h2concept", "=", "instance2conept", "(", "ins2cpt", ",", "h", ")", "\n", "h2concept1", "=", "h2concept", "[", "0", "]", ".", "lower", "(", ")", "\n", "h2concept2", "=", "h2concept", "[", "1", "]", ".", "lower", "(", ")", "\n", "t", "=", "t", ".", "lower", "(", ")", "\n", "t2concept", "=", "instance2conept", "(", "ins2cpt", ",", "t", ")", "\n", "t2concept1", "=", "t2concept", "[", "0", "]", ".", "lower", "(", ")", "\n", "t2concept2", "=", "t2concept", "[", "1", "]", ".", "lower", "(", ")", "\n", "\n", "tokens", ".", "append", "(", "'[unused4]'", ")", "\n", "if", "(", "h2concept1", "==", "'unknowconcept1'", ")", "or", "(", "h2concept1", "==", "'unknowconcept2'", ")", ":", "\n", "# print('-----------I am running-----------------------')", "\n", "            ", "tokens", ".", "append", "(", "h2concept1", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "h2concept1", ")", "\n", "\n", "", "tokens", ".", "append", "(", "'[unused5]'", ")", "\n", "if", "(", "h2concept2", "==", "'unknowconcept1'", ")", "or", "(", "h2concept2", "==", "'unknowconcept2'", ")", ":", "\n", "            ", "tokens", ".", "append", "(", "h2concept2", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "h2concept2", ")", "\n", "\n", "", "tokens", ".", "append", "(", "'[unused6]'", ")", "\n", "if", "(", "t2concept1", "==", "'unknowconcept1'", ")", "or", "(", "t2concept1", "==", "'unknowconcept2'", ")", ":", "\n", "# print('-----------I am running-----------------------')", "\n", "\n", "            ", "tokens", ".", "append", "(", "t2concept1", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "t2concept1", ")", "\n", "\n", "", "tokens", ".", "append", "(", "'[unused7]'", ")", "\n", "if", "(", "t2concept2", "==", "'unknowconcept1'", ")", "or", "(", "t2concept2", "==", "'unknowconcept2'", ")", ":", "\n", "            ", "tokens", ".", "append", "(", "t2concept2", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "t2concept2", ")", "\n", "# print('------------------tokens-----------------------')", "\n", "# print(tokens)", "\n", "", "indexed_tokens", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "return", "indexed_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRSentenceEncoder.tokenize_concept_plus": [[432, 478], ["h.lower.lower.lower", "fewshot_re_kit.conceptgraph_utils.instance2coneptPlus", "t.lower.lower.lower", "fewshot_re_kit.conceptgraph_utils.instance2coneptPlus", "tokens.append", "tokens.append", "test.BERTPAIRSentenceEncoder.tokenizer.convert_tokens_to_ids", "token.lower.lower.lower", "test.BERTPAIRSentenceEncoder.tokenizer.tokenize", "tokens.append", "len", "tokens.append", "len", "tokens.append", "tokens.append", "tokens.append", "test.BERTPAIRSentenceEncoder.tokenizer.tokenize", "tokens.append", "test.BERTPAIRSentenceEncoder.tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.instance2coneptPlus", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.instance2coneptPlus", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "def", "tokenize_concept_plus", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ",", "h", ",", "t", ",", "ins2cpt", ")", ":", "\n", "# token -> index", "\n", "# tokens = ['[CLS]']", "\n", "\n", "        ", "tokens", "=", "[", "]", "\n", "cur_pos", "=", "0", "\n", "pos1_in_index", "=", "0", "\n", "pos2_in_index", "=", "0", "\n", "for", "token", "in", "raw_tokens", ":", "\n", "            ", "token", "=", "token", ".", "lower", "(", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused0]'", ")", "\n", "pos1_in_index", "=", "len", "(", "tokens", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused1]'", ")", "\n", "pos2_in_index", "=", "len", "(", "tokens", ")", "\n", "", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused2]'", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused3]'", ")", "\n", "", "cur_pos", "+=", "1", "\n", "", "'''\u6dfb\u52a0\u5b9e\u4f53\u7684\u6982\u5ff5\u5230tokens\u4e2d'''", "\n", "h", "=", "h", ".", "lower", "(", ")", "\n", "h2concept", "=", "instance2coneptPlus", "(", "ins2cpt", ",", "h", ")", "\n", "t", "=", "t", ".", "lower", "(", ")", "\n", "t2concept", "=", "instance2coneptPlus", "(", "ins2cpt", ",", "t", ")", "\n", "\n", "tokens", ".", "append", "(", "'[unused4]'", ")", "\n", "for", "cpt", "in", "h2concept", ":", "\n", "            ", "if", "cpt", "==", "'unknowConcept'", ":", "\n", "                ", "tokens", ".", "append", "(", "cpt", ")", "\n", "", "else", ":", "\n", "                ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "cpt", ")", "\n", "\n", "", "", "tokens", ".", "append", "(", "'[unused5]'", ")", "\n", "for", "cpt", "in", "t2concept", ":", "\n", "            ", "if", "cpt", "==", "'unknowConcept'", ":", "\n", "                ", "tokens", ".", "append", "(", "cpt", ")", "\n", "", "else", ":", "\n", "                ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "cpt", ")", "\n", "\n", "# print('------------------tokens-----------------------')", "\n", "# print(tokens)", "\n", "", "", "indexed_tokens", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "return", "indexed_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.__init__": [[482, 538], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "print", "transformers.BertModel.from_pretrained", "transformers.BertTokenizer.from_pretrained", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "pretrain_path", ",", "max_length", ",", "conceptEmbedding", ",", "BeyondWordEmbedding", ",", "id2embeddingID", ",", "\n", "id_from", "=", "'kgEmbeddingOrBeyondWordEmbedding'", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "print", "(", "'----------------------BERTPAIRConceptSentenceEncoder initializing----------------------------------'", ")", "\n", "self", ".", "bert", "=", "BertModel", ".", "from_pretrained", "(", "\n", "pretrain_path", ")", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", "\n", "self", ".", "conceptEmbedding", "=", "conceptEmbedding", "\n", "self", ".", "beyondWordEmbedding", "=", "BeyondWordEmbedding", "# \u8bcd\u5411\u91cf500\u7ef4", "\n", "self", ".", "id2embeddingID", "=", "id2embeddingID", "\n", "self", ".", "id_from", "=", "id_from", "\n", "\n", "if", "self", ".", "id_from", "==", "'keEmbedding'", ":", "\n", "            ", "self", ".", "projector1", "=", "torch", ".", "nn", ".", "Linear", "(", "256", ",", "768", ")", "\n", "self", ".", "projector2", "=", "torch", ".", "nn", ".", "Linear", "(", "768", ",", "768", ")", "\n", "self", ".", "fusionLayer", "=", "torch", ".", "nn", ".", "MultiheadAttention", "(", "embed_dim", "=", "768", ",", "num_heads", "=", "12", ",", "dropout", "=", "0.1", ")", "\n", "self", ".", "classifier", "=", "torch", ".", "nn", ".", "Linear", "(", "136", "*", "768", ",", "2", ")", "\n", "\n", "# self.fusionLayer = torch.nn.Linear(1792, 768)", "\n", "# self.classifier = torch.nn.Linear(768, 2)", "\n", "\n", "", "elif", "self", ".", "id_from", "==", "'BeyondWordEmbedding'", ":", "\n", "            ", "self", ".", "projector1", "=", "torch", ".", "nn", ".", "Linear", "(", "500", ",", "128", ")", "\n", "self", ".", "projector2", "=", "torch", ".", "nn", ".", "Linear", "(", "768", ",", "128", ")", "\n", "\n", "# self.fusionLayer = torch.nn.Linear(1792, 128)", "\n", "# self.classifier = torch.nn.Linear(128, 2)", "\n", "\n", "self", ".", "classifier", "=", "torch", ".", "nn", ".", "Linear", "(", "1792", ",", "2", ")", "\n", "", "elif", "self", ".", "id_from", "==", "'MultiHeadAttentionAndBeyondWordEmbedding'", ":", "\n", "# word_dim = 768", "\n", "# self.projector1 = torch.nn.Linear(500, word_dim)", "\n", "# self.projector2 = torch.nn.Linear(768, word_dim)", "\n", "# self.projector3 = torch.nn.Linear(768, 120)", "\n", "\n", "# self.fusionLayer = torch.nn.MultiheadAttention(embed_dim=word_dim, num_heads=12, dropout=0.1)", "\n", "# self.classifier = torch.nn.Linear((self.max_length + 8) * word_dim, 2)", "\n", "            ", "'''Test: only use sentence-concept attention module'''", "\n", "# word_dim = 768", "\n", "# self.projector1 = torch.nn.Linear(500, word_dim)", "\n", "# self.projector2 = torch.nn.Linear(768, word_dim)", "\n", "# self.projector3 = torch.nn.Linear(768, 120)", "\n", "\n", "# self.classifier = torch.nn.Linear(word_dim, 2)", "\n", "'''Test:only use fusion module'''", "\n", "word_dim", "=", "500", "\n", "self", ".", "projector1", "=", "torch", ".", "nn", ".", "Linear", "(", "500", ",", "word_dim", ")", "\n", "self", ".", "projector2", "=", "torch", ".", "nn", ".", "Linear", "(", "768", ",", "word_dim", ")", "\n", "self", ".", "fusionLayer", "=", "torch", ".", "nn", ".", "MultiheadAttention", "(", "embed_dim", "=", "word_dim", ",", "num_heads", "=", "10", ",", "dropout", "=", "0.1", ")", "\n", "self", ".", "classifier", "=", "torch", ".", "nn", ".", "Linear", "(", "(", "self", ".", "max_length", "+", "8", ")", "*", "word_dim", ",", "2", ")", "\n", "\n", "", "else", ":", "\n", "            ", "assert", "(", "'please input right id source'", ")", "\n", "\n", "", "'''\u52a0\u4e2a\u5168\u8054\u63a5\uff0c\u5c06select attention\u8f93\u51fa\u7684\u7ed3\u679c\u7684shape\u8f6c\u6362\u6210\u548cBertForSequenceClassification\u4e00\u81f4'''", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.forward": [[539, 633], ["test.BERTPAIRConceptSentenceEncoder.sentence_have_concept", "test.BERTPAIRConceptSentenceEncoder.sentence_have_concept", "test.BERTPAIRConceptSentenceEncoder.bert", "test.BERTPAIRConceptSentenceEncoder.bert", "test.BERTPAIRConceptSentenceEncoder.sen_pair_cat_cpt", "test.BERTPAIRConceptSentenceEncoder.fusionLayer", "test.BERTPAIRConceptSentenceEncoder.classifier", "test.BERTPAIRConceptSentenceEncoder.bert", "test.BERTPAIRConceptSentenceEncoder.sen_pair_cat_cpt", "test.BERTPAIRConceptSentenceEncoder.classifier", "test.BERTPAIRConceptSentenceEncoder.bert", "test.BERTPAIRConceptSentenceEncoder.pair_projecter", "test.BERTPAIRConceptSentenceEncoder.sen_pair_cat_cpt", "test.BERTPAIRConceptSentenceEncoder.fusionLayer", "sen_cpt_vec.reshape.reshape.reshape", "test.BERTPAIRConceptSentenceEncoder.classifier", "test.BERTPAIRConceptSentenceEncoder.bert"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.sentence_have_concept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.sentence_have_concept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.sen_pair_cat_cpt", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.sen_pair_cat_cpt", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.pair_projecter", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.sen_pair_cat_cpt"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "\n", "        ", "'''select attention'''", "\n", "# TOdo 1.\u8f93\u5165\u7684\u6570\u636e\u7684\u8981\u6c42\u641e\u6e05\u695a 2.\u7f51\u7edc\u529f\u80fd\u7684\u8be6\u7ec6\u4f2a\u4ee3\u7801 3.\u5199\u4ee3\u7801\u5b9e\u73b0 4.\u68c0\u6d4b\u662f\u5426\u7b26\u5408\u9884\u671f\u8981\u6c42", "\n", "'''\n        \u8ba1\u7b97\u53e5\u5b50\u4e0econcept\u7684\u76f8\u4f3c\u5ea6\uff0c\u8fd4\u56de\u503c\u4e3a0\u62161\n        1.\u83b7\u53d6conceptID\uff0c\u7531tensor\u8f6c\u6362\u4e3aint\uff0c\u7136\u540e\u8fdb\u4e00\u6b65\u83b7\u5f97embedding\n        2.\u83b7\u53d6\u53e5\u5b50\u7684embedding\n        3.\u8ba1\u7b97\u53e5\u5b50\u548c\u6982\u5ff5\u7684\u76f8\u4f3c\u5ea6\uff0c\u7531\u4e8e\u53e5\u5b50\u548c\u6982\u5ff5\u7684embedding\u4e0d\u662f\u5728\u4e00\u4e2a\u7a7a\u95f4\u91cc\u5b66\u4e60\u5230\u7684\uff0c\u5148\u8ba9concept embedding\u7ecf\u8fc7\u5168\u8054\u63a5\u7f51\u7edc\u8fdb\u884c\u4e00\u4e2a\u7ebf\u6027\u53d8\u6362\uff0c\n            \u53d8\u6362\u5f97\u5230\u7684\u5411\u91cf\u548c\u53e5\u5b50\u5411\u91cf\u76f8\u4e58\u5f97\u5230scalar,\u7136\u540e\u518d\u8fdb\u8fc7sigmoid\u5c06\u5176\u6620\u5c04\u52300\u548c1\u4e4b\u95f4\u7684\u503c\uff0c\u5982\u679c\u5927\u4e8e0.5\u53d61\uff0c\u5c0f\u4e8e0.5\u53d60\uff0c\u8fd9\u4e48\u505a\u662f\u5426\u5408\u7406\uff08\u53ef\u4ee5\u6536\u96c6\u6570\u636e\uff0c\u505a\u53ef\u89c6\u5316\u5206\u6790\uff09\uff0c\n            \u53c2\u8003\u8f66\u4e07xiang\u8001\u5e08\u7684zhiyuan\u89c6\u5c4f\u3002\n        '''", "\n", "'''\u8ba1\u7b97query\u53e5\u5b50\u4e0e\u5176\u5b9e\u4f53\u5bf9\u5e94\u7684\u6982\u5ff5\u76f8\u4f3c\u5ea6'''", "\n", "query_sen", "=", "self", ".", "bert", "(", "inputs", "[", "'query_sen'", "]", ",", "attention_mask", "=", "inputs", "[", "'query_mask'", "]", ")", "[", "1", "]", "\n", "queryConceptID", "=", "inputs", "[", "'queryConceptID'", "]", "\n", "quer_sen_hava_cpt", "=", "self", ".", "sentence_have_concept", "(", "query_sen", ",", "queryConceptID", ")", "\n", "'''\u8ba1\u7b97support\u53e5\u5b50\u4e0e\u5176\u5b9e\u4f53\u5bf9\u5e94\u7684\u6982\u5ff5\u76f8\u4f3c\u5ea6'''", "\n", "support_sen", "=", "self", ".", "bert", "(", "inputs", "[", "'support_sen'", "]", ",", "attention_mask", "=", "inputs", "[", "'support_mask'", "]", ")", "[", "1", "]", "\n", "supportConceptID", "=", "inputs", "[", "'supportConceptID'", "]", "\n", "support_sen_have_cpt", "=", "self", ".", "sentence_have_concept", "(", "support_sen", ",", "supportConceptID", ")", "\n", "\n", "'''\u5168\u8054\u63a5\u5c42\uff0c\u8f6c\u6362select attention\u7684\u8f93\u51fashape\u4fdd\u6301\u4e0eBertForSequenceClassification\u4e00\u81f4'''", "\n", "# Todo 1.\u641e\u6e05\u695aBertForSequenceClassification\u8f93\u51fa\u7684shape:[-1,2] 2.\u5199\u6ee1\u8db3shape\u7684\u4ee3\u7801 3.\u8bb0\u68c0\u6d4b\u662f\u5426\u7b26\u5408\u8981\u6c42\u3002", "\n", "\n", "'''\u53e5\u5b50\u548c\u6982\u5ff5embedding\u62fc\u63a5'''", "\n", "\n", "if", "self", ".", "id_from", "==", "'keEmbedding'", ":", "\n", "            ", "x", "=", "self", ".", "bert", "(", "inputs", "[", "'word'", "]", ",", "token_type_ids", "=", "inputs", "[", "'seg'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "[", "1", "]", "\n", "sen_cpt_vec", "=", "self", ".", "sen_pair_cat_cpt", "(", "x", ",", "quer_sen_hava_cpt", ",", "\n", "support_sen_have_cpt", ")", "# sen_cpt_vec shape:(-1,768+128*8=1792)", "\n", "sen_cpt_vec", "=", "self", ".", "fusionLayer", "(", "sen_cpt_vec", ")", "\n", "x", "=", "self", ".", "classifier", "(", "sen_cpt_vec", ")", "\n", "", "elif", "self", ".", "id_from", "==", "'BeyondWordEmbedding'", ":", "\n", "            ", "x", "=", "self", ".", "bert", "(", "inputs", "[", "'word'", "]", ",", "token_type_ids", "=", "inputs", "[", "'seg'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "[", "1", "]", "\n", "sen_cpt_vec", "=", "self", ".", "sen_pair_cat_cpt", "(", "x", ",", "quer_sen_hava_cpt", ",", "\n", "support_sen_have_cpt", ")", "# sen_cpt_vec shape:(-1,768+500*8=4768)", "\n", "# sen_cpt_vec = self.fusionLayer(sen_cpt_vec)", "\n", "x", "=", "self", ".", "classifier", "(", "sen_cpt_vec", ")", "\n", "", "elif", "self", ".", "id_from", "==", "'MultiHeadAttentionAndBeyondWordEmbedding'", ":", "\n", "# x = self.bert(inputs['word'], token_type_ids=inputs['seg'], attention_mask=inputs['mask'])[", "\n", "#     0]  # \u53d6\u51fa\u8bcd\u5411\u91cf\u77e9\u9635[-1,\u5355\u8bcd\u4e2a\u6570\uff08\u53e5\u5b50\u957f\u5ea6\uff09\uff0c\u8bcd\u5411\u91cf\u7684\u7ef4\u5ea6]", "\n", "# # word_num = x.shape[1]", "\n", "# x = self.pair_projecter(x)", "\n", "# sen_cpt_vec = self.sen_pair_cat_cpt(x, quer_sen_hava_cpt,", "\n", "#                                     support_sen_have_cpt)", "\n", "#", "\n", "# sen_cpt_vec, _ = self.fusionLayer(sen_cpt_vec, sen_cpt_vec, sen_cpt_vec)", "\n", "#", "\n", "# # print('----------------------------sen_cpt_vec.shape----------------------')", "\n", "# # print(sen_cpt_vec.shape)", "\n", "# # sen_cpt_vec = self.projector3(sen_cpt_vec)", "\n", "# # sen_cpt_vec = sen_cpt_vec.reshape(-1, (word_num + 8) * 384)", "\n", "# sen_cpt_vec = sen_cpt_vec.reshape(-1, sen_cpt_vec.shape[1] * sen_cpt_vec.shape[2])", "\n", "#", "\n", "# # print('---------------------sen_cpt_vec.shape-----------------',sen_cpt_vec.shape)", "\n", "# x = self.classifier(sen_cpt_vec)", "\n", "\n", "            ", "'''Test: only use sentence-concept attention module, not use fusion module'''", "\n", "# x = self.bert(inputs['word'], token_type_ids=inputs['seg'], attention_mask=inputs['mask'])[", "\n", "#     1]  # \u53d6\u51fa\u53e5\u5b50\u5411\u91cf\u77e9\u9635[-1\uff0c\u8bcd\u5411\u91cf\u7684\u7ef4\u5ea6]", "\n", "#", "\n", "# sen_cpt_vec = self.sen_pair_cat_cpt(x, quer_sen_hava_cpt,", "\n", "#                                     support_sen_have_cpt)", "\n", "#", "\n", "# x = self.classifier(sen_cpt_vec)", "\n", "\n", "'''Test:only use fusion module'''", "\n", "# Todo   1.\u964d\u4f4e\u8bcd\u5411\u91cf\u7684\u7ef4\u6570\uff0c\u4e0d\u5bf9concept vec \u8fdb\u884c\u64cd\u4f5c\uff0c\u5c3d\u91cf\u9632\u6b62\u5f15\u5165\u4e0d\u53ef\u53d8\u7684\u56e0\u7d20 2.\u8ba1\u7b97\u76f8\u4f3c\u5ea6\uff0c\u62fc\u63a5\uff0c\u878d\u5408\u3002", "\n", "\n", "x", "=", "self", ".", "bert", "(", "inputs", "[", "'word'", "]", ",", "token_type_ids", "=", "inputs", "[", "'seg'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "[", "\n", "0", "]", "# \u53d6\u51fa\u8bcd\u5411\u91cf\u77e9\u9635[-1,\u5355\u8bcd\u4e2a\u6570\uff08\u53e5\u5b50\u957f\u5ea6\uff09\uff0c\u8bcd\u5411\u91cf\u7684\u7ef4\u5ea6]", "\n", "# word_num = x.shape[1]", "\n", "x", "=", "self", ".", "pair_projecter", "(", "x", ")", "\n", "sen_cpt_vec", "=", "self", ".", "sen_pair_cat_cpt", "(", "x", ",", "quer_sen_hava_cpt", ",", "\n", "support_sen_have_cpt", ")", "\n", "\n", "sen_cpt_vec", ",", "_", "=", "self", ".", "fusionLayer", "(", "sen_cpt_vec", ",", "sen_cpt_vec", ",", "sen_cpt_vec", ")", "\n", "\n", "# print('----------------------------sen_cpt_vec.shape----------------------')", "\n", "# print(sen_cpt_vec.shape)", "\n", "# sen_cpt_vec = self.projector3(sen_cpt_vec)", "\n", "# sen_cpt_vec = sen_cpt_vec.reshape(-1, (word_num + 8) * 384)", "\n", "sen_cpt_vec", "=", "sen_cpt_vec", ".", "reshape", "(", "-", "1", ",", "sen_cpt_vec", ".", "shape", "[", "1", "]", "*", "sen_cpt_vec", ".", "shape", "[", "2", "]", ")", "\n", "\n", "# print('---------------------sen_cpt_vec.shape-----------------',sen_cpt_vec.shape)", "\n", "x", "=", "self", ".", "classifier", "(", "sen_cpt_vec", ")", "\n", "\n", "# Todo \u67e5\u627e\u4ec0\u4e48\u51fd\u6570\u7684\u8f93\u51fa\u503c\u4e3a0\u548c1:\u76ee\u524d\u627e\u5230\u7684\u4e3asigmoid, sigmoid\u56de\u5bfc\u81f4\u68af\u5ea6\u6d88\u5931\uff0c\u6536\u655b\u53d8\u6162\uff0c", "\n", "\n", "", "'''\n        TODO \u52a0\u5165Robinson\u7ea6\u675f\n        '''", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.pair_projecter": [[634, 647], ["torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "range", "test.BERTPAIRConceptSentenceEncoder.projector2", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "pair_projecter", "(", "self", ",", "wordVec", ")", ":", "\n", "\n", "        ", "sample_num", "=", "wordVec", ".", "shape", "[", "0", "]", "\n", "word_num", "=", "wordVec", ".", "shape", "[", "1", "]", "\n", "sen_len", "=", "self", ".", "projector2", ".", "bias", ".", "shape", "[", "0", "]", "\n", "\n", "projected_wordVec", "=", "torch", ".", "zeros", "(", "[", "sample_num", ",", "word_num", ",", "sen_len", "]", ")", ".", "cuda", "(", ")", "\n", "for", "i", "in", "range", "(", "sample_num", ")", ":", "\n", "            ", "wv", "=", "wordVec", "[", "i", ",", ":", ",", ":", "]", "\n", "wv", "=", "self", ".", "projector2", "(", "wv", ")", "\n", "projected_wordVec", "[", "i", ",", ":", ",", ":", "]", "=", "wv", "\n", "# print('projected_wordVec.shape', projected_wordVec.shape)", "\n", "", "return", "projected_wordVec", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.sentence_have_concept": [[648, 737], ["range", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.matmul().float", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "sen_have_cpt.append", "test.BERTPAIRConceptSentenceEncoder.id2embedding().cuda", "test.BERTPAIRConceptSentenceEncoder.id2embedding().cuda", "test.BERTPAIRConceptSentenceEncoder.id2embedding().cuda", "test.BERTPAIRConceptSentenceEncoder.id2embedding().cuda", "test.BERTPAIRConceptSentenceEncoder.projector1", "test.BERTPAIRConceptSentenceEncoder.projector1", "test.BERTPAIRConceptSentenceEncoder.projector1", "test.BERTPAIRConceptSentenceEncoder.projector1", "test.BERTPAIRConceptSentenceEncoder.projector2", "test.BERTPAIRConceptSentenceEncoder.id2BeyondWordEmbedding", "test.BERTPAIRConceptSentenceEncoder.projector2", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "test.BERTPAIRConceptSentenceEncoder.id2embedding", "test.BERTPAIRConceptSentenceEncoder.id2embedding", "test.BERTPAIRConceptSentenceEncoder.id2embedding", "test.BERTPAIRConceptSentenceEncoder.id2embedding", "test.BERTPAIRConceptSentenceEncoder.t", "test.BERTPAIRConceptSentenceEncoder.t", "test.BERTPAIRConceptSentenceEncoder.t", "test.BERTPAIRConceptSentenceEncoder.t"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.id2BeyondWordEmbedding", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.id2embedding", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.id2embedding", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.id2embedding", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.id2embedding"], ["", "def", "sentence_have_concept", "(", "self", ",", "sen", ",", "conceptID", ")", ":", "\n", "        ", "'''\n        \u8ba1\u7b97\u53e5\u5b50\u4e0econcept\u7684\u76f8\u4f3c\u5ea6\uff0c\u503c\u4e3a0\u62161,\u8fd4\u56devalue\u4e3a1\u7684concept embedding\n        1.\u83b7\u53d6conceptID\uff0c\u7531tensor\u8f6c\u6362\u4e3aint\uff0c\u7136\u540e\u8fdb\u4e00\u6b65\u83b7\u5f97embedding\n        2.\u83b7\u53d6\u53e5\u5b50\u7684embedding\uff0c\n        3.\u8ba1\u7b97\u53e5\u5b50\u548c\u6982\u5ff5\u7684\u76f8\u4f3c\u5ea6\uff0c\u7531\u4e8e\u53e5\u5b50\u548c\u6982\u5ff5\u7684embedding\u4e0d\u662f\u5728\u4e00\u4e2a\u7a7a\u95f4\u91cc\u5b66\u4e60\u5230\u7684\uff0c\u5148\u8ba9concept embedding\u7ecf\u8fc7\u5168\u8054\u63a5\u7f51\u7edc\u8fdb\u884c\u4e00\u4e2a\u7ebf\u6027\u53d8\u6362\uff0c\n            \u53d8\u6362\u5f97\u5230\u7684\u5411\u91cf\u548c\u53e5\u5b50\u5411\u91cf\u76f8\u4e58\u5f97\u5230scalar,\u7136\u540e\u518d\u8fdb\u8fc7sigmoid\u5c06\u5176\u6620\u5c04\u52300\u548c1\u4e4b\u95f4\u7684\u503c\uff0c\u5982\u679c\u5927\u4e8e0.5\u53d61\uff0c\u5c0f\u4e8e0.5\u53d60\uff0c\u8fd9\u4e48\u505a\u662f\u5426\u5408\u7406\uff08\u53ef\u4ee5\u6536\u96c6\u6570\u636e\uff0c\u505a\u53ef\u89c6\u5316\u5206\u6790\uff09\uff0c\n            \u53c2\u8003\u8f66\u4e07xiang\u8001\u5e08\u7684zhiyuan\u89c6\u5c4f\u3002\n        '''", "\n", "\n", "sen_have_cpt", "=", "[", "]", "# \u7528\u4e8e\u5b58\u50a8\u6839\u636e\u53e5\u610f\u9009\u62e9\u7684\u5b9e\u4f53", "\n", "sen_num", "=", "sen", ".", "shape", "[", "0", "]", "\n", "sen_len", "=", "sen", ".", "shape", "[", "1", "]", "# 768", "\n", "\n", "for", "i", "in", "range", "(", "sen_num", ")", ":", "\n", "            ", "if", "self", ".", "id_from", "==", "'kgEmbedding'", ":", "\n", "                ", "'''\u83b7\u53d6\u6982\u5ff5\u5411\u91cf'''", "\n", "cptID", "=", "conceptID", "[", "i", "]", "\n", "h_cpt1ID", "=", "cptID", "[", "0", "]", "# TOdo \u8f6c\u6362\u6210int", "\n", "h_cpt2ID", "=", "cptID", "[", "1", "]", "\n", "t_cpt1ID", "=", "cptID", "[", "2", "]", "\n", "t_cpt2ID", "=", "cptID", "[", "3", "]", "\n", "\n", "h_cpt1_vec", "=", "self", ".", "id2embedding", "(", "h_cpt1ID", ",", "self", ".", "conceptEmbedding", ")", ".", "cuda", "(", ")", "\n", "h_cpt2_vec", "=", "self", ".", "id2embedding", "(", "h_cpt2ID", ",", "self", ".", "conceptEmbedding", ")", ".", "cuda", "(", ")", "\n", "t_cpt1_vec", "=", "self", ".", "id2embedding", "(", "t_cpt1ID", ",", "self", ".", "conceptEmbedding", ")", ".", "cuda", "(", ")", "\n", "t_cpt2_vec", "=", "self", ".", "id2embedding", "(", "t_cpt2ID", ",", "self", ".", "conceptEmbedding", ")", ".", "cuda", "(", ")", "\n", "'''\u83b7\u53d6\u53e5\u5b50\u5411\u91cf'''", "\n", "sen_vec", "=", "sen", "[", "i", ",", ":", "]", "\n", "'''\u6982\u5ff5\u5411\u91cf\u6295\u5f71'''", "\n", "h_cpt1_vec", "=", "self", ".", "projector1", "(", "h_cpt1_vec", ")", "# size \u7531(1,256)\u53d8\u6210(1\uff0c128)", "\n", "h_cpt2_vec", "=", "self", ".", "projector1", "(", "h_cpt2_vec", ")", "\n", "t_cpt1_vec", "=", "self", ".", "projector1", "(", "t_cpt1_vec", ")", "\n", "t_cpt2_vec", "=", "self", ".", "projector1", "(", "t_cpt2_vec", ")", "\n", "'''\u53e5\u5b50\u5411\u91cf\u6295\u5f71'''", "\n", "sen_vec", "=", "self", ".", "projector2", "(", "sen_vec", ")", "\n", "", "elif", "(", "self", ".", "id_from", "==", "'BeyondWordEmbedding'", ")", "|", "(", "self", ".", "id_from", "==", "'MultiHeadAttentionAndBeyondWordEmbedding'", ")", ":", "\n", "                ", "'''\u83b7\u53d6\u6982\u5ff5\u5411\u91cf'''", "\n", "cptID", "=", "conceptID", "[", "i", "]", "\n", "all_cpt_vec", "=", "self", ".", "id2BeyondWordEmbedding", "(", "cptID", ")", "\n", "# print('----------------------------all_cpt_vec----------------------------')", "\n", "# print(all_cpt_vec)", "\n", "# print(len(all_cpt_vec))", "\n", "# print(len(all_cpt_vec[0]))", "\n", "h_cpt1_vec", "=", "all_cpt_vec", "[", "0", "]", "\n", "h_cpt2_vec", "=", "all_cpt_vec", "[", "1", "]", "\n", "t_cpt1_vec", "=", "all_cpt_vec", "[", "2", "]", "\n", "t_cpt2_vec", "=", "all_cpt_vec", "[", "3", "]", "\n", "'''\u83b7\u53d6\u53e5\u5b50\u5411\u91cf'''", "\n", "sen_vec", "=", "sen", "[", "i", ",", ":", "]", "\n", "\n", "'''\u6982\u5ff5\u5411\u91cf\u6295\u5f71'''", "\n", "'''Test: only use fusion module,no projector1,no similarity'''", "\n", "# h_cpt1_vec = self.projector1(h_cpt1_vec)  # size \u7531(1,500)\u53d8\u6210(1\uff0c500)", "\n", "# h_cpt2_vec = self.projector1(h_cpt2_vec)", "\n", "# t_cpt1_vec = self.projector1(t_cpt1_vec)", "\n", "# t_cpt2_vec = self.projector1(t_cpt2_vec)", "\n", "'''\u53e5\u5b50\u5411\u91cf\u6295\u5f71'''", "\n", "sen_vec", "=", "self", ".", "projector2", "(", "sen_vec", ")", "\n", "\n", "", "'''\u8ba1\u7b97\u53e5\u5b50\u548c\u6982\u5ff5\u7684\u76f8\u4f3c\u5ea6'''", "\n", "h_cpt1_sen_sim", "=", "torch", ".", "matmul", "(", "sen_vec", ",", "h_cpt1_vec", ".", "t", "(", ")", ")", ".", "float", "(", ")", "# size(1),\u8ba1\u7b97\u7ed3\u679c\u4e3a\u6807\u91cf", "\n", "h_cpt2_sen_sim", "=", "torch", ".", "matmul", "(", "sen_vec", ",", "h_cpt2_vec", ".", "t", "(", ")", ")", ".", "float", "(", ")", "\n", "t_cpt1_sen_sim", "=", "torch", ".", "matmul", "(", "sen_vec", ",", "t_cpt1_vec", ".", "t", "(", ")", ")", ".", "float", "(", ")", "\n", "t_cpt2_sen_sim", "=", "torch", ".", "matmul", "(", "sen_vec", ",", "t_cpt2_vec", ".", "t", "(", ")", ")", ".", "float", "(", ")", "\n", "'''\u76f8\u4f3c\u5ea601\u5316,\u76f8\u4f3c\u5ea6\u7684\u503c\u53ea\u53d60\u62161'''", "\n", "# softmax", "\n", "sim", "=", "torch", ".", "tensor", "(", "[", "h_cpt1_sen_sim", ",", "h_cpt2_sen_sim", ",", "t_cpt1_sen_sim", ",", "t_cpt2_sen_sim", "]", ")", "\n", "sim", "=", "torch", ".", "softmax", "(", "sim", ",", "dim", "=", "0", ")", "\n", "[", "h_cpt1_sen_sim", ",", "h_cpt2_sen_sim", ",", "t_cpt1_sen_sim", ",", "t_cpt2_sen_sim", "]", "=", "sim", "\n", "h_cpt1_sen_sim", "=", "1", "if", "h_cpt1_sen_sim", ">=", "0.5", "else", "0", "\n", "h_cpt2_sen_sim", "=", "1", "if", "h_cpt2_sen_sim", ">=", "0.5", "else", "0", "\n", "t_cpt1_sen_sim", "=", "1", "if", "t_cpt1_sen_sim", ">=", "0.5", "else", "0", "\n", "t_cpt2_sen_sim", "=", "1", "if", "t_cpt2_sen_sim", ">=", "0.5", "else", "0", "\n", "\n", "# sigmoid \u6548\u679c\u4e0d\u597d\uff0c\u7528softmax", "\n", "# h_cpt1_sen_sim = 1 if torch.sigmoid(h_cpt1_sen_sim) >= 0.5 else 0", "\n", "# h_cpt2_sen_sim = 1 if torch.sigmoid(h_cpt2_sen_sim) >= 0.5 else 0", "\n", "# t_cpt1_sen_sim = 1 if torch.sigmoid(t_cpt1_sen_sim) >= 0.5 else 0", "\n", "# t_cpt2_sen_sim = 1 if torch.sigmoid(t_cpt2_sen_sim) >= 0.5 else 0", "\n", "\n", "h_cpt1_vec", "=", "h_cpt1_vec", "*", "h_cpt1_sen_sim", "\n", "h_cpt2_vec", "=", "h_cpt2_vec", "*", "h_cpt2_sen_sim", "\n", "t_cpt1_vec", "=", "t_cpt1_vec", "*", "t_cpt1_sen_sim", "\n", "t_cpt2_vec", "=", "t_cpt2_vec", "*", "t_cpt2_sen_sim", "\n", "\n", "sen_have_cpt", ".", "append", "(", "[", "h_cpt1_vec", ",", "h_cpt2_vec", ",", "t_cpt1_vec", ",", "t_cpt2_vec", "]", ")", "\n", "\n", "", "return", "sen_have_cpt", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.id2BeyondWordEmbedding": [[738, 802], ["range", "range", "all_cpt_id.append", "len", "all_cpt_vec.append", "cpt_id.append", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "all_cpt_vec.append", "str.cpu().numpy", "str", "test.BERTPAIRConceptSentenceEncoder.beyondWordEmbedding[].float().cuda", "cpt_vec.view.view.view", "all_cpt_vec.append", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "str.split", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "torch.zeros().float().cuda", "str.cpu().numpy", "str", "test.BERTPAIRConceptSentenceEncoder.beyondWordEmbedding[].float().cuda", "cpt_vec.view.view.view", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "str.cpu", "test.BERTPAIRConceptSentenceEncoder.beyondWordEmbedding[].float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "str.split", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "str.cpu", "test.BERTPAIRConceptSentenceEncoder.beyondWordEmbedding[].float", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "id2BeyondWordEmbedding", "(", "self", ",", "cptID", ")", ":", "\n", "        ", "'''\n        cptID = torch.tensor([[9.6963e+06, -2.0000e+00, -2.0000e+00, -2.0000e+00, -2.0000e+00],\n                              [-1.0000e+00, -2.0000e+00, -2.0000e+00, -2.0000e+00, -2.0000e+00],\n                              [-1.0000e+00, -2.0000e+00, -2.0000e+00, -2.0000e+00, -2.0000e+00],\n                              [-1.0000e+00, -2.0000e+00, -2.0000e+00, -2.0000e+00, -2.0000e+00]])\n        '''", "\n", "# print('cptID', cptID)", "\n", "# print('cptID', cptID.shape)", "\n", "\n", "cpt_num", "=", "cptID", ".", "shape", "[", "0", "]", "\n", "all_cpt_id", "=", "[", "]", "\n", "all_cpt_vec", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "cpt_num", ")", ":", "\n", "            ", "wordsID", "=", "cptID", "[", "i", "]", "\n", "cpt_id", "=", "[", "]", "\n", "for", "id", "in", "wordsID", ":", "\n", "                ", "if", "id", "==", "-", "2", ":", "# -2\u4e3a\u65e0\u6548ID\uff0c\u586b\u5145\u5411\u91cf\u7528\u7684", "\n", "                    ", "continue", "\n", "", "else", ":", "\n", "                    ", "cpt_id", ".", "append", "(", "id", ")", "\n", "", "", "all_cpt_id", ".", "append", "(", "cpt_id", ")", "\n", "# print('all_cpt_id', all_cpt_id)", "\n", "# print('all_cpt_id', len(all_cpt_id))", "\n", "", "for", "j", "in", "range", "(", "cpt_num", ")", ":", "# \u6982\u5ff5\u53ea\u6709\u4e00\u4e2a\u8bcd\u7ec4\u6210", "\n", "            ", "j_cpt_id", "=", "all_cpt_id", "[", "j", "]", "\n", "# print('j_cpt_id', j_cpt_id)", "\n", "# print(len(j_cpt_id))", "\n", "if", "len", "(", "j_cpt_id", ")", "==", "1", ":", "\n", "                ", "j_id", "=", "j_cpt_id", "[", "0", "]", "\n", "if", "j_id", "==", "-", "1", ":", "\n", "                    ", "cpt_vec", "=", "torch", ".", "zeros", "(", "(", "1", ",", "500", ")", ")", ".", "float", "(", ")", ".", "cuda", "(", ")", "\n", "all_cpt_vec", ".", "append", "(", "cpt_vec", ")", "\n", "", "else", ":", "\n", "# j_id = j_id.long()", "\n", "\n", "                    ", "j_id", "=", "j_id", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "j_id", "=", "str", "(", "j_id", ")", "\n", "j_id", "=", "j_id", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "j_id", "=", "self", ".", "id2embeddingID", "[", "j_id", "]", "\n", "\n", "cpt_vec", "=", "self", ".", "beyondWordEmbedding", "[", "j_id", ",", ":", "]", ".", "float", "(", ")", ".", "cuda", "(", ")", "\n", "cpt_vec", "=", "cpt_vec", ".", "view", "(", "[", "1", ",", "500", "]", ")", "\n", "all_cpt_vec", ".", "append", "(", "cpt_vec", ")", "\n", "", "", "else", ":", "\n", "                ", "for", "id", "in", "j_cpt_id", ":", "# \u6982\u5ff5\u7531\u591a\u4e2a\u8bcd\u7ec4\u6210\uff0c\u628a\u8bcd\u7684\u5411\u91cf\u53e0\u52a0\uff0c\u4f5c\u4e3a\u6982\u5ff5\u7684\u5411\u91cf", "\n", "                    ", "word_vec", "=", "torch", ".", "zeros", "(", "(", "1", ",", "500", ")", ")", ".", "float", "(", ")", ".", "cuda", "(", ")", "\n", "if", "id", "==", "-", "1", ":", "\n", "                        ", "cpt_vec", "=", "torch", ".", "zeros", "(", "(", "1", ",", "500", ")", ")", ".", "float", "(", ")", ".", "cuda", "(", ")", "\n", "word_vec", "=", "word_vec", "+", "cpt_vec", "\n", "", "else", ":", "\n", "# id = id.long()", "\n", "\n", "                        ", "id", "=", "id", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "id", "=", "str", "(", "id", ")", "\n", "id", "=", "id", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "id", "=", "self", ".", "id2embeddingID", "[", "id", "]", "\n", "\n", "cpt_vec", "=", "self", ".", "beyondWordEmbedding", "[", "id", ",", ":", "]", ".", "float", "(", ")", ".", "cuda", "(", ")", "\n", "cpt_vec", "=", "cpt_vec", ".", "view", "(", "[", "1", ",", "500", "]", ")", "\n", "word_vec", "=", "word_vec", "+", "cpt_vec", "\n", "# all_cpt_vec.append(word_vec)", "\n", "", "", "all_cpt_vec", ".", "append", "(", "word_vec", ")", "\n", "", "", "return", "all_cpt_vec", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.id2embedding": [[803, 811], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "cpt_vec.view.view.view"], "methods", ["None"], ["", "def", "id2embedding", "(", "self", ",", "id", ",", "conceptEmbedding", ")", ":", "\n", "        ", "if", "id", "==", "-", "1", ":", "\n", "            ", "cpt_vec", "=", "torch", ".", "zeros", "(", "(", "1", ",", "256", ")", ")", "\n", "", "else", ":", "\n", "            ", "cpt_vec", "=", "conceptEmbedding", "[", "id", ",", ":", "]", "\n", "cpt_vec", "=", "cpt_vec", ".", "view", "(", "[", "1", ",", "256", "]", ")", "\n", "\n", "", "return", "cpt_vec", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.sen_pair_cat_cpt": [[812, 917], ["torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "range", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.cuda", "torch.cat.cuda", "torch.cat.cuda", "sen_vec.view.view.view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "sen_pair_cat_cpt", "(", "self", ",", "sen_pair", ",", "query_sen_hava_cpt", ",", "support_sen_have_cpt", ")", ":", "\n", "        ", "if", "self", ".", "id_from", "==", "'MultiHeadAttentionAndBeyondWordEmbedding'", ":", "\n", "            ", "'''Normal: use sentence-concept attention module and fusion module or only use fusion module'''", "\n", "\n", "sen_num", "=", "sen_pair", ".", "shape", "[", "0", "]", "\n", "word_num", "=", "sen_pair", ".", "shape", "[", "1", "]", "\n", "sen_len", "=", "sen_pair", ".", "shape", "[", "2", "]", "\n", "cpt_num", "=", "8", "\n", "sen_cpt_vec", "=", "torch", ".", "zeros", "(", "[", "sen_num", ",", "word_num", "+", "cpt_num", ",", "sen_len", "]", ")", ".", "cuda", "(", ")", "\n", "\n", "# sen_shape = [sen_pair.shape[1], sen_pair.shape[2]]", "\n", "for", "i", "in", "range", "(", "sen_num", ")", ":", "\n", "                ", "sen_vec", "=", "sen_pair", "[", "i", ",", ":", ",", ":", "]", "\n", "# sen_vec = sen_vec.view(sen_shape)", "\n", "query_cpt_vec", "=", "query_sen_hava_cpt", "[", "i", "]", "\n", "query_h_cpt1_vec", "=", "query_cpt_vec", "[", "0", "]", "\n", "query_h_cpt2_vec", "=", "query_cpt_vec", "[", "1", "]", "\n", "query_t_cpt1_vec", "=", "query_cpt_vec", "[", "2", "]", "\n", "query_t_cpt2_vec", "=", "query_cpt_vec", "[", "3", "]", "\n", "support_cpt_vec", "=", "support_sen_have_cpt", "[", "i", "]", "\n", "support_h_cpt1_vec", "=", "support_cpt_vec", "[", "0", "]", "\n", "support_h_cpt2_vec", "=", "support_cpt_vec", "[", "1", "]", "\n", "support_t_cpt1_vec", "=", "support_cpt_vec", "[", "2", "]", "\n", "support_t_cpt2_vec", "=", "support_cpt_vec", "[", "3", "]", "\n", "i_sen_cpt_vec", "=", "torch", ".", "cat", "(", "\n", "(", "sen_vec", ",", "query_h_cpt1_vec", ",", "query_h_cpt2_vec", ",", "query_t_cpt1_vec", ",", "query_t_cpt2_vec", ",", "\n", "support_h_cpt1_vec", ",", "support_h_cpt2_vec", ",", "support_t_cpt1_vec", ",", "support_t_cpt2_vec", ")", ",", "\n", "0", ")", "\n", "\n", "sen_cpt_vec", "[", "i", ",", ":", ",", ":", "]", "=", "i_sen_cpt_vec", ".", "cuda", "(", ")", "\n", "\n", "", "'''Test: only use sentence-concept attention module, not use fusion module'''", "\n", "# sen_num = sen_pair.shape[0]", "\n", "# sen_vec_dim = sen_pair.shape[1]", "\n", "# sen_cpt_vec = torch.zeros([sen_num, sen_vec_dim]).cuda()", "\n", "# for i in range(sen_num):", "\n", "#     sen_vec = sen_pair[i, :]", "\n", "#     # sen_vec = sen_vec.view(sen_shape)", "\n", "#     query_cpt_vec = query_sen_hava_cpt[i]", "\n", "#     query_h_cpt1_vec = query_cpt_vec[0]", "\n", "#     query_h_cpt2_vec = query_cpt_vec[1]", "\n", "#     query_t_cpt1_vec = query_cpt_vec[2]", "\n", "#     query_t_cpt2_vec = query_cpt_vec[3]", "\n", "#     support_cpt_vec = support_sen_have_cpt[i]", "\n", "#     support_h_cpt1_vec = support_cpt_vec[0]", "\n", "#     support_h_cpt2_vec = support_cpt_vec[1]", "\n", "#     support_t_cpt1_vec = support_cpt_vec[2]", "\n", "#     support_t_cpt2_vec = support_cpt_vec[3]", "\n", "#     # print('query_h_cpt1_vec',query_h_cpt1_vec.shape)", "\n", "#     # print('sen_vec',sen_vec.shape)", "\n", "#", "\n", "#     i_sen_cpt_vec = sen_vec + query_h_cpt1_vec + query_h_cpt2_vec + query_t_cpt1_vec + query_t_cpt2_vec + support_h_cpt1_vec + support_h_cpt2_vec + support_t_cpt1_vec + support_t_cpt2_vec", "\n", "#     sen_cpt_vec[i, :] = i_sen_cpt_vec.cuda()", "\n", "\n", "# print('sen_cpt_vec',sen_cpt_vec.shape)", "\n", "# print('sen_cpt_vec.shape', sen_cpt_vec.shape)", "\n", "\n", "# print('query_sen_hava_cpt', query_sen_hava_cpt)", "\n", "# print('query_sen_hava_cpt type:', type(query_sen_hava_cpt))", "\n", "# query_sen_hava_cpt1 = query_sen_hava_cpt[0]", "\n", "# query_sen_hava_cpt2 = query_sen_hava_cpt[1]", "\n", "# query_sen_hava_cpt3 = query_sen_hava_cpt[2]", "\n", "# query_sen_hava_cpt4 = query_sen_hava_cpt[3]", "\n", "# print('query_sen_hava_cpt1', query_sen_hava_cpt1)", "\n", "# print('query_sen_hava_cpt1 type', type(query_sen_hava_cpt1))", "\n", "# query_sen_hava_cpt = torch.cat(", "\n", "#     [query_sen_hava_cpt1, query_sen_hava_cpt2, query_sen_hava_cpt3, query_sen_hava_cpt4], dim=1)", "\n", "#", "\n", "# # support_sen_have_cpt = torch.cat(support_sen_have_cpt, dim=1)", "\n", "# support_sen_have_cpt1 = query_sen_hava_cpt[0]", "\n", "# support_sen_have_cpt2 = query_sen_hava_cpt[1]", "\n", "# support_sen_have_cpt3 = query_sen_hava_cpt[2]", "\n", "# support_sen_have_cpt4 = query_sen_hava_cpt[3]", "\n", "#", "\n", "# support_sen_have_cpt = torch.cat(", "\n", "#     [support_sen_have_cpt1, support_sen_have_cpt2, support_sen_have_cpt3, support_sen_have_cpt4], dim=1)", "\n", "#", "\n", "# sen_cpt_vec = torch.cat([sen_pair, query_sen_hava_cpt, support_sen_have_cpt], dim=1)", "\n", "\n", "", "else", ":", "\n", "            ", "sen_cpt_vec", "=", "[", "]", "\n", "# print(sen_cpt_vec)", "\n", "sen_num", "=", "sen_pair", ".", "shape", "[", "0", "]", "\n", "sen_len", "=", "sen_pair", ".", "shape", "[", "1", "]", "\n", "for", "i", "in", "range", "(", "sen_num", ")", ":", "\n", "                ", "sen_vec", "=", "sen_pair", "[", "i", ",", ":", "]", "\n", "sen_vec", "=", "sen_vec", ".", "view", "(", "[", "1", ",", "sen_len", "]", ")", "\n", "query_cpt_vec", "=", "query_sen_hava_cpt", "[", "i", "]", "\n", "query_h_cpt1_vec", "=", "query_cpt_vec", "[", "0", "]", "\n", "query_h_cpt2_vec", "=", "query_cpt_vec", "[", "1", "]", "\n", "query_t_cpt1_vec", "=", "query_cpt_vec", "[", "2", "]", "\n", "query_t_cpt2_vec", "=", "query_cpt_vec", "[", "3", "]", "\n", "support_cpt_vec", "=", "support_sen_have_cpt", "[", "i", "]", "\n", "support_h_cpt1_vec", "=", "support_cpt_vec", "[", "0", "]", "\n", "support_h_cpt2_vec", "=", "support_cpt_vec", "[", "1", "]", "\n", "support_t_cpt1_vec", "=", "support_cpt_vec", "[", "2", "]", "\n", "support_t_cpt2_vec", "=", "support_cpt_vec", "[", "3", "]", "\n", "i_sen_cpt_vec", "=", "torch", ".", "cat", "(", "\n", "(", "sen_vec", ",", "query_h_cpt1_vec", ",", "query_h_cpt2_vec", ",", "query_t_cpt1_vec", ",", "query_t_cpt2_vec", ",", "\n", "support_h_cpt1_vec", ",", "support_h_cpt2_vec", ",", "support_t_cpt1_vec", ",", "support_t_cpt2_vec", ")", ",", "\n", "1", ")", "\n", "sen_cpt_vec", ".", "append", "(", "i_sen_cpt_vec", ")", "\n", "", "sen_cpt_vec", "=", "torch", ".", "cat", "(", "sen_cpt_vec", ",", "0", ")", "\n", "\n", "", "return", "sen_cpt_vec", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.tokenize": [[918, 943], ["test.BERTPAIRConceptSentenceEncoder.tokenizer.convert_tokens_to_ids", "token.lower.lower.lower", "test.BERTPAIRConceptSentenceEncoder.tokenizer.tokenize", "tokens.append", "len", "tokens.append", "len", "tokens.append", "tokens.append"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "def", "tokenize", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ")", ":", "\n", "# token -> index", "\n", "# tokens = ['[CLS]']", "\n", "        ", "tokens", "=", "[", "]", "\n", "cur_pos", "=", "0", "\n", "pos1_in_index", "=", "0", "\n", "pos2_in_index", "=", "0", "\n", "for", "token", "in", "raw_tokens", ":", "\n", "            ", "token", "=", "token", ".", "lower", "(", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused0]'", ")", "\n", "pos1_in_index", "=", "len", "(", "tokens", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused1]'", ")", "\n", "pos2_in_index", "=", "len", "(", "tokens", ")", "\n", "", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused2]'", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused3]'", ")", "\n", "", "cur_pos", "+=", "1", "\n", "\n", "", "indexed_tokens", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "return", "indexed_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.tokenize_concept": [[944, 1005], ["h.lower.lower.lower", "fewshot_re_kit.conceptgraph_utils.instance2conept", "h2concept[].lower", "h2concept[].lower", "t.lower.lower.lower", "fewshot_re_kit.conceptgraph_utils.instance2conept", "t2concept[].lower", "t2concept[].lower", "tokens.append", "tokens.append", "tokens.append", "tokens.append", "test.BERTPAIRConceptSentenceEncoder.tokenizer.convert_tokens_to_ids", "token.lower.lower.lower", "test.BERTPAIRConceptSentenceEncoder.tokenizer.tokenize", "tokens.append", "test.BERTPAIRConceptSentenceEncoder.tokenizer.tokenize", "tokens.append", "test.BERTPAIRConceptSentenceEncoder.tokenizer.tokenize", "tokens.append", "test.BERTPAIRConceptSentenceEncoder.tokenizer.tokenize", "tokens.append", "test.BERTPAIRConceptSentenceEncoder.tokenizer.tokenize", "tokens.append", "len", "tokens.append", "len", "tokens.append", "tokens.append"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.instance2conept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.instance2conept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "def", "tokenize_concept", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ",", "h", ",", "t", ",", "ins2cpt", ")", ":", "\n", "# token -> index", "\n", "# tokens = ['[CLS]']", "\n", "        ", "tokens", "=", "[", "]", "\n", "cur_pos", "=", "0", "\n", "pos1_in_index", "=", "0", "\n", "pos2_in_index", "=", "0", "\n", "for", "token", "in", "raw_tokens", ":", "\n", "            ", "token", "=", "token", ".", "lower", "(", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused0]'", ")", "\n", "pos1_in_index", "=", "len", "(", "tokens", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "0", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused1]'", ")", "\n", "pos2_in_index", "=", "len", "(", "tokens", ")", "\n", "", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "if", "cur_pos", "==", "pos_head", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused2]'", ")", "\n", "", "if", "cur_pos", "==", "pos_tail", "[", "-", "1", "]", ":", "\n", "                ", "tokens", ".", "append", "(", "'[unused3]'", ")", "\n", "", "cur_pos", "+=", "1", "\n", "", "'''\u6dfb\u52a0\u5b9e\u4f53\u7684\u6982\u5ff5\u5230tokens\u4e2d'''", "\n", "h", "=", "h", ".", "lower", "(", ")", "\n", "h2concept", "=", "instance2conept", "(", "ins2cpt", ",", "h", ")", "\n", "h2concept1", "=", "h2concept", "[", "0", "]", ".", "lower", "(", ")", "\n", "h2concept2", "=", "h2concept", "[", "1", "]", ".", "lower", "(", ")", "\n", "t", "=", "t", ".", "lower", "(", ")", "\n", "t2concept", "=", "instance2conept", "(", "ins2cpt", ",", "t", ")", "\n", "t2concept1", "=", "t2concept", "[", "0", "]", ".", "lower", "(", ")", "\n", "t2concept2", "=", "t2concept", "[", "1", "]", ".", "lower", "(", ")", "\n", "\n", "tokens", ".", "append", "(", "'[unused4]'", ")", "\n", "if", "(", "h2concept1", "==", "'unknowconcept1'", ")", "or", "(", "h2concept1", "==", "'unknowconcept2'", ")", ":", "\n", "# print('-----------I am running-----------------------')", "\n", "            ", "tokens", ".", "append", "(", "h2concept1", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "h2concept1", ")", "\n", "\n", "", "tokens", ".", "append", "(", "'[unused5]'", ")", "\n", "if", "(", "h2concept2", "==", "'unknowconcept1'", ")", "or", "(", "h2concept2", "==", "'unknowconcept2'", ")", ":", "\n", "            ", "tokens", ".", "append", "(", "h2concept2", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "h2concept2", ")", "\n", "\n", "", "tokens", ".", "append", "(", "'[unused6]'", ")", "\n", "if", "(", "t2concept1", "==", "'unknowconcept1'", ")", "or", "(", "t2concept1", "==", "'unknowconcept2'", ")", ":", "\n", "# print('-----------I am running-----------------------')", "\n", "\n", "            ", "tokens", ".", "append", "(", "t2concept1", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "t2concept1", ")", "\n", "\n", "", "tokens", ".", "append", "(", "'[unused7]'", ")", "\n", "if", "(", "t2concept2", "==", "'unknowconcept1'", ")", "or", "(", "t2concept2", "==", "'unknowconcept2'", ")", ":", "\n", "            ", "tokens", ".", "append", "(", "t2concept2", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "+=", "self", ".", "tokenizer", ".", "tokenize", "(", "t2concept2", ")", "\n", "# print('------------------tokens-----------------------')", "\n", "# print(tokens)", "\n", "", "indexed_tokens", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "return", "indexed_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaSentenceEncoder.__init__": [[1009, 1015], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "transformers.RobertaModel.from_pretrained", "transformers.RobertaTokenizer.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "pretrain_path", ",", "max_length", ",", "cat_entity_rep", "=", "False", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "self", ".", "roberta", "=", "RobertaModel", ".", "from_pretrained", "(", "pretrain_path", ")", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "tokenizer", "=", "RobertaTokenizer", ".", "from_pretrained", "(", "'roberta-base'", ")", "\n", "self", ".", "cat_entity_rep", "=", "cat_entity_rep", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaSentenceEncoder.forward": [[1016, 1027], ["test.RobertaSentenceEncoder.roberta", "test.RobertaSentenceEncoder.roberta", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "inputs[].size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "if", "not", "self", ".", "cat_entity_rep", ":", "\n", "            ", "_", ",", "x", "=", "self", ".", "roberta", "(", "inputs", "[", "'word'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "\n", "return", "x", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "self", ".", "roberta", "(", "inputs", "[", "'word'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "\n", "tensor_range", "=", "torch", ".", "arange", "(", "inputs", "[", "'word'", "]", ".", "size", "(", ")", "[", "0", "]", ")", "\n", "h_state", "=", "outputs", "[", "0", "]", "[", "tensor_range", ",", "inputs", "[", "\"pos1\"", "]", "]", "\n", "t_state", "=", "outputs", "[", "0", "]", "[", "tensor_range", ",", "inputs", "[", "\"pos2\"", "]", "]", "\n", "state", "=", "torch", ".", "cat", "(", "(", "h_state", ",", "t_state", ")", ",", "-", "1", ")", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaSentenceEncoder.tokenize": [[1028, 1101], ["test.RobertaSentenceEncoder.tokenizer.tokenize", "test.RobertaSentenceEncoder.tokenize.getIns"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "", "def", "tokenize", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ")", ":", "\n", "        ", "def", "getIns", "(", "bped", ",", "bpeTokens", ",", "tokens", ",", "L", ")", ":", "\n", "            ", "resL", "=", "0", "\n", "tkL", "=", "\" \"", ".", "join", "(", "tokens", "[", ":", "L", "]", ")", "\n", "bped_tkL", "=", "\" \"", ".", "join", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "tkL", ")", ")", "\n", "if", "bped", ".", "find", "(", "bped_tkL", ")", "==", "0", ":", "\n", "                ", "resL", "=", "len", "(", "bped_tkL", ".", "split", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "tkL", "+=", "\" \"", "\n", "bped_tkL", "=", "\" \"", ".", "join", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "tkL", ")", ")", "\n", "if", "bped", ".", "find", "(", "bped_tkL", ")", "==", "0", ":", "\n", "                    ", "resL", "=", "len", "(", "bped_tkL", ".", "split", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "Exception", "(", "\"Cannot locate the position\"", ")", "\n", "", "", "return", "resL", "\n", "\n", "", "s", "=", "\" \"", ".", "join", "(", "raw_tokens", ")", "\n", "sst", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "s", ")", "\n", "headL", "=", "pos_head", "[", "0", "]", "\n", "headR", "=", "pos_head", "[", "-", "1", "]", "+", "1", "\n", "hiL", "=", "getIns", "(", "\" \"", ".", "join", "(", "sst", ")", ",", "sst", ",", "raw_tokens", ",", "headL", ")", "\n", "hiR", "=", "getIns", "(", "\" \"", ".", "join", "(", "sst", ")", ",", "sst", ",", "raw_tokens", ",", "headR", ")", "\n", "\n", "tailL", "=", "pos_tail", "[", "0", "]", "\n", "tailR", "=", "pos_tail", "[", "-", "1", "]", "+", "1", "\n", "tiL", "=", "getIns", "(", "\" \"", ".", "join", "(", "sst", ")", ",", "sst", ",", "raw_tokens", ",", "tailL", ")", "\n", "tiR", "=", "getIns", "(", "\" \"", ".", "join", "(", "sst", ")", ",", "sst", ",", "raw_tokens", ",", "tailR", ")", "\n", "\n", "E1b", "=", "'madeupword0000'", "\n", "E1e", "=", "'madeupword0001'", "\n", "E2b", "=", "'madeupword0002'", "\n", "E2e", "=", "'madeupword0003'", "\n", "ins", "=", "[", "(", "hiL", ",", "E1b", ")", ",", "(", "hiR", ",", "E1e", ")", ",", "(", "tiL", ",", "E2b", ")", ",", "(", "tiR", ",", "E2e", ")", "]", "\n", "ins", "=", "sorted", "(", "ins", ")", "\n", "pE1", "=", "0", "\n", "pE2", "=", "0", "\n", "pE1_", "=", "0", "\n", "pE2_", "=", "0", "\n", "for", "i", "in", "range", "(", "0", ",", "4", ")", ":", "\n", "            ", "sst", ".", "insert", "(", "ins", "[", "i", "]", "[", "0", "]", "+", "i", ",", "ins", "[", "i", "]", "[", "1", "]", ")", "\n", "if", "ins", "[", "i", "]", "[", "1", "]", "==", "E1b", ":", "\n", "                ", "pE1", "=", "ins", "[", "i", "]", "[", "0", "]", "+", "i", "\n", "", "elif", "ins", "[", "i", "]", "[", "1", "]", "==", "E2b", ":", "\n", "                ", "pE2", "=", "ins", "[", "i", "]", "[", "0", "]", "+", "i", "\n", "", "elif", "ins", "[", "i", "]", "[", "1", "]", "==", "E1e", ":", "\n", "                ", "pE1_", "=", "ins", "[", "i", "]", "[", "0", "]", "+", "i", "\n", "", "else", ":", "\n", "                ", "pE2_", "=", "ins", "[", "i", "]", "[", "0", "]", "+", "i", "\n", "", "", "pos1_in_index", "=", "pE1", "+", "1", "\n", "pos2_in_index", "=", "pE2", "+", "1", "\n", "sst", "=", "[", "'<s>'", "]", "+", "sst", "\n", "indexed_tokens", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "sst", ")", "\n", "\n", "# padding", "\n", "while", "len", "(", "indexed_tokens", ")", "<", "self", ".", "max_length", ":", "\n", "            ", "indexed_tokens", ".", "append", "(", "1", ")", "\n", "", "indexed_tokens", "=", "indexed_tokens", "[", ":", "self", ".", "max_length", "]", "\n", "\n", "# pos", "\n", "pos1", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "pos2", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "max_length", ")", ":", "\n", "            ", "pos1", "[", "i", "]", "=", "i", "-", "pos1_in_index", "+", "self", ".", "max_length", "\n", "pos2", "[", "i", "]", "=", "i", "-", "pos2_in_index", "+", "self", ".", "max_length", "\n", "\n", "# mask", "\n", "", "mask", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "mask", "[", ":", "len", "(", "sst", ")", "]", "=", "1", "\n", "\n", "pos1_in_index", "=", "min", "(", "self", ".", "max_length", ",", "pos1_in_index", ")", "\n", "pos2_in_index", "=", "min", "(", "self", ".", "max_length", ",", "pos2_in_index", ")", "\n", "\n", "return", "indexed_tokens", ",", "pos1_in_index", "-", "1", ",", "pos2_in_index", "-", "1", ",", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.__init__": [[1105, 1112], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "transformers.RobertaForSequenceClassification.from_pretrained", "transformers.RobertaTokenizer.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "pretrain_path", ",", "max_length", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "self", ".", "roberta", "=", "RobertaForSequenceClassification", ".", "from_pretrained", "(", "\n", "pretrain_path", ",", "\n", "num_labels", "=", "2", ")", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "tokenizer", "=", "RobertaTokenizer", ".", "from_pretrained", "(", "'roberta-base'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.forward": [[1113, 1116], ["test.RobertaPAIRSentenceEncoder.roberta"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "x", "=", "self", ".", "roberta", "(", "inputs", "[", "'word'", "]", ",", "attention_mask", "=", "inputs", "[", "'mask'", "]", ")", "[", "0", "]", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize": [[1117, 1155], ["test.RobertaPAIRSentenceEncoder.tokenizer.tokenize", "test.RobertaPAIRSentenceEncoder.tokenize.getIns"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "def", "tokenize", "(", "self", ",", "raw_tokens", ",", "pos_head", ",", "pos_tail", ")", ":", "\n", "        ", "def", "getIns", "(", "bped", ",", "bpeTokens", ",", "tokens", ",", "L", ")", ":", "\n", "            ", "resL", "=", "0", "\n", "tkL", "=", "\" \"", ".", "join", "(", "tokens", "[", ":", "L", "]", ")", "\n", "bped_tkL", "=", "\" \"", ".", "join", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "tkL", ")", ")", "\n", "if", "bped", ".", "find", "(", "bped_tkL", ")", "==", "0", ":", "\n", "                ", "resL", "=", "len", "(", "bped_tkL", ".", "split", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "tkL", "+=", "\" \"", "\n", "bped_tkL", "=", "\" \"", ".", "join", "(", "self", ".", "tokenizer", ".", "tokenize", "(", "tkL", ")", ")", "\n", "if", "bped", ".", "find", "(", "bped_tkL", ")", "==", "0", ":", "\n", "                    ", "resL", "=", "len", "(", "bped_tkL", ".", "split", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "Exception", "(", "\"Cannot locate the position\"", ")", "\n", "", "", "return", "resL", "\n", "\n", "", "s", "=", "\" \"", ".", "join", "(", "raw_tokens", ")", "\n", "sst", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "s", ")", "\n", "headL", "=", "pos_head", "[", "0", "]", "\n", "headR", "=", "pos_head", "[", "-", "1", "]", "+", "1", "\n", "hiL", "=", "getIns", "(", "\" \"", ".", "join", "(", "sst", ")", ",", "sst", ",", "raw_tokens", ",", "headL", ")", "\n", "hiR", "=", "getIns", "(", "\" \"", ".", "join", "(", "sst", ")", ",", "sst", ",", "raw_tokens", ",", "headR", ")", "\n", "\n", "tailL", "=", "pos_tail", "[", "0", "]", "\n", "tailR", "=", "pos_tail", "[", "-", "1", "]", "+", "1", "\n", "tiL", "=", "getIns", "(", "\" \"", ".", "join", "(", "sst", ")", ",", "sst", ",", "raw_tokens", ",", "tailL", ")", "\n", "tiR", "=", "getIns", "(", "\" \"", ".", "join", "(", "sst", ")", ",", "sst", ",", "raw_tokens", ",", "tailR", ")", "\n", "\n", "E1b", "=", "'madeupword0000'", "\n", "E1e", "=", "'madeupword0001'", "\n", "E2b", "=", "'madeupword0002'", "\n", "E2e", "=", "'madeupword0003'", "\n", "ins", "=", "[", "(", "hiL", ",", "E1b", ")", ",", "(", "hiR", ",", "E1e", ")", ",", "(", "tiL", ",", "E2b", ")", ",", "(", "tiR", ",", "E2e", ")", "]", "\n", "ins", "=", "sorted", "(", "ins", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "4", ")", ":", "\n", "            ", "sst", ".", "insert", "(", "ins", "[", "i", "]", "[", "0", "]", "+", "i", ",", "ins", "[", "i", "]", "[", "1", "]", ")", "\n", "", "indexed_tokens", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "sst", ")", "\n", "return", "indexed_tokens", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDatasetPair.__init__": [[20, 46], ["os.path.join", "json.load", "list", "os.path.exists", "print", "print", "open", "data_kg_loader.FewRelDatasetPair.json_data.keys"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load"], ["def", "__init__", "(", "self", ",", "name", ",", "encoder", ",", "N", ",", "K", ",", "Q", ",", "na_rate", ",", "root", ",", "encoder_name", ",", "ins2cpt", ",", "entity2id", ",", "title2id", ",", "word2id", ",", "\n", "id_from", "=", "'kgEmbeddingOrBeyondWordEmbedding'", ")", ":", "\n", "        ", "self", ".", "root", "=", "root", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "name", "+", "\".json\"", ")", "\n", "# print('file path', path)", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "            ", "print", "(", "'file path'", ",", "path", ")", "\n", "print", "(", "\"[ERROR] Data file does not exist!\"", ")", "\n", "assert", "(", "0", ")", "\n", "", "self", ".", "json_data", "=", "json", ".", "load", "(", "open", "(", "path", ")", ")", "\n", "self", ".", "classes", "=", "list", "(", "self", ".", "json_data", ".", "keys", "(", ")", ")", "\n", "self", ".", "N", "=", "N", "\n", "self", ".", "K", "=", "K", "\n", "self", ".", "Q", "=", "Q", "\n", "self", ".", "na_rate", "=", "na_rate", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "encoder_name", "=", "encoder_name", "\n", "self", ".", "max_length", "=", "encoder", ".", "max_length", "\n", "self", ".", "ins2cpt", "=", "ins2cpt", "\n", "self", ".", "entity2id", "=", "entity2id", "\n", "self", ".", "title2id", "=", "title2id", "\n", "self", ".", "word2id", "=", "word2id", "\n", "self", ".", "id_from", "=", "id_from", "\n", "if", "self", ".", "id_from", "==", "'MultiHeadAttentionAndBeyondWordEmbedding'", ":", "\n", "            ", "self", ".", "id_from", "=", "'BeyondWordEmbedding'", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDatasetPair.__getraw__": [[47, 60], ["data_kg_loader.FewRelDatasetPair.encoder.tokenize_concept"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.tokenize_concept"], ["", "", "def", "__getraw__", "(", "self", ",", "item", ",", "ins2cpt", ")", ":", "\n", "# word = self.encoder.tokenize(item['tokens'],", "\n", "#                              item['h'][2][0],", "\n", "#                              item['t'][2][0])", "\n", "\n", "        ", "word", "=", "self", ".", "encoder", ".", "tokenize_concept", "(", "item", "[", "'tokens'", "]", ",", "\n", "item", "[", "'h'", "]", "[", "2", "]", "[", "0", "]", ",", "\n", "item", "[", "'t'", "]", "[", "2", "]", "[", "0", "]", ",", "\n", "item", "[", "'h'", "]", "[", "0", "]", ",", "\n", "item", "[", "'t'", "]", "[", "0", "]", ",", "\n", "ins2cpt", ")", "\n", "\n", "return", "word", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDatasetPair.__additem__": [[61, 66], ["d[].append", "d[].append", "d[].append", "d[].append"], "methods", ["None"], ["", "def", "__additem__", "(", "self", ",", "d", ",", "word", ",", "pos1", ",", "pos2", ",", "mask", ")", ":", "\n", "        ", "d", "[", "'word'", "]", ".", "append", "(", "word", ")", "\n", "d", "[", "'pos1'", "]", ".", "append", "(", "pos1", ")", "\n", "d", "[", "'pos2'", "]", ".", "append", "(", "pos2", ")", "\n", "d", "[", "'mask'", "]", ".", "append", "(", "mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDatasetPair.__getitem__": [[67, 193], ["random.sample", "int", "list", "enumerate", "range", "enumerate", "filter", "numpy.random.choice", "data_kg_loader.FewRelDatasetPair.__getraw__", "query.append", "enumerate", "list", "data_kg_loader.FewRelDatasetPair.__getraw__", "numpy.random.choice", "numpy.random.choice", "range", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "fusion_set[].append", "fusion_set[].append", "fusion_set[].append", "range", "range", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "fusion_set[].append", "fusion_set[].append", "fusion_set[].append", "fusion_set[].append", "data_kg_loader.FewRelDatasetPair.entityPair2concept2id", "data_kg_loader.FewRelDatasetPair.entityPair2concept2id", "data_kg_loader.FewRelDatasetPair.conceptID2tensor", "fusion_set[].append", "fusion_set[].append", "range", "support.append", "support_entitis.append", "query.append", "query_entities.append", "list", "data_kg_loader.FewRelDatasetPair.encoder.tokenizer.convert_tokens_to_ids", "data_kg_loader.FewRelDatasetPair.encoder.tokenizer.convert_tokens_to_ids", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "data_kg_loader.FewRelDatasetPair.encoder.tokenizer.convert_tokens_to_ids", "data_kg_loader.FewRelDatasetPair.encoder.tokenizer.convert_tokens_to_ids", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "min", "min", "min", "len", "range", "len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "min", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "min", "len", "len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "min", "min", "len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.__getraw__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.__getraw__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.entityPair2concept2id", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.entityPair2concept2id", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.conceptID2tensor"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "target_classes", "=", "random", ".", "sample", "(", "self", ".", "classes", ",", "self", ".", "N", ")", "\n", "support", "=", "[", "]", "\n", "query", "=", "[", "]", "\n", "support_entitis", "=", "[", "]", "# [(h1,t1),(h2,t2)...]", "\n", "query_entities", "=", "[", "]", "\n", "# support_sentence = []", "\n", "# query_sentence = []", "\n", "entities", "=", "[", "]", "\n", "# fusion_set = {'word': [], 'mask': [], 'seg': [], 'entities': []}", "\n", "# fusion_set = {'word': [], 'mask': [], 'seg': []}", "\n", "fusion_set", "=", "{", "'word'", ":", "[", "]", ",", "'mask'", ":", "[", "]", ",", "'seg'", ":", "[", "]", ",", "'query_sen'", ":", "[", "]", ",", "'support_sen'", ":", "[", "]", ",", "\n", "'query_mask'", ":", "[", "]", ",", "'support_mask'", ":", "[", "]", ",", "'queryConceptID'", ":", "[", "]", ",", "'supportConceptID'", ":", "[", "]", "}", "\n", "\n", "query_label", "=", "[", "]", "\n", "Q_na", "=", "int", "(", "self", ".", "na_rate", "*", "self", ".", "Q", ")", "\n", "na_classes", "=", "list", "(", "filter", "(", "lambda", "x", ":", "x", "not", "in", "target_classes", ",", "\n", "self", ".", "classes", ")", ")", "\n", "\n", "for", "i", ",", "class_name", "in", "enumerate", "(", "target_classes", ")", ":", "\n", "            ", "indices", "=", "np", ".", "random", ".", "choice", "(", "\n", "list", "(", "range", "(", "len", "(", "self", ".", "json_data", "[", "class_name", "]", ")", ")", ")", ",", "\n", "self", ".", "K", "+", "self", ".", "Q", ",", "False", ")", "\n", "count", "=", "0", "\n", "for", "j", "in", "indices", ":", "\n", "                ", "word", "=", "self", ".", "__getraw__", "(", "\n", "self", ".", "json_data", "[", "class_name", "]", "[", "j", "]", ",", "self", ".", "ins2cpt", ")", "\n", "if", "count", "<", "self", ".", "K", ":", "\n", "                    ", "support", ".", "append", "(", "word", ")", "\n", "'''\u83b7\u53d6\u5934\u5b9e\u4f53\u5c3e\u5b9e\u4f53'''", "\n", "item", "=", "self", ".", "json_data", "[", "class_name", "]", "[", "j", "]", "\n", "h", "=", "item", "[", "'h'", "]", "[", "0", "]", "\n", "t", "=", "item", "[", "'t'", "]", "[", "0", "]", "\n", "support_entitis", ".", "append", "(", "(", "h", ",", "t", ")", ")", "\n", "# support_sentence.append(self.json_data[class_name][j])", "\n", "", "else", ":", "\n", "                    ", "query", ".", "append", "(", "word", ")", "\n", "'''\u83b7\u53d6\u5934\u5b9e\u4f53\u5c3e\u5b9e\u4f53'''", "\n", "item", "=", "self", ".", "json_data", "[", "class_name", "]", "[", "j", "]", "\n", "h", "=", "item", "[", "'h'", "]", "[", "0", "]", "\n", "t", "=", "item", "[", "'t'", "]", "[", "0", "]", "\n", "query_entities", ".", "append", "(", "(", "h", ",", "t", ")", ")", "\n", "# query_sentence.append(self.json_data[class_name][j])", "\n", "", "count", "+=", "1", "\n", "\n", "", "query_label", "+=", "[", "i", "]", "*", "self", ".", "Q", "\n", "\n", "# NA", "\n", "", "for", "j", "in", "range", "(", "Q_na", ")", ":", "\n", "            ", "cur_class", "=", "np", ".", "random", ".", "choice", "(", "na_classes", ",", "1", ",", "False", ")", "[", "0", "]", "\n", "index", "=", "np", ".", "random", ".", "choice", "(", "\n", "list", "(", "range", "(", "len", "(", "self", ".", "json_data", "[", "cur_class", "]", ")", ")", ")", ",", "\n", "1", ",", "False", ")", "[", "0", "]", "\n", "word", "=", "self", ".", "__getraw__", "(", "\n", "self", ".", "json_data", "[", "cur_class", "]", "[", "index", "]", ",", "self", ".", "ins2cpt", ")", "\n", "query", ".", "append", "(", "word", ")", "\n", "", "query_label", "+=", "[", "self", ".", "N", "]", "*", "Q_na", "\n", "\n", "'''\u9a8c\u8bc1\u53e5\u5b50\u662f\u5426\u548c\u5b9e\u4f53\u4e00\u4e00\u5bf9\u5e94'''", "\n", "# for m,s1 in enumerate(query_sentence):", "\n", "#     for n,s2 in enumerate(support_sentence):", "\n", "#         print('-------------------\u5206\u5272\u7ebf------------------------')", "\n", "#         query_h_t = query_entities[m]", "\n", "#         support_h_t = support_entitis[n]", "\n", "#         print((query_h_t, support_h_t))", "\n", "#         print(s1)", "\n", "#         print(s2)", "\n", "#         print('-------------------\u5206\u5272\u7ebf------------------------')", "\n", "\n", "for", "m", ",", "word_query", "in", "enumerate", "(", "query", ")", ":", "\n", "            ", "for", "n", ",", "word_support", "in", "enumerate", "(", "support", ")", ":", "\n", "# print(m, n)", "\n", "\n", "                ", "if", "self", ".", "encoder_name", "==", "'bert'", ":", "\n", "                    ", "SEP", "=", "self", ".", "encoder", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "'[SEP]'", "]", ")", "\n", "CLS", "=", "self", ".", "encoder", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "'[CLS]'", "]", ")", "\n", "word_tensor", "=", "torch", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ")", ".", "long", "(", ")", "\n", "word_query_tensor", "=", "torch", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ")", ".", "long", "(", ")", "\n", "word_support_tensor", "=", "torch", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ")", ".", "long", "(", ")", "\n", "", "else", ":", "\n", "                    ", "SEP", "=", "self", ".", "encoder", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "'</s>'", "]", ")", "\n", "CLS", "=", "self", ".", "encoder", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "'<s>'", "]", ")", "\n", "word_tensor", "=", "torch", ".", "ones", "(", "(", "self", ".", "max_length", ")", ")", ".", "long", "(", ")", "\n", "", "new_word", "=", "CLS", "+", "word_support", "+", "SEP", "+", "word_query", "+", "SEP", "\n", "for", "i", "in", "range", "(", "min", "(", "self", ".", "max_length", ",", "len", "(", "new_word", ")", ")", ")", ":", "\n", "                    ", "word_tensor", "[", "i", "]", "=", "new_word", "[", "i", "]", "\n", "", "mask_tensor", "=", "torch", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ")", ".", "long", "(", ")", "\n", "mask_tensor", "[", ":", "min", "(", "self", ".", "max_length", ",", "len", "(", "new_word", ")", ")", "]", "=", "1", "\n", "seg_tensor", "=", "torch", ".", "ones", "(", "(", "self", ".", "max_length", ")", ")", ".", "long", "(", ")", "\n", "seg_tensor", "[", ":", "min", "(", "self", ".", "max_length", ",", "len", "(", "word_support", ")", "+", "1", ")", "]", "=", "0", "\n", "fusion_set", "[", "'word'", "]", ".", "append", "(", "word_tensor", ")", "\n", "fusion_set", "[", "'mask'", "]", ".", "append", "(", "mask_tensor", ")", "\n", "fusion_set", "[", "'seg'", "]", ".", "append", "(", "seg_tensor", ")", "\n", "\n", "q_w", "=", "CLS", "+", "word_query", "+", "SEP", "\n", "s_w", "=", "CLS", "+", "word_support", "+", "SEP", "\n", "\n", "for", "i", "in", "range", "(", "min", "(", "self", ".", "max_length", ",", "len", "(", "q_w", ")", ")", ")", ":", "\n", "                    ", "word_query_tensor", "[", "i", "]", "=", "q_w", "[", "i", "]", "\n", "\n", "", "for", "i", "in", "range", "(", "min", "(", "self", ".", "max_length", ",", "len", "(", "s_w", ")", ")", ")", ":", "\n", "                    ", "word_support_tensor", "[", "i", "]", "=", "s_w", "[", "i", "]", "\n", "", "'''\u5c06\u67e5\u8be2\u96c6\u4e2d\u7684\u53e5\u5b50\u548c\u652f\u6491\u96c6\u4e2d\u7684\u53e5\u5b50\u5206\u522b\u5b58\u50a8'''", "\n", "query_mask_tensor", "=", "torch", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ")", ".", "long", "(", ")", "\n", "support_mask_tensor", "=", "torch", ".", "zeros", "(", "(", "self", ".", "max_length", ")", ")", ".", "long", "(", ")", "\n", "\n", "query_mask_tensor", "[", ":", "min", "(", "self", ".", "max_length", ",", "len", "(", "q_w", ")", ")", "]", "=", "1", "\n", "support_mask_tensor", "[", ":", "min", "(", "self", ".", "max_length", ",", "len", "(", "s_w", ")", ")", "]", "=", "1", "\n", "\n", "fusion_set", "[", "'query_sen'", "]", ".", "append", "(", "word_query_tensor", ")", "\n", "fusion_set", "[", "'support_sen'", "]", ".", "append", "(", "word_support_tensor", ")", "\n", "\n", "fusion_set", "[", "'query_mask'", "]", ".", "append", "(", "query_mask_tensor", ")", "\n", "fusion_set", "[", "'support_mask'", "]", ".", "append", "(", "support_mask_tensor", ")", "\n", "\n", "query_h_t", "=", "query_entities", "[", "m", "]", "\n", "support_h_t", "=", "support_entitis", "[", "n", "]", "\n", "\n", "'''\u8fd4\u56de\u5b9e\u4f53\u5bf9\u4e2d\u7684\u5b9e\u4f53\u5728concpetgraph\u4e2d\u7684\u6982\u5ff5\u7684id'''", "\n", "queryConceptID", "=", "self", ".", "entityPair2concept2id", "(", "query_h_t", ",", "id_from", "=", "self", ".", "id_from", ")", "\n", "supportConceptID", "=", "self", ".", "entityPair2concept2id", "(", "support_h_t", ",", "id_from", "=", "self", ".", "id_from", ")", "\n", "queryConceptIDtensor", ",", "supportConceptIDtensor", "=", "self", ".", "conceptID2tensor", "(", "queryConceptID", ",", "supportConceptID", ",", "\n", "id_from", "=", "self", ".", "id_from", ")", "\n", "fusion_set", "[", "'queryConceptID'", "]", ".", "append", "(", "queryConceptIDtensor", ")", "\n", "fusion_set", "[", "'supportConceptID'", "]", ".", "append", "(", "supportConceptIDtensor", ")", "\n", "", "", "return", "fusion_set", ",", "query_label", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDatasetPair.__len__": [[194, 196], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "1000000000", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDatasetPair.conceptID2tensor": [[197, 237], ["torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "range", "range", "len", "len", "range", "range", "print", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "len", "range", "len", "range", "len", "len", "print", "print", "print", "print", "print", "print"], "methods", ["None"], ["", "def", "conceptID2tensor", "(", "self", ",", "queryConceptID", ",", "supportConceptID", ",", "id_from", "=", "'kgEmbeddingOrBeyondWordEmbedding'", ")", ":", "\n", "        ", "'''\n        \u7ed9\u5b9a\u67e5\u8be2\u96c6\u548c\u652f\u6491\u96c6\u53e5\u5b50\u4e2d\u7684\u6982\u5ff5ID\u6216ID list\u8fd4\u56de\u5176\u5bf9\u5e94\u7684tensor\n        #Para:id_from: id\u6e90\u4e8e\u9884\u8bad\u7ec3\u7684kg embedding\u4e2d\u8bcd\u7684id \u6216\u8005\u6e90\u4e8e\u8bba\u6587[1]\u4e2d\u9884\u8bad\u7ec3\u8bcd\u5411\u91cfword2id\u4e2d\u8bcd\u7684id\n         References:\n            [1]Beyond Word Embeddings: Learning Entity and Concept Representations from Large Scale Knowledge Bases\n        '''", "\n", "if", "id_from", "==", "'kgEmbedding'", ":", "\n", "            ", "queryConceptIDtensor", "=", "torch", ".", "zeros", "(", "4", ")", ".", "long", "(", ")", "\n", "supportConceptIDtensor", "=", "torch", ".", "zeros", "(", "4", ")", ".", "long", "(", ")", "\n", "\n", "for", "p", "in", "range", "(", "len", "(", "queryConceptID", ")", ")", ":", "\n", "                ", "queryConceptIDtensor", "[", "p", "]", "=", "queryConceptID", "[", "p", "]", "\n", "\n", "", "for", "q", "in", "range", "(", "len", "(", "supportConceptID", ")", ")", ":", "\n", "                ", "supportConceptIDtensor", "[", "q", "]", "=", "supportConceptID", "[", "q", "]", "\n", "", "", "elif", "id_from", "==", "'BeyondWordEmbedding'", ":", "\n", "# \u6982\u5ff5\u4e2d\u7684\u8bcd\u6700\u591a12\u4e2a\uff0c\u5e73\u5747\u503c\u4e3a2.026551292913871", "\n", "            ", "queryConceptIDtensor", "=", "torch", ".", "ones", "(", "[", "4", ",", "12", "]", ")", "*", "-", "2", "# \u521d\u59cb\u5316\u503c\u4e3a-2\u7684tensor,\u67e5\u8be2\u96c6\u6bcf\u4e2a\u53e5\u5b50\u5934\u5c3e\u5b9e\u4f53\u5bf9\u5e94\u56db\u4e2a\u6982\u5ff5\uff0c\u6bcf\u4e2a\u6982\u5ff5\u6700\u591a\u5bf9\u5e94\u4e94\u4e2a\u8bcd", "\n", "supportConceptIDtensor", "=", "torch", ".", "ones", "(", "[", "4", ",", "12", "]", ")", "*", "-", "2", "\n", "for", "i", "in", "range", "(", "len", "(", "queryConceptID", ")", ")", ":", "\n", "                ", "for", "j", "in", "range", "(", "len", "(", "queryConceptID", "[", "i", "]", ")", ")", ":", "\n", "                    ", "try", ":", "\n", "                        ", "queryConceptIDtensor", "[", "i", ",", "j", "]", "=", "queryConceptID", "[", "i", "]", "[", "j", "]", "\n", "", "except", "Exception", "as", "e", ":", "\n", "                        ", "print", "(", "'-------------queryConceptID-----------'", ")", "\n", "print", "(", "queryConceptID", ")", "\n", "print", "(", "i", ",", "j", ")", "\n", "\n", "", "", "", "for", "i", "in", "range", "(", "len", "(", "supportConceptID", ")", ")", ":", "\n", "                ", "for", "j", "in", "range", "(", "len", "(", "supportConceptID", "[", "i", "]", ")", ")", ":", "\n", "                    ", "try", ":", "\n", "                        ", "supportConceptIDtensor", "[", "i", ",", "j", "]", "=", "supportConceptID", "[", "i", "]", "[", "j", "]", "\n", "", "except", "Exception", "as", "e", ":", "\n", "                        ", "print", "(", "'-------------------supportConceptID-----------------'", ")", "\n", "print", "(", "'supportConceptID'", ",", "supportConceptID", ")", "\n", "print", "(", "i", ",", "j", ")", "\n", "", "", "", "", "else", ":", "\n", "            ", "print", "(", "'please input right id source'", ")", "\n", "", "return", "queryConceptIDtensor", ",", "supportConceptIDtensor", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDatasetPair.entityPair2concept2id": [[238, 254], ["range", "len", "data_kg_loader.FewRelDatasetPair.entity2concept2id", "id.append", "id.append"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.entity2concept2id"], ["", "def", "entityPair2concept2id", "(", "self", ",", "entityPair", ",", "id_from", "=", "'kgEmbeddingOrBeyondWordEmbedding'", ")", ":", "\n", "        ", "'''\n        \u8fd4\u56de\u5b9e\u4f53\u5bf9\u4e2d\u7684\u5b9e\u4f53\u5728concpetgraph\u4e2d\u7684\u6982\u5ff5\u7684id\uff0c[h1cpt1toID,h2cpt2toID,t1cpt2toID,t2cpt2toID]\n        h1cpt1toID\uff1a\u5934\u5b9e\u4f53\u5bf9\u5e94\u7684\u6982\u5ff51\u5bf9\u5e94\u7684ID \u6216 ID list\n        #Para:id_from: id\u6e90\u4e8e\u9884\u8bad\u7ec3\u7684kg embedding\u4e2d\u8bcd\u7684id \u6216\u8005\u6e90\u4e8e\u8bba\u6587[1]\u4e2d\u9884\u8bad\u7ec3\u8bcd\u5411\u91cfword2id\u4e2d\u8bcd\u7684id\n        References:\n            [1]Beyond Word Embeddings: Learning Entity and Concept Representations from Large Scale Knowledge Bases\n        '''", "\n", "id", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "entityPair", ")", ")", ":", "\n", "            ", "entity", "=", "entityPair", "[", "i", "]", "\n", "h2concept1toid", ",", "h2concept2toid", "=", "self", ".", "entity2concept2id", "(", "entity", ",", "id_from", ")", "\n", "id", ".", "append", "(", "h2concept1toid", ")", "\n", "id", ".", "append", "(", "h2concept2toid", ")", "\n", "", "return", "id", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDatasetPair.entity2concept2id": [[255, 293], ["entity.lower.lower.lower", "fewshot_re_kit.conceptgraph_utils.instance2conept", "e2concept[].lower", "e2concept[].lower", "print", "data_kg_loader.FewRelDatasetPair.id_from_BeyondWordEmbedding", "print", "print", "data_kg_loader.FewRelDatasetPair.id_from_BeyondWordEmbedding", "print"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.instance2conept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.id_from_BeyondWordEmbedding", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.id_from_BeyondWordEmbedding"], ["", "def", "entity2concept2id", "(", "self", ",", "entity", ",", "id_from", ")", ":", "\n", "        ", "entity", "=", "entity", ".", "lower", "(", ")", "\n", "e2concept", "=", "instance2conept", "(", "self", ".", "ins2cpt", ",", "entity", ")", "\n", "e2concept1", "=", "e2concept", "[", "0", "]", ".", "lower", "(", ")", "\n", "e2concept2", "=", "e2concept", "[", "1", "]", ".", "lower", "(", ")", "\n", "\n", "if", "(", "e2concept1", "==", "'unknowconcept1'", ")", "or", "(", "e2concept1", "==", "'unknowconcept2'", ")", ":", "\n", "\n", "            ", "if", "id_from", "==", "'kgEmbedding'", ":", "\n", "                ", "e2concept1toid", "=", "-", "1", "\n", "", "elif", "id_from", "==", "'BeyondWordEmbedding'", ":", "\n", "                ", "e2concept1toid", "=", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "print", "(", "'please input right id source'", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "id_from", "==", "'kgEmbedding'", ":", "\n", "                ", "e2concept1toid", "=", "self", ".", "entity2id", "[", "e2concept1", "]", "\n", "", "elif", "id_from", "==", "'BeyondWordEmbedding'", ":", "\n", "                ", "e2concept1toid", "=", "self", ".", "id_from_BeyondWordEmbedding", "(", "e2concept1", ")", "\n", "", "else", ":", "\n", "                ", "print", "(", "'please input right id source'", ")", "\n", "\n", "", "", "if", "(", "e2concept2", "==", "'unknowconcept1'", ")", "or", "(", "e2concept2", "==", "'unknowconcept2'", ")", ":", "\n", "            ", "if", "id_from", "==", "'kgEmbedding'", ":", "\n", "                ", "e2concept2toid", "=", "-", "1", "\n", "", "elif", "id_from", "==", "'BeyondWordEmbedding'", ":", "\n", "                ", "e2concept2toid", "=", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "print", "(", "'please input right id source'", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "id_from", "==", "'kgEmbedding'", ":", "\n", "                ", "e2concept2toid", "=", "self", ".", "entity2id", "[", "e2concept2", "]", "\n", "", "elif", "id_from", "==", "'BeyondWordEmbedding'", ":", "\n", "                ", "e2concept2toid", "=", "self", ".", "id_from_BeyondWordEmbedding", "(", "e2concept2", ")", "\n", "", "else", ":", "\n", "                ", "print", "(", "'please input right id source'", ")", "\n", "\n", "", "", "return", "e2concept1toid", ",", "e2concept2toid", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDatasetPair.id_from_BeyondWordEmbedding": [[294, 319], ["concept.lower.lower.lower", "data_kg_loader.FewRelDatasetPair.title2id.get", "concept.lower.lower.split", "data_kg_loader.FewRelDatasetPair.word2id.get", "data_kg_loader.FewRelDatasetPair.word2id.get", "id.append", "id.append", "id.append", "id.append"], "methods", ["None"], ["", "def", "id_from_BeyondWordEmbedding", "(", "self", ",", "concept", ":", "str", ")", "->", "list", ":", "\n", "        ", "'''\n        \u8f93\u5165\u6982\u5ff5\uff0c\u4f18\u5148\u83b7\u5f97\u5176\u5728title2id\u4e2d\u7684id\uff0c\u5982\u679ctitle2id\u7684key\u4e2d\u6ca1\u6709concept,\u628aconcept\u8fdb\u884c\u5206\u8bcd\uff0c\u83b7\u53d6\u6bcf\u4e2a\u8bcd\u5728word2id\u4e2d\u7684id,\u8fd4\u56deID list\n        '''", "\n", "id", "=", "[", "]", "\n", "concept", "=", "concept", ".", "lower", "(", ")", "\n", "cptID", "=", "self", ".", "title2id", ".", "get", "(", "concept", ")", "\n", "if", "cptID", "==", "None", ":", "\n", "            ", "cpt_words", "=", "concept", ".", "split", "(", "' '", ")", "\n", "for", "word", "in", "cpt_words", ":", "\n", "                ", "w2id", "=", "self", ".", "word2id", ".", "get", "(", "word", ")", "\n", "if", "w2id", "==", "None", ":", "\n", "                    ", "w2id", "=", "-", "1", "\n", "id", ".", "append", "(", "w2id", ")", "\n", "", "else", ":", "\n", "                    ", "id", ".", "append", "(", "w2id", ")", "\n", "", "", "", "else", ":", "\n", "            ", "cptID", "=", "cptID", "[", "0", "]", "\n", "cptID", "=", "self", ".", "word2id", ".", "get", "(", "cptID", ")", "\n", "if", "cptID", "==", "None", ":", "\n", "                ", "cptID", "=", "-", "1", "\n", "id", ".", "append", "(", "cptID", ")", "\n", "", "else", ":", "\n", "                ", "id", ".", "append", "(", "cptID", ")", "\n", "", "", "return", "id", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.__init__": [[357, 395], ["os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "json.load", "list", "fewshot_re_kit.conceptgraph_utils.loadingInstance2concept", "data_kg_loader.load_pickle", "data_kg_loader.load_json", "data_kg_loader.load_json", "os.path.exists", "print", "os.path.exists", "print", "os.path.exists", "print", "os.path.exists", "print", "os.path.exists", "print", "open", "data_kg_loader.FewRelDataset.json_data.keys"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.loadingInstance2concept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.load_pickle", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.load_json", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.load_json"], ["def", "__init__", "(", "self", ",", "name", ",", "encoder", ",", "N", ",", "K", ",", "Q", ",", "na_rate", ",", "root", ",", "ins2cpt", ",", "concept", ",", "id_from", ",", "entity2id", ",", "title2id", ",", "word2id", ")", ":", "\n", "        ", "self", ".", "root", "=", "root", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "name", "+", "\".json\"", ")", "\n", "\n", "concept_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "ins2cpt", "+", "'.pickle'", ")", "\n", "entity2id_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "entity2id", "+", "'.pickle'", ")", "\n", "title2id_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "title2id", "+", "'.json'", ")", "\n", "word2id_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "word2id", "+", "'.json'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "            ", "print", "(", "\"[ERROR] Data file does not exist!\"", ",", "path", ")", "\n", "assert", "(", "0", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "concept_path", ")", ":", "\n", "            ", "print", "(", "\"[ERROR] Data file does not exist!\"", ",", "concept_path", ")", "\n", "assert", "(", "0", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "entity2id_path", ")", ":", "\n", "            ", "print", "(", "\"[ERROR] Data file does not exist!\"", ",", "entity2id_path", ")", "\n", "assert", "(", "0", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "title2id_path", ")", ":", "\n", "            ", "print", "(", "\"[ERROR] Data file does not exist!\"", ",", "title2id_path", ")", "\n", "assert", "(", "0", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "word2id_path", ")", ":", "\n", "            ", "print", "(", "\"[ERROR] Data file does not exist!\"", ",", "word2id_path", ")", "\n", "assert", "(", "0", ")", "\n", "", "self", ".", "json_data", "=", "json", ".", "load", "(", "open", "(", "path", ")", ")", "\n", "self", ".", "classes", "=", "list", "(", "self", ".", "json_data", ".", "keys", "(", ")", ")", "\n", "self", ".", "N", "=", "N", "\n", "self", ".", "K", "=", "K", "\n", "self", ".", "Q", "=", "Q", "\n", "self", ".", "na_rate", "=", "na_rate", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "ins2cpt", "=", "loadingInstance2concept", "(", "concept_path", ")", "\n", "self", ".", "concept", "=", "concept", "\n", "self", ".", "id_from", "=", "id_from", "\n", "self", ".", "entity2id", "=", "load_pickle", "(", "entity2id_path", ")", "\n", "self", ".", "title2id", "=", "load_json", "(", "title2id_path", ")", "\n", "self", ".", "word2id", "=", "load_json", "(", "word2id_path", ")", "\n", "if", "self", ".", "id_from", "==", "'MultiHeadAttentionAndBeyondWordEmbedding'", ":", "\n", "            ", "self", ".", "id_from", "=", "'BeyondWordEmbedding'", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.__getraw__": [[396, 412], ["data_kg_loader.FewRelDataset.encoder.tokenize_concept", "data_kg_loader.FewRelDataset.encoder.tokenize"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.BERTPAIRConceptSentenceEncoder.tokenize_concept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.test.RobertaPAIRSentenceEncoder.tokenize"], ["", "", "def", "__getraw__", "(", "self", ",", "item", ")", ":", "\n", "# word, pos1, pos2, mask = self.encoder.tokenize(item['tokens'],", "\n", "#                                                item['h'][2][0],", "\n", "#                                                item['t'][2][0])", "\n", "        ", "if", "self", ".", "concept", ":", "\n", "            ", "word", ",", "pos1", ",", "pos2", ",", "mask", "=", "self", ".", "encoder", ".", "tokenize_concept", "(", "item", "[", "'tokens'", "]", ",", "\n", "item", "[", "'h'", "]", "[", "2", "]", "[", "0", "]", ",", "\n", "item", "[", "'t'", "]", "[", "2", "]", "[", "0", "]", ",", "\n", "item", "[", "'h'", "]", "[", "0", "]", ",", "\n", "item", "[", "'t'", "]", "[", "0", "]", ",", "\n", "self", ".", "ins2cpt", ")", "\n", "", "else", ":", "\n", "            ", "word", ",", "pos1", ",", "pos2", ",", "mask", "=", "self", ".", "encoder", ".", "tokenize", "(", "item", "[", "'tokens'", "]", ",", "\n", "item", "[", "'h'", "]", "[", "2", "]", "[", "0", "]", ",", "\n", "item", "[", "'t'", "]", "[", "2", "]", "[", "0", "]", ")", "\n", "", "return", "word", ",", "pos1", ",", "pos2", ",", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.__additem__": [[414, 420], ["d[].append", "d[].append", "d[].append", "d[].append", "d[].append"], "methods", ["None"], ["", "def", "__additem__", "(", "self", ",", "d", ",", "word", ",", "pos1", ",", "pos2", ",", "mask", ",", "conceptIDtensor", ")", ":", "\n", "        ", "d", "[", "'word'", "]", ".", "append", "(", "word", ")", "\n", "d", "[", "'pos1'", "]", ".", "append", "(", "pos1", ")", "\n", "d", "[", "'pos2'", "]", ".", "append", "(", "pos2", ")", "\n", "d", "[", "'mask'", "]", ".", "append", "(", "mask", ")", "\n", "d", "[", "'conceptID'", "]", ".", "append", "(", "conceptIDtensor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.__getitem__": [[421, 472], ["random.sample", "int", "list", "enumerate", "range", "filter", "numpy.random.choice", "data_kg_loader.FewRelDataset.__getraw__", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "data_kg_loader.FewRelDataset.__additem__", "list", "data_kg_loader.FewRelDataset.__getraw__", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "data_kg_loader.FewRelDataset.entityPair2concept2id", "data_kg_loader.FewRelDataset.conceptID2tensor", "numpy.random.choice", "numpy.random.choice", "range", "data_kg_loader.FewRelDataset.__additem__", "data_kg_loader.FewRelDataset.__additem__", "list", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "range", "len"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.__getraw__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.__additem__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.__getraw__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.entityPair2concept2id", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.conceptID2tensor", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.__additem__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.__additem__"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "target_classes", "=", "random", ".", "sample", "(", "self", ".", "classes", ",", "self", ".", "N", ")", "\n", "support_set", "=", "{", "'word'", ":", "[", "]", ",", "'pos1'", ":", "[", "]", ",", "'pos2'", ":", "[", "]", ",", "'mask'", ":", "[", "]", ",", "'conceptID'", ":", "[", "]", "}", "\n", "query_set", "=", "{", "'word'", ":", "[", "]", ",", "'pos1'", ":", "[", "]", ",", "'pos2'", ":", "[", "]", ",", "'mask'", ":", "[", "]", ",", "'conceptID'", ":", "[", "]", "}", "\n", "query_label", "=", "[", "]", "\n", "Q_na", "=", "int", "(", "self", ".", "na_rate", "*", "self", ".", "Q", ")", "\n", "na_classes", "=", "list", "(", "filter", "(", "lambda", "x", ":", "x", "not", "in", "target_classes", ",", "\n", "self", ".", "classes", ")", ")", "\n", "\n", "for", "i", ",", "class_name", "in", "enumerate", "(", "target_classes", ")", ":", "\n", "            ", "indices", "=", "np", ".", "random", ".", "choice", "(", "\n", "list", "(", "range", "(", "len", "(", "self", ".", "json_data", "[", "class_name", "]", ")", ")", ")", ",", "\n", "self", ".", "K", "+", "self", ".", "Q", ",", "False", ")", "\n", "count", "=", "0", "\n", "for", "j", "in", "indices", ":", "\n", "                ", "word", ",", "pos1", ",", "pos2", ",", "mask", "=", "self", ".", "__getraw__", "(", "\n", "self", ".", "json_data", "[", "class_name", "]", "[", "j", "]", ")", "\n", "word", "=", "torch", ".", "tensor", "(", "word", ")", ".", "long", "(", ")", "\n", "pos1", "=", "torch", ".", "tensor", "(", "pos1", ")", ".", "long", "(", ")", "\n", "pos2", "=", "torch", ".", "tensor", "(", "pos2", ")", ".", "long", "(", ")", "\n", "mask", "=", "torch", ".", "tensor", "(", "mask", ")", ".", "long", "(", ")", "\n", "\n", "item", "=", "self", ".", "json_data", "[", "class_name", "]", "[", "j", "]", "\n", "h", "=", "item", "[", "'h'", "]", "[", "0", "]", "\n", "t", "=", "item", "[", "'t'", "]", "[", "0", "]", "\n", "conceptID", "=", "self", ".", "entityPair2concept2id", "(", "(", "h", ",", "t", ")", ",", "id_from", "=", "self", ".", "id_from", ")", "\n", "conceptIDtensor", "=", "self", ".", "conceptID2tensor", "(", "conceptID", ",", "id_from", "=", "self", ".", "id_from", ")", "\n", "if", "count", "<", "self", ".", "K", ":", "\n", "                    ", "self", ".", "__additem__", "(", "support_set", ",", "word", ",", "pos1", ",", "pos2", ",", "mask", ",", "conceptIDtensor", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "__additem__", "(", "query_set", ",", "word", ",", "pos1", ",", "pos2", ",", "mask", ",", "conceptIDtensor", ")", "\n", "", "count", "+=", "1", "\n", "\n", "", "query_label", "+=", "[", "i", "]", "*", "self", ".", "Q", "\n", "\n", "# NA", "\n", "", "for", "j", "in", "range", "(", "Q_na", ")", ":", "\n", "            ", "cur_class", "=", "np", ".", "random", ".", "choice", "(", "na_classes", ",", "1", ",", "False", ")", "[", "0", "]", "\n", "index", "=", "np", ".", "random", ".", "choice", "(", "\n", "list", "(", "range", "(", "len", "(", "self", ".", "json_data", "[", "cur_class", "]", ")", ")", ")", ",", "\n", "1", ",", "False", ")", "[", "0", "]", "\n", "word", ",", "pos1", ",", "pos2", ",", "mask", "=", "self", ".", "__getraw__", "(", "\n", "self", ".", "json_data", "[", "cur_class", "]", "[", "index", "]", ")", "\n", "word", "=", "torch", ".", "tensor", "(", "word", ")", ".", "long", "(", ")", "\n", "pos1", "=", "torch", ".", "tensor", "(", "pos1", ")", ".", "long", "(", ")", "\n", "pos2", "=", "torch", ".", "tensor", "(", "pos2", ")", ".", "long", "(", ")", "\n", "mask", "=", "torch", ".", "tensor", "(", "mask", ")", ".", "long", "(", ")", "\n", "self", ".", "__additem__", "(", "query_set", ",", "word", ",", "pos1", ",", "pos2", ",", "mask", ")", "\n", "", "query_label", "+=", "[", "self", ".", "N", "]", "*", "Q_na", "\n", "\n", "return", "support_set", ",", "query_set", ",", "query_label", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.__len__": [[473, 475], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "1000000000", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.conceptID2tensor": [[476, 503], ["torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "range", "len", "range", "print", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "len", "range", "len", "print", "print", "print"], "methods", ["None"], ["", "def", "conceptID2tensor", "(", "self", ",", "ConceptID", ",", "id_from", "=", "'kgEmbeddingOrBeyondWordEmbedding'", ")", ":", "\n", "        ", "'''\n        \u7ed9\u5b9a\u67e5\u8be2\u96c6\u548c\u652f\u6491\u96c6\u53e5\u5b50\u4e2d\u7684\u6982\u5ff5ID\u6216ID list\u8fd4\u56de\u5176\u5bf9\u5e94\u7684tensor\n        #Para:id_from: id\u6e90\u4e8e\u9884\u8bad\u7ec3\u7684kg embedding\u4e2d\u8bcd\u7684id \u6216\u8005\u6e90\u4e8e\u8bba\u6587[1]\u4e2d\u9884\u8bad\u7ec3\u8bcd\u5411\u91cfword2id\u4e2d\u8bcd\u7684id\n        References:\n            [1]Beyond Word Embeddings: Learning Entity and Concept Representations from Large Scale Knowledge Bases\n        '''", "\n", "if", "id_from", "==", "'kgEmbedding'", ":", "\n", "            ", "conceptIDtensor", "=", "torch", ".", "zeros", "(", "4", ")", ".", "long", "(", ")", "\n", "\n", "for", "p", "in", "range", "(", "len", "(", "ConceptID", ")", ")", ":", "\n", "                ", "conceptIDtensor", "[", "p", "]", "=", "ConceptID", "[", "p", "]", "\n", "\n", "", "", "elif", "id_from", "==", "'BeyondWordEmbedding'", ":", "\n", "# \u6982\u5ff5\u4e2d\u7684\u8bcd\u6700\u591a12\u4e2a\uff0c\u5e73\u5747\u503c\u4e3a2.026551292913871", "\n", "            ", "conceptIDtensor", "=", "torch", ".", "ones", "(", "[", "4", ",", "12", "]", ")", "*", "-", "2", "# \u521d\u59cb\u5316\u503c\u4e3a-2\u7684tensor,\u67e5\u8be2\u96c6\u6bcf\u4e2a\u53e5\u5b50\u5934\u5c3e\u5b9e\u4f53\u5bf9\u5e94\u56db\u4e2a\u6982\u5ff5\uff0c\u6bcf\u4e2a\u6982\u5ff5\u6700\u591a\u5bf9\u5e94\u4e94\u4e2a\u8bcd", "\n", "for", "i", "in", "range", "(", "len", "(", "ConceptID", ")", ")", ":", "\n", "                ", "for", "j", "in", "range", "(", "len", "(", "ConceptID", "[", "i", "]", ")", ")", ":", "\n", "                    ", "try", ":", "\n", "                        ", "conceptIDtensor", "[", "i", ",", "j", "]", "=", "ConceptID", "[", "i", "]", "[", "j", "]", "\n", "", "except", "Exception", "as", "e", ":", "\n", "                        ", "print", "(", "'-------------queryConceptID-----------'", ")", "\n", "print", "(", "ConceptID", ")", "\n", "print", "(", "i", ",", "j", ")", "\n", "", "", "", "", "else", ":", "\n", "            ", "print", "(", "'please input right id source'", ")", "\n", "", "return", "conceptIDtensor", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.entityPair2concept2id": [[504, 520], ["range", "len", "data_kg_loader.FewRelDataset.entity2concept2id", "id.append", "id.append"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.entity2concept2id"], ["", "def", "entityPair2concept2id", "(", "self", ",", "entityPair", ",", "id_from", "=", "'kgEmbeddingOrBeyondWordEmbedding'", ")", ":", "\n", "        ", "'''\n        \u8fd4\u56de\u5b9e\u4f53\u5bf9\u4e2d\u7684\u5b9e\u4f53\u5728concpetgraph\u4e2d\u7684\u6982\u5ff5\u7684id\uff0c[h1cpt1toID,h2cpt2toID,t1cpt2toID,t2cpt2toID]\n        h1cpt1toID\uff1a\u5934\u5b9e\u4f53\u5bf9\u5e94\u7684\u6982\u5ff51\u5bf9\u5e94\u7684ID \u6216 ID list\n        #Para:id_from: id\u6e90\u4e8e\u9884\u8bad\u7ec3\u7684kg embedding\u4e2d\u8bcd\u7684id \u6216\u8005\u6e90\u4e8e\u8bba\u6587[1]\u4e2d\u9884\u8bad\u7ec3\u8bcd\u5411\u91cfword2id\u4e2d\u8bcd\u7684id\n        References:\n            [1]Beyond Word Embeddings: Learning Entity and Concept Representations from Large Scale Knowledge Bases\n        '''", "\n", "id", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "entityPair", ")", ")", ":", "\n", "            ", "entity", "=", "entityPair", "[", "i", "]", "\n", "h2concept1toid", ",", "h2concept2toid", "=", "self", ".", "entity2concept2id", "(", "entity", ",", "id_from", ")", "\n", "id", ".", "append", "(", "h2concept1toid", ")", "\n", "id", ".", "append", "(", "h2concept2toid", ")", "\n", "", "return", "id", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.entity2concept2id": [[521, 559], ["entity.lower.lower.lower", "fewshot_re_kit.conceptgraph_utils.instance2conept", "e2concept[].lower", "e2concept[].lower", "print", "data_kg_loader.FewRelDataset.id_from_BeyondWordEmbedding", "print", "print", "data_kg_loader.FewRelDataset.id_from_BeyondWordEmbedding", "print"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.instance2conept", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.id_from_BeyondWordEmbedding", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.id_from_BeyondWordEmbedding"], ["", "def", "entity2concept2id", "(", "self", ",", "entity", ",", "id_from", ")", ":", "\n", "        ", "entity", "=", "entity", ".", "lower", "(", ")", "\n", "e2concept", "=", "instance2conept", "(", "self", ".", "ins2cpt", ",", "entity", ")", "\n", "e2concept1", "=", "e2concept", "[", "0", "]", ".", "lower", "(", ")", "\n", "e2concept2", "=", "e2concept", "[", "1", "]", ".", "lower", "(", ")", "\n", "\n", "if", "(", "e2concept1", "==", "'unknowconcept1'", ")", "or", "(", "e2concept1", "==", "'unknowconcept2'", ")", ":", "\n", "\n", "            ", "if", "id_from", "==", "'kgEmbedding'", ":", "\n", "                ", "e2concept1toid", "=", "-", "1", "\n", "", "elif", "id_from", "==", "'BeyondWordEmbedding'", ":", "\n", "                ", "e2concept1toid", "=", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "print", "(", "'please input right id source'", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "id_from", "==", "'kgEmbedding'", ":", "\n", "                ", "e2concept1toid", "=", "self", ".", "entity2id", "[", "e2concept1", "]", "\n", "", "elif", "id_from", "==", "'BeyondWordEmbedding'", ":", "\n", "                ", "e2concept1toid", "=", "self", ".", "id_from_BeyondWordEmbedding", "(", "e2concept1", ")", "\n", "", "else", ":", "\n", "                ", "print", "(", "'please input right id source'", ")", "\n", "\n", "", "", "if", "(", "e2concept2", "==", "'unknowconcept1'", ")", "or", "(", "e2concept2", "==", "'unknowconcept2'", ")", ":", "\n", "            ", "if", "id_from", "==", "'kgEmbedding'", ":", "\n", "                ", "e2concept2toid", "=", "-", "1", "\n", "", "elif", "id_from", "==", "'BeyondWordEmbedding'", ":", "\n", "                ", "e2concept2toid", "=", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "print", "(", "'please input right id source'", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "id_from", "==", "'kgEmbedding'", ":", "\n", "                ", "e2concept2toid", "=", "self", ".", "entity2id", "[", "e2concept2", "]", "\n", "", "elif", "id_from", "==", "'BeyondWordEmbedding'", ":", "\n", "                ", "e2concept2toid", "=", "self", ".", "id_from_BeyondWordEmbedding", "(", "e2concept2", ")", "\n", "", "else", ":", "\n", "                ", "print", "(", "'please input right id source'", ")", "\n", "\n", "", "", "return", "e2concept1toid", ",", "e2concept2toid", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.FewRelDataset.id_from_BeyondWordEmbedding": [[560, 585], ["concept.lower.lower.lower", "data_kg_loader.FewRelDataset.title2id.get", "concept.lower.lower.split", "data_kg_loader.FewRelDataset.word2id.get", "data_kg_loader.FewRelDataset.word2id.get", "id.append", "id.append", "id.append", "id.append"], "methods", ["None"], ["", "def", "id_from_BeyondWordEmbedding", "(", "self", ",", "concept", ":", "str", ")", "->", "list", ":", "\n", "        ", "'''\n        \u8f93\u5165\u6982\u5ff5\uff0c\u4f18\u5148\u83b7\u5f97\u5176\u5728title2id\u4e2d\u7684id\uff0c\u5982\u679ctitle2id\u7684key\u4e2d\u6ca1\u6709concept,\u628aconcept\u8fdb\u884c\u5206\u8bcd\uff0c\u83b7\u53d6\u6bcf\u4e2a\u8bcd\u5728word2id\u4e2d\u7684id,\u8fd4\u56deID list\n        '''", "\n", "id", "=", "[", "]", "\n", "concept", "=", "concept", ".", "lower", "(", ")", "\n", "cptID", "=", "self", ".", "title2id", ".", "get", "(", "concept", ")", "\n", "if", "cptID", "==", "None", ":", "\n", "            ", "cpt_words", "=", "concept", ".", "split", "(", "' '", ")", "\n", "for", "word", "in", "cpt_words", ":", "\n", "                ", "w2id", "=", "self", ".", "word2id", ".", "get", "(", "word", ")", "\n", "if", "w2id", "==", "None", ":", "\n", "                    ", "w2id", "=", "-", "1", "\n", "id", ".", "append", "(", "w2id", ")", "\n", "", "else", ":", "\n", "                    ", "id", ".", "append", "(", "w2id", ")", "\n", "", "", "", "else", ":", "\n", "            ", "cptID", "=", "cptID", "[", "0", "]", "\n", "cptID", "=", "self", ".", "word2id", ".", "get", "(", "cptID", ")", "\n", "if", "cptID", "==", "None", ":", "\n", "                ", "cptID", "=", "-", "1", "\n", "id", ".", "append", "(", "cptID", ")", "\n", "", "else", ":", "\n", "                ", "id", ".", "append", "(", "cptID", ")", "\n", "", "", "return", "id", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.collate_fn_pair": [[321, 336], ["zip", "range", "torch.tensor", "torch.tensor", "len", "torch.stack", "torch.stack"], "function", ["None"], ["", "", "def", "collate_fn_pair", "(", "data", ")", ":", "\n", "    ", "batch_set", "=", "{", "'word'", ":", "[", "]", ",", "'mask'", ":", "[", "]", ",", "'seg'", ":", "[", "]", ",", "'query_sen'", ":", "[", "]", ",", "'support_sen'", ":", "[", "]", ",", "\n", "'query_mask'", ":", "[", "]", ",", "'support_mask'", ":", "[", "]", ",", "'queryConceptID'", ":", "[", "]", ",", "'supportConceptID'", ":", "[", "]", "}", "\n", "batch_label", "=", "[", "]", "\n", "fusion_sets", ",", "query_labels", "=", "zip", "(", "*", "data", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "fusion_sets", ")", ")", ":", "\n", "        ", "for", "k", "in", "fusion_sets", "[", "i", "]", ":", "\n", "            ", "batch_set", "[", "k", "]", "+=", "fusion_sets", "[", "i", "]", "[", "k", "]", "\n", "", "batch_label", "+=", "query_labels", "[", "i", "]", "\n", "", "for", "k", "in", "batch_set", ":", "\n", "        ", "batch_set", "[", "k", "]", "=", "torch", ".", "stack", "(", "batch_set", "[", "k", "]", ",", "0", ")", "\n", "", "batch_label", "=", "torch", ".", "tensor", "(", "batch_label", ")", "\n", "\n", "return", "batch_set", ",", "batch_label", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.get_concept_loader_pair": [[338, 350], ["data_kg_loader.FewRelDatasetPair", "torch.DataLoader", "iter"], "function", ["None"], ["", "def", "get_concept_loader_pair", "(", "name", ",", "ins2cpt", ",", "entity2id", ",", "title2id", ",", "word2id", ",", "encoder", ",", "nWay", ",", "K", ",", "Q", ",", "batch_size", ",", "\n", "num_workers", "=", "8", ",", "collate_fn", "=", "collate_fn_pair", ",", "na_rate", "=", "0", ",", "root", "=", "'./data'", ",", "\n", "encoder_name", "=", "'bert'", ",", "id_from", "=", "'kgEmbeddingOrBeyondWordEmbedding'", ")", ":", "\n", "    ", "dataset", "=", "FewRelDatasetPair", "(", "name", ",", "encoder", ",", "nWay", ",", "K", ",", "Q", ",", "na_rate", ",", "root", ",", "encoder_name", ",", "ins2cpt", ",", "entity2id", ",", "title2id", ",", "\n", "word2id", ",", "id_from", ")", "\n", "data_loader", "=", "data", ".", "DataLoader", "(", "dataset", "=", "dataset", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "shuffle", "=", "False", ",", "\n", "pin_memory", "=", "True", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", "collate_fn", "=", "collate_fn", ")", "\n", "return", "iter", "(", "data_loader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.load_json": [[587, 593], ["tqdm.tqdm", "pbar.update", "open", "json.load"], "function", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load"], ["", "", "def", "load_json", "(", "path", ")", ":", "\n", "    ", "with", "tqdm", "(", "total", "=", "1", ",", "desc", "=", "f'loading'", "+", "path", ")", "as", "pbar", ":", "\n", "        ", "with", "open", "(", "path", ",", "mode", "=", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "fr", ":", "\n", "            ", "data", "=", "json", ".", "load", "(", "fr", ")", "\n", "", "pbar", ".", "update", "(", "1", ")", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.load_pickle": [[595, 601], ["tqdm.tqdm", "pbar.update", "open", "pickle.load"], "function", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load"], ["", "def", "load_pickle", "(", "path", ")", ":", "\n", "    ", "with", "tqdm", "(", "total", "=", "1", ",", "desc", "=", "f'loading'", "+", "path", ")", "as", "pbar", ":", "\n", "        ", "with", "open", "(", "path", ",", "mode", "=", "'rb'", ")", "as", "fr", ":", "\n", "            ", "data", "=", "pickle", ".", "load", "(", "fr", ")", "\n", "", "pbar", ".", "update", "(", "1", ")", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.collate_fn": [[603, 620], ["zip", "range", "torch.tensor", "torch.tensor", "len", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "function", ["None"], ["", "def", "collate_fn", "(", "data", ")", ":", "\n", "    ", "batch_support", "=", "{", "'word'", ":", "[", "]", ",", "'pos1'", ":", "[", "]", ",", "'pos2'", ":", "[", "]", ",", "'mask'", ":", "[", "]", ",", "'conceptID'", ":", "[", "]", "}", "\n", "batch_query", "=", "{", "'word'", ":", "[", "]", ",", "'pos1'", ":", "[", "]", ",", "'pos2'", ":", "[", "]", ",", "'mask'", ":", "[", "]", ",", "'conceptID'", ":", "[", "]", "}", "\n", "batch_label", "=", "[", "]", "\n", "support_sets", ",", "query_sets", ",", "query_labels", "=", "zip", "(", "*", "data", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "support_sets", ")", ")", ":", "\n", "        ", "for", "k", "in", "support_sets", "[", "i", "]", ":", "\n", "            ", "batch_support", "[", "k", "]", "+=", "support_sets", "[", "i", "]", "[", "k", "]", "\n", "", "for", "k", "in", "query_sets", "[", "i", "]", ":", "\n", "            ", "batch_query", "[", "k", "]", "+=", "query_sets", "[", "i", "]", "[", "k", "]", "\n", "", "batch_label", "+=", "query_labels", "[", "i", "]", "\n", "", "for", "k", "in", "batch_support", ":", "\n", "        ", "batch_support", "[", "k", "]", "=", "torch", ".", "stack", "(", "batch_support", "[", "k", "]", ",", "0", ")", "\n", "", "for", "k", "in", "batch_query", ":", "\n", "        ", "batch_query", "[", "k", "]", "=", "torch", ".", "stack", "(", "batch_query", "[", "k", "]", ",", "0", ")", "\n", "", "batch_label", "=", "torch", ".", "tensor", "(", "batch_label", ")", "\n", "return", "batch_support", ",", "batch_query", ",", "batch_label", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.data_kg_loader.get_concept_loader": [[622, 633], ["data_kg_loader.FewRelDataset", "torch.DataLoader", "iter"], "function", ["None"], ["", "def", "get_concept_loader", "(", "name", ",", "encoder", ",", "nWay", ",", "K", ",", "Q", ",", "batch_size", ",", "ins2cpt", ",", "concept", ",", "id_from", ",", "entity2id", ",", "title2id", ",", "word2id", ",", "\n", "num_workers", "=", "32", ",", "collate_fn", "=", "collate_fn", ",", "na_rate", "=", "0", ",", "root", "=", "'./data'", ")", ":", "\n", "    ", "dataset", "=", "FewRelDataset", "(", "name", ",", "encoder", ",", "nWay", ",", "K", ",", "Q", ",", "na_rate", ",", "root", ",", "ins2cpt", ",", "concept", ",", "id_from", ",", "entity2id", ",", "title2id", ",", "\n", "word2id", ")", "\n", "data_loader", "=", "data", ".", "DataLoader", "(", "dataset", "=", "dataset", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "shuffle", "=", "False", ",", "\n", "pin_memory", "=", "True", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", "collate_fn", "=", "collate_fn", ")", "\n", "return", "iter", "(", "data_loader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.utils.loadJson": [[7, 11], ["open", "json.load"], "function", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.conceptgraph_utils.load"], ["def", "loadJson", "(", "path", ":", "str", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "mode", "=", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "data", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.utils.dictSaveToJson": [[13, 16], ["open", "json.dump"], "function", ["None"], ["", "def", "dictSaveToJson", "(", "d", ":", "dict", ",", "Path", ":", "str", ")", ":", "\n", "    ", "with", "open", "(", "Path", ",", "mode", "=", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "d", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.utils.re_split_dataset": [[18, 51], ["print", "os.path.join", "utils.loadJson", "loadJson.items", "print", "print", "print", "datetime.datetime.now", "os.path.exists", "print", "tqdm.tqdm", "utils.dictSaveToJson", "pbar.update", "utils.dictSaveToJson", "pbar.update", "len", "len", "datetime.datetime.now"], "function", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.utils.loadJson", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.utils.dictSaveToJson", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.fewshot_re_kit.utils.dictSaveToJson"], ["", "", "def", "re_split_dataset", "(", "\n", "root", "=", "'/home/yangshan/pycharm2server/KG/FewRel/data/'", ",", "\n", "name", "=", "'train_wiki'", ",", "format", "=", "\".json\"", ")", ":", "\n", "    ", "'''\n    \u5c06\u539f\u59cb\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u5212\u5206\u6210\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\uff0c\u7136\u540e\u539f\u59cb\u7684\u9a8c\u8bc1\u96c6\u5f53\u4f5c\u6d4b\u8bd5\u96c6\n    '''", "\n", "\n", "print", "(", "'starting:'", ",", "datetime", ".", "now", "(", ")", ")", "\n", "path1", "=", "os", ".", "path", ".", "join", "(", "root", ",", "name", "+", "format", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path1", ")", ":", "\n", "        ", "print", "(", "\"[ERROR] {} file does not exist!\"", ".", "format", "(", "name", ")", ")", "\n", "assert", "(", "0", ")", "\n", "", "traindata", "=", "loadJson", "(", "path1", ")", "\n", "# rel_num = len(traindata)", "\n", "new_train", "=", "{", "}", "\n", "new_val", "=", "{", "}", "\n", "count", "=", "0", "\n", "for", "k", ",", "v", "in", "traindata", ".", "items", "(", ")", ":", "\n", "        ", "if", "count", "<", "50", ":", "\n", "            ", "new_train", "[", "k", "]", "=", "v", "\n", "count", "=", "count", "+", "1", "\n", "", "else", ":", "\n", "            ", "new_val", "[", "k", "]", "=", "v", "\n", "count", "=", "count", "+", "1", "\n", "", "", "with", "tqdm", "(", "total", "=", "2", ",", "desc", "=", "f'store new_train and new_val file'", ")", "as", "pbar", ":", "\n", "        ", "dictSaveToJson", "(", "new_train", ",", "root", "+", "'train.json'", ")", "\n", "pbar", ".", "update", "(", "1", ")", "\n", "dictSaveToJson", "(", "new_val", ",", "root", "+", "'val.json'", ")", "\n", "pbar", ".", "update", "(", "1", ")", "\n", "", "print", "(", "'new_train size'", ",", "len", "(", "new_train", ")", ")", "\n", "print", "(", "'new_val size'", ",", "len", "(", "new_val", ")", ")", "\n", "print", "(", "'done:'", ",", "datetime", ".", "now", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.encoder.Encoder.__init__": [[9, 23], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.MaxPool1d", "torch.MaxPool1d", "torch.MaxPool1d", "torch.Embedding", "torch.Embedding", "torch.Embedding", "encoder.Encoder.mask_embedding.weight.data.copy_", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "max_length", ",", "word_embedding_dim", "=", "50", ",", "pos_embedding_dim", "=", "5", ",", "hidden_size", "=", "230", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "embedding_dim", "=", "word_embedding_dim", "+", "pos_embedding_dim", "*", "2", "\n", "self", ".", "conv", "=", "nn", ".", "Conv1d", "(", "self", ".", "embedding_dim", ",", "self", ".", "hidden_size", ",", "3", ",", "padding", "=", "1", ")", "\n", "self", ".", "pool", "=", "nn", ".", "MaxPool1d", "(", "max_length", ")", "\n", "\n", "# For PCNN", "\n", "self", ".", "mask_embedding", "=", "nn", ".", "Embedding", "(", "4", ",", "3", ")", "\n", "self", ".", "mask_embedding", ".", "weight", ".", "data", ".", "copy_", "(", "torch", ".", "FloatTensor", "(", "[", "[", "1", ",", "0", ",", "0", "]", ",", "[", "0", ",", "1", ",", "0", "]", ",", "[", "0", ",", "0", ",", "1", "]", ",", "[", "0", ",", "0", ",", "0", "]", "]", ")", ")", "\n", "self", ".", "mask_embedding", ".", "weight", ".", "requires_grad", "=", "False", "\n", "self", ".", "_minus", "=", "-", "100", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.encoder.Encoder.forward": [[24, 26], ["encoder.Encoder.cnn"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.encoder.Encoder.cnn"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "return", "self", ".", "cnn", "(", "inputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.encoder.Encoder.cnn": [[27, 32], ["encoder.Encoder.conv", "torch.relu", "torch.relu", "torch.relu", "encoder.Encoder.pool", "encoder.Encoder.squeeze", "inputs.transpose"], "methods", ["None"], ["", "def", "cnn", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv", "(", "inputs", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "x", "=", "F", ".", "relu", "(", "x", ")", "\n", "x", "=", "self", ".", "pool", "(", "x", ")", "\n", "return", "x", ".", "squeeze", "(", "2", ")", "# n x hidden_size", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.encoder.Encoder.pcnn": [[33, 41], ["encoder.Encoder.conv", "encoder.Encoder.pool", "encoder.Encoder.pool", "encoder.Encoder.pool", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "x.squeeze.squeeze.squeeze", "inputs.transpose", "encoder.Encoder.mask_embedding().transpose", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "encoder.Encoder.mask_embedding"], "methods", ["None"], ["", "def", "pcnn", "(", "self", ",", "inputs", ",", "mask", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv", "(", "inputs", ".", "transpose", "(", "1", ",", "2", ")", ")", "# n x hidden x length", "\n", "mask", "=", "1", "-", "self", ".", "mask_embedding", "(", "mask", ")", ".", "transpose", "(", "1", ",", "2", ")", "# n x 3 x length", "\n", "pool1", "=", "self", ".", "pool", "(", "F", ".", "relu", "(", "x", "+", "self", ".", "_minus", "*", "mask", "[", ":", ",", "0", ":", "1", ",", ":", "]", ")", ")", "\n", "pool2", "=", "self", ".", "pool", "(", "F", ".", "relu", "(", "x", "+", "self", ".", "_minus", "*", "mask", "[", ":", ",", "1", ":", "2", ",", ":", "]", ")", ")", "\n", "pool3", "=", "self", ".", "pool", "(", "F", ".", "relu", "(", "x", "+", "self", ".", "_minus", "*", "mask", "[", ":", ",", "2", ":", "3", ",", ":", "]", ")", ")", "\n", "x", "=", "torch", ".", "cat", "(", "[", "pool1", ",", "pool2", ",", "pool3", "]", ",", "1", ")", "\n", "x", "=", "x", ".", "squeeze", "(", "2", ")", "# n x (hidden_size * 3) ", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__": [[9, 26], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.Embedding", "torch.Embedding", "torch.Embedding", "embedding.Embedding.word_embedding.weight.data.copy_", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding"], "methods", ["home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__", "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.__init__"], ["    ", "def", "__init__", "(", "self", ",", "word_vec_mat", ",", "max_length", ",", "word_embedding_dim", "=", "50", ",", "pos_embedding_dim", "=", "5", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "word_embedding_dim", "=", "word_embedding_dim", "\n", "self", ".", "pos_embedding_dim", "=", "pos_embedding_dim", "\n", "\n", "# Word embedding", "\n", "# unk = torch.randn(1, word_embedding_dim) / math.sqrt(word_embedding_dim)", "\n", "# blk = torch.zeros(1, word_embedding_dim)", "\n", "word_vec_mat", "=", "torch", ".", "from_numpy", "(", "word_vec_mat", ")", "\n", "self", ".", "word_embedding", "=", "nn", ".", "Embedding", "(", "word_vec_mat", ".", "shape", "[", "0", "]", ",", "self", ".", "word_embedding_dim", ",", "padding_idx", "=", "word_vec_mat", ".", "shape", "[", "0", "]", "-", "1", ")", "\n", "self", ".", "word_embedding", ".", "weight", ".", "data", ".", "copy_", "(", "word_vec_mat", ")", "\n", "\n", "# Position Embedding", "\n", "self", ".", "pos1_embedding", "=", "nn", ".", "Embedding", "(", "2", "*", "max_length", ",", "pos_embedding_dim", ",", "padding_idx", "=", "0", ")", "\n", "self", ".", "pos2_embedding", "=", "nn", ".", "Embedding", "(", "2", "*", "max_length", ",", "pos_embedding_dim", ",", "padding_idx", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LittleGuoKe_ConceptFERE.network.embedding.Embedding.forward": [[27, 36], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "embedding.Embedding.word_embedding", "embedding.Embedding.pos1_embedding", "embedding.Embedding.pos2_embedding"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "word", "=", "inputs", "[", "'word'", "]", "\n", "pos1", "=", "inputs", "[", "'pos1'", "]", "\n", "pos2", "=", "inputs", "[", "'pos2'", "]", "\n", "\n", "x", "=", "torch", ".", "cat", "(", "[", "self", ".", "word_embedding", "(", "word", ")", ",", "\n", "self", ".", "pos1_embedding", "(", "pos1", ")", ",", "\n", "self", ".", "pos2_embedding", "(", "pos2", ")", "]", ",", "2", ")", "\n", "return", "x", "\n", "\n"]]}