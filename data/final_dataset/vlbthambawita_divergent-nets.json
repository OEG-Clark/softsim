{"home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.inference_from_divergentNets_windows.get_argparser": [[18, 42], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "get_argparser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--num_classes\"", ",", "type", "=", "int", ",", "default", "=", "2", ",", "\n", "help", "=", "\"num classes (default: None)\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--output_stride\"", ",", "type", "=", "int", ",", "default", "=", "16", ",", "choices", "=", "[", "8", ",", "16", "]", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--crop_size\"", ",", "type", "=", "int", ",", "default", "=", "512", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--gpu_id\"", ",", "type", "=", "str", ",", "default", "=", "'0'", ",", "\n", "help", "=", "\"GPU ID\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--random_seed\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"random seed (default: 1)\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--task_type\"", ",", "default", "=", "\"segmentation\"", ",", "help", "=", "\"task type\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--chk_paths\"", ",", "default", "=", "\" \"", ",", "nargs", "=", "\"+\"", ",", "help", "=", "\"Checkpoint paths to use in DivergentNets\"", ",", "required", "=", "True", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--input_dir\"", ",", "required", "=", "True", ",", "help", "=", "\"Input directory of images to predict mask.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "required", "=", "True", ",", "help", "=", "\"Output directory to save predicted mask.\"", ",", "default", "=", "\"./predicted_output\"", ")", "\n", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.inference_from_divergentNets_windows.create_predFolder": [[44, 53], ["os.path.join", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir", "os.path.join", "os.path.join"], "function", ["None"], ["", "def", "create_predFolder", "(", "task_type", ")", ":", "\n", "    ", "directoryName", "=", "'EndoCV2021'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "directoryName", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "directoryName", ")", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "directoryName", ",", "task_type", ")", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "os", ".", "path", ".", "join", "(", "directoryName", ",", "task_type", ")", ")", "\n", "\n", "", "return", "os", ".", "path", ".", "join", "(", "directoryName", ",", "task_type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.inference_from_divergentNets_windows.to_tensor": [[55, 57], ["x.transpose().astype", "x.transpose"], "function", ["None"], ["", "def", "to_tensor", "(", "x", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "x", ".", "transpose", "(", "2", ",", "0", ",", "1", ")", ".", "astype", "(", "'float32'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.inference_from_divergentNets_windows.detect_imgs": [[60, 71], ["os.listdir", "numpy.sort", "names.endswith", "names.endswith", "flist.append", "ext.upper", "os.path.join"], "function", ["None"], ["", "def", "detect_imgs", "(", "infolder", ",", "ext", "=", "'.tif'", ")", ":", "\n", "    ", "import", "os", "\n", "\n", "items", "=", "os", ".", "listdir", "(", "infolder", ")", "\n", "\n", "flist", "=", "[", "]", "\n", "for", "names", "in", "items", ":", "\n", "        ", "if", "names", ".", "endswith", "(", "ext", ")", "or", "names", ".", "endswith", "(", "ext", ".", "upper", "(", ")", ")", ":", "\n", "            ", "flist", ".", "append", "(", "os", ".", "path", ".", "join", "(", "infolder", ",", "names", ")", ")", "\n", "\n", "", "", "return", "np", ".", "sort", "(", "flist", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.inference_from_divergentNets_windows.mymodel": [[74, 100], ["torch.device", "torch.device", "torch.load", "torch.load", "print", "print", "models.append", "torch.cuda.is_available", "torch.cuda.is_available"], "function", ["None"], ["", "def", "mymodel", "(", "opt", ")", ":", "\n", "    ", "'''\n    Returns\n    -------\n    model : TYPE\n        DESCRIPTION.\n    device : TYPE\n        DESCRIPTION.\n    '''", "\n", "\n", "os", ".", "environ", "[", "'CUDA_VISIBLE_DEVICES'", "]", "=", "opt", ".", "gpu_id", "\n", "opt", ".", "device", "=", "torch", ".", "device", "(", "'cuda'", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "\n", "\n", "models", "=", "[", "]", "\n", "\n", "for", "chk_path", "in", "opt", ".", "chk_paths", ":", "\n", "        ", "checkpoint", "=", "torch", ".", "load", "(", "chk_path", ",", "map_location", "=", "opt", ".", "device", ")", "\n", "\n", "print", "(", "\"checkpoint path=\"", ",", "chk_path", ")", "\n", "print", "(", "\"checkpoint_best_epoch=\"", ",", "checkpoint", "[", "\"epoch\"", "]", ")", "\n", "\n", "models", ".", "append", "(", "checkpoint", "[", "\"model\"", "]", ")", "\n", "\n", "\n", "", "return", "models", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3_plusplus.train_model": [[193, 246], ["segmentation_models_pytorch.utils.train.TrainEpoch", "segmentation_models_pytorch.utils.train.ValidEpoch", "os.path.join", "range", "print", "smp.utils.train.TrainEpoch.run", "smp.utils.train.ValidEpoch.run", "train_epoch.run.items", "valid_epoch.run.items", "torch.save", "torch.save", "torch.save", "print", "print", "deeplabv3_plusplus.do_test", "print", "print", "writer.add_scalar", "writer.add_scalar"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.do_test"], ["def", "train_model", "(", "train_loader", ",", "valid_loader", ",", "model", ",", "loss", ",", "metrics", ",", "optimizer", ",", "opt", ")", ":", "\n", "\n", "# create epoch runners ", "\n", "# it is a simple loop of iterating over dataloader`s samples", "\n", "    ", "train_epoch", "=", "smp", ".", "utils", ".", "train", ".", "TrainEpoch", "(", "\n", "model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "optimizer", "=", "optimizer", ",", "\n", "device", "=", "DEVICE", ",", "\n", "verbose", "=", "True", ",", "\n", ")", "\n", "\n", "valid_epoch", "=", "smp", ".", "utils", ".", "train", ".", "ValidEpoch", "(", "\n", "model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "device", "=", "DEVICE", ",", "\n", "verbose", "=", "True", ",", "\n", ")", "\n", "\n", "\n", "\n", "max_score", "=", "0", "\n", "\n", "best_chk_path", "=", "os", ".", "path", ".", "join", "(", "CHECKPOINT_DIR", ",", "opt", ".", "best_checkpoint_name", ")", "\n", "\n", "for", "i", "in", "range", "(", "opt", ".", "start_epoch", "+", "1", ",", "opt", ".", "start_epoch", "+", "opt", ".", "num_epochs", "+", "1", ")", ":", "\n", "\n", "        ", "print", "(", "'\\nEpoch: {}'", ".", "format", "(", "i", ")", ")", "\n", "train_logs", "=", "train_epoch", ".", "run", "(", "train_loader", ")", "\n", "valid_logs", "=", "valid_epoch", ".", "run", "(", "valid_loader", ")", "\n", "\n", "# do something (save model, change lr, etc.)", "\n", "if", "max_score", "<", "valid_logs", "[", "'iou_score'", "]", ":", "\n", "            ", "max_score", "=", "valid_logs", "[", "'iou_score'", "]", "\n", "torch", ".", "save", "(", "{", "\"model\"", ":", "model", ",", "\"epoch\"", ":", "i", "}", ",", "best_chk_path", ")", "\n", "print", "(", "'Best Model saved!'", ")", "\n", "print", "(", "\"Testing....\"", ")", "\n", "do_test", "(", "opt", ")", "\n", "print", "(", "\"Tested\"", ")", "\n", "\n", "\n", "", "if", "i", "==", "opt", ".", "lr_change_point", ":", "\n", "            ", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "1e-5", "\n", "print", "(", "'Decrease decoder learning rate to 1e-5!'", ")", "\n", "\n", "# writing to logs to tensorboard", "\n", "", "for", "key", ",", "value", "in", "train_logs", ".", "items", "(", ")", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "f\"Train/{key}\"", ",", "value", ",", "i", ")", "\n", "\n", "", "for", "key", ",", "value", "in", "valid_logs", ".", "items", "(", ")", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "f\"Valid/{key}\"", ",", "value", ",", "i", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3_plusplus.generate_heatmapts": [[255, 271], ["print", "range", "img.squeeze.squeeze", "img.squeeze.detach().cpu().numpy", "matplotlib.imshow", "matplotlib.gcf", "fig_list.append", "matplotlib.close", "img.squeeze.detach().cpu", "img.squeeze.detach"], "function", ["None"], ["", "", "", "def", "generate_heatmapts", "(", "img_tensor", ")", ":", "\n", "    ", "print", "(", "img_tensor", ".", "shape", ")", "\n", "fig_list", "=", "[", "]", "\n", "for", "n", "in", "range", "(", "img_tensor", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "img", "=", "img_tensor", "[", "n", "]", "\n", "img", "=", "img", ".", "squeeze", "(", "dim", "=", "0", ")", "\n", "img_np", "=", "img", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "#img_np = np.transforms(img_np, (1,2,0))", "\n", "\n", "plt", ".", "imshow", "(", "img_np", ",", "cmap", "=", "\"hot\"", ")", "\n", "fig", "=", "plt", ".", "gcf", "(", ")", "\n", "fig_list", ".", "append", "(", "fig", ")", "\n", "# plt.clf()", "\n", "plt", ".", "close", "(", ")", "\n", "\n", "", "return", "fig_list", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3_plusplus.prepare_model": [[277, 290], ["segmentation_models_pytorch.DeepLabV3Plus", "len"], "function", ["None"], ["", "def", "prepare_model", "(", "opt", ")", ":", "\n", "# model = UNet(n_channels=4, n_classes=1) # 4 = 3 channels + 1 grid encode", "\n", "\n", "# create segmentation model with pretrained encoder", "\n", "    ", "model", "=", "smp", ".", "DeepLabV3Plus", "(", "\n", "encoder_name", "=", "opt", ".", "encoder", ",", "\n", "in_channels", "=", "opt", ".", "in_channels", ",", "\n", "encoder_weights", "=", "opt", ".", "encoder_weights", ",", "\n", "classes", "=", "len", "(", "opt", ".", "classes", ")", ",", "\n", "activation", "=", "opt", ".", "activation", ",", "\n", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3_plusplus.run_train": [[294, 312], ["deeplabv3_plusplus.prepare_model", "segmentation_models_pytorch.encoders.get_preprocessing_fn", "data.prepare_data.prepare_data", "segmentation_models_pytorch.utils.losses.DiceLoss", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "deeplabv3_plusplus.train_model", "segmentation_models_pytorch.utils.metrics.IoU", "dict", "prepare_model.parameters"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.prepare_model", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_data", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.train_model"], ["", "def", "run_train", "(", "opt", ")", ":", "\n", "    ", "model", "=", "prepare_model", "(", "opt", ")", "\n", "\n", "preprocessing_fn", "=", "smp", ".", "encoders", ".", "get_preprocessing_fn", "(", "opt", ".", "encoder", ",", "opt", ".", "encoder_weights", ")", "\n", "\n", "train_loader", ",", "val_loader", "=", "prepare_data", "(", "opt", ",", "preprocessing_fn", "=", "None", ")", "\n", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", "ignore_channels", "=", "[", "0", "]", ")", "\n", "\n", "metrics", "=", "[", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ",", "ignore_channels", "=", "[", "0", "]", ")", ",", "\n", "]", "\n", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "[", "\n", "dict", "(", "params", "=", "model", ".", "parameters", "(", ")", ",", "lr", "=", "opt", ".", "lr", ")", ",", "\n", "]", ")", "\n", "\n", "train_model", "(", "train_loader", ",", "val_loader", ",", "model", ",", "loss", ",", "metrics", ",", "optimizer", ",", "opt", ")", "\n", "#====================================", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3_plusplus.run_retrain": [[315, 340], ["torch.load", "torch.load", "torch.load", "print", "print", "segmentation_models_pytorch.encoders.get_preprocessing_fn", "data.prepare_data.prepare_data", "segmentation_models_pytorch.utils.losses.DiceLoss", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "deeplabv3_plusplus.train_model", "os.path.join", "segmentation_models_pytorch.utils.metrics.IoU", "dict", "model.parameters"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_data", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.train_model"], ["", "def", "run_retrain", "(", "opt", ")", ":", "\n", "\n", "    ", "checkpoint_dict", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "CHECKPOINT_DIR", ",", "opt", ".", "best_checkpoint_name", ")", ")", "\n", "\n", "opt", ".", "start_epoch", "=", "checkpoint_dict", "[", "\"epoch\"", "]", "\n", "model", "=", "checkpoint_dict", "[", "\"model\"", "]", "\n", "\n", "print", "(", "\"Model epoch:\"", ",", "checkpoint_dict", "[", "\"epoch\"", "]", ")", "\n", "print", "(", "\"Model retrain started from epoch:\"", ",", "opt", ".", "start_epoch", ")", "\n", "\n", "preprocessing_fn", "=", "smp", ".", "encoders", ".", "get_preprocessing_fn", "(", "opt", ".", "encoder", ",", "opt", ".", "encoder_weights", ")", "\n", "\n", "train_loader", ",", "val_loader", "=", "prepare_data", "(", "opt", ",", "preprocessing_fn", ")", "\n", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", ")", "\n", "\n", "metrics", "=", "[", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ")", ",", "\n", "]", "\n", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "[", "\n", "dict", "(", "params", "=", "model", ".", "parameters", "(", ")", ",", "lr", "=", "opt", ".", "lr", ")", ",", "\n", "]", ")", "\n", "\n", "train_model", "(", "train_loader", ",", "val_loader", ",", "model", ",", "loss", ",", "metrics", ",", "optimizer", ",", "opt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3_plusplus.check_model_graph": [[344, 346], ["None"], "function", ["None"], ["", "def", "check_model_graph", "(", ")", ":", "\n", "    ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3_plusplus.do_test": [[352, 390], ["torch.load", "torch.load", "torch.load", "print", "segmentation_models_pytorch.encoders.get_preprocessing_fn", "data.prepare_data.prepare_test_data", "data.prepare_data.prepare_test_data", "range", "os.path.join", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "best_model.predict", "pr_mask.squeeze().cpu().numpy().round.squeeze().cpu().numpy().round", "utils.visualize", "utils.visualize.savefig", "writer.add_figure", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "pr_mask.squeeze().cpu().numpy().round.squeeze().cpu().numpy", "numpy.transpose().astype", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "pr_mask.squeeze().cpu().numpy().round.squeeze().cpu", "numpy.transpose", "pr_mask.squeeze().cpu().numpy().round.squeeze"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_test_data", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_test_data", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.utils.functions.visualize"], ["", "def", "do_test", "(", "opt", ")", ":", "\n", "\n", "\n", "    ", "checkpoint_dict", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "CHECKPOINT_DIR", ",", "opt", ".", "best_checkpoint_name", ")", ")", "\n", "\n", "test_epoch", "=", "checkpoint_dict", "[", "\"epoch\"", "]", "\n", "best_model", "=", "checkpoint_dict", "[", "\"model\"", "]", "\n", "\n", "print", "(", "\"Model best epoch:\"", ",", "test_epoch", ")", "\n", "\n", "preprocessing_fn", "=", "smp", ".", "encoders", ".", "get_preprocessing_fn", "(", "opt", ".", "encoder", ",", "opt", ".", "encoder_weights", ")", "\n", "test_dataset", "=", "prepare_test_data", "(", "opt", ",", "preprocessing_fn", "=", "None", ")", "\n", "test_dataset_vis", "=", "prepare_test_data", "(", "opt", ",", "preprocessing_fn", "=", "None", ")", "\n", "\n", "\n", "for", "i", "in", "range", "(", "opt", ".", "num_test_samples", ")", ":", "\n", "        ", "image", ",", "mask", "=", "test_dataset", "[", "i", "]", "\n", "image_vis", ",", "_", "=", "test_dataset_vis", "[", "i", "]", "\n", "\n", "#print(image)", "\n", "\n", "mask_tensor", "=", "torch", ".", "from_numpy", "(", "mask", ")", ".", "to", "(", "opt", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "image_tensor", "=", "torch", ".", "from_numpy", "(", "image", ")", ".", "to", "(", "opt", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "pr_mask", "=", "best_model", ".", "predict", "(", "image_tensor", ")", "\n", "\n", "pr_mask", "=", "pr_mask", ".", "squeeze", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "round", "(", ")", "\n", "\n", "fig", "=", "visualize", "(", "\n", "input_image_new", "=", "np", ".", "transpose", "(", "image_vis", ",", "(", "1", ",", "2", ",", "0", ")", ")", ".", "astype", "(", "int", ")", ",", "\n", "GT_mask_0", "=", "mask", "[", "0", ",", ":", ",", ":", "]", ",", "\n", "Pred_mask_0", "=", "pr_mask", "[", "0", ",", ":", ",", ":", "]", ",", "\n", "GT_mask_1", "=", "mask", "[", "1", ",", ":", ",", ":", "]", ",", "\n", "Pred_mask_1", "=", "pr_mask", "[", "1", ",", ":", ",", ":", "]", "\n", ")", "\n", "\n", "fig", ".", "savefig", "(", "f\"./test_202_{i}.png\"", ")", "\n", "writer", ".", "add_figure", "(", "f\"Test_sample/sample-{i}\"", ",", "fig", ",", "global_step", "=", "test_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3_plusplus.check_test_score": [[395, 469], ["torch.load", "torch.load", "torch.load", "print", "segmentation_models_pytorch.encoders.get_preprocessing_fn", "data.prepare_data.prepare_test_data", "torch.utils.data.DataLoader", "segmentation_models_pytorch.utils.losses.DiceLoss", "segmentation_models_pytorch.utils.train.ValidEpoch", "smp.utils.train.ValidEpoch.run", "print", "writer.add_text", "segmentation_models_pytorch.utils.losses.DiceLoss", "segmentation_models_pytorch.utils.train.ValidEpoch", "smp.utils.train.ValidEpoch.run", "print", "writer.add_text", "segmentation_models_pytorch.utils.losses.DiceLoss", "segmentation_models_pytorch.utils.train.ValidEpoch", "smp.utils.train.ValidEpoch.run", "print", "writer.add_text", "os.path.join", "segmentation_models_pytorch.utils.metrics.IoU", "str", "str", "segmentation_models_pytorch.utils.metrics.IoU", "str", "str", "segmentation_models_pytorch.utils.metrics.IoU", "str", "str"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_test_data"], ["", "", "def", "check_test_score", "(", "opt", ")", ":", "\n", "\n", "\n", "\n", "    ", "checkpoint_dict", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "CHECKPOINT_DIR", ",", "opt", ".", "best_checkpoint_name", ")", ")", "\n", "\n", "test_best_epoch", "=", "checkpoint_dict", "[", "\"epoch\"", "]", "\n", "best_model", "=", "checkpoint_dict", "[", "\"model\"", "]", "\n", "\n", "print", "(", "\"Model best epoch:\"", ",", "test_best_epoch", ")", "\n", "\n", "\n", "\n", "preprocessing_fn", "=", "smp", ".", "encoders", ".", "get_preprocessing_fn", "(", "opt", ".", "encoder", ",", "opt", ".", "encoder_weights", ")", "\n", "test_dataset", "=", "prepare_test_data", "(", "opt", ",", "preprocessing_fn", "=", "None", ")", "\n", "\n", "test_dataloader", "=", "DataLoader", "(", "test_dataset", ",", "num_workers", "=", "48", ")", "\n", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", ")", "\n", "# Testing with two class layers", "\n", "metrics", "=", "[", "\n", "#smp.utils.metrics.IoU(threshold=0.5),", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ",", "ignore_channels", "=", "None", ")", ",", "\n", "]", "\n", "\n", "test_epoch", "=", "smp", ".", "utils", ".", "train", ".", "ValidEpoch", "(", "\n", "model", "=", "best_model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "device", "=", "DEVICE", ",", "\n", ")", "\n", "\n", "logs", "=", "test_epoch", ".", "run", "(", "test_dataloader", ")", "\n", "print", "(", "\"logs=\"", ",", "str", "(", "logs", ")", ")", "\n", "writer", ".", "add_text", "(", "f\"{opt.py_file}-test-score\"", ",", "str", "(", "logs", ")", ",", "global_step", "=", "test_best_epoch", ")", "\n", "\n", "# Testing with only class layer 1 (polyps)", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", "ignore_channels", "=", "[", "0", "]", ")", "\n", "metrics", "=", "[", "\n", "#smp.utils.metrics.IoU(threshold=0.5),", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ",", "ignore_channels", "=", "[", "0", "]", ")", ",", "\n", "]", "\n", "\n", "test_epoch", "=", "smp", ".", "utils", ".", "train", ".", "ValidEpoch", "(", "\n", "model", "=", "best_model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "device", "=", "DEVICE", ",", "\n", ")", "\n", "\n", "logs", "=", "test_epoch", ".", "run", "(", "test_dataloader", ")", "\n", "print", "(", "\"logs=\"", ",", "str", "(", "logs", ")", ")", "\n", "writer", ".", "add_text", "(", "f\"{opt.py_file}-test-score-ignore-channel-0\"", ",", "str", "(", "logs", ")", ",", "global_step", "=", "test_best_epoch", ")", "\n", "\n", "\n", "\n", "# Testing with only class layer 0 (BG)", "\n", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", "ignore_channels", "=", "[", "1", "]", ")", "\n", "metrics", "=", "[", "\n", "#smp.utils.metrics.IoU(threshold=0.5),", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ",", "ignore_channels", "=", "[", "1", "]", ")", ",", "\n", "]", "\n", "\n", "test_epoch", "=", "smp", ".", "utils", ".", "train", ".", "ValidEpoch", "(", "\n", "model", "=", "best_model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "device", "=", "DEVICE", ",", "\n", ")", "\n", "\n", "logs", "=", "test_epoch", ".", "run", "(", "test_dataloader", ")", "\n", "print", "(", "\"logs=\"", ",", "str", "(", "logs", ")", ")", "\n", "writer", ".", "add_text", "(", "f\"{opt.py_file}-test-score-ignore-channel-1\"", ",", "str", "(", "logs", ")", ",", "global_step", "=", "test_best_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.inference_from_divergentNets.get_argparser": [[18, 42], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "get_argparser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--num_classes\"", ",", "type", "=", "int", ",", "default", "=", "2", ",", "\n", "help", "=", "\"num classes (default: None)\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--output_stride\"", ",", "type", "=", "int", ",", "default", "=", "16", ",", "choices", "=", "[", "8", ",", "16", "]", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--crop_size\"", ",", "type", "=", "int", ",", "default", "=", "512", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--gpu_id\"", ",", "type", "=", "str", ",", "default", "=", "'0'", ",", "\n", "help", "=", "\"GPU ID\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--random_seed\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"random seed (default: 1)\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--task_type\"", ",", "default", "=", "\"segmentation\"", ",", "help", "=", "\"task type\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--chk_paths\"", ",", "default", "=", "\" \"", ",", "nargs", "=", "\"+\"", ",", "help", "=", "\"Checkpoint paths to use in DivergentNets\"", ",", "required", "=", "True", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--input_dir\"", ",", "required", "=", "True", ",", "help", "=", "\"Input directory of images to predict mask.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "required", "=", "True", ",", "help", "=", "\"Output directory to save predicted mask.\"", ",", "default", "=", "\"./predicted_output\"", ")", "\n", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.inference_from_divergentNets.create_predFolder": [[44, 53], ["os.path.join", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir", "os.path.join", "os.path.join"], "function", ["None"], ["", "def", "create_predFolder", "(", "task_type", ")", ":", "\n", "    ", "directoryName", "=", "'EndoCV2021'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "directoryName", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "directoryName", ")", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "directoryName", ",", "task_type", ")", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "os", ".", "path", ".", "join", "(", "directoryName", ",", "task_type", ")", ")", "\n", "\n", "", "return", "os", ".", "path", ".", "join", "(", "directoryName", ",", "task_type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.inference_from_divergentNets.to_tensor": [[55, 57], ["x.transpose().astype", "x.transpose"], "function", ["None"], ["", "def", "to_tensor", "(", "x", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "x", ".", "transpose", "(", "2", ",", "0", ",", "1", ")", ".", "astype", "(", "'float32'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.inference_from_divergentNets.detect_imgs": [[60, 71], ["os.listdir", "numpy.sort", "names.endswith", "names.endswith", "flist.append", "ext.upper", "os.path.join"], "function", ["None"], ["", "def", "detect_imgs", "(", "infolder", ",", "ext", "=", "'.tif'", ")", ":", "\n", "    ", "import", "os", "\n", "\n", "items", "=", "os", ".", "listdir", "(", "infolder", ")", "\n", "\n", "flist", "=", "[", "]", "\n", "for", "names", "in", "items", ":", "\n", "        ", "if", "names", ".", "endswith", "(", "ext", ")", "or", "names", ".", "endswith", "(", "ext", ".", "upper", "(", ")", ")", ":", "\n", "            ", "flist", ".", "append", "(", "os", ".", "path", ".", "join", "(", "infolder", ",", "names", ")", ")", "\n", "\n", "", "", "return", "np", ".", "sort", "(", "flist", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.inference_from_divergentNets.mymodel": [[74, 100], ["torch.device", "torch.device", "torch.load", "torch.load", "print", "print", "models.append", "torch.cuda.is_available", "torch.cuda.is_available"], "function", ["None"], ["", "def", "mymodel", "(", "opt", ")", ":", "\n", "    ", "'''\n    Returns\n    -------\n    model : TYPE\n        DESCRIPTION.\n    device : TYPE\n        DESCRIPTION.\n    '''", "\n", "\n", "os", ".", "environ", "[", "'CUDA_VISIBLE_DEVICES'", "]", "=", "opt", ".", "gpu_id", "\n", "opt", ".", "device", "=", "torch", ".", "device", "(", "'cuda'", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "\n", "\n", "models", "=", "[", "]", "\n", "\n", "for", "chk_path", "in", "opt", ".", "chk_paths", ":", "\n", "        ", "checkpoint", "=", "torch", ".", "load", "(", "chk_path", ",", "map_location", "=", "opt", ".", "device", ")", "\n", "\n", "print", "(", "\"checkpoint path=\"", ",", "chk_path", ")", "\n", "print", "(", "\"checkpoint_best_epoch=\"", ",", "checkpoint", "[", "\"epoch\"", "]", ")", "\n", "\n", "models", ".", "append", "(", "checkpoint", "[", "\"model\"", "]", ")", "\n", "\n", "\n", "", "return", "models", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.fpn.train_model": [[198, 251], ["segmentation_models_pytorch.utils.train.TrainEpoch", "segmentation_models_pytorch.utils.train.ValidEpoch", "os.path.join", "range", "print", "smp.utils.train.TrainEpoch.run", "smp.utils.train.ValidEpoch.run", "train_epoch.run.items", "valid_epoch.run.items", "torch.save", "torch.save", "torch.save", "print", "print", "fpn.do_test", "print", "print", "writer.add_scalar", "writer.add_scalar"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.do_test"], ["def", "train_model", "(", "train_loader", ",", "valid_loader", ",", "model", ",", "loss", ",", "metrics", ",", "optimizer", ",", "opt", ")", ":", "\n", "\n", "# create epoch runners ", "\n", "# it is a simple loop of iterating over dataloader`s samples", "\n", "    ", "train_epoch", "=", "smp", ".", "utils", ".", "train", ".", "TrainEpoch", "(", "\n", "model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "optimizer", "=", "optimizer", ",", "\n", "device", "=", "DEVICE", ",", "\n", "verbose", "=", "True", ",", "\n", ")", "\n", "\n", "valid_epoch", "=", "smp", ".", "utils", ".", "train", ".", "ValidEpoch", "(", "\n", "model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "device", "=", "DEVICE", ",", "\n", "verbose", "=", "True", ",", "\n", ")", "\n", "\n", "\n", "\n", "max_score", "=", "0", "\n", "\n", "best_chk_path", "=", "os", ".", "path", ".", "join", "(", "CHECKPOINT_DIR", ",", "opt", ".", "best_checkpoint_name", ")", "\n", "\n", "for", "i", "in", "range", "(", "opt", ".", "start_epoch", "+", "1", ",", "opt", ".", "start_epoch", "+", "opt", ".", "num_epochs", "+", "1", ")", ":", "\n", "\n", "        ", "print", "(", "'\\nEpoch: {}'", ".", "format", "(", "i", ")", ")", "\n", "train_logs", "=", "train_epoch", ".", "run", "(", "train_loader", ")", "\n", "valid_logs", "=", "valid_epoch", ".", "run", "(", "valid_loader", ")", "\n", "\n", "# do something (save model, change lr, etc.)", "\n", "if", "max_score", "<", "valid_logs", "[", "'iou_score'", "]", ":", "\n", "            ", "max_score", "=", "valid_logs", "[", "'iou_score'", "]", "\n", "torch", ".", "save", "(", "{", "\"model\"", ":", "model", ",", "\"epoch\"", ":", "i", "}", ",", "best_chk_path", ")", "\n", "print", "(", "'Best Model saved!'", ")", "\n", "print", "(", "\"Testing....\"", ")", "\n", "do_test", "(", "opt", ")", "\n", "print", "(", "\"Tested\"", ")", "\n", "\n", "\n", "", "if", "i", "==", "opt", ".", "lr_change_point", ":", "\n", "            ", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "1e-5", "\n", "print", "(", "'Decrease decoder learning rate to 1e-5!'", ")", "\n", "\n", "# writing to logs to tensorboard", "\n", "", "for", "key", ",", "value", "in", "train_logs", ".", "items", "(", ")", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "f\"Train/{key}\"", ",", "value", ",", "i", ")", "\n", "\n", "", "for", "key", ",", "value", "in", "valid_logs", ".", "items", "(", ")", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "f\"Valid/{key}\"", ",", "value", ",", "i", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.fpn.generate_heatmapts": [[260, 276], ["print", "range", "img.squeeze.squeeze", "img.squeeze.detach().cpu().numpy", "matplotlib.imshow", "matplotlib.gcf", "fig_list.append", "matplotlib.close", "img.squeeze.detach().cpu", "img.squeeze.detach"], "function", ["None"], ["", "", "", "def", "generate_heatmapts", "(", "img_tensor", ")", ":", "\n", "    ", "print", "(", "img_tensor", ".", "shape", ")", "\n", "fig_list", "=", "[", "]", "\n", "for", "n", "in", "range", "(", "img_tensor", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "img", "=", "img_tensor", "[", "n", "]", "\n", "img", "=", "img", ".", "squeeze", "(", "dim", "=", "0", ")", "\n", "img_np", "=", "img", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "#img_np = np.transforms(img_np, (1,2,0))", "\n", "\n", "plt", ".", "imshow", "(", "img_np", ",", "cmap", "=", "\"hot\"", ")", "\n", "fig", "=", "plt", ".", "gcf", "(", ")", "\n", "fig_list", ".", "append", "(", "fig", ")", "\n", "# plt.clf()", "\n", "plt", ".", "close", "(", ")", "\n", "\n", "", "return", "fig_list", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.fpn.prepare_model": [[282, 295], ["segmentation_models_pytorch.FPN", "len"], "function", ["None"], ["", "def", "prepare_model", "(", "opt", ")", ":", "\n", "# model = UNet(n_channels=4, n_classes=1) # 4 = 3 channels + 1 grid encode", "\n", "\n", "# create segmentation model with pretrained encoder", "\n", "    ", "model", "=", "smp", ".", "FPN", "(", "\n", "encoder_name", "=", "opt", ".", "encoder", ",", "\n", "in_channels", "=", "opt", ".", "in_channels", ",", "\n", "encoder_weights", "=", "opt", ".", "encoder_weights", ",", "\n", "classes", "=", "len", "(", "opt", ".", "classes", ")", ",", "\n", "activation", "=", "opt", ".", "activation", ",", "\n", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.fpn.run_train": [[299, 317], ["fpn.prepare_model", "segmentation_models_pytorch.encoders.get_preprocessing_fn", "data.prepare_data.prepare_data", "segmentation_models_pytorch.utils.losses.DiceLoss", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "fpn.train_model", "segmentation_models_pytorch.utils.metrics.IoU", "dict", "prepare_model.parameters"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.prepare_model", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_data", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.train_model"], ["", "def", "run_train", "(", "opt", ")", ":", "\n", "    ", "model", "=", "prepare_model", "(", "opt", ")", "\n", "\n", "preprocessing_fn", "=", "smp", ".", "encoders", ".", "get_preprocessing_fn", "(", "opt", ".", "encoder", ",", "opt", ".", "encoder_weights", ")", "\n", "\n", "train_loader", ",", "val_loader", "=", "prepare_data", "(", "opt", ",", "preprocessing_fn", "=", "None", ")", "\n", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", "ignore_channels", "=", "[", "0", "]", ")", "\n", "\n", "metrics", "=", "[", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ",", "ignore_channels", "=", "[", "0", "]", ")", ",", "\n", "]", "\n", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "[", "\n", "dict", "(", "params", "=", "model", ".", "parameters", "(", ")", ",", "lr", "=", "opt", ".", "lr", ")", ",", "\n", "]", ")", "\n", "\n", "train_model", "(", "train_loader", ",", "val_loader", ",", "model", ",", "loss", ",", "metrics", ",", "optimizer", ",", "opt", ")", "\n", "#====================================", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.fpn.run_retrain": [[320, 345], ["torch.load", "torch.load", "torch.load", "print", "print", "segmentation_models_pytorch.encoders.get_preprocessing_fn", "data.prepare_data.prepare_data", "segmentation_models_pytorch.utils.losses.DiceLoss", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "fpn.train_model", "os.path.join", "segmentation_models_pytorch.utils.metrics.IoU", "dict", "model.parameters"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_data", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.train_model"], ["", "def", "run_retrain", "(", "opt", ")", ":", "\n", "\n", "    ", "checkpoint_dict", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "CHECKPOINT_DIR", ",", "opt", ".", "best_checkpoint_name", ")", ")", "\n", "\n", "opt", ".", "start_epoch", "=", "checkpoint_dict", "[", "\"epoch\"", "]", "\n", "model", "=", "checkpoint_dict", "[", "\"model\"", "]", "\n", "\n", "print", "(", "\"Model epoch:\"", ",", "checkpoint_dict", "[", "\"epoch\"", "]", ")", "\n", "print", "(", "\"Model retrain started from epoch:\"", ",", "opt", ".", "start_epoch", ")", "\n", "\n", "preprocessing_fn", "=", "smp", ".", "encoders", ".", "get_preprocessing_fn", "(", "opt", ".", "encoder", ",", "opt", ".", "encoder_weights", ")", "\n", "\n", "train_loader", ",", "val_loader", "=", "prepare_data", "(", "opt", ",", "preprocessing_fn", ")", "\n", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", ")", "\n", "\n", "metrics", "=", "[", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ")", ",", "\n", "]", "\n", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "[", "\n", "dict", "(", "params", "=", "model", ".", "parameters", "(", ")", ",", "lr", "=", "opt", ".", "lr", ")", ",", "\n", "]", ")", "\n", "\n", "train_model", "(", "train_loader", ",", "val_loader", ",", "model", ",", "loss", ",", "metrics", ",", "optimizer", ",", "opt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.fpn.check_model_graph": [[349, 351], ["None"], "function", ["None"], ["", "def", "check_model_graph", "(", ")", ":", "\n", "    ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.fpn.do_test": [[357, 395], ["torch.load", "torch.load", "torch.load", "print", "segmentation_models_pytorch.encoders.get_preprocessing_fn", "data.prepare_data.prepare_test_data", "data.prepare_data.prepare_test_data", "range", "os.path.join", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "best_model.predict", "pr_mask.squeeze().cpu().numpy().round.squeeze().cpu().numpy().round", "utils.visualize", "utils.visualize.savefig", "writer.add_figure", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "pr_mask.squeeze().cpu().numpy().round.squeeze().cpu().numpy", "numpy.transpose().astype", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "pr_mask.squeeze().cpu().numpy().round.squeeze().cpu", "numpy.transpose", "pr_mask.squeeze().cpu().numpy().round.squeeze"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_test_data", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_test_data", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.utils.functions.visualize"], ["", "def", "do_test", "(", "opt", ")", ":", "\n", "\n", "\n", "    ", "checkpoint_dict", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "CHECKPOINT_DIR", ",", "opt", ".", "best_checkpoint_name", ")", ")", "\n", "\n", "test_epoch", "=", "checkpoint_dict", "[", "\"epoch\"", "]", "\n", "best_model", "=", "checkpoint_dict", "[", "\"model\"", "]", "\n", "\n", "print", "(", "\"Model best epoch:\"", ",", "test_epoch", ")", "\n", "\n", "preprocessing_fn", "=", "smp", ".", "encoders", ".", "get_preprocessing_fn", "(", "opt", ".", "encoder", ",", "opt", ".", "encoder_weights", ")", "\n", "test_dataset", "=", "prepare_test_data", "(", "opt", ",", "preprocessing_fn", "=", "None", ")", "\n", "test_dataset_vis", "=", "prepare_test_data", "(", "opt", ",", "preprocessing_fn", "=", "None", ")", "\n", "\n", "\n", "for", "i", "in", "range", "(", "opt", ".", "num_test_samples", ")", ":", "\n", "        ", "image", ",", "mask", "=", "test_dataset", "[", "i", "]", "\n", "image_vis", ",", "_", "=", "test_dataset_vis", "[", "i", "]", "\n", "\n", "#print(image)", "\n", "\n", "mask_tensor", "=", "torch", ".", "from_numpy", "(", "mask", ")", ".", "to", "(", "opt", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "image_tensor", "=", "torch", ".", "from_numpy", "(", "image", ")", ".", "to", "(", "opt", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "pr_mask", "=", "best_model", ".", "predict", "(", "image_tensor", ")", "\n", "\n", "pr_mask", "=", "pr_mask", ".", "squeeze", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "round", "(", ")", "\n", "\n", "fig", "=", "visualize", "(", "\n", "input_image_new", "=", "np", ".", "transpose", "(", "image_vis", ",", "(", "1", ",", "2", ",", "0", ")", ")", ".", "astype", "(", "int", ")", ",", "\n", "GT_mask_0", "=", "mask", "[", "0", ",", ":", ",", ":", "]", ",", "\n", "Pred_mask_0", "=", "pr_mask", "[", "0", ",", ":", ",", ":", "]", ",", "\n", "GT_mask_1", "=", "mask", "[", "1", ",", ":", ",", ":", "]", ",", "\n", "Pred_mask_1", "=", "pr_mask", "[", "1", ",", ":", ",", ":", "]", "\n", ")", "\n", "\n", "fig", ".", "savefig", "(", "f\"./test_202_{i}.png\"", ")", "\n", "writer", ".", "add_figure", "(", "f\"Test_sample/sample-{i}\"", ",", "fig", ",", "global_step", "=", "test_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.fpn.check_test_score": [[400, 474], ["torch.load", "torch.load", "torch.load", "print", "segmentation_models_pytorch.encoders.get_preprocessing_fn", "data.prepare_data.prepare_test_data", "torch.utils.data.DataLoader", "segmentation_models_pytorch.utils.losses.DiceLoss", "segmentation_models_pytorch.utils.train.ValidEpoch", "smp.utils.train.ValidEpoch.run", "print", "writer.add_text", "segmentation_models_pytorch.utils.losses.DiceLoss", "segmentation_models_pytorch.utils.train.ValidEpoch", "smp.utils.train.ValidEpoch.run", "print", "writer.add_text", "segmentation_models_pytorch.utils.losses.DiceLoss", "segmentation_models_pytorch.utils.train.ValidEpoch", "smp.utils.train.ValidEpoch.run", "print", "writer.add_text", "os.path.join", "segmentation_models_pytorch.utils.metrics.IoU", "str", "str", "segmentation_models_pytorch.utils.metrics.IoU", "str", "str", "segmentation_models_pytorch.utils.metrics.IoU", "str", "str"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_test_data"], ["", "", "def", "check_test_score", "(", "opt", ")", ":", "\n", "\n", "\n", "\n", "    ", "checkpoint_dict", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "CHECKPOINT_DIR", ",", "opt", ".", "best_checkpoint_name", ")", ")", "\n", "\n", "test_best_epoch", "=", "checkpoint_dict", "[", "\"epoch\"", "]", "\n", "best_model", "=", "checkpoint_dict", "[", "\"model\"", "]", "\n", "\n", "print", "(", "\"Model best epoch:\"", ",", "test_best_epoch", ")", "\n", "\n", "\n", "\n", "preprocessing_fn", "=", "smp", ".", "encoders", ".", "get_preprocessing_fn", "(", "opt", ".", "encoder", ",", "opt", ".", "encoder_weights", ")", "\n", "test_dataset", "=", "prepare_test_data", "(", "opt", ",", "preprocessing_fn", "=", "None", ")", "\n", "\n", "test_dataloader", "=", "DataLoader", "(", "test_dataset", ",", "num_workers", "=", "48", ")", "\n", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", ")", "\n", "# Testing with two class layers", "\n", "metrics", "=", "[", "\n", "#smp.utils.metrics.IoU(threshold=0.5),", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ",", "ignore_channels", "=", "None", ")", ",", "\n", "]", "\n", "\n", "test_epoch", "=", "smp", ".", "utils", ".", "train", ".", "ValidEpoch", "(", "\n", "model", "=", "best_model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "device", "=", "DEVICE", ",", "\n", ")", "\n", "\n", "logs", "=", "test_epoch", ".", "run", "(", "test_dataloader", ")", "\n", "print", "(", "\"logs=\"", ",", "str", "(", "logs", ")", ")", "\n", "writer", ".", "add_text", "(", "f\"{opt.py_file}-test-score\"", ",", "str", "(", "logs", ")", ",", "global_step", "=", "test_best_epoch", ")", "\n", "\n", "# Testing with only class layer 1 (polyps)", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", "ignore_channels", "=", "[", "0", "]", ")", "\n", "metrics", "=", "[", "\n", "#smp.utils.metrics.IoU(threshold=0.5),", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ",", "ignore_channels", "=", "[", "0", "]", ")", ",", "\n", "]", "\n", "\n", "test_epoch", "=", "smp", ".", "utils", ".", "train", ".", "ValidEpoch", "(", "\n", "model", "=", "best_model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "device", "=", "DEVICE", ",", "\n", ")", "\n", "\n", "logs", "=", "test_epoch", ".", "run", "(", "test_dataloader", ")", "\n", "print", "(", "\"logs=\"", ",", "str", "(", "logs", ")", ")", "\n", "writer", ".", "add_text", "(", "f\"{opt.py_file}-test-score-ignore-channel-0\"", ",", "str", "(", "logs", ")", ",", "global_step", "=", "test_best_epoch", ")", "\n", "\n", "\n", "\n", "# Testing with only class layer 0 (BG)", "\n", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", "ignore_channels", "=", "[", "1", "]", ")", "\n", "metrics", "=", "[", "\n", "#smp.utils.metrics.IoU(threshold=0.5),", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ",", "ignore_channels", "=", "[", "1", "]", ")", ",", "\n", "]", "\n", "\n", "test_epoch", "=", "smp", ".", "utils", ".", "train", ".", "ValidEpoch", "(", "\n", "model", "=", "best_model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "device", "=", "DEVICE", ",", "\n", ")", "\n", "\n", "logs", "=", "test_epoch", ".", "run", "(", "test_dataloader", ")", "\n", "print", "(", "\"logs=\"", ",", "str", "(", "logs", ")", ")", "\n", "writer", ".", "add_text", "(", "f\"{opt.py_file}-test-score-ignore-channel-1\"", ",", "str", "(", "logs", ")", ",", "global_step", "=", "test_best_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.tri_unet.train_model": [[207, 260], ["segmentation_models_pytorch.utils.train.TrainEpoch", "segmentation_models_pytorch.utils.train.ValidEpoch", "os.path.join", "range", "print", "smp.utils.train.TrainEpoch.run", "smp.utils.train.ValidEpoch.run", "train_epoch.run.items", "valid_epoch.run.items", "torch.save", "torch.save", "torch.save", "print", "print", "tri_unet.do_test", "print", "print", "writer.add_scalar", "writer.add_scalar"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.do_test"], ["def", "train_model", "(", "train_loader", ",", "valid_loader", ",", "model", ",", "loss", ",", "metrics", ",", "optimizer", ",", "opt", ")", ":", "\n", "\n", "# create epoch runners ", "\n", "# it is a simple loop of iterating over dataloader`s samples", "\n", "    ", "train_epoch", "=", "smp", ".", "utils", ".", "train", ".", "TrainEpoch", "(", "\n", "model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "optimizer", "=", "optimizer", ",", "\n", "device", "=", "DEVICE", ",", "\n", "verbose", "=", "True", ",", "\n", ")", "\n", "\n", "valid_epoch", "=", "smp", ".", "utils", ".", "train", ".", "ValidEpoch", "(", "\n", "model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "device", "=", "DEVICE", ",", "\n", "verbose", "=", "True", ",", "\n", ")", "\n", "\n", "\n", "\n", "max_score", "=", "0.7699", "#changed after finding the best epoch value - 06-03-2021", "\n", "\n", "best_chk_path", "=", "os", ".", "path", ".", "join", "(", "CHECKPOINT_DIR", ",", "opt", ".", "best_checkpoint_name", ")", "\n", "\n", "for", "i", "in", "range", "(", "opt", ".", "start_epoch", "+", "1", ",", "opt", ".", "start_epoch", "+", "opt", ".", "num_epochs", "+", "1", ")", ":", "\n", "\n", "        ", "print", "(", "'\\nEpoch: {}'", ".", "format", "(", "i", ")", ")", "\n", "train_logs", "=", "train_epoch", ".", "run", "(", "train_loader", ")", "\n", "valid_logs", "=", "valid_epoch", ".", "run", "(", "valid_loader", ")", "\n", "\n", "# do something (save model, change lr, etc.)", "\n", "if", "max_score", "<", "valid_logs", "[", "'iou_score'", "]", ":", "\n", "            ", "max_score", "=", "valid_logs", "[", "'iou_score'", "]", "\n", "torch", ".", "save", "(", "{", "\"model\"", ":", "model", ",", "\"epoch\"", ":", "i", "}", ",", "best_chk_path", ")", "\n", "print", "(", "'Best Model saved!'", ")", "\n", "print", "(", "\"Testing....\"", ")", "\n", "do_test", "(", "opt", ")", "\n", "print", "(", "\"Tested\"", ")", "\n", "\n", "\n", "", "if", "i", "==", "opt", ".", "lr_change_point", ":", "\n", "            ", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "1e-6", "# changed from 1e-5 yo le-6 for retraining", "\n", "print", "(", "'Decrease decoder learning rate to 1e-5!'", ")", "\n", "\n", "# writing to logs to tensorboard", "\n", "", "for", "key", ",", "value", "in", "train_logs", ".", "items", "(", ")", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "f\"Train/{key}\"", ",", "value", ",", "i", ")", "\n", "\n", "", "for", "key", ",", "value", "in", "valid_logs", ".", "items", "(", ")", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "f\"Valid/{key}\"", ",", "value", ",", "i", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.tri_unet.generate_heatmapts": [[271, 287], ["print", "range", "img.squeeze.squeeze", "img.squeeze.detach().cpu().numpy", "matplotlib.imshow", "matplotlib.gcf", "fig_list.append", "matplotlib.close", "img.squeeze.detach().cpu", "img.squeeze.detach"], "function", ["None"], ["", "", "", "def", "generate_heatmapts", "(", "img_tensor", ")", ":", "\n", "    ", "print", "(", "img_tensor", ".", "shape", ")", "\n", "fig_list", "=", "[", "]", "\n", "for", "n", "in", "range", "(", "img_tensor", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "img", "=", "img_tensor", "[", "n", "]", "\n", "img", "=", "img", ".", "squeeze", "(", "dim", "=", "0", ")", "\n", "img_np", "=", "img", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "#img_np = np.transforms(img_np, (1,2,0))", "\n", "\n", "plt", ".", "imshow", "(", "img_np", ",", "cmap", "=", "\"hot\"", ")", "\n", "fig", "=", "plt", ".", "gcf", "(", ")", "\n", "fig_list", ".", "append", "(", "fig", ")", "\n", "# plt.clf()", "\n", "plt", ".", "close", "(", ")", "\n", "\n", "", "return", "fig_list", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.tri_unet.prepare_model": [[293, 306], ["my_models.triple_models.TripleUnet", "len"], "function", ["None"], ["", "def", "prepare_model", "(", "opt", ")", ":", "\n", "# model = UNet(n_channels=4, n_classes=1) # 4 = 3 channels + 1 grid encode", "\n", "\n", "# create segmentation model with pretrained encoder", "\n", "    ", "model", "=", "TripleUnet", "(", "\n", "encoder_name", "=", "opt", ".", "encoder", ",", "\n", "in_channels", "=", "opt", ".", "in_channels", ",", "\n", "encoder_weights", "=", "opt", ".", "encoder_weights", ",", "\n", "classes", "=", "len", "(", "opt", ".", "classes", ")", ",", "\n", "activation", "=", "opt", ".", "activation", ",", "\n", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.tri_unet.run_train": [[310, 328], ["tri_unet.prepare_model", "segmentation_models_pytorch.encoders.get_preprocessing_fn", "data.prepare_data.prepare_data", "segmentation_models_pytorch.utils.losses.DiceLoss", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "tri_unet.train_model", "segmentation_models_pytorch.utils.metrics.IoU", "dict", "prepare_model.parameters"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.prepare_model", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_data", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.train_model"], ["", "def", "run_train", "(", "opt", ")", ":", "\n", "    ", "model", "=", "prepare_model", "(", "opt", ")", "\n", "\n", "preprocessing_fn", "=", "smp", ".", "encoders", ".", "get_preprocessing_fn", "(", "opt", ".", "encoder", ",", "opt", ".", "encoder_weights", ")", "\n", "\n", "train_loader", ",", "val_loader", "=", "prepare_data", "(", "opt", ",", "preprocessing_fn", "=", "None", ")", "\n", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", "ignore_channels", "=", "[", "0", "]", ")", "\n", "\n", "metrics", "=", "[", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ",", "ignore_channels", "=", "[", "0", "]", ")", ",", "\n", "]", "\n", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "[", "\n", "dict", "(", "params", "=", "model", ".", "parameters", "(", ")", ",", "lr", "=", "opt", ".", "lr", ")", ",", "\n", "]", ")", "\n", "\n", "train_model", "(", "train_loader", ",", "val_loader", ",", "model", ",", "loss", ",", "metrics", ",", "optimizer", ",", "opt", ")", "\n", "#====================================", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.tri_unet.run_retrain": [[331, 356], ["torch.load", "torch.load", "torch.load", "print", "print", "segmentation_models_pytorch.encoders.get_preprocessing_fn", "data.prepare_data.prepare_data", "segmentation_models_pytorch.utils.losses.DiceLoss", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "tri_unet.train_model", "os.path.join", "segmentation_models_pytorch.utils.metrics.IoU", "dict", "model.parameters"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_data", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.train_model"], ["", "def", "run_retrain", "(", "opt", ")", ":", "\n", "\n", "    ", "checkpoint_dict", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "CHECKPOINT_DIR", ",", "opt", ".", "best_checkpoint_name", ")", ",", "map_location", "=", "opt", ".", "device", ")", "\n", "\n", "opt", ".", "start_epoch", "=", "checkpoint_dict", "[", "\"epoch\"", "]", "\n", "model", "=", "checkpoint_dict", "[", "\"model\"", "]", "\n", "\n", "print", "(", "\"Model epoch:\"", ",", "checkpoint_dict", "[", "\"epoch\"", "]", ")", "\n", "print", "(", "\"Model retrain started from epoch:\"", ",", "opt", ".", "start_epoch", ")", "\n", "\n", "preprocessing_fn", "=", "smp", ".", "encoders", ".", "get_preprocessing_fn", "(", "opt", ".", "encoder", ",", "opt", ".", "encoder_weights", ")", "\n", "\n", "train_loader", ",", "val_loader", "=", "prepare_data", "(", "opt", ",", "preprocessing_fn", "=", "None", ")", "\n", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", "ignore_channels", "=", "[", "0", "]", ")", "\n", "\n", "metrics", "=", "[", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ",", "ignore_channels", "=", "[", "0", "]", ")", ",", "\n", "]", "\n", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "[", "\n", "dict", "(", "params", "=", "model", ".", "parameters", "(", ")", ",", "lr", "=", "opt", ".", "lr", ")", ",", "\n", "]", ")", "\n", "\n", "train_model", "(", "train_loader", ",", "val_loader", ",", "model", ",", "loss", ",", "metrics", ",", "optimizer", ",", "opt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.tri_unet.check_model_graph": [[360, 362], ["None"], "function", ["None"], ["", "def", "check_model_graph", "(", ")", ":", "\n", "    ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.tri_unet.do_test": [[368, 406], ["torch.load", "torch.load", "torch.load", "print", "segmentation_models_pytorch.encoders.get_preprocessing_fn", "data.prepare_data.prepare_test_data", "data.prepare_data.prepare_test_data", "range", "os.path.join", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "best_model.predict", "pr_mask.squeeze().cpu().numpy().round.squeeze().cpu().numpy().round", "utils.visualize", "utils.visualize.savefig", "writer.add_figure", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "pr_mask.squeeze().cpu().numpy().round.squeeze().cpu().numpy", "numpy.transpose().astype", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "pr_mask.squeeze().cpu().numpy().round.squeeze().cpu", "numpy.transpose", "pr_mask.squeeze().cpu().numpy().round.squeeze"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_test_data", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_test_data", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.utils.functions.visualize"], ["", "def", "do_test", "(", "opt", ")", ":", "\n", "\n", "\n", "    ", "checkpoint_dict", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "CHECKPOINT_DIR", ",", "opt", ".", "best_checkpoint_name", ")", ",", "map_location", "=", "opt", ".", "device", ")", "\n", "\n", "test_epoch", "=", "checkpoint_dict", "[", "\"epoch\"", "]", "\n", "best_model", "=", "checkpoint_dict", "[", "\"model\"", "]", "\n", "\n", "print", "(", "\"Model best epoch:\"", ",", "test_epoch", ")", "\n", "\n", "preprocessing_fn", "=", "smp", ".", "encoders", ".", "get_preprocessing_fn", "(", "opt", ".", "encoder", ",", "opt", ".", "encoder_weights", ")", "\n", "test_dataset", "=", "prepare_test_data", "(", "opt", ",", "preprocessing_fn", "=", "None", ")", "\n", "test_dataset_vis", "=", "prepare_test_data", "(", "opt", ",", "preprocessing_fn", "=", "None", ")", "\n", "\n", "\n", "for", "i", "in", "range", "(", "opt", ".", "num_test_samples", ")", ":", "\n", "        ", "image", ",", "mask", "=", "test_dataset", "[", "i", "]", "\n", "image_vis", ",", "_", "=", "test_dataset_vis", "[", "i", "]", "\n", "\n", "#print(image)", "\n", "\n", "mask_tensor", "=", "torch", ".", "from_numpy", "(", "mask", ")", ".", "to", "(", "opt", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "image_tensor", "=", "torch", ".", "from_numpy", "(", "image", ")", ".", "to", "(", "opt", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "pr_mask", "=", "best_model", ".", "predict", "(", "image_tensor", ")", "\n", "\n", "pr_mask", "=", "pr_mask", ".", "squeeze", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "round", "(", ")", "\n", "\n", "fig", "=", "visualize", "(", "\n", "input_image_new", "=", "np", ".", "transpose", "(", "image_vis", ",", "(", "1", ",", "2", ",", "0", ")", ")", ".", "astype", "(", "int", ")", ",", "\n", "GT_mask_0", "=", "mask", "[", "0", ",", ":", ",", ":", "]", ",", "\n", "Pred_mask_0", "=", "pr_mask", "[", "0", ",", ":", ",", ":", "]", ",", "\n", "GT_mask_1", "=", "mask", "[", "1", ",", ":", ",", ":", "]", ",", "\n", "Pred_mask_1", "=", "pr_mask", "[", "1", ",", ":", ",", ":", "]", "\n", ")", "\n", "\n", "fig", ".", "savefig", "(", "f\"./test_202_{i}.png\"", ")", "\n", "writer", ".", "add_figure", "(", "f\"Test_sample/sample-{i}\"", ",", "fig", ",", "global_step", "=", "test_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.tri_unet.check_test_score": [[411, 485], ["torch.load", "torch.load", "torch.load", "print", "segmentation_models_pytorch.encoders.get_preprocessing_fn", "data.prepare_data.prepare_test_data", "torch.utils.data.DataLoader", "segmentation_models_pytorch.utils.losses.DiceLoss", "segmentation_models_pytorch.utils.train.ValidEpoch", "smp.utils.train.ValidEpoch.run", "print", "writer.add_text", "segmentation_models_pytorch.utils.losses.DiceLoss", "segmentation_models_pytorch.utils.train.ValidEpoch", "smp.utils.train.ValidEpoch.run", "print", "writer.add_text", "segmentation_models_pytorch.utils.losses.DiceLoss", "segmentation_models_pytorch.utils.train.ValidEpoch", "smp.utils.train.ValidEpoch.run", "print", "writer.add_text", "os.path.join", "segmentation_models_pytorch.utils.metrics.IoU", "str", "str", "segmentation_models_pytorch.utils.metrics.IoU", "str", "str", "segmentation_models_pytorch.utils.metrics.IoU", "str", "str"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_test_data"], ["", "", "def", "check_test_score", "(", "opt", ")", ":", "\n", "\n", "\n", "\n", "    ", "checkpoint_dict", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "CHECKPOINT_DIR", ",", "opt", ".", "best_checkpoint_name", ")", ",", "map_location", "=", "opt", ".", "device", ")", "\n", "\n", "test_best_epoch", "=", "checkpoint_dict", "[", "\"epoch\"", "]", "\n", "best_model", "=", "checkpoint_dict", "[", "\"model\"", "]", "\n", "\n", "print", "(", "\"Model best epoch:\"", ",", "test_best_epoch", ")", "\n", "\n", "\n", "\n", "preprocessing_fn", "=", "smp", ".", "encoders", ".", "get_preprocessing_fn", "(", "opt", ".", "encoder", ",", "opt", ".", "encoder_weights", ")", "\n", "test_dataset", "=", "prepare_test_data", "(", "opt", ",", "preprocessing_fn", "=", "None", ")", "\n", "\n", "test_dataloader", "=", "DataLoader", "(", "test_dataset", ",", "num_workers", "=", "48", ")", "\n", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", ")", "\n", "# Testing with two class layers", "\n", "metrics", "=", "[", "\n", "#smp.utils.metrics.IoU(threshold=0.5),", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ",", "ignore_channels", "=", "None", ")", ",", "\n", "]", "\n", "\n", "test_epoch", "=", "smp", ".", "utils", ".", "train", ".", "ValidEpoch", "(", "\n", "model", "=", "best_model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "device", "=", "DEVICE", ",", "\n", ")", "\n", "\n", "logs", "=", "test_epoch", ".", "run", "(", "test_dataloader", ")", "\n", "print", "(", "\"logs=\"", ",", "str", "(", "logs", ")", ")", "\n", "writer", ".", "add_text", "(", "f\"{opt.py_file}-test-score\"", ",", "str", "(", "logs", ")", ",", "global_step", "=", "test_best_epoch", ")", "\n", "\n", "# Testing with only class layer 1 (polyps)", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", "ignore_channels", "=", "[", "0", "]", ")", "\n", "metrics", "=", "[", "\n", "#smp.utils.metrics.IoU(threshold=0.5),", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ",", "ignore_channels", "=", "[", "0", "]", ")", ",", "\n", "]", "\n", "\n", "test_epoch", "=", "smp", ".", "utils", ".", "train", ".", "ValidEpoch", "(", "\n", "model", "=", "best_model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "device", "=", "DEVICE", ",", "\n", ")", "\n", "\n", "logs", "=", "test_epoch", ".", "run", "(", "test_dataloader", ")", "\n", "print", "(", "\"logs=\"", ",", "str", "(", "logs", ")", ")", "\n", "writer", ".", "add_text", "(", "f\"{opt.py_file}-test-score-ignore-channel-0\"", ",", "str", "(", "logs", ")", ",", "global_step", "=", "test_best_epoch", ")", "\n", "\n", "\n", "\n", "# Testing with only class layer 0 (BG)", "\n", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", "ignore_channels", "=", "[", "1", "]", ")", "\n", "metrics", "=", "[", "\n", "#smp.utils.metrics.IoU(threshold=0.5),", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ",", "ignore_channels", "=", "[", "1", "]", ")", ",", "\n", "]", "\n", "\n", "test_epoch", "=", "smp", ".", "utils", ".", "train", ".", "ValidEpoch", "(", "\n", "model", "=", "best_model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "device", "=", "DEVICE", ",", "\n", ")", "\n", "\n", "logs", "=", "test_epoch", ".", "run", "(", "test_dataloader", ")", "\n", "print", "(", "\"logs=\"", ",", "str", "(", "logs", ")", ")", "\n", "writer", ".", "add_text", "(", "f\"{opt.py_file}-test-score-ignore-channel-1\"", ",", "str", "(", "logs", ")", ",", "global_step", "=", "test_best_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.unet_plusplus.train_model": [[194, 247], ["segmentation_models_pytorch.utils.train.TrainEpoch", "segmentation_models_pytorch.utils.train.ValidEpoch", "os.path.join", "range", "print", "smp.utils.train.TrainEpoch.run", "smp.utils.train.ValidEpoch.run", "train_epoch.run.items", "valid_epoch.run.items", "torch.save", "torch.save", "torch.save", "print", "print", "unet_plusplus.do_test", "print", "print", "writer.add_scalar", "writer.add_scalar"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.do_test"], ["def", "train_model", "(", "train_loader", ",", "valid_loader", ",", "model", ",", "loss", ",", "metrics", ",", "optimizer", ",", "opt", ")", ":", "\n", "\n", "# create epoch runners ", "\n", "# it is a simple loop of iterating over dataloader`s samples", "\n", "    ", "train_epoch", "=", "smp", ".", "utils", ".", "train", ".", "TrainEpoch", "(", "\n", "model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "optimizer", "=", "optimizer", ",", "\n", "device", "=", "DEVICE", ",", "\n", "verbose", "=", "True", ",", "\n", ")", "\n", "\n", "valid_epoch", "=", "smp", ".", "utils", ".", "train", ".", "ValidEpoch", "(", "\n", "model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "device", "=", "DEVICE", ",", "\n", "verbose", "=", "True", ",", "\n", ")", "\n", "\n", "\n", "\n", "max_score", "=", "0", "\n", "\n", "best_chk_path", "=", "os", ".", "path", ".", "join", "(", "CHECKPOINT_DIR", ",", "opt", ".", "best_checkpoint_name", ")", "\n", "\n", "for", "i", "in", "range", "(", "opt", ".", "start_epoch", "+", "1", ",", "opt", ".", "start_epoch", "+", "opt", ".", "num_epochs", "+", "1", ")", ":", "\n", "\n", "        ", "print", "(", "'\\nEpoch: {}'", ".", "format", "(", "i", ")", ")", "\n", "train_logs", "=", "train_epoch", ".", "run", "(", "train_loader", ")", "\n", "valid_logs", "=", "valid_epoch", ".", "run", "(", "valid_loader", ")", "\n", "\n", "# do something (save model, change lr, etc.)", "\n", "if", "max_score", "<", "valid_logs", "[", "'iou_score'", "]", ":", "\n", "            ", "max_score", "=", "valid_logs", "[", "'iou_score'", "]", "\n", "torch", ".", "save", "(", "{", "\"model\"", ":", "model", ",", "\"epoch\"", ":", "i", "}", ",", "best_chk_path", ")", "\n", "print", "(", "'Best Model saved!'", ")", "\n", "print", "(", "\"Testing....\"", ")", "\n", "do_test", "(", "opt", ")", "\n", "print", "(", "\"Tested\"", ")", "\n", "\n", "\n", "", "if", "i", "==", "opt", ".", "lr_change_point", ":", "\n", "            ", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "1e-5", "\n", "print", "(", "'Decrease decoder learning rate to 1e-5!'", ")", "\n", "\n", "# writing to logs to tensorboard", "\n", "", "for", "key", ",", "value", "in", "train_logs", ".", "items", "(", ")", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "f\"Train/{key}\"", ",", "value", ",", "i", ")", "\n", "\n", "", "for", "key", ",", "value", "in", "valid_logs", ".", "items", "(", ")", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "f\"Valid/{key}\"", ",", "value", ",", "i", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.unet_plusplus.generate_heatmapts": [[256, 272], ["print", "range", "img.squeeze.squeeze", "img.squeeze.detach().cpu().numpy", "matplotlib.imshow", "matplotlib.gcf", "fig_list.append", "matplotlib.close", "img.squeeze.detach().cpu", "img.squeeze.detach"], "function", ["None"], ["", "", "", "def", "generate_heatmapts", "(", "img_tensor", ")", ":", "\n", "    ", "print", "(", "img_tensor", ".", "shape", ")", "\n", "fig_list", "=", "[", "]", "\n", "for", "n", "in", "range", "(", "img_tensor", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "img", "=", "img_tensor", "[", "n", "]", "\n", "img", "=", "img", ".", "squeeze", "(", "dim", "=", "0", ")", "\n", "img_np", "=", "img", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "#img_np = np.transforms(img_np, (1,2,0))", "\n", "\n", "plt", ".", "imshow", "(", "img_np", ",", "cmap", "=", "\"hot\"", ")", "\n", "fig", "=", "plt", ".", "gcf", "(", ")", "\n", "fig_list", ".", "append", "(", "fig", ")", "\n", "# plt.clf()", "\n", "plt", ".", "close", "(", ")", "\n", "\n", "", "return", "fig_list", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.unet_plusplus.prepare_model": [[278, 291], ["segmentation_models_pytorch.UnetPlusPlus", "len"], "function", ["None"], ["", "def", "prepare_model", "(", "opt", ")", ":", "\n", "# model = UNet(n_channels=4, n_classes=1) # 4 = 3 channels + 1 grid encode", "\n", "\n", "# create segmentation model with pretrained encoder", "\n", "    ", "model", "=", "smp", ".", "UnetPlusPlus", "(", "\n", "encoder_name", "=", "opt", ".", "encoder", ",", "\n", "in_channels", "=", "opt", ".", "in_channels", ",", "\n", "encoder_weights", "=", "opt", ".", "encoder_weights", ",", "\n", "classes", "=", "len", "(", "opt", ".", "classes", ")", ",", "\n", "activation", "=", "opt", ".", "activation", ",", "\n", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.unet_plusplus.run_train": [[295, 313], ["unet_plusplus.prepare_model", "segmentation_models_pytorch.encoders.get_preprocessing_fn", "data.prepare_data.prepare_data", "segmentation_models_pytorch.utils.losses.DiceLoss", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "unet_plusplus.train_model", "segmentation_models_pytorch.utils.metrics.IoU", "dict", "prepare_model.parameters"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.prepare_model", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_data", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.train_model"], ["", "def", "run_train", "(", "opt", ")", ":", "\n", "    ", "model", "=", "prepare_model", "(", "opt", ")", "\n", "\n", "preprocessing_fn", "=", "smp", ".", "encoders", ".", "get_preprocessing_fn", "(", "opt", ".", "encoder", ",", "opt", ".", "encoder_weights", ")", "\n", "\n", "train_loader", ",", "val_loader", "=", "prepare_data", "(", "opt", ",", "preprocessing_fn", "=", "None", ")", "\n", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", "ignore_channels", "=", "[", "0", "]", ")", "\n", "\n", "metrics", "=", "[", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ",", "ignore_channels", "=", "[", "0", "]", ")", ",", "\n", "]", "\n", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "[", "\n", "dict", "(", "params", "=", "model", ".", "parameters", "(", ")", ",", "lr", "=", "opt", ".", "lr", ")", ",", "\n", "]", ")", "\n", "\n", "train_model", "(", "train_loader", ",", "val_loader", ",", "model", ",", "loss", ",", "metrics", ",", "optimizer", ",", "opt", ")", "\n", "#====================================", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.unet_plusplus.run_retrain": [[316, 341], ["torch.load", "torch.load", "torch.load", "print", "print", "segmentation_models_pytorch.encoders.get_preprocessing_fn", "data.prepare_data.prepare_data", "segmentation_models_pytorch.utils.losses.DiceLoss", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "unet_plusplus.train_model", "os.path.join", "segmentation_models_pytorch.utils.metrics.IoU", "dict", "model.parameters"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_data", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.train_model"], ["", "def", "run_retrain", "(", "opt", ")", ":", "\n", "\n", "    ", "checkpoint_dict", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "CHECKPOINT_DIR", ",", "opt", ".", "best_checkpoint_name", ")", ")", "\n", "\n", "opt", ".", "start_epoch", "=", "checkpoint_dict", "[", "\"epoch\"", "]", "\n", "model", "=", "checkpoint_dict", "[", "\"model\"", "]", "\n", "\n", "print", "(", "\"Model epoch:\"", ",", "checkpoint_dict", "[", "\"epoch\"", "]", ")", "\n", "print", "(", "\"Model retrain started from epoch:\"", ",", "opt", ".", "start_epoch", ")", "\n", "\n", "preprocessing_fn", "=", "smp", ".", "encoders", ".", "get_preprocessing_fn", "(", "opt", ".", "encoder", ",", "opt", ".", "encoder_weights", ")", "\n", "\n", "train_loader", ",", "val_loader", "=", "prepare_data", "(", "opt", ",", "preprocessing_fn", ")", "\n", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", ")", "\n", "\n", "metrics", "=", "[", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ")", ",", "\n", "]", "\n", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "[", "\n", "dict", "(", "params", "=", "model", ".", "parameters", "(", ")", ",", "lr", "=", "opt", ".", "lr", ")", ",", "\n", "]", ")", "\n", "\n", "train_model", "(", "train_loader", ",", "val_loader", ",", "model", ",", "loss", ",", "metrics", ",", "optimizer", ",", "opt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.unet_plusplus.check_model_graph": [[345, 347], ["None"], "function", ["None"], ["", "def", "check_model_graph", "(", ")", ":", "\n", "    ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.unet_plusplus.do_test": [[353, 391], ["torch.load", "torch.load", "torch.load", "print", "segmentation_models_pytorch.encoders.get_preprocessing_fn", "data.prepare_data.prepare_test_data", "data.prepare_data.prepare_test_data", "range", "os.path.join", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "best_model.predict", "pr_mask.squeeze().cpu().numpy().round.squeeze().cpu().numpy().round", "utils.visualize", "utils.visualize.savefig", "writer.add_figure", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "pr_mask.squeeze().cpu().numpy().round.squeeze().cpu().numpy", "numpy.transpose().astype", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "pr_mask.squeeze().cpu().numpy().round.squeeze().cpu", "numpy.transpose", "pr_mask.squeeze().cpu().numpy().round.squeeze"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_test_data", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_test_data", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.utils.functions.visualize"], ["", "def", "do_test", "(", "opt", ")", ":", "\n", "\n", "\n", "    ", "checkpoint_dict", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "CHECKPOINT_DIR", ",", "opt", ".", "best_checkpoint_name", ")", ")", "\n", "\n", "test_epoch", "=", "checkpoint_dict", "[", "\"epoch\"", "]", "\n", "best_model", "=", "checkpoint_dict", "[", "\"model\"", "]", "\n", "\n", "print", "(", "\"Model best epoch:\"", ",", "test_epoch", ")", "\n", "\n", "preprocessing_fn", "=", "smp", ".", "encoders", ".", "get_preprocessing_fn", "(", "opt", ".", "encoder", ",", "opt", ".", "encoder_weights", ")", "\n", "test_dataset", "=", "prepare_test_data", "(", "opt", ",", "preprocessing_fn", "=", "None", ")", "\n", "test_dataset_vis", "=", "prepare_test_data", "(", "opt", ",", "preprocessing_fn", "=", "None", ")", "\n", "\n", "\n", "for", "i", "in", "range", "(", "opt", ".", "num_test_samples", ")", ":", "\n", "        ", "image", ",", "mask", "=", "test_dataset", "[", "i", "]", "\n", "image_vis", ",", "_", "=", "test_dataset_vis", "[", "i", "]", "\n", "\n", "#print(image)", "\n", "\n", "mask_tensor", "=", "torch", ".", "from_numpy", "(", "mask", ")", ".", "to", "(", "opt", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "image_tensor", "=", "torch", ".", "from_numpy", "(", "image", ")", ".", "to", "(", "opt", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "pr_mask", "=", "best_model", ".", "predict", "(", "image_tensor", ")", "\n", "\n", "pr_mask", "=", "pr_mask", ".", "squeeze", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "round", "(", ")", "\n", "\n", "fig", "=", "visualize", "(", "\n", "input_image_new", "=", "np", ".", "transpose", "(", "image_vis", ",", "(", "1", ",", "2", ",", "0", ")", ")", ".", "astype", "(", "int", ")", ",", "\n", "GT_mask_0", "=", "mask", "[", "0", ",", ":", ",", ":", "]", ",", "\n", "Pred_mask_0", "=", "pr_mask", "[", "0", ",", ":", ",", ":", "]", ",", "\n", "GT_mask_1", "=", "mask", "[", "1", ",", ":", ",", ":", "]", ",", "\n", "Pred_mask_1", "=", "pr_mask", "[", "1", ",", ":", ",", ":", "]", "\n", ")", "\n", "\n", "fig", ".", "savefig", "(", "f\"./test_202_{i}.png\"", ")", "\n", "writer", ".", "add_figure", "(", "f\"Test_sample/sample-{i}\"", ",", "fig", ",", "global_step", "=", "test_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.unet_plusplus.check_test_score": [[396, 470], ["torch.load", "torch.load", "torch.load", "print", "segmentation_models_pytorch.encoders.get_preprocessing_fn", "data.prepare_data.prepare_test_data", "torch.utils.data.DataLoader", "segmentation_models_pytorch.utils.losses.DiceLoss", "segmentation_models_pytorch.utils.train.ValidEpoch", "smp.utils.train.ValidEpoch.run", "print", "writer.add_text", "segmentation_models_pytorch.utils.losses.DiceLoss", "segmentation_models_pytorch.utils.train.ValidEpoch", "smp.utils.train.ValidEpoch.run", "print", "writer.add_text", "segmentation_models_pytorch.utils.losses.DiceLoss", "segmentation_models_pytorch.utils.train.ValidEpoch", "smp.utils.train.ValidEpoch.run", "print", "writer.add_text", "os.path.join", "segmentation_models_pytorch.utils.metrics.IoU", "str", "str", "segmentation_models_pytorch.utils.metrics.IoU", "str", "str", "segmentation_models_pytorch.utils.metrics.IoU", "str", "str"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_test_data"], ["", "", "def", "check_test_score", "(", "opt", ")", ":", "\n", "\n", "\n", "\n", "    ", "checkpoint_dict", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "CHECKPOINT_DIR", ",", "opt", ".", "best_checkpoint_name", ")", ")", "\n", "\n", "test_best_epoch", "=", "checkpoint_dict", "[", "\"epoch\"", "]", "\n", "best_model", "=", "checkpoint_dict", "[", "\"model\"", "]", "\n", "\n", "print", "(", "\"Model best epoch:\"", ",", "test_best_epoch", ")", "\n", "\n", "\n", "\n", "preprocessing_fn", "=", "smp", ".", "encoders", ".", "get_preprocessing_fn", "(", "opt", ".", "encoder", ",", "opt", ".", "encoder_weights", ")", "\n", "test_dataset", "=", "prepare_test_data", "(", "opt", ",", "preprocessing_fn", "=", "preprocessing_fn", ")", "\n", "\n", "test_dataloader", "=", "DataLoader", "(", "test_dataset", ",", "num_workers", "=", "48", ")", "\n", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", ")", "\n", "# Testing with two class layers", "\n", "metrics", "=", "[", "\n", "#smp.utils.metrics.IoU(threshold=0.5),", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ",", "ignore_channels", "=", "None", ")", ",", "\n", "]", "\n", "\n", "test_epoch", "=", "smp", ".", "utils", ".", "train", ".", "ValidEpoch", "(", "\n", "model", "=", "best_model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "device", "=", "DEVICE", ",", "\n", ")", "\n", "\n", "logs", "=", "test_epoch", ".", "run", "(", "test_dataloader", ")", "\n", "print", "(", "\"logs=\"", ",", "str", "(", "logs", ")", ")", "\n", "writer", ".", "add_text", "(", "f\"{opt.py_file}-test-score\"", ",", "str", "(", "logs", ")", ",", "global_step", "=", "test_best_epoch", ")", "\n", "\n", "# Testing with only class layer 1 (polyps)", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", "ignore_channels", "=", "[", "0", "]", ")", "\n", "metrics", "=", "[", "\n", "#smp.utils.metrics.IoU(threshold=0.5),", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ",", "ignore_channels", "=", "[", "0", "]", ")", ",", "\n", "]", "\n", "\n", "test_epoch", "=", "smp", ".", "utils", ".", "train", ".", "ValidEpoch", "(", "\n", "model", "=", "best_model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "device", "=", "DEVICE", ",", "\n", ")", "\n", "\n", "logs", "=", "test_epoch", ".", "run", "(", "test_dataloader", ")", "\n", "print", "(", "\"logs=\"", ",", "str", "(", "logs", ")", ")", "\n", "writer", ".", "add_text", "(", "f\"{opt.py_file}-test-score-ignore-channel-0\"", ",", "str", "(", "logs", ")", ",", "global_step", "=", "test_best_epoch", ")", "\n", "\n", "\n", "\n", "# Testing with only class layer 0 (BG)", "\n", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", "ignore_channels", "=", "[", "1", "]", ")", "\n", "metrics", "=", "[", "\n", "#smp.utils.metrics.IoU(threshold=0.5),", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ",", "ignore_channels", "=", "[", "1", "]", ")", ",", "\n", "]", "\n", "\n", "test_epoch", "=", "smp", ".", "utils", ".", "train", ".", "ValidEpoch", "(", "\n", "model", "=", "best_model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "device", "=", "DEVICE", ",", "\n", ")", "\n", "\n", "logs", "=", "test_epoch", ".", "run", "(", "test_dataloader", ")", "\n", "print", "(", "\"logs=\"", ",", "str", "(", "logs", ")", ")", "\n", "writer", ".", "add_text", "(", "f\"{opt.py_file}-test-score-ignore-channel-1\"", ",", "str", "(", "logs", ")", ",", "global_step", "=", "test_best_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.train_model": [[193, 246], ["segmentation_models_pytorch.utils.train.TrainEpoch", "segmentation_models_pytorch.utils.train.ValidEpoch", "os.path.join", "range", "print", "smp.utils.train.TrainEpoch.run", "smp.utils.train.ValidEpoch.run", "train_epoch.run.items", "valid_epoch.run.items", "torch.save", "torch.save", "torch.save", "print", "print", "deeplabv3.do_test", "print", "print", "writer.add_scalar", "writer.add_scalar"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.do_test"], ["def", "train_model", "(", "train_loader", ",", "valid_loader", ",", "model", ",", "loss", ",", "metrics", ",", "optimizer", ",", "opt", ")", ":", "\n", "\n", "# create epoch runners ", "\n", "# it is a simple loop of iterating over dataloader`s samples", "\n", "    ", "train_epoch", "=", "smp", ".", "utils", ".", "train", ".", "TrainEpoch", "(", "\n", "model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "optimizer", "=", "optimizer", ",", "\n", "device", "=", "DEVICE", ",", "\n", "verbose", "=", "True", ",", "\n", ")", "\n", "\n", "valid_epoch", "=", "smp", ".", "utils", ".", "train", ".", "ValidEpoch", "(", "\n", "model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "device", "=", "DEVICE", ",", "\n", "verbose", "=", "True", ",", "\n", ")", "\n", "\n", "\n", "\n", "max_score", "=", "0", "\n", "\n", "best_chk_path", "=", "os", ".", "path", ".", "join", "(", "CHECKPOINT_DIR", ",", "opt", ".", "best_checkpoint_name", ")", "\n", "\n", "for", "i", "in", "range", "(", "opt", ".", "start_epoch", "+", "1", ",", "opt", ".", "start_epoch", "+", "opt", ".", "num_epochs", "+", "1", ")", ":", "\n", "\n", "        ", "print", "(", "'\\nEpoch: {}'", ".", "format", "(", "i", ")", ")", "\n", "train_logs", "=", "train_epoch", ".", "run", "(", "train_loader", ")", "\n", "valid_logs", "=", "valid_epoch", ".", "run", "(", "valid_loader", ")", "\n", "\n", "# do something (save model, change lr, etc.)", "\n", "if", "max_score", "<", "valid_logs", "[", "'iou_score'", "]", ":", "\n", "            ", "max_score", "=", "valid_logs", "[", "'iou_score'", "]", "\n", "torch", ".", "save", "(", "{", "\"model\"", ":", "model", ",", "\"epoch\"", ":", "i", "}", ",", "best_chk_path", ")", "\n", "print", "(", "'Best Model saved!'", ")", "\n", "print", "(", "\"Testing....\"", ")", "\n", "do_test", "(", "opt", ")", "\n", "print", "(", "\"Tested\"", ")", "\n", "\n", "\n", "", "if", "i", "==", "opt", ".", "lr_change_point", ":", "\n", "            ", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "1e-5", "\n", "print", "(", "'Decrease decoder learning rate to 1e-5!'", ")", "\n", "\n", "# writing to logs to tensorboard", "\n", "", "for", "key", ",", "value", "in", "train_logs", ".", "items", "(", ")", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "f\"Train/{key}\"", ",", "value", ",", "i", ")", "\n", "\n", "", "for", "key", ",", "value", "in", "valid_logs", ".", "items", "(", ")", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "f\"Valid/{key}\"", ",", "value", ",", "i", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.generate_heatmapts": [[255, 271], ["print", "range", "img.squeeze.squeeze", "img.squeeze.detach().cpu().numpy", "matplotlib.imshow", "matplotlib.gcf", "fig_list.append", "matplotlib.close", "img.squeeze.detach().cpu", "img.squeeze.detach"], "function", ["None"], ["", "", "", "def", "generate_heatmapts", "(", "img_tensor", ")", ":", "\n", "    ", "print", "(", "img_tensor", ".", "shape", ")", "\n", "fig_list", "=", "[", "]", "\n", "for", "n", "in", "range", "(", "img_tensor", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "img", "=", "img_tensor", "[", "n", "]", "\n", "img", "=", "img", ".", "squeeze", "(", "dim", "=", "0", ")", "\n", "img_np", "=", "img", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "#img_np = np.transforms(img_np, (1,2,0))", "\n", "\n", "plt", ".", "imshow", "(", "img_np", ",", "cmap", "=", "\"hot\"", ")", "\n", "fig", "=", "plt", ".", "gcf", "(", ")", "\n", "fig_list", ".", "append", "(", "fig", ")", "\n", "# plt.clf()", "\n", "plt", ".", "close", "(", ")", "\n", "\n", "", "return", "fig_list", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.prepare_model": [[277, 290], ["segmentation_models_pytorch.DeepLabV3", "len"], "function", ["None"], ["", "def", "prepare_model", "(", "opt", ")", ":", "\n", "# model = UNet(n_channels=4, n_classes=1) # 4 = 3 channels + 1 grid encode", "\n", "\n", "# create segmentation model with pretrained encoder", "\n", "    ", "model", "=", "smp", ".", "DeepLabV3", "(", "\n", "encoder_name", "=", "opt", ".", "encoder", ",", "\n", "in_channels", "=", "opt", ".", "in_channels", ",", "\n", "encoder_weights", "=", "opt", ".", "encoder_weights", ",", "\n", "classes", "=", "len", "(", "opt", ".", "classes", ")", ",", "\n", "activation", "=", "opt", ".", "activation", ",", "\n", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.run_train": [[294, 312], ["deeplabv3.prepare_model", "segmentation_models_pytorch.encoders.get_preprocessing_fn", "data.prepare_data.prepare_data", "segmentation_models_pytorch.utils.losses.DiceLoss", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "deeplabv3.train_model", "segmentation_models_pytorch.utils.metrics.IoU", "dict", "prepare_model.parameters"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.prepare_model", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_data", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.train_model"], ["", "def", "run_train", "(", "opt", ")", ":", "\n", "    ", "model", "=", "prepare_model", "(", "opt", ")", "\n", "\n", "preprocessing_fn", "=", "smp", ".", "encoders", ".", "get_preprocessing_fn", "(", "opt", ".", "encoder", ",", "opt", ".", "encoder_weights", ")", "\n", "\n", "train_loader", ",", "val_loader", "=", "prepare_data", "(", "opt", ",", "preprocessing_fn", "=", "None", ")", "\n", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", "ignore_channels", "=", "[", "0", "]", ")", "\n", "\n", "metrics", "=", "[", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ",", "ignore_channels", "=", "[", "0", "]", ")", ",", "\n", "]", "\n", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "[", "\n", "dict", "(", "params", "=", "model", ".", "parameters", "(", ")", ",", "lr", "=", "opt", ".", "lr", ")", ",", "\n", "]", ")", "\n", "\n", "train_model", "(", "train_loader", ",", "val_loader", ",", "model", ",", "loss", ",", "metrics", ",", "optimizer", ",", "opt", ")", "\n", "#====================================", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.run_retrain": [[315, 340], ["torch.load", "torch.load", "torch.load", "print", "print", "segmentation_models_pytorch.encoders.get_preprocessing_fn", "data.prepare_data.prepare_data", "segmentation_models_pytorch.utils.losses.DiceLoss", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "deeplabv3.train_model", "os.path.join", "segmentation_models_pytorch.utils.metrics.IoU", "dict", "model.parameters"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_data", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.train_model"], ["", "def", "run_retrain", "(", "opt", ")", ":", "\n", "\n", "    ", "checkpoint_dict", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "CHECKPOINT_DIR", ",", "opt", ".", "best_checkpoint_name", ")", ")", "\n", "\n", "opt", ".", "start_epoch", "=", "checkpoint_dict", "[", "\"epoch\"", "]", "\n", "model", "=", "checkpoint_dict", "[", "\"model\"", "]", "\n", "\n", "print", "(", "\"Model epoch:\"", ",", "checkpoint_dict", "[", "\"epoch\"", "]", ")", "\n", "print", "(", "\"Model retrain started from epoch:\"", ",", "opt", ".", "start_epoch", ")", "\n", "\n", "preprocessing_fn", "=", "smp", ".", "encoders", ".", "get_preprocessing_fn", "(", "opt", ".", "encoder", ",", "opt", ".", "encoder_weights", ")", "\n", "\n", "train_loader", ",", "val_loader", "=", "prepare_data", "(", "opt", ",", "preprocessing_fn", ")", "\n", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", ")", "\n", "\n", "metrics", "=", "[", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ")", ",", "\n", "]", "\n", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "[", "\n", "dict", "(", "params", "=", "model", ".", "parameters", "(", ")", ",", "lr", "=", "opt", ".", "lr", ")", ",", "\n", "]", ")", "\n", "\n", "train_model", "(", "train_loader", ",", "val_loader", ",", "model", ",", "loss", ",", "metrics", ",", "optimizer", ",", "opt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.check_model_graph": [[344, 346], ["None"], "function", ["None"], ["", "def", "check_model_graph", "(", ")", ":", "\n", "    ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.do_test": [[352, 390], ["torch.load", "torch.load", "torch.load", "print", "segmentation_models_pytorch.encoders.get_preprocessing_fn", "data.prepare_data.prepare_test_data", "data.prepare_data.prepare_test_data", "range", "os.path.join", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "torch.from_numpy().to().unsqueeze", "best_model.predict", "pr_mask.squeeze().cpu().numpy().round.squeeze().cpu().numpy().round", "utils.visualize", "utils.visualize.savefig", "writer.add_figure", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "pr_mask.squeeze().cpu().numpy().round.squeeze().cpu().numpy", "numpy.transpose().astype", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "pr_mask.squeeze().cpu().numpy().round.squeeze().cpu", "numpy.transpose", "pr_mask.squeeze().cpu().numpy().round.squeeze"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_test_data", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_test_data", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.utils.functions.visualize"], ["", "def", "do_test", "(", "opt", ")", ":", "\n", "\n", "\n", "    ", "checkpoint_dict", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "CHECKPOINT_DIR", ",", "opt", ".", "best_checkpoint_name", ")", ")", "\n", "\n", "test_epoch", "=", "checkpoint_dict", "[", "\"epoch\"", "]", "\n", "best_model", "=", "checkpoint_dict", "[", "\"model\"", "]", "\n", "\n", "print", "(", "\"Model best epoch:\"", ",", "test_epoch", ")", "\n", "\n", "preprocessing_fn", "=", "smp", ".", "encoders", ".", "get_preprocessing_fn", "(", "opt", ".", "encoder", ",", "opt", ".", "encoder_weights", ")", "\n", "test_dataset", "=", "prepare_test_data", "(", "opt", ",", "preprocessing_fn", "=", "None", ")", "\n", "test_dataset_vis", "=", "prepare_test_data", "(", "opt", ",", "preprocessing_fn", "=", "None", ")", "\n", "\n", "\n", "for", "i", "in", "range", "(", "opt", ".", "num_test_samples", ")", ":", "\n", "        ", "image", ",", "mask", "=", "test_dataset", "[", "i", "]", "\n", "image_vis", ",", "_", "=", "test_dataset_vis", "[", "i", "]", "\n", "\n", "#print(image)", "\n", "\n", "mask_tensor", "=", "torch", ".", "from_numpy", "(", "mask", ")", ".", "to", "(", "opt", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "image_tensor", "=", "torch", ".", "from_numpy", "(", "image", ")", ".", "to", "(", "opt", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "pr_mask", "=", "best_model", ".", "predict", "(", "image_tensor", ")", "\n", "\n", "pr_mask", "=", "pr_mask", ".", "squeeze", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "round", "(", ")", "\n", "\n", "fig", "=", "visualize", "(", "\n", "input_image_new", "=", "np", ".", "transpose", "(", "image_vis", ",", "(", "1", ",", "2", ",", "0", ")", ")", ".", "astype", "(", "int", ")", ",", "\n", "GT_mask_0", "=", "mask", "[", "0", ",", ":", ",", ":", "]", ",", "\n", "Pred_mask_0", "=", "pr_mask", "[", "0", ",", ":", ",", ":", "]", ",", "\n", "GT_mask_1", "=", "mask", "[", "1", ",", ":", ",", ":", "]", ",", "\n", "Pred_mask_1", "=", "pr_mask", "[", "1", ",", ":", ",", ":", "]", "\n", ")", "\n", "\n", "fig", ".", "savefig", "(", "f\"./test_202_{i}.png\"", ")", "\n", "writer", ".", "add_figure", "(", "f\"Test_sample/sample-{i}\"", ",", "fig", ",", "global_step", "=", "test_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.None.deeplabv3.check_test_score": [[395, 469], ["torch.load", "torch.load", "torch.load", "print", "segmentation_models_pytorch.encoders.get_preprocessing_fn", "data.prepare_data.prepare_test_data", "torch.utils.data.DataLoader", "segmentation_models_pytorch.utils.losses.DiceLoss", "segmentation_models_pytorch.utils.train.ValidEpoch", "smp.utils.train.ValidEpoch.run", "print", "writer.add_text", "segmentation_models_pytorch.utils.losses.DiceLoss", "segmentation_models_pytorch.utils.train.ValidEpoch", "smp.utils.train.ValidEpoch.run", "print", "writer.add_text", "segmentation_models_pytorch.utils.losses.DiceLoss", "segmentation_models_pytorch.utils.train.ValidEpoch", "smp.utils.train.ValidEpoch.run", "print", "writer.add_text", "os.path.join", "segmentation_models_pytorch.utils.metrics.IoU", "str", "str", "segmentation_models_pytorch.utils.metrics.IoU", "str", "str", "segmentation_models_pytorch.utils.metrics.IoU", "str", "str"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_test_data"], ["", "", "def", "check_test_score", "(", "opt", ")", ":", "\n", "\n", "\n", "\n", "    ", "checkpoint_dict", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "CHECKPOINT_DIR", ",", "opt", ".", "best_checkpoint_name", ")", ")", "\n", "\n", "test_best_epoch", "=", "checkpoint_dict", "[", "\"epoch\"", "]", "\n", "best_model", "=", "checkpoint_dict", "[", "\"model\"", "]", "\n", "\n", "print", "(", "\"Model best epoch:\"", ",", "test_best_epoch", ")", "\n", "\n", "\n", "\n", "preprocessing_fn", "=", "smp", ".", "encoders", ".", "get_preprocessing_fn", "(", "opt", ".", "encoder", ",", "opt", ".", "encoder_weights", ")", "\n", "test_dataset", "=", "prepare_test_data", "(", "opt", ",", "preprocessing_fn", "=", "None", ")", "\n", "\n", "test_dataloader", "=", "DataLoader", "(", "test_dataset", ",", "num_workers", "=", "48", ")", "\n", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", ")", "\n", "# Testing with two class layers", "\n", "metrics", "=", "[", "\n", "#smp.utils.metrics.IoU(threshold=0.5),", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ",", "ignore_channels", "=", "None", ")", ",", "\n", "]", "\n", "\n", "test_epoch", "=", "smp", ".", "utils", ".", "train", ".", "ValidEpoch", "(", "\n", "model", "=", "best_model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "device", "=", "DEVICE", ",", "\n", ")", "\n", "\n", "logs", "=", "test_epoch", ".", "run", "(", "test_dataloader", ")", "\n", "print", "(", "\"logs=\"", ",", "str", "(", "logs", ")", ")", "\n", "writer", ".", "add_text", "(", "f\"{opt.py_file}-test-score\"", ",", "str", "(", "logs", ")", ",", "global_step", "=", "test_best_epoch", ")", "\n", "\n", "# Testing with only class layer 1 (polyps)", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", "ignore_channels", "=", "[", "0", "]", ")", "\n", "metrics", "=", "[", "\n", "#smp.utils.metrics.IoU(threshold=0.5),", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ",", "ignore_channels", "=", "[", "0", "]", ")", ",", "\n", "]", "\n", "\n", "test_epoch", "=", "smp", ".", "utils", ".", "train", ".", "ValidEpoch", "(", "\n", "model", "=", "best_model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "device", "=", "DEVICE", ",", "\n", ")", "\n", "\n", "logs", "=", "test_epoch", ".", "run", "(", "test_dataloader", ")", "\n", "print", "(", "\"logs=\"", ",", "str", "(", "logs", ")", ")", "\n", "writer", ".", "add_text", "(", "f\"{opt.py_file}-test-score-ignore-channel-0\"", ",", "str", "(", "logs", ")", ",", "global_step", "=", "test_best_epoch", ")", "\n", "\n", "\n", "\n", "# Testing with only class layer 0 (BG)", "\n", "\n", "loss", "=", "smp", ".", "utils", ".", "losses", ".", "DiceLoss", "(", "ignore_channels", "=", "[", "1", "]", ")", "\n", "metrics", "=", "[", "\n", "#smp.utils.metrics.IoU(threshold=0.5),", "\n", "smp", ".", "utils", ".", "metrics", ".", "IoU", "(", "threshold", "=", "0.5", ",", "ignore_channels", "=", "[", "1", "]", ")", ",", "\n", "]", "\n", "\n", "test_epoch", "=", "smp", ".", "utils", ".", "train", ".", "ValidEpoch", "(", "\n", "model", "=", "best_model", ",", "\n", "loss", "=", "loss", ",", "\n", "metrics", "=", "metrics", ",", "\n", "device", "=", "DEVICE", ",", "\n", ")", "\n", "\n", "logs", "=", "test_epoch", ".", "run", "(", "test_dataloader", ")", "\n", "print", "(", "\"logs=\"", ",", "str", "(", "logs", ")", ")", "\n", "writer", ".", "add_text", "(", "f\"{opt.py_file}-test-score-ignore-channel-1\"", ",", "str", "(", "logs", ")", ",", "global_step", "=", "test_best_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.utils.functions.reverse_one_hot": [[6, 24], ["numpy.argmax", "numpy.expand_dims", "print"], "function", ["None"], ["def", "reverse_one_hot", "(", "image", ")", ":", "\n", "    ", "\"\"\"\n    Transform a 2D array in one-hot format (depth is num_classes),\n    to a 2D array with only 1 channel, where each pixel value is\n    the classified class key.\n    # Arguments\n        image: The one-hot format image \n        \n    # Returns\n        A 2D array with the same width and hieght as the input, but\n        with a depth size of 1, where each pixel value is the classified \n        class key.\n    \"\"\"", "\n", "x", "=", "np", ".", "argmax", "(", "image", ",", "axis", "=", "1", ")", "\n", "x", "=", "np", ".", "expand_dims", "(", "x", ",", "axis", "=", "1", ")", "\n", "print", "(", "\"x shape=\"", ",", "x", ".", "shape", ")", "\n", "#x = torch.argmax(image, axis = 1)", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.utils.functions.visualize": [[27, 40], ["len", "matplotlib.figure", "enumerate", "images.items", "matplotlib.subplot", "matplotlib.xticks", "matplotlib.yticks", "matplotlib.title", "matplotlib.imshow", "name.split"], "function", ["None"], ["", "def", "visualize", "(", "**", "images", ")", ":", "\n", "    ", "\"\"\"PLot images in one row.\"\"\"", "\n", "n", "=", "len", "(", "images", ")", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "16", ",", "5", ")", ")", "\n", "for", "i", ",", "(", "name", ",", "image", ")", "in", "enumerate", "(", "images", ".", "items", "(", ")", ")", ":", "\n", "        ", "plt", ".", "subplot", "(", "1", ",", "n", ",", "i", "+", "1", ")", "\n", "plt", ".", "xticks", "(", "[", "]", ")", "\n", "plt", ".", "yticks", "(", "[", "]", ")", "\n", "plt", ".", "title", "(", "' '", ".", "join", "(", "name", ".", "split", "(", "'_'", ")", ")", ".", "title", "(", ")", ")", "\n", "plt", ".", "imshow", "(", "image", ")", "\n", "#plt.show()", "\n", "\n", "", "return", "fig", "", "", ""]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.utils.losses.DiceCoeff.forward": [[10, 18], ["losses.DiceCoeff.save_for_backward", "torch.dot", "input.view", "target.view", "losses.DiceCoeff.union.float", "torch.sum", "torch.sum", "losses.DiceCoeff.inter.float"], "methods", ["None"], ["def", "forward", "(", "self", ",", "input", ",", "target", ")", ":", "\n", "        ", "self", ".", "save_for_backward", "(", "input", ",", "target", ")", "\n", "eps", "=", "0.0001", "\n", "self", ".", "inter", "=", "torch", ".", "dot", "(", "input", ".", "view", "(", "-", "1", ")", ",", "target", ".", "view", "(", "-", "1", ")", ")", "\n", "self", ".", "union", "=", "torch", ".", "sum", "(", "input", ")", "+", "torch", ".", "sum", "(", "target", ")", "+", "eps", "\n", "\n", "t", "=", "(", "2", "*", "self", ".", "inter", ".", "float", "(", ")", "+", "eps", ")", "/", "self", ".", "union", ".", "float", "(", ")", "\n", "return", "t", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.utils.losses.DiceCoeff.backward": [[20, 32], ["None"], "methods", ["None"], ["", "def", "backward", "(", "self", ",", "grad_output", ")", ":", "\n", "\n", "        ", "input", ",", "target", "=", "self", ".", "saved_variables", "\n", "grad_input", "=", "grad_target", "=", "None", "\n", "\n", "if", "self", ".", "needs_input_grad", "[", "0", "]", ":", "\n", "            ", "grad_input", "=", "grad_output", "*", "2", "*", "(", "target", "*", "self", ".", "union", "-", "self", ".", "inter", ")", "/", "(", "self", ".", "union", "*", "self", ".", "union", ")", "\n", "", "if", "self", ".", "needs_input_grad", "[", "1", "]", ":", "\n", "            ", "grad_target", "=", "None", "\n", "\n", "", "return", "grad_input", ",", "grad_target", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.utils.losses.dice_coeff": [[34, 45], ["enumerate", "torch.FloatTensor().cuda().zero_", "torch.FloatTensor().zero_", "zip", "losses.DiceCoeff.forward", "torch.FloatTensor().cuda", "torch.FloatTensor", "losses.DiceCoeff", "torch.FloatTensor"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.my_models.triple_models.TripleUnet.forward"], ["", "", "def", "dice_coeff", "(", "input", ",", "target", ")", ":", "\n", "    ", "\"\"\"Dice coeff for batches\"\"\"", "\n", "if", "input", ".", "is_cuda", ":", "\n", "        ", "s", "=", "torch", ".", "FloatTensor", "(", "1", ")", ".", "cuda", "(", ")", ".", "zero_", "(", ")", "\n", "", "else", ":", "\n", "        ", "s", "=", "torch", ".", "FloatTensor", "(", "1", ")", ".", "zero_", "(", ")", "\n", "\n", "", "for", "i", ",", "c", "in", "enumerate", "(", "zip", "(", "input", ",", "target", ")", ")", ":", "\n", "        ", "s", "=", "s", "+", "DiceCoeff", "(", ")", ".", "forward", "(", "c", "[", "0", "]", ",", "c", "[", "1", "]", ")", "\n", "\n", "", "return", "s", "/", "(", "i", "+", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.utils.losses.iou_pytorch": [[49, 74], ["pred.int.squeeze", "gt.int.squeeze", "pred.int.int", "gt.int.int"], "function", ["None"], ["def", "iou_pytorch", "(", "pred", ":", "torch", ".", "Tensor", ",", "gt", ":", "torch", ".", "Tensor", ")", ":", "\n", "# You can comment out this line if you are passing tensors of equal shape", "\n", "# But if you are passing output from UNet or something it will most probably", "\n", "# be with the BATCH x 1 x H x W shape", "\n", "\n", "#print(\"##Pred = \", pred)", "\n", "#print(\"##Pred - shape=\", pred.shape)", "\n", "#print(\"##gt =\", gt)", "\n", "#print(\"##gt.shape=\", gt.shape)", "\n", "\n", "    ", "pred", "=", "pred", ".", "squeeze", "(", "1", ")", "# BATCH x 1 x H x W => BATCH x H x W", "\n", "gt", "=", "gt", ".", "squeeze", "(", "1", ")", "# BATCH x 1 x H x W => BATCH x H x W", "\n", "\n", "pred", "=", "pred", ".", "int", "(", ")", "\n", "gt", "=", "gt", ".", "int", "(", ")", "\n", "\n", "intersection", "=", "(", "pred", "&", "gt", ")", ".", "float", "(", ")", ".", "sum", "(", "(", "1", ",", "2", ")", ")", "# Will be zero if Truth=0 or Prediction=0", "\n", "union", "=", "(", "pred", "|", "gt", ")", ".", "float", "(", ")", ".", "sum", "(", "(", "1", ",", "2", ")", ")", "# Will be zzero if both are 0", "\n", "\n", "iou", "=", "(", "intersection", "+", "SMOOTH", ")", "/", "(", "union", "+", "SMOOTH", ")", "# We smooth our devision to avoid 0/0", "\n", "\n", "#thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds", "\n", "#print(\"iou pytorch=\", iou)", "\n", "\n", "return", "iou", "#thresholded  # Or thresholded.mean() if you are interested in average across the batch", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.utils.losses.iou_numpy": [[79, 90], ["outputs.squeeze.squeeze", "numpy.ceil", "numpy.clip"], "function", ["None"], ["", "def", "iou_numpy", "(", "outputs", ":", "np", ".", "array", ",", "labels", ":", "np", ".", "array", ")", ":", "\n", "    ", "outputs", "=", "outputs", ".", "squeeze", "(", "1", ")", "\n", "\n", "intersection", "=", "(", "outputs", "&", "labels", ")", ".", "sum", "(", "(", "1", ",", "2", ")", ")", "\n", "union", "=", "(", "outputs", "|", "labels", ")", ".", "sum", "(", "(", "1", ",", "2", ")", ")", "\n", "\n", "iou", "=", "(", "intersection", "+", "SMOOTH", ")", "/", "(", "union", "+", "SMOOTH", ")", "\n", "\n", "thresholded", "=", "np", ".", "ceil", "(", "np", ".", "clip", "(", "20", "*", "(", "iou", "-", "0.5", ")", ",", "0", ",", "10", ")", ")", "/", "10", "\n", "\n", "return", "thresholded", "# Or thresholded.mean()", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.utils.losses.iou_sklearn": [[92, 113], ["pred.cpu().numpy.reshape", "gt.cpu().numpy.reshape", "pred.cpu().numpy.int", "gt.cpu().numpy.int", "pred.cpu().numpy.cpu().numpy", "gt.cpu().numpy.cpu().numpy", "sklearn.metrics.jaccard_score", "pred.cpu().numpy.cpu", "gt.cpu().numpy.cpu"], "function", ["None"], ["", "def", "iou_sklearn", "(", "pred", ":", "torch", ".", "Tensor", ",", "gt", ":", "torch", ".", "Tensor", ",", "avg_type", ":", "str", ")", ":", "\n", "#print(\"Pred = \", pred)", "\n", "# print(\"Pred - shape=\", pred.shape)", "\n", "# print(\"gt =\", gt)", "\n", "# print(\"gt.shape=\", gt.shape)", "\n", "\n", "# convert 1x1xHxW into HxW", "\n", "    ", "pred", "=", "pred", ".", "reshape", "(", "(", "pred", ".", "shape", "[", "2", "]", ",", "pred", ".", "shape", "[", "3", "]", ")", ")", "\n", "gt", "=", "gt", ".", "reshape", "(", "(", "gt", ".", "shape", "[", "2", "]", ",", "gt", ".", "shape", "[", "3", "]", ")", ")", "\n", "\n", "# convert into int", "\n", "pred", "=", "pred", ".", "int", "(", ")", "\n", "gt", "=", "gt", ".", "int", "(", ")", "\n", "\n", "# convert inot numpy", "\n", "pred", "=", "pred", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "gt", "=", "gt", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "iou", "=", "jaccard_score", "(", "gt", ",", "pred", ",", "average", "=", "avg_type", ")", "\n", "#print(\"IOU skalearn=\", iou)", "\n", "return", "iou", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.utils.losses.dice_using_sklearn": [[115, 141], ["pred.cpu().numpy.reshape", "gt.cpu().numpy.reshape", "pred.cpu().numpy.int", "gt.cpu().numpy.int", "pred.cpu().numpy.reshape", "pred.cpu().numpy.reshape", "pred.cpu().numpy.cpu().numpy", "gt.cpu().numpy.cpu().numpy", "sklearn.metrics.f1_score", "pred.cpu().numpy.cpu", "gt.cpu().numpy.cpu"], "function", ["None"], ["", "def", "dice_using_sklearn", "(", "pred", ":", "torch", ".", "Tensor", ",", "gt", ":", "torch", ".", "Tensor", ")", ":", "\n", "#print(\"Pred = \", pred)", "\n", "#print(\"Pred - shape=\", pred.shape)", "\n", "#print(\"gt =\", gt)", "\n", "#print(\"gt.shape=\", gt.shape)", "\n", "\n", "# convert 1x1xHxW into HxW", "\n", "    ", "pred", "=", "pred", ".", "reshape", "(", "(", "pred", ".", "shape", "[", "2", "]", ",", "pred", ".", "shape", "[", "3", "]", ")", ")", "\n", "gt", "=", "gt", ".", "reshape", "(", "(", "gt", ".", "shape", "[", "2", "]", ",", "gt", ".", "shape", "[", "3", "]", ")", ")", "\n", "\n", "# convert into int", "\n", "pred", "=", "pred", ".", "int", "(", ")", "\n", "gt", "=", "gt", ".", "int", "(", ")", "\n", "\n", "# flat 2d to 1d", "\n", "pred", "=", "pred", ".", "reshape", "(", "-", "1", ")", "\n", "gt", "=", "pred", ".", "reshape", "(", "-", "1", ")", "\n", "\n", "# convert inot numpy", "\n", "pred", "=", "pred", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "gt", "=", "gt", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "\n", "dice", "=", "f1_score", "(", "gt", ",", "pred", ")", "\n", "#print(\"IOU skalearn=\", iou)", "\n", "return", "dice", "", "", ""]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.my_models.triple_models.TripleUnet.__init__": [[10, 44], ["segmentation_models_pytorch.UnetPlusPlus.__init__", "segmentation_models_pytorch.Unet", "segmentation_models_pytorch.Unet", "segmentation_models_pytorch.Unet"], "methods", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.dataset.Dataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "\n", "encoder_name", ":", "str", "=", "\"resnet34\"", ",", "\n", "encoder_depth", ":", "int", "=", "5", ",", "\n", "encoder_weights", ":", "Optional", "[", "str", "]", "=", "\"imagenet\"", ",", "\n", "decoder_use_batchnorm", ":", "bool", "=", "True", ",", "\n", "decoder_channels", ":", "List", "[", "int", "]", "=", "(", "256", ",", "128", ",", "64", ",", "32", ",", "16", ")", ",", "\n", "decoder_attention_type", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "in_channels", ":", "int", "=", "3", ",", "\n", "classes", ":", "int", "=", "1", ",", "\n", "activation", ":", "Optional", "[", "Union", "[", "str", ",", "callable", "]", "]", "=", "None", ",", "\n", "aux_params", ":", "Optional", "[", "dict", "]", "=", "None", ",", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "in_model_1", "=", "smp", ".", "Unet", "(", "\n", "encoder_name", "=", "encoder_name", ",", "\n", "in_channels", "=", "in_channels", ",", "\n", "encoder_weights", "=", "encoder_weights", ",", "\n", "classes", "=", "classes", ",", "\n", "activation", "=", "activation", ",", ")", "\n", "\n", "self", ".", "in_model_2", "=", "smp", ".", "Unet", "(", "\n", "encoder_name", "=", "encoder_name", ",", "\n", "in_channels", "=", "in_channels", ",", "\n", "encoder_weights", "=", "encoder_weights", ",", "\n", "classes", "=", "classes", ",", "\n", "activation", "=", "activation", ",", ")", "\n", "\n", "self", ".", "out_model", "=", "smp", ".", "Unet", "(", "\n", "encoder_name", "=", "encoder_name", ",", "\n", "in_channels", "=", "4", ",", "\n", "encoder_weights", "=", "encoder_weights", ",", "\n", "classes", "=", "classes", ",", "\n", "activation", "=", "activation", ",", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.my_models.triple_models.TripleUnet.forward": [[45, 55], ["triple_models.TripleUnet.in_model_1", "triple_models.TripleUnet.in_model_2", "torch.cat", "triple_models.TripleUnet.out_model"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "        ", "mask_1", "=", "self", ".", "in_model_1", "(", "x", ")", "\n", "mask_2", "=", "self", ".", "in_model_2", "(", "x", ")", "\n", "\n", "mask_concat", "=", "torch", ".", "cat", "(", "(", "mask_1", ",", "mask_2", ")", ",", "1", ")", "\n", "\n", "mask", "=", "self", ".", "out_model", "(", "mask_concat", ")", "\n", "\n", "return", "mask", "", "", "", ""]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.dataset.Dataset.__init__": [[21, 43], ["pyra_pytorch.PYRADatasetFromDF"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "df", ",", "\n", "classes", "=", "[", "0", ",", "255", "]", ",", "\n", "grid_sizes", "=", "[", "256", "]", ",", "\n", "augmentation", "=", "None", ",", "\n", "preprocessing", "=", "None", ",", "\n", "pyra", "=", "False", "\n", ")", ":", "\n", "#self.ids = os.listdir(images_dir)", "\n", "#self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]", "\n", "#self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]", "\n", "        ", "self", ".", "grid_sizes", "=", "grid_sizes", "\n", "\n", "self", ".", "pyra_dataset", "=", "pyra_pytorch", ".", "PYRADatasetFromDF", "(", "df", ",", "grid_sizes", "=", "grid_sizes", ")", "\n", "\n", "# convert str names to class values on masks", "\n", "self", ".", "class_values", "=", "classes", "\n", "\n", "self", ".", "augmentation", "=", "augmentation", "\n", "self", ".", "preprocessing", "=", "preprocessing", "\n", "self", ".", "pyra", "=", "pyra", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.dataset.Dataset.__getitem__": [[44, 73], ["numpy.expand_dims", "numpy.stack().astype", "dataset.Dataset.augmentation", "numpy.concatenate", "dataset.Dataset.preprocessing", "numpy.stack"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "\n", "# read data", "\n", "        ", "data", "=", "self", ".", "pyra_dataset", "[", "i", "]", "\n", "#image = cv2.imread(self.images_fps[i])", "\n", "#image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)", "\n", "#mask = cv2.imread(self.masks_fps[i], 0)", "\n", "image", "=", "data", "[", "\"img\"", "]", "\n", "grid", "=", "np", ".", "expand_dims", "(", "data", "[", "\"grid_encode\"", "]", ",", "axis", "=", "2", ")", "\n", "mask", "=", "data", "[", "\"mask\"", "]", "\n", "\n", "# extract certain classes from mask (e.g. cars)", "\n", "masks", "=", "[", "(", "mask", "==", "v", ")", "for", "v", "in", "self", ".", "class_values", "]", "\n", "mask", "=", "np", ".", "stack", "(", "masks", ",", "axis", "=", "-", "1", ")", ".", "astype", "(", "'float'", ")", "\n", "\n", "# apply augmentations", "\n", "if", "self", ".", "augmentation", ":", "\n", "            ", "sample", "=", "self", ".", "augmentation", "(", "image", "=", "image", ",", "mask", "=", "mask", ")", "\n", "image", ",", "mask", "=", "sample", "[", "'image'", "]", ",", "sample", "[", "'mask'", "]", "\n", "\n", "", "if", "self", ".", "pyra", ":", "\n", "            ", "image", "=", "np", ".", "concatenate", "(", "[", "image", ",", "grid", "]", ",", "axis", "=", "2", ")", "\n", "\n", "# apply preprocessing", "\n", "", "if", "self", ".", "preprocessing", ":", "\n", "            ", "sample", "=", "self", ".", "preprocessing", "(", "image", "=", "image", ",", "mask", "=", "mask", ")", "\n", "image", ",", "mask", "=", "sample", "[", "'image'", "]", ",", "sample", "[", "'mask'", "]", "\n", "\n", "", "return", "image", ",", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.dataset.Dataset.__len__": [[74, 76], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "pyra_dataset", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.df_from_csv_file_array": [[8, 18], ["pandas.DataFrame", "pandas.read_csv", "df.append.append"], "function", ["None"], ["def", "df_from_csv_file_array", "(", "csv_file_arrya", ")", ":", "\n", "\n", "    ", "df", "=", "pd", ".", "DataFrame", "(", "columns", "=", "[", "\"image\"", ",", "\"path\"", "]", ")", "\n", "\n", "for", "csv", "in", "csv_file_arrya", ":", "\n", "        ", "temp_df", "=", "pd", ".", "read_csv", "(", "csv", ")", "\n", "\n", "df", "=", "df", ".", "append", "(", "temp_df", ")", "\n", "\n", "", "return", "df", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.get_training_augmentation": [[21, 61], ["albumentations.Compose", "albumentations.HorizontalFlip", "albumentations.ShiftScaleRotate", "albumentations.PadIfNeeded", "albumentations.Resize", "albumentations.IAAAdditiveGaussianNoise", "albumentations.IAAPerspective", "albumentations.OneOf", "albumentations.OneOf", "albumentations.OneOf", "albumentations.CLAHE", "albumentations.RandomBrightness", "albumentations.RandomGamma", "albumentations.IAASharpen", "albumentations.Blur", "albumentations.MotionBlur", "albumentations.RandomContrast", "albumentations.HueSaturationValue"], "function", ["None"], ["", "def", "get_training_augmentation", "(", ")", ":", "\n", "    ", "train_transform", "=", "[", "\n", "\n", "albu", ".", "HorizontalFlip", "(", "p", "=", "0.5", ")", ",", "\n", "\n", "albu", ".", "ShiftScaleRotate", "(", "scale_limit", "=", "0.5", ",", "rotate_limit", "=", "0", ",", "shift_limit", "=", "0.1", ",", "p", "=", "1", ",", "border_mode", "=", "0", ")", ",", "\n", "\n", "albu", ".", "PadIfNeeded", "(", "min_height", "=", "256", ",", "min_width", "=", "256", ",", "always_apply", "=", "True", ",", "border_mode", "=", "0", ")", ",", "\n", "albu", ".", "Resize", "(", "height", "=", "256", ",", "width", "=", "256", ",", "always_apply", "=", "True", ")", ",", "\n", "\n", "albu", ".", "IAAAdditiveGaussianNoise", "(", "p", "=", "0.2", ")", ",", "\n", "albu", ".", "IAAPerspective", "(", "p", "=", "0.5", ")", ",", "\n", "\n", "albu", ".", "OneOf", "(", "\n", "[", "\n", "albu", ".", "CLAHE", "(", "p", "=", "1", ")", ",", "\n", "albu", ".", "RandomBrightness", "(", "p", "=", "1", ")", ",", "\n", "albu", ".", "RandomGamma", "(", "p", "=", "1", ")", ",", "\n", "]", ",", "\n", "p", "=", "0.9", ",", "\n", ")", ",", "\n", "\n", "albu", ".", "OneOf", "(", "\n", "[", "\n", "albu", ".", "IAASharpen", "(", "p", "=", "1", ")", ",", "\n", "albu", ".", "Blur", "(", "blur_limit", "=", "3", ",", "p", "=", "1", ")", ",", "\n", "albu", ".", "MotionBlur", "(", "blur_limit", "=", "3", ",", "p", "=", "1", ")", ",", "\n", "]", ",", "\n", "p", "=", "0.9", ",", "\n", ")", ",", "\n", "\n", "albu", ".", "OneOf", "(", "\n", "[", "\n", "albu", ".", "RandomContrast", "(", "p", "=", "1", ")", ",", "\n", "albu", ".", "HueSaturationValue", "(", "p", "=", "1", ")", ",", "\n", "]", ",", "\n", "p", "=", "0.9", ",", "\n", ")", ",", "\n", "]", "\n", "return", "albu", ".", "Compose", "(", "train_transform", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.get_validation_augmentation": [[64, 71], ["albumentations.Compose", "albumentations.PadIfNeeded", "albumentations.Resize"], "function", ["None"], ["", "def", "get_validation_augmentation", "(", ")", ":", "\n", "    ", "\"\"\"Add paddings to make image shape divisible by 32\"\"\"", "\n", "test_transform", "=", "[", "\n", "albu", ".", "PadIfNeeded", "(", "min_height", "=", "256", ",", "min_width", "=", "256", ",", "always_apply", "=", "True", ",", "border_mode", "=", "0", ")", ",", "\n", "albu", ".", "Resize", "(", "height", "=", "256", ",", "width", "=", "256", ",", "always_apply", "=", "True", ")", ",", "\n", "]", "\n", "return", "albu", ".", "Compose", "(", "test_transform", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.to_tensor": [[74, 76], ["x.transpose().astype", "x.transpose"], "function", ["None"], ["", "def", "to_tensor", "(", "x", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "x", ".", "transpose", "(", "2", ",", "0", ",", "1", ")", ".", "astype", "(", "'float32'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.get_preprocessing": [[79, 97], ["_transform.append", "albumentations.Compose", "_transform.append", "albumentations.Lambda", "albumentations.Lambda"], "function", ["None"], ["", "def", "get_preprocessing", "(", "preprocessing_fn", ")", ":", "\n", "    ", "\"\"\"Construct preprocessing transform\n    \n    Args:\n        preprocessing_fn (callbale): data normalization function \n            (can be specific for each pretrained neural network)\n    Return:\n        transform: albumentations.Compose\n    \n    \"\"\"", "\n", "\n", "_transform", "=", "[", "]", "\n", "if", "preprocessing_fn", ":", "\n", "        ", "_transform", ".", "append", "(", "albu", ".", "Lambda", "(", "image", "=", "preprocessing_fn", ")", ")", "\n", "", "_transform", ".", "append", "(", "albu", ".", "Lambda", "(", "image", "=", "to_tensor", ",", "mask", "=", "to_tensor", ")", ")", "\n", "\n", "\n", "return", "albu", ".", "Compose", "(", "_transform", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_data": [[100, 135], ["prepare_data.df_from_csv_file_array", "prepare_data.df_from_csv_file_array", "data.dataset.Dataset", "data.dataset.Dataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "print", "print", "len", "len", "prepare_data.get_training_augmentation", "prepare_data.get_preprocessing", "prepare_data.get_validation_augmentation", "prepare_data.get_preprocessing"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.df_from_csv_file_array", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.df_from_csv_file_array", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.get_training_augmentation", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.get_preprocessing", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.get_validation_augmentation", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.get_preprocessing"], ["", "def", "prepare_data", "(", "opt", ",", "preprocessing_fn", ")", ":", "\n", "\n", "\n", "\n", "    ", "train_df", "=", "df_from_csv_file_array", "(", "opt", ".", "train_CSVs", ")", "\n", "val_df", "=", "df_from_csv_file_array", "(", "opt", ".", "val_CSVs", ")", "\n", "\n", "\n", "train_dataset", "=", "Dataset", "(", "\n", "train_df", ",", "\n", "grid_sizes", "=", "opt", ".", "grid_sizes_train", ",", "\n", "augmentation", "=", "get_training_augmentation", "(", ")", ",", "\n", "preprocessing", "=", "get_preprocessing", "(", "preprocessing_fn", ")", ",", "\n", "classes", "=", "opt", ".", "classes", ",", "\n", "pyra", "=", "opt", ".", "pyra", "\n", ")", "\n", "\n", "valid_dataset", "=", "Dataset", "(", "\n", "val_df", ",", "\n", "grid_sizes", "=", "opt", ".", "grid_sizes_val", ",", "\n", "augmentation", "=", "get_validation_augmentation", "(", ")", ",", "\n", "preprocessing", "=", "get_preprocessing", "(", "preprocessing_fn", ")", ",", "\n", "classes", "=", "opt", ".", "classes", ",", "\n", "pyra", "=", "opt", ".", "pyra", "\n", ")", "\n", "\n", "train_loader", "=", "DataLoader", "(", "train_dataset", ",", "batch_size", "=", "opt", ".", "bs", ",", "shuffle", "=", "True", ",", "num_workers", "=", "12", ")", "\n", "valid_loader", "=", "DataLoader", "(", "valid_dataset", ",", "batch_size", "=", "opt", ".", "val_bs", ",", "shuffle", "=", "False", ",", "num_workers", "=", "4", ")", "\n", "\n", "\n", "\n", "print", "(", "\"dataset train=\"", ",", "len", "(", "train_dataset", ")", ")", "\n", "print", "(", "\"dataset val=\"", ",", "len", "(", "valid_dataset", ")", ")", "\n", "\n", "return", "train_loader", ",", "valid_loader", "\n", "\n"]], "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.prepare_test_data": [[136, 153], ["prepare_data.df_from_csv_file_array", "data.dataset.Dataset", "print", "len", "prepare_data.get_validation_augmentation", "prepare_data.get_preprocessing"], "function", ["home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.df_from_csv_file_array", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.get_validation_augmentation", "home.repos.pwc.inspect_result.vlbthambawita_divergent-nets.data.prepare_data.get_preprocessing"], ["", "def", "prepare_test_data", "(", "opt", ",", "preprocessing_fn", ")", ":", "\n", "\n", "    ", "test_df", "=", "df_from_csv_file_array", "(", "opt", ".", "test_CSVs", ")", "\n", "\n", "# test dataset without transformations for image visualization", "\n", "test_dataset", "=", "Dataset", "(", "\n", "test_df", ",", "\n", "grid_sizes", "=", "opt", ".", "grid_sizes_test", ",", "\n", "augmentation", "=", "get_validation_augmentation", "(", ")", ",", "\n", "preprocessing", "=", "get_preprocessing", "(", "preprocessing_fn", ")", ",", "\n", "classes", "=", "opt", ".", "classes", ",", "\n", "pyra", "=", "opt", ".", "pyra", "\n", ")", "\n", "\n", "print", "(", "\"Test dataset size=\"", ",", "len", "(", "test_dataset", ")", ")", "\n", "\n", "return", "test_dataset", "\n", "", ""]]}