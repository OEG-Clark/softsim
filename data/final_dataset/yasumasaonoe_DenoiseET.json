{"home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.models.ModelBase.__init__": [[25, 30], ["torch.Module.__init__", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["  ", "def", "__init__", "(", "self", ",", "args", ",", "answer_num", ")", ":", "\n", "    ", "super", "(", "ModelBase", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "multitask", "=", "args", ".", "multitask", "\n", "self", ".", "loss_func", "=", "nn", ".", "BCEWithLogitsLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.models.ModelBase.sorted_rnn": [[31, 39], ["model_utils.sort_batch_by_length", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "models.ModelBase.rnn"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.sort_batch_by_length", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.models.ModelBase.rnn"], ["", "def", "sorted_rnn", "(", "self", ",", "sequences", ",", "sequence_lengths", ",", "rnn", ")", ":", "\n", "    ", "sorted_inputs", ",", "sorted_sequence_lengths", ",", "restoration_indices", "=", "sort_batch_by_length", "(", "sequences", ",", "sequence_lengths", ")", "\n", "packed_sequence_input", "=", "pack_padded_sequence", "(", "sorted_inputs", ",", "\n", "sorted_sequence_lengths", ".", "data", ".", "long", "(", ")", ".", "tolist", "(", ")", ",", "\n", "batch_first", "=", "True", ")", "\n", "packed_sequence_output", ",", "_", "=", "rnn", "(", "packed_sequence_input", ",", "None", ")", "\n", "unpacked_sequence_tensor", ",", "_", "=", "pad_packed_sequence", "(", "packed_sequence_output", ",", "batch_first", "=", "True", ")", "\n", "return", "unpacked_sequence_tensor", ".", "index_select", "(", "0", ",", "restoration_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.models.ModelBase.rnn": [[40, 43], ["lstm", "outputs.contiguous"], "methods", ["None"], ["", "def", "rnn", "(", "self", ",", "sequences", ",", "lstm", ")", ":", "\n", "    ", "outputs", ",", "_", "=", "lstm", "(", "sequences", ")", "\n", "return", "outputs", ".", "contiguous", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.models.ModelBase.define_loss": [[44, 90], ["torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "models.ModelBase.loss_func", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "gen_targets.index_select", "models.ModelBase.loss_func", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "fine_targets.index_select", "models.ModelBase.loss_func", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "finer_targets.index_select", "models.ModelBase.loss_func", "torch.min", "torch.min", "torch.min", "torch.min", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.min", "torch.min", "torch.min", "torch.min", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.min", "torch.min", "torch.min", "torch.min", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["None"], ["", "def", "define_loss", "(", "self", ",", "logits", ",", "targets", ",", "data_type", ")", ":", "\n", "    ", "if", "not", "self", ".", "multitask", "or", "data_type", "==", "'onto'", ":", "\n", "      ", "loss", "=", "self", ".", "loss_func", "(", "logits", ",", "targets", ")", "\n", "return", "loss", "\n", "", "if", "data_type", "==", "'wiki'", ":", "\n", "      ", "gen_cutoff", ",", "fine_cutoff", ",", "final_cutoff", "=", "constant", ".", "ANSWER_NUM_DICT", "[", "'gen'", "]", ",", "constant", ".", "ANSWER_NUM_DICT", "[", "'kb'", "]", ",", "constant", ".", "ANSWER_NUM_DICT", "[", "data_type", "]", "\n", "", "else", ":", "\n", "      ", "gen_cutoff", ",", "fine_cutoff", ",", "final_cutoff", "=", "constant", ".", "ANSWER_NUM_DICT", "[", "'gen'", "]", ",", "constant", ".", "ANSWER_NUM_DICT", "[", "'kb'", "]", ",", "None", "\n", "", "loss", "=", "0.0", "\n", "comparison_tensor", "=", "torch", ".", "Tensor", "(", "[", "1.0", "]", ")", ".", "cuda", "(", ")", "\n", "gen_targets", "=", "targets", "[", ":", ",", ":", "gen_cutoff", "]", "\n", "fine_targets", "=", "targets", "[", ":", ",", "gen_cutoff", ":", "fine_cutoff", "]", "\n", "gen_target_sum", "=", "torch", ".", "sum", "(", "gen_targets", ",", "1", ")", "\n", "fine_target_sum", "=", "torch", ".", "sum", "(", "fine_targets", ",", "1", ")", "\n", "\n", "if", "torch", ".", "sum", "(", "gen_target_sum", ".", "data", ")", ">", "0", ":", "\n", "      ", "gen_mask", "=", "torch", ".", "squeeze", "(", "torch", ".", "nonzero", "(", "torch", ".", "min", "(", "gen_target_sum", ".", "data", ",", "comparison_tensor", ")", ")", ",", "dim", "=", "1", ")", "\n", "gen_logit_masked", "=", "logits", "[", ":", ",", ":", "gen_cutoff", "]", "[", "gen_mask", ",", ":", "]", "\n", "gen_mask", "=", "torch", ".", "autograd", ".", "Variable", "(", "gen_mask", ")", ".", "cuda", "(", ")", "\n", "gen_target_masked", "=", "gen_targets", ".", "index_select", "(", "0", ",", "gen_mask", ")", "\n", "gen_loss", "=", "self", ".", "loss_func", "(", "gen_logit_masked", ",", "gen_target_masked", ")", "\n", "loss", "+=", "gen_loss", "\n", "", "if", "torch", ".", "sum", "(", "fine_target_sum", ".", "data", ")", ">", "0", ":", "\n", "      ", "fine_mask", "=", "torch", ".", "squeeze", "(", "torch", ".", "nonzero", "(", "torch", ".", "min", "(", "fine_target_sum", ".", "data", ",", "comparison_tensor", ")", ")", ",", "dim", "=", "1", ")", "\n", "fine_logit_masked", "=", "logits", "[", ":", ",", "gen_cutoff", ":", "fine_cutoff", "]", "[", "fine_mask", ",", ":", "]", "\n", "fine_mask", "=", "torch", ".", "autograd", ".", "Variable", "(", "fine_mask", ")", ".", "cuda", "(", ")", "\n", "fine_target_masked", "=", "fine_targets", ".", "index_select", "(", "0", ",", "fine_mask", ")", "\n", "fine_loss", "=", "self", ".", "loss_func", "(", "fine_logit_masked", ",", "fine_target_masked", ")", "\n", "loss", "+=", "fine_loss", "\n", "\n", "", "if", "not", "data_type", "==", "'kb'", ":", "\n", "      ", "if", "final_cutoff", ":", "\n", "        ", "finer_targets", "=", "targets", "[", ":", ",", "fine_cutoff", ":", "final_cutoff", "]", "\n", "logit_masked", "=", "logits", "[", ":", ",", "fine_cutoff", ":", "final_cutoff", "]", "\n", "", "else", ":", "\n", "        ", "logit_masked", "=", "logits", "[", ":", ",", "fine_cutoff", ":", "]", "\n", "finer_targets", "=", "targets", "[", ":", ",", "fine_cutoff", ":", "]", "\n", "", "if", "torch", ".", "sum", "(", "torch", ".", "sum", "(", "finer_targets", ",", "1", ")", ".", "data", ")", ">", "0", ":", "\n", "        ", "finer_mask", "=", "torch", ".", "squeeze", "(", "torch", ".", "nonzero", "(", "torch", ".", "min", "(", "torch", ".", "sum", "(", "finer_targets", ",", "1", ")", ".", "data", ",", "comparison_tensor", ")", ")", ",", "dim", "=", "1", ")", "\n", "finer_mask", "=", "torch", ".", "autograd", ".", "Variable", "(", "finer_mask", ")", ".", "cuda", "(", ")", "\n", "finer_target_masked", "=", "finer_targets", ".", "index_select", "(", "0", ",", "finer_mask", ")", "\n", "logit_masked", "=", "logit_masked", "[", "finer_mask", ",", ":", "]", "\n", "layer_loss", "=", "self", ".", "loss_func", "(", "logit_masked", ",", "finer_target_masked", ")", "\n", "loss", "+=", "layer_loss", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.models.ModelBase.forward": [[91, 93], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "feed_dict", ",", "data_type", ")", ":", "\n", "    ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.models.ETModel.__init__": [[96, 140], ["models.ModelBase.__init__", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "model_utils.SelfAttentiveSum", "torch.LSTM", "torch.LSTM", "torch.Linear", "torch.Linear", "torch.Sigmoid", "torch.Sigmoid", "model_utils.ELMoWeightedSum", "model_utils.SelfAttentiveSum", "model_utils.CNN", "torch.LSTM", "torch.LSTM", "model_utils.SelfAttentiveSum", "print", "model_utils.MultiSimpleDecoder", "model_utils.SimpleDecoder"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["  ", "def", "__init__", "(", "self", ",", "args", ",", "answer_num", ")", ":", "\n", "    ", "super", "(", "ETModel", ",", "self", ")", ".", "__init__", "(", "args", ",", "answer_num", ")", "\n", "self", ".", "output_dim", "=", "args", ".", "rnn_dim", "*", "2", "\n", "self", ".", "mention_dropout", "=", "nn", ".", "Dropout", "(", "args", ".", "mention_dropout", ")", "\n", "self", ".", "input_dropout", "=", "nn", ".", "Dropout", "(", "args", ".", "input_dropout", ")", "\n", "self", ".", "dim_hidden", "=", "args", ".", "dim_hidden", "\n", "self", ".", "embed_dim", "=", "1024", "\n", "self", ".", "mention_dim", "=", "1024", "\n", "self", ".", "headword_dim", "=", "1024", "\n", "self", ".", "enhanced_mention", "=", "args", ".", "enhanced_mention", "\n", "\n", "self", ".", "add_headword_emb", "=", "args", ".", "add_headword_emb", "\n", "self", ".", "mention_lstm", "=", "args", ".", "mention_lstm", "\n", "\n", "if", "args", ".", "enhanced_mention", ":", "\n", "      ", "self", ".", "head_attentive_sum", "=", "SelfAttentiveSum", "(", "self", ".", "mention_dim", ",", "1", ")", "\n", "self", ".", "cnn", "=", "CNN", "(", ")", "\n", "self", ".", "mention_dim", "+=", "50", "\n", "", "self", ".", "output_dim", "+=", "self", ".", "mention_dim", "\n", "\n", "if", "self", ".", "add_headword_emb", ":", "\n", "      ", "self", ".", "output_dim", "+=", "self", ".", "headword_dim", "\n", "\n", "# Defining LSTM here.  ", "\n", "", "self", ".", "attentive_sum", "=", "SelfAttentiveSum", "(", "args", ".", "rnn_dim", "*", "2", ",", "100", ")", "\n", "self", ".", "lstm", "=", "nn", ".", "LSTM", "(", "self", ".", "embed_dim", "+", "50", ",", "args", ".", "rnn_dim", ",", "bidirectional", "=", "True", ",", "\n", "batch_first", "=", "True", ")", "\n", "self", ".", "token_mask", "=", "nn", ".", "Linear", "(", "4", ",", "50", ")", "\n", "\n", "if", "self", ".", "mention_lstm", ":", "\n", "      ", "self", ".", "lstm_mention", "=", "nn", ".", "LSTM", "(", "self", ".", "embed_dim", ",", "self", ".", "embed_dim", "//", "2", ",", "bidirectional", "=", "True", ",", "\n", "batch_first", "=", "True", ")", "\n", "self", ".", "mention_attentive_sum", "=", "SelfAttentiveSum", "(", "self", ".", "embed_dim", ",", "1", ")", "\n", "\n", "", "self", ".", "sigmoid_fn", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "self", ".", "goal", "=", "args", ".", "goal", "\n", "\n", "if", "args", ".", "data_setup", "==", "'joint'", "and", "args", ".", "multitask", ":", "\n", "      ", "print", "(", "\"Multi-task learning\"", ")", "\n", "self", ".", "decoder", "=", "MultiSimpleDecoder", "(", "self", ".", "output_dim", ")", "\n", "", "else", ":", "\n", "      ", "self", ".", "decoder", "=", "SimpleDecoder", "(", "self", ".", "output_dim", ",", "answer_num", ")", "\n", "\n", "", "self", ".", "weighted_sum", "=", "ELMoWeightedSum", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.models.ETModel.forward": [[141, 172], ["models.ETModel.weighted_sum", "models.ETModel.token_mask", "token_mask_embed.view.view.view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "models.ETModel.input_dropout", "models.ETModel.sorted_rnn", "models.ETModel.attentive_sum", "models.ETModel.weighted_sum", "models.ETModel.mention_dropout", "models.ETModel.decoder", "models.ETModel.define_loss", "feed_dict[].view", "models.ETModel.cnn", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "models.ETModel.weighted_sum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "models.ETModel.size", "models.ETModel.sorted_rnn", "models.ETModel.mention_attentive_sum", "models.ETModel.head_attentive_sum"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.models.ModelBase.sorted_rnn", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.models.ModelBase.define_loss", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.models.ModelBase.sorted_rnn"], ["", "def", "forward", "(", "self", ",", "feed_dict", ",", "data_type", ")", ":", "\n", "    ", "token_embed", "=", "self", ".", "weighted_sum", "(", "feed_dict", "[", "'token_embed'", "]", ")", "\n", "token_mask_embed", "=", "self", ".", "token_mask", "(", "feed_dict", "[", "'token_bio'", "]", ".", "view", "(", "-", "1", ",", "4", ")", ")", "\n", "token_mask_embed", "=", "token_mask_embed", ".", "view", "(", "token_embed", ".", "size", "(", ")", "[", "0", "]", ",", "-", "1", ",", "50", ")", "# location embedding", "\n", "token_embed", "=", "torch", ".", "cat", "(", "(", "token_embed", ",", "token_mask_embed", ")", ",", "2", ")", "\n", "token_embed", "=", "self", ".", "input_dropout", "(", "token_embed", ")", "\n", "context_rep", "=", "self", ".", "sorted_rnn", "(", "token_embed", ",", "feed_dict", "[", "'token_seq_length'", "]", ",", "self", ".", "lstm", ")", "\n", "context_rep", ",", "_", "=", "self", ".", "attentive_sum", "(", "context_rep", ")", "\n", "# Mention Representation", "\n", "mention_embed", "=", "self", ".", "weighted_sum", "(", "feed_dict", "[", "'mention_embed'", "]", ")", "\n", "if", "self", ".", "enhanced_mention", ":", "\n", "      ", "if", "self", ".", "mention_lstm", ":", "\n", "        ", "mention_hid", "=", "self", ".", "sorted_rnn", "(", "mention_embed", ",", "feed_dict", "[", "'mention_span_length'", "]", ",", "self", ".", "lstm_mention", ")", "\n", "mention_embed", ",", "attn_score", "=", "self", ".", "mention_attentive_sum", "(", "mention_hid", ")", "\n", "", "else", ":", "\n", "        ", "mention_embed", ",", "attn_score", "=", "self", ".", "head_attentive_sum", "(", "mention_embed", ")", "\n", "", "span_cnn_embed", "=", "self", ".", "cnn", "(", "feed_dict", "[", "'span_chars'", "]", ")", "\n", "mention_embed", "=", "torch", ".", "cat", "(", "(", "span_cnn_embed", ",", "mention_embed", ")", ",", "1", ")", "\n", "", "else", ":", "\n", "      ", "mention_embed", "=", "torch", ".", "sum", "(", "mention_embed", ",", "dim", "=", "1", ")", "\n", "", "mention_embed", "=", "self", ".", "mention_dropout", "(", "mention_embed", ")", "\n", "\n", "if", "self", ".", "add_headword_emb", ":", "\n", "      ", "mention_headword_embed", "=", "self", ".", "weighted_sum", "(", "feed_dict", "[", "'mention_headword_embed'", "]", ")", "\n", "output", "=", "torch", ".", "cat", "(", "(", "context_rep", ",", "mention_embed", ",", "mention_headword_embed", ")", ",", "1", ")", "# + Headword lstm emb ", "\n", "", "else", ":", "\n", "      ", "output", "=", "torch", ".", "cat", "(", "(", "context_rep", ",", "mention_embed", ")", ",", "1", ")", "\n", "\n", "", "logits", "=", "self", ".", "decoder", "(", "output", ",", "data_type", ")", "\n", "loss", "=", "self", ".", "define_loss", "(", "logits", ",", "feed_dict", "[", "'y'", "]", ",", "data_type", ")", "\n", "return", "loss", ",", "logits", ",", "attn_score", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.models.Bert.__init__": [[176, 193], ["models.ModelBase.__init__", "BertModel", "torch.Dropout", "torch.Dropout", "print", "BertConfig.from_json_file", "print", "model_utils.MultiSimpleDecoder", "model_utils.SimpleDecoder"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BertConfig.from_json_file"], ["def", "__init__", "(", "self", ",", "args", ",", "answer_num", ")", ":", "\n", "    ", "super", "(", "Bert", ",", "self", ")", ".", "__init__", "(", "args", ",", "answer_num", ")", "\n", "\n", "# --- BERT ---", "\n", "if", "args", ".", "model_type", "==", "'bert_uncase_small'", ":", "\n", "      ", "print", "(", "'==> Loading BERT config from '", "+", "constant", ".", "BERT_UNCASED_SMALL_CONFIG", ")", "\n", "self", ".", "bert_config", "=", "BertConfig", ".", "from_json_file", "(", "constant", ".", "BERT_UNCASED_SMALL_CONFIG", ")", "\n", "", "else", ":", "\n", "      ", "raise", "NotImplementedError", "\n", "", "self", ".", "bert", "=", "BertModel", "(", "self", ".", "bert_config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "self", ".", "bert_config", ".", "hidden_dropout_prob", ")", "\n", "\n", "if", "args", ".", "data_setup", "==", "'joint'", "and", "args", ".", "multitask", ":", "\n", "      ", "print", "(", "\"Multi-task learning\"", ")", "\n", "self", ".", "decoder", "=", "MultiSimpleDecoder", "(", "self", ".", "bert_config", ".", "hidden_size", ")", "\n", "", "else", ":", "\n", "      ", "self", ".", "decoder", "=", "SimpleDecoder", "(", "self", ".", "bert_config", ".", "hidden_size", ",", "answer_num", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.models.Bert.forward": [[194, 200], ["models.Bert.bert", "models.Bert.dropout", "models.Bert.decoder", "models.Bert.define_loss"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.models.ModelBase.define_loss"], ["", "", "def", "forward", "(", "self", ",", "feed_dict", ",", "data_type", ")", ":", "\n", "    ", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "feed_dict", "[", "'bert_input_idx'", "]", ",", "feed_dict", "[", "'bert_token_type_idx'", "]", ",", "feed_dict", "[", "'bert_attention_mask'", "]", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "decoder", "(", "pooled_output", ",", "data_type", ")", "\n", "loss", "=", "self", ".", "define_loss", "(", "logits", ",", "feed_dict", "[", "'y'", "]", ",", "data_type", ")", "\n", "return", "loss", ",", "logits", ",", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.TensorboardWriter.__init__": [[46, 49], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "train_log", ":", "SummaryWriter", "=", "None", ",", "validation_log", ":", "SummaryWriter", "=", "None", ")", "->", "None", ":", "\n", "    ", "self", ".", "_train_log", "=", "train_log", "\n", "self", ".", "_validation_log", "=", "validation_log", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.TensorboardWriter.add_train_scalar": [[50, 53], ["main.TensorboardWriter._train_log.add_scalar"], "methods", ["None"], ["", "def", "add_train_scalar", "(", "self", ",", "name", ":", "str", ",", "value", ":", "float", ",", "global_step", ":", "int", ")", "->", "None", ":", "\n", "    ", "if", "self", ".", "_train_log", "is", "not", "None", ":", "\n", "      ", "self", ".", "_train_log", ".", "add_scalar", "(", "name", ",", "value", ",", "global_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.TensorboardWriter.add_validation_scalar": [[54, 57], ["main.TensorboardWriter._validation_log.add_scalar"], "methods", ["None"], ["", "", "def", "add_validation_scalar", "(", "self", ",", "name", ":", "str", ",", "value", ":", "float", ",", "global_step", ":", "int", ")", "->", "None", ":", "\n", "    ", "if", "self", ".", "_validation_log", "is", "not", "None", ":", "\n", "      ", "self", ".", "_validation_log", ".", "add_scalar", "(", "name", ",", "value", ",", "global_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.get_data_gen": [[59, 72], ["data_utils.TypeDataset", "data_utils.TypeDataset.get_batch", "data_utils.TypeDataset.get_batch", "data_utils.TypeDataset.get_batch"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset.get_batch", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset.get_batch", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset.get_batch"], ["", "", "", "def", "get_data_gen", "(", "dataname", ",", "mode", ",", "args", ",", "vocab_set", ",", "goal", ",", "elmo", "=", "None", ",", "bert", "=", "None", ")", ":", "\n", "  ", "dataset", "=", "data_utils", ".", "TypeDataset", "(", "constant", ".", "FILE_ROOT", "+", "dataname", ",", "goal", "=", "goal", ",", "vocab", "=", "vocab_set", ",", "\n", "elmo", "=", "elmo", ",", "bert", "=", "bert", ",", "args", "=", "args", ")", "\n", "if", "mode", "==", "'train'", ":", "\n", "    ", "data_gen", "=", "dataset", ".", "get_batch", "(", "args", ".", "batch_size", ",", "args", ".", "num_epoch", ",", "forever", "=", "False", ",", "eval_data", "=", "False", ",", "\n", "simple_mention", "=", "not", "args", ".", "enhanced_mention", ")", "\n", "", "elif", "mode", "==", "'dev'", ":", "\n", "    ", "data_gen", "=", "dataset", ".", "get_batch", "(", "args", ".", "eval_batch_size", ",", "1", ",", "forever", "=", "True", ",", "eval_data", "=", "True", ",", "\n", "simple_mention", "=", "not", "args", ".", "enhanced_mention", ")", "\n", "", "else", ":", "\n", "    ", "data_gen", "=", "dataset", ".", "get_batch", "(", "args", ".", "eval_batch_size", ",", "1", ",", "forever", "=", "False", ",", "eval_data", "=", "True", ",", "\n", "simple_mention", "=", "not", "args", ".", "enhanced_mention", ")", "\n", "", "return", "data_gen", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.get_joint_datasets": [[74, 116], ["data_utils.init_elmo", "data_utils.get_vocab", "train_gen_list.append", "valid_gen_list.append", "valid_gen_list.append", "train_gen_list.append", "train_gen_list.append", "train_gen_list.append", "train_gen_list.append", "main.get_data_gen", "main.get_data_gen", "main.get_data_gen", "main.get_data_gen", "main.get_data_gen", "main.get_data_gen", "main.get_data_gen"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.init_elmo", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.get_vocab", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.get_data_gen", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.get_data_gen", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.get_data_gen", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.get_data_gen", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.get_data_gen", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.get_data_gen", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.get_data_gen"], ["", "def", "get_joint_datasets", "(", "args", ")", ":", "\n", "\n", "  ", "if", "args", ".", "elmo", ":", "\n", "    ", "vocab", "=", "(", "constant", ".", "CHAR_DICT", ",", "None", ")", "# dummy empty dict", "\n", "elmo", "=", "data_utils", ".", "init_elmo", "(", ")", "\n", "bert", "=", "None", "\n", "", "elif", "args", ".", "bert", ":", "\n", "    ", "vocab", "=", "(", "constant", ".", "CHAR_DICT", ",", "None", ")", "# dummy empty dict ", "\n", "elmo", "=", "None", "\n", "bert", "=", "None", "\n", "", "else", ":", "# glove", "\n", "    ", "vocab", "=", "data_utils", ".", "get_vocab", "(", ")", "\n", "elmo", "=", "None", "\n", "bert", "=", "None", "\n", "", "train_gen_list", "=", "[", "]", "\n", "valid_gen_list", "=", "[", "]", "\n", "if", "args", ".", "mode", "in", "[", "'train'", ",", "'train_labeler'", "]", ":", "\n", "    ", "if", "not", "args", ".", "remove_open", "and", "not", "args", ".", "only_crowd", ":", "\n", "      ", "train_gen_list", ".", "append", "(", "\n", "(", "\"open\"", ",", "get_data_gen", "(", "'train_full/open_train_tree*.json'", ",", "'train'", ",", "args", ",", "vocab", ",", "\"open\"", ",", "elmo", "=", "elmo", ",", "bert", "=", "bert", ")", ")", ")", "\n", "#(\"open\", get_data_gen('distant_supervision/headword_train_tree.json', 'train', args, vocab, \"open\", elmo=elmo, bert=bert)))", "\n", "valid_gen_list", ".", "append", "(", "(", "\"open\"", ",", "get_data_gen", "(", "'distant_supervision/headword_dev_tree.json'", ",", "'dev'", ",", "args", ",", "vocab", ",", "\"open\"", ",", "elmo", "=", "elmo", ",", "bert", "=", "bert", ")", ")", ")", "\n", "", "if", "not", "args", ".", "remove_el", "and", "not", "args", ".", "only_crowd", ":", "\n", "      ", "valid_gen_list", ".", "append", "(", "\n", "(", "\"wiki\"", ",", "\n", "get_data_gen", "(", "'distant_supervision/el_dev_tree.json'", ",", "'dev'", ",", "args", ",", "vocab", ",", "\"wiki\"", "if", "args", ".", "multitask", "else", "\"open\"", ",", "elmo", "=", "elmo", ",", "bert", "=", "bert", ")", ")", ")", "\n", "train_gen_list", ".", "append", "(", "\n", "(", "\"wiki\"", ",", "\n", "#get_data_gen('distant_supervision/el_train_tree.json', 'train', args, vocab, \"wiki\" if args.multitask else \"open\", elmo=elmo, bert=bert)))", "\n", "get_data_gen", "(", "'train_full/el_train_full_tree.json'", ",", "'train'", ",", "args", ",", "vocab", ",", "\"wiki\"", "if", "args", ".", "multitask", "else", "\"open\"", ",", "elmo", "=", "elmo", ",", "bert", "=", "bert", ")", ")", ")", "\n", "", "if", "args", ".", "add_crowd", "or", "args", ".", "only_crowd", ":", "\n", "      ", "train_gen_list", ".", "append", "(", "\n", "(", "\"open\"", ",", "get_data_gen", "(", "'crowd/train_m_tree.json'", ",", "'train'", ",", "args", ",", "vocab", ",", "\"open\"", ",", "elmo", "=", "elmo", ",", "bert", "=", "bert", ")", ")", ")", "\n", "", "if", "args", ".", "add_expanded_head", ":", "\n", "      ", "train_gen_list", ".", "append", "(", "\n", "(", "\"open\"", ",", "get_data_gen", "(", "'train_full/open_train_1m_cls_relabeled.json'", ",", "'train'", ",", "args", ",", "vocab", ",", "\"open\"", ",", "elmo", "=", "elmo", ",", "bert", "=", "bert", ")", ")", ")", "\n", "", "if", "args", ".", "add_expanded_el", ":", "\n", "      ", "train_gen_list", ".", "append", "(", "\n", "(", "\"wiki\"", ",", "get_data_gen", "(", "'train_full/el_train_1m_cls_relabeled.json'", ",", "'train'", ",", "args", ",", "vocab", ",", "\"wiki\"", "if", "args", ".", "multitask", "else", "\"open\"", ",", "elmo", "=", "elmo", ",", "bert", "=", "bert", ")", ")", ")", "\n", "#crowd_dev_gen = get_data_gen('crowd/dev.json', 'dev', args, vocab, \"open\")", "\n", "", "", "crowd_dev_gen", "=", "None", "# get_data_gen('crowd/dev_tree.json', 'dev', args, vocab, \"open\", elmo=elmo, bert=bert)", "\n", "return", "train_gen_list", ",", "valid_gen_list", ",", "crowd_dev_gen", ",", "elmo", ",", "bert", ",", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.get_datasets": [[118, 135], ["data_utils.init_elmo", "data_gen_list.append", "data_utils.get_vocab", "main.get_data_gen"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.init_elmo", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.get_vocab", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.get_data_gen"], ["", "def", "get_datasets", "(", "data_lists", ",", "args", ")", ":", "\n", "  ", "data_gen_list", "=", "[", "]", "\n", "if", "args", ".", "elmo", ":", "\n", "    ", "vocab", "=", "(", "constant", ".", "CHAR_DICT", ",", "None", ")", "# dummy empty dict", "\n", "elmo", "=", "data_utils", ".", "init_elmo", "(", ")", "\n", "bert", "=", "None", "\n", "", "elif", "args", ".", "bert", ":", "\n", "    ", "vocab", "=", "(", "constant", ".", "CHAR_DICT", ",", "None", ")", "# dummy empty dict ", "\n", "elmo", "=", "None", "\n", "bert", "=", "None", "\n", "", "else", ":", "\n", "    ", "vocab", "=", "data_utils", ".", "get_vocab", "(", ")", "\n", "elmo", "=", "None", "\n", "bert", "=", "None", "\n", "", "for", "dataname", ",", "mode", ",", "goal", "in", "data_lists", ":", "\n", "    ", "data_gen_list", ".", "append", "(", "get_data_gen", "(", "dataname", ",", "mode", ",", "args", ",", "vocab", ",", "goal", ",", "elmo", "=", "elmo", ",", "bert", "=", "bert", ")", ")", "\n", "", "return", "data_gen_list", ",", "elmo", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main._train": [[137, 268], ["tensorboardX.SummaryWriter", "tensorboardX.SummaryWriter", "main.TensorboardWriter", "models.Bert.cuda", "time.time", "time.time", "enumerate", "torch.save", "main.get_joint_datasets", "print", "main.get_datasets", "os.path.join", "os.path.join", "print", "models.ETModel", "optimization.BERTAdam", "torch.optim.Adam", "main.load_model", "models.Bert.modules", "logging.info", "print", "models.Bert", "print", "print", "models.Bert.bert.load_state_dict", "models.Bert.parameters", "optim.Adam.zero_grad", "models.Bert.", "loss.backward", "loss.item", "optim.Adam.step", "print", "main.evaluate_data", "print", "main.evaluate_data", "torch.save", "print", "models.Bert.state_dict", "optim.Adam.state_dict", "torch.load", "str", "next", "data_utils.to_torch", "gc.collect", "float", "time.time", "print", "logging.info", "main.TensorboardWriter.add_train_scalar", "model_utils.get_output_index", "model_utils.get_gold_pred_str", "print", "print", "logging.info", "main.TensorboardWriter.add_train_scalar", "torch.save", "print", "str", "logging.info", "print", "torch.save", "time.time", "batch[].data.cpu().clone", "len", "models.Bert.state_dict", "optim.Adam.state_dict", "models.Bert.named_parameters", "models.Bert.named_parameters", "loss.clone().item", "sum", "models.Bert.state_dict", "optim.Adam.state_dict", "str", "models.Bert.state_dict", "optim.Adam.state_dict", "batch[].data.cpu", "data_utils.to_torch", "main.evaluate_batch", "time.time", "loss.clone", "next", "time.time", "set", "set"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.get_joint_datasets", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.get_datasets", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.load_model", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.optimization.BERTAdam.step", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.evaluate_data", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.evaluate_data", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.to_torch", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.TensorboardWriter.add_train_scalar", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_output_index", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_gold_pred_str", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.TensorboardWriter.add_train_scalar", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.to_torch", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.evaluate_batch"], ["", "def", "_train", "(", "args", ")", ":", "\n", "  ", "if", "args", ".", "data_setup", "==", "'joint'", ":", "\n", "    ", "train_gen_list", ",", "val_gen_list", ",", "crowd_dev_gen", ",", "elmo", ",", "bert", ",", "vocab", "=", "get_joint_datasets", "(", "args", ")", "\n", "", "else", ":", "\n", "    ", "train_fname", "=", "args", ".", "train_data", "\n", "dev_fname", "=", "args", ".", "dev_data", "\n", "print", "(", "train_fname", ",", "dev_fname", ")", "\n", "data_gens", ",", "elmo", "=", "get_datasets", "(", "[", "(", "train_fname", ",", "'train'", ",", "args", ".", "goal", ")", ",", "\n", "(", "dev_fname", ",", "'dev'", ",", "args", ".", "goal", ")", "]", ",", "args", ")", "\n", "train_gen_list", "=", "[", "(", "args", ".", "goal", ",", "data_gens", "[", "0", "]", ")", "]", "\n", "val_gen_list", "=", "[", "(", "args", ".", "goal", ",", "data_gens", "[", "1", "]", ")", "]", "\n", "", "train_log", "=", "SummaryWriter", "(", "os", ".", "path", ".", "join", "(", "constant", ".", "EXP_ROOT", ",", "args", ".", "model_id", ",", "\"log\"", ",", "\"train\"", ")", ")", "\n", "validation_log", "=", "SummaryWriter", "(", "os", ".", "path", ".", "join", "(", "constant", ".", "EXP_ROOT", ",", "args", ".", "model_id", ",", "\"log\"", ",", "\"validation\"", ")", ")", "\n", "tensorboard", "=", "TensorboardWriter", "(", "train_log", ",", "validation_log", ")", "\n", "\n", "if", "args", ".", "model_type", "==", "'et_model'", ":", "\n", "    ", "print", "(", "'==> Entity Typing Model'", ")", "\n", "model", "=", "models", ".", "ETModel", "(", "args", ",", "constant", ".", "ANSWER_NUM_DICT", "[", "args", ".", "goal", "]", ")", "\n", "", "elif", "args", ".", "model_type", "==", "'bert_uncase_small'", ":", "\n", "    ", "print", "(", "'==> Bert Uncased Small'", ")", "\n", "model", "=", "models", ".", "Bert", "(", "args", ",", "constant", ".", "ANSWER_NUM_DICT", "[", "args", ".", "goal", "]", ")", "\n", "", "else", ":", "\n", "    ", "print", "(", "'Invalid model type: -model_type '", "+", "args", ".", "model_type", ")", "\n", "raise", "NotImplementedError", "\n", "\n", "", "model", ".", "cuda", "(", ")", "\n", "total_loss", "=", "0", "\n", "batch_num", "=", "0", "\n", "best_macro_f1", "=", "0.", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "init_time", "=", "time", ".", "time", "(", ")", "\n", "if", "args", ".", "bert", ":", "\n", "    ", "if", "args", ".", "bert_param_path", ":", "\n", "      ", "print", "(", "'==> Loading BERT from '", "+", "args", ".", "bert_param_path", ")", "\n", "model", ".", "bert", ".", "load_state_dict", "(", "torch", ".", "load", "(", "args", ".", "bert_param_path", ",", "map_location", "=", "'cpu'", ")", ")", "\n", "", "no_decay", "=", "[", "'bias'", ",", "'gamma'", ",", "'beta'", "]", "\n", "optimizer_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "n", "not", "in", "no_decay", "]", ",", "'weight_decay_rate'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "n", "in", "no_decay", "]", ",", "'weight_decay_rate'", ":", "0.0", "}", "\n", "]", "\n", "optimizer", "=", "BERTAdam", "(", "optimizer_parameters", ",", "\n", "lr", "=", "args", ".", "bert_learning_rate", ",", "\n", "warmup", "=", "args", ".", "bert_warmup_proportion", ",", "\n", "t_total", "=", "-", "1", ")", "# TODO: ", "\n", "", "else", ":", "\n", "    ", "optimizer", "=", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "args", ".", "learning_rate", ")", "\n", "#optimizer = optim.SGD(model.parameters(), lr=1., momentum=0.)", "\n", "\n", "", "if", "args", ".", "load", ":", "\n", "    ", "load_model", "(", "args", ".", "reload_model_name", ",", "constant", ".", "EXP_ROOT", ",", "args", ".", "model_id", ",", "model", ",", "optimizer", ")", "\n", "\n", "", "for", "idx", ",", "m", "in", "enumerate", "(", "model", ".", "modules", "(", ")", ")", ":", "\n", "    ", "logging", ".", "info", "(", "str", "(", "idx", ")", "+", "'->'", "+", "str", "(", "m", ")", ")", "\n", "\n", "", "while", "True", ":", "\n", "    ", "batch_num", "+=", "1", "# single batch composed of all train signal passed by.", "\n", "for", "(", "type_name", ",", "data_gen", ")", "in", "train_gen_list", ":", "\n", "      ", "try", ":", "\n", "        ", "batch", "=", "next", "(", "data_gen", ")", "\n", "batch", ",", "_", "=", "to_torch", "(", "batch", ")", "\n", "", "except", "StopIteration", ":", "\n", "        ", "logging", ".", "info", "(", "type_name", "+", "\" finished at \"", "+", "str", "(", "batch_num", ")", ")", "\n", "print", "(", "'Done!'", ")", "\n", "torch", ".", "save", "(", "{", "'state_dict'", ":", "model", ".", "state_dict", "(", ")", ",", "'optimizer'", ":", "optimizer", ".", "state_dict", "(", ")", "}", ",", "\n", "'{0:s}/{1:s}.pt'", ".", "format", "(", "constant", ".", "EXP_ROOT", ",", "args", ".", "model_id", ")", ")", "\n", "return", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ",", "output_logits", ",", "_", "=", "model", "(", "batch", ",", "type_name", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "if", "batch_num", "%", "args", ".", "log_period", "==", "0", "and", "batch_num", ">", "0", ":", "\n", "        ", "gc", ".", "collect", "(", ")", "\n", "cur_loss", "=", "float", "(", "1.0", "*", "loss", ".", "clone", "(", ")", ".", "item", "(", ")", ")", "\n", "elapsed", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "train_loss_str", "=", "(", "'|loss {0:3f} | at {1:d}step | @ {2:.2f} ms/batch'", ".", "format", "(", "cur_loss", ",", "batch_num", ",", "\n", "elapsed", "*", "1000", "/", "args", ".", "log_period", ")", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "train_loss_str", ")", "\n", "logging", ".", "info", "(", "train_loss_str", ")", "\n", "tensorboard", ".", "add_train_scalar", "(", "'train_loss_'", "+", "type_name", ",", "cur_loss", ",", "batch_num", ")", "\n", "\n", "", "if", "batch_num", "%", "args", ".", "eval_period", "==", "0", "and", "batch_num", ">", "0", ":", "\n", "        ", "output_index", "=", "get_output_index", "(", "output_logits", ",", "threshold", "=", "args", ".", "threshold", ")", "\n", "gold_pred_train", "=", "get_gold_pred_str", "(", "output_index", ",", "batch", "[", "'y'", "]", ".", "data", ".", "cpu", "(", ")", ".", "clone", "(", ")", ",", "args", ".", "goal", ")", "\n", "print", "(", "gold_pred_train", "[", ":", "10", "]", ")", "\n", "accuracy", "=", "sum", "(", "[", "set", "(", "y", ")", "==", "set", "(", "yp", ")", "for", "y", ",", "yp", "in", "gold_pred_train", "]", ")", "*", "1.0", "/", "len", "(", "gold_pred_train", ")", "\n", "\n", "train_acc_str", "=", "'{1:s} Train accuracy: {0:.1f}%'", ".", "format", "(", "accuracy", "*", "100", ",", "type_name", ")", "\n", "print", "(", "train_acc_str", ")", "\n", "logging", ".", "info", "(", "train_acc_str", ")", "\n", "tensorboard", ".", "add_train_scalar", "(", "'train_acc_'", "+", "type_name", ",", "accuracy", ",", "batch_num", ")", "\n", "if", "args", ".", "goal", "!=", "'onto'", ":", "\n", "          ", "for", "(", "val_type_name", ",", "val_data_gen", ")", "in", "val_gen_list", ":", "\n", "            ", "if", "val_type_name", "==", "type_name", ":", "\n", "              ", "eval_batch", ",", "_", "=", "to_torch", "(", "next", "(", "val_data_gen", ")", ")", "\n", "evaluate_batch", "(", "batch_num", ",", "eval_batch", ",", "model", ",", "tensorboard", ",", "val_type_name", ",", "args", ",", "args", ".", "goal", ")", "\n", "\n", "", "", "", "", "", "if", "batch_num", "%", "args", ".", "eval_period", "==", "0", "and", "batch_num", ">", "0", "and", "args", ".", "data_setup", "==", "'joint'", ":", "\n", "# Evaluate Loss on the Turk Dev dataset.", "\n", "      ", "print", "(", "'---- eval at step {0:d} ---'", ".", "format", "(", "batch_num", ")", ")", "\n", "###############", "\n", "#feed_dict = next(crowd_dev_gen)", "\n", "#eval_batch, _ = to_torch(feed_dict)", "\n", "#crowd_eval_loss = evaluate_batch(batch_num, eval_batch, model, tensorboard, \"open\", args.goal, single_type=args.single_type)", "\n", "###############", "\n", "crowd_eval_loss", ",", "macro_f1", "=", "evaluate_data", "(", "batch_num", ",", "'crowd/dev_tree.json'", ",", "model", ",", "\n", "tensorboard", ",", "\"open\"", ",", "args", ",", "elmo", ",", "bert", ")", "\n", "\n", "if", "best_macro_f1", "<", "macro_f1", ":", "\n", "        ", "best_macro_f1", "=", "macro_f1", "\n", "save_fname", "=", "'{0:s}/{1:s}_best.pt'", ".", "format", "(", "constant", ".", "EXP_ROOT", ",", "args", ".", "model_id", ")", "\n", "torch", ".", "save", "(", "{", "'state_dict'", ":", "model", ".", "state_dict", "(", ")", ",", "'optimizer'", ":", "optimizer", ".", "state_dict", "(", ")", "}", ",", "save_fname", ")", "\n", "print", "(", "\n", "'Total {0:.2f} minutes have passed, saving at {1:s} '", ".", "format", "(", "(", "time", ".", "time", "(", ")", "-", "init_time", ")", "/", "60", ",", "save_fname", ")", ")", "\n", "\n", "", "", "if", "batch_num", "%", "args", ".", "eval_period", "==", "0", "and", "batch_num", ">", "0", "and", "args", ".", "goal", "==", "'onto'", ":", "\n", "# Evaluate Loss on the Turk Dev dataset.", "\n", "      ", "print", "(", "'---- OntoNotes: eval at step {0:d} ---'", ".", "format", "(", "batch_num", ")", ")", "\n", "crowd_eval_loss", ",", "macro_f1", "=", "evaluate_data", "(", "batch_num", ",", "args", ".", "dev_data", ",", "model", ",", "tensorboard", ",", "\n", "args", ".", "goal", ",", "args", ",", "elmo", ",", "bert", ")", "\n", "\n", "", "if", "batch_num", "%", "args", ".", "save_period", "==", "0", "and", "batch_num", ">", "30000", ":", "\n", "      ", "save_fname", "=", "'{0:s}/{1:s}_{2:d}.pt'", ".", "format", "(", "constant", ".", "EXP_ROOT", ",", "args", ".", "model_id", ",", "batch_num", ")", "\n", "torch", ".", "save", "(", "{", "'state_dict'", ":", "model", ".", "state_dict", "(", ")", ",", "'optimizer'", ":", "optimizer", ".", "state_dict", "(", ")", "}", ",", "save_fname", ")", "\n", "print", "(", "\n", "'Total {0:.2f} minutes have passed, saving at {1:s} '", ".", "format", "(", "(", "time", ".", "time", "(", ")", "-", "init_time", ")", "/", "60", ",", "save_fname", ")", ")", "\n", "# Training finished! ", "\n", "", "", "torch", ".", "save", "(", "{", "'state_dict'", ":", "model", ".", "state_dict", "(", ")", ",", "'optimizer'", ":", "optimizer", ".", "state_dict", "(", ")", "}", ",", "\n", "'{0:s}/{1:s}.pt'", ".", "format", "(", "constant", ".", "EXP_ROOT", ",", "args", ".", "model_id", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main._train_labeler": [[270, 406], ["tensorboardX.SummaryWriter", "tensorboardX.SummaryWriter", "main.TensorboardWriter", "denoising_models.Filter.cuda", "time.time", "time.time", "enumerate", "torch.save", "main.get_joint_datasets", "print", "main.get_datasets", "os.path.join", "os.path.join", "print", "denoising_models.Labeler", "optimization.BERTAdam", "torch.optim.Adam", "main.load_model", "denoising_models.Filter.modules", "logging.info", "print", "denoising_models.Filter", "print", "print", "denoising_models.Filter.bert.load_state_dict", "denoising_models.Filter.parameters", "optim.Adam.zero_grad", "denoising_models.Filter.", "loss.backward", "loss.item", "optim.Adam.step", "print", "main.evaluate_data", "print", "main.evaluate_data", "torch.save", "print", "denoising_models.Filter.state_dict", "optim.Adam.state_dict", "torch.load", "str", "next", "data_utils.to_torch", "gc.collect", "float", "time.time", "print", "logging.info", "main.TensorboardWriter.add_train_scalar", "model_utils.get_output_index", "model_utils.get_gold_pred_str", "print", "print", "logging.info", "main.TensorboardWriter.add_train_scalar", "torch.save", "print", "str", "logging.info", "print", "torch.save", "time.time", "batch[].data.cpu().clone", "len", "sum", "eval_metric.f1", "print", "denoising_models.Filter.state_dict", "optim.Adam.state_dict", "denoising_models.Filter.named_parameters", "denoising_models.Filter.named_parameters", "loss.clone().item", "sum", "sum", "float", "float", "float", "denoising_models.Filter.state_dict", "optim.Adam.state_dict", "str", "denoising_models.Filter.state_dict", "optim.Adam.state_dict", "batch[].data.cpu", "sum", "sum", "data_utils.to_torch", "main.evaluate_batch", "time.time", "loss.clone", "cls_logits.size", "zip", "batch[].data.cpu().numpy", "next", "time.time", "set", "set", "zip", "batch[].data.cpu().numpy", "batch[].data.cpu().numpy", "batch[].data.cpu", "batch[].data.cpu", "batch[].data.cpu"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.get_joint_datasets", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.get_datasets", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.load_model", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.optimization.BERTAdam.step", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.evaluate_data", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.evaluate_data", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.to_torch", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.TensorboardWriter.add_train_scalar", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_output_index", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_gold_pred_str", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.TensorboardWriter.add_train_scalar", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.eval_metric.f1", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.to_torch", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.evaluate_batch"], ["", "def", "_train_labeler", "(", "args", ")", ":", "\n", "  ", "if", "args", ".", "data_setup", "==", "'joint'", ":", "\n", "    ", "train_gen_list", ",", "val_gen_list", ",", "crowd_dev_gen", ",", "elmo", ",", "bert", ",", "vocab", "=", "get_joint_datasets", "(", "args", ")", "\n", "", "else", ":", "\n", "    ", "train_fname", "=", "args", ".", "train_data", "\n", "dev_fname", "=", "args", ".", "dev_data", "\n", "print", "(", "train_fname", ",", "dev_fname", ")", "\n", "data_gens", ",", "elmo", "=", "get_datasets", "(", "[", "(", "train_fname", ",", "'train'", ",", "args", ".", "goal", ")", ",", "\n", "(", "dev_fname", ",", "'dev'", ",", "args", ".", "goal", ")", "]", ",", "args", ")", "\n", "train_gen_list", "=", "[", "(", "args", ".", "goal", ",", "data_gens", "[", "0", "]", ")", "]", "\n", "val_gen_list", "=", "[", "(", "args", ".", "goal", ",", "data_gens", "[", "1", "]", ")", "]", "\n", "", "train_log", "=", "SummaryWriter", "(", "os", ".", "path", ".", "join", "(", "constant", ".", "EXP_ROOT", ",", "args", ".", "model_id", ",", "\"log\"", ",", "\"train\"", ")", ")", "\n", "validation_log", "=", "SummaryWriter", "(", "os", ".", "path", ".", "join", "(", "constant", ".", "EXP_ROOT", ",", "args", ".", "model_id", ",", "\"log\"", ",", "\"validation\"", ")", ")", "\n", "tensorboard", "=", "TensorboardWriter", "(", "train_log", ",", "validation_log", ")", "\n", "\n", "if", "args", ".", "model_type", "==", "'labeler'", ":", "\n", "    ", "print", "(", "'==> Labeler'", ")", "\n", "model", "=", "denoising_models", ".", "Labeler", "(", "args", ",", "constant", ".", "ANSWER_NUM_DICT", "[", "args", ".", "goal", "]", ")", "\n", "", "elif", "args", ".", "model_type", "==", "'filter'", ":", "\n", "    ", "print", "(", "'==> Filter'", ")", "\n", "model", "=", "denoising_models", ".", "Filter", "(", "args", ",", "constant", ".", "ANSWER_NUM_DICT", "[", "args", ".", "goal", "]", ")", "\n", "", "else", ":", "\n", "    ", "print", "(", "'Invalid model type: -model_type '", "+", "args", ".", "model_type", ")", "\n", "raise", "NotImplementedError", "\n", "\n", "", "model", ".", "cuda", "(", ")", "\n", "total_loss", "=", "0", "\n", "batch_num", "=", "0", "\n", "best_macro_f1", "=", "0.", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "init_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "if", "args", ".", "bert", ":", "\n", "    ", "if", "args", ".", "bert_param_path", ":", "\n", "      ", "print", "(", "'==> Loading BERT from '", "+", "args", ".", "bert_param_path", ")", "\n", "model", ".", "bert", ".", "load_state_dict", "(", "torch", ".", "load", "(", "args", ".", "bert_param_path", ",", "map_location", "=", "'cpu'", ")", ")", "\n", "", "no_decay", "=", "[", "'bias'", ",", "'gamma'", ",", "'beta'", "]", "\n", "optimizer_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "n", "not", "in", "no_decay", "]", ",", "'weight_decay_rate'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "n", "in", "no_decay", "]", ",", "'weight_decay_rate'", ":", "0.0", "}", "\n", "]", "\n", "optimizer", "=", "BERTAdam", "(", "optimizer_parameters", ",", "\n", "lr", "=", "args", ".", "bert_learning_rate", ",", "\n", "warmup", "=", "args", ".", "bert_warmup_proportion", ",", "\n", "t_total", "=", "-", "1", ")", "# TODO: ", "\n", "", "else", ":", "\n", "    ", "optimizer", "=", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "args", ".", "learning_rate", ")", "\n", "#optimizer = optim.SGD(model.parameters(), lr=1., momentum=0.)", "\n", "\n", "", "if", "args", ".", "load", ":", "\n", "    ", "load_model", "(", "args", ".", "reload_model_name", ",", "constant", ".", "EXP_ROOT", ",", "args", ".", "model_id", ",", "model", ",", "optimizer", ")", "\n", "\n", "", "for", "idx", ",", "m", "in", "enumerate", "(", "model", ".", "modules", "(", ")", ")", ":", "\n", "    ", "logging", ".", "info", "(", "str", "(", "idx", ")", "+", "'->'", "+", "str", "(", "m", ")", ")", "\n", "\n", "", "while", "True", ":", "\n", "    ", "batch_num", "+=", "1", "# single batch composed of all train signal passed by.", "\n", "for", "(", "type_name", ",", "data_gen", ")", "in", "train_gen_list", ":", "\n", "      ", "try", ":", "\n", "        ", "batch", "=", "next", "(", "data_gen", ")", "\n", "batch", ",", "_", "=", "to_torch", "(", "batch", ")", "\n", "", "except", "StopIteration", ":", "\n", "        ", "logging", ".", "info", "(", "type_name", "+", "\" finished at \"", "+", "str", "(", "batch_num", ")", ")", "\n", "print", "(", "'Done!'", ")", "\n", "torch", ".", "save", "(", "{", "'state_dict'", ":", "model", ".", "state_dict", "(", ")", ",", "'optimizer'", ":", "optimizer", ".", "state_dict", "(", ")", "}", ",", "\n", "'{0:s}/{1:s}.pt'", ".", "format", "(", "constant", ".", "EXP_ROOT", ",", "args", ".", "model_id", ")", ")", "\n", "return", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ",", "output_logits", ",", "cls_logits", "=", "model", "(", "batch", ",", "type_name", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "if", "batch_num", "%", "args", ".", "log_period", "==", "0", "and", "batch_num", ">", "0", ":", "\n", "        ", "gc", ".", "collect", "(", ")", "\n", "cur_loss", "=", "float", "(", "1.0", "*", "loss", ".", "clone", "(", ")", ".", "item", "(", ")", ")", "\n", "elapsed", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "train_loss_str", "=", "(", "'|loss {0:3f} | at {1:d}step | @ {2:.2f} ms/batch'", ".", "format", "(", "cur_loss", ",", "batch_num", ",", "\n", "elapsed", "*", "1000", "/", "args", ".", "log_period", ")", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "train_loss_str", ")", "\n", "logging", ".", "info", "(", "train_loss_str", ")", "\n", "tensorboard", ".", "add_train_scalar", "(", "'train_loss_'", "+", "type_name", ",", "cur_loss", ",", "batch_num", ")", "\n", "\n", "", "if", "batch_num", "%", "args", ".", "eval_period", "==", "0", "and", "batch_num", ">", "0", ":", "\n", "        ", "output_index", "=", "get_output_index", "(", "output_logits", ",", "threshold", "=", "args", ".", "threshold", ")", "\n", "gold_pred_train", "=", "get_gold_pred_str", "(", "output_index", ",", "batch", "[", "'y'", "]", ".", "data", ".", "cpu", "(", ")", ".", "clone", "(", ")", ",", "args", ".", "goal", ")", "\n", "print", "(", "gold_pred_train", "[", ":", "10", "]", ")", "\n", "accuracy", "=", "sum", "(", "[", "set", "(", "y", ")", "==", "set", "(", "yp", ")", "for", "y", ",", "yp", "in", "gold_pred_train", "]", ")", "*", "1.0", "/", "len", "(", "gold_pred_train", ")", "\n", "\n", "train_acc_str", "=", "'{1:s} Train accuracy: {0:.1f}%'", ".", "format", "(", "accuracy", "*", "100", ",", "type_name", ")", "\n", "if", "cls_logits", "is", "not", "None", ":", "\n", "          ", "cls_accuracy", "=", "sum", "(", "[", "(", "1.", "if", "pred", ">", "0.", "else", "0.", ")", "==", "gold", "for", "pred", ",", "gold", "in", "zip", "(", "cls_logits", ",", "batch", "[", "'y_cls'", "]", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "]", ")", "/", "float", "(", "cls_logits", ".", "size", "(", ")", "[", "0", "]", ")", "\n", "cls_tp", "=", "sum", "(", "[", "(", "1.", "if", "pred", ">", "0.", "else", "0.", ")", "==", "1.", "and", "gold", "==", "1.", "for", "pred", ",", "gold", "in", "zip", "(", "cls_logits", ",", "batch", "[", "'y_cls'", "]", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "]", ")", "\n", "cls_precision", "=", "cls_tp", "/", "float", "(", "sum", "(", "[", "1.", "if", "pred", ">", "0.", "else", "0.", "for", "pred", "in", "cls_logits", "]", ")", ")", "\n", "cls_recall", "=", "cls_tp", "/", "float", "(", "sum", "(", "batch", "[", "'y_cls'", "]", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", ")", "\n", "cls_f1", "=", "f1", "(", "cls_precision", ",", "cls_recall", ")", "\n", "train_cls_acc_str", "=", "'{1:s} Train cls accuracy: {0:.2f}%  P: {2:.3f}  R: {3:.3f}  F1: {4:.3f}'", ".", "format", "(", "cls_accuracy", "*", "100", ",", "type_name", ",", "cls_precision", ",", "cls_recall", ",", "cls_f1", ")", "\n", "", "print", "(", "train_acc_str", ")", "\n", "if", "cls_logits", "is", "not", "None", ":", "\n", "          ", "print", "(", "train_cls_acc_str", ")", "\n", "", "logging", ".", "info", "(", "train_acc_str", ")", "\n", "tensorboard", ".", "add_train_scalar", "(", "'train_acc_'", "+", "type_name", ",", "accuracy", ",", "batch_num", ")", "\n", "if", "args", ".", "goal", "!=", "'onto'", ":", "\n", "          ", "for", "(", "val_type_name", ",", "val_data_gen", ")", "in", "val_gen_list", ":", "\n", "            ", "if", "val_type_name", "==", "type_name", ":", "\n", "              ", "eval_batch", ",", "_", "=", "to_torch", "(", "next", "(", "val_data_gen", ")", ")", "\n", "evaluate_batch", "(", "batch_num", ",", "eval_batch", ",", "model", ",", "tensorboard", ",", "val_type_name", ",", "args", ",", "args", ".", "goal", ")", "\n", "\n", "", "", "", "", "", "if", "batch_num", "%", "args", ".", "eval_period", "==", "0", "and", "batch_num", ">", "0", "and", "args", ".", "data_setup", "==", "'joint'", ":", "\n", "# Evaluate Loss on the Turk Dev dataset.", "\n", "      ", "print", "(", "'---- eval at step {0:d} ---'", ".", "format", "(", "batch_num", ")", ")", "\n", "crowd_eval_loss", ",", "macro_f1", "=", "evaluate_data", "(", "batch_num", ",", "'crowd/dev_tree.json'", ",", "model", ",", "\n", "tensorboard", ",", "\"open\"", ",", "args", ",", "elmo", ",", "bert", ",", "vocab", "=", "vocab", ")", "\n", "\n", "if", "best_macro_f1", "<", "macro_f1", ":", "\n", "        ", "best_macro_f1", "=", "macro_f1", "\n", "save_fname", "=", "'{0:s}/{1:s}_best.pt'", ".", "format", "(", "constant", ".", "EXP_ROOT", ",", "args", ".", "model_id", ")", "\n", "torch", ".", "save", "(", "{", "'state_dict'", ":", "model", ".", "state_dict", "(", ")", ",", "'optimizer'", ":", "optimizer", ".", "state_dict", "(", ")", "}", ",", "save_fname", ")", "\n", "print", "(", "\n", "'Total {0:.2f} minutes have passed, saving at {1:s} '", ".", "format", "(", "(", "time", ".", "time", "(", ")", "-", "init_time", ")", "/", "60", ",", "save_fname", ")", ")", "\n", "\n", "", "", "if", "batch_num", "%", "args", ".", "eval_period", "==", "0", "and", "batch_num", ">", "0", "and", "args", ".", "goal", "==", "'onto'", ":", "\n", "# Evaluate Loss on the Turk Dev dataset.", "\n", "      ", "print", "(", "'---- OntoNotes: eval at step {0:d} ---'", ".", "format", "(", "batch_num", ")", ")", "\n", "crowd_eval_loss", ",", "macro_f1", "=", "evaluate_data", "(", "batch_num", ",", "args", ".", "dev_data", ",", "model", ",", "tensorboard", ",", "\n", "args", ".", "goal", ",", "args", ",", "elmo", ")", "\n", "\n", "", "if", "batch_num", "%", "args", ".", "save_period", "==", "0", "and", "batch_num", ">", "0", ":", "\n", "      ", "save_fname", "=", "'{0:s}/{1:s}_{2:d}.pt'", ".", "format", "(", "constant", ".", "EXP_ROOT", ",", "args", ".", "model_id", ",", "batch_num", ")", "\n", "torch", ".", "save", "(", "{", "'state_dict'", ":", "model", ".", "state_dict", "(", ")", ",", "'optimizer'", ":", "optimizer", ".", "state_dict", "(", ")", "}", ",", "save_fname", ")", "\n", "print", "(", "\n", "'Total {0:.2f} minutes have passed, saving at {1:s} '", ".", "format", "(", "(", "time", ".", "time", "(", ")", "-", "init_time", ")", "/", "60", ",", "save_fname", ")", ")", "\n", "# Training finished! ", "\n", "", "", "torch", ".", "save", "(", "{", "'state_dict'", ":", "model", ".", "state_dict", "(", ")", ",", "'optimizer'", ":", "optimizer", ".", "state_dict", "(", ")", "}", ",", "\n", "'{0:s}/{1:s}.pt'", ".", "format", "(", "constant", ".", "EXP_ROOT", ",", "args", ".", "model_id", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.evaluate_batch": [[408, 426], ["model.eval", "model", "model_utils.get_output_index", "model_utils.get_gold_pred_str", "model_utils.get_eval_string", "loss.clone().item", "tensorboard.add_validation_scalar", "tensorboard.add_validation_scalar", "print", "print", "print", "logging.info", "logging.info", "model.train", "eval_batch[].data.cpu().clone", "len", "sum", "loss.clone", "eval_batch[].data.cpu", "set", "set"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_output_index", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_gold_pred_str", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_eval_string", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.TensorboardWriter.add_validation_scalar", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.TensorboardWriter.add_validation_scalar"], ["", "def", "evaluate_batch", "(", "batch_num", ",", "eval_batch", ",", "model", ",", "tensorboard", ",", "val_type_name", ",", "args", ",", "goal", ")", ":", "\n", "  ", "model", ".", "eval", "(", ")", "\n", "loss", ",", "output_logits", ",", "_", "=", "model", "(", "eval_batch", ",", "val_type_name", ")", "\n", "output_index", "=", "get_output_index", "(", "output_logits", ",", "threshold", "=", "args", ".", "threshold", ")", "\n", "gold_pred", "=", "get_gold_pred_str", "(", "output_index", ",", "eval_batch", "[", "'y'", "]", ".", "data", ".", "cpu", "(", ")", ".", "clone", "(", ")", ",", "goal", ")", "\n", "eval_accu", "=", "sum", "(", "[", "set", "(", "y", ")", "==", "set", "(", "yp", ")", "for", "y", ",", "yp", "in", "gold_pred", "]", ")", "*", "1.0", "/", "len", "(", "gold_pred", ")", "\n", "eval_str", "=", "get_eval_string", "(", "gold_pred", ")", "\n", "eval_loss", "=", "loss", ".", "clone", "(", ")", ".", "item", "(", ")", "\n", "eval_loss_str", "=", "'Eval loss: {0:.7f} at step {1:d}'", ".", "format", "(", "eval_loss", ",", "batch_num", ")", "\n", "tensorboard", ".", "add_validation_scalar", "(", "'eval_acc_'", "+", "val_type_name", ",", "eval_accu", ",", "batch_num", ")", "\n", "tensorboard", ".", "add_validation_scalar", "(", "'eval_loss_'", "+", "val_type_name", ",", "eval_loss", ",", "batch_num", ")", "\n", "print", "(", "val_type_name", "+", "\":\"", "+", "eval_loss_str", ")", "\n", "print", "(", "gold_pred", "[", ":", "3", "]", ")", "\n", "print", "(", "val_type_name", "+", "\":\"", "+", "eval_str", ")", "\n", "logging", ".", "info", "(", "val_type_name", "+", "\":\"", "+", "eval_loss_str", ")", "\n", "logging", ".", "info", "(", "val_type_name", "+", "\":\"", "+", "eval_str", ")", "\n", "model", ".", "train", "(", ")", "\n", "return", "eval_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.evaluate_data": [[428, 482], ["model.eval", "main.get_data_gen", "enumerate", "model_utils.get_eval_string", "eval_metric.macro", "eval_metric.macro", "tensorboard.add_validation_scalar", "tensorboard.add_validation_scalar", "print", "print", "print", "logging.info", "logging.info", "model.train", "len", "data_utils.to_torch", "model_utils.get_output_index", "model_utils.get_gold_pred_str", "loss.clone().item", "len", "eval_metric.f1", "print", "print", "model", "model", "eval_batch[].data.cpu().clone", "sum", "sum", "float", "sum", "float", "float", "loss.clone", "repr", "sum", "sum", "eval_batch[].data.cpu", "cls_logits.size", "set", "set", "zip", "zip"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.get_data_gen", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_eval_string", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.eval_metric.macro", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.eval_metric.macro", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.TensorboardWriter.add_validation_scalar", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.TensorboardWriter.add_validation_scalar", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.to_torch", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_output_index", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_gold_pred_str", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.eval_metric.f1"], ["", "def", "evaluate_data", "(", "batch_num", ",", "dev_fname", ",", "model", ",", "tensorboard", ",", "val_type_name", ",", "args", ",", "elmo", ",", "bert", ",", "actual_f1", "=", "True", ",", "vocab", "=", "None", ")", ":", "\n", "  ", "model", ".", "eval", "(", ")", "\n", "if", "vocab", "is", "None", ":", "\n", "    ", "vocab", "=", "(", "constant", ".", "CHAR_DICT", ",", "None", ")", "\n", "", "dev_gen", "=", "get_data_gen", "(", "dev_fname", ",", "'test'", ",", "args", ",", "vocab", ",", "args", ".", "goal", ",", "elmo", "=", "elmo", ",", "bert", "=", "bert", ")", "\n", "gold_pred", "=", "[", "]", "\n", "binary_out", "=", "[", "]", "\n", "eval_loss", "=", "0.", "\n", "total_ex_count", "=", "0", "\n", "if", "args", ".", "mode", "in", "[", "'train_labeler'", ",", "'test_labeler'", "]", ":", "\n", "    ", "cls_correct", "=", "0.", "\n", "cls_total", "=", "0.", "\n", "cls_tp", "=", "0.", "\n", "cls_t_gold", "=", "0.", "\n", "cls_t_pred", "=", "0.", "\n", "", "for", "n", ",", "batch", "in", "enumerate", "(", "dev_gen", ")", ":", "\n", "    ", "total_ex_count", "+=", "len", "(", "batch", "[", "'y'", "]", ")", "\n", "eval_batch", ",", "annot_ids", "=", "to_torch", "(", "batch", ")", "\n", "if", "args", ".", "mode", "in", "[", "'train_labeler'", ",", "'test_labeler'", "]", ":", "\n", "      ", "loss", ",", "output_logits", ",", "cls_logits", "=", "model", "(", "eval_batch", ",", "val_type_name", ")", "\n", "if", "cls_logits", "is", "not", "None", ":", "\n", "        ", "cls_correct", "+=", "sum", "(", "[", "(", "1.", "if", "pred", ">", "0.", "else", "0.", ")", "==", "gold", "for", "pred", ",", "gold", "in", "zip", "(", "cls_logits", ",", "batch", "[", "'y_cls'", "]", ")", "]", ")", "\n", "cls_total", "+=", "float", "(", "cls_logits", ".", "size", "(", ")", "[", "0", "]", ")", "\n", "cls_tp", "+=", "sum", "(", "[", "(", "1.", "if", "pred", ">", "0.", "else", "0.", ")", "==", "1.", "and", "gold", "==", "1.", "for", "pred", ",", "gold", "in", "zip", "(", "cls_logits", ",", "batch", "[", "'y_cls'", "]", ")", "]", ")", "\n", "cls_t_gold", "+=", "float", "(", "sum", "(", "batch", "[", "'y_cls'", "]", ")", ")", "\n", "cls_t_pred", "+=", "float", "(", "sum", "(", "[", "1.", "if", "pred", ">", "0.", "else", "0.", "for", "pred", "in", "cls_logits", "]", ")", ")", "\n", "", "", "else", ":", "\n", "      ", "loss", ",", "output_logits", ",", "_", "=", "model", "(", "eval_batch", ",", "val_type_name", ")", "\n", "", "output_index", "=", "get_output_index", "(", "output_logits", ",", "threshold", "=", "args", ".", "threshold", ")", "\n", "gold_pred", "+=", "get_gold_pred_str", "(", "output_index", ",", "eval_batch", "[", "'y'", "]", ".", "data", ".", "cpu", "(", ")", ".", "clone", "(", ")", ",", "args", ".", "goal", ")", "\n", "eval_loss", "+=", "loss", ".", "clone", "(", ")", ".", "item", "(", ")", "\n", "", "eval_accu", "=", "sum", "(", "[", "set", "(", "y", ")", "==", "set", "(", "yp", ")", "for", "y", ",", "yp", "in", "gold_pred", "]", ")", "*", "1.0", "/", "len", "(", "gold_pred", ")", "\n", "eval_str", "=", "get_eval_string", "(", "gold_pred", ")", "\n", "_", ",", "_", ",", "_", ",", "_", ",", "_", ",", "macro_f1", "=", "eval_metric", ".", "macro", "(", "gold_pred", ")", "\n", "eval_loss_str", "=", "'Eval loss: {0:.7f} at step {1:d}'", ".", "format", "(", "eval_loss", ",", "batch_num", ")", "\n", "tensorboard", ".", "add_validation_scalar", "(", "'eval_acc_'", "+", "val_type_name", ",", "eval_accu", ",", "batch_num", ")", "\n", "tensorboard", ".", "add_validation_scalar", "(", "'eval_loss_'", "+", "val_type_name", ",", "eval_loss", ",", "batch_num", ")", "\n", "print", "(", "'EVAL: seen '", "+", "repr", "(", "total_ex_count", ")", "+", "' examples.'", ")", "\n", "print", "(", "val_type_name", "+", "\":\"", "+", "eval_loss_str", ")", "\n", "print", "(", "gold_pred", "[", ":", "3", "]", ")", "\n", "if", "args", ".", "mode", "in", "[", "'train_labeler'", ",", "'test_labeler'", "]", "and", "cls_logits", "is", "not", "None", ":", "\n", "    ", "cls_accuracy", "=", "cls_correct", "/", "cls_total", "*", "100.", "\n", "cls_precision", "=", "cls_tp", "/", "cls_t_pred", "\n", "cls_recall", "=", "cls_tp", "/", "cls_t_gold", "\n", "cls_f1", "=", "f1", "(", "cls_precision", ",", "cls_recall", ")", "\n", "cls_str", "=", "'  CLS accuracy: {0:.2f}%  P: {1:.3f}  R: {2:.3f}  F1: {3:.3f}'", ".", "format", "(", "cls_accuracy", ",", "cls_precision", ",", "cls_recall", ",", "cls_f1", ")", "\n", "print", "(", "val_type_name", "+", "\":\"", "+", "eval_str", "+", "cls_str", ")", "\n", "", "else", ":", "\n", "    ", "print", "(", "val_type_name", "+", "\":\"", "+", "eval_str", ")", "\n", "", "logging", ".", "info", "(", "val_type_name", "+", "\":\"", "+", "eval_loss_str", ")", "\n", "logging", ".", "info", "(", "val_type_name", "+", "\":\"", "+", "eval_str", ")", "\n", "model", ".", "train", "(", ")", "\n", "dev_gen", "=", "None", "\n", "return", "eval_loss", ",", "macro_f1", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.evaluate_data_cv": [[484, 523], ["model.eval", "main.get_data_gen", "print", "enumerate", "model_utils.get_eval_string", "tensorboard.add_validation_scalar", "tensorboard.add_validation_scalar", "print", "print", "print", "logging.info", "logging.info", "model.train", "zip", "len", "data_utils.to_torch", "model", "eval_batch[].data.cpu().clone().numpy", "model_utils.get_gold_pred_str", "annot_ids.extend", "loss.clone().item", "len", "repr", "model_utils.get_output_index", "get_output_index_rank", "sum", "eval_batch[].data.cpu().clone", "loss.clone", "repr", "eval_batch[].data.cpu", "set", "set"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.get_data_gen", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_eval_string", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.TensorboardWriter.add_validation_scalar", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.TensorboardWriter.add_validation_scalar", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.to_torch", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_gold_pred_str", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_output_index", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_output_index_rank"], ["", "def", "evaluate_data_cv", "(", "k_fold_count", ",", "batch_num", ",", "model", ",", "tensorboard", ",", "val_type_name", ",", "args", ",", "elmo", ",", "actual_f1", "=", "False", ")", ":", "\n", "  ", "model", ".", "eval", "(", ")", "\n", "data_gen", "=", "get_data_gen", "(", "'crowd/cv_3fold/dev_tree_{0}.json'", ".", "format", "(", "repr", "(", "k_fold_count", ")", ")", ",", "'test'", ",", "args", ",", "(", "constant", ".", "CHAR_DICT", ",", "None", ")", ",", "args", ".", "goal", ",", "elmo", "=", "elmo", ")", "\n", "gold_pred", "=", "[", "]", "\n", "annot_ids", "=", "[", "]", "\n", "binary_out", "=", "[", "]", "\n", "eval_loss", "=", "0.", "\n", "total_ex_count", "=", "0", "\n", "print", "(", "'==> evaluate_data_cv'", ")", "\n", "for", "n", ",", "batch", "in", "enumerate", "(", "data_gen", ")", ":", "\n", "    ", "total_ex_count", "+=", "len", "(", "batch", "[", "'y'", "]", ")", "\n", "eval_batch", ",", "annot_id", "=", "to_torch", "(", "batch", ")", "\n", "loss", ",", "output_logits", ",", "_", "=", "model", "(", "eval_batch", ",", "val_type_name", ")", "\n", "if", "actual_f1", ":", "\n", "      ", "output_index", "=", "get_output_index", "(", "output_logits", ",", "threshold", "=", "args", ".", "threshold", ")", "\n", "", "else", ":", "\n", "      ", "output_index", "=", "get_output_index_rank", "(", "output_logits", ",", "topk", "=", "args", ".", "topk", ")", "\n", "\n", "", "y", "=", "eval_batch", "[", "'y'", "]", ".", "data", ".", "cpu", "(", ")", ".", "clone", "(", ")", ".", "numpy", "(", ")", "\n", "gold_pred", "=", "get_gold_pred_str", "(", "output_index", ",", "y", ",", "args", ".", "goal", ")", "\n", "annot_ids", ".", "extend", "(", "annot_id", ")", "\n", "eval_loss", "+=", "loss", ".", "clone", "(", ")", ".", "item", "(", ")", "\n", "", "eval_accu", "=", "sum", "(", "[", "set", "(", "y", ")", "==", "set", "(", "yp", ")", "for", "y", ",", "yp", "in", "gold_pred", "]", ")", "*", "1.0", "/", "len", "(", "gold_pred", ")", "\n", "eval_str", "=", "get_eval_string", "(", "gold_pred", ")", "\n", "eval_loss_str", "=", "'Eval loss: {0:.7f} at step {1:d}'", ".", "format", "(", "eval_loss", ",", "batch_num", ")", "\n", "tensorboard", ".", "add_validation_scalar", "(", "'eval_acc_'", "+", "val_type_name", ",", "eval_accu", ",", "batch_num", ")", "\n", "tensorboard", ".", "add_validation_scalar", "(", "'eval_loss_'", "+", "val_type_name", ",", "eval_loss", ",", "batch_num", ")", "\n", "print", "(", "'EVAL: seen '", "+", "repr", "(", "total_ex_count", ")", "+", "' examples.'", ")", "\n", "print", "(", "val_type_name", "+", "\":\"", "+", "eval_loss_str", ")", "\n", "#print(gold_pred[:3])", "\n", "print", "(", "val_type_name", "+", "\":\"", "+", "eval_str", ")", "\n", "logging", ".", "info", "(", "val_type_name", "+", "\":\"", "+", "eval_loss_str", ")", "\n", "logging", ".", "info", "(", "val_type_name", "+", "\":\"", "+", "eval_str", ")", "\n", "model", ".", "train", "(", ")", "\n", "data_gen", "=", "None", "\n", "output_dict", "=", "{", "}", "\n", "for", "a_id", ",", "(", "gold", ",", "pred", ")", "in", "zip", "(", "annot_ids", ",", "gold_pred", ")", ":", "\n", "    ", "output_dict", "[", "a_id", "]", "=", "{", "\"gold\"", ":", "gold", ",", "\"pred\"", ":", "pred", "}", "\n", "", "return", "eval_loss", ",", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.load_model": [[525, 549], ["torch.load", "model.load_state_dict", "logging.info", "print", "optimizer.load_state_dict", "logging.info", "print", "elem.size", "print", "elem.size", "constant.EXP_ROOT", "constant.EXP_ROOT", "constant.EXP_ROOT", "constant.EXP_ROOT"], "function", ["None"], ["", "def", "load_model", "(", "reload_model_name", ",", "save_dir", ",", "model_id", ",", "model", ",", "optimizer", "=", "None", ")", ":", "\n", "  ", "if", "reload_model_name", ":", "\n", "    ", "model_file_name", "=", "'{0:s}/{1:s}.pt'", ".", "format", "(", "save_dir", ",", "reload_model_name", ")", "\n", "", "else", ":", "\n", "    ", "model_file_name", "=", "'{0:s}/{1:s}.pt'", ".", "format", "(", "save_dir", ",", "model_id", ")", "\n", "", "checkpoint", "=", "torch", ".", "load", "(", "model_file_name", ")", "\n", "model", ".", "load_state_dict", "(", "checkpoint", "[", "'state_dict'", "]", ")", "\n", "if", "optimizer", ":", "\n", "    ", "optimizer", ".", "load_state_dict", "(", "checkpoint", "[", "'optimizer'", "]", ")", "\n", "", "else", ":", "\n", "    ", "total_params", "=", "0", "\n", "# Log params", "\n", "for", "k", "in", "checkpoint", "[", "'state_dict'", "]", ":", "\n", "      ", "elem", "=", "checkpoint", "[", "'state_dict'", "]", "[", "k", "]", "\n", "param_s", "=", "1", "\n", "for", "size_dim", "in", "elem", ".", "size", "(", ")", ":", "\n", "        ", "param_s", "=", "size_dim", "*", "param_s", "\n", "", "print", "(", "k", ",", "elem", ".", "size", "(", ")", ")", "\n", "total_params", "+=", "param_s", "\n", "", "param_str", "=", "(", "'Number of total parameters..{0:d}'", ".", "format", "(", "total_params", ")", ")", "\n", "logging", ".", "info", "(", "param_str", ")", "\n", "print", "(", "param_str", ")", "\n", "", "logging", ".", "info", "(", "\"Loading old file from {0:s}\"", ".", "format", "(", "model_file_name", ")", ")", "\n", "print", "(", "'Loading model from ... {0:s}'", ".", "format", "(", "model_file_name", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.load_model_partially": [[551, 583], ["torch.load", "model.state_dict", "model.state_dict.update", "model.load_state_dict", "logging.info", "print", "model.named_parameters", "optimizer.load_state_dict", "logging.info", "print", "pretrained_state_dict.items", "elem.size", "print", "elem.size"], "function", ["None"], ["", "def", "load_model_partially", "(", "reload_model_name", ",", "save_dir", ",", "model_id", ",", "model", ",", "freeze", "=", "False", ",", "optimizer", "=", "None", ")", ":", "\n", "  ", "if", "reload_model_name", ":", "\n", "    ", "model_file_name", "=", "'{0:s}/{1:s}.pt'", ".", "format", "(", "save_dir", ",", "reload_model_name", ")", "\n", "", "else", ":", "\n", "    ", "model_file_name", "=", "'{0:s}/{1:s}.pt'", ".", "format", "(", "save_dir", ",", "model_id", ")", "\n", "", "checkpoint", "=", "torch", ".", "load", "(", "model_file_name", ")", "\n", "pretrained_state_dict", "=", "checkpoint", "[", "'state_dict'", "]", "\n", "model_state_dict", "=", "model", ".", "state_dict", "(", ")", "\n", "pretrained_state_dict", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "pretrained_state_dict", ".", "items", "(", ")", "if", "k", "in", "model_state_dict", "}", "\n", "model_state_dict", ".", "update", "(", "pretrained_state_dict", ")", "\n", "model", ".", "load_state_dict", "(", "model_state_dict", ")", "\n", "if", "freeze", ":", "\n", "    ", "for", "pname", ",", "param", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "      ", "if", "pname", "in", "pretrained_state_dict", ":", "\n", "        ", "param", ".", "requires_grad", "=", "False", "\n", "", "", "", "if", "optimizer", ":", "\n", "    ", "optimizer", ".", "load_state_dict", "(", "checkpoint", "[", "'optimizer'", "]", ")", "\n", "", "else", ":", "\n", "    ", "total_params", "=", "0", "\n", "# Log params", "\n", "for", "k", "in", "checkpoint", "[", "'state_dict'", "]", ":", "\n", "      ", "elem", "=", "checkpoint", "[", "'state_dict'", "]", "[", "k", "]", "\n", "param_s", "=", "1", "\n", "for", "size_dim", "in", "elem", ".", "size", "(", ")", ":", "\n", "        ", "param_s", "=", "size_dim", "*", "param_s", "\n", "", "print", "(", "k", ",", "elem", ".", "size", "(", ")", ")", "\n", "total_params", "+=", "param_s", "\n", "", "param_str", "=", "(", "'Number of total parameters..{0:d}'", ".", "format", "(", "total_params", ")", ")", "\n", "logging", ".", "info", "(", "param_str", ")", "\n", "print", "(", "param_str", ")", "\n", "", "logging", ".", "info", "(", "\"Loading old file from {0:s}\"", ".", "format", "(", "model_file_name", ")", ")", "\n", "print", "(", "'Loading model from ... {0:s}'", ".", "format", "(", "model_file_name", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main._test": [[585, 646], ["main.get_datasets", "models.Bert.cuda", "models.Bert.eval", "main.load_model", "print", "models.ETModel", "print", "enumerate", "model_utils.get_eval_string", "print", "logging.info", "logging.info", "print", "models.Bert", "print", "print", "data_utils.to_torch", "models.Bert.", "model_utils.get_output_index", "eval_batch[].data.cpu().clone().numpy", "model_utils.get_gold_pred_str", "total_gold_pred.extend", "total_annot_ids.extend", "open", "zip", "json.dump", "isinstance", "print", "eval_batch[].data.cpu().clone", "eval_batch[].data.cpu"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.get_datasets", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.load_model", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_eval_string", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.to_torch", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_output_index", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_gold_pred_str"], ["", "def", "_test", "(", "args", ")", ":", "\n", "  ", "assert", "args", ".", "load", "\n", "test_fname", "=", "args", ".", "eval_data", "\n", "data_gens", ",", "_", "=", "get_datasets", "(", "[", "(", "test_fname", ",", "'test'", ",", "args", ".", "goal", ")", "]", ",", "args", ")", "\n", "if", "args", ".", "model_type", "==", "'et_model'", ":", "\n", "    ", "print", "(", "'==> Entity Typing Model'", ")", "\n", "model", "=", "models", ".", "ETModel", "(", "args", ",", "constant", ".", "ANSWER_NUM_DICT", "[", "args", ".", "goal", "]", ")", "\n", "", "elif", "args", ".", "model_type", "==", "'bert_uncase_small'", ":", "\n", "    ", "print", "(", "'==> Bert Uncased Small'", ")", "\n", "model", "=", "models", ".", "Bert", "(", "args", ",", "constant", ".", "ANSWER_NUM_DICT", "[", "args", ".", "goal", "]", ")", "\n", "", "else", ":", "\n", "    ", "print", "(", "'Invalid model type: -model_type '", "+", "args", ".", "model_type", ")", "\n", "raise", "NotImplementedError", "\n", "", "model", ".", "cuda", "(", ")", "\n", "model", ".", "eval", "(", ")", "\n", "load_model", "(", "args", ".", "reload_model_name", ",", "constant", ".", "EXP_ROOT", ",", "args", ".", "model_id", ",", "model", ")", "\n", "\n", "for", "name", ",", "dataset", "in", "[", "(", "test_fname", ",", "data_gens", "[", "0", "]", ")", "]", ":", "\n", "    ", "print", "(", "'Processing... '", "+", "name", ")", "\n", "total_gold_pred", "=", "[", "]", "\n", "total_annot_ids", "=", "[", "]", "\n", "total_probs", "=", "[", "]", "\n", "total_ys", "=", "[", "]", "\n", "batch_attn", "=", "[", "]", "\n", "for", "batch_num", ",", "batch", "in", "enumerate", "(", "dataset", ")", ":", "\n", "      ", "print", "(", "batch_num", ")", "\n", "if", "not", "isinstance", "(", "batch", ",", "dict", ")", ":", "\n", "        ", "print", "(", "'==> batch: '", ",", "batch", ")", "\n", "", "eval_batch", ",", "annot_ids", "=", "to_torch", "(", "batch", ")", "\n", "loss", ",", "output_logits", ",", "attn_score", "=", "model", "(", "eval_batch", ",", "args", ".", "goal", ")", "\n", "#batch_attn.append((batch, attn_score.data))", "\n", "output_index", "=", "get_output_index", "(", "output_logits", ",", "threshold", "=", "args", ".", "threshold", ")", "\n", "#output_prob = model.sigmoid_fn(output_logits).data.cpu().clone().numpy()", "\n", "y", "=", "eval_batch", "[", "'y'", "]", ".", "data", ".", "cpu", "(", ")", ".", "clone", "(", ")", ".", "numpy", "(", ")", "\n", "gold_pred", "=", "get_gold_pred_str", "(", "output_index", ",", "y", ",", "args", ".", "goal", ")", "\n", "#total_probs.extend(output_prob)", "\n", "#total_ys.extend(y)", "\n", "total_gold_pred", ".", "extend", "(", "gold_pred", ")", "\n", "total_annot_ids", ".", "extend", "(", "annot_ids", ")", "\n", "#mrr_val = mrr(total_probs, total_ys)", "\n", "#print('mrr_value: ', mrr_val)", "\n", "#pickle.dump({'gold_id_array': total_ys, 'pred_dist': total_probs},", "\n", "#            open('./{0:s}.p'.format(args.reload_model_name), \"wb\"))", "\n", "", "with", "open", "(", "'./{0:s}.json'", ".", "format", "(", "args", ".", "reload_model_name", ")", ",", "'w'", ")", "as", "f_out", ":", "\n", "      ", "output_dict", "=", "{", "}", "\n", "counter", "=", "0", "\n", "for", "a_id", ",", "(", "gold", ",", "pred", ")", "in", "zip", "(", "total_annot_ids", ",", "total_gold_pred", ")", ":", "\n", "#attn = batch_attn[0][1].squeeze(2)[counter]", "\n", "#attn = attn.cpu().numpy().tolist()", "\n", "#print(attn, int(batch_attn[0][0]['mention_span_length'][counter]), sum(attn))", "\n", "#print(mntn_emb[counter])", "\n", "#print()", "\n", "#print(int(batch_attn[0][0]['mention_span_length'][counter]), batch_attn[0][0]['mention_embed'][counter].shape)", "\n", "#attn = attn[:int(batch_attn[0][0]['mention_span_length'][counter])]", "\n", "        ", "output_dict", "[", "a_id", "]", "=", "{", "\"gold\"", ":", "gold", ",", "\"pred\"", ":", "pred", "}", "#, \"attn\": attn, \"mntn_len\": int(batch_attn[0][0]['mention_span_length'][counter])}", "\n", "counter", "+=", "1", "\n", "", "json", ".", "dump", "(", "output_dict", ",", "f_out", ")", "\n", "", "eval_str", "=", "get_eval_string", "(", "total_gold_pred", ")", "\n", "print", "(", "eval_str", ")", "\n", "logging", ".", "info", "(", "'processing: '", "+", "name", ")", "\n", "logging", ".", "info", "(", "eval_str", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main._test_labeler": [[648, 720], ["main.get_datasets", "denoising_models.Filter.cuda", "denoising_models.Filter.eval", "main.load_model", "print", "denoising_models.Labeler", "print", "enumerate", "pickle.dump", "model_utils.get_eval_string", "print", "logging.info", "logging.info", "print", "denoising_models.Filter", "print", "print", "data_utils.to_torch", "denoising_models.Filter.", "model_utils.get_output_index", "eval_batch[].data.cpu().clone().numpy", "eval_batch[].data.cpu().clone().numpy", "eval_batch[].data.cpu().clone().numpy", "model_utils.get_gold_pred_str", "total_gold_pred_pcls_ycls_ynoise.extend", "total_annot_ids.extend", "open", "open", "json.dump", "list", "isinstance", "print", "zip", "zip", "eval_batch[].data.cpu().clone", "eval_batch[].data.cpu().clone", "eval_batch[].data.cpu().clone", "zip", "print", "eval_batch[].data.cpu", "eval_batch[].data.cpu", "eval_batch[].data.cpu", "list", "zip"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.get_datasets", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.main.load_model", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_eval_string", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.to_torch", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_output_index", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_gold_pred_str"], ["", "", "def", "_test_labeler", "(", "args", ")", ":", "\n", "  ", "assert", "args", ".", "load", "\n", "test_fname", "=", "args", ".", "eval_data", "\n", "data_gens", ",", "_", "=", "get_datasets", "(", "[", "(", "test_fname", ",", "'test'", ",", "args", ".", "goal", ")", "]", ",", "args", ")", "\n", "if", "args", ".", "model_type", "==", "'labeler'", ":", "\n", "    ", "print", "(", "'==> Labeler'", ")", "\n", "model", "=", "denoising_models", ".", "Labeler", "(", "args", ",", "constant", ".", "ANSWER_NUM_DICT", "[", "args", ".", "goal", "]", ")", "\n", "", "elif", "args", ".", "model_type", "==", "'filter'", ":", "\n", "    ", "print", "(", "'==> Filter'", ")", "\n", "model", "=", "denoising_models", ".", "Filter", "(", "args", ",", "constant", ".", "ANSWER_NUM_DICT", "[", "args", ".", "goal", "]", ")", "\n", "", "else", ":", "\n", "    ", "print", "(", "'Invalid model type: -model_type '", "+", "args", ".", "model_type", ")", "\n", "raise", "NotImplementedError", "\n", "\n", "", "model", ".", "cuda", "(", ")", "\n", "model", ".", "eval", "(", ")", "\n", "load_model", "(", "args", ".", "reload_model_name", ",", "constant", ".", "EXP_ROOT", ",", "args", ".", "model_id", ",", "model", ")", "\n", "\n", "for", "name", ",", "dataset", "in", "[", "(", "test_fname", ",", "data_gens", "[", "0", "]", ")", "]", ":", "\n", "    ", "print", "(", "'Processing... '", "+", "name", ")", "\n", "total_gold_pred_pcls_ycls_ynoise", "=", "[", "]", "\n", "total_annot_ids", "=", "[", "]", "\n", "total_probs", "=", "[", "]", "\n", "total_ys", "=", "[", "]", "\n", "batch_attn", "=", "[", "]", "\n", "for", "batch_num", ",", "batch", "in", "enumerate", "(", "dataset", ")", ":", "\n", "      ", "print", "(", "batch_num", ")", "\n", "if", "not", "isinstance", "(", "batch", ",", "dict", ")", ":", "\n", "        ", "print", "(", "'==> batch: '", ",", "batch", ")", "\n", "", "eval_batch", ",", "annot_ids", "=", "to_torch", "(", "batch", ")", "\n", "#print('eval_batch')", "\n", "#for k, v in eval_batch.items():", "\n", "#  print(k, v.size())", "\n", "loss", ",", "output_logits", ",", "cls_logits", "=", "model", "(", "eval_batch", ",", "args", ".", "goal", ")", "\n", "#print('loss', loss)", "\n", "#print('output_logits', output_logits)", "\n", "#print('cls_logits', cls_logits)", "\n", "#batch_attn.append((batch, attn_score.data))", "\n", "output_index", "=", "get_output_index", "(", "output_logits", ",", "threshold", "=", "args", ".", "threshold", ")", "\n", "#print('output_index', output_index)", "\n", "#output_prob = model.sigmoid_fn(output_logits).data.cpu().clone().numpy()", "\n", "y", "=", "eval_batch", "[", "'y'", "]", ".", "data", ".", "cpu", "(", ")", ".", "clone", "(", ")", ".", "numpy", "(", ")", "\n", "y_cls", "=", "eval_batch", "[", "'y_cls'", "]", ".", "data", ".", "cpu", "(", ")", ".", "clone", "(", ")", ".", "numpy", "(", ")", "\n", "y_noisy_idx", "=", "eval_batch", "[", "'y_noisy_idx'", "]", ".", "data", ".", "cpu", "(", ")", ".", "clone", "(", ")", ".", "numpy", "(", ")", "\n", "gold_pred_pcls_ycls_ynoise", "=", "get_gold_pred_str", "(", "output_index", ",", "y", ",", "args", ".", "goal", ",", "cls_logits", "=", "cls_logits", ",", "y_cls", "=", "y_cls", ",", "y_noisy_idx", "=", "y_noisy_idx", ")", "\n", "#print('gold_pred_pcls_ycls_ynoise', gold_pred_pcls_ycls_ynoise)", "\n", "#total_probs.extend(output_prob)", "\n", "#total_ys.extend(y)", "\n", "total_gold_pred_pcls_ycls_ynoise", ".", "extend", "(", "gold_pred_pcls_ycls_ynoise", ")", "\n", "total_annot_ids", ".", "extend", "(", "annot_ids", ")", "\n", "#mrr_val = mrr(total_probs, total_ys)", "\n", "#print('mrr_value: ', mrr_val)", "\n", "#pickle.dump({'gold_id_array': total_ys, 'pred_dist': total_probs},", "\n", "#            open('./{0:s}.p'.format(args.reload_model_name), \"wb\"))", "\n", "", "pickle", ".", "dump", "(", "(", "total_annot_ids", ",", "total_gold_pred_pcls_ycls_ynoise", ")", ",", "\n", "open", "(", "'./{0:s}_gold_pred.p'", ".", "format", "(", "args", ".", "reload_model_name", ")", ",", "\"wb\"", ")", ")", "\n", "with", "open", "(", "'./{0:s}.json'", ".", "format", "(", "args", ".", "model_id", ")", ",", "'w'", ")", "as", "f_out", ":", "\n", "      ", "output_dict", "=", "{", "}", "\n", "if", "args", ".", "model_type", "==", "'filter'", ":", "\n", "        ", "for", "a_id", ",", "(", "gold", ",", "pred", ",", "cls", ",", "ycls", ",", "ynoise", ")", "in", "zip", "(", "total_annot_ids", ",", "total_gold_pred_pcls_ycls_ynoise", ")", ":", "\n", "          ", "output_dict", "[", "a_id", "]", "=", "{", "\"gold\"", ":", "gold", ",", "\"pred\"", ":", "pred", ",", "\"cls_pred\"", ":", "cls", ",", "\"cls_gold\"", ":", "ycls", ",", "\"y_noisy\"", ":", "ynoise", "}", "\n", "", "", "elif", "args", ".", "model_type", "==", "'labeler'", ":", "\n", "        ", "for", "a_id", ",", "(", "gold", ",", "pred", ")", "in", "zip", "(", "total_annot_ids", ",", "total_gold_pred_pcls_ycls_ynoise", ")", ":", "\n", "          ", "output_dict", "[", "a_id", "]", "=", "{", "\"gold\"", ":", "gold", ",", "\"pred\"", ":", "pred", "}", "\n", "", "", "else", ":", "\n", "        ", "print", "(", "'Invalid model type: -model_type '", "+", "args", ".", "model_type", ")", "\n", "raise", "NotImplementedError", "\n", "", "json", ".", "dump", "(", "output_dict", ",", "f_out", ")", "\n", "", "eval_str", "=", "get_eval_string", "(", "list", "(", "zip", "(", "*", "list", "(", "zip", "(", "*", "gold_pred_pcls_ycls_ynoise", ")", ")", "[", ":", "2", "]", ")", ")", ")", "\n", "print", "(", "eval_str", ")", "\n", "logging", ".", "info", "(", "'processing: '", "+", "name", ")", "\n", "logging", ".", "info", "(", "eval_str", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.MultiSimpleDecoder.__init__": [[223, 227], ["torch.nn.Module.__init__", "torch.nn.Linear().cuda", "torch.nn.Linear().cuda", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "output_dim", ")", ":", "\n", "    ", "super", "(", "MultiSimpleDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "output_dim", ",", "constant", ".", "ANSWER_NUM_DICT", "[", "'open'", "]", ",", "\n", "bias", "=", "True", ")", ".", "cuda", "(", ")", "# (out_features x in_features)  #### bias", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.MultiSimpleDecoder.forward": [[228, 239], ["model_utils.MultiSimpleDecoder.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "ValueError"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "output_type", ")", ":", "\n", "    ", "if", "output_type", "==", "\"open\"", ":", "\n", "      ", "return", "self", ".", "linear", "(", "inputs", ")", "\n", "", "elif", "output_type", "==", "'wiki'", ":", "\n", "      ", "return", "F", ".", "linear", "(", "inputs", ",", "self", ".", "linear", ".", "weight", "[", ":", "constant", ".", "ANSWER_NUM_DICT", "[", "'wiki'", "]", ",", ":", "]", ",", "self", ".", "linear", ".", "bias", "[", ":", "constant", ".", "ANSWER_NUM_DICT", "[", "'wiki'", "]", "]", ")", "\n", "#return F.linear(inputs, self.linear.weight[:constant.ANSWER_NUM_DICT['wiki'], :])", "\n", "", "elif", "output_type", "==", "'kb'", ":", "\n", "      ", "return", "F", ".", "linear", "(", "inputs", ",", "self", ".", "linear", ".", "weight", "[", ":", "constant", ".", "ANSWER_NUM_DICT", "[", "'kb'", "]", ",", ":", "]", ",", "self", ".", "linear", ".", "bias", "[", ":", "constant", ".", "ANSWER_NUM_DICT", "[", "'kb'", "]", "]", ")", "\n", "#return F.linear(inputs, self.linear.weight[:constant.ANSWER_NUM_DICT['kb'], :])", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "'Decoder error: output type not one of the valid'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.MultiSimpleDecoderBinary.__init__": [[246, 250], ["torch.nn.Module.__init__", "torch.nn.Linear().cuda", "torch.nn.Linear().cuda", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "output_dim", ")", ":", "\n", "    ", "super", "(", "MultiSimpleDecoderBinary", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "output_dim", ",", "1", ",", "\n", "bias", "=", "False", ")", ".", "cuda", "(", ")", "# (out_features x in_features)", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.MultiSimpleDecoderBinary.forward": [[251, 260], ["model_utils.MultiSimpleDecoderBinary.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "ValueError"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "output_type", ")", ":", "\n", "    ", "if", "output_type", "==", "\"open\"", ":", "\n", "      ", "return", "self", ".", "linear", "(", "inputs", ")", "\n", "", "elif", "output_type", "==", "'wiki'", ":", "\n", "      ", "return", "F", ".", "linear", "(", "inputs", ",", "self", ".", "linear", ".", "weight", "[", ":", "constant", ".", "ANSWER_NUM_DICT", "[", "'wiki'", "]", ",", ":", "]", ",", "self", ".", "linear", ".", "bias", ")", "\n", "", "elif", "output_type", "==", "'kb'", ":", "\n", "      ", "return", "F", ".", "linear", "(", "inputs", ",", "self", ".", "linear", ".", "weight", "[", ":", "constant", ".", "ANSWER_NUM_DICT", "[", "'kb'", "]", ",", ":", "]", ",", "self", ".", "linear", ".", "bias", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "'Decoder error: output type not one of the valid'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.SimpleDecoder.__init__": [[263, 267], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["  ", "def", "__init__", "(", "self", ",", "output_dim", ",", "answer_num", ")", ":", "\n", "    ", "super", "(", "SimpleDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "answer_num", "=", "answer_num", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "output_dim", ",", "answer_num", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.SimpleDecoder.forward": [[268, 271], ["model_utils.SimpleDecoder.linear"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "output_type", ")", ":", "\n", "    ", "output_embed", "=", "self", ".", "linear", "(", "inputs", ")", "\n", "return", "output_embed", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.CNN.__init__": [[274, 278], ["torch.nn.Module.__init__", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Embedding", "torch.nn.Embedding"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["  ", "def", "__init__", "(", "self", ")", ":", "\n", "    ", "super", "(", "CNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv1d", "=", "nn", ".", "Conv1d", "(", "100", ",", "50", ",", "5", ")", "# input, output, filter_number", "\n", "self", ".", "char_W", "=", "nn", ".", "Embedding", "(", "115", ",", "100", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.CNN.forward": [[279, 286], ["model_utils.CNN.char_W().transpose", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "model_utils.CNN.conv1d", "torch.relu", "torch.relu", "torch.max_pool1d", "torch.max_pool1d", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model_utils.CNN.char_W", "i.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "span_chars", ")", ":", "\n", "    ", "char_embed", "=", "self", ".", "char_W", "(", "span_chars", ")", ".", "transpose", "(", "1", ",", "2", ")", "# [batch_size, char_embedding, max_char_seq]", "\n", "conv_output", "=", "[", "self", ".", "conv1d", "(", "char_embed", ")", "]", "# list of [batch_size, filter_dim, max_char_seq, filter_number]", "\n", "conv_output", "=", "[", "F", ".", "relu", "(", "c", ")", "for", "c", "in", "conv_output", "]", "# batch_size, filter_dim, max_char_seq, filter_num", "\n", "cnn_rep", "=", "[", "F", ".", "max_pool1d", "(", "i", ",", "i", ".", "size", "(", "2", ")", ")", "for", "i", "in", "conv_output", "]", "# batch_size, filter_dim, 1, filter_num", "\n", "cnn_output", "=", "torch", ".", "squeeze", "(", "torch", ".", "cat", "(", "cnn_rep", ",", "1", ")", ",", "2", ")", "# batch_size, filter_num * filter_dim, 1", "\n", "return", "cnn_output", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.SelfAttentiveSum.__init__": [[293, 300], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Softmax", "torch.nn.Softmax"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "output_dim", ",", "hidden_dim", ")", ":", "\n", "    ", "super", "(", "SelfAttentiveSum", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "key_maker", "=", "nn", ".", "Linear", "(", "output_dim", ",", "hidden_dim", ",", "bias", "=", "False", ")", "\n", "self", ".", "key_rel", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "\n", "self", ".", "key_output", "=", "nn", ".", "Linear", "(", "hidden_dim", ",", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "key_softmax", "=", "nn", ".", "Softmax", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.SelfAttentiveSum._masked_softmax": [[301, 310], ["torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.max", "torch.max", "torch.max", "torch.max", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["None"], ["", "def", "_masked_softmax", "(", "self", ",", "X", ",", "mask", "=", "None", ",", "alpha", "=", "1e-20", ")", ":", "\n", "# X, (batch_size, seq_length)", "\n", "    ", "X_max", "=", "torch", ".", "max", "(", "X", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "[", "0", "]", "\n", "X_exp", "=", "torch", ".", "exp", "(", "X", "-", "X_max", ")", "\n", "if", "mask", "is", "None", ":", "\n", "      ", "mask", "=", "(", "X", "!=", "0", ")", ".", "float", "(", ")", "\n", "", "X_exp", "=", "X_exp", "*", "mask", "\n", "X_softmax", "=", "X_exp", "/", "(", "torch", ".", "sum", "(", "X_exp", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "+", "alpha", ")", "\n", "return", "X_softmax", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.SelfAttentiveSum.forward": [[311, 324], ["input_embed.view", "model_utils.SelfAttentiveSum.key_maker", "model_utils.SelfAttentiveSum.key_rel", "model_utils.SelfAttentiveSum._masked_softmax().view", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "model_utils.SelfAttentiveSum.view", "model_utils.SelfAttentiveSum.key_output().view", "input_embed.size", "model_utils.SelfAttentiveSum._masked_softmax", "input_embed.size", "input_embed.size", "model_utils.SelfAttentiveSum.key_output", "input_embed.size"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.TypeAttentiveSum._masked_softmax"], ["", "def", "forward", "(", "self", ",", "input_embed", ")", ":", "\n", "    ", "mask", "=", "(", "input_embed", "[", ":", ",", ":", ",", "0", "]", "!=", "0", ")", ".", "float", "(", ")", "\n", "input_embed_squeezed", "=", "input_embed", ".", "view", "(", "-", "1", ",", "input_embed", ".", "size", "(", ")", "[", "2", "]", ")", "\n", "k_d", "=", "self", ".", "key_maker", "(", "input_embed_squeezed", ")", "\n", "k_d", "=", "self", ".", "key_rel", "(", "k_d", ")", "# this leads all zeros", "\n", "if", "self", ".", "hidden_dim", "==", "1", ":", "\n", "      ", "k", "=", "k_d", ".", "view", "(", "input_embed", ".", "size", "(", ")", "[", "0", "]", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "      ", "k", "=", "self", ".", "key_output", "(", "k_d", ")", ".", "view", "(", "input_embed", ".", "size", "(", ")", "[", "0", "]", ",", "-", "1", ")", "# (batch_size, seq_length)", "\n", "", "weighted_keys", "=", "self", ".", "_masked_softmax", "(", "k", ",", "mask", "=", "mask", ")", ".", "view", "(", "input_embed", ".", "size", "(", ")", "[", "0", "]", ",", "-", "1", ",", "1", ")", "\n", "#weighted_keys = self.key_softmax(k).view(input_embed.size()[0], -1, 1)", "\n", "weighted_values", "=", "torch", ".", "sum", "(", "weighted_keys", "*", "input_embed", ",", "1", ")", "# batch_size, seq_length, embed_dim", "\n", "return", "weighted_values", ",", "weighted_keys", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils._Loss.__init__": [[328, 334], ["torch.nn.Module.__init__", "_Reduction.legacy_get_string"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "size_average", "=", "None", ",", "reduce", "=", "None", ",", "reduction", "=", "'elementwise_mean'", ")", ":", "\n", "        ", "super", "(", "_Loss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "size_average", "is", "not", "None", "or", "reduce", "is", "not", "None", ":", "\n", "            ", "self", ".", "reduction", "=", "_Reduction", ".", "legacy_get_string", "(", "size_average", ",", "reduce", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "reduction", "=", "reduction", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.BCEWithLogitsLossCustom.__init__": [[336, 341], ["model_utils._Loss.__init__", "model_utils.BCEWithLogitsLossCustom.register_buffer", "model_utils.BCEWithLogitsLossCustom.register_buffer", "model_utils.BCEWithLogitsLossCustom.register_buffer"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "weight", "=", "None", ",", "size_average", "=", "None", ",", "reduce", "=", "None", ",", "reduction", "=", "'elementwise_mean'", ",", "pos_weight", "=", "None", ",", "neg_weight", "=", "None", ")", ":", "\n", "        ", "super", "(", "BCEWithLogitsLossCustom", ",", "self", ")", ".", "__init__", "(", "size_average", ",", "reduce", ",", "reduction", ")", "\n", "self", ".", "register_buffer", "(", "'weight'", ",", "weight", ")", "\n", "self", ".", "register_buffer", "(", "'pos_weight'", ",", "pos_weight", ")", "\n", "self", ".", "register_buffer", "(", "'neg_weight'", ",", "neg_weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.BCEWithLogitsLossCustom.forward": [[342, 352], ["model_utils.binary_cross_entropy_with_logits_custom"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.binary_cross_entropy_with_logits_custom"], ["", "def", "forward", "(", "self", ",", "input", ",", "target", ",", "pos_weight", "=", "None", ",", "neg_weight", "=", "None", ")", ":", "\n", "        ", "if", "pos_weight", "is", "None", ":", "\n", "            ", "pos_weight", "=", "self", ".", "pos_weight", "\n", "", "if", "neg_weight", "is", "None", ":", "\n", "            ", "neg_weight", "=", "self", ".", "neg_weight", "\n", "", "return", "binary_cross_entropy_with_logits_custom", "(", "input", ",", "target", ",", "\n", "self", ".", "weight", ",", "\n", "pos_weight", "=", "pos_weight", ",", "\n", "neg_weight", "=", "neg_weight", ",", "\n", "reduction", "=", "self", ".", "reduction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.StructuredPerceptronLoss.__init__": [[389, 391], ["model_utils._Loss.__init__"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["  ", "def", "__init__", "(", "self", ",", "size_average", "=", "None", ",", "reduce", "=", "None", ",", "reduction", "=", "'elementwise_mean'", ")", ":", "\n", "    ", "super", "(", "StructuredPerceptronLoss", ",", "self", ")", ".", "__init__", "(", "size_average", ",", "reduce", ",", "reduction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.StructuredPerceptronLoss.forward": [[392, 402], ["torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp.mean", "torch.clamp.mean", "torch.clamp.sum", "torch.clamp.sum"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ",", "target", ")", ":", "\n", "\n", "    ", "loss", "=", "torch", ".", "clamp", "(", "input", "-", "target", ",", "min", "=", "0.", ")", "\n", "\n", "if", "self", ".", "reduction", "==", "'none'", ":", "\n", "        ", "return", "loss", "\n", "", "elif", "self", ".", "reduction", "==", "'elementwise_mean'", ":", "\n", "        ", "return", "loss", ".", "mean", "(", ")", "\n", "", "else", ":", "\n", "        ", "return", "loss", ".", "sum", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.CNNReducer.__init__": [[435, 443], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.MaxPool2d", "torch.nn.MaxPool2d", "torch.nn.MaxPool2d", "torch.nn.MaxPool2d", "torch.nn.BatchNorm2d", "torch.nn.BatchNorm2d", "torch.nn.BatchNorm2d", "torch.nn.BatchNorm2d"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["  ", "def", "__init__", "(", "self", ")", ":", "\n", "    ", "super", "(", "CNNReducer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv2d_1", "=", "nn", ".", "Conv2d", "(", "1", ",", "8", ",", "(", "3", ",", "3", ")", ",", "stride", "=", "(", "1", ",", "1", ")", ",", "padding", "=", "(", "1", ",", "1", ")", ")", "# in, out, kernel size", "\n", "self", ".", "conv2d_2", "=", "nn", ".", "Conv2d", "(", "8", ",", "16", ",", "(", "3", ",", "3", ")", ",", "stride", "=", "(", "1", ",", "1", ")", ",", "padding", "=", "(", "1", ",", "1", ")", ")", "\n", "self", ".", "pool1", "=", "nn", ".", "MaxPool2d", "(", "(", "4", ",", "2", ")", ",", "stride", "=", "(", "4", ",", "1", ")", ",", "padding", "=", "0", ",", "ceil_mode", "=", "False", ")", "\n", "self", ".", "pool2", "=", "nn", ".", "MaxPool2d", "(", "(", "4", ",", "2", ")", ",", "stride", "=", "(", "4", ",", "1", ")", ",", "padding", "=", "0", ",", "ceil_mode", "=", "False", ")", "\n", "self", ".", "bn1", "=", "nn", ".", "BatchNorm2d", "(", "8", ")", "\n", "self", ".", "bn2", "=", "nn", ".", "BatchNorm2d", "(", "16", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.CNNReducer.forward": [[444, 455], ["torch.relu", "torch.relu", "model_utils.CNNReducer.pool1", "torch.relu", "torch.relu", "model_utils.CNNReducer.pool2", "x.view.view.view", "model_utils.CNNReducer.bn1", "model_utils.CNNReducer.bn2", "model_utils.CNNReducer.conv2d_1", "model_utils.CNNReducer.conv2d_2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "    ", "\"\"\"\n    x:  ELMo vectors of (batch size, 1024, 3).\n\n    \"\"\"", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "bn1", "(", "self", ".", "conv2d_1", "(", "x", ")", ")", ")", "# 1024x3 -> 1024x3", "\n", "x", "=", "self", ".", "pool1", "(", "x", ")", "# 1024x3 -> 256x2", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "bn2", "(", "self", ".", "conv2d_2", "(", "x", ")", ")", ")", "# 256x2  -> 256x2", "\n", "x", "=", "self", ".", "pool2", "(", "x", ")", "# 256x2  -> 64x1", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "1024", ")", "# 64x16 = 1024, (batch size x dim)", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.ELMoWeightedSum.__init__": [[458, 463], ["torch.nn.Module.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Softmax", "torch.nn.Softmax", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["  ", "def", "__init__", "(", "self", ")", ":", "\n", "    ", "super", "(", "ELMoWeightedSum", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "gamma", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "1", ")", ")", "\n", "self", ".", "S", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "1", ",", "3", ")", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.ELMoWeightedSum.forward": [[464, 484], ["model_utils.ELMoWeightedSum.softmax", "x.view.view.dim", "x.view.view.permute().contiguous().view", "x.view.view.view", "x.view.view.dim", "x.view.view.permute().contiguous().view", "x.view.view.view", "print", "x.view.view.permute().contiguous", "x.view.view.permute().contiguous", "repr", "x.view.view.permute", "x.view.view.dim", "x.view.view.permute"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "    ", "\"\"\"\n    x:  ELMo vectors of (batch size, 3, 1024) or (batch size, 3, seq len, 1024).\n\n    \"\"\"", "\n", "S", "=", "self", ".", "softmax", "(", "self", ".", "S", ")", "# normalize", "\n", "if", "x", ".", "dim", "(", ")", "==", "3", ":", "\n", "      ", "batch_size", ",", "n_layers", ",", "emb_dim", "=", "x", ".", "shape", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "3", ")", "# (batch_size*1024, 3)", "\n", "x", "=", "(", "x", "*", "S", ")", ".", "sum", "(", "1", ")", "*", "self", ".", "gamma", "# (batch_size*1024, 1)", "\n", "x", "=", "x", ".", "view", "(", "batch_size", ",", "emb_dim", ")", "# (batch_size, 1024)", "\n", "", "elif", "x", ".", "dim", "(", ")", "==", "4", ":", "\n", "      ", "batch_size", ",", "n_layers", ",", "seq_len", ",", "emb_dim", "=", "x", ".", "shape", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "3", ")", "# (batch_size*seq_len*1024, 3)", "\n", "x", "=", "(", "x", "*", "S", ")", ".", "sum", "(", "1", ")", "*", "self", ".", "gamma", "# (batch_size*seq_len*1024, 1)", "\n", "x", "=", "x", ".", "view", "(", "batch_size", ",", "seq_len", ",", "emb_dim", ")", "# (batch_size, seq_len, 1024)", "\n", "", "else", ":", "\n", "      ", "print", "(", "'Wrong input dimension: x.dim() = '", "+", "repr", "(", "x", ".", "dim", "(", ")", ")", ")", "\n", "raise", "ValueError", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.LinearProjection.__init__": [[487, 492], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["  ", "def", "__init__", "(", "self", ",", "in_features", ",", "out_features", ",", "bias", "=", "True", ")", ":", "\n", "    ", "super", "(", "LinearProjection", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "in_features", "=", "in_features", "\n", "self", ".", "out_features", "=", "out_features", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.LinearProjection.forward": [[493, 512], ["x.view.view.dim", "x.view.view.view", "model_utils.LinearProjection.linear", "x.view.view.view", "x.view.view.dim", "x.view.view.view", "model_utils.LinearProjection.linear", "x.view.view.view", "print", "repr", "x.view.view.dim"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "    ", "\"\"\"\n    x:  ELMo vectors of (batch size, 3, 1024) or (batch size, 3, seq len, 1024).\n\n    \"\"\"", "\n", "if", "x", ".", "dim", "(", ")", "==", "2", ":", "\n", "      ", "batch_size", ",", "emb_dim", "=", "x", ".", "shape", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "self", ".", "in_features", ")", "# (batch_size, 1024)", "\n", "x", "=", "self", ".", "linear", "(", "x", ")", "\n", "x", "=", "x", ".", "view", "(", "batch_size", ",", "self", ".", "out_features", ")", "# (batch_size, 300)", "\n", "", "elif", "x", ".", "dim", "(", ")", "==", "3", ":", "\n", "      ", "batch_size", ",", "seq_len", ",", "emb_dim", "=", "x", ".", "shape", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "self", ".", "in_features", ")", "# (batch_size*seq_len, 300)", "\n", "x", "=", "self", ".", "linear", "(", "x", ")", "\n", "x", "=", "x", ".", "view", "(", "batch_size", ",", "seq_len", ",", "self", ".", "out_features", ")", "# (batch_size, seq_len, 300)", "\n", "", "else", ":", "\n", "      ", "print", "(", "'Wrong input dimension: x.dim() = '", "+", "repr", "(", "x", ".", "dim", "(", ")", ")", ")", "\n", "raise", "ValueError", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.MultiRNNoutput2Logits.__init__": [[519, 523], ["torch.nn.Module.__init__", "torch.nn.Linear().cuda", "torch.nn.Linear().cuda", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "output_dim", ")", ":", "\n", "    ", "super", "(", "MultiSimpleDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "output_dim", ",", "constant", ".", "ANSWER_NUM_DICT", "[", "'open'", "]", ",", "\n", "bias", "=", "False", ")", ".", "cuda", "(", ")", "# (out_features x in_features)", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.MultiRNNoutput2Logits.forward": [[524, 533], ["model_utils.MultiRNNoutput2Logits.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "ValueError"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "output_type", ")", ":", "\n", "    ", "if", "output_type", "==", "\"open\"", ":", "\n", "      ", "return", "self", ".", "linear", "(", "inputs", ")", "\n", "", "elif", "output_type", "==", "'wiki'", ":", "\n", "      ", "return", "F", ".", "linear", "(", "inputs", ",", "self", ".", "linear", ".", "weight", "[", ":", "constant", ".", "ANSWER_NUM_DICT", "[", "'wiki'", "]", ",", ":", "]", ",", "self", ".", "linear", ".", "bias", ")", "\n", "", "elif", "output_type", "==", "'kb'", ":", "\n", "      ", "return", "F", ".", "linear", "(", "inputs", ",", "self", ".", "linear", ".", "weight", "[", ":", "constant", ".", "ANSWER_NUM_DICT", "[", "'kb'", "]", ",", ":", "]", ",", "self", ".", "linear", ".", "bias", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "'Decoder error: output type not one of the valid'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.RNNDecoderState.__init__": [[539, 554], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.LogSoftmax", "torch.nn.LogSoftmax"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "hidden_size", ",", "rnn_cell", "=", "'lstm'", ",", "type_emb_size", "=", "300", ",", "n_layers", "=", "1", ")", ":", "\n", "    ", "super", "(", "RNNDecoderState", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dec_emb", "=", "nn", ".", "Embedding", "(", "\n", "constant", ".", "ANSWER_NUM_DICT", "[", "'open'", "]", "+", "3", ",", "# add <BOS> and <EOS> ", "\n", "type_emb_size", "\n", ")", "\n", "self", ".", "rnn_cell", "=", "self", ".", "RNN_CELL", "[", "rnn_cell", "]", "(", "\n", "input_size", "=", "type_emb_size", ",", "\n", "hidden_size", "=", "hidden_size", ",", "\n", "bias", "=", "True", "\n", ")", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "constant", ".", "ANSWER_NUM_DICT", "[", "'open'", "]", "+", "3", ",", "bias", "=", "True", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "1", ")", "\n", "self", ".", "n_layers", "=", "n_layers", "\n", "self", ".", "_rnn_cell", "=", "rnn_cell", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.RNNDecoderState.forward": [[555, 566], ["model_utils.RNNDecoderState.dec_emb", "range", "model_utils.RNNDecoderState.linear", "model_utils.RNNDecoderState.softmax", "model_utils.RNNDecoderState.rnn_cell", "curr_hiddens.append"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_", ",", "prev_hidden", ",", "enc_hiddens", ")", ":", "\n", "    ", "input_", "=", "self", ".", "dec_emb", "(", "input_", ")", "\n", "curr_hiddens", "=", "[", "]", "\n", "for", "layer_idx", "in", "range", "(", "self", ".", "n_layers", ")", ":", "\n", "      ", "curr_hidden", "=", "self", ".", "rnn_cell", "(", "input_", ",", "prev_hidden", "[", "layer_idx", "]", ")", "# hidden: (ht, ct) for LSTM, ht for GRU/Elman", "\n", "input_", "=", "curr_hidden", "[", "0", "]", "if", "self", ".", "_rnn_cell", "==", "'lstm'", "else", "curr_hidden", "\n", "curr_hiddens", ".", "append", "(", "curr_hidden", ")", "\n", "", "ht", "=", "input_", "\n", "logit", "=", "self", ".", "linear", "(", "ht", ")", "\n", "prob", "=", "self", ".", "softmax", "(", "logit", ")", "\n", "return", "prob", ",", "curr_hiddens", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.ConditionedAttentiveSum.__init__": [[570, 578], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Softmax", "torch.nn.Softmax"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["  ", "def", "__init__", "(", "self", ",", "output_dim", ",", "hidden_dim", ",", "mention_emb_dim", ")", ":", "\n", "    ", "super", "(", "ConditionedAttentiveSum", ",", "self", ")", ".", "__init__", "(", ")", "\n", "#self.dim_adjuster = nn.Linear(mention_emb_dim, hidden_dim, bias=False)", "\n", "self", ".", "key_maker", "=", "nn", ".", "Linear", "(", "output_dim", ",", "hidden_dim", ",", "bias", "=", "False", ")", "\n", "self", ".", "key_rel", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "\n", "#self.key_output = nn.Bilinear(hidden_dim, hidden_dim, 1, bias=False)", "\n", "self", ".", "key_softmax", "=", "nn", ".", "Softmax", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.ConditionedAttentiveSum._masked_softmax": [[579, 588], ["torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.max", "torch.max", "torch.max", "torch.max", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["None"], ["", "def", "_masked_softmax", "(", "self", ",", "X", ",", "mask", "=", "None", ",", "alpha", "=", "1e-20", ")", ":", "\n", "# X, (batch_size, seq_length)", "\n", "    ", "X_max", "=", "torch", ".", "max", "(", "X", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "[", "0", "]", "\n", "X_exp", "=", "torch", ".", "exp", "(", "X", "-", "X_max", ")", "\n", "if", "mask", "is", "None", ":", "\n", "      ", "mask", "=", "(", "X", "!=", "0", ")", ".", "float", "(", ")", "\n", "", "X_exp", "=", "X_exp", "*", "mask", "\n", "X_softmax", "=", "X_exp", "/", "(", "torch", ".", "sum", "(", "X_exp", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "+", "alpha", ")", "\n", "return", "X_softmax", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.ConditionedAttentiveSum.forward": [[589, 609], ["input_embed.size", "input_embed.view", "model_utils.ConditionedAttentiveSum.key_rel", "model_utils.ConditionedAttentiveSum.key_maker", "model_utils.ConditionedAttentiveSum.key_rel", "model_utils.ConditionedAttentiveSum._masked_softmax().view", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "k_d.view.view.view", "cond_vec.unsqueeze().repeat.unsqueeze().repeat.unsqueeze().repeat", "k_d.view.view.view", "model_utils.ConditionedAttentiveSum._masked_softmax", "input_embed.size", "cond_vec.unsqueeze().repeat.unsqueeze().repeat.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.TypeAttentiveSum._masked_softmax"], ["", "def", "forward", "(", "self", ",", "input_embed", ",", "cond_vec", ")", ":", "\n", "    ", "batch_size", ",", "max_length", ",", "emb_dim", "=", "input_embed", ".", "size", "(", ")", "\n", "mask", "=", "(", "input_embed", "[", ":", ",", ":", ",", "0", "]", "!=", "0", ")", ".", "float", "(", ")", "\n", "input_embed_squeezed", "=", "input_embed", ".", "view", "(", "-", "1", ",", "emb_dim", ")", "\n", "#cond_vec = self.dim_adjuster(cond_vec)", "\n", "cond_vec", "=", "self", ".", "key_rel", "(", "cond_vec", ")", "\n", "k_d", "=", "self", ".", "key_maker", "(", "input_embed_squeezed", ")", "\n", "k_d", "=", "self", ".", "key_rel", "(", "k_d", ")", "# this leads all zeros", "\n", "if", "self", ".", "hidden_dim", "==", "1", ":", "\n", "      ", "k", "=", "k_d", ".", "view", "(", "batch_size", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "#cond_vec = cond_vec.unsqueeze(1).repeat(1, max_length, 1).view(batch_size*max_length, -1)", "\n", "#k = self.key_output(k_d, cond_vec).view(input_embed.size()[0], -1)  # (batch_size, seq_length)", "\n", "      ", "cond_vec", "=", "cond_vec", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "max_length", ",", "1", ")", "\n", "k_d", "=", "k_d", ".", "view", "(", "batch_size", ",", "max_length", ",", "-", "1", ")", "\n", "k", "=", "(", "k_d", "*", "cond_vec", ")", ".", "sum", "(", "2", ")", "\n", "", "weighted_keys", "=", "self", ".", "_masked_softmax", "(", "k", ",", "mask", "=", "mask", ")", ".", "view", "(", "input_embed", ".", "size", "(", ")", "[", "0", "]", ",", "-", "1", ",", "1", ")", "\n", "#weighted_keys = self.key_softmax(k).view(input_embed.size()[0], -1, 1)", "\n", "weighted_values", "=", "torch", ".", "sum", "(", "weighted_keys", "*", "input_embed", ",", "1", ")", "# batch_size, seq_length, embed_dim", "\n", "return", "weighted_values", ",", "weighted_keys", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.TypeAttentiveSum.__init__": [[613, 618], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Bilinear", "torch.nn.Bilinear"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["  ", "def", "__init__", "(", "self", ",", "output_dim", ",", "hidden_dim", ")", ":", "\n", "    ", "super", "(", "TypeAttentiveSum", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dim_adjuster", "=", "nn", ".", "Linear", "(", "output_dim", ",", "hidden_dim", ",", "bias", "=", "True", ")", "\n", "self", ".", "key_maker", "=", "nn", ".", "Bilinear", "(", "hidden_dim", ",", "hidden_dim", ",", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.TypeAttentiveSum._masked_softmax": [[619, 628], ["torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.max", "torch.max", "torch.max", "torch.max", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["None"], ["", "def", "_masked_softmax", "(", "self", ",", "X", ",", "mask", "=", "None", ",", "alpha", "=", "1e-20", ")", ":", "\n", "# X, (batch_size, seq_length)", "\n", "    ", "X_max", "=", "torch", ".", "max", "(", "X", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "[", "0", "]", "\n", "X_exp", "=", "torch", ".", "exp", "(", "X", "-", "X_max", ")", "\n", "if", "mask", "is", "None", ":", "\n", "      ", "mask", "=", "(", "X", "!=", "0", ")", ".", "float", "(", ")", "\n", "", "X_exp", "=", "X_exp", "*", "mask", "\n", "X_softmax", "=", "X_exp", "/", "(", "torch", ".", "sum", "(", "X_exp", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "+", "alpha", ")", "\n", "return", "X_softmax", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.TypeAttentiveSum.forward": [[629, 638], ["input_embed.size", "model_utils.TypeAttentiveSum.dim_adjuster", "cond_vec.unsqueeze().repeat.unsqueeze().repeat.unsqueeze().repeat", "model_utils.TypeAttentiveSum.key_maker().squeeze", "model_utils.TypeAttentiveSum._masked_softmax().view", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "cond_vec.unsqueeze().repeat.unsqueeze().repeat.unsqueeze", "model_utils.TypeAttentiveSum.key_maker", "model_utils.TypeAttentiveSum._masked_softmax", "input_embed.size"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.TypeAttentiveSum._masked_softmax"], ["", "def", "forward", "(", "self", ",", "input_embed", ",", "cond_vec", ")", ":", "\n", "    ", "batch_size", ",", "max_length", ",", "emb_dim", "=", "input_embed", ".", "size", "(", ")", "\n", "mask", "=", "(", "input_embed", "[", ":", ",", ":", ",", "0", "]", "!=", "0", ")", ".", "float", "(", ")", "\n", "cond_vec", "=", "self", ".", "dim_adjuster", "(", "cond_vec", ")", "\n", "cond_vec", "=", "cond_vec", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "max_length", ",", "1", ")", "\n", "k", "=", "self", ".", "key_maker", "(", "input_embed", ",", "cond_vec", ")", ".", "squeeze", "(", "2", ")", "\n", "weighted_keys", "=", "self", ".", "_masked_softmax", "(", "k", ",", "mask", "=", "mask", ")", ".", "view", "(", "input_embed", ".", "size", "(", ")", "[", "0", "]", ",", "-", "1", ",", "1", ")", "\n", "weighted_values", "=", "torch", ".", "sum", "(", "weighted_keys", "*", "input_embed", ",", "1", ")", "# batch_size, seq_length, embed_dim", "\n", "return", "weighted_values", ",", "weighted_keys", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.RerankerAttentiveSum.__init__": [[641, 652], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Bilinear", "torch.nn.Bilinear"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["  ", "def", "__init__", "(", "self", ",", "sent_dim", ",", "type_emb_dim", ",", "hidden_dim", ",", "score", "=", "'bilinear'", ")", ":", "\n", "    ", "super", "(", "RerankerAttentiveSum", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "sent_dim", "=", "sent_dim", "\n", "self", ".", "type_emb_dim", "=", "type_emb_dim", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "\n", "if", "score", "==", "'bilinear'", ":", "\n", "      ", "self", ".", "key_output", "=", "nn", ".", "Bilinear", "(", "hidden_dim", ",", "hidden_dim", ",", "1", ",", "bias", "=", "True", ")", "\n", "", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "type_emb_dim", ",", "hidden_dim", ",", "bias", "=", "True", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "key_softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "1", ")", "\n", "self", ".", "score", "=", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.RerankerAttentiveSum.forward": [[653, 677], ["model_utils.RerankerAttentiveSum.size", "model_utils.RerankerAttentiveSum.linear().view", "model_utils.RerankerAttentiveSum.key_softmax().view", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "k_d.view", "tgt_x.unsqueeze().repeat.unsqueeze().repeat.unsqueeze().repeat", "model_utils.RerankerAttentiveSum.linear", "model_utils.RerankerAttentiveSum.key_output", "model_utils.RerankerAttentiveSum.key_softmax", "model_utils.RerankerAttentiveSum.view", "tgt_x.unsqueeze().repeat.unsqueeze().repeat.unsqueeze", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "type_embed", ",", "tgt_idx", ")", ":", "\n", "    ", "batch_size", ",", "k", ",", "_", "=", "type_embed", ".", "size", "(", ")", "\n", "type_embed", "=", "self", ".", "linear", "(", "type_embed", ".", "view", "(", "batch_size", "*", "k", ",", "self", ".", "type_emb_dim", ")", ")", ".", "view", "(", "batch_size", ",", "k", ",", "self", ".", "hidden_dim", ")", "\n", "#type_embed = self.relu(type_embed)", "\n", "if", "self", ".", "hidden_dim", "==", "1", ":", "\n", "      ", "k", "=", "k_d", ".", "view", "(", "batch_size", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "      ", "tgt_x", "=", "type_embed", "[", ":", ",", "tgt_idx", ",", ":", "]", "\n", "tgt_x", "=", "tgt_x", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "k", ",", "1", ")", "\n", "if", "self", ".", "score", "==", "'bilinear'", ":", "\n", "        ", "k", "=", "self", ".", "key_output", "(", "tgt_x", ",", "type_embed", ")", "\n", "", "elif", "self", ".", "score", "==", "'dot'", ":", "\n", "        ", "k", "=", "torch", ".", "sum", "(", "tgt_x", "*", "type_embed", ",", "2", ")", "\n", "", "elif", "self", ".", "score", "==", "'cos'", ":", "\n", "        ", "k", "=", "torch", ".", "sum", "(", "tgt_x", "*", "type_embed", ",", "2", ")", "\n", "tgt_x_norm", "=", "torch", ".", "sqrt", "(", "torch", ".", "sum", "(", "tgt_x", "*", "tgt_x", ",", "2", ")", ")", "\n", "type_embed_norm", "=", "torch", ".", "sqrt", "(", "torch", ".", "sum", "(", "type_embed", "*", "type_embed", ",", "2", ")", ")", "\n", "k", "=", "k", "/", "tgt_x_norm", "/", "type_embed_norm", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "#print('k', k, k.size())", "\n", "", "", "weighted_keys", "=", "self", ".", "key_softmax", "(", "k", ")", ".", "view", "(", "batch_size", ",", "-", "1", ",", "1", ")", "\n", "weighted_vecs", "=", "torch", ".", "sum", "(", "weighted_keys", "*", "type_embed", ",", "1", ")", "# batch_size, embed_dim", "\n", "return", "weighted_vecs", ",", "weighted_keys", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.RerankerSimpleScore.__init__": [[680, 683], ["torch.nn.Module.__init__", "torch.nn.Bilinear", "torch.nn.Bilinear"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["  ", "def", "__init__", "(", "self", ",", "sent_dim", ",", "type_emb_dim", ")", ":", "\n", "    ", "super", "(", "RerankerSimpleScore", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "bilinear", "=", "nn", ".", "Bilinear", "(", "sent_dim", ",", "type_emb_dim", ",", "1", ",", "bias", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.RerankerSimpleScore.forward": [[684, 691], ["type_embed.size", "model_utils.RerankerSimpleScore.bilinear().view", "model_utils.RerankerSimpleScore.bilinear"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "type_embed", ",", "sent_vec", ")", ":", "\n", "    ", "\"\"\" type_embed: (batch_size, dim1)\n        sent_vec:   (batch_size, dim2)\n    \"\"\"", "\n", "batch_size", ",", "_", "=", "type_embed", ".", "size", "(", ")", "\n", "score", "=", "self", ".", "bilinear", "(", "sent_vec", ",", "type_embed", ")", ".", "view", "(", "batch_size", ")", "\n", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.HighwayNetwork.__init__": [[694, 704], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.Sigmoid", "torch.nn.Sigmoid", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "range", "range", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["  ", "def", "__init__", "(", "self", ",", "input_dim", ",", "n_layers", ",", "activation", ")", ":", "\n", "    ", "super", "(", "HighwayNetwork", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "n_layers", "=", "n_layers", "\n", "self", ".", "nonlinear", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Linear", "(", "input_dim", ",", "input_dim", ")", "for", "_", "in", "range", "(", "n_layers", ")", "]", ")", "\n", "#self.linear = nn.ModuleList([nn.Linear(input_dim, input_dim) for _ in range(n_layers)])", "\n", "self", ".", "gate", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Linear", "(", "input_dim", ",", "input_dim", ")", "for", "_", "in", "range", "(", "n_layers", ")", "]", ")", "\n", "for", "layer", "in", "self", ".", "gate", ":", "\n", "      ", "layer", ".", "bias", "=", "torch", ".", "nn", ".", "Parameter", "(", "0.", "*", "torch", ".", "ones_like", "(", "layer", ".", "bias", ")", ")", "# init bias", "\n", "", "self", ".", "activation", "=", "activation", "\n", "self", ".", "sigmoid", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.HighwayNetwork.forward": [[705, 712], ["range", "model_utils.HighwayNetwork.sigmoid", "model_utils.HighwayNetwork.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "    ", "for", "layer_idx", "in", "range", "(", "self", ".", "n_layers", ")", ":", "\n", "      ", "gate_values", "=", "self", ".", "sigmoid", "(", "self", ".", "gate", "[", "layer_idx", "]", "(", "x", ")", ")", "\n", "nonlinear", "=", "self", ".", "activation", "(", "self", ".", "nonlinear", "[", "layer_idx", "]", "(", "x", ")", ")", "\n", "#linear = self.linear[layer_idx](x)", "\n", "x", "=", "gate_values", "*", "nonlinear", "+", "(", "1.", "-", "gate_values", ")", "*", "x", "\n", "", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_eval_string": [[24, 35], ["eval_metric.micro", "eval_metric.macro", "len", "sum", "set", "set"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.eval_metric.micro", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.eval_metric.macro"], ["def", "get_eval_string", "(", "true_prediction", ")", ":", "\n", "  ", "\"\"\"\n  Given a list of (gold, prediction)s, generate output string.\n  \"\"\"", "\n", "count", ",", "pred_count", ",", "avg_pred_count", ",", "p", ",", "r", ",", "f1", "=", "eval_metric", ".", "micro", "(", "true_prediction", ")", "\n", "_", ",", "_", ",", "_", ",", "ma_p", ",", "ma_r", ",", "ma_f1", "=", "eval_metric", ".", "macro", "(", "true_prediction", ")", "\n", "output_str", "=", "\"Eval: {0} {1} {2:.3f} P:{3:.3f} R:{4:.3f} F1:{5:.3f} Ma_P:{6:.3f} Ma_R:{7:.3f} Ma_F1:{8:.3f}\"", ".", "format", "(", "\n", "count", ",", "pred_count", ",", "avg_pred_count", ",", "p", ",", "r", ",", "f1", ",", "ma_p", ",", "ma_r", ",", "ma_f1", ")", "\n", "accuracy", "=", "sum", "(", "[", "set", "(", "y", ")", "==", "set", "(", "yp", ")", "for", "y", ",", "yp", "in", "true_prediction", "]", ")", "*", "1.0", "/", "len", "(", "true_prediction", ")", "\n", "output_str", "+=", "'\\t Dev accuracy: {0:.1f}%'", ".", "format", "(", "accuracy", "*", "100", ")", "\n", "return", "output_str", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_eval_string_binary": [[37, 50], ["len", "sum", "sum", "sum", "eval_metric.f1", "len", "len", "int", "int", "int", "sum", "float", "len", "zip", "int", "int", "zip", "int", "int"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.eval_metric.f1"], ["", "def", "get_eval_string_binary", "(", "binary_out", ",", "y", ")", ":", "\n", "  ", "assert", "len", "(", "binary_out", ")", "==", "len", "(", "y", ")", "\n", "count", "=", "len", "(", "binary_out", ")", "\n", "TP_FN_counts", "=", "sum", "(", "[", "1.", "for", "gold", "in", "y", "if", "int", "(", "gold", ")", "==", "1", "]", ")", "\n", "TP_FP_counts", "=", "sum", "(", "[", "1.", "for", "pred", "in", "binary_out", "if", "int", "(", "pred", ")", "==", "1", "]", ")", "\n", "TP_counts", "=", "sum", "(", "[", "1.", "for", "pred", ",", "gold", "in", "zip", "(", "binary_out", ",", "y", ")", "if", "int", "(", "pred", ")", "==", "1", "and", "int", "(", "gold", ")", "==", "1", "]", ")", "\n", "p", "=", "TP_counts", "/", "TP_FP_counts", "if", "TP_FP_counts", ">", "0", "else", "0.", "\n", "r", "=", "TP_counts", "/", "TP_FN_counts", "if", "TP_FN_counts", ">", "0", "else", "0.", "\n", "f1", "=", "eval_metric", ".", "f1", "(", "p", ",", "r", ")", "\n", "output_str", "=", "\"Eval: {0} TP:{1} TP+FP:{2} TP+FN:{3} P:{4:.3f} R:{5:.3f} F1:{6:.3f}\"", ".", "format", "(", "count", ",", "int", "(", "TP_counts", ")", ",", "int", "(", "TP_FP_counts", ")", ",", "int", "(", "TP_FN_counts", ")", ",", "p", ",", "r", ",", "f1", ")", "\n", "accuracy", "=", "sum", "(", "[", "pred", "==", "gold", "for", "pred", ",", "gold", "in", "zip", "(", "binary_out", ",", "y", ")", "]", ")", "/", "float", "(", "len", "(", "binary_out", ")", ")", "\n", "output_str", "+=", "'\\t Dev accuracy: {0:.1f}%'", ".", "format", "(", "accuracy", "*", "100", ")", "\n", "return", "output_str", ",", "accuracy", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_output_index": [[52, 68], ["sigmoid_fn().data.cpu().clone", "single_dist.numpy.numpy", "numpy.argmax", "pred_id.extend", "pred_idx.append", "sigmoid_fn().data.cpu", "range", "sigmoid_fn", "len"], "function", ["None"], ["", "def", "get_output_index", "(", "outputs", ",", "threshold", "=", "0.5", ")", ":", "\n", "  ", "\"\"\"\n  Given outputs from the decoder, generate prediction index.\n  :param outputs:\n  :return:\n  \"\"\"", "\n", "pred_idx", "=", "[", "]", "\n", "outputs", "=", "sigmoid_fn", "(", "outputs", ")", ".", "data", ".", "cpu", "(", ")", ".", "clone", "(", ")", "\n", "for", "single_dist", "in", "outputs", ":", "\n", "    ", "single_dist", "=", "single_dist", ".", "numpy", "(", ")", "\n", "arg_max_ind", "=", "np", ".", "argmax", "(", "single_dist", ")", "\n", "pred_id", "=", "[", "arg_max_ind", "]", "\n", "pred_id", ".", "extend", "(", "\n", "[", "i", "for", "i", "in", "range", "(", "len", "(", "single_dist", ")", ")", "if", "single_dist", "[", "i", "]", ">", "threshold", "and", "i", "!=", "arg_max_ind", "]", ")", "\n", "pred_idx", ".", "append", "(", "pred_id", ")", "\n", "", "return", "pred_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_output_index_reranker": [[70, 87], ["sigmoid_fn().data.cpu().clone", "enumerate", "single_dist.numpy.numpy", "numpy.argmax", "pred_id.extend", "pred_idx.append", "sigmoid_fn().data.cpu", "int", "type_idx.cpu().numpy", "list", "type_idx.cpu", "range", "sigmoid_fn", "len"], "function", ["None"], ["", "def", "get_output_index_reranker", "(", "outputs", ",", "type_idx", ",", "threshold", "=", "0.5", ")", ":", "\n", "  ", "\"\"\"\n  Given outputs from the decoder, generate prediction index.\n  :param outputs:\n  :return:\n  \"\"\"", "\n", "pred_idx", "=", "[", "]", "\n", "outputs", "=", "sigmoid_fn", "(", "outputs", ")", ".", "data", ".", "cpu", "(", ")", ".", "clone", "(", ")", "\n", "type_idx", "=", "[", "[", "int", "(", "n", ")", "for", "n", "in", "list", "(", "arr", ")", "]", "for", "arr", "in", "type_idx", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "]", "\n", "for", "b", ",", "single_dist", "in", "enumerate", "(", "outputs", ")", ":", "\n", "    ", "single_dist", "=", "single_dist", ".", "numpy", "(", ")", "\n", "arg_max_ind", "=", "np", ".", "argmax", "(", "single_dist", ")", "\n", "arg_max_ind", "=", "type_idx", "[", "b", "]", "[", "arg_max_ind", "]", "\n", "pred_id", "=", "[", "arg_max_ind", "]", "\n", "pred_id", ".", "extend", "(", "[", "type_idx", "[", "b", "]", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "single_dist", ")", ")", "if", "single_dist", "[", "i", "]", ">", "threshold", "and", "type_idx", "[", "b", "]", "[", "i", "]", "!=", "arg_max_ind", "]", ")", "\n", "pred_idx", ".", "append", "(", "pred_id", ")", "\n", "", "return", "pred_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_output_index_perceptron": [[89, 96], ["padded_pred_idx.data.cpu().clone", "ppi.numpy.numpy", "pred_idx.append", "padded_pred_idx.data.cpu", "int", "int"], "function", ["None"], ["", "def", "get_output_index_perceptron", "(", "padded_pred_idx", ")", ":", "\n", "  ", "pred_idx", "=", "[", "]", "\n", "padded_pred_idx_cpu", "=", "padded_pred_idx", ".", "data", ".", "cpu", "(", ")", ".", "clone", "(", ")", "\n", "for", "ppi", "in", "padded_pred_idx_cpu", ":", "\n", "    ", "ppi", "=", "ppi", ".", "numpy", "(", ")", "\n", "pred_idx", ".", "append", "(", "[", "int", "(", "idx", ")", "for", "idx", "in", "ppi", "if", "int", "(", "idx", ")", "!=", "constant", ".", "TYPE_PAD_IDX", "]", ")", "\n", "", "return", "pred_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_output_index_rank": [[98, 113], ["sigmoid_fn().data.cpu().clone", "single_dist.numpy.numpy", "pred_idx.append", "sigmoid_fn().data.cpu", "numpy.argsort().tolist", "sigmoid_fn", "numpy.argsort"], "function", ["None"], ["", "def", "get_output_index_rank", "(", "outputs", ",", "topk", "=", "10", ",", "shuffle_order", "=", "True", ")", ":", "\n", "  ", "\"\"\"\n  Given outputs from the decoder, generate prediction index.\n  :param outputs:\n  :return:\n  \"\"\"", "\n", "pred_idx", "=", "[", "]", "\n", "outputs", "=", "sigmoid_fn", "(", "outputs", ")", ".", "data", ".", "cpu", "(", ")", ".", "clone", "(", ")", "\n", "for", "single_dist", "in", "outputs", ":", "\n", "    ", "single_dist", "=", "single_dist", ".", "numpy", "(", ")", "\n", "pred_id", "=", "np", ".", "argsort", "(", "single_dist", ")", ".", "tolist", "(", ")", "[", "-", "topk", ":", "]", "[", ":", ":", "-", "1", "]", "\n", "pred_idx", ".", "append", "(", "pred_id", ")", "\n", "#if shuffle_order:", "\n", "#  shuffle(pred_idx)", "\n", "", "return", "pred_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_output_binary": [[115, 131], ["sigmoid_fn().data.cpu().clone", "single_dist.numpy.numpy", "sigmoid_fn().data.cpu", "len", "binary.append", "binary.append", "sigmoid_fn"], "function", ["None"], ["", "def", "get_output_binary", "(", "outputs", ",", "threshold", "=", "0.5", ")", ":", "\n", "  ", "\"\"\"\n  Given outputs from the decoder, generate prediction index.\n  :param outputs:\n  :return:\n  \"\"\"", "\n", "binary", "=", "[", "]", "\n", "outputs", "=", "sigmoid_fn", "(", "outputs", ")", ".", "data", ".", "cpu", "(", ")", ".", "clone", "(", ")", "\n", "for", "single_dist", "in", "outputs", ":", "\n", "    ", "single_dist", "=", "single_dist", ".", "numpy", "(", ")", "\n", "assert", "len", "(", "single_dist", ")", "==", "1", "\n", "if", "single_dist", "[", "0", "]", ">", "threshold", ":", "\n", "      ", "binary", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "      ", "binary", ".", "append", "(", "0", ")", "\n", "", "", "return", "binary", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_gold_pred_str": [[133, 157], ["gold_strs.append", "pred_strs.append", "list", "list", "cls_pred.append", "y_cls_.append", "y_noisy_idx_.append", "zip", "zip", "int", "range", "len", "int", "int"], "function", ["None"], ["", "def", "get_gold_pred_str", "(", "pred_idx", ",", "gold", ",", "goal", ",", "cls_logits", "=", "None", ",", "y_cls", "=", "None", ",", "y_noisy_idx", "=", "None", ")", ":", "\n", "  ", "\"\"\"\n  Given predicted ids and gold ids, generate a list of (gold, pred) pairs of length batch_size.\n  \"\"\"", "\n", "id2word_dict", "=", "constant", ".", "ID2ANS_DICT", "[", "goal", "]", "\n", "gold_strs", "=", "[", "]", "\n", "for", "gold_i", "in", "gold", ":", "\n", "    ", "gold_strs", ".", "append", "(", "[", "id2word_dict", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "gold_i", ")", ")", "if", "gold_i", "[", "i", "]", "==", "1", "]", ")", "\n", "", "pred_strs", "=", "[", "]", "\n", "for", "pred_idx1", "in", "pred_idx", ":", "\n", "    ", "pred_strs", ".", "append", "(", "[", "(", "id2word_dict", "[", "ind", "]", ")", "for", "ind", "in", "pred_idx1", "]", ")", "\n", "", "if", "cls_logits", "is", "not", "None", "and", "y_cls", "is", "not", "None", "and", "y_noisy_idx", "is", "not", "None", ":", "\n", "    ", "cls_pred", "=", "[", "]", "\n", "for", "cls", "in", "cls_logits", ":", "\n", "      ", "cls_pred", ".", "append", "(", "1", "if", "cls", ">=", "0.", "else", "0", ")", "\n", "", "y_cls_", "=", "[", "]", "\n", "for", "yc", "in", "y_cls", ":", "\n", "      ", "y_cls_", ".", "append", "(", "int", "(", "yc", ")", ")", "\n", "", "y_noisy_idx_", "=", "[", "]", "\n", "for", "yni", "in", "y_noisy_idx", ":", "\n", "      ", "y_noisy_idx_", ".", "append", "(", "[", "(", "id2word_dict", "[", "int", "(", "ind", ")", "]", ")", "for", "ind", "in", "yni", "if", "int", "(", "ind", ")", "!=", "constant", ".", "TYPE_PAD_IDX", "]", ")", "\n", "", "return", "list", "(", "zip", "(", "gold_strs", ",", "pred_strs", ",", "cls_pred", ",", "y_cls_", ",", "y_noisy_idx_", ")", ")", "\n", "", "else", ":", "\n", "    ", "return", "list", "(", "zip", "(", "gold_strs", ",", "pred_strs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.get_gold_pred_str_reranker": [[159, 175], ["enumerate", "list", "pred_strs.append", "zip", "gold_strs.append", "gold_strs.append", "range", "ans_idx.int().item", "ans_idx.int().item", "[].int().item", "len", "ans_idx.int", "ans_idx.int", "[].int"], "function", ["None"], ["", "", "def", "get_gold_pred_str_reranker", "(", "pred_idx", ",", "gold", ",", "type_idx_map", ",", "goal", ",", "y_full", "=", "False", ")", ":", "\n", "  ", "\"\"\"\n  Given predicted ids and gold ids, generate a list of (gold, pred) pairs of length batch_size.\n  \"\"\"", "\n", "id2word_dict", "=", "constant", ".", "ID2ANS_DICT", "[", "goal", "]", "\n", "gold_strs", "=", "[", "]", "\n", "for", "idx", ",", "gold_i", "in", "enumerate", "(", "gold", ")", ":", "\n", "    ", "if", "y_full", ":", "\n", "      ", "gold_strs", ".", "append", "(", "[", "id2word_dict", "[", "ans_idx", ".", "int", "(", ")", ".", "item", "(", ")", "]", "for", "ans_idx", "in", "gold_i", "if", "ans_idx", ".", "int", "(", ")", ".", "item", "(", ")", "not", "in", "(", "constant", ".", "TYPE_BOS_IDX", ",", "constant", ".", "TYPE_EOS_IDX", ",", "constant", ".", "TYPE_PAD_IDX", ")", "]", ")", "\n", "", "else", ":", "\n", "      ", "gold_strs", ".", "append", "(", "[", "id2word_dict", "[", "type_idx_map", "[", "idx", "]", "[", "i", "]", ".", "int", "(", ")", ".", "item", "(", ")", "]", "for", "i", "in", "range", "(", "len", "(", "gold_i", ")", ")", "if", "gold_i", "[", "i", "]", "==", "1", "]", ")", "\n", "", "", "pred_strs", "=", "[", "]", "\n", "for", "pred_idx1", "in", "pred_idx", ":", "\n", "#print(pred_idx1) ", "\n", "    ", "pred_strs", ".", "append", "(", "[", "(", "id2word_dict", "[", "ind", "]", ")", "for", "ind", "in", "pred_idx1", "]", ")", "\n", "", "return", "list", "(", "zip", "(", "gold_strs", ",", "pred_strs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.sort_batch_by_length": [[177, 216], ["sequence_lengths.sort", "tensor.index_select", "sequence_lengths.data.clone().copy_", "torch.autograd.Variable", "permutation_index.sort", "torch.autograd.Variable.index_select", "ValueError", "torch.arange", "torch.arange", "torch.autograd.Variable.long", "isinstance", "isinstance", "sequence_lengths.data.clone", "len"], "function", ["None"], ["", "def", "sort_batch_by_length", "(", "tensor", ":", "torch", ".", "autograd", ".", "Variable", ",", "sequence_lengths", ":", "torch", ".", "autograd", ".", "Variable", ")", ":", "\n", "  ", "\"\"\"\n  @ from allennlp\n  Sort a batch first tensor by some specified lengths.\n\n  Parameters\n  ----------\n  tensor : Variable(torch.FloatTensor), required.\n      A batch first Pytorch tensor.\n  sequence_lengths : Variable(torch.LongTensor), required.\n      A tensor representing the lengths of some dimension of the tensor which\n      we want to sort by.\n\n  Returns\n  -------\n  sorted_tensor : Variable(torch.FloatTensor)\n      The original tensor sorted along the batch dimension with respect to sequence_lengths.\n  sorted_sequence_lengths : Variable(torch.LongTensor)\n      The original sequence_lengths sorted by decreasing size.\n  restoration_indices : Variable(torch.LongTensor)\n      Indices into the sorted_tensor such that\n      ``sorted_tensor.index_select(0, restoration_indices) == original_tensor``\n  \"\"\"", "\n", "\n", "if", "not", "isinstance", "(", "tensor", ",", "Variable", ")", "or", "not", "isinstance", "(", "sequence_lengths", ",", "Variable", ")", ":", "\n", "    ", "raise", "ValueError", "(", "\"Both the tensor and sequence lengths must be torch.autograd.Variables.\"", ")", "\n", "\n", "", "sorted_sequence_lengths", ",", "permutation_index", "=", "sequence_lengths", ".", "sort", "(", "0", ",", "descending", "=", "True", ")", "\n", "sorted_tensor", "=", "tensor", ".", "index_select", "(", "0", ",", "permutation_index", ")", "\n", "# This is ugly, but required - we are creating a new variable at runtime, so we", "\n", "# must ensure it has the correct CUDA vs non-CUDA type. We do this by cloning and", "\n", "# refilling one of the inputs to the function.", "\n", "index_range", "=", "sequence_lengths", ".", "data", ".", "clone", "(", ")", ".", "copy_", "(", "torch", ".", "arange", "(", "0", ",", "len", "(", "sequence_lengths", ")", ")", ")", "\n", "# This is the equivalent of zipping with index, sorting by the original", "\n", "# sequence lengths and returning the now sorted indices.", "\n", "index_range", "=", "Variable", "(", "index_range", ".", "long", "(", ")", ")", "\n", "_", ",", "reverse_mapping", "=", "permutation_index", ".", "sort", "(", "0", ",", "descending", "=", "False", ")", "\n", "restoration_indices", "=", "index_range", ".", "index_select", "(", "0", ",", "reverse_mapping", ")", "\n", "return", "sorted_tensor", ",", "sorted_sequence_lengths", ",", "restoration_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.model_utils.binary_cross_entropy_with_logits_custom": [[353, 387], ["torch.nn.LogSigmoid", "_Reduction.legacy_get_string", "ValueError", "target.size", "input.size", "loss.mean", "loss.sum", "target.size", "input.size"], "function", ["None"], ["", "", "def", "binary_cross_entropy_with_logits_custom", "(", "input", ",", "target", ",", "weight", "=", "None", ",", "size_average", "=", "None", ",", "\n", "reduce", "=", "None", ",", "reduction", "=", "'elementwise_mean'", ",", "pos_weight", "=", "None", ",", "neg_weight", "=", "None", ")", ":", "\n", "    ", "if", "size_average", "is", "not", "None", "or", "reduce", "is", "not", "None", ":", "\n", "        ", "reduction", "=", "_Reduction", ".", "legacy_get_string", "(", "size_average", ",", "reduce", ")", "\n", "", "if", "not", "(", "target", ".", "size", "(", ")", "==", "input", ".", "size", "(", ")", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"Target size ({}) must be the same as input size ({})\"", ".", "format", "(", "target", ".", "size", "(", ")", ",", "input", ".", "size", "(", ")", ")", ")", "\n", "\n", "", "max_val", "=", "(", "-", "input", ")", ".", "clamp", "(", "min", "=", "0", ")", "\n", "\n", "log_sigmoid", "=", "nn", ".", "LogSigmoid", "(", ")", "\n", "\n", "if", "pos_weight", "is", "None", "and", "neg_weight", "is", "None", ":", "# no positive/negative weight", "\n", "        ", "loss", "=", "input", "-", "input", "*", "target", "+", "max_val", "+", "(", "(", "-", "max_val", ")", ".", "exp", "(", ")", "+", "(", "-", "input", "-", "max_val", ")", ".", "exp", "(", ")", ")", ".", "log", "(", ")", "\n", "", "elif", "pos_weight", "is", "not", "None", "and", "neg_weight", "is", "not", "None", ":", "# both positive/negative weight", "\n", "        ", "log_weight", "=", "neg_weight", "+", "(", "pos_weight", "-", "neg_weight", ")", "*", "target", "\n", "loss", "=", "neg_weight", "*", "input", "-", "neg_weight", "*", "input", "*", "target", "+", "log_weight", "*", "(", "max_val", "+", "(", "(", "-", "max_val", ")", ".", "exp", "(", ")", "+", "(", "-", "input", "-", "max_val", ")", ".", "exp", "(", ")", ")", ".", "log", "(", ")", ")", "\n", "#loss = - pos_weight * target * log_sigmoid(input) - neg_weight * (1 - target) * log_sigmoid(-input)", "\n", "", "elif", "pos_weight", "is", "not", "None", ":", "# only positive weight", "\n", "        ", "log_weight", "=", "1", "+", "(", "pos_weight", "-", "1", ")", "*", "target", "\n", "loss", "=", "input", "-", "input", "*", "target", "+", "log_weight", "*", "(", "max_val", "+", "(", "(", "-", "max_val", ")", ".", "exp", "(", ")", "+", "(", "-", "input", "-", "max_val", ")", ".", "exp", "(", ")", ")", ".", "log", "(", ")", ")", "\n", "", "else", ":", "# only negative weight", "\n", "        ", "log_weight", "=", "neg_weight", "+", "(", "1", "-", "neg_weight", ")", "*", "target", "\n", "loss", "=", "neg_weight", "*", "input", "-", "neg_weight", "*", "input", "*", "target", "+", "log_weight", "*", "(", "max_val", "+", "(", "(", "-", "max_val", ")", ".", "exp", "(", ")", "+", "(", "-", "input", "-", "max_val", ")", ".", "exp", "(", ")", ")", ".", "log", "(", ")", ")", "\n", "#loss = - target * log_sigmoid(input) - neg_weight * (1 - target) * log_sigmoid(-input)", "\n", "\n", "", "if", "weight", "is", "not", "None", ":", "\n", "        ", "loss", "=", "loss", "*", "weight", "\n", "\n", "", "if", "reduction", "==", "'none'", ":", "\n", "        ", "return", "loss", "\n", "", "elif", "reduction", "==", "'elementwise_mean'", ":", "\n", "        ", "return", "loss", ".", "mean", "(", ")", "\n", "", "else", ":", "\n", "        ", "return", "loss", ".", "sum", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset.__init__": [[406, 438], ["glob.glob", "random.random.shuffle", "print", "print", "logging.info", "print", "tokenization.FullTokenizer", "data_utils.TypeDataset._load_type_set", "data_utils.TypeDataset._load_all_types", "len", "len"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset._load_type_set", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset._load_all_types"], ["def", "__init__", "(", "self", ",", "filepattern", ",", "vocab", ",", "goal", ",", "elmo", "=", "None", ",", "bert", "=", "None", ",", "args", "=", "None", ")", ":", "\n", "    ", "self", ".", "_all_shards", "=", "glob", ".", "glob", "(", "filepattern", ")", "\n", "self", ".", "goal", "=", "goal", "\n", "self", ".", "answer_num", "=", "constant", ".", "ANSWER_NUM_DICT", "[", "goal", "]", "\n", "shuffle", "(", "self", ".", "_all_shards", ")", "\n", "self", ".", "char_vocab", ",", "self", ".", "glove_dict", "=", "vocab", "\n", "self", ".", "elmo", "=", "elmo", "\n", "self", ".", "bert", "=", "bert", "# BERT model obj", "\n", "self", ".", "finetune_bert", "=", "args", ".", "bert", "# True/False", "\n", "if", "args", ".", "model_type", "==", "'bert_uncase_small'", ":", "\n", "      ", "print", "(", "'==> Init tokenizer from '", "+", "constant", ".", "BERT_UNCASED_SMALL_VOCAB", "+", "', do_lower_case=True'", ")", "\n", "self", ".", "bert_tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "vocab_file", "=", "constant", ".", "BERT_UNCASED_SMALL_VOCAB", ",", "do_lower_case", "=", "True", ")", "\n", "", "else", ":", "\n", "      ", "self", ".", "bert_tokenizer", "=", "None", "\n", "", "if", "args", ".", "model_type", "in", "[", "'labeler'", ",", "'filter'", "]", ":", "\n", "      ", "self", ".", "use_type_definition", "=", "True", "\n", "", "else", ":", "\n", "      ", "self", ".", "use_type_definition", "=", "False", "\n", "", "self", ".", "args", "=", "args", "\n", "self", ".", "is_labeler", "=", "True", "if", "args", ".", "mode", "in", "[", "'train_labeler'", ",", "'test_labeler'", "]", "else", "False", "\n", "self", ".", "is_training_labeler", "=", "True", "if", "args", ".", "mode", "in", "[", "'train_labeler'", "]", "else", "False", "\n", "self", ".", "type_set", "=", "self", ".", "_load_type_set", "(", ")", "if", "self", ".", "is_labeler", "else", "None", "\n", "self", ".", "all_types", "=", "self", ".", "_load_all_types", "(", ")", "if", "self", ".", "is_labeler", "else", "None", "\n", "self", ".", "type_definition", "=", "constant", ".", "DEFINITION", "\n", "self", ".", "type_def_word2id", "=", "constant", ".", "DEF_VOCAB_S2I", "\n", "self", ".", "word2id", "=", "constant", ".", "ANS2ID_DICT", "[", "goal", "]", "\n", "print", "(", "\"Answer num %d\"", "%", "(", "self", ".", "answer_num", ")", ")", "\n", "print", "(", "'Found %d shards at %s'", "%", "(", "len", "(", "self", ".", "_all_shards", ")", ",", "filepattern", ")", ")", "\n", "logging", ".", "info", "(", "'Found %d shards at %s'", "%", "(", "len", "(", "self", ".", "_all_shards", ")", ",", "filepattern", ")", ")", "\n", "self", ".", "exclude_types", "=", "[", "\n", "'entity'", ",", "'object'", ",", "'whole'", ",", "'living_thing'", ",", "'organism'", ",", "'social_group'", ",", "'measure'", ",", "\n", "'medimum_of_exchange'", ",", "'artifact'", ",", "'geographical_area'", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset._load_npz": [[440, 444], ["open", "numpy.load"], "methods", ["None"], ["", "def", "_load_npz", "(", "self", ",", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "'rb'", ")", "as", "f", ":", "\n", "      ", "data", "=", "np", ".", "load", "(", "f", ")", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset._load_type_set": [[445, 449], ["open", "line.strip().split", "f.readlines", "line.strip"], "methods", ["None"], ["", "def", "_load_type_set", "(", "self", ",", "path", "=", "'./resources/type_set.csv'", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "'r'", ")", "as", "f", ":", "\n", "      ", "type_set", "=", "[", "line", ".", "strip", "(", ")", ".", "split", "(", "','", ")", "for", "line", "in", "f", ".", "readlines", "(", ")", "]", "\n", "", "return", "type_set", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset._load_all_types": [[450, 454], ["open", "line.strip", "f.readlines"], "methods", ["None"], ["", "def", "_load_all_types", "(", "self", ",", "path", "=", "'./resources/types.txt'", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "'r'", ")", "as", "f", ":", "\n", "      ", "types", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "f", ".", "readlines", "(", ")", "]", "\n", "", "return", "types", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset._get_fake_labels": [[455, 470], ["random.random.random", "list", "random.random.shuffle", "range", "copy.deepcopy", "len", "len", "set().intersection", "random.random.sample", "set", "set"], "methods", ["None"], ["", "def", "_get_fake_labels", "(", "self", ",", "labels", ",", "th", "=", ".7", ")", ":", "\n", "    ", "if", "random", "(", ")", ">", "th", ":", "\n", "      ", "fake_labels", "=", "None", "\n", "fake_idx", "=", "list", "(", "range", "(", "len", "(", "self", ".", "type_set", ")", ")", ")", "\n", "shuffle", "(", "fake_idx", ")", "\n", "for", "i", "in", "fake_idx", ":", "\n", "        ", "if", "len", "(", "set", "(", "labels", ")", ".", "intersection", "(", "set", "(", "self", ".", "type_set", "[", "i", "]", ")", ")", ")", "<", "1", ":", "\n", "          ", "fake_labels", "=", "self", ".", "type_set", "[", "i", "]", "\n", "break", "\n", "", "", "if", "fake_labels", ":", "\n", "        ", "return", "fake_labels", ",", "1", "\n", "", "else", ":", "\n", "        ", "return", "sample", "(", "self", ".", "type_set", ",", "1", ")", "[", "0", "]", ",", "1", "\n", "", "", "else", ":", "\n", "      ", "return", "copy", ".", "deepcopy", "(", "labels", ")", ",", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset.get_synonyms": [[471, 477], ["nltk.corpus.wordnet.synsets", "list", "syn.lemmas", "set", "synonyms.append", "l.name"], "methods", ["None"], ["", "", "def", "get_synonyms", "(", "self", ",", "word", ")", ":", "\n", "    ", "synonyms", "=", "[", "]", "\n", "for", "syn", "in", "wordnet", ".", "synsets", "(", "word", ")", ":", "\n", "      ", "for", "l", "in", "syn", ".", "lemmas", "(", ")", ":", "\n", "        ", "synonyms", ".", "append", "(", "l", ".", "name", "(", ")", ")", "\n", "", "", "return", "list", "(", "set", "(", "synonyms", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset.get_hypernyms": [[478, 486], ["word.hypernyms", "word.pos", "all_hyps.append", "data_utils.TypeDataset.flatten", "data_utils.TypeDataset.get_hypernyms", "h.pos"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset.flatten", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset.get_hypernyms"], ["", "def", "get_hypernyms", "(", "self", ",", "word", ")", ":", "\n", "    ", "all_hyps", "=", "[", "]", "\n", "if", "word", ".", "pos", "(", ")", "==", "'n'", ":", "\n", "      ", "all_hyps", ".", "append", "(", "word", ")", "\n", "", "hyps", "=", "word", ".", "hypernyms", "(", ")", "\n", "if", "hyps", ":", "\n", "      ", "all_hyps", "+=", "self", ".", "flatten", "(", "[", "self", ".", "get_hypernyms", "(", "h", ")", "for", "h", "in", "hyps", "if", "h", ".", "pos", "(", ")", "==", "'n'", "]", ")", "\n", "", "return", "all_hyps", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset.flatten": [[487, 493], ["isinstance", "isinstance", "data_utils.TypeDataset.flatten"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset.flatten"], ["", "def", "flatten", "(", "self", ",", "lst", ")", ":", "\n", "    ", "for", "elem", "in", "lst", ":", "\n", "      ", "if", "isinstance", "(", "elem", ",", "collections", ".", "Iterable", ")", "and", "not", "isinstance", "(", "elem", ",", "(", "str", ",", "bytes", ")", ")", ":", "\n", "        ", "yield", "from", "self", ".", "flatten", "(", "elem", ")", "\n", "", "else", ":", "\n", "        ", "yield", "elem", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset.get_text_from_synset": [[494, 501], ["list", "syn.pos", "syn.lemmas", "set", "l.name().lower", "texts.append", "l.name"], "methods", ["None"], ["", "", "", "def", "get_text_from_synset", "(", "self", ",", "syn", ")", ":", "\n", "    ", "texts", "=", "[", "]", "\n", "if", "syn", ".", "pos", "(", ")", "==", "'n'", ":", "\n", "      ", "for", "l", "in", "syn", ".", "lemmas", "(", ")", ":", "\n", "        ", "name", "=", "l", ".", "name", "(", ")", ".", "lower", "(", ")", "\n", "texts", ".", "append", "(", "name", ")", "\n", "", "", "return", "list", "(", "set", "(", "texts", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset.get_all_hypernyms": [[502, 509], ["nltk.corpus.wordnet.synsets", "s.pos", "hyp.append", "data_utils.TypeDataset.get_hypernyms"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset.get_hypernyms"], ["", "def", "get_all_hypernyms", "(", "self", ",", "types", ")", ":", "\n", "    ", "hyp", "=", "[", "]", "\n", "for", "t", "in", "types", ":", "\n", "      ", "for", "s", "in", "wordnet", ".", "synsets", "(", "t", ")", ":", "# choose the first synset ", "\n", "        ", "if", "s", ".", "pos", "(", ")", "==", "'n'", ":", "\n", "          ", "hyp", ".", "append", "(", "self", ".", "get_hypernyms", "(", "s", ")", ")", "\n", "", "", "", "return", "hyp", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset.expand_types_wordnet": [[510, 516], ["list", "data_utils.TypeDataset.get_text_from_synset", "data_utils.TypeDataset.get_synonyms", "set", "data_utils.TypeDataset.get_all_hypernyms", "list", "set"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset.get_text_from_synset", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset.get_synonyms", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset.get_all_hypernyms"], ["", "def", "expand_types_wordnet", "(", "self", ",", "labels", ")", ":", "\n", "    ", "hyp", "=", "[", "self", ".", "get_text_from_synset", "(", "synset", ")", "for", "synsets", "in", "self", ".", "get_all_hypernyms", "(", "labels", ")", "for", "synset", "in", "synsets", "]", "\n", "syn", "=", "[", "self", ".", "get_synonyms", "(", "t", ")", "for", "t", "in", "labels", "]", "\n", "hyp", "=", "[", "t", "for", "tt", "in", "hyp", "for", "t", "in", "tt", "]", "\n", "syn", "=", "[", "t", "for", "tt", "in", "syn", "for", "t", "in", "tt", "]", "\n", "return", "list", "(", "set", "(", "[", "t", "for", "t", "in", "list", "(", "set", "(", "hyp", "+", "syn", ")", ")", "if", "t", "in", "self", ".", "all_types", "and", "t", "not", "in", "self", ".", "exclude_types", "]", "+", "labels", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset.drop_types_randomly": [[517, 528], ["len", "len", "random.random.sample", "random.random.random", "selected_types.append"], "methods", ["None"], ["", "def", "drop_types_randomly", "(", "self", ",", "type_idx", ",", "th", "=", ".7", ")", ":", "\n", "    ", "if", "len", "(", "type_idx", ")", "==", "1", ":", "\n", "      ", "return", "type_idx", "\n", "", "selected_types", "=", "[", "]", "\n", "for", "s", "in", "type_idx", ":", "\n", "      ", "if", "random", "(", ")", ">", "th", ":", "\n", "        ", "selected_types", ".", "append", "(", "s", ")", "\n", "", "", "if", "len", "(", "selected_types", ")", ">", "0", ":", "\n", "      ", "return", "selected_types", "\n", "", "else", ":", "\n", "      ", "return", "sample", "(", "type_idx", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset.drop_coarse_types": [[529, 540], ["len", "len", "random.random.sample", "set", "selected_types.append"], "methods", ["None"], ["", "", "def", "drop_coarse_types", "(", "self", ",", "type_idx", ",", "coarse_types", ")", ":", "\n", "    ", "if", "len", "(", "type_idx", ")", "==", "1", ":", "\n", "      ", "return", "type_idx", "\n", "", "selected_types", "=", "[", "]", "\n", "for", "s", "in", "type_idx", ":", "\n", "      ", "if", "s", "not", "in", "set", "(", "coarse_types", ")", ":", "\n", "        ", "selected_types", ".", "append", "(", "s", ")", "\n", "", "", "if", "len", "(", "selected_types", ")", ">", "0", ":", "\n", "      ", "return", "selected_types", "\n", "", "else", ":", "\n", "      ", "return", "sample", "(", "type_idx", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset._load_shard": [[541, 607], ["open", "enumerate", "enumerate", "zip", "enumerate", "zip", "json.loads", "line_elem[].split", "len", "enumerate", "y_ids.append", "y_str_list_noisy_.append", "y_ids_noisy.append", "data_utils.TypeDataset.expand_types_wordnet", "y_ids_noisy_wordnet_expanded.append", "y_ids_noisy_def.append", "y_str_noisy_def.append", "y_ids.append", "sent.strip", "f.readlines", "filtered.append", "data_utils.TypeDataset._get_fake_labels", "data_utils.TypeDataset._get_fake_labels", "data_utils.TypeDataset.drop_types_randomly", "data_utils.TypeDataset.drop_types_randomly", "list", "len", "len", "len", "data_utils.TypeDataset.bert_tokenizer.tokenize", "data_utils.TypeDataset.bert_tokenizer.tokenize", "line_elem[].split"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset.expand_types_wordnet", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset._get_fake_labels", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset._get_fake_labels", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.drop_types_randomly", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.drop_types_randomly", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.tokenize"], ["", "", "def", "_load_shard", "(", "self", ",", "shard_name", ",", "eval_data", ")", ":", "\n", "    ", "\"\"\"Read one file and convert to ids.\n    Args:\n      shard_name: file path.\n    Returns:\n      list of (id, global_word_id) tuples.\n    \"\"\"", "\n", "with", "open", "(", "shard_name", ")", "as", "f", ":", "\n", "      ", "line_elems", "=", "[", "json", ".", "loads", "(", "sent", ".", "strip", "(", ")", ")", "for", "sent", "in", "f", ".", "readlines", "(", ")", "]", "\n", "# drop examples with empty mention span ", "\n", "filtered", "=", "[", "]", "\n", "for", "line_elem", "in", "line_elems", ":", "\n", "        ", "if", "line_elem", "[", "\"mention_span\"", "]", ":", "\n", "          ", "filtered", ".", "append", "(", "line_elem", ")", "\n", "# print(shard_name, ', before / after:', len(line_elems), '/', len(filtered))", "\n", "", "", "line_elems", "=", "filtered", "\n", "if", "not", "eval_data", ":", "\n", "        ", "line_elems", "=", "[", "line_elem", "for", "line_elem", "in", "line_elems", "if", "len", "(", "line_elem", "[", "'mention_span'", "]", ".", "split", "(", ")", ")", "<", "11", "]", "\n", "", "annot_ids", "=", "[", "line_elem", "[", "\"annot_id\"", "]", "for", "line_elem", "in", "line_elems", "]", "\n", "mention_span", "=", "[", "[", "self", ".", "char_vocab", "[", "x", "]", "for", "x", "in", "list", "(", "line_elem", "[", "\"mention_span\"", "]", ")", "]", "for", "line_elem", "in", "line_elems", "]", "\n", "mention_seq", "=", "[", "line_elem", "[", "\"mention_span\"", "]", ".", "split", "(", ")", "for", "line_elem", "in", "line_elems", "]", "\n", "mention_headword", "=", "[", "[", "w", "[", "\"text\"", "]", "for", "w", "in", "line_elem", "[", "\"mention_span_tree\"", "]", "if", "w", "[", "\"dep\"", "]", "==", "\"ROOT\"", "]", "[", "0", "]", "for", "line_elem", "in", "line_elems", "]", "\n", "left_seq", "=", "[", "line_elem", "[", "'left_context_token'", "]", "for", "line_elem", "in", "line_elems", "]", "\n", "right_seq", "=", "[", "line_elem", "[", "'right_context_token'", "]", "for", "line_elem", "in", "line_elems", "]", "\n", "y_str_list", "=", "[", "line_elem", "[", "'y_str'", "]", "for", "line_elem", "in", "line_elems", "]", "\n", "head_wordpiece_idx", "=", "[", "0", "]", "*", "len", "(", "left_seq", ")", "\n", "if", "self", ".", "args", ".", "model_type", "==", "'bert_uncase_small'", ":", "\n", "        ", "for", "i", ",", "ls", "in", "enumerate", "(", "left_seq", ")", ":", "\n", "          ", "if", "' '", ".", "join", "(", "ls", ")", ":", "\n", "            ", "head_wordpiece_idx", "[", "i", "]", "+=", "len", "(", "self", ".", "bert_tokenizer", ".", "tokenize", "(", "' '", ".", "join", "(", "ls", ")", ")", ")", "\n", "", "for", "w", "in", "line_elems", "[", "i", "]", "[", "\"mention_span_tree\"", "]", ":", "\n", "            ", "if", "w", "[", "\"dep\"", "]", "==", "\"ROOT\"", ":", "\n", "              ", "break", "\n", "", "head_wordpiece_idx", "[", "i", "]", "+=", "len", "(", "self", ".", "bert_tokenizer", ".", "tokenize", "(", "w", "[", "\"text\"", "]", ")", ")", "\n", "", "", "", "", "if", "self", ".", "is_labeler", ":", "\n", "      ", "y_ids", "=", "[", "]", "\n", "for", "iid", ",", "y_strs", "in", "enumerate", "(", "y_str_list", ")", ":", "\n", "        ", "y_ids", ".", "append", "(", "[", "self", ".", "word2id", "[", "x", "]", "for", "x", "in", "y_strs", "if", "x", "in", "self", ".", "word2id", "]", ")", "\n", "", "y_ids_noisy", "=", "[", "]", "\n", "y_str_list_noisy_", "=", "[", "]", "\n", "y_ids_noisy_wordnet_expanded", "=", "[", "]", "\n", "y_ids_noisy_def", "=", "[", "]", "\n", "y_str_noisy_def", "=", "[", "]", "\n", "if", "self", ".", "is_training_labeler", ":", "\n", "        ", "noisy", "=", "[", "self", ".", "_get_fake_labels", "(", "y", ")", "for", "y", "in", "y_str_list", "]", "# randomly swap type set during training", "\n", "", "else", ":", "\n", "        ", "noisy", "=", "[", "self", ".", "_get_fake_labels", "(", "y", ",", "th", "=", "1.", ")", "for", "y", "in", "y_str_list", "]", "# we don't add noise during testing", "\n", "", "y_str_list_noisy", "=", "[", "y", "[", "0", "]", "for", "y", "in", "noisy", "]", "# noisy labels for filter", "\n", "y_cls", "=", "[", "y", "[", "1", "]", "for", "y", "in", "noisy", "]", "\n", "for", "iid_n", ",", "y_strs_n", "in", "enumerate", "(", "y_str_list_noisy", ")", ":", "\n", "        ", "if", "self", ".", "is_training_labeler", ":", "\n", "          ", "y_strs_n", "=", "self", ".", "drop_types_randomly", "(", "y_strs_n", ")", "# randomly drop types during training", "\n", "", "else", ":", "\n", "          ", "y_strs_n", "=", "self", ".", "drop_types_randomly", "(", "y_strs_n", ",", "th", "=", "0.", ")", "# we don't add noise during testing", "\n", "", "y_str_list_noisy_", ".", "append", "(", "y_strs_n", ")", "\n", "y_ids_noisy", ".", "append", "(", "[", "self", ".", "word2id", "[", "x", "]", "for", "x", "in", "y_strs_n", "if", "x", "in", "self", ".", "word2id", "]", ")", "\n", "y_strs_wn", "=", "self", ".", "expand_types_wordnet", "(", "y_strs_n", ")", "\n", "y_ids_noisy_wordnet_expanded", ".", "append", "(", "[", "self", ".", "word2id", "[", "x", "]", "for", "x", "in", "y_strs_wn", "if", "x", "in", "self", ".", "word2id", "]", ")", "\n", "y_ids_noisy_def", ".", "append", "(", "[", "[", "self", ".", "type_def_word2id", "[", "w", "]", "for", "w", "in", "self", ".", "type_definition", "[", "x", "]", "]", "for", "x", "in", "y_strs_n", "if", "x", "in", "self", ".", "word2id", "]", ")", "\n", "y_str_noisy_def", ".", "append", "(", "[", "[", "w", "for", "w", "in", "self", ".", "type_definition", "[", "x", "]", "]", "for", "x", "in", "y_strs_n", "if", "x", "in", "self", ".", "word2id", "]", ")", "\n", "", "return", "zip", "(", "annot_ids", ",", "left_seq", ",", "right_seq", ",", "mention_seq", ",", "y_ids", ",", "mention_span", ",", "mention_headword", ",", "y_str_list", ",", "y_str_list_noisy_", ",", "y_ids_noisy", ",", "y_cls", ",", "head_wordpiece_idx", ",", "y_ids_noisy_wordnet_expanded", ",", "y_ids_noisy_def", ",", "y_str_noisy_def", ")", "\n", "", "else", ":", "\n", "      ", "y_ids", "=", "[", "]", "\n", "for", "iid", ",", "y_strs", "in", "enumerate", "(", "y_str_list", ")", ":", "\n", "        ", "y_ids", ".", "append", "(", "[", "self", ".", "word2id", "[", "x", "]", "for", "x", "in", "y_strs", "if", "x", "in", "self", ".", "word2id", "]", ")", "\n", "", "return", "zip", "(", "annot_ids", ",", "left_seq", ",", "right_seq", ",", "mention_seq", ",", "y_ids", ",", "mention_span", ",", "mention_headword", ",", "y_str_list", ",", "head_wordpiece_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset._get_sentence": [[608, 614], ["range", "data_utils.TypeDataset._load_shard"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset._load_shard"], ["", "", "def", "_get_sentence", "(", "self", ",", "epoch", ",", "forever", ",", "eval_data", ")", ":", "\n", "    ", "for", "i", "in", "range", "(", "0", ",", "epoch", "if", "not", "forever", "else", "100000000000000", ")", ":", "\n", "      ", "for", "shard", "in", "self", ".", "_all_shards", ":", "\n", "        ", "ids", "=", "self", ".", "_load_shard", "(", "shard", ",", "eval_data", ")", "\n", "for", "current_ids", "in", "ids", ":", "\n", "          ", "yield", "current_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset.get_batch": [[615, 629], ["data_utils.get_example", "data_utils.TypeDataset._get_sentence"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.get_example", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.TypeDataset._get_sentence"], ["", "", "", "", "def", "get_batch", "(", "self", ",", "batch_size", "=", "128", ",", "epoch", "=", "5", ",", "forever", "=", "False", ",", "eval_data", "=", "False", ",", "simple_mention", "=", "True", ")", ":", "\n", "    ", "return", "get_example", "(", "self", ".", "_get_sentence", "(", "epoch", ",", "forever", "=", "forever", ",", "eval_data", "=", "eval_data", ")", ",", "\n", "self", ".", "glove_dict", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "answer_num", "=", "self", ".", "answer_num", ",", "\n", "eval_data", "=", "eval_data", ",", "\n", "simple_mention", "=", "simple_mention", ",", "\n", "elmo", "=", "self", ".", "elmo", ",", "\n", "bert", "=", "self", ".", "bert", ",", "\n", "finetune_bert", "=", "self", ".", "finetune_bert", ",", "\n", "bert_tokenizer", "=", "self", ".", "bert_tokenizer", ",", "\n", "is_labeler", "=", "self", ".", "is_labeler", ",", "\n", "all_types", "=", "self", ".", "all_types", ",", "\n", "use_type_definition", "=", "self", ".", "use_type_definition", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.to_torch": [[33, 50], ["feed_dict.items", "feed_dict.pop", "torch.autograd.Variable().cuda().float", "torch.autograd.Variable().cuda().float", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.from_numpy().cuda", "torch.autograd.Variable", "torch.autograd.Variable", "torch.from_numpy", "torch.autograd.Variable", "torch.from_numpy", "torch.autograd.Variable", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "function", ["None"], ["def", "to_torch", "(", "feed_dict", ")", ":", "\n", "  ", "torch_feed_dict", "=", "{", "}", "\n", "annot_ids", "=", "None", "\n", "if", "'annot_id'", "in", "feed_dict", ":", "\n", "    ", "annot_ids", "=", "feed_dict", ".", "pop", "(", "'annot_id'", ")", "\n", "", "for", "k", ",", "v", "in", "feed_dict", ".", "items", "(", ")", ":", "\n", "    ", "if", "'embed'", "in", "k", ":", "\n", "      ", "torch_feed_dict", "[", "k", "]", "=", "torch", ".", "autograd", ".", "Variable", "(", "torch", ".", "from_numpy", "(", "v", ")", ",", "requires_grad", "=", "False", ")", ".", "cuda", "(", ")", ".", "float", "(", ")", "\n", "", "elif", "'token_bio'", "==", "k", ":", "\n", "      ", "torch_feed_dict", "[", "k", "]", "=", "torch", ".", "autograd", ".", "Variable", "(", "torch", ".", "from_numpy", "(", "v", ")", ",", "requires_grad", "=", "False", ")", ".", "cuda", "(", ")", ".", "float", "(", ")", "\n", "", "elif", "'y'", "==", "k", "or", "k", "==", "'mention_start_ind'", "or", "k", "==", "'mention_end_ind'", "or", "'length'", "in", "k", ":", "\n", "      ", "torch_feed_dict", "[", "k", "]", "=", "torch", ".", "autograd", ".", "Variable", "(", "torch", ".", "from_numpy", "(", "v", ")", ",", "requires_grad", "=", "False", ")", ".", "cuda", "(", ")", "\n", "", "elif", "k", "==", "'span_chars'", ":", "\n", "      ", "torch_feed_dict", "[", "k", "]", "=", "torch", ".", "autograd", ".", "Variable", "(", "torch", ".", "from_numpy", "(", "v", ")", ",", "requires_grad", "=", "False", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "      ", "torch_feed_dict", "[", "k", "]", "=", "torch", ".", "from_numpy", "(", "v", ")", ".", "cuda", "(", ")", "\n", "", "", "return", "torch_feed_dict", ",", "annot_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.load_embedding_dict": [[52, 67], ["print", "numpy.zeros", "collections.defaultdict", "print", "open", "enumerate", "f.readlines", "line.split", "numpy.array", "len", "len", "float", "constant.GLOVE_VEC"], "function", ["None"], ["", "def", "load_embedding_dict", "(", "embedding_path", ",", "embedding_size", ")", ":", "\n", "  ", "print", "(", "\"Loading word embeddings from {}...\"", ".", "format", "(", "embedding_path", ")", ")", "\n", "default_embedding", "=", "np", ".", "zeros", "(", "embedding_size", ")", "\n", "embedding_dict", "=", "defaultdict", "(", "lambda", ":", "default_embedding", ")", "\n", "with", "open", "(", "embedding_path", ")", "as", "f", ":", "\n", "    ", "for", "i", ",", "line", "in", "enumerate", "(", "f", ".", "readlines", "(", ")", ")", ":", "\n", "      ", "splits", "=", "line", ".", "split", "(", ")", "\n", "if", "len", "(", "splits", ")", "!=", "embedding_size", "+", "1", ":", "\n", "        ", "continue", "\n", "", "assert", "len", "(", "splits", ")", "==", "embedding_size", "+", "1", "\n", "word", "=", "splits", "[", "0", "]", "\n", "embedding", "=", "np", ".", "array", "(", "[", "float", "(", "s", ")", "for", "s", "in", "splits", "[", "1", ":", "]", "]", ")", "\n", "embedding_dict", "[", "word", "]", "=", "embedding", "\n", "", "", "print", "(", "\"Done loading word embeddings!\"", ")", "\n", "return", "embedding_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.get_vocab": [[69, 76], ["data_utils.load_embedding_dict"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.load_embedding_dict"], ["", "def", "get_vocab", "(", ")", ":", "\n", "  ", "\"\"\"\n  Get vocab file [word -> embedding]\n  \"\"\"", "\n", "char_vocab", "=", "constant", ".", "CHAR_DICT", "\n", "glove_word_vocab", "=", "load_embedding_dict", "(", "constant", ".", "GLOVE_VEC", ",", "300", ")", "\n", "return", "char_vocab", ",", "glove_word_vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.get_type_elmo_vec": [[78, 80], ["None"], "function", ["None"], ["", "def", "get_type_elmo_vec", "(", ")", ":", "\n", "  ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.pad_slice": [[82, 94], ["len", "len", "len", "len"], "function", ["None"], ["", "def", "pad_slice", "(", "seq", ",", "seq_length", ",", "cut_left", "=", "False", ",", "pad_token", "=", "\"<none>\"", ")", ":", "\n", "  ", "if", "len", "(", "seq", ")", ">=", "seq_length", ":", "\n", "    ", "if", "not", "cut_left", ":", "\n", "      ", "return", "seq", "[", ":", "seq_length", "]", "\n", "", "else", ":", "\n", "      ", "output_seq", "=", "[", "x", "for", "x", "in", "seq", "if", "x", "!=", "pad_token", "]", "\n", "if", "len", "(", "output_seq", ")", ">=", "seq_length", ":", "\n", "        ", "return", "output_seq", "[", "-", "seq_length", ":", "]", "\n", "", "else", ":", "\n", "        ", "return", "[", "pad_token", "]", "*", "(", "seq_length", "-", "len", "(", "output_seq", ")", ")", "+", "output_seq", "\n", "", "", "", "else", ":", "\n", "    ", "return", "seq", "+", "(", "[", "pad_token", "]", "*", "(", "seq_length", "-", "len", "(", "seq", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.get_word_vec": [[96, 100], ["None"], "function", ["None"], ["", "", "def", "get_word_vec", "(", "word", ",", "vec_dict", ")", ":", "\n", "  ", "if", "word", "in", "vec_dict", ":", "\n", "    ", "return", "vec_dict", "[", "word", "]", "\n", "", "return", "vec_dict", "[", "'unk'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.init_elmo": [[102, 107], ["print", "print", "print", "allennlp.commands.elmo.ElmoEmbedder"], "function", ["None"], ["", "def", "init_elmo", "(", ")", ":", "\n", "  ", "print", "(", "'Preparing ELMo...'", ")", "\n", "print", "(", "\"Loading options from {}...\"", ".", "format", "(", "constant", ".", "ELMO_OPTIONS_FILE", ")", ")", "\n", "print", "(", "\"Loading weith from {}...\"", ".", "format", "(", "constant", ".", "ELMO_WEIGHT_FILE", ")", ")", "\n", "return", "ElmoEmbedder", "(", "constant", ".", "ELMO_OPTIONS_FILE", ",", "constant", ".", "ELMO_WEIGHT_FILE", ",", "cuda_device", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.get_elmo_vec": [[109, 113], ["elmo.embed_sentence"], "function", ["None"], ["", "def", "get_elmo_vec", "(", "sentence", ",", "elmo", ")", ":", "\n", "  ", "\"\"\" sentence must be a list of words \"\"\"", "\n", "emb", "=", "elmo", ".", "embed_sentence", "(", "sentence", ")", "\n", "return", "emb", "# (3, len, dim)", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.get_elmo_vec_batch": [[115, 119], ["elmo.embed_batch"], "function", ["None"], ["", "def", "get_elmo_vec_batch", "(", "sentences", ",", "elmo", ")", ":", "\n", "  ", "\"\"\" sentence must be a list of words \"\"\"", "\n", "emb", "=", "elmo", ".", "embed_batch", "(", "sentences", ")", "\n", "return", "emb", "# (batch, 3, len, dim)", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.init_bert": [[121, 123], ["BertEmbedder().cuda", "BertEmbedder"], "function", ["None"], ["", "def", "init_bert", "(", "args", ",", "answer_num", ")", ":", "\n", "  ", "return", "BertEmbedder", "(", "args", ",", "answer_num", ")", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.get_bert_vec_batch": [[125, 127], ["bert"], "function", ["None"], ["", "def", "get_bert_vec_batch", "(", "input_dict", ",", "bert", ")", ":", "\n", "  ", "return", "bert", "(", "input_dict", ",", "None", ")", "# set data_type arg None, it's not used", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.drop_types_randomly": [[129, 145], ["list", "len", "len", "zip", "random.sample", "len", "random.sample", "len", "len", "len", "random.random", "selected_types.append"], "function", ["None"], ["", "def", "drop_types_randomly", "(", "type_idx", ",", "type_str", ",", "all_types", ")", ":", "\n", "  ", "assert", "len", "(", "type_idx", ")", "==", "len", "(", "type_str", ")", "\n", "types", "=", "list", "(", "zip", "(", "type_idx", ",", "type_str", ")", ")", "\n", "types_", "=", "[", "tup", "for", "tup", "in", "types", "if", "tup", "[", "1", "]", "]", "# not in set(all_types[0:130])]    # gen=0:9, fine=9:130, finer=130:", "\n", "if", "len", "(", "types_", ")", "==", "0", "and", "len", "(", "types", ")", ">", "0", ":", "\n", "    ", "return", "sample", "(", "types", ",", "1", ")", "\n", "", "elif", "len", "(", "types_", ")", "==", "1", ":", "\n", "    ", "return", "types_", "\n", "", "selected_types", "=", "[", "]", "\n", "for", "i", ",", "s", "in", "types_", ":", "\n", "    ", "if", "random", "(", ")", ">", "0.7", ":", "####################### th", "\n", "      ", "selected_types", ".", "append", "(", "(", "i", ",", "s", ")", ")", "\n", "", "", "if", "len", "(", "selected_types", ")", ">", "0", ":", "\n", "    ", "return", "selected_types", "\n", "", "else", ":", "\n", "    ", "return", "sample", "(", "types_", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.get_example": [[147, 401], ["range", "min", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "min", "min", "max", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "range", "max", "max", "max", "max", "numpy.zeros", "numpy.array", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "range", "min", "min", "len", "enumerate", "enumerate", "enumerate", "min", "data_utils.pad_slice", "min", "list", "max", "len", "list", "len", "numpy.ones", "max", "numpy.zeros", "token_seqs.append", "keys.append", "data_utils.get_elmo_vec_batch", "data_utils.get_elmo_vec_batch", "len", "len", "len", "enumerate", "float", "data_utils.get_word_vec", "len", "next", "len", "zip", "numpy.ones", "len", "print", "len", "len", "enumerate", "bert_utils.convert_sentence_and_mention_to_features", "len", "range", "len", "len", "len", "len", "len", "data_utils.get_elmo_vec", "len", "len", "len", "len", "mention_seq.index", "enumerate", "enumerate", "data_utils.get_word_vec", "data_utils.get_word_vec", "data_utils.get_word_vec", "len", "len", "len", "len", "len", "data_utils.get_word_vec", "len", "len", "min", "min", "data_utils.get_word_vec", "len", "data_utils.get_elmo_vec", "data_utils.get_word_vec", "min", "min", "enumerate", "data_utils.get_word_vec", "len", "len", "data_utils.get_word_vec", "len", "len", "len", "len", "len", "data_utils.get_word_vec"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.pad_slice", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.get_elmo_vec_batch", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.get_elmo_vec_batch", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.get_word_vec", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.bert_utils.convert_sentence_and_mention_to_features", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.get_elmo_vec", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.get_word_vec", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.get_word_vec", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.get_word_vec", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.get_word_vec", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.get_word_vec", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.get_elmo_vec", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.get_word_vec", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.get_word_vec", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.get_word_vec", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.data_utils.get_word_vec"], ["", "", "def", "get_example", "(", "generator", ",", "glove_dict", ",", "batch_size", ",", "answer_num", ",", "\n", "eval_data", "=", "False", ",", "simple_mention", "=", "True", ",", "\n", "elmo", "=", "None", ",", "bert", "=", "None", ",", "bert_tokenizer", "=", "None", ",", "finetune_bert", "=", "False", ",", "\n", "data_config", "=", "None", ",", "is_labeler", "=", "False", ",", "type_elmo", "=", "None", ",", "all_types", "=", "None", ",", "\n", "use_type_definition", "=", "False", ")", ":", "\n", "\n", "  ", "use_elmo_batch", "=", "True", "if", "elmo", "is", "not", "None", "else", "False", "### use elmo batch", "\n", "#use_elmo_batch = True if not eval_data else False ### use elmo batch", "\n", "\n", "embed_dim", "=", "300", "if", "elmo", "is", "None", "else", "1024", "\n", "cur_stream", "=", "[", "None", "]", "*", "batch_size", "\n", "no_more_data", "=", "False", "\n", "\n", "while", "True", ":", "\n", "    ", "bsz", "=", "batch_size", "\n", "seq_length", "=", "25", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "      ", "try", ":", "\n", "        ", "cur_stream", "[", "i", "]", "=", "list", "(", "next", "(", "generator", ")", ")", "\n", "", "except", "StopIteration", ":", "\n", "        ", "no_more_data", "=", "True", "\n", "bsz", "=", "i", "\n", "break", "\n", "\n", "", "", "max_seq_length", "=", "min", "(", "50", ",", "max", "(", "[", "len", "(", "elem", "[", "1", "]", ")", "+", "len", "(", "elem", "[", "2", "]", ")", "+", "len", "(", "elem", "[", "3", "]", ")", "for", "elem", "in", "cur_stream", "if", "elem", "]", ")", ")", "\n", "token_embed", "=", "np", ".", "zeros", "(", "[", "bsz", ",", "max_seq_length", ",", "embed_dim", "]", ",", "np", ".", "float32", ")", "\n", "token_seq_length", "=", "np", ".", "zeros", "(", "[", "bsz", "]", ",", "np", ".", "float32", ")", "\n", "token_bio", "=", "np", ".", "zeros", "(", "[", "bsz", ",", "max_seq_length", ",", "4", "]", ",", "np", ".", "float32", ")", "\n", "mention_start_ind", "=", "np", ".", "zeros", "(", "[", "bsz", ",", "1", "]", ",", "np", ".", "int64", ")", "\n", "mention_end_ind", "=", "np", ".", "zeros", "(", "[", "bsz", ",", "1", "]", ",", "np", ".", "int64", ")", "\n", "max_mention_length", "=", "min", "(", "20", ",", "max", "(", "[", "len", "(", "elem", "[", "3", "]", ")", "for", "elem", "in", "cur_stream", "if", "elem", "]", ")", ")", "\n", "max_span_chars", "=", "min", "(", "25", ",", "max", "(", "max", "(", "[", "len", "(", "elem", "[", "5", "]", ")", "for", "elem", "in", "cur_stream", "if", "elem", "]", ")", ",", "5", ")", ")", "\n", "max_n_target", "=", "max", "(", "[", "len", "(", "elem", "[", "4", "]", "[", ":", "]", ")", "for", "elem", "in", "cur_stream", "if", "elem", "]", ")", "\n", "annot_ids", "=", "np", ".", "zeros", "(", "[", "bsz", "]", ",", "np", ".", "object", ")", "\n", "span_chars", "=", "np", ".", "zeros", "(", "[", "bsz", ",", "max_span_chars", "]", ",", "np", ".", "int64", ")", "\n", "mention_embed", "=", "np", ".", "zeros", "(", "[", "bsz", ",", "max_mention_length", ",", "embed_dim", "]", ",", "np", ".", "float32", ")", "\n", "targets", "=", "np", ".", "zeros", "(", "[", "bsz", ",", "answer_num", "]", ",", "np", ".", "float32", ")", "\n", "\n", "if", "is_labeler", ":", "\n", "      ", "y_tups", "=", "[", "list", "(", "zip", "(", "elem", "[", "9", "]", ",", "elem", "[", "8", "]", ")", ")", "for", "elem", "in", "cur_stream", "if", "elem", "]", "# noisy y (9, 8) ", "\n", "y_noisy", "=", "[", "[", "t", "[", "1", "]", "for", "t", "in", "tup", "]", "for", "tup", "in", "y_tups", "]", "\n", "y_noisy_idx", "=", "[", "[", "t", "[", "0", "]", "for", "t", "in", "tup", "]", "for", "tup", "in", "y_tups", "]", "\n", "y_noisy_lengths", "=", "[", "len", "(", "yn", ")", "for", "yn", "in", "y_noisy", "]", "\n", "max_y_noisy", "=", "max", "(", "y_noisy_lengths", ")", "\n", "y_noisy_embed", "=", "np", ".", "zeros", "(", "[", "bsz", ",", "max_y_noisy", ",", "embed_dim", "]", ",", "np", ".", "float32", ")", "# assum  ELMo ", "\n", "y_noisy_lengths_np", "=", "np", ".", "array", "(", "y_noisy_lengths", ",", "np", ".", "int64", ")", "\n", "y_noisy_idx_np", "=", "np", ".", "ones", "(", "[", "bsz", ",", "max_y_noisy", "]", ",", "np", ".", "int64", ")", "*", "constant", ".", "TYPE_PAD_IDX", "\n", "y_cls", "=", "np", ".", "zeros", "(", "[", "bsz", "]", ",", "np", ".", "float32", ")", "\n", "if", "use_type_definition", ":", "\n", "        ", "max_def_length", "=", "max", "(", "[", "len", "(", "x", ")", "for", "elem", "in", "cur_stream", "if", "elem", "for", "x", "in", "elem", "[", "13", "]", "]", ")", "\n", "type_definition_idx", "=", "np", ".", "ones", "(", "[", "bsz", ",", "max_y_noisy", ",", "max_def_length", "]", ",", "np", ".", "int64", ")", "*", "constant", ".", "DEF_PAD_IDX", "\n", "type_definition_length", "=", "np", ".", "zeros", "(", "[", "bsz", ",", "max_y_noisy", "]", ",", "np", ".", "int64", ")", "\n", "\n", "", "", "mention_headword_embed", "=", "np", ".", "zeros", "(", "[", "bsz", ",", "embed_dim", "]", ",", "np", ".", "float32", ")", "\n", "mention_span_length", "=", "np", ".", "zeros", "(", "[", "bsz", "]", ",", "np", ".", "float32", ")", "\n", "\n", "if", "elmo", "is", "not", "None", ":", "\n", "      ", "token_embed", "=", "np", ".", "zeros", "(", "[", "bsz", ",", "3", ",", "max_seq_length", ",", "embed_dim", "]", ",", "np", ".", "float32", ")", "\n", "mention_embed", "=", "np", ".", "zeros", "(", "[", "bsz", ",", "3", ",", "max_mention_length", ",", "embed_dim", "]", ",", "np", ".", "float32", ")", "\n", "mention_headword_embed", "=", "np", ".", "zeros", "(", "[", "bsz", ",", "3", ",", "embed_dim", "]", ",", "np", ".", "float32", ")", "\n", "elmo_mention_first", "=", "np", ".", "zeros", "(", "[", "bsz", ",", "3", ",", "embed_dim", "]", ",", "np", ".", "float32", ")", "\n", "elmo_mention_last", "=", "np", ".", "zeros", "(", "[", "bsz", ",", "3", ",", "embed_dim", "]", ",", "np", ".", "float32", ")", "\n", "", "if", "glove_dict", "is", "not", "None", "and", "elmo", "is", "not", "None", "and", "not", "is_labeler", ":", "\n", "      ", "token_embed", "=", "np", ".", "zeros", "(", "[", "bsz", ",", "4", ",", "max_seq_length", ",", "embed_dim", "]", ",", "np", ".", "float32", ")", "\n", "mention_embed", "=", "np", ".", "zeros", "(", "[", "bsz", ",", "4", ",", "max_mention_length", ",", "embed_dim", "]", ",", "np", ".", "float32", ")", "\n", "mention_headword_embed", "=", "np", ".", "zeros", "(", "[", "bsz", ",", "4", ",", "embed_dim", "]", ",", "np", ".", "float32", ")", "\n", "elmo_mention_first", "=", "np", ".", "zeros", "(", "[", "bsz", ",", "4", ",", "embed_dim", "]", ",", "np", ".", "float32", ")", "\n", "elmo_mention_last", "=", "np", ".", "zeros", "(", "[", "bsz", ",", "4", ",", "embed_dim", "]", ",", "np", ".", "float32", ")", "\n", "", "if", "finetune_bert", ":", "\n", "      ", "bert_max_seq_length", "=", "128", "\n", "bert_input_idx", "=", "np", ".", "zeros", "(", "[", "bsz", ",", "bert_max_seq_length", "]", ",", "np", ".", "int64", ")", "\n", "bert_token_type_idx", "=", "np", ".", "zeros", "(", "[", "bsz", ",", "bert_max_seq_length", "]", ",", "np", ".", "int64", ")", "\n", "bert_attention_mask", "=", "np", ".", "zeros", "(", "[", "bsz", ",", "bert_max_seq_length", "]", ",", "np", ".", "int64", ")", "\n", "bert_head_wordpiece_idx", "=", "np", ".", "zeros", "(", "[", "bsz", "]", ",", "np", ".", "int64", ")", "\n", "\n", "# Only Train: batch to ELMo embeddings", "\n", "# Will get CUDA memory error if batch size is large", "\n", "", "if", "use_elmo_batch", ":", "\n", "      ", "token_seqs", "=", "[", "]", "\n", "keys", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "bsz", ")", ":", "\n", "        ", "left_seq", "=", "cur_stream", "[", "i", "]", "[", "1", "]", "\n", "if", "len", "(", "left_seq", ")", ">", "seq_length", ":", "\n", "          ", "left_seq", "=", "left_seq", "[", "-", "seq_length", ":", "]", "\n", "", "mention_seq", "=", "cur_stream", "[", "i", "]", "[", "3", "]", "\n", "right_seq", "=", "cur_stream", "[", "i", "]", "[", "2", "]", "\n", "token_seqs", ".", "append", "(", "left_seq", "+", "mention_seq", "+", "right_seq", ")", "\n", "keys", ".", "append", "(", "cur_stream", "[", "i", "]", "[", "0", "]", ")", "\n", "\n", "", "try", ":", "\n", "        ", "elmo_emb_batch", "=", "get_elmo_vec_batch", "(", "token_seqs", ",", "elmo", ")", "# (batch, 3, len, dim)", "\n", "", "except", ":", "\n", "        ", "print", "(", "'ERROR:'", ",", "bsz", ",", "token_seqs", ",", "cur_stream", "[", "i", "]", ")", "\n", "raise", "\n", "\n", "", "if", "is_labeler", ":", "\n", "        ", "elmo_y_noisy_emb_batch", "=", "get_elmo_vec_batch", "(", "y_noisy", ",", "elmo", ")", "\n", "elmo_y_noisy_emb_batch", "=", "[", "x", "[", "0", ",", ":", ",", ":", "]", "for", "x", "in", "elmo_y_noisy_emb_batch", "]", "\n", "\n", "", "", "for", "i", "in", "range", "(", "bsz", ")", ":", "\n", "      ", "left_seq", "=", "cur_stream", "[", "i", "]", "[", "1", "]", "\n", "if", "len", "(", "left_seq", ")", ">", "seq_length", ":", "\n", "        ", "left_seq", "=", "left_seq", "[", "-", "seq_length", ":", "]", "\n", "", "mention_seq", "=", "cur_stream", "[", "i", "]", "[", "3", "]", "\n", "annot_ids", "[", "i", "]", "=", "cur_stream", "[", "i", "]", "[", "0", "]", "\n", "right_seq", "=", "cur_stream", "[", "i", "]", "[", "2", "]", "\n", "mention_headword", "=", "cur_stream", "[", "i", "]", "[", "6", "]", "\n", "\n", "token_seq", "=", "left_seq", "+", "mention_seq", "+", "right_seq", "\n", "mention_start_ind", "[", "i", "]", "=", "min", "(", "seq_length", ",", "len", "(", "left_seq", ")", ")", "\n", "mention_end_ind", "[", "i", "]", "=", "min", "(", "49", ",", "len", "(", "left_seq", ")", "+", "len", "(", "mention_seq", ")", "-", "1", ")", "\n", "mention_start_actual", "=", "len", "(", "left_seq", ")", "\n", "mention_end_actual", "=", "len", "(", "left_seq", ")", "+", "len", "(", "mention_seq", ")", "-", "1", "\n", "if", "elmo", "is", "None", "and", "bert", "is", "None", ":", "# GLoVe or BERT", "\n", "        ", "if", "not", "finetune_bert", ":", "# GLoVe", "\n", "          ", "for", "j", ",", "word", "in", "enumerate", "(", "token_seq", ")", ":", "\n", "            ", "if", "j", "<", "max_seq_length", ":", "\n", "              ", "token_embed", "[", "i", ",", "j", ",", ":", "embed_dim", "]", "=", "get_word_vec", "(", "word", ",", "glove_dict", ")", "\n", "", "", "", "else", ":", "# For BERT Ver.2", "\n", "          ", "input_idx", ",", "input_mask", ",", "segment_idx", ",", "_", ",", "_", "=", "convert_sentence_and_mention_to_features", "(", "\n", "' '", ".", "join", "(", "token_seq", ")", ",", "' '", ".", "join", "(", "mention_seq", ")", ",", "bert_max_seq_length", ",", "bert_tokenizer", "\n", ")", "\n", "bert_input_idx", "[", "i", ",", ":", "]", "=", "input_idx", "\n", "bert_token_type_idx", "[", "i", ",", ":", "]", "=", "segment_idx", "\n", "bert_attention_mask", "[", "i", ",", ":", "]", "=", "input_mask", "\n", "bert_head_wordpiece_idx", "[", "i", "]", "=", "cur_stream", "[", "i", "]", "[", "11", "]", "if", "is_labeler", "else", "cur_stream", "[", "i", "]", "[", "8", "]", "\n", "", "", "elif", "elmo", "is", "not", "None", "and", "bert", "is", "None", ":", "# ELMo", "\n", "# sentence", "\n", "        ", "if", "use_elmo_batch", ":", "# Train", "\n", "          ", "elmo_emb", "=", "elmo_emb_batch", "[", "i", "]", "# (3, len, dim)", "\n", "if", "is_labeler", ":", "\n", "            ", "y_noisy_embed", "[", "i", ",", ":", "y_noisy_lengths", "[", "i", "]", ",", ":", "]", "=", "elmo_y_noisy_emb_batch", "[", "i", "]", "\n", "", "", "else", ":", "# Eval", "\n", "          ", "elmo_emb", "=", "get_elmo_vec", "(", "token_seq", ",", "elmo", ")", "\n", "if", "is_labeler", ":", "\n", "            ", "y_noisy_embed", "[", "i", ",", ":", "y_noisy_lengths", "[", "i", "]", ",", ":", "]", "=", "get_elmo_vec", "(", "y_noisy", "[", "i", "]", ",", "elmo", ")", "[", "0", ",", ":", ",", ":", "]", "\n", "", "", "n_layers", ",", "seq_len", ",", "elmo_dim", "=", "elmo_emb", ".", "shape", "\n", "assert", "n_layers", "==", "3", ",", "n_layers", "\n", "assert", "seq_len", "==", "len", "(", "token_seq", ")", ",", "(", "seq_len", ",", "len", "(", "token_seq", ")", ",", "token_seq", ",", "elmo_emb", ".", "shape", ")", "\n", "assert", "elmo_dim", "==", "embed_dim", ",", "(", "elmo_dim", ",", "embed_dim", ")", "\n", "if", "seq_len", "<=", "max_seq_length", ":", "\n", "          ", "token_embed", "[", "i", ",", ":", "n_layers", ",", ":", "seq_len", ",", ":", "]", "=", "elmo_emb", "\n", "", "else", ":", "\n", "          ", "token_embed", "[", "i", ",", ":", "n_layers", ",", ":", ",", ":", "]", "=", "elmo_emb", "[", ":", ",", ":", "max_seq_length", ",", ":", "]", "\n", "# mention span", "\n", "", "start_ind", "=", "len", "(", "left_seq", ")", "\n", "end_ind", "=", "len", "(", "left_seq", ")", "+", "len", "(", "mention_seq", ")", "-", "1", "\n", "elmo_mention", "=", "elmo_emb", "[", ":", ",", "start_ind", ":", "end_ind", "+", "1", ",", ":", "]", "\n", "mention_len", "=", "end_ind", "-", "start_ind", "+", "1", "\n", "assert", "mention_len", "==", "elmo_mention", ".", "shape", "[", "1", "]", "==", "len", "(", "mention_seq", ")", ",", "(", "mention_len", ",", "elmo_mention", ".", "shape", "[", "1", "]", ",", "len", "(", "mention_seq", ")", ",", "mention_seq", ",", "elmo_mention", ".", "shape", ",", "token_seq", ",", "elmo_emb", ".", "shape", ")", "# (mention_len, elmo_mention.shape[0], len(mention_seq))", "\n", "if", "mention_len", "<", "max_mention_length", ":", "\n", "          ", "mention_embed", "[", "i", ",", ":", "n_layers", ",", ":", "mention_len", ",", ":", "]", "=", "elmo_mention", "\n", "", "else", ":", "\n", "          ", "mention_embed", "[", "i", ",", ":", "n_layers", ",", ":", "mention_len", ",", ":", "]", "=", "elmo_mention", "[", ":", ",", ":", "max_mention_length", ",", ":", "]", "\n", "# mention first & last words", "\n", "", "elmo_mention_first", "[", "i", ",", ":", "n_layers", ",", ":", "]", "=", "elmo_mention", "[", ":", ",", "0", ",", ":", "]", "\n", "elmo_mention_last", "[", "i", ",", ":", "n_layers", ",", ":", "]", "=", "elmo_mention", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "# headword", "\n", "try", ":", "\n", "          ", "headword_location", "=", "mention_seq", ".", "index", "(", "mention_headword", ")", "\n", "", "except", ":", "\n", "#print('WARNING: ' + mention_headword + ' / ' + ' '.join(mention_seq))", "\n", "# find the headword", "\n", "          ", "headword_location", "=", "0", "\n", "headword_candidates", "=", "[", "i", "for", "i", ",", "word", "in", "enumerate", "(", "mention_seq", ")", "if", "mention_headword", "in", "word", "]", "\n", "if", "headword_candidates", ":", "\n", "            ", "headword_location", "=", "headword_candidates", "[", "0", "]", "\n", "", "", "mention_headword_embed", "[", "i", ",", ":", "n_layers", ",", ":", "]", "=", "elmo_mention", "[", ":", ",", "headword_location", ",", ":", "]", "\n", "# add 300d-GLoVe", "\n", "if", "glove_dict", "is", "not", "None", "and", "not", "is_labeler", ":", "\n", "# sentence", "\n", "          ", "for", "j", ",", "word", "in", "enumerate", "(", "token_seq", ")", ":", "\n", "            ", "if", "j", "<", "max_seq_length", ":", "\n", "              ", "token_embed", "[", "i", ",", "3", ",", "j", ",", ":", "300", "]", "=", "get_word_vec", "(", "word", ",", "glove_dict", ")", "\n", "# mention span", "\n", "", "", "for", "j", ",", "mention_word", "in", "enumerate", "(", "mention_seq", ")", ":", "\n", "            ", "if", "j", "<", "max_mention_length", ":", "\n", "              ", "if", "simple_mention", ":", "\n", "                ", "mention_embed", "[", "i", ",", "3", ",", "j", ",", ":", "300", "]", "=", "[", "k", "/", "len", "(", "cur_stream", "[", "i", "]", "[", "3", "]", ")", "for", "k", "in", "\n", "get_word_vec", "(", "mention_word", ",", "glove_dict", ")", "]", "\n", "", "else", ":", "\n", "                ", "mention_embed", "[", "i", ",", "3", ",", "j", ",", ":", "300", "]", "=", "get_word_vec", "(", "mention_word", ",", "glove_dict", ")", "\n", "# mention first & last words", "\n", "", "", "", "elmo_mention_first", "[", "i", ",", "3", ",", ":", "300", "]", "=", "get_word_vec", "(", "mention_seq", "[", "0", "]", ",", "glove_dict", ")", "\n", "elmo_mention_last", "[", "i", ",", "3", ",", ":", "300", "]", "=", "get_word_vec", "(", "mention_seq", "[", "-", "1", "]", ",", "glove_dict", ")", "\n", "# headword", "\n", "mention_headword_embed", "[", "i", ",", "3", ",", ":", "300", "]", "=", "get_word_vec", "(", "mention_headword", ",", "glove_dict", ")", "\n", "", "", "for", "j", ",", "_", "in", "enumerate", "(", "left_seq", ")", ":", "\n", "        ", "token_bio", "[", "i", ",", "min", "(", "j", ",", "49", ")", ",", "0", "]", "=", "1.0", "# token bio: 0(left) start(1) inside(2)  3(after)", "\n", "", "for", "j", ",", "_", "in", "enumerate", "(", "right_seq", ")", ":", "\n", "        ", "token_bio", "[", "i", ",", "min", "(", "j", "+", "len", "(", "mention_seq", ")", "+", "len", "(", "left_seq", ")", ",", "49", ")", ",", "3", "]", "=", "1.0", "\n", "", "for", "j", ",", "_", "in", "enumerate", "(", "mention_seq", ")", ":", "\n", "        ", "if", "j", "==", "0", "and", "len", "(", "mention_seq", ")", "==", "1", ":", "\n", "          ", "token_bio", "[", "i", ",", "min", "(", "j", "+", "len", "(", "left_seq", ")", ",", "49", ")", ",", "1", "]", "=", "1.0", "\n", "", "else", ":", "\n", "          ", "token_bio", "[", "i", ",", "min", "(", "j", "+", "len", "(", "left_seq", ")", ",", "49", ")", ",", "2", "]", "=", "1.0", "\n", "", "", "token_seq_length", "[", "i", "]", "=", "min", "(", "50", ",", "len", "(", "token_seq", ")", ")", "\n", "\n", "if", "elmo", "is", "None", "and", "not", "finetune_bert", ":", "\n", "        ", "for", "j", ",", "mention_word", "in", "enumerate", "(", "mention_seq", ")", ":", "\n", "          ", "if", "j", "<", "max_mention_length", ":", "\n", "            ", "if", "simple_mention", ":", "\n", "              ", "mention_embed", "[", "i", ",", "j", ",", ":", "embed_dim", "]", "=", "[", "k", "/", "len", "(", "cur_stream", "[", "i", "]", "[", "3", "]", ")", "for", "k", "in", "\n", "get_word_vec", "(", "mention_word", ",", "glove_dict", ")", "]", "\n", "", "else", ":", "\n", "              ", "mention_embed", "[", "i", ",", "j", ",", ":", "embed_dim", "]", "=", "get_word_vec", "(", "mention_word", ",", "glove_dict", ")", "\n", "", "", "", "", "span_chars", "[", "i", ",", ":", "]", "=", "pad_slice", "(", "cur_stream", "[", "i", "]", "[", "5", "]", ",", "max_span_chars", ",", "pad_token", "=", "0", ")", "\n", "for", "answer_ind", "in", "cur_stream", "[", "i", "]", "[", "4", "]", ":", "\n", "        ", "targets", "[", "i", ",", "answer_ind", "]", "=", "1.0", "\n", "", "if", "is_labeler", ":", "\n", "        ", "y_noisy_idx_np", "[", "i", ",", ":", "len", "(", "y_noisy_idx", "[", "i", "]", ")", "]", "=", "y_noisy_idx", "[", "i", "]", "\n", "y_cls", "[", "i", "]", "=", "float", "(", "cur_stream", "[", "i", "]", "[", "10", "]", ")", "\n", "if", "use_type_definition", ":", "\n", "          ", "for", "t_idx", "in", "range", "(", "len", "(", "cur_stream", "[", "i", "]", "[", "13", "]", ")", ")", ":", "\n", "#print(len(cur_stream), i, t_idx, len(cur_stream[i][13]), cur_stream[i][13])", "\n", "            ", "type_definition_idx", "[", "i", ",", "t_idx", ",", ":", "len", "(", "cur_stream", "[", "i", "]", "[", "13", "]", "[", "t_idx", "]", ")", "]", "=", "cur_stream", "[", "i", "]", "[", "13", "]", "[", "t_idx", "]", "\n", "type_definition_length", "[", "i", ",", "t_idx", "]", "=", "len", "(", "cur_stream", "[", "i", "]", "[", "13", "]", "[", "t_idx", "]", ")", "\n", "", "", "", "if", "elmo", "is", "None", "and", "not", "finetune_bert", ":", "\n", "        ", "mention_headword_embed", "[", "i", ",", ":", "embed_dim", "]", "=", "get_word_vec", "(", "mention_headword", ",", "glove_dict", ")", "\n", "", "mention_span_length", "[", "i", "]", "=", "min", "(", "len", "(", "mention_seq", ")", ",", "20", ")", "\n", "\n", "", "feed_dict", "=", "{", "\"annot_id\"", ":", "annot_ids", ",", "\n", "\"mention_embed\"", ":", "mention_embed", ",", "\n", "\"span_chars\"", ":", "span_chars", ",", "\n", "\"y\"", ":", "targets", ",", "\n", "\"mention_headword_embed\"", ":", "mention_headword_embed", ",", "\n", "\"mention_span_length\"", ":", "mention_span_length", "}", "\n", "\n", "feed_dict", "[", "\"token_bio\"", "]", "=", "token_bio", "\n", "feed_dict", "[", "\"token_embed\"", "]", "=", "token_embed", "\n", "feed_dict", "[", "\"token_seq_length\"", "]", "=", "token_seq_length", "\n", "feed_dict", "[", "\"mention_start_ind\"", "]", "=", "mention_start_ind", "\n", "feed_dict", "[", "\"mention_end_ind\"", "]", "=", "mention_end_ind", "\n", "if", "elmo", "is", "not", "None", ":", "\n", "      ", "feed_dict", "[", "\"mention_first\"", "]", "=", "elmo_mention_first", "\n", "feed_dict", "[", "\"mention_last\"", "]", "=", "elmo_mention_last", "\n", "", "if", "bert", "is", "not", "None", "or", "finetune_bert", ":", "\n", "      ", "feed_dict", "[", "\"bert_input_idx\"", "]", "=", "bert_input_idx", "\n", "feed_dict", "[", "\"bert_token_type_idx\"", "]", "=", "bert_token_type_idx", "\n", "feed_dict", "[", "\"bert_attention_mask\"", "]", "=", "bert_attention_mask", "\n", "feed_dict", "[", "\"bert_head_wordpiece_idx\"", "]", "=", "bert_head_wordpiece_idx", "\n", "", "if", "is_labeler", ":", "\n", "      ", "feed_dict", "[", "\"y_noisy_embed\"", "]", "=", "y_noisy_embed", "\n", "feed_dict", "[", "\"y_noisy_lengths\"", "]", "=", "y_noisy_lengths_np", "\n", "feed_dict", "[", "\"y_noisy_idx\"", "]", "=", "y_noisy_idx_np", "\n", "feed_dict", "[", "\"y_cls\"", "]", "=", "y_cls", "\n", "if", "use_type_definition", ":", "\n", "        ", "feed_dict", "[", "\"type_definition_idx\"", "]", "=", "type_definition_idx", "\n", "feed_dict", "[", "\"type_definition_length\"", "]", "=", "type_definition_length", "\n", "", "", "if", "no_more_data", ":", "\n", "      ", "if", "eval_data", "and", "bsz", ">", "0", ":", "\n", "        ", "yield", "feed_dict", "\n", "", "break", "\n", "", "yield", "feed_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.denoising_models.LabelerBase.__init__": [[26, 63], ["models.ModelBase.__init__", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "model_utils.SelfAttentiveSum", "torch.LSTM", "torch.LSTM", "torch.Linear", "torch.Linear", "torch.Sigmoid", "torch.Sigmoid", "model_utils.ELMoWeightedSum", "torch.LSTM", "torch.LSTM", "model_utils.TypeAttentiveSum", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss", "model_utils.SelfAttentiveSum", "model_utils.CNN", "torch.LSTM", "torch.LSTM", "model_utils.SelfAttentiveSum", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["  ", "def", "__init__", "(", "self", ",", "args", ",", "answer_num", ")", ":", "\n", "    ", "super", "(", "LabelerBase", ",", "self", ")", ".", "__init__", "(", "args", ",", "answer_num", ")", "\n", "self", ".", "output_dim", "=", "args", ".", "rnn_dim", "*", "2", "\n", "self", ".", "mention_dropout", "=", "nn", ".", "Dropout", "(", "args", ".", "mention_dropout", ")", "\n", "self", ".", "input_dropout", "=", "nn", ".", "Dropout", "(", "args", ".", "input_dropout", ")", "\n", "self", ".", "dim_hidden", "=", "args", ".", "dim_hidden", "\n", "self", ".", "embed_dim", "=", "1024", "\n", "self", ".", "mention_dim", "=", "1024", "\n", "self", ".", "headword_dim", "=", "1024", "\n", "self", ".", "enhanced_mention", "=", "args", ".", "enhanced_mention", "\n", "self", ".", "add_headword_emb", "=", "args", ".", "add_headword_emb", "\n", "self", ".", "mention_lstm", "=", "args", ".", "mention_lstm", "\n", "if", "args", ".", "enhanced_mention", ":", "\n", "      ", "self", ".", "head_attentive_sum", "=", "SelfAttentiveSum", "(", "self", ".", "mention_dim", ",", "1", ")", "\n", "self", ".", "cnn", "=", "CNN", "(", ")", "\n", "self", ".", "mention_dim", "+=", "50", "\n", "", "self", ".", "output_dim", "+=", "self", ".", "mention_dim", "\n", "if", "self", ".", "add_headword_emb", ":", "\n", "      ", "self", ".", "output_dim", "+=", "self", ".", "headword_dim", "\n", "# Defining LSTM here.  ", "\n", "", "self", ".", "attentive_sum", "=", "SelfAttentiveSum", "(", "args", ".", "rnn_dim", "*", "2", ",", "100", ")", "\n", "self", ".", "lstm", "=", "nn", ".", "LSTM", "(", "self", ".", "embed_dim", "+", "50", ",", "args", ".", "rnn_dim", ",", "bidirectional", "=", "True", ",", "\n", "batch_first", "=", "True", ")", "\n", "self", ".", "token_mask", "=", "nn", ".", "Linear", "(", "4", ",", "50", ")", "\n", "if", "self", ".", "mention_lstm", ":", "\n", "      ", "self", ".", "lstm_mention", "=", "nn", ".", "LSTM", "(", "self", ".", "embed_dim", ",", "self", ".", "embed_dim", "//", "2", ",", "bidirectional", "=", "True", ",", "\n", "batch_first", "=", "True", ")", "\n", "self", ".", "mention_attentive_sum", "=", "SelfAttentiveSum", "(", "self", ".", "embed_dim", ",", "1", ")", "\n", "", "self", ".", "sigmoid_fn", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "self", ".", "goal", "=", "args", ".", "goal", "\n", "self", ".", "hidden_dim", "=", "300", "\n", "self", ".", "weighted_sum", "=", "ELMoWeightedSum", "(", ")", "\n", "# init labeler params", "\n", "self", ".", "type_hid_dim", "=", "1024", "\n", "self", ".", "lstm_label", "=", "nn", ".", "LSTM", "(", "self", ".", "embed_dim", ",", "self", ".", "type_hid_dim", "//", "2", ",", "bidirectional", "=", "True", ",", "batch_first", "=", "True", ")", "\n", "self", ".", "type_attentive_sum", "=", "TypeAttentiveSum", "(", "self", ".", "output_dim", ",", "self", ".", "type_hid_dim", ")", "\n", "self", ".", "cls_loss_func", "=", "nn", ".", "BCEWithLogitsLoss", "(", "pos_weight", "=", "torch", ".", "Tensor", "(", "[", "1.", "]", ")", ".", "cuda", "(", ")", ")", "########### scale positive side", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.denoising_models.LabelerBase.define_cls_loss": [[64, 67], ["denoising_models.LabelerBase.cls_loss_func"], "methods", ["None"], ["", "def", "define_cls_loss", "(", "self", ",", "logits", ",", "targets", ")", ":", "\n", "    ", "loss", "=", "self", ".", "cls_loss_func", "(", "logits", ",", "targets", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.denoising_models.LabelerBase.define_type_loss": [[68, 71], ["denoising_models.LabelerBase.define_loss"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.models.ModelBase.define_loss"], ["", "def", "define_type_loss", "(", "self", ",", "logits", ",", "targets", ",", "cls_targets", ",", "data_type", ")", ":", "\n", "    ", "loss", "=", "self", ".", "define_loss", "(", "logits", ",", "targets", ",", "data_type", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.denoising_models.LabelerBase.sent_encoder": [[72, 99], ["denoising_models.LabelerBase.weighted_sum", "denoising_models.LabelerBase.token_mask", "token_mask_embed.view.view.view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "denoising_models.LabelerBase.input_dropout", "denoising_models.LabelerBase.sorted_rnn", "denoising_models.LabelerBase.attentive_sum", "denoising_models.LabelerBase.weighted_sum", "denoising_models.LabelerBase.mention_dropout", "feed_dict[].view", "denoising_models.LabelerBase.cnn", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "denoising_models.LabelerBase.weighted_sum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "denoising_models.LabelerBase.size", "denoising_models.LabelerBase.sorted_rnn", "denoising_models.LabelerBase.mention_attentive_sum", "denoising_models.LabelerBase.head_attentive_sum"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.models.ModelBase.sorted_rnn", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.models.ModelBase.sorted_rnn"], ["", "def", "sent_encoder", "(", "self", ",", "feed_dict", ",", "data_type", ")", ":", "\n", "    ", "token_embed", "=", "self", ".", "weighted_sum", "(", "feed_dict", "[", "'token_embed'", "]", ")", "\n", "token_mask_embed", "=", "self", ".", "token_mask", "(", "feed_dict", "[", "'token_bio'", "]", ".", "view", "(", "-", "1", ",", "4", ")", ")", "\n", "token_mask_embed", "=", "token_mask_embed", ".", "view", "(", "token_embed", ".", "size", "(", ")", "[", "0", "]", ",", "-", "1", ",", "50", ")", "# location embedding", "\n", "token_embed", "=", "torch", ".", "cat", "(", "(", "token_embed", ",", "token_mask_embed", ")", ",", "2", ")", "\n", "token_embed", "=", "self", ".", "input_dropout", "(", "token_embed", ")", "\n", "context_rep", "=", "self", ".", "sorted_rnn", "(", "token_embed", ",", "feed_dict", "[", "'token_seq_length'", "]", ",", "self", ".", "lstm", ")", "\n", "context_rep", ",", "_", "=", "self", ".", "attentive_sum", "(", "context_rep", ")", "\n", "# Mention Representation", "\n", "mention_embed", "=", "self", ".", "weighted_sum", "(", "feed_dict", "[", "'mention_embed'", "]", ")", "\n", "if", "self", ".", "enhanced_mention", ":", "\n", "      ", "if", "self", ".", "mention_lstm", ":", "\n", "        ", "mention_hid", "=", "self", ".", "sorted_rnn", "(", "mention_embed", ",", "feed_dict", "[", "'mention_span_length'", "]", ",", "self", ".", "lstm_mention", ")", "\n", "mention_embed", ",", "attn_score", "=", "self", ".", "mention_attentive_sum", "(", "mention_hid", ")", "\n", "", "else", ":", "\n", "        ", "mention_embed", ",", "attn_score", "=", "self", ".", "head_attentive_sum", "(", "mention_embed", ")", "\n", "", "span_cnn_embed", "=", "self", ".", "cnn", "(", "feed_dict", "[", "'span_chars'", "]", ")", "\n", "mention_embed", "=", "torch", ".", "cat", "(", "(", "span_cnn_embed", ",", "mention_embed", ")", ",", "1", ")", "\n", "", "else", ":", "\n", "      ", "mention_embed", "=", "torch", ".", "sum", "(", "mention_embed", ",", "dim", "=", "1", ")", "\n", "", "mention_embed", "=", "self", ".", "mention_dropout", "(", "mention_embed", ")", "\n", "if", "self", ".", "add_headword_emb", ":", "\n", "      ", "mention_headword_embed", "=", "self", ".", "weighted_sum", "(", "feed_dict", "[", "'mention_headword_embed'", "]", ")", "\n", "output", "=", "torch", ".", "cat", "(", "(", "context_rep", ",", "mention_embed", ",", "mention_headword_embed", ")", ",", "1", ")", "# + Headword lstm emb ", "\n", "", "else", ":", "\n", "      ", "output", "=", "torch", ".", "cat", "(", "(", "context_rep", ",", "mention_embed", ")", ",", "1", ")", "\n", "", "return", "output", ",", "attn_score", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.denoising_models.LabelerBase.type_encoder": [[100, 104], ["denoising_models.LabelerBase.sorted_rnn", "denoising_models.LabelerBase.type_attentive_sum"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.models.ModelBase.sorted_rnn"], ["", "def", "type_encoder", "(", "self", ",", "types_embed", ",", "lengths", ",", "sent_vec", ")", ":", "\n", "    ", "hid", "=", "self", ".", "sorted_rnn", "(", "types_embed", ",", "lengths", ",", "self", ".", "lstm_label", ")", "\n", "types_embed", ",", "attn_score", "=", "self", ".", "type_attentive_sum", "(", "hid", ",", "sent_vec", ")", "\n", "return", "types_embed", ",", "attn_score", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.denoising_models.LabelerBase.forward": [[105, 107], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "feed_dict", ",", "data_type", ")", ":", "\n", "    ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.denoising_models.Labeler.__init__": [[111, 130], ["denoising_models.LabelerBase.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.LSTM", "torch.LSTM", "model_utils.TypeAttentiveSum", "print", "model_utils.MultiSimpleDecoder", "model_utils.SimpleDecoder"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["  ", "def", "__init__", "(", "self", ",", "args", ",", "answer_num", ")", ":", "\n", "    ", "super", "(", "Labeler", ",", "self", ")", ".", "__init__", "(", "args", ",", "answer_num", ")", "\n", "self", ".", "type_embed_dim", "=", "1024", "\n", "self", ".", "def_embed_dim", "=", "1024", "\n", "if", "args", ".", "data_setup", "==", "'joint'", "and", "args", ".", "multitask", ":", "\n", "      ", "print", "(", "\"Multi-task learning\"", ")", "\n", "self", ".", "decoder", "=", "MultiSimpleDecoder", "(", "self", ".", "output_dim", "+", "self", ".", "type_embed_dim", "+", "self", ".", "def_embed_dim", ")", "\n", "", "else", ":", "\n", "      ", "self", ".", "decoder", "=", "SimpleDecoder", "(", "self", ".", "output_dim", "+", "self", ".", "type_embed_dim", "+", "self", ".", "def_embed_dim", ",", "answer_num", ")", "\n", "", "self", ".", "type_vocab_size", "=", "constant", ".", "ANSWER_NUM_DICT", "[", "'open'", "]", "+", "3", "\n", "self", ".", "bos_idx", "=", "constant", ".", "TYPE_BOS_IDX", "\n", "self", ".", "eos_idx", "=", "constant", ".", "TYPE_EOS_IDX", "\n", "self", ".", "pad_idx", "=", "constant", ".", "TYPE_PAD_IDX", "\n", "self", ".", "def_vocab_size", "=", "constant", ".", "DEF_VOCAB_SIZE", "\n", "self", ".", "def_pad_idx", "=", "constant", ".", "DEF_PAD_IDX", "\n", "self", ".", "type_embedding", "=", "nn", ".", "Embedding", "(", "self", ".", "type_vocab_size", ",", "self", ".", "type_embed_dim", ",", "padding_idx", "=", "self", ".", "pad_idx", ")", "\n", "self", ".", "def_embedding", "=", "nn", ".", "Embedding", "(", "self", ".", "def_vocab_size", ",", "self", ".", "def_embed_dim", ",", "padding_idx", "=", "self", ".", "def_pad_idx", ")", "\n", "self", ".", "lstm_def", "=", "nn", ".", "LSTM", "(", "self", ".", "def_embed_dim", ",", "self", ".", "def_embed_dim", "//", "2", ",", "bidirectional", "=", "True", ",", "batch_first", "=", "True", ")", "\n", "self", ".", "def_attentive_sum", "=", "TypeAttentiveSum", "(", "self", ".", "output_dim", ",", "self", ".", "type_hid_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.denoising_models.Labeler.forward": [[131, 149], ["denoising_models.Labeler.sent_encoder", "denoising_models.Labeler.type_embedding", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "denoising_models.Labeler.def_embedding", "denoising_models.Labeler.size", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "range", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "denoising_models.Labeler.decoder", "denoising_models.Labeler.define_loss", "denoising_models.Labeler.sorted_rnn", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.denoising_models.LabelerBase.sent_encoder", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.models.ModelBase.define_loss", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.models.ModelBase.sorted_rnn"], ["", "def", "forward", "(", "self", ",", "feed_dict", ",", "data_type", ")", ":", "\n", "    ", "sent_vec", ",", "sent_attn_score", "=", "self", ".", "sent_encoder", "(", "feed_dict", ",", "data_type", ")", "\n", "type_idx", "=", "feed_dict", "[", "'y_noisy_idx'", "]", "\n", "type_embed", "=", "self", ".", "type_embedding", "(", "type_idx", ")", "\n", "type_vec", "=", "torch", ".", "sum", "(", "type_embed", ",", "1", ")", "\n", "def_idx", "=", "feed_dict", "[", "'type_definition_idx'", "]", "\n", "def_embed", "=", "self", ".", "def_embedding", "(", "def_idx", ")", "\n", "batch_size", ",", "max_n_types", ",", "max_def_len", ",", "def_dim", "=", "def_embed", ".", "size", "(", ")", "\n", "def_vec", "=", "torch", ".", "zeros", "(", "batch_size", ",", "max_n_types", ",", "def_dim", ")", ".", "cuda", "(", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "      ", "n_types", "=", "feed_dict", "[", "\"y_noisy_lengths\"", "]", "[", "i", "]", "\n", "hid", "=", "self", ".", "sorted_rnn", "(", "def_embed", "[", "i", ",", ":", "n_types", "]", ",", "feed_dict", "[", "'type_definition_length'", "]", "[", "i", ",", ":", "n_types", "]", ",", "self", ".", "lstm_def", ")", "\n", "def_vec", "[", "i", ",", ":", "n_types", ",", ":", "]", "=", "hid", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "", "def_vec", "=", "torch", ".", "sum", "(", "def_vec", ",", "1", ")", "\n", "output", "=", "torch", ".", "cat", "(", "(", "sent_vec", ",", "type_vec", ",", "def_vec", ")", ",", "1", ")", "\n", "logits", "=", "self", ".", "decoder", "(", "output", ",", "data_type", ")", "\n", "loss", "=", "self", ".", "define_loss", "(", "logits", ",", "feed_dict", "[", "'y'", "]", ",", "data_type", ")", "\n", "return", "loss", ",", "logits", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.denoising_models.Filter.__init__": [[153, 172], ["denoising_models.LabelerBase.__init__", "torch.Embedding", "torch.Embedding", "torch.Linear", "torch.Linear", "torch.Embedding", "torch.Embedding", "torch.LSTM", "torch.LSTM", "model_utils.TypeAttentiveSum", "torch.ReLU", "torch.ReLU", "model_utils.HighwayNetwork"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["  ", "def", "__init__", "(", "self", ",", "args", ",", "answer_num", ")", ":", "\n", "    ", "super", "(", "Filter", ",", "self", ")", ".", "__init__", "(", "args", ",", "answer_num", ")", "\n", "self", ".", "answer_num", "=", "answer_num", "\n", "self", ".", "type_embed_dim", "=", "1024", "\n", "self", ".", "def_embed_dim", "=", "1024", "\n", "self", ".", "vocab_size", "=", "constant", ".", "ANSWER_NUM_DICT", "[", "'open'", "]", "+", "3", "\n", "self", ".", "bos_idx", "=", "constant", ".", "TYPE_BOS_IDX", "\n", "self", ".", "eos_idx", "=", "constant", ".", "TYPE_EOS_IDX", "\n", "self", ".", "pad_idx", "=", "constant", ".", "TYPE_PAD_IDX", "\n", "self", ".", "type_embedding", "=", "nn", ".", "Embedding", "(", "self", ".", "vocab_size", ",", "self", ".", "embed_dim", ",", "padding_idx", "=", "self", ".", "pad_idx", ")", "\n", "self", ".", "def_vocab_size", "=", "constant", ".", "DEF_VOCAB_SIZE", "\n", "self", ".", "def_pad_idx", "=", "constant", ".", "DEF_PAD_IDX", "\n", "self", ".", "cls_decoder", "=", "nn", ".", "Linear", "(", "self", ".", "output_dim", "+", "self", ".", "embed_dim", "+", "self", ".", "def_embed_dim", ",", "1", ",", "bias", "=", "True", ")", "\n", "self", ".", "def_embedding", "=", "nn", ".", "Embedding", "(", "self", ".", "def_vocab_size", ",", "self", ".", "def_embed_dim", ",", "padding_idx", "=", "self", ".", "def_pad_idx", ")", "\n", "self", ".", "lstm_def", "=", "nn", ".", "LSTM", "(", "self", ".", "def_embed_dim", ",", "self", ".", "def_embed_dim", "//", "2", ",", "bidirectional", "=", "True", ",", "batch_first", "=", "True", ")", "\n", "self", ".", "def_attentive_sum", "=", "TypeAttentiveSum", "(", "self", ".", "output_dim", ",", "self", ".", "type_hid_dim", ")", "\n", "self", ".", "alpha", "=", "1.", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "highway", "=", "HighwayNetwork", "(", "self", ".", "output_dim", "+", "self", ".", "embed_dim", "+", "self", ".", "def_embed_dim", ",", "1", ",", "self", ".", "relu", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.denoising_models.Filter.forward": [[173, 194], ["denoising_models.Filter.sent_encoder", "denoising_models.Filter.type_embedding", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "denoising_models.Filter.def_embedding", "denoising_models.Filter.size", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "range", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "denoising_models.Filter.highway", "denoising_models.Filter.cls_decoder", "denoising_models.Filter.define_cls_loss", "denoising_models.Filter.sorted_rnn", "denoising_models.Filter.squeeze", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "denoising_models.Filter.size"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.denoising_models.LabelerBase.sent_encoder", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.denoising_models.LabelerBase.define_cls_loss", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.models.ModelBase.sorted_rnn"], ["", "def", "forward", "(", "self", ",", "feed_dict", ",", "data_type", ")", ":", "\n", "    ", "sent_vec", ",", "sent_attn_score", "=", "self", ".", "sent_encoder", "(", "feed_dict", ",", "data_type", ")", "\n", "type_idx", "=", "feed_dict", "[", "'y_noisy_idx'", "]", "\n", "type_embed", "=", "self", ".", "type_embedding", "(", "type_idx", ")", "\n", "type_vec", "=", "torch", ".", "sum", "(", "type_embed", ",", "1", ")", "\n", "def_idx", "=", "feed_dict", "[", "'type_definition_idx'", "]", "\n", "def_embed", "=", "self", ".", "def_embedding", "(", "def_idx", ")", "\n", "batch_size", ",", "max_n_types", ",", "max_def_len", ",", "def_dim", "=", "def_embed", ".", "size", "(", ")", "\n", "def_vec", "=", "torch", ".", "zeros", "(", "batch_size", ",", "max_n_types", ",", "def_dim", ")", ".", "cuda", "(", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "      ", "n_types", "=", "feed_dict", "[", "\"y_noisy_lengths\"", "]", "[", "i", "]", "\n", "hid", "=", "self", ".", "sorted_rnn", "(", "def_embed", "[", "i", ",", ":", "n_types", "]", ",", "feed_dict", "[", "'type_definition_length'", "]", "[", "i", ",", ":", "n_types", "]", ",", "self", ".", "lstm_def", ")", "\n", "def_vec", "[", "i", ",", ":", "n_types", ",", ":", "]", "=", "hid", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "", "def_vec", "=", "torch", ".", "sum", "(", "def_vec", ",", "1", ")", "\n", "output", "=", "torch", ".", "cat", "(", "(", "sent_vec", ",", "type_vec", ",", "def_vec", ")", ",", "1", ")", "\n", "cls_input", "=", "self", ".", "highway", "(", "output", ")", "\n", "cls_logits", "=", "self", ".", "cls_decoder", "(", "cls_input", ")", "\n", "cls_loss", "=", "self", ".", "define_cls_loss", "(", "cls_logits", ".", "squeeze", "(", "1", ")", ",", "feed_dict", "[", "'y_cls'", "]", ")", "\n", "dummy_logits", "=", "-", "1.", "*", "torch", ".", "ones", "(", "(", "cls_logits", ".", "size", "(", ")", "[", "0", "]", ",", "self", ".", "answer_num", ")", ")", "\n", "#dummy_logits[:, :3] = 1.", "\n", "return", "cls_loss", ",", "dummy_logits", ",", "cls_logits", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.eval_metric.f1": [[11, 15], ["float"], "function", ["None"], ["def", "f1", "(", "p", ",", "r", ")", ":", "\n", "  ", "if", "r", "==", "0.", ":", "\n", "    ", "return", "0.", "\n", "", "return", "2", "*", "p", "*", "r", "/", "float", "(", "p", "+", "r", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.eval_metric.strict": [[16, 23], ["len", "eval_metric.f1", "set", "set"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.eval_metric.f1"], ["", "def", "strict", "(", "true_and_prediction", ")", ":", "\n", "  ", "num_entities", "=", "len", "(", "true_and_prediction", ")", "\n", "correct_num", "=", "0.", "\n", "for", "true_labels", ",", "predicted_labels", "in", "true_and_prediction", ":", "\n", "    ", "correct_num", "+=", "set", "(", "true_labels", ")", "==", "set", "(", "predicted_labels", ")", "\n", "", "precision", "=", "recall", "=", "correct_num", "/", "num_entities", "\n", "return", "precision", ",", "recall", ",", "f1", "(", "precision", ",", "recall", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.eval_metric.macro": [[24, 47], ["len", "len", "eval_metric.f1", "len", "len", "float", "len", "float", "set().intersection", "len", "set().intersection", "len", "set", "set", "set", "set"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.eval_metric.f1"], ["", "def", "macro", "(", "true_and_prediction", ")", ":", "\n", "  ", "num_examples", "=", "len", "(", "true_and_prediction", ")", "\n", "p", "=", "0.", "\n", "r", "=", "0.", "\n", "pred_example_count", "=", "0.", "\n", "pred_label_count", "=", "0.", "\n", "gold_label_count", "=", "0.", "\n", "for", "true_labels", ",", "predicted_labels", "in", "true_and_prediction", ":", "\n", "    ", "if", "predicted_labels", ":", "\n", "      ", "pred_example_count", "+=", "1", "\n", "pred_label_count", "+=", "len", "(", "predicted_labels", ")", "\n", "per_p", "=", "len", "(", "set", "(", "predicted_labels", ")", ".", "intersection", "(", "set", "(", "true_labels", ")", ")", ")", "/", "float", "(", "len", "(", "predicted_labels", ")", ")", "\n", "p", "+=", "per_p", "\n", "", "if", "len", "(", "true_labels", ")", ":", "\n", "      ", "gold_label_count", "+=", "1", "\n", "per_r", "=", "len", "(", "set", "(", "predicted_labels", ")", ".", "intersection", "(", "set", "(", "true_labels", ")", ")", ")", "/", "float", "(", "len", "(", "true_labels", ")", ")", "\n", "r", "+=", "per_r", "\n", "", "", "if", "pred_example_count", ">", "0", ":", "\n", "    ", "precision", "=", "p", "/", "pred_example_count", "\n", "", "if", "gold_label_count", ">", "0", ":", "\n", "    ", "recall", "=", "r", "/", "gold_label_count", "\n", "", "avg_elem_per_pred", "=", "pred_label_count", "/", "pred_example_count", "\n", "return", "num_examples", ",", "pred_example_count", ",", "avg_elem_per_pred", ",", "precision", ",", "recall", ",", "f1", "(", "precision", ",", "recall", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.eval_metric.micro": [[48, 66], ["len", "len", "len", "len", "eval_metric.f1", "set().intersection", "set", "set"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.eval_metric.f1"], ["", "def", "micro", "(", "true_and_prediction", ")", ":", "\n", "  ", "num_examples", "=", "len", "(", "true_and_prediction", ")", "\n", "num_predicted_labels", "=", "0.", "\n", "num_true_labels", "=", "0.", "\n", "num_correct_labels", "=", "0.", "\n", "pred_example_count", "=", "0.", "\n", "for", "true_labels", ",", "predicted_labels", "in", "true_and_prediction", ":", "\n", "    ", "if", "predicted_labels", ":", "\n", "      ", "pred_example_count", "+=", "1", "\n", "", "num_predicted_labels", "+=", "len", "(", "predicted_labels", ")", "\n", "num_true_labels", "+=", "len", "(", "true_labels", ")", "\n", "num_correct_labels", "+=", "len", "(", "set", "(", "predicted_labels", ")", ".", "intersection", "(", "set", "(", "true_labels", ")", ")", ")", "\n", "", "if", "pred_example_count", "==", "0", ":", "\n", "    ", "return", "num_examples", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "\n", "", "precision", "=", "num_correct_labels", "/", "num_predicted_labels", "\n", "recall", "=", "num_correct_labels", "/", "num_true_labels", "\n", "avg_elem_per_pred", "=", "num_predicted_labels", "/", "pred_example_count", "\n", "return", "num_examples", ",", "pred_example_count", ",", "avg_elem_per_pred", ",", "precision", ",", "recall", ",", "f1", "(", "precision", ",", "recall", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.eval_metric.mrr": [[67, 87], ["numpy.array", "numpy.argsort", "enumerate", "mrr_per_example.append", "len", "range", "numpy.mean", "sum", "range", "len", "len", "rr_per_array.append"], "function", ["None"], ["", "def", "mrr", "(", "dist_list", ",", "gold", ")", ":", "\n", "  ", "\"\"\"\n  dist_list: list of list of label probability for all labels.\n  gold: list of gold indexes.\n\n  Get mean reciprocal rank. (this is slow, as have to sort for 10K vocab)\n  \"\"\"", "\n", "mrr_per_example", "=", "[", "]", "\n", "dist_arrays", "=", "np", ".", "array", "(", "dist_list", ")", "\n", "dist_sorted", "=", "np", ".", "argsort", "(", "-", "dist_arrays", ",", "axis", "=", "1", ")", "\n", "for", "ind", ",", "gold_i", "in", "enumerate", "(", "gold", ")", ":", "\n", "    ", "gold_i_where", "=", "[", "i", "for", "i", "in", "range", "(", "len", "(", "gold_i", ")", ")", "if", "gold_i", "[", "i", "]", "==", "1", "]", "\n", "rr_per_array", "=", "[", "]", "\n", "sorted_index", "=", "dist_sorted", "[", "ind", ",", ":", "]", "\n", "for", "gold_i_where_i", "in", "gold_i_where", ":", "\n", "      ", "for", "k", "in", "range", "(", "len", "(", "sorted_index", ")", ")", ":", "\n", "        ", "if", "sorted_index", "[", "k", "]", "==", "gold_i_where_i", ":", "\n", "          ", "rr_per_array", ".", "append", "(", "1.0", "/", "(", "k", "+", "1", ")", ")", "\n", "", "", "", "mrr_per_example", ".", "append", "(", "np", ".", "mean", "(", "rr_per_array", ")", ")", "\n", "", "return", "sum", "(", "mrr_per_example", ")", "*", "1.0", "/", "len", "(", "mrr_per_example", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.scorer.stratify": [[13, 22], ["None"], "function", ["None"], ["def", "stratify", "(", "all_labels", ",", "types", ")", ":", "\n", "  ", "\"\"\"\n  Divide label into three categories.\n  \"\"\"", "\n", "coarse", "=", "types", "[", ":", "9", "]", "\n", "fine", "=", "types", "[", "9", ":", "130", "]", "\n", "return", "(", "[", "l", "for", "l", "in", "all_labels", "if", "l", "in", "coarse", "]", ",", "\n", "[", "l", "for", "l", "in", "all_labels", "if", "(", "(", "l", "in", "fine", ")", "and", "(", "not", "l", "in", "coarse", ")", ")", "]", ",", "\n", "[", "l", "for", "l", "in", "all_labels", "if", "(", "not", "l", "in", "coarse", ")", "and", "(", "not", "l", "in", "fine", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.scorer.get_mrr": [[23, 27], ["pickle.load", "eval_metric.mrr", "open"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.eval_metric.mrr"], ["", "def", "get_mrr", "(", "pred_fname", ")", ":", "\n", "  ", "dicts", "=", "pickle", ".", "load", "(", "open", "(", "pred_fname", ",", "\"rb\"", ")", ")", "\n", "mrr_value", "=", "mrr", "(", "dicts", "[", "'pred_dist'", "]", ",", "dicts", "[", "'gold_id_array'", "]", ")", "\n", "return", "mrr_value", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.scorer.compute_prf1": [[28, 38], ["json.load.items", "eval_metric.macro", "print", "open", "json.load", "true_and_predictions.append"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.eval_metric.macro"], ["", "def", "compute_prf1", "(", "fname", ")", ":", "\n", "  ", "with", "open", "(", "fname", ")", "as", "f", ":", "\n", "    ", "total", "=", "json", ".", "load", "(", "f", ")", "\n", "", "true_and_predictions", "=", "[", "]", "\n", "for", "k", ",", "v", "in", "total", ".", "items", "(", ")", ":", "\n", "    ", "true_and_predictions", ".", "append", "(", "(", "v", "[", "'gold'", "]", ",", "v", "[", "'pred'", "]", ")", ")", "\n", "", "count", ",", "pred_count", ",", "avg_pred_count", ",", "p", ",", "r", ",", "f1", "=", "macro", "(", "true_and_predictions", ")", "\n", "perf_total", "=", "\"{0}\\t{1:.2f}\\tP:{2:.1f}\\tR:{3:.1f}\\tF1:{4:.1f}\"", ".", "format", "(", "count", ",", "avg_pred_count", ",", "p", "*", "100", ",", "\n", "r", "*", "100", ",", "f1", "*", "100", ")", "\n", "print", "(", "perf_total", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.scorer.compute_granul_prf1": [[39, 59], ["json.load.items", "open", "json.load", "open", "scorer.stratify", "scorer.stratify", "coarse_true_and_predictions.append", "fine_true_and_predictions.append", "finer_true_and_predictions.append", "eval_metric.macro", "print", "x.strip", "f.readlines"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.scorer.stratify", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.scorer.stratify", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.eval_metric.macro"], ["", "def", "compute_granul_prf1", "(", "fname", ",", "type_fname", ")", ":", "\n", "  ", "with", "open", "(", "fname", ")", "as", "f", ":", "\n", "    ", "total", "=", "json", ".", "load", "(", "f", ")", "\n", "", "coarse_true_and_predictions", "=", "[", "]", "\n", "fine_true_and_predictions", "=", "[", "]", "\n", "finer_true_and_predictions", "=", "[", "]", "\n", "with", "open", "(", "type_fname", ")", "as", "f", ":", "\n", "    ", "types", "=", "[", "x", ".", "strip", "(", ")", "for", "x", "in", "f", ".", "readlines", "(", ")", "]", "\n", "", "for", "k", ",", "v", "in", "total", ".", "items", "(", ")", ":", "\n", "    ", "coarse_gold", ",", "fine_gold", ",", "finer_gold", "=", "stratify", "(", "v", "[", "'gold'", "]", ",", "types", ")", "\n", "coarse_pred", ",", "fine_pred", ",", "finer_pred", "=", "stratify", "(", "v", "[", "'pred'", "]", ",", "types", ")", "\n", "coarse_true_and_predictions", ".", "append", "(", "(", "coarse_gold", ",", "coarse_pred", ")", ")", "\n", "fine_true_and_predictions", ".", "append", "(", "(", "fine_gold", ",", "fine_pred", ")", ")", "\n", "finer_true_and_predictions", ".", "append", "(", "(", "finer_gold", ",", "finer_pred", ")", ")", "\n", "\n", "", "for", "true_and_predictions", "in", "[", "coarse_true_and_predictions", ",", "fine_true_and_predictions", ",", "finer_true_and_predictions", "]", ":", "\n", "    ", "count", ",", "pred_count", ",", "avg_pred_count", ",", "p", ",", "r", ",", "f1", "=", "macro", "(", "true_and_predictions", ")", "\n", "perf", "=", "\"{0}\\t{1:.2f}\\tP:{2:.1f}\\tR:{3:.1f}\\tF1:{4:.1f}\"", ".", "format", "(", "count", ",", "avg_pred_count", ",", "p", "*", "100", ",", "\n", "r", "*", "100", ",", "f1", "*", "100", ")", "\n", "print", "(", "perf", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.scorer.load_augmented_input": [[60, 68], ["open", "json.loads", "json.loads.pop", "line.strip"], "function", ["None"], ["", "", "def", "load_augmented_input", "(", "fname", ")", ":", "\n", "  ", "output_dict", "=", "{", "}", "\n", "with", "open", "(", "fname", ")", "as", "f", ":", "\n", "    ", "for", "line", "in", "f", ":", "\n", "      ", "elem", "=", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ")", "\n", "mention_id", "=", "elem", ".", "pop", "(", "\"annot_id\"", ")", "\n", "output_dict", "[", "mention_id", "]", "=", "elem", "\n", "", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.scorer.visualize": [[69, 83], ["scorer.load_augmented_input", "json.load.items", "open", "json.load", "open", "print", "x.strip", "f.readlines"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.scorer.load_augmented_input"], ["", "def", "visualize", "(", "gold_pred_fname", ",", "original_fname", ",", "type_fname", ")", ":", "\n", "  ", "with", "open", "(", "gold_pred_fname", ")", "as", "f", ":", "\n", "    ", "total", "=", "json", ".", "load", "(", "f", ")", "\n", "", "original", "=", "load_augmented_input", "(", "original_fname", ")", "\n", "with", "open", "(", "type_fname", ")", "as", "f", ":", "\n", "    ", "types", "=", "[", "x", ".", "strip", "(", ")", "for", "x", "in", "f", ".", "readlines", "(", ")", "]", "\n", "", "for", "annot_id", ",", "v", "in", "total", ".", "items", "(", ")", ":", "\n", "    ", "elem", "=", "original", "[", "annot_id", "]", "\n", "mention", "=", "elem", "[", "'mention_span'", "]", "\n", "left", "=", "elem", "[", "'left_context_token'", "]", "\n", "right", "=", "elem", "[", "'right_context_token'", "]", "\n", "text_str", "=", "' '", ".", "join", "(", "left", ")", "+", "\" __\"", "+", "mention", "+", "\"__ \"", "+", "' '", ".", "join", "(", "right", ")", "\n", "gold", "=", "v", "[", "'gold'", "]", "\n", "print", "(", "'  |  '", ".", "join", "(", "[", "text_str", ",", "', '", ".", "join", "(", "[", "(", "\"__\"", "+", "v", "+", "\"__\"", "if", "v", "in", "gold", "else", "v", ")", "for", "v", "in", "v", "[", "'pred'", "]", "]", ")", ",", "','", ".", "join", "(", "gold", ")", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.scorer.compute_length_prf1": [[85, 102], ["scorer.load_augmented_input", "json.load.items", "open", "json.load", "len", "counts[].append", "sorted", "eval_metric.macro", "print", "ex[].strip().split", "counts.items", "ex[].strip"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.scorer.load_augmented_input", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.eval_metric.macro"], ["", "", "def", "compute_length_prf1", "(", "fname", ",", "data_fname", ")", ":", "\n", "  ", "with", "open", "(", "fname", ")", "as", "f", ":", "\n", "    ", "total", "=", "json", ".", "load", "(", "f", ")", "\n", "", "data", "=", "original", "=", "load_augmented_input", "(", "data_fname", ")", "\n", "counts", "=", "{", "}", "\n", "for", "annot_id", ",", "v", "in", "total", ".", "items", "(", ")", ":", "\n", "      ", "ex", "=", "data", "[", "annot_id", "]", "\n", "mention_len", "=", "len", "(", "ex", "[", "'mention_span'", "]", ".", "strip", "(", ")", ".", "split", "(", ")", ")", "\n", "if", "mention_len", "not", "in", "counts", ":", "\n", "        ", "counts", "[", "mention_len", "]", "=", "[", "]", "\n", "", "counts", "[", "mention_len", "]", ".", "append", "(", "(", "v", "[", "'gold'", "]", ",", "v", "[", "'pred'", "]", ")", ")", "\n", "\n", "", "for", "k", ",", "v", "in", "sorted", "(", "counts", ".", "items", "(", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "[", ":", "20", "]", ":", "\n", "    ", "count", ",", "pred_count", ",", "avg_pred_count", ",", "p", ",", "r", ",", "f1", "=", "macro", "(", "v", ")", "\n", "perf", "=", "\"{0}\\t{1:.2f}\\tP:{2:.1f}\\tR:{3:.1f}\\tF1:{4:.1f}\\tLEN:{5}\"", ".", "format", "(", "count", ",", "avg_pred_count", ",", "p", "*", "100", ",", "\n", "r", "*", "100", ",", "f1", "*", "100", ",", "k", ")", "\n", "print", "(", "perf", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.scorer.load_json": [[104, 108], ["open", "f.readlines", "json.loads", "line.strip"], "function", ["None"], ["", "", "def", "load_json", "(", "path", ")", ":", "\n", "  ", "with", "open", "(", "path", ",", "'r'", ")", "as", "f", ":", "\n", "    ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "", "return", "[", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ")", "for", "line", "in", "lines", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.scorer.compute_acc_by_type_freq": [[110, 157], ["json.load.items", "sorted", "open", "json.load", "open", "pickle.load", "open", "set().intersection", "set", "set", "eval_metric.f1", "print", "t.strip", "pickle.load.items", "set", "int", "f.readlines", "set", "TP_counts.items", "int", "x.split"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.eval_metric.f1"], ["", "def", "compute_acc_by_type_freq", "(", "fname", ",", "type_bucket_count_file", ",", "types_file", ")", ":", "\n", "  ", "with", "open", "(", "fname", ")", "as", "f", ":", "\n", "    ", "total", "=", "json", ".", "load", "(", "f", ")", "\n", "", "with", "open", "(", "type_bucket_count_file", ",", "'rb'", ")", "as", "f", ":", "\n", "    ", "type_bucket_count", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "with", "open", "(", "types_file", ",", "'r'", ")", "as", "f", ":", "\n", "    ", "types", "=", "[", "t", ".", "strip", "(", ")", "for", "t", "in", "f", ".", "readlines", "(", ")", "]", "\n", "#print('TOTAL:', sum([len(v['gold']) for k, v in total.items()]))", "\n", "", "type2bucket", "=", "{", "t", "[", "0", "]", ":", "k", "for", "k", ",", "v", "in", "type_bucket_count", ".", "items", "(", ")", "for", "t", "in", "v", "}", "\n", "TP_FP_counts", "=", "{", "'unseen'", ":", "0.", "}", "\n", "TP_FN_counts", "=", "{", "'unseen'", ":", "0.", "}", "\n", "TP_counts", "=", "{", "'unseen'", ":", "0.", "}", "\n", "for", "annot_id", ",", "v", "in", "total", ".", "items", "(", ")", ":", "\n", "    ", "gold", "=", "v", "[", "'gold'", "]", "\n", "pred", "=", "v", "[", "'pred'", "]", "\n", "for", "t", "in", "set", "(", "pred", ")", ".", "intersection", "(", "set", "(", "gold", ")", ")", ":", "\n", "      ", "if", "t", "in", "type2bucket", ":", "\n", "        ", "bucket", "=", "type2bucket", "[", "t", "]", "\n", "if", "bucket", "not", "in", "TP_counts", ":", "\n", "          ", "TP_counts", "[", "bucket", "]", "=", "0.", "\n", "", "TP_counts", "[", "bucket", "]", "+=", "1.", "\n", "", "else", ":", "\n", "        ", "TP_counts", "[", "'unseen'", "]", "+=", "1.", "\n", "", "", "for", "t", "in", "set", "(", "pred", ")", ":", "\n", "      ", "if", "t", "in", "type2bucket", ":", "\n", "        ", "bucket", "=", "type2bucket", "[", "t", "]", "\n", "if", "bucket", "not", "in", "TP_FP_counts", ":", "\n", "          ", "TP_FP_counts", "[", "bucket", "]", "=", "0.", "\n", "", "TP_FP_counts", "[", "bucket", "]", "+=", "1.", "\n", "", "else", ":", "\n", "        ", "TP_FP_counts", "[", "'unseen'", "]", "+=", "1.", "\n", "", "", "for", "t", "in", "set", "(", "gold", ")", ":", "\n", "      ", "if", "t", "in", "type2bucket", ":", "\n", "        ", "bucket", "=", "type2bucket", "[", "t", "]", "\n", "if", "bucket", "not", "in", "TP_FN_counts", ":", "\n", "          ", "TP_FN_counts", "[", "bucket", "]", "=", "0.", "\n", "", "TP_FN_counts", "[", "bucket", "]", "+=", "1.", "\n", "", "else", ":", "\n", "        ", "TP_FN_counts", "[", "'unseen'", "]", "+=", "1.", "\n", "\n", "", "", "", "ordered_keys", "=", "sorted", "(", "[", "k", "for", "k", ",", "v", "in", "TP_counts", ".", "items", "(", ")", "if", "k", "!=", "'unseen'", "]", ",", "key", "=", "lambda", "x", ":", "int", "(", "x", ".", "split", "(", "'-'", ")", "[", "0", "]", ")", ",", "reverse", "=", "True", ")", "# + ['unseen']", "\n", "for", "k", "in", "ordered_keys", ":", "\n", "    ", "precision", "=", "TP_counts", "[", "k", "]", "/", "TP_FP_counts", "[", "k", "]", "\n", "recall", "=", "TP_counts", "[", "k", "]", "/", "TP_FN_counts", "[", "k", "]", "\n", "f1_score", "=", "f1", "(", "precision", ",", "recall", ")", "\n", "perf", "=", "\"{0}\\tCORRECT:{1}\\tP:{2:.2f}\\tR:{3:.2f}\\tF1:{4:.2}\"", ".", "format", "(", "k", ",", "int", "(", "TP_counts", "[", "k", "]", ")", ",", "precision", "*", "100.", ",", "recall", "*", "100.", ",", "f1_score", "*", "100.", ")", "\n", "print", "(", "perf", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.scorer.compute_prf1_single_type": [[158, 185], ["print", "json.load.items", "len", "sum", "sum", "sum", "eval_metric.f1", "print", "open", "json.load", "int", "int", "int", "sum", "float", "gold_binary.append", "gold_binary.append", "pred_binary.append", "scorer.print_example", "pred_binary.append", "len", "zip", "int", "int", "zip", "int", "int"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.eval_metric.f1", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.scorer.print_example"], ["", "", "def", "compute_prf1_single_type", "(", "fname", ",", "type_", ",", "data", "=", "None", ")", ":", "\n", "  ", "print", "(", "'---------- '", "+", "type_", "+", "' ----------'", ")", "\n", "with", "open", "(", "fname", ")", "as", "f", ":", "\n", "    ", "total", "=", "json", ".", "load", "(", "f", ")", "\n", "", "gold_binary", "=", "[", "]", "\n", "pred_binary", "=", "[", "]", "\n", "for", "k", ",", "v", "in", "total", ".", "items", "(", ")", ":", "\n", "    ", "if", "type_", "in", "v", "[", "'gold'", "]", ":", "\n", "      ", "gold_binary", ".", "append", "(", "1.", ")", "\n", "", "else", ":", "\n", "      ", "gold_binary", ".", "append", "(", "0.", ")", "\n", "", "if", "type_", "in", "v", "[", "'pred'", "]", ":", "\n", "      ", "pred_binary", ".", "append", "(", "1.", ")", "\n", "print_example", "(", "data", "[", "k", "]", ")", "\n", "", "else", ":", "\n", "      ", "pred_binary", ".", "append", "(", "0.", ")", "\n", "", "", "count", "=", "len", "(", "gold_binary", ")", "\n", "TP_FN_counts", "=", "sum", "(", "[", "1.", "for", "gold", "in", "gold_binary", "if", "int", "(", "gold", ")", "==", "1", "]", ")", "\n", "TP_FP_counts", "=", "sum", "(", "[", "1.", "for", "pred", "in", "pred_binary", "if", "int", "(", "pred", ")", "==", "1", "]", ")", "\n", "TP_counts", "=", "sum", "(", "[", "1.", "for", "pred", ",", "gold", "in", "zip", "(", "pred_binary", ",", "gold_binary", ")", "if", "int", "(", "pred", ")", "==", "1", "and", "int", "(", "gold", ")", "==", "1", "]", ")", "\n", "p", "=", "TP_counts", "/", "TP_FP_counts", "if", "TP_FP_counts", ">", "0", "else", "0.", "\n", "r", "=", "TP_counts", "/", "TP_FN_counts", "if", "TP_FN_counts", ">", "0", "else", "0.", "\n", "f1_", "=", "f1", "(", "p", ",", "r", ")", "\n", "output_str", "=", "\"Type: {0}\\t#: {1} TP:{2} TP+FP:{3} TP+FN:{4} P:{5:.3f} R:{6:.3f} F1:{7:.3f}\"", ".", "format", "(", "type_", ",", "count", ",", "int", "(", "TP_counts", ")", ",", "int", "(", "TP_FP_counts", ")", ",", "int", "(", "TP_FN_counts", ")", ",", "p", ",", "r", ",", "f1_", ")", "\n", "accuracy", "=", "sum", "(", "[", "pred", "==", "gold", "for", "pred", ",", "gold", "in", "zip", "(", "pred_binary", ",", "gold_binary", ")", "]", ")", "/", "float", "(", "len", "(", "gold_binary", ")", ")", "\n", "output_str", "+=", "'\\t Dev accuracy: {0:.1f}%'", ".", "format", "(", "accuracy", "*", "100", ")", "\n", "print", "(", "output_str", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.None.scorer.print_example": [[186, 193], ["print", "print", "print", "print"], "function", ["None"], ["", "def", "print_example", "(", "ex", ")", ":", "\n", "  ", "gold", "=", "ex", "[", "'y_str'", "]", "\n", "sent", "=", "' '", ".", "join", "(", "ex", "[", "'left_context_token'", "]", ")", "+", "' ['", "+", "ex", "[", "'mention_span'", "]", "+", "'] '", "+", "' '", ".", "join", "(", "ex", "[", "'right_context_token'", "]", ")", "\n", "print", "(", "'ID  : '", "+", "ex", "[", "'annot_id'", "]", ")", "\n", "print", "(", "'SENT: '", "+", "sent", ")", "\n", "print", "(", "'GOLD: '", "+", "',  '", ".", "join", "(", "gold", ")", ")", "\n", "print", "(", ")", "\n", "#######", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.resources.constant.load_vocab_dict": [[4, 14], ["open", "x.strip", "dict", "dict", "f.readlines", "zip", "zip", "range", "range", "len", "len"], "function", ["None"], ["def", "load_vocab_dict", "(", "vocab_file_name", ",", "vocab_max_size", "=", "None", ",", "start_vocab_count", "=", "None", ")", ":", "\n", "  ", "with", "open", "(", "vocab_file_name", ")", "as", "f", ":", "\n", "    ", "text", "=", "[", "x", ".", "strip", "(", ")", "for", "x", "in", "f", ".", "readlines", "(", ")", "]", "\n", "if", "vocab_max_size", ":", "\n", "      ", "text", "=", "text", "[", ":", "vocab_max_size", "]", "\n", "", "if", "start_vocab_count", ":", "\n", "      ", "file_content", "=", "dict", "(", "zip", "(", "text", ",", "range", "(", "0", "+", "start_vocab_count", ",", "len", "(", "text", ")", "+", "start_vocab_count", ")", ")", ")", "\n", "", "else", ":", "\n", "      ", "file_content", "=", "dict", "(", "zip", "(", "text", ",", "range", "(", "0", ",", "len", "(", "text", ")", ")", ")", ")", "\n", "", "", "return", "file_content", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.resources.constant.load_definition_dict": [[16, 21], ["open", "v.strip().split", "y.strip", "f.readlines", "x.strip().split", "v.strip", "x.strip"], "function", ["None"], ["", "def", "load_definition_dict", "(", "path", ")", ":", "\n", "  ", "with", "open", "(", "path", ",", "'r'", ")", "as", "f", ":", "\n", "    ", "definition", "=", "[", "[", "y", ".", "strip", "(", ")", "for", "y", "in", "x", ".", "strip", "(", ")", ".", "split", "(", "'<sep>'", ")", "]", "for", "x", "in", "f", ".", "readlines", "(", ")", "]", "\n", "definition", "=", "{", "k", ":", "v", ".", "strip", "(", ")", ".", "split", "(", ")", "for", "k", ",", "v", "in", "definition", "}", "\n", "", "return", "definition", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.resources.constant.get_definition_vocab": [[23, 36], ["def_dict.items", "sorted", "counts.items"], "function", ["None"], ["", "def", "get_definition_vocab", "(", "def_dict", ")", ":", "\n", "  ", "counts", "=", "{", "}", "\n", "for", "_", ",", "v", "in", "def_dict", ".", "items", "(", ")", ":", "\n", "    ", "for", "word", "in", "v", ":", "\n", "      ", "if", "word", "not", "in", "counts", ":", "\n", "        ", "counts", "[", "word", "]", "=", "0", "\n", "", "counts", "[", "word", "]", "+=", "1", "\n", "", "", "vocab", "=", "{", "'<unk>'", ":", "0", ",", "'<pad>'", ":", "1", "}", "\n", "idx", "=", "2", "\n", "for", "k", ",", "_", "in", "sorted", "(", "counts", ".", "items", "(", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", ":", "\n", "    ", "vocab", "[", "k", "]", "=", "idx", "\n", "idx", "+=", "1", "\n", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.data_tools.add_tree.load_json": [[15, 19], ["open", "f.readlines", "json.loads", "line.strip"], "function", ["None"], ["def", "load_json", "(", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "", "return", "[", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ")", "for", "line", "in", "lines", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.data_tools.add_tree.save_json": [[20, 25], ["open", "json.dump", "f.write"], "function", ["None"], ["", "def", "save_json", "(", "path", ",", "data", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "d", "in", "data", ":", "\n", "            ", "json", ".", "dump", "(", "d", ",", "f", ")", "\n", "f", ".", "write", "(", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.data_tools.add_tree.get_tree": [[26, 41], ["nlp", "tree.append"], "function", ["None"], ["", "", "", "def", "get_tree", "(", "sent", ")", ":", "\n", "    ", "doc", "=", "nlp", "(", "sent", ")", "\n", "tree", "=", "[", "]", "\n", "for", "token", "in", "doc", ":", "\n", "        ", "d", "=", "{", "}", "\n", "d", "[", "'text'", "]", "=", "token", ".", "text", "\n", "d", "[", "'lemma'", "]", "=", "token", ".", "lemma_", "\n", "d", "[", "'pos'", "]", "=", "token", ".", "pos_", "\n", "d", "[", "'tag'", "]", "=", "token", ".", "tag_", "\n", "d", "[", "'dep'", "]", "=", "token", ".", "dep_", "\n", "d", "[", "'shape'", "]", "=", "token", ".", "shape_", "\n", "d", "[", "'is_alpha'", "]", "=", "token", ".", "is_alpha", "\n", "d", "[", "'is_stop'", "]", "=", "token", ".", "is_stop", "\n", "tree", ".", "append", "(", "d", ")", "\n", "", "return", "tree", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.data_tools.add_tree.add_tree": [[42, 46], ["add_tree.get_tree", "x[].strip"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.data_tools.add_tree.get_tree"], ["", "def", "add_tree", "(", "x", ")", ":", "\n", "#print('processing: ' + x['annot_id'])", "\n", "    ", "x", "[", "'mention_span_tree'", "]", "=", "get_tree", "(", "x", "[", "'mention_span'", "]", ".", "strip", "(", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BertConfig.__init__": [[40, 87], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "vocab_size", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "16", ",", "\n", "initializer_range", "=", "0.02", ")", ":", "\n", "        ", "\"\"\"Constructs BertConfig.\n\n        Args:\n            vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        \"\"\"", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "self", ".", "intermediate_size", "=", "intermediate_size", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "type_vocab_size", "=", "type_vocab_size", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BertConfig.from_dict": [[88, 95], ["modeling.BertConfig", "six.iteritems"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "BertConfig", "(", "vocab_size", "=", "None", ")", "\n", "for", "(", "key", ",", "value", ")", "in", "six", ".", "iteritems", "(", "json_object", ")", ":", "\n", "            ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BertConfig.from_json_file": [[96, 102], ["cls.from_dict", "open", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BertConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"", "\n", "with", "open", "(", "json_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "            ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BertConfig.to_dict": [[103, 107], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BertConfig.to_json_string": [[108, 111], ["json.dumps", "modeling.BertConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BertConfig.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BERTLayerNorm.__init__": [[114, 121], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "variance_epsilon", "=", "1e-12", ")", ":", "\n", "        ", "\"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n        \"\"\"", "\n", "super", "(", "BERTLayerNorm", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "gamma", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "config", ".", "hidden_size", ")", ")", "\n", "self", ".", "beta", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "config", ".", "hidden_size", ")", ")", "\n", "self", ".", "variance_epsilon", "=", "variance_epsilon", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BERTLayerNorm.forward": [[122, 127], ["x.mean", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "u", "=", "x", ".", "mean", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "s", "=", "(", "x", "-", "u", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "x", "=", "(", "x", "-", "u", ")", "/", "torch", ".", "sqrt", "(", "s", "+", "self", ".", "variance_epsilon", ")", "\n", "return", "self", ".", "gamma", "*", "x", "+", "self", ".", "beta", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BERTEmbeddings.__init__": [[129, 141], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "modeling.BERTLayerNorm", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTEmbeddings", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\"\"\"Construct the embedding module from word, position and token_type embeddings.\n        \"\"\"", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "BERTLayerNorm", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BERTEmbeddings.forward": [[142, 157], ["input_ids.size", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "modeling.BERTEmbeddings.word_embeddings", "modeling.BERTEmbeddings.position_embeddings", "modeling.BERTEmbeddings.token_type_embeddings", "modeling.BERTEmbeddings.LayerNorm", "modeling.BERTEmbeddings.dropout", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ")", ":", "\n", "        ", "seq_length", "=", "input_ids", ".", "size", "(", "1", ")", "\n", "position_ids", "=", "torch", ".", "arange", "(", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "input_ids", ")", "\n", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "words_embeddings", "+", "position_embeddings", "+", "token_type_embeddings", "\n", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BERTSelfAttention.__init__": [[160, 175], ["torch.Module.__init__", "int", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "ValueError"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTSelfAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", ")", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BERTSelfAttention.transpose_for_scores": [[176, 180], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BERTSelfAttention.forward": [[181, 208], ["modeling.BERTSelfAttention.query", "modeling.BERTSelfAttention.key", "modeling.BERTSelfAttention.value", "modeling.BERTSelfAttention.transpose_for_scores", "modeling.BERTSelfAttention.transpose_for_scores", "modeling.BERTSelfAttention.transpose_for_scores", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "modeling.BERTSelfAttention.dropout", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "modeling.BERTSelfAttention.transpose", "math.sqrt", "torch.Softmax", "torch.Softmax", "context_layer.view.view.permute", "context_layer.view.view.size"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BERTSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BERTSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BERTSelfAttention.transpose_for_scores"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ")", ":", "\n", "        ", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "hidden_states", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "hidden_states", ")", "\n", "\n", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_key_layer", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_value_layer", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "return", "context_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BERTSelfOutput.__init__": [[211, 216], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "modeling.BERTLayerNorm", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTSelfOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BERTLayerNorm", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BERTSelfOutput.forward": [[217, 222], ["modeling.BERTSelfOutput.dense", "modeling.BERTSelfOutput.dropout", "modeling.BERTSelfOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BERTAttention.__init__": [[225, 229], ["torch.Module.__init__", "modeling.BERTSelfAttention", "modeling.BERTSelfOutput"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BERTSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BERTSelfOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BERTAttention.forward": [[230, 234], ["modeling.BERTAttention.self", "modeling.BERTAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "attention_mask", ")", ":", "\n", "        ", "self_output", "=", "self", ".", "self", "(", "input_tensor", ",", "attention_mask", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_output", ",", "input_tensor", ")", "\n", "return", "attention_output", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BERTIntermediate.__init__": [[237, 241], ["torch.Module.__init__", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTIntermediate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "self", ".", "intermediate_act_fn", "=", "gelu", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BERTIntermediate.forward": [[242, 246], ["modeling.BERTIntermediate.dense", "modeling.BERTIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BERTOutput.__init__": [[249, 254], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "modeling.BERTLayerNorm", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BERTLayerNorm", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BERTOutput.forward": [[255, 260], ["modeling.BERTOutput.dense", "modeling.BERTOutput.dropout", "modeling.BERTOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BERTLayer.__init__": [[263, 268], ["torch.Module.__init__", "modeling.BERTAttention", "modeling.BERTIntermediate", "modeling.BERTOutput"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention", "=", "BERTAttention", "(", "config", ")", "\n", "self", ".", "intermediate", "=", "BERTIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BERTOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BERTLayer.forward": [[269, 274], ["modeling.BERTLayer.attention", "modeling.BERTLayer.intermediate", "modeling.BERTLayer.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ")", ":", "\n", "        ", "attention_output", "=", "self", ".", "attention", "(", "hidden_states", ",", "attention_mask", ")", "\n", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BERTEncoder.__init__": [[277, 281], ["torch.Module.__init__", "modeling.BERTLayer", "torch.ModuleList", "torch.ModuleList", "copy.deepcopy", "range"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "layer", "=", "BERTLayer", "(", "config", ")", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "layer", ")", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BERTEncoder.forward": [[282, 288], ["layer_module", "all_encoder_layers.append"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ")", ":", "\n", "        ", "all_encoder_layers", "=", "[", "]", "\n", "for", "layer_module", "in", "self", ".", "layer", ":", "\n", "            ", "hidden_states", "=", "layer_module", "(", "hidden_states", ",", "attention_mask", ")", "\n", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "return", "all_encoder_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BERTPooler.__init__": [[291, 295], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BERTPooler", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BERTPooler.forward": [[296, 303], ["modeling.BERTPooler.dense", "modeling.BERTPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BertModel.__init__": [[322, 332], ["torch.Module.__init__", "modeling.BERTEmbeddings", "modeling.BERTEncoder", "modeling.BERTPooler"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ":", "BertConfig", ")", ":", "\n", "        ", "\"\"\"Constructor for BertModel.\n\n        Args:\n            config: `BertConfig` instance.\n        \"\"\"", "\n", "super", "(", "BertModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embeddings", "=", "BERTEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BERTEncoder", "(", "config", ")", "\n", "self", ".", "pooler", "=", "BERTPooler", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BertModel.forward": [[333, 359], ["torch.ones_like.unsqueeze().unsqueeze", "torch.ones_like.unsqueeze().unsqueeze", "extended_attention_mask.float.float.float", "modeling.BertModel.embeddings", "modeling.BertModel.encoder", "modeling.BertModel.pooler", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ")", ":", "\n", "        ", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones_like", "(", "input_ids", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "# We create a 3D attention mask from a 2D tensor mask.", "\n", "# Sizes are [batch_size, 1, 1, from_seq_length]", "\n", "# So we can broadcast to [batch_size, num_heads, to_seq_length, from_seq_length]", "\n", "# this attention mask is more simple than the triangular masking of causal attention", "\n", "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.", "\n", "", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "extended_attention_mask", "=", "extended_attention_mask", ".", "float", "(", ")", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "input_ids", ",", "token_type_ids", ")", "\n", "all_encoder_layers", "=", "self", ".", "encoder", "(", "embedding_output", ",", "extended_attention_mask", ")", "\n", "sequence_output", "=", "all_encoder_layers", "[", "-", "1", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "\n", "return", "all_encoder_layers", ",", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BertForSequenceClassification.__init__": [[381, 398], ["torch.Module.__init__", "modeling.BertModel", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "modeling.BertForSequenceClassification.apply", "isinstance", "isinstance", "module.weight.data.normal_", "isinstance", "module.bias.data.zero_", "module.beta.data.normal_", "module.gamma.data.normal_"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_labels", ")", ":", "\n", "        ", "super", "(", "BertForSequenceClassification", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "\n", "def", "init_weights", "(", "module", ")", ":", "\n", "            ", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "                ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "BERTLayerNorm", ")", ":", "\n", "                ", "module", ".", "beta", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "config", ".", "initializer_range", ")", "\n", "module", ".", "gamma", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "config", ".", "initializer_range", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "                ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "self", ".", "apply", "(", "init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BertForSequenceClassification.forward": [[399, 410], ["modeling.BertForSequenceClassification.bert", "modeling.BertForSequenceClassification.dropout", "modeling.BertForSequenceClassification.classifier", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss."], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "labels", "=", "None", ")", ":", "\n", "        ", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ",", "labels", ")", "\n", "return", "loss", ",", "logits", "\n", "", "else", ":", "\n", "            ", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BertForQuestionAnswering.__init__": [[430, 448], ["torch.Module.__init__", "modeling.BertModel", "torch.Linear", "torch.Linear", "modeling.BertForQuestionAnswering.apply", "isinstance", "isinstance", "module.weight.data.normal_", "isinstance", "module.bias.data.zero_", "module.beta.data.normal_", "module.gamma.data.normal_"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForQuestionAnswering", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "# TODO check with Google if it's normal there is no dropout on the token classifier of SQuAD in the TF version", "\n", "# self.dropout = nn.Dropout(config.hidden_dropout_prob)", "\n", "self", ".", "qa_outputs", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "\n", "def", "init_weights", "(", "module", ")", ":", "\n", "            ", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "                ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "BERTLayerNorm", ")", ":", "\n", "                ", "module", ".", "beta", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "config", ".", "initializer_range", ")", "\n", "module", ".", "gamma", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "config", ".", "initializer_range", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "                ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "self", ".", "apply", "(", "init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BertForQuestionAnswering.forward": [[449, 473], ["modeling.BertForQuestionAnswering.bert", "modeling.BertForQuestionAnswering.qa_outputs", "modeling.BertForQuestionAnswering.split", "start_logits.squeeze.squeeze.squeeze", "end_logits.squeeze.squeeze.squeeze", "start_positions.squeeze.squeeze.squeeze", "end_positions.squeeze.squeeze.squeeze", "start_logits.squeeze.squeeze.size", "start_positions.squeeze.squeeze.clamp_", "end_positions.squeeze.squeeze.clamp_", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss."], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "start_positions", "=", "None", ",", "end_positions", "=", "None", ")", ":", "\n", "        ", "all_encoder_layers", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", "\n", "sequence_output", "=", "all_encoder_layers", "[", "-", "1", "]", "\n", "logits", "=", "self", ".", "qa_outputs", "(", "sequence_output", ")", "\n", "start_logits", ",", "end_logits", "=", "logits", ".", "split", "(", "1", ",", "dim", "=", "-", "1", ")", "\n", "start_logits", "=", "start_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "end_logits", "=", "end_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "if", "start_positions", "is", "not", "None", "and", "end_positions", "is", "not", "None", ":", "\n", "# If we are on multi-GPU, split add a dimension - if not this is a no-op", "\n", "            ", "start_positions", "=", "start_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "end_positions", "=", "end_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "# sometimes the start/end positions are outside our model inputs, we ignore these terms", "\n", "ignored_index", "=", "start_logits", ".", "size", "(", "1", ")", "\n", "start_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "end_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "ignored_index", ")", "\n", "start_loss", "=", "loss_fct", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "loss_fct", "(", "end_logits", ",", "end_positions", ")", "\n", "total_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2", "\n", "return", "total_loss", ",", "(", "start_logits", ",", "end_logits", ")", "\n", "", "else", ":", "\n", "            ", "return", "start_logits", ",", "end_logits", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.gelu": [[29, 35], ["torch.erf", "torch.erf", "math.sqrt"], "function", ["None"], ["def", "gelu", "(", "x", ")", ":", "\n", "    ", "\"\"\"Implementation of the gelu activation function.\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n    \"\"\"", "\n", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.optimization.BERTAdam.__init__": [[58, 77], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", ",", "warmup", "=", "-", "1", ",", "t_total", "=", "-", "1", ",", "schedule", "=", "'warmup_linear'", ",", "\n", "b1", "=", "0.9", ",", "b2", "=", "0.999", ",", "e", "=", "1e-6", ",", "weight_decay_rate", "=", "0.01", ",", "\n", "max_grad_norm", "=", "1.0", ")", ":", "\n", "        ", "if", "not", "lr", ">=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {} - should be >= 0.0\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "schedule", "not", "in", "SCHEDULES", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid schedule parameter: {}\"", ".", "format", "(", "schedule", ")", ")", "\n", "", "if", "not", "0.0", "<=", "warmup", "<", "1.0", "and", "not", "warmup", "==", "-", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid warmup: {} - should be in [0.0, 1.0[ or -1\"", ".", "format", "(", "warmup", ")", ")", "\n", "", "if", "not", "0.0", "<=", "b1", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "b1", ")", ")", "\n", "", "if", "not", "0.0", "<=", "b2", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "b2", ")", ")", "\n", "", "if", "not", "e", ">=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {} - should be >= 0.0\"", ".", "format", "(", "e", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "schedule", "=", "schedule", ",", "warmup", "=", "warmup", ",", "t_total", "=", "t_total", ",", "\n", "b1", "=", "b1", ",", "b2", "=", "b2", ",", "e", "=", "e", ",", "weight_decay_rate", "=", "weight_decay_rate", ",", "\n", "max_grad_norm", "=", "max_grad_norm", ")", "\n", "super", "(", "BERTAdam", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.optimization.BERTAdam.get_lr": [[78, 92], ["lr.append", "len", "schedule_fct"], "methods", ["None"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "lr", "=", "[", "]", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "return", "[", "0", "]", "\n", "", "if", "group", "[", "'t_total'", "]", "!=", "-", "1", ":", "\n", "                    ", "schedule_fct", "=", "SCHEDULES", "[", "group", "[", "'schedule'", "]", "]", "\n", "lr_scheduled", "=", "group", "[", "'lr'", "]", "*", "schedule_fct", "(", "state", "[", "'step'", "]", "/", "group", "[", "'t_total'", "]", ",", "group", "[", "'warmup'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "lr_scheduled", "=", "group", "[", "'lr'", "]", "\n", "", "lr", ".", "append", "(", "lr_scheduled", ")", "\n", "", "", "return", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.optimization.BERTAdam.to": [[93, 98], ["optimization.BERTAdam.state.values", "state[].to", "state[].to"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.optimization.BERTAdam.to", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.optimization.BERTAdam.to"], ["", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "        ", "\"\"\" Move the optimizer state to a specified device\"\"\"", "\n", "for", "state", "in", "self", ".", "state", ".", "values", "(", ")", ":", "\n", "            ", "state", "[", "'exp_avg'", "]", ".", "to", "(", "device", ")", "\n", "state", "[", "'exp_avg_sq'", "]", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.optimization.BERTAdam.initialize_step": [[99, 113], ["torch.zeros_like", "torch.zeros_like"], "methods", ["None"], ["", "", "def", "initialize_step", "(", "self", ",", "initial_step", ")", ":", "\n", "        ", "\"\"\"Initialize state with a defined step (but we don't have stored averaged).\n        Arguments:\n            initial_step (int): Initial step number.\n        \"\"\"", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "# State initialization", "\n", "state", "[", "'step'", "]", "=", "initial_step", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.optimization.BERTAdam.step": [[114, 182], ["closure", "next_m.mul_().add_", "next_v.mul_().addcmul_", "p.data.add_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "torch.nn.utils.clip_grad_norm_", "next_m.mul_", "next_v.mul_", "next_v.sqrt", "schedule_fct"], "methods", ["None"], ["", "", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Adam does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'next_m'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'next_v'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "next_m", ",", "next_v", "=", "state", "[", "'next_m'", "]", ",", "state", "[", "'next_v'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'b1'", "]", ",", "group", "[", "'b2'", "]", "\n", "\n", "# Add grad clipping", "\n", "if", "group", "[", "'max_grad_norm'", "]", ">", "0", ":", "\n", "                    ", "clip_grad_norm_", "(", "p", ",", "group", "[", "'max_grad_norm'", "]", ")", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "# In-place operations to update the averages at the same time", "\n", "", "next_m", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "next_v", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "update", "=", "next_m", "/", "(", "next_v", ".", "sqrt", "(", ")", "+", "group", "[", "'e'", "]", ")", "\n", "\n", "# Just adding the square of the weights to the loss function is *not*", "\n", "# the correct way of using L2 regularization/weight decay with Adam,", "\n", "# since that will interact with the m and v parameters in strange ways.", "\n", "#", "\n", "# Instead we want ot decay the weights in a manner that doesn't interact", "\n", "# with the m/v parameters. This is equivalent to adding the square", "\n", "# of the weights to the loss with plain (non-momentum) SGD.", "\n", "if", "group", "[", "'weight_decay_rate'", "]", ">", "0.0", ":", "\n", "                    ", "update", "+=", "group", "[", "'weight_decay_rate'", "]", "*", "p", ".", "data", "\n", "\n", "", "if", "group", "[", "'t_total'", "]", "!=", "-", "1", ":", "\n", "                    ", "schedule_fct", "=", "SCHEDULES", "[", "group", "[", "'schedule'", "]", "]", "\n", "lr_scheduled", "=", "group", "[", "'lr'", "]", "*", "schedule_fct", "(", "state", "[", "'step'", "]", "/", "group", "[", "'t_total'", "]", ",", "group", "[", "'warmup'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "lr_scheduled", "=", "group", "[", "'lr'", "]", "\n", "\n", "", "update_with_lr", "=", "lr_scheduled", "*", "update", "\n", "p", ".", "data", ".", "add_", "(", "-", "update_with_lr", ")", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1", "\n", "# bias_correction1 = 1 - beta1 ** state['step']", "\n", "# bias_correction2 = 1 - beta2 ** state['step']", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.optimization.warmup_cosine": [[22, 26], ["torch.cos"], "function", ["None"], ["def", "warmup_cosine", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "0.5", "*", "(", "1.0", "+", "torch", ".", "cos", "(", "math", ".", "pi", "*", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.optimization.warmup_constant": [[27, 31], ["None"], "function", ["None"], ["", "def", "warmup_constant", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.optimization.warmup_linear": [[32, 36], ["None"], "function", ["None"], ["", "def", "warmup_linear", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "1.0", "-", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.InputExample.__init__": [[46, 62], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "guid", ",", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "None", ")", ":", "\n", "        ", "\"\"\"Constructs a InputExample.\n\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        \"\"\"", "\n", "self", ".", "guid", "=", "guid", "\n", "self", ".", "text_a", "=", "text_a", "\n", "self", ".", "text_b", "=", "text_b", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.InputFeatures.__init__": [[67, 72], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_id", ")", ":", "\n", "        ", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "label_id", "=", "label_id", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.DataProcessor.get_train_examples": [[77, 80], ["NotImplementedError"], "methods", ["None"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.DataProcessor.get_dev_examples": [[81, 84], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.DataProcessor.get_labels": [[85, 88], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"Gets the list of labels for this data set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.DataProcessor._read_tsv": [[89, 98], ["open", "csv.reader", "lines.append"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "_read_tsv", "(", "cls", ",", "input_file", ",", "quotechar", "=", "None", ")", ":", "\n", "        ", "\"\"\"Reads a tab separated value file.\"\"\"", "\n", "with", "open", "(", "input_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "reader", "=", "csv", ".", "reader", "(", "f", ",", "delimiter", "=", "\"\\t\"", ",", "quotechar", "=", "quotechar", ")", "\n", "lines", "=", "[", "]", "\n", "for", "line", "in", "reader", ":", "\n", "                ", "lines", ".", "append", "(", "line", ")", "\n", "", "return", "lines", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.MrpcProcessor.get_train_examples": [[103, 108], ["logger.info", "run_classifier.MrpcProcessor._create_examples", "run_classifier.MrpcProcessor._read_tsv", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.ColaProcessor._create_examples", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.DataProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "logger", ".", "info", "(", "\"LOOKING AT {}\"", ".", "format", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.tsv\"", ")", ")", ")", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.tsv\"", ")", ")", ",", "\"train\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.MrpcProcessor.get_dev_examples": [[109, 113], ["run_classifier.MrpcProcessor._create_examples", "run_classifier.MrpcProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.ColaProcessor._create_examples", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.DataProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"dev.tsv\"", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.MrpcProcessor.get_labels": [[114, 117], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "\"0\"", ",", "\"1\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.MrpcProcessor._create_examples": [[118, 131], ["enumerate", "tokenization.convert_to_unicode", "tokenization.convert_to_unicode", "tokenization.convert_to_unicode", "examples.append", "run_classifier.InputExample"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.convert_to_unicode"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "text_a", "=", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "3", "]", ")", "\n", "text_b", "=", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "4", "]", ")", "\n", "label", "=", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "0", "]", ")", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.MnliProcessor.get_train_examples": [[136, 140], ["run_classifier.MnliProcessor._create_examples", "run_classifier.MnliProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.ColaProcessor._create_examples", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.DataProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.tsv\"", ")", ")", ",", "\"train\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.MnliProcessor.get_dev_examples": [[141, 146], ["run_classifier.MnliProcessor._create_examples", "run_classifier.MnliProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.ColaProcessor._create_examples", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.DataProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"dev_matched.tsv\"", ")", ")", ",", "\n", "\"dev_matched\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.MnliProcessor.get_labels": [[147, 150], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "\"contradiction\"", ",", "\"entailment\"", ",", "\"neutral\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.MnliProcessor._create_examples": [[151, 164], ["enumerate", "tokenization.convert_to_unicode", "tokenization.convert_to_unicode", "tokenization.convert_to_unicode", "examples.append", "run_classifier.InputExample", "tokenization.convert_to_unicode"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.convert_to_unicode"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "0", "]", ")", ")", "\n", "text_a", "=", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "8", "]", ")", "\n", "text_b", "=", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "9", "]", ")", "\n", "label", "=", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "-", "1", "]", ")", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.ColaProcessor.get_train_examples": [[169, 173], ["run_classifier.ColaProcessor._create_examples", "run_classifier.ColaProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.ColaProcessor._create_examples", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.DataProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.tsv\"", ")", ")", ",", "\"train\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.ColaProcessor.get_dev_examples": [[174, 178], ["run_classifier.ColaProcessor._create_examples", "run_classifier.ColaProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.ColaProcessor._create_examples", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.DataProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"dev.tsv\"", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.ColaProcessor.get_labels": [[179, 182], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "\"0\"", ",", "\"1\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.ColaProcessor._create_examples": [[183, 193], ["enumerate", "tokenization.convert_to_unicode", "tokenization.convert_to_unicode", "examples.append", "run_classifier.InputExample"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.convert_to_unicode"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "text_a", "=", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "3", "]", ")", "\n", "label", "=", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "1", "]", ")", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.convert_examples_to_features": [[195, 290], ["enumerate", "enumerate", "tokenizer.tokenize", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "tokenizer.convert_tokens_to_ids", "features.append", "tokenizer.tokenize", "run_classifier._truncate_seq_pair", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "segment_ids.append", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "run_classifier.InputFeatures", "len", "tokens.append", "segment_ids.append", "tokenization.printable_text", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.convert_tokens_to_ids", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.bert_utils._truncate_seq_pair", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.printable_text"], ["", "", "def", "convert_examples_to_features", "(", "examples", ",", "label_list", ",", "max_seq_length", ",", "tokenizer", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "label_map", "=", "{", "}", "\n", "for", "(", "i", ",", "label", ")", "in", "enumerate", "(", "label_list", ")", ":", "\n", "        ", "label_map", "[", "label", "]", "=", "i", "\n", "\n", "", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "tokens_a", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "text_a", ")", "\n", "\n", "tokens_b", "=", "None", "\n", "if", "example", ".", "text_b", ":", "\n", "            ", "tokens_b", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "text_b", ")", "\n", "\n", "", "if", "tokens_b", ":", "\n", "# Modifies `tokens_a` and `tokens_b` in place so that the total", "\n", "# length is less than the specified length.", "\n", "# Account for [CLS], [SEP], [SEP] with \"- 3\"", "\n", "            ", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_seq_length", "-", "3", ")", "\n", "", "else", ":", "\n", "# Account for [CLS] and [SEP] with \"- 2\"", "\n", "            ", "if", "len", "(", "tokens_a", ")", ">", "max_seq_length", "-", "2", ":", "\n", "                ", "tokens_a", "=", "tokens_a", "[", "0", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "# The convention in BERT is:", "\n", "# (a) For sequence pairs:", "\n", "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]", "\n", "#  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1", "\n", "# (b) For single sequences:", "\n", "#  tokens:   [CLS] the dog is hairy . [SEP]", "\n", "#  type_ids: 0   0   0   0  0     0 0", "\n", "#", "\n", "# Where \"type_ids\" are used to indicate whether this is the first", "\n", "# sequence or the second sequence. The embedding vectors for `type=0` and", "\n", "# `type=1` were learned during pre-training and are added to the wordpiece", "\n", "# embedding vector (and position vector). This is not *strictly* necessary", "\n", "# since the [SEP] token unambigiously separates the sequences, but it makes", "\n", "# it easier for the model to learn the concept of sequences.", "\n", "#", "\n", "# For classification tasks, the first vector (corresponding to [CLS]) is", "\n", "# used as as the \"sentence vector\". Note that this only makes sense because", "\n", "# the entire model is fine-tuned.", "\n", "", "", "tokens", "=", "[", "]", "\n", "segment_ids", "=", "[", "]", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "for", "token", "in", "tokens_a", ":", "\n", "            ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "if", "tokens_b", ":", "\n", "            ", "for", "token", "in", "tokens_b", ":", "\n", "                ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "\n", "", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids", ")", "<", "max_seq_length", ":", "\n", "            ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "\n", "label_id", "=", "label_map", "[", "example", ".", "label", "]", "\n", "if", "ex_index", "<", "5", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"guid: %s\"", "%", "(", "example", ".", "guid", ")", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "tokenization", ".", "printable_text", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "logger", ".", "info", "(", "\n", "\"segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"label: %s (id = %d)\"", "%", "(", "example", ".", "label", ",", "label_id", ")", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "label_id", "=", "label_id", ")", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier._truncate_seq_pair": [[292, 307], ["len", "len", "len", "len", "tokens_a.pop", "tokens_b.pop"], "function", ["None"], ["", "def", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_length", ")", ":", "\n", "    ", "\"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"", "\n", "\n", "# This is a simple heuristic which will always truncate the longer sequence", "\n", "# one token at a time. This makes more sense than truncating an equal percent", "\n", "# of tokens from each, since if one sequence is very short then each token", "\n", "# that's truncated likely contains more information than a longer sequence.", "\n", "while", "True", ":", "\n", "        ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_length", ":", "\n", "            ", "break", "\n", "", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "            ", "tokens_a", ".", "pop", "(", ")", "\n", "", "else", ":", "\n", "            ", "tokens_b", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.accuracy": [[308, 311], ["numpy.argmax", "numpy.sum"], "function", ["None"], ["", "", "", "def", "accuracy", "(", "out", ",", "labels", ")", ":", "\n", "    ", "outputs", "=", "np", ".", "argmax", "(", "out", ",", "axis", "=", "1", ")", "\n", "return", "np", ".", "sum", "(", "outputs", "==", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.main": [[312, 596], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logger.info", "int", "random.seed", "numpy.random.seed", "torch.manual_seed", "modeling.BertConfig.from_json_file", "os.makedirs", "parser.parse_args.task_name.lower", "processor.get_labels", "tokenization.FullTokenizer", "modeling.BertForSequenceClassification", "torch.nn.DataParallel.to", "optimization.BERTAdam", "torch.device", "torch.cuda.device_count", "torch.device", "torch.distributed.init_process_group", "bool", "ValueError", "torch.cuda.manual_seed_all", "ValueError", "ValueError", "os.path.exists", "os.listdir", "ValueError", "ValueError", "processor.get_train_examples", "int", "len", "torch.nn.DataParallel.bert.load_state_dict", "torch.nn.parallel.DistributedDataParallel", "run_classifier.convert_examples_to_features", "logger.info", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "torch.nn.DataParallel.train", "tqdm.trange", "processor.get_dev_examples", "run_classifier.convert_examples_to_features", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "torch.nn.DataParallel.eval", "os.path.join", "torch.load", "torch.nn.DataParallel", "len", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "int", "enumerate", "len", "torch.utils.data.SequentialSampler", "torch.utils.data.distributed.DistributedSampler", "input_ids.to.to", "input_mask.to.to", "segment_ids.to.to", "label_ids.to.to", "torch.nn.DataParallel.", "logits.detach().cpu().numpy.detach().cpu().numpy", "label_ids.to.to().numpy", "run_classifier.accuracy", "tmp_eval_loss.mean().item", "input_ids.to.size", "open", "logger.info", "sorted", "tqdm.tqdm", "input_ids.to.to", "input_mask.to.to", "segment_ids.to.to", "label_ids.to.to", "torch.nn.DataParallel.", "loss.mean.item", "input_ids.to.size", "loss.mean.backward", "result.keys", "logger.info", "writer.write", "torch.cuda.is_available", "len", "torch.nn.DataParallel.named_parameters", "torch.nn.DataParallel.named_parameters", "loss.mean.mean", "optimization.BERTAdam.step", "torch.nn.DataParallel.zero_grad", "logits.detach().cpu().numpy.detach().cpu", "label_ids.to.to", "tmp_eval_loss.mean", "str", "logits.detach().cpu().numpy.detach", "str"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.modeling.BertConfig.from_json_file", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.ColaProcessor.get_labels", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.optimization.BERTAdam.to", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.ColaProcessor.get_train_examples", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.convert_examples_to_features", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.ColaProcessor.get_dev_examples", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.convert_examples_to_features", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.optimization.BERTAdam.to", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.optimization.BERTAdam.to", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.optimization.BERTAdam.to", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.optimization.BERTAdam.to", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.run_classifier.accuracy", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.optimization.BERTAdam.to", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.optimization.BERTAdam.to", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.optimization.BERTAdam.to", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.optimization.BERTAdam.to", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.optimization.BERTAdam.step", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.optimization.BERTAdam.to"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "## Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_config_file\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The config json file corresponding to the pre-trained BERT model. \\n\"", "\n", "\"This specifies the model architecture.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--task_name\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The name of the task to train.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--vocab_file\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The vocabulary file that the BERT model was trained on.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model checkpoints will be written.\"", ")", "\n", "\n", "## Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--init_checkpoint\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Initial checkpoint (usually from a pre-trained BERT model).\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to lower case the input text. True for uncased models, False for cased models.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "\n", "default", "=", "32", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_batch_size\"", ",", "\n", "default", "=", "8", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for eval.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "\n", "default", "=", "5e-5", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "\n", "default", "=", "3.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_proportion\"", ",", "\n", "default", "=", "0.1", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Proportion of training to perform linear learning rate warmup for. \"", "\n", "\"E.g., 0.1 = 10%% of training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_checkpoints_steps\"", ",", "\n", "default", "=", "1000", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"How often to save the model checkpoint.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--accumulate_gradients\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of steps to accumulate gradient on (divide the batch_size and accumulate)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumualte before performing a backward/update pass.\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "processors", "=", "{", "\n", "\"cola\"", ":", "ColaProcessor", ",", "\n", "\"mnli\"", ":", "MnliProcessor", ",", "\n", "\"mrpc\"", ":", "MrpcProcessor", ",", "\n", "}", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "logger", ".", "info", "(", "\"device %s n_gpu %d distributed training %r\"", ",", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ")", "\n", "\n", "if", "args", ".", "accumulate_gradients", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid accumulate_gradients parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "accumulate_gradients", ")", ")", "\n", "\n", "", "args", ".", "train_batch_size", "=", "int", "(", "args", ".", "train_batch_size", "/", "args", ".", "accumulate_gradients", ")", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "if", "not", "args", ".", "do_train", "and", "not", "args", ".", "do_eval", ":", "\n", "        ", "raise", "ValueError", "(", "\"At least one of `do_train` or `do_eval` must be True.\"", ")", "\n", "\n", "", "bert_config", "=", "BertConfig", ".", "from_json_file", "(", "args", ".", "bert_config_file", ")", "\n", "\n", "if", "args", ".", "max_seq_length", ">", "bert_config", ".", "max_position_embeddings", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"Cannot use sequence length {} because the BERT model was only trained up to sequence length {}\"", ".", "format", "(", "\n", "args", ".", "max_seq_length", ",", "bert_config", ".", "max_position_embeddings", ")", ")", "\n", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", "and", "os", ".", "listdir", "(", "args", ".", "output_dir", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"Output directory ({}) already exists and is not empty.\"", ".", "format", "(", "args", ".", "output_dir", ")", ")", "\n", "", "os", ".", "makedirs", "(", "args", ".", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "task_name", "=", "args", ".", "task_name", ".", "lower", "(", ")", "\n", "\n", "if", "task_name", "not", "in", "processors", ":", "\n", "        ", "raise", "ValueError", "(", "\"Task not found: %s\"", "%", "(", "task_name", ")", ")", "\n", "\n", "", "processor", "=", "processors", "[", "task_name", "]", "(", ")", "\n", "\n", "label_list", "=", "processor", ".", "get_labels", "(", ")", "\n", "\n", "tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "\n", "vocab_file", "=", "args", ".", "vocab_file", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "train_examples", "=", "None", "\n", "num_train_steps", "=", "None", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "train_examples", "=", "processor", ".", "get_train_examples", "(", "args", ".", "data_dir", ")", "\n", "num_train_steps", "=", "int", "(", "\n", "len", "(", "train_examples", ")", "/", "args", ".", "train_batch_size", "*", "args", ".", "num_train_epochs", ")", "\n", "\n", "", "model", "=", "BertForSequenceClassification", "(", "bert_config", ",", "len", "(", "label_list", ")", ")", "\n", "if", "args", ".", "init_checkpoint", "is", "not", "None", ":", "\n", "        ", "model", ".", "bert", ".", "load_state_dict", "(", "torch", ".", "load", "(", "args", ".", "init_checkpoint", ",", "map_location", "=", "'cpu'", ")", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "parallel", ".", "DistributedDataParallel", "(", "model", ",", "device_ids", "=", "[", "args", ".", "local_rank", "]", ",", "\n", "output_device", "=", "args", ".", "local_rank", ")", "\n", "", "elif", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "", "no_decay", "=", "[", "'bias'", ",", "'gamma'", ",", "'beta'", "]", "\n", "optimizer_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "n", "not", "in", "no_decay", "]", ",", "'weight_decay_rate'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "n", "in", "no_decay", "]", ",", "'weight_decay_rate'", ":", "0.0", "}", "\n", "]", "\n", "\n", "optimizer", "=", "BERTAdam", "(", "optimizer_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "warmup", "=", "args", ".", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_steps", ")", "\n", "\n", "global_step", "=", "0", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "train_features", "=", "convert_examples_to_features", "(", "\n", "train_examples", ",", "label_list", ",", "args", ".", "max_seq_length", ",", "tokenizer", ")", "\n", "logger", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "train_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Num steps = %d\"", ",", "num_train_steps", ")", "\n", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "train_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_label_ids", ")", "\n", "if", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "            ", "train_sampler", "=", "RandomSampler", "(", "train_data", ")", "\n", "", "else", ":", "\n", "            ", "train_sampler", "=", "DistributedSampler", "(", "train_data", ")", "\n", "", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "sampler", "=", "train_sampler", ",", "batch_size", "=", "args", ".", "train_batch_size", ")", "\n", "\n", "model", ".", "train", "(", ")", "\n", "for", "epoch", "in", "trange", "(", "int", "(", "args", ".", "num_train_epochs", ")", ",", "desc", "=", "\"Epoch\"", ")", ":", "\n", "            ", "tr_loss", "=", "0", "\n", "nb_tr_examples", ",", "nb_tr_steps", "=", "0", ",", "0", "\n", "for", "step", ",", "(", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_ids", ")", "in", "enumerate", "(", "tqdm", "(", "train_dataloader", ",", "desc", "=", "\"Iteration\"", ")", ")", ":", "\n", "                ", "input_ids", "=", "input_ids", ".", "to", "(", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "device", ")", "\n", "segment_ids", "=", "segment_ids", ".", "to", "(", "device", ")", "\n", "label_ids", "=", "label_ids", ".", "to", "(", "device", ")", "\n", "\n", "loss", ",", "_", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "label_ids", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "", "tr_loss", "+=", "loss", ".", "item", "(", ")", "\n", "nb_tr_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_tr_steps", "+=", "1", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                    ", "optimizer", ".", "step", "(", ")", "# We have accumulated enought gradients", "\n", "model", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "\n", "", "", "", "", "if", "args", ".", "do_eval", ":", "\n", "        ", "eval_examples", "=", "processor", ".", "get_dev_examples", "(", "args", ".", "data_dir", ")", "\n", "eval_features", "=", "convert_examples_to_features", "(", "\n", "eval_examples", ",", "label_list", ",", "args", ".", "max_seq_length", ",", "tokenizer", ")", "\n", "\n", "logger", ".", "info", "(", "\"***** Running evaluation *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "eval_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "eval_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_label_ids", ")", "\n", "if", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "            ", "eval_sampler", "=", "SequentialSampler", "(", "eval_data", ")", "\n", "", "else", ":", "\n", "            ", "eval_sampler", "=", "DistributedSampler", "(", "eval_data", ")", "\n", "", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "eval_loss", ",", "eval_accuracy", "=", "0", ",", "0", "\n", "nb_eval_steps", ",", "nb_eval_examples", "=", "0", ",", "0", "\n", "for", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_ids", "in", "eval_dataloader", ":", "\n", "            ", "input_ids", "=", "input_ids", ".", "to", "(", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "device", ")", "\n", "segment_ids", "=", "segment_ids", ".", "to", "(", "device", ")", "\n", "label_ids", "=", "label_ids", ".", "to", "(", "device", ")", "\n", "\n", "tmp_eval_loss", ",", "logits", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "label_ids", ")", "\n", "\n", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "label_ids", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "tmp_eval_accuracy", "=", "accuracy", "(", "logits", ",", "label_ids", ")", "\n", "\n", "eval_loss", "+=", "tmp_eval_loss", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "eval_accuracy", "+=", "tmp_eval_accuracy", "\n", "\n", "nb_eval_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_eval_steps", "+=", "1", "\n", "\n", "", "eval_loss", "=", "eval_loss", "/", "nb_eval_steps", "#len(eval_dataloader)", "\n", "eval_accuracy", "=", "eval_accuracy", "/", "nb_eval_examples", "#len(eval_dataloader)", "\n", "\n", "result", "=", "{", "'eval_loss'", ":", "eval_loss", ",", "\n", "'eval_accuracy'", ":", "eval_accuracy", ",", "\n", "'global_step'", ":", "global_step", ",", "\n", "'loss'", ":", "tr_loss", "/", "nb_tr_steps", "}", "#'loss': loss.item()}", "\n", "\n", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"eval_results.txt\"", ")", "\n", "with", "open", "(", "output_eval_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "            ", "logger", ".", "info", "(", "\"***** Eval results *****\"", ")", "\n", "for", "key", "in", "sorted", "(", "result", ".", "keys", "(", ")", ")", ":", "\n", "                ", "logger", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", "\n", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.FullTokenizer.__init__": [[104, 108], ["tokenization.load_vocab", "tokenization.BasicTokenizer", "tokenization.WordpieceTokenizer"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.load_vocab"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.FullTokenizer.tokenize": [[109, 116], ["tokenization.FullTokenizer.basic_tokenizer.tokenize", "tokenization.FullTokenizer.wordpiece_tokenizer.tokenize", "split_tokens.append"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", ":", "\n", "            ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.FullTokenizer.convert_tokens_to_ids": [[117, 119], ["tokenization.FullTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.convert_tokens_to_ids"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "return", "convert_tokens_to_ids", "(", "self", ".", "vocab", ",", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.BasicTokenizer.__init__": [[124, 131], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "do_lower_case", "=", "True", ")", ":", "\n", "        ", "\"\"\"Constructs a BasicTokenizer.\n\n        Args:\n          do_lower_case: Whether to lower case the input.\n        \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.BasicTokenizer.tokenize": [[132, 146], ["tokenization.convert_to_unicode", "tokenization.BasicTokenizer._clean_text", "tokenization.whitespace_tokenize", "tokenization.whitespace_tokenize", "split_tokens.extend", "tokenization.BasicTokenizer.lower", "tokenization.BasicTokenizer._run_strip_accents", "tokenization.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "            ", "if", "self", ".", "do_lower_case", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.BasicTokenizer._run_strip_accents": [[147, 157], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["None"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "                ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.BasicTokenizer._run_split_on_punc": [[158, 177], ["list", "len", "tokenization._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "            ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "start_new_word", ":", "\n", "                    ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.BasicTokenizer._clean_text": [[178, 190], ["ord", "tokenization._is_whitespace", "tokenization._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization._is_whitespace", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "                ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.__init__": [[195, 199], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "100", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.tokenize": [[200, 252], ["tokenization.convert_to_unicode", "tokenization.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.whitespace_tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = \"unaffable\"\n          output = [\"un\", \"##aff\", \"##able\"]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer.\n\n        Returns:\n          A list of wordpiece tokens.\n        \"\"\"", "\n", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "            ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "                ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "                    ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "                        ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "                        ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "                    ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.convert_to_unicode": [[26, 44], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "text.decode", "isinstance", "ValueError", "type", "type"], "function", ["None"], ["def", "convert_to_unicode", "(", "text", ")", ":", "\n", "    ", "\"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"", "\n", "if", "six", ".", "PY3", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "            ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "            ", "return", "text", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.printable_text": [[46, 67], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "isinstance", "text.encode", "ValueError", "type", "type"], "function", ["None"], ["", "", "def", "printable_text", "(", "text", ")", ":", "\n", "    ", "\"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"", "\n", "\n", "# These functions want `str` for both Python2 and Python3, but in one case", "\n", "# it's a Unicode string and in the other it's a byte string.", "\n", "if", "six", ".", "PY3", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "            ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "            ", "return", "text", ".", "encode", "(", "\"utf-8\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.load_vocab": [[69, 82], ["collections.OrderedDict", "open", "tokenization.convert_to_unicode", "token.strip.strip", "reader.readline"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.convert_to_unicode"], ["", "", "def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "    ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "index", "=", "0", "\n", "with", "open", "(", "vocab_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "token", "=", "convert_to_unicode", "(", "reader", ".", "readline", "(", ")", ")", "\n", "if", "not", "token", ":", "\n", "                ", "break", "\n", "", "token", "=", "token", ".", "strip", "(", ")", "\n", "vocab", "[", "token", "]", "=", "index", "\n", "index", "+=", "1", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.convert_tokens_to_ids": [[84, 90], ["ids.append"], "function", ["None"], ["", "def", "convert_tokens_to_ids", "(", "vocab", ",", "tokens", ")", ":", "\n", "    ", "\"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"", "\n", "ids", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "        ", "ids", ".", "append", "(", "vocab", "[", "token", "]", ")", "\n", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.whitespace_tokenize": [[92, 99], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "        ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization._is_whitespace": [[254, 264], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization._is_control": [[266, 276], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"C\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization._is_punctuation": [[278, 292], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "", ""]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.bert_utils.convert_sentence_and_mention_to_features": [[24, 94], ["tokenizer.tokenize", "tokenizer.tokenize", "bert_utils._truncate_seq_pair", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "tokenizer.convert_tokens_to_ids", "print", "print", "print", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "segment_ids.append", "len", "len", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.bert_utils._truncate_seq_pair", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.convert_tokens_to_ids"], ["def", "convert_sentence_and_mention_to_features", "(", "sentence", ",", "mention", ",", "max_seq_length", ",", "tokenizer", ")", ":", "\n", "\n", "  ", "sentence", "=", "tokenizer", ".", "tokenize", "(", "sentence", ")", "\n", "mention", "=", "tokenizer", ".", "tokenize", "(", "mention", ")", "\n", "_truncate_seq_pair", "(", "sentence", ",", "mention", ",", "max_seq_length", "-", "3", ")", "\n", "\n", "# The convention in BERT is:", "\n", "# (a) For sequence pairs:", "\n", "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]", "\n", "#  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1", "\n", "# (b) For single sequences:", "\n", "#  tokens:   [CLS] the dog is hairy . [SEP]", "\n", "#  type_ids: 0   0   0   0  0     0 0", "\n", "#", "\n", "# Where \"type_ids\" are used to indicate whether this is the first", "\n", "# sequence or the second sequence. The embedding vectors for `type=0` and", "\n", "# `type=1` were learned during pre-training and are added to the wordpiece", "\n", "# embedding vector (and position vector). This is not *strictly* necessary", "\n", "# since the [SEP] token unambigiously separates the sequences, but it makes", "\n", "# it easier for the model to learn the concept of sequences.", "\n", "#", "\n", "# For classification tasks, the first vector (corresponding to [CLS]) is", "\n", "# used as as the \"sentence vector\". Note that this only makes sense because", "\n", "# the entire model is fine-tuned.", "\n", "\n", "tokens", "=", "[", "]", "\n", "segment_ids", "=", "[", "]", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "idx_tracker", "=", "0", "\n", "sentence_start_idx", "=", "1", "\n", "\n", "for", "token", "in", "sentence", ":", "\n", "    ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "idx_tracker", "+=", "1", "\n", "\n", "", "sentence_end_idx", "=", "idx_tracker", "\n", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "idx_tracker", "+=", "1", "\n", "mention_start_idx", "=", "idx_tracker", "+", "1", "\n", "\n", "for", "token", "in", "mention", ":", "\n", "    ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "idx_tracker", "+=", "1", "\n", "\n", "", "mention_end_idx", "=", "idx_tracker", "\n", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids", ")", "<", "max_seq_length", ":", "\n", "    ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", ",", "print", "(", "input_ids", ",", "len", "(", "input_ids", ")", ",", "max_seq_length", ")", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", ",", "print", "(", "input_mask", ",", "len", "(", "input_mask", ")", ",", "max_seq_length", ")", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", ",", "print", "(", "segment_ids", ",", "len", "(", "segment_ids", ")", ",", "max_seq_length", ")", "\n", "\n", "return", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "(", "sentence_start_idx", ",", "sentence_end_idx", ")", ",", "(", "mention_start_idx", ",", "mention_end_idx", ")", "# inclusive", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.bert_utils.convert_sentence_to_features": [[96, 135], ["tokenizer.tokenize", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "tokenizer.convert_tokens_to_ids", "print", "print", "print", "tokens.append", "segment_ids.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "segment_ids.append", "len", "len", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.tokenization.convert_tokens_to_ids"], ["", "def", "convert_sentence_to_features", "(", "sentence", ",", "max_seq_length", ",", "tokenizer", ")", ":", "\n", "\n", "  ", "sentence", "=", "tokenizer", ".", "tokenize", "(", "sentence", ")", "\n", "sentence", "=", "sentence", "[", "0", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "tokens", "=", "[", "]", "\n", "segment_ids", "=", "[", "]", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "idx_tracker", "=", "0", "\n", "sentence_start_idx", "=", "1", "\n", "\n", "for", "token", "in", "sentence", ":", "\n", "    ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "idx_tracker", "+=", "1", "\n", "\n", "", "sentence_end_idx", "=", "idx_tracker", "\n", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "idx_tracker", "+=", "1", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids", ")", "<", "max_seq_length", ":", "\n", "    ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", ",", "print", "(", "input_ids", ",", "len", "(", "input_ids", ")", ",", "max_seq_length", ")", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", ",", "print", "(", "input_mask", ",", "len", "(", "input_mask", ")", ",", "max_seq_length", ")", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", ",", "print", "(", "segment_ids", ",", "len", "(", "segment_ids", ")", ",", "max_seq_length", ")", "\n", "\n", "return", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "(", "sentence_start_idx", ",", "sentence_end_idx", ")", ",", "(", "None", ",", "None", ")", "# inclusive", "\n", "\n"]], "home.repos.pwc.inspect_result.yasumasaonoe_DenoiseET.bert.bert_utils._truncate_seq_pair": [[137, 149], ["tokens_a.pop", "len", "len"], "function", ["None"], ["", "def", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_length", ")", ":", "\n", "    ", "\"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"", "\n", "\n", "# This is a simple heuristic which will always truncate the longer sequence", "\n", "# one token at a time. This makes more sense than truncating an equal percent", "\n", "# of tokens from each, since if one sequence is very short then each token", "\n", "# that's truncated likely contains more information than a longer sequence.", "\n", "while", "True", ":", "\n", "        ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_length", ":", "\n", "            ", "break", "\n", "", "tokens_a", ".", "pop", "(", ")", "\n", "#if len(tokens_a) > len(tokens_b):", "\n"]]}