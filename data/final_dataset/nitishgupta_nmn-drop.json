{"home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.fenwicktree.FenwickTree.__init__": [[5, 13], ["fenwicktree.FenwickTree.construct"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.fenwicktree.FenwickTree.construct"], ["    ", "def", "__init__", "(", "self", ",", "n", ":", "int", ",", "arr", ":", "List", "[", "int", "]", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Initialize fenwick tree for an array of size n\n        :param n: Size of array\n        :param arr: Pre-initialization of array. Almost all use-cases wouldn't require this argument.\n        \"\"\"", "\n", "self", ".", "BITTree", "=", "self", ".", "construct", "(", "n", ")", "\n", "self", ".", "n", "=", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.fenwicktree.FenwickTree.getsum": [[17, 31], ["None"], "methods", ["None"], ["", "def", "getsum", "(", "self", ",", "i", ")", ":", "\n", "        ", "s", "=", "0", "# initialize result", "\n", "\n", "# index in BITree[] is 1 more than the index in arr[]", "\n", "i", "=", "i", "+", "1", "\n", "\n", "# Traverse ancestors of BITree[index]", "\n", "while", "i", ">", "0", ":", "\n", "# Add current element of BITree to sum", "\n", "            ", "s", "+=", "self", ".", "BITTree", "[", "i", "]", "\n", "\n", "# Move index to parent node in getSum View", "\n", "i", "-=", "i", "&", "(", "-", "i", ")", "\n", "", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.fenwicktree.FenwickTree.updatebit": [[35, 46], ["None"], "methods", ["None"], ["", "def", "updatebit", "(", "self", ",", "i", ",", "v", ")", ":", "\n", "# index in BITree[] is 1 more than the index in arr[]", "\n", "        ", "i", "+=", "1", "\n", "\n", "# Traverse all ancestors and add 'val'", "\n", "while", "i", "<=", "self", ".", "n", ":", "\n", "# Add 'val' to current node of BI Tree", "\n", "            ", "self", ".", "BITTree", "[", "i", "]", "+=", "v", "\n", "\n", "# Update index to that of parent in update View", "\n", "i", "+=", "i", "&", "(", "-", "i", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.fenwicktree.FenwickTree.construct": [[49, 60], ["None"], "methods", ["None"], ["", "", "def", "construct", "(", "self", ",", "n", ",", "arr", "=", "None", ")", ":", "\n", "# Create and initialize BITree[] as 0", "\n", "        ", "BITTree", "=", "[", "0", "]", "*", "(", "n", "+", "1", ")", "\n", "# # Store the actual values in BITree[] using update()", "\n", "# for i in range(n):", "\n", "#     self.updatebit(BITTree, n, i, arr[i])", "\n", "\n", "# Uncomment below lines to see contents of BITree[]", "\n", "# for i in range(1,n+1):", "\n", "#     print BITTree[i],", "\n", "return", "BITTree", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.fenwicktree.FenwickTree.getlistidx": [[61, 63], ["fenwicktree.FenwickTree.getsum"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.fenwicktree.FenwickTree.getsum"], ["", "def", "getlistidx", "(", "self", ",", "idx", ")", ":", "\n", "        ", "return", "idx", "+", "self", ".", "getsum", "(", "idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.TAUtils.getCCGNLPLocalPipeline": [[6, 9], ["ccg_nlpy.local_pipeline.LocalPipeline"], "function", ["None"], ["def", "getCCGNLPLocalPipeline", "(", ")", ":", "\n", "    ", "ccg_nlp", "=", "LocalPipeline", "(", ")", "\n", "return", "ccg_nlp", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.TAUtils.getOntonotesNER": [[11, 26], ["ccg_nlp.doc", "ners.append"], "function", ["None"], ["", "def", "getOntonotesNER", "(", "sentence", ":", "List", "[", "str", "]", ",", "ccg_nlp", ":", "LocalPipeline", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "ccgdoc", ":", "TextAnnotation", "=", "ccg_nlp", ".", "doc", "(", "[", "sentence", "]", ",", "pretokenized", "=", "True", ")", "\n", "", "except", ":", "\n", "        ", "return", "[", "]", "\n", "\n", "", "ner_view", "=", "ccgdoc", ".", "get_ner_ontonotes", "\n", "\n", "ners", "=", "[", "]", "\n", "\n", "if", "ner_view", ".", "cons_list", "is", "not", "None", ":", "\n", "        ", "for", "cons", "in", "ner_view", ".", "cons_list", ":", "\n", "            ", "ners", ".", "append", "(", "(", "cons", "[", "\"tokens\"", "]", ",", "cons", "[", "\"start\"", "]", ",", "cons", "[", "\"end\"", "]", ",", "cons", "[", "\"label\"", "]", "+", "\"_ccg\"", ")", ")", "\n", "\n", "", "", "return", "ners", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.TAUtils.thresholdSentLength": [[28, 50], ["TAUtils.get_sentences", "lp.doc", "len", "new_sentences.append", "new_sentences.append", "len", "new_sentences.append"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.TAUtils.get_sentences"], ["", "def", "thresholdSentLength", "(", "ta", ":", "TextAnnotation", ",", "lp", ":", "LocalPipeline", ",", "maxlen", ":", "int", "=", "120", ")", "->", "TextAnnotation", ":", "\n", "    ", "\"\"\" Make a new text annotation by spliting sentences longer than maxlen tokens \n    :rtype: TextAnnotation\n    \"\"\"", "\n", "\n", "sentences", ":", "List", "[", "List", "[", "str", "]", "]", "=", "get_sentences", "(", "ta", ")", "\n", "\n", "new_sentences", "=", "[", "]", "\n", "for", "sent", "in", "sentences", ":", "\n", "        ", "if", "len", "(", "sent", ")", "<=", "maxlen", ":", "\n", "            ", "new_sentences", ".", "append", "(", "sent", ")", "\n", "", "else", ":", "\n", "# Make chunks of maxlen", "\n", "            ", "while", "len", "(", "sent", ")", ">", "maxlen", ":", "\n", "                ", "new_sentences", ".", "append", "(", "sent", "[", "0", ":", "maxlen", "]", ")", "\n", "sent", "=", "sent", "[", "maxlen", ":", "]", "\n", "# Put the last remaining sent i.e. < maxlen", "\n", "", "new_sentences", ".", "append", "(", "sent", ")", "\n", "\n", "", "", "new_ta", "=", "lp", ".", "doc", "(", "new_sentences", ",", "True", ")", "\n", "\n", "return", "new_ta", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.TAUtils.getNPChunks_perSent": [[52, 71], ["TAUtils.getAll_SentIdAndTokenOffset", "len", "range", "nps_withinSentIdxs[].append"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getAll_SentIdAndTokenOffset"], ["", "def", "getNPChunks_perSent", "(", "ta", ":", "TextAnnotation", ")", "->", "List", "[", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", ":", "\n", "    ", "\"\"\"TA -> For each sentence, within sentence token offsets for NPs.\"\"\"", "\n", "\n", "token_sentIdxWithinSentIdx", "=", "getAll_SentIdAndTokenOffset", "(", "ta", ")", "\n", "numSents", "=", "len", "(", "ta", ".", "sentence_end_position", ")", "\n", "\n", "nps_withinSentIdxs", "=", "[", "[", "]", "for", "i", "in", "range", "(", "0", ",", "numSents", ")", "]", "\n", "\n", "chunk_cons", "=", "ta", ".", "get_shallow_parse", ".", "cons_list", "\n", "for", "cons", "in", "chunk_cons", ":", "\n", "        ", "if", "cons", "[", "\"label\"", "]", "==", "\"NP\"", ":", "\n", "            ", "(", "sentidx1", ",", "startOffset", ")", "=", "token_sentIdxWithinSentIdx", "[", "cons", "[", "\"start\"", "]", "]", "\n", "(", "sentidx2", ",", "endOffset", ")", "=", "token_sentIdxWithinSentIdx", "[", "cons", "[", "\"end\"", "]", "-", "1", "]", "\n", "if", "sentidx1", "!=", "sentidx2", ":", "\n", "# Ignore cross sentence NPs", "\n", "                ", "continue", "\n", "", "nps_withinSentIdxs", "[", "sentidx1", "]", ".", "append", "(", "(", "startOffset", ",", "endOffset", "+", "1", ")", ")", "\n", "\n", "", "", "return", "nps_withinSentIdxs", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.TAUtils.getPOS_forDoc": [[73, 83], ["len", "len"], "function", ["None"], ["", "def", "getPOS_forDoc", "(", "ta", ":", "TextAnnotation", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "\"\"\"TA -> For complete doc get POS.\"\"\"", "\n", "\n", "pos_cons", "=", "ta", ".", "get_pos", ".", "cons_list", "\n", "\n", "pos_tags", "=", "[", "cons", "[", "\"label\"", "]", "for", "cons", "in", "pos_cons", "]", "\n", "\n", "assert", "len", "(", "ta", ".", "tokens", ")", "==", "len", "(", "pos_tags", ")", "\n", "\n", "return", "pos_tags", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.TAUtils.getPOS_perSent": [[85, 102], ["TAUtils.getAll_SentIdAndTokenOffset", "len", "range", "len", "len", "len", "pos_perSent[].append", "range"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getAll_SentIdAndTokenOffset"], ["", "def", "getPOS_perSent", "(", "ta", ":", "TextAnnotation", ")", "->", "List", "[", "List", "[", "str", "]", "]", ":", "\n", "    ", "\"\"\"TA -> For each sentence, within sentence token offsets for NPs.\"\"\"", "\n", "\n", "token_sentIdxWithinSentIdx", "=", "getAll_SentIdAndTokenOffset", "(", "ta", ")", "\n", "numSents", "=", "len", "(", "ta", ".", "sentence_end_position", ")", "\n", "\n", "pos_perSent", "=", "[", "[", "]", "for", "i", "in", "range", "(", "0", ",", "numSents", ")", "]", "\n", "\n", "pos_cons", "=", "ta", ".", "get_pos", ".", "cons_list", "\n", "\n", "assert", "len", "(", "ta", ".", "tokens", ")", "==", "len", "(", "pos_cons", ")", "\n", "\n", "for", "idx", "in", "range", "(", "0", ",", "len", "(", "ta", ".", "tokens", ")", ")", ":", "\n", "        ", "pos_label", "=", "pos_cons", "[", "idx", "]", "[", "\"label\"", "]", "\n", "(", "sentidx1", ",", "_", ")", "=", "token_sentIdxWithinSentIdx", "[", "idx", "]", "\n", "pos_perSent", "[", "sentidx1", "]", ".", "append", "(", "pos_label", ")", "\n", "", "return", "pos_perSent", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.TAUtils.getNPsWithGlobalOffsets": [[104, 115], ["np_chunk_cons.append"], "function", ["None"], ["", "def", "getNPsWithGlobalOffsets", "(", "ta", ":", "TextAnnotation", ")", "->", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ":", "\n", "    ", "\"\"\"TA -> Token offsets for all NPs (with global offsets).\"\"\"", "\n", "chunk_cons", "=", "ta", ".", "get_shallow_parse", ".", "cons_list", "\n", "np_chunk_cons", "=", "[", "]", "\n", "for", "cons", "in", "chunk_cons", ":", "\n", "        ", "if", "cons", "[", "\"label\"", "]", "==", "\"NP\"", ":", "\n", "            ", "np_chunk_cons", ".", "append", "(", "cons", ")", "\n", "\n", "", "", "np_chunk_startend", "=", "[", "(", "cons", "[", "\"start\"", "]", ",", "cons", "[", "\"end\"", "]", ")", "for", "cons", "in", "np_chunk_cons", "]", "\n", "\n", "return", "np_chunk_startend", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.TAUtils.get_sentences": [[117, 129], ["sentences.append", "len", "len"], "function", ["None"], ["", "def", "get_sentences", "(", "ta", ":", "TextAnnotation", ")", "->", "List", "[", "List", "[", "str", "]", "]", ":", "\n", "    ", "\"\"\"Get tokenized sentences from TextAnnotation as list of list of str.\"\"\"", "\n", "\n", "start", "=", "0", "\n", "sentences", "=", "[", "]", "\n", "tokens", "=", "ta", ".", "tokens", "\n", "sentence_end_positions", "=", "ta", ".", "sentence_end_position", "\n", "for", "end", "in", "sentence_end_positions", ":", "\n", "        ", "sentences", ".", "append", "(", "tokens", "[", "start", ":", "end", "]", ")", "\n", "start", "=", "end", "\n", "", "assert", "len", "(", "sentences", ")", "==", "len", "(", "sentence_end_positions", ")", "\n", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.TAUtils.getAll_SentIdAndTokenOffset": [[131, 153], ["len", "range", "tokenIdxs.append"], "function", ["None"], ["", "def", "getAll_SentIdAndTokenOffset", "(", "ta", ":", "TextAnnotation", ")", "->", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ":", "\n", "    ", "\"\"\"Get (sentence idx, withinSentOffset) for all tokens.\"\"\"", "\n", "tokens", "=", "ta", ".", "tokens", "\n", "numTokens", "=", "len", "(", "tokens", ")", "\n", "tokenIdxs", "=", "[", "]", "\n", "\n", "sentence_end_pos", "=", "ta", ".", "sentence_end_position", "\n", "# sentence_start_pos = [0]", "\n", "# sentence_start_pos.extend(sentence_end_pos)", "\n", "# sentence_start_pos = sentence_start_pos[:-1]", "\n", "\n", "sent_idx", "=", "0", "\n", "withinsent_tokenidx", "=", "0", "\n", "for", "i", "in", "range", "(", "0", ",", "numTokens", ")", ":", "\n", "        ", "if", "i", "==", "sentence_end_pos", "[", "sent_idx", "]", ":", "\n", "            ", "sent_idx", "+=", "1", "\n", "withinsent_tokenidx", "=", "0", "\n", "\n", "", "tokenIdxs", ".", "append", "(", "(", "sent_idx", ",", "withinsent_tokenidx", ")", ")", "\n", "withinsent_tokenidx", "+=", "1", "\n", "\n", "", "return", "tokenIdxs", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.TAUtils.getSentIdAndTokenOffset": [[155, 162], ["range", "len", "tokenIdxs.append", "TAUtils.getSentIdx_WithinSentTokenIdx"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.TAUtils.getSentIdx_WithinSentTokenIdx"], ["", "def", "getSentIdAndTokenOffset", "(", "ta", ":", "TextAnnotation", ")", "->", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ":", "\n", "    ", "tokenIdxs", "=", "[", "]", "\n", "sentence_end_pos", "=", "ta", ".", "sentence_end_position", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "ta", ".", "tokens", ")", ")", ":", "\n", "        ", "tokenIdxs", ".", "append", "(", "getSentIdx_WithinSentTokenIdx", "(", "sentence_end_pos", ",", "i", ")", ")", "\n", "\n", "", "return", "tokenIdxs", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.TAUtils.getSentEndPosArray": [[164, 176], ["enumerate", "len", "sentence_end_pos.append"], "function", ["None"], ["", "def", "getSentEndPosArray", "(", "sentences", ":", "List", "[", "List", "[", "str", "]", "]", ")", "->", "List", "[", "int", "]", ":", "\n", "    ", "\"\"\"\n    Get sentences end position array for tokenized sentences\n    :param sentences: Tokenized sentences\n    :return: sentence_end_pos: List containing (ex) tokenoffsets of sentence ends. Length == number of sentences\n    \"\"\"", "\n", "sum", "=", "0", "\n", "sentence_end_pos", "=", "[", "]", "\n", "for", "i", ",", "sent", "in", "enumerate", "(", "sentences", ")", ":", "\n", "        ", "sum", "+=", "len", "(", "sent", ")", "\n", "sentence_end_pos", ".", "append", "(", "sum", ")", "\n", "", "return", "sentence_end_pos", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.TAUtils.tokenizedSentToStr": [[178, 184], ["docstr.strip"], "function", ["None"], ["", "def", "tokenizedSentToStr", "(", "sentences", ":", "List", "[", "List", "[", "str", "]", "]", ")", "->", "str", ":", "\n", "    ", "docstr", "=", "\"\"", "\n", "for", "sent", "in", "sentences", ":", "\n", "        ", "docstr", "+=", "\" \"", ".", "join", "(", "sent", ")", "\n", "docstr", "+=", "\"\\n\"", "\n", "", "return", "docstr", ".", "strip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.TAUtils.getSentIdx_WithinSentTokenIdx": [[186, 199], ["TAUtils.get_closest_value"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.TAUtils.get_closest_value"], ["", "def", "getSentIdx_WithinSentTokenIdx", "(", "sentence_end_pos", ":", "List", "[", "int", "]", ",", "tokenidx", ":", "int", ")", "->", "Tuple", "[", "int", ",", "int", "]", ":", "\n", "    ", "\"\"\" Get sent idx and within sent idx from global token offset\n    :param sentence_end_pos: List containing (ex) global tokenoffsets of sentence ends. Length == number of sentences\n    :param tokenidx: Global token offset\n    :return: (sent_id, within_sent_tokenidx)\n    \"\"\"", "\n", "sentidx", "=", "get_closest_value", "(", "sentence_end_pos", ",", "tokenidx", ")", "\n", "if", "sentidx", "==", "0", ":", "\n", "        ", "return", "(", "sentidx", ",", "tokenidx", ")", "\n", "", "else", ":", "\n", "        ", "withinsentidx", "=", "tokenidx", "-", "sentence_end_pos", "[", "sentidx", "-", "1", "]", "\n", "\n", "return", "(", "sentidx", ",", "withinsentidx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.TAUtils.get_closest_value": [[201, 233], ["len", "print"], "function", ["None"], ["", "", "def", "get_closest_value", "(", "arr", ",", "target", ")", ":", "\n", "    ", "arr", "=", "[", "(", "i", "-", "1", ")", "for", "i", "in", "arr", "]", "\n", "n", "=", "len", "(", "arr", ")", "\n", "left", "=", "0", "\n", "right", "=", "n", "-", "1", "\n", "mid", "=", "0", "\n", "\n", "if", "target", ">", "arr", "[", "-", "1", "]", "or", "target", "<", "0", ":", "\n", "        ", "print", "(", "\"Token Offset not in range: target:{} max:{} \"", ".", "format", "(", "target", ",", "arr", "[", "-", "1", "]", ")", ")", "\n", "raise", "Exception", "\n", "\n", "# edge case - last or above all", "\n", "", "if", "target", "==", "arr", "[", "-", "1", "]", ":", "\n", "        ", "return", "n", "-", "1", "\n", "# edge case - first or below all", "\n", "", "if", "target", "<=", "arr", "[", "0", "]", ":", "\n", "        ", "return", "0", "\n", "# BSearch solution: Time & Space: Log(N)", "\n", "", "while", "left", "<", "right", ":", "\n", "        ", "mid", "=", "(", "left", "+", "right", ")", "//", "2", "# find the mid", "\n", "\n", "if", "target", "<", "arr", "[", "mid", "]", ":", "\n", "            ", "right", "=", "mid", "\n", "", "elif", "target", ">", "arr", "[", "mid", "]", ":", "\n", "            ", "left", "=", "mid", "+", "1", "\n", "", "else", ":", "\n", "            ", "return", "mid", "\n", "\n", "", "", "if", "arr", "[", "mid", "]", "<", "target", ":", "\n", "        ", "return", "mid", "+", "1", "\n", "", "else", ":", "\n", "        ", "return", "mid", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.makedir": [[10, 14], ["os.path.exists", "print", "os.makedirs"], "function", ["None"], ["def", "makedir", "(", "dirpath", ":", "str", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "dirpath", ")", ":", "\n", "        ", "print", "(", "f\"Making directory: {dirpath}\"", ")", "\n", "os", ".", "makedirs", "(", "dirpath", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.count_list": [[16, 29], ["len", "sum", "util.count_list"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.count_list"], ["", "", "def", "count_list", "(", "input_list", ":", "List", "[", "Any", "]", ",", "depth", ":", "int", "=", "1", ")", "->", "int", ":", "\n", "    ", "\"\"\"\n    Count the number of elements in a nested list\n    :param input_list: nested list\n    :param depth: depth of the list. depth = 1 indicates vanilla list\n    :return: int\n    \"\"\"", "\n", "num_elems", "=", "0", "\n", "if", "depth", "==", "1", ":", "\n", "        ", "return", "len", "(", "input_list", ")", "\n", "\n", "", "else", ":", "\n", "        ", "return", "sum", "(", "[", "count_list", "(", "l", ",", "depth", "-", "1", ")", "for", "l", "in", "input_list", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.pruneMultipleSpaces": [[31, 49], ["sentence.strip.strip", "sentence.strip.split", "len"], "function", ["None"], ["", "", "def", "pruneMultipleSpaces", "(", "sentence", ":", "str", ")", ":", "\n", "    ", "\"\"\" Prune multiple spaces in a sentence and replace with single space\n    Parameters:\n    -----------\n    sentence: Sentence string with multiple spaces\n\n    Returns:\n    --------\n    cleaned_sentence: String with only single spaces.\n    \"\"\"", "\n", "\n", "sentence", "=", "sentence", ".", "strip", "(", ")", "\n", "tokens", "=", "sentence", ".", "split", "(", "\" \"", ")", "\n", "tokens", "=", "[", "t", "for", "t", "in", "tokens", "if", "t", "!=", "\"\"", "]", "\n", "if", "len", "(", "tokens", ")", "==", "1", ":", "\n", "        ", "return", "tokens", "[", "0", "]", "\n", "", "else", ":", "\n", "        ", "return", "\" \"", ".", "join", "(", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.stopWords": [[51, 61], ["open", "open.readlines", "set", "w.strip", "w.strip"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.readlines"], ["", "", "def", "stopWords", "(", ")", ":", "\n", "    ", "\"\"\"Returns a set of stopwords.\"\"\"", "\n", "\n", "global", "stopwords", "\n", "if", "stopwords", "is", "None", ":", "\n", "        ", "f", "=", "open", "(", "\"utils/stopwords.txt\"", ",", "\"r\"", ")", "\n", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "words", "=", "[", "w", ".", "strip", "(", ")", "for", "w", "in", "lines", "if", "w", ".", "strip", "(", ")", "]", "\n", "stopwords", "=", "set", "(", "words", ")", "\n", "", "return", "stopwords", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.readlines": [[63, 69], ["open", "f.read().strip", "f.read().strip.split", "f.read"], "function", ["None"], ["", "def", "readlines", "(", "fp", ")", ":", "\n", "    ", "\"\"\" Read all lines from a filepath. \"\"\"", "\n", "with", "open", "(", "fp", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "text", "=", "f", ".", "read", "(", ")", ".", "strip", "(", ")", "\n", "lines", "=", "text", ".", "split", "(", "\"\\n\"", ")", "\n", "", "return", "lines", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.readJsonlDocs": [[71, 76], ["util.readlines", "json.loads"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.readlines"], ["", "def", "readJsonlDocs", "(", "jsonlfp", ":", "str", ")", "->", "List", "[", "Dict", "]", ":", "\n", "    ", "\"\"\" Read all docs from jsonl file. \"\"\"", "\n", "lines", "=", "readlines", "(", "jsonlfp", ")", "\n", "docs", "=", "[", "json", ".", "loads", "(", "line", ")", "for", "line", "in", "lines", "]", "\n", "return", "docs", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.isSpanOverlap": [[78, 91], ["max", "min"], "function", ["None"], ["", "def", "isSpanOverlap", "(", "s1", ",", "s2", ",", "srt_idx", "=", "0", ",", "end_idx", "=", "1", ",", "exclusive", "=", "True", ")", ":", "\n", "    ", "\"\"\" Returns True if the spans overlap. Works with exclusive end spans\n\n    s1, s2 : Tuples containing span start and end (exclusive)\n    srt_idx, end_idx: Idxs of span_srt and span_end in the input tuples\n    \"\"\"", "\n", "start1", ",", "end1", "=", "s1", "[", "srt_idx", "]", ",", "s1", "[", "end_idx", "]", "\n", "start2", ",", "end2", "=", "s2", "[", "srt_idx", "]", ",", "s2", "[", "end_idx", "]", "\n", "if", "not", "exclusive", ":", "\n", "        ", "end1", "-=", "1", "\n", "end2", "-=", "1", "\n", "\n", "", "return", "max", "(", "start1", ",", "start2", ")", "<=", "(", "min", "(", "end1", ",", "end2", ")", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.doSpansIntersect": [[93, 101], ["set", "set", "range", "range", "len", "set.intersection"], "function", ["None"], ["", "def", "doSpansIntersect", "(", "span1", ":", "Tuple", "[", "int", ",", "int", "]", ",", "span2", ":", "Tuple", "[", "int", ",", "int", "]", ")", "->", "bool", ":", "\n", "    ", "span1", "=", "set", "(", "range", "(", "span1", "[", "0", "]", ",", "span1", "[", "1", "]", ")", ")", "\n", "span2", "=", "set", "(", "range", "(", "span2", "[", "0", "]", ",", "span2", "[", "1", "]", ")", ")", "\n", "\n", "if", "len", "(", "span1", ".", "intersection", "(", "span2", ")", ")", ">", "0", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.getContiguousSpansOfElement": [[103, 132], ["enumerate", "spans.append", "spans.append"], "function", ["None"], ["", "", "def", "getContiguousSpansOfElement", "(", "l", ":", "List", "[", "Any", "]", ",", "element", ":", "Any", ")", "->", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ":", "\n", "    ", "\"\"\" Get contiguous spans of element in list\n\n    (start, end) -- start is inclusie, end exclusive\n\n    Returns:\n        List of (start, end) tuples\n    \"\"\"", "\n", "\n", "start", "=", "-", "1", "\n", "end", "=", "-", "1", "\n", "spans", "=", "[", "]", "\n", "\n", "for", "i", ",", "e", "in", "enumerate", "(", "l", ")", ":", "\n", "        ", "if", "e", "==", "element", ":", "\n", "            ", "if", "start", "==", "-", "1", ":", "\n", "                ", "start", "=", "i", "\n", "end", "=", "i", "+", "1", "\n", "", "else", ":", "\n", "                ", "end", "=", "i", "+", "1", "\n", "", "", "else", ":", "\n", "            ", "if", "start", "!=", "-", "1", ":", "\n", "                ", "spans", ".", "append", "(", "(", "start", ",", "end", ")", ")", "\n", "start", "=", "-", "1", "\n", "\n", "", "", "", "if", "start", "!=", "-", "1", ":", "\n", "        ", "spans", ".", "append", "(", "(", "start", ",", "end", ")", ")", "\n", "\n", "", "return", "spans", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.mostFreqKeysInDict": [[134, 146], ["util.sortDictByValue", "mostFreqElems.append"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.sortDictByValue"], ["", "def", "mostFreqKeysInDict", "(", "d", ":", "Dict", ")", "->", "List", "[", "Any", "]", ":", "\n", "    ", "\"\"\" d is a dict with values as frequencies \"\"\"", "\n", "sortedDict", "=", "sortDictByValue", "(", "d", ",", "True", ")", "\n", "maxVal", "=", "sortedDict", "[", "0", "]", "[", "1", "]", "\n", "mostFreqElems", "=", "[", "]", "\n", "for", "elem", ",", "frq", "in", "sortedDict", ":", "\n", "        ", "if", "frq", "==", "maxVal", ":", "\n", "            ", "mostFreqElems", ".", "append", "(", "elem", ")", "\n", "", "else", ":", "\n", "            ", "break", "\n", "\n", "", "", "return", "mostFreqElems", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.sortDictByValue": [[148, 150], ["sorted", "d.items"], "function", ["None"], ["", "def", "sortDictByValue", "(", "d", ",", "decreasing", "=", "False", ")", ":", "\n", "    ", "return", "sorted", "(", "d", ".", "items", "(", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "decreasing", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.sortDictByKey": [[152, 154], ["sorted", "d.items"], "function", ["None"], ["", "def", "sortDictByKey", "(", "d", ",", "decreasing", "=", "False", ")", ":", "\n", "    ", "return", "sorted", "(", "d", ".", "items", "(", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "decreasing", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.getContextAroundSpan": [[156, 179], ["max", "min", "len", "len"], "function", ["None"], ["", "def", "getContextAroundSpan", "(", "seq", ":", "List", "[", "Any", "]", ",", "span", ":", "Tuple", "[", "int", ",", "int", "]", ",", "context_len", ":", "int", ")", "->", "Tuple", "[", "List", "[", "Any", "]", ",", "List", "[", "Any", "]", "]", ":", "\n", "    ", "\"\"\" For a given seq of items and a span in, Return the left and right context\n\n    Args:\n        seq: input list of items\n        span: Start/End positions of span (inclusive/exclusive)\n        context_len: length of context on each side\n\n    Returns:\n        (left_context, right_context): list of context items. Max length of both is context_len\n    \"\"\"", "\n", "assert", "context_len", ">", "0", ",", "f\"Context length cannot be <= 0:  {context_len}\"", "\n", "assert", "span", "[", "0", "]", "!=", "span", "[", "1", "]", ",", "f\"Span length cannot be zero: {span}\"", "\n", "assert", "span", "[", "0", "]", ">=", "0", ",", "f\"Span start invalid: {span}\"", "\n", "assert", "span", "[", "1", "]", "<=", "len", "(", "seq", ")", ",", "f\"Span end invalid: {span}\"", "\n", "\n", "left_start_index", "=", "max", "(", "0", ",", "span", "[", "0", "]", "-", "context_len", ")", "\n", "right_end_index", "=", "min", "(", "len", "(", "seq", ")", ",", "span", "[", "1", "]", "+", "context_len", ")", "\n", "\n", "left_context", "=", "seq", "[", "left_start_index", ":", "span", "[", "0", "]", "]", "\n", "right_context", "=", "seq", "[", "span", "[", "1", "]", ":", "right_end_index", "]", "\n", "\n", "return", "(", "left_context", ",", "right_context", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.getMatchingSubSpans": [[181, 196], ["list", "util._KnuthMorrisPratt", "len", "zip"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util._KnuthMorrisPratt"], ["", "def", "getMatchingSubSpans", "(", "seq", ":", "List", "[", "Any", "]", ",", "pattern", ":", "List", "[", "Any", "]", ")", "->", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ":", "\n", "    ", "\"\"\" Returns the (start, end) spans of the pattern in the text list\n\n    Like always, end is exclusive\n\n    text: [5,1,5,1,1,2,1,1,2,1,2,1]\n    pattern: [2,1]\n    Return: [(5,7), (8, 10), (10, 12)]\n    \"\"\"", "\n", "\n", "startPositions", "=", "list", "(", "_KnuthMorrisPratt", "(", "seq", ",", "pattern", ")", ")", "\n", "endPositions", "=", "[", "spos", "+", "len", "(", "pattern", ")", "for", "spos", "in", "startPositions", "]", "\n", "matchingSpans", "=", "[", "(", "s", ",", "e", ")", "for", "s", ",", "e", "in", "zip", "(", "startPositions", ",", "endPositions", ")", "]", "\n", "\n", "return", "matchingSpans", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util._KnuthMorrisPratt": [[198, 231], ["list", "range", "len", "len", "len", "len"], "function", ["None"], ["", "def", "_KnuthMorrisPratt", "(", "text", ",", "pattern", ")", ":", "\n", "# Knuth-Morris-Pratt string matching", "\n", "# David Eppstein, UC Irvine, 1 Mar 2002", "\n", "\n", "# from http://code.activestate.com/recipes/117214/", "\n", "    ", "\"\"\"Yields all starting positions of copies of the pattern in the text.\n    Calling conventions are similar to string.find, but its arguments can be\n    lists or iterators, not just strings, it returns all matches, not just\n    the first one, and it does not need the whole text in memory at once.\n    Whenever it yields, it will have read the text exactly up to and including\n    the match that caused the yield.\"\"\"", "\n", "\n", "# allow indexing into pattern and protect against change during yield", "\n", "pattern", "=", "list", "(", "pattern", ")", "\n", "\n", "# build table of shift amounts", "\n", "shifts", "=", "[", "1", "]", "*", "(", "len", "(", "pattern", ")", "+", "1", ")", "\n", "shift", "=", "1", "\n", "for", "pos", "in", "range", "(", "len", "(", "pattern", ")", ")", ":", "\n", "        ", "while", "shift", "<=", "pos", "and", "pattern", "[", "pos", "]", "!=", "pattern", "[", "pos", "-", "shift", "]", ":", "\n", "            ", "shift", "+=", "shifts", "[", "pos", "-", "shift", "]", "\n", "", "shifts", "[", "pos", "+", "1", "]", "=", "shift", "\n", "\n", "# do the actual search", "\n", "", "startPos", "=", "0", "\n", "matchLen", "=", "0", "\n", "for", "c", "in", "text", ":", "\n", "        ", "while", "matchLen", "==", "len", "(", "pattern", ")", "or", "matchLen", ">=", "0", "and", "pattern", "[", "matchLen", "]", "!=", "c", ":", "\n", "            ", "startPos", "+=", "shifts", "[", "matchLen", "]", "\n", "matchLen", "-=", "shifts", "[", "matchLen", "]", "\n", "", "matchLen", "+=", "1", "\n", "if", "matchLen", "==", "len", "(", "pattern", ")", ":", "\n", "            ", "yield", "startPos", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.normalizeD1byD2": [[233, 238], ["dict1.items", "float", "float"], "function", ["None"], ["", "", "", "def", "normalizeD1byD2", "(", "dict1", ",", "dict2", ")", ":", "\n", "    ", "d", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "dict1", ".", "items", "(", ")", ":", "\n", "        ", "d", "[", "k", "]", "=", "float", "(", "v", ")", "/", "float", "(", "dict2", "[", "k", "]", ")", "\n", "", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.normalizeDictbyK": [[240, 245], ["dict1.items", "float"], "function", ["None"], ["", "def", "normalizeDictbyK", "(", "dict1", ",", "constant", ")", ":", "\n", "    ", "d", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "dict1", ".", "items", "(", ")", ":", "\n", "        ", "d", "[", "k", "]", "=", "float", "(", "v", ")", "/", "constant", "\n", "", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.topKindices": [[247, 250], ["sorted", "range", "len"], "function", ["None"], ["", "def", "topKindices", "(", "l", ":", "List", ",", "k", "=", "1", ")", ":", "\n", "    ", "\"\"\" Return the indices of the top-K values in the list \"\"\"", "\n", "return", "sorted", "(", "range", "(", "len", "(", "l", ")", ")", ",", "key", "=", "lambda", "i", ":", "l", "[", "i", "]", ")", "[", ":", ":", "-", "1", "]", "[", "0", ":", "k", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all": [[252, 267], ["isinstance", "isinstance", "isinstance", "isinstance", "tuple", "round", "stuff.items", "util.round_all", "float", "round", "util.round_all"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all"], ["", "def", "round_all", "(", "stuff", ",", "prec", ")", ":", "\n", "    ", "\"\"\" Round all the number elems in nested stuff. \"\"\"", "\n", "if", "isinstance", "(", "stuff", ",", "list", ")", ":", "\n", "        ", "return", "[", "round_all", "(", "x", ",", "prec", ")", "for", "x", "in", "stuff", "]", "\n", "", "if", "isinstance", "(", "stuff", ",", "tuple", ")", ":", "\n", "        ", "return", "tuple", "(", "round_all", "(", "x", ",", "prec", ")", "for", "x", "in", "stuff", ")", "\n", "", "if", "isinstance", "(", "stuff", ",", "float", ")", ":", "\n", "        ", "return", "round", "(", "float", "(", "stuff", ")", ",", "prec", ")", "\n", "", "if", "isinstance", "(", "stuff", ",", "dict", ")", ":", "\n", "        ", "d", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "stuff", ".", "items", "(", ")", ":", "\n", "            ", "d", "[", "k", "]", "=", "round", "(", "v", ",", "prec", ")", "\n", "", "return", "d", "\n", "", "else", ":", "\n", "        ", "return", "stuff", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.removeOverlappingSpans": [[269, 302], ["sorted", "range", "len", "len", "max", "util.removeOverlappingSpans.spanOverlap"], "function", ["None"], ["", "", "def", "removeOverlappingSpans", "(", "spans", ")", ":", "\n", "    ", "\"\"\"\n    Remove overlapping spans by keeping the longest ones. Span end are exclusive\n\n    spans: List of (start, end) tuples\n    \"\"\"", "\n", "\n", "def", "spanOverlap", "(", "s1", ",", "s2", ")", ":", "\n", "        ", "\"\"\" Works with exclusive end spans \"\"\"", "\n", "start1", ",", "end1", "=", "s1", "\n", "start2", ",", "end2", "=", "s2", "\n", "return", "max", "(", "start1", ",", "start2", ")", "<=", "(", "min", "(", "end1", ",", "end2", ")", "-", "1", ")", "\n", "\n", "", "if", "len", "(", "spans", ")", "==", "0", ":", "\n", "        ", "return", "spans", "\n", "# Spans sorted by increasing order", "\n", "", "sorted_spans", "=", "sorted", "(", "spans", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "\n", "final_spans", "=", "[", "sorted_spans", "[", "0", "]", "]", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "sorted_spans", ")", ")", ":", "\n", "        ", "last_span", "=", "final_spans", "[", "-", "1", "]", "\n", "span", "=", "sorted_spans", "[", "i", "]", "\n", "if", "not", "spanOverlap", "(", "last_span", ",", "span", ")", ":", "\n", "            ", "final_spans", ".", "append", "(", "span", ")", "\n", "", "else", ":", "\n", "            ", "len1", "=", "last_span", "[", "1", "]", "-", "last_span", "[", "0", "]", "\n", "len2", "=", "span", "[", "1", "]", "-", "span", "[", "0", "]", "\n", "# If incoming span is longer, delete last span and put", "\n", "if", "len2", ">", "len1", ":", "\n", "                ", "final_spans", ".", "pop", "(", ")", "\n", "final_spans", ".", "append", "(", "span", ")", "\n", "\n", "", "", "", "return", "final_spans", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList": [[304, 308], ["isinstance", "var.detach().cpu().numpy().tolist", "var.detach().cpu().numpy", "var.detach().cpu", "var.detach"], "function", ["None"], ["", "def", "tocpuNPList", "(", "var", ")", ":", "\n", "    ", "if", "isinstance", "(", "var", ",", "float", ")", ":", "\n", "        ", "return", "var", "\n", "", "return", "var", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.mergeSpansAndRemoveOverlap": [[310, 379], ["sorted", "sorted", "sorted.extend", "sorted", "len", "len", "len", "len", "spans_to_add.append", "util.isSpanOverlap", "spans_to_add.append"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.isSpanOverlap"], ["", "def", "mergeSpansAndRemoveOverlap", "(", "orig_spans", ",", "new_spans", ",", "srt_idx", ",", "end_idx", ",", "exclusive", "=", "True", ")", ":", "\n", "    ", "\"\"\" Merge a list of spans in another given list resulting in non-overlapping spans.\n    Assumes that both span lists are independently non-overlapping.\n\n    While merging, if incoming span overlaps, keep the original.\n\n    Parameters:\n    -----------\n    orig_spans: List of (..., start, ..., end, ...) tuples. Original spans\n    new_spans: New spans. Same as above\n    srt_idx: Index of span_srt in the span tuple\n    end_idx: Index of span_end in the span tuple\n    exclusive: Bool flag to indicate if the span end in exclusive or inclusive\n\n    Returns:\n    --------\n    final_spans: List of merged spans sorted by span start.\n\n    \"\"\"", "\n", "\n", "if", "len", "(", "orig_spans", ")", "==", "0", ":", "\n", "        ", "return", "new_spans", "\n", "", "if", "len", "(", "new_spans", ")", "==", "0", ":", "\n", "        ", "return", "orig_spans", "\n", "\n", "# Spans sorted by increasing order", "\n", "", "sorted_orig_spans", "=", "sorted", "(", "orig_spans", ",", "key", "=", "lambda", "x", ":", "x", "[", "srt_idx", "]", ")", "\n", "sorted_new_spans", "=", "sorted", "(", "new_spans", ",", "key", "=", "lambda", "x", ":", "x", "[", "srt_idx", "]", ")", "\n", "\n", "# These will act as the head pointers in the two lists", "\n", "orig_span_idx", "=", "0", "\n", "new_span_idx", "=", "0", "\n", "\n", "spans_to_add", "=", "[", "]", "\n", "\n", "while", "True", ":", "\n", "# No new spans to merge", "\n", "        ", "if", "new_span_idx", "==", "len", "(", "sorted_new_spans", ")", ":", "\n", "            ", "break", "\n", "\n", "# Original List is done", "\n", "", "if", "orig_span_idx", "==", "len", "(", "sorted_orig_spans", ")", ":", "\n", "            ", "spans_to_add", ".", "append", "(", "sorted_new_spans", "[", "new_span_idx", "]", ")", "\n", "new_span_idx", "+=", "1", "\n", "", "else", ":", "\n", "            ", "orig_span", "=", "sorted_orig_spans", "[", "orig_span_idx", "]", "\n", "new_span", "=", "sorted_new_spans", "[", "new_span_idx", "]", "\n", "\n", "# Spans overlap, move head to the next new span", "\n", "if", "isSpanOverlap", "(", "orig_span", ",", "new_span", ",", "srt_idx", ",", "end_idx", ",", "exclusive", "=", "exclusive", ")", ":", "\n", "                ", "new_span_idx", "+=", "1", "\n", "", "else", ":", "\n", "# If new span starts after the current original span's end, then move the current original span head", "\n", "                ", "if", "new_span", "[", "srt_idx", "]", ">=", "orig_span", "[", "end_idx", "]", ":", "\n", "                    ", "orig_span_idx", "+=", "1", "\n", "# continue", "\n", "\n", "# New span ends before the current head start", "\n", "# Previous condition ensures that the new_span_start is after the previous orig_span_end", "\n", "# Hence this span doesn't overlap with original spans ==> merge", "\n", "", "if", "new_span", "[", "end_idx", "]", "<=", "orig_span", "[", "srt_idx", "]", ":", "\n", "                    ", "spans_to_add", ".", "append", "(", "new_span", ")", "\n", "new_span_idx", "+=", "1", "\n", "# continue", "\n", "\n", "", "", "", "", "sorted_orig_spans", ".", "extend", "(", "spans_to_add", ")", "\n", "sorted_orig_spans", "=", "sorted", "(", "sorted_orig_spans", ",", "key", "=", "lambda", "x", ":", "x", "[", "srt_idx", "]", ")", "\n", "\n", "return", "sorted_orig_spans", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.cleanMentionSurface": [[381, 383], ["util._getLnrm"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util._getLnrm"], ["", "def", "cleanMentionSurface", "(", "arg", ")", ":", "\n", "    ", "return", "_getLnrm", "(", "arg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util._getLnrm": [[385, 393], ["arg.lower.lower", "unicodedata.normalize", "unicodedata.category", "set"], "function", ["None"], ["", "def", "_getLnrm", "(", "arg", ")", ":", "\n", "    ", "\"\"\"Normalizes the given arg by stripping it of diacritics, lowercasing, and\n    removing all non-alphanumeric characters.\n    \"\"\"", "\n", "arg", "=", "\"\"", ".", "join", "(", "[", "c", "for", "c", "in", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "arg", ")", "if", "unicodedata", ".", "category", "(", "c", ")", "!=", "\"Mn\"", "]", ")", "\n", "arg", "=", "arg", ".", "lower", "(", ")", "\n", "arg", "=", "\"\"", ".", "join", "(", "[", "c", "for", "c", "in", "arg", "if", "c", "in", "set", "(", "\"abcdefghijklmnopqrstuvwxyz0123456789\"", ")", "]", ")", "\n", "return", "arg", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.getGlobalTokenOffset": [[395, 411], ["enumerate", "enumerate"], "function", ["None"], ["", "def", "getGlobalTokenOffset", "(", "sentences", ":", "List", "[", "List", "[", "str", "]", "]", ")", ":", "\n", "    ", "\"\"\"For a tokenized doc, get mapping from sent_idx, within_tokenidx to global token idx\n\n    Returns:\n        Dict[(int, int): int]: (sent_idx, within_sent_tokenidx) ---> global token idx\n    \"\"\"", "\n", "\n", "sentIdxTokenIdx2GlobalTokenIdx", "=", "{", "}", "\n", "\n", "globaltokidx", "=", "0", "\n", "for", "sentidx", ",", "sent", "in", "enumerate", "(", "sentences", ")", ":", "\n", "        ", "for", "tokenidx", ",", "_", "in", "enumerate", "(", "sent", ")", ":", "\n", "            ", "sentIdxTokenIdx2GlobalTokenIdx", "[", "(", "sentidx", ",", "tokenidx", ")", "]", "=", "globaltokidx", "\n", "globaltokidx", "+=", "1", "\n", "\n", "", "", "return", "sentIdxTokenIdx2GlobalTokenIdx", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.WhitespaceTokenizer.__init__": [[9, 11], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "vocab", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.WhitespaceTokenizer.__call__": [[12, 17], ["text.split", "spacy.tokens.Doc", "len"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "text", ")", ":", "\n", "        ", "words", "=", "text", ".", "split", "(", "\" \"", ")", "\n", "# All tokens 'own' a subsequent space character in this tokenizer", "\n", "spaces", "=", "[", "True", "]", "*", "len", "(", "words", ")", "\n", "return", "Doc", "(", "self", ".", "vocab", ",", "words", "=", "words", ",", "spaces", "=", "spaces", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getWhiteTokenizerSpacyNLP": [[19, 23], ["spacyutils.getSpacyNLP", "spacyutils.WhitespaceTokenizer"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getSpacyNLP"], ["", "", "def", "getWhiteTokenizerSpacyNLP", "(", "disable_list", ":", "List", "[", "str", "]", "=", "[", "\"textcat\"", "]", ")", ":", "\n", "    ", "nlp", "=", "getSpacyNLP", "(", "disable_list", ")", "\n", "nlp", ".", "tokenizer", "=", "WhitespaceTokenizer", "(", "nlp", ".", "vocab", ")", "\n", "return", "nlp", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getSpacyNLP": [[25, 29], ["spacy.load"], "function", ["None"], ["", "def", "getSpacyNLP", "(", "disable_list", ":", "List", "[", "str", "]", "=", "[", "\"textcat\"", "]", ")", ":", "\n", "# nlp = spacy.load('en', disable=disable_list)", "\n", "    ", "nlp", "=", "spacy", ".", "load", "(", "\"en_core_web_lg\"", ",", "disable", "=", "disable_list", ")", "\n", "return", "nlp", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getSpacyDocs": [[31, 34], ["list", "nlp.pipe"], "function", ["None"], ["", "def", "getSpacyDocs", "(", "sents", ":", "List", "[", "str", "]", ",", "nlp", ")", ":", "\n", "    ", "\"\"\" Batch processing of sentences into Spacy docs.\"\"\"", "\n", "return", "list", "(", "nlp", ".", "pipe", "(", "sents", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getSpacyDoc": [[36, 39], ["nlp"], "function", ["None"], ["", "def", "getSpacyDoc", "(", "sent", ":", "str", ",", "nlp", ")", "->", "Doc", ":", "\n", "    ", "\"\"\" Single sent to Spacy doc \"\"\"", "\n", "return", "nlp", "(", "sent", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getNER": [[41, 50], ["ner_tags.append"], "function", ["None"], ["", "def", "getNER", "(", "spacydoc", ":", "Doc", ")", "->", "List", "[", "Tuple", "[", "str", ",", "int", ",", "int", ",", "str", "]", "]", ":", "\n", "    ", "\"\"\"Returns a list of (ner_text, ner_start, ner_end, ner_label). ner_end is exclusive. \"\"\"", "\n", "assert", "spacydoc", ".", "is_tagged", "is", "True", ",", "\"NER needs to run.\"", "\n", "\n", "ner_tags", "=", "[", "]", "\n", "for", "ent", "in", "spacydoc", ".", "ents", ":", "\n", "        ", "ner_tags", ".", "append", "(", "(", "ent", ".", "text", ",", "ent", ".", "start", ",", "ent", ".", "end", ",", "ent", ".", "label_", ")", ")", "\n", "\n", "", "return", "ner_tags", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getPropnSpans": [[52, 61], ["spacyutils.getPOSTags", "utils.util.getContiguousSpansOfElement"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getPOSTags", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.getContiguousSpansOfElement"], ["", "def", "getPropnSpans", "(", "spacydoc", ":", "Doc", ")", "->", "List", "[", "Tuple", "[", "str", ",", "int", ",", "int", ",", "str", "]", "]", ":", "\n", "    ", "pos_tags", "=", "getPOSTags", "(", "spacydoc", ")", "\n", "propn_span_srtend", "=", "util", ".", "getContiguousSpansOfElement", "(", "pos_tags", ",", "\"PROPN\"", ")", "\n", "propn_spans", "=", "[", "\n", "(", "spacydoc", "[", "propnspan", "[", "0", "]", ":", "propnspan", "[", "1", "]", "]", ".", "text", ",", "propnspan", "[", "0", "]", ",", "propnspan", "[", "1", "]", ",", "\"PROPN\"", ")", "\n", "for", "propnspan", "in", "propn_span_srtend", "\n", "]", "\n", "\n", "return", "propn_spans", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getNER_and_PROPN": [[63, 88], ["spacyutils.getNER", "spacyutils.getPOSTags", "utils.util.getContiguousSpansOfElement", "getNER.append", "utils.util.doSpansIntersect", "propn_spans_tokeep.append"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getNER", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getPOSTags", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.getContiguousSpansOfElement", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.doSpansIntersect"], ["", "def", "getNER_and_PROPN", "(", "spacydoc", ":", "Doc", ")", "->", "List", "[", "Tuple", "[", "str", ",", "int", ",", "int", ",", "str", "]", "]", ":", "\n", "    ", "\"\"\"Returns a list of (ner_text, ner_start, ner_end, ner_label). ner_end is exclusive.\n    This also includes PROPN spans that are not part of a NER\n    \"\"\"", "\n", "ner_tags", "=", "getNER", "(", "spacydoc", ")", "\n", "ner_spans", "=", "[", "(", "x", ",", "y", ")", "for", "(", "_", ",", "x", ",", "y", ",", "_", ")", "in", "ner_tags", "]", "\n", "\n", "pos_tags", "=", "getPOSTags", "(", "spacydoc", ")", "\n", "propn_spans", "=", "util", ".", "getContiguousSpansOfElement", "(", "pos_tags", ",", "\"PROPN\"", ")", "\n", "\n", "propn_spans_tokeep", "=", "[", "]", "\n", "for", "propnspan", "in", "propn_spans", ":", "\n", "        ", "add_propn", "=", "True", "\n", "for", "nerspan", "in", "ner_spans", ":", "\n", "            ", "if", "util", ".", "doSpansIntersect", "(", "propnspan", ",", "nerspan", ")", ":", "\n", "                ", "add_propn", "=", "False", "\n", "break", "\n", "\n", "", "", "if", "add_propn", ":", "\n", "            ", "propn_spans_tokeep", ".", "append", "(", "propnspan", ")", "\n", "\n", "", "", "for", "propnspan", "in", "propn_spans_tokeep", ":", "\n", "        ", "ner_tags", ".", "append", "(", "(", "spacydoc", "[", "propnspan", "[", "0", "]", ":", "propnspan", "[", "1", "]", "]", ".", "text", ",", "propnspan", "[", "0", "]", ",", "propnspan", "[", "1", "]", ",", "\"PROPN\"", ")", ")", "\n", "\n", "", "return", "ner_tags", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getPOSTags": [[90, 94], ["None"], "function", ["None"], ["", "def", "getPOSTags", "(", "spacydoc", ":", "Doc", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "\"\"\" Returns a list of POS tags for the doc. \"\"\"", "\n", "pos_tags", "=", "[", "token", ".", "pos_", "for", "token", "in", "spacydoc", "]", "\n", "return", "pos_tags", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getTokens": [[96, 99], ["None"], "function", ["None"], ["", "def", "getTokens", "(", "spacydoc", ":", "Doc", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "tokens", "=", "[", "token", ".", "text", "for", "token", "in", "spacydoc", "]", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getWhiteSpacedSent": [[101, 105], ["spacyutils.getTokens"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getTokens"], ["", "def", "getWhiteSpacedSent", "(", "spacydoc", ":", "Doc", ")", "->", "str", ":", "\n", "    ", "\"\"\"Return a whitespaced delimited spacydoc. \"\"\"", "\n", "tokens", "=", "getTokens", "(", "spacydoc", ")", "\n", "return", "\" \"", ".", "join", "(", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getAll_SentIdAndTokenOffset": [[107, 123], ["len", "range", "tokenIdxs.append"], "function", ["None"], ["", "def", "getAll_SentIdAndTokenOffset", "(", "spacydoc", ":", "Doc", ")", "->", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ":", "\n", "    ", "\"\"\"Get (sentence idx, withinSentOffset) for all tokens.\"\"\"", "\n", "numTokens", "=", "len", "(", "spacydoc", ")", "\n", "tokenIdxs", "=", "[", "]", "\n", "sentence_end_pos", "=", "[", "sent", ".", "end", "for", "sent", "in", "spacydoc", ".", "sents", "]", "\n", "sent_idx", "=", "0", "\n", "withinsent_tokenidx", "=", "0", "\n", "\n", "for", "i", "in", "range", "(", "0", ",", "numTokens", ")", ":", "\n", "        ", "if", "i", "==", "sentence_end_pos", "[", "sent_idx", "]", ":", "\n", "            ", "sent_idx", "+=", "1", "\n", "withinsent_tokenidx", "=", "0", "\n", "\n", "", "tokenIdxs", ".", "append", "(", "(", "sent_idx", ",", "withinsent_tokenidx", ")", ")", "\n", "withinsent_tokenidx", "+=", "1", "\n", "", "return", "tokenIdxs", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getSpanHead": [[125, 141], ["len"], "function", ["None"], ["", "def", "getSpanHead", "(", "doc", ":", "Doc", ",", "span", ":", "Tuple", "[", "int", ",", "int", "]", ")", ":", "\n", "    ", "\"\"\"\n    Returns token idx of the span root.\n    :param doc: Spacy doc\n    :param span_srt: Span start\n    :param span_end: Span end (exclusive)\n    :return: Token idx of the span head\n    \"\"\"", "\n", "assert", "doc", ".", "is_parsed", ",", "\"Doc isn't dep parsed.\"", "\n", "doclength", "=", "len", "(", "doc", ")", "\n", "(", "span_srt", ",", "span_end", ")", "=", "span", "\n", "assert", "(", "span_srt", ">=", "0", ")", "and", "(", "span_srt", "<", "doclength", ")", "\n", "assert", "(", "span_end", ">", "0", ")", "and", "(", "span_end", "<=", "doclength", ")", "\n", "span", ":", "Span", "=", "doc", "[", "span_srt", ":", "span_end", "]", "\n", "spanroot", ":", "Token", "=", "span", ".", "root", "\n", "return", "spanroot", ".", "i", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getNERInToken": [[143, 164], ["print"], "function", ["None"], ["", "def", "getNERInToken", "(", "doc", ":", "Doc", ",", "token_idx", ":", "int", ")", ":", "\n", "    ", "\"\"\"\n    If the given token is a part of NE, return the NE span, otherwise the input token's span\n    :param doc: Spacy doc\n    :param token_idx: int idx of the token\n    :return: (srt-inclusive, end-exclusive)  of the NER (if matches) else (token_idx, token_idx + 1)\n    \"\"\"", "\n", "token", ":", "Token", "=", "doc", "[", "token_idx", "]", "\n", "ner_spans", "=", "[", "(", "ent", ".", "start", ",", "ent", ".", "end", ")", "for", "ent", "in", "doc", ".", "ents", "]", "\n", "\n", "if", "token", ".", "ent_iob_", "==", "\"O\"", ":", "\n", "# Input token is not a NER", "\n", "        ", "return", "(", "token_idx", ",", "token_idx", "+", "1", ")", "\n", "", "else", ":", "\n", "# Token is an NER, find which span", "\n", "# NER spans (srt, end) are in increasing order", "\n", "        ", "for", "(", "srt", ",", "end", ")", "in", "ner_spans", ":", "\n", "            ", "if", "token_idx", ">=", "srt", "and", "token_idx", "<", "end", ":", "\n", "                ", "return", "(", "srt", ",", "end", ")", "\n", "", "", "", "print", "(", "\"I SHOULDN'T BE HERE\"", ")", "\n", "return", "(", "token_idx", ",", "token_idx", "+", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.sizes.NPStats": [[7, 19], ["None"], "function", ["None"], ["def", "NPStats", "(", "nps", ":", "List", "[", "Tuple", "[", "int", "]", "]", ")", "->", "Tuple", "[", "int", ",", "int", "]", ":", "\n", "    ", "\"\"\" Get number of nps and maxlen of a NP span\"\"\"", "\n", "num_nps", "=", "0", "\n", "maxlen", "=", "0", "\n", "\n", "for", "sent", "in", "nps", ":", "\n", "        ", "for", "np", "in", "sent", ":", "\n", "            ", "num_nps", "+=", "1", "\n", "nplen", "=", "np", "[", "1", "]", "-", "np", "[", "0", "]", "\n", "maxlen", "=", "nplen", "if", "maxlen", "<", "nplen", "else", "maxlen", "\n", "\n", "", "", "return", "(", "num_nps", ",", "maxlen", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.sizes.dataStats": [[21, 92], ["print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "open", "float", "float", "float", "float", "json.loads", "len", "len", "enumerate", "sizes.NPStats", "line.strip", "len", "len", "len", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.sizes.NPStats"], ["", "def", "dataStats", "(", "input_jsonl", ":", "str", ",", "sent_key", ":", "str", ",", "nps_key", ":", "str", ")", "->", "None", ":", "\n", "    ", "print", "(", "\"Reading dataset: {}\"", ".", "format", "(", "input_jsonl", ")", ")", "\n", "\n", "maxDocSents", "=", "-", "1", "\n", "maxSenLength", "=", "-", "1", "\n", "numSents", "=", "0", "\n", "maxDocWords", "=", "0", "\n", "minDocWords", "=", "100000", "\n", "\n", "longestsentence", "=", "[", "]", "\n", "smallestDoc", "=", "\"\"", "\n", "\n", "avgDocWords", "=", "0", "\n", "maxDocLenIdx", "=", "0", "\n", "minDocLenIdx", "=", "0", "\n", "num_NPs", "=", "0", "\n", "maxNP_perdoc", "=", "0", "\n", "maxLenNP", "=", "0", "\n", "\n", "docidx", "=", "0", "\n", "\n", "with", "open", "(", "input_jsonl", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "if", "not", "line", ".", "strip", "(", ")", ":", "\n", "                ", "continue", "\n", "", "doc", "=", "json", ".", "loads", "(", "line", ")", "\n", "sentences", "=", "doc", "[", "sent_key", "]", "\n", "\n", "numSents", "+=", "len", "(", "sentences", ")", "\n", "if", "len", "(", "sentences", ")", ">", "maxDocSents", ":", "\n", "                ", "maxDocSents", "=", "len", "(", "sentences", ")", "\n", "\n", "", "sent_lens", "=", "[", "len", "(", "s", ")", "for", "s", "in", "sentences", "]", "\n", "\n", "tokens", "=", "[", "word", "for", "sent", "in", "sentences", "for", "word", "in", "sent", "]", "\n", "avgDocWords", "+=", "len", "(", "tokens", ")", "\n", "if", "len", "(", "tokens", ")", ">", "maxDocWords", ":", "\n", "                ", "maxDocWords", "=", "len", "(", "tokens", ")", "\n", "maxDocLenIdx", "=", "docidx", "\n", "\n", "", "if", "len", "(", "tokens", ")", "<", "minDocWords", ":", "\n", "                ", "minDocWords", "=", "len", "(", "tokens", ")", "\n", "smallestDoc", "=", "doc", "\n", "minDocLenIdx", "=", "docidx", "\n", "\n", "", "for", "i", ",", "lens", "in", "enumerate", "(", "sent_lens", ")", ":", "\n", "                ", "if", "lens", ">", "maxSenLength", ":", "\n", "                    ", "maxSenLength", "=", "lens", "\n", "longestsentence", "=", "sentences", "[", "i", "]", "\n", "\n", "", "", "(", "numnp", ",", "maxlen_np", ")", "=", "NPStats", "(", "nps", "=", "doc", "[", "nps_key", "]", ")", "\n", "num_NPs", "+=", "numnp", "\n", "maxLenNP", "=", "maxlen_np", "if", "maxlen_np", ">", "maxLenNP", "else", "maxLenNP", "\n", "maxNP_perdoc", "=", "numnp", "if", "numnp", ">", "maxNP_perdoc", "else", "maxNP_perdoc", "\n", "docidx", "+=", "1", "\n", "\n", "", "", "avgDocWords", "=", "float", "(", "avgDocWords", ")", "/", "float", "(", "docidx", ")", "\n", "avgNPPerdoc", "=", "float", "(", "num_NPs", ")", "/", "float", "(", "docidx", ")", "\n", "print", "(", "\"MaxDocSents: {}\"", ".", "format", "(", "maxDocSents", ")", ")", "\n", "print", "(", "\"maxSenLens: {}\"", ".", "format", "(", "maxSenLength", ")", ")", "\n", "print", "(", "\"numSents: {}\"", ".", "format", "(", "numSents", ")", ")", "\n", "print", "(", "\"\\n\"", ")", "\n", "print", "(", "\"maxDocWords: {} idx: {}\"", ".", "format", "(", "maxDocWords", ",", "maxDocLenIdx", ")", ")", "\n", "print", "(", "\"minDocWords: {} idx: {}\"", ".", "format", "(", "minDocWords", ",", "minDocLenIdx", ")", ")", "\n", "print", "(", "\"avgDocWords: {}\"", ".", "format", "(", "avgDocWords", ")", ")", "\n", "# print(\"Smallest Doc: \\n {}\".format(smallestDoc))", "\n", "print", "(", "\"\\n\"", ")", "\n", "print", "(", "f\"avgNum NPs per doc: {avgNPPerdoc}\"", ")", "\n", "print", "(", "f\"Max num of NP in a doc: {maxNP_perdoc}\"", ")", "\n", "print", "(", "f\"totalNum NPs: {num_NPs}\"", ")", "\n", "print", "(", "f\"Max Lenof NP: {maxLenNP}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.sizes.main": [[94, 97], ["sizes.dataStats"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.sizes.dataStats"], ["", "def", "main", "(", "args", ")", ":", "\n", "\n", "    ", "dataStats", "(", "input_jsonl", "=", "args", ".", "input_jsonl", ",", "sent_key", "=", "args", ".", "sent_key", ",", "nps_key", "=", "args", ".", "nps_key", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.sortDocs._writeSortedDocs": [[9, 48], ["os.path.exists", "print", "print", "print", "print", "open", "f.readlines", "sum", "open", "json.loads", "sorted", "outf.write", "outf.write", "len", "zip", "json.dumps"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.readlines", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["def", "_writeSortedDocs", "(", "input_jsonl", ":", "str", ",", "output_jsonl", ":", "str", ",", "sort_key", ":", "str", ",", "reverse", ":", "bool", ")", "->", "None", ":", "\n", "    ", "\"\"\" Converts WDW jsonl files from xmlReader to tokenized, NP chunked jsonl files.\n\n    Json keys:\n    qid, contextId -- strings\n    qleftContext, qrightContext -- list of tokens (Each considered as a single sentence)\n    contextPara -- list of list of tokens\n    correctChoice -- list of tokens\n    candidateChoices -- list of list of tokens\n    qleftContext_NPs, qrightContext_NPs -- list of int-tuples [start, end (exclusive)]\n    contextPara_NPs -- list of list of int-tuples. Len of outer list == numSentences\n    \"\"\"", "\n", "\n", "assert", "os", ".", "path", ".", "exists", "(", "input_jsonl", ")", "\n", "\n", "print", "(", "\"Reading input ... \"", ",", "end", "=", "\"\"", ",", "flush", "=", "True", ")", "\n", "\n", "with", "open", "(", "input_jsonl", ",", "\"r\"", ")", "as", "f", ":", "\n", "\n", "        ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "docDicts", "=", "[", "json", ".", "loads", "(", "s", ")", "for", "s", "in", "lines", "]", "\n", "\n", "", "doc_lengths", "=", "[", "sum", "(", "[", "len", "(", "s", ")", "for", "s", "in", "doc", "[", "sort_key", "]", "]", ")", "for", "doc", "in", "docDicts", "]", "\n", "\n", "print", "(", "\"Sorting input ... \"", ",", "end", "=", "\"\"", ",", "flush", "=", "True", ")", "\n", "\n", "sorted_docDicts", "=", "[", "x", "for", "_", ",", "x", "in", "sorted", "(", "zip", "(", "doc_lengths", ",", "docDicts", ")", ",", "key", "=", "lambda", "pair", ":", "pair", "[", "0", "]", ",", "reverse", "=", "reverse", ")", "]", "\n", "\n", "numdocswritten", "=", "0", "\n", "\n", "print", "(", "\"Writing output ... \"", ",", "end", "=", "\"\"", ",", "flush", "=", "True", ")", "\n", "\n", "with", "open", "(", "output_jsonl", ",", "\"w\"", ")", "as", "outf", ":", "\n", "        ", "for", "doc", "in", "sorted_docDicts", ":", "\n", "            ", "outf", ".", "write", "(", "json", ".", "dumps", "(", "doc", ")", ")", "\n", "outf", ".", "write", "(", "\"\\n\"", ")", "\n", "numdocswritten", "+=", "1", "\n", "\n", "", "", "print", "(", "\"Output written!\"", ",", "flush", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.sortDocs._getdocweights": [[50, 64], ["zip", "doc_weights.append", "utils.util.count_list"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.count_list"], ["", "def", "_getdocweights", "(", "docs", ":", "List", "[", "Dict", "]", ",", "sort_keys", ":", "List", "[", "str", "]", ",", "depths", ":", "List", "[", "int", "]", ",", "combination", ":", "str", "=", "\"x*y\"", ")", "->", "List", "[", "int", "]", ":", "\n", "    ", "doc_weights", "=", "[", "]", "\n", "for", "doc", "in", "docs", ":", "\n", "        ", "docw", "=", "1", "if", "combination", "==", "\"x*y\"", "else", "0", "\n", "for", "key", ",", "depth", "in", "zip", "(", "sort_keys", ",", "depths", ")", ":", "\n", "            ", "val", "=", "doc", "[", "key", "]", "\n", "len", "=", "util", ".", "count_list", "(", "input_list", "=", "val", ",", "depth", "=", "depth", ")", "\n", "if", "combination", "==", "\"x*y\"", ":", "\n", "                ", "docw", "*=", "len", "\n", "", "elif", "combination", "==", "\"x+y\"", ":", "\n", "                ", "docw", "+=", "len", "\n", "", "", "doc_weights", ".", "append", "(", "docw", ")", "\n", "\n", "", "return", "doc_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.sortDocs.writeSortedDocs": [[66, 114], ["os.path.exists", "print", "sortDocs._getdocweights", "print", "print", "print", "open", "f.readlines", "open", "json.loads", "sorted", "outf.write", "outf.write", "zip", "json.dumps"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.sortDocs._getdocweights", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.readlines"], ["", "def", "writeSortedDocs", "(", "\n", "input_jsonl", ":", "str", ",", "\n", "output_jsonl", ":", "str", ",", "\n", "sort_keys", ":", "List", "[", "str", "]", ",", "\n", "depths", ":", "List", "[", "int", "]", ",", "\n", "combination", ":", "str", "=", "\"x*y\"", ",", "\n", "reverse", ":", "bool", "=", "True", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Sort input jsonl documents based on values from multiple keys.\n    Each sorting criteria is a nested list, whose depth is specified. For example, counting number of mentions in \n    a list of clusters, where each cluster is a list of mentions   \n    :param input_jsonl: input jsonl file\n    :param output_jsonl: output jsonl file\n    :param sort_keys: List of keys whose values will be used for sorting\n    :param depths: the depth of the value lists corresponding to the keys \n    :param combination: combination to arrive at the final sorting order. Possible values \"x*y\" or \"x+y\"\n    :param reverse: True indicates decreasing order\n    :return: \n    \"\"\"", "\"\"", "\n", "possible_combinations", "=", "[", "\"x*y\"", ",", "\"x+y\"", "]", "\n", "assert", "combination", "in", "possible_combinations", ",", "\"Combination value is invalid\"", "\n", "assert", "os", ".", "path", ".", "exists", "(", "input_jsonl", ")", ",", "\"Input jsonl doesn't exist\"", "\n", "\n", "print", "(", "\"Reading input ... \"", ",", "end", "=", "\"\"", ",", "flush", "=", "True", ")", "\n", "\n", "with", "open", "(", "input_jsonl", ",", "\"r\"", ")", "as", "f", ":", "\n", "\n", "        ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "docDicts", "=", "[", "json", ".", "loads", "(", "s", ")", "for", "s", "in", "lines", "]", "\n", "\n", "", "doc_weights", "=", "_getdocweights", "(", "docDicts", ",", "sort_keys", ",", "depths", ",", "combination", ")", "\n", "\n", "print", "(", "\"Sorting input ... \"", ",", "end", "=", "\"\"", ",", "flush", "=", "True", ")", "\n", "\n", "sorted_docDicts", "=", "[", "x", "for", "_", ",", "x", "in", "sorted", "(", "zip", "(", "doc_weights", ",", "docDicts", ")", ",", "key", "=", "lambda", "pair", ":", "pair", "[", "0", "]", ",", "reverse", "=", "reverse", ")", "]", "\n", "\n", "numdocswritten", "=", "0", "\n", "\n", "print", "(", "\"Writing output ... \"", ",", "end", "=", "\"\"", ",", "flush", "=", "True", ")", "\n", "\n", "with", "open", "(", "output_jsonl", ",", "\"w\"", ")", "as", "outf", ":", "\n", "        ", "for", "doc", "in", "sorted_docDicts", ":", "\n", "            ", "outf", ".", "write", "(", "json", ".", "dumps", "(", "doc", ")", ")", "\n", "outf", ".", "write", "(", "\"\\n\"", ")", "\n", "numdocswritten", "+=", "1", "\n", "\n", "", "", "print", "(", "\"Output written!\"", ",", "flush", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.sortDocs.checkSort": [[116, 145], ["os.path.exists", "print", "print", "sortDocs._getdocweights", "all", "open", "f.readlines", "print", "all", "json.loads", "print", "print", "range", "range", "len", "len"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.sortDocs._getdocweights", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.readlines"], ["", "def", "checkSort", "(", "input_jsonl", ":", "str", ",", "sort_keys", ":", "List", "[", "str", "]", ",", "depths", ":", "List", "[", "int", "]", ",", "combination", ":", "str", "=", "\"x*y\"", ")", "->", "None", ":", "\n", "    ", "\"\"\" Checks if a jsonl is sorted in the key of sort_key. \"\"\"", "\n", "\n", "assert", "os", ".", "path", ".", "exists", "(", "input_jsonl", ")", "\n", "\n", "print", "(", "\"Reading input ... \"", ",", "end", "=", "\"\"", ",", "flush", "=", "True", ")", "\n", "\n", "with", "open", "(", "input_jsonl", ",", "\"r\"", ")", "as", "f", ":", "\n", "\n", "        ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "docDicts", "=", "[", "json", ".", "loads", "(", "s", ")", "for", "s", "in", "lines", "]", "\n", "\n", "", "print", "(", "\"Checking sort ... \"", ",", "flush", "=", "True", ")", "\n", "\n", "doc_weights", "=", "_getdocweights", "(", "docDicts", ",", "sort_keys", ",", "depths", ",", "combination", ")", "\n", "\n", "reverse_sorteddocs", "=", "all", "(", "doc_weights", "[", "i", "]", ">=", "doc_weights", "[", "i", "+", "1", "]", "for", "i", "in", "range", "(", "len", "(", "doc_weights", ")", "-", "1", ")", ")", "\n", "\n", "if", "reverse_sorteddocs", ":", "\n", "        ", "print", "(", "\"Docs are sorted in decreasing order\"", ")", "\n", "return", "None", "\n", "", "else", ":", "\n", "        ", "sorteddocs", "=", "all", "(", "doc_weights", "[", "i", "]", "<=", "doc_weights", "[", "i", "+", "1", "]", "for", "i", "in", "range", "(", "len", "(", "doc_weights", ")", "-", "1", ")", ")", "\n", "if", "sorteddocs", ":", "\n", "            ", "print", "(", "\"Docs are sorted in increasing order\"", ")", "\n", "return", "None", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Docs are NOT sorted\"", ")", "\n", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.sortDocs.main": [[147, 194], ["args.sort_keys.split", "print", "print", "int", "len", "len", "print", "sortDocs.checkSort", "print", "print", "sortDocs.writeSortedDocs", "print", "args.depths.split", "os.remove", "os.rename"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.sortDocs.checkSort", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.sortDocs.writeSortedDocs"], ["", "", "", "def", "main", "(", "args", ")", ":", "\n", "\n", "    ", "reverse_bool", "=", "True", "if", "args", ".", "decreasing", "==", "\"true\"", "else", "False", "\n", "sort_keys", "=", "args", ".", "sort_keys", ".", "split", "(", "\",\"", ")", "\n", "depths", "=", "[", "int", "(", "x", ")", "for", "x", "in", "args", ".", "depths", ".", "split", "(", "\",\"", ")", "]", "\n", "combination", "=", "args", ".", "combination", "\n", "\n", "assert", "len", "(", "sort_keys", ")", "==", "len", "(", "depths", ")", "\n", "\n", "print", "(", "f\"Sorting keys and depths: {sort_keys}  {depths}\"", ")", "\n", "print", "(", "f\"Combination: {combination}\"", ")", "\n", "\n", "if", "args", ".", "checksort", ":", "\n", "        ", "print", "(", "\"Checking sort of docs in : {}\"", ".", "format", "(", "args", ".", "input_jsonl", ")", ")", "\n", "checkSort", "(", "input_jsonl", "=", "args", ".", "input_jsonl", ",", "sort_keys", "=", "sort_keys", ",", "depths", "=", "depths", ",", "combination", "=", "combination", ")", "\n", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Sort in decreasing: {}\"", ".", "format", "(", "reverse_bool", ")", ")", "\n", "print", "(", "\"Sorting doc: {}\"", ".", "format", "(", "args", ".", "input_jsonl", ")", ")", "\n", "\n", "temp_jsonl", "=", "None", "\n", "\n", "# If output in new file", "\n", "if", "args", ".", "new_file", ":", "\n", "            ", "assert", "args", ".", "output_jsonl", "is", "not", "None", ",", "\"output_jsonl for storing sorted docs is required\"", "\n", "output_jsonl", "=", "args", ".", "output_jsonl", "\n", "# If output in same file --- make a temp file that'll be deleted later", "\n", "", "else", ":", "\n", "            ", "temp_jsonl", "=", "args", ".", "input_jsonl", "+", "\".bak\"", "\n", "output_jsonl", "=", "temp_jsonl", "\n", "\n", "", "writeSortedDocs", "(", "\n", "input_jsonl", "=", "args", ".", "input_jsonl", ",", "\n", "output_jsonl", "=", "output_jsonl", ",", "\n", "sort_keys", "=", "sort_keys", ",", "\n", "depths", "=", "depths", ",", "\n", "reverse", "=", "reverse_bool", ",", "\n", "combination", "=", "combination", ",", "\n", ")", "\n", "\n", "# Remove old input file, replace it with temp", "\n", "if", "not", "args", ".", "new_file", ":", "\n", "            ", "assert", "temp_jsonl", "is", "not", "None", ",", "\"temp_jsonl cannot be None here.\"", "\n", "os", ".", "remove", "(", "args", ".", "input_jsonl", ")", "\n", "os", ".", "rename", "(", "temp_jsonl", ",", "args", ".", "input_jsonl", ")", "\n", "\n", "", "print", "(", "\"Done sorting\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.bidaf_utils.PretrainedBidafModelUtils.__init__": [[23, 70], ["super().__init__", "logger.info", "allennlp.models.archival.load_archive", "allennlp.models.archival.load_archive", "logger.info", "logger.info", "logger.info", "bidaf_utils.PretrainedBidafModelUtils._bidaf_model._text_field_embedder._token_embedders.items", "logger.info", "bidaf_utils.PretrainedBidafModelUtils._bidaf_model._phrase_layer.is_bidirectional", "bidaf_utils.PretrainedBidafModelUtils._bidaf_model._phrase_layer.get_output_dim", "logger.info", "bidaf_utils.PretrainedBidafModelUtils._bidaf_model.parameters", "getattr", "isinstance", "torch.nn.Dropout", "getattr.extend_vocab", "getattr.weight.size"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "bidaf_model_path", ":", "str", ",", "\n", "bidaf_wordemb_file", ":", "str", ",", "\n", "dropout", ":", "float", "=", "0.0", ",", "\n", "fine_tune_bidaf", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", "PretrainedBidafModelUtils", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "bidaf_model_path", "is", "None", ":", "\n", "            ", "logger", ".", "info", "(", "f\"NOT loading pretrained bidaf model. bidaf_model_path - not given\"", ")", "\n", "raise", "NotImplementedError", "\n", "\n", "", "logger", ".", "info", "(", "f\"Loading BIDAF model: {bidaf_model_path}\"", ")", "\n", "bidaf_model_archive", "=", "load_archive", "(", "bidaf_model_path", ")", "\n", "self", ".", "_bidaf_model", ":", "BidirectionalAttentionFlow", "=", "bidaf_model_archive", ".", "model", "\n", "\n", "# Needs to be a tuple", "\n", "# untuneable_parameter_prefixes = ('_text_field_embedder', '_highway_layer')", "\n", "\n", "# for n, p in self.bidaf_model.named_parameters(recurse=True):", "\n", "#     n: str = n", "\n", "#     if n.startswith(untuneable_parameter_prefixes):", "\n", "#         p.requires_grad = False", "\n", "\n", "if", "not", "fine_tune_bidaf", ":", "\n", "            ", "for", "p", "in", "self", ".", "_bidaf_model", ".", "parameters", "(", ")", ":", "\n", "                ", "p", ".", "requires_grad", "=", "False", "\n", "\n", "", "", "logger", ".", "info", "(", "f\"Bidaf model successfully loaded! Bidaf fine-tuning is set to {fine_tune_bidaf}\"", ")", "\n", "logger", ".", "info", "(", "f\"Extending bidaf model's embedders based on the extended_vocab\"", ")", "\n", "logger", ".", "info", "(", "f\"Preatrained word embedding file being used: {bidaf_wordemb_file}\"", ")", "\n", "\n", "# Extending token embedding matrix for Bidaf based on current vocab", "\n", "for", "key", ",", "_", "in", "self", ".", "_bidaf_model", ".", "_text_field_embedder", ".", "_token_embedders", ".", "items", "(", ")", ":", "\n", "            ", "token_embedder", "=", "getattr", "(", "self", ".", "_bidaf_model", ".", "_text_field_embedder", ",", "\"token_embedder_{}\"", ".", "format", "(", "key", ")", ")", "\n", "if", "isinstance", "(", "token_embedder", ",", "Embedding", ")", ":", "\n", "                ", "token_embedder", ".", "extend_vocab", "(", "extended_vocab", "=", "vocab", ",", "pretrained_file", "=", "bidaf_wordemb_file", ")", "\n", "", "", "logger", ".", "info", "(", "f\"Embedder for bidaf extended. New size: {token_embedder.weight.size()}\"", ")", "\n", "\n", "self", ".", "bidaf_encoder_bidirectional", "=", "self", ".", "_bidaf_model", ".", "_phrase_layer", ".", "is_bidirectional", "(", ")", "\n", "self", ".", "_bidaf_encoded_dim", "=", "self", ".", "_bidaf_model", ".", "_phrase_layer", ".", "get_output_dim", "(", ")", "\n", "\n", "if", "dropout", ">", "0", ":", "\n", "            ", "self", ".", "_dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_dropout", "=", "lambda", "x", ":", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.bidaf_utils.PretrainedBidafModelUtils.embed_ques_passages": [[71, 103], ["bidaf_utils.PretrainedBidafModelUtils._bidaf_model._highway_layer", "allennlp.get_text_field_mask().float", "allennlp.get_text_field_mask().float", "next", "contexts.items", "range", "torch.cat", "torch.cat", "bidaf_utils.PretrainedBidafModelUtils._bidaf_model._text_field_embedder", "iter", "range", "bidaf_utils.PretrainedBidafModelUtils._bidaf_model._highway_layer", "embedded_contexts_list.append", "allennlp.get_text_field_mask().float", "allennlp.get_text_field_mask().float", "contexts_mask_list.append", "allennlp.get_text_field_mask", "allennlp.get_text_field_mask", "contexts.items", "indexed_tensor.size", "indexed_tensor.size", "range", "bidaf_utils.PretrainedBidafModelUtils._bidaf_model._text_field_embedder", "x.unsqueeze", "x.unsqueeze", "allennlp.get_text_field_mask", "allennlp.get_text_field_mask"], "methods", ["None"], ["", "", "def", "embed_ques_passages", "(", "self", ",", "question", ",", "contexts", ")", ":", "\n", "# Shape: (B, question_length, D)", "\n", "        ", "embedded_question", "=", "self", ".", "_bidaf_model", ".", "_highway_layer", "(", "self", ".", "_bidaf_model", ".", "_text_field_embedder", "(", "question", ")", ")", "\n", "# Shape: (B, question_length)", "\n", "question_mask", "=", "allenutil", ".", "get_text_field_mask", "(", "question", ")", ".", "float", "(", ")", "\n", "\n", "(", "tokenindexer", ",", "indexed_tensor", ")", "=", "next", "(", "iter", "(", "contexts", ".", "items", "(", ")", ")", ")", "\n", "batch_size", ",", "num_contexts", "=", "indexed_tensor", ".", "size", "(", ")", "[", "0", "]", ",", "indexed_tensor", ".", "size", "(", ")", "[", "1", "]", "\n", "# Making a separate batched token_indexer_dict for each context -- [{token_inderxer: (C, T, *)}]", "\n", "contexts_indices_list", ":", "List", "[", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "]", "=", "[", "{", "}", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "for", "token_indexer_name", ",", "token_indices_tensor", "in", "contexts", ".", "items", "(", ")", ":", "\n", "# print(f\"{token_indexer_name}  : {token_indices_tensor.size()}\")", "\n", "            ", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "# For a tensor shape (B, C, *), this will slice from dim-0 a tensor of shape (C, *)", "\n", "                ", "contexts_indices_list", "[", "i", "]", "[", "token_indexer_name", "]", "=", "token_indices_tensor", "[", "i", ",", "...", "]", "\n", "\n", "# Each tensor of shape (num_contexts, context_len, D)", "\n", "", "", "embedded_contexts_list", "=", "[", "]", "\n", "contexts_mask_list", "=", "[", "]", "\n", "# Shape: (num_contexts, context_length, D)", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "embedded_contexts_i", "=", "self", ".", "_bidaf_model", ".", "_highway_layer", "(", "\n", "self", ".", "_bidaf_model", ".", "_text_field_embedder", "(", "contexts_indices_list", "[", "i", "]", ")", "\n", ")", "\n", "embedded_contexts_list", ".", "append", "(", "embedded_contexts_i", ")", "\n", "contexts_mask_i", "=", "allenutil", ".", "get_text_field_mask", "(", "contexts_indices_list", "[", "i", "]", ")", ".", "float", "(", ")", "\n", "contexts_mask_list", ".", "append", "(", "contexts_mask_i", ")", "\n", "\n", "", "embedded_passages", "=", "torch", ".", "cat", "(", "[", "x", ".", "unsqueeze", "(", "0", ")", "for", "x", "in", "embedded_contexts_list", "]", ",", "dim", "=", "0", ")", "\n", "passages_mask", "=", "torch", ".", "cat", "(", "[", "x", ".", "unsqueeze", "(", "0", ")", "for", "x", "in", "contexts_mask_list", "]", ",", "dim", "=", "0", ")", "\n", "\n", "return", "embedded_question", ",", "embedded_passages", ",", "question_mask", ",", "passages_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.bidaf_utils.PretrainedBidafModelUtils.encode_question": [[104, 107], ["bidaf_utils.PretrainedBidafModelUtils._dropout", "bidaf_utils.PretrainedBidafModelUtils._bidaf_model._phrase_layer"], "methods", ["None"], ["", "def", "encode_question", "(", "self", ",", "embedded_question", ",", "question_lstm_mask", ")", ":", "\n", "        ", "encoded_question", "=", "self", ".", "_dropout", "(", "self", ".", "_bidaf_model", ".", "_phrase_layer", "(", "embedded_question", ",", "question_lstm_mask", ")", ")", "\n", "return", "encoded_question", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.bidaf_utils.PretrainedBidafModelUtils.encode_context": [[108, 111], ["bidaf_utils.PretrainedBidafModelUtils._dropout", "bidaf_utils.PretrainedBidafModelUtils._bidaf_model._phrase_layer"], "methods", ["None"], ["", "def", "encode_context", "(", "self", ",", "embedded_passage", ",", "passage_lstm_mask", ")", ":", "\n", "        ", "encoded_passage", "=", "self", ".", "_dropout", "(", "self", ".", "_bidaf_model", ".", "_phrase_layer", "(", "embedded_passage", ",", "passage_lstm_mask", ")", ")", "\n", "return", "encoded_passage", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.bidaf_utils.PretrainedBidafModelUtils.forward_bidaf": [[112, 188], ["encoded_question.size", "encoded_passage.size", "encoded_question.size", "bidaf_utils.PretrainedBidafModelUtils._bidaf_model._matrix_attention", "allennlp.masked_softmax", "allennlp.masked_softmax", "allennlp.weighted_sum", "allennlp.weighted_sum", "allennlp.replace_masked_values", "allennlp.replace_masked_values", "[].squeeze", "allennlp.masked_softmax", "allennlp.masked_softmax", "allennlp.weighted_sum", "allennlp.weighted_sum", "allennlp.weighted_sum.unsqueeze().expand", "torch.cat", "bidaf_utils.PretrainedBidafModelUtils._dropout", "question_lstm_mask.unsqueeze", "bidaf_utils.PretrainedBidafModelUtils._bidaf_model._modeling_layer", "allennlp.weighted_sum.unsqueeze", "allennlp.replace_masked_values.max"], "methods", ["None"], ["", "def", "forward_bidaf", "(", "\n", "self", ",", "\n", "encoded_question", ":", "torch", ".", "FloatTensor", ",", "\n", "encoded_passage", ":", "torch", ".", "FloatTensor", ",", "\n", "question_lstm_mask", ":", "torch", ".", "FloatTensor", ",", "\n", "passage_lstm_mask", ":", "torch", ".", "FloatTensor", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Runs bidaf model on already embedded question and passage. This function can be used to run on sub-questions,\n        or sub-questions concatenated.\n\n        Parameters\n        ----------\n        question : ``torch.FloatTensor``\n            This should be a (B, Q_T, D) sized tensor, say a slice of the output of the highway layer\n        passage : ``torch.FloatTensor``\n            This should be a (B, C_T, D) sized tensor, say a slice of the output of the highway layer\n        question_lstm_mask: ``torch.FloatTensor``\n            Question mask of shape (B, Q_T). Name is lstm mask to stay consistent with bidaf's code\n        passage_lstm_mask: ``torch.FloatTensor``\n            Passage mask of shape (B, C_T). Name is lstm mask to stay consistent with bidaf's code\n\n        Returns:\n        --------\n\n        \"\"\"", "\n", "batch_size", "=", "encoded_question", ".", "size", "(", "0", ")", "\n", "passage_length", "=", "encoded_passage", ".", "size", "(", "1", ")", "\n", "encoding_dim", "=", "encoded_question", ".", "size", "(", "-", "1", ")", "\n", "\n", "# Shape: (batch_size, passage_length, question_length)", "\n", "passage_question_similarity", "=", "self", ".", "_bidaf_model", ".", "_matrix_attention", "(", "encoded_passage", ",", "encoded_question", ")", "\n", "# Shape: (batch_size, passage_length, question_length)", "\n", "passage_question_attention", "=", "allenutil", ".", "masked_softmax", "(", "passage_question_similarity", ",", "question_lstm_mask", ")", "\n", "# Shape: (batch_size, passage_length, encoding_dim)", "\n", "passage_question_vectors", "=", "allenutil", ".", "weighted_sum", "(", "encoded_question", ",", "passage_question_attention", ")", "\n", "\n", "# We replace masked values with something really negative here, so they don't affect the", "\n", "# max below.", "\n", "masked_similarity", "=", "allenutil", ".", "replace_masked_values", "(", "\n", "passage_question_similarity", ",", "question_lstm_mask", ".", "unsqueeze", "(", "1", ")", ",", "-", "1e7", "\n", ")", "\n", "# Shape: (batch_size, passage_length)", "\n", "question_passage_similarity", "=", "masked_similarity", ".", "max", "(", "dim", "=", "-", "1", ")", "[", "0", "]", ".", "squeeze", "(", "-", "1", ")", "\n", "# Shape: (batch_size, passage_length)", "\n", "question_passage_attention", "=", "allenutil", ".", "masked_softmax", "(", "question_passage_similarity", ",", "passage_lstm_mask", ")", "\n", "# Shape: (batch_size, encoding_dim)", "\n", "question_passage_vector", "=", "allenutil", ".", "weighted_sum", "(", "encoded_passage", ",", "question_passage_attention", ")", "\n", "# Shape: (batch_size, passage_length, encoding_dim)", "\n", "tiled_question_passage_vector", "=", "question_passage_vector", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "\n", "batch_size", ",", "passage_length", ",", "encoding_dim", "\n", ")", "\n", "\n", "# Shape: (batch_size, passage_length, encoding_dim * 4)", "\n", "final_merged_passage", "=", "torch", ".", "cat", "(", "\n", "[", "\n", "encoded_passage", ",", "\n", "passage_question_vectors", ",", "\n", "encoded_passage", "*", "passage_question_vectors", ",", "\n", "encoded_passage", "*", "tiled_question_passage_vector", ",", "\n", "]", ",", "\n", "dim", "=", "-", "1", ",", "\n", ")", "\n", "\n", "modeled_passage", "=", "self", ".", "_dropout", "(", "self", ".", "_bidaf_model", ".", "_modeling_layer", "(", "final_merged_passage", ",", "passage_lstm_mask", ")", ")", "\n", "\n", "output_dict", "=", "{", "\n", "\"encoded_question\"", ":", "encoded_question", ",", "\n", "\"encoded_passage\"", ":", "encoded_passage", ",", "\n", "\"passage_vector\"", ":", "question_passage_vector", ",", "\n", "\"final_merged_passage\"", ":", "final_merged_passage", ",", "\n", "\"modeled_passage\"", ":", "modeled_passage", ",", "\n", "\"passage_question_attention\"", ":", "passage_question_attention", ",", "\n", "}", "\n", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.bidaf_utils.PretrainedBidafModelUtils.bidaf_reprs": [[189, 267], ["bidaf_utils.PretrainedBidafModelUtils.embed_ques_passages", "range", "bidaf_utils.PretrainedBidafModelUtils.encode_question", "allennlp.get_final_encoder_states", "allennlp.get_final_encoder_states", "range", "range", "embedded_question_tensor.size", "embedded_passages_tensor.size", "embedded_questions.append", "embedded_contexts.append", "questions_mask.append", "contexts_mask.append", "encoded_questions.append", "bidaf_utils.PretrainedBidafModelUtils.encode_context", "encoded_contexts.append", "encoded_ques.unsqueeze().expand", "ques_mask.unsqueeze().expand", "bidaf_utils.PretrainedBidafModelUtils.forward_bidaf", "modeled_contexts.append", "encoded_ques.unsqueeze", "encoded_ques.size", "ques_mask.unsqueeze", "ques_mask.size"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.bidaf_utils.PretrainedBidafModelUtils.embed_ques_passages", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.bidaf_utils.PretrainedBidafModelUtils.encode_question", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.bidaf_utils.PretrainedBidafModelUtils.encode_context", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.bidaf_utils.PretrainedBidafModelUtils.forward_bidaf"], ["", "def", "bidaf_reprs", "(", "self", ",", "question", ",", "contexts", ")", ":", "\n", "# Shape: (B, ques_len, D), (B, num_contexts, context_len, D)", "\n", "        ", "(", "\n", "embedded_question_tensor", ",", "\n", "embedded_passages_tensor", ",", "\n", "question_mask_tensor", ",", "\n", "passages_mask_tensor", ",", "\n", ")", "=", "self", ".", "embed_ques_passages", "(", "question", ",", "contexts", ")", "\n", "\n", "batch_size", "=", "embedded_question_tensor", ".", "size", "(", ")", "[", "0", "]", "\n", "num_contexts", "=", "embedded_passages_tensor", ".", "size", "(", ")", "[", "1", "]", "\n", "\n", "embedded_questions", "=", "[", "]", "\n", "questions_mask", "=", "[", "]", "\n", "embedded_contexts", "=", "[", "]", "\n", "contexts_mask", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "0", ",", "batch_size", ")", ":", "\n", "            ", "embedded_questions", ".", "append", "(", "embedded_question_tensor", "[", "i", "]", ")", "\n", "embedded_contexts", ".", "append", "(", "embedded_passages_tensor", "[", "i", "]", ")", "\n", "questions_mask", ".", "append", "(", "question_mask_tensor", "[", "i", "]", ")", "\n", "contexts_mask", ".", "append", "(", "passages_mask_tensor", "[", "i", "]", ")", "\n", "\n", "# Shape: (B, ques_len, D)", "\n", "", "encoded_ques_tensor", "=", "self", ".", "encode_question", "(", "\n", "embedded_question", "=", "embedded_question_tensor", ",", "question_lstm_mask", "=", "question_mask_tensor", "\n", ")", "\n", "\n", "# Shape: (B, D)", "\n", "ques_encoded_final_state", "=", "allenutil", ".", "get_final_encoder_states", "(", "\n", "encoded_ques_tensor", ",", "question_mask_tensor", ",", "self", ".", "bidaf_encoder_bidirectional", "\n", ")", "\n", "\n", "# List of tensors: (question_len, D)", "\n", "encoded_questions", "=", "[", "]", "\n", "# List of tensors: (num_contexts, context_len, D)", "\n", "encoded_contexts", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "batch_size", ")", ":", "\n", "# Shape: (1, ques_len, D)", "\n", "# encoded_ques = self.encode_question(embedded_question=embedded_questions[i].unsqueeze(0),", "\n", "#                                     question_lstm_mask=questions_mask[i].unsqueeze(0))", "\n", "            ", "encoded_questions", ".", "append", "(", "encoded_ques_tensor", "[", "i", "]", ")", "\n", "# Shape: (num_contexts, context_len, D)", "\n", "encoded_context", "=", "self", ".", "encode_context", "(", "\n", "embedded_passage", "=", "embedded_contexts", "[", "i", "]", ",", "passage_lstm_mask", "=", "contexts_mask", "[", "i", "]", "\n", ")", "\n", "encoded_contexts", ".", "append", "(", "encoded_context", ")", "\n", "\n", "", "modeled_contexts", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "batch_size", ")", ":", "\n", "# Shape: (question_len, D)", "\n", "            ", "encoded_ques", "=", "encoded_questions", "[", "i", "]", "\n", "ques_mask", "=", "questions_mask", "[", "i", "]", "\n", "encoded_ques_ex", "=", "encoded_ques", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "num_contexts", ",", "*", "encoded_ques", ".", "size", "(", ")", ")", "\n", "ques_mask_ex", "=", "ques_mask", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "num_contexts", ",", "*", "ques_mask", ".", "size", "(", ")", ")", "\n", "\n", "output_dict", "=", "self", ".", "forward_bidaf", "(", "\n", "encoded_question", "=", "encoded_ques_ex", ",", "\n", "encoded_passage", "=", "encoded_contexts", "[", "i", "]", ",", "\n", "question_lstm_mask", "=", "ques_mask_ex", ",", "\n", "passage_lstm_mask", "=", "contexts_mask", "[", "i", "]", ",", "\n", ")", "\n", "\n", "# Shape: (num_contexts, context_len, D)", "\n", "modeled_context", "=", "output_dict", "[", "\"modeled_passage\"", "]", "\n", "modeled_contexts", ".", "append", "(", "modeled_context", ")", "\n", "\n", "", "return", "(", "\n", "ques_encoded_final_state", ",", "\n", "encoded_ques_tensor", ",", "\n", "question_mask_tensor", ",", "\n", "embedded_questions", ",", "\n", "questions_mask", ",", "\n", "embedded_contexts", ",", "\n", "contexts_mask", ",", "\n", "encoded_questions", ",", "\n", "encoded_contexts", ",", "\n", "modeled_contexts", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.generic_utils.embed_and_encode_ques_contexts": [[8, 96], ["text_field_embedder", "allennlp.get_text_field_mask().float", "qencoder", "allennlp.get_final_encoder_states", "text_field_embedder", "allennlp.get_text_field_mask().float", "qencoder.is_bidirectional", "allennlp.get_text_field_mask", "range", "range", "range", "allennlp.get_text_field_mask", "range", "range"], "function", ["None"], ["def", "embed_and_encode_ques_contexts", "(", "\n", "text_field_embedder", ":", "TextFieldEmbedder", ",", "\n", "qencoder", ":", "Seq2SeqEncoder", ",", "\n", "batch_size", ":", "int", ",", "\n", "question", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", ",", "\n", "contexts", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", ",", "\n", ")", ":", "\n", "    ", "\"\"\" Embed and Encode question and contexts\n\n        Parameters:\n        -----------\n        text_field_embedder: ``TextFieldEmbedder``\n        qencoder: ``Seq2SeqEncoder``\n        question: Dict[str, torch.LongTensor]\n            Output of a TextField. Should yield tensors of shape (B, ques_length, D)\n        contexts: Dict[str, torch.LongTensor]\n            Output of a TextField. Should yield tensors of shape (B, num_contexts, ques_length, D)\n\n        Returns:\n        ---------\n        embedded_questions: List[(ques_length, D)]\n            Batch-sized list of embedded questions from the text_field_embedder\n        encoded_questions: List[(ques_length, D)]\n            Batch-sized list of encoded questions from the qencoder\n        questions_mask: List[(ques_length)]\n            Batch-sized list of questions masks\n        encoded_ques_tensor: Shape: (batch_size, ques_len, D)\n            Output of the qencoder\n        questions_mask_tensor: Shape: (batch_size, ques_length)\n            Questions mask as a tensor\n        ques_encoded_final_state: Shape: (batch_size, D)\n            For each question, the final state of the qencoder\n        embedded_contexts: List[(num_contexts, context_length, D)]\n            Batch-sized list of embedded contexts for each instance from the text_field_embedder\n        contexts_mask: List[(num_contexts, context_length)]\n            Batch-sized list of contexts_mask for each context in the instance\n\n        \"\"\"", "\n", "# Shape: (B, question_length, D)", "\n", "embedded_questions_tensor", "=", "text_field_embedder", "(", "question", ")", "\n", "# Shape: (B, question_length)", "\n", "questions_mask_tensor", "=", "allenutil", ".", "get_text_field_mask", "(", "question", ")", ".", "float", "(", ")", "\n", "embedded_questions", "=", "[", "embedded_questions_tensor", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "questions_mask", "=", "[", "questions_mask_tensor", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "# Shape: (B, ques_len, D)", "\n", "encoded_ques_tensor", "=", "qencoder", "(", "embedded_questions_tensor", ",", "questions_mask_tensor", ")", "\n", "# Shape: (B, D)", "\n", "ques_encoded_final_state", "=", "allenutil", ".", "get_final_encoder_states", "(", "\n", "encoded_ques_tensor", ",", "questions_mask_tensor", ",", "qencoder", ".", "is_bidirectional", "(", ")", "\n", ")", "\n", "encoded_questions", "=", "[", "encoded_ques_tensor", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "# # contexts is a (B, num_contexts, context_length, *) tensors", "\n", "# (tokenindexer, indexed_tensor) = next(iter(contexts.items()))", "\n", "# num_contexts = indexed_tensor.size()[1]", "\n", "# # Making a separate batched token_indexer_dict for each context -- [{token_inderxer: (C, T, *)}]", "\n", "# contexts_indices_list: List[Dict[str, torch.LongTensor]] = [{} for _ in range(batch_size)]", "\n", "# for token_indexer_name, token_indices_tensor in contexts.items():", "\n", "#         print(f\"{token_indexer_name}: {token_indices_tensor.size()}\")", "\n", "#         for i in range(batch_size):", "\n", "#                 contexts_indices_list[i][token_indexer_name] = token_indices_tensor[i, ...]", "\n", "#", "\n", "# # Each tensor of shape (num_contexts, context_len, D)", "\n", "# embedded_contexts = []", "\n", "# contexts_mask = []", "\n", "# # Shape: (num_contexts, context_length, D)", "\n", "# for i in range(batch_size):", "\n", "#         embedded_contexts_i = text_field_embedder(contexts_indices_list[i])", "\n", "#         embedded_contexts.append(embedded_contexts_i)", "\n", "#         contexts_mask_i = allenutil.get_text_field_mask(contexts_indices_list[i]).float()", "\n", "#         contexts_mask.append(contexts_mask_i)", "\n", "\n", "embedded_contexts_tensor", "=", "text_field_embedder", "(", "contexts", ",", "num_wrapping_dims", "=", "1", ")", "\n", "contexts_mask_tensor", "=", "allenutil", ".", "get_text_field_mask", "(", "contexts", ",", "num_wrapping_dims", "=", "1", ")", ".", "float", "(", ")", "\n", "\n", "embedded_contexts", "=", "[", "embedded_contexts_tensor", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "contexts_mask", "=", "[", "contexts_mask_tensor", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "return", "(", "\n", "embedded_questions", ",", "\n", "encoded_questions", ",", "\n", "questions_mask", ",", "\n", "encoded_ques_tensor", ",", "\n", "questions_mask_tensor", ",", "\n", "ques_encoded_final_state", ",", "\n", "embedded_contexts", ",", "\n", "contexts_mask", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.semparse_utils._convert_finalstates_to_actions": [[12, 24], ["semparse_utils._get_actionseq_idxs_and_scores", "semparse_utils._get_actionseq_strings"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.semparse_utils._get_actionseq_idxs_and_scores", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.semparse_utils._get_actionseq_strings"], ["def", "_convert_finalstates_to_actions", "(", "\n", "best_final_states", ":", "Dict", "[", "int", ",", "List", "[", "StateType", "]", "]", ",", "possible_actions", ":", "List", "[", "List", "[", "ProductionRule", "]", "]", ",", "batch_size", ":", "int", "\n", ")", "->", "Tuple", "[", "List", "[", "List", "[", "List", "[", "int", "]", "]", "]", ",", "List", "[", "List", "[", "List", "[", "str", "]", "]", "]", ",", "List", "[", "List", "[", "torch", ".", "Tensor", "]", "]", ",", "List", "[", "List", "[", "List", "[", "Dict", "]", "]", "]", "]", ":", "\n", "\n", "    ", "(", "\n", "instanceidx2actionseq_idxs", ",", "\n", "instanceidx2actionseq_scores", ",", "\n", "instanceidx2actionseq_sideargs", ",", "\n", ")", "=", "_get_actionseq_idxs_and_scores", "(", "best_final_states", ",", "batch_size", ")", "\n", "\n", "return", "_get_actionseq_strings", "(", "\n", "possible_actions", ",", "instanceidx2actionseq_idxs", ",", "instanceidx2actionseq_scores", ",", "instanceidx2actionseq_sideargs", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.semparse_utils._get_actionseq_idxs_and_scores": [[27, 46], ["range"], "function", ["None"], ["", "def", "_get_actionseq_idxs_and_scores", "(", "best_final_states", ":", "Dict", "[", "int", ",", "List", "[", "StateType", "]", "]", ",", "batch_size", ":", "int", ")", ":", "\n", "    ", "instanceidx2actionseq_idxs", "=", "{", "}", "\n", "instanceidx2actionseq_scores", "=", "{", "}", "\n", "instanceidx2actionseq_sideargs", "=", "{", "}", "\n", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "# Decoding may not have terminated with any completed logical forms, if `num_steps`", "\n", "# isn't long enough (or if the model is not trained enough and gets into an", "\n", "# infinite action loop).", "\n", "        ", "if", "i", "in", "best_final_states", ":", "\n", "# Since the group size for any state is 1, action_history[0] can be used.", "\n", "            ", "instance_actionseq_idxs", "=", "[", "final_state", ".", "action_history", "[", "0", "]", "for", "final_state", "in", "best_final_states", "[", "i", "]", "]", "\n", "instance_actionseq_scores", "=", "[", "final_state", ".", "score", "[", "0", "]", "for", "final_state", "in", "best_final_states", "[", "i", "]", "]", "\n", "instanceidx2actionseq_idxs", "[", "i", "]", "=", "instance_actionseq_idxs", "\n", "instanceidx2actionseq_scores", "[", "i", "]", "=", "instance_actionseq_scores", "\n", "instance_actionseq_sideargs", "=", "[", "final_state", ".", "debug_info", "[", "0", "]", "for", "final_state", "in", "best_final_states", "[", "i", "]", "]", "\n", "instanceidx2actionseq_sideargs", "[", "i", "]", "=", "instance_actionseq_sideargs", "\n", "\n", "", "", "return", "(", "instanceidx2actionseq_idxs", ",", "instanceidx2actionseq_scores", ",", "instanceidx2actionseq_sideargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.semparse_utils._get_actionseq_strings": [[48, 82], ["len", "range", "all_action_indices.append", "all_action_strings.append", "all_action_scores.append", "all_debuginfos.append"], "function", ["None"], ["", "def", "_get_actionseq_strings", "(", "\n", "possible_actions", ":", "List", "[", "List", "[", "ProductionRule", "]", "]", ",", "\n", "b2actionindices", ":", "Dict", "[", "int", ",", "List", "[", "List", "[", "int", "]", "]", "]", ",", "\n", "b2actionscores", ":", "Dict", "[", "int", ",", "List", "[", "torch", ".", "Tensor", "]", "]", ",", "\n", "b2debuginfos", ":", "Dict", "[", "int", ",", "List", "[", "List", "[", "Dict", "]", "]", "]", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "List", "[", "List", "[", "List", "[", "int", "]", "]", "]", ",", "List", "[", "List", "[", "List", "[", "str", "]", "]", "]", ",", "List", "[", "List", "[", "torch", ".", "Tensor", "]", "]", ",", "List", "[", "List", "[", "List", "[", "Dict", "]", "]", "]", "]", ":", "\n", "    ", "\"\"\"\n    Takes a list of possible actions and indices of decoded actions into those possible actions\n    for a batch and returns sequences of action strings. We assume ``action_indices`` is a dict\n    mapping batch indices to k-best decoded sequence lists.\n    \"\"\"", "\n", "all_action_indices", ":", "List", "[", "List", "[", "List", "[", "int", "]", "]", "]", "=", "[", "]", "\n", "all_action_strings", ":", "List", "[", "List", "[", "List", "[", "str", "]", "]", "]", "=", "[", "]", "\n", "all_action_scores", ":", "List", "[", "List", "[", "torch", ".", "Tensor", "]", "]", "=", "[", "]", "\n", "all_debuginfos", ":", "List", "[", "List", "[", "List", "[", "Dict", "]", "]", "]", "=", "[", "]", "if", "b2debuginfos", "is", "not", "None", "else", "None", "\n", "batch_size", "=", "len", "(", "possible_actions", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "batch_actions", "=", "possible_actions", "[", "i", "]", "\n", "instance_actionindices", "=", "b2actionindices", "[", "i", "]", "if", "i", "in", "b2actionindices", "else", "[", "]", "\n", "instance_actionscores", "=", "b2actionscores", "[", "i", "]", "if", "i", "in", "b2actionscores", "else", "[", "]", "\n", "# This will append an empty list to ``all_action_strings`` if ``batch_best_sequences``", "\n", "# is empty.", "\n", "action_strings", "=", "[", "[", "batch_actions", "[", "rule_id", "]", "[", "0", "]", "for", "rule_id", "in", "sequence", "]", "for", "sequence", "in", "instance_actionindices", "]", "\n", "\n", "all_action_indices", ".", "append", "(", "instance_actionindices", ")", "\n", "all_action_strings", ".", "append", "(", "action_strings", ")", "\n", "all_action_scores", ".", "append", "(", "instance_actionscores", ")", "\n", "if", "b2debuginfos", "is", "not", "None", ":", "\n", "            ", "instance_debuginfos", "=", "b2debuginfos", "[", "i", "]", "if", "i", "in", "b2debuginfos", "else", "[", "]", "\n", "all_debuginfos", ".", "append", "(", "instance_debuginfos", ")", "\n", "\n", "# batch_actionseq_probs = _convert_actionscores_to_probs(batch_actionseq_scores=all_action_scores)", "\n", "# return all_action_indices, all_action_strings, all_action_scores, batch_actionseq_probs, all_debuginfos", "\n", "", "", "return", "all_action_indices", ",", "all_action_strings", ",", "all_action_scores", ",", "all_debuginfos", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.semparse_utils._convert_actionscores_to_probs": [[84, 105], ["allennlp.get_device_of", "allennlp.move_to_device", "torch.nn.functional.softmax", "batch_actionseq_probs.append", "torch.cat", "x.view"], "function", ["None"], ["", "def", "_convert_actionscores_to_probs", "(", "batch_actionseq_scores", ":", "List", "[", "List", "[", "torch", ".", "Tensor", "]", "]", ")", "->", "List", "[", "torch", ".", "FloatTensor", "]", ":", "\n", "    ", "\"\"\" Normalize program scores in a beam for an instance to get probabilities\n\n    Returns:\n    ---------\n    List[torch.FloatTensor]:\n        For each instance, a tensor the size of number of predicted programs\n        containing normalized probabilities\n    \"\"\"", "\n", "# Convert batch_action_scores to a single tensor the size of number of programs for each instance", "\n", "device_id", "=", "allenutil", ".", "get_device_of", "(", "batch_actionseq_scores", "[", "0", "]", "[", "0", "]", ")", "\n", "# Inside List[torch.Tensor] is a list of scalar-tensor with prob of each program for this instance", "\n", "# The prob is normalized across the programs in the beam", "\n", "batch_actionseq_probs", "=", "[", "]", "\n", "for", "score_list", "in", "batch_actionseq_scores", ":", "\n", "        ", "scores_astensor", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "cat", "(", "[", "x", ".", "view", "(", "1", ")", "for", "x", "in", "score_list", "]", ")", ",", "device_id", ")", "\n", "# allenutil.masked_softmax(scores_astensor, mask=None)", "\n", "action_probs", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "scores_astensor", ",", "dim", "=", "-", "1", ")", "\n", "batch_actionseq_probs", ".", "append", "(", "action_probs", ")", "\n", "\n", "", "return", "batch_actionseq_probs", "\n", "", ""]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.gru_test.gru_func": [[12, 24], ["torch.randn().cuda", "torch.randn().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.stack", "torch.stack", "gru_net", "batched_output.squeeze", "torch.stack.unsqueeze", "torch.FloatTensor().cuda.unsqueeze", "torch.randn", "torch.randn", "torch.FloatTensor", "torch.FloatTensor"], "function", ["None"], ["def", "gru_func", "(", "max_length", ",", "seq_length", ",", "gru_net", ")", ":", "\n", "    ", "x", "=", "torch", ".", "randn", "(", "max_length", ")", ".", "cuda", "(", ")", "\n", "mask", "=", "torch", ".", "FloatTensor", "(", "[", "1.0", "]", "*", "seq_length", "+", "[", "0.0", "]", "*", "(", "max_length", "-", "seq_length", ")", ")", ".", "cuda", "(", ")", "\n", "scaled_x", "=", "[", "x", "*", "sf", "for", "sf", "in", "scalingvals", "]", "\n", "# [length, size]", "\n", "scaled_x", "=", "torch", ".", "stack", "(", "scaled_x", ",", "dim", "=", "1", ")", "\n", "# [1, length, direction*output]", "\n", "batched_output", ",", "_", "=", "gru_net", "(", "scaled_x", ".", "unsqueeze", "(", "1", ")", ")", "\n", "# [length, direction*output]", "\n", "output", "=", "batched_output", ".", "squeeze", "(", "1", ")", "\n", "masked_output", "=", "output", "*", "mask", ".", "unsqueeze", "(", "1", ")", "\n", "return", "masked_output", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.gru_test.s2s_func": [[27, 41], ["torch.randn().cuda", "torch.randn().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.stack", "torch.stack", "gru_net", "gru_net.squeeze", "torch.stack.unsqueeze", "torch.FloatTensor().cuda.unsqueeze", "torch.FloatTensor().cuda.unsqueeze", "torch.randn", "torch.randn", "torch.FloatTensor", "torch.FloatTensor"], "function", ["None"], ["", "def", "s2s_func", "(", "max_length", ",", "seq_length", ",", "gru_net", ")", ":", "\n", "    ", "x", "=", "torch", ".", "randn", "(", "max_length", ")", ".", "cuda", "(", ")", "\n", "mask", "=", "torch", ".", "FloatTensor", "(", "[", "1.0", "]", "*", "seq_length", "+", "[", "0.0", "]", "*", "(", "max_length", "-", "seq_length", ")", ")", ".", "cuda", "(", ")", "\n", "\n", "scaled_x", "=", "[", "x", "*", "sf", "for", "sf", "in", "scalingvals", "]", "\n", "# [length, size]", "\n", "scaled_x", "=", "torch", ".", "stack", "(", "scaled_x", ",", "dim", "=", "1", ")", "\n", "# [1, length, direction*output]", "\n", "batched_output", "=", "gru_net", "(", "scaled_x", ".", "unsqueeze", "(", "0", ")", ",", "mask", ".", "unsqueeze", "(", "0", ")", ")", "\n", "# [length, direction*output]", "\n", "output", "=", "batched_output", ".", "squeeze", "(", "0", ")", "\n", "masked_output", "=", "output", "*", "mask", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "return", "masked_output", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.gru_test.gru_test": [[43, 51], ["torch.nn.GRU", "torch.nn.GRU.cuda", "print", "semqa.profiler.profile.Profile", "range", "semqa.profiler.profile.Profile.to_string", "gru_test.gru_func"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.to_string", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.gru_test.gru_func"], ["", "def", "gru_test", "(", "nsteps", ")", ":", "\n", "    ", "gru_net", "=", "GRU", "(", "input_size", "=", "4", ",", "hidden_size", "=", "20", ",", "num_layers", "=", "3", ",", "batch_first", "=", "False", ",", "dropout", "=", "0.0", ",", "bidirectional", "=", "True", ")", "\n", "gru_net", ".", "cuda", "(", "device", "=", "0", ")", "\n", "\n", "with", "Profile", "(", "\"gru\"", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "nsteps", ")", ":", "\n", "            ", "gru_func", "(", "max_length", "=", "450", ",", "seq_length", "=", "350", ",", "gru_net", "=", "gru_net", ")", "\n", "", "", "print", "(", "Profile", ".", "to_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.gru_test.s2s_test": [[53, 66], ["allennlp.common.Params", "allennlp.modules.Seq2SeqEncoder.from_params", "Seq2SeqEncoder.from_params.cuda", "print", "semqa.profiler.profile.Profile", "range", "semqa.profiler.profile.Profile.to_string", "gru_test.s2s_func"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.to_string", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.gru_test.s2s_func"], ["", "def", "s2s_test", "(", "nsteps", ")", ":", "\n", "    ", "params", "=", "Params", "(", "params", "=", "{", "\"type\"", ":", "\"gru\"", ",", "\n", "\"input_size\"", ":", "4", ",", "\n", "\"hidden_size\"", ":", "20", ",", "\n", "\"num_layers\"", ":", "3", ",", "\n", "\"bidirectional\"", ":", "True", "}", ")", "\n", "gru_net", "=", "Seq2SeqEncoder", ".", "from_params", "(", "params", "=", "params", ")", "\n", "gru_net", ".", "cuda", "(", "device", "=", "0", ")", "\n", "\n", "with", "Profile", "(", "\"s2s\"", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "nsteps", ")", ":", "\n", "            ", "s2s_func", "(", "max_length", "=", "450", ",", "seq_length", "=", "350", ",", "gru_net", "=", "gru_net", ")", "\n", "", "", "print", "(", "Profile", ".", "to_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.read_qid2ques_and_passage": [[12, 26], ["dataset.items"], "function", ["None"], ["def", "read_qid2ques_and_passage", "(", "dataset", ")", ":", "\n", "    ", "qid2ques", "=", "{", "}", "\n", "qid2passage", "=", "{", "}", "\n", "for", "pid", ",", "pinfo", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "passage", "=", "pinfo", "[", "\"passage\"", "]", "\n", "for", "qa_pair", "in", "pinfo", "[", "\"qa_pairs\"", "]", ":", "\n", "            ", "qid", "=", "qa_pair", "[", "\"query_id\"", "]", "\n", "question", "=", "qa_pair", "[", "\"question\"", "]", "\n", "qid2ques", "[", "qid", "]", "=", "question", "\n", "qid2passage", "[", "qid", "]", "=", "passage", "\n", "# if \"longest\" in question or \"shortest\" in question:", "\n", "#     minmax_qid2ques[qid] = question", "\n", "\n", "", "", "return", "qid2ques", ",", "qid2passage", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.get_minmax_ques": [[28, 37], ["dataset.items"], "function", ["None"], ["", "def", "get_minmax_ques", "(", "dataset", ")", ":", "\n", "    ", "minmax_qid2ques", "=", "{", "}", "\n", "for", "pid", ",", "pinfo", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "for", "qa_pair", "in", "pinfo", "[", "\"qa_pairs\"", "]", ":", "\n", "            ", "qid", "=", "qa_pair", "[", "\"query_id\"", "]", "\n", "question", "=", "qa_pair", "[", "\"question\"", "]", "\n", "if", "\"longest\"", "in", "question", "or", "\"shortest\"", "in", "question", ":", "\n", "                ", "minmax_qid2ques", "[", "qid", "]", "=", "question", "\n", "", "", "", "return", "minmax_qid2ques", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.count_min_max": [[39, 54], ["pred_programs.items", "print", "print", "print", "float", "program.count", "max"], "function", ["None"], ["", "def", "count_min_max", "(", "pred_programs", ")", ":", "\n", "    ", "num_min_max_progs", "=", "0", "\n", "total_values_in_min_max", "=", "0", "\n", "max_values_in_prog", "=", "0", "\n", "for", "qid", ",", "program", "in", "pred_programs", ".", "items", "(", ")", ":", "\n", "        ", "if", "\"MIN(\"", "in", "program", "or", "\"MAX(\"", "in", "program", ":", "\n", "            ", "num_min_max_progs", "+=", "1", "\n", "num_values", "=", "program", ".", "count", "(", "\"VALUE(\"", ")", "\n", "total_values_in_min_max", "+=", "num_values", "\n", "max_values_in_prog", "=", "max", "(", "max_values_in_prog", ",", "num_values", ")", "\n", "\n", "", "", "avg_num_values", "=", "float", "(", "total_values_in_min_max", ")", "/", "num_min_max_progs", "\n", "print", "(", "f\"Num min/max progs: {num_min_max_progs}\"", ")", "\n", "print", "(", "f\"Avg num values: {avg_num_values}\"", ")", "\n", "print", "(", "f\"max num values: {max_values_in_prog}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.count_arg_min_max": [[56, 74], ["pred_programs.items", "print", "print", "print", "float", "program.count", "max", "argminmax_qids.append"], "function", ["None"], ["", "def", "count_arg_min_max", "(", "pred_programs", ")", ":", "\n", "    ", "num_min_max_progs", "=", "0", "\n", "total_values_in_min_max", "=", "0", "\n", "max_values_in_prog", "=", "0", "\n", "argminmax_qids", "=", "[", "]", "\n", "for", "qid", ",", "program", "in", "pred_programs", ".", "items", "(", ")", ":", "\n", "        ", "if", "\"ARGMIN(\"", "in", "program", "or", "\"ARGMAX(\"", "in", "program", ":", "\n", "            ", "num_min_max_progs", "+=", "1", "\n", "num_values", "=", "program", ".", "count", "(", "\"KV(\"", ")", "\n", "total_values_in_min_max", "+=", "num_values", "\n", "max_values_in_prog", "=", "max", "(", "max_values_in_prog", ",", "num_values", ")", "\n", "argminmax_qids", ".", "append", "(", "qid", ")", "\n", "\n", "", "", "avg_num_values", "=", "float", "(", "total_values_in_min_max", ")", "/", "num_min_max_progs", "\n", "print", "(", "f\"Num ARG min/max progs: {num_min_max_progs}\"", ")", "\n", "print", "(", "f\"Avg num key-values: {avg_num_values}\"", ")", "\n", "print", "(", "f\"max num key-values: {max_values_in_prog}\"", ")", "\n", "return", "argminmax_qids", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.is_minmax_prog": [[76, 81], ["None"], "function", ["None"], ["", "def", "is_minmax_prog", "(", "program", ")", ":", "\n", "    ", "if", "\"MIN(\"", "in", "program", "or", "\"MAX(\"", "in", "program", "or", "\"ARGMIN(\"", "in", "program", "or", "\"ARGMAX(\"", "in", "program", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.num_of_minmaxpred": [[83, 111], ["len", "minmax_qid2ques.items", "print", "print", "print", "print", "nerd_analysis.is_minmax_prog", "nerd_analysis.is_correct_func", "int", "int", "qid_not_minmax.append"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.is_minmax_prog", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.is_correct_func"], ["", "", "def", "num_of_minmaxpred", "(", "minmax_qid2ques", ",", "pred_programs", ",", "dev_qid2passage", ",", "predictions", ",", "nbest_preds", ")", ":", "\n", "    ", "total_qa", "=", "len", "(", "minmax_qid2ques", ")", "\n", "total_minmax_predprogram", "=", "0", "\n", "qid_not_minmax", "=", "[", "]", "\n", "\n", "correct_minmaxpred", "=", "0", "\n", "correct_not_minmaxpred", "=", "0", "\n", "\n", "for", "qid", ",", "question", "in", "minmax_qid2ques", ".", "items", "(", ")", ":", "\n", "        ", "predprogram", "=", "pred_programs", "[", "qid", "]", "\n", "isminmax", "=", "is_minmax_prog", "(", "predprogram", ")", "\n", "pred_ans", "=", "predictions", "[", "qid", "]", "\n", "ref_answers", "=", "nbest_preds", "[", "qid", "]", "[", "0", "]", "[", "\"ref_answer\"", "]", "\n", "iscorrect", "=", "is_correct_func", "(", "pred_ans", ",", "ref_answers", ")", "\n", "# if isminmax:", "\n", "#     print(\"{}\\n{}\\n{}\\n\".format(question, predprogram, dev_qid2passage[qid]))", "\n", "\n", "total_minmax_predprogram", "+=", "1", "if", "isminmax", "else", "0", "\n", "correct_minmaxpred", "+=", "int", "(", "iscorrect", ")", "if", "isminmax", "else", "0", "\n", "correct_not_minmaxpred", "+=", "int", "(", "iscorrect", ")", "if", "not", "isminmax", "else", "0", "\n", "if", "not", "isminmax", ":", "\n", "            ", "qid_not_minmax", ".", "append", "(", "qid", ")", "\n", "\n", "", "", "print", "(", "\"Total min/max questions : {}\"", ".", "format", "(", "total_qa", ")", ")", "\n", "print", "(", "\"Pred program are min/max: : {}\"", ".", "format", "(", "total_minmax_predprogram", ")", ")", "\n", "print", "(", "\"Correct amongst min/max pred: : {}\"", ".", "format", "(", "correct_minmaxpred", ")", ")", "\n", "print", "(", "\"Correct amongst NON min/max pred: : {}\"", ".", "format", "(", "correct_not_minmaxpred", ")", ")", "\n", "return", "qid_not_minmax", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.is_correct_func": [[113, 120], ["str"], "function", ["None"], ["", "def", "is_correct_func", "(", "pred", ",", "refs", ")", ":", "\n", "    ", "pred", "=", "[", "str", "(", "x", ")", "for", "x", "in", "pred", "]", "\n", "for", "ref", "in", "refs", ":", "\n", "        ", "if", "ref", "==", "pred", ":", "\n", "            ", "return", "True", "\n", "\n", "", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.write_not_minmax_to_file": [[122, 134], ["open", "outf.write"], "function", ["None"], ["", "def", "write_not_minmax_to_file", "(", "\n", "txt_outfile", ",", "qid_not_minmax", ",", "predictions", ",", "pred_programs", ",", "dev_qid2ques", ",", "dev_qid2passage", ",", "nbest_preds", "\n", ")", ":", "\n", "    ", "with", "open", "(", "txt_outfile", ",", "\"w\"", ")", "as", "outf", ":", "\n", "        ", "for", "qid", "in", "qid_not_minmax", ":", "\n", "            ", "question", "=", "dev_qid2ques", "[", "qid", "]", "\n", "passage", "=", "dev_qid2passage", "[", "qid", "]", "\n", "predprogram", "=", "pred_programs", "[", "qid", "]", "\n", "pred_ans", "=", "predictions", "[", "qid", "]", "\n", "ref_answers", "=", "nbest_preds", "[", "qid", "]", "[", "0", "]", "[", "\"ref_answer\"", "]", "\n", "\n", "outf", ".", "write", "(", "\"{}\\n{}\\n{}\\npred:{}\\nref:{}\\n\\n\"", ".", "format", "(", "question", ",", "predprogram", ",", "passage", ",", "pred_ans", ",", "ref_answers", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.write_incorrect_to_txtfile": [[136, 152], ["print", "len", "print", "open", "dev_qid2ques.items", "nerd_analysis.is_correct_func", "outf.write"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.is_correct_func"], ["", "", "", "def", "write_incorrect_to_txtfile", "(", "txt_outfile", ",", "predictions", ",", "pred_programs", ",", "dev_qid2ques", ",", "dev_qid2passage", ",", "nbest_preds", ")", ":", "\n", "    ", "print", "(", "\"\\nWriting incorrect dev predictions ... \"", ")", "\n", "total_ques_written", "=", "0", "\n", "total_ques", "=", "len", "(", "dev_qid2ques", ")", "\n", "with", "open", "(", "txt_outfile", ",", "\"w\"", ")", "as", "outf", ":", "\n", "        ", "for", "qid", ",", "question", "in", "dev_qid2ques", ".", "items", "(", ")", ":", "\n", "            ", "passage", "=", "dev_qid2passage", "[", "qid", "]", "\n", "predprogram", "=", "pred_programs", "[", "qid", "]", "\n", "pred_ans", "=", "predictions", "[", "qid", "]", "\n", "ref_answers", "=", "nbest_preds", "[", "qid", "]", "[", "0", "]", "[", "\"ref_answer\"", "]", "\n", "iscorrect", "=", "is_correct_func", "(", "pred_ans", ",", "ref_answers", ")", "\n", "if", "iscorrect", ":", "\n", "                ", "continue", "\n", "", "outf", ".", "write", "(", "\"{}\\n{}\\n{}\\npred:{}\\nref:{}\\n\\n\"", ".", "format", "(", "question", ",", "passage", ",", "predprogram", ",", "pred_ans", ",", "ref_answers", ")", ")", "\n", "total_ques_written", "+=", "1", "\n", "", "", "print", "(", "\"Total: {} Incorrect:{}\"", ".", "format", "(", "total_ques", ",", "total_ques_written", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.write_incorrect_to_tsvfile": [[154, 171], ["print", "len", "print", "open", "outf.write", "dev_qid2ques.items", "nerd_analysis.is_correct_func", "outf.write"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.is_correct_func"], ["", "def", "write_incorrect_to_tsvfile", "(", "txt_outfile", ",", "predictions", ",", "pred_programs", ",", "dev_qid2ques", ",", "dev_qid2passage", ",", "nbest_preds", ")", ":", "\n", "    ", "print", "(", "\"\\nWriting incorrect dev predictions ... \"", ")", "\n", "total_ques_written", "=", "0", "\n", "total_ques", "=", "len", "(", "dev_qid2ques", ")", "\n", "with", "open", "(", "txt_outfile", ",", "\"w\"", ")", "as", "outf", ":", "\n", "        ", "outf", ".", "write", "(", "f\"Question\\tPredProgram\\tPredAns\\tGoldAns\\tPassage\\n\"", ")", "\n", "for", "qid", ",", "question", "in", "dev_qid2ques", ".", "items", "(", ")", ":", "\n", "            ", "passage", "=", "dev_qid2passage", "[", "qid", "]", "\n", "predprogram", "=", "pred_programs", "[", "qid", "]", "\n", "pred_ans", "=", "predictions", "[", "qid", "]", "\n", "ref_answers", "=", "nbest_preds", "[", "qid", "]", "[", "0", "]", "[", "\"ref_answer\"", "]", "\n", "iscorrect", "=", "is_correct_func", "(", "pred_ans", ",", "ref_answers", ")", "\n", "if", "iscorrect", ":", "\n", "                ", "continue", "\n", "", "outf", ".", "write", "(", "f\"{question}\\t{predprogram}\\t{pred_ans}\\t{ref_answers}\\t{passage}\\n\"", ")", "\n", "total_ques_written", "+=", "1", "\n", "", "", "print", "(", "\"Total: {} Incorrect:{}\"", ".", "format", "(", "total_ques", ",", "total_ques_written", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.main": [[173, 233], ["json.load", "json.load", "json.load", "json.load", "json.load", "nerd_analysis.read_qid2ques_and_passage", "nerd_analysis.get_minmax_ques", "nerd_analysis.get_minmax_ques", "nerd_analysis.count_min_max", "nerd_analysis.count_arg_min_max", "set().difference", "print", "print", "nerd_analysis.num_of_minmaxpred", "print", "nerd_analysis.num_of_minmaxpred", "print", "nerd_analysis.write_not_minmax_to_file", "print", "nerd_analysis.write_not_minmax_to_file", "nerd_analysis.write_incorrect_to_tsvfile", "open", "open", "open", "open", "open", "set", "set", "get_minmax_ques.keys"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.read_qid2ques_and_passage", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.get_minmax_ques", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.get_minmax_ques", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.count_min_max", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.count_arg_min_max", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.num_of_minmaxpred", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.num_of_minmaxpred", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.write_not_minmax_to_file", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.write_not_minmax_to_file", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.nerd_analysis.write_incorrect_to_tsvfile"], ["", "def", "main", "(", ")", ":", "\n", "    ", "hmyw_dev", "=", "json", ".", "load", "(", "open", "(", "hmyw_dev_json", ")", ")", "\n", "whoarg_dev", "=", "json", ".", "load", "(", "open", "(", "whoarg_dev_json", ")", ")", "\n", "dev_dataset", "=", "json", ".", "load", "(", "open", "(", "dev_dataset_json", ")", ")", "\n", "# Dict from qid to List containing predictions", "\n", "predictions", "=", "json", ".", "load", "(", "open", "(", "predictions_json", ")", ")", "\n", "nbest_preds", "=", "json", ".", "load", "(", "open", "(", "nbest_preds_json", ")", ")", "\n", "# Dict from qid to List[program_tokens]", "\n", "pred_programs", "=", "predictions", "[", "\"pred_programs\"", "]", "\n", "\n", "dev_qid2ques", ",", "dev_qid2passage", "=", "read_qid2ques_and_passage", "(", "dev_dataset", ")", "\n", "\n", "whoarg_minmax_qid2ques", "=", "get_minmax_ques", "(", "whoarg_dev", ")", "\n", "hmyw_minmax_qid2ques", "=", "get_minmax_ques", "(", "hmyw_dev", ")", "\n", "\n", "count_min_max", "(", "pred_programs", ")", "\n", "argminmax_qids", "=", "count_arg_min_max", "(", "pred_programs", ")", "\n", "\n", "argminmax_not_whoarg", "=", "set", "(", "argminmax_qids", ")", ".", "difference", "(", "set", "(", "whoarg_minmax_qid2ques", ".", "keys", "(", ")", ")", ")", "\n", "print", "(", "[", "dev_qid2ques", "[", "qid", "]", "for", "qid", "in", "argminmax_not_whoarg", "]", ")", "\n", "\n", "print", "(", "\"HMYW\"", ")", "\n", "hmyw_not_minmaxpreds", "=", "num_of_minmaxpred", "(", "\n", "hmyw_minmax_qid2ques", ",", "pred_programs", ",", "dev_qid2passage", ",", "predictions", ",", "nbest_preds", "\n", ")", "\n", "print", "(", "\"Who Arg\"", ")", "\n", "whoarg_not_minmaxpreds", "=", "num_of_minmaxpred", "(", "\n", "whoarg_minmax_qid2ques", ",", "pred_programs", ",", "dev_qid2passage", ",", "predictions", ",", "nbest_preds", "\n", ")", "\n", "\n", "print", "(", "\"\\nWriting incorrect min/max HMYW dev predictions ... \"", ")", "\n", "write_not_minmax_to_file", "(", "\n", "txt_outfile", "=", "\"/shared/nitishg/data/drop_acl/nerd-preds/hmwy_notminmax.json\"", ",", "\n", "qid_not_minmax", "=", "hmyw_not_minmaxpreds", ",", "\n", "predictions", "=", "predictions", ",", "\n", "pred_programs", "=", "pred_programs", ",", "\n", "dev_qid2ques", "=", "dev_qid2ques", ",", "\n", "dev_qid2passage", "=", "dev_qid2passage", ",", "\n", "nbest_preds", "=", "nbest_preds", ",", "\n", ")", "\n", "\n", "print", "(", "\"\\nWriting incorrect min/max WHO-ARG dev predictions ... \"", ")", "\n", "write_not_minmax_to_file", "(", "\n", "txt_outfile", "=", "\"/shared/nitishg/data/drop_acl/nerd-preds/whoarg_notminmax.json\"", ",", "\n", "qid_not_minmax", "=", "whoarg_not_minmaxpreds", ",", "\n", "predictions", "=", "predictions", ",", "\n", "pred_programs", "=", "pred_programs", ",", "\n", "dev_qid2ques", "=", "dev_qid2ques", ",", "\n", "dev_qid2passage", "=", "dev_qid2passage", ",", "\n", "nbest_preds", "=", "nbest_preds", ",", "\n", ")", "\n", "\n", "# write_incorrect_to_txtfile", "\n", "write_incorrect_to_tsvfile", "(", "\n", "txt_outfile", "=", "\"/shared/nitishg/data/drop_acl/nerd-preds/dev_incorrect.tsv\"", ",", "\n", "predictions", "=", "predictions", ",", "\n", "pred_programs", "=", "pred_programs", ",", "\n", "dev_qid2ques", "=", "dev_qid2ques", ",", "\n", "dev_qid2passage", "=", "dev_qid2passage", ",", "\n", "nbest_preds", "=", "nbest_preds", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.date_data_augmentation.quesEvents": [[18, 67], ["qstr.split", "qstr.split", "qstr.split.index", "min", "len", "qstr.split.index", "qstr.split.index", "qstr.split.index", "len", "qstr.split.index", "len", "qstr.split.index", "qstr.split.index", "qstr.split.index"], "function", ["None"], ["", "def", "quesEvents", "(", "qstr", ")", ":", "\n", "    ", "\"\"\" Returns (start, end) span tuples for event1 and event2 in the question \"\"\"", "\n", "or_split", "=", "qstr", ".", "split", "(", "\" or \"", ")", "\n", "if", "len", "(", "or_split", ")", "!=", "2", ":", "\n", "        ", "return", "None", "\n", "\n", "", "tokens", "=", "qstr", ".", "split", "(", "\" \"", ")", "\n", "\n", "or_idx", "=", "tokens", ".", "index", "(", "\"or\"", ")", "\n", "# Last token is ? which we don't want to attend to", "\n", "event2", "=", "tokens", "[", "or_idx", "+", "1", ":", "len", "(", "tokens", ")", "-", "1", "]", "\n", "event2_span", "=", "(", "or_idx", "+", "1", ",", "len", "(", "tokens", ")", "-", "1", ")", "\n", "\n", "# Gets first index of the item", "\n", "try", ":", "\n", "        ", "comma_idx", "=", "tokens", ".", "index", "(", "\",\"", ")", "\n", "", "except", ":", "\n", "        ", "comma_idx", "=", "100000", "\n", "", "try", ":", "\n", "        ", "colon_idx", "=", "tokens", ".", "index", "(", "\":\"", ")", "\n", "", "except", ":", "\n", "        ", "colon_idx", "=", "100000", "\n", "\n", "", "try", ":", "\n", "        ", "hyphen_idx", "=", "tokens", ".", "index", "(", "\"-\"", ")", "\n", "", "except", ":", "\n", "        ", "hyphen_idx", "=", "100000", "\n", "\n", "", "split_idx", "=", "min", "(", "comma_idx", ",", "colon_idx", ",", "hyphen_idx", ")", "\n", "\n", "if", "split_idx", "==", "100000", "or", "(", "or_idx", "-", "split_idx", "<=", "1", ")", ":", "\n", "# print(f\"{qstr} first_split:{split_idx} or:{or_idx}\")", "\n", "        ", "if", "\"first\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"first\"", ")", "\n", "", "elif", "\"second\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"second\"", ")", "\n", "", "elif", "\"last\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"last\"", ")", "\n", "", "elif", "\"later\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"later\"", ")", "\n", "", "else", ":", "\n", "            ", "split_idx", "=", "-", "1", "\n", "\n", "", "", "assert", "split_idx", "!=", "-", "1", ",", "f\"{qstr} {split_idx} {or_idx}\"", "\n", "\n", "event1", "=", "tokens", "[", "split_idx", "+", "1", ":", "or_idx", "]", "\n", "event1_span", "=", "(", "split_idx", "+", "1", ",", "or_idx", ")", "\n", "\n", "return", "event1_span", ",", "event2_span", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.date_data_augmentation.getFlippedQuestions": [[69, 98], ["dataset.items", "date_data_augmentation.quesEvents", "question_tokenized_text.split", "print", "print", "print"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_comparison_prune.quesEvents"], ["", "def", "getFlippedQuestions", "(", "dataset", ")", ":", "\n", "    ", "qa", "=", "[", "]", "\n", "for", "passage_id", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "for", "question_answer", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "            ", "question_tokenized_text", "=", "question_answer", "[", "constants", ".", "tokenized_question", "]", "\n", "original_question_text", "=", "question_answer", "[", "constants", ".", "original_question", "]", "\n", "question_charidxs", "=", "question_answer", "[", "constants", ".", "question_charidxs", "]", "\n", "event_spans", "=", "quesEvents", "(", "question_tokenized_text", ")", "\n", "if", "event_spans", "is", "None", ":", "\n", "                ", "continue", "\n", "\n", "", "event1_span", ",", "event2_span", "=", "event_spans", "\n", "\n", "tokens", "=", "question_tokenized_text", ".", "split", "(", "\" \"", ")", "\n", "pretext", "=", "tokens", "[", "0", ":", "event1_span", "[", "0", "]", "]", "\n", "event2_text", "=", "tokens", "[", "event2_span", "[", "0", "]", ":", "event2_span", "[", "1", "]", "]", "\n", "mid_text", "=", "tokens", "[", "event1_span", "[", "1", "]", ":", "event2_span", "[", "0", "]", "]", "\n", "event1_text", "=", "tokens", "[", "event1_span", "[", "0", "]", ":", "event1_span", "[", "1", "]", "]", "\n", "end_text", "=", "tokens", "[", "event2_span", "[", "1", "]", ":", "]", "\n", "\n", "new_question_tokens", "=", "pretext", "+", "event2_text", "+", "mid_text", "+", "event1_text", "+", "end_text", "\n", "new_question_text", "=", "\" \"", ".", "join", "(", "new_question_tokens", ")", "\n", "\n", "print", "(", "question_tokenized_text", ")", "\n", "print", "(", "new_question_text", ")", "\n", "print", "(", ")", "\n", "\n", "# qa.append((question_text, span, first_or_last, events))", "\n", "", "", "return", "qa", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.blockdiagonal.masking_blockdiagonal": [[5, 32], ["torch.clamp", "torch.clamp", "torch.clamp.unsqueeze", "torch.clamp.unsqueeze", "allennlp.get_range_vector().unsqueeze", "allennlp.get_range_vector().unsqueeze", "allennlp.get_range_vector", "allennlp.get_range_vector", "allennlp.get_range_vector", "allennlp.get_range_vector"], "function", ["None"], ["def", "masking_blockdiagonal", "(", "passage_length", ",", "window", ",", "device_id", ")", ":", "\n", "    ", "\"\"\" Make a (passage_length, passage_length) tensor M of 1 and -1 in which for each row x,\n        M[x, y] = -1 if y < x - window or y > x + window, else it is 1.\n        Basically for the x-th row, the [x-win, x+win] columns should be 1, and rest -1\n    \"\"\"", "\n", "\n", "# The lower and upper limit of token-idx that won't be masked for a given token", "\n", "lower", "=", "allenutil", ".", "get_range_vector", "(", "passage_length", ",", "device", "=", "device_id", ")", "-", "window", "\n", "upper", "=", "allenutil", ".", "get_range_vector", "(", "passage_length", ",", "device", "=", "device_id", ")", "+", "window", "\n", "lower", "=", "torch", ".", "clamp", "(", "lower", ",", "min", "=", "0", ",", "max", "=", "passage_length", "-", "1", ")", "\n", "upper", "=", "torch", ".", "clamp", "(", "upper", ",", "min", "=", "0", ",", "max", "=", "passage_length", "-", "1", ")", "\n", "lower_un", "=", "lower", ".", "unsqueeze", "(", "1", ")", "\n", "upper_un", "=", "upper", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# Range vector for each row", "\n", "lower_range_vector", "=", "allenutil", ".", "get_range_vector", "(", "passage_length", ",", "device", "=", "device_id", ")", ".", "unsqueeze", "(", "0", ")", "\n", "upper_range_vector", "=", "allenutil", ".", "get_range_vector", "(", "passage_length", ",", "device", "=", "device_id", ")", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "# Masks for lower and upper limits of the mask", "\n", "lower_mask", "=", "lower_range_vector", ">=", "lower_un", "\n", "upper_mask", "=", "upper_range_vector", "<=", "upper_un", "\n", "\n", "# Final-mask that we require", "\n", "inwindow_mask", "=", "(", "lower_mask", "==", "upper_mask", ")", ".", "float", "(", ")", "\n", "outwindow_mask", "=", "(", "lower_mask", "!=", "upper_mask", ")", ".", "float", "(", ")", "\n", "\n", "return", "inwindow_mask", ",", "outwindow_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.func_call.func": [[4, 8], ["print", "print", "print"], "function", ["None"], ["def", "func", "(", "a", ",", "b", ",", "**", "kwargs", ")", ":", "\n", "    ", "print", "(", "a", ")", "\n", "print", "(", "b", ")", "\n", "print", "(", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.date_first_or_last.gold_first_last": [[18, 32], ["question.split"], "function", ["None"], ["", "def", "gold_first_last", "(", "question", ")", ":", "\n", "    ", "question_tokens", "=", "question", ".", "split", "(", "\" \"", ")", "\n", "lesser_tokens", "=", "[", "\"first\"", ",", "\"earlier\"", ",", "\"forst\"", ",", "\"firts\"", "]", "\n", "greater_tokens", "=", "[", "\"later\"", ",", "\"last\"", ",", "\"second\"", "]", "\n", "\n", "for", "t", "in", "lesser_tokens", ":", "\n", "        ", "if", "t", "in", "question_tokens", ":", "\n", "            ", "return", "\"first\"", "\n", "\n", "", "", "for", "t", "in", "greater_tokens", ":", "\n", "        ", "if", "t", "in", "question_tokens", ":", "\n", "            ", "return", "\"last\"", "\n", "\n", "", "", "return", "\"last\"", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.date_first_or_last.quesEvents": [[34, 80], ["qstr.split", "qstr.split", "qstr.split.index", "min", "len", "qstr.split.index", "qstr.split.index", "qstr.split.index", "qstr.split.index", "len", "qstr.split.index", "qstr.split.index", "qstr.split.index"], "function", ["None"], ["", "def", "quesEvents", "(", "qstr", ")", ":", "\n", "    ", "or_split", "=", "qstr", ".", "split", "(", "\" or \"", ")", "\n", "if", "len", "(", "or_split", ")", "!=", "2", ":", "\n", "        ", "return", "None", "\n", "\n", "", "tokens", "=", "qstr", ".", "split", "(", "\" \"", ")", "\n", "\n", "or_idx", "=", "tokens", ".", "index", "(", "\"or\"", ")", "\n", "# Last token is ? which we don't want to attend to", "\n", "event2", "=", "tokens", "[", "or_idx", "+", "1", ":", "len", "(", "tokens", ")", "-", "1", "]", "\n", "\n", "# Gets first index of the item", "\n", "try", ":", "\n", "        ", "comma_idx", "=", "tokens", ".", "index", "(", "\",\"", ")", "\n", "", "except", ":", "\n", "        ", "comma_idx", "=", "100000", "\n", "", "try", ":", "\n", "        ", "colon_idx", "=", "tokens", ".", "index", "(", "\":\"", ")", "\n", "", "except", ":", "\n", "        ", "colon_idx", "=", "100000", "\n", "\n", "", "try", ":", "\n", "        ", "hyphen_idx", "=", "tokens", ".", "index", "(", "\"-\"", ")", "\n", "", "except", ":", "\n", "        ", "hyphen_idx", "=", "100000", "\n", "\n", "", "split_idx", "=", "min", "(", "comma_idx", ",", "colon_idx", ",", "hyphen_idx", ")", "\n", "\n", "if", "split_idx", "==", "100000", "or", "(", "or_idx", "-", "split_idx", "<=", "1", ")", ":", "\n", "# print(f\"{qstr} first_split:{split_idx} or:{or_idx}\")", "\n", "        ", "if", "\"first\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"first\"", ")", "\n", "", "elif", "\"second\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"second\"", ")", "\n", "", "elif", "\"last\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"last\"", ")", "\n", "", "elif", "\"later\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"later\"", ")", "\n", "", "else", ":", "\n", "            ", "split_idx", "=", "-", "1", "\n", "\n", "", "", "assert", "split_idx", "!=", "-", "1", ",", "f\"{qstr} {split_idx} {or_idx}\"", "\n", "\n", "event1", "=", "tokens", "[", "split_idx", "+", "1", ":", "or_idx", "]", "\n", "\n", "return", "event1", ",", "event2", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.date_first_or_last.getQuesAnsTuples": [[82, 95], ["dataset.items", "date_first_or_last.gold_first_last", "date_first_or_last.quesEvents", "qa.append"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.date_first_or_last.gold_first_last", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_comparison_prune.quesEvents"], ["", "def", "getQuesAnsTuples", "(", "dataset", ")", ":", "\n", "    ", "qa", "=", "[", "]", "\n", "for", "passage_id", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "for", "question_answer", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "            ", "question_text", "=", "question_answer", "[", "constants", ".", "tokenized_question", "]", "\n", "first_or_last", "=", "gold_first_last", "(", "question_text", ")", "\n", "answer_annotation", "=", "question_answer", "[", "\"answer\"", "]", "\n", "span", "=", "answer_annotation", "[", "\"spans\"", "]", "[", "0", "]", "\n", "events", "=", "quesEvents", "(", "question_text", ")", "\n", "if", "events", "is", "None", ":", "\n", "                ", "continue", "\n", "", "qa", ".", "append", "(", "(", "question_text", ",", "span", ",", "first_or_last", ",", "events", ")", ")", "\n", "", "", "return", "qa", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.date_first_or_last.first_or_last": [[97, 109], ["set", "span.split", "set", "set", "len", "len", "event1.intersection", "event2.intersection"], "function", ["None"], ["", "def", "first_or_last", "(", "qa_pairs", ")", ":", "\n", "    ", "first_last_match", "=", "0", "\n", "for", "qa_pair", "in", "qa_pairs", ":", "\n", "        ", "(", "question_text", ",", "span", ",", "first_or_last", ",", "events", ")", "=", "qa_pair", "\n", "ans_tokens", "=", "set", "(", "span", ".", "split", "(", "\" \"", ")", ")", "\n", "event1", ",", "event2", "=", "set", "(", "events", "[", "0", "]", ")", ",", "set", "(", "events", "[", "1", "]", ")", "\n", "ans_event", "=", "\"first\"", "if", "len", "(", "event1", ".", "intersection", "(", "ans_tokens", ")", ")", ">", "len", "(", "event2", ".", "intersection", "(", "ans_tokens", ")", ")", "else", "\"last\"", "\n", "if", "ans_event", "==", "first_or_last", ":", "\n", "# print(f\"{ans_tokens}  ::  {event1}   {event2}\")", "\n", "            ", "first_last_match", "+=", "1", "\n", "\n", "", "", "return", "first_last_match", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.count_distribution.forward": [[9, 27], ["print", "loss.backward", "print", "torch.pow", "torch.exp", "torch.sum", "torch.log", "torch.no_grad", "mean.grad.zero_"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["def", "forward", "(", "mean", ",", "answer", ")", ":", "\n", "    ", "l2_by_vsquared", "=", "torch", ".", "pow", "(", "countvals", "-", "mean", ",", "2", ")", "/", "(", "2", "*", "variance", "*", "variance", ")", "\n", "# print(l2_by_vsquared)", "\n", "\n", "exp_val", "=", "torch", ".", "exp", "(", "-", "1", "*", "l2_by_vsquared", ")", "+", "1e-30", "\n", "# print(exp_val)", "\n", "\n", "distribution", "=", "exp_val", "/", "(", "torch", ".", "sum", "(", "exp_val", ")", ")", "\n", "print", "(", "distribution", ")", "\n", "\n", "loss", "=", "-", "1", "*", "torch", ".", "log", "(", "distribution", "[", "answer", "]", ")", "\n", "loss", ".", "backward", "(", "retain_graph", "=", "True", ")", "\n", "grad", "=", "mean", ".", "grad", "\n", "print", "(", "f\"Grad: {grad}\"", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# mean.data = mean.data + -1 * grad * lr", "\n", "        ", "mean", "-=", "mean", ".", "grad", "*", "lr", "\n", "mean", ".", "grad", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.multiprocess.process_chunk": [[9, 17], ["time.sleep", "d.strip"], "function", ["None"], ["def", "process_chunk", "(", "d", ")", ":", "\n", "    ", "\"\"\"Replace this with your own function\n    that processes data one line at a\n    time\"\"\"", "\n", "\n", "d", "=", "d", ".", "strip", "(", ")", "+", "\" processed\"", "\n", "time", ".", "sleep", "(", "0.001", ")", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.multiprocess.grouper": [[19, 25], ["range", "len"], "function", ["None"], ["", "def", "grouper", "(", "n", ",", "iterable", ",", "padvalue", "=", "None", ")", ":", "\n", "    ", "\"\"\"grouper(3, 'abcdefg', 'x') -->\n\t('a','b','c'), ('d','e','f'), ('g','x','x')\"\"\"", "\n", "\n", "chunk_size", "=", "n", "\n", "return", "[", "iterable", "[", "i", ":", "i", "+", "chunk_size", "]", "for", "i", "in", "range", "(", "0", ",", "len", "(", "iterable", ")", ",", "chunk_size", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.sample_data.readDataset": [[8, 12], ["open", "json.load"], "function", ["None"], ["def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.sample_data.make_sample": [[14, 24], ["dataset.items", "print"], "function", ["None"], ["", "def", "make_sample", "(", "dataset", ",", "num_paras", ")", ":", "\n", "    ", "output_dataset", "=", "{", "}", "\n", "paras_done", "=", "0", "\n", "for", "pid", ",", "pinfo", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "output_dataset", "[", "pid", "]", "=", "pinfo", "\n", "paras_done", "+=", "1", "\n", "if", "paras_done", "==", "num_paras", ":", "\n", "            ", "break", "\n", "", "", "print", "(", "f\"Paras sampled: {paras_done}\"", ")", "\n", "return", "output_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.sample_data.write_sample": [[26, 31], ["sample_data.readDataset", "sample_data.make_sample", "open", "json.dump"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.data_manipulation.simplify_programs.readDataset", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.sample_data.make_sample"], ["", "def", "write_sample", "(", "input_json", ",", "output_json", ",", "num_paras", ")", ":", "\n", "    ", "input_dataset", "=", "readDataset", "(", "input_json", ")", "\n", "output_dataset", "=", "make_sample", "(", "input_dataset", ",", "num_paras", "=", "num_paras", ")", "\n", "with", "open", "(", "output_json", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "output_dataset", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.sample_data.main": [[33, 41], ["os.path.join", "os.path.join", "sample_data.write_sample", "os.path.join", "os.path.join", "sample_data.write_sample"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.sample_data.write_sample", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.sample_data.write_sample"], ["", "", "def", "main", "(", ")", ":", "\n", "    ", "train_json", "=", "os", ".", "path", ".", "join", "(", "input_dir", ",", "\"drop_dataset_train.json\"", ")", "\n", "output_json", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"sample_train.json\"", ")", "\n", "write_sample", "(", "train_json", ",", "output_json", ",", "50", ")", "\n", "\n", "dev_json", "=", "os", ".", "path", ".", "join", "(", "input_dir", ",", "\"drop_dataset_dev.json\"", ")", "\n", "output_json", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"sample_dev.json\"", ")", "\n", "write_sample", "(", "dev_json", ",", "output_json", ",", "50", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.tokenization.Module.__init__": [[12, 15], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "name", ":", "str", ",", "identifier", ":", "int", ")", ":", "\n", "        ", "self", ".", "name", "=", "name", "\n", "self", ".", "identifier", "=", "identifier", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.tokenization.Module.to_dict": [[16, 22], ["None"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "json_dict", "=", "{", "\n", "\"name\"", ":", "self", ".", "name", ",", "\n", "\"identifier\"", ":", "self", ".", "identifier", "\n", "}", "\n", "return", "json_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.tokenization.add_identifier": [[24, 48], ["isinstance", "range", "tokenization.Module", "sub_expression.insert", "tokenization.Module", "len", "tokenization.add_identifier", "sub_expression.append"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.add_identifier"], ["", "", "def", "add_identifier", "(", "nested_expression", ",", "count", ")", ":", "\n", "    ", "\"\"\"Convert the nested_expression into a representation that contains the order in which the modules are executed.\n\n    This function converts the nested_expression of module-as-str into expression with module-as-Module class where the\n    class stores an `identifier` key which is the number at which the module was executed.\n\n    Since the program-tree is executed in a left-to-right post-traversal order we will traverse the tree in a similar\n    manner to number the modules in the nested-expression.\n    \"\"\"", "\n", "# If expression is not a list (hence a str) it's a Module", "\n", "if", "not", "isinstance", "(", "nested_expression", ",", "list", ")", ":", "\n", "        ", "return", "Module", "(", "name", "=", "nested_expression", ",", "identifier", "=", "count", ")", ",", "count", "+", "1", "\n", "# If expression is tree", "\n", "", "else", ":", "\n", "        ", "sub_expression", "=", "[", "]", "\n", "# Performing left-to-right post traversal of the tree", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "nested_expression", ")", ")", ":", "\n", "            ", "arg_i", ",", "count", "=", "add_identifier", "(", "nested_expression", "[", "i", "]", ",", "count", ")", "\n", "sub_expression", ".", "append", "(", "arg_i", ")", "\n", "# Then add the root-module of the tree", "\n", "", "arg_0", "=", "Module", "(", "name", "=", "nested_expression", "[", "0", "]", ",", "identifier", "=", "count", ")", "\n", "sub_expression", ".", "insert", "(", "0", ",", "arg_0", ")", "\n", "\n", "return", "sub_expression", ",", "count", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.tokenization.convert_module_expression_tree_to_dict": [[50, 60], ["enumerate", "isinstance", "mapped_expression.append", "isinstance", "tokenization.convert_module_expression_tree_to_dict", "mapped_expression.append", "argument.to_dict"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.convert_module_expression_tree_to_dict", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.Module.to_dict"], ["", "", "def", "convert_module_expression_tree_to_dict", "(", "module_expression", ")", ":", "\n", "    ", "mapped_expression", "=", "[", "]", "\n", "for", "i", ",", "argument", "in", "enumerate", "(", "module_expression", ")", ":", "\n", "        ", "if", "isinstance", "(", "argument", ",", "list", ")", ":", "\n", "            ", "mapped_expression", ".", "append", "(", "convert_module_expression_tree_to_dict", "(", "argument", ")", ")", "\n", "", "elif", "isinstance", "(", "argument", ",", "Module", ")", ":", "\n", "            ", "mapped_expression", ".", "append", "(", "argument", ".", "to_dict", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "", "return", "mapped_expression", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.relocate.Output.__init__": [[5, 9], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "input_name", ":", "str", ",", "values", ":", "List", "[", "float", "]", ",", "label", ":", "Optional", "[", "str", "]", "=", "None", ")", ":", "\n", "        ", "self", ".", "input_name", "=", "input_name", "\n", "self", ".", "values", "=", "values", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.relocate.Output.toJSON": [[10, 17], ["None"], "methods", ["None"], ["", "def", "toJSON", "(", "self", ")", ":", "\n", "        ", "json_dict", "=", "{", "\n", "\"input_name\"", ":", "self", ".", "input_name", ",", "\n", "\"values\"", ":", "self", ".", "values", ",", "\n", "\"label\"", ":", "self", ".", "label", "\n", "}", "\n", "return", "json_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.allowed_prefix_beamsearch.SimpleState.__init__": [[20, 29], ["super().__init__", "len"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "batch_indices", ":", "List", "[", "int", "]", ",", "\n", "action_history", ":", "List", "[", "List", "[", "int", "]", "]", ",", "\n", "score", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", "start_values", ":", "List", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "batch_indices", ",", "action_history", ",", "score", ")", "\n", "self", ".", "start_values", "=", "start_values", "or", "[", "0", "]", "*", "len", "(", "batch_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.allowed_prefix_beamsearch.SimpleState.is_finished": [[30, 32], ["None"], "methods", ["None"], ["", "def", "is_finished", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "self", ".", "action_history", "[", "0", "]", "[", "-", "1", "]", "==", "4", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.allowed_prefix_beamsearch.SimpleState.combine_states": [[33, 42], ["allowed_prefix_beamsearch.SimpleState"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "combine_states", "(", "cls", ",", "states", ")", "->", "\"SimpleState\"", ":", "\n", "        ", "batch_indices", "=", "[", "batch_index", "for", "state", "in", "states", "for", "batch_index", "in", "state", ".", "batch_indices", "]", "\n", "action_histories", "=", "[", "\n", "action_history", "for", "state", "in", "states", "for", "action_history", "in", "state", ".", "action_history", "\n", "]", "\n", "scores", "=", "[", "score", "for", "state", "in", "states", "for", "score", "in", "state", ".", "score", "]", "\n", "start_values", "=", "[", "start_value", "for", "state", "in", "states", "for", "start_value", "in", "state", ".", "start_values", "]", "\n", "return", "SimpleState", "(", "batch_indices", ",", "action_histories", ",", "scores", ",", "start_values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.allowed_prefix_beamsearch.SimpleState.__repr__": [[43, 45], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "f\"{self.action_history}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.allowed_prefix_beamsearch.SimpleTransitionFunction.__init__": [[48, 56], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "valid_actions", ":", "Set", "[", "int", "]", "=", "None", ",", "include_value_in_score", ":", "bool", "=", "False", "\n", ")", "->", "None", ":", "\n", "# The default allowed actions are adding 1 or 2 to the last element.", "\n", "        ", "self", ".", "_valid_actions", "=", "valid_actions", "or", "{", "1", ",", "2", "}", "\n", "# If True, we will add a small multiple of the action take to the score, to encourage", "\n", "# getting higher numbers first (and to differentiate action sequences).", "\n", "self", ".", "_include_value_in_score", "=", "include_value_in_score", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.allowed_prefix_beamsearch.SimpleTransitionFunction.take_step": [[57, 91], ["collections.defaultdict", "zip", "indexed_next_states.values", "sorted_next_states.sort", "next_states.extend", "len", "int", "allowed_prefix_beamsearch.SimpleState", "indexed_next_states[].append"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "take_step", "(", "\n", "self", ",", "state", ":", "SimpleState", ",", "max_actions", ":", "int", "=", "None", ",", "allowed_actions", ":", "List", "[", "Set", "]", "=", "None", "\n", ")", "->", "List", "[", "SimpleState", "]", ":", "\n", "        ", "indexed_next_states", ":", "Dict", "[", "int", ",", "List", "[", "SimpleState", "]", "]", "=", "defaultdict", "(", "list", ")", "\n", "if", "not", "allowed_actions", ":", "\n", "            ", "allowed_actions", "=", "[", "None", "]", "*", "len", "(", "state", ".", "batch_indices", ")", "\n", "", "for", "batch_index", ",", "action_history", ",", "score", ",", "start_value", ",", "actions", "in", "zip", "(", "\n", "state", ".", "batch_indices", ",", "\n", "state", ".", "action_history", ",", "\n", "state", ".", "score", ",", "\n", "state", ".", "start_values", ",", "\n", "allowed_actions", ",", "\n", ")", ":", "\n", "            ", "prev_action", "=", "action_history", "[", "-", "1", "]", "if", "action_history", "else", "start_value", "\n", "for", "action", "in", "self", ".", "_valid_actions", ":", "\n", "                ", "next_item", "=", "int", "(", "prev_action", "+", "action", ")", "\n", "if", "actions", "and", "next_item", "not", "in", "actions", ":", "\n", "                    ", "continue", "\n", "", "new_history", "=", "action_history", "+", "[", "next_item", "]", "\n", "# For every action taken, we reduce the score by 1.", "\n", "new_score", "=", "score", "-", "1", "\n", "if", "self", ".", "_include_value_in_score", ":", "\n", "                    ", "new_score", "+=", "0.01", "*", "next_item", "\n", "", "new_state", "=", "SimpleState", "(", "[", "batch_index", "]", ",", "[", "new_history", "]", ",", "[", "new_score", "]", ")", "\n", "indexed_next_states", "[", "batch_index", "]", ".", "append", "(", "new_state", ")", "\n", "", "", "next_states", ":", "List", "[", "SimpleState", "]", "=", "[", "]", "\n", "for", "batch_next_states", "in", "indexed_next_states", ".", "values", "(", ")", ":", "\n", "            ", "sorted_next_states", "=", "[", "(", "-", "state", ".", "score", "[", "0", "]", ".", "data", "[", "0", "]", ",", "state", ")", "for", "state", "in", "batch_next_states", "]", "\n", "sorted_next_states", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "if", "max_actions", "is", "not", "None", ":", "\n", "                ", "sorted_next_states", "=", "sorted_next_states", "[", ":", "max_actions", "]", "\n", "", "next_states", ".", "extend", "(", "state", "[", "1", "]", "for", "state", "in", "sorted_next_states", ")", "\n", "", "return", "next_states", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.allowed_prefix_beamsearch.test_constraints": [[92, 178], ["allowed_prefix_beamsearch.SimpleState", "semqa.state_machines.prefixed_beam_search.PrefixedConstrainedBeamSearch", "print", "allowed_prefix_beamsearch.SimpleTransitionFunction", "PrefixedBeamSearch.search", "print", "beam_search.search.items", "print", "exit", "PrefixedBeamSearch", "allowed_prefix_beamsearch.SimpleTransitionFunction", "PrefixedBeamSearch.search", "beam_snapshots.get", "enumerate", "print", "len", "len", "len", "len", "all", "print", "len", "any", "torch.Tensor", "torch.Tensor", "torch.Tensor", "len"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.state_machines.prefixed_beam_search.PrefixedConstrainedBeamSearch.search", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.state_machines.prefixed_beam_search.PrefixedConstrainedBeamSearch.search"], ["", "", "def", "test_constraints", "(", ")", ":", "\n", "# The simple transition system starts at some number, adds one or two at each state, and", "\n", "# tries to get to 4.  The highest scoring path has the shortest length and the highest", "\n", "# numbers (so always add two, unless you're at 3).  From -3, there are lots of possible", "\n", "# sequences: [-2, -1, 0, 1, 2, 3, 4], [-1, 1, 3, 4], ...  We'll specify a few of those up", "\n", "# front as \"allowed\", and use that to test the constrained beam search implementation.", "\n", "    ", "initial_state", "=", "SimpleState", "(", "\n", "batch_indices", "=", "[", "0", ",", "1", ",", "2", "]", ",", "\n", "action_history", "=", "[", "[", "]", ",", "[", "]", ",", "[", "]", "]", ",", "\n", "score", "=", "[", "torch", ".", "Tensor", "(", "[", "0.0", "]", ")", ",", "torch", ".", "Tensor", "(", "[", "0.0", "]", ")", ",", "torch", ".", "Tensor", "(", "[", "0.0", "]", ")", "]", ",", "\n", "start_values", "=", "[", "-", "3", ",", "0", ",", "-", "1", "]", "\n", ")", "\n", "\n", "beam_size", "=", "5", "\n", "# initial_sequence = torch.Tensor([-2, -1, 0, 1])", "\n", "initial_sequence", "=", "[", "[", "[", "-", "2", ",", "-", "1", "]", ",", "[", "-", "1", ",", "0", ",", "1", "]", "]", ",", "[", "]", ",", "[", "[", "1", "]", ",", "[", "0", ",", "2", ",", "4", "]", "]", "]", "\n", "\n", "# bm = BeamSearch(beam_size, initial_sequence=torch.Tensor([-2, -1, 0, 1]))", "\n", "# print(bm._allowed_transitions)", "\n", "\n", "beam_search", "=", "PrefixedConstrainedBeamSearch", "(", "beam_size", ",", "allowed_sequences", "=", "initial_sequence", ")", "\n", "print", "(", "beam_search", ".", "_allowed_transitions", ")", "\n", "\n", "decoder_step", "=", "SimpleTransitionFunction", "(", "include_value_in_score", "=", "True", ")", "\n", "# best_states = beam_search.search(7, initial_state, decoder_step)", "\n", "best_states", "=", "beam_search", ".", "search", "(", "num_steps", "=", "7", ",", "initial_state", "=", "initial_state", ",", "transition_function", "=", "decoder_step", ",", "\n", "keep_final_unfinished_states", "=", "True", ")", "\n", "\n", "print", "(", "\"\\nBest States after Beam Search\"", ")", "\n", "for", "k", ",", "states", "in", "best_states", ".", "items", "(", ")", ":", "\n", "        ", "print", "(", "f\"BatchIndex: {k}\"", ")", "\n", "for", "st", "in", "states", ":", "\n", "            ", "print", "(", "f\"\\t Action Seq: {st.action_history[0]}\"", ")", "\n", "", "", "print", "(", "\"----\\n\"", ")", "\n", "exit", "(", ")", "\n", "\n", "# assert len(best_states) == 1", "\n", "\n", "# After the constraint runs out, we generate [3], [2],", "\n", "# then we generate [3, 5], [3, 4], [2, 4], the latter two of which are finished,", "\n", "# then we generate [3, 5, 7], [3, 5, 6], and we're out of steps, so we keep the former", "\n", "assert", "best_states", "[", "0", "]", "[", "0", "]", ".", "action_history", "[", "0", "]", "==", "[", "-", "2", ",", "-", "1", ",", "0", ",", "1", ",", "3", ",", "4", "]", "\n", "assert", "best_states", "[", "0", "]", "[", "1", "]", ".", "action_history", "[", "0", "]", "==", "[", "-", "2", ",", "-", "1", ",", "0", ",", "1", ",", "2", ",", "4", "]", "\n", "assert", "best_states", "[", "0", "]", "[", "2", "]", ".", "action_history", "[", "0", "]", "==", "[", "-", "2", ",", "-", "1", ",", "0", ",", "1", ",", "3", ",", "5", ",", "7", "]", "\n", "\n", "assert", "best_states", "[", "1", "]", "[", "0", "]", ".", "action_history", "[", "0", "]", "==", "[", "-", "2", ",", "-", "1", ",", "0", ",", "1", ",", "3", ",", "4", "]", "\n", "assert", "best_states", "[", "1", "]", "[", "1", "]", ".", "action_history", "[", "0", "]", "==", "[", "-", "2", ",", "-", "1", ",", "0", ",", "1", ",", "2", ",", "4", "]", "\n", "assert", "best_states", "[", "1", "]", "[", "2", "]", ".", "action_history", "[", "0", "]", "==", "[", "-", "2", ",", "-", "1", ",", "0", ",", "1", ",", "3", ",", "5", ",", "7", "]", "\n", "\n", "\n", "# Now set the beam size to 6, we generate [3], [2]", "\n", "# then [3, 5], [2, 3], [3, 4], [2, 4] (the latter two of which are finished)", "\n", "# then [3, 5, 6], [3, 5, 7], [2, 3, 5], [2, 3, 4] (the last is finished)", "\n", "beam_size", "=", "6", "\n", "beam_search", "=", "PrefixedBeamSearch", "(", "\n", "beam_size", ",", "initial_sequence", "=", "initial_sequence", ",", "keep_beam_details", "=", "True", "\n", ")", "\n", "decoder_step", "=", "SimpleTransitionFunction", "(", "include_value_in_score", "=", "True", ")", "\n", "best_states", "=", "beam_search", ".", "search", "(", "\n", "7", ",", "initial_state", ",", "decoder_step", ",", "keep_final_unfinished_states", "=", "False", "\n", ")", "\n", "\n", "assert", "len", "(", "best_states", ")", "==", "2", "\n", "assert", "len", "(", "best_states", "[", "0", "]", ")", "==", "3", "\n", "assert", "len", "(", "best_states", "[", "1", "]", ")", "==", "3", "\n", "assert", "best_states", "[", "0", "]", "[", "0", "]", ".", "action_history", "[", "0", "]", "==", "[", "-", "2", ",", "-", "1", ",", "0", ",", "1", ",", "3", ",", "4", "]", "\n", "assert", "best_states", "[", "0", "]", "[", "1", "]", ".", "action_history", "[", "0", "]", "==", "[", "-", "2", ",", "-", "1", ",", "0", ",", "1", ",", "2", ",", "4", "]", "\n", "assert", "best_states", "[", "0", "]", "[", "2", "]", ".", "action_history", "[", "0", "]", "==", "[", "-", "2", ",", "-", "1", ",", "0", ",", "1", ",", "2", ",", "3", ",", "4", "]", "\n", "\n", "assert", "best_states", "[", "1", "]", "[", "0", "]", ".", "action_history", "[", "0", "]", "==", "[", "-", "2", ",", "-", "1", ",", "0", ",", "1", ",", "3", ",", "4", "]", "\n", "assert", "best_states", "[", "1", "]", "[", "1", "]", ".", "action_history", "[", "0", "]", "==", "[", "-", "2", ",", "-", "1", ",", "0", ",", "1", ",", "2", ",", "4", "]", "\n", "assert", "best_states", "[", "1", "]", "[", "2", "]", ".", "action_history", "[", "0", "]", "==", "[", "-", "2", ",", "-", "1", ",", "0", ",", "1", ",", "2", ",", "3", ",", "4", "]", "\n", "\n", "# Check that beams are correct", "\n", "best_action_sequence", "=", "best_states", "[", "0", "]", "[", "0", "]", ".", "action_history", "[", "0", "]", "\n", "\n", "beam_snapshots", "=", "beam_search", ".", "beam_snapshots", "\n", "assert", "len", "(", "beam_snapshots", ")", "==", "2", "\n", "\n", "beam_snapshots0", "=", "beam_snapshots", ".", "get", "(", "0", ")", "\n", "assert", "beam_snapshots0", "is", "not", "None", "\n", "\n", "for", "i", ",", "beam", "in", "enumerate", "(", "beam_snapshots0", ")", ":", "\n", "        ", "assert", "all", "(", "len", "(", "sequence", ")", "==", "i", "+", "1", "for", "_", ",", "sequence", "in", "beam", ")", "\n", "if", "i", "<", "len", "(", "best_action_sequence", ")", ":", "\n", "            ", "assert", "any", "(", "sequence", "[", "-", "1", "]", "==", "best_action_sequence", "[", "i", "]", "for", "_", ",", "sequence", "in", "beam", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.allowed_prefix_beamsearch.main": [[180, 182], ["allowed_prefix_beamsearch.test_constraints"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.allowed_prefix_beamsearch.test_constraints"], ["", "", "", "def", "main", "(", ")", ":", "\n", "    ", "test_constraints", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.date_compare_attn.questionAttns": [[18, 73], ["qstr.split", "qstr.split", "qstr.split.index", "min", "len", "sum", "len", "len", "qstr.split.index", "qstr.split.index", "qstr.split.index", "sum", "len", "len", "range", "range", "qstr.split.index", "len", "len", "len", "len", "qstr.split.index", "qstr.split.index", "qstr.split.index"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "def", "questionAttns", "(", "qstr", ")", ":", "\n", "    ", "or_split", "=", "qstr", ".", "split", "(", "\" or \"", ")", "\n", "if", "len", "(", "or_split", ")", "!=", "2", ":", "\n", "        ", "return", "(", "0", ",", "0", ")", "\n", "\n", "", "tokens", "=", "qstr", ".", "split", "(", "\" \"", ")", "\n", "attn_1", "=", "[", "0", "for", "_", "in", "range", "(", "len", "(", "tokens", ")", ")", "]", "\n", "attn_2", "=", "[", "0", "for", "_", "in", "range", "(", "len", "(", "tokens", ")", ")", "]", "\n", "\n", "or_idx", "=", "tokens", ".", "index", "(", "\"or\"", ")", "\n", "# Last token is ? which we don't want to attend to", "\n", "attn_2", "[", "or_idx", "+", "1", ":", "len", "(", "tokens", ")", "-", "1", "]", "=", "[", "1", "]", "*", "(", "len", "(", "tokens", ")", "-", "or_idx", "-", "2", ")", "\n", "assert", "sum", "(", "attn_2", ")", ">", "0", "\n", "assert", "len", "(", "tokens", ")", "==", "len", "(", "attn_2", ")", "\n", "\n", "# Gets first index of the item", "\n", "try", ":", "\n", "        ", "comma_idx", "=", "tokens", ".", "index", "(", "\",\"", ")", "\n", "", "except", ":", "\n", "        ", "comma_idx", "=", "100000", "\n", "", "try", ":", "\n", "        ", "colon_idx", "=", "tokens", ".", "index", "(", "\":\"", ")", "\n", "", "except", ":", "\n", "        ", "colon_idx", "=", "100000", "\n", "\n", "", "try", ":", "\n", "        ", "hyphen_idx", "=", "tokens", ".", "index", "(", "\"-\"", ")", "\n", "", "except", ":", "\n", "        ", "hyphen_idx", "=", "100000", "\n", "\n", "", "split_idx", "=", "min", "(", "comma_idx", ",", "colon_idx", ",", "hyphen_idx", ")", "\n", "\n", "if", "split_idx", "==", "100000", "or", "(", "or_idx", "-", "split_idx", "<=", "1", ")", ":", "\n", "# print(f\"{qstr} first_split:{split_idx} or:{or_idx}\")", "\n", "        ", "if", "\"first\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"first\"", ")", "\n", "", "elif", "\"second\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"second\"", ")", "\n", "", "elif", "\"last\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"last\"", ")", "\n", "", "elif", "\"later\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"later\"", ")", "\n", "", "else", ":", "\n", "            ", "split_idx", "=", "-", "1", "\n", "\n", "", "", "assert", "split_idx", "!=", "-", "1", ",", "f\"{qstr} {split_idx} {or_idx}\"", "\n", "\n", "attn_1", "[", "split_idx", "+", "1", ":", "or_idx", "]", "=", "[", "1", "]", "*", "(", "or_idx", "-", "split_idx", "-", "1", ")", "\n", "\n", "assert", "sum", "(", "attn_1", ")", ">", "0", ",", "f\"{qstr} {split_idx} {or_idx}\"", "\n", "assert", "len", "(", "tokens", ")", "==", "len", "(", "attn_1", ")", "\n", "\n", "# print([(x,y,z) for x,y,z in zip(tokens, attn_1, attn_2)])", "\n", "\n", "return", "attn_1", ",", "attn_2", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.date_compare_attn.quesAttnsDataset": [[75, 82], ["collections.defaultdict", "print", "date_compare_attn.questionAttns"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.questionAttns"], ["", "def", "quesAttnsDataset", "(", "questions", ")", ":", "\n", "    ", "or_split_count", "=", "defaultdict", "(", "int", ")", "\n", "\n", "for", "q", "in", "questions", ":", "\n", "        ", "(", "a1", ",", "a2", ")", "=", "questionAttns", "(", "q", ")", "\n", "\n", "", "print", "(", "\"all done\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.lf_dist.read_demo_ouput": [[6, 24], ["open", "json.loads", "print", "print", "print", "print", "module_dict.items", "print", "module_output_dict.items", "print"], "function", ["None"], ["def", "read_demo_ouput", "(", "jsonl_filepath", ")", ":", "\n", "    ", "with", "open", "(", "jsonl_filepath", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "demo_output", "=", "json", ".", "loads", "(", "line", ")", "\n", "question", "=", "demo_output", "[", "\"question\"", "]", "\n", "program_lisp", "=", "demo_output", "[", "\"program_lisp\"", "]", "\n", "program_nested_expression", "=", "demo_output", "[", "\"program_nested_expression\"", "]", "\n", "\n", "print", "(", "\"question: {}\"", ".", "format", "(", "question", ")", ")", "\n", "print", "(", "\"program lisp: {}\"", ".", "format", "(", "program_lisp", ")", ")", "\n", "print", "(", "\"program_nested_expression: {}\"", ".", "format", "(", "program_nested_expression", ")", ")", "\n", "program_execution", "=", "demo_output", "[", "\"program_execution\"", "]", "\n", "print", "(", "\"program_execution\"", ")", "\n", "for", "module_dict", "in", "program_execution", ":", "\n", "                ", "for", "module_name", ",", "module_output_dict", "in", "module_dict", ".", "items", "(", ")", ":", "\n", "                    ", "print", "(", "\"  {}\"", ".", "format", "(", "module_name", ")", ")", "\n", "for", "output_type", ",", "attention", "in", "module_output_dict", ".", "items", "(", ")", ":", "\n", "                        ", "print", "(", "\"    {}\"", ".", "format", "(", "output_type", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.date_compare.compute_date_greater_than_matrix": [[9, 22], ["range", "allennlp.move_to_device", "allennlp.move_to_device", "len", "range", "torch.FloatTensor", "torch.FloatTensor", "range", "range", "len", "range", "len", "range", "len", "len", "len"], "function", ["None"], ["def", "compute_date_greater_than_matrix", "(", "date_values", ":", "List", "[", "Date", "]", ",", "device_id", ":", "int", ")", ":", "\n", "    ", "date_greater_than_mat", "=", "[", "[", "0", "for", "_", "in", "range", "(", "len", "(", "date_values", ")", ")", "]", "for", "_", "in", "range", "(", "len", "(", "date_values", ")", ")", "]", "\n", "date_lesser_than_mat", "=", "[", "[", "0", "for", "_", "in", "range", "(", "len", "(", "date_values", ")", ")", "]", "for", "_", "in", "range", "(", "len", "(", "date_values", ")", ")", "]", "\n", "# self.encoded_passage.new_zeros(self.num_passage_dates, self.num_passage_dates)", "\n", "for", "i", "in", "range", "(", "len", "(", "date_values", ")", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "len", "(", "date_values", ")", ")", ":", "\n", "            ", "date_greater_than_mat", "[", "i", "]", "[", "j", "]", "=", "1.0", "if", "date_values", "[", "i", "]", ">", "date_values", "[", "j", "]", "else", "0.0", "\n", "date_lesser_than_mat", "[", "i", "]", "[", "j", "]", "=", "1.0", "if", "date_values", "[", "i", "]", "<", "date_values", "[", "j", "]", "else", "0.0", "\n", "# date_greater_than_mat[j][i] = 1.0 - date_greater_than_mat[i][j]", "\n", "", "", "date_greater_than_mat", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "FloatTensor", "(", "date_greater_than_mat", ")", ",", "device_id", ")", "\n", "date_lesser_than_mat", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "FloatTensor", "(", "date_lesser_than_mat", ")", ",", "device_id", ")", "\n", "\n", "return", "date_greater_than_mat", ",", "date_lesser_than_mat", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.date_compare.dt_greater": [[24, 28], ["torch.matmul", "dt1.unsqueeze", "dt2.unsqueeze"], "function", ["None"], ["", "def", "dt_greater", "(", "dt1", ",", "dt2", ",", "date_gt_mat", ")", ":", "\n", "    ", "joint_prob", "=", "torch", ".", "matmul", "(", "dt1", ".", "unsqueeze", "(", "1", ")", ",", "dt2", ".", "unsqueeze", "(", "0", ")", ")", "\n", "bool_greater", "=", "(", "date_gt_mat", "*", "joint_prob", ")", ".", "sum", "(", ")", "\n", "return", "bool_greater", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.date_compare.dt_lesser": [[30, 34], ["torch.matmul", "dt1.unsqueeze", "dt2.unsqueeze"], "function", ["None"], ["", "def", "dt_lesser", "(", "dt1", ",", "dt2", ",", "date_lt_mat", ")", ":", "\n", "    ", "joint_prob", "=", "torch", ".", "matmul", "(", "dt1", ".", "unsqueeze", "(", "1", ")", ",", "dt2", ".", "unsqueeze", "(", "0", ")", ")", "\n", "bool_lesser", "=", "(", "date_lt_mat", "*", "joint_prob", ")", ".", "sum", "(", ")", "\n", "return", "bool_lesser", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.f1.f1metric": [[7, 24], ["print", "allennlp.tools.squad_eval.metric_max_over_ground_truths", "allennlp.tools.drop_eval.answer_json_to_strings"], "function", ["None"], ["def", "f1metric", "(", "prediction", ":", "Union", "[", "str", ",", "List", "]", ",", "ground_truths", ":", "List", ")", ":", "# type: ignore", "\n", "    ", "\"\"\"\n    Parameters\n    ----------a\n    prediction: ``Union[str, List]``\n        The predicted answer from the model evaluated. This could be a string, or a list of string\n        when multiple spans are predicted as answer.\n    ground_truths: ``List``\n        All the ground truth answer annotations.\n    \"\"\"", "\n", "# If you wanted to split this out by answer type, you could look at [1] here and group by", "\n", "# that, instead of only keeping [0].", "\n", "ground_truth_answer_strings", "=", "[", "answer_json_to_strings", "(", "annotation", ")", "[", "0", "]", "for", "annotation", "in", "ground_truths", "]", "\n", "print", "(", "ground_truth_answer_strings", ")", "\n", "exact_match", ",", "f1_score", "=", "metric_max_over_ground_truths", "(", "drop_em_and_f1", ",", "prediction", ",", "ground_truth_answer_strings", ")", "\n", "\n", "return", "(", "exact_match", ",", "f1_score", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.number_token_max.max_number_distribution": [[5, 11], ["num_dist.cumsum", "torch.clamp", "torch.cat", "cum_dist_n.new_zeros"], "function", ["None"], ["def", "max_number_distribution", "(", "num_dist", ")", ":", "\n", "    ", "cum_dist", "=", "num_dist", ".", "cumsum", "(", "0", ")", "\n", "cum_dist_n", "=", "cum_dist", "**", "5", "\n", "maximum_distribution", "=", "cum_dist_n", "-", "torch", ".", "cat", "(", "[", "cum_dist_n", ".", "new_zeros", "(", "1", ")", ",", "cum_dist_n", "[", ":", "-", "1", "]", "]", ")", "\n", "maximum_distribution", "=", "torch", ".", "clamp", "(", "maximum_distribution", ",", "min", "=", "1e-10", ",", "max", "=", "1", "-", "1e-10", ")", "\n", "return", "maximum_distribution", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.number_token_max.find_numerindices_sorted_order": [[13, 24], ["list", "sorted", "zip", "zip"], "function", ["None"], ["", "def", "find_numerindices_sorted_order", "(", "number_values", ",", "number_token_indices", ",", "passage_number_entidxs", ")", ":", "\n", "# These are the values for the numbers for the number-tokens", "\n", "    ", "number_token_numbervalues", "=", "[", "number_values", "[", "x", "]", "for", "x", "in", "passage_number_entidxs", "]", "\n", "\n", "number_tokenidx_values", "=", "list", "(", "zip", "(", "number_token_indices", ",", "number_token_numbervalues", ")", ")", "\n", "\n", "sorted_numberidx_value_tuples", "=", "sorted", "(", "number_tokenidx_values", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ")", "\n", "\n", "sorted_number_indices", ",", "_", "=", "zip", "(", "*", "sorted_numberidx_value_tuples", ")", "\n", "\n", "return", "sorted_number_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.number_token_max.to_long_tensor": [[26, 28], ["torch.LongTensor"], "function", ["None"], ["", "def", "to_long_tensor", "(", "array", ")", ":", "\n", "    ", "return", "torch", ".", "LongTensor", "(", "array", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.max_min_dist.max_dist": [[4, 12], ["range", "len", "cum_dist.append", "math.pow", "zip"], "function", ["None"], ["def", "max_dist", "(", "probs", ",", "samples", "=", "10", ")", ":", "\n", "    ", "cum_dist", "=", "[", "probs", "[", "0", "]", "]", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "probs", ")", ")", ":", "\n", "        ", "cum_dist", ".", "append", "(", "probs", "[", "i", "]", "+", "cum_dist", "[", "i", "-", "1", "]", ")", "\n", "", "cum_dist_n", "=", "[", "math", ".", "pow", "(", "x", ",", "samples", ")", "for", "x", "in", "cum_dist", "]", "\n", "cum_dist_strict_less_n", "=", "[", "0", "]", "+", "cum_dist_n", "[", ":", "-", "1", "]", "\n", "max_dist", "=", "[", "x", "-", "y", "for", "(", "x", ",", "y", ")", "in", "zip", "(", "cum_dist_n", ",", "cum_dist_strict_less_n", ")", "]", "\n", "return", "max_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.max_min_dist.min_dist": [[37, 45], ["range", "len", "inverse_cum_dist.append", "math.pow", "zip"], "function", ["None"], ["def", "min_dist", "(", "probs", ",", "samples", "=", "10", ")", ":", "\n", "    ", "inverse_cum_dist", "=", "[", "1", "]", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "probs", ")", ")", ":", "\n", "        ", "inverse_cum_dist", ".", "append", "(", "inverse_cum_dist", "[", "i", "-", "1", "]", "-", "probs", "[", "i", "-", "1", "]", ")", "\n", "", "inverse_cum_dist_n", "=", "[", "math", ".", "pow", "(", "x", ",", "samples", ")", "for", "x", "in", "inverse_cum_dist", "]", "\n", "inverse_cum_dist_shift_n", "=", "inverse_cum_dist_n", "[", "1", ":", "]", "+", "[", "0", "]", "\n", "min_dist", "=", "[", "x", "-", "y", "for", "(", "x", ",", "y", ")", "in", "zip", "(", "inverse_cum_dist_n", ",", "inverse_cum_dist_shift_n", ")", "]", "\n", "return", "min_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.date_gold_lf.questionAttns": [[25, 43], ["qstr.split", "print"], "function", ["None"], ["def", "questionAttns", "(", "qstr", ")", ":", "\n", "    ", "tokens", "=", "qstr", ".", "split", "(", "\" \"", ")", "\n", "\n", "lesser_tokens", "=", "[", "\"first\"", ",", "\"earlier\"", ",", "\"forst\"", ",", "\"firts\"", "]", "\n", "greater_tokens", "=", "[", "\"later\"", ",", "\"last\"", ",", "\"second\"", "]", "\n", "\n", "func", "=", "None", "\n", "\n", "for", "t", "in", "lesser_tokens", ":", "\n", "        ", "if", "t", "in", "tokens", ":", "\n", "            ", "return", "date_lesser", "\n", "\n", "", "", "for", "t", "in", "greater_tokens", ":", "\n", "        ", "if", "t", "in", "tokens", ":", "\n", "            ", "return", "date_greater", "\n", "\n", "", "", "print", "(", "qstr", ")", "\n", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.date_gold_lf.quesAttnsDataset": [[45, 61], ["collections.defaultdict", "print", "print", "date_gold_lf.questionAttns", "print"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.questionAttns"], ["", "def", "quesAttnsDataset", "(", "questions", ")", ":", "\n", "    ", "not_parsed", "=", "0", "\n", "func_counter", "=", "defaultdict", "(", "int", ")", "\n", "\n", "for", "q", "in", "questions", ":", "\n", "\n", "        ", "func", "=", "questionAttns", "(", "q", ")", "\n", "func_counter", "[", "func", "]", "+=", "1", "\n", "\n", "print", "(", "f\"{func} : {q}\"", ")", "\n", "\n", "if", "not", "func", ":", "\n", "            ", "not_parsed", "+=", "1", "\n", "\n", "", "", "print", "(", "func_counter", ")", "\n", "print", "(", "f\"Not parsed: {not_parsed}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.attention2count.number2count_auxloss": [[9, 45], ["len", "max", "allennlp.move_to_device", "allennlp.get_range_vector", "print", "mask.new_zeros().normal_().abs_", "mask.new_zeros().normal_().abs_.new_zeros().long", "enumerate", "len", "torch.LongTensor", "random.randint", "torch.sum().unsqueeze", "mask.new_zeros().normal_", "mask.new_zeros().normal_().abs_.new_zeros", "min", "random.sample", "util.get_range_vector.unsqueeze", "util.move_to_device.unsqueeze", "range", "torch.sum", "mask.new_zeros"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["def", "number2count_auxloss", "(", "passage_number_values", ":", "List", "[", "List", "[", "float", "]", "]", ",", "device_id", "=", "-", "1", ")", ":", "\n", "    ", "\"\"\" Using passage numnbers, make a (batch_size, max_passage_numbers) (padded) tensor, each containing a\n        noisy distribution with mass distributed over x-numbers. The corresponding count-answer will be x.\n        Use the attention2count rnn to predict a count value and compute the loss.\n    \"\"\"", "\n", "batch_size", "=", "len", "(", "passage_number_values", ")", "\n", "# List of length -- batch-size", "\n", "num_of_passage_numbers", "=", "[", "len", "(", "nums", ")", "for", "nums", "in", "passage_number_values", "]", "\n", "max_passage_numbers", "=", "max", "(", "num_of_passage_numbers", ")", "\n", "\n", "# Shape: (batch_size, )", "\n", "num_pasasge_numbers", "=", "util", ".", "move_to_device", "(", "torch", ".", "LongTensor", "(", "num_of_passage_numbers", ")", ",", "cuda_device", "=", "device_id", ")", "\n", "# Shape: (max_passage_numbers, )", "\n", "range_vector", "=", "util", ".", "get_range_vector", "(", "size", "=", "max_passage_numbers", ",", "device", "=", "device_id", ")", "\n", "\n", "mask", "=", "(", "range_vector", ".", "unsqueeze", "(", "0", ")", "<", "num_pasasge_numbers", ".", "unsqueeze", "(", "1", ")", ")", ".", "float", "(", ")", "\n", "print", "(", "mask", ")", "\n", "\n", "number_distributions", "=", "mask", ".", "new_zeros", "(", "batch_size", ",", "max_passage_numbers", ")", ".", "normal_", "(", "0", ",", "0.01", ")", ".", "abs_", "(", ")", "\n", "count_answers", "=", "number_distributions", ".", "new_zeros", "(", "batch_size", ",", "max_passage_numbers", ")", ".", "long", "(", ")", "\n", "\n", "for", "i", ",", "num_numbers", "in", "enumerate", "(", "num_of_passage_numbers", ")", ":", "\n", "        ", "\"\"\" Sample a count value between [0, min(5, num_numbers)]. Sample indices in this range, and set them as 1.\n            Add gaussian noise to the whole tensor and normalize. \n        \"\"\"", "\n", "# Pick a count answer", "\n", "count_value", "=", "random", ".", "randint", "(", "0", ",", "min", "(", "7", ",", "num_numbers", ")", ")", "\n", "count_answers", "[", "i", ",", "count_value", "]", "=", "1", "\n", "# Pick the indices that will have mass", "\n", "if", "count_value", ">", "0", ":", "\n", "            ", "indices", "=", "random", ".", "sample", "(", "range", "(", "num_numbers", ")", ",", "count_value", ")", "\n", "# Add 1.0 to all sampled indices", "\n", "number_distributions", "[", "i", ",", "indices", "]", "+=", "1.0", "\n", "\n", "", "", "number_distributions", "=", "number_distributions", "*", "mask", "\n", "number_distributions", "=", "number_distributions", "/", "torch", ".", "sum", "(", "number_distributions", ",", "dim", "=", "1", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.profiler_test.Model.__init__": [[15, 17], ["print"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "print", "(", "\"Init class\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.profiler_test.Model.__call__": [[18, 29], ["random.random", "profiler_test.Model.my_func", "random.random", "profiler_test.Profiler", "time.sleep", "profiler_test.Profiler", "time.sleep"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.profiler_test.Model.my_func"], ["", "@", "profile_func_decorator", "\n", "def", "__call__", "(", "self", ")", ":", "\n", "        ", "randnum", "=", "random", ".", "random", "(", ")", "\n", "if", "randnum", ">", "0.6", ":", "\n", "            ", "with", "Profiler", "(", "scope_name", "=", "\"scope_1\"", ")", ":", "\n", "                ", "time", ".", "sleep", "(", "1", ")", "\n", "", "", "else", ":", "\n", "            ", "with", "Profiler", "(", "scope_name", "=", "\"scope_2\"", ")", ":", "\n", "                ", "time", ".", "sleep", "(", "0.5", ")", "\n", "\n", "", "", "self", ".", "my_func", "(", "random", ".", "random", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.profiler_test.Model.my_func": [[30, 34], ["time.sleep"], "methods", ["None"], ["", "@", "profile_func_decorator", "\n", "def", "my_func", "(", "self", ",", "x", ")", ":", "\n", "        ", "time", ".", "sleep", "(", "0.3", ")", "\n", "return", "x", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.profiler_test.Profiler.__init__": [[40, 45], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "scope_name", ":", "str", ")", ":", "\n", "        ", "self", ".", "scope_name", "=", "scope_name", "\n", "self", ".", "start_time", "=", "None", "\n", "self", ".", "end_time", "=", "None", "\n", "self", ".", "total_time", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.profiler_test.Profiler.__enter__": [[46, 48], ["time.time"], "methods", ["None"], ["", "def", "__enter__", "(", "self", ")", ":", "\n", "        ", "self", ".", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.profiler_test.Profiler.__exit__": [[49, 54], ["time.time"], "methods", ["None"], ["", "def", "__exit__", "(", "self", ",", "exc_type", ",", "exc_val", ",", "exc_tb", ")", ":", "\n", "        ", "self", ".", "end_time", "=", "time", ".", "time", "(", ")", "\n", "self", ".", "total_time", "=", "self", ".", "end_time", "-", "self", ".", "start_time", "\n", "Profiler", ".", "timer_dict", "[", "self", ".", "scope_name", "]", "+=", "self", ".", "total_time", "\n", "Profiler", ".", "num_calls", "[", "self", ".", "scope_name", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.profiler_test.Profiler.to_string": [[55, 64], ["Profiler.timer_dict.items"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "to_string", "(", ")", ":", "\n", "        ", "s", "=", "\"------------------------  Profiler Stats  ------------------------\\n\"", "\n", "s", "+=", "\"Scope \\t Num_Calls \\t TimeElapsed\\n\"", "\n", "for", "k", ",", "v", "in", "Profiler", ".", "timer_dict", ".", "items", "(", ")", ":", "\n", "            ", "num_calls", "=", "Profiler", ".", "num_calls", "[", "k", "]", "\n", "s", "+=", "\"{} \\t {} \\t {} seconds\\n\"", ".", "format", "(", "k", ",", "num_calls", ",", "v", ")", "\n", "", "s", "+=", "\"----------------------------------------------------------------\"", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.profiler_test.profile_func_decorator": [[6, 12], ["profiler_test.Profiler", "input_func"], "function", ["None"], ["def", "profile_func_decorator", "(", "input_func", ")", ":", "\n", "    ", "def", "timed_func", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "Profiler", "(", "input_func", ".", "__name__", ")", ":", "\n", "            ", "result", "=", "input_func", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "", "return", "result", "\n", "", "return", "timed_func", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.remove_execution_supervision.readDataset": [[17, 21], ["open", "json.load"], "function", ["None"], ["def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.remove_execution_supervision.removeExecutionSupervision": [[23, 55], ["len", "collections.defaultdict", "dataset.items", "print", "print", "print", "print", "len"], "function", ["None"], ["", "def", "removeExecutionSupervision", "(", "dataset", ")", ":", "\n", "    ", "\"\"\" Given a dataset, remove execution supervision from all questions. \"\"\"", "\n", "\n", "total_num_passages", "=", "len", "(", "dataset", ")", "\n", "\n", "supervision_dict", "=", "defaultdict", "(", "int", ")", "\n", "exec_supervision_keys", "=", "[", "constants", ".", "exection_supervised", ",", "constants", ".", "strongly_supervised", "]", "\n", "supervision_keys", "=", "[", "\n", "constants", ".", "program_supervised", ",", "\n", "constants", ".", "qattn_supervised", ",", "\n", "constants", ".", "exection_supervised", ",", "\n", "constants", ".", "strongly_supervised", ",", "\n", "]", "\n", "\n", "total_num_qa", "=", "0", "\n", "\n", "for", "passage_idx", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "total_num_qa", "+=", "len", "(", "passage_info", "[", "constants", ".", "qa_pairs", "]", ")", "\n", "for", "qa", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "            ", "for", "key", "in", "exec_supervision_keys", ":", "\n", "                ", "qa", "[", "key", "]", "=", "False", "\n", "\n", "", "for", "key", "in", "supervision_keys", ":", "\n", "                ", "if", "key", "in", "qa", ":", "\n", "                    ", "supervision_dict", "[", "key", "]", "+=", "1", "if", "qa", "[", "key", "]", "is", "True", "else", "0", "\n", "\n", "", "", "", "", "print", "(", ")", "\n", "print", "(", "f\"TotalNumPassages: {total_num_passages}\"", ")", "\n", "print", "(", "f\"Num of original question: {total_num_qa}\"", ")", "\n", "print", "(", "f\"Supervision Dict: {supervision_dict}\"", ")", "\n", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.remove_strong_supervision.readDataset": [[18, 22], ["open", "json.load"], "function", ["None"], ["def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.remove_strong_supervision.count_num_exec_sup": [[24, 33], ["None"], "function", ["None"], ["", "def", "count_num_exec_sup", "(", "dataset", ",", "choosen_pids", ")", ":", "\n", "    ", "num_exec_sup", "=", "0", "\n", "for", "passage_idx", "in", "choosen_pids", ":", "\n", "        ", "passage_info", "=", "dataset", "[", "passage_idx", "]", "\n", "for", "qa", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "            ", "if", "constants", ".", "exection_supervised", "in", "qa", ":", "\n", "                ", "if", "qa", "[", "constants", ".", "exection_supervised", "]", ":", "\n", "                    ", "num_exec_sup", "+=", "1", "\n", "", "", "", "", "return", "num_exec_sup", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.remove_strong_supervision.make_supervision_dict": [[35, 57], ["collections.defaultdict", "collections.defaultdict", "dataset.items", "len"], "function", ["None"], ["", "def", "make_supervision_dict", "(", "dataset", ")", ":", "\n", "    ", "basic_keys", "=", "[", "constants", ".", "program_supervised", ",", "constants", ".", "qattn_supervised", ",", "constants", ".", "exection_supervised", "]", "\n", "qtype_dict", "=", "defaultdict", "(", "int", ")", "\n", "total_num_qa", "=", "0", "\n", "supervision_dict", "=", "defaultdict", "(", "int", ")", "\n", "for", "passage_idx", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "total_num_qa", "+=", "len", "(", "passage_info", "[", "constants", ".", "qa_pairs", "]", ")", "\n", "for", "qa", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "            ", "if", "constants", ".", "qtype", "in", "qa", ":", "\n", "                ", "qtype_dict", "[", "qa", "[", "constants", ".", "qtype", "]", "]", "+=", "1", "\n", "\n", "", "all_basic_true", "=", "False", "\n", "for", "key", "in", "basic_keys", ":", "\n", "                ", "if", "key", "in", "qa", ":", "\n", "                    ", "supervision_dict", "[", "key", "]", "+=", "1", "if", "qa", "[", "key", "]", "else", "0", "\n", "all_basic_true", "=", "True", "if", "qa", "[", "key", "]", "else", "False", "\n", "", "else", ":", "\n", "                    ", "all_basic_true", "=", "False", "\n", "", "", "if", "all_basic_true", ":", "\n", "                ", "supervision_dict", "[", "constants", ".", "strongly_supervised", "]", "+=", "1", "\n", "\n", "", "", "", "return", "supervision_dict", ",", "qtype_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.remove_strong_supervision.remove_all_annotations": [[59, 114], ["len", "list", "remove_strong_supervision.make_supervision_dict", "dataset.items", "remove_strong_supervision.make_supervision_dict", "print", "print", "print", "print", "print", "print", "dataset.keys", "random.shuffle", "remove_strong_supervision.count_num_exec_sup", "len", "qa.pop"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.sample_dataset.make_supervision_dict", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.sample_dataset.make_supervision_dict", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.sample_dataset.count_num_exec_sup"], ["", "def", "remove_all_annotations", "(", "dataset", ",", "annotation_for_numpassages", ")", ":", "\n", "    ", "\"\"\" Given a dataset containing date-comparison questions that are heuristically strongly annotated\n        and the number of passages that need to remain strongly annotated, we remove the strong annotations for other\n        passages. These annotations include: question-attention, event-date-groundings, etc.\n        Fields from which annotation is removed:\n        1. constants.datecomp_ques_event_date_groundings, constants.datecomp_ques_event_date_values\n        2. constants.datecomp_ques_event_attentions\n        3. constants.strongly_annotated - is set to False for all questions\n\n    \"\"\"", "\n", "\n", "total_num_passages", "=", "len", "(", "dataset", ")", "\n", "if", "annotation_for_numpassages", "==", "-", "1", ":", "\n", "        ", "annotation_for_numpassages", "=", "total_num_passages", "\n", "\n", "", "passage_idxs", "=", "list", "(", "dataset", ".", "keys", "(", ")", ")", "\n", "num_qaexec_sup", "=", "0", "\n", "choosen_passage_idxs", "=", "None", "\n", "while", "num_qaexec_sup", "<", "annotation_for_numpassages", "+", "320", ":", "\n", "        ", "random", ".", "shuffle", "(", "passage_idxs", ")", "\n", "choosen_passage_idxs", "=", "passage_idxs", "[", "0", ":", "annotation_for_numpassages", "]", "\n", "num_qaexec_sup", "=", "count_num_exec_sup", "(", "dataset", ",", "choosen_passage_idxs", ")", "\n", "\n", "", "total_num_qa", "=", "0", "\n", "\n", "supervision_keys", "=", "[", "\n", "constants", ".", "program_supervised", ",", "\n", "constants", ".", "qattn_supervised", ",", "\n", "constants", ".", "exection_supervised", ",", "\n", "constants", ".", "strongly_supervised", ",", "\n", "]", "\n", "\n", "orig_supervision_dict", ",", "orig_qtype_dict", "=", "make_supervision_dict", "(", "dataset", ")", "\n", "\n", "for", "passage_idx", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "total_num_qa", "+=", "len", "(", "passage_info", "[", "constants", ".", "qa_pairs", "]", ")", "\n", "if", "passage_idx", "not", "in", "choosen_passage_idxs", ":", "\n", "# Removing the strong annotations for all QAs in this passage", "\n", "            ", "for", "qa", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "# Setting all keys for supervised = False", "\n", "                ", "for", "key", "in", "supervision_keys", ":", "\n", "                    ", "qa", "[", "key", "]", "=", "False", "\n", "", "if", "constants", ".", "qtype", "in", "qa", ":", "\n", "                    ", "qa", ".", "pop", "(", "constants", ".", "qtype", ")", "\n", "\n", "", "", "", "", "pruned_supervision_dict", ",", "pruned_qtype_dict", "=", "make_supervision_dict", "(", "dataset", ")", "\n", "\n", "print", "(", ")", "\n", "print", "(", "f\"TotalNumPassages: {total_num_passages}  Passages remaining annotated: {annotation_for_numpassages}\"", ")", "\n", "print", "(", "f\"Num of original question: {total_num_qa}\"", ")", "\n", "print", "(", "f\"Original Supervision Dict: {orig_supervision_dict}\"", ")", "\n", "print", "(", "f\"Supervision Dict: {pruned_supervision_dict}\"", ")", "\n", "print", "(", "f\"Ques Type Dict: {pruned_qtype_dict}\"", ")", "\n", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.split_qtype_dev.readDataset": [[19, 23], ["open", "json.load"], "function", ["None"], ["def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.split_qtype_dev.splitDataset": [[25, 43], ["len", "dataset.items", "print", "print", "len", "len"], "function", ["None"], ["", "def", "splitDataset", "(", "dataset", ",", "test_paraids", ":", "List", "[", "str", "]", ")", ":", "\n", "    ", "\"\"\"Split dataset based on given test para ids\"\"\"", "\n", "\n", "total_num_passages", "=", "len", "(", "dataset", ")", "\n", "\n", "dev_dataset", "=", "{", "}", "\n", "test_dataset", "=", "{", "}", "\n", "\n", "for", "passage_idx", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "if", "passage_idx", "in", "test_paraids", ":", "\n", "            ", "test_dataset", "[", "passage_idx", "]", "=", "passage_info", "\n", "", "else", ":", "\n", "            ", "dev_dataset", "[", "passage_idx", "]", "=", "passage_info", "\n", "\n", "", "", "print", "(", "f\"TotalNumPassages: {total_num_passages}\"", ")", "\n", "print", "(", "f\"Size of dev: {len(dev_dataset)} Size of test: {len(test_dataset)}\"", ")", "\n", "\n", "return", "dev_dataset", ",", "test_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.sample_dataset.readDataset": [[13, 17], ["open", "json.load"], "function", ["None"], ["def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.sample_dataset.count_num_exec_sup": [[19, 28], ["None"], "function", ["None"], ["", "def", "count_num_exec_sup", "(", "dataset", ",", "choosen_pids", ")", ":", "\n", "    ", "num_exec_sup", "=", "0", "\n", "for", "passage_idx", "in", "choosen_pids", ":", "\n", "        ", "passage_info", "=", "dataset", "[", "passage_idx", "]", "\n", "for", "qa", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "            ", "if", "constants", ".", "exection_supervised", "in", "qa", ":", "\n", "                ", "if", "qa", "[", "constants", ".", "exection_supervised", "]", ":", "\n", "                    ", "num_exec_sup", "+=", "1", "\n", "", "", "", "", "return", "num_exec_sup", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.sample_dataset.compute_pid2supervisioncount": [[30, 46], ["collections.defaultdict", "dataset.items"], "function", ["None"], ["", "def", "compute_pid2supervisioncount", "(", "dataset", ")", ":", "\n", "    ", "\"\"\"Program and Exectution are counted as 1 each.\"\"\"", "\n", "pid2supexamples", "=", "defaultdict", "(", "int", ")", "\n", "for", "passage_idx", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "num_sup_examples", "=", "0", "\n", "for", "qa", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "            ", "if", "constants", ".", "program_supervised", "in", "qa", ":", "\n", "                ", "if", "qa", "[", "constants", ".", "program_supervised", "]", ":", "\n", "                    ", "num_sup_examples", "+=", "1", "\n", "\n", "", "", "if", "constants", ".", "exection_supervised", "in", "qa", ":", "\n", "                ", "if", "qa", "[", "constants", ".", "exection_supervised", "]", ":", "\n", "                    ", "num_sup_examples", "+=", "1", "\n", "", "", "", "pid2supexamples", "[", "passage_idx", "]", "=", "num_sup_examples", "\n", "\n", "", "return", "pid2supexamples", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.sample_dataset.compute_choosen_pids": [[48, 54], ["utils.util.sortDictByValue", "len", "int"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.sortDictByValue"], ["", "def", "compute_choosen_pids", "(", "pid2supexamples", ",", "perc", ":", "float", ")", ":", "\n", "    ", "sorted_pid2numsup", "=", "util", ".", "sortDictByValue", "(", "pid2supexamples", ",", "decreasing", "=", "True", ")", "\n", "num_pids", "=", "len", "(", "sorted_pid2numsup", ")", "\n", "num_choosen_pids", "=", "int", "(", "perc", "*", "num_pids", ")", "\n", "choosen_pids", "=", "[", "x", "[", "0", "]", "for", "x", "in", "sorted_pid2numsup", "[", "0", ":", "num_choosen_pids", "]", "]", "\n", "return", "choosen_pids", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.sample_dataset.make_supervision_dict": [[56, 78], ["collections.defaultdict", "collections.defaultdict", "dataset.items", "len"], "function", ["None"], ["", "def", "make_supervision_dict", "(", "dataset", ")", ":", "\n", "    ", "basic_keys", "=", "[", "constants", ".", "program_supervised", ",", "constants", ".", "qattn_supervised", ",", "constants", ".", "exection_supervised", "]", "\n", "qtype_dict", "=", "defaultdict", "(", "int", ")", "\n", "total_num_qa", "=", "0", "\n", "supervision_dict", "=", "defaultdict", "(", "int", ")", "\n", "for", "passage_idx", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "total_num_qa", "+=", "len", "(", "passage_info", "[", "constants", ".", "qa_pairs", "]", ")", "\n", "for", "qa", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "            ", "if", "constants", ".", "qtype", "in", "qa", ":", "\n", "                ", "qtype_dict", "[", "qa", "[", "constants", ".", "qtype", "]", "]", "+=", "1", "\n", "\n", "", "all_basic_true", "=", "False", "\n", "for", "key", "in", "basic_keys", ":", "\n", "                ", "if", "key", "in", "qa", ":", "\n", "                    ", "supervision_dict", "[", "key", "]", "+=", "1", "if", "qa", "[", "key", "]", "else", "0", "\n", "all_basic_true", "=", "True", "if", "qa", "[", "key", "]", "else", "False", "\n", "", "else", ":", "\n", "                    ", "all_basic_true", "=", "False", "\n", "", "", "if", "all_basic_true", ":", "\n", "                ", "supervision_dict", "[", "constants", ".", "strongly_supervised", "]", "+=", "1", "\n", "\n", "", "", "", "return", "supervision_dict", ",", "qtype_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.sample_dataset.sample_dataset": [[80, 120], ["len", "sample_dataset.compute_pid2supervisioncount", "list", "len", "int", "dataset.items", "sample_dataset.make_supervision_dict", "print", "print", "print", "print", "dataset.keys", "random.shuffle", "sample_dataset.count_num_exec_sup", "len"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.sample_dataset.compute_pid2supervisioncount", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.sample_dataset.make_supervision_dict", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.sample_dataset.count_num_exec_sup"], ["", "def", "sample_dataset", "(", "dataset", ",", "perc", ")", ":", "\n", "    ", "\"\"\" Given a dataset containing date-comparison questions that are heuristically strongly annotated\n        and the number of passages that need to remain strongly annotated, we remove the strong annotations for other\n        passages. These annotations include: question-attention, event-date-groundings, etc.\n        Fields from which annotation is removed:\n        1. constants.datecomp_ques_event_date_groundings, constants.datecomp_ques_event_date_values\n        2. constants.datecomp_ques_event_attentions\n        3. constants.strongly_annotated - is set to False for all questions\n\n    \"\"\"", "\n", "\n", "total_num_passages", "=", "len", "(", "dataset", ")", "\n", "\n", "pid2supexamples", "=", "compute_pid2supervisioncount", "(", "dataset", ")", "\n", "# choosen_pids = compute_choosen_pids(pid2supexamples, perc)", "\n", "\n", "passage_idxs", "=", "list", "(", "dataset", ".", "keys", "(", ")", ")", "\n", "num_pids", "=", "len", "(", "passage_idxs", ")", "\n", "num_choosen_pids", "=", "int", "(", "perc", "*", "num_pids", ")", "\n", "\n", "num_exec_sup", "=", "0", "\n", "while", "num_exec_sup", "<", "80", ":", "\n", "        ", "random", ".", "shuffle", "(", "passage_idxs", ")", "\n", "choosen_pids", "=", "passage_idxs", "[", "0", ":", "num_choosen_pids", "]", "\n", "num_exec_sup", "=", "count_num_exec_sup", "(", "dataset", ",", "choosen_pids", ")", "\n", "\n", "# choosen_pids = passage_idxs[0:num_choosen_pids]", "\n", "\n", "", "new_dataset", "=", "{", "}", "\n", "for", "passage_idx", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "if", "passage_idx", "in", "choosen_pids", ":", "\n", "            ", "new_dataset", "[", "passage_idx", "]", "=", "passage_info", "\n", "\n", "", "", "pruned_supervision_dict", ",", "pruned_qtype_dict", "=", "make_supervision_dict", "(", "new_dataset", ")", "\n", "print", "(", ")", "\n", "print", "(", "f\"TotalNumPassages: {total_num_passages}  Passages remaining: {len(new_dataset)}\"", ")", "\n", "print", "(", "f\"Supervision Dict: {pruned_supervision_dict}\"", ")", "\n", "print", "(", "f\"Ques Type Dict: {pruned_qtype_dict}\"", ")", "\n", "\n", "return", "new_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.split_dev.readDataset": [[14, 18], ["open", "json.load"], "function", ["None"], ["def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.split_dev.splitDataset": [[20, 49], ["len", "int", "list", "random.shuffle", "dataset.items", "print", "print", "dataset.keys", "len", "len"], "function", ["None"], ["", "def", "splitDataset", "(", "dataset", ",", "perc_split", ":", "float", ")", ":", "\n", "    ", "\"\"\"Split dataset based on given percentage\"\"\"", "\n", "\n", "total_num_passages", "=", "len", "(", "dataset", ")", "\n", "num_dev_paras", "=", "int", "(", "perc_split", "*", "total_num_passages", ")", "\n", "num_test_paras", "=", "total_num_passages", "-", "num_dev_paras", "\n", "\n", "passage_idxs", "=", "list", "(", "dataset", ".", "keys", "(", ")", ")", "\n", "random", ".", "shuffle", "(", "passage_idxs", ")", "\n", "dev_passage_idxs", "=", "passage_idxs", "[", "0", ":", "num_dev_paras", "]", "\n", "test_passage_idxs", "=", "passage_idxs", "[", "num_dev_paras", ":", "]", "\n", "\n", "total_num_qa", "=", "0", "\n", "\n", "dev_dataset", "=", "{", "}", "\n", "test_dataset", "=", "{", "}", "\n", "\n", "for", "passage_idx", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "if", "passage_idx", "in", "dev_passage_idxs", ":", "\n", "            ", "dev_dataset", "[", "passage_idx", "]", "=", "passage_info", "\n", "", "elif", "passage_idx", "in", "test_passage_idxs", ":", "\n", "            ", "test_dataset", "[", "passage_idx", "]", "=", "passage_info", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "", "", "print", "(", "f\"TotalNumPassages: {total_num_passages}\"", ")", "\n", "print", "(", "f\"Size of dev: {len(dev_dataset)} Size of test: {len(test_dataset)}\"", ")", "\n", "\n", "return", "dev_dataset", ",", "test_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.split_dev_ratio.readDataset": [[27, 31], ["open", "json.load"], "function", ["None"], ["def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.split_dev_ratio.get_num_passage_ques": [[33, 40], ["len", "dataset.items", "len"], "function", ["None"], ["", "def", "get_num_passage_ques", "(", "dataset", ")", ":", "\n", "    ", "total_num_passages", "=", "len", "(", "dataset", ")", "\n", "total_num_qas", "=", "0", "\n", "for", "passage_idx", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "total_num_qas", "+=", "len", "(", "passage_info", "[", "dropconstants", ".", "qa_pairs", "]", ")", "\n", "\n", "", "return", "total_num_passages", ",", "total_num_qas", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.split_dev_ratio.splitDataset": [[42, 60], ["len", "dataset.items", "print", "print", "len", "len"], "function", ["None"], ["", "def", "splitDataset", "(", "dataset", ",", "test_paraids", ":", "List", "[", "str", "]", ")", ":", "\n", "    ", "\"\"\"Split dataset based on given test para ids\"\"\"", "\n", "\n", "total_num_passages", "=", "len", "(", "dataset", ")", "\n", "\n", "dev_dataset", "=", "{", "}", "\n", "test_dataset", "=", "{", "}", "\n", "\n", "for", "passage_idx", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "if", "passage_idx", "in", "test_paraids", ":", "\n", "            ", "test_dataset", "[", "passage_idx", "]", "=", "passage_info", "\n", "", "else", ":", "\n", "            ", "dev_dataset", "[", "passage_idx", "]", "=", "passage_info", "\n", "\n", "", "", "print", "(", "f\"TotalNumPassages: {total_num_passages}\"", ")", "\n", "print", "(", "f\"Size of dev: {len(dev_dataset)} Size of test: {len(test_dataset)}\"", ")", "\n", "\n", "return", "dev_dataset", ",", "test_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.split_dev_ratio.get_split_paragraphids": [[62, 111], ["list", "len", "sum", "int", "range", "print", "print", "fulldataset.keys", "random.shuffle", "set", "set", "qtype2dataset.items", "max", "set", "set", "len", "len", "sum", "ratios.append", "abs", "ratio_diffs.append", "set.intersection", "float", "float", "len", "len"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "def", "get_split_paragraphids", "(", "fulldataset", ",", "qtype2dataset", ",", "split_ratio", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "    ", "\"\"\"Split dataset based on given test para ids\"\"\"", "\n", "\n", "passage_idxs", "=", "list", "(", "fulldataset", ".", "keys", "(", ")", ")", "\n", "total_num_passages", "=", "len", "(", "passage_idxs", ")", "\n", "total_num_ques", "=", "sum", "(", "[", "len", "(", "fulldataset", "[", "pid", "]", "[", "dropconstants", ".", "qa_pairs", "]", ")", "for", "pid", "in", "fulldataset", "]", ")", "\n", "num_dev_paras", "=", "int", "(", "split_ratio", "*", "total_num_passages", ")", "\n", "\n", "# Try multiple splits, since ratio of questions might vary even if paras are split according to ratio", "\n", "num_of_splits_to_try", "=", "5000", "\n", "\n", "best_diff_split_actual_ratio", "=", "100.0", "\n", "best_ratios", "=", "[", "]", "\n", "best_mydev_para_ids", "=", "None", "\n", "best_mytest_para_ids", "=", "None", "\n", "for", "trynum", "in", "range", "(", "num_of_splits_to_try", ")", ":", "\n", "# print(\"Try number: {}\".format(trynum))", "\n", "        ", "random", ".", "shuffle", "(", "passage_idxs", ")", "\n", "mydev_passage_idxs", "=", "passage_idxs", "[", "0", ":", "num_dev_paras", "]", "\n", "mytest_passage_idxs", "=", "passage_idxs", "[", "num_dev_paras", ":", "]", "\n", "mydev_passage_idxs", "=", "set", "(", "mydev_passage_idxs", ")", "\n", "mytest_passage_idxs", "=", "set", "(", "mytest_passage_idxs", ")", "\n", "assert", "len", "(", "mydev_passage_idxs", ".", "intersection", "(", "mytest_passage_idxs", ")", ")", "==", "0", "\n", "\n", "ratio_diffs", "=", "[", "]", "\n", "ratios", "=", "[", "]", "\n", "for", "qtype", ",", "qtypedataset", "in", "qtype2dataset", ".", "items", "(", ")", ":", "\n", "            ", "qtype_totalnum_qas", "=", "sum", "(", "len", "(", "qtypedataset", "[", "pid", "]", "[", "dropconstants", ".", "qa_pairs", "]", ")", "for", "pid", "in", "qtypedataset", ")", "\n", "qtype_mydevnum_qas", "=", "0", "\n", "for", "pid", "in", "mydev_passage_idxs", ":", "\n", "                ", "if", "pid", "in", "qtypedataset", ":", "\n", "                    ", "qtype_mydevnum_qas", "+=", "len", "(", "qtypedataset", "[", "pid", "]", "[", "dropconstants", ".", "qa_pairs", "]", ")", "\n", "\n", "", "", "qtype_mydev_ratio", "=", "float", "(", "qtype_mydevnum_qas", ")", "/", "float", "(", "qtype_totalnum_qas", ")", "\n", "ratios", ".", "append", "(", "qtype_mydev_ratio", ")", "\n", "ratio_diff", "=", "abs", "(", "qtype_mydev_ratio", "-", "split_ratio", ")", "\n", "ratio_diffs", ".", "append", "(", "ratio_diff", ")", "\n", "\n", "", "max_ratio_diff", "=", "max", "(", "ratio_diffs", ")", "\n", "if", "max_ratio_diff", "<", "best_diff_split_actual_ratio", ":", "\n", "            ", "best_diff_split_actual_ratio", "=", "max_ratio_diff", "\n", "best_ratios", "=", "ratios", "\n", "best_mydev_para_ids", "=", "mydev_passage_idxs", "\n", "best_mytest_para_ids", "=", "mytest_passage_idxs", "\n", "\n", "", "", "print", "(", "\"Best dev/test qa ratio diff: {}\"", ".", "format", "(", "best_diff_split_actual_ratio", ")", ")", "\n", "print", "(", "\"Best ratios: {}\"", ".", "format", "(", "best_ratios", ")", ")", "\n", "\n", "return", "set", "(", "best_mydev_para_ids", ")", ",", "set", "(", "best_mytest_para_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.split_dev_ratio.make_qtype2dataset_map": [[113, 126], ["os.path.join", "split_dev_ratio.readDataset"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.data_manipulation.simplify_programs.readDataset"], ["", "def", "make_qtype2dataset_map", "(", "qtype_datasets_rootdir", ",", "qtype_datasets", ",", "filename", ")", ":", "\n", "# Qtype 2 Dataset map", "\n", "    ", "qtype2dataset", "=", "{", "}", "\n", "for", "qtype_dirname", "in", "qtype_datasets", ":", "\n", "        ", "data_json", "=", "os", ".", "path", ".", "join", "(", "qtype_datasets_rootdir", ",", "qtype_dirname", ",", "filename", ")", "\n", "qtype_data", "=", "readDataset", "(", "data_json", ")", "\n", "qtype2dataset", "[", "qtype_dirname", "]", "=", "qtype_data", "\n", "# num_passages_qtype, num_qas_qtype = get_num_passage_ques(qtype_data)", "\n", "# ratio_of_full_dev = float(num_qas_qtype) * 100.0 / num_qas_dev", "\n", "# print(\"Number in {}; passages: {}  QA:{} Ratio_dev: {}\".format(", "\n", "#     qtype_dirname, num_passages_qtype, num_qas_qtype, ratio_of_full_dev))", "\n", "\n", "", "return", "qtype2dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.split_dev_ratio.qtyperatio2fulldataset": [[128, 136], ["split_dev_ratio.get_num_passage_ques", "qtype2dataset.items", "split_dev_ratio.get_num_passage_ques", "print", "float"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.split_dev_ratio.get_num_passage_ques", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.split_dev_ratio.get_num_passage_ques"], ["", "def", "qtyperatio2fulldataset", "(", "full_dataset", ",", "qtype2dataset", ")", ":", "\n", "    ", "num_passages_full", ",", "num_qas_full", "=", "get_num_passage_ques", "(", "full_dataset", ")", "\n", "for", "qtype", ",", "dataset", "in", "qtype2dataset", ".", "items", "(", ")", ":", "\n", "        ", "num_passages_qtype", ",", "num_qas_qtype", "=", "get_num_passage_ques", "(", "dataset", ")", "\n", "ratio_of_full_dev", "=", "float", "(", "num_qas_qtype", ")", "*", "100.0", "/", "num_qas_full", "\n", "print", "(", "\n", "\"Number in {}; passages: {}  QA:{} Ratio_dev: {}\"", ".", "format", "(", "\n", "qtype", ",", "num_passages_qtype", ",", "num_qas_qtype", ",", "ratio_of_full_dev", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.split_dev_ratio.write_dataset": [[140, 143], ["open", "json.dump", "os.path.join"], "function", ["None"], ["", "", "def", "write_dataset", "(", "dataset", ",", "output_dir", ",", "output_filname", ")", ":", "\n", "    ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "output_filname", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "dataset", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.merge_datasets.count_supervision_types": [[11, 27], ["collections.defaultdict", "passage_dict.items"], "function", ["None"], ["def", "count_supervision_types", "(", "passage_dict", ")", ":", "\n", "    ", "supervision_keys", "=", "[", "\n", "constants", ".", "program_supervised", ",", "\n", "constants", ".", "qattn_supervised", ",", "\n", "constants", ".", "exection_supervised", ",", "\n", "constants", ".", "strongly_supervised", ",", "\n", "]", "\n", "supervision_dict", "=", "defaultdict", "(", "int", ")", "\n", "for", "_", ",", "pinfo", "in", "passage_dict", ".", "items", "(", ")", ":", "\n", "        ", "qa_pairs", "=", "pinfo", "[", "constants", ".", "qa_pairs", "]", "\n", "for", "qa", "in", "qa_pairs", ":", "\n", "            ", "for", "key", "in", "supervision_keys", ":", "\n", "                ", "if", "key", "in", "qa", "and", "qa", "[", "key", "]", "is", "True", ":", "\n", "                    ", "supervision_dict", "[", "key", "]", "+=", "1", "\n", "\n", "", "", "", "", "return", "supervision_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.merge_datasets.mergeDatasets": [[29, 92], ["set", "set", "set.union", "len", "len", "len", "merge_datasets.count_supervision_types", "print", "print", "print", "print", "print", "print", "open", "json.load", "open", "json.load", "json.load.keys", "json.load.keys", "len", "open", "json.dump", "len", "len", "list", "copy.deepcopy", "copy.deepcopy", "len", "copy.deepcopy", "len"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.fulldata_setting.merge_mydata.count_supervision_types"], ["", "def", "mergeDatasets", "(", "input_json1", ":", "str", ",", "input_json2", ":", "str", ",", "output_json", ":", "str", ")", "->", "None", ":", "\n", "    ", "\"\"\" Merge DROP datasets from two different files.\n        First make a union list of passages.\n            If a passage only appears in one - add as it is\n            If occurs in both, merge questions inside\n                Since question_answer is a dict; check for duplicacy by making a set of question_dicts first.\n    \"\"\"", "\n", "\n", "# Input file contains single json obj with list of questions as jsonobjs inside it", "\n", "with", "open", "(", "input_json1", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset1", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "with", "open", "(", "input_json2", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset2", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "passage_ids_1", "=", "set", "(", "dataset1", ".", "keys", "(", ")", ")", "\n", "passage_ids_2", "=", "set", "(", "dataset2", ".", "keys", "(", ")", ")", "\n", "\n", "union_passage_ids", "=", "passage_ids_1", ".", "union", "(", "passage_ids_2", ")", "\n", "\n", "num_passage_1", "=", "len", "(", "passage_ids_1", ")", "\n", "num_passage_2", "=", "len", "(", "passage_ids_2", ")", "\n", "num_merged_passages", "=", "len", "(", "union_passage_ids", ")", "\n", "\n", "output_passage_dict", "=", "{", "}", "\n", "num_output_qas", "=", "0", "\n", "num_qas_1", ",", "num_qas_2", "=", "0", ",", "0", "\n", "\n", "for", "passage_id", "in", "union_passage_ids", ":", "\n", "        ", "if", "passage_id", "in", "passage_ids_1", "and", "passage_id", "in", "passage_ids_2", ":", "\n", "# These assumes that all passage-level parsing is same in both datasets, only qa_pairs differ", "\n", "            ", "qa_pairs_1", "=", "dataset1", "[", "passage_id", "]", "[", "constants", ".", "qa_pairs", "]", "\n", "num_qas_1", "+=", "len", "(", "qa_pairs_1", ")", "\n", "qa_pairs_2", "=", "dataset2", "[", "passage_id", "]", "[", "constants", ".", "qa_pairs", "]", "\n", "num_qas_2", "+=", "len", "(", "qa_pairs_2", ")", "\n", "\n", "merged_qa_pairs", "=", "list", "(", "{", "qa", "[", "constants", ".", "query_id", "]", ":", "qa", "for", "qa", "in", "qa_pairs_1", "+", "qa_pairs_2", "}", ".", "values", "(", ")", ")", "\n", "\n", "passage_info", "=", "copy", ".", "deepcopy", "(", "dataset1", "[", "passage_id", "]", ")", "\n", "passage_info", "[", "constants", ".", "qa_pairs", "]", "=", "merged_qa_pairs", "\n", "\n", "", "elif", "passage_id", "in", "passage_ids_1", ":", "\n", "            ", "passage_info", "=", "copy", ".", "deepcopy", "(", "dataset1", "[", "passage_id", "]", ")", "\n", "num_qas_1", "+=", "len", "(", "passage_info", "[", "constants", ".", "qa_pairs", "]", ")", "\n", "", "elif", "passage_id", "in", "passage_ids_2", ":", "\n", "            ", "passage_info", "=", "copy", ".", "deepcopy", "(", "dataset2", "[", "passage_id", "]", ")", "\n", "num_qas_2", "+=", "len", "(", "passage_info", "[", "constants", ".", "qa_pairs", "]", ")", "\n", "", "else", ":", "\n", "            ", "raise", "RuntimeError", "\n", "\n", "", "output_passage_dict", "[", "passage_id", "]", "=", "passage_info", "\n", "num_output_qas", "+=", "len", "(", "passage_info", "[", "constants", ".", "qa_pairs", "]", ")", "\n", "\n", "", "with", "open", "(", "output_json", ",", "\"w\"", ")", "as", "outf", ":", "\n", "        ", "json", ".", "dump", "(", "output_passage_dict", ",", "outf", ",", "indent", "=", "4", ")", "\n", "\n", "", "supervision_dict", "=", "count_supervision_types", "(", "output_passage_dict", ")", "\n", "print", "(", ")", "\n", "print", "(", "f\"Number of passages 1: {num_passage_1}\\nNumber of questions 1: {num_qas_1}\"", ")", "\n", "print", "(", "f\"Number of passages 2: {num_passage_2}\\nNumber of questions 2: {num_qas_2}\"", ")", "\n", "print", "(", "f\"Number of merged passages: {num_merged_passages}\\nNumber of merged questions: {num_output_qas}\"", ")", "\n", "print", "(", "f\"SupervisionDict: {supervision_dict}\"", ")", "\n", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.drop_parser_test.DROPSemanticParserTest.setUp": [[10, 15], ["super().setUp", "print", "drop_parser_test.DROPSemanticParserTest.set_up_model"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.drop_parser_test.DROPSemanticParserTest.setUp"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setUp", "(", ")", "\n", "print", "(", "self", ".", "FIXTURES_ROOT", ")", "\n", "self", ".", "set_up_model", "(", "\n", "\"semqa/tests/fixtures/drop_parser/experiment.json\"", ",", "\"semqa/tests/data/drop_old/date/drop_old.json\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.drop.drop_parser_test.DROPSemanticParserTest.test_drop_dates": [[23, 33], ["semqa.domain_languages.drop_language.DropLanguage.compute_date_comparison_matrices", "print", "semqa.domain_languages.drop_language.Date", "semqa.domain_languages.drop_language.Date", "semqa.domain_languages.drop_language.Date"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.compute_date_comparison_matrices"], ["@", "flaky", "\n", "def", "test_drop_dates", "(", "self", ")", ":", "\n", "\n", "        ", "dates", "=", "[", "Date", "(", "year", "=", "1354", ",", "month", "=", "-", "1", ",", "day", "=", "-", "1", ")", ",", "Date", "(", "year", "=", "1364", ",", "month", "=", "-", "1", ",", "day", "=", "-", "1", ")", ",", "Date", "(", "year", "=", "1364", ",", "month", "=", "5", ",", "day", "=", "7", ")", "]", "\n", "\n", "date_mat", "=", "DropLanguage", ".", "compute_date_comparison_matrices", "(", "date_values", "=", "dates", ",", "device_id", "=", "-", "1", ")", "\n", "\n", "print", "(", "date_mat", ")", "\n", "\n", "assert", "1", "==", "1", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.fulldata_setting.merge_mydata.count_supervision_types": [[17, 36], ["collections.defaultdict", "collections.defaultdict", "passage_dict.items"], "function", ["None"], ["def", "count_supervision_types", "(", "passage_dict", ")", ":", "\n", "    ", "supervision_keys", "=", "[", "\n", "constants", ".", "program_supervised", ",", "\n", "constants", ".", "qattn_supervised", ",", "\n", "constants", ".", "exection_supervised", ",", "\n", "constants", ".", "strongly_supervised", ",", "\n", "]", "\n", "questypes_dict", "=", "defaultdict", "(", "int", ")", "\n", "supervision_dict", "=", "defaultdict", "(", "int", ")", "\n", "for", "_", ",", "pinfo", "in", "passage_dict", ".", "items", "(", ")", ":", "\n", "        ", "qa_pairs", "=", "pinfo", "[", "constants", ".", "qa_pairs", "]", "\n", "for", "qa", "in", "qa_pairs", ":", "\n", "            ", "for", "key", "in", "supervision_keys", ":", "\n", "                ", "if", "key", "in", "qa", "and", "qa", "[", "key", "]", "is", "True", ":", "\n", "                    ", "supervision_dict", "[", "key", "]", "+=", "1", "\n", "", "", "if", "constants", ".", "qtype", "in", "qa", ":", "\n", "                ", "questypes_dict", "[", "qa", "[", "constants", ".", "qtype", "]", "]", "+=", "1", "\n", "\n", "", "", "", "return", "supervision_dict", ",", "questypes_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.fulldata_setting.merge_mydata.read_dataset": [[38, 42], ["open", "json.load"], "function", ["None"], ["", "def", "read_dataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.fulldata_setting.merge_mydata.convert_mydata_into_qadict": [[45, 58], ["mydata.items"], "function", ["None"], ["", "def", "convert_mydata_into_qadict", "(", "mydata", ")", ":", "\n", "# Dict from passage_id : Dict[qid: qa_pair]", "\n", "    ", "passageid2qid2qapair", "=", "{", "}", "\n", "\n", "for", "passage_id", ",", "passage_info", "in", "mydata", ".", "items", "(", ")", ":", "\n", "        ", "qa_pairs", "=", "passage_info", "[", "constants", ".", "qa_pairs", "]", "\n", "qid2qapair", "=", "{", "}", "\n", "for", "qa_pair", "in", "qa_pairs", ":", "\n", "            ", "qid", "=", "qa_pair", "[", "constants", ".", "query_id", "]", "\n", "qid2qapair", "[", "qid", "]", "=", "qa_pair", "\n", "", "passageid2qid2qapair", "[", "passage_id", "]", "=", "qid2qapair", "\n", "\n", "", "return", "passageid2qid2qapair", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.fulldata_setting.merge_mydata.mergeDatasets": [[61, 147], ["merge_mydata.read_dataset", "merge_mydata.read_dataset", "set", "set", "set.union", "merge_mydata.count_supervision_types", "print", "print", "print", "print", "print", "print", "print", "print", "print", "read_dataset.keys", "read_dataset.keys", "open", "json.dump", "len", "len", "len", "len", "len", "copy.copy", "set", "len", "set", "len", "set.union", "len", "full_qid2qapair.keys", "my_qid2qapair.keys", "new_qapairs.append", "new_qapairs.append"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.fulldata_setting.merge_mydata.read_dataset", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.fulldata_setting.merge_mydata.read_dataset", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.fulldata_setting.merge_mydata.count_supervision_types"], ["", "def", "mergeDatasets", "(", "mydata_json", ":", "str", ",", "fulldata_json", ":", "str", ",", "output_json", ":", "str", ")", "->", "Dict", "[", "str", ",", "Dict", "]", ":", "\n", "    ", "\"\"\" Merge MyData subset of the full DROP dataset w/ the full dataset.\n        Following conditions need to be handled\n        1. QA only in FullData -- add as it is\n        2. QA only in MyData -- add as it is (these would be DateComp and NumComp augmentations)\n        3. QA in both -- add version from MyData (it would probably have auxiliary supervision)\n    \"\"\"", "\n", "\n", "mydata", "=", "read_dataset", "(", "mydata_json", ")", "\n", "fulldata", "=", "read_dataset", "(", "fulldata_json", ")", "\n", "\n", "mydata_passage_ids", "=", "set", "(", "mydata", ".", "keys", "(", ")", ")", "\n", "fulldata_passage_ids", "=", "set", "(", "fulldata", ".", "keys", "(", ")", ")", "\n", "\n", "union_passage_ids", "=", "mydata_passage_ids", ".", "union", "(", "fulldata_passage_ids", ")", "\n", "\n", "merged_data", "=", "{", "}", "\n", "\n", "num_output_qas", "=", "0", "\n", "num_qas_1", ",", "num_qas_2", "=", "0", ",", "0", "\n", "\n", "num_my_qa", "=", "0", "\n", "num_full_qa", "=", "0", "\n", "num_merged_qa", "=", "0", "\n", "\n", "for", "passage_id", "in", "union_passage_ids", ":", "\n", "# Passage in Full data and not in MyData", "\n", "        ", "if", "passage_id", "in", "fulldata", "and", "passage_id", "not", "in", "mydata", ":", "\n", "            ", "merged_data", "[", "passage_id", "]", "=", "fulldata", "[", "passage_id", "]", "\n", "num_qas", "=", "len", "(", "fulldata", "[", "passage_id", "]", "[", "constants", ".", "qa_pairs", "]", ")", "\n", "num_full_qa", "+=", "num_qas", "\n", "num_merged_qa", "+=", "num_qas", "\n", "\n", "# Passage in MyData but not in FullData", "\n", "", "elif", "passage_id", "in", "mydata", "and", "passage_id", "not", "in", "fulldata", ":", "\n", "            ", "merged_data", "[", "passage_id", "]", "=", "mydata", "[", "passage_id", "]", "\n", "num_qas", "=", "len", "(", "mydata", "[", "passage_id", "]", "[", "constants", ".", "qa_pairs", "]", ")", "\n", "num_my_qa", "+=", "num_qas", "\n", "num_merged_qa", "+=", "num_qas", "\n", "\n", "", "elif", "passage_id", "in", "fulldata", "and", "passage_id", "in", "mydata", ":", "\n", "            ", "full_pinfo", "=", "fulldata", "[", "passage_id", "]", "\n", "my_pinfo", "=", "mydata", "[", "passage_id", "]", "\n", "\n", "new_pinfo", "=", "copy", ".", "copy", "(", "full_pinfo", ")", "\n", "\n", "full_qa_pairs", "=", "full_pinfo", "[", "constants", ".", "qa_pairs", "]", "\n", "full_qid2qapair", "=", "{", "qapair", "[", "constants", ".", "query_id", "]", ":", "qapair", "for", "qapair", "in", "full_qa_pairs", "}", "\n", "full_qids", "=", "set", "(", "full_qid2qapair", ".", "keys", "(", ")", ")", "\n", "num_full_qa", "+=", "len", "(", "full_qid2qapair", ")", "\n", "\n", "my_qa_pairs", "=", "my_pinfo", "[", "constants", ".", "qa_pairs", "]", "\n", "my_qid2qapair", "=", "{", "qapair", "[", "constants", ".", "query_id", "]", ":", "qapair", "for", "qapair", "in", "my_qa_pairs", "}", "\n", "my_qids", "=", "set", "(", "my_qid2qapair", ".", "keys", "(", ")", ")", "\n", "num_my_qa", "+=", "len", "(", "my_qid2qapair", ")", "\n", "\n", "new_qapairs", "=", "[", "]", "\n", "union_qa_ids", "=", "full_qids", ".", "union", "(", "my_qids", ")", "\n", "for", "qid", "in", "union_qa_ids", ":", "\n", "                ", "if", "qid", "in", "my_qid2qapair", ":", "\n", "                    ", "new_qapairs", ".", "append", "(", "my_qid2qapair", "[", "qid", "]", ")", "\n", "", "elif", "qid", "in", "full_qid2qapair", ":", "\n", "                    ", "new_qapairs", ".", "append", "(", "full_qid2qapair", "[", "qid", "]", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "RuntimeError", "\n", "\n", "", "", "new_pinfo", "[", "constants", ".", "qa_pairs", "]", "=", "new_qapairs", "\n", "num_merged_qa", "+=", "len", "(", "new_qapairs", ")", "\n", "\n", "merged_data", "[", "passage_id", "]", "=", "new_pinfo", "\n", "", "else", ":", "\n", "            ", "raise", "RuntimeError", "\n", "\n", "", "", "supervision_dict", ",", "questypes_dict", "=", "count_supervision_types", "(", "merged_data", ")", "\n", "print", "(", ")", "\n", "print", "(", "f\"Number of passages MY: {len(mydata_passage_ids)}\\nNumber of questions MY: {num_my_qa}\"", ")", "\n", "print", "(", "f\"Number of passages FULL: {len(fulldata_passage_ids)}\\nNumber of questions FULL: {num_full_qa}\"", ")", "\n", "print", "(", "f\"Number of merged passages: {len(merged_data)}\\nNumber of merged questions: {num_merged_qa}\"", ")", "\n", "print", "(", "f\"SupervisionDict: {supervision_dict}\"", ")", "\n", "print", "(", "f\"Ques-program types: {questypes_dict}\"", ")", "\n", "print", "(", ")", "\n", "\n", "print", "(", "\"Writing merged data to : {}\"", ".", "format", "(", "output_json", ")", ")", "\n", "with", "open", "(", "output_json", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "merged_data", ",", "f", ",", "indent", "=", "4", ")", "\n", "", "print", "(", "\"Written!\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.synthetic.pattn2count.sample_spansfor_variablelength": [[11, 30], ["sum", "range", "enumerate", "sorted", "result.append", "random.sample"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["def", "sample_spansfor_variablelength", "(", "seqlen", ",", "num_spans", ",", "span_lengths", ":", "List", "[", "int", "]", ")", ":", "\n", "    ", "sum_lengths", "=", "sum", "(", "span_lengths", ")", "\n", "# We need a gap of atleast 1 token between two spans. Number of heads is computed based on longer spans (+1)", "\n", "# and offset is also up by +1", "\n", "# Range of Number of possible span starts", "\n", "num_heads", "=", "seqlen", "-", "(", "sum_lengths", "-", "num_spans", "+", "num_spans", ")", "\n", "if", "num_heads", "<", "num_spans", ":", "\n", "        ", "return", "None", "\n", "", "indices", "=", "range", "(", "seqlen", "-", "(", "sum_lengths", "-", "num_spans", ")", ")", "\n", "result", "=", "[", "]", "\n", "offset", "=", "0", "\n", "# Randomly sample n=num_spans heads", "\n", "for", "i", ",", "idx", "in", "enumerate", "(", "sorted", "(", "random", ".", "sample", "(", "indices", ",", "num_spans", ")", ")", ")", ":", "\n", "# These heads are 0-indexed, to this we add the offset we've covered in the seq", "\n", "        ", "idx", "+=", "offset", "\n", "span_length", "=", "span_lengths", "[", "i", "]", "\n", "result", ".", "append", "(", "(", "idx", ",", "idx", "+", "span_length", ")", ")", "\n", "offset", "+=", "span_length", "-", "1", "+", "1", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.synthetic.pattn2count.make_instance": [[32, 55], ["random.randint", "numpy.abs", "sum", "numpy.random.normal", "pattn2count.sample_spansfor_variablelength", "random.randint", "range"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.pattn2count_reader.PAttn2CountReader.sample_spansfor_variablelength"], ["", "def", "make_instance", "(", "\n", "min_passage_length", ":", "int", ",", "max_passage_length", ":", "int", ",", "min_span_length", ":", "int", ",", "max_span_length", ":", "int", ",", "count_value", ":", "int", "\n", ")", ":", "\n", "\n", "    ", "passage_length", "=", "random", ".", "randint", "(", "min_passage_length", ",", "max_passage_length", ")", "\n", "# Mean: 0, Std: 0.2, Size: PassageLength", "\n", "attention", "=", "np", ".", "abs", "(", "np", ".", "random", ".", "normal", "(", "0.0", ",", "0.1", ",", "passage_length", ")", ")", "\n", "\n", "if", "count_value", ">", "0", ":", "\n", "        ", "span_lengths", "=", "[", "random", ".", "randint", "(", "min_span_length", ",", "max_span_length", ")", "for", "_", "in", "range", "(", "count_value", ")", "]", "\n", "# Sample n=count_value spans of the same length. Ends are exclusive", "\n", "# sampled_spans = self.sample_spans(passage_length, count_value, span_length)", "\n", "sampled_spans", "=", "sample_spansfor_variablelength", "(", "passage_length", ",", "count_value", ",", "span_lengths", ")", "\n", "if", "sampled_spans", "is", "None", ":", "\n", "            ", "return", "None", "\n", "\n", "", "for", "(", "start", ",", "end", ")", "in", "sampled_spans", ":", "\n", "            ", "attention", "[", "start", ":", "end", "]", "+=", "1.0", "\n", "\n", "", "", "attention_sum", "=", "sum", "(", "attention", ")", "\n", "attention", "=", "attention", "/", "attention_sum", "\n", "\n", "return", "attention", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.synthetic.pattn2count._get_length_buckets": [[57, 82], ["list", "max_length_buckets.append", "zip", "max_length_buckets.append", "min_length_buckets.append", "len"], "function", ["None"], ["", "def", "_get_length_buckets", "(", "min_passage_length", ",", "max_passage_length", ")", ":", "\n", "    ", "if", "min_passage_length", "==", "max_passage_length", ":", "\n", "        ", "return", "[", "(", "min_passage_length", ",", "max_passage_length", ")", "]", "\n", "\n", "", "min_length_buckets", "=", "[", "min_passage_length", "]", "\n", "max_length_buckets", "=", "[", "]", "\n", "\n", "# Add start, end + 100 until end <= max_passage_length", "\n", "i", "=", "1", "\n", "while", "True", ":", "\n", "        ", "potential_max_len", "=", "i", "*", "100", "+", "min_passage_length", "\n", "if", "potential_max_len", "<=", "max_passage_length", ":", "\n", "            ", "max_length_buckets", ".", "append", "(", "potential_max_len", ")", "\n", "min_length_buckets", ".", "append", "(", "max_length_buckets", "[", "-", "1", "]", ")", "# Last end is next's start", "\n", "\n", "i", "+=", "1", "\n", "", "else", ":", "\n", "            ", "break", "\n", "", "", "if", "len", "(", "max_length_buckets", ")", "==", "0", "or", "max_length_buckets", "[", "-", "1", "]", "!=", "max_passage_length", ":", "# This was left out", "\n", "        ", "max_length_buckets", ".", "append", "(", "max_passage_length", ")", "\n", "\n", "", "if", "min_length_buckets", "[", "-", "1", "]", "==", "max_passage_length", ":", "\n", "        ", "min_length_buckets", "=", "min_length_buckets", "[", ":", "-", "1", "]", "\n", "\n", "", "return", "list", "(", "zip", "(", "min_length_buckets", ",", "max_length_buckets", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.synthetic.pattn2count.make_data": [[84, 126], ["pattn2count._get_length_buckets", "collections.defaultdict", "range", "print", "print", "print", "range", "print", "pattn2count.make_instance", "attention.tolist.tolist", "data_dicts.append", "collections.defaultdict"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.pattn2count_reader.PAttn2CountReader._get_length_buckets", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.pattn2count_reader.PAttn2CountReader.make_instance"], ["", "def", "make_data", "(", "\n", "min_passage_length", ",", "\n", "max_passage_length", ",", "\n", "min_span_length", ",", "\n", "max_span_length", ",", "\n", "samples_per_bucket_count", ":", "int", ",", "\n", "max_count_value", ":", "int", "=", "7", ",", "\n", ")", ":", "\n", "# For each 100 length bucket, and count value, generate 1000 examples in train mode, and 100 in val mode", "\n", "    ", "num_instances_per_bucket_per_count", "=", "samples_per_bucket_count", "\n", "\n", "# List of min and max passage", "\n", "minmax_passagelen_tuples", "=", "_get_length_buckets", "(", "min_passage_length", ",", "max_passage_length", ")", "\n", "data_dicts", "=", "[", "]", "\n", "\n", "lenbucket_count_dict", "=", "defaultdict", "(", ")", "\n", "\n", "for", "count_value", "in", "range", "(", "0", ",", "max_count_value", "+", "1", ")", ":", "\n", "        ", "print", "(", "f\"Count Value: {count_value}\"", ")", "\n", "for", "min_plen", ",", "max_plen", "in", "minmax_passagelen_tuples", ":", "\n", "            ", "instances_for_bucket", "=", "0", "\n", "for", "i", "in", "range", "(", "num_instances_per_bucket_per_count", ")", ":", "\n", "                ", "attention", "=", "make_instance", "(", "\n", "min_passage_length", "=", "min_plen", ",", "\n", "max_passage_length", "=", "max_plen", ",", "\n", "min_span_length", "=", "min_span_length", ",", "\n", "max_span_length", "=", "max_span_length", ",", "\n", "count_value", "=", "count_value", ",", "\n", ")", "\n", "if", "attention", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "if", "count_value", "not", "in", "lenbucket_count_dict", ":", "\n", "                    ", "lenbucket_count_dict", "[", "count_value", "]", "=", "defaultdict", "(", "int", ")", "\n", "", "lenbucket_count_dict", "[", "count_value", "]", "[", "(", "min_plen", ",", "max_plen", ")", "]", "+=", "1", "\n", "attention", "=", "attention", ".", "tolist", "(", ")", "\n", "data_dicts", ".", "append", "(", "{", "\"attention\"", ":", "attention", ",", "\"count_value\"", ":", "count_value", "}", ")", "\n", "instances_for_bucket", "+=", "1", "\n", "", "print", "(", "f\"{min_plen}, {max_plen} :: {instances_for_bucket}\"", ")", "\n", "", "print", "(", "\"\\n\"", ")", "\n", "\n", "", "print", "(", "lenbucket_count_dict", ")", "\n", "return", "data_dicts", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.synthetic.pattn2count.write_data_to_file": [[128, 131], ["open", "json.dump"], "function", ["None"], ["", "def", "write_data_to_file", "(", "data", ",", "filepath", ")", ":", "\n", "    ", "with", "open", "(", "filepath", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "data", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.synthetic.count_ques.readDataset": [[24, 28], ["open", "json.load"], "function", ["None"], ["\"how many touchdown passes did\"", ",", "\n", "\"how many touchdowns did the\"", ",", "\n", "\"how many touchdowns were scored\"", ",", "\n", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.synthetic.count_ques.contains": [[30, 41], ["range", "range", "len", "starting_positions.append", "len", "len"], "function", ["None"], ["def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n", "\n", "", "def", "filter_questionattention", "(", "tokenized_queslower", ":", "str", ")", ":", "\n", "    ", "\"\"\" Here we'll annotate questions with one/two attentions depending on if the program type is\n        1. find(QuestionAttention)\n        2. filter(QuestionAttention, find(QuestionAttention))\n    \"\"\"", "\n", "question_lower", "=", "tokenized_queslower", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.synthetic.count_ques.preprocess_HowManyYardsCount_ques": [[43, 132], ["len", "collections.defaultdict", "dataset.items", "collections.defaultdict.items", "len", "print", "print", "utils.util.round_all", "original_question.lower", "tokenized_passage.split", "any", "len", "len", "count_ques.make_count_instance", "str", "new_qa_pairs.append", "float"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.make_count_instance"], ["qlen", "=", "len", "(", "question_tokens", ")", "\n", "\n", "if", "\"how many field goals were\"", "in", "question_lower", ":", "\n", "# Non-filter question", "\n", "        ", "if", "question_lower", "in", "[", "\n", "\"how many field goals were kicked ?\"", ",", "\n", "\"how many field goals were kicked in the game ?\"", ",", "\n", "\"how many field goals were made ?\"", ",", "\n", "\"how many field goals were made in the game ?\"", ",", "\n", "\"how many field goals were scored ?\"", ",", "\n", "\"how many field goals were scored in the game ?\"", ",", "\n", "\"how many field goals were in the game ?\"", ",", "\n", "\"how many field goals were made in this game ?\"", ",", "\n", "\"how many field goals were in this game?\"", ",", "\n", "]", ":", "\n", "            ", "qtype", "=", "constants", ".", "COUNT_find_qtype", "\n", "question_attention_find", "=", "[", "2", ",", "3", "]", "# Inclusive", "\n", "question_attention_filter", "=", "None", "\n", "\n", "", "else", ":", "\n", "# QAttn1 (filter) is everything after \"how many f gs were\" until ?. QAttn2 (find) is F Gs, i.e. [2, 3]", "\n", "            ", "qtype", "=", "constants", ".", "COUNT_filter_find_qtype", "\n", "question_attention_filter", "=", "[", "5", ",", "qlen", "-", "2", "]", "# skipping first 5 tokens and ?", "\n", "question_attention_find", "=", "[", "2", ",", "3", "]", "\n", "\n", "", "", "elif", "\"how many field goals did\"", "in", "question_lower", ":", "\n", "        ", "qtype", "=", "constants", ".", "COUNT_filter_find_qtype", "\n", "question_attention_find", "=", "[", "2", ",", "3", "]", "\n", "question_attention_filter", "=", "[", "5", ",", "qlen", "-", "2", "]", "# skipping first 5 tokens and ?", "\n", "\n", "", "elif", "\"how many interceptions did\"", "in", "question_lower", ":", "\n", "        ", "qtype", "=", "constants", ".", "COUNT_filter_find_qtype", "\n", "question_attention_find", "=", "[", "2", ",", "2", "]", "\n", "question_attention_filter", "=", "[", "4", ",", "qlen", "-", "2", "]", "# skipping first 4 tokens and ?", "\n", "\n", "", "elif", "\"how many passes\"", "in", "question_lower", ":", "\n", "        ", "qtype", "=", "constants", ".", "COUNT_filter_find_qtype", "\n", "question_attention_find", "=", "[", "2", ",", "2", "]", "\n", "question_attention_filter", "=", "[", "4", ",", "qlen", "-", "2", "]", "# skipping first 4 tokens and ?", "\n", "\n", "", "elif", "\"how many rushing\"", "in", "question_lower", ":", "\n", "# Most questions are How many rushing touchdowns/yards were / did", "\n", "        ", "qtype", "=", "constants", ".", "COUNT_filter_find_qtype", "\n", "question_attention_find", "=", "[", "2", ",", "3", "]", "\n", "question_attention_filter", "=", "[", "5", ",", "qlen", "-", "2", "]", "# skipping first 5 tokens and ?", "\n", "\n", "", "elif", "\"how many touchdown passes did\"", "in", "question_lower", ":", "\n", "        ", "qtype", "=", "constants", ".", "COUNT_filter_find_qtype", "\n", "question_attention_find", "=", "[", "2", ",", "3", "]", "\n", "question_attention_filter", "=", "[", "5", ",", "qlen", "-", "2", "]", "# skipping first 5 tokens and ?", "\n", "\n", "", "elif", "\"how many touchdowns did the\"", "in", "question_lower", ":", "\n", "        ", "qtype", "=", "constants", ".", "COUNT_filter_find_qtype", "\n", "question_attention_find", "=", "[", "2", ",", "2", "]", "\n", "question_attention_filter", "=", "[", "5", ",", "qlen", "-", "2", "]", "# skipping first 5 tokens and ?", "\n", "\n", "", "elif", "\"how many touchdowns were scored\"", "in", "question_lower", ":", "\n", "        ", "if", "question_lower", "in", "[", "\n", "\"how many touchdowns were scored in the game ?\"", ",", "\n", "\"how many touchdowns were scored ?\"", ",", "\n", "\"how many touchdowns were scored in total ?\"", ",", "\n", "]", ":", "\n", "            ", "qtype", "=", "constants", ".", "COUNT_find_qtype", "\n", "question_attention_find", "=", "[", "2", ",", "2", "]", "# Inclusive", "\n", "question_attention_filter", "=", "None", "\n", "", "else", ":", "\n", "            ", "qtype", "=", "constants", ".", "COUNT_filter_find_qtype", "\n", "question_attention_find", "=", "[", "2", ",", "2", "]", "\n", "question_attention_filter", "=", "[", "5", ",", "qlen", "-", "2", "]", "# skipping first 5 tokens and ?", "\n", "\n", "", "", "return", "qtype", ",", "question_attention_filter", ",", "question_attention_find", "\n", "\n", "\n", "", "def", "convert_span_to_attention", "(", "qlen", ",", "span", ")", ":", "\n", "# span is inclusive on both ends", "\n", "    ", "qattn", "=", "[", "0.0", "]", "*", "qlen", "\n", "for", "x", "in", "range", "(", "span", "[", "0", "]", ",", "span", "[", "1", "]", "+", "1", ")", ":", "\n", "        ", "qattn", "[", "x", "]", "=", "1.0", "\n", "\n", "", "return", "qattn", "\n", "\n", "\n", "", "def", "preprocess_HowManyYardsCount_ques", "(", "dataset", ")", ":", "\n", "    ", "\"\"\" This function prunes for questions that are count based questions.\n\n        Along with pruning, we also supervise the with the qtype and program_supervised flag\n    \"\"\"", "\n", "\n", "new_dataset", "=", "{", "}", "\n", "total_ques", "=", "0", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.synthetic.count_ques.make_count_instance": [[134, 201], ["len", "random.random", "sum", "len", "random.randint", "relevant_spans[].split", "count_ques.contains", "len", "len", "len", "len", "random.shuffle", "random.randint", "min", "random.randint", "len", "range", "abs", "float", "random.gauss", "len"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.contains"], ["questions_w_attn", "=", "0", "\n", "num_passages", "=", "len", "(", "dataset", ")", "\n", "qtype_dist", "=", "defaultdict", "(", "int", ")", "\n", "\n", "for", "passage_id", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "new_qa_pairs", "=", "[", "]", "\n", "for", "question_answer", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "            ", "total_ques", "+=", "1", "\n", "\n", "original_question", "=", "question_answer", "[", "constants", ".", "cleaned_question", "]", "\n", "question_lower", "=", "original_question", ".", "lower", "(", ")", "\n", "tokenized_ques", "=", "question_answer", "[", "constants", ".", "tokenized_question", "]", "\n", "tokens", "=", "tokenized_ques", ".", "split", "(", "\" \"", ")", "\n", "qlen", "=", "len", "(", "tokens", ")", "\n", "if", "any", "(", "span", "in", "question_lower", "for", "span", "in", "COUNT_TRIGRAMS", ")", ":", "\n", "                ", "(", "qtype", ",", "question_attention_filter_span", ",", "question_attention_find_span", ")", "=", "filter_questionattention", "(", "\n", "tokenized_queslower", "=", "tokenized_ques", ".", "lower", "(", ")", "\n", ")", "\n", "\n", "if", "question_attention_filter_span", "is", "not", "None", ":", "\n", "                    ", "filter_qattn", "=", "convert_span_to_attention", "(", "qlen", ",", "question_attention_filter_span", ")", "\n", "", "else", ":", "\n", "                    ", "filter_qattn", "=", "None", "\n", "\n", "", "find_qattn", "=", "convert_span_to_attention", "(", "qlen", ",", "question_attention_find_span", ")", "\n", "\n", "question_answer", "[", "constants", ".", "qtype", "]", "=", "qtype", "\n", "question_answer", "[", "constants", ".", "program_supervised", "]", "=", "True", "\n", "qtype_dist", "[", "qtype", "]", "+=", "1", "\n", "\n", "# Also adding qattn -- everything apart from the first two tokens", "\n", "if", "filter_qattn", "is", "not", "None", ":", "\n", "                    ", "question_answer", "[", "constants", ".", "ques_attention_supervision", "]", "=", "[", "filter_qattn", ",", "find_qattn", "]", "\n", "", "else", ":", "\n", "                    ", "question_answer", "[", "constants", ".", "ques_attention_supervision", "]", "=", "[", "find_qattn", "]", "\n", "", "question_answer", "[", "constants", ".", "qattn_supervised", "]", "=", "True", "\n", "questions_w_attn", "+=", "1", "\n", "\n", "new_qa_pairs", ".", "append", "(", "question_answer", ")", "\n", "\n", "", "", "if", "len", "(", "new_qa_pairs", ")", ">", "0", ":", "\n", "            ", "passage_info", "[", "constants", ".", "qa_pairs", "]", "=", "new_qa_pairs", "\n", "new_dataset", "[", "passage_id", "]", "=", "passage_info", "\n", "after_pruning_ques", "+=", "len", "(", "new_qa_pairs", ")", "\n", "\n", "", "", "num_passages_after_prune", "=", "len", "(", "new_dataset", ")", "\n", "print", "(", "f\"Passages original:{num_passages}  Questions original:{total_ques}\"", ")", "\n", "print", "(", "f\"Passages after-pruning:{num_passages_after_prune}  Question after-pruning:{after_pruning_ques}\"", ")", "\n", "print", "(", "f\"Ques with attn: {questions_w_attn}\"", ")", "\n", "print", "(", "f\"QType distribution: {qtype_dist}\"", ")", "\n", "\n", "return", "new_dataset", "\n", "\n", "\n", "", "if", "__name__", "==", "\"__main__\"", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--input_dir\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "train_json", "=", "\"drop_dataset_train.json\"", "\n", "dev_json", "=", "\"drop_dataset_dev.json\"", "\n", "\n", "input_dir", "=", "args", ".", "input_dir", "\n", "output_dir", "=", "args", ".", "output_dir", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "output_dir", ",", "exist_ok", "=", "True", ")", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.synthetic.num_grounding.readDataset": [[26, 30], ["open", "json.load"], "function", ["None"], ["def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.synthetic.num_grounding.generateNumGroundingQues": [[32, 145], ["len", "collections.defaultdict", "dataset.items", "len", "print", "tokenized_passage.split", "sorted", "str", "new_qa_pairs.append", "max", "min", "range", "len", "len", "len", "len", "sorted.append"], "function", ["None"], ["", "def", "generateNumGroundingQues", "(", "dataset", ")", ":", "\n", "    ", "\"\"\" Here we make synthetic data for counting questions.\n        Idea is to generate semi-gold passage attentions and count-answer to train the count module.\n\n        Each question we generate will be UNK (irrelevant), containing:\n            - qytpe and program-supervision -- (count findPassageAttention_FAKE)\n                This findPassageAttention_FAKE will not take question-attention as input, in fact the gold passage-attn\n                as a side-arg.\n            - passage_attention & count value\n                We will generate semi-gold passage-attentions and count-values. These passage-attentions will be\n                used in the program above.\n\n        We generate these questions for passages that contain count-questions\n    \"\"\"", "\n", "\n", "new_dataset", "=", "{", "}", "\n", "total_ques", "=", "0", "\n", "num_passages", "=", "len", "(", "dataset", ")", "\n", "\n", "num_of_gen_ques", "=", "0", "\n", "count_distribution", "=", "defaultdict", "(", "int", ")", "\n", "\n", "BACKWARD_WINDOW_SIZE", "=", "5", "\n", "FORWARD_WINDOW_SIZE", "=", "10", "\n", "\n", "for", "passage_id", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "\n", "        ", "tokenized_passage", "=", "passage_info", "[", "constants", ".", "tokenized_passage", "]", "\n", "passage_num_mens", "=", "passage_info", "[", "constants", ".", "passage_num_mens", "]", "\n", "passage_num_values", "=", "passage_info", "[", "constants", ".", "passage_num_normalized_values", "]", "\n", "\n", "# Keeping passage to a maximum of 400 tokens to avoid conflicts later", "\n", "passage_tokens", "=", "tokenized_passage", ".", "split", "(", "\" \"", ")", "\n", "\n", "new_qa_pairs", "=", "[", "]", "\n", "\n", "token_number_idx_pairs", "=", "[", "]", "\n", "\n", "# The num corresponding to the first relevant-token will be the answer to fake question, keeping track", "\n", "min_token_idx", "=", "1000000", "\n", "answer_number_value", "=", "-", "1", "\n", "# Only keeping values that are within 400 tokens so we don't have to prune in the reader.", "\n", "for", "(", "str_val", ",", "num_token_idx", ",", "num_value", ")", "in", "passage_num_mens", ":", "\n", "            ", "if", "num_token_idx", ">=", "400", ":", "\n", "                ", "continue", "\n", "", "starting_limit", "=", "max", "(", "0", ",", "num_token_idx", "-", "BACKWARD_WINDOW_SIZE", ")", "# Inclusive", "\n", "ending_limit", "=", "min", "(", "len", "(", "passage_tokens", ")", ",", "400", ",", "num_token_idx", "+", "FORWARD_WINDOW_SIZE", ")", "# Exclusive", "\n", "for", "i", "in", "range", "(", "starting_limit", ",", "ending_limit", ")", ":", "\n", "                ", "if", "passage_tokens", "[", "i", "]", "in", "RELEVANT_TOKENS", "and", "num_value", "in", "passage_num_values", ":", "\n", "                    ", "token_number_idx_pairs", ".", "append", "(", "(", "i", ",", "num_token_idx", ")", ")", "\n", "if", "num_token_idx", "<", "min_token_idx", ":", "\n", "                        ", "min_token_idx", "=", "num_token_idx", "\n", "answer_number_value", "=", "num_value", "\n", "\n", "", "", "", "", "if", "answer_number_value", "==", "-", "1", ":", "\n", "            ", "continue", "\n", "\n", "# Now we have token_number_idx_pairs: [ (tokenidx, numberidx) ]", "\n", "# Sort according to tokenidx, and make a fake question with", "\n", "#   1. passage-attention on first tokenidx and corresponding number as answer", "\n", "\n", "", "token_number_idx_pairs", "=", "sorted", "(", "token_number_idx_pairs", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "\n", "answer_token_idx", "=", "token_number_idx_pairs", "[", "0", "]", "[", "0", "]", "\n", "attention", "=", "[", "0.0", "]", "*", "len", "(", "passage_tokens", ")", "\n", "attention", "[", "answer_token_idx", "]", "=", "1.0", "\n", "\n", "question_answer", "=", "passage_info", "[", "constants", ".", "qa_pairs", "]", "[", "0", "]", "\n", "\n", "answer", "=", "question_answer", "[", "constants", ".", "answer", "]", "\n", "answer", "[", "\"spans\"", "]", "=", "[", "]", "\n", "answer", "[", "\"number\"", "]", "=", "str", "(", "answer_number_value", ")", "\n", "\n", "question_answer", "[", "constants", ".", "answer_passage_spans", "]", "=", "[", "]", "\n", "question_answer", "[", "constants", ".", "answer_question_spans", "]", "=", "[", "]", "\n", "\n", "query_id", "=", "question_answer", "[", "constants", ".", "query_id", "]", "\n", "query_id", "+=", "\"-synthetic-numgrounding\"", "\n", "question_answer", "[", "constants", ".", "query_id", "]", "=", "query_id", "\n", "\n", "question_answer", "[", "constants", ".", "question", "]", "=", "\"synthetic number\"", "\n", "question_answer", "[", "constants", ".", "tokenized_question", "]", "=", "\"synthetic number\"", "\n", "question_answer", "[", "constants", ".", "cleaned_question", "]", "=", "\"synthetic number\"", "\n", "question_answer", "[", "constants", ".", "question_charidxs", "]", "=", "[", "0", ",", "10", "]", "\n", "question_answer", "[", "constants", ".", "answer_type", "]", "=", "constants", ".", "NUM_TYPE", "\n", "\n", "question_answer", "[", "constants", ".", "SYN_NUMGROUND_METADATA", "]", "=", "token_number_idx_pairs", "\n", "\n", "question_answer", "[", "constants", ".", "qtype", "]", "=", "constants", ".", "SYN_NUMGROUND_qtype", "\n", "question_answer", "[", "constants", ".", "program_supervised", "]", "=", "True", "\n", "\n", "question_answer", "[", "constants", ".", "pattn_supervised", "]", "=", "True", "\n", "question_answer", "[", "constants", ".", "passage_attn_supervision", "]", "=", "attention", "\n", "\n", "# Adding this so that the instance remains strongly supervised", "\n", "question_answer", "[", "constants", ".", "qattn_supervised", "]", "=", "True", "\n", "question_answer", "[", "constants", ".", "ques_attention_supervision", "]", "=", "[", "[", "1.0", ",", "1.0", "]", "]", "# Single attention vector of size=1", "\n", "\n", "# The final output of the program is enough to train, so no aux loss / execution supervision is needed", "\n", "# Still label as execution_supervised as it requires passing the pattn as side-arg", "\n", "question_answer", "[", "constants", ".", "exection_supervised", "]", "=", "True", "\n", "\n", "new_qa_pairs", ".", "append", "(", "question_answer", ")", "\n", "\n", "if", "len", "(", "new_qa_pairs", ")", ">", "0", ":", "\n", "            ", "passage_info", "[", "constants", ".", "qa_pairs", "]", "=", "new_qa_pairs", "\n", "new_dataset", "[", "passage_id", "]", "=", "passage_info", "\n", "num_of_gen_ques", "+=", "len", "(", "new_qa_pairs", ")", "\n", "\n", "", "", "num_passages_after_prune", "=", "len", "(", "new_dataset", ")", "\n", "print", "(", "f\"Passages:{num_passages_after_prune}  Questions:{num_of_gen_ques}\"", ")", "\n", "\n", "return", "new_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.supervision_stats.readDataset": [[11, 15], ["open", "json.load"], "function", ["None"], ["def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.supervision_stats.supervisionStats": [[17, 59], ["supervision_stats.readDataset", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "readDataset.items", "print", "print", "print", "collections.defaultdict", "json.dumps", "json.dumps"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.data_manipulation.simplify_programs.readDataset"], ["", "def", "supervisionStats", "(", "input_json", ")", ":", "\n", "    ", "\"\"\" For this dataset, find the following:\n        1. How many questions have a qtype, i.e. program_supervised\n        2. For each qtype, find the number of questions with qattn_supervised and execution_supervised\n    \"\"\"", "\n", "dataset", "=", "readDataset", "(", "input_json", ")", "\n", "\n", "numparas", "=", "0", "\n", "numques", "=", "0", "\n", "\n", "qtype_dict", "=", "defaultdict", "(", "int", ")", "\n", "supervision_dict", "=", "defaultdict", "(", "int", ")", "\n", "supervision_dict", "[", "\"TOTAL\"", "]", "=", "defaultdict", "(", "int", ")", "\n", "\n", "for", "pid", ",", "pinfo", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "numparas", "+=", "1", "\n", "passage", "=", "pinfo", "[", "constants", ".", "tokenized_passage", "]", "\n", "\n", "qa_pairs", "=", "pinfo", "[", "constants", ".", "qa_pairs", "]", "\n", "\n", "for", "qa", "in", "qa_pairs", ":", "\n", "            ", "numques", "+=", "1", "\n", "\n", "if", "constants", ".", "qtype", "in", "qa", ":", "\n", "                ", "qtype", "=", "qa", "[", "constants", ".", "qtype", "]", "\n", "assert", "qa", "[", "constants", ".", "program_supervised", "]", "is", "True", ",", "f\"Qtype: {qtype}, program_supervised: False\"", "\n", "", "else", ":", "\n", "                ", "qtype", "=", "\"UNK\"", "\n", "\n", "", "qtype_dict", "[", "qtype", "]", "+=", "1", "\n", "if", "qtype", "not", "in", "supervision_dict", ":", "\n", "                ", "supervision_dict", "[", "qtype", "]", "=", "defaultdict", "(", "int", ")", "\n", "\n", "", "for", "supervision_key", "in", "SUPERVISION_KEYS", ":", "\n", "                ", "if", "supervision_key", "in", "qa", ":", "\n", "                    ", "qa_supervionkey_val", "=", "1", "if", "qa", "[", "supervision_key", "]", "is", "True", "else", "0", "\n", "supervision_dict", "[", "qtype", "]", "[", "supervision_key", "]", "+=", "qa_supervionkey_val", "\n", "supervision_dict", "[", "\"TOTAL\"", "]", "[", "supervision_key", "]", "+=", "qa_supervionkey_val", "\n", "\n", "", "", "", "", "print", "(", "f\"Paras: {numparas}  NumQues:{numques}\"", ")", "\n", "print", "(", "f\"Qtypes Count: {json.dumps(qtype_dict, indent=2)}\"", ")", "\n", "print", "(", "f\"Per Qtype supervision amount:\\n {json.dumps(supervision_dict, indent=2)}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.answer_type.answerTypeAnalysis": [[8, 58], ["print", "print", "collections.defaultdict", "collections.defaultdict", "json.load.items", "print", "print", "print", "print", "print", "open", "json.load", "len", "print", "len"], "function", ["None"], ["def", "answerTypeAnalysis", "(", "input_json", ":", "str", ")", "->", "None", ":", "\n", "    ", "\"\"\" Perform some analysis on answer types. \"\"\"", "\n", "\n", "print", "(", "\"Reading input json: {}\"", ".", "format", "(", "input_json", ")", ")", "\n", "\n", "# Input file contains single json obj with list of questions as jsonobjs inside it", "\n", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "print", "(", "\"Number of docs: {}\"", ".", "format", "(", "len", "(", "dataset", ")", ")", ")", "\n", "\n", "anstypes_count", "=", "defaultdict", "(", "int", ")", "\n", "num_pspan_ans", "=", "0", "\n", "num_qspan_ans", "=", "0", "\n", "num_bothspan_ans", "=", "0", "\n", "num_nospan_ans", "=", "0", "\n", "num_qa", "=", "0", "\n", "spanans_numspandist", "=", "defaultdict", "(", "int", ")", "\n", "\n", "for", "passage_id", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "qa_pairs", "=", "passage_info", "[", "constants", ".", "qa_pairs", "]", "\n", "for", "qa", "in", "qa_pairs", ":", "\n", "            ", "num_qa", "+=", "1", "\n", "ans_type", "=", "qa", "[", "constants", ".", "answer_type", "]", "\n", "anstypes_count", "[", "ans_type", "]", "+=", "1", "\n", "\n", "answer_dict", "=", "qa", "[", "constants", ".", "answer", "]", "\n", "\n", "if", "ans_type", "==", "constants", ".", "NUM_TYPE", ":", "\n", "                ", "print", "(", "qa", ")", "\n", "\n", "", "if", "ans_type", "==", "constants", ".", "SPAN_TYPE", ":", "\n", "                ", "answer_span_texts", "=", "answer_dict", "[", "\"spans\"", "]", "\n", "spanans_numspandist", "[", "len", "(", "answer_span_texts", ")", "]", "+=", "1", "\n", "q_ans_spans", "=", "qa", "[", "constants", ".", "answer_question_spans", "]", "\n", "p_ans_spans", "=", "qa", "[", "constants", ".", "answer_passage_spans", "]", "\n", "if", "q_ans_spans", "and", "p_ans_spans", ":", "\n", "                    ", "num_bothspan_ans", "+=", "1", "\n", "", "elif", "q_ans_spans", ":", "\n", "                    ", "num_qspan_ans", "+=", "1", "\n", "", "elif", "p_ans_spans", ":", "\n", "                    ", "num_pspan_ans", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "num_nospan_ans", "+=", "1", "\n", "\n", "", "", "", "", "print", "(", "f\"Num of QA:{num_qa}\"", ")", "\n", "print", "(", "f\"Answer Types: {anstypes_count}\"", ")", "\n", "print", "(", "\"Among span answers:\"", ")", "\n", "print", "(", "f\"PSpans: {num_pspan_ans}. QSpans:{num_qspan_ans}, BothSpans:{num_bothspan_ans}, NoSpans:{num_nospan_ans}\"", ")", "\n", "print", "(", "f\"SpanType ans - number of correct spans: {spanans_numspandist}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.write_ques_str.get_dataset": [[13, 17], ["open", "json.load"], "function", ["None"], ["def", "get_dataset", "(", "infile", ")", ":", "\n", "    ", "with", "open", "(", "infile", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.write_ques_str.printDataset": [[19, 43], ["open", "dataset.items", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write"], "function", ["None"], ["", "def", "printDataset", "(", "dataset", ",", "outfile", ")", ":", "\n", "    ", "with", "open", "(", "outfile", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "for", "pid", ",", "pinfo", "in", "dataset", ".", "items", "(", ")", ":", "\n", "            ", "passage", "=", "pinfo", "[", "constants", ".", "original_passage", "]", "\n", "qapairs", "=", "pinfo", "[", "\"qa_pairs\"", "]", "\n", "dates", "=", "pinfo", "[", "constants", ".", "passage_date_normalized_values", "]", "\n", "for", "qapair", "in", "qapairs", ":", "\n", "                ", "q", "=", "qapair", "[", "constants", ".", "original_question", "]", "\n", "if", "constants", ".", "qspan_dategrounding_supervision", "in", "qapair", ":", "\n", "                    ", "passage_event_date_groundings", "=", "qapair", "[", "constants", ".", "qspan_dategrounding_supervision", "]", "\n", "", "else", ":", "\n", "                    ", "passage_event_date_groundings", "=", "[", "]", "\n", "", "if", "constants", ".", "question_event_date_values", "in", "qapair", ":", "\n", "                    ", "passage_event_date_values", "=", "qapair", "[", "constants", ".", "question_event_date_values", "]", "\n", "", "else", ":", "\n", "                    ", "passage_event_date_values", "=", "[", "]", "\n", "\n", "", "answer", "=", "qapair", "[", "constants", ".", "answer", "]", "\n", "f", ".", "write", "(", "f\"{q}\\n\"", ")", "\n", "f", ".", "write", "(", "f\"{passage}\\n\"", ")", "\n", "f", ".", "write", "(", "f\"{dates}\\n\"", ")", "\n", "f", ".", "write", "(", "f\"{answer}\\n\"", ")", "\n", "f", ".", "write", "(", "f\"event_date_values: {passage_event_date_values}\"", ")", "\n", "f", ".", "write", "(", "f\"event_date_groundings: {passage_event_date_groundings}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.quespara_size.readDataset": [[9, 13], ["open", "json.load"], "function", ["None"], ["def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.quespara_size.quesParaSize": [[15, 55], ["quespara_size.readDataset", "collections.defaultdict", "readDataset.items", "print", "print", "print", "collections.defaultdict.items", "print", "print", "print", "print", "print", "len", "utils.util.round_all", "passage.split", "len", "float"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.data_manipulation.simplify_programs.readDataset", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all"], ["", "def", "quesParaSize", "(", "input_json", ")", ":", "\n", "    ", "dataset", "=", "readDataset", "(", "input_json", ")", "\n", "\n", "numparas", "=", "0", "\n", "numques", "=", "0", "\n", "maxparalen", "=", "0", "\n", "maxqueslen", "=", "0", "\n", "qtype_dist", "=", "defaultdict", "(", "int", ")", "\n", "\n", "for", "pid", ",", "pinfo", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "numparas", "+=", "1", "\n", "passage", "=", "pinfo", "[", "constants", ".", "tokenized_passage", "]", "\n", "plen", "=", "len", "(", "passage", ".", "split", "(", "\" \"", ")", ")", "\n", "maxparalen", "=", "plen", "if", "plen", ">", "maxparalen", "else", "maxparalen", "\n", "\n", "qa_pairs", "=", "pinfo", "[", "constants", ".", "qa_pairs", "]", "\n", "\n", "for", "qa", "in", "qa_pairs", ":", "\n", "            ", "numques", "+=", "1", "\n", "qlen", "=", "len", "(", "qa", "[", "constants", ".", "tokenized_question", "]", ")", "\n", "maxqueslen", "=", "qlen", "if", "qlen", ">", "maxqueslen", "else", "maxqueslen", "\n", "\n", "if", "constants", ".", "qtype", "in", "qa", ":", "\n", "                ", "qtype_dist", "[", "qa", "[", "constants", ".", "qtype", "]", "]", "+=", "1", "\n", "", "else", ":", "\n", "                ", "qtype_dist", "[", "\"UNK\"", "]", "+=", "1", "\n", "\n", "", "", "", "print", "(", "\"\\nCount of QTypes\"", ")", "\n", "print", "(", "qtype_dist", ")", "\n", "print", "(", ")", "\n", "\n", "for", "k", ",", "v", "in", "qtype_dist", ".", "items", "(", ")", ":", "\n", "        ", "qtype_dist", "[", "k", "]", "=", "round_all", "(", "100", "*", "(", "float", "(", "v", ")", "/", "numques", ")", ",", "1", ")", "\n", "\n", "", "print", "(", "\"\\nPercentage of QTypes:\"", ")", "\n", "print", "(", "qtype_dist", ")", "\n", "print", "(", ")", "\n", "\n", "print", "(", "f\"Paras: {numparas}  MaxParaLen:{maxparalen}\"", ")", "\n", "print", "(", "f\"Questions: {numques}  MaxQuesLen:{maxqueslen}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.ngram_split.get_dataset": [[86, 90], ["open", "json.load"], "function", ["None"], ["def", "get_dataset", "(", "infile", ")", ":", "\n", "    ", "with", "open", "(", "infile", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.ngram_split.write_ques": [[92, 130], ["open", "open", "train_dataset.items", "open.close", "open.close", "open", "json.dump", "q.lower", "copy.deepcopy", "relevant_qapairs.append", "open.write", "open.write"], "function", ["None"], ["", "def", "write_ques", "(", "train_dataset", ",", "ngram", ":", "str", ",", "ques_filepath", ":", "str", ",", "qapara_filepath", ":", "str", ",", "output_json", ")", ":", "\n", "    ", "qfile", "=", "open", "(", "ques_filepath", ",", "\"w\"", ")", "\n", "qafile", "=", "open", "(", "qapara_filepath", ",", "\"w\"", ")", "\n", "\n", "output_dataset", "=", "{", "}", "\n", "\n", "for", "pid", ",", "pinfo", "in", "train_dataset", ".", "items", "(", ")", ":", "\n", "        ", "passage", "=", "pinfo", "[", "constants", ".", "cleaned_passage", "]", "\n", "qapairs", "=", "pinfo", "[", "constants", ".", "qa_pairs", "]", "\n", "\n", "relevant_qapairs", "=", "[", "]", "\n", "for", "qapair", "in", "qapairs", ":", "\n", "            ", "q", "=", "qapair", "[", "constants", ".", "cleaned_question", "]", "\n", "q_lower", "=", "q", ".", "lower", "(", ")", "\n", "if", "constants", ".", "answer", "in", "qapair", ":", "\n", "                ", "answer", "=", "qapair", "[", "constants", ".", "answer", "]", "\n", "", "else", ":", "\n", "                ", "answer", "=", "{", "}", "\n", "\n", "", "if", "constants", ".", "answer_passage_spans", "in", "qapair", ":", "\n", "                ", "ans_as_passage_span", "=", "qapair", "[", "constants", ".", "answer_passage_spans", "]", "\n", "", "else", ":", "\n", "                ", "ans_as_passage_span", "=", "\"NONE\"", "\n", "\n", "", "if", "ngram", "in", "q_lower", ":", "\n", "                ", "relevant_qapairs", ".", "append", "(", "qapair", ")", "\n", "qfile", ".", "write", "(", "f\"{q}\\n\"", ")", "\n", "qafile", ".", "write", "(", "f\"Questions: {q}\\nAnswer: {answer}\\nAnsAsPassage: {ans_as_passage_span}\\n{passage}\\n\\n\"", ")", "\n", "", "", "if", "relevant_qapairs", ":", "\n", "            ", "new_pinfo", "=", "copy", ".", "deepcopy", "(", "pinfo", ")", "\n", "new_pinfo", "[", "constants", ".", "qa_pairs", "]", "=", "relevant_qapairs", "\n", "output_dataset", "[", "pid", "]", "=", "new_pinfo", "\n", "\n", "", "", "with", "open", "(", "output_json", ",", "\"w\"", ")", "as", "outf", ":", "\n", "        ", "json", ".", "dump", "(", "output_dataset", ",", "outf", ",", "indent", "=", "4", ")", "\n", "\n", "", "qfile", ".", "close", "(", ")", "\n", "qafile", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.ngram_split.writeNgramOtherQues": [[132, 175], ["open", "open", "train_dataset.items", "open.close", "open.close", "open", "json.dump", "q.lower", "copy.deepcopy", "any", "relevant_qapairs.append", "open.write", "open.write"], "function", ["None"], ["", "def", "writeNgramOtherQues", "(", "\n", "answer_key", ":", "str", ",", "ngram_list", ",", "train_dataset", ",", "ques_filepath", ":", "str", ",", "qapara_filepath", ":", "str", ",", "output_json", ":", "str", "\n", ")", ":", "\n", "    ", "qfile", "=", "open", "(", "ques_filepath", ",", "\"w\"", ")", "\n", "qafile", "=", "open", "(", "qapara_filepath", ",", "\"w\"", ")", "\n", "\n", "output_dataset", "=", "{", "}", "\n", "\n", "for", "pid", ",", "pinfo", "in", "train_dataset", ".", "items", "(", ")", ":", "\n", "        ", "passage", "=", "pinfo", "[", "constants", ".", "cleaned_passage", "]", "\n", "qapairs", "=", "pinfo", "[", "constants", ".", "qa_pairs", "]", "\n", "\n", "relevant_qapairs", "=", "[", "]", "\n", "for", "qapair", "in", "qapairs", ":", "\n", "            ", "q", "=", "qapair", "[", "constants", ".", "cleaned_question", "]", "\n", "q_lower", "=", "q", ".", "lower", "(", ")", "\n", "if", "constants", ".", "answer", "in", "qapair", ":", "\n", "                ", "answer", "=", "qapair", "[", "constants", ".", "answer", "]", "\n", "", "else", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "answer_key", "in", "answer", "and", "answer", "[", "answer_key", "]", ":", "\n", "                ", "if", "not", "any", "(", "span", "in", "q_lower", "for", "span", "in", "ngram_list", ")", ":", "\n", "                    ", "if", "constants", ".", "answer_passage_spans", "in", "qapair", ":", "\n", "                        ", "ans_as_passage_span", "=", "qapair", "[", "constants", ".", "answer_passage_spans", "]", "\n", "", "else", ":", "\n", "                        ", "ans_as_passage_span", "=", "\"NONE\"", "\n", "", "relevant_qapairs", ".", "append", "(", "qapair", ")", "\n", "qfile", ".", "write", "(", "f\"{q}\\n\"", ")", "\n", "qafile", ".", "write", "(", "\n", "f\"Questions: {q}\\nAnswer: {answer}\\nAnsAsPassage: {ans_as_passage_span}\\n{passage}\\n\\n\"", "\n", ")", "\n", "\n", "", "", "", "if", "relevant_qapairs", ":", "\n", "            ", "new_pinfo", "=", "copy", ".", "deepcopy", "(", "pinfo", ")", "\n", "new_pinfo", "[", "constants", ".", "qa_pairs", "]", "=", "relevant_qapairs", "\n", "output_dataset", "[", "pid", "]", "=", "new_pinfo", "\n", "\n", "", "", "with", "open", "(", "output_json", ",", "\"w\"", ")", "as", "outf", ":", "\n", "        ", "json", ".", "dump", "(", "output_dataset", ",", "outf", ",", "indent", "=", "4", ")", "\n", "\n", "", "qfile", ".", "close", "(", ")", "\n", "qafile", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.ngram_split.writeNGramFiles": [[177, 215], ["ngram_split.get_dataset", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "ngram_split.writeNgramOtherQues", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "ngram_split.writeNgramOtherQues", "ngram.replace().replace", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "ngram_split.write_ques", "os.path.exists", "os.makedirs", "ngram.replace().replace", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "ngram_split.write_ques", "os.path.exists", "os.makedirs", "os.path.exists", "os.makedirs", "os.path.exists", "os.makedirs", "ngram.replace", "ngram.replace"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.write_qidqstr.get_dataset", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.ngram_split.writeNgramOtherQues", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.ngram_split.writeNgramOtherQues", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.ngram_split.write_ques", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.ngram_split.write_ques"], ["", "def", "writeNGramFiles", "(", "input_json", ",", "outputdir_root", ",", "questions_output_filename", ",", "quespara_output_filename", ",", "output_json", ")", ":", "\n", "    ", "dataset", "=", "get_dataset", "(", "input_json", ")", "\n", "\n", "for", "ngram", "in", "NUM_NGRAMS", ":", "\n", "        ", "dirname", "=", "ngram", ".", "replace", "(", "\" \"", ",", "\"_\"", ")", ".", "replace", "(", "\"'\"", ",", "\"\"", ")", "\n", "num_outputdir", "=", "os", ".", "path", ".", "join", "(", "outputdir_root", ",", "\"num\"", ",", "dirname", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "num_outputdir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "num_outputdir", ",", "exist_ok", "=", "True", ")", "\n", "", "ques_file", "=", "os", ".", "path", ".", "join", "(", "num_outputdir", ",", "questions_output_filename", ")", "\n", "qapara_file", "=", "os", ".", "path", ".", "join", "(", "num_outputdir", ",", "quespara_output_filename", ")", "\n", "output_json_path", "=", "os", ".", "path", ".", "join", "(", "num_outputdir", ",", "output_json", ")", "\n", "write_ques", "(", "dataset", ",", "ngram", ",", "ques_file", ",", "qapara_file", ",", "output_json_path", ")", "\n", "\n", "", "numother_outputdir", "=", "os", ".", "path", ".", "join", "(", "outputdir_root", ",", "\"num_other\"", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "numother_outputdir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "numother_outputdir", ",", "exist_ok", "=", "True", ")", "\n", "", "ques_file", "=", "os", ".", "path", ".", "join", "(", "numother_outputdir", ",", "questions_output_filename", ")", "\n", "qapara_file", "=", "os", ".", "path", ".", "join", "(", "numother_outputdir", ",", "quespara_output_filename", ")", "\n", "output_json_path", "=", "os", ".", "path", ".", "join", "(", "numother_outputdir", ",", "output_json", ")", "\n", "writeNgramOtherQues", "(", "\"number\"", ",", "NUM_NGRAMS", ",", "dataset", ",", "ques_file", ",", "qapara_file", ",", "output_json_path", ")", "\n", "\n", "for", "ngram", "in", "SPAN_NGRAMS", ":", "\n", "        ", "dirname", "=", "ngram", ".", "replace", "(", "\" \"", ",", "\"_\"", ")", ".", "replace", "(", "\"'\"", ",", "\"\"", ")", "\n", "span_outputdir", "=", "os", ".", "path", ".", "join", "(", "outputdir_root", ",", "\"span\"", ",", "dirname", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "span_outputdir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "span_outputdir", ",", "exist_ok", "=", "True", ")", "\n", "", "ques_file", "=", "os", ".", "path", ".", "join", "(", "span_outputdir", ",", "questions_output_filename", ")", "\n", "qapara_file", "=", "os", ".", "path", ".", "join", "(", "span_outputdir", ",", "quespara_output_filename", ")", "\n", "output_json_path", "=", "os", ".", "path", ".", "join", "(", "span_outputdir", ",", "output_json", ")", "\n", "write_ques", "(", "dataset", ",", "ngram", ",", "ques_file", ",", "qapara_file", ",", "output_json_path", ")", "\n", "\n", "", "spanother_outputdir", "=", "os", ".", "path", ".", "join", "(", "outputdir_root", ",", "\"span_other\"", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "spanother_outputdir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "spanother_outputdir", ",", "exist_ok", "=", "True", ")", "\n", "", "ques_file", "=", "os", ".", "path", ".", "join", "(", "spanother_outputdir", ",", "questions_output_filename", ")", "\n", "qapara_file", "=", "os", ".", "path", ".", "join", "(", "spanother_outputdir", ",", "quespara_output_filename", ")", "\n", "output_json_path", "=", "os", ".", "path", ".", "join", "(", "spanother_outputdir", ",", "output_json", ")", "\n", "writeNgramOtherQues", "(", "\"spans\"", ",", "SPAN_NGRAMS", ",", "dataset", ",", "ques_file", ",", "qapara_file", ",", "output_json_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.write_qidqstr.get_dataset": [[8, 12], ["open", "json.load"], "function", ["None"], ["def", "get_dataset", "(", "infile", ")", ":", "\n", "    ", "with", "open", "(", "infile", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.write_qidqstr.printDataset": [[14, 23], ["open", "dataset.items", "f.write"], "function", ["None"], ["", "def", "printDataset", "(", "dataset", ",", "outfile", ")", ":", "\n", "    ", "with", "open", "(", "outfile", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "for", "pid", ",", "pinfo", "in", "dataset", ".", "items", "(", ")", ":", "\n", "            ", "qapairs", "=", "pinfo", "[", "\"qa_pairs\"", "]", "\n", "for", "qapair", "in", "qapairs", ":", "\n", "                ", "q", "=", "qapair", "[", "constants", ".", "question", "]", "\n", "qid", "=", "qapair", "[", "constants", ".", "query_id", "]", "\n", "\n", "f", ".", "write", "(", "\"{}\\t{}\\n\"", ".", "format", "(", "qid", ",", "q", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.parabucketedsize.readDataset": [[7, 11], ["open", "json.load"], "function", ["None"], ["def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.parabucketedsize.quesParaSize": [[13, 60], ["parabucketedsize.readDataset", "readDataset.items", "print", "print", "print", "print", "print", "print", "print", "print", "print", "len", "float", "passage.split"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.data_manipulation.simplify_programs.readDataset"], ["", "def", "quesParaSize", "(", "input_json", ")", ":", "\n", "    ", "dataset", "=", "readDataset", "(", "input_json", ")", "\n", "numparas", "=", "0", "\n", "maxparalen", "=", "0", "\n", "passage_len_sums", "=", "0", "\n", "plen_lt_100_cnt", "=", "0", "\n", "plen_lt_200_cnt", "=", "0", "\n", "plen_lt_400_cnt", "=", "0", "\n", "plen_lt_500_cnt", "=", "0", "\n", "plen_lt_600_cnt", "=", "0", "\n", "plen_lt_800_cnt", "=", "0", "\n", "plen_lt_1000_cnt", "=", "0", "\n", "\n", "for", "pid", ",", "pinfo", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "numparas", "+=", "1", "\n", "passage", "=", "pinfo", "[", "constants", ".", "tokenized_passage", "]", "\n", "plen", "=", "len", "(", "passage", ".", "split", "(", "\" \"", ")", ")", "\n", "maxparalen", "=", "plen", "if", "plen", ">", "maxparalen", "else", "maxparalen", "\n", "\n", "passage_len_sums", "+=", "plen", "\n", "\n", "if", "plen", "<", "100", ":", "\n", "            ", "plen_lt_100_cnt", "+=", "1", "\n", "", "if", "plen", "<", "200", ":", "\n", "            ", "plen_lt_200_cnt", "+=", "1", "\n", "", "if", "plen", "<", "400", ":", "\n", "            ", "plen_lt_400_cnt", "+=", "1", "\n", "", "if", "plen", "<", "500", ":", "\n", "            ", "plen_lt_500_cnt", "+=", "1", "\n", "", "if", "plen", "<", "600", ":", "\n", "            ", "plen_lt_600_cnt", "+=", "1", "\n", "", "if", "plen", "<", "800", ":", "\n", "            ", "plen_lt_800_cnt", "+=", "1", "\n", "", "if", "plen", "<", "1000", ":", "\n", "            ", "plen_lt_1000_cnt", "+=", "1", "\n", "\n", "", "", "avg_plen", "=", "float", "(", "passage_len_sums", ")", "/", "numparas", "\n", "\n", "print", "(", "f\"Paras: {numparas}  MaxParaLen:{maxparalen}\"", ")", "\n", "print", "(", "f\"Avg Para len: {avg_plen}\"", ")", "\n", "print", "(", "f\"Plen < 100: {plen_lt_100_cnt}\"", ")", "\n", "print", "(", "f\"Plen < 200: {plen_lt_200_cnt}\"", ")", "\n", "print", "(", "f\"Plen < 400: {plen_lt_400_cnt}\"", ")", "\n", "print", "(", "f\"Plen < 500: {plen_lt_500_cnt}\"", ")", "\n", "print", "(", "f\"Plen < 600: {plen_lt_600_cnt}\"", ")", "\n", "print", "(", "f\"Plen < 800: {plen_lt_800_cnt}\"", ")", "\n", "print", "(", "f\"Plen < 1000: {plen_lt_1000_cnt}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.num_steps_analysis.read_numsteps_annotation": [[7, 20], ["open", "f.readlines", "line.strip().split", "int", "line.strip"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.readlines"], ["def", "read_numsteps_annotation", "(", "filepath", "=", "\"analysis/dev_numsteps_annotation.tsv\"", ")", ":", "\n", "    ", "with", "open", "(", "filepath", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "\n", "", "instances", "=", "[", "line", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "for", "line", "in", "lines", "]", "\n", "\n", "qid2steps", "=", "{", "}", "\n", "for", "instance", "in", "instances", ":", "\n", "        ", "qid", "=", "instance", "[", "0", "]", "\n", "steps", "=", "instance", "[", "2", "]", "\n", "qid2steps", "[", "qid", "]", "=", "int", "(", "steps", ")", "\n", "\n", "", "return", "qid2steps", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.num_steps_analysis.read_qid2prediction": [[22, 35], ["open", "f.readlines", "line.strip().split", "line.strip"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.readlines"], ["", "def", "read_qid2prediction", "(", "filepath", ")", ":", "\n", "    ", "with", "open", "(", "filepath", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "\n", "", "instances", "=", "[", "line", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "for", "line", "in", "lines", "]", "\n", "\n", "qid2pred", "=", "{", "}", "\n", "for", "instance", "in", "instances", ":", "\n", "        ", "qid", "=", "instance", "[", "0", "]", "\n", "pred", "=", "1", "if", "instance", "[", "2", "]", "==", "\"C\"", "else", "0", "\n", "qid2pred", "[", "qid", "]", "=", "pred", "\n", "\n", "", "return", "qid2pred", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.num_steps_analysis.count_step2correct": [[37, 59], ["collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "qid2steps.items", "collections.defaultdict.items", "print", "print", "print", "float"], "function", ["None"], ["", "def", "count_step2correct", "(", "qid2steps", ",", "qid2pred", ")", ":", "\n", "    ", "steps2total", "=", "defaultdict", "(", "float", ")", "\n", "steps2correct", "=", "defaultdict", "(", "int", ")", "\n", "steps2incorrect", "=", "defaultdict", "(", "float", ")", "\n", "\n", "steps2crrectratio", "=", "defaultdict", "(", "float", ")", "\n", "\n", "for", "qid", ",", "steps", "in", "qid2steps", ".", "items", "(", ")", ":", "\n", "        ", "if", "qid", "in", "qid2pred", ":", "\n", "            ", "steps2total", "[", "steps", "]", "+=", "1", "\n", "steps2correct", "[", "steps", "]", "+=", "qid2pred", "[", "qid", "]", "\n", "\n", "", "", "for", "steps", ",", "total", "in", "steps2total", ".", "items", "(", ")", ":", "\n", "        ", "correct", "=", "steps2correct", "[", "steps", "]", "\n", "\n", "ration", "=", "float", "(", "correct", ")", "/", "total", "\n", "steps2crrectratio", "[", "steps", "]", "=", "ration", "\n", "\n", "", "print", "(", "steps2total", ")", "\n", "print", "(", "steps2correct", ")", "\n", "print", "(", "steps2crrectratio", ")", "\n", "return", "steps2crrectratio", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.compare_MTMSN_Ours.make_correct_incorrect_dicts": [[8, 28], ["set", "set", "preds.items", "set.add", "set.add"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add"], ["def", "make_correct_incorrect_dicts", "(", "preds", ")", ":", "\n", "    ", "\"\"\"Make separate sets of question ids for correctly and in-correctly answered questions.\n\n    Args:\n        instances: is a 3(4)-tuple with (q-id, ques-text, 'C'/'NC', logical_form (optional) )\n\n    Returns:\n        correct_qids: Set of qids answered correctly\n        incorrect_qids: Set of qids answered incorrectly\n    \"\"\"", "\n", "correct_qids", "=", "set", "(", ")", "\n", "incorrect_qids", "=", "set", "(", ")", "\n", "\n", "for", "query_id", ",", "pred_dict", "in", "preds", ".", "items", "(", ")", ":", "\n", "        ", "if", "pred_dict", "[", "\"f1\"", "]", ">=", "0.7", ":", "\n", "            ", "correct_qids", ".", "add", "(", "query_id", ")", "\n", "", "else", ":", "\n", "            ", "incorrect_qids", ".", "add", "(", "query_id", ")", "\n", "\n", "", "", "return", "correct_qids", ",", "incorrect_qids", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.compare_MTMSN_Ours.make_qid2questiondict": [[30, 44], ["our_model_preds.items"], "function", ["None"], ["", "def", "make_qid2questiondict", "(", "our_model_preds", ")", ":", "\n", "    ", "\"\"\"Make question_id to question text map\n\n    Args:\n        our_model_preds: keys are query_id and the value is a dictionary containing the key \"question\"\n\n    Returns:\n        qid2qtext: Dict from qid 2 qtext\n    \"\"\"", "\n", "qid2qtext", "=", "{", "}", "\n", "for", "query_id", ",", "pred_dict", "in", "our_model_preds", ".", "items", "(", ")", ":", "\n", "        ", "question", "=", "pred_dict", "[", "\"question\"", "]", "\n", "qid2qtext", "[", "query_id", "]", "=", "question", "\n", "", "return", "qid2qtext", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.compare_MTMSN_Ours.make_qid2logicalform": [[46, 60], ["preds.items"], "function", ["None"], ["", "def", "make_qid2logicalform", "(", "preds", ")", ":", "\n", "    ", "\"\"\"Make question_id to predicted logical_form map\n\n    Args:\n        preds: prediction dict with \"type\" key containing LF\n\n    Returns:\n        qid2lf: Dict from qid 2 logical form\n    \"\"\"", "\n", "qid2lf", "=", "{", "}", "\n", "for", "query_id", ",", "pred_dict", "in", "preds", ".", "items", "(", ")", ":", "\n", "        ", "logical_form", "=", "pred_dict", "[", "\"type\"", "]", "\n", "qid2lf", "[", "query_id", "]", "=", "logical_form", "\n", "", "return", "qid2lf", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.compare_MTMSN_Ours.print_qtexts": [[62, 65], ["enumerate", "print"], "function", ["None"], ["", "def", "print_qtexts", "(", "qids", ",", "qid2qtext", ")", ":", "\n", "    ", "for", "i", ",", "qid", "in", "enumerate", "(", "qids", ")", ":", "\n", "        ", "print", "(", "\"{}: {}\"", ".", "format", "(", "i", ",", "qid2qtext", "[", "qid", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.compare_MTMSN_Ours.print_qtext_lf": [[67, 70], ["enumerate", "print"], "function", ["None"], ["", "", "def", "print_qtext_lf", "(", "qids", ",", "qid2qtext", ",", "qid2lf", ")", ":", "\n", "    ", "for", "i", ",", "qid", "in", "enumerate", "(", "qids", ")", ":", "\n", "        ", "print", "(", "\"{}: {} -- {}\"", ".", "format", "(", "i", ",", "qid2qtext", "[", "qid", "]", ",", "qid2lf", "[", "qid", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.compare_MTMSN_Ours.print_qtext_bothlfs": [[72, 76], ["enumerate", "print", "print"], "function", ["None"], ["", "", "def", "print_qtext_bothlfs", "(", "qids", ",", "qid2qtext", ",", "qid2lf1", ",", "qid2lf2", ")", ":", "\n", "    ", "for", "i", ",", "qid", "in", "enumerate", "(", "qids", ")", ":", "\n", "        ", "print", "(", "\"qid: {}\"", ".", "format", "(", "qid", ")", ")", "\n", "print", "(", "\"{}: {}\\n{}\\n{}\\n\"", ".", "format", "(", "i", ",", "qid2qtext", "[", "qid", "]", ",", "qid2lf1", "[", "qid", "]", ",", "qid2lf2", "[", "qid", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.compare_MTMSN_Ours.diff_in_lfs": [[78, 88], ["enumerate"], "function", ["None"], ["", "", "def", "diff_in_lfs", "(", "qids", ",", "qid2lf1", ",", "qid2lf2", ")", ":", "\n", "    ", "num_diff_lf", "=", "0", "\n", "for", "i", ",", "qid", "in", "enumerate", "(", "qids", ")", ":", "\n", "        ", "lf1", "=", "qid2lf1", "[", "qid", "]", "\n", "lf2", "=", "qid2lf2", "[", "qid", "]", "\n", "\n", "if", "lf1", "!=", "lf2", ":", "\n", "            ", "num_diff_lf", "+=", "1", "\n", "\n", "", "", "return", "num_diff_lf", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.compare_MTMSN_Ours.overlap_in_correct_incorrect_questext": [[90, 99], ["set", "set", "set.intersection", "print", "print", "print", "len", "len", "len"], "function", ["None"], ["", "def", "overlap_in_correct_incorrect_questext", "(", "correct_qids", ",", "incorrect_qids", ",", "qid2ques", ")", ":", "\n", "    ", "correct_qtexts", "=", "set", "(", "[", "qid2ques", "[", "qid", "]", "for", "qid", "in", "correct_qids", "]", ")", "\n", "incorrect_qtexts", "=", "set", "(", "[", "qid2ques", "[", "qid", "]", "for", "qid", "in", "incorrect_qids", "]", ")", "\n", "\n", "overlap", "=", "correct_qtexts", ".", "intersection", "(", "incorrect_qtexts", ")", "\n", "\n", "print", "(", "\"Unique correct questions: {}\"", ".", "format", "(", "len", "(", "correct_qtexts", ")", ")", ")", "\n", "print", "(", "\"Unique incorrect questions: {}\"", ".", "format", "(", "len", "(", "incorrect_qtexts", ")", ")", ")", "\n", "print", "(", "\"Unique question overlap: {}\"", ".", "format", "(", "len", "(", "overlap", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.compare_MTMSN_Ours.error_overlap_statistics": [[101, 171], ["len", "compare_MTMSN_Ours.make_qid2questiondict", "compare_MTMSN_Ours.make_qid2logicalform", "compare_MTMSN_Ours.make_qid2logicalform", "compare_MTMSN_Ours.make_correct_incorrect_dicts", "compare_MTMSN_Ours.make_correct_incorrect_dicts", "correct_qids_ours.intersection", "correct_qids_ours.intersection", "incorrect_qids_ours.intersection", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "len", "float", "len", "float", "os.path.exists", "os.makedirs", "open", "open", "len", "len", "len", "len", "len", "len", "len", "os.path.join", "f.write", "os.path.join", "f.write"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.make_qid2questiondict", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.make_qid2logicalform", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.make_qid2logicalform", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.make_correct_incorrect_dicts", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.make_correct_incorrect_dicts"], ["", "def", "error_overlap_statistics", "(", "our_model_preds", ",", "base_model_preds", ",", "analysis_output_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"Perform analysis on predictions of two different models and see the level of overlap in their predictions.\n    \"\"\"", "\n", "num_instances", "=", "len", "(", "our_model_preds", ")", "\n", "\n", "qid2ques", "=", "make_qid2questiondict", "(", "our_model_preds", ")", "\n", "\n", "qid2lf_ours", "=", "make_qid2logicalform", "(", "our_model_preds", ")", "\n", "qid2lf_base", "=", "make_qid2logicalform", "(", "base_model_preds", ")", "\n", "\n", "correct_qids_ours", ",", "incorrect_qids_ours", "=", "make_correct_incorrect_dicts", "(", "our_model_preds", ")", "\n", "correct_qids_base", ",", "incorrect_qids_base", "=", "make_correct_incorrect_dicts", "(", "base_model_preds", ")", "\n", "\n", "perf1", "=", "len", "(", "correct_qids_ours", ")", "/", "float", "(", "num_instances", ")", "\n", "perf2", "=", "len", "(", "correct_qids_base", ")", "/", "float", "(", "num_instances", ")", "\n", "\n", "correct_overlap_qids", "=", "correct_qids_ours", ".", "intersection", "(", "correct_qids_base", ")", "\n", "\n", "correct1_incorrect2_qids", "=", "correct_qids_ours", ".", "intersection", "(", "incorrect_qids_base", ")", "\n", "incorrect1_correct2_qids", "=", "incorrect_qids_ours", ".", "intersection", "(", "correct_qids_base", ")", "\n", "\n", "\n", "# # For questions predicted correct by M1, and incorrect by M2 -- the number of ques with diff LFs", "\n", "# c1_nc2_lf_diff = diff_in_lfs(correct1_incorrect2_qids, qid2lf_ours, qid2lf_base)", "\n", "# nc1_c2_lf_diff = diff_in_lfs(incorrect1_correct2_qids, qid2lf1, qid2lf2)", "\n", "\n", "# print(\"Correct in Model 1, Incorrect in Model 2\")", "\n", "# print_qtext_bothlfs(correct1_incorrect2_qids, qid2qtext2, qid2lf1, qid2lf2)", "\n", "\n", "# print(\"Incorrect in Model 1, Correct in Model 2\")", "\n", "# print_qtext_bothlfs(incorrect1_correct2_qids, qid2ques, qid2lf1, qid2lf2)", "\n", "\n", "print", "(", "\"Model Ours\"", ")", "\n", "print", "(", "\n", "\"Num instances: {} Correct: {} Incorrect: {} Perf: {}\"", ".", "format", "(", "\n", "num_instances", ",", "len", "(", "correct_qids_ours", ")", ",", "len", "(", "incorrect_qids_ours", ")", ",", "perf1", "\n", ")", "\n", ")", "\n", "print", "(", ")", "\n", "print", "(", "\"Model Base\"", ")", "\n", "print", "(", "\n", "\"Num instances: {} Correct: {} Incorrect: {} Perf: {}\"", ".", "format", "(", "\n", "num_instances", ",", "len", "(", "correct_qids_base", ")", ",", "len", "(", "incorrect_qids_base", ")", ",", "perf2", "\n", ")", "\n", ")", "\n", "\n", "print", "(", "\"Correct Overlap : {}\"", ".", "format", "(", "len", "(", "correct_overlap_qids", ")", ")", ")", "\n", "print", "(", ")", "\n", "\n", "print", "(", "\"Correct in M1; Incorrect in M2: {}\"", ".", "format", "(", "len", "(", "correct1_incorrect2_qids", ")", ")", ")", "\n", "print", "(", ")", "\n", "\n", "print", "(", "\"Incorrect in M1; Correct in M2: {}\"", ".", "format", "(", "len", "(", "incorrect1_correct2_qids", ")", ")", ")", "\n", "print", "(", ")", "\n", "\n", "\n", "if", "analysis_output_dir", "is", "None", ":", "\n", "        ", "return", "\n", "\n", "", "print", "(", "\"\\nWriting analysis to files in {}\"", ".", "format", "(", "analysis_output_dir", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "analysis_output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "analysis_output_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "analysis_output_dir", ",", "\"correct_ours_incorrect_base.txt\"", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "for", "qid", "in", "correct1_incorrect2_qids", ":", "\n", "            ", "f", ".", "write", "(", "\"{} \\t {}\\n\"", ".", "format", "(", "qid", ",", "qid2ques", "[", "qid", "]", ")", ")", "\n", "\n", "", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "analysis_output_dir", ",", "\"incorrect_ours_correct_base.txt\"", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "for", "qid", "in", "incorrect1_correct2_qids", ":", "\n", "            ", "f", ".", "write", "(", "\"{} \\t {}\\n\"", ".", "format", "(", "qid", ",", "qid2ques", "[", "qid", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.compare_MTMSN_Ours.mtmsn_get_first_prediction": [[181, 192], ["mtmsn_preds.items"], "function", ["None"], ["", "", "", "def", "mtmsn_get_first_prediction", "(", "mtmsn_preds", ")", ":", "\n", "    ", "\"\"\"MTMSN has the ability to predict multiple spans as the answer. This in the predictions is stored as\n    {qid: List of dicts} with each dictionary having the key \"text\" as the predicted span.\n\n    We store \"f1\" and \"em\" in each dictionary which is computed based on all predicted spans.\n    This function only keeps the first dictionary for analysis and converts the mtmsn_preds into a {qid: Dict} map.\n    \"\"\"", "\n", "mtmsn_preds_onlyfirst", "=", "{", "}", "\n", "for", "qid", ",", "preds", "in", "mtmsn_preds", ".", "items", "(", ")", ":", "\n", "        ", "mtmsn_preds_onlyfirst", "[", "qid", "]", "=", "preds", "[", "0", "]", "\n", "", "return", "mtmsn_preds_onlyfirst", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.compare_MTMSN_Ours.convert_ourlist_to_dict": [[195, 203], ["print"], "function", ["None"], ["", "def", "convert_ourlist_to_dict", "(", "data", ":", "List", "[", "Dict", "]", ")", "->", "Dict", ":", "\n", "    ", "output_dict", "=", "{", "}", "\n", "for", "d", "in", "data", ":", "\n", "        ", "query_id", "=", "d", "[", "\"query_id\"", "]", "\n", "if", "query_id", "in", "output_dict", ":", "\n", "            ", "print", "(", "\"Duplicate : {}\"", ".", "format", "(", "query_id", ")", ")", "\n", "", "output_dict", "[", "query_id", "]", "=", "d", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.compare_MTMSN_Ours.read_json": [[205, 209], ["open", "json.load"], "function", ["None"], ["", "def", "read_json", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "data", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.compare_MTMSN_Ours.read_jsonl": [[211, 217], ["open", "data.append", "json.loads"], "function", ["None"], ["", "def", "read_jsonl", "(", "input_jsonl", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "with", "open", "(", "input_jsonl", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "data", ".", "append", "(", "json", ".", "loads", "(", "line", ")", ")", "\n", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.read_prediction_file": [[6, 14], ["open", "f.readlines", "line.strip().split", "line.strip"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.readlines"], ["def", "read_prediction_file", "(", "file_path", ")", ":", "\n", "    ", "\"\"\"Input files are instance-per-line w/ each line being tab-separated.\"\"\"", "\n", "with", "open", "(", "file_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "\n", "", "instances", "=", "[", "line", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "for", "line", "in", "lines", "]", "\n", "\n", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.make_correct_incorrect_dicts": [[16, 36], ["set", "set", "set.add", "set.add"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add"], ["", "def", "make_correct_incorrect_dicts", "(", "instances", ")", ":", "\n", "    ", "\"\"\"Make separate sets of question ids for correctly and in-correctly answered questions.\n\n    Args:\n        instances: is a 3(4)-tuple with (q-id, ques-text, 'C'/'NC', logical_form (optional) )\n\n    Returns:\n        correct_qids: Set of qids answered correctly\n        incorrect_qids: Set of qids answered incorrectly\n    \"\"\"", "\n", "correct_qids", "=", "set", "(", ")", "\n", "incorrect_qids", "=", "set", "(", ")", "\n", "\n", "for", "instance", "in", "instances", ":", "\n", "        ", "if", "instance", "[", "2", "]", "==", "\"C\"", ":", "\n", "            ", "correct_qids", ".", "add", "(", "instance", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "            ", "incorrect_qids", ".", "add", "(", "instance", "[", "0", "]", ")", "\n", "\n", "", "", "return", "correct_qids", ",", "incorrect_qids", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.make_qid2questiondict": [[38, 55], ["None"], "function", ["None"], ["", "def", "make_qid2questiondict", "(", "instances", ")", ":", "\n", "    ", "\"\"\"Make question_id to question text map\n\n    Args:\n        instances: is a 3(4)-tuple with (q-id, ques-text, 'C'/'NC', logical_form (optional) )\n\n    Returns:\n        qid2qtext: Dict from qid 2 qtext\n    \"\"\"", "\n", "qid2qtext", "=", "{", "}", "\n", "\n", "for", "instance", "in", "instances", ":", "\n", "        ", "qid", "=", "instance", "[", "0", "]", "\n", "qtext", "=", "instance", "[", "1", "]", "\n", "qid2qtext", "[", "qid", "]", "=", "qtext", "\n", "\n", "", "return", "qid2qtext", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.make_qid2logicalform": [[57, 77], ["len"], "function", ["None"], ["", "def", "make_qid2logicalform", "(", "instances", ")", ":", "\n", "    ", "\"\"\"Make question_id to predicted logical_form map\n\n    Args:\n        instances: is a 3(4)-tuple with (q-id, ques-text, 'C'/'NC', logical_form (optional) )\n\n    Returns:\n        qid2lf: Dict from qid 2 logical form\n    \"\"\"", "\n", "qid2lf", "=", "{", "}", "\n", "\n", "for", "instance", "in", "instances", ":", "\n", "        ", "qid", "=", "instance", "[", "0", "]", "\n", "if", "len", "(", "instance", ")", "==", "4", ":", "\n", "            ", "lf", "=", "instance", "[", "3", "]", "\n", "", "else", ":", "\n", "            ", "lf", "=", "\"\"", "\n", "", "qid2lf", "[", "qid", "]", "=", "lf", "\n", "\n", "", "return", "qid2lf", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.print_qtexts": [[79, 82], ["enumerate", "print"], "function", ["None"], ["", "def", "print_qtexts", "(", "qids", ",", "qid2qtext", ")", ":", "\n", "    ", "for", "i", ",", "qid", "in", "enumerate", "(", "qids", ")", ":", "\n", "        ", "print", "(", "\"{}: {}\"", ".", "format", "(", "i", ",", "qid2qtext", "[", "qid", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.print_qtext_lf": [[84, 87], ["enumerate", "print"], "function", ["None"], ["", "", "def", "print_qtext_lf", "(", "qids", ",", "qid2qtext", ",", "qid2lf", ")", ":", "\n", "    ", "for", "i", ",", "qid", "in", "enumerate", "(", "qids", ")", ":", "\n", "        ", "print", "(", "\"{}: {} -- {}\"", ".", "format", "(", "i", ",", "qid2qtext", "[", "qid", "]", ",", "qid2lf", "[", "qid", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.print_qtext_bothlfs": [[89, 93], ["enumerate", "print", "print"], "function", ["None"], ["", "", "def", "print_qtext_bothlfs", "(", "qids", ",", "qid2qtext", ",", "qid2lf1", ",", "qid2lf2", ")", ":", "\n", "    ", "for", "i", ",", "qid", "in", "enumerate", "(", "qids", ")", ":", "\n", "        ", "print", "(", "\"qid: {}\"", ".", "format", "(", "qid", ")", ")", "\n", "print", "(", "\"{}: {}\\n{}\\n{}\\n\"", ".", "format", "(", "i", ",", "qid2qtext", "[", "qid", "]", ",", "qid2lf1", "[", "qid", "]", ",", "qid2lf2", "[", "qid", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.diff_in_lfs": [[95, 105], ["enumerate"], "function", ["None"], ["", "", "def", "diff_in_lfs", "(", "qids", ",", "qid2lf1", ",", "qid2lf2", ")", ":", "\n", "    ", "num_diff_lf", "=", "0", "\n", "for", "i", ",", "qid", "in", "enumerate", "(", "qids", ")", ":", "\n", "        ", "lf1", "=", "qid2lf1", "[", "qid", "]", "\n", "lf2", "=", "qid2lf2", "[", "qid", "]", "\n", "\n", "if", "lf1", "!=", "lf2", ":", "\n", "            ", "num_diff_lf", "+=", "1", "\n", "\n", "", "", "return", "num_diff_lf", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.overlap_in_correct_incorrect_questext": [[107, 116], ["set", "set", "set.intersection", "print", "print", "print", "len", "len", "len"], "function", ["None"], ["", "def", "overlap_in_correct_incorrect_questext", "(", "correct_qids", ",", "incorrect_qids", ",", "qid2qtext", ")", ":", "\n", "    ", "correct_qtexts", "=", "set", "(", "[", "qid2qtext", "[", "qid", "]", "for", "qid", "in", "correct_qids", "]", ")", "\n", "incorrect_qtexts", "=", "set", "(", "[", "qid2qtext", "[", "qid", "]", "for", "qid", "in", "incorrect_qids", "]", ")", "\n", "\n", "overlap", "=", "correct_qtexts", ".", "intersection", "(", "incorrect_qtexts", ")", "\n", "\n", "print", "(", "\"Unique correct questions: {}\"", ".", "format", "(", "len", "(", "correct_qtexts", ")", ")", ")", "\n", "print", "(", "\"Unique incorrect questions: {}\"", ".", "format", "(", "len", "(", "incorrect_qtexts", ")", ")", ")", "\n", "print", "(", "\"Unique question overlap: {}\"", ".", "format", "(", "len", "(", "overlap", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.error_overlap_statistics": [[118, 187], ["comparative_error_analysis.read_prediction_file", "comparative_error_analysis.read_prediction_file", "comparative_error_analysis.make_qid2questiondict", "comparative_error_analysis.make_qid2logicalform", "comparative_error_analysis.make_qid2questiondict", "comparative_error_analysis.make_qid2logicalform", "len", "len", "comparative_error_analysis.make_correct_incorrect_dicts", "comparative_error_analysis.make_correct_incorrect_dicts", "correct_qids1.intersection", "correct_qids1.intersection", "incorrect_qids1.intersection", "comparative_error_analysis.diff_in_lfs", "comparative_error_analysis.diff_in_lfs", "print", "comparative_error_analysis.print_qtext_bothlfs", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "comparative_error_analysis.overlap_in_correct_incorrect_questext", "print", "comparative_error_analysis.overlap_in_correct_incorrect_questext", "len", "float", "len", "float", "len", "len", "len", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.read_prediction_file", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.read_prediction_file", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.make_qid2questiondict", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.make_qid2logicalform", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.make_qid2questiondict", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.make_qid2logicalform", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.make_correct_incorrect_dicts", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.make_correct_incorrect_dicts", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.diff_in_lfs", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.diff_in_lfs", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.print_qtext_bothlfs", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.overlap_in_correct_incorrect_questext", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.analysis.comparative_error_analysis.overlap_in_correct_incorrect_questext"], ["", "def", "error_overlap_statistics", "(", "file1", ",", "file2", ")", ":", "\n", "    ", "\"\"\"Perform analysis on predictions of two different models and see the level of overlap in their predictions.\n\n\n    The two files are tab-separated, with the form\n        Question-id    Question-text    'C' or 'NC'     LogicalForm\n    The logical form is only for predictions from our models\n    \"\"\"", "\n", "instances1", "=", "read_prediction_file", "(", "file1", ")", "\n", "instances2", "=", "read_prediction_file", "(", "file2", ")", "\n", "\n", "qid2qtext1", "=", "make_qid2questiondict", "(", "instances1", ")", "\n", "qid2lf1", "=", "make_qid2logicalform", "(", "instances1", ")", "\n", "\n", "qid2qtext2", "=", "make_qid2questiondict", "(", "instances2", ")", "\n", "qid2lf2", "=", "make_qid2logicalform", "(", "instances2", ")", "\n", "\n", "num_instances1", "=", "len", "(", "instances1", ")", "\n", "num_instances2", "=", "len", "(", "instances2", ")", "\n", "\n", "correct_qids1", ",", "incorrect_qids1", "=", "make_correct_incorrect_dicts", "(", "instances1", ")", "\n", "correct_qids2", ",", "incorrect_qids2", "=", "make_correct_incorrect_dicts", "(", "instances2", ")", "\n", "\n", "perf1", "=", "len", "(", "correct_qids1", ")", "/", "float", "(", "num_instances1", ")", "\n", "perf2", "=", "len", "(", "correct_qids2", ")", "/", "float", "(", "num_instances2", ")", "\n", "\n", "correct_overlap_qids", "=", "correct_qids1", ".", "intersection", "(", "correct_qids2", ")", "\n", "\n", "correct1_incorrect2_qids", "=", "correct_qids1", ".", "intersection", "(", "incorrect_qids2", ")", "\n", "incorrect1_correct2_qids", "=", "incorrect_qids1", ".", "intersection", "(", "correct_qids2", ")", "\n", "\n", "# For questions predicted correct by M1, and incorrect by M2 -- the number of ques with diff LFs", "\n", "c1_nc2_lf_diff", "=", "diff_in_lfs", "(", "correct1_incorrect2_qids", ",", "qid2lf1", ",", "qid2lf2", ")", "\n", "nc1_c2_lf_diff", "=", "diff_in_lfs", "(", "incorrect1_correct2_qids", ",", "qid2lf1", ",", "qid2lf2", ")", "\n", "\n", "# print(\"Correct in Model 1, Incorrect in Model 2\")", "\n", "# print_qtext_bothlfs(correct1_incorrect2_qids, qid2qtext2, qid2lf1, qid2lf2)", "\n", "\n", "print", "(", "\"Incorrect in Model 1, Correct in Model 2\"", ")", "\n", "print_qtext_bothlfs", "(", "incorrect1_correct2_qids", ",", "qid2qtext1", ",", "qid2lf1", ",", "qid2lf2", ")", "\n", "\n", "print", "(", "\"Model 1\"", ")", "\n", "print", "(", "\n", "\"Num instances: {} Correct: {} Incorrect: {} Perf: {}\"", ".", "format", "(", "\n", "num_instances1", ",", "len", "(", "correct_qids1", ")", ",", "len", "(", "incorrect_qids1", ")", ",", "perf1", "\n", ")", "\n", ")", "\n", "print", "(", ")", "\n", "print", "(", "\"Model 2\"", ")", "\n", "print", "(", "\n", "\"Num instances: {} Correct: {} Incorrect: {} Perf: {}\"", ".", "format", "(", "\n", "num_instances2", ",", "len", "(", "correct_qids2", ")", ",", "len", "(", "incorrect_qids2", ")", ",", "perf2", "\n", ")", "\n", ")", "\n", "\n", "print", "(", "\"Correct Overlap : {}\"", ".", "format", "(", "len", "(", "correct_overlap_qids", ")", ")", ")", "\n", "\n", "print", "(", "\"Correct in M1; Incorrect in M2: {}\"", ".", "format", "(", "len", "(", "correct1_incorrect2_qids", ")", ")", ")", "\n", "print", "(", "\"Num of ques w/ different LF predictions: {}\"", ".", "format", "(", "c1_nc2_lf_diff", ")", ")", "\n", "print", "(", ")", "\n", "\n", "print", "(", "\"Incorrect in M1; Correct in M2: {}\"", ".", "format", "(", "len", "(", "incorrect1_correct2_qids", ")", ")", ")", "\n", "print", "(", "\"Num of ques w/ different LF predictions: {}\"", ".", "format", "(", "nc1_c2_lf_diff", ")", ")", "\n", "\n", "print", "(", "\"Model1\"", ")", "\n", "overlap_in_correct_incorrect_questext", "(", "correct_qids1", ",", "incorrect_qids1", ",", "qid2qtext1", ")", "\n", "\n", "print", "(", "\"Model2\"", ")", "\n", "overlap_in_correct_incorrect_questext", "(", "correct_qids2", ",", "incorrect_qids2", ",", "qid2qtext2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.tokenize.split_token_by_delimiter": [[35, 50], ["token.text.split", "split_tokens.append", "len", "split_tokens.pop", "len", "split_tokens.append", "len", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token"], "function", ["None"], ["def", "split_token_by_delimiter", "(", "token", ":", "Token", ",", "delimiter", ":", "str", ")", "->", "List", "[", "Token", "]", ":", "\n", "    ", "split_tokens", "=", "[", "]", "\n", "char_offset", "=", "token", ".", "idx", "\n", "for", "sub_str", "in", "token", ".", "text", ".", "split", "(", "delimiter", ")", ":", "\n", "        ", "if", "sub_str", ":", "\n", "            ", "split_tokens", ".", "append", "(", "Token", "(", "text", "=", "sub_str", ",", "idx", "=", "char_offset", ")", ")", "\n", "char_offset", "+=", "len", "(", "sub_str", ")", "\n", "", "split_tokens", ".", "append", "(", "Token", "(", "text", "=", "delimiter", ",", "idx", "=", "char_offset", ")", ")", "\n", "char_offset", "+=", "len", "(", "delimiter", ")", "\n", "", "if", "split_tokens", ":", "\n", "        ", "split_tokens", ".", "pop", "(", "-", "1", ")", "\n", "char_offset", "-=", "len", "(", "delimiter", ")", "\n", "return", "split_tokens", "\n", "", "else", ":", "\n", "        ", "return", "[", "token", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.tokenize.split_tokens_by_hyphen": [[52, 72], ["any", "new_tokens.append", "tokenize.split_token_by_delimiter", "split_tokens.append"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.split_token_by_delimiter"], ["", "", "def", "split_tokens_by_hyphen", "(", "tokens", ":", "List", "[", "Token", "]", ")", "->", "List", "[", "Token", "]", ":", "\n", "    ", "hyphens", "=", "[", "\"-\"", ",", "\"\u2013\"", ",", "\"~\"", "]", "\n", "new_tokens", ":", "List", "[", "Token", "]", "=", "[", "]", "\n", "\n", "for", "token", "in", "tokens", ":", "\n", "        ", "if", "any", "(", "hyphen", "in", "token", ".", "text", "for", "hyphen", "in", "hyphens", ")", ":", "\n", "            ", "unsplit_tokens", "=", "[", "token", "]", "\n", "split_tokens", ":", "List", "[", "Token", "]", "=", "[", "]", "\n", "for", "hyphen", "in", "hyphens", ":", "\n", "                ", "for", "unsplit_token", "in", "unsplit_tokens", ":", "\n", "                    ", "if", "hyphen", "in", "token", ".", "text", ":", "\n", "                        ", "split_tokens", "+=", "split_token_by_delimiter", "(", "unsplit_token", ",", "hyphen", ")", "\n", "", "else", ":", "\n", "                        ", "split_tokens", ".", "append", "(", "unsplit_token", ")", "\n", "", "", "unsplit_tokens", ",", "split_tokens", "=", "split_tokens", ",", "[", "]", "\n", "", "new_tokens", "+=", "unsplit_tokens", "\n", "", "else", ":", "\n", "            ", "new_tokens", ".", "append", "(", "token", ")", "\n", "\n", "", "", "return", "new_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.tokenize.grouper": [[74, 80], ["range", "len"], "function", ["None"], ["", "def", "grouper", "(", "n", ",", "iterable", ",", "padvalue", "=", "None", ")", ":", "\n", "    ", "\"\"\"grouper(3, 'abcdefg', 'x') -->\n\t('a','b','c'), ('d','e','f'), ('g','x','x')\"\"\"", "\n", "\n", "chunk_size", "=", "n", "\n", "return", "[", "iterable", "[", "i", ":", "i", "+", "chunk_size", "]", "for", "i", "in", "range", "(", "0", ",", "len", "(", "iterable", ")", ",", "chunk_size", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.tokenize._check_validity_of_spans": [[82, 87], ["None"], "function", ["None"], ["", "def", "_check_validity_of_spans", "(", "spans", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ",", "len_seq", ":", "int", ")", ":", "\n", "    ", "\"\"\"All spans are inclusive start and end\"\"\"", "\n", "for", "span", "in", "spans", ":", "\n", "        ", "assert", "span", "[", "0", "]", ">=", "0", "\n", "assert", "span", "[", "1", "]", "<", "len_seq", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.tokenize.find_valid_spans": [[89, 130], ["collections.defaultdict", "enumerate", "token.lower().strip", "word_positions[].append", "answer_text.split", "len", "token.lower().strip", "token.lower", "spans.append", "token.lower", "len", "answer_tokens[].strip"], "function", ["None"], ["", "", "def", "find_valid_spans", "(", "passage_tokens", ":", "List", "[", "str", "]", ",", "answer_texts", ":", "List", "[", "str", "]", ")", "->", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ":", "\n", "\n", "# debug = False", "\n", "# if 'T. J. Houshmandzadeh' in answer_texts:", "\n", "#     debug = True", "\n", "    ", "normalized_tokens", "=", "[", "token", ".", "lower", "(", ")", ".", "strip", "(", "STRIPPED_CHARACTERS", ")", "for", "token", "in", "passage_tokens", "]", "\n", "# if debug:", "\n", "#     print('\\n')", "\n", "#     print(normalized_tokens)", "\n", "#     print()", "\n", "\n", "word_positions", ":", "Dict", "[", "str", ",", "List", "[", "int", "]", "]", "=", "defaultdict", "(", "list", ")", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "normalized_tokens", ")", ":", "\n", "        ", "word_positions", "[", "token", "]", ".", "append", "(", "i", ")", "\n", "", "spans", "=", "[", "]", "\n", "for", "answer_text", "in", "answer_texts", ":", "\n", "# answer_tokens = answer_text.lower().strip(STRIPPED_CHARACTERS).split()", "\n", "        ", "answer_text_tokens", "=", "answer_text", ".", "split", "(", ")", "\n", "answer_tokens", "=", "[", "token", ".", "lower", "(", ")", ".", "strip", "(", "STRIPPED_CHARACTERS", ")", "for", "token", "in", "answer_text_tokens", "]", "\n", "# if debug:", "\n", "#     print(answer_tokens)", "\n", "\n", "num_answer_tokens", "=", "len", "(", "answer_tokens", ")", "\n", "if", "answer_tokens", "[", "0", "]", "not", "in", "word_positions", ":", "\n", "            ", "continue", "\n", "\n", "", "for", "span_start", "in", "word_positions", "[", "answer_tokens", "[", "0", "]", "]", ":", "\n", "            ", "span_end", "=", "span_start", "# span_end is _inclusive_", "\n", "answer_index", "=", "1", "\n", "while", "answer_index", "<", "num_answer_tokens", "and", "span_end", "+", "1", "<", "len", "(", "normalized_tokens", ")", ":", "\n", "                ", "token", "=", "normalized_tokens", "[", "span_end", "+", "1", "]", "\n", "if", "answer_tokens", "[", "answer_index", "]", ".", "strip", "(", "STRIPPED_CHARACTERS", ")", "==", "token", ":", "\n", "                    ", "answer_index", "+=", "1", "\n", "span_end", "+=", "1", "\n", "", "elif", "token", "in", "IGNORED_TOKENS", ":", "\n", "                    ", "span_end", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "break", "\n", "", "", "if", "num_answer_tokens", "==", "answer_index", ":", "\n", "                ", "spans", ".", "append", "(", "(", "span_start", ",", "span_end", ")", ")", "\n", "", "", "", "return", "spans", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.tokenize.convert_answer": [[132, 159], ["any", "answer_annotation[].values"], "function", ["None"], ["", "def", "convert_answer", "(", "answer_annotation", ":", "Dict", "[", "str", ",", "Union", "[", "str", ",", "Dict", ",", "List", "]", "]", ")", "->", "Tuple", "[", "str", ",", "List", "]", ":", "\n", "    ", "answer_type", "=", "None", "\n", "if", "answer_annotation", "[", "\"spans\"", "]", ":", "\n", "        ", "answer_type", "=", "\"spans\"", "\n", "", "elif", "answer_annotation", "[", "\"number\"", "]", ":", "\n", "        ", "answer_type", "=", "\"number\"", "\n", "", "elif", "any", "(", "answer_annotation", "[", "\"date\"", "]", ".", "values", "(", ")", ")", ":", "\n", "        ", "answer_type", "=", "\"date\"", "\n", "\n", "", "answer_content", "=", "answer_annotation", "[", "answer_type", "]", "if", "answer_type", "is", "not", "None", "else", "None", "\n", "\n", "answer_texts", "=", "[", "]", "\n", "if", "answer_type", "is", "None", ":", "# No answer", "\n", "        ", "return", "None", "\n", "", "elif", "answer_type", "==", "\"spans\"", ":", "\n", "# answer_content is a list of string in this case", "\n", "        ", "answer_texts", "=", "answer_content", "\n", "", "elif", "answer_type", "==", "\"date\"", ":", "\n", "# answer_content is a dict with \"month\", \"day\", \"year\" as the keys", "\n", "        ", "date_tokens", "=", "[", "\n", "answer_content", "[", "key", "]", "for", "key", "in", "[", "\"month\"", ",", "\"day\"", ",", "\"year\"", "]", "if", "key", "in", "answer_content", "and", "answer_content", "[", "key", "]", "\n", "]", "\n", "answer_texts", "=", "date_tokens", "\n", "", "elif", "answer_type", "==", "\"number\"", ":", "\n", "# answer_content is a string of number", "\n", "        ", "answer_texts", "=", "[", "answer_content", "]", "\n", "", "return", "answer_type", ",", "answer_texts", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.tokenize.processPassage": [[161, 302], ["passage_info[].strip", "unicodedata.normalize", "utils.util.pruneMultipleSpaces", "utils.spacyutils.getSpacyDoc", "tokenize.split_tokens_by_hyphen", "utils.spacyutils.getSpacyDoc", "sorted", "utils.spacyutils.getNER", "datasets.drop.preprocess.ner_process.parseDateNERS", "tokenize._check_validity_of_spans", "datasets.drop.preprocess.ner_process.parseNumNERS", "len", "len", "len", "len", "qa[].strip", "unicodedata.normalize", "utils.util.pruneMultipleSpaces", "utils.spacyutils.getSpacyDoc", "tokenize.split_tokens_by_hyphen", "utils.spacyutils.getSpacyDoc", "utils.spacyutils.getNER", "datasets.drop.preprocess.ner_process.parseDateNERS", "tokenize._check_validity_of_spans", "datasets.drop.preprocess.ner_process.parseNumNERS", "tokenize.convert_answer", "tokenize._check_validity_of_spans", "len", "len", "len", "print", "unicodedata.normalize", "utils.util.pruneMultipleSpaces", "utils.spacyutils.getSpacyDoc", "tokenize.split_tokens_by_hyphen", "tokenized_answer_texts.append", "tokenize.find_valid_spans", "tokenize.find_valid_spans", "len", "len", "print", "print", "print", "print", "print"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.pruneMultipleSpaces", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getSpacyDoc", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.split_tokens_by_hyphen", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getSpacyDoc", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getNER", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process.parseDateNERS", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.tokenize._check_validity_of_spans", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process.parseNumNERS", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.pruneMultipleSpaces", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getSpacyDoc", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.split_tokens_by_hyphen", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getSpacyDoc", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getNER", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process.parseDateNERS", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.tokenize._check_validity_of_spans", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process.parseNumNERS", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.convert_answer", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.tokenize._check_validity_of_spans", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.pruneMultipleSpaces", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getSpacyDoc", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.split_tokens_by_hyphen", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.find_valid_spans", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.find_valid_spans"], ["", "def", "processPassage", "(", "input_args", ")", ":", "\n", "    ", "\"\"\" Helper function for multiprocessing. See tokenizeDocs for details. \"\"\"", "\n", "\n", "passage_id", ",", "passage_info", "=", "input_args", "\n", "\n", "passage_text", ":", "str", "=", "passage_info", "[", "constants", ".", "passage", "]", ".", "strip", "(", ")", "\n", "cleaned_passage_text", "=", "unicodedata", ".", "normalize", "(", "\"NFKD\"", ",", "passage_text", ")", "\n", "cleaned_passage_text", "=", "util", ".", "pruneMultipleSpaces", "(", "cleaned_passage_text", ")", "\n", "passage_spacydoc", "=", "spacyutils", ".", "getSpacyDoc", "(", "cleaned_passage_text", ",", "spacy_nlp", ")", "\n", "passage_tokens", "=", "[", "t", "for", "t", "in", "passage_spacydoc", "]", "\n", "passage_tokens", ":", "List", "[", "Token", "]", "=", "split_tokens_by_hyphen", "(", "passage_tokens", ")", "\n", "\n", "passage_token_charidxs", "=", "[", "token", ".", "idx", "for", "token", "in", "passage_tokens", "]", "\n", "passage_token_texts", ":", "List", "[", "str", "]", "=", "[", "t", ".", "text", "for", "t", "in", "passage_tokens", "]", "\n", "\n", "# Adding tokenized passage, and", "\n", "passage_info", "[", "constants", ".", "cleaned_passage", "]", "=", "cleaned_passage_text", "\n", "passage_info", "[", "constants", ".", "tokenized_passage", "]", "=", "\" \"", ".", "join", "(", "passage_token_texts", ")", "\n", "passage_info", "[", "constants", ".", "passage_charidxs", "]", "=", "passage_token_charidxs", "\n", "\n", "# Remaking the doc for running NER on new tokenization", "\n", "new_passage_doc", "=", "spacyutils", ".", "getSpacyDoc", "(", "\" \"", ".", "join", "(", "passage_token_texts", ")", ",", "spacy_whitespacetokenizer", ")", "\n", "\n", "assert", "len", "(", "passage_tokens", ")", "==", "len", "(", "\" \"", ".", "join", "(", "passage_token_texts", ")", ".", "split", "(", "\" \"", ")", ")", "\n", "assert", "len", "(", "new_passage_doc", ")", "==", "len", "(", "passage_tokens", ")", "\n", "\n", "# List[Tuple[int, int]] -- start (inclusive) and end (exclusive) token idxs for sentence boundaries", "\n", "sentence_idxs", "=", "sorted", "(", "[", "(", "sentence", ".", "start", ",", "sentence", ".", "end", ")", "for", "sentence", "in", "new_passage_doc", ".", "sents", "]", ",", "\n", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "passage_info", "[", "constants", ".", "passage_sent_idxs", "]", "=", "sentence_idxs", "\n", "\n", "passage_ners", "=", "spacyutils", ".", "getNER", "(", "new_passage_doc", ")", "\n", "\n", "(", "parsed_dates", ",", "normalized_date_idxs", ",", "normalized_date_values", ",", "num_date_entities", ")", "=", "ner_process", ".", "parseDateNERS", "(", "\n", "passage_ners", ",", "passage_token_texts", "\n", ")", "\n", "_check_validity_of_spans", "(", "spans", "=", "[", "(", "s", ",", "e", ")", "for", "_", ",", "(", "s", ",", "e", ")", ",", "_", "in", "parsed_dates", "]", ",", "len_seq", "=", "len", "(", "passage_tokens", ")", ")", "\n", "(", "parsed_nums", ",", "normalized_num_idxs", ",", "normalized_number_values", ",", "num_num_entities", ")", "=", "ner_process", ".", "parseNumNERS", "(", "\n", "passage_ners", ",", "passage_token_texts", "\n", ")", "\n", "\n", "# Adding NER Value", "\n", "passage_info", "[", "constants", ".", "passage_date_mens", "]", "=", "parsed_dates", "\n", "passage_info", "[", "constants", ".", "passage_date_entidx", "]", "=", "normalized_date_idxs", "\n", "passage_info", "[", "constants", ".", "passage_date_normalized_values", "]", "=", "normalized_date_values", "\n", "\n", "passage_info", "[", "constants", ".", "passage_num_mens", "]", "=", "parsed_nums", "\n", "passage_info", "[", "constants", ".", "passage_num_entidx", "]", "=", "normalized_num_idxs", "\n", "passage_info", "[", "constants", ".", "passage_num_normalized_values", "]", "=", "normalized_number_values", "\n", "\n", "# Maybe add whitespace info later", "\n", "qa_pairs", ":", "List", "[", "Dict", "]", "=", "passage_info", "[", "constants", ".", "qa_pairs", "]", "\n", "for", "qa", "in", "qa_pairs", ":", "\n", "        ", "question", ":", "str", "=", "qa", "[", "constants", ".", "question", "]", ".", "strip", "(", ")", "\n", "cleaned_question", "=", "unicodedata", ".", "normalize", "(", "\"NFKD\"", ",", "question", ")", "\n", "cleaned_question", "=", "util", ".", "pruneMultipleSpaces", "(", "cleaned_question", ")", "\n", "\n", "q_spacydoc", "=", "spacyutils", ".", "getSpacyDoc", "(", "cleaned_question", ",", "spacy_nlp", ")", "\n", "question_tokens", "=", "[", "t", "for", "t", "in", "q_spacydoc", "]", "\n", "question_tokens", "=", "split_tokens_by_hyphen", "(", "question_tokens", ")", "\n", "question_token_charidxs", "=", "[", "token", ".", "idx", "for", "token", "in", "question_tokens", "]", "\n", "question_token_texts", "=", "[", "t", ".", "text", "for", "t", "in", "question_tokens", "]", "\n", "\n", "qa", "[", "constants", ".", "cleaned_question", "]", "=", "cleaned_question", "\n", "qa", "[", "constants", ".", "tokenized_question", "]", "=", "\" \"", ".", "join", "(", "question_token_texts", ")", "\n", "# new_qa[constants.original_question] = original_question", "\n", "qa", "[", "constants", ".", "question_charidxs", "]", "=", "question_token_charidxs", "\n", "\n", "# Remaking the doc for running NER on new tokenization", "\n", "new_question_doc", "=", "spacyutils", ".", "getSpacyDoc", "(", "\" \"", ".", "join", "(", "question_token_texts", ")", ",", "spacy_whitespacetokenizer", ")", "\n", "assert", "len", "(", "new_question_doc", ")", "==", "len", "(", "question_tokens", ")", "\n", "\n", "q_ners", "=", "spacyutils", ".", "getNER", "(", "new_question_doc", ")", "\n", "(", "parsed_dates", ",", "normalized_date_idxs", ",", "normalized_date_values", ",", "num_date_entities", ")", "=", "ner_process", ".", "parseDateNERS", "(", "\n", "q_ners", ",", "question_token_texts", "\n", ")", "\n", "_check_validity_of_spans", "(", "spans", "=", "[", "(", "s", ",", "e", ")", "for", "_", ",", "(", "s", ",", "e", ")", ",", "_", "in", "parsed_dates", "]", ",", "len_seq", "=", "len", "(", "question_tokens", ")", ")", "\n", "(", "parsed_nums", ",", "normalized_num_idxs", ",", "normalized_number_values", ",", "num_num_entities", ")", "=", "ner_process", ".", "parseNumNERS", "(", "\n", "q_ners", ",", "question_token_texts", "\n", ")", "\n", "\n", "qa", "[", "constants", ".", "q_date_mens", "]", "=", "parsed_dates", "\n", "qa", "[", "constants", ".", "q_date_entidx", "]", "=", "normalized_date_idxs", "\n", "qa", "[", "constants", ".", "q_date_normalized_values", "]", "=", "normalized_date_values", "\n", "\n", "qa", "[", "constants", ".", "q_num_mens", "]", "=", "parsed_nums", "\n", "qa", "[", "constants", ".", "q_date_mens", "]", "=", "normalized_num_idxs", "\n", "qa", "[", "constants", ".", "q_num_normalized_values", "]", "=", "normalized_number_values", "\n", "\n", "answer", "=", "qa", "[", "constants", ".", "answer", "]", "\n", "\n", "# Answer type and list of answer-texts", "\n", "answer_content", ":", "Tuple", "[", "str", ",", "List", "]", "=", "convert_answer", "(", "answer", ")", "\n", "if", "answer_content", "is", "None", ":", "\n", "# Some qa don't have any answer annotated", "\n", "            ", "print", "(", "f\"Couldn't resolve answer: {answer}\"", ")", "\n", "continue", "\n", "\n", "", "answer_type", ",", "answer_texts", "=", "answer_content", "\n", "\n", "# answer_texts_for_evaluation = [' '.join(answer_texts)]", "\n", "tokenized_answer_texts", "=", "[", "]", "\n", "for", "answer_text", "in", "answer_texts", ":", "\n", "            ", "answer_text", "=", "unicodedata", ".", "normalize", "(", "\"NFKD\"", ",", "answer_text", ")", "\n", "answer_text", "=", "util", ".", "pruneMultipleSpaces", "(", "answer_text", ")", "\n", "answer_spacydoc", "=", "spacyutils", ".", "getSpacyDoc", "(", "answer_text", ",", "spacy_nlp", ")", "\n", "answer_tokens", "=", "[", "t", "for", "t", "in", "answer_spacydoc", "]", "\n", "answer_tokens", "=", "split_tokens_by_hyphen", "(", "answer_tokens", ")", "\n", "answer_token_texts", "=", "[", "t", ".", "text", "for", "t", "in", "answer_tokens", "]", "\n", "tokenized_answer_texts", ".", "append", "(", "\" \"", ".", "join", "(", "answer_token_texts", ")", ")", "\n", "\n", "", "valid_passage_spans", "=", "(", "\n", "find_valid_spans", "(", "passage_token_texts", ",", "tokenized_answer_texts", ")", "if", "tokenized_answer_texts", "else", "[", "]", "\n", ")", "\n", "valid_question_spans", "=", "(", "\n", "find_valid_spans", "(", "question_token_texts", ",", "tokenized_answer_texts", ")", "if", "tokenized_answer_texts", "else", "[", "]", "\n", ")", "\n", "\n", "_check_validity_of_spans", "(", "valid_question_spans", ",", "len", "(", "question_tokens", ")", ")", "\n", "\n", "# Adding question/passage spans as answers", "\n", "qa", "[", "constants", ".", "answer_passage_spans", "]", "=", "valid_passage_spans", "\n", "qa", "[", "constants", ".", "answer_question_spans", "]", "=", "valid_question_spans", "\n", "\n", "# ANSWER TYPE", "\n", "if", "answer_type", "==", "\"spans\"", ":", "\n", "            ", "if", "valid_passage_spans", "or", "valid_question_spans", ":", "\n", "                ", "qa", "[", "constants", ".", "answer_type", "]", "=", "constants", ".", "SPAN_TYPE", "\n", "", "else", ":", "\n", "                ", "print", "(", "f\"Answer span not found in passage. passageid: {passage_id}. queryid: {qa[constants.query_id]}\"", ")", "\n", "print", "(", "f\"Answer Texts: {answer_texts}\"", ")", "\n", "print", "(", "f\"Tokenized_Answer Texts: {tokenized_answer_texts}\"", ")", "\n", "print", "(", "passage_info", "[", "constants", ".", "tokenized_passage", "]", ")", "\n", "qa", "[", "constants", ".", "answer_type", "]", "=", "constants", ".", "UNK_TYPE", "\n", "print", "(", ")", "\n", "", "", "elif", "answer_type", "==", "\"number\"", ":", "\n", "            ", "qa", "[", "constants", ".", "answer_type", "]", "=", "constants", ".", "NUM_TYPE", "\n", "", "elif", "answer_type", "==", "\"date\"", ":", "\n", "            ", "qa", "[", "constants", ".", "answer_type", "]", "=", "constants", ".", "DATE_TYPE", "\n", "\n", "", "", "return", "{", "passage_id", ":", "passage_info", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.tokenize.tokenizeDocs": [[304, 369], ["print", "print", "print", "multiprocessing.Pool", "list", "print", "tokenize.grouper", "print", "time.time", "print", "print", "print", "open", "json.load", "json.load.items", "len", "multiprocessing.Pool.map", "print", "open", "json.dump", "len", "output_dict.items", "output_passage_id_info_dict.update", "time.time", "float", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.tokenize.grouper"], ["", "def", "tokenizeDocs", "(", "input_json", ":", "str", ",", "output_json", ":", "str", ",", "nump", ":", "int", ")", "->", "None", ":", "\n", "    ", "\"\"\" Tokenize the question, answer and context in the HotPotQA Json.\n\n    Returns:\n    --------\n    Jsonl file with same datatypes as input with the modification/addition of:\n    Modifications:\n        q_field: The question is tokenized\n        context_field: Context sentences are now tokenized, but stored with white-space delimition\n\n    Additions:\n        ans_tokenized_field: tokenized answer if needed\n        q_ner_field: NER tags for question. Each NER tag is (spantext, start, end, label) with exclusive-end.\n        ans_ner_field: NER tags in answer\n        context_ner_field: NER tags in each of the context sentences\n    \"\"\"", "\n", "\n", "print", "(", "\"Reading input jsonl: {}\"", ".", "format", "(", "input_json", ")", ")", "\n", "print", "(", "\"Output filepath: {}\"", ".", "format", "(", "output_json", ")", ")", "\n", "\n", "# Input file contains single json obj with list of questions as jsonobjs inside it", "\n", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "print", "(", "\"Number of docs: {}\"", ".", "format", "(", "len", "(", "dataset", ")", ")", ")", "\n", "numdocswritten", "=", "0", "\n", "\n", "process_pool", "=", "multiprocessing", ".", "Pool", "(", "nump", ")", "\n", "\n", "num_input_qas", "=", "0", "\n", "\n", "# List of tuples with (passage_id, passage_info)", "\n", "passage_id_infos", "=", "list", "(", "dataset", ".", "items", "(", ")", ")", "\n", "for", "(", "_", ",", "pinfo", ")", "in", "passage_id_infos", ":", "\n", "        ", "num_input_qas", "+=", "len", "(", "pinfo", "[", "constants", ".", "qa_pairs", "]", ")", "\n", "\n", "", "print", "(", "\"Making jsonobj chunks\"", ")", "\n", "passage_id_info_chunks", "=", "grouper", "(", "100", ",", "passage_id_infos", ")", "\n", "print", "(", "f\"Number of chunks made: {len(passage_id_info_chunks)}\"", ")", "\n", "\n", "output_passage_id_info_dict", "=", "{", "}", "\n", "group_num", "=", "1", "\n", "\n", "num_qa_parsed", "=", "0", "\n", "\n", "stime", "=", "time", ".", "time", "(", ")", "\n", "for", "chunk", "in", "passage_id_info_chunks", ":", "\n", "# The main function that processes the input jsonobj", "\n", "        ", "result", "=", "process_pool", ".", "map", "(", "processPassage", ",", "chunk", ")", "\n", "for", "output_dict", "in", "result", ":", "\n", "            ", "for", "(", "pid", ",", "pinfo", ")", "in", "output_dict", ".", "items", "(", ")", ":", "\n", "                ", "num_qa_parsed", "+=", "len", "(", "pinfo", "[", "constants", ".", "qa_pairs", "]", ")", "\n", "", "output_passage_id_info_dict", ".", "update", "(", "output_dict", ")", "\n", "\n", "", "ttime", "=", "time", ".", "time", "(", ")", "-", "stime", "\n", "ttime", "=", "float", "(", "ttime", ")", "/", "60.0", "\n", "print", "(", "f\"Groups done: {group_num} in {ttime} mins\"", ")", "\n", "group_num", "+=", "1", "\n", "\n", "", "with", "open", "(", "output_json", ",", "\"w\"", ")", "as", "outf", ":", "\n", "        ", "json", ".", "dump", "(", "output_passage_id_info_dict", ",", "outf", ",", "indent", "=", "4", ")", "\n", "\n", "", "print", "(", "f\"Number of QA pairs input: {num_input_qas}\"", ")", "\n", "print", "(", "f\"Number of QA pairs parsed: {num_qa_parsed}\"", ")", "\n", "print", "(", "f\"Multiprocessing finished. Total elems in output: {len(output_passage_id_info_dict)}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process.parseDateNERS": [[51, 85], ["ner_process.extract_years_from_text", "ner_process.merge_datener_with_yearmentions", "len", "normalized_date_idxs.append", "len", "len", "ner_process.normalizeDATE", "len", "normalized_date_values.append", "merge_datener_with_yearmentions.extend"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process.extract_years_from_text", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process.merge_datener_with_yearmentions", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process.normalizeDATE"], ["def", "parseDateNERS", "(", "ner_spans", ",", "passage_tokens", ":", "List", "[", "str", "]", ")", "->", "Tuple", "[", "List", ",", "List", ",", "List", ",", "int", "]", ":", "\n", "    ", "\"\"\" Returns (List1, List2, int)\n        1. List of (text, (start, end), normalized_value_tuple) tuples (end inclusive)\n        2. List of date_ent_idxs for equivalent dates (same length as 1.)\n        3. List of (normalized_date) values in order of idxs\n        4. Number of date_entities\n    \"\"\"", "\n", "\n", "parsed_dates", ":", "List", "[", "str", ",", "Tuple", ",", "Tuple", "]", "=", "[", "]", "\n", "for", "ner", "in", "ner_spans", ":", "\n", "        ", "if", "ner", "[", "-", "1", "]", "==", "constants", ".", "DATE_TYPE", ":", "\n", "# normalized_dates = List of (text, (start, end), normalized_value_tuple)", "\n", "            ", "normalized_dates", "=", "normalizeDATE", "(", "ner", ",", "dateparser_en", ")", "\n", "if", "normalized_dates", "is", "not", "None", ":", "\n", "                ", "parsed_dates", ".", "extend", "(", "normalized_dates", ")", "\n", "\n", "", "", "", "year_mentions", ":", "List", "[", "str", ",", "Tuple", ",", "Tuple", "]", "=", "extract_years_from_text", "(", "passage_tokens", "=", "passage_tokens", ")", "\n", "\n", "# Mention format -- (span_string, (start_idx, end_idx), normalized_value_tuple)", "\n", "parsed_dates", "=", "merge_datener_with_yearmentions", "(", "parsed_dates", ",", "year_mentions", ")", "\n", "\n", "date2idx", "=", "{", "}", "\n", "normalized_date_idxs", "=", "[", "]", "\n", "normalized_date_values", "=", "[", "]", "\n", "for", "(", "_", ",", "_", ",", "value", ")", "in", "parsed_dates", ":", "\n", "        ", "if", "value", "not", "in", "date2idx", ":", "\n", "            ", "date2idx", "[", "value", "]", "=", "len", "(", "date2idx", ")", "\n", "normalized_date_values", ".", "append", "(", "value", ")", "\n", "", "normalized_date_idxs", ".", "append", "(", "date2idx", "[", "value", "]", ")", "\n", "\n", "", "num_date_entities", "=", "len", "(", "normalized_date_values", ")", "\n", "assert", "len", "(", "parsed_dates", ")", "==", "len", "(", "normalized_date_idxs", ")", "\n", "\n", "return", "(", "parsed_dates", ",", "normalized_date_idxs", ",", "normalized_date_values", ",", "num_date_entities", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process.parseNumNERS": [[87, 127], ["enumerate", "sorted", "len", "ner_process._str2float", "normalized_num_idxs.append", "len", "len", "parsed_nums.append", "len", "normalized_number_values.append", "int", "int"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process._str2float"], ["", "def", "parseNumNERS", "(", "ner_spans", ",", "tokens", ":", "List", "[", "str", "]", ")", "->", "Tuple", "[", "List", ",", "List", ",", "List", ",", "int", "]", ":", "\n", "    ", "\"\"\" Returns (List1, List2, int)\n        1. List of (text, token_idx, normalized_value)\n        2. List of num_ent_idxs for equivalent numbers - same length as 1.\n        3. List of normalized_num values in order of idxs\n        4. Number of number_entities\n    \"\"\"", "\n", "# List of (token_str, token_idx, normalized_value) -- if going by single token version", "\n", "# List of ((menstr, start, end, NUM), normalized_value) - if going by the mention route", "\n", "parsed_nums", "=", "[", "]", "\n", "# for ner in ner_spans:", "\n", "#     if ner[-1] in NUM_NER_TYPES:", "\n", "#         # (token_str, token_idx, normalized_value)", "\n", "#         normalized_num = normalizeNUM(ner, tokens)", "\n", "#         if normalized_num is not None:", "\n", "#             parsed_nums.append(normalized_num)", "\n", "\n", "for", "token_idx", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "        ", "normalized_value", "=", "_str2float", "(", "token", ")", "\n", "if", "normalized_value", "is", "not", "None", ":", "\n", "# The number is int, store as one.", "\n", "            ", "normalized_value", "=", "int", "(", "normalized_value", ")", "if", "int", "(", "normalized_value", ")", "==", "normalized_value", "else", "normalized_value", "\n", "parsed_nums", ".", "append", "(", "(", "token", ",", "token_idx", ",", "normalized_value", ")", ")", "\n", "\n", "# Store the passage number values in a sorted manner -- this makes number computations easier in the model", "\n", "", "", "sorted_parsed_numbers", "=", "sorted", "(", "parsed_nums", ",", "key", "=", "lambda", "x", ":", "x", "[", "2", "]", ")", "\n", "\n", "num2idx", "=", "{", "}", "\n", "normalized_num_idxs", "=", "[", "]", "\n", "normalized_number_values", "=", "[", "]", "\n", "for", "(", "_", ",", "_", ",", "value", ")", "in", "sorted_parsed_numbers", ":", "\n", "        ", "if", "value", "not", "in", "num2idx", ":", "\n", "            ", "num2idx", "[", "value", "]", "=", "len", "(", "num2idx", ")", "\n", "normalized_number_values", ".", "append", "(", "value", ")", "\n", "", "normalized_num_idxs", ".", "append", "(", "num2idx", "[", "value", "]", ")", "\n", "\n", "", "num_number_entities", "=", "len", "(", "normalized_number_values", ")", "\n", "assert", "len", "(", "parsed_nums", ")", "==", "len", "(", "normalized_num_idxs", ")", "\n", "\n", "return", "(", "sorted_parsed_numbers", ",", "normalized_num_idxs", ",", "normalized_number_values", ",", "num_number_entities", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process.merge_datener_with_yearmentions": [[129, 149], ["sorted.extend", "sorted", "year_mentions_to_keep.append"], "function", ["None"], ["", "def", "merge_datener_with_yearmentions", "(", "date_mentions", ",", "year_mentions", ")", ":", "\n", "    ", "\"\"\" Year mentions are single token long. Using an expensive (O(n^2)) process here.\n        All mentions are (text, (start, end(inclusive)), normalized_value)\n    \"\"\"", "\n", "year_mentions_to_keep", "=", "[", "]", "\n", "for", "year_mention", "in", "year_mentions", ":", "\n", "        ", "token_idx", "=", "year_mention", "[", "1", "]", "[", "0", "]", "\n", "keep", "=", "True", "\n", "for", "datemen", "in", "date_mentions", ":", "\n", "            ", "start", ",", "end", "=", "datemen", "[", "1", "]", "[", "0", "]", ",", "datemen", "[", "1", "]", "[", "1", "]", "\n", "if", "token_idx", ">=", "start", "and", "token_idx", "<=", "end", ":", "\n", "                ", "keep", "=", "False", "\n", "", "", "if", "keep", ":", "\n", "            ", "year_mentions_to_keep", ".", "append", "(", "year_mention", ")", "\n", "\n", "", "", "final_mentions", "=", "date_mentions", "\n", "final_mentions", ".", "extend", "(", "year_mentions_to_keep", ")", "\n", "final_mentions", "=", "sorted", "(", "final_mentions", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", "[", "0", "]", ")", "\n", "\n", "return", "final_mentions", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process.extract_years_from_text": [[151, 166], ["enumerate", "len", "int", "year_date_mentions.append"], "function", ["None"], ["", "def", "extract_years_from_text", "(", "passage_tokens", ")", "->", "List", "[", "Tuple", "[", "str", ",", "Tuple", ",", "Tuple", "]", "]", ":", "\n", "    ", "\"\"\" Extract 4 digit and 3 digit year mentions.\n\n        Normalized date value: (day, month, year)\n    \"\"\"", "\n", "year_date_mentions", "=", "[", "]", "\n", "for", "idx", ",", "token", "in", "enumerate", "(", "passage_tokens", ")", ":", "\n", "        ", "if", "len", "(", "token", ")", "==", "4", ":", "# or len(token) == 3:", "\n", "            ", "try", ":", "\n", "                ", "int_token", "=", "int", "(", "token", ")", "\n", "year_date_mentions", ".", "append", "(", "(", "token", ",", "(", "idx", ",", "idx", ")", ",", "(", "-", "1", ",", "-", "1", ",", "int_token", ")", ")", ")", "\n", "", "except", ":", "\n", "                ", "continue", "\n", "\n", "", "", "", "return", "year_date_mentions", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process.normalizeDATE": [[168, 318], ["ner_process.normalizeDATE.parseDate"], "function", ["None"], ["", "def", "normalizeDATE", "(", "date_ner_span", ",", "dateparser_en", ")", ":", "\n", "    ", "def", "parseDate", "(", "date_str", ",", "dateparser_en", ")", ":", "\n", "        ", "if", "date_str", "not", "in", "dateStr2DateObj_cache", ":", "\n", "            ", "date", "=", "dateparser_en", ".", "get_date_data", "(", "date_str", ")", "\n", "dateStr2DateObj_cache", "[", "date_str", "]", "=", "date", "[", "\"date_obj\"", "]", "\n", "# if parse fails, date['date_obj'] is None", "\n", "", "return", "dateStr2DateObj_cache", "[", "date_str", "]", "\n", "\n", "", "\"\"\" Normalize DATE ner span\n\n        If normalized -- return\n        Else: Try the two patterns that contain two dates, but parsed as single mention.\n        For eg. \"July 5, 1942 \u2013 January 11, 2012\"\n\n        Parse such into two dates, and return the two values.\n\n        Normalized date value: (day, month, year)\n    \"\"\"", "\n", "\n", "(", "nertext", ",", "start", ",", "end", ",", "type", ")", "=", "date_ner_span", "\n", "assert", "type", "==", "\"DATE\"", "\n", "\n", "if", "nertext", ".", "lower", "(", ")", "==", "\"today\"", "or", "nertext", ".", "lower", "(", ")", "==", "\"tomorrow\"", "or", "nertext", ".", "lower", "(", ")", "==", "\"yesterday\"", ":", "\n", "        ", "return", "None", "\n", "\n", "", "try", ":", "\n", "# Single date in the span", "\n", "        ", "date", "=", "parseDate", "(", "nertext", ",", "dateparser_en", ")", "\n", "\n", "year", "=", "date", ".", "year", "\n", "month", "=", "date", ".", "month", "\n", "day", "=", "date", ".", "day", "\n", "\n", "# If span is incomplete date", "\n", "# Only Year: 1980 / 2017", "\n", "if", "len", "(", "nertext", ".", "split", "(", "\" \"", ")", ")", "==", "1", ":", "\n", "            ", "if", "len", "(", "nertext", ")", "==", "4", ":", "\n", "                ", "month", "=", "-", "1", "\n", "day", "=", "-", "1", "\n", "", "elif", "nertext", ".", "lower", "in", "MONTHS", ":", "\n", "                ", "day", "=", "-", "1", "\n", "year", "=", "-", "1", "\n", "", "else", ":", "\n", "# These are usually words like \"Monday\", \"year\", etc.", "\n", "                ", "return", "None", "\n", "# Month Year -- January 2012 OR Day Month", "\n", "", "", "elif", "len", "(", "nertext", ".", "split", "(", "\" \"", ")", ")", "==", "2", ":", "\n", "            ", "if", "len", "(", "nertext", ".", "split", "(", "\" \"", ")", "[", "1", "]", ")", "==", "4", "and", "nertext", ".", "split", "(", "\" \"", ")", "[", "1", "]", ".", "lower", "(", ")", "not", "in", "MONTHS", ":", "\n", "                ", "day", "=", "-", "1", "\n", "", "if", "nertext", ".", "split", "(", "\" \"", ")", "[", "1", "]", ".", "lower", "(", ")", "in", "MONTHS", ":", "\n", "                ", "year", "=", "-", "1", "\n", "\n", "", "", "normalized_val", "=", "(", "day", ",", "month", ",", "year", ")", "\n", "\n", "if", "year", "==", "2019", ":", "\n", "            ", "return", "None", "\n", "\n", "", "return", "[", "(", "nertext", ",", "(", "start", ",", "end", "-", "1", ")", ",", "normalized_val", ")", "]", "\n", "\n", "", "except", ":", "\n", "# Additionally try parsing strings of the kind:", "\n", "#   \"July 5, 1942 \u2013 January 11, 2012\"   start:5 end:14", "\n", "#   \"September 14, 2006 to March 18, 2007\" start:5 end:14", "\n", "#   \"14 December 1875 \u2013 1 January 1964\"  start:7 end:14", "\n", "#   \"1899 to 1968\"  start: 21 end: 24", "\n", "\n", "        ", "tokens", "=", "nertext", ".", "split", "(", "\" \"", ")", "\n", "if", "\"to\"", "in", "tokens", "or", "\"-\"", "in", "tokens", ":", "\n", "# Covers first three cases", "\n", "            ", "if", "len", "(", "tokens", ")", "==", "7", ":", "\n", "\n", "                ", "string1", "=", "\" \"", ".", "join", "(", "tokens", "[", "0", ":", "3", "]", ")", "\n", "string2", "=", "\" \"", ".", "join", "(", "tokens", "[", "4", ":", "7", "]", ")", "\n", "\n", "date1", ",", "date2", "=", "None", ",", "None", "\n", "try", ":", "\n", "                    ", "date1", "=", "parseDate", "(", "string1", ",", "dateparser_en", ")", "\n", "date2", "=", "parseDate", "(", "string2", ",", "dateparser_en", ")", "\n", "", "except", ":", "\n", "                    ", "date1", ",", "date2", "=", "None", ",", "None", "\n", "\n", "", "if", "date1", "is", "None", "or", "date2", "is", "None", ":", "\n", "                    ", "return", "None", "\n", "\n", "", "text1", "=", "string1", "\n", "date1", "=", "(", "date1", ".", "day", ",", "date1", ".", "month", ",", "date1", ".", "year", ")", "\n", "text2", "=", "string2", "\n", "date2", "=", "(", "date2", ".", "day", ",", "date2", ".", "month", ",", "date2", ".", "year", ")", "\n", "\n", "# Covers first 2 cases", "\n", "if", "end", "-", "start", "==", "9", ":", "\n", "                    ", "start1", "=", "start", "\n", "end1", "=", "start", "+", "4", "\n", "\n", "start2", "=", "start", "+", "5", "\n", "end2", "=", "end", "\n", "\n", "# Covers 3rd case", "\n", "", "elif", "end", "-", "start", "==", "7", ":", "\n", "                    ", "start1", "=", "start", "\n", "end1", "=", "start", "+", "3", "\n", "\n", "start2", "=", "start", "+", "4", "\n", "end2", "=", "end", "\n", "\n", "", "else", ":", "\n", "                    ", "return", "None", "\n", "\n", "# Covers 4th case", "\n", "", "", "elif", "len", "(", "tokens", ")", "==", "3", ":", "\n", "                ", "string1", "=", "tokens", "[", "0", "]", "\n", "string2", "=", "tokens", "[", "2", "]", "\n", "\n", "date1", ",", "date2", "=", "None", ",", "None", "\n", "try", ":", "\n", "                    ", "date1", "=", "parseDate", "(", "string1", ",", "dateparser_en", ")", "\n", "date2", "=", "parseDate", "(", "string2", ",", "dateparser_en", ")", "\n", "", "except", ":", "\n", "                    ", "date1", ",", "date2", "=", "None", ",", "None", "\n", "\n", "", "if", "date1", "is", "None", "or", "date2", "is", "None", ":", "\n", "                    ", "return", "None", "\n", "\n", "", "text1", "=", "string1", "\n", "start1", "=", "start", "\n", "end1", "=", "start", "+", "1", "\n", "date1", "=", "(", "-", "1", ",", "-", "1", ",", "date1", ".", "year", ")", "\n", "\n", "text2", "=", "string2", "\n", "start2", "=", "start", "+", "2", "\n", "end2", "=", "end", "\n", "date2", "=", "(", "-", "1", ",", "-", "1", ",", "date2", ".", "year", ")", "\n", "", "else", ":", "\n", "                ", "return", "None", "\n", "\n", "# We need inclusive ends now", "\n", "", "new_ner_span_1", "=", "(", "text1", ",", "start1", ",", "end1", "-", "1", ",", "constants", ".", "DATE_TYPE", ")", "\n", "normalized_val_1", "=", "date1", "\n", "\n", "# We need inclusive ends now", "\n", "new_ner_span_2", "=", "(", "text2", ",", "start2", ",", "end2", "-", "1", ",", "constants", ".", "DATE_TYPE", ")", "\n", "normalized_val_2", "=", "date2", "\n", "\n", "if", "normalized_val_1", "[", "-", "1", "]", "==", "2019", "or", "normalized_val_2", "[", "-", "1", "]", "==", "2019", ":", "\n", "                ", "return", "None", "\n", "\n", "", "return", "[", "(", "text1", ",", "(", "start1", ",", "end1", "-", "1", ")", ",", "normalized_val_1", ")", ",", "(", "text2", ",", "(", "start2", ",", "end2", "-", "1", ")", ",", "normalized_val_2", ")", "]", "\n", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process._str2float": [[320, 333], ["string_val.replace", "string_val.replace.lower", "int", "float", "string_val.replace.lower"], "function", ["None"], ["", "", "", "def", "_str2float", "(", "string_val", ")", ":", "\n", "# Remove , for strings like 70,0000", "\n", "    ", "no_comma_string", "=", "string_val", ".", "replace", "(", "\",\"", ",", "\"\"", ")", "\n", "\n", "if", "no_comma_string", ".", "lower", "(", ")", "in", "WORD_NUMBER_MAP", ":", "\n", "        ", "return", "int", "(", "WORD_NUMBER_MAP", "[", "no_comma_string", ".", "lower", "(", ")", "]", ")", "\n", "\n", "", "try", ":", "\n", "        ", "val", "=", "float", "(", "no_comma_string", ")", "\n", "return", "val", "\n", "# If error in float parse, return None", "\n", "", "except", ":", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process.normalizeNUM": [[335, 350], ["enumerate", "ner_process._str2float"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process._str2float"], ["", "", "def", "normalizeNUM", "(", "num_ner", ",", "tokens", ":", "List", "[", "str", "]", ")", ":", "\n", "    ", "\"\"\" This normalized num mention in a way to extract a single token.\n        For given ner mention, try to resolve the tokens from left2right in the mention.\n        If any token resolves, return the (token_str, tokenidx, normalized_value) else NONE\n    \"\"\"", "\n", "# End is exclusive", "\n", "(", "text", ",", "start", ",", "end", ",", "nertype", ")", "=", "num_ner", "\n", "relevant_tokens", "=", "tokens", "[", "start", ":", "end", "]", "\n", "for", "idx", ",", "token", "in", "enumerate", "(", "relevant_tokens", ")", ":", "\n", "        ", "normalized_value", "=", "_str2float", "(", "token", ")", "\n", "if", "normalized_value", "is", "not", "None", ":", "\n", "            ", "return", "(", "token", ",", "start", "+", "idx", ",", "normalized_value", ")", "\n", "\n", "# None of the tokens could be normalized", "\n", "", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process.normalizeCARDINAL": [[365, 391], ["ner_process._str2float", "len", "nertext.split"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process._str2float"], ["", "def", "normalizeCARDINAL", "(", "cardinal_ner_span", ")", ":", "\n", "    ", "\"\"\" Normalize CARDINAL ner span\n\n    CARDINAL: 1, 2-time, three, 40,000, one,\n    Solution: If\n                (1) single token\n                (2) string in one-ten map or parses into float\n                parse into float and assign NUM type\n              else\n                return None\n    \"\"\"", "\n", "\n", "(", "nertext", ",", "start", ",", "end", ",", "type", ")", "=", "cardinal_ner_span", "\n", "assert", "type", "==", "\"CARDINAL\"", "\n", "\n", "# Only parse single token cardinals", "\n", "if", "len", "(", "nertext", ".", "split", "(", "\" \"", ")", ")", ">", "1", ":", "\n", "        ", "return", "None", "\n", "\n", "", "floatval", "=", "_str2float", "(", "nertext", ")", "\n", "\n", "if", "floatval", "is", "None", ":", "\n", "        ", "return", "None", "\n", "", "else", ":", "\n", "        ", "new_ner_span", "=", "(", "nertext", ",", "start", ",", "end", "-", "1", ",", "constants", ".", "NUM_TYPE", ")", "\n", "return", "(", "new_ner_span", ",", "floatval", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process.normalizePERCENT": [[393, 425], ["nertext.lower", "len", "ner_process._str2float", "nertext.split"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process._str2float"], ["", "", "def", "normalizePERCENT", "(", "percent_ner_span", ")", ":", "\n", "    ", "\"\"\" Normalize PERCENT ner span\n\n    PERCENT:  30% / 68.2% / 1%\n    Solution: If\n                (1) single token\n                (2) ending in the % char\n                parse rest into float and assign NUM type\n              else\n                return None\n    \"\"\"", "\n", "\n", "(", "nertext", ",", "start", ",", "end", ",", "type", ")", "=", "percent_ner_span", "\n", "assert", "type", "==", "\"PERCENT\"", "\n", "\n", "# Only parse single token cardinals", "\n", "# Since % is a separate token, resort to space based tokenization", "\n", "if", "len", "(", "nertext", ".", "split", "(", "\" \"", ")", ")", ">", "1", ":", "\n", "        ", "return", "None", "\n", "\n", "", "stringval", "=", "nertext", ".", "lower", "(", ")", "\n", "\n", "if", "stringval", "[", "-", "1", "]", "==", "\"%\"", ":", "\n", "        ", "floatval", "=", "_str2float", "(", "stringval", "[", ":", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "        ", "floatval", "=", "None", "\n", "\n", "", "if", "floatval", "is", "None", ":", "\n", "        ", "return", "None", "\n", "", "else", ":", "\n", "        ", "new_ner_span", "=", "(", "nertext", ",", "start", ",", "end", "-", "1", ",", "constants", ".", "NUM_TYPE", ")", "\n", "return", "(", "new_ner_span", ",", "floatval", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process.normalizeMONEY": [[427, 460], ["nertext.split", "t.lower", "ner_process._str2float"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process._str2float"], ["", "", "def", "normalizeMONEY", "(", "money_ner_span", ")", ":", "\n", "    ", "\"\"\" Normalize MONEY ner span\n\n    PERCENT:  up to $600,000 / 70,000 / 400\n    Solution: If\n                (1) If any token gets parsed to float\n                keep that value as normalization and assign NUM type\n              else\n                return None\n\n    TODO: Need solution for million / billion / trillion\n    \"\"\"", "\n", "\n", "(", "nertext", ",", "start", ",", "end", ",", "type", ")", "=", "money_ner_span", "\n", "\n", "# assert type == \"MONEY\"", "\n", "\n", "ner_text_tokens", "=", "nertext", ".", "split", "(", "\" \"", ")", "\n", "ner_text_tokens", "=", "[", "t", ".", "lower", "(", ")", "for", "t", "in", "ner_text_tokens", "]", "\n", "\n", "floatval", "=", "None", "\n", "for", "token", "in", "ner_text_tokens", ":", "\n", "        ", "if", "token", "[", "0", "]", "==", "\"$\"", ":", "\n", "            ", "token", "=", "token", "[", "1", ":", "]", "\n", "", "floatval", "=", "_str2float", "(", "token", ")", "\n", "if", "floatval", "is", "not", "None", ":", "\n", "            ", "break", "\n", "\n", "", "", "if", "floatval", "is", "None", ":", "\n", "        ", "return", "None", "\n", "", "else", ":", "\n", "        ", "new_ner_span", "=", "(", "nertext", ",", "start", ",", "end", "-", "1", ",", "constants", ".", "NUM_TYPE", ")", "\n", "return", "(", "new_ner_span", ",", "floatval", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process.normalizeQUANTITY": [[462, 475], ["ner_process.normalizeMONEY"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process.normalizeMONEY"], ["", "", "def", "normalizeQUANTITY", "(", "quantity_ner_span", ")", ":", "\n", "    ", "\"\"\" Normalize QUANTITY ner span\n\n    PERCENT:  20 fluid ounces / 600 ml / 140 acres\n    Solution: If\n                (1) If any token gets parsed to float\n                keep that value as normalization and assign NUM type\n              else\n                return None\n\n    TODO: Need solution for million / billion / trillion\n    \"\"\"", "\n", "return", "normalizeMONEY", "(", "quantity_ner_span", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_data_augmentation.readDataset": [[26, 30], ["open", "json.load"], "function", ["None"], ["or_idx", "=", "tokens", ".", "index", "(", "\"or\"", ")", "\n", "# Last token is ? which we don't want to attend to", "\n", "event2", "=", "tokens", "[", "or_idx", "+", "1", ":", "len", "(", "tokens", ")", "-", "1", "]", "\n", "event2_span", "=", "(", "or_idx", "+", "1", ",", "len", "(", "tokens", ")", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_data_augmentation.find_valid_spans": [[32, 73], ["collections.defaultdict", "enumerate", "token.lower().strip", "word_positions[].append", "answer_text.split", "len", "token.lower().strip", "token.lower", "spans.append", "token.lower", "len", "answer_tokens[].strip"], "function", ["None"], ["try", ":", "\n", "        ", "comma_idx", "=", "tokens", ".", "index", "(", "\",\"", ")", "\n", "", "except", ":", "\n", "        ", "comma_idx", "=", "100000", "\n", "", "try", ":", "\n", "        ", "colon_idx", "=", "tokens", ".", "index", "(", "\":\"", ")", "\n", "", "except", ":", "\n", "        ", "colon_idx", "=", "100000", "\n", "\n", "", "try", ":", "\n", "        ", "hyphen_idx", "=", "tokens", ".", "index", "(", "\"-\"", ")", "\n", "", "except", ":", "\n", "        ", "hyphen_idx", "=", "100000", "\n", "\n", "", "split_idx", "=", "min", "(", "comma_idx", ",", "colon_idx", ",", "hyphen_idx", ")", "\n", "\n", "if", "split_idx", "==", "100000", "or", "(", "or_idx", "-", "split_idx", "<=", "1", ")", ":", "\n", "# print(f\"{qstr} first_split:{split_idx} or:{or_idx}\")", "\n", "        ", "if", "\"first\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"first\"", ")", "\n", "", "elif", "\"second\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"second\"", ")", "\n", "", "elif", "\"last\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"last\"", ")", "\n", "", "elif", "\"later\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"later\"", ")", "\n", "", "else", ":", "\n", "            ", "split_idx", "=", "-", "1", "\n", "\n", "", "", "assert", "split_idx", "!=", "-", "1", ",", "f\"{qstr} {split_idx} {or_idx}\"", "\n", "\n", "event1", "=", "tokens", "[", "split_idx", "+", "1", ":", "or_idx", "]", "\n", "event1_span", "=", "(", "split_idx", "+", "1", ",", "or_idx", ")", "\n", "\n", "return", "event1_span", ",", "event2_span", "\n", "\n", "\n", "", "def", "getFlippedQuestions", "(", "dataset", ")", ":", "\n", "    ", "qa", "=", "[", "]", "\n", "for", "passage_id", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "for", "question_answer", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "            ", "question_tokenized_text", "=", "question_answer", "[", "constants", ".", "tokenized_question", "]", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_data_augmentation.get_answer_event_order": [[75, 79], ["set", "set", "len", "len", "event1.intersection", "event2.intersection"], "function", ["None"], ["question_charidxs", "=", "question_answer", "[", "constants", ".", "question_charidxs", "]", "\n", "event_spans", "=", "quesEvents", "(", "question_tokenized_text", ")", "\n", "if", "event_spans", "is", "None", ":", "\n", "                ", "continue", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_data_augmentation.getQuestionComparisonOperator": [[81, 94], ["question.split"], "function", ["None"], ["\n", "tokens", "=", "question_tokenized_text", ".", "split", "(", "\" \"", ")", "\n", "pretext", "=", "tokens", "[", "0", ":", "event1_span", "[", "0", "]", "]", "\n", "event2_text", "=", "tokens", "[", "event2_span", "[", "0", "]", ":", "event2_span", "[", "1", "]", "]", "\n", "mid_text", "=", "tokens", "[", "event1_span", "[", "1", "]", ":", "event2_span", "[", "0", "]", "]", "\n", "event1_text", "=", "tokens", "[", "event1_span", "[", "0", "]", ":", "event1_span", "[", "1", "]", "]", "\n", "end_text", "=", "tokens", "[", "event2_span", "[", "1", "]", ":", "]", "\n", "\n", "new_question_tokens", "=", "pretext", "+", "event2_text", "+", "mid_text", "+", "event1_text", "+", "end_text", "\n", "new_question_text", "=", "\" \"", ".", "join", "(", "new_question_tokens", ")", "\n", "\n", "print", "(", "question_tokenized_text", ")", "\n", "print", "(", "new_question_text", ")", "\n", "print", "(", ")", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_data_augmentation.quesEvents": [[96, 151], ["qstr.split", "qstr.split", "qstr.split.index", "min", "len", "qstr.split.index", "qstr.split.index", "qstr.split.index", "len", "qstr.split.index", "len", "qstr.split.index", "qstr.split.index", "qstr.split.index"], "function", ["None"], ["# qa.append((question_text, span, first_or_last, events))", "\n", "", "", "return", "qa", "\n", "\n", "\n", "", "if", "__name__", "==", "\"__main__\"", ":", "\n", "    ", "tr_qa", "=", "getFlippedQuestions", "(", "train_dateset", ")", "\n", "dev_qa", "=", "getFlippedQuestions", "(", "dev_dateset", ")", "\n", "\n", "tr_size", "=", "len", "(", "tr_qa", ")", "\n", "dev_size", "=", "len", "(", "dev_qa", ")", "\n", "print", "(", "f\"Number of tr: {tr_size}\"", ")", "\n", "print", "(", "f\"Number of dev: {dev_size}\"", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_data_augmentation.getEventOrderSwitchQuestion": [[153, 205], ["date_data_augmentation.quesEvents", "copy.deepcopy", "question_tokenized_text.split"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_comparison_prune.quesEvents"], []], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_data_augmentation.getQuestionOperatorSwitchQA_wo_QSA": [[207, 307], ["copy.deepcopy", "date_data_augmentation.quesEvents", "question_tokenized_text.split", "passage_tokenized_text.split", "date_data_augmentation.get_answer_event_order", "date_data_augmentation.find_valid_spans", "date_data_augmentation.getQuestionComparisonOperator", "answer_text.split", "copy.deepcopy.pop", "random.choice", "random.choice", "new_question_text.replace.replace"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_comparison_prune.quesEvents", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_data_augmentation.get_answer_event_order", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.find_valid_spans", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.getQuestionComparisonOperator"], []], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_data_augmentation.addQuestionAttentionVectors": [[309, 335], ["question_tokenized_text.split", "len", "date_data_augmentation.quesEvents", "range", "range"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_comparison_prune.quesEvents"], []], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_data_augmentation.strongSupervisionFlagAndQType": [[337, 366], ["collections.defaultdict"], "function", ["None"], []], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_data_augmentation.augmentDateComparisonData": [[368, 485], ["collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "dataset.items", "print", "print", "print", "print", "print", "print", "len", "date_data_augmentation.strongSupervisionFlagAndQType", "para_supervision_dist.items", "set", "len", "date_data_augmentation.getQuestionComparisonOperator", "date_data_augmentation.addQuestionAttentionVectors", "new_qa_pairs.append", "date_data_augmentation.getEventOrderSwitchQuestion", "date_data_augmentation.getQuestionOperatorSwitchQA_wo_QSA", "len", "len", "date_data_augmentation.addQuestionAttentionVectors", "date_data_augmentation.getQuestionComparisonOperator", "new_qa_pairs.append", "date_data_augmentation.addQuestionAttentionVectors", "date_data_augmentation.getQuestionComparisonOperator", "new_qa_pairs.append", "date_data_augmentation.getQuestionOperatorSwitchQA_wo_QSA", "date_data_augmentation.addQuestionAttentionVectors", "date_data_augmentation.getQuestionComparisonOperator", "new_qa_pairs.append"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_data_augmentation.strongSupervisionFlagAndQType", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.getQuestionComparisonOperator", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_data_augmentation.addQuestionAttentionVectors", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_data_augmentation.getEventOrderSwitchQuestion", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_data_augmentation.getQuestionOperatorSwitchQA_wo_QSA", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_data_augmentation.addQuestionAttentionVectors", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.getQuestionComparisonOperator", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_data_augmentation.addQuestionAttentionVectors", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.getQuestionComparisonOperator", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_data_augmentation.getQuestionOperatorSwitchQA_wo_QSA", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_data_augmentation.addQuestionAttentionVectors", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.getQuestionComparisonOperator"], []], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_comparison_prune.readDataset": [[30, 34], ["open", "json.load"], "function", ["None"], ["def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_comparison_prune.is_date_comparison": [[36, 42], ["question.lower", "any"], "function", ["None"], ["", "def", "is_date_comparison", "(", "question", ":", "str", ")", ":", "\n", "    ", "question_lower", "=", "question", ".", "lower", "(", ")", "\n", "if", "any", "(", "span", "in", "question_lower", "for", "span", "in", "DATE_COMPARISON_TRIGRAMS", ")", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_comparison_prune.getQuestionComparisonOperator": [[44, 58], ["None"], "function", ["None"], ["", "", "def", "getQuestionComparisonOperator", "(", "question_tokens", ":", "List", "[", "str", "]", ")", "->", "str", ":", "\n", "# Correct if Attn1 is first event", "\n", "    ", "lesser_tokens", "=", "[", "\"first\"", ",", "\"earlier\"", ",", "\"forst\"", ",", "\"firts\"", "]", "\n", "greater_tokens", "=", "[", "\"later\"", ",", "\"last\"", ",", "\"second\"", "]", "\n", "\n", "for", "t", "in", "lesser_tokens", ":", "\n", "        ", "if", "t", "in", "question_tokens", ":", "\n", "            ", "return", "FIRST", "\n", "\n", "", "", "for", "t", "in", "greater_tokens", ":", "\n", "        ", "if", "t", "in", "question_tokens", ":", "\n", "            ", "return", "SECOND", "\n", "\n", "", "", "return", "SECOND", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_comparison_prune.getDateTokenIdxs": [[60, 74], ["zip", "passage_date_tokens.extend", "passage_datetoken_entidxs.extend", "len", "len", "range", "range"], "function", ["None"], ["", "def", "getDateTokenIdxs", "(", "\n", "p_date_mens", ":", "List", "[", "Tuple", "[", "str", ",", "Tuple", "[", "int", ",", "int", "]", ",", "Tuple", "[", "int", ",", "int", ",", "int", "]", "]", "]", ",", "p_date_entidxs", ":", "List", "[", "int", "]", "\n", ")", "->", "Tuple", "[", "List", "[", "int", "]", ",", "List", "[", "int", "]", "]", ":", "\n", "    ", "\"\"\"List of date token idxs, and list of their date-ent-idxs.\"\"\"", "\n", "# List of token idxs that are dates", "\n", "passage_date_tokens", "=", "[", "]", "\n", "passage_datetoken_entidxs", "=", "[", "]", "\n", "for", "date_men", ",", "date_idx", "in", "zip", "(", "p_date_mens", ",", "p_date_entidxs", ")", ":", "\n", "        ", "(", "s", ",", "e", ")", "=", "date_men", "[", "1", "]", "\n", "passage_date_tokens", ".", "extend", "(", "[", "x", "for", "x", "in", "range", "(", "s", ",", "e", "+", "1", ")", "]", ")", "\n", "passage_datetoken_entidxs", ".", "extend", "(", "[", "date_idx", "for", "_", "in", "range", "(", "s", ",", "e", "+", "1", ")", "]", ")", "\n", "\n", "", "assert", "len", "(", "passage_date_tokens", ")", "==", "len", "(", "passage_datetoken_entidxs", ")", "\n", "return", "passage_date_tokens", ",", "passage_datetoken_entidxs", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_comparison_prune.quesEvents": [[76, 123], ["qstr.split", "qstr.split", "qstr.split.index", "min", "len", "qstr.split.index", "qstr.split.index", "qstr.split.index", "len", "qstr.split.index", "qstr.split.index", "qstr.split.index", "qstr.split.index"], "function", ["None"], ["", "def", "quesEvents", "(", "qstr", ")", "->", "Tuple", "[", "Tuple", "[", "int", ",", "int", "]", ",", "Tuple", "[", "int", ",", "int", "]", "]", ":", "\n", "    ", "\"\"\" Returns (start, end) span tuples for event1 and event2 in the question. end is exclusive\"\"\"", "\n", "or_split", "=", "qstr", ".", "split", "(", "\" or \"", ")", "\n", "if", "len", "(", "or_split", ")", "!=", "2", ":", "\n", "        ", "return", "None", "\n", "\n", "", "tokens", "=", "qstr", ".", "split", "(", "\" \"", ")", "\n", "\n", "or_idx", "=", "tokens", ".", "index", "(", "\"or\"", ")", "\n", "# Last token is ? which we don't want to attend to", "\n", "event2_span", "=", "(", "or_idx", "+", "1", ",", "len", "(", "tokens", ")", "-", "1", ")", "\n", "\n", "# Gets first index of the item", "\n", "try", ":", "\n", "        ", "comma_idx", "=", "tokens", ".", "index", "(", "\",\"", ")", "\n", "", "except", ":", "\n", "        ", "comma_idx", "=", "100000", "\n", "", "try", ":", "\n", "        ", "colon_idx", "=", "tokens", ".", "index", "(", "\":\"", ")", "\n", "", "except", ":", "\n", "        ", "colon_idx", "=", "100000", "\n", "\n", "", "try", ":", "\n", "        ", "hyphen_idx", "=", "tokens", ".", "index", "(", "\"-\"", ")", "\n", "", "except", ":", "\n", "        ", "hyphen_idx", "=", "100000", "\n", "\n", "", "split_idx", "=", "min", "(", "comma_idx", ",", "colon_idx", ",", "hyphen_idx", ")", "\n", "\n", "if", "split_idx", "==", "100000", "or", "(", "or_idx", "-", "split_idx", "<=", "1", ")", ":", "\n", "# print(f\"{qstr} first_split:{split_idx} or:{or_idx}\")", "\n", "        ", "if", "\"first\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"first\"", ")", "\n", "", "elif", "\"second\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"second\"", ")", "\n", "", "elif", "\"last\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"last\"", ")", "\n", "", "elif", "\"later\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"later\"", ")", "\n", "", "else", ":", "\n", "            ", "split_idx", "=", "-", "1", "\n", "\n", "", "", "assert", "split_idx", "!=", "-", "1", ",", "f\"{qstr} {split_idx} {or_idx}\"", "\n", "\n", "event1_span", "=", "(", "split_idx", "+", "1", ",", "or_idx", ")", "\n", "\n", "return", "event1_span", ",", "event2_span", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_comparison_prune.find_answer_event": [[125, 130], ["set", "answer_span.split", "set", "set", "len", "len", "event1.intersection", "event2.intersection"], "function", ["None"], ["", "def", "find_answer_event", "(", "answer_span", ":", "str", ",", "event1_tokens", ":", "List", "[", "str", "]", ",", "event2_tokens", ":", "List", "[", "str", "]", ")", "->", "str", ":", "\n", "    ", "ans_tokens", "=", "set", "(", "answer_span", ".", "split", "(", "\" \"", ")", ")", "\n", "event1", ",", "event2", "=", "set", "(", "event1_tokens", ")", ",", "set", "(", "event2_tokens", ")", "\n", "ans_event", "=", "FIRST", "if", "len", "(", "event1", ".", "intersection", "(", "ans_tokens", ")", ")", ">", "len", "(", "event2", ".", "intersection", "(", "ans_tokens", ")", ")", "else", "SECOND", "\n", "return", "ans_event", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_comparison_prune.difference_in_successive_terms": [[132, 137], ["range", "len"], "function", ["None"], ["", "def", "difference_in_successive_terms", "(", "l", ":", "List", "[", "int", "]", ")", ":", "\n", "    ", "sum_of_differences", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "l", ")", "-", "1", ")", ":", "\n", "        ", "sum_of_differences", "+=", "l", "[", "i", "+", "1", "]", "-", "l", "[", "i", "]", "\n", "", "return", "sum_of_differences", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_comparison_prune.matchEventToPassage": [[139, 168], ["enumerate", "len", "range", "t.lower", "date_comparison_prune.difference_in_successive_terms", "passage_token.lower", "relevant_passage_tokenidxs.append", "t.lower", "len"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.difference_in_successive_terms"], ["", "def", "matchEventToPassage", "(", "event_tokens", ":", "List", "[", "str", "]", ",", "passage_tokens", ":", "List", "[", "str", "]", ")", "->", "List", "[", "int", "]", ":", "\n", "    ", "\"\"\" Match a given event's tokens (from ques) to relevant tokens in the passage. \"\"\"", "\n", "important_event_tokens", "=", "[", "t", ".", "lower", "(", ")", "for", "t", "in", "event_tokens", "if", "t", ".", "lower", "(", ")", "not", "in", "STOP_WORDS", "]", "\n", "\n", "# These are (auto) sorted in increasing order -- these imp. event tokens found in the passage", "\n", "relevant_passage_tokenidxs", "=", "[", "]", "\n", "for", "(", "idx", ",", "passage_token", ")", "in", "enumerate", "(", "passage_tokens", ")", ":", "\n", "        ", "if", "passage_token", ".", "lower", "(", ")", "in", "important_event_tokens", ":", "\n", "            ", "relevant_passage_tokenidxs", ".", "append", "(", "idx", ")", "\n", "\n", "# Since event tokens can match spuriously at many locations, the idea is to find a list of important tokens in the", "\n", "# passage that are close to each other, aka tighest span", "\n", "", "", "num_imp_event_tokens", "=", "len", "(", "important_event_tokens", ")", "\n", "best_diff", "=", "100000", "\n", "best_start_point", "=", "0", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "relevant_passage_tokenidxs", ")", "-", "num_imp_event_tokens", "+", "1", ")", ":", "\n", "# Starting at i, taking the next n=num_imp_event_tokens passage tokens", "\n", "        ", "passage_token_span", "=", "relevant_passage_tokenidxs", "[", "i", ":", "i", "+", "num_imp_event_tokens", "]", "\n", "# This is tightness in these tokens", "\n", "sum_of_token_diffs", "=", "difference_in_successive_terms", "(", "passage_token_span", ")", "\n", "# If best tightness, then i is a good starting point", "\n", "if", "sum_of_token_diffs", "<", "best_diff", ":", "\n", "            ", "best_start_point", "=", "i", "\n", "best_diff", "=", "sum_of_token_diffs", "\n", "\n", "# These tokens now are passage-tokens where the event tokens ground", "\n", "", "", "pruned_relevant_passage_tokenidxs", "=", "relevant_passage_tokenidxs", "[", "best_start_point", ":", "best_start_point", "+", "num_imp_event_tokens", "]", "\n", "\n", "return", "pruned_relevant_passage_tokenidxs", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_comparison_prune.dateInNeighborhood": [[170, 203], ["max", "zip", "distance_to_dates.append", "closest_date_entidxs.append", "len", "float", "len", "set", "abs", "sum"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "def", "dateInNeighborhood", "(", "\n", "passage_tokenidxs", ":", "List", "[", "int", "]", ",", "passage_date_tokenidxs", ":", "List", "[", "int", "]", ",", "passage_datetoken_entidxs", ":", "List", "[", "int", "]", ",", "threshold", "=", "20", "\n", ")", ":", "\n", "    ", "\"\"\" Given a list of relevant-passage-tokens, and list of date-tokens in the passage, figure out -\n        if there's a date in the neighborhood of the relevant tokens\n        For each passage-token, first find the min-distance to a date token. Then find the min-amongst that.\n        If this distance crosses a threshold, then a date is not considered in the neighborhood of the passage-tokens\n    \"\"\"", "\n", "\n", "distance_to_dates", "=", "[", "]", "\n", "closest_date_entidxs", "=", "[", "]", "\n", "for", "tokenidx", "in", "passage_tokenidxs", ":", "\n", "        ", "min_distance_to_date", "=", "10000", "\n", "closest_dateidx", "=", "-", "1", "\n", "for", "date_tokenidx", ",", "date_idx", "in", "zip", "(", "passage_date_tokenidxs", ",", "passage_datetoken_entidxs", ")", ":", "\n", "            ", "dis", "=", "abs", "(", "date_tokenidx", "-", "tokenidx", ")", "\n", "if", "dis", "<", "min_distance_to_date", ":", "\n", "                ", "min_distance_to_date", "=", "dis", "\n", "closest_dateidx", "=", "date_idx", "\n", "", "", "distance_to_dates", ".", "append", "(", "min_distance_to_date", ")", "\n", "closest_date_entidxs", ".", "append", "(", "closest_dateidx", ")", "\n", "\n", "", "if", "len", "(", "distance_to_dates", ")", "==", "0", ":", "\n", "        ", "return", "False", ",", "-", "1", "\n", "\n", "", "avg_distance_to_dates", "=", "float", "(", "sum", "(", "distance_to_dates", ")", ")", "/", "len", "(", "distance_to_dates", ")", "\n", "# Mode", "\n", "closest_date_entidx", "=", "max", "(", "set", "(", "closest_date_entidxs", ")", ",", "key", "=", "closest_date_entidxs", ".", "count", ")", "\n", "\n", "if", "avg_distance_to_dates", ">", "threshold", ":", "\n", "        ", "return", "False", ",", "closest_date_entidx", "\n", "", "else", ":", "\n", "        ", "return", "True", ",", "closest_date_entidx", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_comparison_prune.pruneDateQuestions": [[205, 347], ["len", "dataset.items", "len", "print", "print", "print", "passage.split", "date_comparison_prune.getDateTokenIdxs", "question_tokenized_text.split", "date_comparison_prune.quesEvents", "date_comparison_prune.matchEventToPassage", "date_comparison_prune.matchEventToPassage", "date_comparison_prune.dateInNeighborhood", "date_comparison_prune.dateInNeighborhood", "date_comparison_prune.getQuestionComparisonOperator", "date_comparison_prune.find_answer_event", "new_qa_pairs.append", "len", "len", "date_comparison_prune.is_date_comparison", "semqa.domain_languages.drop_language.Date", "semqa.domain_languages.drop_language.Date", "len", "len"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_comparison_prune.getDateTokenIdxs", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_comparison_prune.quesEvents", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.matchEventToPassage", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.matchEventToPassage", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_comparison_prune.dateInNeighborhood", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_comparison_prune.dateInNeighborhood", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.getQuestionComparisonOperator", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.find_answer_event", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.datecomp.date_comparison_prune.is_date_comparison"], ["", "", "def", "pruneDateQuestions", "(", "dataset", ")", ":", "\n", "    ", "\"\"\" Prunes questions where a date is not present near both events in the question.\n        1. Find events in the question\n        2. Find whether a nearby date exists or not; if yes, which date.\n\n        Additionally provides a weak-supervison for which date grounding of the ques-events. We don't keep all date\n        supervision from previous step. Don't trust the annotation if:\n        1. Dates for both events are the same.\n        2. Our date prediction for the events don't match the question's answer. For example, if the questions asks for\n           the earlier event, but according to our annotation the answer-event happened later, then the date annotation\n           must be wrong.\n\n        This annotation is stored as:\n        1. constants.datecomp_ques_event_date_groundings -- a one hot-vector the size of num_passage_dates\n        2. constants.datecomp_ques_event_date_values -- A two-tuple, each containing (day, month, year)\n        In cases where we don't store the date annotation, the grounding vector is left to zeros, and date values to -1.\n        The model should use this fact to figure out a mask in case no annotation is provided\n    \"\"\"", "\n", "\n", "new_dataset", "=", "{", "}", "\n", "total_ques", "=", "0", "\n", "after_pruning_ques", "=", "0", "\n", "num_passages", "=", "len", "(", "dataset", ")", "\n", "\n", "numexamaples_w_dates_annotated", "=", "0", "\n", "\n", "for", "passage_id", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "passage", "=", "passage_info", "[", "constants", ".", "tokenized_passage", "]", "\n", "passage_tokens", ":", "List", "[", "str", "]", "=", "passage", ".", "split", "(", "\" \"", ")", "\n", "p_date_mens", ":", "List", "[", "Tuple", "[", "str", ",", "Tuple", "[", "int", ",", "int", "]", ",", "Tuple", "[", "int", ",", "int", ",", "int", "]", "]", "]", "=", "passage_info", "[", "constants", ".", "passage_date_mens", "]", "\n", "p_date_entidxs", "=", "passage_info", "[", "constants", ".", "passage_date_entidx", "]", "\n", "p_date_values", ":", "List", "[", "Tuple", "]", "=", "passage_info", "[", "constants", ".", "passage_date_normalized_values", "]", "\n", "\n", "passage_date_tokenidxs", ",", "passage_datetoken_entidxs", "=", "getDateTokenIdxs", "(", "p_date_mens", ",", "p_date_entidxs", ")", "\n", "\n", "new_qa_pairs", "=", "[", "]", "\n", "for", "question_answer", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "            ", "total_ques", "+=", "1", "\n", "\n", "question_tokenized_text", "=", "question_answer", "[", "constants", ".", "tokenized_question", "]", "\n", "question_tokens", ":", "List", "[", "str", "]", "=", "question_tokenized_text", ".", "split", "(", "\" \"", ")", "\n", "answer_annotation", "=", "question_answer", "[", "constants", ".", "answer", "]", "\n", "\n", "if", "not", "is_date_comparison", "(", "question_tokenized_text", ")", ":", "\n", "                ", "continue", "\n", "", "if", "question_answer", "[", "constants", ".", "answer_type", "]", "!=", "constants", ".", "SPAN_TYPE", ":", "\n", "                ", "continue", "\n", "# Two (start, end) tuples for the two events mentioned in the question (end exclusive)", "\n", "", "event_spans", "=", "quesEvents", "(", "question_tokenized_text", ")", "\n", "answer_span", ":", "str", "=", "answer_annotation", "[", "\"spans\"", "]", "[", "0", "]", "\n", "\n", "# If events are not found, we cannot find dates etc. therefore add this question", "\n", "if", "event_spans", "is", "None", ":", "\n", "                ", "continue", "\n", "\n", "# For questions with identified events, we'll try to ground dates", "\n", "", "event1_span", ",", "event2_span", "=", "event_spans", "\n", "\n", "event1_tokens", "=", "question_tokens", "[", "event1_span", "[", "0", "]", ":", "event1_span", "[", "1", "]", "]", "\n", "event2_tokens", "=", "question_tokens", "[", "event2_span", "[", "0", "]", ":", "event2_span", "[", "1", "]", "]", "\n", "\n", "# List of tokenidxs in passage that is a rough grounding for event 1/2", "\n", "event1_passage_tokenidxs", ":", "List", "[", "int", "]", "=", "matchEventToPassage", "(", "event1_tokens", ",", "passage_tokens", ")", "\n", "event2_passage_tokenidxs", ":", "List", "[", "int", "]", "=", "matchEventToPassage", "(", "event2_tokens", ",", "passage_tokens", ")", "\n", "\n", "date_near_event1", ",", "event1_date_idx", "=", "dateInNeighborhood", "(", "\n", "event1_passage_tokenidxs", ",", "passage_date_tokenidxs", ",", "passage_datetoken_entidxs", ",", "threshold", "=", "THRESHOLD", "\n", ")", "\n", "date_near_event2", ",", "event2_date_idx", "=", "dateInNeighborhood", "(", "\n", "event2_passage_tokenidxs", ",", "passage_date_tokenidxs", ",", "passage_datetoken_entidxs", ",", "threshold", "=", "THRESHOLD", "\n", ")", "\n", "\n", "if", "not", "date_near_event1", "or", "not", "date_near_event2", ":", "\n", "                ", "continue", "\n", "\n", "", "question_operator", "=", "getQuestionComparisonOperator", "(", "question_tokens", ")", "\n", "answer_event", "=", "find_answer_event", "(", "answer_span", ",", "event1_tokens", ",", "event2_tokens", ")", "\n", "\n", "# First find if the date groundings are relevant, checks:", "\n", "# 1. Date grounding shouldn't be -1", "\n", "# 2. Shouldn't be equal", "\n", "# 3. The date grounding should be consistent with answer annotation", "\n", "keep_dates", "=", "True", "\n", "if", "not", "(", "event1_date_idx", "==", "-", "1", "or", "event2_date_idx", "==", "-", "1", "or", "(", "event1_date_idx", "==", "event2_date_idx", ")", ")", ":", "\n", "                ", "answer_event_date", "=", "event1_date_idx", "if", "answer_event", "==", "FIRST", "else", "event2_date_idx", "\n", "other_event_date", "=", "event2_date_idx", "if", "answer_event", "==", "FIRST", "else", "event1_date_idx", "\n", "\n", "answer_event_date", "=", "Date", "(", "\n", "year", "=", "p_date_values", "[", "answer_event_date", "]", "[", "2", "]", ",", "\n", "month", "=", "p_date_values", "[", "answer_event_date", "]", "[", "1", "]", ",", "\n", "day", "=", "p_date_values", "[", "answer_event_date", "]", "[", "0", "]", ",", "\n", ")", "\n", "other_event_date", "=", "Date", "(", "\n", "year", "=", "p_date_values", "[", "other_event_date", "]", "[", "2", "]", ",", "\n", "month", "=", "p_date_values", "[", "other_event_date", "]", "[", "1", "]", ",", "\n", "day", "=", "p_date_values", "[", "other_event_date", "]", "[", "0", "]", ",", "\n", ")", "\n", "\n", "# If the questions asks for what happened first, and our date grounding says that answer happened later,", "\n", "# means our date annotation is wrong.", "\n", "if", "question_operator", "==", "FIRST", ":", "\n", "                    ", "if", "answer_event_date", ">", "other_event_date", ":", "\n", "                        ", "keep_dates", "=", "False", "\n", "", "", "if", "question_operator", "==", "SECOND", ":", "\n", "                    ", "if", "answer_event_date", "<", "other_event_date", ":", "\n", "                        ", "keep_dates", "=", "False", "\n", "", "", "", "else", ":", "\n", "                ", "keep_dates", "=", "False", "\n", "\n", "# Adding a tuple of zero vectors and empty_values to later store the date grounding of the two ques-events", "\n", "", "event1_date_grounding", "=", "[", "0", "]", "*", "len", "(", "p_date_values", ")", "\n", "event2_date_grounding", "=", "[", "0", "]", "*", "len", "(", "p_date_values", ")", "\n", "event1_date_value", "=", "[", "-", "1", ",", "-", "1", ",", "-", "1", "]", "\n", "event2_date_value", "=", "[", "-", "1", ",", "-", "1", ",", "-", "1", "]", "\n", "\n", "question_answer", "[", "constants", ".", "exection_supervised", "]", "=", "False", "\n", "\n", "if", "keep_dates", ":", "\n", "                ", "numexamaples_w_dates_annotated", "+=", "1", "\n", "event1_date_grounding", "[", "event1_date_idx", "]", "=", "1", "\n", "event1_date_value", "=", "p_date_values", "[", "event1_date_idx", "]", "\n", "event2_date_grounding", "[", "event2_date_idx", "]", "=", "1", "\n", "event2_date_value", "=", "p_date_values", "[", "event2_date_idx", "]", "\n", "question_answer", "[", "constants", ".", "exection_supervised", "]", "=", "True", "\n", "\n", "", "\"\"\" We store the groundings in the reverse order since they seem to help \"\"\"", "\n", "question_answer", "[", "constants", ".", "qspan_dategrounding_supervision", "]", "=", "[", "event2_date_grounding", ",", "event1_date_grounding", "]", "\n", "question_answer", "[", "constants", ".", "qspan_datevalue_supervision", "]", "=", "[", "event2_date_value", ",", "event1_date_value", "]", "\n", "\n", "new_qa_pairs", ".", "append", "(", "question_answer", ")", "\n", "\n", "", "if", "len", "(", "new_qa_pairs", ")", ">", "0", ":", "\n", "            ", "passage_info", "[", "constants", ".", "qa_pairs", "]", "=", "new_qa_pairs", "\n", "new_dataset", "[", "passage_id", "]", "=", "passage_info", "\n", "after_pruning_ques", "+=", "len", "(", "new_qa_pairs", ")", "\n", "\n", "", "", "num_passages_after_prune", "=", "len", "(", "new_dataset", ")", "\n", "print", "(", "f\"Passages original:{num_passages}  After Pruning:{num_passages_after_prune}\"", ")", "\n", "print", "(", "f\"Questions original:{total_ques}  After pruning:{after_pruning_ques}\"", ")", "\n", "print", "(", "f\"Num of QA with annotated dates: {numexamaples_w_dates_annotated}\"", ")", "\n", "\n", "return", "new_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.who_relocate.relocate_wprogs.readDataset": [[11, 15], ["open", "json.load"], "function", ["None"], ["def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.who_relocate.relocate_wprogs.relocate_program_qattn": [[17, 114], ["question_lower.split", "len", "any", "enumerate", "sum", "sum", "sum", "sum", "sum", "sum"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "def", "relocate_program_qattn", "(", "tokenized_queslower", ":", "str", ")", ":", "\n", "    ", "\"\"\" Here we'll annotate questions with one/two attentions depending on if the program type is\n        1. find(QuestionAttention)\n        2. filter(QuestionAttention, find(QuestionAttention))\n    \"\"\"", "\n", "question_lower", "=", "tokenized_queslower", "\n", "question_tokens", ":", "List", "[", "str", "]", "=", "question_lower", ".", "split", "(", "\" \"", ")", "\n", "qlen", "=", "len", "(", "question_tokens", ")", "\n", "\n", "qtype", "=", "None", "\n", "reloc_qattn", "=", "[", "0.0", "]", "*", "qlen", "\n", "filter_qattn", "=", "[", "0.0", "]", "*", "qlen", "\n", "find_qattn", "=", "[", "0.0", "]", "*", "qlen", "\n", "\n", "tokens_with_find_attention", "=", "[", "\n", "\"touchdown\"", ",", "\n", "\"run\"", ",", "\n", "\"pass\"", ",", "\n", "\"field\"", ",", "\n", "\"goal\"", ",", "\n", "\"passing\"", ",", "\n", "\"TD\"", ",", "\n", "\"td\"", ",", "\n", "\"rushing\"", ",", "\n", "\"kick\"", ",", "\n", "\"scoring\"", ",", "\n", "\"drive\"", ",", "\n", "\"touchdowns\"", ",", "\n", "\"reception\"", ",", "\n", "\"interception\"", ",", "\n", "\"return\"", ",", "\n", "\"goals\"", ",", "\n", "\"passes\"", ",", "\n", "]", "\n", "tokens_with_no_attention", "=", "[", "\n", "\"which\"", ",", "\n", "\"Which\"", ",", "\n", "\"who\"", ",", "\n", "\"Who\"", ",", "\n", "\"How\"", ",", "\n", "\"many\"", ",", "\n", "\"yards\"", ",", "\n", "\"yard\"", ",", "\n", "\"was\"", ",", "\n", "\"the\"", ",", "\n", "\"longest\"", ",", "\n", "\"shortest\"", ",", "\n", "\"?\"", ",", "\n", "\"kicked\"", ",", "\n", "\"caught\"", ",", "\n", "\"threw\"", ",", "\n", "\"player\"", ",", "\n", "\"scored\"", ",", "\n", "\"of\"", ",", "\n", "\"in\"", ",", "\n", "\"game\"", ",", "\n", "\"most\"", ",", "\n", "]", "\n", "\n", "if", "any", "(", "\n", "span", "in", "question_lower", "\n", "for", "span", "in", "[", "\"who threw the\"", ",", "\"who caught the\"", ",", "\"who kicked the\"", ",", "\"who scored the\"", ",", "\"which player scored\"", "]", "\n", ")", ":", "\n", "# first deal with non longest / shortest -- strategy is everything that is not in find or relocate is filter", "\n", "        ", "reloc_qattn", "[", "1", "]", "=", "1.0", "\n", "for", "i", ",", "t", "in", "enumerate", "(", "question_tokens", ")", ":", "\n", "            ", "if", "t", "in", "tokens_with_find_attention", ":", "\n", "                ", "find_qattn", "[", "i", "]", "=", "1.0", "\n", "", "elif", "t", "in", "tokens_with_no_attention", ":", "\n", "                ", "continue", "\n", "", "else", ":", "\n", "                ", "filter_qattn", "[", "i", "]", "=", "1.0", "\n", "\n", "", "", "if", "\"longest\"", "in", "question_tokens", ":", "\n", "            ", "if", "sum", "(", "filter_qattn", ")", "!=", "0", ":", "\n", "                ", "qtype", "=", "constants", ".", "RELOC_maxfilterfind_qtype", "\n", "", "else", ":", "\n", "                ", "qtype", "=", "constants", ".", "RELOC_maxfind_qtype", "\n", "", "", "elif", "\"shortest\"", "in", "question_tokens", ":", "\n", "            ", "if", "sum", "(", "filter_qattn", ")", "!=", "0", ":", "\n", "                ", "qtype", "=", "constants", ".", "RELOC_minfilterfind_qtype", "\n", "", "else", ":", "\n", "                ", "qtype", "=", "constants", ".", "RELOC_minfind_qtype", "\n", "", "", "else", ":", "\n", "            ", "if", "sum", "(", "filter_qattn", ")", "!=", "0", ":", "\n", "                ", "qtype", "=", "constants", ".", "RELOC_filterfind_qtype", "\n", "", "else", ":", "\n", "                ", "qtype", "=", "constants", ".", "RELOC_find_qtype", "\n", "\n", "", "", "", "if", "sum", "(", "reloc_qattn", ")", "==", "0", ":", "\n", "        ", "reloc_qattn", "=", "None", "\n", "", "if", "sum", "(", "filter_qattn", ")", "==", "0", ":", "\n", "        ", "filter_qattn", "=", "None", "\n", "", "if", "sum", "(", "find_qattn", ")", "==", "0", ":", "\n", "        ", "find_qattn", "=", "None", "\n", "\n", "", "return", "qtype", ",", "reloc_qattn", ",", "filter_qattn", ",", "find_qattn", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.who_relocate.relocate_wprogs.convert_span_to_attention": [[116, 123], ["range"], "function", ["None"], ["", "def", "convert_span_to_attention", "(", "qlen", ",", "span", ")", ":", "\n", "# span is inclusive on both ends", "\n", "    ", "qattn", "=", "[", "0.0", "]", "*", "qlen", "\n", "for", "x", "in", "range", "(", "span", "[", "0", "]", ",", "span", "[", "1", "]", "+", "1", ")", ":", "\n", "        ", "qattn", "[", "x", "]", "=", "1.0", "\n", "\n", "", "return", "qattn", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.who_relocate.relocate_wprogs.preprocess_Relocate_ques_wattn": [[125, 200], ["len", "collections.defaultdict", "dataset.items", "len", "print", "print", "print", "print", "original_question.lower", "tokenized_ques.split", "len", "any", "len", "len", "relocate_wprogs.relocate_program_qattn", "new_qa_pairs.append", "qattn_tuple.append", "tokenized_ques.lower", "qattn_tuple.append", "qattn_tuple.append"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.who_relocate.relocate_wprogs.relocate_program_qattn"], ["", "def", "preprocess_Relocate_ques_wattn", "(", "dataset", ")", ":", "\n", "    ", "\"\"\" This function prunes for questions that are count based questions.\n\n        Along with pruning, we also supervise the with the qtype and program_supervised flag\n    \"\"\"", "\n", "\n", "new_dataset", "=", "{", "}", "\n", "total_ques", "=", "0", "\n", "after_pruning_ques", "=", "0", "\n", "questions_w_attn", "=", "0", "\n", "num_passages", "=", "len", "(", "dataset", ")", "\n", "qtype_dist", "=", "defaultdict", "(", "int", ")", "\n", "\n", "for", "passage_id", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "new_qa_pairs", "=", "[", "]", "\n", "for", "question_answer", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "            ", "total_ques", "+=", "1", "\n", "\n", "original_question", "=", "question_answer", "[", "constants", ".", "cleaned_question", "]", "\n", "question_lower", "=", "original_question", ".", "lower", "(", ")", "\n", "tokenized_ques", "=", "question_answer", "[", "constants", ".", "tokenized_question", "]", "\n", "tokens", "=", "tokenized_ques", ".", "split", "(", "\" \"", ")", "\n", "qlen", "=", "len", "(", "tokens", ")", "\n", "if", "any", "(", "span", "in", "question_lower", "for", "span", "in", "WHO_RELOCATE_NGRAMS", ")", ":", "\n", "                ", "(", "qtype", ",", "reloc_qattn", ",", "filter_qattn", ",", "find_qattn", ")", "=", "relocate_program_qattn", "(", "\n", "tokenized_queslower", "=", "tokenized_ques", ".", "lower", "(", ")", "\n", ")", "\n", "\n", "# if question_attention_filter_span is not None:", "\n", "#     filter_qattn = convert_span_to_attention(qlen, question_attention_filter_span)", "\n", "# else:", "\n", "#     filter_qattn = None", "\n", "\n", "# find_qattn = convert_span_to_attention(qlen, question_attention_find_span)", "\n", "\n", "if", "qtype", "is", "not", "None", "and", "reloc_qattn", "is", "not", "None", "and", "find_qattn", "is", "not", "None", ":", "\n", "                    ", "question_answer", "[", "constants", ".", "qtype", "]", "=", "qtype", "\n", "question_answer", "[", "constants", ".", "program_supervised", "]", "=", "True", "\n", "qtype_dist", "[", "qtype", "]", "+=", "1", "\n", "\n", "# Inserting qattns so that the order is (reloc_attn, filter_qattn, find_qattn)", "\n", "# Definitely a reloc attn and find.", "\n", "qattn_tuple", "=", "[", "]", "\n", "qattn_tuple", ".", "append", "(", "reloc_qattn", ")", "\n", "question_answer", "[", "constants", ".", "qattn_supervised", "]", "=", "True", "\n", "\n", "if", "filter_qattn", "is", "not", "None", ":", "\n", "                        ", "qattn_tuple", ".", "append", "(", "filter_qattn", ")", "\n", "\n", "", "if", "find_qattn", "is", "not", "None", ":", "\n", "                        ", "qattn_tuple", ".", "append", "(", "find_qattn", ")", "\n", "\n", "# # Also adding qattn -- everything apart from the first two tokens", "\n", "# if filter_qattn is not None:", "\n", "#     question_answer[constants.ques_attention_supervision] = [filter_qattn, find_qattn]", "\n", "# else:", "\n", "#     question_answer[constants.ques_attention_supervision] = [find_qattn]", "\n", "", "question_answer", "[", "constants", ".", "ques_attention_supervision", "]", "=", "qattn_tuple", "\n", "question_answer", "[", "constants", ".", "qattn_supervised", "]", "=", "True", "\n", "questions_w_attn", "+=", "1", "\n", "\n", "", "new_qa_pairs", ".", "append", "(", "question_answer", ")", "\n", "\n", "", "", "if", "len", "(", "new_qa_pairs", ")", ">", "0", ":", "\n", "            ", "passage_info", "[", "constants", ".", "qa_pairs", "]", "=", "new_qa_pairs", "\n", "new_dataset", "[", "passage_id", "]", "=", "passage_info", "\n", "after_pruning_ques", "+=", "len", "(", "new_qa_pairs", ")", "\n", "\n", "", "", "num_passages_after_prune", "=", "len", "(", "new_dataset", ")", "\n", "print", "(", "f\"Passages original:{num_passages}  Questions original:{total_ques}\"", ")", "\n", "print", "(", "f\"Passages after-pruning:{num_passages_after_prune}  Question after-pruning:{after_pruning_ques}\"", ")", "\n", "print", "(", "f\"Ques with attn: {questions_w_attn}\"", ")", "\n", "print", "(", "f\"QType distribution: {qtype_dist}\"", ")", "\n", "\n", "return", "new_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.who_relocate.relocate_wprogs_alternate.convert_start_end_to_attention_vector": [[55, 60], ["None"], "function", ["None"], ["def", "convert_start_end_to_attention_vector", "(", "length", ",", "start", ",", "end", ")", ":", "\n", "    ", "\"\"\"Convert start/end of a span into a binary vector. start/end is inclusive/exclusive.\"\"\"", "\n", "attention_vector", "=", "[", "0.0", "]", "*", "length", "\n", "attention_vector", "[", "start", ":", "end", "]", "=", "[", "1.0", "]", "*", "(", "end", "-", "start", ")", "\n", "return", "attention_vector", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.who_relocate.relocate_wprogs_alternate.generate_relocate_find_questions": [[62, 72], ["itertools.product", "print", "relocate_find_questions.append", "relocate_find_questions.append", "len"], "function", ["None"], ["", "def", "generate_relocate_find_questions", "(", ")", ":", "\n", "    ", "relocate_find_questions", "=", "[", "]", "\n", "for", "relocate_query", ",", "find_modifier", ",", "event", "in", "itertools", ".", "product", "(", "RELOCATE_QUERIES", ",", "FIND_MODIFIERS", ",", "EVENTS", ")", ":", "\n", "        ", "question1", "=", "relocate_query", "+", "\" the \"", "+", "find_modifier", "+", "\" \"", "+", "event", "+", "\" ?\"", "\n", "question2", "=", "relocate_query", "+", "\" the \"", "+", "find_modifier", "+", "\" \"", "+", "event", "+", "\" of the game ?\"", "\n", "relocate_find_questions", ".", "append", "(", "question1", ")", "\n", "relocate_find_questions", ".", "append", "(", "question2", ")", "\n", "\n", "", "print", "(", "f\"Num of Relocate-Find questions in repo: {len(relocate_find_questions)}\"", ")", "\n", "return", "relocate_find_questions", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.who_relocate.relocate_wprogs_alternate.get_relocate_find_attention": [[74, 95], ["len", "question_tokens.index", "relocate_wprogs_alternate.convert_start_end_to_attention_vector", "relocate_wprogs_alternate.convert_start_end_to_attention_vector"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.convert_start_end_to_attention_vector", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.convert_start_end_to_attention_vector"], ["", "def", "get_relocate_find_attention", "(", "question_tokens", ":", "List", "[", "str", "]", ")", ":", "\n", "    ", "\"\"\"Get the relocate and find attention for relocate-find questions.\n    These are of the kind: \"relocate_query + \" the \" + find_modifier + \" \" + event + \" of the game ?\"\n    Everything before the first \"the\" is relocate-attn\n    Everything after the first \"the\" until \"?\" or \"of the game ?\" is find-attn\n    \"\"\"", "\n", "question_len", "=", "len", "(", "question_tokens", ")", "\n", "first_the_idx", "=", "question_tokens", ".", "index", "(", "\"the\"", ")", "\n", "relocate_start", "=", "0", "\n", "relocate_end", "=", "first_the_idx", "# exclusive,", "\n", "\n", "find_start", "=", "first_the_idx", "+", "1", "# inclusive", "\n", "if", "\"of the game\"", "in", "\" \"", ".", "join", "(", "question_tokens", ")", ":", "\n", "        ", "find_end", "=", "question_len", "-", "4", "\n", "", "else", ":", "\n", "        ", "find_end", "=", "question_len", "-", "1", "\n", "\n", "", "relocate_attention", "=", "convert_start_end_to_attention_vector", "(", "question_len", ",", "relocate_start", ",", "relocate_end", ")", "\n", "find_attention", "=", "convert_start_end_to_attention_vector", "(", "question_len", ",", "find_start", ",", "find_end", ")", "\n", "\n", "return", "relocate_attention", ",", "find_attention", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.who_relocate.relocate_wprogs_alternate.generate_relocate_minmax_find_questions": [[97, 107], ["itertools.product", "print", "relocate_minmax_find_questions.append", "relocate_minmax_find_questions.append", "len"], "function", ["None"], ["", "def", "generate_relocate_minmax_find_questions", "(", ")", ":", "\n", "    ", "relocate_minmax_find_questions", "=", "[", "]", "\n", "for", "relocate_query", ",", "mixmax_modifier", ",", "event", "in", "itertools", ".", "product", "(", "RELOCATE_QUERIES", ",", "MIN_MAX_MODIFIERS", ",", "EVENTS", ")", ":", "\n", "        ", "question1", "=", "relocate_query", "+", "\" the \"", "+", "mixmax_modifier", "+", "\" \"", "+", "event", "+", "\" ?\"", "\n", "question2", "=", "relocate_query", "+", "\" the \"", "+", "mixmax_modifier", "+", "\" \"", "+", "event", "+", "\" of the game ?\"", "\n", "relocate_minmax_find_questions", ".", "append", "(", "question1", ")", "\n", "relocate_minmax_find_questions", ".", "append", "(", "question2", ")", "\n", "\n", "", "print", "(", "f\"Num of Relocate-MinMax-Find questions in repo: {len(relocate_minmax_find_questions)}\"", ")", "\n", "return", "relocate_minmax_find_questions", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.who_relocate.relocate_wprogs_alternate.get_relocate_minmax_find_attention": [[109, 129], ["len", "question_tokens.index", "relocate_wprogs_alternate.convert_start_end_to_attention_vector", "relocate_wprogs_alternate.convert_start_end_to_attention_vector"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.convert_start_end_to_attention_vector", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.convert_start_end_to_attention_vector"], ["", "def", "get_relocate_minmax_find_attention", "(", "question_tokens", ":", "List", "[", "str", "]", ")", ":", "\n", "    ", "\"\"\"Get the relocate and find attention for relocate-find questions.\n    These are of the kind: \"relocate_query + \" the \" + mixmax_modifier + \" \" + event + \" of the game ?\"\n    Everything before the first \"the\" is relocate-attn\n    Everything after the first \"the mixmax_modifier\" until \"?\" or \"of the game ?\" is find-attn\n    \"\"\"", "\n", "question_len", "=", "len", "(", "question_tokens", ")", "\n", "first_the_idx", "=", "question_tokens", ".", "index", "(", "\"the\"", ")", "\n", "relocate_start", "=", "0", "\n", "relocate_end", "=", "first_the_idx", "# exclusive,", "\n", "\n", "find_start", "=", "first_the_idx", "+", "2", "# inclusive, skipping \"the minmax_modifier\"", "\n", "if", "\"of the game\"", "in", "\" \"", ".", "join", "(", "question_tokens", ")", ":", "\n", "        ", "find_end", "=", "question_len", "-", "4", "\n", "", "else", ":", "\n", "        ", "find_end", "=", "question_len", "-", "1", "\n", "\n", "", "relocate_attention", "=", "convert_start_end_to_attention_vector", "(", "question_len", ",", "relocate_start", ",", "relocate_end", ")", "\n", "find_attention", "=", "convert_start_end_to_attention_vector", "(", "question_len", ",", "find_start", ",", "find_end", ")", "\n", "return", "relocate_attention", ",", "find_attention", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.who_relocate.relocate_wprogs_alternate.generate_relocate_minmax_filter_find_questions": [[131, 144], ["itertools.product", "print", "relocate_minmax_filter_find_questions.append", "relocate_minmax_filter_find_questions.append", "relocate_minmax_filter_find_questions.append", "len"], "function", ["None"], ["", "def", "generate_relocate_minmax_filter_find_questions", "(", ")", ":", "\n", "    ", "relocate_minmax_filter_find_questions", "=", "[", "]", "\n", "for", "relocate_query", ",", "mixmax_modifier", ",", "event", ",", "filter_modifier", "in", "itertools", ".", "product", "(", "\n", "RELOCATE_QUERIES", ",", "MIN_MAX_MODIFIERS", ",", "EVENTS", ",", "FILTER_MODIFIERS", ")", ":", "\n", "        ", "question1", "=", "relocate_query", "+", "\" the \"", "+", "mixmax_modifier", "+", "\" \"", "+", "event", "+", "\" of \"", "+", "filter_modifier", "+", "\" ?\"", "\n", "question2", "=", "relocate_query", "+", "\" the \"", "+", "mixmax_modifier", "+", "\" \"", "+", "event", "+", "\" in \"", "+", "filter_modifier", "+", "\" ?\"", "\n", "question3", "=", "relocate_query", "+", "\" the \"", "+", "mixmax_modifier", "+", "\" \"", "+", "event", "+", "\" during \"", "+", "filter_modifier", "+", "\" ?\"", "\n", "relocate_minmax_filter_find_questions", ".", "append", "(", "question1", ")", "\n", "relocate_minmax_filter_find_questions", ".", "append", "(", "question2", ")", "\n", "relocate_minmax_filter_find_questions", ".", "append", "(", "question3", ")", "\n", "\n", "", "print", "(", "f\"Num of Relocate-MinMax-Filter-Find questions in repo: {len(relocate_minmax_filter_find_questions)}\"", ")", "\n", "return", "relocate_minmax_filter_find_questions", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.who_relocate.relocate_wprogs_alternate.get_relocate_minmax_filter_find_attention": [[146, 176], ["len", "question_tokens.index", "relocate_wprogs_alternate.convert_start_end_to_attention_vector", "relocate_wprogs_alternate.convert_start_end_to_attention_vector", "relocate_wprogs_alternate.convert_start_end_to_attention_vector", "question_tokens.index"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.convert_start_end_to_attention_vector", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.convert_start_end_to_attention_vector", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.convert_start_end_to_attention_vector"], ["", "def", "get_relocate_minmax_filter_find_attention", "(", "question_tokens", ":", "List", "[", "str", "]", ")", ":", "\n", "    ", "\"\"\"Get the relocate and find attention for relocate-find questions.\n    These are : \"relocate_query + \" the \" + mixmax_modifier + \" \" + event + \" of/in/during \" + filter_modifier + \" ?\"\n    relocate-attn: Everything before the first \"the\" is\n    find-attn:     Everything after the first \"the mixmax_modifier\" until the first \"of/in/during\"\n    filter-attn:   Everything after the first \"of/in/during\"\n\n    Since the only instance of \"of\"/\"in\"/\"during\" would be as a filler in these questions, the above rules should work.\n    \"\"\"", "\n", "question_len", "=", "len", "(", "question_tokens", ")", "\n", "first_the_idx", "=", "question_tokens", ".", "index", "(", "\"the\"", ")", "\n", "relocate_start", "=", "0", "\n", "relocate_end", "=", "first_the_idx", "# exclusive,", "\n", "relocate_attention", "=", "convert_start_end_to_attention_vector", "(", "question_len", ",", "relocate_start", ",", "relocate_end", ")", "\n", "\n", "preposition_idx", "=", "None", "\n", "for", "prep", "in", "[", "\"of\"", ",", "\"in\"", ",", "\"during\"", "]", ":", "\n", "        ", "if", "prep", "in", "question_tokens", ":", "\n", "            ", "preposition_idx", "=", "question_tokens", ".", "index", "(", "prep", ")", "\n", "", "", "assert", "preposition_idx", "is", "not", "None", "\n", "\n", "find_start", "=", "first_the_idx", "+", "2", "# inclusive, skipping \"the minmax_modifier\"", "\n", "find_end", "=", "preposition_idx", "# exclusive", "\n", "find_attention", "=", "convert_start_end_to_attention_vector", "(", "question_len", ",", "find_start", ",", "find_end", ")", "\n", "\n", "filter_start", "=", "preposition_idx", "+", "1", "# inclusive, start after the preposition", "\n", "filter_end", "=", "question_len", "-", "1", "# exclusive, skip the \"?\" at the end", "\n", "filter_attention", "=", "convert_start_end_to_attention_vector", "(", "question_len", ",", "filter_start", ",", "filter_end", ")", "\n", "\n", "return", "relocate_attention", ",", "filter_attention", ",", "find_attention", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.who_relocate.relocate_wprogs_alternate.readDataset": [[178, 182], ["open", "json.load"], "function", ["None"], ["", "def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.who_relocate.relocate_wprogs_alternate.preprocess_Relocate_ques_wattn": [[184, 263], ["relocate_wprogs_alternate.generate_relocate_find_questions", "relocate_wprogs_alternate.generate_relocate_minmax_find_questions", "relocate_wprogs_alternate.generate_relocate_minmax_filter_find_questions", "len", "collections.defaultdict", "dataset.items", "len", "print", "print", "print", "original_question.lower", "tokenized_ques.lower().split", "any", "len", "len", "new_qa_pairs.append", "tokenized_ques.lower", "tokenized_ques.lower", "relocate_wprogs_alternate.get_relocate_find_attention", "tokenized_ques.lower", "relocate_wprogs_alternate.get_relocate_minmax_find_attention", "tokenized_ques.lower", "relocate_wprogs_alternate.get_relocate_minmax_filter_find_attention"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.who_relocate.relocate_wprogs_alternate.generate_relocate_find_questions", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.who_relocate.relocate_wprogs_alternate.generate_relocate_minmax_find_questions", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.who_relocate.relocate_wprogs_alternate.generate_relocate_minmax_filter_find_questions", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.who_relocate.relocate_wprogs_alternate.get_relocate_find_attention", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.who_relocate.relocate_wprogs_alternate.get_relocate_minmax_find_attention", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.who_relocate.relocate_wprogs_alternate.get_relocate_minmax_filter_find_attention"], ["", "def", "preprocess_Relocate_ques_wattn", "(", "dataset", ")", ":", "\n", "    ", "\"\"\" This function prunes for questions that are count based questions.\n\n        Along with pruning, we also supervise the with the qtype and program_supervised flag\n    \"\"\"", "\n", "\n", "relocate_find_questions", "=", "generate_relocate_find_questions", "(", ")", "\n", "relocate_minmax_find_questions", "=", "generate_relocate_minmax_find_questions", "(", ")", "\n", "relocate_minmax_filter_find_questions", "=", "generate_relocate_minmax_filter_find_questions", "(", ")", "\n", "\n", "new_dataset", "=", "{", "}", "\n", "total_ques", "=", "0", "\n", "after_pruning_ques", "=", "0", "\n", "num_passages", "=", "len", "(", "dataset", ")", "\n", "program_dist", "=", "defaultdict", "(", "int", ")", "\n", "\n", "for", "passage_id", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "new_qa_pairs", "=", "[", "]", "\n", "for", "question_answer", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "            ", "total_ques", "+=", "1", "\n", "\n", "original_question", "=", "question_answer", "[", "constants", ".", "cleaned_question", "]", "\n", "question_lower", "=", "original_question", ".", "lower", "(", ")", "\n", "tokenized_ques", "=", "question_answer", "[", "constants", ".", "tokenized_question", "]", "\n", "question_tokens", "=", "tokenized_ques", ".", "lower", "(", ")", ".", "split", "(", "\" \"", ")", "\n", "if", "any", "(", "span", "in", "question_lower", "for", "span", "in", "WHO_RELOCATE_NGRAMS", ")", ":", "\n", "\n", "                ", "if", "tokenized_ques", ".", "lower", "(", ")", "in", "relocate_find_questions", ":", "\n", "                    ", "qtype", "=", "constants", ".", "RELOC_find_qtype", "\n", "question_answer", "[", "constants", ".", "qtype", "]", "=", "qtype", "\n", "question_answer", "[", "constants", ".", "program_supervised", "]", "=", "True", "\n", "relocate_qattn", ",", "find_qattn", "=", "get_relocate_find_attention", "(", "question_tokens", ")", "\n", "question_answer", "[", "constants", ".", "ques_attention_supervision", "]", "=", "[", "relocate_qattn", ",", "find_qattn", "]", "\n", "question_answer", "[", "constants", ".", "qattn_supervised", "]", "=", "True", "\n", "program_dist", "[", "qtype", "]", "+=", "1", "\n", "\n", "", "if", "tokenized_ques", ".", "lower", "(", ")", "in", "relocate_minmax_find_questions", ":", "\n", "                    ", "if", "\"longest\"", "in", "question_tokens", ":", "\n", "                        ", "qtype", "=", "constants", ".", "RELOC_maxfind_qtype", "\n", "", "elif", "\"shortest\"", "in", "question_tokens", ":", "\n", "                        ", "qtype", "=", "constants", ".", "RELOC_minfind_qtype", "\n", "", "else", ":", "\n", "                        ", "raise", "NotImplementedError", "\n", "", "question_answer", "[", "constants", ".", "qtype", "]", "=", "qtype", "\n", "question_answer", "[", "constants", ".", "program_supervised", "]", "=", "True", "\n", "relocate_qattn", ",", "find_qattn", "=", "get_relocate_minmax_find_attention", "(", "question_tokens", ")", "\n", "question_answer", "[", "constants", ".", "ques_attention_supervision", "]", "=", "[", "relocate_qattn", ",", "find_qattn", "]", "\n", "question_answer", "[", "constants", ".", "qattn_supervised", "]", "=", "True", "\n", "program_dist", "[", "qtype", "]", "+=", "1", "\n", "\n", "", "if", "tokenized_ques", ".", "lower", "(", ")", "in", "relocate_minmax_filter_find_questions", ":", "\n", "                    ", "if", "\"longest\"", "in", "question_tokens", ":", "\n", "                        ", "qtype", "=", "constants", ".", "RELOC_maxfilterfind_qtype", "\n", "", "elif", "\"shortest\"", "in", "question_tokens", ":", "\n", "                        ", "qtype", "=", "constants", ".", "RELOC_minfilterfind_qtype", "\n", "", "else", ":", "\n", "                        ", "raise", "NotImplementedError", "\n", "", "question_answer", "[", "constants", ".", "qtype", "]", "=", "qtype", "\n", "question_answer", "[", "constants", ".", "program_supervised", "]", "=", "True", "\n", "relocate_qattn", ",", "filter_qattn", ",", "find_qattn", "=", "get_relocate_minmax_filter_find_attention", "(", "\n", "question_tokens", ")", "\n", "question_answer", "[", "constants", ".", "ques_attention_supervision", "]", "=", "[", "relocate_qattn", ",", "filter_qattn", ",", "find_qattn", "]", "\n", "question_answer", "[", "constants", ".", "qattn_supervised", "]", "=", "True", "\n", "program_dist", "[", "qtype", "]", "+=", "1", "\n", "\n", "", "new_qa_pairs", ".", "append", "(", "question_answer", ")", "\n", "\n", "", "", "if", "len", "(", "new_qa_pairs", ")", ">", "0", ":", "\n", "            ", "passage_info", "[", "constants", ".", "qa_pairs", "]", "=", "new_qa_pairs", "\n", "new_dataset", "[", "passage_id", "]", "=", "passage_info", "\n", "after_pruning_ques", "+=", "len", "(", "new_qa_pairs", ")", "\n", "\n", "", "", "num_passages_after_prune", "=", "len", "(", "new_dataset", ")", "\n", "print", "(", "f\"Passages original:{num_passages}  Questions original:{total_ques}\"", ")", "\n", "print", "(", "f\"Passages after-pruning:{num_passages_after_prune}  Question after-pruning:{after_pruning_ques}\"", ")", "\n", "\n", "print", "(", "f\"\\nProgram Dist: {program_dist}\"", ")", "\n", "\n", "return", "new_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.how_many_yards_alternate.readDataset": [[114, 118], ["open", "json.load"], "function", ["None"], ["def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.how_many_yards_alternate.is_num_find_question": [[120, 127], ["NUM_FIND_REGEX.fullmatch"], "function", ["None"], ["", "def", "is_num_find_question", "(", "tokenized_question_lower", ":", "str", ")", ":", "\n", "    ", "match_result", "=", "NUM_FIND_REGEX", ".", "fullmatch", "(", "tokenized_question_lower", ")", "\n", "if", "(", "match_result", "is", "not", "None", "and", "\"longest\"", "not", "in", "tokenized_question_lower", "and", "\n", "\"shortest\"", "not", "in", "tokenized_question_lower", ")", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.how_many_yards_alternate.is_num_minmax_find_question": [[129, 135], ["NUM_MINMAX_FIND_REGEX.fullmatch"], "function", ["None"], ["", "", "def", "is_num_minmax_find_question", "(", "tokenized_question_lower", ":", "str", ")", ":", "\n", "    ", "match_result", "=", "NUM_MINMAX_FIND_REGEX", ".", "fullmatch", "(", "tokenized_question_lower", ")", "\n", "if", "match_result", "is", "not", "None", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.how_many_yards_alternate.is_num_minmax_filter_find_question": [[137, 143], ["NUM_MINMAX_FILTER_FIND_REGEX.fullmatch"], "function", ["None"], ["", "", "def", "is_num_minmax_filter_find_question", "(", "tokenized_question_lower", ":", "str", ")", ":", "\n", "    ", "match_result", "=", "NUM_MINMAX_FILTER_FIND_REGEX", ".", "fullmatch", "(", "tokenized_question_lower", ")", "\n", "if", "match_result", "is", "not", "None", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.how_many_yards_alternate.convert_start_end_to_attention_vector": [[145, 150], ["None"], "function", ["None"], ["", "", "def", "convert_start_end_to_attention_vector", "(", "length", ",", "start", ",", "end", ")", ":", "\n", "    ", "\"\"\"Convert start/end of a span into a binary vector. start/end is inclusive/exclusive.\"\"\"", "\n", "attention_vector", "=", "[", "0.0", "]", "*", "length", "\n", "attention_vector", "[", "start", ":", "end", "]", "=", "[", "1.0", "]", "*", "(", "end", "-", "start", ")", "\n", "return", "attention_vector", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.how_many_yards_alternate.get_question_attention": [[153, 208], ["len", "how_many_yards_alternate.convert_start_end_to_attention_vector", "how_many_yards_alternate.convert_start_end_to_attention_vector", "question_tokens.index", "how_many_yards_alternate.convert_start_end_to_attention_vector", "how_many_yards_alternate.convert_start_end_to_attention_vector", "how_many_yards_alternate.convert_start_end_to_attention_vector", "question_tokens.index", "zip", "question_tokens.index", "how_many_yards_alternate.convert_start_end_to_attention_vector", "question_tokens.index", "zip"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.convert_start_end_to_attention_vector", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.convert_start_end_to_attention_vector", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.convert_start_end_to_attention_vector", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.convert_start_end_to_attention_vector", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.convert_start_end_to_attention_vector", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.convert_start_end_to_attention_vector"], ["", "def", "get_question_attention", "(", "question_tokens", ":", "List", "[", "str", "]", ",", "qtype", ":", "str", ")", ":", "\n", "    ", "qlen", "=", "len", "(", "question_tokens", ")", "\n", "\n", "if", "qtype", "in", "[", "constants", ".", "NUM_find_qtype", "]", ":", "\n", "        ", "find_start", "=", "4", "# exclude \"how many yards was\"", "\n", "find_end", "=", "qlen", "-", "1", "# exclusive, don't attend to ?", "\n", "find_attention_vector", "=", "convert_start_end_to_attention_vector", "(", "qlen", ",", "find_start", ",", "\n", "find_end", ")", "\n", "filter_attention_vector", "=", "None", "\n", "\n", "", "elif", "qtype", "in", "[", "constants", ".", "MAX_find_qtype", ",", "constants", ".", "MIN_find_qtype", "]", ":", "\n", "# Two spans of find -- after \"how many yards was\" until longest/shortest and after until the end", "\n", "        ", "if", "qtype", "==", "constants", ".", "MAX_find_qtype", ":", "\n", "            ", "longest_shortest_idx", "=", "question_tokens", ".", "index", "(", "\"longest\"", ")", "\n", "", "elif", "qtype", "==", "constants", ".", "MIN_find_qtype", ":", "\n", "            ", "longest_shortest_idx", "=", "question_tokens", ".", "index", "(", "\"shortest\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "find_start1", "=", "4", "\n", "find_end1", "=", "longest_shortest_idx", "# exclusive, excluding longest/shortest", "\n", "find_start2", "=", "longest_shortest_idx", "+", "1", "\n", "find_end2", "=", "qlen", "-", "1", "# exclusive, don't attend to ?", "\n", "if", "question_tokens", "[", "find_start1", ":", "find_end1", "]", "!=", "[", "\"the\"", "]", ":", "\n", "            ", "find_attention_vector1", "=", "convert_start_end_to_attention_vector", "(", "qlen", ",", "find_start1", ",", "find_end1", ")", "\n", "", "else", ":", "\n", "            ", "find_attention_vector1", "=", "[", "0.0", "]", "*", "qlen", "\n", "", "find_attention_vector2", "=", "convert_start_end_to_attention_vector", "(", "qlen", ",", "find_start2", ",", "find_end2", ")", "\n", "find_attention_vector", "=", "[", "x", "+", "y", "for", "x", ",", "y", "in", "zip", "(", "find_attention_vector1", ",", "find_attention_vector2", ")", "]", "\n", "filter_attention_vector", "=", "None", "\n", "\n", "", "elif", "qtype", "in", "[", "constants", ".", "MAX_filter_find_qtype", ",", "constants", ".", "MIN_filter_find_qtype", "]", ":", "\n", "# Two spans of find -- after \"how many yards was\" until longest/shortest & after that until \\S* \\S* \\S* half ?", "\n", "        ", "if", "qtype", "==", "constants", ".", "MAX_filter_find_qtype", ":", "\n", "            ", "longest_shortest_idx", "=", "question_tokens", ".", "index", "(", "\"longest\"", ")", "\n", "", "elif", "qtype", "==", "constants", ".", "MIN_filter_find_qtype", ":", "\n", "            ", "longest_shortest_idx", "=", "question_tokens", ".", "index", "(", "\"shortest\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "filter_start", "=", "qlen", "-", "5", "# inclusive, the last 4 tokens excluding \"?\"", "\n", "filter_end", "=", "qlen", "-", "1", "# exclusive, last token before \"?\"", "\n", "find_start1", "=", "4", "\n", "find_end1", "=", "longest_shortest_idx", "# exclusive, excluding longest/shortest", "\n", "find_start2", "=", "longest_shortest_idx", "+", "1", "\n", "find_end2", "=", "qlen", "-", "5", "# exclusive, don't attend to ?", "\n", "if", "question_tokens", "[", "find_start1", ":", "find_end1", "]", "!=", "[", "\"the\"", "]", ":", "\n", "            ", "find_attention_vector1", "=", "convert_start_end_to_attention_vector", "(", "qlen", ",", "find_start1", ",", "find_end1", ")", "\n", "", "else", ":", "\n", "            ", "find_attention_vector1", "=", "[", "0.0", "]", "*", "qlen", "\n", "", "find_attention_vector2", "=", "convert_start_end_to_attention_vector", "(", "qlen", ",", "find_start2", ",", "find_end2", ")", "\n", "find_attention_vector", "=", "[", "x", "+", "y", "for", "x", ",", "y", "in", "zip", "(", "find_attention_vector1", ",", "find_attention_vector2", ")", "]", "\n", "filter_attention_vector", "=", "convert_start_end_to_attention_vector", "(", "qlen", ",", "filter_start", ",", "filter_end", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n", "", "return", "find_attention_vector", ",", "filter_attention_vector", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.how_many_yards_alternate.get_number_distribution_supervision": [[210, 313], ["tokenized_passage.split", "tokenized_question.split", "set", "enumerate", "set.remove", "set.add", "set.remove", "set.add", "set.remove", "set.add", "max", "min", "set", "set.intersection", "set", "zip", "set.remove", "len", "set.remove", "set.add", "set.remove", "set.add", "set.remove", "set.add", "relevant_number_tokenidxs.append", "relevant_number_entidxs.append", "relevant_number_values.append", "len", "set.add", "list", "max", "min"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add"], ["", "def", "get_number_distribution_supervision", "(", "\n", "tokenized_question", ",", "\n", "tokenized_passage", ",", "\n", "num_answer", ",", "\n", "attention", ",", "\n", "passage_num_mens", ",", "\n", "passage_num_entidxs", ",", "\n", "passage_num_vals", ",", "\n", ")", ":", "\n", "    ", "WINDOW", "=", "10", "\n", "passage_tokens", "=", "tokenized_passage", ".", "split", "(", "\" \"", ")", "\n", "question_tokens", "=", "tokenized_question", ".", "split", "(", "\" \"", ")", "\n", "\n", "# Only supervised longest / shortest questions -- cannot do the first / last kind of questions", "\n", "if", "\"longest\"", "not", "in", "question_tokens", "and", "\"shortest\"", "not", "in", "question_tokens", ":", "\n", "        ", "return", "None", ",", "None", "\n", "", "if", "num_answer", "is", "None", ":", "\n", "        ", "return", "None", ",", "None", "\n", "\n", "# These are the relevant tokens in the question. We'd like to find numbers that are surrounded by these tokens", "\n", "", "attended_tokens", "=", "[", "token", "for", "att", ",", "token", "in", "zip", "(", "attention", ",", "question_tokens", ")", "if", "att", ">", "0", "]", "\n", "attended_tokens", "=", "set", "(", "attended_tokens", ")", "\n", "# Replacing TD with touchdown", "\n", "if", "\"TD\"", "in", "attended_tokens", ":", "\n", "        ", "attended_tokens", ".", "remove", "(", "\"TD\"", ")", "\n", "attended_tokens", ".", "add", "(", "\"touchdown\"", ")", "\n", "", "if", "\"goals\"", "in", "attended_tokens", ":", "\n", "        ", "attended_tokens", ".", "remove", "(", "\"goals\"", ")", "\n", "attended_tokens", ".", "add", "(", "\"goal\"", ")", "\n", "", "if", "\"touchdowns\"", "in", "attended_tokens", ":", "\n", "        ", "attended_tokens", ".", "remove", "(", "\"touchdowns\"", ")", "\n", "attended_tokens", ".", "add", "(", "\"touchdown\"", ")", "\n", "", "irrelevant_tokens", "=", "[", "\"'\"", ",", "\"'s\"", ",", "\"of\"", ",", "\"the\"", ",", "\"game\"", ",", "\"games\"", ",", "\"in\"", "]", "\n", "# Remove irrelevant tokens from attended-tokens", "\n", "for", "t", "in", "irrelevant_tokens", ":", "\n", "        ", "if", "t", "in", "attended_tokens", ":", "\n", "            ", "attended_tokens", ".", "remove", "(", "t", ")", "\n", "\n", "# Num of passage number tokens", "\n", "", "", "number_token_idxs", "=", "[", "x", "for", "(", "_", ",", "x", ",", "_", ")", "in", "passage_num_mens", "]", "\n", "\n", "relevant_number_tokenidxs", "=", "[", "]", "\n", "relevant_number_entidxs", "=", "[", "]", "\n", "relevant_number_values", "=", "[", "]", "\n", "\n", "for", "menidx", ",", "number_token_idx", "in", "enumerate", "(", "number_token_idxs", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "if", "passage_tokens", "[", "number_token_idx", "+", "1", "]", "!=", "\"-\"", "or", "passage_tokens", "[", "number_token_idx", "+", "2", "]", "!=", "\"yard\"", ":", "\n", "                ", "continue", "\n", "", "", "except", ":", "\n", "            ", "continue", "\n", "", "starting_tokenidx", "=", "max", "(", "0", ",", "number_token_idx", "-", "WINDOW", ")", "# Inclusive", "\n", "ending_tokenidx", "=", "min", "(", "len", "(", "passage_tokens", ")", ",", "number_token_idx", "+", "WINDOW", "+", "1", ")", "# Exclusive", "\n", "surrounding_passage_tokens", "=", "set", "(", "passage_tokens", "[", "starting_tokenidx", ":", "ending_tokenidx", "]", ")", "\n", "if", "\"TD\"", "in", "surrounding_passage_tokens", ":", "\n", "            ", "surrounding_passage_tokens", ".", "remove", "(", "\"TD\"", ")", "\n", "surrounding_passage_tokens", ".", "add", "(", "\"touchdown\"", ")", "\n", "", "if", "\"goals\"", "in", "surrounding_passage_tokens", ":", "\n", "            ", "surrounding_passage_tokens", ".", "remove", "(", "\"goals\"", ")", "\n", "surrounding_passage_tokens", ".", "add", "(", "\"goal\"", ")", "\n", "", "if", "\"touchdowns\"", "in", "surrounding_passage_tokens", ":", "\n", "            ", "surrounding_passage_tokens", ".", "remove", "(", "\"touchdowns\"", ")", "\n", "surrounding_passage_tokens", ".", "add", "(", "\"touchdown\"", ")", "\n", "", "intersection_tokens", "=", "surrounding_passage_tokens", ".", "intersection", "(", "attended_tokens", ")", "\n", "if", "intersection_tokens", "==", "attended_tokens", ":", "\n", "            ", "relevant_number_tokenidxs", ".", "append", "(", "number_token_idx", ")", "\n", "relevant_number_entidxs", ".", "append", "(", "passage_num_entidxs", "[", "menidx", "]", ")", "\n", "relevant_number_values", ".", "append", "(", "passage_num_vals", "[", "passage_num_entidxs", "[", "menidx", "]", "]", ")", "\n", "\n", "", "", "if", "relevant_number_entidxs", ":", "\n", "        ", "number_grounding", "=", "[", "0", "]", "*", "len", "(", "passage_num_vals", ")", "\n", "number_values", "=", "set", "(", ")", "\n", "for", "entidx", "in", "relevant_number_entidxs", ":", "\n", "            ", "number_grounding", "[", "entidx", "]", "=", "1", "\n", "number_values", ".", "add", "(", "passage_num_vals", "[", "entidx", "]", ")", "\n", "", "number_grounding", "=", "[", "number_grounding", "]", "\n", "number_values", "=", "[", "list", "(", "number_values", ")", "]", "\n", "if", "num_answer", "not", "in", "number_values", "[", "0", "]", ":", "# It's now a list", "\n", "            ", "number_grounding", "=", "None", "\n", "number_values", "=", "None", "\n", "", "else", ":", "\n", "            ", "if", "\"longest\"", "in", "question_tokens", "and", "num_answer", "!=", "max", "(", "number_values", "[", "0", "]", ")", ":", "\n", "                ", "number_grounding", "=", "None", "\n", "number_values", "=", "None", "\n", "", "if", "\"shortest\"", "in", "question_tokens", "and", "num_answer", "!=", "min", "(", "number_values", "[", "0", "]", ")", ":", "\n", "                ", "number_grounding", "=", "None", "\n", "number_values", "=", "None", "\n", "\n", "", "", "", "else", ":", "\n", "        ", "number_grounding", "=", "None", "\n", "number_values", "=", "None", "\n", "\n", "\n", "# if number_grounding is not None:", "\n", "#     print(tokenized_question)", "\n", "#     print(attended_tokens)", "\n", "#     print(f\"Answer: {num_answer}\")", "\n", "#     print(tokenized_passage)", "\n", "#     print(passage_num_vals)", "\n", "#     print(f\"Annotation: {number_values}\")", "\n", "#     print()", "\n", "\n", "", "return", "number_grounding", ",", "number_values", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.how_many_yards_alternate.preprocess_HowManyYardsWasThe_ques": [[315, 437], ["collections.defaultdict", "len", "dataset.items", "len", "print", "print", "print", "print", "print", "print", "tokenized_question.lower().split", "original_question.lower", "len", "len", "how_many_yards_alternate.is_num_find_question", "how_many_yards_alternate.is_num_minmax_find_question", "how_many_yards_alternate.is_num_minmax_filter_find_question", "new_qa_pairs.append", "tokenized_question.lower", "tokenized_question.lower", "tokenized_question.lower", "tokenized_question.lower", "how_many_yards_alternate.get_question_attention", "copy.deepcopy", "how_many_yards_alternate.get_number_distribution_supervision", "float"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.how_many_yards_alternate.is_num_find_question", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.how_many_yards_alternate.is_num_minmax_find_question", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.how_many_yards_alternate.is_num_minmax_filter_find_question", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.how_many_yards.get_question_attention", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.how_many_yards.get_number_distribution_supervision"], ["", "def", "preprocess_HowManyYardsWasThe_ques", "(", "dataset", ",", "ques_attn", ":", "bool", ",", "number_supervision", ":", "bool", ")", ":", "\n", "    ", "\"\"\" This function prunes questions that start with \"How many yards was\".\n        Mostly, longest, shortest style questions. We can also prune for these; look at longestshortest_ques.py\n\n        Along with pruning, we also supervise the longest/shortest/second longest/second shortest questions\n        by adding the question_type for those questions.\n\n        Currently ---\n        We prune out questions like `the second longest / shortest`.\n        Still does prune questions like `Tom Brady's second longest/shortest` infact we label them as longest/shortest\n        instead of second longest/shortest. But their size is minuscule\n\n        Question-attention\n        If the `ques_attn` flag is ON, we also add question-attention supervision\n\n    \"\"\"", "\n", "\n", "how_many_yards_was", "=", "\"how many yards was\"", "\n", "\n", "new_dataset", "=", "{", "}", "\n", "total_ques", "=", "0", "\n", "after_pruning_ques", "=", "0", "\n", "questions_w_attn", "=", "0", "\n", "questions_w_numground", "=", "0", "\n", "qtype_dist", "=", "defaultdict", "(", "int", ")", "\n", "num_passages", "=", "len", "(", "dataset", ")", "\n", "counter", "=", "1", "\n", "\n", "\n", "num_programsupervised_ques", "=", "0", "\n", "\n", "for", "passage_id", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "\n", "        ", "passage_num_mens", "=", "passage_info", "[", "constants", ".", "passage_num_mens", "]", "\n", "passage_num_entidxs", "=", "passage_info", "[", "constants", ".", "passage_num_entidx", "]", "\n", "passage_num_vals", "=", "passage_info", "[", "constants", ".", "passage_num_normalized_values", "]", "\n", "tokenized_passage", "=", "passage_info", "[", "constants", ".", "tokenized_passage", "]", "\n", "\n", "new_qa_pairs", "=", "[", "]", "\n", "for", "question_answer", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "            ", "answer", "=", "question_answer", "[", "constants", ".", "answer", "]", "\n", "\n", "total_ques", "+=", "1", "\n", "\n", "original_question", "=", "question_answer", "[", "constants", ".", "cleaned_question", "]", "\n", "tokenized_question", "=", "question_answer", "[", "constants", ".", "tokenized_question", "]", "\n", "ques_lower_tokens", "=", "tokenized_question", ".", "lower", "(", ")", ".", "split", "(", "\" \"", ")", "\n", "question_lower", "=", "original_question", ".", "lower", "(", ")", "\n", "\n", "# Keep questions that contain \"how many yards was\"", "\n", "if", "how_many_yards_was", "in", "question_lower", ":", "\n", "                ", "if", "\"second longest\"", "in", "question_lower", "or", "\"second shortest\"", "in", "question_lower", ":", "\n", "                    ", "continue", "\n", "\n", "", "qtype", "=", "None", "\n", "if", "is_num_find_question", "(", "tokenized_question", ".", "lower", "(", ")", ")", ":", "\n", "                    ", "qtype", "=", "constants", ".", "NUM_find_qtype", "\n", "", "if", "is_num_minmax_find_question", "(", "tokenized_question", ".", "lower", "(", ")", ")", ":", "\n", "                    ", "if", "\"longest\"", "in", "tokenized_question", ":", "\n", "                        ", "qtype", "=", "constants", ".", "MAX_find_qtype", "\n", "", "elif", "\"shortest\"", "in", "tokenized_question", ":", "\n", "                        ", "qtype", "=", "constants", ".", "MIN_find_qtype", "\n", "", "", "if", "is_num_minmax_filter_find_question", "(", "tokenized_question", ".", "lower", "(", ")", ")", ":", "\n", "                    ", "if", "\"longest\"", "in", "tokenized_question", ":", "\n", "                        ", "qtype", "=", "constants", ".", "MAX_filter_find_qtype", "\n", "", "elif", "\"shortest\"", "in", "tokenized_question", ":", "\n", "                        ", "qtype", "=", "constants", ".", "MIN_filter_find_qtype", "\n", "\n", "", "", "if", "qtype", "is", "not", "None", ":", "\n", "                    ", "num_programsupervised_ques", "+=", "1", "\n", "qtype_dist", "[", "qtype", "]", "+=", "1", "\n", "find_qattn", ",", "filter_qattn", "=", "get_question_attention", "(", "question_tokens", "=", "ques_lower_tokens", ",", "qtype", "=", "qtype", ")", "\n", "\n", "question_answer", "[", "constants", ".", "qtype", "]", "=", "qtype", "\n", "question_answer", "[", "constants", ".", "program_supervised", "]", "=", "True", "\n", "\n", "question_answer", "[", "constants", ".", "qattn_supervised", "]", "=", "True", "\n", "if", "filter_qattn", "is", "not", "None", ":", "\n", "                        ", "question_answer", "[", "constants", ".", "ques_attention_supervision", "]", "=", "[", "filter_qattn", ",", "find_qattn", "]", "\n", "", "else", ":", "\n", "                        ", "question_answer", "[", "constants", ".", "ques_attention_supervision", "]", "=", "[", "find_qattn", "]", "\n", "", "questions_w_attn", "+=", "1", "\n", "\n", "if", "number_supervision", "is", "True", ":", "\n", "                        ", "num_answer_str", "=", "answer", "[", "\"number\"", "]", "\n", "num_answer", "=", "float", "(", "num_answer_str", ")", "if", "num_answer_str", "else", "None", "\n", "\n", "qattn", "=", "copy", ".", "deepcopy", "(", "find_qattn", ")", "\n", "# if filter_qattn is not None:", "\n", "#     qattn = [x + y for (x, y) in zip(qattn, filter_qattn)]", "\n", "\n", "number_grounding", ",", "number_values", "=", "get_number_distribution_supervision", "(", "\n", "tokenized_question", ",", "\n", "tokenized_passage", ",", "\n", "num_answer", ",", "\n", "qattn", ",", "\n", "passage_num_mens", ",", "\n", "passage_num_entidxs", ",", "\n", "passage_num_vals", ",", "\n", ")", "\n", "if", "number_grounding", "is", "not", "None", ":", "\n", "                            ", "question_answer", "[", "constants", ".", "exection_supervised", "]", "=", "True", "\n", "question_answer", "[", "constants", ".", "qspan_numgrounding_supervision", "]", "=", "number_grounding", "\n", "question_answer", "[", "constants", ".", "qspan_numvalue_supervision", "]", "=", "number_values", "\n", "questions_w_numground", "+=", "1", "\n", "\n", "", "", "", "new_qa_pairs", ".", "append", "(", "question_answer", ")", "\n", "\n", "", "", "if", "len", "(", "new_qa_pairs", ")", ">", "0", ":", "\n", "            ", "passage_info", "[", "constants", ".", "qa_pairs", "]", "=", "new_qa_pairs", "\n", "new_dataset", "[", "passage_id", "]", "=", "passage_info", "\n", "after_pruning_ques", "+=", "len", "(", "new_qa_pairs", ")", "\n", "\n", "", "", "num_passages_after_prune", "=", "len", "(", "new_dataset", ")", "\n", "print", "(", "f\"Passages original:{num_passages}  After Pruning:{num_passages_after_prune}\"", ")", "\n", "print", "(", "f\"Questions original:{total_ques}  After pruning:{after_pruning_ques}\"", ")", "\n", "print", "(", "f\"Num of QA with attention supervised: {questions_w_attn}\"", ")", "\n", "print", "(", "f\"Num of QA with num-grounding supervised: {questions_w_numground}\"", ")", "\n", "print", "(", "f\"Number of program supervised questions: {num_programsupervised_ques}\"", ")", "\n", "print", "(", "f\"Qtype dist: {qtype_dist}\"", ")", "\n", "\n", "return", "new_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.yards_difference.readDataset": [[16, 20], ["open", "json.load"], "function", ["None"], ["def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.yards_difference.get_question_type_and_attention": [[22, 70], ["question_tokens.index", "question_tokens.index", "question_tokens.index", "question_tokens.index"], "function", ["None"], ["", "def", "get_question_type_and_attention", "(", "question_tokens", ":", "List", "[", "str", "]", ")", ":", "\n", "    ", "numbertype_to_qtype_mapping", "=", "{", "\n", "(", "\"num\"", ",", "\"num\"", ")", ":", "constants", ".", "DIFF_NUMNUM_qtype", ",", "\n", "(", "\"num\"", ",", "\"min\"", ")", ":", "constants", ".", "DIFF_NUMMIN_qtype", ",", "\n", "(", "\"num\"", ",", "\"max\"", ")", ":", "constants", ".", "DIFF_NUMMAX_qtype", ",", "\n", "(", "\"min\"", ",", "\"num\"", ")", ":", "constants", ".", "DIFF_MINNUM_qtype", ",", "\n", "(", "\"min\"", ",", "\"max\"", ")", ":", "constants", ".", "DIFF_MINMAX_qtype", ",", "\n", "(", "\"min\"", ",", "\"min\"", ")", ":", "constants", ".", "DIFF_MINMIN_qtype", ",", "\n", "(", "\"max\"", ",", "\"num\"", ")", ":", "constants", ".", "DIFF_MAXNUM_qtype", ",", "\n", "(", "\"max\"", ",", "\"min\"", ")", ":", "constants", ".", "DIFF_MAXMIN_qtype", ",", "\n", "(", "\"max\"", ",", "\"max\"", ")", ":", "constants", ".", "DIFF_MAXMAX_qtype", ",", "\n", "}", "\n", "\n", "split_point", "=", "-", "1", "\n", "if", "\"compared\"", "in", "question_tokens", ":", "\n", "        ", "split_point", "=", "question_tokens", ".", "index", "(", "\"compared\"", ")", "\n", "", "elif", "\"than\"", "in", "question_tokens", ":", "\n", "        ", "split_point", "=", "question_tokens", ".", "index", "(", "\"than\"", ")", "\n", "", "elif", "\"and\"", "in", "question_tokens", ":", "\n", "        ", "split_point", "=", "question_tokens", ".", "index", "(", "\"and\"", ")", "\n", "", "elif", "\"over\"", "in", "question_tokens", ":", "\n", "        ", "split_point", "=", "question_tokens", ".", "index", "(", "\"over\"", ")", "\n", "", "else", ":", "\n", "        ", "pass", "\n", "\n", "", "if", "split_point", "==", "-", "1", ":", "\n", "        ", "return", "None", "\n", "\n", "", "first_half_tokens", "=", "question_tokens", "[", "0", ":", "split_point", "]", "\n", "second_half_tokens", "=", "question_tokens", "[", "split_point", ":", "]", "\n", "\n", "if", "\"longest\"", "in", "first_half_tokens", ":", "\n", "        ", "first_number", "=", "\"max\"", "\n", "", "elif", "\"shortest\"", "in", "first_half_tokens", ":", "\n", "        ", "first_number", "=", "\"min\"", "\n", "", "else", ":", "\n", "        ", "first_number", "=", "\"num\"", "\n", "\n", "", "if", "\"longest\"", "in", "second_half_tokens", ":", "\n", "        ", "second_number", "=", "\"max\"", "\n", "", "elif", "\"shortest\"", "in", "second_half_tokens", ":", "\n", "        ", "second_number", "=", "\"min\"", "\n", "", "else", ":", "\n", "        ", "second_number", "=", "\"num\"", "\n", "\n", "", "qtype", "=", "numbertype_to_qtype_mapping", "[", "(", "first_number", ",", "second_number", ")", "]", "\n", "\n", "return", "qtype", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.yards_difference.preprocess_HowManyYardsDifference_ques": [[72, 122], ["collections.defaultdict", "len", "dataset.items", "len", "print", "print", "print", "print", "print", "original_question.lower", "any", "len", "len", "new_qa_pairs.append"], "function", ["None"], ["", "def", "preprocess_HowManyYardsDifference_ques", "(", "dataset", ")", ":", "\n", "    ", "\"\"\" This function prunes questions that start with \"How many yards longer was\" and \"How many yards difference\"\n\n        Also add qtype for program supervision\n    \"\"\"", "\n", "\n", "difference_ngrams", "=", "[", "\"how many yards difference\"", ",", "\"how many yards longer was\"", "]", "\n", "\n", "new_dataset", "=", "{", "}", "\n", "total_ques", "=", "0", "\n", "after_pruning_ques", "=", "0", "\n", "questions_w_qtypes", "=", "0", "\n", "qtype_dist", "=", "defaultdict", "(", "int", ")", "\n", "num_passages", "=", "len", "(", "dataset", ")", "\n", "\n", "missed_questions", "=", "0", "\n", "\n", "for", "passage_id", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "new_qa_pairs", "=", "[", "]", "\n", "for", "question_answer", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "            ", "total_ques", "+=", "1", "\n", "\n", "original_question", "=", "question_answer", "[", "constants", ".", "tokenized_question", "]", "\n", "question_lower", "=", "original_question", ".", "lower", "(", ")", "\n", "\n", "if", "any", "(", "span", "in", "question_lower", "for", "span", "in", "difference_ngrams", ")", ":", "\n", "# qtype = get_question_type_and_attention(question_lower.split(\" \"))", "\n", "# if qtype is not None:", "\n", "#     question_answer[constants.qtype] = qtype", "\n", "#     question_answer[constants.program_supervised] = True", "\n", "#     qtype_dist[qtype] += 1", "\n", "#     questions_w_qtypes += 1", "\n", "# else:", "\n", "#     missed_questions += 1", "\n", "\n", "                ", "new_qa_pairs", ".", "append", "(", "question_answer", ")", "\n", "\n", "", "", "if", "len", "(", "new_qa_pairs", ")", ">", "0", ":", "\n", "            ", "passage_info", "[", "constants", ".", "qa_pairs", "]", "=", "new_qa_pairs", "\n", "new_dataset", "[", "passage_id", "]", "=", "passage_info", "\n", "after_pruning_ques", "+=", "len", "(", "new_qa_pairs", ")", "\n", "\n", "", "", "num_passages_after_prune", "=", "len", "(", "new_dataset", ")", "\n", "print", "(", "f\"Passages original:{num_passages}  After Pruning:{num_passages_after_prune}\"", ")", "\n", "print", "(", "f\"Questions original:{total_ques}  After pruning:{after_pruning_ques}\"", ")", "\n", "print", "(", "f\"Num of QA with qtypes and program supervised: {questions_w_qtypes}\"", ")", "\n", "print", "(", "f\"Qtype dist: {qtype_dist}\"", ")", "\n", "print", "(", "f\"Missed questions: {missed_questions}\"", ")", "\n", "\n", "return", "new_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques.readDataset": [[30, 34], ["open", "json.load"], "function", ["None"], ["def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques.filter_questionattention": [[36, 114], ["question_lower.split", "len"], "function", ["None"], ["", "def", "filter_questionattention", "(", "tokenized_queslower", ":", "str", ")", ":", "\n", "    ", "\"\"\" Here we'll annotate questions with one/two attentions depending on if the program type is\n        1. find(QuestionAttention)\n        2. filter(QuestionAttention, find(QuestionAttention))\n    \"\"\"", "\n", "question_lower", "=", "tokenized_queslower", "\n", "question_tokens", ":", "List", "[", "str", "]", "=", "question_lower", ".", "split", "(", "\" \"", ")", "\n", "qlen", "=", "len", "(", "question_tokens", ")", "\n", "\n", "if", "\"how many field goals were\"", "in", "question_lower", ":", "\n", "# Non-filter question", "\n", "        ", "if", "question_lower", "in", "[", "\n", "\"how many field goals were kicked ?\"", ",", "\n", "\"how many field goals were kicked in the game ?\"", ",", "\n", "\"how many field goals were made ?\"", ",", "\n", "\"how many field goals were made in the game ?\"", ",", "\n", "\"how many field goals were scored ?\"", ",", "\n", "\"how many field goals were scored in the game ?\"", ",", "\n", "\"how many field goals were in the game ?\"", ",", "\n", "\"how many field goals were made in this game ?\"", ",", "\n", "\"how many field goals were in this game?\"", ",", "\n", "]", ":", "\n", "            ", "qtype", "=", "constants", ".", "COUNT_find_qtype", "\n", "question_attention_find", "=", "[", "2", ",", "3", "]", "# Inclusive", "\n", "question_attention_filter", "=", "None", "\n", "\n", "", "else", ":", "\n", "# QAttn1 (filter) is everything after \"how many f gs were\" until ?. QAttn2 (find) is F Gs, i.e. [2, 3]", "\n", "            ", "qtype", "=", "constants", ".", "COUNT_filter_find_qtype", "\n", "question_attention_filter", "=", "[", "5", ",", "qlen", "-", "2", "]", "# skipping first 5 tokens and ?", "\n", "question_attention_find", "=", "[", "2", ",", "3", "]", "\n", "\n", "", "", "elif", "\"how many field goals did\"", "in", "question_lower", ":", "\n", "        ", "qtype", "=", "constants", ".", "COUNT_filter_find_qtype", "\n", "question_attention_find", "=", "[", "2", ",", "3", "]", "\n", "question_attention_filter", "=", "[", "5", ",", "qlen", "-", "2", "]", "# skipping first 5 tokens and ?", "\n", "\n", "", "elif", "\"how many interceptions did\"", "in", "question_lower", ":", "\n", "        ", "qtype", "=", "constants", ".", "COUNT_filter_find_qtype", "\n", "question_attention_find", "=", "[", "2", ",", "2", "]", "\n", "question_attention_filter", "=", "[", "4", ",", "qlen", "-", "2", "]", "# skipping first 4 tokens and ?", "\n", "\n", "", "elif", "\"how many passes\"", "in", "question_lower", ":", "\n", "        ", "qtype", "=", "constants", ".", "COUNT_filter_find_qtype", "\n", "question_attention_find", "=", "[", "2", ",", "2", "]", "\n", "question_attention_filter", "=", "[", "4", ",", "qlen", "-", "2", "]", "# skipping first 4 tokens and ?", "\n", "\n", "", "elif", "\"how many rushing\"", "in", "question_lower", ":", "\n", "# Most questions are How many rushing touchdowns/yards were / did", "\n", "        ", "qtype", "=", "constants", ".", "COUNT_filter_find_qtype", "\n", "question_attention_find", "=", "[", "2", ",", "3", "]", "\n", "question_attention_filter", "=", "[", "5", ",", "qlen", "-", "2", "]", "# skipping first 5 tokens and ?", "\n", "\n", "", "elif", "\"how many touchdown passes did\"", "in", "question_lower", ":", "\n", "        ", "qtype", "=", "constants", ".", "COUNT_filter_find_qtype", "\n", "question_attention_find", "=", "[", "2", ",", "3", "]", "\n", "question_attention_filter", "=", "[", "5", ",", "qlen", "-", "2", "]", "# skipping first 5 tokens and ?", "\n", "\n", "", "elif", "\"how many touchdowns did the\"", "in", "question_lower", ":", "\n", "        ", "qtype", "=", "constants", ".", "COUNT_filter_find_qtype", "\n", "question_attention_find", "=", "[", "2", ",", "2", "]", "\n", "question_attention_filter", "=", "[", "5", ",", "qlen", "-", "2", "]", "# skipping first 5 tokens and ?", "\n", "\n", "", "elif", "\"how many touchdowns were scored\"", "in", "question_lower", ":", "\n", "        ", "if", "question_lower", "in", "[", "\n", "\"how many touchdowns were scored in the game ?\"", ",", "\n", "\"how many touchdowns were scored ?\"", ",", "\n", "\"how many touchdowns were scored in total ?\"", ",", "\n", "]", ":", "\n", "            ", "qtype", "=", "constants", ".", "COUNT_find_qtype", "\n", "question_attention_find", "=", "[", "2", ",", "2", "]", "# Inclusive", "\n", "question_attention_filter", "=", "None", "\n", "", "else", ":", "\n", "            ", "qtype", "=", "constants", ".", "COUNT_filter_find_qtype", "\n", "question_attention_find", "=", "[", "2", ",", "2", "]", "\n", "question_attention_filter", "=", "[", "5", ",", "qlen", "-", "2", "]", "# skipping first 5 tokens and ?", "\n", "\n", "", "", "return", "qtype", ",", "question_attention_filter", ",", "question_attention_find", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques.convert_span_to_attention": [[116, 123], ["range"], "function", ["None"], ["", "def", "convert_span_to_attention", "(", "qlen", ",", "span", ")", ":", "\n", "# span is inclusive on both ends", "\n", "    ", "qattn", "=", "[", "0.0", "]", "*", "qlen", "\n", "for", "x", "in", "range", "(", "span", "[", "0", "]", ",", "span", "[", "1", "]", "+", "1", ")", ":", "\n", "        ", "qattn", "[", "x", "]", "=", "1.0", "\n", "\n", "", "return", "qattn", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques.preprocess_HowManyYardsCount_ques": [[125, 186], ["len", "collections.defaultdict", "dataset.items", "len", "print", "print", "print", "print", "original_question.lower", "tokenized_ques.split", "len", "any", "len", "len", "count_ques.filter_questionattention", "count_ques.convert_span_to_attention", "new_qa_pairs.append", "count_ques.convert_span_to_attention", "tokenized_ques.lower"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques.filter_questionattention", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques.convert_span_to_attention", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques.convert_span_to_attention"], ["", "def", "preprocess_HowManyYardsCount_ques", "(", "dataset", ")", ":", "\n", "    ", "\"\"\" This function prunes for questions that are count based questions.\n\n        Along with pruning, we also supervise the with the qtype and program_supervised flag\n    \"\"\"", "\n", "\n", "new_dataset", "=", "{", "}", "\n", "total_ques", "=", "0", "\n", "after_pruning_ques", "=", "0", "\n", "questions_w_attn", "=", "0", "\n", "num_passages", "=", "len", "(", "dataset", ")", "\n", "qtype_dist", "=", "defaultdict", "(", "int", ")", "\n", "\n", "for", "passage_id", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "new_qa_pairs", "=", "[", "]", "\n", "for", "question_answer", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "            ", "total_ques", "+=", "1", "\n", "\n", "original_question", "=", "question_answer", "[", "constants", ".", "cleaned_question", "]", "\n", "question_lower", "=", "original_question", ".", "lower", "(", ")", "\n", "tokenized_ques", "=", "question_answer", "[", "constants", ".", "tokenized_question", "]", "\n", "tokens", "=", "tokenized_ques", ".", "split", "(", "\" \"", ")", "\n", "qlen", "=", "len", "(", "tokens", ")", "\n", "if", "any", "(", "span", "in", "question_lower", "for", "span", "in", "COUNT_TRIGRAMS", ")", ":", "\n", "                ", "(", "qtype", ",", "question_attention_filter_span", ",", "question_attention_find_span", ")", "=", "filter_questionattention", "(", "\n", "tokenized_queslower", "=", "tokenized_ques", ".", "lower", "(", ")", "\n", ")", "\n", "\n", "if", "question_attention_filter_span", "is", "not", "None", ":", "\n", "                    ", "filter_qattn", "=", "convert_span_to_attention", "(", "qlen", ",", "question_attention_filter_span", ")", "\n", "", "else", ":", "\n", "                    ", "filter_qattn", "=", "None", "\n", "\n", "", "find_qattn", "=", "convert_span_to_attention", "(", "qlen", ",", "question_attention_find_span", ")", "\n", "\n", "question_answer", "[", "constants", ".", "qtype", "]", "=", "qtype", "\n", "question_answer", "[", "constants", ".", "program_supervised", "]", "=", "True", "\n", "qtype_dist", "[", "qtype", "]", "+=", "1", "\n", "\n", "# Also adding qattn -- everything apart from the first two tokens", "\n", "if", "filter_qattn", "is", "not", "None", ":", "\n", "                    ", "question_answer", "[", "constants", ".", "ques_attention_supervision", "]", "=", "[", "filter_qattn", ",", "find_qattn", "]", "\n", "", "else", ":", "\n", "                    ", "question_answer", "[", "constants", ".", "ques_attention_supervision", "]", "=", "[", "find_qattn", "]", "\n", "", "question_answer", "[", "constants", ".", "qattn_supervised", "]", "=", "True", "\n", "questions_w_attn", "+=", "1", "\n", "\n", "new_qa_pairs", ".", "append", "(", "question_answer", ")", "\n", "\n", "", "", "if", "len", "(", "new_qa_pairs", ")", ">", "0", ":", "\n", "            ", "passage_info", "[", "constants", ".", "qa_pairs", "]", "=", "new_qa_pairs", "\n", "new_dataset", "[", "passage_id", "]", "=", "passage_info", "\n", "after_pruning_ques", "+=", "len", "(", "new_qa_pairs", ")", "\n", "\n", "", "", "num_passages_after_prune", "=", "len", "(", "new_dataset", ")", "\n", "print", "(", "f\"Passages original:{num_passages}  Questions original:{total_ques}\"", ")", "\n", "print", "(", "f\"Passages after-pruning:{num_passages_after_prune}  Question after-pruning:{after_pruning_ques}\"", ")", "\n", "print", "(", "f\"Ques with attn: {questions_w_attn}\"", ")", "\n", "print", "(", "f\"QType distribution: {qtype_dist}\"", ")", "\n", "\n", "return", "new_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.readDataset": [[72, 76], ["open", "json.load"], "function", ["None"], ["def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.is_count_find_question": [[78, 84], ["COUNT_FIND_REGEX.fullmatch"], "function", ["None"], ["", "def", "is_count_find_question", "(", "tokenized_question_lower", ":", "str", ")", ":", "\n", "    ", "match_result", "=", "COUNT_FIND_REGEX", ".", "fullmatch", "(", "tokenized_question_lower", ")", "\n", "if", "match_result", "is", "not", "None", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.is_count_filter_find_question": [[86, 92], ["COUNT_FILTER_FIND_REGEX.fullmatch"], "function", ["None"], ["", "", "def", "is_count_filter_find_question", "(", "tokenized_question_lower", ":", "str", ")", ":", "\n", "    ", "match_result", "=", "COUNT_FILTER_FIND_REGEX", ".", "fullmatch", "(", "tokenized_question_lower", ")", "\n", "if", "match_result", "is", "not", "None", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.convert_start_end_to_attention_vector": [[94, 99], ["None"], "function", ["None"], ["", "", "def", "convert_start_end_to_attention_vector", "(", "length", ",", "start", ",", "end", ")", ":", "\n", "    ", "\"\"\"Convert start/end of a span into a binary vector. start/end is inclusive/exclusive.\"\"\"", "\n", "attention_vector", "=", "[", "0.0", "]", "*", "length", "\n", "attention_vector", "[", "start", ":", "end", "]", "=", "[", "1.0", "]", "*", "(", "end", "-", "start", ")", "\n", "return", "attention_vector", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.get_count_find_question_attention": [[101, 112], ["len", "count_ques_alternate.convert_start_end_to_attention_vector"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.convert_start_end_to_attention_vector"], ["", "def", "get_count_find_question_attention", "(", "question_tokens", ":", "List", "[", "str", "]", ")", ":", "\n", "    ", "\"\"\"Compute the find question attention for count-find questions.\n\n    These are of the kind \"how many field goals did \\w+ \\w+ \\w+ \\?\" where every token after \"how many\" is find-arg\n    \"\"\"", "\n", "len_question", "=", "len", "(", "question_tokens", ")", "\n", "find_start", "=", "2", "\n", "find_end", "=", "len_question", "-", "1", "# exclusive, don't attend to ?", "\n", "find_attention_vector", "=", "convert_start_end_to_attention_vector", "(", "len_question", ",", "find_start", ",", "\n", "find_end", ")", "\n", "return", "find_attention_vector", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.get_count_filter_find_question_attention": [[114, 132], ["len", "count_ques_alternate.convert_start_end_to_attention_vector", "count_ques_alternate.convert_start_end_to_attention_vector"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.convert_start_end_to_attention_vector", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.convert_start_end_to_attention_vector"], ["", "def", "get_count_filter_find_question_attention", "(", "question_tokens", ":", "List", "[", "str", "]", ")", ":", "\n", "    ", "\"\"\"Compute the find question attention for count-find questions.\n\n    These are of the kind \"how many passes did \\w+ \\w+ in/during the \\w+ \\w+ \\?\"\n        filter_attention: last 3 tokens \"the \\w+ \\w+\"\n        find_attention: token after \"how many\" and before \"in/during\"\n    \"\"\"", "\n", "len_question", "=", "len", "(", "question_tokens", ")", "\n", "filter_start", "=", "len_question", "-", "4", "# inclusive, the last 3 tokens excluding \"?\"", "\n", "filter_end", "=", "len_question", "-", "1", "# exclusive, last token before \"?\"", "\n", "\n", "find_end", "=", "filter_start", "-", "1", "# exclusive, need to skip \"in\" before filter-start but only -1 since end is exclusive", "\n", "#  and start is inclusive", "\n", "find_start", "=", "2", "# inclusive, token after \"how many\"", "\n", "\n", "find_attention_vector", "=", "convert_start_end_to_attention_vector", "(", "len_question", ",", "find_start", ",", "find_end", ")", "\n", "filter_attention_vector", "=", "convert_start_end_to_attention_vector", "(", "len_question", ",", "filter_start", ",", "filter_end", ")", "\n", "return", "filter_attention_vector", ",", "find_attention_vector", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.preprocess_HowManyYardsCount_ques": [[134, 189], ["len", "collections.defaultdict", "dataset.items", "len", "print", "print", "print", "original_question.lower", "tokenized_ques.split", "any", "len", "len", "count_ques_alternate.is_count_find_question", "count_ques_alternate.is_count_filter_find_question", "new_qa_pairs.append", "tokenized_ques.lower", "count_ques_alternate.get_count_find_question_attention", "tokenized_ques.lower", "count_ques_alternate.get_count_filter_find_question_attention"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.is_count_find_question", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.is_count_filter_find_question", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.get_count_find_question_attention", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.count_ques_alternate.get_count_filter_find_question_attention"], ["", "def", "preprocess_HowManyYardsCount_ques", "(", "dataset", ")", ":", "\n", "    ", "\"\"\" This function prunes for questions that are count based questions.\n\n        Along with pruning, we also supervise the with the qtype and program_supervised flag\n    \"\"\"", "\n", "\n", "new_dataset", "=", "{", "}", "\n", "total_ques", "=", "0", "\n", "after_pruning_ques", "=", "0", "\n", "num_passages", "=", "len", "(", "dataset", ")", "\n", "prog_dist", "=", "defaultdict", "(", "int", ")", "\n", "\n", "for", "passage_id", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "new_qa_pairs", "=", "[", "]", "\n", "for", "question_answer", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "            ", "total_ques", "+=", "1", "\n", "\n", "original_question", "=", "question_answer", "[", "constants", ".", "cleaned_question", "]", "\n", "question_lower", "=", "original_question", ".", "lower", "(", ")", "\n", "tokenized_ques", "=", "question_answer", "[", "constants", ".", "tokenized_question", "]", "\n", "question_tokens", "=", "tokenized_ques", ".", "split", "(", "\" \"", ")", "\n", "if", "any", "(", "span", "in", "question_lower", "for", "span", "in", "COUNT_TRIGRAMS", ")", ":", "\n", "                ", "is_count_find", "=", "is_count_find_question", "(", "tokenized_ques", ".", "lower", "(", ")", ")", "\n", "if", "is_count_find", ":", "\n", "                    ", "qtype", "=", "constants", ".", "COUNT_find_qtype", "\n", "question_answer", "[", "constants", ".", "qtype", "]", "=", "qtype", "\n", "question_answer", "[", "constants", ".", "program_supervised", "]", "=", "True", "\n", "find_qattn", "=", "get_count_find_question_attention", "(", "question_tokens", ")", "\n", "question_answer", "[", "constants", ".", "ques_attention_supervision", "]", "=", "[", "find_qattn", "]", "\n", "question_answer", "[", "constants", ".", "qattn_supervised", "]", "=", "True", "\n", "prog_dist", "[", "qtype", "]", "+=", "1", "\n", "\n", "", "is_count_filter_find", "=", "is_count_filter_find_question", "(", "tokenized_ques", ".", "lower", "(", ")", ")", "\n", "if", "is_count_filter_find", ":", "\n", "                    ", "qtype", "=", "constants", ".", "COUNT_filter_find_qtype", "\n", "question_answer", "[", "constants", ".", "qtype", "]", "=", "qtype", "\n", "question_answer", "[", "constants", ".", "program_supervised", "]", "=", "True", "\n", "filter_qattn", ",", "find_qattn", "=", "get_count_filter_find_question_attention", "(", "question_tokens", ")", "\n", "question_answer", "[", "constants", ".", "ques_attention_supervision", "]", "=", "[", "filter_qattn", ",", "find_qattn", "]", "\n", "question_answer", "[", "constants", ".", "qattn_supervised", "]", "=", "True", "\n", "prog_dist", "[", "qtype", "]", "+=", "1", "\n", "\n", "", "new_qa_pairs", ".", "append", "(", "question_answer", ")", "\n", "\n", "", "", "if", "len", "(", "new_qa_pairs", ")", ">", "0", ":", "\n", "            ", "passage_info", "[", "constants", ".", "qa_pairs", "]", "=", "new_qa_pairs", "\n", "new_dataset", "[", "passage_id", "]", "=", "passage_info", "\n", "after_pruning_ques", "+=", "len", "(", "new_qa_pairs", ")", "\n", "\n", "", "", "num_passages_after_prune", "=", "len", "(", "new_dataset", ")", "\n", "print", "(", "f\"Passages original:{num_passages}  Questions original:{total_ques}\"", ")", "\n", "print", "(", "f\"Passages after-pruning:{num_passages_after_prune}  Question after-pruning:{after_pruning_ques}\"", ")", "\n", "print", "(", "f\"Program distribution: {prog_dist}\"", ")", "\n", "\n", "return", "new_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.how_many_yards.readDataset": [[17, 21], ["open", "json.load"], "function", ["None"], ["def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.how_many_yards.get_number_distribution_supervision": [[46, 140], ["tokenized_passage.split", "tokenized_question.split", "set", "enumerate", "set.remove", "set.add", "set.remove", "set.add", "set.remove", "set.add", "max", "min", "set", "set.intersection", "set", "zip", "set.remove", "len", "set.remove", "set.add", "set.remove", "set.add", "set.remove", "set.add", "relevant_number_tokenidxs.append", "relevant_number_entidxs.append", "relevant_number_values.append", "len", "set.add", "list"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add"], ["def", "get_number_distribution_supervision", "(", "\n", "tokenized_question", ",", "\n", "tokenized_passage", ",", "\n", "num_answer", ",", "\n", "attention", ",", "\n", "passage_num_mens", ",", "\n", "passage_num_entidxs", ",", "\n", "passage_num_vals", ",", "\n", ")", ":", "\n", "    ", "WINDOW", "=", "10", "\n", "passage_tokens", "=", "tokenized_passage", ".", "split", "(", "\" \"", ")", "\n", "question_tokens", "=", "tokenized_question", ".", "split", "(", "\" \"", ")", "\n", "\n", "# Only supervised longest / shortest questions -- cannot do the first / last kind of questions", "\n", "if", "\"longest\"", "not", "in", "question_tokens", "and", "\"shortest\"", "not", "in", "question_tokens", ":", "\n", "        ", "return", "None", ",", "None", "\n", "", "if", "num_answer", "is", "None", ":", "\n", "        ", "return", "None", ",", "None", "\n", "\n", "# These are the relevant tokens in the question. We'd like to find numbers that are surrounded by these tokens", "\n", "", "attended_tokens", "=", "[", "token", "for", "att", ",", "token", "in", "zip", "(", "attention", ",", "question_tokens", ")", "if", "att", ">", "0", "]", "\n", "attended_tokens", "=", "set", "(", "attended_tokens", ")", "\n", "# Replacing TD with touchdown", "\n", "if", "\"TD\"", "in", "attended_tokens", ":", "\n", "        ", "attended_tokens", ".", "remove", "(", "\"TD\"", ")", "\n", "attended_tokens", ".", "add", "(", "\"touchdown\"", ")", "\n", "", "if", "\"goals\"", "in", "attended_tokens", ":", "\n", "        ", "attended_tokens", ".", "remove", "(", "\"goals\"", ")", "\n", "attended_tokens", ".", "add", "(", "\"goal\"", ")", "\n", "", "if", "\"touchdowns\"", "in", "attended_tokens", ":", "\n", "        ", "attended_tokens", ".", "remove", "(", "\"touchdowns\"", ")", "\n", "attended_tokens", ".", "add", "(", "\"touchdown\"", ")", "\n", "", "irrelevant_tokens", "=", "[", "\"'\"", ",", "\"'s\"", ",", "\"of\"", ",", "\"the\"", ",", "\"game\"", ",", "\"games\"", ",", "\"in\"", "]", "\n", "# Remove irrelevant tokens from attended-tokens", "\n", "for", "t", "in", "irrelevant_tokens", ":", "\n", "        ", "if", "t", "in", "attended_tokens", ":", "\n", "            ", "attended_tokens", ".", "remove", "(", "t", ")", "\n", "\n", "# Num of passage number tokens", "\n", "", "", "number_token_idxs", "=", "[", "x", "for", "(", "_", ",", "x", ",", "_", ")", "in", "passage_num_mens", "]", "\n", "\n", "relevant_number_tokenidxs", "=", "[", "]", "\n", "relevant_number_entidxs", "=", "[", "]", "\n", "relevant_number_values", "=", "[", "]", "\n", "\n", "for", "menidx", ",", "number_token_idx", "in", "enumerate", "(", "number_token_idxs", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "if", "passage_tokens", "[", "number_token_idx", "+", "1", "]", "!=", "\"-\"", "or", "passage_tokens", "[", "number_token_idx", "+", "2", "]", "!=", "\"yard\"", ":", "\n", "                ", "continue", "\n", "", "", "except", ":", "\n", "            ", "continue", "\n", "", "starting_tokenidx", "=", "max", "(", "0", ",", "number_token_idx", "-", "WINDOW", ")", "# Inclusive", "\n", "ending_tokenidx", "=", "min", "(", "len", "(", "passage_tokens", ")", ",", "number_token_idx", "+", "WINDOW", "+", "1", ")", "# Exclusive", "\n", "surrounding_passage_tokens", "=", "set", "(", "passage_tokens", "[", "starting_tokenidx", ":", "ending_tokenidx", "]", ")", "\n", "if", "\"TD\"", "in", "surrounding_passage_tokens", ":", "\n", "            ", "surrounding_passage_tokens", ".", "remove", "(", "\"TD\"", ")", "\n", "surrounding_passage_tokens", ".", "add", "(", "\"touchdown\"", ")", "\n", "", "if", "\"goals\"", "in", "surrounding_passage_tokens", ":", "\n", "            ", "surrounding_passage_tokens", ".", "remove", "(", "\"goals\"", ")", "\n", "surrounding_passage_tokens", ".", "add", "(", "\"goal\"", ")", "\n", "", "if", "\"touchdowns\"", "in", "surrounding_passage_tokens", ":", "\n", "            ", "surrounding_passage_tokens", ".", "remove", "(", "\"touchdowns\"", ")", "\n", "surrounding_passage_tokens", ".", "add", "(", "\"touchdown\"", ")", "\n", "", "intersection_tokens", "=", "surrounding_passage_tokens", ".", "intersection", "(", "attended_tokens", ")", "\n", "if", "intersection_tokens", "==", "attended_tokens", ":", "\n", "            ", "relevant_number_tokenidxs", ".", "append", "(", "number_token_idx", ")", "\n", "relevant_number_entidxs", ".", "append", "(", "passage_num_entidxs", "[", "menidx", "]", ")", "\n", "relevant_number_values", ".", "append", "(", "passage_num_vals", "[", "passage_num_entidxs", "[", "menidx", "]", "]", ")", "\n", "\n", "", "", "if", "relevant_number_entidxs", ":", "\n", "        ", "number_grounding", "=", "[", "0", "]", "*", "len", "(", "passage_num_vals", ")", "\n", "number_values", "=", "set", "(", ")", "\n", "for", "entidx", "in", "relevant_number_entidxs", ":", "\n", "            ", "number_grounding", "[", "entidx", "]", "=", "1", "\n", "number_values", ".", "add", "(", "passage_num_vals", "[", "entidx", "]", ")", "\n", "", "number_grounding", "=", "[", "number_grounding", "]", "\n", "number_values", "=", "[", "list", "(", "number_values", ")", "]", "\n", "if", "num_answer", "not", "in", "number_values", "[", "0", "]", ":", "# It's now a list", "\n", "            ", "number_grounding", "=", "None", "\n", "number_values", "=", "None", "\n", "\n", "", "", "else", ":", "\n", "        ", "number_grounding", "=", "None", "\n", "number_values", "=", "None", "\n", "\n", "# print(tokenized_question)", "\n", "# print(attended_tokens)", "\n", "# print(f\"Answer: {num_answer}\")", "\n", "# print(tokenized_passage)", "\n", "# print(passage_num_vals)", "\n", "# print(number_values)", "\n", "# print()", "\n", "\n", "", "return", "number_grounding", ",", "number_values", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.how_many_yards.get_question_attention": [[142, 194], ["len", "enumerate", "sum", "sum"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "def", "get_question_attention", "(", "question_tokens", ":", "str", ")", ":", "\n", "    ", "tokens_with_find_attention", "=", "[", "\n", "\"touchdown\"", ",", "\n", "\"run\"", ",", "\n", "\"pass\"", ",", "\n", "\"field\"", ",", "\n", "\"goal\"", ",", "\n", "\"passing\"", ",", "\n", "\"TD\"", ",", "\n", "\"td\"", ",", "\n", "\"rushing\"", ",", "\n", "\"kick\"", ",", "\n", "\"scoring\"", ",", "\n", "\"drive\"", ",", "\n", "\"touchdowns\"", ",", "\n", "\"reception\"", ",", "\n", "\"interception\"", ",", "\n", "\"return\"", ",", "\n", "\"goals\"", ",", "\n", "]", "\n", "tokens_with_no_attention", "=", "[", "\n", "\"how\"", ",", "\n", "\"How\"", ",", "\n", "\"many\"", ",", "\n", "\"yards\"", ",", "\n", "\"was\"", ",", "\n", "\"the\"", ",", "\n", "\"longest\"", ",", "\n", "\"shortest\"", ",", "\n", "\"?\"", ",", "\n", "\"of\"", ",", "\n", "\"in\"", ",", "\n", "\"game\"", ",", "\n", "]", "\n", "qlen", "=", "len", "(", "question_tokens", ")", "\n", "find_qattn", "=", "[", "0.0", "]", "*", "qlen", "\n", "filter_qattn", "=", "[", "0.0", "]", "*", "qlen", "\n", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "question_tokens", ")", ":", "\n", "        ", "if", "token", "in", "tokens_with_no_attention", ":", "\n", "            ", "continue", "\n", "", "if", "token", "in", "tokens_with_find_attention", ":", "\n", "            ", "find_qattn", "[", "i", "]", "=", "1.0", "\n", "", "else", ":", "\n", "            ", "filter_qattn", "[", "i", "]", "=", "1.0", "\n", "\n", "", "", "if", "sum", "(", "find_qattn", ")", "==", "0", ":", "\n", "        ", "find_qattn", "=", "None", "\n", "", "if", "sum", "(", "filter_qattn", ")", "==", "0", ":", "\n", "        ", "filter_qattn", "=", "None", "\n", "\n", "", "return", "find_qattn", ",", "filter_qattn", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.how_many_yards.qtype_from_findfilter_maxminnum": [[196, 224], ["None"], "function", ["None"], ["", "def", "qtype_from_findfilter_maxminnum", "(", "find_or_filter", ",", "longest_shortest_or_num", ")", ":", "\n", "    ", "if", "longest_shortest_or_num", "==", "\"longest\"", ":", "\n", "        ", "if", "find_or_filter", "==", "\"find\"", ":", "\n", "            ", "qtype", "=", "constants", ".", "MAX_find_qtype", "\n", "", "elif", "find_or_filter", "==", "\"filter\"", ":", "\n", "            ", "qtype", "=", "constants", ".", "MAX_filter_find_qtype", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "", "", "elif", "longest_shortest_or_num", "==", "\"shortest\"", ":", "\n", "        ", "if", "find_or_filter", "==", "\"find\"", ":", "\n", "            ", "qtype", "=", "constants", ".", "MIN_find_qtype", "\n", "", "elif", "find_or_filter", "==", "\"filter\"", ":", "\n", "            ", "qtype", "=", "constants", ".", "MIN_filter_find_qtype", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "", "", "elif", "longest_shortest_or_num", "==", "\"num\"", ":", "\n", "        ", "if", "find_or_filter", "==", "\"find\"", ":", "\n", "            ", "qtype", "=", "constants", ".", "NUM_find_qtype", "\n", "", "elif", "find_or_filter", "==", "\"filter\"", ":", "\n", "            ", "qtype", "=", "constants", ".", "NUM_filter_find_qtype", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n", "", "return", "qtype", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.how_many_yards.preprocess_HowManyYardsWasThe_ques": [[226, 364], ["collections.defaultdict", "len", "dataset.items", "len", "print", "print", "print", "print", "print", "print", "tokenized_question.lower().split", "original_question.lower", "len", "len", "how_many_yards.get_question_attention", "how_many_yards.qtype_from_findfilter_maxminnum", "new_qa_pairs.append", "tokenized_question.lower", "copy.deepcopy", "how_many_yards.get_number_distribution_supervision", "float", "zip"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.how_many_yards.get_question_attention", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.how_many_yards.qtype_from_findfilter_maxminnum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.how_many_yards.get_number_distribution_supervision"], ["", "def", "preprocess_HowManyYardsWasThe_ques", "(", "dataset", ",", "ques_attn", ":", "bool", ",", "number_supervision", ":", "bool", ")", ":", "\n", "    ", "\"\"\" This function prunes questions that start with \"How many yards was\".\n        Mostly, longest, shortest style questions. We can also prune for these; look at longestshortest_ques.py\n\n        Along with pruning, we also supervise the longest/shortest/second longest/second shortest questions\n        by adding the question_type for those questions.\n\n        Currently ---\n        We prune out questions like `the second longest / shortest`.\n        Still does prune questions like `Tom Brady's second longest/shortest` infact we label them as longest/shortest\n        instead of second longest/shortest. But their size is minuscule\n\n        Question-attention\n        If the `ques_attn` flag is ON, we also add question-attention supervision\n\n    \"\"\"", "\n", "\n", "how_many_yards_was", "=", "\"how many yards was\"", "\n", "\n", "new_dataset", "=", "{", "}", "\n", "total_ques", "=", "0", "\n", "after_pruning_ques", "=", "0", "\n", "questions_w_qtypes", "=", "0", "\n", "questions_w_attn", "=", "0", "\n", "questions_w_numground", "=", "0", "\n", "qtype_dist", "=", "defaultdict", "(", "int", ")", "\n", "num_passages", "=", "len", "(", "dataset", ")", "\n", "counter", "=", "1", "\n", "\n", "for", "passage_id", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "\n", "        ", "passage_num_mens", "=", "passage_info", "[", "constants", ".", "passage_num_mens", "]", "\n", "passage_num_entidxs", "=", "passage_info", "[", "constants", ".", "passage_num_entidx", "]", "\n", "passage_num_vals", "=", "passage_info", "[", "constants", ".", "passage_num_normalized_values", "]", "\n", "tokenized_passage", "=", "passage_info", "[", "constants", ".", "tokenized_passage", "]", "\n", "\n", "new_qa_pairs", "=", "[", "]", "\n", "for", "question_answer", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "            ", "answer", "=", "question_answer", "[", "constants", ".", "answer", "]", "\n", "\n", "total_ques", "+=", "1", "\n", "\n", "original_question", "=", "question_answer", "[", "constants", ".", "cleaned_question", "]", "\n", "tokenized_question", "=", "question_answer", "[", "constants", ".", "tokenized_question", "]", "\n", "ques_lower_tokens", "=", "tokenized_question", ".", "lower", "(", ")", ".", "split", "(", "\" \"", ")", "\n", "question_lower", "=", "original_question", ".", "lower", "(", ")", "\n", "\n", "# Keep questions that contain \"how many yards was\"", "\n", "if", "how_many_yards_was", "in", "question_lower", ":", "\n", "\n", "                ", "if", "\"second longest\"", "in", "question_lower", "or", "\"second shortest\"", "in", "question_lower", ":", "\n", "                    ", "continue", "\n", "\n", "# Rest of the questions can be of these kinds:", "\n", "# 1. Find or Filter(Find)", "\n", "# 2. Longest/Shortest/FindNum", "\n", "\n", "# We will find the ques-attentions for find vs. filter", "\n", "# Using the existence of longest / shortest word we can figure out between Max/Min/Num", "\n", "\n", "", "find_qattn", ",", "filter_qattn", "=", "get_question_attention", "(", "question_tokens", "=", "ques_lower_tokens", ")", "\n", "\n", "find_or_filter", "=", "None", "\n", "if", "find_qattn", "is", "None", "and", "filter_qattn", "is", "None", ":", "\n", "                    ", "pass", "\n", "", "elif", "find_qattn", "is", "None", ":", "\n", "                    ", "find_qattn", "=", "filter_qattn", "\n", "filter_qattn", "=", "None", "\n", "find_or_filter", "=", "\"find\"", "\n", "", "elif", "filter_qattn", "is", "None", ":", "\n", "                    ", "find_or_filter", "=", "\"find\"", "\n", "pass", "\n", "", "else", ":", "\n", "# Both are not None", "\n", "                    ", "find_or_filter", "=", "\"filter\"", "\n", "\n", "# Now need to figure out whether it's a findNumber / maxNumber / minNumber", "\n", "", "longest_shortest_or_num", "=", "None", "\n", "if", "\"longest\"", "in", "tokenized_question", ":", "\n", "                    ", "longest_shortest_or_num", "=", "\"longest\"", "\n", "", "elif", "\"shortest\"", "in", "tokenized_question", ":", "\n", "                    ", "longest_shortest_or_num", "=", "\"shortest\"", "\n", "", "else", ":", "\n", "                    ", "longest_shortest_or_num", "=", "\"num\"", "\n", "\n", "", "qtype", "=", "qtype_from_findfilter_maxminnum", "(", "find_or_filter", ",", "longest_shortest_or_num", ")", "\n", "\n", "question_answer", "[", "constants", ".", "qtype", "]", "=", "qtype", "\n", "question_answer", "[", "constants", ".", "program_supervised", "]", "=", "True", "\n", "qtype_dist", "[", "qtype", "]", "+=", "1", "\n", "questions_w_qtypes", "+=", "1", "\n", "\n", "question_answer", "[", "constants", ".", "qattn_supervised", "]", "=", "True", "\n", "if", "filter_qattn", "is", "not", "None", ":", "\n", "                    ", "question_answer", "[", "constants", ".", "ques_attention_supervision", "]", "=", "[", "filter_qattn", ",", "find_qattn", "]", "\n", "", "else", ":", "\n", "                    ", "question_answer", "[", "constants", ".", "ques_attention_supervision", "]", "=", "[", "find_qattn", "]", "\n", "", "questions_w_attn", "+=", "1", "\n", "\n", "if", "number_supervision", "is", "True", ":", "\n", "                    ", "num_answer_str", "=", "answer", "[", "\"number\"", "]", "\n", "num_answer", "=", "float", "(", "num_answer_str", ")", "if", "num_answer_str", "else", "None", "\n", "\n", "qattn", "=", "copy", ".", "deepcopy", "(", "find_qattn", ")", "\n", "if", "filter_qattn", "is", "not", "None", ":", "\n", "                        ", "qattn", "=", "[", "x", "+", "y", "for", "(", "x", ",", "y", ")", "in", "zip", "(", "qattn", ",", "filter_qattn", ")", "]", "\n", "\n", "", "number_grounding", ",", "number_values", "=", "get_number_distribution_supervision", "(", "\n", "tokenized_question", ",", "\n", "tokenized_passage", ",", "\n", "num_answer", ",", "\n", "qattn", ",", "\n", "passage_num_mens", ",", "\n", "passage_num_entidxs", ",", "\n", "passage_num_vals", ",", "\n", ")", "\n", "if", "number_grounding", "is", "not", "None", ":", "\n", "                        ", "question_answer", "[", "constants", ".", "exection_supervised", "]", "=", "True", "\n", "question_answer", "[", "constants", ".", "qspan_numgrounding_supervision", "]", "=", "number_grounding", "\n", "question_answer", "[", "constants", ".", "qspan_numvalue_supervision", "]", "=", "number_values", "\n", "questions_w_numground", "+=", "1", "\n", "\n", "", "", "new_qa_pairs", ".", "append", "(", "question_answer", ")", "\n", "\n", "", "", "if", "len", "(", "new_qa_pairs", ")", ">", "0", ":", "\n", "            ", "passage_info", "[", "constants", ".", "qa_pairs", "]", "=", "new_qa_pairs", "\n", "new_dataset", "[", "passage_id", "]", "=", "passage_info", "\n", "after_pruning_ques", "+=", "len", "(", "new_qa_pairs", ")", "\n", "\n", "", "", "num_passages_after_prune", "=", "len", "(", "new_dataset", ")", "\n", "print", "(", "f\"Passages original:{num_passages}  After Pruning:{num_passages_after_prune}\"", ")", "\n", "print", "(", "f\"Questions original:{total_ques}  After pruning:{after_pruning_ques}\"", ")", "\n", "print", "(", "f\"Num of QA with qtypes and program supervised: {questions_w_qtypes}\"", ")", "\n", "print", "(", "f\"Num of QA with attention supervised: {questions_w_attn}\"", ")", "\n", "print", "(", "f\"Num of QA with num-grounding supervised: {questions_w_numground}\"", ")", "\n", "print", "(", "f\"Qtype dist: {qtype_dist}\"", ")", "\n", "\n", "return", "new_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.all_num_ques.readDataset": [[29, 33], ["open", "json.load"], "function", ["None"], ["def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.how_many_yards.all_num_ques.preprocess_HowManyYardsWasThe_ques": [[35, 109], ["collections.defaultdict", "len", "dataset.items", "len", "print", "print", "print", "print", "original_question.lower", "any", "len", "len", "new_qa_pairs.append"], "function", ["None"], ["", "def", "preprocess_HowManyYardsWasThe_ques", "(", "dataset", ")", ":", "\n", "    ", "\"\"\" This function prunes questions that start with \"How many yards was\".\n        Mostly, longest, shortest style questions. We can also prune for these; look at longestshortest_ques.py\n\n        Along with pruning, we also supervise the longest/shortest/second longest/second shortest questions\n        by adding the question_type for those questions.\n    \"\"\"", "\n", "\n", "longest_question_ngram", "=", "\"How many yards was the longest\"", "\n", "shortest_question_ngram", "=", "\"How many yards was the shortest\"", "\n", "second_longest_question_ngram", "=", "\"How many yards was the second longest\"", "\n", "second_shortest_question_ngram", "=", "\"How many yards was the second shortest\"", "\n", "\n", "longest_qtype", "=", "\"how_many_yards_longest\"", "\n", "shortest_qtype", "=", "\"how_many_yards_shortest\"", "\n", "second_longest_qtype", "=", "\"how_many_yards_second_longest\"", "\n", "second_shortest_qtype", "=", "\"how_many_yards_second_shortest\"", "\n", "\n", "new_dataset", "=", "{", "}", "\n", "total_ques", "=", "0", "\n", "after_pruning_ques", "=", "0", "\n", "questions_w_qtypes", "=", "0", "\n", "qtype_dist", "=", "defaultdict", "(", "int", ")", "\n", "num_passages", "=", "len", "(", "dataset", ")", "\n", "\n", "for", "passage_id", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "new_qa_pairs", "=", "[", "]", "\n", "for", "question_answer", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "            ", "total_ques", "+=", "1", "\n", "\n", "original_question", "=", "question_answer", "[", "constants", ".", "cleaned_question", "]", "\n", "question_lower", "=", "original_question", ".", "lower", "(", ")", "\n", "\n", "if", "any", "(", "span", "in", "question_lower", "for", "span", "in", "RELEVANT_NGRAMS", ")", ":", "\n", "                ", "if", "longest_question_ngram", "in", "original_question", ":", "\n", "                    ", "question_answer", "[", "constants", ".", "qtype", "]", "=", "constants", ".", "YARDS_longest_qtype", "\n", "question_answer", "[", "constants", ".", "program_supervised", "]", "=", "True", "\n", "qtype_dist", "[", "longest_qtype", "]", "+=", "1", "\n", "questions_w_qtypes", "+=", "1", "\n", "\n", "", "elif", "shortest_question_ngram", "in", "original_question", ":", "\n", "                    ", "question_answer", "[", "constants", ".", "qtype", "]", "=", "constants", ".", "YARDS_shortest_qtype", "\n", "question_answer", "[", "constants", ".", "program_supervised", "]", "=", "True", "\n", "qtype_dist", "[", "shortest_qtype", "]", "+=", "1", "\n", "questions_w_qtypes", "+=", "1", "\n", "\n", "# elif second_longest_question_ngram in original_question:", "\n", "#     question_answer[constants.qtype] = constants.YARDS_second_longest_qtype", "\n", "#     question_answer[constants.program_supervised] = True", "\n", "#     qtype_dist[second_longest_qtype] += 1", "\n", "#     questions_w_qtypes += 1", "\n", "#", "\n", "# elif second_shortest_question_ngram in original_question:", "\n", "#     question_answer[constants.qtype] = constants.YARDS_second_shortest_qtype", "\n", "#     question_answer[constants.program_supervised] = True", "\n", "#     qtype_dist[second_shortest_qtype] += 1", "\n", "#     questions_w_qtypes += 1", "\n", "", "else", ":", "\n", "                    ", "question_answer", "[", "constants", ".", "program_supervised", "]", "=", "False", "\n", "\n", "", "new_qa_pairs", ".", "append", "(", "question_answer", ")", "\n", "\n", "", "", "if", "len", "(", "new_qa_pairs", ")", ">", "0", ":", "\n", "            ", "passage_info", "[", "constants", ".", "qa_pairs", "]", "=", "new_qa_pairs", "\n", "new_dataset", "[", "passage_id", "]", "=", "passage_info", "\n", "after_pruning_ques", "+=", "len", "(", "new_qa_pairs", ")", "\n", "\n", "", "", "num_passages_after_prune", "=", "len", "(", "new_dataset", ")", "\n", "print", "(", "f\"Passages original:{num_passages}  After Pruning:{num_passages_after_prune}\"", ")", "\n", "print", "(", "f\"Questions original:{total_ques}  After pruning:{after_pruning_ques}\"", ")", "\n", "print", "(", "f\"Num of QA with qtypes and program supervised: {questions_w_qtypes}\"", ")", "\n", "print", "(", "f\"Qtype dist: {qtype_dist}\"", ")", "\n", "\n", "return", "new_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.getQuestionComparisonOperator": [[25, 35], ["None"], "function", ["None"], ["def", "getQuestionComparisonOperator", "(", "question_tokens", ":", "List", "[", "str", "]", ")", "->", "str", ":", "\n", "    ", "for", "t", "in", "lesser_than_tokens", ":", "\n", "        ", "if", "t", "in", "question_tokens", ":", "\n", "            ", "return", "LT_OPERATOR", "\n", "\n", "", "", "for", "t", "in", "greater_than_tokens", ":", "\n", "        ", "if", "t", "in", "question_tokens", ":", "\n", "            ", "return", "GT_OPERATOR", "\n", "\n", "", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.find_answer_event": [[37, 42], ["set", "answer_span.split", "set", "set", "len", "len", "event1.intersection", "event2.intersection"], "function", ["None"], ["", "def", "find_answer_event", "(", "answer_span", ":", "str", ",", "event1_tokens", ":", "List", "[", "str", "]", ",", "event2_tokens", ":", "List", "[", "str", "]", ")", "->", "str", ":", "\n", "    ", "ans_tokens", "=", "set", "(", "answer_span", ".", "split", "(", "\" \"", ")", ")", "\n", "event1", ",", "event2", "=", "set", "(", "event1_tokens", ")", ",", "set", "(", "event2_tokens", ")", "\n", "ans_event", "=", "FIRST", "if", "len", "(", "event1", ".", "intersection", "(", "ans_tokens", ")", ")", ">", "len", "(", "event2", ".", "intersection", "(", "ans_tokens", ")", ")", "else", "SECOND", "\n", "return", "ans_event", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.getNumTokenIdxs": [[44, 55], ["zip", "passage_num_tokens.append", "passage_numtoken_entidxs.append", "len", "len"], "function", ["None"], ["", "def", "getNumTokenIdxs", "(", "p_num_mens", ":", "List", "[", "Tuple", "[", "str", ",", "int", ",", "int", "]", "]", ",", "passage_num_entidx", ":", "List", "[", "int", "]", ")", "->", "List", "[", "int", "]", ":", "\n", "    ", "\"\"\" Lists telling which tokens are numbers, and num_ent_idx they ground to. \"\"\"", "\n", "passage_num_tokens", "=", "[", "]", "\n", "passage_numtoken_entidxs", "=", "[", "]", "\n", "for", "num_men", ",", "num_idx", "in", "zip", "(", "p_num_mens", ",", "passage_num_entidx", ")", ":", "\n", "        ", "num_token_idx", "=", "num_men", "[", "1", "]", "\n", "passage_num_tokens", ".", "append", "(", "num_token_idx", ")", "\n", "passage_numtoken_entidxs", ".", "append", "(", "num_idx", ")", "\n", "\n", "", "assert", "len", "(", "passage_num_tokens", ")", "==", "len", "(", "passage_numtoken_entidxs", ")", "\n", "return", "passage_num_tokens", ",", "passage_numtoken_entidxs", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.questionAttns": [[57, 110], ["qstr.split", "qstr.split", "qstr.split.index", "min", "len", "qstr.split.index", "qstr.split.index", "qstr.split.index", "print", "len", "qstr.split.index", "len", "qstr.split.index", "qstr.split.index", "qstr.split.index", "qstr.split.index"], "function", ["None"], ["", "def", "questionAttns", "(", "qstr", ":", "str", ")", "->", "Tuple", "[", "Tuple", "[", "int", ",", "int", "]", ",", "Tuple", "[", "int", ",", "int", "]", "]", ":", "\n", "    ", "\"\"\" For the \"which group\" questions output the two-relevant question attentions \"\"\"", "\n", "or_split", "=", "qstr", ".", "split", "(", "\" or \"", ")", "\n", "if", "len", "(", "or_split", ")", "!=", "2", ":", "\n", "        ", "return", "None", "\n", "\n", "", "tokens", "=", "qstr", ".", "split", "(", "\" \"", ")", "\n", "\n", "or_idx", "=", "tokens", ".", "index", "(", "\"or\"", ")", "\n", "# Last token is ? which we don't want to attend to", "\n", "event2", "=", "tokens", "[", "or_idx", "+", "1", ":", "len", "(", "tokens", ")", "-", "1", "]", "\n", "event2_span", "=", "(", "or_idx", "+", "1", ",", "len", "(", "tokens", ")", "-", "1", ")", "\n", "\n", "# Gets first index of the item", "\n", "try", ":", "\n", "        ", "comma_idx", "=", "tokens", ".", "index", "(", "\",\"", ")", "\n", "", "except", ":", "\n", "        ", "comma_idx", "=", "100000", "\n", "", "try", ":", "\n", "        ", "colon_idx", "=", "tokens", ".", "index", "(", "\":\"", ")", "\n", "", "except", ":", "\n", "        ", "colon_idx", "=", "100000", "\n", "\n", "", "try", ":", "\n", "        ", "hyphen_idx", "=", "tokens", ".", "index", "(", "\"-\"", ")", "\n", "", "except", ":", "\n", "        ", "hyphen_idx", "=", "100000", "\n", "\n", "", "split_idx", "=", "min", "(", "comma_idx", ",", "colon_idx", ",", "hyphen_idx", ")", "\n", "\n", "if", "split_idx", "==", "100000", "or", "(", "or_idx", "-", "split_idx", "<=", "1", ")", ":", "\n", "# print(f\"{qstr} first_split:{split_idx} or:{or_idx}\")", "\n", "        ", "if", "\"more\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"more\"", ")", "\n", "", "elif", "\"fewer\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"fewer\"", ")", "\n", "", "elif", "\"last\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"last\"", ")", "\n", "", "elif", "\"later\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"later\"", ")", "\n", "", "elif", "\"larger\"", "in", "tokens", ":", "\n", "            ", "split_idx", "=", "tokens", ".", "index", "(", "\"larger\"", ")", "\n", "", "else", ":", "\n", "            ", "split_idx", "=", "-", "1", "\n", "\n", "", "", "if", "split_idx", "==", "-", "1", ":", "\n", "        ", "print", "(", "f\"Cannot split -- {qstr} {split_idx} {or_idx}\"", ")", "\n", "return", "None", "\n", "\n", "", "event1", "=", "tokens", "[", "split_idx", "+", "1", ":", "or_idx", "]", "\n", "event1_span", "=", "(", "split_idx", "+", "1", ",", "or_idx", ")", "\n", "\n", "return", "event1_span", ",", "event2_span", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.difference_in_successive_terms": [[112, 117], ["range", "len"], "function", ["None"], ["", "def", "difference_in_successive_terms", "(", "l", ":", "List", "[", "int", "]", ")", ":", "\n", "    ", "sum_of_differences", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "l", ")", "-", "1", ")", ":", "\n", "        ", "sum_of_differences", "+=", "l", "[", "i", "+", "1", "]", "-", "l", "[", "i", "]", "\n", "", "return", "sum_of_differences", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.matchEventToPassage": [[119, 149], ["enumerate", "len", "range", "t.lower", "add_supervision.difference_in_successive_terms", "passage_token.lower", "relevant_passage_tokenidxs.append", "t.lower", "len"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.difference_in_successive_terms"], ["", "def", "matchEventToPassage", "(", "event_tokens", ":", "List", "[", "str", "]", ",", "passage_tokens", ":", "List", "[", "str", "]", ")", "->", "List", "[", "int", "]", ":", "\n", "    ", "\"\"\" Match a given event's tokens and find relevant tokens in the passage. \"\"\"", "\n", "relevant_event_tokens", "=", "[", "t", ".", "lower", "(", ")", "for", "t", "in", "event_tokens", "if", "t", ".", "lower", "(", ")", "not", "in", "STOP_WORDS", "]", "\n", "\n", "relevant_passage_tokenidxs", "=", "[", "]", "\n", "for", "(", "idx", ",", "passage_token", ")", "in", "enumerate", "(", "passage_tokens", ")", ":", "\n", "        ", "if", "passage_token", ".", "lower", "(", ")", "in", "relevant_event_tokens", ":", "\n", "            ", "relevant_passage_tokenidxs", ".", "append", "(", "idx", ")", "\n", "\n", "# Since event tokens can match spuriously at many locations -", "\n", "# Here we try to find the tightest span, i.e. a span that is the same length as event_span and contains close-tokens", "\n", "", "", "len_event_span", "=", "len", "(", "relevant_event_tokens", ")", "\n", "best_diff", "=", "100000", "\n", "best_start_point", "=", "0", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "relevant_passage_tokenidxs", ")", "-", "len_event_span", "+", "1", ")", ":", "\n", "        ", "passage_token_span", "=", "relevant_passage_tokenidxs", "[", "i", ":", "i", "+", "len_event_span", "]", "\n", "sum_of_token_diffs", "=", "difference_in_successive_terms", "(", "passage_token_span", ")", "\n", "if", "sum_of_token_diffs", "<", "best_diff", ":", "\n", "            ", "best_start_point", "=", "i", "\n", "best_diff", "=", "sum_of_token_diffs", "\n", "\n", "", "", "pruned_relevant_passage_tokenidxs", "=", "relevant_passage_tokenidxs", "[", "best_start_point", ":", "best_start_point", "+", "len_event_span", "]", "\n", "\"\"\"\n    passage_str = \"\"\n    for idx, token in enumerate(passage_tokens):\n        passage_str += f\"{token}|{idx} \"\n    print(f\"{passage_str}\")\n    print(f\"{relevant_event_tokens}\\n{relevant_passage_tokenidxs}\\n{pruned_relevant_passage_tokenidxs}\\n\")\n    \"\"\"", "\n", "return", "pruned_relevant_passage_tokenidxs", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.numInNeighborhood": [[151, 198], ["max", "zip", "distance_to_nums.append", "closest_num_entidxs.append", "len", "float", "len", "set", "sum", "abs"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "def", "numInNeighborhood", "(", "\n", "relevant_passage_tokenidxs", ":", "List", "[", "int", "]", ",", "\n", "passage_num_tokenidxs", ":", "List", "[", "int", "]", ",", "\n", "passage_numtoken_entidxs", ":", "List", "[", "int", "]", ",", "\n", "threshold", ",", "\n", ")", ":", "\n", "    ", "\"\"\" Given a list of relevant-passage-tokens, and list of date-tokens in the passage, figure out -\n        if there's a date in the neighborhood of the relevant tokens\n        For each passage-token, first find the min-distance to a date token. Then find the min-amongst that.\n        If this distance crosses a threshold, then a date is not considered in the neighborhood of the passage-tokens\n    \"\"\"", "\n", "\n", "distance_to_nums", "=", "[", "]", "\n", "closest_num_entidxs", "=", "[", "]", "\n", "for", "tokenidx", "in", "relevant_passage_tokenidxs", ":", "\n", "        ", "min_distance_to_num", "=", "10000", "\n", "closest_numidx", "=", "-", "1", "\n", "for", "num_tokenidx", ",", "num_idx", "in", "zip", "(", "passage_num_tokenidxs", ",", "passage_numtoken_entidxs", ")", ":", "\n", "# Since some spans are made up of numbers themselves (age groups) don't consider those numbers", "\n", "            ", "if", "num_tokenidx", "in", "relevant_passage_tokenidxs", ":", "\n", "                ", "continue", "\n", "# Only keeping numbers that appear before the span as in these questions it seems to better ground", "\n", "", "if", "(", "num_tokenidx", "-", "tokenidx", ")", ">", "0", ":", "\n", "                ", "dis", "=", "1000", "\n", "", "else", ":", "\n", "                ", "dis", "=", "abs", "(", "num_tokenidx", "-", "tokenidx", ")", "\n", "\n", "", "if", "dis", "==", "0", ":", "\n", "                ", "dis", "=", "1000", "\n", "\n", "", "if", "dis", "<", "min_distance_to_num", ":", "\n", "                ", "min_distance_to_num", "=", "dis", "\n", "closest_numidx", "=", "num_idx", "\n", "", "", "distance_to_nums", ".", "append", "(", "min_distance_to_num", ")", "\n", "closest_num_entidxs", ".", "append", "(", "closest_numidx", ")", "\n", "\n", "", "if", "len", "(", "distance_to_nums", ")", "==", "0", ":", "\n", "        ", "return", "False", ",", "-", "1", "\n", "\n", "", "avg_distance_to_dates", "=", "float", "(", "sum", "(", "distance_to_nums", ")", ")", "/", "len", "(", "distance_to_nums", ")", "\n", "# Mode", "\n", "closest_date_entidx", "=", "max", "(", "set", "(", "closest_num_entidxs", ")", ",", "key", "=", "closest_num_entidxs", ".", "count", ")", "\n", "\n", "if", "avg_distance_to_dates", ">", "threshold", ":", "\n", "        ", "return", "False", ",", "closest_date_entidx", "\n", "", "else", ":", "\n", "        ", "return", "True", ",", "closest_date_entidx", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.addSupervision": [[200, 408], ["open", "collections.defaultdict", "collections.defaultdict", "json.load.items", "open.close", "print", "print", "print", "print", "print", "open", "json.load", "passage.split", "len", "add_supervision.getNumTokenIdxs", "open", "json.dump", "question.split", "len", "add_supervision.getQuestionComparisonOperator", "add_supervision.questionAttns", "new_qa_pairs.append", "len", "range", "range", "add_supervision.matchEventToPassage", "add_supervision.matchEventToPassage", "add_supervision.numInNeighborhood", "add_supervision.numInNeighborhood", "len", "len", "sum", "sum", "add_supervision.find_answer_event", "len", "len", "open.write", "open.write", "open.write", "open.write", "len", "len"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.getNumTokenIdxs", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.getQuestionComparisonOperator", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.questionAttns", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.matchEventToPassage", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.matchEventToPassage", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.numInNeighborhood", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.numInNeighborhood", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.add_supervision.find_answer_event"], ["", "", "def", "addSupervision", "(", "input_json", ":", "str", ",", "output_json", ":", "str", ",", "output_txt", ":", "str", ",", "THRESHOLD", "=", "10", ")", "->", "None", ":", "\n", "    ", "\"\"\" Questions of number comparison from [\"were there more/fewer\", \"which age group\", \"which group\"]\n\n        For each question, try to find the following supervision:\n            1. Question attention -- two attention values for the spans for which the numbers will be compared\n                constants.ques_attention_supervision -- holds a 2-tuple of attentions\n            2. Number supervision -- for the two spans, a passage_num_grounding and their values. These satisfy the\n                coherency between the question operator and the values we ground to. Similar to depr_prune_date_comparison.py\n\n                Fields added:\n                constants.numcomp_qspan_num_groundings - 2-tuple containing number grounding\n                constants.numcomp_qspan_num_values - 2-tuple containing number values\n\n            3. QTYPE - constants.qtype is set to NUMCOMP_QTYPE\n\n            4. Strongly Supervised - If 1. and 2. are found, then we set the strongly_supervised flag to True\n                constants.strongly_supervised = True\n    \"\"\"", "\n", "\n", "# Input file contains single json obj with list of questions as jsonobjs inside it", "\n", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "txtfile", "=", "open", "(", "output_txt", ",", "\"w\"", ")", "\n", "\n", "new_dataset", "=", "{", "}", "\n", "\n", "num_original_questions", "=", "0", "\n", "qoperator_dict", "=", "defaultdict", "(", "float", ")", "\n", "qoperator_undefined", "=", "0", "\n", "num_grounding_unsuccessful", "=", "0", "\n", "supervision_distribution", "=", "defaultdict", "(", "int", ")", "\n", "\n", "for", "passage_id", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "passage", "=", "passage_info", "[", "constants", ".", "tokenized_passage", "]", "\n", "passage_tokens", "=", "passage", ".", "split", "(", "\" \"", ")", "\n", "qa_pairs", "=", "passage_info", "[", "constants", ".", "qa_pairs", "]", "\n", "num_original_questions", "+=", "len", "(", "qa_pairs", ")", "\n", "\n", "passage_num_mens", "=", "passage_info", "[", "constants", ".", "passage_num_mens", "]", "\n", "passage_num_idxs", "=", "passage_info", "[", "constants", ".", "passage_num_entidx", "]", "\n", "passage_num_values", "=", "passage_info", "[", "constants", ".", "passage_num_normalized_values", "]", "\n", "\n", "(", "passage_num_tokenidxs", ",", "passage_numtoken_entidxs", ")", "=", "getNumTokenIdxs", "(", "passage_num_mens", ",", "passage_num_idxs", ")", "\n", "\n", "new_qa_pairs", "=", "[", "]", "\n", "for", "qa_pair", "in", "qa_pairs", ":", "\n", "            ", "question", "=", "qa_pair", "[", "constants", ".", "tokenized_question", "]", "\n", "question_tokens", "=", "question", ".", "split", "(", "\" \"", ")", "\n", "answer_span", ":", "str", "=", "qa_pair", "[", "constants", ".", "answer", "]", "[", "\"spans\"", "]", "[", "0", "]", "\n", "qlen", "=", "len", "(", "question_tokens", ")", "\n", "\n", "qoperator", "=", "getQuestionComparisonOperator", "(", "question_tokens", ")", "\n", "if", "qoperator", "is", "None", ":", "\n", "                ", "qoperator_undefined", "+=", "1", "\n", "program_supervision", "=", "False", "\n", "", "else", ":", "\n", "                ", "program_supervision", "=", "True", "\n", "qoperator_dict", "[", "qoperator", "]", "+=", "1", "\n", "\n", "", "\"\"\" QUESTION ATTENTION SUPERVISION \"\"\"", "\n", "# Exclusive ends", "\n", "attention1", "=", "[", "0.0", "]", "*", "qlen", "\n", "attention2", "=", "[", "0.0", "]", "*", "qlen", "\n", "ques_spans", "=", "questionAttns", "(", "question", ")", "\n", "if", "ques_spans", "and", "program_supervision", ":", "\n", "# These span ends are exclusive", "\n", "                ", "span1", ",", "span2", "=", "ques_spans", "\n", "for", "i", "in", "range", "(", "span1", "[", "0", "]", ",", "span1", "[", "1", "]", ")", ":", "\n", "                    ", "attention1", "[", "i", "]", "=", "1.0", "\n", "", "for", "i", "in", "range", "(", "span2", "[", "0", "]", ",", "span2", "[", "1", "]", ")", ":", "\n", "                    ", "attention2", "[", "i", "]", "=", "1.0", "\n", "\n", "", "", "annotated_qtokens", "=", "(", "\n", "question_tokens", "[", "0", ":", "span1", "[", "0", "]", "]", "\n", "+", "[", "\"[[\"", "]", "\n", "+", "question_tokens", "[", "span1", "[", "0", "]", ":", "span1", "[", "1", "]", "]", "\n", "+", "[", "\"]]\"", "]", "\n", "+", "question_tokens", "[", "span1", "[", "1", "]", ":", "span2", "[", "0", "]", "]", "\n", "+", "[", "\"[[\"", "]", "\n", "+", "question_tokens", "[", "span2", "[", "0", "]", ":", "span2", "[", "1", "]", "]", "\n", "+", "[", "\"]]\"", "]", "\n", "+", "question_tokens", "[", "span2", "[", "1", "]", ":", "]", "\n", ")", "\n", "annotated_qtxt", "=", "\" \"", ".", "join", "(", "annotated_qtokens", ")", "\n", "\n", "if", "sum", "(", "attention1", ")", ">", "0", "and", "sum", "(", "attention2", ")", ">", "0", ":", "\n", "                ", "qattn_supervision", "=", "True", "\n", "", "else", ":", "\n", "                ", "qattn_supervision", "=", "False", "\n", "\n", "", "\"\"\" Keeping reverse question attentions \"\"\"", "\n", "qa_pair", "[", "constants", ".", "ques_attention_supervision", "]", "=", "(", "attention2", ",", "attention1", ")", "\n", "\n", "\"\"\" NUMBER GROUNDING SUPERVISION \"\"\"", "\n", "if", "program_supervision", "and", "qattn_supervision", ":", "\n", "                ", "span1", ",", "span2", "=", "ques_spans", "\n", "span1_tokens", "=", "question_tokens", "[", "span1", "[", "0", "]", ":", "span1", "[", "1", "]", "]", "\n", "span2_tokens", "=", "question_tokens", "[", "span2", "[", "0", "]", ":", "span2", "[", "1", "]", "]", "\n", "\n", "# List of tokenidxs in passage that is a rough grounding for event 1/2", "\n", "event1_passage_tokenidxs", ":", "List", "[", "int", "]", "=", "matchEventToPassage", "(", "span1_tokens", ",", "passage_tokens", ")", "\n", "event2_passage_tokenidxs", ":", "List", "[", "int", "]", "=", "matchEventToPassage", "(", "span2_tokens", ",", "passage_tokens", ")", "\n", "\n", "num_near_event1", ",", "event1_num_idx", "=", "numInNeighborhood", "(", "\n", "event1_passage_tokenidxs", ",", "passage_num_tokenidxs", ",", "passage_numtoken_entidxs", ",", "threshold", "=", "THRESHOLD", "\n", ")", "\n", "\n", "num_near_event2", ",", "event2_num_idx", "=", "numInNeighborhood", "(", "\n", "event2_passage_tokenidxs", ",", "passage_num_tokenidxs", ",", "passage_numtoken_entidxs", ",", "threshold", "=", "THRESHOLD", "\n", ")", "\n", "if", "num_near_event1", ":", "\n", "                    ", "value1", "=", "passage_num_values", "[", "event1_num_idx", "]", "\n", "", "else", ":", "\n", "                    ", "value1", "=", "-", "1000000", "\n", "", "if", "num_near_event2", ":", "\n", "                    ", "value2", "=", "passage_num_values", "[", "event2_num_idx", "]", "\n", "", "else", ":", "\n", "                    ", "value2", "=", "-", "1000000", "\n", "\n", "", "execution_supervision", "=", "True", "\n", "if", "value1", "==", "-", "1000000", "or", "value2", "==", "-", "1000000", ":", "\n", "                    ", "num_grounding_unsuccessful", "+=", "1", "\n", "execution_supervision", "=", "False", "\n", "\n", "", "else", ":", "\n", "# First or Second", "\n", "                    ", "answer_event", "=", "find_answer_event", "(", "answer_span", ",", "span1_tokens", ",", "span2_tokens", ")", "\n", "# Need to check if the values are coherent with the operator and the answer", "\n", "grounded_answer_num_value", "=", "value1", "if", "answer_event", "==", "FIRST", "else", "value2", "\n", "grounded_other_num_value", "=", "value2", "if", "answer_event", "==", "FIRST", "else", "value1", "\n", "\n", "if", "qoperator", "==", "LT_OPERATOR", ":", "\n", "                        ", "if", "grounded_answer_num_value", ">=", "grounded_other_num_value", ":", "\n", "                            ", "num_grounding_unsuccessful", "+=", "1", "\n", "execution_supervision", "=", "False", "\n", "", "", "if", "qoperator", "==", "GT_OPERATOR", ":", "\n", "                        ", "if", "grounded_answer_num_value", "<=", "grounded_other_num_value", ":", "\n", "                            ", "num_grounding_unsuccessful", "+=", "1", "\n", "execution_supervision", "=", "False", "\n", "\n", "", "", "", "span1_num_grounding", "=", "[", "0", "]", "*", "len", "(", "passage_num_values", ")", "\n", "span2_num_grounding", "=", "[", "0", "]", "*", "len", "(", "passage_num_values", ")", "\n", "\n", "if", "execution_supervision", "is", "True", ":", "\n", "                    ", "span2_num_grounding", "[", "event2_num_idx", "]", "=", "1", "\n", "span1_num_grounding", "[", "event1_num_idx", "]", "=", "1", "\n", "\n", "txtfile", ".", "write", "(", "f\"{annotated_qtxt} -- {qoperator}\\n\"", ")", "\n", "txtfile", ".", "write", "(", "f\"{passage}\\n\"", ")", "\n", "event1_tokens", "=", "[", "passage_tokens", "[", "x", "]", "for", "x", "in", "event1_passage_tokenidxs", "]", "\n", "event2_tokens", "=", "[", "passage_tokens", "[", "x", "]", "for", "x", "in", "event2_passage_tokenidxs", "]", "\n", "txtfile", ".", "write", "(", "f\"{event1_tokens} - {value1}\\n\"", ")", "\n", "txtfile", ".", "write", "(", "f\"{event2_tokens} {value2}\\n\\n\"", ")", "\n", "\n", "# else:", "\n", "#     txtfile.write(f\"{annotated_qtxt} -- {qoperator}\\n\")", "\n", "#     txtfile.write(f\"{passage}\\n\")", "\n", "#     event1_tokens = [passage_tokens[x] for x in event1_passage_tokenidxs]", "\n", "#     event2_tokens = [passage_tokens[x] for x in event2_passage_tokenidxs]", "\n", "#     txtfile.write(f\"{event1_tokens}  {event2_tokens}\")", "\n", "#     txtfile.write(f\"{value1}  {value2}\\n\\n\")", "\n", "\n", "", "\"\"\" Storing reversed supervision since it helps a little \"\"\"", "\n", "qa_pair", "[", "constants", ".", "qspan_numgrounding_supervision", "]", "=", "(", "span2_num_grounding", ",", "span1_num_grounding", ")", "\n", "qa_pair", "[", "constants", ".", "qspan_numvalue_supervision", "]", "=", "(", "value2", ",", "value1", ")", "\n", "", "else", ":", "\n", "                ", "execution_supervision", "=", "False", "\n", "span1_num_grounding", "=", "[", "0", "]", "*", "len", "(", "passage_num_values", ")", "\n", "span2_num_grounding", "=", "[", "0", "]", "*", "len", "(", "passage_num_values", ")", "\n", "qa_pair", "[", "constants", ".", "qspan_numgrounding_supervision", "]", "=", "(", "span2_num_grounding", ",", "span1_num_grounding", ")", "\n", "qa_pair", "[", "constants", ".", "qspan_numvalue_supervision", "]", "=", "(", "-", "1", ",", "-", "1", ")", "\n", "\n", "", "\"\"\" QTYPE SUPERVISION \"\"\"", "\n", "if", "program_supervision", "is", "True", ":", "\n", "                ", "qa_pair", "[", "constants", ".", "qtype", "]", "=", "constants", ".", "NUMCOMP_QTYPE", "\n", "", "qa_pair", "[", "constants", ".", "program_supervised", "]", "=", "program_supervision", "\n", "qa_pair", "[", "constants", ".", "qattn_supervised", "]", "=", "qattn_supervision", "\n", "qa_pair", "[", "constants", ".", "exection_supervised", "]", "=", "execution_supervision", "\n", "\n", "if", "program_supervision", "and", "qattn_supervision", "and", "execution_supervision", ":", "\n", "                ", "strongly_supervised", "=", "True", "\n", "", "else", ":", "\n", "                ", "strongly_supervised", "=", "False", "\n", "\n", "", "qa_pair", "[", "constants", ".", "strongly_supervised", "]", "=", "strongly_supervised", "\n", "\n", "supervision_distribution", "[", "\"program_supervision\"", "]", "+=", "1", "if", "program_supervision", "else", "0", "\n", "supervision_distribution", "[", "\"qattn_supervision\"", "]", "+=", "1", "if", "qattn_supervision", "else", "0", "\n", "supervision_distribution", "[", "\"execution_supervision\"", "]", "+=", "1", "if", "execution_supervision", "else", "0", "\n", "supervision_distribution", "[", "\"strongly_supervised\"", "]", "+=", "1", "if", "strongly_supervised", "else", "0", "\n", "\n", "new_qa_pairs", ".", "append", "(", "qa_pair", ")", "\n", "\n", "", "if", "len", "(", "new_qa_pairs", ")", ">", "0", ":", "\n", "            ", "passage_info", "[", "constants", ".", "qa_pairs", "]", "=", "new_qa_pairs", "\n", "new_dataset", "[", "passage_id", "]", "=", "passage_info", "\n", "\n", "", "", "with", "open", "(", "output_json", ",", "\"w\"", ")", "as", "outf", ":", "\n", "        ", "json", ".", "dump", "(", "new_dataset", ",", "outf", ",", "indent", "=", "4", ")", "\n", "\n", "", "txtfile", ".", "close", "(", ")", "\n", "\n", "print", "(", "f\"Number of input passages: {len(dataset)}\\nNumber of input QA pairs: {num_original_questions}\"", ")", "\n", "print", "(", "f\"Number of output passages: {len(new_dataset)}\"", ")", "\n", "print", "(", "f\"Num grounding unsuccess: {num_grounding_unsuccessful}\"", ")", "\n", "print", "(", "f\"{qoperator_dict}\"", ")", "\n", "print", "(", "supervision_distribution", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.prune_numcomp.number_comparison_filter": [[10, 30], ["question.lower", "any", "question.lower.split", "question.lower.split", "any", "len", "len"], "function", ["None"], ["def", "number_comparison_filter", "(", "question", ":", "str", ")", ":", "\n", "    ", "question_lower", "=", "question", ".", "lower", "(", ")", "\n", "football_ques_spans", "=", "[", "\"first half\"", ",", "\"second half\"", ",", "\"quarter\"", ",", "\"touchdown\"", ",", "\"field goals\"", "]", "\n", "relevant", "=", "True", "\n", "if", "any", "(", "span", "in", "question_lower", "for", "span", "in", "NUMBER_COMPARISON", ")", ":", "\n", "        ", "or_split", "=", "question_lower", ".", "split", "(", "\" or \"", ")", "\n", "if", "len", "(", "or_split", ")", "!=", "2", ":", "\n", "            ", "relevant", "=", "False", "\n", "\n", "", "comma_split", "=", "question_lower", ".", "split", "(", "\",\"", ")", "\n", "if", "len", "(", "comma_split", ")", ">", "2", ":", "\n", "            ", "relevant", "=", "False", "\n", "\n", "# were there more / fewer -- remove these difficult football questions", "\n", "", "if", "any", "(", "span", "in", "question_lower", "for", "span", "in", "football_ques_spans", ")", ":", "\n", "            ", "relevant", "=", "False", "\n", "", "", "else", ":", "\n", "        ", "relevant", "=", "False", "\n", "\n", "", "return", "relevant", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.prune_numcomp.pruneDataset": [[32, 102], ["list", "open", "json.load.items", "open.close", "print", "print", "print", "open", "json.load", "json.load.items", "len", "len", "open", "json.dump", "len", "prune_numcomp.number_comparison_filter", "relevant_qa_pairs.append", "open.write", "len", "len"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.numcomp.prune_numcomp.number_comparison_filter"], ["", "def", "pruneDataset", "(", "input_json", ":", "str", ",", "output_json", ":", "str", ",", "output_txt", ":", "str", ")", "->", "None", ":", "\n", "    ", "\"\"\" Prune dataset to only contain questions that qualify after certain NUM comparison question tests.\n        Currently only keeping questions with a single passage SpanType answer.\n    \"\"\"", "\n", "\n", "# Input file contains single json obj with list of questions as jsonobjs inside it", "\n", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "num_input_qas", "=", "0", "\n", "\n", "# List of tuples with (passage_id, passage_info)", "\n", "passage_id_infos", "=", "list", "(", "dataset", ".", "items", "(", ")", ")", "\n", "for", "(", "_", ",", "pinfo", ")", "in", "passage_id_infos", ":", "\n", "        ", "num_input_qas", "+=", "len", "(", "pinfo", "[", "constants", ".", "qa_pairs", "]", ")", "\n", "\n", "", "output_passage_dict", "=", "{", "}", "\n", "num_output_qas", "=", "0", "\n", "\n", "txtfile", "=", "open", "(", "output_txt", ",", "\"w\"", ")", "\n", "\n", "for", "passage_id", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "qa_pairs", "=", "passage_info", "[", "constants", ".", "qa_pairs", "]", "\n", "relevant_qa_pairs", "=", "[", "]", "\n", "\n", "for", "qa_pair", "in", "qa_pairs", ":", "\n", "            ", "keep", "=", "True", "\n", "question", "=", "qa_pair", "[", "constants", ".", "tokenized_question", "]", "\n", "\n", "# Number Comparison questions we care about", "\n", "if", "not", "number_comparison_filter", "(", "question", ")", ":", "\n", "                ", "keep", "=", "False", "\n", "continue", "\n", "\n", "# Only SPAN type questions", "\n", "", "if", "constants", ".", "answer_type", "in", "qa_pair", ":", "\n", "                ", "if", "qa_pair", "[", "constants", ".", "answer_type", "]", "!=", "constants", ".", "SPAN_TYPE", ":", "\n", "                    ", "keep", "=", "False", "\n", "continue", "\n", "", "", "else", ":", "\n", "                ", "keep", "=", "False", "\n", "continue", "\n", "\n", "# To avoid duplication", "\n", "", "if", "keep", ":", "\n", "                ", "relevant_qa_pairs", ".", "append", "(", "qa_pair", ")", "\n", "question", "=", "qa_pair", "[", "constants", ".", "tokenized_question", "]", "\n", "passage", "=", "passage_info", "[", "constants", ".", "tokenized_passage", "]", "\n", "ans", "=", "qa_pair", "[", "constants", ".", "answer", "]", "\n", "txtfile", ".", "write", "(", "f\"{question}\\n\"", ")", "\n", "# txtfile.write(f\"{passage}\\n{ans}\\n\\n\")", "\n", "\n", "", "", "if", "len", "(", "relevant_qa_pairs", ")", "==", "0", ":", "\n", "            ", "continue", "\n", "\n", "", "passage_info", "[", "constants", ".", "qa_pairs", "]", "=", "relevant_qa_pairs", "\n", "num_output_qas", "+=", "len", "(", "relevant_qa_pairs", ")", "\n", "\n", "# new_p_info = processPassage(passage_info)", "\n", "# if new_p_info is None:", "\n", "#     continue", "\n", "output_passage_dict", "[", "passage_id", "]", "=", "passage_info", "\n", "\n", "", "with", "open", "(", "output_json", ",", "\"w\"", ")", "as", "outf", ":", "\n", "        ", "json", ".", "dump", "(", "output_passage_dict", ",", "outf", ",", "indent", "=", "4", ")", "\n", "\n", "", "txtfile", ".", "close", "(", ")", "\n", "print", "(", "f\"Number of input passages: {len(passage_id_infos)}\\nNumber of input QA pairs: {num_input_qas}\"", ")", "\n", "print", "(", "f\"Number of output passages: {len(output_passage_dict)}\\nNumber of output QA pairs: {num_output_qas}\"", ")", "\n", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.year_diff.year_diff.is_single_event_yeardiff_question": [[25, 45], ["None"], "function", ["None"], ["def", "is_single_event_yeardiff_question", "(", "question_lower", ":", "str", ")", ":", "\n", "    ", "single_event_ques", ":", "bool", "=", "None", "\n", "if", "\"how many years was\"", "in", "question_lower", ":", "\n", "        ", "if", "\"how many years was it between\"", "in", "question_lower", "or", "\"how many years was it from\"", "in", "question_lower", ":", "\n", "            ", "single_event_ques", "=", "False", "\n", "", "else", ":", "\n", "            ", "single_event_ques", "=", "True", "\n", "\n", "# If \"from\" doesn't exist then surely single_event; o/w don't know", "\n", "", "", "if", "\"how many years did it\"", "in", "question_lower", ":", "\n", "        ", "if", "\"from\"", "not", "in", "question_lower", ":", "\n", "            ", "single_event_ques", "=", "True", "\n", "\n", "", "", "if", "\"how many years did the\"", "in", "question_lower", ":", "\n", "        ", "single_event_ques", "=", "True", "\n", "\n", "", "if", "\"how many years passed between\"", "in", "question_lower", "or", "\"how many years after the\"", "in", "question_lower", ":", "\n", "        ", "single_event_ques", "=", "False", "\n", "\n", "", "return", "single_event_ques", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.year_diff.year_diff.readDataset": [[47, 51], ["open", "json.load"], "function", ["None"], ["", "def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.year_diff.year_diff.prune_YearDiffQues": [[53, 97], ["len", "collections.defaultdict", "dataset.items", "len", "print", "print", "print", "original_question.lower", "any", "len", "len", "year_diff.is_single_event_yeardiff_question", "new_qa_pairs.append"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.year_diff.year_diff.is_single_event_yeardiff_question"], ["", "def", "prune_YearDiffQues", "(", "dataset", ")", ":", "\n", "    ", "\"\"\" Extract all questions that contain any one of the YEAR_DIFF_TRIGRAMS \"\"\"", "\n", "\n", "new_dataset", "=", "{", "}", "\n", "total_ques", "=", "0", "\n", "after_pruning_ques", "=", "0", "\n", "num_passages", "=", "len", "(", "dataset", ")", "\n", "\n", "qtype_dist", "=", "defaultdict", "(", "int", ")", "\n", "\n", "for", "passage_id", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "new_qa_pairs", "=", "[", "]", "\n", "for", "question_answer", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "            ", "total_ques", "+=", "1", "\n", "\n", "original_question", "=", "question_answer", "[", "constants", ".", "cleaned_question", "]", "\n", "question_lower", "=", "original_question", ".", "lower", "(", ")", "\n", "\n", "if", "any", "(", "span", "in", "question_lower", "for", "span", "in", "YEAR_DIFF_NGRAMS", ")", ":", "\n", "                ", "single_event_ques", ":", "bool", "=", "is_single_event_yeardiff_question", "(", "question_lower", ")", "\n", "# Return value of None means un-identifiable type", "\n", "if", "single_event_ques", "is", "not", "None", ":", "\n", "                    ", "if", "single_event_ques", ":", "\n", "                        ", "question_answer", "[", "constants", ".", "qtype", "]", "=", "constants", ".", "YEARDIFF_SE_qtype", "\n", "question_answer", "[", "constants", ".", "program_supervised", "]", "=", "True", "\n", "qtype_dist", "[", "constants", ".", "YEARDIFF_SE_qtype", "]", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "question_answer", "[", "constants", ".", "qtype", "]", "=", "constants", ".", "YEARDIFF_TE_qtype", "\n", "question_answer", "[", "constants", ".", "program_supervised", "]", "=", "True", "\n", "qtype_dist", "[", "constants", ".", "YEARDIFF_TE_qtype", "]", "+=", "1", "\n", "\n", "", "", "new_qa_pairs", ".", "append", "(", "question_answer", ")", "\n", "\n", "", "", "if", "len", "(", "new_qa_pairs", ")", ">", "0", ":", "\n", "            ", "passage_info", "[", "constants", ".", "qa_pairs", "]", "=", "new_qa_pairs", "\n", "new_dataset", "[", "passage_id", "]", "=", "passage_info", "\n", "after_pruning_ques", "+=", "len", "(", "new_qa_pairs", ")", "\n", "\n", "", "", "num_passages_after_prune", "=", "len", "(", "new_dataset", ")", "\n", "print", "(", "f\"Passages original:{num_passages}  After Pruning:{num_passages_after_prune}\"", ")", "\n", "print", "(", "f\"Questions original:{total_ques}  After pruning:{after_pruning_ques}\"", ")", "\n", "print", "(", "f\"Qtype dict: {qtype_dist}\"", ")", "\n", "\n", "return", "new_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.percent.percent.readDataset": [[11, 15], ["open", "json.load"], "function", ["None"], ["def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.percent.percent.prune_PercentDataset": [[17, 49], ["len", "collections.defaultdict", "dataset.items", "len", "print", "print", "print", "original_question.lower", "any", "len", "len", "new_qa_pairs.append"], "function", ["None"], ["", "def", "prune_PercentDataset", "(", "dataset", ")", ":", "\n", "    ", "\"\"\" Extract all questions that contain any one of the PERCENT_N-GRAMS \"\"\"", "\n", "\n", "new_dataset", "=", "{", "}", "\n", "total_ques", "=", "0", "\n", "after_pruning_ques", "=", "0", "\n", "num_passages", "=", "len", "(", "dataset", ")", "\n", "\n", "qtype_dist", "=", "defaultdict", "(", "int", ")", "\n", "\n", "for", "passage_id", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "new_qa_pairs", "=", "[", "]", "\n", "for", "question_answer", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "            ", "total_ques", "+=", "1", "\n", "\n", "original_question", "=", "question_answer", "[", "constants", ".", "cleaned_question", "]", "\n", "question_lower", "=", "original_question", ".", "lower", "(", ")", "\n", "\n", "if", "any", "(", "span", "in", "question_lower", "for", "span", "in", "PERCENT_NGRAMS", ")", ":", "\n", "                ", "new_qa_pairs", ".", "append", "(", "question_answer", ")", "\n", "\n", "", "", "if", "len", "(", "new_qa_pairs", ")", ">", "0", ":", "\n", "            ", "passage_info", "[", "constants", ".", "qa_pairs", "]", "=", "new_qa_pairs", "\n", "new_dataset", "[", "passage_id", "]", "=", "passage_info", "\n", "after_pruning_ques", "+=", "len", "(", "new_qa_pairs", ")", "\n", "\n", "", "", "num_passages_after_prune", "=", "len", "(", "new_dataset", ")", "\n", "print", "(", "f\"Passages original:{num_passages}  After Pruning:{num_passages_after_prune}\"", ")", "\n", "print", "(", "f\"Questions original:{total_ques}  After pruning:{after_pruning_ques}\"", ")", "\n", "print", "(", "f\"Qtype dict: {qtype_dist}\"", ")", "\n", "\n", "return", "new_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.Example.__init__": [[150, 157], ["compute_interpretability.Example.convert_attention_to_np"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.Example.convert_attention_to_np"], ["    ", "def", "__init__", "(", "self", ",", "example_dict", ":", "Dict", ")", ":", "\n", "        ", "self", ".", "passage_id", "=", "example_dict", "[", "\"passage_id\"", "]", "\n", "self", ".", "query_id", "=", "example_dict", "[", "\"query_id\"", "]", "\n", "self", ".", "question", "=", "example_dict", "[", "\"question\"", "]", "\n", "self", ".", "qtype", "=", "example_dict", "[", "\"qtype\"", "]", "\n", "self", ".", "predicted_program", "=", "example_dict", "[", "\"predicted_logical_form\"", "]", "\n", "self", ".", "module_outputs", "=", "self", ".", "convert_attention_to_np", "(", "example_dict", "[", "\"module_outputs\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.Example.convert_attention_to_np": [[159, 165], ["compute_interpretability.convert_to_nparray", "module_outputs_np.append"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.convert_to_nparray"], ["", "def", "convert_attention_to_np", "(", "self", ",", "module_outputs", ")", ":", "\n", "        ", "module_outputs_np", "=", "[", "]", "\n", "for", "(", "module_name", ",", "attention", ")", "in", "module_outputs", ":", "\n", "            ", "np_attention", "=", "convert_to_nparray", "(", "attention", ")", "\n", "module_outputs_np", ".", "append", "(", "(", "module_name", ",", "np_attention", ")", ")", "\n", "", "return", "module_outputs_np", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.compute_interpretability_loss": [[43, 56], ["numpy.sum", "numpy.log", "numpy.log"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["def", "compute_interpretability_loss", "(", "passage_attention", ":", "np", ".", "array", ",", "spans", ":", "List", "[", "Tuple", "]", ")", ":", "\n", "    ", "interpretability_loss", "=", "0.0", "\n", "for", "span", "in", "spans", ":", "\n", "        ", "span_prob", "=", "np", ".", "sum", "(", "passage_attention", "[", "span", "[", "0", "]", ":", "span", "[", "1", "]", "]", ")", "\n", "# span_prob = max(1e-20, span_prob)", "\n", "if", "span_prob", ">", "1e-20", ":", "\n", "            ", "span_neg_log_prob", "=", "-", "1.0", "*", "np", ".", "log", "(", "span_prob", ")", "\n", "interpretability_loss", "+=", "span_neg_log_prob", "\n", "", "else", ":", "\n", "            ", "span_neg_log_prob", "=", "-", "1.0", "*", "np", ".", "log", "(", "1e-20", ")", "\n", "interpretability_loss", "+=", "span_neg_log_prob", "\n", "# interpretability_loss /= float(len(spans))", "\n", "", "", "return", "interpretability_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.interpretability_FIND_TWO_SYMBOL": [[62, 92], ["numpy.maximum", "compute_interpretability.compute_interpretability_loss", "compute_interpretability.compute_interpretability_loss", "MODULE_SCORE.append"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.compute_interpretability_loss", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.compute_interpretability_loss"], ["def", "interpretability_FIND_TWO_SYMBOL", "(", "predicted_module_outputs", ",", "gold_module_outputs", ")", ":", "\n", "    ", "\"\"\" The gold for this would contain two modules, \"find-one-event\" and \"ground-two-{dates, nums}\"\n        The predicted for this would contain a single \"find\" and two \"date1\" and \"date2\" / \"num1\" and \"num2\"\n\n        The predicted \"find\" can be directly compared to gold \"find-one-event\", but\n        the two predicted symbol distributions need to be merged into one and compared to \"ground-two-{dates, nums}\"\n    \"\"\"", "\n", "gold_find_distribution", "=", "gold_module_outputs", "[", "0", "]", "[", "1", "]", "\n", "gold_symbol_distribution", "=", "gold_module_outputs", "[", "1", "]", "[", "1", "]", "\n", "\n", "predicted_find_distribution", "=", "predicted_module_outputs", "[", "0", "]", "[", "1", "]", "\n", "predicted_symbol_1_distribution", "=", "predicted_module_outputs", "[", "1", "]", "[", "1", "]", "\n", "predicted_symbol_2_distribution", "=", "predicted_module_outputs", "[", "2", "]", "[", "1", "]", "\n", "predicted_symbol_distribution", "=", "np", ".", "maximum", "(", "predicted_symbol_1_distribution", ",", "predicted_symbol_2_distribution", ")", "\n", "\n", "find_interpretability_loss", "=", "compute_interpretability_loss", "(", "predicted_find_distribution", ",", "gold_find_distribution", ")", "\n", "symbols_interpretability_loss", "=", "compute_interpretability_loss", "(", "predicted_symbol_distribution", ",", "\n", "gold_symbol_distribution", ")", "\n", "final_interpretability_loss", "=", "find_interpretability_loss", "+", "symbols_interpretability_loss", "\n", "\n", "# Removing the training 1 in num1/date1", "\n", "symbolname", "=", "predicted_module_outputs", "[", "1", "]", "[", "0", "]", "[", ":", "-", "1", "]", "\n", "MODULEWISE_INTERPRETABILITY", "[", "symbolname", "]", "+=", "symbols_interpretability_loss", "\n", "MODULEWISE_COUNT", "[", "symbolname", "]", "+=", "1", "\n", "MODULEWISE_INTERPRETABILITY", "[", "\"find\"", "]", "+=", "find_interpretability_loss", "\n", "MODULEWISE_COUNT", "[", "\"find\"", "]", "+=", "1", "\n", "\n", "MODULE_SCORE", ".", "append", "(", "find_interpretability_loss", ")", "\n", "\n", "return", "final_interpretability_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.interpretability_TWO_FINDS_TWO_SYMBOL": [[94, 126], ["numpy.maximum", "numpy.maximum", "compute_interpretability.compute_interpretability_loss", "compute_interpretability.compute_interpretability_loss", "MODULE_SCORE.append"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.compute_interpretability_loss", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.compute_interpretability_loss"], ["", "def", "interpretability_TWO_FINDS_TWO_SYMBOL", "(", "predicted_module_outputs", ",", "gold_module_outputs", ")", ":", "\n", "    ", "\"\"\" The gold for this would contain two modules, \"find-two-events\" and \"ground-two-{dates, nums}\"\n        The predicted for this would first contain a two \"finds\" and then two \"{date,num}{1,2}\"\n\n        The predicted \"find\"s need to be combined to be compared with the gold \"find-two-events\"\n        the two predicted symbol distributions need to be merged into one and compared to \"ground-two-{dates, nums}\"\n    \"\"\"", "\n", "gold_find_distribution", "=", "gold_module_outputs", "[", "0", "]", "[", "1", "]", "\n", "gold_symbol_distribution", "=", "gold_module_outputs", "[", "1", "]", "[", "1", "]", "\n", "\n", "predicted_find_1_distribution", "=", "predicted_module_outputs", "[", "0", "]", "[", "1", "]", "\n", "predicted_find_2_distribution", "=", "predicted_module_outputs", "[", "1", "]", "[", "1", "]", "\n", "predicted_symbol_1_distribution", "=", "predicted_module_outputs", "[", "2", "]", "[", "1", "]", "\n", "predicted_symbol_2_distribution", "=", "predicted_module_outputs", "[", "3", "]", "[", "1", "]", "\n", "\n", "predicted_find_distribution", "=", "np", ".", "maximum", "(", "predicted_find_1_distribution", ",", "predicted_find_2_distribution", ")", "\n", "predicted_symbol_distribution", "=", "np", ".", "maximum", "(", "predicted_symbol_1_distribution", ",", "predicted_symbol_2_distribution", ")", "\n", "\n", "find_interpretability_loss", "=", "compute_interpretability_loss", "(", "predicted_find_distribution", ",", "gold_find_distribution", ")", "\n", "symbols_interpretability_loss", "=", "compute_interpretability_loss", "(", "predicted_symbol_distribution", ",", "\n", "gold_symbol_distribution", ")", "\n", "final_interpretability_loss", "=", "find_interpretability_loss", "+", "symbols_interpretability_loss", "\n", "\n", "symbolname", "=", "predicted_module_outputs", "[", "2", "]", "[", "0", "]", "[", ":", "-", "1", "]", "\n", "MODULEWISE_INTERPRETABILITY", "[", "symbolname", "]", "+=", "symbols_interpretability_loss", "\n", "MODULEWISE_COUNT", "[", "symbolname", "]", "+=", "1", "\n", "MODULEWISE_INTERPRETABILITY", "[", "\"find\"", "]", "+=", "find_interpretability_loss", "\n", "MODULEWISE_COUNT", "[", "\"find\"", "]", "+=", "1", "\n", "\n", "MODULE_SCORE", ".", "append", "(", "find_interpretability_loss", ")", "\n", "\n", "return", "final_interpretability_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.interpretability_N": [[128, 147], ["range", "compute_interpretability.compute_interpretability_loss", "MODULE_SCORE.append"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.compute_interpretability_loss"], ["", "def", "interpretability_N", "(", "predicted_module_outputs", ",", "gold_module_outputs", ",", "N", ":", "str", ")", ":", "\n", "    ", "\"\"\" The first modules are used for interpretability evaluation \"\"\"", "\n", "num_mapping", "=", "{", "'ONE'", ":", "1", ",", "'TWO'", ":", "2", ",", "'THREE'", ":", "3", ",", "'FOUR'", ":", "4", ",", "'FIVE'", ":", "5", "}", "\n", "N", ":", "int", "=", "num_mapping", "[", "N", "]", "\n", "final_interpretability_loss", "=", "0.0", "\n", "for", "n", "in", "range", "(", "N", ")", ":", "\n", "        ", "predicted_module_name", "=", "predicted_module_outputs", "[", "n", "]", "[", "0", "]", "\n", "gold_module_name", "=", "gold_module_outputs", "[", "n", "]", "[", "0", "]", "\n", "predicted_find_attention", "=", "predicted_module_outputs", "[", "n", "]", "[", "1", "]", "\n", "gold_find_spans", "=", "gold_module_outputs", "[", "n", "]", "[", "1", "]", "\n", "interpretability_loss", "=", "compute_interpretability_loss", "(", "predicted_find_attention", ",", "gold_find_spans", ")", "\n", "final_interpretability_loss", "+=", "interpretability_loss", "\n", "# print(f\"{gold_module_name}  {predicted_module_name}\")", "\n", "MODULEWISE_INTERPRETABILITY", "[", "predicted_module_name", "]", "+=", "interpretability_loss", "\n", "MODULEWISE_COUNT", "[", "predicted_module_name", "]", "+=", "1", "\n", "\n", "MODULE_SCORE", ".", "append", "(", "interpretability_loss", ")", "\n", "\n", "", "return", "final_interpretability_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.readDataset": [[167, 171], ["open", "json.load"], "function", ["None"], ["", "", "def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.read_jsonl": [[173, 179], ["open", "dicts.append", "json.loads"], "function", ["None"], ["", "def", "read_jsonl", "(", "input_jsonl", ")", ":", "\n", "    ", "dicts", "=", "[", "]", "\n", "with", "open", "(", "input_jsonl", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "dicts", ".", "append", "(", "json", ".", "loads", "(", "line", ")", ")", "\n", "", "", "return", "dicts", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.convert_to_nparray": [[181, 183], ["numpy.array"], "function", ["None"], ["", "def", "convert_to_nparray", "(", "attention_score", ")", ":", "\n", "    ", "return", "np", ".", "array", "(", "attention_score", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.get_queryid2annotations": [[185, 195], ["module_output_gold.items"], "function", ["None"], ["", "def", "get_queryid2annotations", "(", "module_output_gold", ":", "Dict", ")", ":", "\n", "    ", "qid2annotations", "=", "{", "}", "\n", "for", "pid", ",", "pinfo", "in", "module_output_gold", ".", "items", "(", ")", ":", "\n", "        ", "qa_pairs", "=", "pinfo", "[", "\"qa_pairs\"", "]", "\n", "for", "qa", "in", "qa_pairs", ":", "\n", "            ", "query_id", "=", "qa", "[", "\"query_id\"", "]", "\n", "annotations", "=", "qa", "[", "\"module_output_annotations\"", "]", "\n", "qid2annotations", "[", "query_id", "]", "=", "annotations", "\n", "\n", "", "", "return", "qid2annotations", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.compute_interpretability_score": [[197, 297], ["compute_interpretability.get_queryid2annotations", "print", "print", "sum", "sum", "sum", "sum", "MODULEWISE_INTERPRETABILITY.items", "print", "print", "compute_interpretability.Example", "float", "MODULEWISE_INTERPRETABILITY.pop", "MODULEWISE_COUNT.pop", "print", "open", "print", "print", "print", "compute_interpretability.interpretability_TWO_FINDS_TWO_SYMBOL", "outf.write", "outf.write", "compute_interpretability.interpretability_FIND_TWO_SYMBOL", "str", "compute_interpretability.interpretability_N", "print"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.get_queryid2annotations", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.interpretability_TWO_FINDS_TWO_SYMBOL", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.interpretability_FIND_TWO_SYMBOL", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.interpretability_N"], ["", "def", "compute_interpretability_score", "(", "module_output_predictions", ":", "List", "[", "Dict", "]", ",", "module_output_gold", ":", "Dict", ")", ":", "\n", "    ", "\"\"\" Compute interpretability scores given module predictions and gold-annotations\n\n    For each question we assume the prediction is made for the gold program. Each prediction hence would contain a\n    passage-attention for the relevant modules in the program. Similarly, the gold data would contain gold passage\n    attentions for the relevant modules. Interpretability score will be computed using the predicted and gold p-attn.\n\n    The gold data sometimes combines p-attn for multiple modules, mainly for the find-module. For e.g. date-compare\n    requires two calls to the find module and hence the predicted attentions would contain two calls to the same.\n    Whereas, the gold annotation would contain a single annotation under the name of \"find-events\".\n\n    Interpretability score for a (predicted, gold) passage attention pair is the cross-entropy loss.\n\n\n    Args:\n    module_output_predictions: `List[Dict]`\n        Each example contains \"query_id\", \"question\", \"qtype\", \"f1\", \"em\", \"predicted_logical_form\", and\n        \"module_outputs\" as keys. \"module_outputs\" is a list of (module_name, pattn) tuples\n\n    module_output_gold: `Dict`\n        Subset of the DROP dev data which is Interpret-dev gold data. Each qa_pair contains an additional key,\n        \"module_output_annotations\" which is a List containing (\"module_name\", List[Span]) annotations.\n        Each span is token-offset tuple (start, end) with exclusive-end.\n    \"\"\"", "\n", "skipped_due_to_predicted", "=", "0", "\n", "qid2annotations", "=", "get_queryid2annotations", "(", "module_output_gold", ")", "\n", "interpretability_loss", "=", "0.0", "\n", "num_examples", "=", "0", "\n", "for", "example_dict", "in", "module_output_predictions", ":", "\n", "        ", "example", "=", "Example", "(", "example_dict", ")", "\n", "gold_annotations", "=", "qid2annotations", "[", "example", ".", "query_id", "]", "\n", "qtype", "=", "example", ".", "qtype", "\n", "gold_logical_forms", "=", "lfs", ".", "qtype2logicalforms", "[", "qtype", "]", "\n", "\n", "# TODO: Add an assert that the predicted logical form conforms to the gold-qtype", "\n", "if", "example", ".", "predicted_program", "not", "in", "gold_logical_forms", ":", "\n", "            ", "skipped_due_to_predicted", "+=", "1", "\n", "print", "(", "example", ".", "question", ")", "\n", "print", "(", "example", ".", "qtype", ")", "\n", "print", "(", "example", ".", "predicted_program", ")", "\n", "continue", "\n", "\n", "", "num_examples", "+=", "1", "\n", "# Depending on the qtype, we have expectations on the modules that the annotation and gold would contain.", "\n", "# We would now compute the interpretability score based on that", "\n", "supervision_type", "=", "QTYPE_TO_SUPERVISION", "[", "qtype", "]", "\n", "if", "supervision_type", "==", "'TWO_FINDS_TWO_SYMBOL'", ":", "\n", "            ", "interpretability_loss", "+=", "interpretability_TWO_FINDS_TWO_SYMBOL", "(", "\n", "predicted_module_outputs", "=", "example", ".", "module_outputs", ",", "gold_module_outputs", "=", "gold_annotations", ")", "\n", "", "elif", "supervision_type", "==", "'FIND_TWO_SYMBOL'", ":", "\n", "            ", "interpretability_loss", "+=", "interpretability_FIND_TWO_SYMBOL", "(", "predicted_module_outputs", "=", "example", ".", "module_outputs", ",", "\n", "gold_module_outputs", "=", "gold_annotations", ")", "\n", "", "elif", "supervision_type", "in", "[", "'ONE'", ",", "'TWO'", ",", "'THREE'", ",", "'FOUR'", ",", "'FIVE'", "]", ":", "\n", "            ", "interpretability_loss", "+=", "interpretability_N", "(", "predicted_module_outputs", "=", "example", ".", "module_outputs", ",", "\n", "gold_module_outputs", "=", "gold_annotations", ",", "N", "=", "supervision_type", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "supervision_type", ")", "\n", "# raise NotImplementedError(\"Supervision Type is unknown: {}\".format(supervision_type))", "\n", "\n", "", "", "interpretability_loss", "=", "float", "(", "interpretability_loss", ")", "/", "num_examples", "\n", "print", "(", "\"Skipped due to predicted: {}\"", ".", "format", "(", "skipped_due_to_predicted", ")", ")", "\n", "print", "(", "\"Interpretability Loss (lower is better): {}\"", ".", "format", "(", "interpretability_loss", ")", ")", "\n", "\n", "\n", "# Merge min and max interpretability scores --", "\n", "keys_to_merge", "=", "[", "'max-pattn'", ",", "'min-pattn'", "]", "\n", "MODULEWISE_INTERPRETABILITY", "[", "'minmax-pattn'", "]", "=", "sum", "(", "[", "MODULEWISE_INTERPRETABILITY", "[", "x", "]", "for", "x", "in", "keys_to_merge", "]", ")", "\n", "MODULEWISE_COUNT", "[", "'minmax-pattn'", "]", "=", "sum", "(", "[", "MODULEWISE_COUNT", "[", "x", "]", "for", "x", "in", "keys_to_merge", "]", ")", "\n", "for", "key", "in", "keys_to_merge", ":", "\n", "        ", "MODULEWISE_INTERPRETABILITY", ".", "pop", "(", "key", ")", "\n", "MODULEWISE_COUNT", ".", "pop", "(", "key", ")", "\n", "\n", "", "keys_to_merge", "=", "[", "'find-date'", ",", "'find-num'", "]", "\n", "MODULEWISE_INTERPRETABILITY", "[", "'find-arg'", "]", "=", "sum", "(", "[", "MODULEWISE_INTERPRETABILITY", "[", "x", "]", "for", "x", "in", "keys_to_merge", "]", ")", "\n", "MODULEWISE_COUNT", "[", "'find-arg'", "]", "=", "sum", "(", "[", "MODULEWISE_COUNT", "[", "x", "]", "for", "x", "in", "keys_to_merge", "]", ")", "\n", "# for key in keys_to_merge:", "\n", "#     MODULEWISE_INTERPRETABILITY.pop(key)", "\n", "#     MODULEWISE_COUNT.pop(key)", "\n", "\n", "MODULEWISE_INTERPRETABILITY_AVG", "=", "{", "}", "\n", "micro_total_score", "=", "0.0", "\n", "micro_sum", "=", "0", "\n", "\n", "\n", "for", "module", ",", "int_score", "in", "MODULEWISE_INTERPRETABILITY", ".", "items", "(", ")", ":", "\n", "        ", "print", "(", "module", ")", "\n", "MODULEWISE_INTERPRETABILITY_AVG", "[", "module", "]", "=", "int_score", "/", "MODULEWISE_COUNT", "[", "module", "]", "\n", "micro_total_score", "+=", "int_score", "\n", "micro_sum", "+=", "MODULEWISE_COUNT", "[", "module", "]", "\n", "\n", "", "micro_avg", "=", "micro_total_score", "/", "micro_sum", "\n", "\n", "print", "(", "\"Interpretability Micro Avg (lower is better): {}\"", ".", "format", "(", "micro_avg", ")", ")", "\n", "\n", "print", "(", "MODULEWISE_INTERPRETABILITY_AVG", ")", "\n", "\n", "with", "open", "(", "\"interpret_drop/strong.txt\"", ",", "'w'", ")", "as", "outf", ":", "\n", "        ", "for", "score", "in", "MODULE_SCORE", ":", "\n", "            ", "outf", ".", "write", "(", "str", "(", "score", ")", ")", "\n", "outf", ".", "write", "(", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.main": [[299, 305], ["compute_interpretability.read_jsonl", "compute_interpretability.readDataset", "compute_interpretability.compute_interpretability_score"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.read_jsonl", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.data_manipulation.simplify_programs.readDataset", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.interpret_drop.compute_interpretability.compute_interpretability_score"], ["", "", "", "def", "main", "(", "args", ")", ":", "\n", "    ", "module_output_pred", "=", "read_jsonl", "(", "args", ".", "module_output_pred_jsonl", ")", "\n", "module_output_gold", "=", "readDataset", "(", "args", ".", "module_output_anno_json", ")", "\n", "\n", "compute_interpretability_score", "(", "module_output_predictions", "=", "module_output_pred", ",", "\n", "module_output_gold", "=", "module_output_gold", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.data_manipulation.simplify_programs.readDataset": [[37, 41], ["open", "json.load"], "function", ["None"], ["def", "readDataset", "(", "input_json", ")", ":", "\n", "    ", "with", "open", "(", "input_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "dataset", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.data_manipulation.simplify_programs.replace_program_supervision": [[43, 57], ["dataset.items", "qa.pop"], "function", ["None"], ["", "def", "replace_program_supervision", "(", "dataset", ")", ":", "\n", "    ", "for", "para_id", ",", "para_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "        ", "qa_pairs", "=", "para_info", "[", "\"qa_pairs\"", "]", "\n", "for", "qa", "in", "qa_pairs", ":", "\n", "            ", "if", "\"qtype\"", "in", "qa", ":", "\n", "                ", "qtype", "=", "qa", "[", "\"qtype\"", "]", "\n", "if", "qtype", "in", "ORIG_TO_SIMPLE_MAPPING", ":", "\n", "                    ", "new_qtype", "=", "ORIG_TO_SIMPLE_MAPPING", "[", "qtype", "]", "\n", "qa", "[", "\"qtype\"", "]", "=", "new_qtype", "\n", "", "if", "qtype", "in", "REMOVE_PROGRAM_SUP", ":", "\n", "                    ", "qa", ".", "pop", "(", "\"qtype\"", ")", "\n", "for", "key", "in", "supervision_keys", ":", "\n", "                        ", "qa", "[", "key", "]", "=", "False", "\n", "", "", "", "", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.data_manipulation.simplify_programs.main": [[59, 85], ["os.path.join", "os.path.join", "os.path.join", "os.path.join", "simplify_programs.readDataset", "simplify_programs.readDataset", "simplify_programs.replace_program_supervision", "simplify_programs.replace_program_supervision", "os.path.exists", "os.makedirs", "open", "json.dump", "open", "json.dump"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.data_manipulation.simplify_programs.readDataset", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.data_manipulation.simplify_programs.readDataset", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.data_manipulation.simplify_programs.replace_program_supervision", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.data_manipulation.simplify_programs.replace_program_supervision"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "input_dir", "=", "args", ".", "input_dir", "\n", "output_dir", "=", "args", ".", "output_dir", "\n", "\n", "train_json", "=", "\"drop_dataset_train.json\"", "\n", "dev_json", "=", "\"drop_dataset_dev.json\"", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "", "input_trnfp", "=", "os", ".", "path", ".", "join", "(", "input_dir", ",", "train_json", ")", "\n", "input_devfp", "=", "os", ".", "path", ".", "join", "(", "input_dir", ",", "dev_json", ")", "\n", "output_trnfp", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "train_json", ")", "\n", "output_devfp", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "dev_json", ")", "\n", "\n", "train_dataset", "=", "readDataset", "(", "input_trnfp", ")", "\n", "dev_dataset", "=", "readDataset", "(", "input_devfp", ")", "\n", "\n", "new_tr_dataset", "=", "replace_program_supervision", "(", "train_dataset", ")", "\n", "new_dev_dataset", "=", "replace_program_supervision", "(", "dev_dataset", ")", "\n", "\n", "with", "open", "(", "output_trnfp", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "new_tr_dataset", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n", "", "with", "open", "(", "output_devfp", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "new_dev_dataset", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.pattn2count_predictor.Pattn2CountPredictor.__init__": [[18, 20], ["allennlp.predictors.predictor.Predictor.__init__"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__"], ["def", "__init__", "(", "self", ",", "model", ":", "Model", ",", "dataset_reader", ":", "DatasetReader", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model", ",", "dataset_reader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.pattn2count_predictor.Pattn2CountPredictor.predict_instance": [[21, 25], ["pattn2count_predictor.Pattn2CountPredictor._model.forward_on_instance", "allennlp.common.util.sanitize"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "predict_instance", "(", "self", ",", "instance", ":", "Instance", ")", "->", "JsonDict", ":", "\n", "        ", "outputs", "=", "self", ".", "_model", ".", "forward_on_instance", "(", "instance", ")", "\n", "return", "sanitize", "(", "outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.pattn2count_predictor.Pattn2CountPredictor.predict_batch_instance": [[26, 29], ["pattn2count_predictor.Pattn2CountPredictor._model.forward_on_instances", "allennlp.common.util.sanitize"], "methods", ["None"], ["", "def", "predict_batch_instance", "(", "self", ",", "instances", ":", "List", "[", "Instance", "]", ")", "->", "List", "[", "JsonDict", "]", ":", "\n", "        ", "outputs", "=", "self", ".", "_model", ".", "forward_on_instances", "(", "instances", ")", "\n", "return", "sanitize", "(", "outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.pattn2count_predictor.Pattn2CountPredictor.dump_line": [[30, 60], ["utils.round_all", "utils.round_all", "list", "zip", "len", "sum", "sum"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "@", "overrides", "\n", "def", "dump_line", "(", "self", ",", "outputs", ":", "JsonDict", ")", "->", "str", ":", "# pylint: disable=no-self-use", "\n", "# Use json.dumps(outputs) + \"\\n\" to dump a dictionary", "\n", "\n", "        ", "out_str", "=", "\"\"", "\n", "pattn", "=", "outputs", "[", "\"passage_attention\"", "]", "\n", "pattn", "=", "myutils", ".", "round_all", "(", "pattn", ",", "4", ")", "\n", "psigmoid", "=", "outputs", "[", "\"passage_sigmoid\"", "]", "\n", "psigmoid", "=", "myutils", ".", "round_all", "(", "psigmoid", ",", "4", ")", "\n", "\n", "attn_sigm", "=", "list", "(", "zip", "(", "pattn", ",", "psigmoid", ")", ")", "\n", "\n", "passage_count_mean", "=", "outputs", "[", "\"count_mean\"", "]", "\n", "count_distribution", "=", "outputs", "[", "\"count_distritbuion\"", "]", "\n", "count_answer", "=", "outputs", "[", "\"count_answer\"", "]", "\n", "pred_count_idx", "=", "outputs", "[", "\"pred_count\"", "]", "\n", "\n", "out_str", "+=", "f\"Pattn: {pattn}\"", "+", "\"\\n\"", "\n", "out_str", "+=", "f\"Psigm: {psigmoid}\"", "+", "\"\\n\"", "\n", "out_str", "+=", "f\"Pattn_sigm: {attn_sigm}\"", "+", "\"\\n\"", "\n", "out_str", "+=", "f\"Plen: {len(pattn)}\"", "+", "\"\\n\"", "\n", "out_str", "+=", "f\"PattnSum: {sum(pattn)}\"", "+", "\"\\n\"", "\n", "out_str", "+=", "f\"PSigmSum: {sum(psigmoid)}\"", "+", "\"\\n\"", "\n", "out_str", "+=", "f\"CountMean: {passage_count_mean}\"", "+", "\"\\n\"", "\n", "out_str", "+=", "f\"CountDist: {count_distribution}\"", "+", "\"\\n\"", "\n", "out_str", "+=", "f\"CountAnswer: {count_answer}\"", "+", "\"\\n\"", "\n", "out_str", "+=", "f\"Predicted CountAnswer: {pred_count_idx}\"", "+", "\"\\n\"", "\n", "out_str", "+=", "\"--------------------------------------------------\\n\"", "\n", "\n", "return", "out_str", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.drop_parser_predictor.DropNMNPredictor.__init__": [[268, 270], ["allennlp.predictors.predictor.Predictor.__init__"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__"], ["def", "__init__", "(", "self", ",", "model", ":", "Model", ",", "dataset_reader", ":", "DatasetReader", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model", ",", "dataset_reader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.drop_parser_predictor.DropNMNPredictor.predict_instance": [[271, 275], ["drop_parser_predictor.DropNMNPredictor._model.forward_on_instance", "allennlp.common.util.sanitize"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "predict_instance", "(", "self", ",", "instance", ":", "Instance", ")", "->", "JsonDict", ":", "\n", "        ", "outputs", "=", "self", ".", "_model", ".", "forward_on_instance", "(", "instance", ")", "\n", "return", "sanitize", "(", "outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.drop_parser_predictor.DropNMNPredictor.predict_batch_instance": [[276, 279], ["drop_parser_predictor.DropNMNPredictor._model.forward_on_instances", "allennlp.common.util.sanitize"], "methods", ["None"], ["", "def", "predict_batch_instance", "(", "self", ",", "instances", ":", "List", "[", "Instance", "]", ")", "->", "List", "[", "JsonDict", "]", ":", "\n", "        ", "outputs", "=", "self", ".", "_model", ".", "forward_on_instances", "(", "instances", ")", "\n", "return", "sanitize", "(", "outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.drop_parser_predictor.DropNMNPredictor._print_ExecutionValTree": [[280, 293], ["str", "str", "debug_value.replace.replace.replace", "len", "drop_parser_predictor.DropNMNPredictor._print_ExecutionValTree"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.DROPDemoPredictor._print_ExecutionValTree"], ["", "def", "_print_ExecutionValTree", "(", "self", ",", "exval_tree", ",", "depth", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        exval_tree: [[root_func_name, value], [], [], []]\n        \"\"\"", "\n", "tabs", "=", "\"\\t\"", "*", "depth", "\n", "func_name", "=", "str", "(", "exval_tree", "[", "0", "]", "[", "0", "]", ")", "\n", "debug_value", "=", "str", "(", "exval_tree", "[", "0", "]", "[", "1", "]", ")", "\n", "debug_value", "=", "debug_value", ".", "replace", "(", "\"\\n\"", ",", "\"\\n\"", "+", "tabs", ")", "\n", "outstr", "=", "f\"{tabs}{func_name}  :\\n {tabs}{debug_value}\\n\"", "\n", "if", "len", "(", "exval_tree", ")", ">", "1", ":", "\n", "            ", "for", "child", "in", "exval_tree", "[", "1", ":", "]", ":", "\n", "                ", "outstr", "+=", "self", ".", "_print_ExecutionValTree", "(", "child", ",", "depth", "+", "1", ")", "\n", "", "", "return", "outstr", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.drop_parser_predictor.DropNMNPredictor.dump_line": [[294, 352], ["drop_parser_predictor.f1metric", "int", "zip", "output_dict[].append", "json.dumps", "sum", "len", "list", "module_dict.items"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.f1metric", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "@", "overrides", "\n", "def", "dump_line", "(", "self", ",", "outputs", ":", "JsonDict", ")", "->", "str", ":", "# pylint: disable=no-self-use", "\n", "# Use json.dumps(outputs) + \"\\n\" to dump a dictionary", "\n", "\n", "        ", "metadata", "=", "outputs", "[", "\"metadata\"", "]", "\n", "predicted_ans", "=", "outputs", "[", "\"predicted_answer\"", "]", "\n", "module_debug_infos", "=", "outputs", "[", "\"modules_debug_infos\"", "]", "\n", "passage_mask", "=", "outputs", "[", "\"passage_mask\"", "]", "\n", "passage_token_idxs", "=", "outputs", "[", "\"passage_token_idxs\"", "]", "\n", "\n", "gold_passage_span_ans", "=", "metadata", "[", "\"answer_passage_spans\"", "]", "if", "\"answer_passage_spans\"", "in", "metadata", "else", "[", "]", "\n", "gold_question_span_ans", "=", "metadata", "[", "\"answer_question_spans\"", "]", "if", "\"answer_question_spans\"", "in", "metadata", "else", "[", "]", "\n", "\n", "# instance_spans_for_all_progs = outputs['predicted_spans']", "\n", "# best_span = instance_spans_for_all_progs[0]", "\n", "question_id", "=", "metadata", "[", "\"question_id\"", "]", "\n", "question", "=", "metadata", "[", "\"original_question\"", "]", "\n", "qtype", "=", "metadata", "[", "\"qtype\"", "]", "\n", "passage", "=", "metadata", "[", "\"original_passage\"", "]", "\n", "passage_id", "=", "metadata", "[", "\"passage_id\"", "]", "\n", "passage_tokens", "=", "metadata", "[", "\"passage_orig_tokens\"", "]", "\n", "passage_wps", "=", "metadata", "[", "\"passage_tokens\"", "]", "\n", "passage_wpidx2tokenidx", "=", "metadata", "[", "\"passage_wpidx2tokenidx\"", "]", "\n", "answer_annotation_dicts", "=", "metadata", "[", "\"answer_annotations\"", "]", "\n", "passage_date_values", "=", "metadata", "[", "\"passage_date_values\"", "]", "\n", "passage_num_values", "=", "metadata", "[", "\"passage_number_values\"", "]", "\n", "composed_numbers", "=", "metadata", "[", "\"composed_numbers\"", "]", "\n", "passage_year_diffs", "=", "metadata", "[", "\"passage_year_diffs\"", "]", "\n", "(", "exact_match", ",", "f1_score", ")", "=", "f1metric", "(", "predicted_ans", ",", "answer_annotation_dicts", ")", "\n", "\n", "output_dict", "=", "{", "\n", "\"passage_id\"", ":", "passage_id", ",", "\n", "\"query_id\"", ":", "question_id", ",", "\n", "\"question\"", ":", "question", ",", "\n", "\"qtype\"", ":", "qtype", ",", "\n", "\"f1\"", ":", "f1_score", ",", "\n", "\"em\"", ":", "exact_match", "\n", "}", "\n", "logical_forms", "=", "outputs", "[", "\"logical_forms\"", "]", "\n", "best_logical_form", "=", "logical_forms", "[", "0", "]", "\n", "output_dict", "[", "\"predicted_logical_form\"", "]", "=", "best_logical_form", "\n", "\n", "# List of dictionary where each dictionary contains a single module_name: pattn-value pair", "\n", "module_debug_info", ":", "List", "[", "Dict", "]", "=", "module_debug_infos", "[", "0", "]", "\n", "# This is the length of the passage-wps w/o [SEP] at the end", "\n", "len_passage_wps", "=", "int", "(", "sum", "(", "passage_mask", ")", ")", "-", "1", "\n", "output_dict", "[", "\"module_outputs\"", "]", "=", "[", "]", "\n", "module_names", "=", "\"\"", "\n", "for", "module_dict", "in", "module_debug_info", ":", "\n", "            ", "passage_attention", "=", "[", "0.0", "]", "*", "len", "(", "passage_tokens", ")", "\n", "module_name", ",", "pattn_wps", "=", "list", "(", "module_dict", ".", "items", "(", ")", ")", "[", "0", "]", "\n", "module_names", "+=", "module_name", "+", "\" \"", "\n", "pattn_wps", "=", "pattn_wps", "[", ":", "len_passage_wps", "]", "# passage-attention over word-pieces", "\n", "for", "token_idx", ",", "attn_value", "in", "zip", "(", "passage_wpidx2tokenidx", ",", "pattn_wps", ")", ":", "\n", "                ", "passage_attention", "[", "token_idx", "]", "+=", "attn_value", "\n", "", "output_dict", "[", "\"module_outputs\"", "]", ".", "append", "(", "(", "module_name", ",", "passage_attention", ")", ")", "\n", "\n", "", "return", "json", ".", "dumps", "(", "output_dict", ")", "+", "\"\\n\"", "", "", "", ""]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.drop_parser_predictor.DropQANetPredictor.__init__": [[164, 166], ["allennlp.predictors.predictor.Predictor.__init__"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__"], ["def", "__init__", "(", "self", ",", "model", ":", "Model", ",", "dataset_reader", ":", "DatasetReader", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model", ",", "dataset_reader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.drop_parser_predictor.DropQANetPredictor.predict_instance": [[167, 171], ["drop_parser_predictor.DropQANetPredictor._model.forward_on_instance", "allennlp.common.util.sanitize"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "predict_instance", "(", "self", ",", "instance", ":", "Instance", ")", "->", "JsonDict", ":", "\n", "        ", "outputs", "=", "self", ".", "_model", ".", "forward_on_instance", "(", "instance", ")", "\n", "return", "sanitize", "(", "outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.drop_parser_predictor.DropQANetPredictor.predict_batch_instance": [[172, 175], ["drop_parser_predictor.DropQANetPredictor._model.forward_on_instances", "allennlp.common.util.sanitize"], "methods", ["None"], ["", "def", "predict_batch_instance", "(", "self", ",", "instances", ":", "List", "[", "Instance", "]", ")", "->", "List", "[", "JsonDict", "]", ":", "\n", "        ", "outputs", "=", "self", ".", "_model", ".", "forward_on_instances", "(", "instances", ")", "\n", "return", "sanitize", "(", "outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.drop_parser_predictor.DropQANetPredictor.dump_line": [[176, 203], ["drop_parser_predictor.f1metric"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.f1metric"], ["", "@", "overrides", "\n", "def", "dump_line", "(", "self", ",", "outputs", ":", "JsonDict", ")", "->", "str", ":", "# pylint: disable=no-self-use", "\n", "# Use json.dumps(outputs) + \"\\n\" to dump a dictionary", "\n", "\n", "        ", "out_str", "=", "\"\"", "\n", "metadata", "=", "outputs", "[", "\"metadata\"", "]", "\n", "predicted_ans", "=", "outputs", "[", "\"predicted_answer\"", "]", "\n", "\n", "# instance_spans_for_all_progs = outputs['predicted_spans']", "\n", "# best_span = instance_spans_for_all_progs[0]", "\n", "question_id", "=", "metadata", "[", "\"question_id\"", "]", "\n", "question", "=", "metadata", "[", "\"original_question\"", "]", "\n", "answer_annotation_dicts", "=", "metadata", "[", "\"answer_annotations\"", "]", "\n", "(", "exact_match", ",", "f1_score", ")", "=", "f1metric", "(", "predicted_ans", ",", "answer_annotation_dicts", ")", "\n", "\n", "correct_or_not", "=", "\"NC\"", "\n", "if", "f1_score", ">=", "0.75", ":", "\n", "            ", "correct_or_not", "=", "\"C\"", "\n", "\n", "", "logical_form", "=", "outputs", "[", "\"logical_forms\"", "]", "[", "0", "]", "\n", "\n", "out_str", "+=", "question_id", "+", "\"\\t\"", "\n", "out_str", "+=", "question", "+", "\"\\t\"", "\n", "out_str", "+=", "correct_or_not", "+", "\"\\t\"", "\n", "out_str", "+=", "logical_form", "+", "\"\\n\"", "\n", "\n", "return", "out_str", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.drop_parser_predictor.MTMSNStylePredictor.__init__": [[211, 213], ["allennlp.predictors.predictor.Predictor.__init__"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__"], ["def", "__init__", "(", "self", ",", "model", ":", "Model", ",", "dataset_reader", ":", "DatasetReader", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model", ",", "dataset_reader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.drop_parser_predictor.MTMSNStylePredictor.predict_instance": [[214, 218], ["drop_parser_predictor.MTMSNStylePredictor._model.forward_on_instance", "allennlp.common.util.sanitize"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "predict_instance", "(", "self", ",", "instance", ":", "Instance", ")", "->", "JsonDict", ":", "\n", "        ", "outputs", "=", "self", ".", "_model", ".", "forward_on_instance", "(", "instance", ")", "\n", "return", "sanitize", "(", "outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.drop_parser_predictor.MTMSNStylePredictor.predict_batch_instance": [[219, 222], ["drop_parser_predictor.MTMSNStylePredictor._model.forward_on_instances", "allennlp.common.util.sanitize"], "methods", ["None"], ["", "def", "predict_batch_instance", "(", "self", ",", "instances", ":", "List", "[", "Instance", "]", ")", "->", "List", "[", "JsonDict", "]", ":", "\n", "        ", "outputs", "=", "self", ".", "_model", ".", "forward_on_instances", "(", "instances", ")", "\n", "return", "sanitize", "(", "outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.drop_parser_predictor.MTMSNStylePredictor.dump_line": [[223, 260], ["drop_parser_predictor.f1metric", "json.dumps"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.f1metric"], ["", "@", "overrides", "\n", "def", "dump_line", "(", "self", ",", "outputs", ":", "JsonDict", ")", "->", "str", ":", "# pylint: disable=no-self-use", "\n", "# Use json.dumps(outputs) + \"\\n\" to dump a dictionary", "\n", "\n", "        ", "out_str", "=", "\"\"", "\n", "metadata", "=", "outputs", "[", "\"metadata\"", "]", "\n", "predicted_ans", "=", "outputs", "[", "\"predicted_answer\"", "]", "\n", "\n", "# instance_spans_for_all_progs = outputs['predicted_spans']", "\n", "# best_span = instance_spans_for_all_progs[0]", "\n", "question_id", "=", "metadata", "[", "\"question_id\"", "]", "\n", "question", "=", "metadata", "[", "\"original_question\"", "]", "\n", "answer_annotation_dicts", "=", "metadata", "[", "\"answer_annotations\"", "]", "\n", "program_probs", "=", "outputs", "[", "\"batch_actionseq_probs\"", "]", "# List size is the same as number of programs predicted", "\n", "(", "exact_match", ",", "f1_score", ")", "=", "f1metric", "(", "predicted_ans", ",", "answer_annotation_dicts", ")", "\n", "logical_forms", "=", "outputs", "[", "\"logical_forms\"", "]", "\n", "if", "logical_forms", ":", "\n", "            ", "logical_form", "=", "logical_forms", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "logical_form", "=", "\"NO PROGRAM PREDICTED\"", "\n", "", "if", "program_probs", ":", "\n", "            ", "prog_prob", "=", "program_probs", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "prog_prob", "=", "0.0", "\n", "\n", "", "output_dict", "=", "{", "\n", "\"query_id\"", ":", "question_id", ",", "\n", "\"question\"", ":", "question", ",", "\n", "\"text\"", ":", "predicted_ans", ",", "\n", "\"type\"", ":", "logical_form", ",", "\n", "\"prog_prob\"", ":", "prog_prob", ",", "\n", "\"f1\"", ":", "f1_score", ",", "\n", "\"em\"", ":", "exact_match", ",", "\n", "\"gold_answer\"", ":", "answer_annotation_dicts", "\n", "}", "\n", "\n", "return", "json", ".", "dumps", "(", "output_dict", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.drop_parser_predictor.f1metric": [[16, 32], ["allennlp.tools.squad_eval.metric_max_over_ground_truths", "allennlp.tools.drop_eval.answer_json_to_strings"], "function", ["None"], ["def", "f1metric", "(", "prediction", ":", "Union", "[", "str", ",", "List", "]", ",", "ground_truths", ":", "List", ")", ":", "# type: ignore", "\n", "    ", "\"\"\"\n    Parameters\n    ----------a\n    prediction: ``Union[str, List]``\n        The predicted answer from the model evaluated. This could be a string, or a list of string\n        when multiple spans are predicted as answer.\n    ground_truths: ``List``\n        All the ground truth answer annotations.\n    \"\"\"", "\n", "# If you wanted to split this out by answer type, you could look at [1] here and group by", "\n", "# that, instead of only keeping [0].", "\n", "ground_truth_answer_strings", "=", "[", "answer_json_to_strings", "(", "annotation", ")", "[", "0", "]", "for", "annotation", "in", "ground_truths", "]", "\n", "exact_match", ",", "f1_score", "=", "metric_max_over_ground_truths", "(", "drop_em_and_f1", ",", "prediction", ",", "ground_truth_answer_strings", ")", "\n", "\n", "return", "(", "exact_match", ",", "f1_score", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.Input.__init__": [[121, 124], ["str"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "name", ":", "str", ",", "tokens", ":", "List", ")", ":", "\n", "        ", "self", ".", "name", ":", "str", "=", "name", "\n", "self", ".", "tokens", ":", "List", "[", "str", "]", "=", "[", "str", "(", "t", ")", "for", "t", "in", "tokens", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.Input.to_dict": [[125, 131], ["None"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "json_dict", "=", "{", "\n", "\"name\"", ":", "self", ".", "name", ",", "\n", "\"tokens\"", ":", "self", ".", "tokens", "\n", "}", "\n", "return", "json_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.Output.__init__": [[134, 138], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "input_name", ":", "str", ",", "values", ":", "List", "[", "float", "]", ",", "label", ":", "Optional", "[", "str", "]", "=", "None", ")", ":", "\n", "        ", "self", ".", "input_name", "=", "input_name", "\n", "self", ".", "values", "=", "values", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.Output.to_dict": [[139, 146], ["None"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "json_dict", "=", "{", "\n", "\"input_name\"", ":", "self", ".", "input_name", ",", "\n", "\"values\"", ":", "self", ".", "values", ",", "\n", "\"label\"", ":", "self", ".", "label", "\n", "}", "\n", "return", "json_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.Module.__init__": [[149, 152], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "name", ":", "str", ",", "identifier", ":", "int", ")", ":", "\n", "        ", "self", ".", "name", "=", "name", "\n", "self", ".", "identifier", "=", "identifier", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.Module.to_dict": [[153, 159], ["None"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "json_dict", "=", "{", "\n", "\"name\"", ":", "self", ".", "name", ",", "\n", "\"identifier\"", ":", "self", ".", "identifier", "\n", "}", "\n", "return", "json_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.DROPDemoPredictor.__init__": [[205, 209], ["allennlp.predictors.predictor.Predictor.__init__", "utils.spacyutils.getSpacyNLP", "utils.spacyutils.getWhiteTokenizerSpacyNLP"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getSpacyNLP", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getWhiteTokenizerSpacyNLP"], ["def", "__init__", "(", "self", ",", "model", ":", "Model", ",", "dataset_reader", ":", "DatasetReader", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model", ",", "dataset_reader", ")", "\n", "self", ".", "spacy_nlp", "=", "spacyutils", ".", "getSpacyNLP", "(", ")", "\n", "self", ".", "spacy_whitespacetokenizer", "=", "spacyutils", ".", "getWhiteTokenizerSpacyNLP", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.DROPDemoPredictor._json_to_instance": [[210, 296], ["unicodedata.normalize", "utils.util.pruneMultipleSpaces", "utils.spacyutils.getSpacyDoc", "demo_predictor.split_tokens_by_hyphen", "utils.spacyutils.getSpacyDoc", "sorted", "utils.spacyutils.getNER", "datasets.drop.preprocess.ner_process.parseDateNERS", "datasets.drop.preprocess.ner_process.parseNumNERS", "question_text.strip", "unicodedata.normalize", "utils.util.pruneMultipleSpaces", "utils.spacyutils.getSpacyDoc", "demo_predictor.split_tokens_by_hyphen", "utils.spacyutils.getSpacyDoc", "utils.spacyutils.getNER", "datasets.drop.preprocess.ner_process.parseDateNERS", "datasets.drop.preprocess.ner_process.parseNumNERS", "demo_predictor.DROPDemoPredictor._dataset_reader.text_to_instance", "len", "len", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.pruneMultipleSpaces", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getSpacyDoc", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.split_tokens_by_hyphen", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getSpacyDoc", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getNER", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process.parseDateNERS", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process.parseNumNERS", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.pruneMultipleSpaces", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getSpacyDoc", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.split_tokens_by_hyphen", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getSpacyDoc", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.spacyutils.getNER", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process.parseDateNERS", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.preprocess.ner_process.parseNumNERS", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.text_to_instance"], ["", "@", "overrides", "\n", "def", "_json_to_instance", "(", "self", ",", "json_dict", ":", "JsonDict", ")", "->", "Instance", ":", "\n", "        ", "question_text", "=", "json_dict", "[", "\"question\"", "]", "\n", "passage_text", "=", "json_dict", "[", "\"passage\"", "]", "\n", "\n", "spacy_nlp", "=", "self", ".", "spacy_nlp", "\n", "spacy_whitespacetokenizer", "=", "self", ".", "spacy_whitespacetokenizer", "\n", "\n", "# From datasets.drop.preprocess.tokenize", "\n", "# Parse passage", "\n", "cleaned_passage_text", "=", "unicodedata", ".", "normalize", "(", "\"NFKD\"", ",", "passage_text", ")", "\n", "cleaned_passage_text", "=", "util", ".", "pruneMultipleSpaces", "(", "cleaned_passage_text", ")", "\n", "passage_spacydoc", "=", "spacyutils", ".", "getSpacyDoc", "(", "cleaned_passage_text", ",", "spacy_nlp", ")", "\n", "passage_tokens", "=", "[", "t", "for", "t", "in", "passage_spacydoc", "]", "\n", "passage_tokens", ":", "List", "[", "Token", "]", "=", "split_tokens_by_hyphen", "(", "passage_tokens", ")", "\n", "\n", "passage_token_charidxs", "=", "[", "token", ".", "idx", "for", "token", "in", "passage_tokens", "]", "\n", "passage_token_texts", ":", "List", "[", "str", "]", "=", "[", "t", ".", "text", "for", "t", "in", "passage_tokens", "]", "\n", "# Remaking the doc for running NER on new tokenization", "\n", "new_passage_doc", "=", "spacyutils", ".", "getSpacyDoc", "(", "\" \"", ".", "join", "(", "passage_token_texts", ")", ",", "spacy_whitespacetokenizer", ")", "\n", "\n", "assert", "len", "(", "passage_tokens", ")", "==", "len", "(", "\" \"", ".", "join", "(", "passage_token_texts", ")", ".", "split", "(", "\" \"", ")", ")", "\n", "assert", "len", "(", "new_passage_doc", ")", "==", "len", "(", "passage_tokens", ")", "\n", "\n", "# List[Tuple[int, int]] -- start (inclusive) and end (exclusive) token idxs for sentence boundaries", "\n", "passage_sent_idxs", "=", "sorted", "(", "[", "(", "sentence", ".", "start", ",", "sentence", ".", "end", ")", "for", "sentence", "in", "new_passage_doc", ".", "sents", "]", ",", "\n", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "passage_ners", "=", "spacyutils", ".", "getNER", "(", "new_passage_doc", ")", "\n", "\n", "(", "p_parsed_dates", ",", "p_normalized_date_idxs", ",", "\n", "p_normalized_date_values", ",", "_", ")", "=", "ner_process", ".", "parseDateNERS", "(", "passage_ners", ",", "passage_token_texts", ")", "\n", "(", "p_parsed_nums", ",", "p_normalized_num_idxs", ",", "\n", "p_normalized_number_values", ",", "_", ")", "=", "ner_process", ".", "parseNumNERS", "(", "passage_ners", ",", "passage_token_texts", ")", "\n", "\n", "# Parse question", "\n", "question", ":", "str", "=", "question_text", ".", "strip", "(", ")", "\n", "cleaned_question", "=", "unicodedata", ".", "normalize", "(", "\"NFKD\"", ",", "question", ")", "\n", "cleaned_question", "=", "util", ".", "pruneMultipleSpaces", "(", "cleaned_question", ")", "\n", "\n", "q_spacydoc", "=", "spacyutils", ".", "getSpacyDoc", "(", "cleaned_question", ",", "spacy_nlp", ")", "\n", "question_tokens", "=", "[", "t", "for", "t", "in", "q_spacydoc", "]", "\n", "question_tokens", "=", "split_tokens_by_hyphen", "(", "question_tokens", ")", "\n", "question_token_charidxs", "=", "[", "token", ".", "idx", "for", "token", "in", "question_tokens", "]", "\n", "question_token_texts", "=", "[", "t", ".", "text", "for", "t", "in", "question_tokens", "]", "\n", "\n", "# Remaking the doc for running NER on new tokenization", "\n", "new_question_doc", "=", "spacyutils", ".", "getSpacyDoc", "(", "\" \"", ".", "join", "(", "question_token_texts", ")", ",", "spacy_whitespacetokenizer", ")", "\n", "assert", "len", "(", "new_question_doc", ")", "==", "len", "(", "question_tokens", ")", "\n", "\n", "q_ners", "=", "spacyutils", ".", "getNER", "(", "new_question_doc", ")", "\n", "(", "q_parsed_dates", ",", "q_normalized_date_idxs", ",", "\n", "q_normalized_date_values", ",", "q_num_date_entities", ")", "=", "ner_process", ".", "parseDateNERS", "(", "q_ners", ",", "question_token_texts", ")", "\n", "(", "q_parsed_nums", ",", "q_normalized_num_idxs", ",", "\n", "q_normalized_number_values", ",", "q_num_num_entities", ")", "=", "ner_process", ".", "parseNumNERS", "(", "q_ners", ",", "question_token_texts", ")", "\n", "\n", "return", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "\n", "question_text", "=", "\" \"", ".", "join", "(", "question_token_texts", ")", ",", "\n", "original_ques_text", "=", "question_text", ",", "\n", "question_charidxs", "=", "question_token_charidxs", ",", "\n", "passage_text", "=", "\" \"", ".", "join", "(", "passage_token_texts", ")", ",", "\n", "original_passage_text", "=", "passage_text", ",", "\n", "passage_charidxs", "=", "passage_token_charidxs", ",", "\n", "p_sent_boundaries", "=", "passage_sent_idxs", ",", "\n", "p_date_mens", "=", "p_parsed_dates", ",", "\n", "p_date_entidxs", "=", "p_normalized_date_idxs", ",", "\n", "p_date_normvals", "=", "p_normalized_date_values", ",", "\n", "p_num_mens", "=", "p_parsed_nums", ",", "\n", "p_num_entidxs", "=", "p_normalized_num_idxs", ",", "\n", "p_num_normvals", "=", "p_normalized_number_values", ",", "\n", "qtype", "=", "\"UNK\"", ",", "\n", "program_supervised", "=", "False", ",", "\n", "qattn_supervised", "=", "False", ",", "\n", "execution_supervised", "=", "False", ",", "\n", "pattn_supervised", "=", "False", ",", "\n", "strongly_supervised", "=", "False", ",", "\n", "ques_attn_supervision", "=", "None", ",", "\n", "date_grounding_supervision", "=", "None", ",", "\n", "num_grounding_supervision", "=", "None", ",", "\n", "passage_attn_supervision", "=", "None", ",", "\n", "synthetic_numground_metadata", "=", "None", ",", "\n", "answer_passage_spans", "=", "None", ",", "\n", "answer_question_spans", "=", "None", ",", "\n", "question_id", "=", "\"demo_question\"", ",", "\n", "passage_id", "=", "\"demo_passage\"", ",", "\n", "answer_annotations", "=", "None", ",", "\n", "max_question_len", "=", "50", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.DROPDemoPredictor.get_module_name_mapping": [[298, 319], ["module_name_mapping.get"], "methods", ["None"], ["", "def", "get_module_name_mapping", "(", "self", ",", "module_name", ")", ":", "\n", "        ", "module_name_mapping", "=", "{", "\n", "\"find_PassageAttention\"", ":", "\"find\"", ",", "\n", "\"filter_PassageAttention\"", ":", "\"filter\"", ",", "\n", "\"relocate_PassageAttention\"", ":", "\"relocate\"", ",", "\n", "\"compare_date_lesser_than\"", ":", "\"compare-date-lt\"", ",", "\n", "\"compare_date_greater_than\"", ":", "\"compare-date-gt\"", ",", "\n", "\"compare_num_lesser_than\"", ":", "\"compare_num_lt\"", ",", "\n", "\"compare_num_greater_than\"", ":", "\"compare_num_gt\"", ",", "\n", "\"year_difference\"", ":", "\"year-diff\"", ",", "\n", "\"year_difference_single_event\"", ":", "\"year-diff\"", ",", "\n", "\"find_passageSpanAnswer\"", ":", "\"span\"", ",", "\n", "\"passageAttn2Count\"", ":", "\"count\"", ",", "\n", "\"find_PassageNumber\"", ":", "\"find-num\"", ",", "\n", "\"minNumPattn\"", ":", "\"find-min-num\"", ",", "\n", "\"maxNumPattn\"", ":", "\"find-max-num\"", ",", "\n", "\"passagenumber_difference\"", ":", "\"number-difference\"", ",", "\n", "\"passagenumber_addition\"", ":", "\"number-addition\"", ",", "\n", "}", "\n", "\n", "return", "module_name_mapping", ".", "get", "(", "module_name", ",", "module_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.DROPDemoPredictor.rename_modules_in_nested_expression": [[321, 331], ["enumerate", "isinstance", "mapped_expression.append", "isinstance", "demo_predictor.DROPDemoPredictor.get_module_name_mapping", "mapped_expression.append", "demo_predictor.DROPDemoPredictor.rename_modules_in_nested_expression"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.DROPDemoPredictor.get_module_name_mapping", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.DROPDemoPredictor.rename_modules_in_nested_expression"], ["", "def", "rename_modules_in_nested_expression", "(", "self", ",", "nested_expression", ")", ":", "\n", "        ", "mapped_expression", "=", "[", "]", "\n", "for", "i", ",", "argument", "in", "enumerate", "(", "nested_expression", ")", ":", "\n", "            ", "if", "isinstance", "(", "argument", ",", "str", ")", ":", "\n", "                ", "mapped_expression", ".", "append", "(", "self", ".", "get_module_name_mapping", "(", "argument", ")", ")", "\n", "", "elif", "isinstance", "(", "argument", ",", "list", ")", ":", "\n", "                ", "mapped_expression", ".", "append", "(", "self", ".", "rename_modules_in_nested_expression", "(", "argument", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "\n", "", "", "return", "mapped_expression", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.DROPDemoPredictor.nested_expression_to_lisp": [[332, 341], ["isinstance", "isinstance", "demo_predictor.DROPDemoPredictor.nested_expression_to_lisp"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.DROPDemoPredictor.nested_expression_to_lisp"], ["", "def", "nested_expression_to_lisp", "(", "self", ",", "nested_expression", ")", ":", "\n", "        ", "if", "isinstance", "(", "nested_expression", ",", "str", ")", ":", "\n", "            ", "return", "nested_expression", "\n", "\n", "", "elif", "isinstance", "(", "nested_expression", ",", "List", ")", ":", "\n", "            ", "lisp_expressions", "=", "[", "self", ".", "nested_expression_to_lisp", "(", "x", ")", "for", "x", "in", "nested_expression", "]", "\n", "return", "\"(\"", "+", "\" \"", ".", "join", "(", "lisp_expressions", ")", "+", "\")\"", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.DROPDemoPredictor.convert_execution_attention_to_tokens": [[342, 366], ["module_dict.items", "infos_dict.items", "output_type.split", "demo_predictor.convert_wordpiece_attention_to_tokens", "demo_predictor.convert_wordpiece_attention_to_tokens"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.convert_wordpiece_attention_to_tokens", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.convert_wordpiece_attention_to_tokens"], ["", "", "def", "convert_execution_attention_to_tokens", "(", "self", ",", "program_execution", ",", "passage_tokens", ",", "passage_wps", ",", "\n", "passage_wpidx2tokenidx", ",", "question_tokens", ",", "question_wps", ",", "\n", "question_wpidx2tokenidx", ")", ":", "\n", "        ", "\"\"\"\n        program_execution is a list of dicts, i.e. [{\"module_name\": Dict}], where each inner dict is {\"type\", attention}\n\n        This function converts attention for \"question\" and \"passage\" types from attention over word-pieces to tokens.\n        \"\"\"", "\n", "for", "module_dict", "in", "program_execution", ":", "\n", "            ", "for", "module", ",", "infos_dict", "in", "module_dict", ".", "items", "(", ")", ":", "\n", "                ", "for", "output_type", ",", "attention", "in", "infos_dict", ".", "items", "(", ")", ":", "\n", "\n", "                    ", "output_type_canonical", "=", "output_type", ".", "split", "(", "\"_\"", ")", "[", "0", "]", "\n", "if", "output_type_canonical", "==", "\"passage\"", ":", "\n", "                        ", "passage_attention", "=", "convert_wordpiece_attention_to_tokens", "(", "attention", ",", "passage_tokens", ",", "\n", "passage_wps", ",", "\n", "passage_wpidx2tokenidx", ")", "\n", "infos_dict", "[", "output_type", "]", "=", "passage_attention", "\n", "", "elif", "output_type_canonical", "==", "\"question\"", ":", "\n", "                        ", "question_attention", "=", "convert_wordpiece_attention_to_tokens", "(", "attention", ",", "question_tokens", ",", "\n", "question_wps", ",", "\n", "question_wpidx2tokenidx", ")", "\n", "infos_dict", "[", "output_type", "]", "=", "question_attention", "\n", "", "", "", "", "return", "program_execution", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.DROPDemoPredictor.convert_module_outputs_to_list": [[368, 498], ["modified_program_execution.append", "list", "demo_predictor.Output", "demo_predictor.Output", "module_outputs.extend", "module_exec_dict.items", "demo_predictor.Output", "outputs.append", "demo_predictor.Output", "demo_predictor.Output", "demo_predictor.Output", "demo_predictor.Output", "demo_predictor.Output", "module_outputs.extend", "demo_predictor.Output", "demo_predictor.Output", "demo_predictor.Output", "demo_predictor.Output", "demo_predictor.Output", "module_outputs.extend", "demo_predictor.Output", "demo_predictor.Output", "demo_predictor.Output", "demo_predictor.Output", "demo_predictor.Output", "module_outputs.extend", "demo_predictor.Output", "demo_predictor.Output", "demo_predictor.Output", "module_outputs.extend", "demo_predictor.Output", "demo_predictor.Output", "demo_predictor.Output", "demo_predictor.Output", "module_outputs.extend", "demo_predictor.Output", "demo_predictor.Output", "demo_predictor.Output", "module_outputs.extend", "demo_predictor.Output", "demo_predictor.Output", "module_outputs.extend", "demo_predictor.Output", "demo_predictor.Output", "demo_predictor.Output", "module_outputs.extend"], "methods", ["None"], ["", "def", "convert_module_outputs_to_list", "(", "self", ",", "program_execution", ":", "List", "[", "Dict", "]", ")", ":", "\n", "        ", "\"\"\"Modify the program_execution, which stores each modules output as a dict into a list of Output struct.\n\n        Arguments:\n        ----------\n        program_execution: ``List[Dict]``\n            Each program execution is linearized in the order that the modules were executed.\n            This execution is a list of dicts, i.e. [{\"module_name\": Dict}] where each dict contains\n            different outputs a module produces. Each output is a dict itself {\"type\", attention}, where \"type\"\n            indicates the type of support the attention is produced over. E.g. \"paragraph\", \"question\", \"number\", etc.\n            If two or more attentions are produced over the same type (e.g. num-compare-lt produces two number attns),\n            the type is differentiated by appending _N (underscore number). E.g. \"number_1\", \"number_2\", etc.\n\n        Returns:\n        --------\n        program_execution: ``List[Dict]``\n            [{\"module_name\": List[Output]}]\n\n        \"\"\"", "\n", "\n", "modified_program_execution", ":", "List", "[", "Dict", "]", "=", "[", "]", "\n", "for", "module_exec_dict", "in", "program_execution", ":", "\n", "# Size of module_exec_dict == 1", "\n", "            ", "module_name", ",", "module_dict", "=", "list", "(", "module_exec_dict", ".", "items", "(", ")", ")", "[", "0", "]", "\n", "# Convert the module_dict into a list of Output", "\n", "module_outputs", ":", "List", "[", "Output", "]", "=", "[", "]", "\n", "\n", "# Modules that output a single question and paragraph attention", "\n", "if", "module_name", "in", "[", "\"find\"", ",", "\"filter\"", ",", "\"relocate\"", "]", ":", "\n", "                ", "question_output", "=", "Output", "(", "input_name", "=", "\"question\"", ",", "values", "=", "module_dict", "[", "\"question\"", "]", ",", "\n", "label", "=", "\"question_attention\"", ")", "\n", "passage_output", "=", "Output", "(", "input_name", "=", "\"passage\"", ",", "values", "=", "module_dict", "[", "\"passage\"", "]", ",", "label", "=", "\"module_output\"", ")", "\n", "outputs", "=", "[", "question_output", ",", "passage_output", "]", "\n", "if", "\"passage_input\"", "in", "module_dict", ":", "\n", "                    ", "passage_input", "=", "Output", "(", "input_name", "=", "\"passage\"", ",", "values", "=", "module_dict", "[", "\"passage_input\"", "]", ",", "\n", "label", "=", "\"module_input\"", ")", "\n", "outputs", ".", "append", "(", "passage_input", ")", "\n", "", "module_outputs", ".", "extend", "(", "outputs", ")", "\n", "\n", "# Modules that output two date_distributions and one passage distribution", "\n", "", "elif", "module_name", "in", "[", "\"compare-date-lt\"", ",", "\"compare-date-gt\"", "]", ":", "\n", "                ", "passage_output", "=", "Output", "(", "input_name", "=", "\"passage\"", ",", "values", "=", "module_dict", "[", "\"passage\"", "]", ",", "label", "=", "\"module_output\"", ")", "\n", "passage_date_1", "=", "Output", "(", "input_name", "=", "\"passage\"", ",", "values", "=", "module_dict", "[", "\"passage_date_1\"", "]", ",", "\n", "label", "=", "\"passage_date_1\"", ")", "\n", "passage_date_2", "=", "Output", "(", "input_name", "=", "\"passage\"", ",", "values", "=", "module_dict", "[", "\"passage_date_2\"", "]", ",", "\n", "label", "=", "\"passage_date_2\"", ")", "\n", "date_1", "=", "Output", "(", "input_name", "=", "\"dates\"", ",", "values", "=", "module_dict", "[", "\"date_1\"", "]", ",", "label", "=", "\"date_1\"", ")", "\n", "date_2", "=", "Output", "(", "input_name", "=", "\"dates\"", ",", "values", "=", "module_dict", "[", "\"date_2\"", "]", ",", "label", "=", "\"date_2\"", ")", "\n", "module_outputs", ".", "extend", "(", "[", "passage_output", ",", "passage_date_1", ",", "passage_date_2", ",", "date_1", ",", "date_2", "]", ")", "\n", "\n", "# Modules that output two dates and a year diff", "\n", "", "elif", "module_name", "in", "[", "\"year-diff\"", "]", ":", "\n", "                ", "year_diff", "=", "Output", "(", "input_name", "=", "\"year_diffs\"", ",", "values", "=", "module_dict", "[", "\"year-diff\"", "]", ",", "\n", "label", "=", "\"output_year_diff_attention\"", ")", "\n", "passage_date_1", "=", "Output", "(", "input_name", "=", "\"passage\"", ",", "values", "=", "module_dict", "[", "\"passage_date_1\"", "]", ",", "\n", "label", "=", "\"passage_date_1\"", ")", "\n", "passage_date_2", "=", "Output", "(", "input_name", "=", "\"passage\"", ",", "values", "=", "module_dict", "[", "\"passage_date_2\"", "]", ",", "\n", "label", "=", "\"passage_date_2\"", ")", "\n", "date_1", "=", "Output", "(", "input_name", "=", "\"dates\"", ",", "values", "=", "module_dict", "[", "\"date_1\"", "]", ",", "label", "=", "\"date_1\"", ")", "\n", "date_2", "=", "Output", "(", "input_name", "=", "\"dates\"", ",", "values", "=", "module_dict", "[", "\"date_2\"", "]", ",", "label", "=", "\"date_2\"", ")", "\n", "module_outputs", ".", "extend", "(", "[", "year_diff", ",", "passage_date_1", ",", "passage_date_2", ",", "date_1", ",", "date_2", "]", ")", "\n", "\n", "# Modules that output two num_distributions and one passage distribution", "\n", "", "elif", "module_name", "in", "[", "\"compare-num-lt\"", ",", "\"compare-num-gt\"", "]", ":", "\n", "                ", "passage_output", "=", "Output", "(", "input_name", "=", "\"passage\"", ",", "values", "=", "module_dict", "[", "\"passage\"", "]", ",", "label", "=", "\"module_output\"", ")", "\n", "passage_number_1", "=", "Output", "(", "input_name", "=", "\"passage\"", ",", "values", "=", "module_dict", "[", "\"passage_number_1\"", "]", ",", "\n", "label", "=", "\"passage_number_1\"", ")", "\n", "passage_number_2", "=", "Output", "(", "input_name", "=", "\"passage\"", ",", "values", "=", "module_dict", "[", "\"passage_number_2\"", "]", ",", "\n", "label", "=", "\"passage_number_2\"", ")", "\n", "number_1", "=", "Output", "(", "input_name", "=", "\"numbers\"", ",", "values", "=", "module_dict", "[", "\"number_1\"", "]", ",", "label", "=", "\"number_1\"", ")", "\n", "number_2", "=", "Output", "(", "input_name", "=", "\"numbers\"", ",", "values", "=", "module_dict", "[", "\"number_2\"", "]", ",", "label", "=", "\"number_2\"", ")", "\n", "module_outputs", ".", "extend", "(", "[", "passage_output", ",", "passage_number_1", ",", "passage_number_2", ",", "number_1", ",", "number_2", "]", ")", "\n", "\n", "# Modules that output one num_distribution", "\n", "", "elif", "module_name", "in", "[", "\"find-num\"", "]", ":", "\n", "                ", "passage_input", "=", "Output", "(", "input_name", "=", "\"passage\"", ",", "values", "=", "module_dict", "[", "\"passage_input\"", "]", ",", "label", "=", "\"module_input\"", ")", "\n", "passage_number", "=", "Output", "(", "input_name", "=", "\"passage\"", ",", "values", "=", "module_dict", "[", "\"passage_number\"", "]", ",", "\n", "label", "=", "\"passage_number_attention\"", ")", "\n", "number", "=", "Output", "(", "input_name", "=", "\"numbers\"", ",", "values", "=", "module_dict", "[", "\"number\"", "]", ",", "label", "=", "\"number_distribution\"", ")", "\n", "module_outputs", ".", "extend", "(", "[", "passage_input", ",", "passage_number", ",", "number", "]", ")", "\n", "\n", "# Find-max-num and Find-min-num", "\n", "", "elif", "module_name", "in", "[", "\"find-max-num\"", ",", "\"find-min-num\"", "]", ":", "\n", "                ", "passage_input", "=", "Output", "(", "input_name", "=", "\"passage\"", ",", "values", "=", "module_dict", "[", "\"passage_input\"", "]", ",", "label", "=", "\"module_input\"", ")", "\n", "passage_output", "=", "Output", "(", "input_name", "=", "\"passage\"", ",", "values", "=", "module_dict", "[", "\"passage\"", "]", ",", "label", "=", "\"module_output\"", ")", "\n", "input_passage_number", "=", "Output", "(", "input_name", "=", "\"passage\"", ",", "values", "=", "module_dict", "[", "\"passage_input_number\"", "]", ",", "\n", "label", "=", "\"input_pattn_number_attention\"", ")", "\n", "minmax_passage_number", "=", "Output", "(", "input_name", "=", "\"passage\"", ",", "values", "=", "module_dict", "[", "\"passage_minmax_number\"", "]", ",", "\n", "label", "=", "\"minmax_number_attention\"", ")", "\n", "# Not displaying the input number distribution aggregated over numbers", "\n", "# input_number = Output(input_name=\"numbers\", values=module_dict[\"number_input\"],", "\n", "#                       label=\"input_number_distribution\")", "\n", "module_outputs", ".", "extend", "(", "[", "passage_input", ",", "passage_output", ",", "input_passage_number", ",", "\n", "minmax_passage_number", "]", ")", "\n", "\n", "# Addition subtraction modules", "\n", "", "elif", "module_name", "in", "[", "\"number-difference\"", ",", "\"number-addition\"", "]", ":", "\n", "                ", "if", "module_name", "==", "\"number-difference\"", ":", "\n", "                    ", "output_distribution", "=", "module_dict", "[", "\"difference_value\"", "]", "\n", "label", "=", "\"difference_distribution\"", "\n", "", "else", ":", "\n", "                    ", "output_distribution", "=", "module_dict", "[", "\"addition_value\"", "]", "\n", "label", "=", "\"addition_distribution\"", "\n", "", "composed_number", "=", "Output", "(", "input_name", "=", "\"composed_numbers\"", ",", "values", "=", "output_distribution", ",", "\n", "label", "=", "label", ")", "\n", "number_1", "=", "Output", "(", "input_name", "=", "\"numbers\"", ",", "values", "=", "module_dict", "[", "\"input_number_1\"", "]", ",", "label", "=", "\"input_number_1\"", ")", "\n", "number_2", "=", "Output", "(", "input_name", "=", "\"numbers\"", ",", "values", "=", "module_dict", "[", "\"input_number_2\"", "]", ",", "label", "=", "\"input_number_2\"", ")", "\n", "module_outputs", ".", "extend", "(", "[", "composed_number", ",", "number_1", ",", "number_2", "]", ")", "\n", "\n", "# Modules that output count", "\n", "", "elif", "module_name", "in", "[", "\"count\"", "]", ":", "\n", "                ", "passage_input", "=", "Output", "(", "input_name", "=", "\"passage\"", ",", "values", "=", "module_dict", "[", "\"passage_input\"", "]", ",", "label", "=", "\"module_input\"", ")", "\n", "count", "=", "Output", "(", "input_name", "=", "\"count\"", ",", "values", "=", "module_dict", "[", "\"count\"", "]", ",", "label", "=", "\"module_output\"", ")", "\n", "module_outputs", ".", "extend", "(", "[", "passage_input", ",", "count", "]", ")", "\n", "\n", "# span module", "\n", "", "elif", "module_name", "in", "[", "\"span\"", "]", ":", "\n", "                ", "passage_input", "=", "Output", "(", "input_name", "=", "\"passage\"", ",", "values", "=", "module_dict", "[", "\"passage_input\"", "]", ",", "label", "=", "\"module_input\"", ")", "\n", "passage_output", "=", "Output", "(", "input_name", "=", "\"passage\"", ",", "values", "=", "module_dict", "[", "\"token_probs\"", "]", ",", "\n", "label", "=", "\"aggregated_token_probabilities\"", ")", "\n", "span_probs", "=", "Output", "(", "input_name", "=", "\"span_probabilities\"", ",", "values", "=", "module_dict", "[", "\"span_probs\"", "]", ",", "\n", "label", "=", "\"span_probabilities\"", ")", "\n", "module_outputs", ".", "extend", "(", "[", "passage_input", ",", "passage_output", ",", "span_probs", "]", ")", "\n", "\n", "", "else", ":", "\n", "                ", "continue", "\n", "\n", "", "modified_program_execution", ".", "append", "(", "{", "module_name", ":", "module_outputs", "}", ")", "\n", "\n", "", "return", "modified_program_execution", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.DROPDemoPredictor.predict_json": [[499, 603], ["demo_predictor.DROPDemoPredictor._json_to_instance", "demo_predictor.DROPDemoPredictor.predict_instance", "demo_predictor.DROPDemoPredictor.rename_modules_in_nested_expression", "demo_predictor.DROPDemoPredictor.nested_expression_to_lisp", "demo_predictor.add_identifier", "demo_predictor.convert_module_expression_tree_to_dict", "demo_predictor.DROPDemoPredictor.convert_execution_attention_to_tokens", "demo_predictor.Input", "demo_predictor.Input", "demo_predictor.Input", "demo_predictor.Input", "demo_predictor.Input", "demo_predictor.Input", "demo_predictor.Input", "demo_predictor.DROPDemoPredictor.convert_module_outputs_to_list", "allennlp_semparse.common.util.lisp_to_nested_expression", "module_dict.items", "i.to_dict", "program_execution_jsonserializable.append", "utils.util.round_all", "list", "list", "o.to_dict", "demo_predictor.get_topk_spans", "module_output_dict.pop", "module_output_dict.pop", "range", "module_exec_dict.items"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.DROPDemoPredictor._json_to_instance", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.drop_parser_predictor.MTMSNStylePredictor.predict_instance", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.DROPDemoPredictor.rename_modules_in_nested_expression", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.DROPDemoPredictor.nested_expression_to_lisp", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.add_identifier", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.convert_module_expression_tree_to_dict", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.DROPDemoPredictor.convert_execution_attention_to_tokens", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.DROPDemoPredictor.convert_module_outputs_to_list", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.Module.to_dict", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.Module.to_dict", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.get_topk_spans"], ["", "@", "overrides", "\n", "def", "predict_json", "(", "self", ",", "inputs", ":", "JsonDict", ")", "->", "JsonDict", ":", "\n", "        ", "instance", "=", "self", ".", "_json_to_instance", "(", "inputs", ")", "\n", "outputs", "=", "self", ".", "predict_instance", "(", "instance", ")", "\n", "logical_programs", "=", "outputs", "[", "\"batch_logical_programs\"", "]", "\n", "metadata", "=", "outputs", "[", "\"metadata\"", "]", "\n", "question", "=", "metadata", "[", "\"original_question\"", "]", "\n", "passage", "=", "metadata", "[", "\"original_passage\"", "]", "\n", "date_values", "=", "metadata", "[", "\"passage_date_values\"", "]", "\n", "num_values", "=", "metadata", "[", "\"passage_number_values\"", "]", "\n", "year_diff_values", "=", "metadata", "[", "\"passage_year_diffs\"", "]", "\n", "composed_numbers", "=", "metadata", "[", "\"composed_numbers\"", "]", "\n", "passage_mask", "=", "outputs", "[", "\"passage_mask\"", "]", "\n", "\n", "question_tokens", "=", "metadata", "[", "\"question_orig_tokens\"", "]", "\n", "question_wps", "=", "metadata", "[", "\"question_tokens\"", "]", "\n", "question_wpidx2tokenidx", "=", "metadata", "[", "\"question_wpidx2tokenidx\"", "]", "\n", "\n", "# Last wordpiece is '[SEP]'", "\n", "passage_tokens", "=", "metadata", "[", "\"passage_orig_tokens\"", "]", "\n", "passage_wps", "=", "metadata", "[", "\"passage_tokens\"", "]", "\n", "passage_wpidx2tokenidx", "=", "metadata", "[", "\"passage_wpidx2tokenidx\"", "]", "\n", "\n", "predicted_ans", "=", "outputs", "[", "\"predicted_answer\"", "]", "\n", "\n", "program_nested_expressions", "=", "[", "lisp_to_nested_expression", "(", "program", ")", "for", "program", "in", "logical_programs", "]", "\n", "program_nested_expression", "=", "program_nested_expressions", "[", "0", "]", "\n", "# Mapping nested expression's modules to module-names used in the paper", "\n", "program_nested_expression", "=", "self", ".", "rename_modules_in_nested_expression", "(", "program_nested_expression", ")", "\n", "program_lisp", "=", "self", ".", "nested_expression_to_lisp", "(", "program_nested_expression", ")", "\n", "# This contains each module as a dict{\"name\": module_name, \"identifier\": stepnum_of_module_exection}", "\n", "module_expression", ",", "_", "=", "add_identifier", "(", "program_nested_expression", ",", "count", "=", "1", ")", "\n", "program_nested_expression", "=", "convert_module_expression_tree_to_dict", "(", "module_expression", ")", "\n", "\n", "# Is a list which contains for each program executed for this instance, its module execution info.", "\n", "# Since programs are sorted in decreasing order of score, the first element in the list should be argmax program", "\n", "modules_debug_infos", "=", "outputs", "[", "\"modules_debug_infos\"", "]", "\n", "# Each program execution is linearized in the order that the modules were executed.", "\n", "# This execution is a list of dicts, i.e. [{\"module_name\": Dict}] where each dict contains", "\n", "# different outputs a module produces. Each output is a dict itself {\"type\", attention}, where \"type\"", "\n", "# indicates the type of support the attention is produced over. E.g. \"paragraph\", \"question\", \"number\", etc.", "\n", "# If two or more attentions are produced over the same type (e.g. num-compare-lt produces two number attns),", "\n", "# the type is differentiated by appending _N (underscore number). E.g. \"number_1\", \"number_2\", etc.", "\n", "program_execution", ":", "List", "[", "Dict", "]", "=", "modules_debug_infos", "[", "0", "]", "\n", "\n", "# Aggregate attention predicted over word-pieces to attention over tokens", "\n", "program_execution", "=", "self", ".", "convert_execution_attention_to_tokens", "(", "program_execution", ",", "passage_tokens", ",", "passage_wps", ",", "\n", "passage_wpidx2tokenidx", ",", "question_tokens", ",", "\n", "question_wps", ",", "question_wpidx2tokenidx", ")", "\n", "\n", "# Convert span start/end logits to token probs and span probs", "\n", "for", "module_dict", "in", "program_execution", ":", "\n", "            ", "for", "module_name", ",", "module_output_dict", "in", "module_dict", ".", "items", "(", ")", ":", "\n", "                ", "if", "module_name", "==", "\"span\"", ":", "\n", "                    ", "span_start_logits", "=", "module_output_dict", "[", "\"span_start_logits\"", "]", "\n", "span_end_logits", "=", "module_output_dict", "[", "\"span_end_logits\"", "]", "\n", "token_probs", ",", "span_probs", "=", "get_topk_spans", "(", "span_start_logits", "=", "span_start_logits", ",", "\n", "span_end_logits", "=", "span_end_logits", ",", "passage_wps", "=", "passage_wps", ",", "\n", "passage_tokens", "=", "passage_tokens", ",", "\n", "passage_wpidx2tokenidx", "=", "passage_wpidx2tokenidx", ")", "\n", "module_output_dict", "[", "\"token_probs\"", "]", "=", "token_probs", "\n", "module_output_dict", "[", "\"span_probs\"", "]", "=", "span_probs", "\n", "module_output_dict", ".", "pop", "(", "\"span_start_logits\"", ")", "\n", "module_output_dict", ".", "pop", "(", "\"span_end_logits\"", ")", "\n", "\n", "\n", "# Make List[Input] for different kind of inputs that the model gets", "\n", "", "", "", "question_Input", "=", "Input", "(", "name", "=", "\"question\"", ",", "tokens", "=", "question_tokens", ")", "\n", "passage_Input", "=", "Input", "(", "name", "=", "\"passage\"", ",", "tokens", "=", "passage_tokens", ")", "\n", "numbers_Input", "=", "Input", "(", "name", "=", "\"numbers\"", ",", "tokens", "=", "num_values", ")", "\n", "dates_Input", "=", "Input", "(", "name", "=", "\"dates\"", ",", "tokens", "=", "date_values", ")", "\n", "year_diffs_Input", "=", "Input", "(", "name", "=", "\"year_diffs\"", ",", "tokens", "=", "year_diff_values", ")", "\n", "composed_numbers_Input", "=", "Input", "(", "name", "=", "\"composed_numbers\"", ",", "tokens", "=", "util", ".", "round_all", "(", "composed_numbers", ",", "prec", "=", "2", ")", ")", "\n", "count_Input", "=", "Input", "(", "name", "=", "\"count\"", ",", "tokens", "=", "list", "(", "range", "(", "10", ")", ")", ")", "\n", "inputs", ":", "List", "[", "Input", "]", "=", "[", "question_Input", ",", "passage_Input", ",", "numbers_Input", ",", "dates_Input", ",", "composed_numbers_Input", ",", "\n", "year_diffs_Input", ",", "count_Input", "]", "\n", "input_jsonserializable", "=", "[", "i", ".", "to_dict", "(", ")", "for", "i", "in", "inputs", "]", "\n", "\n", "# Convert module_outputs in program_execution from Dict to List[Output]", "\n", "program_execution", "=", "self", ".", "convert_module_outputs_to_list", "(", "program_execution", ")", "\n", "program_execution_jsonserializable", "=", "[", "]", "\n", "for", "module_exec_dict", "in", "program_execution", ":", "\n", "# Size of module_exec_dict == 1", "\n", "            ", "module_name", ",", "module_outputs", "=", "list", "(", "module_exec_dict", ".", "items", "(", ")", ")", "[", "0", "]", "\n", "module_outputs_dicts", "=", "[", "o", ".", "to_dict", "(", ")", "for", "o", "in", "module_outputs", "]", "# module_outputs: List[Output]", "\n", "program_execution_jsonserializable", ".", "append", "(", "{", "module_name", ":", "module_outputs_dicts", "}", ")", "\n", "# outputs[\"program_execution\"] = program_execution_jsonserializable", "\n", "\n", "", "output_dict", "=", "{", "\n", "\"question\"", ":", "question", ",", "\n", "\"passage\"", ":", "passage", ",", "\n", "\"predicted_ans\"", ":", "predicted_ans", ",", "\n", "\"answer\"", ":", "predicted_ans", ",", "\n", "# \"question_tokens\": question_tokens,", "\n", "# \"passage_tokens\": passage_tokens,", "\n", "# \"numbers\": num_values,", "\n", "# \"dates\": date_values,", "\n", "# \"year_diff_values\": year_diff_values,", "\n", "\"inputs\"", ":", "input_jsonserializable", ",", "\n", "\"program_nested_expression\"", ":", "program_nested_expression", ",", "\n", "\"program_lisp\"", ":", "program_lisp", ",", "\n", "\"program_execution\"", ":", "program_execution_jsonserializable", ",", "\n", "}", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.DROPDemoPredictor.dump_line": [[604, 621], ["json.dumps"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "dump_line", "(", "self", ",", "outputs", ":", "JsonDict", ")", "->", "str", ":", "\n", "        ", "\"\"\"Convert output from predict_json to JSON serializable due to presence of Input and Output objects.\"\"\"", "\n", "# inputs: List[Input] = outputs[\"inputs\"]", "\n", "# input_jsonserializable = [i.to_dict() for i in inputs]", "\n", "# outputs[\"inputs\"] = input_jsonserializable", "\n", "#", "\n", "# program_execution = outputs[\"program_execution\"]", "\n", "# program_execution_jsonserializable = []", "\n", "# for module_exec_dict in program_execution:", "\n", "#     # Size of module_exec_dict == 1", "\n", "#     module_name, module_outputs = list(module_exec_dict.items())[0]", "\n", "#     module_outputs_dicts = [o.to_dict() for o in module_outputs]   # module_outputs: List[Output]", "\n", "#     program_execution_jsonserializable.append({module_name: module_outputs_dicts})", "\n", "# outputs[\"program_execution\"] = program_execution_jsonserializable", "\n", "\n", "return", "json", ".", "dumps", "(", "outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.DROPDemoPredictor._print_ExecutionValTree": [[622, 635], ["str", "str", "debug_value.replace.replace.replace", "len", "demo_predictor.DROPDemoPredictor._print_ExecutionValTree"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.DROPDemoPredictor._print_ExecutionValTree"], ["", "def", "_print_ExecutionValTree", "(", "self", ",", "exval_tree", ",", "depth", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        exval_tree: [[root_func_name, value], [], [], []]\n        \"\"\"", "\n", "tabs", "=", "\"\\t\"", "*", "depth", "\n", "func_name", "=", "str", "(", "exval_tree", "[", "0", "]", "[", "0", "]", ")", "\n", "debug_value", "=", "str", "(", "exval_tree", "[", "0", "]", "[", "1", "]", ")", "\n", "debug_value", "=", "debug_value", ".", "replace", "(", "\"\\n\"", ",", "\"\\n\"", "+", "tabs", ")", "\n", "outstr", "=", "f\"{tabs}{func_name}  :\\n {tabs}{debug_value}\\n\"", "\n", "if", "len", "(", "exval_tree", ")", ">", "1", ":", "\n", "            ", "for", "child", "in", "exval_tree", "[", "1", ":", "]", ":", "\n", "                ", "outstr", "+=", "self", ".", "_print_ExecutionValTree", "(", "child", ",", "depth", "+", "1", ")", "\n", "", "", "return", "outstr", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.f1metric": [[26, 42], ["allennlp.tools.squad_eval.metric_max_over_ground_truths", "allennlp.tools.drop_eval.answer_json_to_strings"], "function", ["None"], ["def", "f1metric", "(", "prediction", ":", "Union", "[", "str", ",", "List", "]", ",", "ground_truths", ":", "List", ")", ":", "# type: ignore", "\n", "    ", "\"\"\"\n    Parameters\n    ----------a\n    prediction: ``Union[str, List]``\n        The predicted answer from the model evaluated. This could be a string, or a list of string\n        when multiple spans are predicted as answer.\n    ground_truths: ``List``\n        All the ground truth answer annotations.\n    \"\"\"", "\n", "# If you wanted to split this out by answer type, you could look at [1] here and group by", "\n", "# that, instead of only keeping [0].", "\n", "ground_truth_answer_strings", "=", "[", "answer_json_to_strings", "(", "annotation", ")", "[", "0", "]", "for", "annotation", "in", "ground_truths", "]", "\n", "exact_match", ",", "f1_score", "=", "metric_max_over_ground_truths", "(", "drop_em_and_f1", ",", "prediction", ",", "ground_truth_answer_strings", ")", "\n", "\n", "return", "(", "exact_match", ",", "f1_score", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.split_token_by_delimiter": [[44, 59], ["token.text.split", "split_tokens.append", "len", "split_tokens.pop", "len", "split_tokens.append", "len", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token"], "function", ["None"], ["", "def", "split_token_by_delimiter", "(", "token", ":", "Token", ",", "delimiter", ":", "str", ")", "->", "List", "[", "Token", "]", ":", "\n", "    ", "split_tokens", "=", "[", "]", "\n", "char_offset", "=", "token", ".", "idx", "\n", "for", "sub_str", "in", "token", ".", "text", ".", "split", "(", "delimiter", ")", ":", "\n", "        ", "if", "sub_str", ":", "\n", "            ", "split_tokens", ".", "append", "(", "Token", "(", "text", "=", "sub_str", ",", "idx", "=", "char_offset", ")", ")", "\n", "char_offset", "+=", "len", "(", "sub_str", ")", "\n", "", "split_tokens", ".", "append", "(", "Token", "(", "text", "=", "delimiter", ",", "idx", "=", "char_offset", ")", ")", "\n", "char_offset", "+=", "len", "(", "delimiter", ")", "\n", "", "if", "split_tokens", ":", "\n", "        ", "split_tokens", ".", "pop", "(", "-", "1", ")", "\n", "char_offset", "-=", "len", "(", "delimiter", ")", "\n", "return", "split_tokens", "\n", "", "else", ":", "\n", "        ", "return", "[", "token", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.split_tokens_by_hyphen": [[61, 81], ["any", "new_tokens.append", "demo_predictor.split_token_by_delimiter", "split_tokens.append"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.split_token_by_delimiter"], ["", "", "def", "split_tokens_by_hyphen", "(", "tokens", ":", "List", "[", "Token", "]", ")", "->", "List", "[", "Token", "]", ":", "\n", "    ", "hyphens", "=", "[", "\"-\"", ",", "\"\u2013\"", ",", "\"~\"", "]", "\n", "new_tokens", ":", "List", "[", "Token", "]", "=", "[", "]", "\n", "\n", "for", "token", "in", "tokens", ":", "\n", "        ", "if", "any", "(", "hyphen", "in", "token", ".", "text", "for", "hyphen", "in", "hyphens", ")", ":", "\n", "            ", "unsplit_tokens", "=", "[", "token", "]", "\n", "split_tokens", ":", "List", "[", "Token", "]", "=", "[", "]", "\n", "for", "hyphen", "in", "hyphens", ":", "\n", "                ", "for", "unsplit_token", "in", "unsplit_tokens", ":", "\n", "                    ", "if", "hyphen", "in", "token", ".", "text", ":", "\n", "                        ", "split_tokens", "+=", "split_token_by_delimiter", "(", "unsplit_token", ",", "hyphen", ")", "\n", "", "else", ":", "\n", "                        ", "split_tokens", ".", "append", "(", "unsplit_token", ")", "\n", "", "", "unsplit_tokens", ",", "split_tokens", "=", "split_tokens", ",", "[", "]", "\n", "", "new_tokens", "+=", "unsplit_tokens", "\n", "", "else", ":", "\n", "            ", "new_tokens", ".", "append", "(", "token", ")", "\n", "\n", "", "", "return", "new_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.print_execution_for_debugging": [[83, 107], ["module_dict.items", "print", "infos_dict.items", "output_type.split", "print", "print", "print", "zip", "print", "zip", "zip", "zip"], "function", ["None"], ["", "def", "print_execution_for_debugging", "(", "program_execution", ",", "passage_tokens", ",", "question_tokens", ",", "\n", "num_values", ",", "date_values", ")", ":", "\n", "    ", "for", "module_dict", "in", "program_execution", ":", "\n", "        ", "for", "module", ",", "infos_dict", "in", "module_dict", ".", "items", "(", ")", ":", "\n", "# infos : Tuple[\"type\", attention]", "\n", "            ", "print", "(", "f\"{module}: {[x for x in infos_dict]} \"", ")", "\n", "for", "output_type", ",", "attention", "in", "infos_dict", ".", "items", "(", ")", ":", "\n", "                ", "output_type", "=", "output_type", ".", "split", "(", "\"_\"", ")", "[", "0", "]", "\n", "if", "output_type", "==", "\"passage\"", ":", "\n", "# passage_attention = self.convert_wordpiece_attention_to_tokens(attention, passage_tokens,", "\n", "#                                                                passage_wps,", "\n", "#                                                                passage_wpidx2tokenidx)", "\n", "                    ", "print", "(", "[", "(", "i", ",", "j", ")", "for", "(", "i", ",", "j", ")", "in", "zip", "(", "passage_tokens", ",", "attention", ")", "]", ")", "\n", "", "elif", "output_type", "==", "\"question\"", ":", "\n", "# question_attention = self.convert_wordpiece_attention_to_tokens(attention, question_tokens,", "\n", "#                                                                 question_wps,", "\n", "#                                                                 question_wpidx2tokenidx)", "\n", "                    ", "print", "(", "[", "(", "i", ",", "j", ")", "for", "(", "i", ",", "j", ")", "in", "zip", "(", "question_tokens", ",", "attention", ")", "]", ")", "\n", "", "elif", "output_type", "==", "\"number\"", ":", "\n", "                    ", "print", "(", "[", "(", "i", ",", "j", ")", "for", "(", "i", ",", "j", ")", "in", "zip", "(", "num_values", ",", "attention", ")", "]", ")", "\n", "", "elif", "output_type", "==", "\"date\"", ":", "\n", "                    ", "print", "(", "[", "(", "i", ",", "j", ")", "for", "(", "i", ",", "j", ")", "in", "zip", "(", "date_values", ",", "attention", ")", "]", ")", "\n", "", "else", ":", "\n", "                    ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.convert_wordpiece_attention_to_tokens": [[109, 118], ["len", "len", "zip"], "function", ["None"], ["", "", "", "", "", "def", "convert_wordpiece_attention_to_tokens", "(", "wp_attention", ",", "tokens", ",", "wps", ",", "wpidx2tokenidx", ")", ":", "\n", "    ", "tokens_len", "=", "len", "(", "tokens", ")", "\n", "wps_len", "=", "len", "(", "wps", ")", "\n", "wp_attention", "=", "wp_attention", "[", ":", "wps_len", "]", "# attention over word-pieces", "\n", "token_attention", "=", "[", "0.0", "]", "*", "tokens_len", "# attention over tokens", "\n", "for", "token_idx", ",", "attn_value", "in", "zip", "(", "wpidx2tokenidx", ",", "wp_attention", ")", ":", "\n", "        ", "if", "token_idx", ">=", "0", "and", "token_idx", "<", "tokens_len", ":", "\n", "            ", "token_attention", "[", "token_idx", "]", "+=", "attn_value", "\n", "", "", "return", "token_attention", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.add_identifier": [[161, 185], ["isinstance", "range", "demo_predictor.Module", "sub_expression.insert", "demo_predictor.Module", "len", "demo_predictor.add_identifier", "sub_expression.append"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.add_identifier"], ["", "", "def", "add_identifier", "(", "nested_expression", ",", "count", ")", ":", "\n", "    ", "\"\"\"Convert the nested_expression into a representation that contains the order in which the modules are executed.\n\n    This function converts the nested_expression of module-as-str into expression with module-as-Module class where the\n    class stores an `identifier` key which is the number at which the module was executed.\n\n    Since the program-tree is executed in a left-to-right post-traversal order we will traverse the tree in a similar\n    manner to number the modules in the nested-expression.\n    \"\"\"", "\n", "# If expression is not a list (hence a str) it's a Module", "\n", "if", "not", "isinstance", "(", "nested_expression", ",", "list", ")", ":", "\n", "        ", "return", "Module", "(", "name", "=", "nested_expression", ",", "identifier", "=", "count", ")", ",", "count", "+", "1", "\n", "# If expression is tree", "\n", "", "else", ":", "\n", "        ", "sub_expression", "=", "[", "]", "\n", "# Performing left-to-right post traversal of the tree", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "nested_expression", ")", ")", ":", "\n", "            ", "arg_i", ",", "count", "=", "add_identifier", "(", "nested_expression", "[", "i", "]", ",", "count", ")", "\n", "sub_expression", ".", "append", "(", "arg_i", ")", "\n", "# Then add the root-module of the tree", "\n", "", "arg_0", "=", "Module", "(", "name", "=", "nested_expression", "[", "0", "]", ",", "identifier", "=", "count", ")", "\n", "sub_expression", ".", "insert", "(", "0", ",", "arg_0", ")", "\n", "\n", "return", "sub_expression", ",", "count", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.convert_module_expression_tree_to_dict": [[187, 197], ["enumerate", "isinstance", "mapped_expression.append", "isinstance", "demo_predictor.convert_module_expression_tree_to_dict", "mapped_expression.append", "argument.to_dict"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.convert_module_expression_tree_to_dict", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.Module.to_dict"], ["", "", "def", "convert_module_expression_tree_to_dict", "(", "module_expression", ")", ":", "\n", "    ", "mapped_expression", "=", "[", "]", "\n", "for", "i", ",", "argument", "in", "enumerate", "(", "module_expression", ")", ":", "\n", "        ", "if", "isinstance", "(", "argument", ",", "list", ")", ":", "\n", "            ", "mapped_expression", ".", "append", "(", "convert_module_expression_tree_to_dict", "(", "argument", ")", ")", "\n", "", "elif", "isinstance", "(", "argument", ",", "Module", ")", ":", "\n", "            ", "mapped_expression", ".", "append", "(", "argument", ".", "to_dict", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "", "return", "mapped_expression", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.get_topk_spans": [[638, 685], ["torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze.size", "torch.triu().log", "valid_span_log_probs.view", "torch.nn.functional.softmax", "torch.sort", "numpy.array", "range", "demo_predictor.convert_wordpiece_attention_to_tokens", "torch.Tensor().unsqueeze.unsqueeze", "torch.Tensor().unsqueeze.unsqueeze", "span_start_indices.detach().cpu().numpy().tolist", "span_end_indices.detach().cpu().numpy().tolist", "wps_span_probs.detach().cpu().numpy().tolist", "wp_span_probs.append", "token_span_probs.append", "torch.Tensor", "torch.Tensor", "torch.triu", "torch.ones", "span_start_indices.detach().cpu().numpy", "span_end_indices.detach().cpu().numpy", "wps_span_probs.detach().cpu().numpy", "span_start_indices.detach().cpu", "span_end_indices.detach().cpu", "wps_span_probs.detach().cpu", "span_start_indices.detach", "span_end_indices.detach", "wps_span_probs.detach"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.predictors.demo_predictor.convert_wordpiece_attention_to_tokens"], ["", "", "def", "get_topk_spans", "(", "span_start_logits", ",", "span_end_logits", ",", "passage_wps", ",", "passage_tokens", ",", "passage_wpidx2tokenidx", ")", ":", "\n", "    ", "max_span_length", "=", "20", "\n", "max_num_spans", "=", "100", "\n", "\n", "span_start_logits", "=", "torch", ".", "Tensor", "(", "span_start_logits", ")", ".", "unsqueeze", "(", "0", ")", "\n", "span_end_logits", "=", "torch", ".", "Tensor", "(", "span_end_logits", ")", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "batch_size", ",", "passage_length", "=", "span_start_logits", ".", "size", "(", ")", "\n", "device", "=", "span_start_logits", ".", "device", "\n", "span_log_probs", "=", "span_start_logits", ".", "unsqueeze", "(", "2", ")", "+", "span_end_logits", ".", "unsqueeze", "(", "1", ")", "\n", "span_log_mask", "=", "torch", ".", "triu", "(", "torch", ".", "ones", "(", "(", "passage_length", ",", "passage_length", ")", ",", "device", "=", "device", ")", ")", ".", "log", "(", ")", "\n", "\n", "valid_span_log_probs", "=", "span_log_probs", "+", "span_log_mask", "\n", "\n", "# Shape: (batch_size, passage_length*passage_length)", "\n", "valid_span_log_probs_flat", "=", "valid_span_log_probs", ".", "view", "(", "batch_size", ",", "-", "1", ")", "\n", "valid_span_probs", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "valid_span_log_probs_flat", ",", "dim", "=", "-", "1", ")", "# computing span probs", "\n", "wps_span_probs", ",", "span_indices", "=", "torch", ".", "sort", "(", "valid_span_probs", ",", "descending", "=", "True", ")", "\n", "span_start_indices", "=", "span_indices", "//", "passage_length", "# Span start wordpiece index", "\n", "span_end_indices", "=", "span_indices", "%", "passage_length", "# Span end wordpiece index", "\n", "\n", "# Assuming demo is running with batch_size of 1, hence removed the first dim", "\n", "span_start_indices", "=", "span_start_indices", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "[", "0", "]", "\n", "span_end_indices", "=", "span_end_indices", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "[", "0", "]", "\n", "wps_span_probs", "=", "wps_span_probs", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "[", "0", "]", "\n", "\n", "# Computing wordpiece-prob as the sum of probability of spans that this wordpiece occurs in", "\n", "wps_probs", "=", "np", ".", "array", "(", "[", "0.0", "]", "*", "passage_length", ")", "\n", "wp_span_probs", "=", "[", "]", "# List of [[start, end], prob]", "\n", "for", "span_num", "in", "range", "(", "0", ",", "max_num_spans", ")", ":", "\n", "        ", "start", ",", "end", "=", "span_start_indices", "[", "span_num", "]", ",", "span_end_indices", "[", "span_num", "]", "\n", "prob", "=", "wps_span_probs", "[", "span_num", "]", "\n", "wp_span_probs", ".", "append", "(", "[", "[", "start", ",", "end", "]", ",", "prob", "]", ")", "\n", "wps_probs", "[", "start", ":", "end", "+", "1", "]", "+=", "prob", "# end + 1 since end is inclusive", "\n", "\n", "# Converting wordpiece probabilities token-probabilities", "\n", "", "token_probs", "=", "convert_wordpiece_attention_to_tokens", "(", "wp_attention", "=", "wps_probs", ",", "tokens", "=", "passage_tokens", ",", "\n", "wps", "=", "passage_wps", ",", "wpidx2tokenidx", "=", "passage_wpidx2tokenidx", ")", "\n", "# Converting wps_span_probs list to token-spans", "\n", "token_span_probs", "=", "[", "]", "\n", "for", "span", ",", "prob", "in", "wp_span_probs", ":", "\n", "        ", "token_start", ",", "token_end", "=", "passage_wpidx2tokenidx", "[", "span", "[", "0", "]", "]", ",", "passage_wpidx2tokenidx", "[", "span", "[", "1", "]", "]", "\n", "if", "token_end", "-", "token_start", ">", "max_span_length", ":", "\n", "            ", "continue", "\n", "", "token_span_probs", ".", "append", "(", "[", "[", "token_start", ",", "token_end", "]", ",", "prob", "]", ")", "\n", "\n", "", "return", "token_probs", ",", "token_span_probs", "\n", "", ""]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.passage_attn_to_count.PassageAttnToCount.__init__": [[23, 58], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "allennlp.training.metrics.Average", "initializers", "len", "passage_attn_to_count.PassageAttnToCount.passage_attention_to_count.get_input_dim", "passage_attn_to_count.PassageAttnToCount.passage_attention_to_count.get_output_dim", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "passage_attention_to_count", ":", "Seq2SeqEncoder", ",", "\n", "dropout", ":", "float", "=", "0.2", ",", "\n", "initializers", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "super", "(", "PassageAttnToCount", ",", "self", ")", ".", "__init__", "(", "vocab", "=", "vocab", ")", "\n", "\n", "self", ".", "scaling_vals", "=", "[", "1", ",", "2", ",", "5", ",", "10", "]", "\n", "\n", "self", ".", "passage_attention_to_count", "=", "passage_attention_to_count", "\n", "\n", "assert", "len", "(", "self", ".", "scaling_vals", ")", "==", "self", ".", "passage_attention_to_count", ".", "get_input_dim", "(", ")", "\n", "\n", "self", ".", "num_counts", "=", "10", "\n", "# self.passage_count_predictor = torch.nn.Linear(self.passage_attention_to_count.get_output_dim(),", "\n", "#                                                self.num_counts, bias=False)", "\n", "\n", "# We want to predict a score for each passage token", "\n", "self", ".", "passage_count_hidden2logits", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "self", ".", "passage_attention_to_count", ".", "get_output_dim", "(", ")", ",", "1", ",", "bias", "=", "True", "\n", ")", "\n", "\n", "self", ".", "passagelength_to_bias", "=", "torch", ".", "nn", ".", "Linear", "(", "1", ",", "1", ",", "bias", "=", "True", ")", "\n", "\n", "self", ".", "count_acc", "=", "Average", "(", ")", "\n", "\n", "if", "dropout", ">", "0", ":", "\n", "            ", "self", ".", "_dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_dropout", "=", "lambda", "x", ":", "x", "\n", "\n", "", "initializers", "(", "self", ")", "\n", "# self.passage_count_hidden2logits.bias.data.fill_(-1.0)", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.passage_attn_to_count.PassageAttnToCount.device_id": [[61, 63], ["allennlp.get_device_of"], "methods", ["None"], ["", "def", "device_id", "(", "self", ")", ":", "\n", "        ", "allenutil", ".", "get_device_of", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.passage_attn_to_count.PassageAttnToCount.forward": [[64, 147], ["allennlp.get_device_of", "passage_attention.size", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "passage_attn_to_count.PassageAttnToCount.passagelength_to_bias", "passage_attn_to_count.PassageAttnToCount.passage_attention_to_count", "passage_attn_to_count.PassageAttnToCount.passage_count_hidden2logits", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "allennlp.get_range_vector().unsqueeze().float", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "passage_mask.sum", "passage_mask.unsqueeze", "passage_attn_to_count.PassageAttnToCount.squeeze", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "passage_count_mean.squeeze.squeeze.squeeze", "torch.mse_loss", "torch.mse_loss", "passage_count_mean.squeeze.squeeze.detach().cpu().numpy", "numpy.round_", "count_answer.detach().cpu().numpy", "passage_attn_to_count.PassageAttnToCount.count_acc", "allennlp.get_range_vector().unsqueeze", "sum", "count_answer.float", "passage_count_mean.squeeze.squeeze.detach().cpu", "count_answer.detach().cpu", "allennlp.get_range_vector", "passage_count_mean.squeeze.squeeze.detach", "count_answer.detach"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "passage_attention", ":", "torch", ".", "Tensor", ",", "\n", "passage_lengths", ":", "List", "[", "int", "]", ",", "\n", "count_answer", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "device_id", "=", "allenutil", ".", "get_device_of", "(", "passage_attention", ")", "\n", "\n", "batch_size", ",", "max_passage_length", "=", "passage_attention", ".", "size", "(", ")", "\n", "\n", "# Shape: (B, passage_length)", "\n", "passage_mask", "=", "(", "passage_attention", ">=", "0", ")", ".", "float", "(", ")", "\n", "\n", "# List of (B, P) shaped tensors", "\n", "scaled_attentions", "=", "[", "passage_attention", "*", "sf", "for", "sf", "in", "self", ".", "scaling_vals", "]", "\n", "# Shape: (B, passage_length, num_scaling_factors)", "\n", "scaled_passage_attentions", "=", "torch", ".", "stack", "(", "scaled_attentions", ",", "dim", "=", "2", ")", "\n", "\n", "# Shape (batch_size, 1)", "\n", "passage_len_bias", "=", "self", ".", "passagelength_to_bias", "(", "passage_mask", ".", "sum", "(", "1", ",", "keepdim", "=", "True", ")", ")", "\n", "\n", "scaled_passage_attentions", "=", "scaled_passage_attentions", "*", "passage_mask", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Shape: (B, passage_length, hidden_dim)", "\n", "count_hidden_repr", "=", "self", ".", "passage_attention_to_count", "(", "scaled_passage_attentions", ",", "passage_mask", ")", "\n", "\n", "# Shape: (B, passage_length, 1) -- score for each token", "\n", "passage_span_logits", "=", "self", ".", "passage_count_hidden2logits", "(", "count_hidden_repr", ")", "\n", "# Shape: (B, passage_length) -- sigmoid on token-score", "\n", "token_sigmoids", "=", "torch", ".", "sigmoid", "(", "passage_span_logits", ".", "squeeze", "(", "2", ")", ")", "\n", "token_sigmoids", "=", "token_sigmoids", "*", "passage_mask", "\n", "\n", "# Shape: (B, 1) -- sum of sigmoids. This will act as the predicted mean", "\n", "# passage_count_mean = torch.sum(token_sigmoids, dim=1, keepdim=True) + passage_len_bias", "\n", "passage_count_mean", "=", "torch", ".", "sum", "(", "token_sigmoids", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "\n", "# Shape: (1, count_vals)", "\n", "self", ".", "countvals", "=", "allenutil", ".", "get_range_vector", "(", "10", ",", "device", "=", "device_id", ")", ".", "unsqueeze", "(", "0", ")", ".", "float", "(", ")", "\n", "\n", "variance", "=", "0.2", "\n", "\n", "# Shape: (batch_size, count_vals)", "\n", "l2_by_vsquared", "=", "torch", ".", "pow", "(", "self", ".", "countvals", "-", "passage_count_mean", ",", "2", ")", "/", "(", "2", "*", "variance", "*", "variance", ")", "\n", "exp_val", "=", "torch", ".", "exp", "(", "-", "1", "*", "l2_by_vsquared", ")", "+", "1e-30", "\n", "# Shape: (batch_size, count_vals)", "\n", "count_distribution", "=", "exp_val", "/", "(", "torch", ".", "sum", "(", "exp_val", ",", "1", ",", "keepdim", "=", "True", ")", ")", "\n", "\n", "# Loss computation", "\n", "output_dict", "=", "{", "}", "\n", "loss", "=", "0.0", "\n", "pred_count_idx", "=", "torch", ".", "argmax", "(", "count_distribution", ",", "1", ")", "\n", "if", "count_answer", "is", "not", "None", ":", "\n", "# L2-loss", "\n", "            ", "passage_count_mean", "=", "passage_count_mean", ".", "squeeze", "(", "1", ")", "\n", "L2Loss", "=", "F", ".", "mse_loss", "(", "input", "=", "passage_count_mean", ",", "target", "=", "count_answer", ".", "float", "(", ")", ")", "\n", "loss", "=", "L2Loss", "\n", "predictions", "=", "passage_count_mean", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "predictions", "=", "np", ".", "round_", "(", "predictions", ")", "\n", "\n", "gold_count", "=", "count_answer", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "correct_vec", "=", "predictions", "==", "gold_count", "\n", "correct_perc", "=", "sum", "(", "correct_vec", ")", "/", "batch_size", "\n", "# print(f\"{correct_perc} {predictions} {gold_count}\")", "\n", "self", ".", "count_acc", "(", "correct_perc", ")", "\n", "\n", "# loss = F.cross_entropy(input=count_distribution, target=count_answer)", "\n", "# List of predicted count idxs, Shape: (B,)", "\n", "# correct_vec = (pred_count_idx == count_answer).float()", "\n", "# correct_perc = torch.sum(correct_vec) / batch_size", "\n", "# self.count_acc(correct_perc.item())", "\n", "\n", "", "batch_loss", "=", "loss", "/", "batch_size", "\n", "output_dict", "[", "\"loss\"", "]", "=", "batch_loss", "\n", "output_dict", "[", "\"passage_attention\"", "]", "=", "passage_attention", "\n", "output_dict", "[", "\"passage_sigmoid\"", "]", "=", "token_sigmoids", "\n", "output_dict", "[", "\"count_mean\"", "]", "=", "passage_count_mean", "\n", "output_dict", "[", "\"count_distritbuion\"", "]", "=", "count_distribution", "\n", "output_dict", "[", "\"count_answer\"", "]", "=", "count_answer", "\n", "output_dict", "[", "\"pred_count\"", "]", "=", "pred_count_idx", "\n", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.passage_attn_to_count.PassageAttnToCount.get_metrics": [[148, 154], ["passage_attn_to_count.PassageAttnToCount.count_acc.get_metric", "metric_dict.update"], "methods", ["None"], ["", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "metric_dict", "=", "{", "}", "\n", "count_acc", "=", "self", ".", "count_acc", ".", "get_metric", "(", "reset", ")", "\n", "metric_dict", ".", "update", "(", "{", "\"acc\"", ":", "count_acc", "}", ")", "\n", "\n", "return", "metric_dict", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_base.DROPParserBase.__init__": [[29, 67], ["allennlp.models.model.Model.__init__", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.modules.Embedding", "torch.nn.Parameter", "torch.nn.init.normal_", "torch.nn.Dropout", "torch.FloatTensor", "vocab.get_vocab_size"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "action_embedding_dim", ":", "int", ",", "\n", "text_field_embedder", ":", "TextFieldEmbedder", "=", "None", ",", "\n", "dropout", ":", "float", "=", "0.0", ",", "\n", "rule_namespace", ":", "str", "=", "\"rule_labels\"", ",", "\n", "debug", ":", "bool", "=", "False", ",", "\n", "regularizer", ":", "Optional", "[", "RegularizerApplicator", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", "DROPParserBase", ",", "self", ")", ".", "__init__", "(", "vocab", "=", "vocab", ",", "regularizer", "=", "regularizer", ")", "\n", "\n", "# To call garbage collection frequently", "\n", "self", ".", "num_forward_calls", "=", "0", "\n", "\n", "self", ".", "_denotation_accuracy", "=", "Average", "(", ")", "\n", "self", ".", "_consistency", "=", "Average", "(", ")", "\n", "\n", "if", "dropout", ">", "0", ":", "\n", "            ", "self", ".", "_dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_dropout", "=", "lambda", "x", ":", "x", "\n", "", "self", ".", "_rule_namespace", "=", "rule_namespace", "\n", "\n", "# This flag turns on the debugging mode which prints a bunch of stuff in self.decode (inside functions as well)", "\n", "self", ".", "_debug", "=", "debug", "\n", "\n", "self", ".", "_action_embedder", "=", "Embedding", "(", "\n", "num_embeddings", "=", "vocab", ".", "get_vocab_size", "(", "self", ".", "_rule_namespace", ")", ",", "\n", "embedding_dim", "=", "action_embedding_dim", ",", "\n", "vocab_namespace", "=", "self", ".", "_rule_namespace", ",", "\n", ")", "\n", "\n", "self", ".", "_action_embedding_dim", "=", "action_embedding_dim", "\n", "# This is what we pass as input in the first step of decoding, when we don't have a", "\n", "# previous action.", "\n", "self", ".", "_first_action_embedding", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "FloatTensor", "(", "action_embedding_dim", ")", ")", "\n", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "self", ".", "_first_action_embedding", ",", "mean", "=", "0.0", ",", "std", "=", "0.001", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_base.DROPParserBase.forward": [[68, 73], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "**", "kwargs", ")", ":", "# type: ignore", "\n", "# pylint: disable=arguments-differ", "\n", "# Sub-classes should define their own logic here.", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_base.DROPParserBase._get_initial_rnn_state": [[74, 116], ["question_encoded_finalstate.size", "question_encoded_finalstate.new_zeros", "drop_parser_base.DROPParserBase._decoder_step.attend_on_question", "range", "question_encoded_finalstate.size", "initial_rnn_state.append", "allennlp_semparse.state_machines.states.RnnStatelet"], "methods", ["None"], ["", "def", "_get_initial_rnn_state", "(", "\n", "self", ",", "\n", "question_encoded", ":", "torch", ".", "FloatTensor", ",", "\n", "question_mask", ":", "torch", ".", "Tensor", ",", "\n", "question_encoded_finalstate", ":", "torch", ".", "FloatTensor", ",", "\n", "question_encoded_aslist", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", "question_mask_aslist", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", ")", ":", "\n", "        ", "\"\"\" Get the initial RnnStatelet for the decoder based on the question encoding\n\n        Parameters:\n        -----------\n        ques_repr: (B, question_length, D)\n        ques_mask: (B, question_length)\n        question_final_repr: (B, D)\n        ques_encoded_list: [(question_length, D)] - List of length B\n        ques_mask_list: [(question_length)] - List of length B\n        \"\"\"", "\n", "\n", "batch_size", "=", "question_encoded_finalstate", ".", "size", "(", "0", ")", "\n", "ques_encoded_dim", "=", "question_encoded_finalstate", ".", "size", "(", ")", "[", "-", "1", "]", "\n", "\n", "# Shape: (B, D)", "\n", "memory_cell", "=", "question_encoded_finalstate", ".", "new_zeros", "(", "batch_size", ",", "ques_encoded_dim", ")", "\n", "# TODO(nitish): Why does WikiTablesParser use '_first_attended_question' embedding and not this", "\n", "attended_sentence", ",", "_", "=", "self", ".", "_decoder_step", ".", "attend_on_question", "(", "\n", "question_encoded_finalstate", ",", "question_encoded", ",", "question_mask", "\n", ")", "\n", "\n", "initial_rnn_state", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "initial_rnn_state", ".", "append", "(", "\n", "RnnStatelet", "(", "\n", "question_encoded_finalstate", "[", "i", "]", ",", "\n", "memory_cell", "[", "i", "]", ",", "\n", "self", ".", "_first_action_embedding", ",", "\n", "attended_sentence", "[", "i", "]", ",", "\n", "question_encoded_aslist", ",", "\n", "question_mask_aslist", ",", "\n", ")", "\n", ")", "\n", "", "return", "initial_rnn_state", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_base.DROPParserBase._create_grammar_statelet": [[117, 186], ["enumerate", "language.get_nonterminal_productions", "language.get_nonterminal_productions.items", "actionidx2actionstr.append", "allennlp_semparse.state_machines.states.GrammarStatelet", "zip", "torch.cat", "drop_parser_base.DROPParserBase._action_embedder", "global_actions.append", "list"], "methods", ["None"], ["", "def", "_create_grammar_statelet", "(", "\n", "self", ",", "language", ":", "DropLanguage", ",", "possible_actions", ":", "List", "[", "ProductionRule", "]", "\n", ")", "->", "Tuple", "[", "GrammarStatelet", ",", "Dict", "[", "str", ",", "int", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "# linked_rule2idx: Dict = None,", "\n", "# action2ques_linkingscore: torch.FloatTensor = None,", "\n", "# quesspan_action_emb: torch.FloatTensor = None) -> GrammarStatelet:", "\n", "        ", "\"\"\" Make grammar state for a particular instance in the batch using the global and instance-specific actions.\n        For each instance-specific action we have a linking_score vector (size:ques_tokens), and an action embedding\n\n        Parameters:\n        ------------\n        world: `SampleHotpotWorld` The world for this instance\n        possible_actions: All possible actions, global and instance-specific\n\n        linked_rule2idx: Dict from linked_action to idx used for the next two members\n        action2ques_linkingscore: Linking score matrix of size (instance-specific_actions, num_ques_tokens)\n            The indexing is based on the linked_rule2idx dict. The num_ques_tokens is to a padded length\n            The num_ques_tokens is to a padded length, because of which not using a dictionary but a tensor.\n        quesspan_action_emb: Similarly, a (instance-specific_actions, action_embedding_dim) matrix.\n            The indexing is based on the linked_rule2idx dict.\n        \"\"\"", "\n", "# ProductionRule: (rule, is_global_rule, rule_id, nonterminal)", "\n", "action2actionidx", "=", "{", "}", "\n", "actionidx2actionstr", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "for", "action_index", ",", "action", "in", "enumerate", "(", "possible_actions", ")", ":", "\n", "            ", "action_string", "=", "action", "[", "0", "]", "\n", "action2actionidx", "[", "action_string", "]", "=", "action_index", "\n", "actionidx2actionstr", ".", "append", "(", "action_string", ")", "\n", "\n", "", "valid_actions", "=", "language", ".", "get_nonterminal_productions", "(", ")", "\n", "translated_valid_actions", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", ",", "List", "[", "int", "]", "]", "]", "]", "=", "{", "}", "\n", "\n", "for", "key", ",", "action_strings", "in", "valid_actions", ".", "items", "(", ")", ":", "\n", "            ", "translated_valid_actions", "[", "key", "]", "=", "{", "}", "\n", "# `key` here is a non-terminal from the grammar, and `action_strings` are all the valid", "\n", "# productions of that non-terminal.  We'll first split those productions by global vs.", "\n", "# linked action.", "\n", "action_indices", "=", "[", "action2actionidx", "[", "action_string", "]", "for", "action_string", "in", "action_strings", "]", "\n", "production_rule_arrays", "=", "[", "(", "possible_actions", "[", "index", "]", ",", "index", ")", "for", "index", "in", "action_indices", "]", "\n", "\n", "# For global_actions: (rule_vocab_id_tensor, action_index)", "\n", "global_actions", "=", "[", "]", "\n", "\n", "for", "production_rule_array", ",", "action_index", "in", "production_rule_arrays", ":", "\n", "# production_rule_array: ProductionRule", "\n", "                ", "if", "production_rule_array", "[", "1", "]", ":", "\n", "                    ", "global_actions", ".", "append", "(", "(", "production_rule_array", "[", "2", "]", ",", "action_index", ")", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "NotImplementedError", "\n", "\n", "# First: Get the embedded representations of the global actions", "\n", "", "", "if", "global_actions", ":", "\n", "                ", "global_action_tensors", ",", "global_action_ids", "=", "zip", "(", "*", "global_actions", ")", "\n", "global_action_tensor", "=", "torch", ".", "cat", "(", "global_action_tensors", ",", "dim", "=", "0", ")", "\n", "# TODO(nitish): Figure out if need action_bias and separate input/output action embeddings", "\n", "# if self._add_action_bias:", "\n", "#     global_action_biases = self._action_biases(global_action_tensor)", "\n", "#     global_input_embeddings = torch.cat([global_input_embeddings, global_action_biases], dim=-1)", "\n", "global_output_embeddings", "=", "self", ".", "_action_embedder", "(", "global_action_tensor", ")", "\n", "translated_valid_actions", "[", "key", "]", "[", "\"global\"", "]", "=", "(", "\n", "global_output_embeddings", ",", "\n", "global_output_embeddings", ",", "\n", "list", "(", "global_action_ids", ")", ",", "\n", ")", "\n", "\n", "", "", "return", "(", "\n", "GrammarStatelet", "(", "[", "START_SYMBOL", "]", ",", "translated_valid_actions", ",", "language", ".", "is_nonterminal", ")", ",", "\n", "action2actionidx", ",", "\n", "actionidx2actionstr", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_base.DROPParserBase._get_denotations": [[188, 231], ["range", "len", "range", "all_denotations.append", "all_denotation_types.append", "len", "instance_language.modules_debug_info.append", "instance_language.execute_action_sequence", "instance_denotations.append", "instance_denotation_types.append"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.execute_action_sequence"], ["", "@", "staticmethod", "\n", "def", "_get_denotations", "(", "\n", "action_strings", ":", "List", "[", "List", "[", "List", "[", "str", "]", "]", "]", ",", "languages", ":", "List", "[", "DropLanguage", "]", ",", "sideargs", ":", "List", "[", "List", "[", "List", "[", "Dict", "]", "]", "]", "=", "None", "\n", ")", "->", "Tuple", "[", "List", "[", "List", "[", "Any", "]", "]", ",", "List", "[", "List", "[", "str", "]", "]", "]", ":", "\n", "        ", "\"\"\" Get denotations for all action-sequences for  every instance in a batch.\n\n        Parameters:\n        -----------\n        action_strings: ``List[List[List[str]]]``\n            Each program represented as a list of actions(str),  for all decoded programs for each instance\n        languages: ``List[HotpotQALanguage]``\n            Language instance for each instance\n        sideargs: ``List[List[List[Dict]]]``\n            Required for languages that use side_args\n            Debug-info as List[Dict] for each program. This list should be the same size as the number of actions in\n            the program. This debug-info is present for each program, for each decoded program for each instance.\n        \"\"\"", "\n", "all_denotations", ":", "List", "[", "List", "[", "Any", "]", "]", "=", "[", "]", "\n", "all_denotation_types", ":", "List", "[", "List", "[", "str", "]", "]", "=", "[", "]", "\n", "for", "insidx", "in", "range", "(", "len", "(", "languages", ")", ")", ":", "\n", "            ", "instance_language", ":", "DropLanguage", "=", "languages", "[", "insidx", "]", "\n", "instance_action_sequences", "=", "action_strings", "[", "insidx", "]", "\n", "instance_sideargs", "=", "sideargs", "[", "insidx", "]", "\n", "instance_denotations", ":", "List", "[", "Any", "]", "=", "[", "]", "\n", "instance_denotation_types", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "for", "pidx", "in", "range", "(", "len", "(", "instance_action_sequences", ")", ")", ":", "\n", "                ", "action_sequence", "=", "instance_action_sequences", "[", "pidx", "]", "\n", "program_sideargs", "=", "instance_sideargs", "[", "pidx", "]", "\n", "instance_language", ".", "modules_debug_info", ".", "append", "(", "[", "]", ")", "\n", "# print(instance_action_strings)", "\n", "if", "not", "action_sequence", ":", "\n", "                    ", "continue", "\n", "", "actionseq_denotation", "=", "instance_language", ".", "execute_action_sequence", "(", "action_sequence", ",", "program_sideargs", ")", "\n", "# instance_actionseq_denotation = instance_language.execute(logical_form)", "\n", "instance_denotations", ".", "append", "(", "actionseq_denotation", ")", "\n", "instance_actionseq_type", "=", "(", "\n", "actionseq_denotation", ".", "__class__", ".", "__name__", "\n", ")", "# instance_language.typeobj_to_typename(actionseq_denotation)", "\n", "instance_denotation_types", ".", "append", "(", "instance_actionseq_type", ")", "\n", "\n", "", "all_denotations", ".", "append", "(", "instance_denotations", ")", "\n", "all_denotation_types", ".", "append", "(", "instance_denotation_types", ")", "\n", "", "return", "all_denotations", ",", "all_denotation_types", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_base.DROPParserBase.decode": [[232, 290], ["range", "output_dict.pop", "output_dict.pop", "len", "range", "logical_forms.append", "execution_vals.append", "modules_debug_infos.append", "len", "instance_logical_forms.append", "l.modules_debug_info.append", "semqa.execute_action_sequence", "instance_execution_vals.append", "instance_logical_forms.append", "instance_execution_vals.append", "l.action_sequence_to_logical_form"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.execute_action_sequence"], ["", "@", "overrides", "\n", "def", "decode", "(", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        This method overrides ``Model.decode``, which gets called after ``Model.forward``, at test\n        time, to finalize predictions. We only transform the action string sequences into logical\n        forms here.\n        \"\"\"", "\n", "# if 'languages' in output_dict:", "\n", "#     output_dict.pop('languages', None)", "\n", "# return output_dict", "\n", "\n", "best_action_strings", "=", "output_dict", "[", "\"batch_action_seqs\"", "]", "\n", "batch_actionseq_sideargs", "=", "output_dict", "[", "\"batch_actionseq_sideargs\"", "]", "\n", "languages", "=", "output_dict", "[", "\"languages\"", "]", "\n", "metadatas", "=", "output_dict", "[", "\"metadata\"", "]", "\n", "\n", "# This currectly works because there aren't any instance-specific arguments to the language.", "\n", "logical_forms", "=", "[", "]", "\n", "execution_vals", "=", "[", "]", "\n", "modules_debug_infos", "=", "[", "]", "\n", "for", "insidx", "in", "range", "(", "len", "(", "languages", ")", ")", ":", "\n", "# for instance_action_sequences, instance_action_sideargs, l in zip(best_action_strings,", "\n", "#                                                                   batch_actionseq_sideargs,", "\n", "#                                                                   languages):", "\n", "            ", "l", ":", "DropLanguage", "=", "languages", "[", "insidx", "]", "\n", "l", ".", "debug", "=", "self", ".", "_debug", "\n", "l", ".", "metadata", "=", "metadatas", "[", "insidx", "]", "\n", "\n", "instance_action_sequences", ":", "List", "[", "List", "[", "str", "]", "]", "=", "best_action_strings", "[", "insidx", "]", "\n", "instance_action_sideargs", "=", "batch_actionseq_sideargs", "[", "insidx", "]", "\n", "\n", "instance_logical_forms", "=", "[", "]", "\n", "instance_execution_vals", "=", "[", "]", "\n", "for", "pidx", "in", "range", "(", "len", "(", "instance_action_sequences", ")", ")", ":", "\n", "# for action_strings, side_args in zip(instance_action_sequences, instance_action_sideargs):", "\n", "                ", "action_strings", "=", "instance_action_sequences", "[", "pidx", "]", "\n", "side_args", "=", "instance_action_sideargs", "[", "pidx", "]", "if", "instance_action_sideargs", "else", "None", "\n", "if", "action_strings", ":", "\n", "                    ", "instance_logical_forms", ".", "append", "(", "l", ".", "action_sequence_to_logical_form", "(", "action_strings", ")", ")", "\n", "# Custom function that copies the execution from domain_languages, but is used for debugging", "\n", "l", ".", "modules_debug_info", ".", "append", "(", "[", "]", ")", "\n", "denotation", ",", "ex_vals", "=", "dl_utils", ".", "execute_action_sequence", "(", "l", ",", "action_strings", ",", "side_args", ")", "\n", "instance_execution_vals", ".", "append", "(", "ex_vals", ")", "\n", "", "else", ":", "\n", "                    ", "instance_logical_forms", ".", "append", "(", "\"\"", ")", "\n", "instance_execution_vals", ".", "append", "(", "[", "]", ")", "\n", "\n", "", "", "logical_forms", ".", "append", "(", "instance_logical_forms", ")", "\n", "execution_vals", ".", "append", "(", "instance_execution_vals", ")", "\n", "modules_debug_infos", ".", "append", "(", "l", ".", "modules_debug_info", ")", "\n", "\n", "", "output_dict", "[", "\"logical_forms\"", "]", "=", "logical_forms", "\n", "output_dict", "[", "\"execution_vals\"", "]", "=", "execution_vals", "\n", "output_dict", "[", "\"modules_debug_infos\"", "]", "=", "modules_debug_infos", "\n", "output_dict", ".", "pop", "(", "\"languages\"", ",", "None", ")", "\n", "output_dict", ".", "pop", "(", "\"batch_actionseq_sideargs\"", ",", "None", ")", "\n", "\n", "return", "output_dict", "\n", "# '''", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser.__init__": [[50, 202], ["allennlp.nn.InitializerApplicator", "semqa.models.drop_parser_base.DROPParserBase.__init__", "phrase_layer.get_output_dim", "allennlp.state_machines.transition_functions.BasicTransitionFunction", "allennlp.state_machines.trainers.maximum_marginal_likelihood.MaximumMarginalLikelihood", "allennlp.state_machines.BeamSearch", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "drop_parser.DROPParser._text_field_embedder.get_output_dim", "phrase_layer.get_input_dim", "phrase_layer.get_output_dim", "modeling_layer.get_input_dim", "modeling_layer.get_output_dim", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "allennlp.modules.Highway", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "allennlp.modules.matrix_attention.LinearMatrixAttention", "allennlp.modules.matrix_attention.DotProductMatrixAttention", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "semqa.domain_languages.drop_execution_parameters.ExecutorParameters", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.training.metrics.DropEmAndF1", "initializers", "drop_parser.DROPParser.named_parameters", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "drop_parser.DROPParser.passage_attention_to_count.get_output_dim", "drop_parser.DROPParser.passage_attention_to_count.get_output_dim", "drop_parser.DROPParser.named_parameters", "any", "allennlp.nn.Activation.by_name"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "action_embedding_dim", ":", "int", ",", "\n", "transitionfunc_attention", ":", "Attention", ",", "\n", "num_highway_layers", ":", "int", ",", "\n", "phrase_layer", ":", "Seq2SeqEncoder", ",", "\n", "matrix_attention_layer", ":", "MatrixAttention", ",", "\n", "modeling_layer", ":", "Seq2SeqEncoder", ",", "\n", "passage_attention_to_span", ":", "Seq2SeqEncoder", ",", "\n", "question_attention_to_span", ":", "Seq2SeqEncoder", ",", "\n", "passage_attention_to_count", ":", "Seq2SeqEncoder", ",", "\n", "beam_size", ":", "int", ",", "\n", "max_decoding_steps", ":", "int", ",", "\n", "modeltype", ":", "str", "=", "\"modeled\"", ",", "# or encoded", "\n", "countfixed", ":", "bool", "=", "False", ",", "\n", "auxwinloss", ":", "bool", "=", "False", ",", "\n", "denotationloss", ":", "bool", "=", "True", ",", "\n", "excloss", ":", "bool", "=", "False", ",", "\n", "qattloss", ":", "bool", "=", "False", ",", "\n", "mmlloss", ":", "bool", "=", "False", ",", "\n", "dropout", ":", "float", "=", "0.0", ",", "\n", "text_field_embedder", ":", "TextFieldEmbedder", "=", "None", ",", "\n", "debug", ":", "bool", "=", "False", ",", "\n", "initializers", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", "regularizer", ":", "Optional", "[", "RegularizerApplicator", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "super", "(", "DROPParser", ",", "self", ")", ".", "__init__", "(", "\n", "vocab", "=", "vocab", ",", "\n", "action_embedding_dim", "=", "action_embedding_dim", ",", "\n", "dropout", "=", "dropout", ",", "\n", "debug", "=", "debug", ",", "\n", "regularizer", "=", "regularizer", ",", "\n", ")", "\n", "\n", "question_encoding_dim", "=", "phrase_layer", ".", "get_output_dim", "(", ")", "\n", "\n", "self", ".", "_decoder_step", "=", "BasicTransitionFunction", "(", "\n", "encoder_output_dim", "=", "question_encoding_dim", ",", "\n", "action_embedding_dim", "=", "action_embedding_dim", ",", "\n", "input_attention", "=", "transitionfunc_attention", ",", "\n", "activation", "=", "Activation", ".", "by_name", "(", "\"tanh\"", ")", "(", ")", ",", "\n", "predict_start_type_separately", "=", "False", ",", "\n", "num_start_types", "=", "1", ",", "\n", "add_action_bias", "=", "False", ",", "\n", "dropout", "=", "dropout", ",", "\n", ")", "\n", "self", ".", "_mml", "=", "MaximumMarginalLikelihood", "(", ")", "\n", "\n", "self", ".", "modeltype", "=", "modeltype", "\n", "\n", "self", ".", "_beam_size", "=", "beam_size", "\n", "self", ".", "_decoder_beam_search", "=", "BeamSearch", "(", "beam_size", "=", "self", ".", "_beam_size", ")", "\n", "self", ".", "_max_decoding_steps", "=", "max_decoding_steps", "\n", "self", ".", "_action_padding_index", "=", "-", "1", "\n", "\n", "# This metrircs measure accuracy of", "\n", "# (1) Top-predicted program, (2) ExpectedDenotation from the beam (3) Best accuracy from topK(5) programs", "\n", "self", ".", "top1_acc_metric", "=", "Average", "(", ")", "\n", "self", ".", "expden_acc_metric", "=", "Average", "(", ")", "\n", "self", ".", "topk_acc_metric", "=", "Average", "(", ")", "\n", "self", ".", "aux_goldparse_loss", "=", "Average", "(", ")", "\n", "self", ".", "qent_loss", "=", "Average", "(", ")", "\n", "self", ".", "qattn_cov_loss_metric", "=", "Average", "(", ")", "\n", "\n", "self", ".", "_text_field_embedder", "=", "text_field_embedder", "\n", "\n", "text_embed_dim", "=", "self", ".", "_text_field_embedder", ".", "get_output_dim", "(", ")", "\n", "\n", "encoding_in_dim", "=", "phrase_layer", ".", "get_input_dim", "(", ")", "\n", "encoding_out_dim", "=", "phrase_layer", ".", "get_output_dim", "(", ")", "\n", "modeling_in_dim", "=", "modeling_layer", ".", "get_input_dim", "(", ")", "\n", "modeling_out_dim", "=", "modeling_layer", ".", "get_output_dim", "(", ")", "\n", "\n", "self", ".", "_embedding_proj_layer", "=", "torch", ".", "nn", ".", "Linear", "(", "text_embed_dim", ",", "encoding_in_dim", ")", "\n", "self", ".", "_highway_layer", "=", "Highway", "(", "encoding_in_dim", ",", "num_highway_layers", ")", "\n", "\n", "self", ".", "_encoding_proj_layer", "=", "torch", ".", "nn", ".", "Linear", "(", "encoding_in_dim", ",", "encoding_in_dim", ")", "\n", "self", ".", "_phrase_layer", "=", "phrase_layer", "\n", "\n", "# Used for modeling", "\n", "self", ".", "_matrix_attention", "=", "matrix_attention_layer", "\n", "\n", "self", ".", "_modeling_proj_layer", "=", "torch", ".", "nn", ".", "Linear", "(", "encoding_out_dim", "*", "4", ",", "modeling_in_dim", ")", "\n", "self", ".", "_modeling_layer", "=", "modeling_layer", "\n", "\n", "# Use a separate encoder for passage - date - num similarity", "\n", "\n", "self", ".", "qp_matrix_attention", "=", "LinearMatrixAttention", "(", "\n", "tensor_1_dim", "=", "encoding_out_dim", ",", "tensor_2_dim", "=", "modeling_out_dim", ",", "combination", "=", "\"x,y,x*y\"", "\n", ")", "\n", "\n", "# self.passage_token_to_date = passage_token_to_date", "\n", "self", ".", "dotprod_matrix_attn", "=", "DotProductMatrixAttention", "(", ")", "\n", "\n", "if", "dropout", ">", "0", ":", "\n", "            ", "self", ".", "_dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_dropout", "=", "lambda", "x", ":", "x", "\n", "\n", "", "self", ".", "num_counts", "=", "10", "\n", "self", ".", "passage_attention_to_count", "=", "passage_attention_to_count", "\n", "self", ".", "passage_count_predictor", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "self", ".", "passage_attention_to_count", ".", "get_output_dim", "(", ")", ",", "self", ".", "num_counts", ",", "bias", "=", "False", "\n", ")", "\n", "self", ".", "passage_count_hidden2logits", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "self", ".", "passage_attention_to_count", ".", "get_output_dim", "(", ")", ",", "1", ",", "bias", "=", "True", "\n", ")", "\n", "\n", "# self.passage_count_predictor.bias.data.zero_()", "\n", "# self.passage_count_predictor.bias.requires_grad = False", "\n", "\n", "self", ".", "_executor_parameters", "=", "ExecutorParameters", "(", "\n", "question_encoding_dim", "=", "encoding_out_dim", ",", "\n", "passage_encoding_dim", "=", "encoding_out_dim", ",", "\n", "passage_attention_to_span", "=", "passage_attention_to_span", ",", "\n", "question_attention_to_span", "=", "question_attention_to_span", ",", "\n", "passage_attention_to_count", "=", "self", ".", "passage_attention_to_count", ",", "\n", "passage_count_predictor", "=", "self", ".", "passage_count_predictor", ",", "\n", "passage_count_hidden2logits", "=", "self", ".", "passage_count_hidden2logits", ",", "\n", "dropout", "=", "dropout", ",", "\n", ")", "\n", "\n", "self", ".", "modelloss_metric", "=", "Average", "(", ")", "\n", "self", ".", "excloss_metric", "=", "Average", "(", ")", "\n", "self", ".", "qattloss_metric", "=", "Average", "(", ")", "\n", "self", ".", "mmlloss_metric", "=", "Average", "(", ")", "\n", "self", ".", "auxwinloss_metric", "=", "Average", "(", ")", "\n", "self", ".", "_drop_metrics", "=", "DropEmAndF1", "(", ")", "\n", "\n", "self", ".", "auxwinloss", "=", "auxwinloss", "\n", "\n", "# Main loss for QA", "\n", "self", ".", "denotation_loss", "=", "denotationloss", "\n", "# Auxiliary losses, such as - Prog-MML, QAttn, DateGrounding etc.", "\n", "self", ".", "excloss", "=", "excloss", "\n", "self", ".", "qattloss", "=", "qattloss", "\n", "self", ".", "mmlloss", "=", "mmlloss", "\n", "\n", "initializers", "(", "self", ")", "\n", "\n", "for", "name", ",", "parameter", "in", "self", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "name", "==", "\"_text_field_embedder.token_embedder_tokens.weight\"", ":", "\n", "                ", "parameter", ".", "requires_grad", "=", "False", "\n", "\n", "# # # Fix parameters for Counting", "\n", "", "", "count_parameter_names", "=", "[", "\"passage_attention_to_count\"", ",", "\"passage_count_hidden2logits\"", ",", "\"passage_count_predictor\"", "]", "\n", "if", "countfixed", ":", "\n", "            ", "for", "name", ",", "parameter", "in", "self", ".", "named_parameters", "(", ")", ":", "\n", "                ", "if", "any", "(", "span", "in", "name", "for", "span", "in", "count_parameter_names", ")", ":", "\n", "                    ", "parameter", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser.forward": [[210, 987], ["len", "drop_parser.DROPParser._get_prediction_device", "allennlp.get_text_field_mask().float", "allennlp.get_text_field_mask().float", "drop_parser.DROPParser._dropout", "drop_parser.DROPParser._dropout", "drop_parser.DROPParser._highway_layer", "drop_parser.DROPParser._highway_layer", "drop_parser.DROPParser._dropout", "drop_parser.DROPParser._dropout", "drop_parser.DROPParser._matrix_attention", "drop_parser.DROPParser.transpose", "allennlp.masked_softmax", "allennlp.masked_softmax", "drop_parser.DROPParser.compute_token_date_alignments", "drop_parser.DROPParser.compute_token_date_alignments", "drop_parser.DROPParser.compute_token_date_alignments", "drop_parser.DROPParser._executor_parameters.passage_to_num_attention", "allennlp.masked_softmax", "allennlp.get_final_encoder_states", "languages[].all_possible_productions", "semqa.models.utils.semparse_utils._convert_finalstates_to_actions", "drop_parser.DROPParser.passage_attention_to_sidearg", "drop_parser.DROPParser.datecompare_eventdategr_to_sideargs", "drop_parser.DROPParser.numcompare_eventnumgr_to_sideargs", "drop_parser.DROPParser._get_denotations", "drop_parser.DROPParser._text_field_embedder", "drop_parser.DROPParser._text_field_embedder", "drop_parser.DROPParser._embedding_proj_layer", "drop_parser.DROPParser._embedding_proj_layer", "allennlp.get_text_field_mask().float.size", "drop_parser.DROPParser._phrase_layer", "drop_parser.DROPParser._phrase_layer", "drop_parser.DROPParser.compute_avg_norm", "print", "drop_parser.DROPParser.compute_avg_norm", "print", "drop_parser.DROPParser.compute_avg_norm", "print", "allennlp.weighted_sum", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "allennlp.weighted_sum", "drop_parser.DROPParser._dropout", "drop_parser.DROPParser._modeling_proj_layer", "drop_parser.DROPParser._dropout", "drop_parser.DROPParser.qp_matrix_attention", "drop_parser.DROPParser.transpose", "allennlp.masked_softmax", "allennlp.masked_softmax", "allennlp.get_text_field_mask().float.unsqueeze", "allennlp.get_text_field_mask().float.unsqueeze", "passage_tokenidx2numidx_mask.unsqueeze", "drop_parser.DROPParser.masking_blockdiagonal", "drop_parser.DROPParser.window_loss_numdate", "drop_parser.DROPParser.window_loss_numdate", "drop_parser.DROPParser.window_loss_numdate", "drop_parser.DROPParser.window_loss_numdate", "drop_parser.DROPParser._phrase_layer.is_bidirectional", "semqa.domain_languages.drop_language.DropLanguage", "any", "drop_parser.DROPParser.getInitialDecoderState", "drop_parser.DROPParser._decoder_beam_search.search", "drop_parser.DROPParser.size", "allennlp.move_to_device().float", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "allennlp.move_to_device", "allennlp.move_to_device", "drop_parser.DROPParser.modelloss_metric", "range", "allennlp.get_text_field_mask", "allennlp.get_text_field_mask", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "drop_parser.DROPParser._modeling_layer", "allennlp.get_text_field_mask().float.unsqueeze", "allennlp.get_text_field_mask().float.unsqueeze", "passage_tokenidx2numidx_mask.unsqueeze", "range", "range", "range", "range", "range", "range", "range", "range", "range", "range", "range", "range", "range", "range", "range", "range", "enumerate", "drop_parser.DROPParser._select_indices_from_list", "zip", "list", "list", "drop_parser.DROPParser.initialState_forInstanceIndices", "allennlp.state_machines.ConstrainedBeamSearch", "allennlp.state_machines.ConstrainedBeamSearch.search", "allennlp.state_machines.ConstrainedBeamSearch.search.values", "drop_parser.DROPParser.merge_final_states", "drop_parser.DROPParser.getInitialDecoderState", "drop_parser.DROPParser.get_valid_start_actionids", "semqa.state_machines.constrained_beam_search.FirstStepConstrainedBeamSearch", "semqa.state_machines.constrained_beam_search.FirstStepConstrainedBeamSearch.search", "drop_parser.DROPParser.auxwinloss_metric", "drop_parser.DROPParser._ques_attention_loss", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "range", "allennlp.move_to_device.item", "range", "output_dict[].append", "output_dict[].append", "metadata[].get", "drop_parser.DROPParser._drop_metrics", "languages[].all_possible_productions", "len", "len", "drop_parser.DROPParser.initialState_forInstanceIndices", "drop_parser.DROPParser._select_indices_from_list", "drop_parser.DROPParser.get_valid_start_actionids", "semqa.state_machines.constrained_beam_search.FirstStepConstrainedBeamSearch", "semqa.state_machines.constrained_beam_search.FirstStepConstrainedBeamSearch.search", "allennlp.move_to_device", "aux_win_loss.item", "drop_parser.DROPParser.excloss_metric", "drop_parser.DROPParser.qattloss_metric", "drop_parser.DROPParser.mmlloss_metric", "range", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "allennlp.logsumexp", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "len", "all_instance_progs_predicted_answer_strs.append", "len", "all_instance_progs_predicted_answer_strs.append", "enumerate", "enumerate", "state.score[].view", "allennlp.logsumexp", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "batch_exec_loss.item", "drop_parser.DROPParser.item", "mml_loss.item", "len", "len", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "new_instance_progs_logprob_list.append", "instance_log_likelihood_list.append", "logger.info", "logger.info", "logger.info", "allennlp.models.reading_comprehension.util.get_best_span().squeeze", "tuple", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "drop_parser.DROPParser._get_span_answer_log_prob", "logger.info", "allennlp.move_to_device", "logger.info", "allennlp.move_to_device", "allennlp.models.reading_comprehension.util.get_best_span().squeeze.detach().cpu().numpy", "allennlp.models.reading_comprehension.util.get_best_span().squeeze", "tuple", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "print", "print", "print", "print", "print", "drop_parser.DROPParser._get_span_answer_log_prob", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "allennlp.models.reading_comprehension.util.get_best_span", "allennlp.models.reading_comprehension.util.get_best_span().squeeze.detach().cpu().numpy", "denotation._value.detach().cpu().numpy", "numpy.argmax", "str", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "print", "print", "print", "print", "print", "torch.log", "torch.log", "torch.log", "torch.log", "allennlp.move_to_device", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "allennlp.models.reading_comprehension.util.get_best_span().squeeze.detach().cpu", "allennlp.models.reading_comprehension.util.get_best_span", "torch.argmax().detach().cpu().numpy", "torch.argmax().detach().cpu().numpy", "torch.argmax().detach().cpu().numpy", "torch.argmax().detach().cpu().numpy", "str", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.log", "torch.log", "torch.log", "torch.log", "allennlp.move_to_device", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "denotation._value[].unsqueeze", "denotation._value[].unsqueeze", "allennlp.models.reading_comprehension.util.get_best_span().squeeze.detach().cpu", "denotation._value.detach().cpu", "int", "torch.argmax().detach().cpu().numpy", "torch.argmax().detach().cpu().numpy", "torch.argmax().detach().cpu().numpy", "torch.argmax().detach().cpu().numpy", "str", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.log", "torch.log", "torch.log", "torch.log", "allennlp.move_to_device", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "allennlp.models.reading_comprehension.util.get_best_span().squeeze.detach", "denotation._value[].unsqueeze", "denotation._value[].unsqueeze", "torch.argmax().detach().cpu", "torch.argmax().detach().cpu", "torch.argmax().detach().cpu", "torch.argmax().detach().cpu", "int", "int", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "allennlp.models.reading_comprehension.util.get_best_span().squeeze.detach", "denotation._value.detach", "torch.argmax().detach().cpu", "torch.argmax().detach().cpu", "torch.argmax().detach().cpu", "torch.argmax().detach().cpu", "int", "torch.argmax().detach", "torch.argmax().detach", "torch.argmax().detach", "torch.argmax().detach", "torch.argmax().detach", "torch.argmax().detach", "torch.argmax().detach", "torch.argmax().detach", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser.compute_token_date_alignments", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser.compute_token_date_alignments", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser.compute_token_date_alignments", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.semparse_utils._convert_finalstates_to_actions", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.passage_attention_to_sidearg", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.datecompare_eventdategr_to_sideargs", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.numcompare_eventnumgr_to_sideargs", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_base.DROPParserBase._get_denotations", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.compute_avg_norm", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.compute_avg_norm", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.compute_avg_norm", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.masking_blockdiagonal", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.window_loss_numdate", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.window_loss_numdate", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.window_loss_numdate", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.window_loss_numdate", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.getInitialDecoderState", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.state_machines.prefixed_beam_search.PrefixedConstrainedBeamSearch.search", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT._select_indices_from_list", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.initialState_forInstanceIndices", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.state_machines.prefixed_beam_search.PrefixedConstrainedBeamSearch.search", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.merge_final_states", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.getInitialDecoderState", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.get_valid_start_actionids", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.state_machines.prefixed_beam_search.PrefixedConstrainedBeamSearch.search", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT._ques_attention_loss", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.initialState_forInstanceIndices", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT._select_indices_from_list", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.get_valid_start_actionids", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.state_machines.prefixed_beam_search.PrefixedConstrainedBeamSearch.search", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.qanet.QANet._get_span_answer_log_prob", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.qanet.QANet._get_span_answer_log_prob", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "", "", "", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "question", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", ",", "\n", "passage", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", ",", "\n", "passageidx2numberidx", ":", "torch", ".", "LongTensor", ",", "\n", "passage_number_values", ":", "List", "[", "List", "[", "float", "]", "]", ",", "\n", "passage_number_sortedtokenidxs", ":", "List", "[", "List", "[", "int", "]", "]", ",", "\n", "passageidx2dateidx", ":", "torch", ".", "LongTensor", ",", "\n", "passage_date_values", ":", "List", "[", "List", "[", "Date", "]", "]", ",", "\n", "actions", ":", "List", "[", "List", "[", "ProductionRule", "]", "]", ",", "\n", "year_differences", ":", "List", "[", "List", "[", "int", "]", "]", ",", "\n", "year_differences_mat", ":", "List", "[", "np", ".", "array", "]", ",", "\n", "count_values", ":", "List", "[", "List", "[", "int", "]", "]", ",", "\n", "passagenumber_difference_values", ":", "List", "[", "List", "[", "float", "]", "]", ",", "\n", "passagenumber_differences_mat", ":", "List", "[", "np", ".", "array", "]", ",", "\n", "answer_program_start_types", ":", "List", "[", "Union", "[", "List", "[", "str", "]", ",", "None", "]", "]", "=", "None", ",", "\n", "answer_as_passage_spans", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "answer_as_question_spans", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "answer_as_passage_number", ":", "List", "[", "List", "[", "int", "]", "]", "=", "None", ",", "\n", "answer_as_year_difference", ":", "List", "[", "List", "[", "int", "]", "]", "=", "None", ",", "\n", "answer_as_count", ":", "List", "[", "List", "[", "int", "]", "]", "=", "None", ",", "\n", "answer_as_passagenum_difference", ":", "List", "[", "List", "[", "int", "]", "]", "=", "None", ",", "\n", "datecomp_ques_event_date_groundings", ":", "List", "[", "Tuple", "[", "List", "[", "int", "]", ",", "List", "[", "int", "]", "]", "]", "=", "None", ",", "\n", "numcomp_qspan_num_groundings", ":", "List", "[", "Tuple", "[", "List", "[", "int", "]", ",", "List", "[", "int", "]", "]", "]", "=", "None", ",", "\n", "strongly_supervised", ":", "List", "[", "bool", "]", "=", "None", ",", "\n", "program_supervised", ":", "List", "[", "bool", "]", "=", "None", ",", "\n", "qattn_supervised", ":", "List", "[", "bool", "]", "=", "None", ",", "\n", "pattn_supervised", ":", "List", "[", "bool", "]", "=", "None", ",", "\n", "execution_supervised", ":", "List", "[", "bool", "]", "=", "None", ",", "\n", "qtypes", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "gold_action_seqs", ":", "List", "[", "Tuple", "[", "List", "[", "List", "[", "int", "]", "]", ",", "List", "[", "List", "[", "int", "]", "]", "]", "]", "=", "None", ",", "\n", "qattn_supervision", ":", "torch", ".", "FloatTensor", "=", "None", ",", "\n", "passage_attn_supervision", ":", "List", "[", "List", "[", "float", "]", "]", "=", "None", ",", "\n", "synthetic_numground_metadata", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "=", "None", ",", "\n", "epoch_num", ":", "List", "[", "int", "]", "=", "None", ",", "\n", "metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "\n", "aux_passage_attention", "=", "None", ",", "\n", "aux_answer_as_count", "=", "None", ",", "\n", "aux_count_mask", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "batch_size", "=", "len", "(", "actions", ")", "\n", "device_id", "=", "self", ".", "_get_prediction_device", "(", ")", "\n", "\n", "if", "epoch_num", "is", "not", "None", ":", "\n", "# epoch_num in allennlp starts from 0", "\n", "            ", "epoch", "=", "epoch_num", "[", "0", "]", "+", "1", "\n", "", "else", ":", "\n", "            ", "epoch", "=", "None", "\n", "\n", "", "question_mask", "=", "allenutil", ".", "get_text_field_mask", "(", "question", ")", ".", "float", "(", ")", "\n", "passage_mask", "=", "allenutil", ".", "get_text_field_mask", "(", "passage", ")", ".", "float", "(", ")", "\n", "\n", "rawemb_question", "=", "self", ".", "_dropout", "(", "self", ".", "_text_field_embedder", "(", "question", ")", ")", "\n", "rawemb_passage", "=", "self", ".", "_dropout", "(", "self", ".", "_text_field_embedder", "(", "passage", ")", ")", "\n", "\n", "embedded_question", "=", "self", ".", "_highway_layer", "(", "self", ".", "_embedding_proj_layer", "(", "rawemb_question", ")", ")", "\n", "embedded_passage", "=", "self", ".", "_highway_layer", "(", "self", ".", "_embedding_proj_layer", "(", "rawemb_passage", ")", ")", "\n", "\n", "passage_length", "=", "passage_mask", ".", "size", "(", ")", "[", "1", "]", "\n", "\n", "# projected_embedded_question = self._encoding_proj_layer(embedded_question)", "\n", "# projected_embedded_passage = self._encoding_proj_layer(embedded_passage)", "\n", "\n", "# stripped version", "\n", "projected_embedded_question", "=", "embedded_question", "\n", "projected_embedded_passage", "=", "embedded_passage", "\n", "\n", "# Shape: (batch_size, question_length, encoding_dim)", "\n", "encoded_question", "=", "self", ".", "_dropout", "(", "self", ".", "_phrase_layer", "(", "projected_embedded_question", ",", "question_mask", ")", ")", "\n", "# Shape: (batch_size, passage_length, encoding_dim)", "\n", "encoded_passage", "=", "self", ".", "_dropout", "(", "self", ".", "_phrase_layer", "(", "projected_embedded_passage", ",", "passage_mask", ")", ")", "\n", "\n", "if", "self", ".", "_debug", ":", "\n", "            ", "rawemb_passage_norm", "=", "self", ".", "compute_avg_norm", "(", "rawemb_passage", ")", "\n", "print", "(", "f\"Raw embedded passage Norm: {rawemb_passage_norm}\"", ")", "\n", "projected_embedded_passage_norm", "=", "self", ".", "compute_avg_norm", "(", "projected_embedded_passage", ")", "\n", "print", "(", "f\"Projected embedded passage Norm: {projected_embedded_passage_norm}\"", ")", "\n", "encoded_passage_norm", "=", "self", ".", "compute_avg_norm", "(", "encoded_passage", ")", "\n", "print", "(", "f\"Encoded passage Norm: {encoded_passage_norm}\"", ")", "\n", "\n", "# Shape: (batch_size, question_length, passage_length)", "\n", "# Shape: (batch_size, question_length, passage_length)", "\n", "# if self.sim_key == 'dot':", "\n", "#     question_passage_similarity_phrase = self._executor_parameters.dotprod_matrix_attn(question_sim_repr,", "\n", "#                                                                                        passage_sim_repr)", "\n", "#     passage_question_similarity_phrase = question_passage_similarity_phrase.transpose(1, 2)", "\n", "# elif self.sim_key == 'bi':", "\n", "#     question_passage_similarity_phrase = self.q2p_bilinear_matrixattn(question_sim_repr,", "\n", "#                                                                       passage_sim_repr)", "\n", "#     passage_question_similarity_phrase = question_passage_similarity_phrase.transpose(1, 2)", "\n", "# elif self.sim_key == 'ma':", "\n", "\n", "# question_passage_similarity = self._matrix_attention(question_sim_repr, passage_sim_repr)", "\n", "# else:", "\n", "#     raise NotImplementedError", "\n", "\n", "", "passage_question_similarity_phrase", "=", "self", ".", "_matrix_attention", "(", "encoded_passage", ",", "encoded_question", ")", "\n", "question_passage_similarity_phrase", "=", "passage_question_similarity_phrase", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "# Shape: (batch_size, passage_length, question_length)", "\n", "passage_question_attention_phrase", "=", "allenutil", ".", "masked_softmax", "(", "\n", "passage_question_similarity_phrase", ",", "question_mask", ",", "memory_efficient", "=", "True", "\n", ")", "\n", "\n", "# Shape: (batch_size, question_length, passage_length)", "\n", "question_passage_attention_phrase", "=", "allenutil", ".", "masked_softmax", "(", "\n", "question_passage_similarity_phrase", ",", "passage_mask", ",", "memory_efficient", "=", "True", "\n", ")", "\n", "if", "self", ".", "modeltype", "==", "\"modeled\"", ":", "\n", "# Shape: (batch_size, passage_length, encoding_dim)", "\n", "            ", "passage_question_vectors", "=", "allenutil", ".", "weighted_sum", "(", "encoded_question", ",", "passage_question_attention_phrase", ")", "\n", "# Shape: (batch_size, passage_length, passage_length)", "\n", "attention_over_attention", "=", "torch", ".", "bmm", "(", "passage_question_attention_phrase", ",", "question_passage_attention_phrase", ")", "\n", "# Shape: (batch_size, passage_length, encoding_dim)", "\n", "passage_passage_vectors", "=", "allenutil", ".", "weighted_sum", "(", "encoded_passage", ",", "attention_over_attention", ")", "\n", "\n", "# Shape: (batch_size, passage_length, encoding_dim * 4)", "\n", "merged_passage_attention_vectors", "=", "self", ".", "_dropout", "(", "\n", "torch", ".", "cat", "(", "\n", "[", "\n", "encoded_passage", ",", "\n", "passage_question_vectors", ",", "\n", "encoded_passage", "*", "passage_question_vectors", ",", "\n", "encoded_passage", "*", "passage_passage_vectors", ",", "\n", "]", ",", "\n", "dim", "=", "-", "1", ",", "\n", ")", "\n", ")", "\n", "\n", "modeled_passage_input", "=", "self", ".", "_modeling_proj_layer", "(", "merged_passage_attention_vectors", ")", "\n", "modeled_passage", "=", "self", ".", "_dropout", "(", "self", ".", "_modeling_layer", "(", "modeled_passage_input", ",", "passage_mask", ")", ")", "\n", "\n", "question_passage_similarity", "=", "self", ".", "qp_matrix_attention", "(", "encoded_question", ",", "modeled_passage", ")", "\n", "passage_question_similarity", "=", "question_passage_similarity", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "question_passage_attention", "=", "allenutil", ".", "masked_softmax", "(", "\n", "question_passage_similarity", ",", "passage_mask", ".", "unsqueeze", "(", "1", ")", ",", "memory_efficient", "=", "True", "\n", ")", "\n", "\n", "passage_question_attention", "=", "allenutil", ".", "masked_softmax", "(", "\n", "passage_question_similarity", ",", "question_mask", ".", "unsqueeze", "(", "1", ")", ",", "memory_efficient", "=", "True", "\n", ")", "\n", "", "elif", "self", ".", "modeltype", "==", "\"encoded\"", ":", "\n", "            ", "modeled_passage", "=", "encoded_passage", "\n", "question_passage_similarity", "=", "question_passage_similarity_phrase", "\n", "passage_question_similarity", "=", "passage_question_similarity_phrase", "\n", "question_passage_attention", "=", "question_passage_attention_phrase", "\n", "passage_question_attention", "=", "passage_question_attention_phrase", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "# Shape: (batch_size, passage_length, passage_length)", "\n", "", "passage_passage_token2date_alignment", "=", "self", ".", "compute_token_date_alignments", "(", "\n", "modeled_passage", "=", "modeled_passage", ",", "\n", "passage_mask", "=", "passage_mask", ",", "\n", "passageidx2dateidx", "=", "passageidx2dateidx", ",", "\n", "passage_to_date_attention_params", "=", "self", ".", "_executor_parameters", ".", "passage_to_date_attention", ",", "\n", ")", "\n", "\n", "passage_passage_token2startdate_alignment", "=", "self", ".", "compute_token_date_alignments", "(", "\n", "modeled_passage", "=", "modeled_passage", ",", "\n", "passage_mask", "=", "passage_mask", ",", "\n", "passageidx2dateidx", "=", "passageidx2dateidx", ",", "\n", "passage_to_date_attention_params", "=", "self", ".", "_executor_parameters", ".", "passage_to_start_date_attention", ",", "\n", ")", "\n", "\n", "passage_passage_token2enddate_alignment", "=", "self", ".", "compute_token_date_alignments", "(", "\n", "modeled_passage", "=", "modeled_passage", ",", "\n", "passage_mask", "=", "passage_mask", ",", "\n", "passageidx2dateidx", "=", "passageidx2dateidx", ",", "\n", "passage_to_date_attention_params", "=", "self", ".", "_executor_parameters", ".", "passage_to_end_date_attention", ",", "\n", ")", "\n", "\n", "passage_tokenidx2dateidx_mask", "=", "(", "passageidx2numberidx", ">", "-", "1", ")", ".", "float", "(", ")", "\n", "\n", "# Shape: (batch_size, passage_length, passage_length)", "\n", "# passage_passage_token2num_similarity = self._executor_parameters.passage_to_num_attention(encoded_passage,", "\n", "#                                                                                           encoded_passage)", "\n", "passage_passage_token2num_similarity", "=", "self", ".", "_executor_parameters", ".", "passage_to_num_attention", "(", "\n", "modeled_passage", ",", "modeled_passage", "\n", ")", "\n", "passage_passage_token2num_similarity", "=", "passage_passage_token2num_similarity", "*", "passage_mask", ".", "unsqueeze", "(", "1", ")", "\n", "passage_passage_token2num_similarity", "=", "passage_passage_token2num_similarity", "*", "passage_mask", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Shape: (batch_size, passage_length) -- masking for number tokens in the passage", "\n", "passage_tokenidx2numidx_mask", "=", "(", "passageidx2numberidx", ">", "-", "1", ")", ".", "float", "(", ")", "\n", "# Shape: (batch_size, passage_length, passage_length)", "\n", "passage_passage_token2num_similarity", "=", "(", "\n", "passage_passage_token2num_similarity", "*", "passage_tokenidx2numidx_mask", ".", "unsqueeze", "(", "1", ")", "\n", ")", "\n", "# Shape: (batch_size, passage_length, passage_length)", "\n", "passage_passage_token2num_alignment", "=", "allenutil", ".", "masked_softmax", "(", "\n", "passage_passage_token2num_similarity", ",", "mask", "=", "passage_tokenidx2numidx_mask", ".", "unsqueeze", "(", "1", ")", ",", "memory_efficient", "=", "True", "\n", ")", "\n", "\n", "# json_dicts = []", "\n", "# for i in range(batch_size):", "\n", "#     ques_tokens = metadata[i]['question_tokens']", "\n", "#     passage_tokens = metadata[i]['passage_tokens']", "\n", "#     p2num_sim = myutils.tocpuNPList(passage_passage_token2num_similarity[i])", "\n", "#     outdict = {'q': ques_tokens, 'p': passage_tokens, 'num': p2num_sim}", "\n", "#     json_dicts.append(outdict)", "\n", "# json.dump(json_dicts, num_attentions_f)", "\n", "# num_attentions_f.close()", "\n", "# exit()", "\n", "\n", "\"\"\" Aux Loss \"\"\"", "\n", "if", "self", ".", "auxwinloss", ":", "\n", "            ", "inwindow_mask", ",", "outwindow_mask", "=", "self", ".", "masking_blockdiagonal", "(", "batch_size", ",", "passage_length", ",", "10", ",", "device_id", ")", "\n", "num_aux_loss", "=", "self", ".", "window_loss_numdate", "(", "\n", "passage_passage_token2num_alignment", ",", "passage_tokenidx2numidx_mask", ",", "inwindow_mask", ",", "outwindow_mask", "\n", ")", "\n", "\n", "date_aux_loss", "=", "self", ".", "window_loss_numdate", "(", "\n", "passage_passage_token2date_alignment", ",", "passage_tokenidx2dateidx_mask", ",", "inwindow_mask", ",", "outwindow_mask", "\n", ")", "\n", "\n", "start_date_aux_loss", "=", "self", ".", "window_loss_numdate", "(", "\n", "passage_passage_token2startdate_alignment", ",", "passage_tokenidx2dateidx_mask", ",", "inwindow_mask", ",", "outwindow_mask", "\n", ")", "\n", "\n", "end_date_aux_loss", "=", "self", ".", "window_loss_numdate", "(", "\n", "passage_passage_token2enddate_alignment", ",", "passage_tokenidx2dateidx_mask", ",", "inwindow_mask", ",", "outwindow_mask", "\n", ")", "\n", "\n", "# count_loss = self.number2count_auxloss(passage_number_values=passage_number_values,", "\n", "#                                        device_id=device_id)", "\n", "count_loss", "=", "0.0", "\n", "aux_win_loss", "=", "num_aux_loss", "+", "date_aux_loss", "+", "count_loss", "+", "start_date_aux_loss", "+", "end_date_aux_loss", "\n", "\n", "", "else", ":", "\n", "            ", "aux_win_loss", "=", "0.0", "\n", "\n", "", "\"\"\" Parser setup \"\"\"", "\n", "# Shape: (B, encoding_dim)", "\n", "question_encoded_final_state", "=", "allenutil", ".", "get_final_encoder_states", "(", "\n", "encoded_question", ",", "question_mask", ",", "self", ".", "_phrase_layer", ".", "is_bidirectional", "(", ")", "\n", ")", "\n", "question_rawemb_aslist", "=", "[", "rawemb_question", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "question_embedded_aslist", "=", "[", "projected_embedded_question", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "question_encoded_aslist", "=", "[", "encoded_question", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "question_mask_aslist", "=", "[", "question_mask", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "passage_rawemb_aslist", "=", "[", "rawemb_passage", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "passage_embedded_aslist", "=", "[", "projected_embedded_passage", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "passage_encoded_aslist", "=", "[", "encoded_passage", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "passage_modeled_aslist", "=", "[", "modeled_passage", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "passage_mask_aslist", "=", "[", "passage_mask", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "q2p_attention_aslist", "=", "[", "question_passage_attention", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "p2q_attention_aslist", "=", "[", "passage_question_attention", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "# p2pdate_similarity_aslist = [passage_passage_token2date_similarity[i] for i in range(batch_size)]", "\n", "# p2pnum_similarity_aslist = [passage_passage_token2num_similarity[i] for i in range(batch_size)]", "\n", "p2pdate_alignment_aslist", "=", "[", "passage_passage_token2date_alignment", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "p2pstartdate_alignment_aslist", "=", "[", "passage_passage_token2startdate_alignment", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "p2penddate_alignment_aslist", "=", "[", "passage_passage_token2enddate_alignment", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "p2pnum_alignment_aslist", "=", "[", "passage_passage_token2num_alignment", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "# passage_token2datetoken_sim_aslist = [passage_token2datetoken_similarity[i] for i in range(batch_size)]", "\n", "\n", "languages", "=", "[", "\n", "DropLanguage", "(", "\n", "rawemb_question", "=", "question_rawemb_aslist", "[", "i", "]", ",", "\n", "embedded_question", "=", "question_embedded_aslist", "[", "i", "]", ",", "\n", "encoded_question", "=", "question_encoded_aslist", "[", "i", "]", ",", "\n", "rawemb_passage", "=", "passage_rawemb_aslist", "[", "i", "]", ",", "\n", "embedded_passage", "=", "passage_embedded_aslist", "[", "i", "]", ",", "\n", "encoded_passage", "=", "passage_encoded_aslist", "[", "i", "]", ",", "\n", "modeled_passage", "=", "passage_modeled_aslist", "[", "i", "]", ",", "\n", "# passage_token2datetoken_sim=None, #passage_token2datetoken_sim_aslist[i],", "\n", "question_mask", "=", "question_mask_aslist", "[", "i", "]", ",", "\n", "passage_mask", "=", "passage_mask_aslist", "[", "i", "]", ",", "\n", "passage_tokenidx2dateidx", "=", "passageidx2dateidx", "[", "i", "]", ",", "\n", "passage_date_values", "=", "passage_date_values", "[", "i", "]", ",", "\n", "passage_tokenidx2numidx", "=", "passageidx2numberidx", "[", "i", "]", ",", "\n", "passage_num_values", "=", "passage_number_values", "[", "i", "]", ",", "\n", "passage_number_sortedtokenidxs", "=", "passage_number_sortedtokenidxs", "[", "i", "]", ",", "\n", "year_differences", "=", "year_differences", "[", "i", "]", ",", "\n", "year_differences_mat", "=", "year_differences_mat", "[", "i", "]", ",", "\n", "count_num_values", "=", "count_values", "[", "i", "]", ",", "\n", "passagenum_differences", "=", "passagenumber_difference_values", "[", "i", "]", ",", "\n", "passagenum_differences_mat", "=", "passagenumber_differences_mat", "[", "i", "]", ",", "\n", "question_passage_attention", "=", "q2p_attention_aslist", "[", "i", "]", ",", "\n", "passage_question_attention", "=", "p2q_attention_aslist", "[", "i", "]", ",", "\n", "passage_token2date_alignment", "=", "p2pdate_alignment_aslist", "[", "i", "]", ",", "\n", "passage_token2startdate_alignment", "=", "p2pstartdate_alignment_aslist", "[", "i", "]", ",", "\n", "passage_token2enddate_alignment", "=", "p2penddate_alignment_aslist", "[", "i", "]", ",", "\n", "passage_token2num_alignment", "=", "p2pnum_alignment_aslist", "[", "i", "]", ",", "\n", "parameters", "=", "self", ".", "_executor_parameters", ",", "\n", "start_types", "=", "None", ",", "# batch_start_types[i],", "\n", "device_id", "=", "device_id", ",", "\n", "debug", "=", "self", ".", "_debug", ",", "\n", "metadata", "=", "metadata", "[", "i", "]", ",", "\n", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", "\n", "]", "\n", "\n", "action2idx_map", "=", "{", "rule", ":", "i", "for", "i", ",", "rule", "in", "enumerate", "(", "languages", "[", "0", "]", ".", "all_possible_productions", "(", ")", ")", "}", "\n", "idx2action_map", "=", "languages", "[", "0", "]", ".", "all_possible_productions", "(", ")", "\n", "\n", "\"\"\"\n        While training, we know the correct start-types for all instances and the gold-programs for some.\n        For instances,\n            #   with gold-programs, we should run a ConstrainedBeamSearch with target_sequences,\n            #   with start-types, figure out the valid start-action-ids and run ConstrainedBeamSearch with firststep_allo..\n        During Validation, we should **always** be running an un-constrained BeamSearch on the full language\n        \"\"\"", "\n", "mml_loss", "=", "0", "\n", "if", "self", ".", "training", ":", "\n", "# If any instance is provided with goldprog, we need to divide the batch into supervised / unsupervised", "\n", "# and run fully-constrained decoding on supervised, and start-type-constrained-decoding on the rest", "\n", "            ", "if", "any", "(", "program_supervised", ")", ":", "\n", "                ", "supervised_instances", "=", "[", "i", "for", "(", "i", ",", "ss", ")", "in", "enumerate", "(", "program_supervised", ")", "if", "ss", "is", "True", "]", "\n", "unsupervised_instances", "=", "[", "i", "for", "(", "i", ",", "ss", ")", "in", "enumerate", "(", "program_supervised", ")", "if", "ss", "is", "False", "]", "\n", "\n", "# List of (gold_actionseq_idxs, gold_actionseq_masks) -- for supervised instances", "\n", "supervised_gold_actionseqs", "=", "self", ".", "_select_indices_from_list", "(", "gold_action_seqs", ",", "supervised_instances", ")", "\n", "s_gold_actionseq_idxs", ",", "s_gold_actionseq_masks", "=", "zip", "(", "*", "supervised_gold_actionseqs", ")", "\n", "s_gold_actionseq_idxs", "=", "list", "(", "s_gold_actionseq_idxs", ")", "\n", "s_gold_actionseq_masks", "=", "list", "(", "s_gold_actionseq_masks", ")", "\n", "(", "supervised_initial_state", ",", "_", ",", "_", ")", "=", "self", ".", "initialState_forInstanceIndices", "(", "\n", "supervised_instances", ",", "\n", "languages", ",", "\n", "actions", ",", "\n", "encoded_question", ",", "\n", "question_mask", ",", "\n", "question_encoded_final_state", ",", "\n", "question_encoded_aslist", ",", "\n", "question_mask_aslist", ",", "\n", ")", "\n", "constrained_search", "=", "ConstrainedBeamSearch", "(", "\n", "self", ".", "_beam_size", ",", "\n", "allowed_sequences", "=", "s_gold_actionseq_idxs", ",", "\n", "allowed_sequence_mask", "=", "s_gold_actionseq_masks", ",", "\n", ")", "\n", "\n", "# print()", "\n", "# print(supervised_instances)", "\n", "# print(qtypes)", "\n", "# print(s_gold_actionseq_idxs)", "\n", "# for i in range(batch_size):", "\n", "#     print(metadata[i][\"original_question\"])", "\n", "# print()", "\n", "\n", "supervised_final_states", "=", "constrained_search", ".", "search", "(", "\n", "initial_state", "=", "supervised_initial_state", ",", "transition_function", "=", "self", ".", "_decoder_step", "\n", ")", "\n", "\n", "for", "instance_states", "in", "supervised_final_states", ".", "values", "(", ")", ":", "\n", "                    ", "scores", "=", "[", "state", ".", "score", "[", "0", "]", ".", "view", "(", "-", "1", ")", "for", "state", "in", "instance_states", "]", "\n", "mml_loss", "+=", "-", "allenutil", ".", "logsumexp", "(", "torch", ".", "cat", "(", "scores", ")", ")", "\n", "", "mml_loss", "=", "mml_loss", "/", "len", "(", "supervised_final_states", ")", "\n", "\n", "if", "len", "(", "unsupervised_instances", ")", ">", "0", ":", "\n", "                    ", "(", "unsupervised_initial_state", ",", "_", ",", "_", ")", "=", "self", ".", "initialState_forInstanceIndices", "(", "\n", "unsupervised_instances", ",", "\n", "languages", ",", "\n", "actions", ",", "\n", "encoded_question", ",", "\n", "question_mask", ",", "\n", "question_encoded_final_state", ",", "\n", "question_encoded_aslist", ",", "\n", "question_mask_aslist", ",", "\n", ")", "\n", "\n", "unsupervised_answer_types", ":", "List", "[", "List", "[", "str", "]", "]", "=", "self", ".", "_select_indices_from_list", "(", "\n", "answer_program_start_types", ",", "unsupervised_instances", "\n", ")", "\n", "unsupervised_ins_start_actionids", ":", "List", "[", "Set", "[", "int", "]", "]", "=", "self", ".", "get_valid_start_actionids", "(", "\n", "answer_types", "=", "unsupervised_answer_types", ",", "action2actionidx", "=", "action2idx_map", "\n", ")", "\n", "\n", "firststep_constrained_search", "=", "MyConstrainedBeamSearch", "(", "self", ".", "_beam_size", ")", "\n", "unsup_final_states", "=", "firststep_constrained_search", ".", "search", "(", "\n", "num_steps", "=", "self", ".", "_max_decoding_steps", ",", "\n", "initial_state", "=", "unsupervised_initial_state", ",", "\n", "transition_function", "=", "self", ".", "_decoder_step", ",", "\n", "firststep_allowed_actions", "=", "unsupervised_ins_start_actionids", ",", "\n", "keep_final_unfinished_states", "=", "False", ",", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "unsup_final_states", "=", "[", "]", "\n", "\n", "# Merge final_states for supervised and unsupervised instances", "\n", "", "best_final_states", "=", "self", ".", "merge_final_states", "(", "\n", "supervised_final_states", ",", "unsup_final_states", ",", "supervised_instances", ",", "unsupervised_instances", "\n", ")", "\n", "\n", "", "else", ":", "\n", "                ", "(", "initial_state", ",", "_", ",", "_", ")", "=", "self", ".", "getInitialDecoderState", "(", "\n", "languages", ",", "\n", "actions", ",", "\n", "encoded_question", ",", "\n", "question_mask", ",", "\n", "question_encoded_final_state", ",", "\n", "question_encoded_aslist", ",", "\n", "question_mask_aslist", ",", "\n", "batch_size", ",", "\n", ")", "\n", "batch_valid_start_actionids", ":", "List", "[", "Set", "[", "int", "]", "]", "=", "self", ".", "get_valid_start_actionids", "(", "\n", "answer_types", "=", "answer_program_start_types", ",", "action2actionidx", "=", "action2idx_map", "\n", ")", "\n", "search", "=", "MyConstrainedBeamSearch", "(", "self", ".", "_beam_size", ")", "\n", "# Mapping[int, Sequence[StateType]])", "\n", "best_final_states", "=", "search", ".", "search", "(", "\n", "self", ".", "_max_decoding_steps", ",", "\n", "initial_state", ",", "\n", "self", ".", "_decoder_step", ",", "\n", "firststep_allowed_actions", "=", "batch_valid_start_actionids", ",", "\n", "keep_final_unfinished_states", "=", "False", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "(", "initial_state", ",", "_", ",", "_", ")", "=", "self", ".", "getInitialDecoderState", "(", "\n", "languages", ",", "\n", "actions", ",", "\n", "encoded_question", ",", "\n", "question_mask", ",", "\n", "question_encoded_final_state", ",", "\n", "question_encoded_aslist", ",", "\n", "question_mask_aslist", ",", "\n", "batch_size", ",", "\n", ")", "\n", "# This is unconstrained beam-search", "\n", "best_final_states", "=", "self", ".", "_decoder_beam_search", ".", "search", "(", "\n", "self", ".", "_max_decoding_steps", ",", "initial_state", ",", "self", ".", "_decoder_step", ",", "keep_final_unfinished_states", "=", "False", "\n", ")", "\n", "\n", "# batch_actionidxs: List[List[List[int]]]: All action sequence indices for each instance in the batch", "\n", "# batch_actionseqs: List[List[List[str]]]: All decoded action sequences for each instance in the batch", "\n", "# batch_actionseq_scores: List[List[torch.Tensor]]: Score for each program of each instance", "\n", "# batch_actionseq_probs: List[torch.FloatTensor]: Tensor containing normalized_prog_probs for each instance - no longer", "\n", "# batch_actionseq_sideargs: List[List[List[Dict]]]: List of side_args for each program of each instance", "\n", "# The actions here should be in the exact same order as passed when creating the initial_grammar_state ...", "\n", "# since the action_ids are assigned based on the order passed there.", "\n", "", "(", "\n", "batch_actionidxs", ",", "\n", "batch_actionseqs", ",", "\n", "batch_actionseq_scores", ",", "\n", "batch_actionseq_sideargs", ",", "\n", ")", "=", "semparse_utils", ".", "_convert_finalstates_to_actions", "(", "\n", "best_final_states", "=", "best_final_states", ",", "possible_actions", "=", "actions", ",", "batch_size", "=", "batch_size", "\n", ")", "\n", "\n", "# Adding Date-Comparison supervised event groundings to relevant actions", "\n", "max_passage_len", "=", "encoded_passage", ".", "size", "(", ")", "[", "1", "]", "\n", "self", ".", "passage_attention_to_sidearg", "(", "\n", "qtypes", ",", "\n", "batch_actionseqs", ",", "\n", "batch_actionseq_sideargs", ",", "\n", "pattn_supervised", ",", "\n", "passage_attn_supervision", ",", "\n", "max_passage_len", ",", "\n", "device_id", ",", "\n", ")", "\n", "\n", "self", ".", "datecompare_eventdategr_to_sideargs", "(", "\n", "qtypes", ",", "batch_actionseqs", ",", "batch_actionseq_sideargs", ",", "datecomp_ques_event_date_groundings", ",", "device_id", "\n", ")", "\n", "\n", "self", ".", "numcompare_eventnumgr_to_sideargs", "(", "\n", "qtypes", ",", "\n", "execution_supervised", ",", "\n", "batch_actionseqs", ",", "\n", "batch_actionseq_sideargs", ",", "\n", "numcomp_qspan_num_groundings", ",", "\n", "device_id", ",", "\n", ")", "\n", "\n", "# # For printing predicted - programs", "\n", "# for idx, instance_progs in enumerate(batch_actionseqs):", "\n", "#     print(f\"InstanceIdx:{idx}\")", "\n", "#     print(metadata[idx][\"question_tokens\"])", "\n", "#     scores = batch_actionseq_scores[idx]", "\n", "#     for prog, score in zip(instance_progs, scores):", "\n", "#         print(f\"{languages[idx].action_sequence_to_logical_form(prog)} : {score}\")", "\n", "#         # print(f\"{prog} : {score}\")", "\n", "# print()", "\n", "\n", "# List[List[Any]], List[List[str]]: Denotations and their types for all instances", "\n", "batch_denotations", ",", "batch_denotation_types", "=", "self", ".", "_get_denotations", "(", "\n", "batch_actionseqs", ",", "languages", ",", "batch_actionseq_sideargs", "\n", ")", "\n", "output_dict", "=", "{", "}", "\n", "# Computing losses if gold answers are given", "\n", "if", "answer_program_start_types", "is", "not", "None", ":", "\n", "# Execution losses --", "\n", "            ", "total_aux_loss", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "tensor", "(", "0.0", ")", ",", "device_id", ")", ".", "float", "(", ")", "\n", "\n", "total_aux_loss", "+=", "aux_win_loss", "\n", "if", "aux_win_loss", "!=", "0", ":", "\n", "                ", "self", ".", "auxwinloss_metric", "(", "aux_win_loss", ".", "item", "(", ")", ")", "\n", "\n", "", "if", "self", ".", "excloss", ":", "\n", "                ", "exec_loss", "=", "0.0", "\n", "batch_exec_loss", "=", "0.0", "\n", "execloss_normalizer", "=", "0.0", "\n", "for", "ins_dens", "in", "batch_denotations", ":", "\n", "                    ", "for", "den", "in", "ins_dens", ":", "\n", "                        ", "execloss_normalizer", "+=", "1.0", "\n", "exec_loss", "+=", "den", ".", "loss", "\n", "", "", "if", "execloss_normalizer", ">", "0", ":", "\n", "                    ", "batch_exec_loss", "=", "exec_loss", "/", "execloss_normalizer", "\n", "# This check is made explicit here since not all batches have this loss, hence a 0.0 value", "\n", "# only bloats the denominator in the metric. This is also done for other losses in below", "\n", "", "if", "batch_exec_loss", "!=", "0.0", ":", "\n", "                    ", "self", ".", "excloss_metric", "(", "batch_exec_loss", ".", "item", "(", ")", ")", "\n", "", "total_aux_loss", "+=", "batch_exec_loss", "\n", "\n", "# # Num-grounding Loss on synthetic data -- DEPRECATED", "\n", "# synthetic_numground_loss = self.synthetic_num_grounding_loss(qtypes,", "\n", "#                                                              synthetic_numground_metadata,", "\n", "#                                                              passage_passage_token2num_similarity,", "\n", "#                                                              passageidx2numberidx)", "\n", "# total_aux_loss += synthetic_numground_loss", "\n", "# if synthetic_numground_loss != 0:", "\n", "#     self.excloss_metric(synthetic_numground_loss.item())", "\n", "\n", "", "if", "self", ".", "qattloss", ":", "\n", "# Compute Question Attention Supervision auxiliary loss", "\n", "                ", "qattn_loss", "=", "self", ".", "_ques_attention_loss", "(", "\n", "batch_actionseqs", ",", "batch_actionseq_sideargs", ",", "qtypes", ",", "qattn_supervised", ",", "qattn_supervision", "\n", ")", "\n", "if", "qattn_loss", "!=", "0.0", ":", "\n", "                    ", "self", ".", "qattloss_metric", "(", "qattn_loss", ".", "item", "(", ")", ")", "\n", "", "total_aux_loss", "+=", "qattn_loss", "\n", "\n", "", "if", "self", ".", "mmlloss", ":", "\n", "# This is computed above during beam search", "\n", "                ", "if", "mml_loss", "!=", "0.0", ":", "\n", "                    ", "self", ".", "mmlloss_metric", "(", "mml_loss", ".", "item", "(", ")", ")", "\n", "", "total_aux_loss", "+=", "mml_loss", "\n", "\n", "", "if", "torch", ".", "isnan", "(", "total_aux_loss", ")", ":", "\n", "                ", "logger", ".", "info", "(", "f\"TotalAuxLoss is nan.\"", ")", "\n", "total_aux_loss", "=", "0.0", "\n", "\n", "", "denotation_loss", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "tensor", "(", "0.0", ")", ",", "device_id", ")", "\n", "batch_denotation_loss", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "tensor", "(", "0.0", ")", ",", "device_id", ")", "\n", "if", "self", ".", "denotation_loss", ":", "\n", "                ", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "# Programs for an instance can be of multiple types;", "\n", "# For each program, based on it's return type, we compute the log-likelihood", "\n", "# against the appropriate gold-answer and add it to the instance_log_likelihood_list", "\n", "# This is then weighed by the program-log-likelihood and added to the batch_loss", "\n", "\n", "                    ", "instance_prog_denotations", ",", "instance_prog_types", "=", "(", "batch_denotations", "[", "i", "]", ",", "batch_denotation_types", "[", "i", "]", ")", "\n", "instance_progs_logprob_list", "=", "batch_actionseq_scores", "[", "i", "]", "\n", "\n", "# This instance does not have completed programs that were found in beam-search", "\n", "if", "len", "(", "instance_prog_denotations", ")", "==", "0", ":", "\n", "                        ", "continue", "\n", "\n", "", "instance_log_likelihood_list", "=", "[", "]", "\n", "new_instance_progs_logprob_list", "=", "[", "]", "\n", "for", "progidx", "in", "range", "(", "len", "(", "instance_prog_denotations", ")", ")", ":", "\n", "                        ", "denotation", "=", "instance_prog_denotations", "[", "progidx", "]", "\n", "progtype", "=", "instance_prog_types", "[", "progidx", "]", "\n", "prog_logprob", "=", "instance_progs_logprob_list", "[", "progidx", "]", "\n", "\n", "if", "progtype", "==", "\"PassageSpanAnswer\"", ":", "\n", "# Tuple of start, end log_probs", "\n", "                            ", "denotation", ":", "PassageSpanAnswer", "=", "denotation", "\n", "log_likelihood", "=", "self", ".", "_get_span_answer_log_prob", "(", "\n", "answer_as_spans", "=", "answer_as_passage_spans", "[", "i", "]", ",", "span_log_probs", "=", "denotation", ".", "_value", "\n", ")", "\n", "\n", "if", "torch", ".", "isnan", "(", "log_likelihood", ")", "==", "1", ":", "\n", "                                ", "print", "(", "f\"Batch index: {i}\"", ")", "\n", "print", "(", "f\"AnsAsPassageSpans:{answer_as_passage_spans[i]}\"", ")", "\n", "print", "(", "\"\\nPassageSpan Start and End logits\"", ")", "\n", "print", "(", "denotation", ".", "start_logits", ")", "\n", "print", "(", "denotation", ".", "end_logits", ")", "\n", "\n", "", "", "elif", "progtype", "==", "\"QuestionSpanAnswer\"", ":", "\n", "# Tuple of start, end log_probs", "\n", "                            ", "denotation", ":", "QuestionSpanAnswer", "=", "denotation", "\n", "log_likelihood", "=", "self", ".", "_get_span_answer_log_prob", "(", "\n", "answer_as_spans", "=", "answer_as_question_spans", "[", "i", "]", ",", "span_log_probs", "=", "denotation", ".", "_value", "\n", ")", "\n", "if", "torch", ".", "isnan", "(", "log_likelihood", ")", "==", "1", ":", "\n", "                                ", "print", "(", "f\"Batch index: {i}\"", ")", "\n", "print", "(", "f\"AnsAsQuestionSpans:{answer_as_question_spans[i]}\"", ")", "\n", "print", "(", "\"\\nQuestionSpan Start and End logits\"", ")", "\n", "print", "(", "denotation", ".", "start_logits", ")", "\n", "print", "(", "denotation", ".", "end_logits", ")", "\n", "\n", "", "", "elif", "progtype", "==", "\"YearDifference\"", ":", "\n", "# Distribution over year_differences", "\n", "                            ", "denotation", ":", "YearDifference", "=", "denotation", "\n", "pred_year_difference_dist", "=", "denotation", ".", "_value", "\n", "pred_year_diff_log_probs", "=", "torch", ".", "log", "(", "pred_year_difference_dist", "+", "1e-40", ")", "\n", "gold_year_difference_dist", "=", "allenutil", ".", "move_to_device", "(", "\n", "torch", ".", "FloatTensor", "(", "answer_as_year_difference", "[", "i", "]", ")", ",", "cuda_device", "=", "device_id", "\n", ")", "\n", "log_likelihood", "=", "torch", ".", "sum", "(", "pred_year_diff_log_probs", "*", "gold_year_difference_dist", ")", "\n", "\n", "", "elif", "progtype", "==", "\"PassageNumber\"", ":", "\n", "# Distribution over PassageNumbers", "\n", "                            ", "denotation", ":", "PassageNumber", "=", "denotation", "\n", "pred_passagenumber_dist", "=", "denotation", ".", "_value", "\n", "pred_passagenumber_logprobs", "=", "torch", ".", "log", "(", "pred_passagenumber_dist", "+", "1e-40", ")", "\n", "gold_passagenum_dist", "=", "allenutil", ".", "move_to_device", "(", "\n", "torch", ".", "FloatTensor", "(", "answer_as_passage_number", "[", "i", "]", ")", ",", "cuda_device", "=", "device_id", "\n", ")", "\n", "log_likelihood", "=", "torch", ".", "sum", "(", "pred_passagenumber_logprobs", "*", "gold_passagenum_dist", ")", "\n", "\n", "", "elif", "progtype", "==", "\"CountNumber\"", ":", "\n", "                            ", "denotation", ":", "CountNumber", "=", "denotation", "\n", "count_distribution", "=", "denotation", ".", "_value", "\n", "count_log_probs", "=", "torch", ".", "log", "(", "count_distribution", "+", "1e-40", ")", "\n", "gold_count_distribution", "=", "allenutil", ".", "move_to_device", "(", "\n", "torch", ".", "FloatTensor", "(", "answer_as_count", "[", "i", "]", ")", ",", "cuda_device", "=", "device_id", "\n", ")", "\n", "log_likelihood", "=", "torch", ".", "sum", "(", "count_log_probs", "*", "gold_count_distribution", ")", "\n", "\n", "# Here implement losses for other program-return-types in else-ifs", "\n", "", "else", ":", "\n", "                            ", "raise", "NotImplementedError", "\n", "", "\"\"\"\n                        elif progtype == \"QuestionSpanAnswer\":\n                            denotation: QuestionSpanAnswer = denotation\n                            log_likelihood = self._get_span_answer_log_prob(\n                                answer_as_spans=answer_as_question_spans[i],answer_texts\n                                span_log_probs=denotation._value)\n                            if torch.isnan(log_likelihood) == 1:\n                                print(\"\\nQuestionSpan\")\n                                print(denotation.start_logits)\n                                print(denotation.end_logits)\n                                print(denotation._value)\n                        \"\"\"", "\n", "if", "torch", ".", "isnan", "(", "log_likelihood", ")", ":", "\n", "                            ", "logger", ".", "info", "(", "f\"Nan-loss encountered for denotation_log_likelihood\"", ")", "\n", "log_likelihood", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "tensor", "(", "0.0", ")", ",", "device_id", ")", "\n", "", "if", "torch", ".", "isnan", "(", "prog_logprob", ")", ":", "\n", "                            ", "logger", ".", "info", "(", "f\"Nan-loss encountered for ProgType: {progtype}\"", ")", "\n", "prog_logprob", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "tensor", "(", "0.0", ")", ",", "device_id", ")", "\n", "\n", "", "new_instance_progs_logprob_list", ".", "append", "(", "prog_logprob", ")", "\n", "instance_log_likelihood_list", ".", "append", "(", "log_likelihood", ")", "\n", "\n", "# Each is the shape of (number_of_progs,)", "\n", "", "instance_denotation_log_likelihoods", "=", "torch", ".", "stack", "(", "instance_log_likelihood_list", ",", "dim", "=", "-", "1", ")", "\n", "# instance_progs_log_probs = torch.stack(instance_progs_logprob_list, dim=-1)", "\n", "instance_progs_log_probs", "=", "torch", ".", "stack", "(", "new_instance_progs_logprob_list", ",", "dim", "=", "-", "1", ")", "\n", "\n", "allprogs_log_marginal_likelihoods", "=", "instance_denotation_log_likelihoods", "+", "instance_progs_log_probs", "\n", "instance_marginal_log_likelihood", "=", "allenutil", ".", "logsumexp", "(", "allprogs_log_marginal_likelihoods", ")", "\n", "# Added sum to remove empty-dim", "\n", "instance_marginal_log_likelihood", "=", "torch", ".", "sum", "(", "instance_marginal_log_likelihood", ")", "\n", "if", "torch", ".", "isnan", "(", "instance_marginal_log_likelihood", ")", ":", "\n", "                        ", "logger", ".", "info", "(", "f\"Nan-loss encountered for instance_marginal_log_likelihood\"", ")", "\n", "logger", ".", "info", "(", "f\"Prog log probs: {instance_progs_log_probs}\"", ")", "\n", "logger", ".", "info", "(", "f\"Instance denotation likelihoods: {instance_denotation_log_likelihoods}\"", ")", "\n", "\n", "instance_marginal_log_likelihood", "=", "0.0", "\n", "", "denotation_loss", "+=", "-", "1.0", "*", "instance_marginal_log_likelihood", "\n", "\n", "if", "torch", ".", "isnan", "(", "denotation_loss", ")", ":", "\n", "                        ", "denotation_loss", "=", "0.0", "\n", "\n", "", "", "batch_denotation_loss", "=", "denotation_loss", "/", "batch_size", "\n", "\n", "", "self", ".", "modelloss_metric", "(", "batch_denotation_loss", ".", "item", "(", ")", ")", "\n", "output_dict", "[", "\"loss\"", "]", "=", "batch_denotation_loss", "+", "total_aux_loss", "\n", "\n", "# Get the predicted answers irrespective of loss computation.", "\n", "# For each program, given it's return type compute the predicted answer string", "\n", "", "if", "metadata", "is", "not", "None", ":", "\n", "            ", "output_dict", "[", "\"predicted_answer\"", "]", "=", "[", "]", "\n", "output_dict", "[", "\"all_predicted_answers\"", "]", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "                ", "original_question", "=", "metadata", "[", "i", "]", "[", "\"original_question\"", "]", "\n", "original_passage", "=", "metadata", "[", "i", "]", "[", "\"original_passage\"", "]", "\n", "question_token_offsets", "=", "metadata", "[", "i", "]", "[", "\"question_token_offsets\"", "]", "\n", "passage_token_offsets", "=", "metadata", "[", "i", "]", "[", "\"passage_token_offsets\"", "]", "\n", "instance_year_differences", "=", "year_differences", "[", "i", "]", "\n", "instance_passage_numbers", "=", "passage_number_values", "[", "i", "]", "\n", "instance_passagenum_diffs", "=", "passagenumber_difference_values", "[", "i", "]", "\n", "instance_count_values", "=", "count_values", "[", "i", "]", "\n", "answer_annotations", "=", "metadata", "[", "i", "]", "[", "\"answer_annotations\"", "]", "\n", "\n", "instance_prog_denotations", ",", "instance_prog_types", "=", "(", "batch_denotations", "[", "i", "]", ",", "batch_denotation_types", "[", "i", "]", ")", "\n", "instance_progs_logprob_list", "=", "batch_actionseq_scores", "[", "i", "]", "\n", "\n", "all_instance_progs_predicted_answer_strs", ":", "List", "[", "str", "]", "=", "[", "]", "# List of answers from diff instance progs", "\n", "for", "progidx", "in", "range", "(", "len", "(", "instance_prog_denotations", ")", ")", ":", "\n", "                    ", "denotation", "=", "instance_prog_denotations", "[", "progidx", "]", "\n", "progtype", "=", "instance_prog_types", "[", "progidx", "]", "\n", "if", "progtype", "==", "\"PassageSpanAnswer\"", ":", "\n", "# Tuple of start, end log_probs", "\n", "                        ", "denotation", ":", "PassageSpanAnswer", "=", "denotation", "\n", "# Shape: (2, ) -- start / end token ids", "\n", "best_span", "=", "get_best_span", "(", "\n", "span_start_logits", "=", "denotation", ".", "_value", "[", "0", "]", ".", "unsqueeze", "(", "0", ")", ",", "\n", "span_end_logits", "=", "denotation", ".", "_value", "[", "1", "]", ".", "unsqueeze", "(", "0", ")", ",", "\n", ")", ".", "squeeze", "(", "0", ")", "\n", "\n", "predicted_span", "=", "tuple", "(", "best_span", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "start_offset", "=", "passage_token_offsets", "[", "predicted_span", "[", "0", "]", "]", "[", "0", "]", "\n", "end_offset", "=", "passage_token_offsets", "[", "predicted_span", "[", "1", "]", "]", "[", "1", "]", "\n", "predicted_answer", "=", "original_passage", "[", "start_offset", ":", "end_offset", "]", "\n", "\n", "", "elif", "progtype", "==", "\"QuestionSpanAnswer\"", ":", "\n", "# Tuple of start, end log_probs", "\n", "                        ", "denotation", ":", "QuestionSpanAnswer", "=", "denotation", "\n", "# Shape: (2, ) -- start / end token ids", "\n", "best_span", "=", "get_best_span", "(", "\n", "span_start_logits", "=", "denotation", ".", "_value", "[", "0", "]", ".", "unsqueeze", "(", "0", ")", ",", "\n", "span_end_logits", "=", "denotation", ".", "_value", "[", "1", "]", ".", "unsqueeze", "(", "0", ")", ",", "\n", ")", ".", "squeeze", "(", "0", ")", "\n", "\n", "predicted_span", "=", "tuple", "(", "best_span", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "start_offset", "=", "question_token_offsets", "[", "predicted_span", "[", "0", "]", "]", "[", "0", "]", "\n", "end_offset", "=", "question_token_offsets", "[", "predicted_span", "[", "1", "]", "]", "[", "1", "]", "\n", "predicted_answer", "=", "original_question", "[", "start_offset", ":", "end_offset", "]", "\n", "\n", "", "elif", "progtype", "==", "\"YearDifference\"", ":", "\n", "# Distribution over year_differences vector", "\n", "                        ", "denotation", ":", "YearDifference", "=", "denotation", "\n", "year_differences_dist", "=", "denotation", ".", "_value", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "predicted_yeardiff_idx", "=", "np", ".", "argmax", "(", "year_differences_dist", ")", "\n", "# If not predicting year_diff = 0", "\n", "# if predicted_yeardiff_idx == 0 and len(instance_year_differences) > 1:", "\n", "#     predicted_yeardiff_idx = np.argmax(year_differences_dist[1:])", "\n", "#     predicted_yeardiff_idx += 1", "\n", "predicted_year_difference", "=", "instance_year_differences", "[", "predicted_yeardiff_idx", "]", "# int", "\n", "predicted_answer", "=", "str", "(", "predicted_year_difference", ")", "\n", "\n", "", "elif", "progtype", "==", "\"PassageNumber\"", ":", "\n", "                        ", "denotation", ":", "PassageNumber", "=", "denotation", "\n", "predicted_passagenum_idx", "=", "torch", ".", "argmax", "(", "denotation", ".", "_value", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "predicted_passage_number", "=", "instance_passage_numbers", "[", "predicted_passagenum_idx", "]", "# int/float", "\n", "predicted_passage_number", "=", "(", "\n", "int", "(", "predicted_passage_number", ")", "\n", "if", "int", "(", "predicted_passage_number", ")", "==", "predicted_passage_number", "\n", "else", "predicted_passage_number", "\n", ")", "\n", "predicted_answer", "=", "str", "(", "predicted_passage_number", ")", "\n", "\n", "", "elif", "progtype", "==", "\"CountNumber\"", ":", "\n", "                        ", "denotation", ":", "CountNumber", "=", "denotation", "\n", "count_idx", "=", "torch", ".", "argmax", "(", "denotation", ".", "_value", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "predicted_count_answer", "=", "instance_count_values", "[", "count_idx", "]", "\n", "predicted_count_answer", "=", "(", "\n", "int", "(", "predicted_count_answer", ")", "\n", "if", "int", "(", "predicted_count_answer", ")", "==", "predicted_count_answer", "\n", "else", "predicted_count_answer", "\n", ")", "\n", "predicted_answer", "=", "str", "(", "predicted_count_answer", ")", "\n", "\n", "", "else", ":", "\n", "                        ", "raise", "NotImplementedError", "\n", "\n", "", "all_instance_progs_predicted_answer_strs", ".", "append", "(", "predicted_answer", ")", "\n", "# If no program was found in beam-search", "\n", "", "if", "len", "(", "all_instance_progs_predicted_answer_strs", ")", "==", "0", ":", "\n", "                    ", "all_instance_progs_predicted_answer_strs", ".", "append", "(", "\"\"", ")", "\n", "# Since the programs are sorted by decreasing scores, we can directly take the first pred answer", "\n", "", "instance_predicted_answer", "=", "all_instance_progs_predicted_answer_strs", "[", "0", "]", "\n", "output_dict", "[", "\"predicted_answer\"", "]", ".", "append", "(", "instance_predicted_answer", ")", "\n", "output_dict", "[", "\"all_predicted_answers\"", "]", ".", "append", "(", "all_instance_progs_predicted_answer_strs", ")", "\n", "\n", "answer_annotations", "=", "metadata", "[", "i", "]", ".", "get", "(", "\"answer_annotations\"", ",", "[", "]", ")", "\n", "self", ".", "_drop_metrics", "(", "instance_predicted_answer", ",", "answer_annotations", ")", "\n", "\n", "", "if", "not", "self", ".", "training", ":", "\n", "                ", "output_dict", "[", "\"metadata\"", "]", "=", "metadata", "\n", "# output_dict['best_span_ans_str'] = predicted_answers", "\n", "output_dict", "[", "\"answer_as_passage_spans\"", "]", "=", "answer_as_passage_spans", "\n", "# output_dict['predicted_spans'] = batch_best_spans", "\n", "\n", "output_dict", "[", "\"batch_action_seqs\"", "]", "=", "batch_actionseqs", "\n", "output_dict", "[", "\"batch_actionseq_scores\"", "]", "=", "batch_actionseq_scores", "\n", "output_dict", "[", "\"batch_actionseq_sideargs\"", "]", "=", "batch_actionseq_sideargs", "\n", "output_dict", "[", "\"languages\"", "]", "=", "languages", "\n", "\n", "", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser.compute_token_date_alignments": [[988, 1028], ["passage_to_date_attention_params", "allennlp.masked_softmax", "passage_mask.unsqueeze", "passage_mask.unsqueeze", "passage_tokenidx2dateidx_mask.unsqueeze", "passage_tokenidx2dateidx_mask.unsqueeze"], "methods", ["None"], ["", "def", "compute_token_date_alignments", "(", "\n", "self", ",", "modeled_passage", ",", "passage_mask", ",", "passageidx2dateidx", ",", "passage_to_date_attention_params", "\n", ")", ":", "\n", "        ", "\"\"\"Compute the passage_token-to-passage_date alignment matrix.\n\n        Args:\n        -----\n            modeled_passage: (batch_size, passage_length, hidden_dim)\n                Contextual passage repr.\n            passage_mask: (batch_size, passage_length)\n                Passage mask\n            passageidx2dateidx: (batch_size, passage_length)\n                For date-tokens, the index of the date-entity it belongs to, o/w masked with value = -1\n            passage_to_date_attention_params: Some matrix-attention parameterization for computing the alignment matrix\n\n        Returns:\n        --------\n            pasage_passage_token2date_aligment: (batch_size, passage_length, passage_length)\n                Alignment matrix from passage_token (dim=1) to passage_date (dim=2)\n                Should be masked in dim=2 for tokens that are not date-tokens\n        \"\"\"", "\n", "# ### Passage Token - Date Alignment", "\n", "# Shape: (batch_size, passage_length, passage_length)", "\n", "passage_passage_token2date_similarity", "=", "passage_to_date_attention_params", "(", "modeled_passage", ",", "modeled_passage", ")", "\n", "passage_passage_token2date_similarity", "=", "passage_passage_token2date_similarity", "*", "passage_mask", ".", "unsqueeze", "(", "1", ")", "\n", "passage_passage_token2date_similarity", "=", "passage_passage_token2date_similarity", "*", "passage_mask", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Shape: (batch_size, passage_length) -- masking for number tokens in the passage", "\n", "passage_tokenidx2dateidx_mask", "=", "(", "passageidx2dateidx", ">", "-", "1", ")", ".", "float", "(", ")", "\n", "# Shape: (batch_size, passage_length, passage_length)", "\n", "passage_passage_token2date_similarity", "=", "(", "\n", "passage_passage_token2date_similarity", "*", "passage_tokenidx2dateidx_mask", ".", "unsqueeze", "(", "1", ")", "\n", ")", "\n", "# Shape: (batch_size, passage_length, passage_length)", "\n", "pasage_passage_token2date_aligment", "=", "allenutil", ".", "masked_softmax", "(", "\n", "passage_passage_token2date_similarity", ",", "\n", "mask", "=", "passage_tokenidx2dateidx_mask", ".", "unsqueeze", "(", "1", ")", ",", "\n", "memory_efficient", "=", "True", ",", "\n", ")", "\n", "return", "pasage_passage_token2date_aligment", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser.compute_avg_norm": [[1029, 1036], ["tensor.size", "tensor.size", "tensor.norm().sum", "tensor.norm"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "def", "compute_avg_norm", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "dim0_size", "=", "tensor", ".", "size", "(", ")", "[", "0", "]", "\n", "dim1_size", "=", "tensor", ".", "size", "(", ")", "[", "1", "]", "\n", "\n", "tensor_norm", "=", "tensor", ".", "norm", "(", "p", "=", "2", ",", "dim", "=", "2", ")", ".", "sum", "(", ")", "/", "(", "dim0_size", "*", "dim1_size", ")", "\n", "\n", "return", "tensor_norm", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser.get_metrics": [[1037, 1058], ["drop_parser.DROPParser.modelloss_metric.get_metric", "drop_parser.DROPParser.excloss_metric.get_metric", "drop_parser.DROPParser.qattloss_metric.get_metric", "drop_parser.DROPParser.mmlloss_metric.get_metric", "drop_parser.DROPParser.auxwinloss_metric.get_metric", "drop_parser.DROPParser._drop_metrics.get_metric", "metric_dict.update"], "methods", ["None"], ["", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "metric_dict", "=", "{", "}", "\n", "model_loss", "=", "self", ".", "modelloss_metric", ".", "get_metric", "(", "reset", ")", "\n", "exec_loss", "=", "self", ".", "excloss_metric", ".", "get_metric", "(", "reset", ")", "\n", "qatt_loss", "=", "self", ".", "qattloss_metric", ".", "get_metric", "(", "reset", ")", "\n", "mml_loss", "=", "self", ".", "mmlloss_metric", ".", "get_metric", "(", "reset", ")", "\n", "winloss", "=", "self", ".", "auxwinloss_metric", ".", "get_metric", "(", "reset", ")", "\n", "exact_match", ",", "f1_score", "=", "self", ".", "_drop_metrics", ".", "get_metric", "(", "reset", ")", "\n", "metric_dict", ".", "update", "(", "\n", "{", "\n", "\"em\"", ":", "exact_match", ",", "\n", "\"f1\"", ":", "f1_score", ",", "\n", "\"ans\"", ":", "model_loss", ",", "\n", "\"exc\"", ":", "exec_loss", ",", "\n", "\"qatt\"", ":", "qatt_loss", ",", "\n", "\"mml\"", ":", "mml_loss", ",", "\n", "\"win\"", ":", "winloss", ",", "\n", "}", "\n", ")", "\n", "\n", "return", "metric_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser._get_span_answer_log_prob": [[1059, 1113], ["answer_as_spans.unsqueeze.unsqueeze.unsqueeze", "span_start_log_probs.unsqueeze.unsqueeze.unsqueeze", "span_end_log_probs.unsqueeze.unsqueeze.unsqueeze", "allennlp.replace_masked_values", "allennlp.replace_masked_values", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "allennlp.replace_masked_values", "allennlp.logsumexp", "log_marginal_likelihood_for_span.squeeze.squeeze.squeeze"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_span_answer_log_prob", "(", "\n", "answer_as_spans", ":", "torch", ".", "LongTensor", ",", "span_log_probs", ":", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\" Compute the log_marginal_likelihood for the answer_spans given log_probs for start/end\n            Compute log_likelihood (product of start/end probs) of each ans_span\n            Sum the prob (logsumexp) for each span and return the log_likelihood\n\n        Parameters:\n        -----------\n        answer: ``torch.LongTensor`` Shape: (number_of_spans, 2)\n            These are the gold spans\n        span_log_probs: ``torch.FloatTensor``\n            2-Tuple with tensors of Shape: (length_of_sequence) for span_start/span_end log_probs\n\n        Returns:\n        log_marginal_likelihood_for_passage_span\n        \"\"\"", "\n", "\n", "# Unsqueezing dim=0 to make a batch_size of 1", "\n", "answer_as_spans", "=", "answer_as_spans", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "span_start_log_probs", ",", "span_end_log_probs", "=", "span_log_probs", "\n", "span_start_log_probs", "=", "span_start_log_probs", ".", "unsqueeze", "(", "0", ")", "\n", "span_end_log_probs", "=", "span_end_log_probs", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "# (batch_size, number_of_ans_spans)", "\n", "gold_passage_span_starts", "=", "answer_as_spans", "[", ":", ",", ":", ",", "0", "]", "\n", "gold_passage_span_ends", "=", "answer_as_spans", "[", ":", ",", ":", ",", "1", "]", "\n", "# Some spans are padded with index -1,", "\n", "# so we clamp those paddings to 0 and then mask after `torch.gather()`.", "\n", "gold_passage_span_mask", "=", "(", "gold_passage_span_starts", "!=", "-", "1", ")", ".", "long", "(", ")", "\n", "clamped_gold_passage_span_starts", "=", "allenutil", ".", "replace_masked_values", "(", "\n", "gold_passage_span_starts", ",", "gold_passage_span_mask", ",", "0", "\n", ")", "\n", "clamped_gold_passage_span_ends", "=", "allenutil", ".", "replace_masked_values", "(", "\n", "gold_passage_span_ends", ",", "gold_passage_span_mask", ",", "0", "\n", ")", "\n", "# Shape: (batch_size, # of answer spans)", "\n", "log_likelihood_for_span_starts", "=", "torch", ".", "gather", "(", "span_start_log_probs", ",", "1", ",", "clamped_gold_passage_span_starts", ")", "\n", "log_likelihood_for_span_ends", "=", "torch", ".", "gather", "(", "span_end_log_probs", ",", "1", ",", "clamped_gold_passage_span_ends", ")", "\n", "# Shape: (batch_size, # of answer spans)", "\n", "log_likelihood_for_spans", "=", "log_likelihood_for_span_starts", "+", "log_likelihood_for_span_ends", "\n", "# For those padded spans, we set their log probabilities to be very small negative value", "\n", "log_likelihood_for_spans", "=", "allenutil", ".", "replace_masked_values", "(", "\n", "log_likelihood_for_spans", ",", "gold_passage_span_mask", ",", "-", "1e7", "\n", ")", "\n", "# Shape: (batch_size, )", "\n", "log_marginal_likelihood_for_span", "=", "allenutil", ".", "logsumexp", "(", "log_likelihood_for_spans", ")", "\n", "\n", "# Squeezing the batch-size 1", "\n", "log_marginal_likelihood_for_span", "=", "log_marginal_likelihood_for_span", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "return", "log_marginal_likelihood_for_span", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser.get_valid_start_actionids": [[1114, 1156], ["range", "len", "set", "valid_start_action_ids.append", "set.add", "logging.error"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add"], ["", "@", "staticmethod", "\n", "def", "get_valid_start_actionids", "(", "answer_types", ":", "List", "[", "List", "[", "str", "]", "]", ",", "action2actionidx", ":", "Dict", "[", "str", ",", "int", "]", ")", "->", "List", "[", "Set", "[", "int", "]", "]", ":", "\n", "        ", "\"\"\" For each instances, given answer_types as man-made strings, return the set of valid start action ids\n            that return an object of that type.\n            For example, given start_type as 'passage_span', '@start@ -> PassageSpanAnswer' is a valid start action\n            See the reader for all possible values of start_types\n\n            TODO(nitish): Instead of the reader passing in arbitrary strings and maintaining a mapping here,\n            TODO(nitish): we can pass in the names of the LanguageObjects directly and make the start-action-str\n                          in a programmatic manner.\n\n            This is used while *training* to run constrained-beam-search to only search through programs for which\n            the gold answer is known. These *** shouldn't *** beused during validation\n        Returns:\n        --------\n        start_types: `List[Set[int]]`\n            For each instance, a set of possible start_types\n        \"\"\"", "\n", "\n", "# Map from string passed by reader to LanguageType class", "\n", "answer_type_to_action_mapping", "=", "{", "\n", "\"passage_span\"", ":", "\"@start@ -> PassageSpanAnswer\"", ",", "\n", "\"year_difference\"", ":", "\"@start@ -> YearDifference\"", ",", "\n", "\"passage_number\"", ":", "\"@start@ -> PassageNumber\"", ",", "\n", "\"question_span\"", ":", "\"@start@ -> QuestionSpanAnswer\"", ",", "\n", "\"count_number\"", ":", "\"@start@ -> CountNumber\"", ",", "\n", "}", "\n", "\n", "valid_start_action_ids", ":", "List", "[", "Set", "[", "int", "]", "]", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "answer_types", ")", ")", ":", "\n", "            ", "answer_program_types", ":", "List", "[", "str", "]", "=", "answer_types", "[", "i", "]", "\n", "start_action_ids", "=", "set", "(", ")", "\n", "for", "start_type", "in", "answer_program_types", ":", "\n", "                ", "if", "start_type", "in", "answer_type_to_action_mapping", ":", "\n", "                    ", "actionstr", "=", "answer_type_to_action_mapping", "[", "start_type", "]", "\n", "action_id", "=", "action2actionidx", "[", "actionstr", "]", "\n", "start_action_ids", ".", "add", "(", "action_id", ")", "\n", "", "else", ":", "\n", "                    ", "logging", ".", "error", "(", "f\"StartType: {start_type} has no valid action in {answer_type_to_action_mapping}\"", ")", "\n", "", "", "valid_start_action_ids", ".", "append", "(", "start_action_ids", ")", "\n", "\n", "", "return", "valid_start_action_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser.synthetic_num_grounding_loss": [[1157, 1189], ["allennlp.masked_softmax", "enumerate", "zip", "passageidx2numberidx_mask.unsqueeze", "torch.log", "torch.log", "torch.log", "torch.log"], "methods", ["None"], ["", "def", "synthetic_num_grounding_loss", "(", "\n", "self", ",", "qtypes", ",", "synthetic_numgrounding_metadata", ",", "passage_passage_num_similarity", ",", "passageidx2numberidx", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Parameters:\n        -----------\n        passage_passage_num_similarity: (B, P, P)\n        passage_tokenidx2dateidx: (B, P) containing non -1 values for number tokens. We'll use this for masking.\n        synthetic_numgrounding_metadata: For each instance, list of (token_idx, number_idx), i.e.\n            for row = token_idx, column = number_idx should be high\n        \"\"\"", "\n", "\n", "# (B, P)", "\n", "passageidx2numberidx_mask", "=", "(", "passageidx2numberidx", ">", "-", "1", ")", ".", "float", "(", ")", "\n", "\n", "# (B, P, P) -- with each row now normalized for number tokens", "\n", "passage_passage_num_attention", "=", "allenutil", ".", "masked_softmax", "(", "\n", "passage_passage_num_similarity", ",", "mask", "=", "passageidx2numberidx_mask", ".", "unsqueeze", "(", "1", ")", "\n", ")", "\n", "log_likelihood", "=", "0", "\n", "normalizer", "=", "0", "\n", "for", "idx", ",", "(", "qtype", ",", "token_number_idx_pairs", ")", "in", "enumerate", "(", "zip", "(", "qtypes", ",", "synthetic_numgrounding_metadata", ")", ")", ":", "\n", "            ", "if", "qtype", "==", "dropconstants", ".", "SYN_NUMGROUND_qtype", ":", "\n", "                ", "for", "token_idx", ",", "number_idx", "in", "token_number_idx_pairs", ":", "\n", "                    ", "log_likelihood", "+=", "torch", ".", "log", "(", "passage_passage_num_attention", "[", "idx", ",", "token_idx", ",", "number_idx", "]", "+", "1e-40", ")", "\n", "normalizer", "+=", "1", "\n", "", "", "", "if", "normalizer", ">", "0", ":", "\n", "            ", "log_likelihood", "=", "log_likelihood", "/", "normalizer", "\n", "\n", "", "loss", "=", "-", "1", "*", "log_likelihood", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser._ques_attention_loss": [[1190, 1328], ["range", "len", "len", "zip", "zip", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.log", "torch.log", "torch.log", "torch.log", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "print"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "def", "_ques_attention_loss", "(", "\n", "self", ",", "\n", "batch_actionseqs", ":", "List", "[", "List", "[", "List", "[", "str", "]", "]", "]", ",", "\n", "batch_actionseq_sideargs", ":", "List", "[", "List", "[", "List", "[", "Dict", "]", "]", "]", ",", "\n", "qtypes", ":", "List", "[", "str", "]", ",", "\n", "qattn_supervised", ":", "List", "[", "bool", "]", ",", "\n", "qattn_supervision", ":", "torch", ".", "FloatTensor", ",", "\n", ")", ":", "\n", "\n", "        ", "\"\"\" Compute QAttn supervision loss for different kind of questions. Different question-types have diff.\n            gold-programs and can have different number of qattn-supervision for each instance.\n            There, the shape of qattn_supervision is (B, R, QLen) where R is the maximum number of attn-supervisions\n            provided for an instance in this batch. For instances with less number of relevant actions\n            the corresponding instance_slice will be padded with all zeros-tensors.\n\n            We hard-code the question-types supported, and for each qtype, the relevant actions for which the\n            qattn supervision will (should) be provided. For example, the gold program for date-comparison questions\n            contains two 'PassageAttention -> find_PassageAttention' actions which use the question_attention sidearg\n            for which the supervision is\n             provided. Hence, qtype2relevant_actions_list - contains the two actions for the\n            date-comparison question.\n\n            The loss computed is the negative-log of sum of relevant probabilities.\n\n            NOTE: This loss is only computed for instances that are marked as strongly-annotated and hence we don't\n            check if the qattns-supervision needs masking.\n        \"\"\"", "\n", "find_passage_attention", "=", "\"PassageAttention -> find_PassageAttention\"", "\n", "filter_passage_attention", "=", "\"<PassageAttention:PassageAttention> -> filter_PassageAttention\"", "\n", "relocate_passage_attention", "=", "\"<PassageAttention:PassageAttention_answer> -> relocate_PassageAttention\"", "\n", "\n", "single_find_passage_attention_list", "=", "[", "find_passage_attention", "]", "\n", "double_find_passage_attentions_list", "=", "[", "find_passage_attention", ",", "find_passage_attention", "]", "\n", "filter_find_passage_attention_list", "=", "[", "filter_passage_attention", ",", "find_passage_attention", "]", "\n", "relocate_find_passage_attention_list", "=", "[", "relocate_passage_attention", ",", "find_passage_attention", "]", "\n", "relocate_filterfind_passage_attention_list", "=", "[", "\n", "relocate_passage_attention", ",", "\n", "filter_passage_attention", ",", "\n", "find_passage_attention", ",", "\n", "]", "\n", "\n", "qtypes_w_findPA", "=", "[", "\n", "dropconstants", ".", "NUM_find_qtype", ",", "\n", "dropconstants", ".", "MAX_find_qtype", ",", "\n", "dropconstants", ".", "MIN_find_qtype", ",", "\n", "dropconstants", ".", "COUNT_find_qtype", ",", "\n", "dropconstants", ".", "YARDS_findnum_qtype", ",", "\n", "dropconstants", ".", "YARDS_longest_qtype", ",", "\n", "dropconstants", ".", "YARDS_shortest_qtype", ",", "\n", "]", "\n", "\n", "qtypes_w_filterfindPA", "=", "[", "\n", "dropconstants", ".", "NUM_filter_find_qtype", ",", "\n", "dropconstants", ".", "MAX_filter_find_qtype", ",", "\n", "dropconstants", ".", "MIN_filter_find_qtype", ",", "\n", "dropconstants", ".", "COUNT_filter_find_qtype", ",", "\n", "]", "\n", "\n", "qtypes_w_two_findPA", "=", "[", "dropconstants", ".", "DATECOMP_QTYPE", ",", "dropconstants", ".", "NUMCOMP_QTYPE", "]", "\n", "\n", "qtypes_w_relocatefindPA", "=", "[", "\n", "dropconstants", ".", "RELOC_find_qtype", ",", "\n", "dropconstants", ".", "RELOC_maxfind_qtype", ",", "\n", "dropconstants", ".", "RELOC_minfind_qtype", ",", "\n", "]", "\n", "qtypes_w_relocate_filterfindPA", "=", "[", "\n", "dropconstants", ".", "RELOC_filterfind_qtype", ",", "\n", "dropconstants", ".", "RELOC_maxfilterfind_qtype", ",", "\n", "dropconstants", ".", "RELOC_minfilterfind_qtype", ",", "\n", "]", "\n", "\n", "qtype2relevant_actions_list", "=", "{", "}", "\n", "\n", "for", "qtype", "in", "qtypes_w_findPA", ":", "\n", "            ", "qtype2relevant_actions_list", "[", "qtype", "]", "=", "single_find_passage_attention_list", "\n", "", "for", "qtype", "in", "qtypes_w_two_findPA", ":", "\n", "            ", "qtype2relevant_actions_list", "[", "qtype", "]", "=", "double_find_passage_attentions_list", "\n", "", "for", "qtype", "in", "qtypes_w_filterfindPA", ":", "\n", "            ", "qtype2relevant_actions_list", "[", "qtype", "]", "=", "filter_find_passage_attention_list", "\n", "", "for", "qtype", "in", "qtypes_w_relocatefindPA", ":", "\n", "            ", "qtype2relevant_actions_list", "[", "qtype", "]", "=", "relocate_find_passage_attention_list", "\n", "", "for", "qtype", "in", "qtypes_w_relocate_filterfindPA", ":", "\n", "            ", "qtype2relevant_actions_list", "[", "qtype", "]", "=", "relocate_filterfind_passage_attention_list", "\n", "\n", "", "loss", "=", "0.0", "\n", "normalizer", "=", "0", "\n", "\n", "for", "ins_idx", "in", "range", "(", "len", "(", "batch_actionseqs", ")", ")", ":", "\n", "            ", "qattn_supervised_instance", "=", "qattn_supervised", "[", "ins_idx", "]", "\n", "if", "not", "qattn_supervised_instance", ":", "\n", "# no point even bothering", "\n", "                ", "continue", "\n", "", "qtype", "=", "qtypes", "[", "ins_idx", "]", "\n", "if", "qtype", "not", "in", "qtype2relevant_actions_list", ":", "\n", "                ", "continue", "\n", "\n", "", "instance_programs", "=", "batch_actionseqs", "[", "ins_idx", "]", "\n", "instance_prog_sideargs", "=", "batch_actionseq_sideargs", "[", "ins_idx", "]", "\n", "# Shape: (R, question_length)", "\n", "instance_qattn_supervision", "=", "qattn_supervision", "[", "ins_idx", "]", "\n", "# These are the actions for which qattn_supervision should be provided.", "\n", "relevant_actions", "=", "qtype2relevant_actions_list", "[", "qtype", "]", "\n", "num_relevant_actions", "=", "len", "(", "relevant_actions", ")", "\n", "for", "program", ",", "side_args", "in", "zip", "(", "instance_programs", ",", "instance_prog_sideargs", ")", ":", "\n", "# Counter to keep a track of which relevant action we're looking for next", "\n", "                ", "relevant_action_idx", "=", "0", "\n", "relevant_action", "=", "relevant_actions", "[", "relevant_action_idx", "]", "\n", "gold_qattn", "=", "instance_qattn_supervision", "[", "relevant_action_idx", "]", "\n", "for", "action", ",", "side_arg", "in", "zip", "(", "program", ",", "side_args", ")", ":", "\n", "                    ", "if", "action", "==", "relevant_action", ":", "\n", "                        ", "question_attention", "=", "side_arg", "[", "\"question_attention\"", "]", "\n", "if", "torch", ".", "sum", "(", "gold_qattn", ")", "!=", "0.0", ":", "\n", "# Sum of probs -- model can distribute gold mass however it likes", "\n", "# l = torch.sum(question_attention * gold_qattn)", "\n", "# loss += torch.log(l)", "\n", "# Prod of probs -- forces model to evenly distribute mass on gold-attn", "\n", "                            ", "log_question_attention", "=", "torch", ".", "log", "(", "question_attention", "+", "1e-40", ")", "\n", "l", "=", "torch", ".", "sum", "(", "log_question_attention", "*", "gold_qattn", ")", "\n", "loss", "+=", "l", "\n", "normalizer", "+=", "1", "\n", "", "else", ":", "\n", "                            ", "print", "(", "\n", "f\"\\nGold attention sum == 0.0.\"", "\n", "f\"\\nQattnSupervised: {qattn_supervised_instance}\"", "\n", "f\"\\nQtype: {qtype}\"", "\n", ")", "\n", "", "relevant_action_idx", "+=", "1", "\n", "\n", "# All relevant actions for this instance in this program are found", "\n", "if", "relevant_action_idx", ">=", "num_relevant_actions", ":", "\n", "                            ", "break", "\n", "", "else", ":", "\n", "                            ", "relevant_action", "=", "relevant_actions", "[", "relevant_action_idx", "]", "\n", "gold_qattn", "=", "instance_qattn_supervision", "[", "relevant_action_idx", "]", "\n", "", "", "", "", "", "if", "normalizer", "==", "0", ":", "\n", "            ", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "-", "1", "*", "(", "loss", "/", "normalizer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser._get_best_spans": [[1329, 1405], ["range", "len", "zip", "batch_best_spans.append", "batch_predicted_answers.append", "allennlp.models.reading_comprehension.util.get_best_span().squeeze", "instance_best_spans.append", "tuple", "instance_predicted_ans.append", "allennlp.models.reading_comprehension.util.get_best_span().squeeze.detach().cpu().numpy", "allennlp.models.reading_comprehension.util.get_best_span", "allennlp.models.reading_comprehension.util.get_best_span().squeeze.detach().cpu", "print", "print", "print", "print", "print", "print", "print", "denotation._value[].unsqueeze", "denotation._value[].unsqueeze", "print", "print", "print", "print", "print", "allennlp.models.reading_comprehension.util.get_best_span().squeeze.detach", "question_mask_aslist[].size", "len", "len", "passage_mask_aslist[].size", "len", "len"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "_get_best_spans", "(", "\n", "batch_denotations", ",", "\n", "batch_denotation_types", ",", "\n", "question_char_offsets", ",", "\n", "question_strs", ",", "\n", "passage_char_offsets", ",", "\n", "passage_strs", ",", "\n", "*", "args", ",", "\n", ")", ":", "\n", "        ", "\"\"\" For all SpanType denotations, get the best span\n\n        Parameters:\n        ----------\n        batch_denotations: List[List[Any]]\n        batch_denotation_types: List[List[str]]\n        \"\"\"", "\n", "\n", "(", "question_mask_aslist", ",", "passage_mask_aslist", ")", "=", "args", "\n", "\n", "batch_best_spans", "=", "[", "]", "\n", "batch_predicted_answers", "=", "[", "]", "\n", "\n", "for", "instance_idx", "in", "range", "(", "len", "(", "batch_denotations", ")", ")", ":", "\n", "            ", "instance_prog_denotations", "=", "batch_denotations", "[", "instance_idx", "]", "\n", "instance_prog_types", "=", "batch_denotation_types", "[", "instance_idx", "]", "\n", "\n", "instance_best_spans", "=", "[", "]", "\n", "instance_predicted_ans", "=", "[", "]", "\n", "\n", "for", "denotation", ",", "progtype", "in", "zip", "(", "instance_prog_denotations", ",", "instance_prog_types", ")", ":", "\n", "# if progtype == \"QuestionSpanAnswwer\":", "\n", "# Distinction between QuestionSpanAnswer and PassageSpanAnswer is not needed currently,", "\n", "# since both classes store the start/end logits as a tuple", "\n", "# Shape: (2, )", "\n", "                ", "best_span", "=", "get_best_span", "(", "\n", "span_start_logits", "=", "denotation", ".", "_value", "[", "0", "]", ".", "unsqueeze", "(", "0", ")", ",", "\n", "span_end_logits", "=", "denotation", ".", "_value", "[", "1", "]", ".", "unsqueeze", "(", "0", ")", ",", "\n", ")", ".", "squeeze", "(", "0", ")", "\n", "instance_best_spans", ".", "append", "(", "best_span", ")", "\n", "\n", "predicted_span", "=", "tuple", "(", "best_span", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "if", "progtype", "==", "\"QuestionSpanAnswer\"", ":", "\n", "                    ", "try", ":", "\n", "                        ", "start_offset", "=", "question_char_offsets", "[", "instance_idx", "]", "[", "predicted_span", "[", "0", "]", "]", "[", "0", "]", "\n", "end_offset", "=", "question_char_offsets", "[", "instance_idx", "]", "[", "predicted_span", "[", "1", "]", "]", "[", "1", "]", "\n", "predicted_answer", "=", "question_strs", "[", "instance_idx", "]", "[", "start_offset", ":", "end_offset", "]", "\n", "", "except", ":", "\n", "                        ", "print", "(", ")", "\n", "print", "(", "f\"PredictedSpan: {predicted_span}\"", ")", "\n", "print", "(", "f\"QuesMaskLen: {question_mask_aslist[instance_idx].size()}\"", ")", "\n", "print", "(", "f\"StartLogProbs:{denotation._value[0]}\"", ")", "\n", "print", "(", "f\"EndLogProbs:{denotation._value[1]}\"", ")", "\n", "print", "(", "f\"LenofOffsets: {len(question_char_offsets[instance_idx])}\"", ")", "\n", "print", "(", "f\"QuesStrLen: {len(question_strs[instance_idx])}\"", ")", "\n", "\n", "", "", "elif", "progtype", "==", "\"PassageSpanAnswer\"", ":", "\n", "                    ", "try", ":", "\n", "                        ", "start_offset", "=", "passage_char_offsets", "[", "instance_idx", "]", "[", "predicted_span", "[", "0", "]", "]", "[", "0", "]", "\n", "end_offset", "=", "passage_char_offsets", "[", "instance_idx", "]", "[", "predicted_span", "[", "1", "]", "]", "[", "1", "]", "\n", "predicted_answer", "=", "passage_strs", "[", "instance_idx", "]", "[", "start_offset", ":", "end_offset", "]", "\n", "", "except", ":", "\n", "                        ", "print", "(", ")", "\n", "print", "(", "f\"PredictedSpan: {predicted_span}\"", ")", "\n", "print", "(", "f\"PassageMaskLen: {passage_mask_aslist[instance_idx].size()}\"", ")", "\n", "print", "(", "f\"LenofOffsets: {len(passage_char_offsets[instance_idx])}\"", ")", "\n", "print", "(", "f\"PassageStrLen: {len(passage_strs[instance_idx])}\"", ")", "\n", "", "", "else", ":", "\n", "                    ", "raise", "NotImplementedError", "\n", "\n", "", "instance_predicted_ans", ".", "append", "(", "predicted_answer", ")", "\n", "\n", "", "batch_best_spans", ".", "append", "(", "instance_best_spans", ")", "\n", "batch_predicted_answers", ".", "append", "(", "instance_predicted_ans", ")", "\n", "\n", "", "return", "batch_best_spans", ",", "batch_predicted_answers", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser.passage_attention_to_sidearg": [[1406, 1430], ["range", "len", "zip", "zip"], "methods", ["None"], ["", "def", "passage_attention_to_sidearg", "(", "\n", "self", ",", "\n", "qtypes", ":", "List", "[", "str", "]", ",", "\n", "batch_actionseqs", ":", "List", "[", "List", "[", "List", "[", "str", "]", "]", "]", ",", "\n", "batch_actionseq_sideargs", ":", "List", "[", "List", "[", "List", "[", "Dict", "]", "]", "]", ",", "\n", "pattn_supervised", ":", "List", "[", "bool", "]", ",", "\n", "passage_attn_supervision", ":", "torch", ".", "FloatTensor", ",", "\n", "max_passage_len", ":", "int", ",", "\n", "device_id", ",", "\n", ")", ":", "\n", "        ", "\"\"\" If instance has passage attention supervision, add it to 'PassageAttention -> find_PassageAttention' \"\"\"", "\n", "\n", "relevant_action", "=", "\"PassageAttention -> find_PassageAttention\"", "\n", "for", "ins_idx", "in", "range", "(", "len", "(", "batch_actionseqs", ")", ")", ":", "\n", "            ", "instance_programs", "=", "batch_actionseqs", "[", "ins_idx", "]", "\n", "instance_prog_sideargs", "=", "batch_actionseq_sideargs", "[", "ins_idx", "]", "\n", "instance_pattn_supervised", "=", "pattn_supervised", "[", "ins_idx", "]", "\n", "pattn_supervision", "=", "passage_attn_supervision", "[", "ins_idx", "]", "\n", "if", "not", "instance_pattn_supervised", ":", "\n", "                ", "pattn_supervision", "=", "None", "\n", "", "for", "program", ",", "side_args", "in", "zip", "(", "instance_programs", ",", "instance_prog_sideargs", ")", ":", "\n", "                ", "for", "action", ",", "sidearg_dict", "in", "zip", "(", "program", ",", "side_args", ")", ":", "\n", "                    ", "if", "action", "==", "relevant_action", ":", "\n", "                        ", "sidearg_dict", "[", "\"passage_attention\"", "]", "=", "pattn_supervision", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser.datecompare_eventdategr_to_sideargs": [[1431, 1461], ["drop_parser.DROPParser.get_gold_question_event_date_grounding", "range", "len", "zip", "zip"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.get_gold_question_event_date_grounding"], ["", "", "", "", "", "def", "datecompare_eventdategr_to_sideargs", "(", "\n", "self", ",", "\n", "qtypes", ":", "List", "[", "str", "]", ",", "\n", "batch_actionseqs", ":", "List", "[", "List", "[", "List", "[", "str", "]", "]", "]", ",", "\n", "batch_actionseq_sideargs", ":", "List", "[", "List", "[", "List", "[", "Dict", "]", "]", "]", ",", "\n", "datecomp_ques_event_date_groundings", ":", "List", "[", "Tuple", "[", "List", "[", "float", "]", ",", "List", "[", "float", "]", "]", "]", ",", "\n", "device_id", ",", "\n", ")", ":", "\n", "        ", "\"\"\" batch_event_date_groundings: For each question, a two-tuple containing the correct date-grounding for the\n            two events mentioned in the question.\n            These are in order of the annotation (order of events in question) but later the question attention\n            might be predicted in reverse order and these will then be the wrong (reverse) annotations. Take care later.\n        \"\"\"", "\n", "# List[Tuple[torch.Tensor, torch.Tensor]]", "\n", "q_event_date_groundings", "=", "self", ".", "get_gold_question_event_date_grounding", "(", "\n", "datecomp_ques_event_date_groundings", ",", "device_id", "\n", ")", "\n", "\n", "relevant_action1", "=", "\"<PassageAttention,PassageAttention:PassageAttention_answer> -> compare_date_greater_than\"", "\n", "relevant_action2", "=", "\"<PassageAttention,PassageAttention:PassageAttention_answer> -> compare_date_lesser_than\"", "\n", "relevant_actions", "=", "[", "relevant_action1", ",", "relevant_action2", "]", "\n", "\n", "for", "ins_idx", "in", "range", "(", "len", "(", "batch_actionseqs", ")", ")", ":", "\n", "            ", "instance_programs", "=", "batch_actionseqs", "[", "ins_idx", "]", "\n", "instance_prog_sideargs", "=", "batch_actionseq_sideargs", "[", "ins_idx", "]", "\n", "event_date_groundings", "=", "q_event_date_groundings", "[", "ins_idx", "]", "\n", "for", "program", ",", "side_args", "in", "zip", "(", "instance_programs", ",", "instance_prog_sideargs", ")", ":", "\n", "                ", "for", "action", ",", "sidearg_dict", "in", "zip", "(", "program", ",", "side_args", ")", ":", "\n", "                    ", "if", "action", "in", "relevant_actions", ":", "\n", "                        ", "sidearg_dict", "[", "\"event_date_groundings\"", "]", "=", "event_date_groundings", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser.numcompare_eventnumgr_to_sideargs": [[1462, 1512], ["drop_parser.DROPParser.get_gold_question_event_date_grounding", "range", "len", "zip", "zip"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.get_gold_question_event_date_grounding"], ["", "", "", "", "", "def", "numcompare_eventnumgr_to_sideargs", "(", "\n", "self", ",", "\n", "qtypes", ",", "\n", "execution_supervised", ",", "\n", "batch_actionseqs", ":", "List", "[", "List", "[", "List", "[", "str", "]", "]", "]", ",", "\n", "batch_actionseq_sideargs", ":", "List", "[", "List", "[", "List", "[", "Dict", "]", "]", "]", ",", "\n", "numcomp_qspan_num_groundings", ":", "List", "[", "Tuple", "[", "List", "[", "float", "]", ",", "List", "[", "float", "]", "]", "]", ",", "\n", "device_id", ",", "\n", ")", ":", "\n", "        ", "\"\"\" UPDATE: function name suggest only numpcomp, but works for other questions also\n            numcomp_qspan_num_groundings - is a List of 1- or 2- or maybe n- tuple of number-grounding\n        \"\"\"", "\n", "\"\"\" batch_event_num_groundings: For each question, a 1- or 2--tuple containing the correct num-grounding for the\n            two events mentioned in the question.\n            \n            Currently, for each qtype, we only have supervision for one of the actions, hence this function works\n            (The tuple contains both groundings for the same action)\n            If we need somthing like qattn, where multiple supervisions are provided, things will have to change\n        \"\"\"", "\n", "# Reusing the function written for dates -- should work fine", "\n", "# List[Tuple[torch.Tensor, torch.Tensor]]", "\n", "q_event_num_groundings", "=", "self", ".", "get_gold_question_event_date_grounding", "(", "numcomp_qspan_num_groundings", ",", "device_id", ")", "\n", "numcomp_action_gt", "=", "\"<PassageAttention,PassageAttention:PassageAttention_answer> -> compare_num_greater_than\"", "\n", "numcomp_action_lt", "=", "\"<PassageAttention,PassageAttention:PassageAttention_answer> -> compare_num_greater_than\"", "\n", "findnum_action", "=", "\"<PassageAttention:PassageNumber> -> find_PassageNumber\"", "\n", "maxNumPattn_action", "=", "\"<PassageAttention:PassageAttention> -> maxNumPattn\"", "\n", "minNumPattn_action", "=", "\"<PassageAttention:PassageAttention> -> minNumPattn\"", "\n", "\n", "qtype2relevant_actions_list", "=", "{", "\n", "dropconstants", ".", "NUMCOMP_QTYPE", ":", "[", "numcomp_action_gt", ",", "numcomp_action_lt", "]", ",", "\n", "dropconstants", ".", "NUM_find_qtype", ":", "[", "findnum_action", "]", ",", "\n", "dropconstants", ".", "NUM_filter_find_qtype", ":", "[", "findnum_action", "]", ",", "\n", "dropconstants", ".", "MAX_find_qtype", ":", "[", "maxNumPattn_action", "]", ",", "\n", "dropconstants", ".", "MAX_filter_find_qtype", ":", "[", "maxNumPattn_action", "]", ",", "\n", "dropconstants", ".", "MIN_find_qtype", ":", "[", "minNumPattn_action", "]", ",", "\n", "dropconstants", ".", "MIN_filter_find_qtype", ":", "[", "minNumPattn_action", "]", ",", "\n", "}", "\n", "\n", "for", "ins_idx", "in", "range", "(", "len", "(", "batch_actionseqs", ")", ")", ":", "\n", "            ", "instance_programs", "=", "batch_actionseqs", "[", "ins_idx", "]", "\n", "instance_prog_sideargs", "=", "batch_actionseq_sideargs", "[", "ins_idx", "]", "\n", "event_num_groundings", "=", "q_event_num_groundings", "[", "ins_idx", "]", "\n", "qtype", "=", "qtypes", "[", "ins_idx", "]", "# Could be UNK", "\n", "if", "qtype", "not", "in", "qtype2relevant_actions_list", ":", "\n", "                ", "continue", "\n", "", "relevant_actions", "=", "qtype2relevant_actions_list", "[", "qtype", "]", "\n", "for", "program", ",", "side_args", "in", "zip", "(", "instance_programs", ",", "instance_prog_sideargs", ")", ":", "\n", "                ", "for", "action", ",", "sidearg_dict", "in", "zip", "(", "program", ",", "side_args", ")", ":", "\n", "                    ", "if", "action", "in", "relevant_actions", ":", "\n", "                        ", "sidearg_dict", "[", "\"event_num_groundings\"", "]", "=", "event_num_groundings", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser.get_gold_question_event_date_grounding": [[1513, 1531], ["question_date_groundings.append", "allennlp.move_to_device", "groundings_tensors.append", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["None"], ["", "", "", "", "", "def", "get_gold_question_event_date_grounding", "(", "\n", "self", ",", "question_event_date_groundings", ":", "List", "[", "Tuple", "[", "List", "[", "int", "]", ",", "List", "[", "int", "]", "]", "]", ",", "device_id", ":", "int", "\n", ")", "->", "List", "[", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", "]", ":", "\n", "        ", "\"\"\" Converts input event date groundings (date-comparison) to FloatTensors \"\"\"", "\n", "question_date_groundings", "=", "[", "]", "\n", "# for grounding_1, grounding_2 in question_event_date_groundings:", "\n", "#     g1 = allenutil.move_to_device(torch.FloatTensor(grounding_1), device_id)", "\n", "#     g2 = allenutil.move_to_device(torch.FloatTensor(grounding_2), device_id)", "\n", "#     question_date_groundings.append((g1, g2))", "\n", "\n", "# Reader passes two groundings if not provided, hence all elements have tensors and no need to check for None", "\n", "for", "groundings", "in", "question_event_date_groundings", ":", "\n", "            ", "groundings_tensors", "=", "[", "]", "\n", "for", "grounding", "in", "groundings", ":", "\n", "                ", "g", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "FloatTensor", "(", "grounding", ")", ",", "device_id", ")", "\n", "groundings_tensors", ".", "append", "(", "g", ")", "\n", "", "question_date_groundings", ".", "append", "(", "groundings_tensors", ")", "\n", "", "return", "question_date_groundings", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser.passageAnsSpan_to_PassageAttention": [[1532, 1581], ["answer_as_passage_spans.long", "span_starts.unsqueeze", "span_ends.unsqueeze", "allennlp.get_range_vector", "range_vector.unsqueeze().unsqueeze.unsqueeze().unsqueeze.unsqueeze().unsqueeze", "span_range_mask.sum", "passage_mask.size", "allennlp.get_device_of", "answers_mask.unsqueeze", "span_range_mask.sum.sum", "range_vector.unsqueeze().unsqueeze.unsqueeze().unsqueeze.unsqueeze", "range", "normalized_attention.size"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "def", "passageAnsSpan_to_PassageAttention", "(", "self", ",", "answer_as_passage_spans", ",", "passage_mask", ")", ":", "\n", "        ", "\"\"\" Convert answers as passage span into passage attention for model introspection\n\n        Parameters:\n        ----------\n        answer_as_passage_spans: `torch.Tensor`\n            Tensor of shape (batch_size, number_of_ans_spans, 2) containing start / end positions\n        passage_mask: `torch.FloatTensor`\n            Tensor of shape (batch_size, passage_length)\n\n        Returns:\n        --------\n        attention: `torch.FloatTensor`\n            List of (passage_length, ) shaped tensor containing normalized attention for gold spans\n        \"\"\"", "\n", "# Shape: (batch_size, number_of_ans_spans, 2)", "\n", "answer_as_spans", "=", "answer_as_passage_spans", ".", "long", "(", ")", "\n", "\n", "# TODO(nitish): ONLY USING FIRST CORRECT SPAN OUT OF MULTIPLE POSSIBLE", "\n", "# answer_as_spans = answer_as_spans[:, 0, :].unsqueeze(1)", "\n", "\n", "# Shape: (batch_size, number_of_ans_spans)", "\n", "span_starts", "=", "answer_as_spans", "[", ":", ",", ":", ",", "0", "]", "\n", "span_ends", "=", "answer_as_spans", "[", ":", ",", ":", ",", "1", "]", "\n", "answers_mask", "=", "(", "span_starts", ">=", "0", ")", ".", "float", "(", ")", "\n", "\n", "# Shape: (batch_size, 1, number_of_ans_spans)", "\n", "span_starts_ex", "=", "span_starts", ".", "unsqueeze", "(", "1", ")", "\n", "span_ends_ex", "=", "span_ends", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# Idea: Make a range vector from 0 <-> seq_len - 1 and convert into boolean with (val > start) and (val < end)", "\n", "# Such items in the sequence are within the span range", "\n", "# Shape: (passage_length, )", "\n", "range_vector", "=", "allenutil", ".", "get_range_vector", "(", "passage_mask", ".", "size", "(", "1", ")", ",", "allenutil", ".", "get_device_of", "(", "passage_mask", ")", ")", "\n", "\n", "# Shape: (1, passage_length, 1)", "\n", "range_vector", "=", "range_vector", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Shape: (batch_size, passage_length, number_of_ans_spans) - 1 as tokens in the span, 0 otherwise", "\n", "span_range_mask", "=", "(", "range_vector", ">=", "span_starts_ex", ")", ".", "float", "(", ")", "*", "(", "range_vector", "<=", "span_ends_ex", ")", ".", "float", "(", ")", "\n", "span_range_mask", "=", "span_range_mask", "*", "answers_mask", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# Shape: (batch_size, passage_length)", "\n", "unnormalized_attention", "=", "span_range_mask", ".", "sum", "(", "2", ")", "\n", "normalized_attention", "=", "unnormalized_attention", "/", "unnormalized_attention", ".", "sum", "(", "1", ",", "keepdim", "=", "True", ")", "\n", "\n", "attention_aslist", "=", "[", "normalized_attention", "[", "i", ",", ":", "]", "for", "i", "in", "range", "(", "normalized_attention", ".", "size", "(", "0", ")", ")", "]", "\n", "\n", "return", "attention_aslist", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser.passageattn_to_startendlogits": [[1582, 1603], ["passage_attention.new_zeros", "passage_attention.new_zeros", "print", "allennlp.replace_masked_values", "allennlp.replace_masked_values", "passage_attention.size", "passage_attention.size"], "methods", ["None"], ["", "def", "passageattn_to_startendlogits", "(", "self", ",", "passage_attention", ",", "passage_mask", ")", ":", "\n", "        ", "span_start_logits", "=", "passage_attention", ".", "new_zeros", "(", "passage_attention", ".", "size", "(", ")", ")", "\n", "span_end_logits", "=", "passage_attention", ".", "new_zeros", "(", "passage_attention", ".", "size", "(", ")", ")", "\n", "\n", "nonzeroindcs", "=", "(", "passage_attention", ">", "0", ")", ".", "nonzero", "(", ")", "\n", "\n", "startidx", "=", "nonzeroindcs", "[", "0", "]", "\n", "endidx", "=", "nonzeroindcs", "[", "-", "1", "]", "\n", "\n", "print", "(", "f\"{startidx} {endidx}\"", ")", "\n", "\n", "span_start_logits", "[", "startidx", "]", "=", "2.0", "\n", "span_end_logits", "[", "endidx", "]", "=", "2.0", "\n", "\n", "span_start_logits", "=", "allenutil", ".", "replace_masked_values", "(", "span_start_logits", ",", "passage_mask", ",", "-", "1e32", ")", "\n", "span_end_logits", "=", "allenutil", ".", "replace_masked_values", "(", "span_end_logits", ",", "passage_mask", ",", "-", "1e32", ")", "\n", "\n", "span_start_logits", "+=", "1e-7", "\n", "span_end_logits", "+=", "1e-7", "\n", "\n", "return", "(", "span_start_logits", ",", "span_end_logits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser.passage_ans_attn_to_sideargs": [[1604, 1622], ["range", "len", "zip", "zip"], "methods", ["None"], ["", "def", "passage_ans_attn_to_sideargs", "(", "\n", "self", ",", "\n", "batch_actionseqs", ":", "List", "[", "List", "[", "List", "[", "str", "]", "]", "]", ",", "\n", "batch_actionseq_sideargs", ":", "List", "[", "List", "[", "List", "[", "Dict", "]", "]", "]", ",", "\n", "batch_gold_attentions", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", ")", ":", "\n", "\n", "        ", "for", "ins_idx", "in", "range", "(", "len", "(", "batch_actionseqs", ")", ")", ":", "\n", "            ", "instance_programs", "=", "batch_actionseqs", "[", "ins_idx", "]", "\n", "instance_prog_sideargs", "=", "batch_actionseq_sideargs", "[", "ins_idx", "]", "\n", "instance_gold_attention", "=", "batch_gold_attentions", "[", "ins_idx", "]", "\n", "for", "program", ",", "side_args", "in", "zip", "(", "instance_programs", ",", "instance_prog_sideargs", ")", ":", "\n", "                ", "first_qattn", "=", "True", "# This tells model which qent attention to use", "\n", "# print(side_args)", "\n", "# print()", "\n", "for", "action", ",", "sidearg_dict", "in", "zip", "(", "program", ",", "side_args", ")", ":", "\n", "                    ", "if", "action", "==", "\"PassageSpanAnswer -> find_passageSpanAnswer\"", ":", "\n", "                        ", "sidearg_dict", "[", "\"passage_attention\"", "]", "=", "instance_gold_attention", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser.getInitialDecoderState": [[1623, 1674], ["range", "drop_parser.DROPParser._get_initial_rnn_state", "allennlp.state_machines.states.GrammarBasedState", "encoded_question.new_zeros", "drop_parser.DROPParser._create_grammar_statelet", "initial_grammar_statelets.append", "batch_actionidx2actionstr.append", "batch_action2actionidx.append", "range", "range", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_base.DROPParserBase._get_initial_rnn_state", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_base.DROPParserBase._create_grammar_statelet"], ["", "", "", "", "", "def", "getInitialDecoderState", "(", "\n", "self", ",", "\n", "languages", ":", "List", "[", "DropLanguage", "]", ",", "\n", "actions", ":", "List", "[", "List", "[", "ProductionRule", "]", "]", ",", "\n", "encoded_question", ":", "torch", ".", "FloatTensor", ",", "\n", "question_mask", ":", "torch", ".", "FloatTensor", ",", "\n", "question_encoded_final_state", ":", "torch", ".", "FloatTensor", ",", "\n", "question_encoded_aslist", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", "question_mask_aslist", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", "batch_size", ":", "int", ",", "\n", ")", ":", "\n", "# List[torch.Tensor(0.0)] -- Initial log-score list for the decoding", "\n", "        ", "initial_score_list", "=", "[", "encoded_question", ".", "new_zeros", "(", "1", ",", "dtype", "=", "torch", ".", "float", ")", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "initial_grammar_statelets", "=", "[", "]", "\n", "batch_action2actionidx", ":", "List", "[", "Dict", "[", "str", ",", "int", "]", "]", "=", "[", "]", "\n", "# This is kind of useless, only needed for debugging in BasicTransitionFunction", "\n", "batch_actionidx2actionstr", ":", "List", "[", "List", "[", "str", "]", "]", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "(", "grammar_statelet", ",", "action2actionidx", ",", "actionidx2actionstr", ")", "=", "self", ".", "_create_grammar_statelet", "(", "\n", "languages", "[", "i", "]", ",", "actions", "[", "i", "]", "\n", ")", "\n", "\n", "initial_grammar_statelets", ".", "append", "(", "grammar_statelet", ")", "\n", "batch_actionidx2actionstr", ".", "append", "(", "actionidx2actionstr", ")", "\n", "batch_action2actionidx", ".", "append", "(", "action2actionidx", ")", "\n", "\n", "", "initial_rnn_states", "=", "self", ".", "_get_initial_rnn_state", "(", "\n", "question_encoded", "=", "encoded_question", ",", "\n", "question_mask", "=", "question_mask", ",", "\n", "question_encoded_finalstate", "=", "question_encoded_final_state", ",", "\n", "question_encoded_aslist", "=", "question_encoded_aslist", ",", "\n", "question_mask_aslist", "=", "question_mask_aslist", ",", "\n", ")", "\n", "\n", "initial_side_args", "=", "[", "[", "]", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "# Initial grammar state for the complete batch", "\n", "initial_state", "=", "GrammarBasedState", "(", "\n", "batch_indices", "=", "list", "(", "range", "(", "batch_size", ")", ")", ",", "\n", "action_history", "=", "[", "[", "]", "for", "_", "in", "range", "(", "batch_size", ")", "]", ",", "\n", "score", "=", "initial_score_list", ",", "\n", "rnn_state", "=", "initial_rnn_states", ",", "\n", "grammar_state", "=", "initial_grammar_statelets", ",", "\n", "possible_actions", "=", "actions", ",", "\n", "extras", "=", "batch_actionidx2actionstr", ",", "\n", "debug_info", "=", "initial_side_args", ",", "\n", ")", "\n", "\n", "return", "(", "initial_state", ",", "batch_action2actionidx", ",", "batch_actionidx2actionstr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser._select_indices_from_list": [[1675, 1678], ["None"], "methods", ["None"], ["", "def", "_select_indices_from_list", "(", "self", ",", "list", ",", "indices", ")", ":", "\n", "        ", "new_list", "=", "[", "list", "[", "i", "]", "for", "i", "in", "indices", "]", "\n", "return", "new_list", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser.initialState_forInstanceIndices": [[1679, 1710], ["drop_parser.DROPParser._select_indices_from_list", "drop_parser.DROPParser._select_indices_from_list", "drop_parser.DROPParser._select_indices_from_list", "drop_parser.DROPParser._select_indices_from_list", "len", "drop_parser.DROPParser.getInitialDecoderState"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT._select_indices_from_list", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT._select_indices_from_list", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT._select_indices_from_list", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT._select_indices_from_list", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.getInitialDecoderState"], ["", "def", "initialState_forInstanceIndices", "(", "\n", "self", ",", "\n", "instances_list", ":", "List", "[", "int", "]", ",", "\n", "languages", ":", "List", "[", "DropLanguage", "]", ",", "\n", "actions", ":", "List", "[", "List", "[", "ProductionRule", "]", "]", ",", "\n", "encoded_question", ":", "torch", ".", "FloatTensor", ",", "\n", "question_mask", ":", "torch", ".", "FloatTensor", ",", "\n", "question_encoded_final_state", ":", "torch", ".", "FloatTensor", ",", "\n", "question_encoded_aslist", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", "question_mask_aslist", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", ")", ":", "\n", "\n", "        ", "s_languages", "=", "self", ".", "_select_indices_from_list", "(", "languages", ",", "instances_list", ")", "\n", "s_actions", "=", "self", ".", "_select_indices_from_list", "(", "actions", ",", "instances_list", ")", "\n", "s_encoded_question", "=", "encoded_question", "[", "instances_list", "]", "\n", "s_question_mask", "=", "question_mask", "[", "instances_list", "]", "\n", "s_question_encoded_final_state", "=", "question_encoded_final_state", "[", "instances_list", "]", "\n", "s_question_encoded_aslist", "=", "self", ".", "_select_indices_from_list", "(", "question_encoded_aslist", ",", "instances_list", ")", "\n", "s_question_mask_aslist", "=", "self", ".", "_select_indices_from_list", "(", "question_mask_aslist", ",", "instances_list", ")", "\n", "\n", "num_instances", "=", "len", "(", "instances_list", ")", "\n", "\n", "return", "self", ".", "getInitialDecoderState", "(", "\n", "s_languages", ",", "\n", "s_actions", ",", "\n", "s_encoded_question", ",", "\n", "s_question_mask", ",", "\n", "s_question_encoded_final_state", ",", "\n", "s_question_encoded_aslist", ",", "\n", "s_question_mask_aslist", ",", "\n", "num_instances", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser.merge_final_states": [[1712, 1752], ["range", "len", "len", "len", "len", "supervised_instances.index", "unsupervised_instances.index"], "methods", ["None"], ["", "def", "merge_final_states", "(", "\n", "self", ",", "\n", "supervised_final_states", ",", "\n", "unsupervised_final_states", ",", "\n", "supervised_instances", ":", "List", "[", "int", "]", ",", "\n", "unsupervised_instances", ":", "List", "[", "int", "]", ",", "\n", ")", ":", "\n", "\n", "        ", "\"\"\" Supervised and unsupervised final_states are dicts with keys in order from 0 - len(dict)\n            The final keys are the instances' batch index which is stored in (un)supervised_instances list\n            i.e. index = supervised_instances[0] is the index of the instance in the original batch\n            whose final state is now in supervised_final_states[0].\n            Therefore the final_state should contain; final_state[index] = supervised_final_states[0]\n        \"\"\"", "\n", "\n", "if", "len", "(", "supervised_instances", ")", "==", "0", ":", "\n", "            ", "return", "unsupervised_final_states", "\n", "\n", "", "if", "len", "(", "unsupervised_instances", ")", "==", "0", ":", "\n", "            ", "return", "supervised_final_states", "\n", "\n", "", "batch_size", "=", "len", "(", "supervised_instances", ")", "+", "len", "(", "unsupervised_instances", ")", "\n", "\n", "final_states", "=", "{", "}", "\n", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "if", "i", "in", "supervised_instances", ":", "\n", "                ", "idx", "=", "supervised_instances", ".", "index", "(", "i", ")", "\n", "state_value", "=", "supervised_final_states", "[", "idx", "]", "\n", "final_states", "[", "i", "]", "=", "state_value", "\n", "", "else", ":", "\n", "                ", "idx", "=", "unsupervised_instances", ".", "index", "(", "i", ")", "\n", "# Unsupervised instances go through beam search and not always is a program found for them", "\n", "# If idx does not exist in unsupervised_final_states, don't add in final_states", "\n", "# Only add a instance_idx if it exists in final states -- not all beam-searches result in valid-paths", "\n", "if", "idx", "in", "unsupervised_final_states", ":", "\n", "                    ", "state_value", "=", "unsupervised_final_states", "[", "idx", "]", "\n", "final_states", "[", "i", "]", "=", "state_value", "\n", "\n", "", "", "", "return", "final_states", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser.aux_count_loss": [[1753, 1789], ["torch.stack", "torch.stack", "torch.stack", "torch.stack", "drop_parser.DROPParser._executor_parameters.passage_attention_to_count", "drop_parser.DROPParser._executor_parameters.passage_count_predictor", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "passage_attention.size", "answer_as_count.float.float.float", "torch.log", "torch.log", "torch.log", "torch.log", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.sum().float", "torch.sum().float", "torch.sum().float", "torch.sum().float", "count_mask.float", "count_mask.unsqueeze().float", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "count_mask.unsqueeze", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "def", "aux_count_loss", "(", "self", ",", "passage_attention", ",", "passage_mask", ",", "answer_as_count", ",", "count_mask", ")", ":", "\n", "        ", "if", "torch", ".", "sum", "(", "count_mask", ")", "==", "0", ":", "\n", "            ", "loss", ",", "accuracy", "=", "0.0", ",", "0.0", "\n", "return", "loss", ",", "accuracy", "\n", "\n", "", "batch_size", "=", "passage_attention", ".", "size", "(", ")", "[", "0", "]", "\n", "# List of (B, P) shaped tensors", "\n", "scaled_attentions", "=", "[", "passage_attention", "*", "sf", "for", "sf", "in", "self", ".", "_executor_parameters", ".", "passage_attention_scalingvals", "]", "\n", "# Shape: (B, passage_length, num_scaling_factors)", "\n", "scaled_passage_attentions", "=", "torch", ".", "stack", "(", "scaled_attentions", ",", "dim", "=", "2", ")", "\n", "# Shape: (B, hidden_dim)", "\n", "count_hidden_repr", "=", "self", ".", "_executor_parameters", ".", "passage_attention_to_count", "(", "\n", "scaled_passage_attentions", ",", "passage_mask", "\n", ")", "\n", "# Shape: (B, num_counts)", "\n", "passage_span_logits", "=", "self", ".", "_executor_parameters", ".", "passage_count_predictor", "(", "count_hidden_repr", ")", "\n", "count_distribution", "=", "torch", ".", "softmax", "(", "passage_span_logits", ",", "dim", "=", "1", ")", "\n", "\n", "loss", "=", "0", "\n", "accuracy", "=", "0", "\n", "if", "answer_as_count", "is", "not", "None", ":", "\n", "# (B, num_counts)", "\n", "            ", "answer_as_count", "=", "answer_as_count", ".", "float", "(", ")", "\n", "count_log_probs", "=", "torch", ".", "log", "(", "count_distribution", "+", "1e-40", ")", "\n", "log_likelihood", "=", "torch", ".", "sum", "(", "count_log_probs", "*", "answer_as_count", "*", "count_mask", ".", "unsqueeze", "(", "1", ")", ".", "float", "(", ")", ")", "\n", "\n", "loss", "=", "-", "1", "*", "log_likelihood", "\n", "loss", "=", "loss", "/", "torch", ".", "sum", "(", "count_mask", ")", ".", "float", "(", ")", "\n", "\n", "# List of predicted count idxs", "\n", "count_idx", "=", "torch", ".", "argmax", "(", "count_distribution", ",", "1", ")", "\n", "gold_count_idxs", "=", "torch", ".", "argmax", "(", "answer_as_count", ",", "1", ")", "\n", "correct_vec", "=", "(", "count_idx", "==", "gold_count_idxs", ")", ".", "float", "(", ")", "*", "count_mask", ".", "float", "(", ")", "\n", "accuracy", "=", "(", "torch", ".", "sum", "(", "correct_vec", ")", "/", "torch", ".", "sum", "(", "count_mask", ")", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "", "return", "loss", ",", "accuracy", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser.masking_blockdiagonal": [[1790, 1819], ["allennlp.move_to_device", "allennlp.move_to_device", "allennlp.move_to_device.unsqueeze", "allennlp.move_to_device.unsqueeze", "allennlp.get_range_vector().unsqueeze", "allennlp.get_range_vector().unsqueeze", "max", "min", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "range", "range", "allennlp.get_range_vector", "allennlp.get_range_vector"], "methods", ["None"], ["", "def", "masking_blockdiagonal", "(", "self", ",", "batch_size", ",", "passage_length", ",", "window", ",", "device_id", ")", ":", "\n", "        ", "\"\"\" Make a (batch_size, passage_length, passage_length) tensor M of 1 and -1 in which for each row x,\n            M[:, x, y] = -1 if y < x - window or y > x + window, else it is 1.\n            Basically for the x-th row, the [x-win, x+win] columns should be 1, and rest -1\n        \"\"\"", "\n", "\n", "lower_limit", "=", "[", "max", "(", "0", ",", "i", "-", "window", ")", "for", "i", "in", "range", "(", "passage_length", ")", "]", "\n", "upper_limit", "=", "[", "min", "(", "passage_length", ",", "i", "+", "window", ")", "for", "i", "in", "range", "(", "passage_length", ")", "]", "\n", "\n", "# Tensors of lower and upper limits for each row", "\n", "lower", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "LongTensor", "(", "lower_limit", ")", ",", "cuda_device", "=", "device_id", ")", "\n", "upper", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "LongTensor", "(", "upper_limit", ")", ",", "cuda_device", "=", "device_id", ")", "\n", "lower_un", "=", "lower", ".", "unsqueeze", "(", "1", ")", "\n", "upper_un", "=", "upper", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# Range vector for each row", "\n", "lower_range_vector", "=", "allenutil", ".", "get_range_vector", "(", "passage_length", ",", "device", "=", "device_id", ")", ".", "unsqueeze", "(", "0", ")", "\n", "upper_range_vector", "=", "allenutil", ".", "get_range_vector", "(", "passage_length", ",", "device", "=", "device_id", ")", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "# Masks for lower and upper limits of the mask", "\n", "lower_mask", "=", "lower_range_vector", ">=", "lower_un", "\n", "upper_mask", "=", "upper_range_vector", "<=", "upper_un", "\n", "\n", "# Final-mask that we require", "\n", "# Shape: (passage_length, passage_length); (passage_length, passage_length)", "\n", "inwindow_mask", "=", "(", "lower_mask", "==", "upper_mask", ")", ".", "float", "(", ")", "\n", "outwindow_mask", "=", "(", "lower_mask", "!=", "upper_mask", ")", ".", "float", "(", ")", "\n", "\n", "return", "inwindow_mask", ",", "outwindow_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser.window_loss_numdate": [[1820, 1883], ["inwindow_probs.sum", "allennlp.replace_masked_values", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "allennlp.replace_masked_values", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "inwindow_mask.unsqueeze", "passage_tokenidx_mask.unsqueeze", "torch.log", "torch.log", "torch.log", "torch.log", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "outwindow_mask.unsqueeze", "passage_tokenidx_mask.unsqueeze", "torch.log", "torch.log", "torch.log", "torch.log", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "inwindow_mask.sum"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "def", "window_loss_numdate", "(", "self", ",", "passage_passage_alignment", ",", "passage_tokenidx_mask", ",", "inwindow_mask", ",", "outwindow_mask", ")", ":", "\n", "        ", "\"\"\"\n        The idea is to first softmax the similarity_scores to get a distribution over the date/num tokens from each\n        passage token.\n\n        For each passage_token,\n            -- increase the sum of prob for date/num tokens around it in a window (don't know which date/num is correct)\n            -- decrease the prod of prob for date/num tokens outside the window (know that all date/num are wrong)\n\n        Parameters:\n        -----------\n        passage_passage_similarity_scores: (batch_size, passage_length, passage_length)\n            For each passage_token, a similarity score to other passage_tokens for data/num\n            This should ideally, already be masked\n\n        passage_tokenidx_mask: (batch_size, passage_length)\n            Mask for tokens that are num/date\n\n        inwindow_mask: (passage_length, passage_length)\n            For row x, inwindow_mask[x, x-window : x+window] = 1 and 0 otherwise. Mask for a window around the token\n        outwindow_mask: (passage_length, passage_length)\n            Opposite of inwindow_mask. For each row x, the colums are 1 outside of a window around x\n        \"\"\"", "\n", "# (batch_size, passage_length, passage_length)", "\n", "# passage_passage_alignment_matrix = allenutil.masked_softmax(passage_passage_similarity_scores,", "\n", "#                                                             passage_tokenidx_mask.unsqueeze(1),", "\n", "#                                                             memory_efficient=True)", "\n", "\n", "inwindow_mask", "=", "inwindow_mask", ".", "unsqueeze", "(", "0", ")", "*", "passage_tokenidx_mask", ".", "unsqueeze", "(", "1", ")", "\n", "inwindow_probs", "=", "passage_passage_alignment", "*", "inwindow_mask", "\n", "# This signifies that each token can distribute it's prob to nearby-date/num in anyway", "\n", "# Shape: (batch_size, passage_length)", "\n", "sum_inwindow_probs", "=", "inwindow_probs", ".", "sum", "(", "2", ")", "\n", "mask_sum", "=", "(", "inwindow_mask", ".", "sum", "(", "2", ")", ">", "0", ")", ".", "float", "(", ")", "\n", "# Imagine a row where mask = 0, there sum of probs will be zero and we need to compute masked_log", "\n", "masked_sum_inwindow_probs", "=", "allenutil", ".", "replace_masked_values", "(", "sum_inwindow_probs", ",", "mask_sum", ",", "replace_with", "=", "1e-40", ")", "\n", "log_sum_inwindow_probs", "=", "torch", ".", "log", "(", "masked_sum_inwindow_probs", "+", "1e-40", ")", "*", "mask_sum", "\n", "inwindow_likelihood", "=", "torch", ".", "sum", "(", "log_sum_inwindow_probs", ")", "\n", "if", "torch", ".", "sum", "(", "inwindow_mask", ")", ">", "0", ":", "\n", "            ", "inwindow_likelihood", "=", "inwindow_likelihood", "/", "torch", ".", "sum", "(", "inwindow_mask", ")", "\n", "", "else", ":", "\n", "            ", "inwindow_likelihood", "=", "0.0", "\n", "\n", "", "outwindow_mask", "=", "outwindow_mask", ".", "unsqueeze", "(", "0", ")", "*", "passage_tokenidx_mask", ".", "unsqueeze", "(", "1", ")", "\n", "# For tokens outside the window, increase entropy of the distribution. i.e. -\\sum p*log(p)", "\n", "# Since we'd like to distribute the weight equally to things outside the window", "\n", "# Shape: (batch_size, passage_length, passage_length)", "\n", "outwindow_probs", "=", "passage_passage_alignment", "*", "outwindow_mask", "\n", "\n", "masked_outwindow_probs", "=", "allenutil", ".", "replace_masked_values", "(", "outwindow_probs", ",", "outwindow_mask", ",", "replace_with", "=", "1e-40", ")", "\n", "outwindow_probs_log", "=", "torch", ".", "log", "(", "masked_outwindow_probs", "+", "1e-40", ")", "*", "outwindow_mask", "\n", "# Shape: (batch_length, passage_length)", "\n", "outwindow_negentropies", "=", "torch", ".", "sum", "(", "outwindow_probs", "*", "outwindow_probs_log", ")", "\n", "\n", "if", "torch", ".", "sum", "(", "outwindow_mask", ")", ">", "0", ":", "\n", "            ", "outwindow_negentropies", "=", "outwindow_negentropies", "/", "torch", ".", "sum", "(", "outwindow_mask", ")", "\n", "", "else", ":", "\n", "            ", "outwindow_negentropies", "=", "0.0", "\n", "\n", "# Increase inwindow likelihod and decrease outwindow-negative-entropy", "\n", "", "loss", "=", "-", "1", "*", "inwindow_likelihood", "+", "outwindow_negentropies", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser.DROPParser.number2count_auxloss": [[1884, 1937], ["len", "max", "allennlp.move_to_device", "allennlp.get_range_vector", "mask.new_zeros().normal_().abs_", "mask.new_zeros().normal_().abs_.new_zeros().long", "enumerate", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "drop_parser.DROPParser.passage_attention_to_count", "drop_parser.DROPParser.passage_count_predictor", "torch.cross_entropy", "torch.cross_entropy", "len", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "random.randint", "torch.sum().unsqueeze", "torch.sum().unsqueeze", "torch.sum().unsqueeze", "torch.sum().unsqueeze", "mask.new_zeros().normal_", "mask.new_zeros().normal_().abs_.new_zeros", "min", "random.sample", "allennlp.get_range_vector.unsqueeze", "allennlp.move_to_device.unsqueeze", "range", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "mask.new_zeros"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "def", "number2count_auxloss", "(", "self", ",", "passage_number_values", ":", "List", "[", "List", "[", "float", "]", "]", ",", "device_id", "=", "-", "1", ")", ":", "\n", "        ", "\"\"\" Using passage numnbers, make a (batch_size, max_passage_numbers) (padded) tensor, each containing a\n            noisy distribution with mass distributed over x-numbers. The corresponding count-answer will be x.\n            Use the attention2count rnn to predict a count value and compute the loss.\n        \"\"\"", "\n", "batch_size", "=", "len", "(", "passage_number_values", ")", "\n", "# List of length -- batch-size", "\n", "num_of_passage_numbers", "=", "[", "len", "(", "nums", ")", "for", "nums", "in", "passage_number_values", "]", "\n", "max_passage_numbers", "=", "max", "(", "num_of_passage_numbers", ")", "\n", "\n", "# Shape: (batch_size, )", "\n", "num_pasasge_numbers", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "LongTensor", "(", "num_of_passage_numbers", ")", ",", "cuda_device", "=", "device_id", ")", "\n", "# Shape: (max_passage_numbers, )", "\n", "range_vector", "=", "allenutil", ".", "get_range_vector", "(", "size", "=", "max_passage_numbers", ",", "device", "=", "device_id", ")", "\n", "\n", "# Shape: (batch_size, maxnum_passage_numbers)", "\n", "mask", "=", "(", "range_vector", ".", "unsqueeze", "(", "0", ")", "<", "num_pasasge_numbers", ".", "unsqueeze", "(", "1", ")", ")", ".", "float", "(", ")", "\n", "\n", "number_distributions", "=", "mask", ".", "new_zeros", "(", "batch_size", ",", "max_passage_numbers", ")", ".", "normal_", "(", "0", ",", "0.05", ")", ".", "abs_", "(", ")", "\n", "count_answers", "=", "number_distributions", ".", "new_zeros", "(", "batch_size", ")", ".", "long", "(", ")", "\n", "for", "i", ",", "num_numbers", "in", "enumerate", "(", "num_of_passage_numbers", ")", ":", "\n", "            ", "\"\"\" Sample a count value between [0, min(5, num_numbers)]. Sample indices in this range, and set them as 1.\n                Add gaussian noise to the whole tensor and normalize. \n            \"\"\"", "\n", "# Pick a count answer", "\n", "count_value", "=", "random", ".", "randint", "(", "1", ",", "min", "(", "7", ",", "num_numbers", ")", ")", "\n", "count_answers", "[", "i", "]", "=", "count_value", "\n", "# Pick the indices that will have mass", "\n", "if", "count_value", ">", "0", ":", "\n", "                ", "indices", "=", "random", ".", "sample", "(", "range", "(", "num_numbers", ")", ",", "count_value", ")", "\n", "# Add 1.0 to all sampled indices", "\n", "number_distributions", "[", "i", ",", "indices", "]", "+=", "1.0", "\n", "\n", "", "", "number_distributions", "=", "number_distributions", "*", "mask", "\n", "# Shape: (batch_size, maxnum_passage_numbers)", "\n", "number_distributions", "=", "number_distributions", "/", "torch", ".", "sum", "(", "number_distributions", ",", "dim", "=", "1", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# Distributions made; computing loss", "\n", "scaled_attentions", "=", "[", "\n", "number_distributions", "*", "sf", "for", "sf", "in", "self", ".", "_executor_parameters", ".", "passage_attention_scalingvals", "\n", "]", "\n", "# Shape: (batch_size, maxnum_passage_numbers, num_scaling_factors)", "\n", "stacked_scaled_attentions", "=", "torch", ".", "stack", "(", "scaled_attentions", ",", "dim", "=", "2", ")", "\n", "\n", "# Shape: (batch_size, hidden_dim)", "\n", "count_hidden_repr", "=", "self", ".", "passage_attention_to_count", "(", "stacked_scaled_attentions", ",", "mask", ")", "\n", "\n", "# Shape: (batch_size, num_counts)", "\n", "count_logits", "=", "self", ".", "passage_count_predictor", "(", "count_hidden_repr", ")", "\n", "\n", "count_loss", "=", "F", ".", "cross_entropy", "(", "input", "=", "count_logits", ",", "target", "=", "count_answers", ")", "\n", "\n", "return", "count_loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.__init__": [[52, 225], ["allennlp.nn.InitializerApplicator", "semqa.models.drop_parser_base.DROPParserBase.__init__", "allennlp_semparse.state_machines.transition_functions.BasicTransitionFunction", "allennlp_semparse.state_machines.trainers.maximum_marginal_likelihood.MaximumMarginalLikelihood", "allennlp_semparse.state_machines.BeamSearch", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.modules.matrix_attention.LinearMatrixAttention", "allennlp.modules.matrix_attention.DotProductMatrixAttention", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "len", "semqa.domain_languages.drop_execution_parameters.ExecutorParameters", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.training.metrics.DropEmAndF1", "initializers", "drop_parser_bert.DROPParserBERT.named_parameters", "RuntimeError", "RuntimeError", "pytorch_pretrained_bert.BertModel", "torch.nn.Dropout", "drop_parser_bert.DROPParserBERT.passage_attention_to_span.get_output_dim", "drop_parser_bert.DROPParserBERT.passage_attention_to_count.get_output_dim", "drop_parser_bert.DROPParserBERT.passage_attention_to_count.get_output_dim", "drop_parser_bert.DROPParserBERT.named_parameters", "allennlp.modules.token_embedders.bert_token_embedder.PretrainedBertEmbedder", "pytorch_pretrained_bert.BertModel.from_pretrained", "any", "pytorch_pretrained_bert.BertConfig", "allennlp.nn.Activation.by_name"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "max_ques_len", ":", "int", ",", "\n", "action_embedding_dim", ":", "int", ",", "\n", "transitionfunc_attention", ":", "Attention", ",", "\n", "passage_attention_to_span", ":", "Seq2SeqEncoder", ",", "\n", "question_attention_to_span", ":", "Seq2SeqEncoder", ",", "\n", "passage_attention_to_count", ":", "Seq2SeqEncoder", ",", "\n", "beam_size", ":", "int", ",", "\n", "max_decoding_steps", ":", "int", ",", "\n", "bert_config_json", ":", "str", "=", "None", ",", "\n", "pretrained_bert_model", ":", "str", "=", "None", ",", "\n", "scaling_bert", ":", "bool", "=", "False", ",", "\n", "countfixed", ":", "bool", "=", "False", ",", "\n", "auxwinloss", ":", "bool", "=", "False", ",", "\n", "excloss", ":", "bool", "=", "False", ",", "\n", "qattloss", ":", "bool", "=", "False", ",", "\n", "mmlloss", ":", "bool", "=", "False", ",", "\n", "hardem_epoch", ":", "int", "=", "0", ",", "\n", "dropout", ":", "float", "=", "0.0", ",", "\n", "debug", ":", "bool", "=", "False", ",", "\n", "interpret", ":", "bool", "=", "False", ",", "# This is a flag for interpret-acl20", "\n", "profile_freq", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "cuda_device", ":", "int", "=", "-", "1", ",", "\n", "initializers", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", "regularizer", ":", "Optional", "[", "RegularizerApplicator", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "super", "(", "DROPParserBERT", ",", "self", ")", ".", "__init__", "(", "\n", "vocab", "=", "vocab", ",", "\n", "action_embedding_dim", "=", "action_embedding_dim", ",", "\n", "dropout", "=", "dropout", ",", "\n", "debug", "=", "debug", ",", "\n", "regularizer", "=", "regularizer", ",", "\n", ")", "\n", "\n", "if", "pretrained_bert_model", "is", "None", "and", "bert_config_json", "is", "None", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Both 'pretrained_bert_model' and 'bert_config_json' cannot be None\"", ")", "\n", "\n", "", "if", "pretrained_bert_model", "is", "not", "None", "and", "bert_config_json", "is", "not", "None", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Only one of 'pretrained_bert_model' and 'bert_config_json' should be specified.\"", ")", "\n", "\n", "", "self", ".", "scaling_bert", "=", "scaling_bert", "\n", "if", "pretrained_bert_model", "is", "None", ":", "\n", "            ", "self", ".", "BERT", "=", "BertModel", "(", "config", "=", "BertConfig", "(", "vocab_size_or_config_json_file", "=", "bert_config_json", ")", ")", "\n", "self", ".", "scaling_bert", "=", "False", "\n", "", "else", ":", "\n", "            ", "if", "scaling_bert", ":", "\n", "                ", "self", ".", "bert_token_embedder", "=", "PretrainedBertEmbedder", "(", "pretrained_model", "=", "pretrained_bert_model", ",", "\n", "requires_grad", "=", "False", ",", "top_layer_only", "=", "False", ")", "\n", "self", ".", "BERT", "=", "None", "\n", "self", ".", "bert_dim", "=", "self", ".", "bert_token_embedder", ".", "output_dim", "\n", "\n", "", "else", ":", "\n", "                ", "self", ".", "BERT", "=", "BertModel", ".", "from_pretrained", "(", "pretrained_bert_model", ")", "\n", "self", ".", "bert_token_embedder", "=", "None", "\n", "self", ".", "bert_dim", "=", "self", ".", "BERT", ".", "pooler", ".", "dense", ".", "out_features", "\n", "\n", "# bert_dim = self.BERT.pooler.dense.out_features", "\n", "# self.bert_dim = bert_dim", "\n", "\n", "", "", "self", ".", "max_ques_len", "=", "max_ques_len", "\n", "\n", "question_encoding_dim", "=", "self", ".", "bert_dim", "\n", "\n", "self", ".", "_decoder_step", "=", "BasicTransitionFunction", "(", "\n", "encoder_output_dim", "=", "question_encoding_dim", ",", "\n", "action_embedding_dim", "=", "action_embedding_dim", ",", "\n", "input_attention", "=", "transitionfunc_attention", ",", "\n", "activation", "=", "Activation", ".", "by_name", "(", "\"tanh\"", ")", "(", ")", ",", "\n", "add_action_bias", "=", "False", ",", "\n", "dropout", "=", "dropout", ",", "\n", ")", "\n", "self", ".", "_mml", "=", "MaximumMarginalLikelihood", "(", ")", "\n", "\n", "# self.modeltype = modeltype", "\n", "\n", "self", ".", "_beam_size", "=", "beam_size", "\n", "self", ".", "_decoder_beam_search", "=", "BeamSearch", "(", "beam_size", "=", "self", ".", "_beam_size", ")", "\n", "self", ".", "_max_decoding_steps", "=", "max_decoding_steps", "\n", "self", ".", "_action_padding_index", "=", "-", "1", "\n", "\n", "# This metrircs measure accuracy of", "\n", "# (1) Top-predicted program, (2) ExpectedDenotation from the beam (3) Best accuracy from topK(5) programs", "\n", "self", ".", "top1_acc_metric", "=", "Average", "(", ")", "\n", "self", ".", "expden_acc_metric", "=", "Average", "(", ")", "\n", "self", ".", "topk_acc_metric", "=", "Average", "(", ")", "\n", "self", ".", "aux_goldparse_loss", "=", "Average", "(", ")", "\n", "self", ".", "qent_loss", "=", "Average", "(", ")", "\n", "self", ".", "qattn_cov_loss_metric", "=", "Average", "(", ")", "\n", "\n", "# Use a separate encoder for passage - date - num similarity", "\n", "\n", "self", ".", "qp_matrix_attention", "=", "LinearMatrixAttention", "(", "\n", "tensor_1_dim", "=", "self", ".", "bert_dim", ",", "tensor_2_dim", "=", "self", ".", "bert_dim", ",", "combination", "=", "\"x,y,x*y\"", "\n", ")", "\n", "\n", "# self.passage_token_to_date = passage_token_to_date", "\n", "self", ".", "dotprod_matrix_attn", "=", "DotProductMatrixAttention", "(", ")", "\n", "\n", "if", "dropout", ">", "0", ":", "\n", "            ", "self", ".", "_dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_dropout", "=", "lambda", "x", ":", "x", "\n", "\n", "", "self", ".", "passage_attention_to_span", "=", "passage_attention_to_span", "\n", "self", ".", "passage_startend_predictor", "=", "torch", ".", "nn", ".", "Linear", "(", "self", ".", "passage_attention_to_span", ".", "get_output_dim", "(", ")", ",", "2", ")", "\n", "\n", "self", ".", "num_counts", "=", "10", "\n", "self", ".", "passage_attention_to_count", "=", "passage_attention_to_count", "\n", "self", ".", "passage_count_predictor", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "self", ".", "passage_attention_to_count", ".", "get_output_dim", "(", ")", ",", "self", ".", "num_counts", ",", "bias", "=", "False", "\n", ")", "\n", "self", ".", "passage_count_hidden2logits", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "self", ".", "passage_attention_to_count", ".", "get_output_dim", "(", ")", ",", "1", ",", "bias", "=", "True", "\n", ")", "\n", "\n", "# self.passage_count_predictor.bias.data.zero_()", "\n", "# self.passage_count_predictor.bias.requires_grad = False", "\n", "\n", "self", ".", "_num_implicit_nums", "=", "len", "(", "DropLanguage", ".", "implicit_numbers", ")", "\n", "\n", "self", ".", "_executor_parameters", "=", "ExecutorParameters", "(", "\n", "question_encoding_dim", "=", "self", ".", "bert_dim", ",", "\n", "passage_encoding_dim", "=", "self", ".", "bert_dim", ",", "\n", "passage_attention_to_span", "=", "self", ".", "passage_attention_to_span", ",", "\n", "passage_startend_predictor", "=", "self", ".", "passage_startend_predictor", ",", "\n", "question_attention_to_span", "=", "question_attention_to_span", ",", "\n", "passage_attention_to_count", "=", "self", ".", "passage_attention_to_count", ",", "\n", "passage_count_predictor", "=", "self", ".", "passage_count_predictor", ",", "\n", "passage_count_hidden2logits", "=", "self", ".", "passage_count_hidden2logits", ",", "\n", "num_implicit_nums", "=", "self", ".", "_num_implicit_nums", ",", "\n", "dropout", "=", "dropout", ",", "\n", ")", "\n", "\n", "self", ".", "modelloss_metric", "=", "Average", "(", ")", "\n", "self", ".", "excloss_metric", "=", "Average", "(", ")", "\n", "self", ".", "qattloss_metric", "=", "Average", "(", ")", "\n", "self", ".", "mmlloss_metric", "=", "Average", "(", ")", "\n", "self", ".", "auxwinloss_metric", "=", "Average", "(", ")", "\n", "self", ".", "_drop_metrics", "=", "DropEmAndF1", "(", ")", "\n", "\n", "self", ".", "auxwinloss", "=", "auxwinloss", "\n", "\n", "# Main loss for QA", "\n", "# Auxiliary losses, such as - Prog-MML, QAttn, DateGrounding etc.", "\n", "self", ".", "excloss", "=", "excloss", "\n", "self", ".", "qattloss", "=", "qattloss", "\n", "self", ".", "mmlloss", "=", "mmlloss", "\n", "\n", "# Hard-EM will start from this epoch (0-indexed meaning from the beginning); until then MML loss will be used", "\n", "self", ".", "hardem_epoch", "=", "hardem_epoch", "if", "hardem_epoch", ">=", "0", "else", "None", "\n", "\n", "initializers", "(", "self", ")", "\n", "\n", "for", "name", ",", "parameter", "in", "self", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "name", "==", "\"_text_field_embedder.token_embedder_tokens.weight\"", ":", "\n", "                ", "parameter", ".", "requires_grad", "=", "False", "\n", "\n", "# # # Fix parameters for Counting", "\n", "", "", "count_parameter_names", "=", "[", "\"passage_attention_to_count\"", ",", "\"passage_count_hidden2logits\"", ",", "\"passage_count_predictor\"", "]", "\n", "if", "countfixed", ":", "\n", "            ", "for", "name", ",", "parameter", "in", "self", ".", "named_parameters", "(", ")", ":", "\n", "                ", "if", "any", "(", "span", "in", "name", "for", "span", "in", "count_parameter_names", ")", ":", "\n", "                    ", "parameter", ".", "requires_grad", "=", "False", "\n", "\n", "", "", "", "self", ".", "profile_steps", "=", "0", "\n", "self", ".", "profile_freq", "=", "None", "if", "profile_freq", "==", "0", "else", "profile_freq", "\n", "\n", "self", ".", "device_id", "=", "None", "\n", "\n", "self", ".", "interpret", "=", "interpret", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.forward": [[231, 1014], ["pad_mask[].float", "pad_mask[].float", "len", "drop_parser_bert.DROPParserBERT.qp_matrix_attention", "drop_parser_bert.DROPParserBERT.transpose", "allennlp.masked_softmax", "allennlp.masked_softmax", "drop_parser_bert.DROPParserBERT.compute_token_symbol_alignments", "drop_parser_bert.DROPParserBERT.compute_token_symbol_alignments", "drop_parser_bert.DROPParserBERT.compute_token_symbol_alignments", "drop_parser_bert.DROPParserBERT.compute_token_symbol_alignments", "list", "allennlp.get_device_of", "semqa.profiler.profile.Profile", "modeled_passage.size", "pad_mask[].float.unsqueeze", "pad_mask[].float.unsqueeze", "len", "semqa.profiler.profile.Profile", "action2idx_map.values", "semqa.profiler.profile.Profile", "semqa.models.utils.semparse_utils._convert_finalstates_to_actions", "drop_parser_bert.DROPParserBERT.passage_attention_to_sidearg", "drop_parser_bert.DROPParserBERT.datecompare_eventdategr_to_sideargs", "drop_parser_bert.DROPParserBERT.numcompare_eventnumgr_to_sideargs", "semqa.profiler.profile.Profile", "drop_parser_bert.DROPParserBERT._get_denotations", "allennlp.move_to_device().float", "torch.isnan", "allennlp.move_to_device", "range", "drop_parser_bert.DROPParserBERT.modelloss_metric", "range", "logger.info", "drop_parser_bert.DROPParserBERT.BERT", "drop_parser_bert.DROPParserBERT.bert_token_embedder", "semqa.profiler.profile.Profile", "drop_parser_bert.DROPParserBERT.masking_blockdiagonal", "semqa.profiler.profile.Profile", "drop_parser_bert.DROPParserBERT.window_loss_numdate", "drop_parser_bert.DROPParserBERT.window_loss_numdate", "drop_parser_bert.DROPParserBERT.window_loss_numdate", "drop_parser_bert.DROPParserBERT.window_loss_numdate", "range", "range", "range", "range", "range", "range", "range", "range", "range", "range", "range", "range", "range", "range", "range", "range", "range", "semqa.domain_languages.drop_language.DropLanguage", "range", "any", "encoded_passage.size", "drop_parser_bert.DROPParserBERT.auxwinloss_metric", "drop_parser_bert.DROPParserBERT._ques_attention_loss", "logger.info", "torch.tensor", "range", "batch_ansprob_list.append", "torch.stack", "torch.stack", "torch.sum", "torch.isnan", "torch.isnan", "batch_denotation_loss.item", "range", "output_dict[].append", "output_dict[].append", "metadata[].get", "enumerate", "semqa.profiler.profile.Profile.to_string", "range", "enumerate", "drop_parser_bert.DROPParserBERT._select_indices_from_list", "zip", "list", "list", "drop_parser_bert.DROPParserBERT.initialState_forInstanceIndices", "allennlp_semparse.state_machines.ConstrainedBeamSearch", "allennlp_semparse.state_machines.ConstrainedBeamSearch.search", "allennlp_semparse.state_machines.ConstrainedBeamSearch.search.values", "drop_parser_bert.DROPParserBERT.merge_final_states", "drop_parser_bert.DROPParserBERT.getInitialDecoderState", "drop_parser_bert.DROPParserBERT.get_valid_start_actionids", "semqa.state_machines.prefixed_beam_search.PrefixedConstrainedBeamSearch", "semqa.state_machines.prefixed_beam_search.PrefixedConstrainedBeamSearch.search", "drop_parser_bert.DROPParserBERT.getInitialDecoderState", "drop_parser_bert.DROPParserBERT._decoder_beam_search.search", "all", "all", "zip", "list", "list", "drop_parser_bert.DROPParserBERT.getInitialDecoderState", "allennlp_semparse.state_machines.ConstrainedBeamSearch", "allennlp_semparse.state_machines.ConstrainedBeamSearch.search", "torch.exp", "allennlp.move_to_device", "aux_win_loss.item", "drop_parser_bert.DROPParserBERT.excloss_metric", "drop_parser_bert.DROPParserBERT.qattloss_metric", "drop_parser_bert.DROPParserBERT.mmlloss_metric", "len", "len", "torch.isnan", "torch.isnan", "instance_log_likelihood_list.append", "torch.exp", "torch.argmax", "allennlp.logsumexp", "logger.info", "logger.info", "logger.info", "len", "all_instance_progs_predicted_answer_strs.append", "len", "all_instance_progs_predicted_answer_strs.append", "drop_parser_bert.DROPParserBERT._drop_metrics", "batch_logical_programs.append", "languages[].all_possible_productions", "len", "len", "drop_parser_bert.DROPParserBERT.initialState_forInstanceIndices", "drop_parser_bert.DROPParserBERT._select_indices_from_list", "drop_parser_bert.DROPParserBERT._select_indices_from_list", "drop_parser_bert.DROPParserBERT._select_indices_from_list", "drop_parser_bert.DROPParserBERT.get_valid_start_actionids", "semqa.state_machines.prefixed_beam_search.PrefixedConstrainedBeamSearch", "semqa.state_machines.prefixed_beam_search.PrefixedConstrainedBeamSearch.search", "torch.tensor", "batch_exec_loss.item", "drop_parser_bert.DROPParserBERT.item", "mml_loss.item", "drop_parser_bert.DROPParserBERT._get_span_answer_log_prob", "logger.info", "allennlp.move_to_device", "logger.info", "allennlp.move_to_device", "allennlp.models.reading_comprehension.util.get_best_span().squeeze", "tuple", "languages[].action_sequence_to_logical_form", "instance_logical_progs.append", "enumerate", "enumerate", "state.score[].view", "allennlp.logsumexp", "drop_parser_bert.DROPParserBERT._get_span_answer_log_prob", "torch.tensor", "torch.tensor", "allennlp.models.reading_comprehension.util.get_best_span().squeeze.detach().cpu().numpy", "allennlp.models.reading_comprehension.util.get_best_span().squeeze", "tuple", "torch.cat", "torch.log", "allennlp.move_to_device", "torch.sum", "allennlp.models.reading_comprehension.util.get_best_span", "allennlp.models.reading_comprehension.util.get_best_span().squeeze.detach().cpu().numpy", "denotation._value.detach().cpu().numpy", "numpy.argmax", "str", "torch.FloatTensor", "torch.log", "allennlp.move_to_device", "torch.sum", "allennlp.models.reading_comprehension.util.get_best_span().squeeze.detach().cpu", "allennlp.models.reading_comprehension.util.get_best_span", "torch.argmax().detach().cpu().numpy", "str", "torch.FloatTensor", "torch.log", "allennlp.move_to_device", "torch.sum", "denotation._value[].unsqueeze", "denotation._value[].unsqueeze", "allennlp.models.reading_comprehension.util.get_best_span().squeeze.detach().cpu", "denotation._value.detach().cpu", "int", "torch.argmax().detach().cpu().numpy", "str", "torch.FloatTensor", "torch.log", "allennlp.move_to_device", "torch.sum", "allennlp.models.reading_comprehension.util.get_best_span().squeeze.detach", "denotation._value[].unsqueeze", "denotation._value[].unsqueeze", "torch.argmax().detach().cpu", "int", "int", "torch.argmax().detach().cpu().numpy", "str", "torch.FloatTensor", "allennlp.models.reading_comprehension.util.get_best_span().squeeze.detach", "denotation._value.detach", "torch.argmax().detach().cpu", "int", "int", "torch.argmax().detach", "torch.argmax().detach().cpu", "int", "torch.argmax().detach", "torch.argmax", "torch.argmax().detach", "torch.argmax", "torch.argmax"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.compute_token_symbol_alignments", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.compute_token_symbol_alignments", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.compute_token_symbol_alignments", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.compute_token_symbol_alignments", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.semparse_utils._convert_finalstates_to_actions", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.passage_attention_to_sidearg", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.datecompare_eventdategr_to_sideargs", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.numcompare_eventnumgr_to_sideargs", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_base.DROPParserBase._get_denotations", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.masking_blockdiagonal", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.window_loss_numdate", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.window_loss_numdate", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.window_loss_numdate", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.window_loss_numdate", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT._ques_attention_loss", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.to_string", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT._select_indices_from_list", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.initialState_forInstanceIndices", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.state_machines.prefixed_beam_search.PrefixedConstrainedBeamSearch.search", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.merge_final_states", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.getInitialDecoderState", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.get_valid_start_actionids", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.state_machines.prefixed_beam_search.PrefixedConstrainedBeamSearch.search", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.getInitialDecoderState", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.state_machines.prefixed_beam_search.PrefixedConstrainedBeamSearch.search", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.getInitialDecoderState", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.state_machines.prefixed_beam_search.PrefixedConstrainedBeamSearch.search", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.initialState_forInstanceIndices", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT._select_indices_from_list", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT._select_indices_from_list", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT._select_indices_from_list", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.get_valid_start_actionids", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.state_machines.prefixed_beam_search.PrefixedConstrainedBeamSearch.search", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.qanet.QANet._get_span_answer_log_prob", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.qanet.QANet._get_span_answer_log_prob", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "@", "profile_func_decorator", "\n", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "question_passage", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", ",", "\n", "question", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", ",", "\n", "passage", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", ",", "\n", "p_sentboundary_wps", ":", "torch", ".", "LongTensor", ",", "\n", "passageidx2numberidx", ":", "torch", ".", "Tensor", ",", "\n", "passage_number_values", ":", "List", "[", "List", "[", "float", "]", "]", ",", "\n", "composed_numbers", ":", "List", "[", "List", "[", "float", "]", "]", ",", "\n", "add_number_combinations_indices", ":", "torch", ".", "LongTensor", ",", "\n", "sub_number_combinations_indices", ":", "torch", ".", "LongTensor", ",", "\n", "max_num_add_combs", ":", "List", "[", "int", "]", ",", "\n", "max_num_sub_combs", ":", "List", "[", "int", "]", ",", "\n", "passage_number_sortedtokenidxs", ":", "List", "[", "List", "[", "int", "]", "]", ",", "\n", "passageidx2dateidx", ":", "torch", ".", "LongTensor", ",", "\n", "passage_date_values", ":", "List", "[", "List", "[", "Date", "]", "]", ",", "\n", "actions", ":", "List", "[", "List", "[", "ProductionRule", "]", "]", ",", "\n", "year_differences", ":", "List", "[", "List", "[", "int", "]", "]", ",", "\n", "year_differences_mat", ":", "List", "[", "np", ".", "array", "]", ",", "\n", "count_values", ":", "List", "[", "List", "[", "int", "]", "]", ",", "\n", "answer_program_start_types", ":", "List", "[", "Union", "[", "List", "[", "str", "]", ",", "None", "]", "]", "=", "None", ",", "\n", "answer_as_passage_spans", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "answer_as_question_spans", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "answer_as_passage_number", ":", "List", "[", "List", "[", "int", "]", "]", "=", "None", ",", "\n", "answer_as_composed_number", ":", "List", "[", "List", "[", "int", "]", "]", "=", "None", ",", "\n", "answer_as_year_difference", ":", "List", "[", "List", "[", "int", "]", "]", "=", "None", ",", "\n", "answer_as_count", ":", "List", "[", "List", "[", "int", "]", "]", "=", "None", ",", "\n", "composed_num_ans_composition_types", ":", "List", "[", "Set", "[", "str", "]", "]", "=", "None", ",", "\n", "datecomp_ques_event_date_groundings", ":", "List", "[", "Tuple", "[", "List", "[", "int", "]", ",", "List", "[", "int", "]", "]", "]", "=", "None", ",", "\n", "numcomp_qspan_num_groundings", ":", "List", "[", "Tuple", "[", "List", "[", "int", "]", ",", "List", "[", "int", "]", "]", "]", "=", "None", ",", "\n", "strongly_supervised", ":", "List", "[", "bool", "]", "=", "None", ",", "\n", "program_supervised", ":", "List", "[", "bool", "]", "=", "None", ",", "\n", "qattn_supervised", ":", "List", "[", "bool", "]", "=", "None", ",", "\n", "pattn_supervised", ":", "List", "[", "bool", "]", "=", "None", ",", "\n", "execution_supervised", ":", "List", "[", "bool", "]", "=", "None", ",", "\n", "qtypes", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "gold_action_seqs", ":", "List", "[", "Tuple", "[", "List", "[", "List", "[", "int", "]", "]", ",", "List", "[", "List", "[", "int", "]", "]", "]", "]", "=", "None", ",", "\n", "qattn_supervision", ":", "torch", ".", "FloatTensor", "=", "None", ",", "\n", "passage_attn_supervision", ":", "List", "[", "List", "[", "float", "]", "]", "=", "None", ",", "\n", "synthetic_numground_metadata", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "=", "None", ",", "\n", "epoch_num", ":", "List", "[", "int", "]", "=", "None", ",", "\n", "metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "\n", "aux_passage_attention", "=", "None", ",", "\n", "aux_answer_as_count", "=", "None", ",", "\n", "aux_count_mask", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "if", "self", ".", "device_id", "is", "None", ":", "\n", "            ", "self", ".", "device_id", "=", "allenutil", ".", "get_device_of", "(", "self", ".", "qp_matrix_attention", ".", "_weight_vector", ".", "data", ")", "\n", "\n", "", "self", ".", "profile_steps", "+=", "1", "\n", "if", "self", ".", "profile_freq", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "profile_steps", "%", "self", ".", "profile_freq", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "Profile", ".", "to_string", "(", ")", ")", "\n", "\n", "", "", "question_passage_tokens", "=", "question_passage", "[", "\"tokens\"", "]", "\n", "pad_mask", "=", "question_passage", "[", "\"mask\"", "]", "\n", "segment_ids", "=", "question_passage", "[", "\"tokens-type-ids\"", "]", "\n", "\n", "# Padding \"[PAD]\" tokens in the question", "\n", "pad_mask", "=", "(", "question_passage_tokens", ">", "0", ")", ".", "long", "(", ")", "*", "pad_mask", "\n", "\n", "with", "Profile", "(", "scope_name", "=", "\"bert-run\"", ")", ":", "\n", "# Shape: (batch_size, seqlen, bert_dim); (batch_size, bert_dim)", "\n", "            ", "if", "not", "self", ".", "scaling_bert", ":", "\n", "                ", "bert_out", ",", "bert_pooled_out", "=", "self", ".", "BERT", "(", "\n", "question_passage_tokens", ",", "segment_ids", ",", "pad_mask", ",", "output_all_encoded_layers", "=", "False", "\n", ")", "\n", "", "else", ":", "\n", "                ", "bert_out", "=", "self", ".", "bert_token_embedder", "(", "input_ids", "=", "question_passage_tokens", ",", "token_type_ids", "=", "segment_ids", ")", "\n", "bert_pooled_out", "=", "bert_out", "[", ":", ",", "0", ",", ":", "]", "\n", "\n", "# Skip [CLS]; then the next max_ques_len tokens are question tokens", "\n", "", "", "encoded_question", "=", "bert_out", "[", ":", ",", "1", ":", "self", ".", "max_ques_len", "+", "1", ",", ":", "]", "\n", "question_mask", "=", "(", "pad_mask", "[", ":", ",", "1", ":", "self", ".", "max_ques_len", "+", "1", "]", ")", ".", "float", "(", ")", "\n", "# Skip [CLS] Q_tokens [SEP]", "\n", "encoded_passage", "=", "bert_out", "[", ":", ",", "1", "+", "self", ".", "max_ques_len", "+", "1", ":", ",", ":", "]", "\n", "passage_mask", "=", "(", "pad_mask", "[", ":", ",", "1", "+", "self", ".", "max_ques_len", "+", "1", ":", "]", ")", ".", "float", "(", ")", "\n", "passage_token_idxs", "=", "question_passage_tokens", "[", ":", ",", "1", "+", "self", ".", "max_ques_len", "+", "1", ":", "]", "\n", "\n", "batch_size", "=", "len", "(", "actions", ")", "\n", "\n", "if", "epoch_num", "is", "not", "None", ":", "\n", "# epoch_num in allennlp starts from 0", "\n", "            ", "epoch", "=", "epoch_num", "[", "0", "]", "+", "1", "\n", "", "else", ":", "\n", "            ", "epoch", "=", "None", "\n", "\n", "", "modeled_passage", "=", "encoded_passage", "\n", "passage_length", "=", "modeled_passage", ".", "size", "(", ")", "[", "1", "]", "\n", "\n", "question_passage_similarity", "=", "self", ".", "qp_matrix_attention", "(", "encoded_question", ",", "modeled_passage", ")", "\n", "passage_question_similarity", "=", "question_passage_similarity", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "question_passage_attention", "=", "allenutil", ".", "masked_softmax", "(", "\n", "question_passage_similarity", ",", "passage_mask", ".", "unsqueeze", "(", "1", ")", ",", "memory_efficient", "=", "True", "\n", ")", "\n", "\n", "passage_question_attention", "=", "allenutil", ".", "masked_softmax", "(", "\n", "passage_question_similarity", ",", "question_mask", ".", "unsqueeze", "(", "1", ")", ",", "memory_efficient", "=", "True", "\n", ")", "\n", "\n", "# Passage Token - Date Alignment", "\n", "# Shape: (batch_size, passage_length, passage_length)", "\n", "passage_passage_token2date_alignment", "=", "self", ".", "compute_token_symbol_alignments", "(", "\n", "modeled_passage", "=", "modeled_passage", ",", "\n", "passage_mask", "=", "passage_mask", ",", "\n", "passageidx2symbolidx", "=", "passageidx2dateidx", ",", "\n", "passage_to_symbol_attention_params", "=", "self", ".", "_executor_parameters", ".", "passage_to_date_attention", "\n", ")", "\n", "\n", "passage_passage_token2startdate_alignment", "=", "self", ".", "compute_token_symbol_alignments", "(", "\n", "modeled_passage", "=", "modeled_passage", ",", "\n", "passage_mask", "=", "passage_mask", ",", "\n", "passageidx2symbolidx", "=", "passageidx2dateidx", ",", "\n", "passage_to_symbol_attention_params", "=", "self", ".", "_executor_parameters", ".", "passage_to_start_date_attention", "\n", ")", "\n", "\n", "passage_passage_token2enddate_alignment", "=", "self", ".", "compute_token_symbol_alignments", "(", "\n", "modeled_passage", "=", "modeled_passage", ",", "\n", "passage_mask", "=", "passage_mask", ",", "\n", "passageidx2symbolidx", "=", "passageidx2dateidx", ",", "\n", "passage_to_symbol_attention_params", "=", "self", ".", "_executor_parameters", ".", "passage_to_end_date_attention", "\n", ")", "\n", "# Passage Token - Num Alignment", "\n", "passage_passage_token2num_alignment", "=", "self", ".", "compute_token_symbol_alignments", "(", "\n", "modeled_passage", "=", "modeled_passage", ",", "\n", "passage_mask", "=", "passage_mask", ",", "\n", "passageidx2symbolidx", "=", "passageidx2numberidx", ",", "\n", "passage_to_symbol_attention_params", "=", "self", ".", "_executor_parameters", ".", "passage_to_num_attention", "\n", ")", "\n", "# json_dicts = []", "\n", "# for i in range(batch_size):", "\n", "#     ques_tokens = metadata[i]['question_tokens']", "\n", "#     passage_tokens = metadata[i]['passage_tokens']", "\n", "#     p2num_sim = myutils.tocpuNPList(passage_passage_token2num_similarity[i])", "\n", "#     outdict = {'q': ques_tokens, 'p': passage_tokens, 'num': p2num_sim}", "\n", "#     json_dicts.append(outdict)", "\n", "# json.dump(json_dicts, num_attentions_f)", "\n", "# num_attentions_f.close()", "\n", "# exit()", "\n", "\"\"\" Aux Loss \"\"\"", "\n", "if", "self", ".", "auxwinloss", ":", "\n", "            ", "with", "Profile", "(", "\"win-mask\"", ")", ":", "\n", "                ", "inwindow_mask", ",", "outwindow_mask", "=", "self", ".", "masking_blockdiagonal", "(", "passage_length", ",", "15", ",", "self", ".", "device_id", ")", "\n", "", "with", "Profile", "(", "\"act-loss\"", ")", ":", "\n", "                ", "passage_tokenidx2numidx_mask", "=", "(", "passageidx2numberidx", ">", "-", "1", ")", ".", "float", "(", ")", "\n", "num_aux_loss", "=", "self", ".", "window_loss_numdate", "(", "\n", "passage_passage_token2num_alignment", ",", "passage_tokenidx2numidx_mask", ",", "inwindow_mask", ",", "outwindow_mask", "\n", ")", "\n", "\n", "passage_tokenidx2dateidx_mask", "=", "(", "passageidx2dateidx", ">", "-", "1", ")", ".", "float", "(", ")", "\n", "date_aux_loss", "=", "self", ".", "window_loss_numdate", "(", "\n", "passage_passage_token2date_alignment", ",", "passage_tokenidx2dateidx_mask", ",", "inwindow_mask", ",", "outwindow_mask", "\n", ")", "\n", "\n", "start_date_aux_loss", "=", "self", ".", "window_loss_numdate", "(", "\n", "passage_passage_token2startdate_alignment", ",", "passage_tokenidx2dateidx_mask", ",", "inwindow_mask", ",", "\n", "outwindow_mask", ")", "\n", "\n", "end_date_aux_loss", "=", "self", ".", "window_loss_numdate", "(", "\n", "passage_passage_token2enddate_alignment", ",", "passage_tokenidx2dateidx_mask", ",", "inwindow_mask", ",", "\n", "outwindow_mask", ")", "\n", "aux_win_loss", "=", "num_aux_loss", "+", "date_aux_loss", "+", "start_date_aux_loss", "+", "end_date_aux_loss", "\n", "", "", "else", ":", "\n", "            ", "aux_win_loss", "=", "0.0", "\n", "\n", "", "\"\"\" Parser setup \"\"\"", "\n", "# Shape: (B, encoding_dim)", "\n", "question_encoded_final_state", "=", "bert_pooled_out", "\n", "rawemb_question", "=", "encoded_question", "\n", "projected_embedded_question", "=", "encoded_question", "\n", "rawemb_passage", "=", "modeled_passage", "\n", "projected_embedded_passage", "=", "modeled_passage", "\n", "question_rawemb_aslist", "=", "[", "rawemb_question", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "question_embedded_aslist", "=", "[", "projected_embedded_question", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "question_encoded_aslist", "=", "[", "encoded_question", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "question_mask_aslist", "=", "[", "question_mask", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "passage_rawemb_aslist", "=", "[", "rawemb_passage", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "passage_embedded_aslist", "=", "[", "projected_embedded_passage", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "passage_encoded_aslist", "=", "[", "encoded_passage", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "passage_modeled_aslist", "=", "[", "modeled_passage", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "passage_mask_aslist", "=", "[", "passage_mask", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "q2p_attention_aslist", "=", "[", "question_passage_attention", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "p2q_attention_aslist", "=", "[", "passage_question_attention", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "# p2pdate_similarity_aslist = [passage_passage_token2date_similarity[i] for i in range(batch_size)]", "\n", "# p2pnum_similarity_aslist = [passage_passage_token2num_similarity[i] for i in range(batch_size)]", "\n", "p2pdate_alignment_aslist", "=", "[", "passage_passage_token2date_alignment", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "p2pstartdate_alignment_aslist", "=", "[", "passage_passage_token2startdate_alignment", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "p2penddate_alignment_aslist", "=", "[", "passage_passage_token2enddate_alignment", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "p2pnum_alignment_aslist", "=", "[", "passage_passage_token2num_alignment", "[", "i", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "# passage_token2datetoken_sim_aslist = [passage_token2datetoken_similarity[i] for i in range(batch_size)]", "\n", "size_composednums_aslist", "=", "[", "len", "(", "x", ")", "for", "x", "in", "composed_numbers", "]", "\n", "# Shape: (size_num_support_i, max_num_add_combs_i, 2) where _i is per instance", "\n", "add_num_combination_aslist", "=", "[", "\n", "add_number_combinations_indices", "[", "i", ",", "0", ":", "size_composednums_aslist", "[", "i", "]", ",", "0", ":", "max_num_add_combs", "[", "i", "]", ",", ":", "]", "\n", "for", "i", "in", "range", "(", "batch_size", ")", "\n", "]", "\n", "sub_num_combination_aslist", "=", "[", "\n", "sub_number_combinations_indices", "[", "i", ",", "0", ":", "size_composednums_aslist", "[", "i", "]", ",", "0", ":", "max_num_sub_combs", "[", "i", "]", ",", ":", "]", "\n", "for", "i", "in", "range", "(", "batch_size", ")", "\n", "]", "\n", "\n", "with", "Profile", "(", "\"lang_init\"", ")", ":", "\n", "            ", "languages", "=", "[", "\n", "DropLanguage", "(", "\n", "rawemb_question", "=", "question_rawemb_aslist", "[", "i", "]", ",", "\n", "embedded_question", "=", "question_embedded_aslist", "[", "i", "]", ",", "\n", "encoded_question", "=", "question_encoded_aslist", "[", "i", "]", ",", "\n", "rawemb_passage", "=", "passage_rawemb_aslist", "[", "i", "]", ",", "\n", "embedded_passage", "=", "passage_embedded_aslist", "[", "i", "]", ",", "\n", "encoded_passage", "=", "passage_encoded_aslist", "[", "i", "]", ",", "\n", "modeled_passage", "=", "passage_modeled_aslist", "[", "i", "]", ",", "\n", "question_mask", "=", "question_mask_aslist", "[", "i", "]", ",", "\n", "passage_mask", "=", "passage_mask", "[", "i", "]", ",", "# passage_mask_aslist[i],", "\n", "passage_sentence_boundaries", "=", "p_sentboundary_wps", "[", "i", "]", ",", "\n", "passage_tokenidx2dateidx", "=", "passageidx2dateidx", "[", "i", "]", ",", "\n", "passage_date_values", "=", "passage_date_values", "[", "i", "]", ",", "\n", "passage_tokenidx2numidx", "=", "passageidx2numberidx", "[", "i", "]", ",", "\n", "passage_num_values", "=", "passage_number_values", "[", "i", "]", ",", "\n", "composed_numbers", "=", "composed_numbers", "[", "i", "]", ",", "\n", "passage_number_sortedtokenidxs", "=", "passage_number_sortedtokenidxs", "[", "i", "]", ",", "\n", "add_num_combination_indices", "=", "add_num_combination_aslist", "[", "i", "]", ",", "\n", "sub_num_combination_indices", "=", "sub_num_combination_aslist", "[", "i", "]", ",", "\n", "year_differences", "=", "year_differences", "[", "i", "]", ",", "\n", "year_differences_mat", "=", "year_differences_mat", "[", "i", "]", ",", "\n", "count_num_values", "=", "count_values", "[", "i", "]", ",", "\n", "question_passage_attention", "=", "q2p_attention_aslist", "[", "i", "]", ",", "\n", "passage_question_attention", "=", "p2q_attention_aslist", "[", "i", "]", ",", "\n", "passage_token2date_alignment", "=", "p2pdate_alignment_aslist", "[", "i", "]", ",", "\n", "passage_token2startdate_alignment", "=", "p2pstartdate_alignment_aslist", "[", "i", "]", ",", "\n", "passage_token2enddate_alignment", "=", "p2penddate_alignment_aslist", "[", "i", "]", ",", "\n", "passage_token2num_alignment", "=", "p2pnum_alignment_aslist", "[", "i", "]", ",", "\n", "parameters", "=", "self", ".", "_executor_parameters", ",", "\n", "start_types", "=", "None", ",", "# batch_start_types[i],", "\n", "device_id", "=", "self", ".", "device_id", ",", "\n", "debug", "=", "self", ".", "_debug", ",", "\n", "metadata", "=", "metadata", "[", "i", "]", ",", "\n", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", "\n", "]", "\n", "\n", "# This works because all instance-languages have the same action space", "\n", "action2idx_map", "=", "{", "rule", ":", "i", "for", "i", ",", "rule", "in", "enumerate", "(", "languages", "[", "0", "]", ".", "all_possible_productions", "(", ")", ")", "}", "\n", "\n", "", "action_indices", "=", "list", "(", "action2idx_map", ".", "values", "(", ")", ")", "\n", "batch_action_indices", ":", "List", "[", "List", "[", "int", "]", "]", "=", "[", "action_indices", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "\"\"\"\n        While training, we know the correct start-types for all instances and the gold-programs for some.\n        For instances,\n            #   with gold-programs, we should run a ConstrainedBeamSearch with target_sequences,\n            #   with start-types, figure out the valid start-action-ids and run ConstrainedBeamSearch with firststep_allo..\n        During Validation, we should **always** be running an un-constrained BeamSearch on the full language\n        \"\"\"", "\n", "\n", "# print(qtypes)", "\n", "# print(program_supervised)", "\n", "# print(gold_action_seqs)", "\n", "\n", "mml_loss", "=", "0", "\n", "with", "Profile", "(", "\"beam-sear\"", ")", ":", "\n", "            ", "if", "self", ".", "training", ":", "\n", "# If any instance is provided with goldprog, we need to divide the batch into supervised / unsupervised", "\n", "# and run fully-constrained decoding on supervised, and start-type-constrained-decoding on the rest", "\n", "                ", "if", "any", "(", "program_supervised", ")", ":", "\n", "                    ", "supervised_instances", "=", "[", "i", "for", "(", "i", ",", "ss", ")", "in", "enumerate", "(", "program_supervised", ")", "if", "ss", "is", "True", "]", "\n", "unsupervised_instances", "=", "[", "i", "for", "(", "i", ",", "ss", ")", "in", "enumerate", "(", "program_supervised", ")", "if", "ss", "is", "False", "]", "\n", "\n", "# List of (gold_actionseq_idxs, gold_actionseq_masks) -- for supervised instances", "\n", "supervised_gold_actionseqs", "=", "self", ".", "_select_indices_from_list", "(", "gold_action_seqs", ",", "supervised_instances", ")", "\n", "s_gold_actionseq_idxs", ",", "s_gold_actionseq_masks", "=", "zip", "(", "*", "supervised_gold_actionseqs", ")", "\n", "s_gold_actionseq_idxs", "=", "list", "(", "s_gold_actionseq_idxs", ")", "\n", "s_gold_actionseq_masks", "=", "list", "(", "s_gold_actionseq_masks", ")", "\n", "(", "supervised_initial_state", ",", "_", ",", "_", ")", "=", "self", ".", "initialState_forInstanceIndices", "(", "\n", "supervised_instances", ",", "\n", "languages", ",", "\n", "actions", ",", "\n", "encoded_question", ",", "\n", "question_mask", ",", "\n", "question_encoded_final_state", ",", "\n", "question_encoded_aslist", ",", "\n", "question_mask_aslist", ",", "\n", ")", "\n", "constrained_search", "=", "ConstrainedBeamSearch", "(", "\n", "self", ".", "_beam_size", ",", "\n", "allowed_sequences", "=", "s_gold_actionseq_idxs", ",", "\n", "allowed_sequence_mask", "=", "s_gold_actionseq_masks", ",", "\n", ")", "\n", "\n", "supervised_final_states", "=", "constrained_search", ".", "search", "(", "\n", "initial_state", "=", "supervised_initial_state", ",", "transition_function", "=", "self", ".", "_decoder_step", "\n", ")", "\n", "# Computing MML Loss", "\n", "for", "instance_states", "in", "supervised_final_states", ".", "values", "(", ")", ":", "\n", "                        ", "scores", "=", "[", "state", ".", "score", "[", "0", "]", ".", "view", "(", "-", "1", ")", "for", "state", "in", "instance_states", "]", "\n", "mml_loss", "+=", "-", "allenutil", ".", "logsumexp", "(", "torch", ".", "cat", "(", "scores", ")", ")", "\n", "", "mml_loss", "=", "mml_loss", "/", "len", "(", "supervised_final_states", ")", "\n", "\n", "if", "len", "(", "unsupervised_instances", ")", ">", "0", ":", "\n", "                        ", "(", "unsupervised_initial_state", ",", "_", ",", "_", ")", "=", "self", ".", "initialState_forInstanceIndices", "(", "\n", "unsupervised_instances", ",", "\n", "languages", ",", "\n", "actions", ",", "\n", "encoded_question", ",", "\n", "question_mask", ",", "\n", "question_encoded_final_state", ",", "\n", "question_encoded_aslist", ",", "\n", "question_mask_aslist", ",", "\n", ")", "\n", "\n", "unsupervised_answer_types", ":", "List", "[", "List", "[", "str", "]", "]", "=", "self", ".", "_select_indices_from_list", "(", "\n", "answer_program_start_types", ",", "unsupervised_instances", "\n", ")", "\n", "unsupervised_numcomposition_types", ":", "List", "[", "Set", "[", "str", "]", "]", "=", "self", ".", "_select_indices_from_list", "(", "\n", "composed_num_ans_composition_types", ",", "unsupervised_instances", "\n", ")", "\n", "unsupervised_action_indices", ":", "List", "[", "List", "[", "int", "]", "]", "=", "self", ".", "_select_indices_from_list", "(", "\n", "batch_action_indices", ",", "unsupervised_instances", ")", "\n", "\n", "# unsupervised_ins_start_actionids: List[Set[int]] = \\", "\n", "unsupervised_action_prefixes", ":", "List", "[", "List", "[", "List", "[", "int", "]", "]", "]", "=", "self", ".", "get_valid_start_actionids", "(", "\n", "answer_types", "=", "unsupervised_answer_types", ",", "\n", "action2actionidx", "=", "action2idx_map", ",", "\n", "valid_numcomposition_types", "=", "unsupervised_numcomposition_types", ",", "\n", ")", "\n", "\n", "prefixed_beam_search", "=", "PrefixedConstrainedBeamSearch", "(", "\n", "beam_size", "=", "self", ".", "_beam_size", ",", "allowed_sequences", "=", "unsupervised_action_prefixes", ",", "\n", "all_action_indices", "=", "unsupervised_action_indices", ")", "\n", "\n", "unsup_final_states", "=", "prefixed_beam_search", ".", "search", "(", "\n", "initial_state", "=", "unsupervised_initial_state", ",", "\n", "transition_function", "=", "self", ".", "_decoder_step", ",", "\n", "num_steps", "=", "self", ".", "_max_decoding_steps", ",", "\n", "keep_final_unfinished_states", "=", "False", ",", "\n", ")", "\n", "", "else", ":", "\n", "                        ", "unsup_final_states", "=", "[", "]", "\n", "\n", "# Merge final_states for supervised and unsupervised instances", "\n", "", "best_final_states", "=", "self", ".", "merge_final_states", "(", "\n", "supervised_final_states", ",", "unsup_final_states", ",", "supervised_instances", ",", "unsupervised_instances", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "(", "initial_state", ",", "_", ",", "_", ")", "=", "self", ".", "getInitialDecoderState", "(", "\n", "languages", ",", "\n", "actions", ",", "\n", "encoded_question", ",", "\n", "question_mask", ",", "\n", "question_encoded_final_state", ",", "\n", "question_encoded_aslist", ",", "\n", "question_mask_aslist", ",", "\n", "batch_size", ",", "\n", ")", "\n", "batch_action_prefixes", ":", "List", "[", "List", "[", "List", "[", "int", "]", "]", "]", "=", "self", ".", "get_valid_start_actionids", "(", "\n", "answer_types", "=", "answer_program_start_types", ",", "\n", "action2actionidx", "=", "action2idx_map", ",", "\n", "valid_numcomposition_types", "=", "composed_num_ans_composition_types", ",", "\n", ")", "\n", "prefixed_beam_search", "=", "PrefixedConstrainedBeamSearch", "(", "\n", "beam_size", "=", "self", ".", "_beam_size", ",", "allowed_sequences", "=", "batch_action_prefixes", ",", "\n", "all_action_indices", "=", "batch_action_indices", ")", "\n", "# Mapping[int, Sequence[StateType]])", "\n", "best_final_states", "=", "prefixed_beam_search", ".", "search", "(", "\n", "initial_state", "=", "initial_state", ",", "\n", "transition_function", "=", "self", ".", "_decoder_step", ",", "\n", "num_steps", "=", "self", ".", "_max_decoding_steps", ",", "\n", "keep_final_unfinished_states", "=", "False", ",", "\n", ")", "\n", "# Prediction Mode", "\n", "", "", "elif", "not", "self", ".", "interpret", ":", "\n", "                ", "(", "initial_state", ",", "_", ",", "_", ")", "=", "self", ".", "getInitialDecoderState", "(", "\n", "languages", ",", "\n", "actions", ",", "\n", "encoded_question", ",", "\n", "question_mask", ",", "\n", "question_encoded_final_state", ",", "\n", "question_encoded_aslist", ",", "\n", "question_mask_aslist", ",", "\n", "batch_size", ",", "\n", ")", "\n", "# This is unconstrained beam-search", "\n", "best_final_states", "=", "self", ".", "_decoder_beam_search", ".", "search", "(", "\n", "self", ".", "_max_decoding_steps", ",", "initial_state", ",", "self", ".", "_decoder_step", ",", "keep_final_unfinished_states", "=", "False", "\n", ")", "\n", "# Running constrained decoding for dev-set assuming that all dev examples have gold-program labels", "\n", "", "else", ":", "\n", "                ", "assert", "self", ".", "training", "is", "False", "and", "self", ".", "interpret", "is", "True", "\n", "assert", "all", "(", "program_supervised", ")", "\n", "assert", "all", "(", "[", "x", "is", "not", "\"UNK\"", "for", "x", "in", "qtypes", "]", ")", "\n", "actionseq_idxs", ",", "actionseq_masks", "=", "zip", "(", "*", "gold_action_seqs", ")", "\n", "actionseq_idxs", "=", "list", "(", "actionseq_idxs", ")", "\n", "actionseq_masks", "=", "list", "(", "actionseq_masks", ")", "\n", "\n", "(", "initial_state", ",", "_", ",", "_", ")", "=", "self", ".", "getInitialDecoderState", "(", "\n", "languages", ",", "\n", "actions", ",", "\n", "encoded_question", ",", "\n", "question_mask", ",", "\n", "question_encoded_final_state", ",", "\n", "question_encoded_aslist", ",", "\n", "question_mask_aslist", ",", "\n", "batch_size", ",", "\n", ")", "\n", "constrained_search", "=", "ConstrainedBeamSearch", "(", "\n", "self", ".", "_beam_size", ",", "\n", "allowed_sequences", "=", "actionseq_idxs", ",", "\n", "allowed_sequence_mask", "=", "actionseq_masks", ",", "\n", ")", "\n", "\n", "best_final_states", "=", "constrained_search", ".", "search", "(", "\n", "initial_state", "=", "initial_state", ",", "transition_function", "=", "self", ".", "_decoder_step", "\n", ")", "\n", "\n", "# batch_actionidxs: List[List[List[int]]]: All action sequence indices for each instance in the batch", "\n", "# batch_actionseqs: List[List[List[str]]]: All decoded action sequences for each instance in the batch", "\n", "# batch_actionseq_scores: List[List[torch.Tensor]]: Score for each program of each instance", "\n", "# batch_actionseq_probs: List[torch.FloatTensor]: Tensor containing normalized_prog_probs for each instance - no longer", "\n", "# batch_actionseq_sideargs: List[List[List[Dict]]]: List of side_args for each program of each instance", "\n", "# The actions here should be in the exact same order as passed when creating the initial_grammar_state ...", "\n", "# since the action_ids are assigned based on the order passed there.", "\n", "", "(", "\n", "batch_actionidxs", ",", "\n", "batch_actionseqs", ",", "\n", "batch_actionseq_logprobs", ",", "\n", "batch_actionseq_sideargs", ",", "\n", ")", "=", "semparse_utils", ".", "_convert_finalstates_to_actions", "(", "\n", "best_final_states", "=", "best_final_states", ",", "possible_actions", "=", "actions", ",", "batch_size", "=", "batch_size", "\n", ")", "\n", "batch_actionseq_probs", "=", "[", "[", "torch", ".", "exp", "(", "logprob", ")", "for", "logprob", "in", "instance_programs", "]", "\n", "for", "instance_programs", "in", "batch_actionseq_logprobs", "]", "\n", "\n", "# Adding Date-Comparison supervised event groundings to relevant actions", "\n", "max_passage_len", "=", "encoded_passage", ".", "size", "(", ")", "[", "1", "]", "\n", "self", ".", "passage_attention_to_sidearg", "(", "\n", "qtypes", ",", "\n", "batch_actionseqs", ",", "\n", "batch_actionseq_sideargs", ",", "\n", "pattn_supervised", ",", "\n", "passage_attn_supervision", ",", "\n", "max_passage_len", ",", "\n", "self", ".", "device_id", ",", "\n", ")", "\n", "\n", "self", ".", "datecompare_eventdategr_to_sideargs", "(", "\n", "qtypes", ",", "batch_actionseqs", ",", "batch_actionseq_sideargs", ",", "datecomp_ques_event_date_groundings", ",", "self", ".", "device_id", "\n", ")", "\n", "\n", "self", ".", "numcompare_eventnumgr_to_sideargs", "(", "\n", "qtypes", ",", "\n", "execution_supervised", ",", "\n", "batch_actionseqs", ",", "\n", "batch_actionseq_sideargs", ",", "\n", "numcomp_qspan_num_groundings", ",", "\n", "self", ".", "device_id", ",", "\n", ")", "\n", "\n", "# # PRINT PRED PROGRAMS", "\n", "# for idx, instance_progs in enumerate(batch_actionseqs):", "\n", "#     print(f\"--------  InstanceIdx: {idx}  ----------\")", "\n", "#     print(metadata[idx][\"original_question\"])", "\n", "#     probs = batch_actionseq_probs[idx]", "\n", "#     logprobs = batch_actionseq_logprobs[idx]", "\n", "#     start_types = answer_program_start_types[idx]", "\n", "#     numcomposition_types = composed_num_ans_composition_types[idx]", "\n", "#     print(f\"AnsTypes: {start_types} \\t CompositionFunctions:{numcomposition_types}\")", "\n", "#     for prog, prob, logprob in zip(instance_progs, probs, logprobs):", "\n", "#         print(f\"{prob}: {languages[idx].action_sequence_to_logical_form(prog)} -- {logprob}\")", "\n", "# print()", "\n", "# import pdb", "\n", "# pdb.set_trace()", "\n", "\n", "", "with", "Profile", "(", "\"get-deno\"", ")", ":", "\n", "# List[List[Any]], List[List[str]]: Denotations and their types for all instances", "\n", "            ", "batch_denotations", ",", "batch_denotation_types", "=", "self", ".", "_get_denotations", "(", "\n", "batch_actionseqs", ",", "languages", ",", "batch_actionseq_sideargs", "\n", ")", "\n", "\n", "", "output_dict", "=", "{", "}", "\n", "# Computing losses if gold answers are given", "\n", "if", "answer_program_start_types", "is", "not", "None", ":", "\n", "# Execution losses --", "\n", "            ", "total_aux_loss", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "tensor", "(", "0.0", ")", ",", "self", ".", "device_id", ")", ".", "float", "(", ")", "\n", "\n", "total_aux_loss", "+=", "aux_win_loss", "\n", "if", "aux_win_loss", "!=", "0", ":", "\n", "                ", "self", ".", "auxwinloss_metric", "(", "aux_win_loss", ".", "item", "(", ")", ")", "\n", "\n", "", "if", "self", ".", "excloss", ":", "\n", "                ", "exec_loss", "=", "0.0", "\n", "batch_exec_loss", "=", "0.0", "\n", "execloss_normalizer", "=", "0.0", "\n", "for", "ins_dens", "in", "batch_denotations", ":", "\n", "                    ", "for", "den", "in", "ins_dens", ":", "\n", "                        ", "execloss_normalizer", "+=", "1.0", "\n", "exec_loss", "+=", "den", ".", "loss", "\n", "", "", "if", "execloss_normalizer", ">", "0", ":", "\n", "                    ", "batch_exec_loss", "=", "exec_loss", "/", "execloss_normalizer", "\n", "# This check is made explicit here since not all batches have this loss, hence a 0.0 value", "\n", "# only bloats the denominator in the metric. This is also done for other losses in below", "\n", "", "if", "batch_exec_loss", "!=", "0.0", ":", "\n", "                    ", "self", ".", "excloss_metric", "(", "batch_exec_loss", ".", "item", "(", ")", ")", "\n", "", "total_aux_loss", "+=", "batch_exec_loss", "\n", "\n", "", "if", "self", ".", "qattloss", ":", "\n", "# Compute Question Attention Supervision auxiliary loss", "\n", "                ", "qattn_loss", "=", "self", ".", "_ques_attention_loss", "(", "\n", "batch_actionseqs", ",", "batch_actionseq_sideargs", ",", "qtypes", ",", "qattn_supervised", ",", "qattn_supervision", "\n", ")", "\n", "if", "qattn_loss", "!=", "0.0", ":", "\n", "                    ", "self", ".", "qattloss_metric", "(", "qattn_loss", ".", "item", "(", ")", ")", "\n", "", "total_aux_loss", "+=", "qattn_loss", "\n", "", "if", "self", ".", "mmlloss", ":", "\n", "# This is computed above during beam search", "\n", "                ", "if", "mml_loss", "!=", "0.0", ":", "\n", "                    ", "self", ".", "mmlloss_metric", "(", "mml_loss", ".", "item", "(", ")", ")", "\n", "", "total_aux_loss", "+=", "mml_loss", "\n", "\n", "", "if", "torch", ".", "isnan", "(", "total_aux_loss", ")", ":", "\n", "                ", "logger", ".", "info", "(", "f\"TotalAuxLoss is nan.\"", ")", "\n", "total_aux_loss", "=", "0.0", "\n", "\n", "", "total_denotation_loss", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "tensor", "(", "0.0", ")", ",", "self", ".", "device_id", ")", "\n", "batch_ansprob_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "# Programs for an instance can be of multiple types;", "\n", "# For each program, based on it's return type, we compute the log-likelihood", "\n", "# against the appropriate gold-answer and add it to the instance_log_likelihood_list", "\n", "# This is then weighed by the program-log-likelihood and added to the batch_loss", "\n", "\n", "                ", "instance_prog_denotations", ",", "instance_prog_types", "=", "(", "batch_denotations", "[", "i", "]", ",", "batch_denotation_types", "[", "i", "]", ")", "\n", "instance_progs_logprob_list", "=", "batch_actionseq_logprobs", "[", "i", "]", "\n", "\n", "# This instance does not have completed programs that were found in beam-search", "\n", "if", "len", "(", "instance_prog_denotations", ")", "==", "0", ":", "\n", "                    ", "continue", "\n", "\n", "", "instance_log_likelihood_list", "=", "[", "]", "\n", "# new_instance_progs_logprob_list = []", "\n", "for", "progidx", "in", "range", "(", "len", "(", "instance_prog_denotations", ")", ")", ":", "\n", "                    ", "denotation", "=", "instance_prog_denotations", "[", "progidx", "]", "\n", "progtype", "=", "instance_prog_types", "[", "progidx", "]", "\n", "prog_logprob", "=", "instance_progs_logprob_list", "[", "progidx", "]", "\n", "\n", "if", "progtype", "==", "\"PassageSpanAnswer\"", ":", "\n", "# Tuple of start, end log_probs", "\n", "                        ", "log_likelihood", "=", "self", ".", "_get_span_answer_log_prob", "(", "\n", "answer_as_spans", "=", "answer_as_passage_spans", "[", "i", "]", ",", "span_log_probs", "=", "denotation", ".", "_value", "\n", ")", "\n", "", "elif", "progtype", "==", "\"QuestionSpanAnswer\"", ":", "\n", "# Tuple of start, end log_probs", "\n", "                        ", "log_likelihood", "=", "self", ".", "_get_span_answer_log_prob", "(", "\n", "answer_as_spans", "=", "answer_as_question_spans", "[", "i", "]", ",", "span_log_probs", "=", "denotation", ".", "_value", "\n", ")", "\n", "", "elif", "progtype", "==", "\"YearDifference\"", ":", "\n", "# Distribution over year_differences", "\n", "                        ", "pred_year_difference_dist", "=", "denotation", ".", "_value", "\n", "pred_year_diff_log_probs", "=", "torch", ".", "log", "(", "pred_year_difference_dist", "+", "1e-40", ")", "\n", "gold_year_difference_dist", "=", "allenutil", ".", "move_to_device", "(", "\n", "torch", ".", "FloatTensor", "(", "answer_as_year_difference", "[", "i", "]", ")", ",", "cuda_device", "=", "self", ".", "device_id", "\n", ")", "\n", "log_likelihood", "=", "torch", ".", "sum", "(", "pred_year_diff_log_probs", "*", "gold_year_difference_dist", ")", "\n", "", "elif", "progtype", "==", "\"PassageNumber\"", ":", "\n", "# Distribution over PassageNumbers", "\n", "                        ", "pred_passagenumber_dist", "=", "denotation", ".", "_value", "\n", "pred_passagenumber_logprobs", "=", "torch", ".", "log", "(", "pred_passagenumber_dist", "+", "1e-40", ")", "\n", "gold_passagenum_dist", "=", "allenutil", ".", "move_to_device", "(", "\n", "torch", ".", "FloatTensor", "(", "answer_as_passage_number", "[", "i", "]", ")", ",", "cuda_device", "=", "self", ".", "device_id", "\n", ")", "\n", "log_likelihood", "=", "torch", ".", "sum", "(", "pred_passagenumber_logprobs", "*", "gold_passagenum_dist", ")", "\n", "", "elif", "progtype", "==", "\"ComposedNumber\"", ":", "\n", "# Distribution over ComposedNumbers", "\n", "                        ", "pred_composednumber_dist", "=", "denotation", ".", "_value", "\n", "pred_composednumber_logprobs", "=", "torch", ".", "log", "(", "pred_composednumber_dist", "+", "1e-40", ")", "\n", "gold_composednum_dist", "=", "allenutil", ".", "move_to_device", "(", "\n", "torch", ".", "FloatTensor", "(", "answer_as_composed_number", "[", "i", "]", ")", ",", "cuda_device", "=", "self", ".", "device_id", "\n", ")", "\n", "log_likelihood", "=", "torch", ".", "sum", "(", "pred_composednumber_logprobs", "*", "gold_composednum_dist", ")", "\n", "", "elif", "progtype", "==", "\"CountNumber\"", ":", "\n", "                        ", "count_distribution", "=", "denotation", ".", "_value", "\n", "count_log_probs", "=", "torch", ".", "log", "(", "count_distribution", "+", "1e-40", ")", "\n", "gold_count_distribution", "=", "allenutil", ".", "move_to_device", "(", "\n", "torch", ".", "FloatTensor", "(", "answer_as_count", "[", "i", "]", ")", ",", "cuda_device", "=", "self", ".", "device_id", "\n", ")", "\n", "log_likelihood", "=", "torch", ".", "sum", "(", "count_log_probs", "*", "gold_count_distribution", ")", "\n", "# Here implement losses for other program-return-types in else-ifs", "\n", "", "else", ":", "\n", "                        ", "raise", "NotImplementedError", "\n", "", "if", "torch", ".", "isnan", "(", "log_likelihood", ")", ":", "\n", "                        ", "logger", ".", "info", "(", "f\"Nan-loss encountered for denotation_log_likelihood\"", ")", "\n", "log_likelihood", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "tensor", "(", "0.0", ")", ",", "self", ".", "device_id", ")", "\n", "", "if", "torch", ".", "isnan", "(", "prog_logprob", ")", ":", "\n", "                        ", "logger", ".", "info", "(", "f\"Nan-loss encountered for ProgType: {progtype}\"", ")", "\n", "prog_logprob", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "tensor", "(", "0.0", ")", ",", "self", ".", "device_id", ")", "\n", "\n", "# new_instance_progs_logprob_list.append(prog_logprob)", "\n", "", "instance_log_likelihood_list", ".", "append", "(", "log_likelihood", ")", "\n", "\n", "", "instance_ansprob_list", "=", "[", "torch", ".", "exp", "(", "x", ")", "for", "x", "in", "instance_log_likelihood_list", "]", "\n", "batch_ansprob_list", ".", "append", "(", "instance_ansprob_list", ")", "\n", "# Each is the shape of (number_of_progs,)", "\n", "# tensor of log p(y_i|z_i)", "\n", "instance_denotation_log_likelihoods", "=", "torch", ".", "stack", "(", "instance_log_likelihood_list", ",", "dim", "=", "-", "1", ")", "\n", "# print(f\"{i}: {instance_log_likelihood_list}\")", "\n", "\n", "# tensor of log p(z_i|x)", "\n", "instance_progs_log_probs", "=", "torch", ".", "stack", "(", "instance_progs_logprob_list", ",", "dim", "=", "-", "1", ")", "\n", "# instance_progs_log_probs = torch.stack(new_instance_progs_logprob_list, dim=-1)", "\n", "# tensor of \\log(p(y_i|z_i) * p(z_i|x))", "\n", "allprogs_log_marginal_likelihoods", "=", "instance_denotation_log_likelihoods", "+", "instance_progs_log_probs", "\n", "if", "self", ".", "hardem_epoch", "is", "not", "None", "and", "epoch", "is", "not", "None", "and", "epoch", ">=", "(", "self", ".", "hardem_epoch", "+", "1", ")", ":", "\n", "                    ", "max_prg_idx", "=", "torch", ".", "argmax", "(", "allprogs_log_marginal_likelihoods", ")", "\n", "instance_marginal_log_likelihood", "=", "allprogs_log_marginal_likelihoods", "[", "max_prg_idx", "]", "\n", "", "else", ":", "\n", "# tensor of \\log[\\sum_i \\exp (log(p(y_i|z_i) * p(z_i|x)))] = \\log[\\sum_i p(y_i|z_i) * p(z_i|x)]", "\n", "                    ", "instance_marginal_log_likelihood", "=", "allenutil", ".", "logsumexp", "(", "allprogs_log_marginal_likelihoods", ")", "\n", "# Added sum to remove empty-dim", "\n", "", "instance_marginal_log_likelihood", "=", "torch", ".", "sum", "(", "instance_marginal_log_likelihood", ")", "\n", "if", "torch", ".", "isnan", "(", "instance_marginal_log_likelihood", ")", ":", "\n", "                    ", "logger", ".", "info", "(", "f\"Nan-loss encountered for instance_marginal_log_likelihood\"", ")", "\n", "logger", ".", "info", "(", "f\"Prog log probs: {instance_progs_log_probs}\"", ")", "\n", "logger", ".", "info", "(", "f\"Instance denotation likelihoods: {instance_denotation_log_likelihoods}\"", ")", "\n", "\n", "instance_marginal_log_likelihood", "=", "0.0", "\n", "", "total_denotation_loss", "+=", "-", "1.0", "*", "instance_marginal_log_likelihood", "\n", "\n", "if", "torch", ".", "isnan", "(", "total_denotation_loss", ")", ":", "\n", "                    ", "total_denotation_loss", "=", "0.0", "\n", "\n", "", "", "batch_denotation_loss", "=", "total_denotation_loss", "/", "batch_size", "\n", "self", ".", "modelloss_metric", "(", "batch_denotation_loss", ".", "item", "(", ")", ")", "\n", "output_dict", "[", "\"loss\"", "]", "=", "batch_denotation_loss", "+", "total_aux_loss", "\n", "\n", "", "\"\"\" DEBUG\n        if self.training:\n            self.num_train_steps += 1\n            if self.num_train_steps % self.log_freq == 0:\n                for idx in range(batch_size):\n                    question = metadata[idx][\"original_question\"]\n                    action_seqs = batch_actionseqs[idx]\n                    probs = batch_actionseq_probs[idx]\n                    ans_probs = batch_ansprob_list[idx]\n                    self.logfile.write(f\"{self.num_train_steps}\\t{question}\\n\")\n                    for acseq, prob, ansp in zip(action_seqs, probs, ans_probs):\n                        lf = languages[idx].action_sequence_to_logical_form(acseq)\n                        self.logfile.write(f\"{prob}\\t{ansp}\\t{lf}\\n\")\n                    self.logfile.write(\"\\n\")\n\n            if self.num_train_steps % 5000 == 0:\n                print(Profile.to_string())\n            if self.num_train_steps >= self.num_log_steps:\n                self.logfile.close()\n                print(Profile.to_string())\n                exit()\n        END DEBUG \"\"\"", "\n", "\n", "\n", "# Get the predicted answers irrespective of loss computation.", "\n", "# For each program, given it's return type compute the predicted answer string", "\n", "if", "metadata", "is", "not", "None", ":", "\n", "            ", "output_dict", "[", "\"predicted_answer\"", "]", "=", "[", "]", "\n", "output_dict", "[", "\"all_predicted_answers\"", "]", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "                ", "original_question", "=", "metadata", "[", "i", "]", "[", "\"original_question\"", "]", "\n", "original_passage", "=", "metadata", "[", "i", "]", "[", "\"original_passage\"", "]", "\n", "question_token_offsets", "=", "metadata", "[", "i", "]", "[", "\"question_token_offsets\"", "]", "\n", "passage_token_offsets", "=", "metadata", "[", "i", "]", "[", "\"passage_token_offsets\"", "]", "\n", "instance_year_differences", "=", "year_differences", "[", "i", "]", "\n", "instance_passage_numbers", "=", "passage_number_values", "[", "i", "]", "\n", "instance_composed_numbers", "=", "composed_numbers", "[", "i", "]", "\n", "instance_count_values", "=", "count_values", "[", "i", "]", "\n", "instance_prog_denotations", ",", "instance_prog_types", "=", "(", "batch_denotations", "[", "i", "]", ",", "batch_denotation_types", "[", "i", "]", ")", "\n", "\n", "all_instance_progs_predicted_answer_strs", ":", "List", "[", "str", "]", "=", "[", "]", "# List of answers from diff instance progs", "\n", "for", "progidx", "in", "range", "(", "len", "(", "instance_prog_denotations", ")", ")", ":", "\n", "                    ", "denotation", "=", "instance_prog_denotations", "[", "progidx", "]", "\n", "progtype", "=", "instance_prog_types", "[", "progidx", "]", "\n", "if", "progtype", "==", "\"PassageSpanAnswer\"", ":", "\n", "# Tuple of start, end log_probs", "\n", "# Shape: (2, ) -- start / end token ids", "\n", "                        ", "best_span", "=", "get_best_span", "(", "\n", "span_start_logits", "=", "denotation", ".", "_value", "[", "0", "]", ".", "unsqueeze", "(", "0", ")", ",", "\n", "span_end_logits", "=", "denotation", ".", "_value", "[", "1", "]", ".", "unsqueeze", "(", "0", ")", ",", "\n", ")", ".", "squeeze", "(", "0", ")", "\n", "predicted_span", "=", "tuple", "(", "best_span", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "start_char_offset", "=", "passage_token_offsets", "[", "predicted_span", "[", "0", "]", "]", "[", "0", "]", "\n", "end_char_offset", "=", "passage_token_offsets", "[", "predicted_span", "[", "1", "]", "]", "[", "1", "]", "\n", "predicted_answer", "=", "original_passage", "[", "start_char_offset", ":", "end_char_offset", "]", "\n", "", "elif", "progtype", "==", "\"QuestionSpanAnswer\"", ":", "\n", "# Tuple of start, end log_probs", "\n", "# Shape: (2, ) -- start / end token ids", "\n", "                        ", "best_span", "=", "get_best_span", "(", "\n", "span_start_logits", "=", "denotation", ".", "_value", "[", "0", "]", ".", "unsqueeze", "(", "0", ")", ",", "\n", "span_end_logits", "=", "denotation", ".", "_value", "[", "1", "]", ".", "unsqueeze", "(", "0", ")", ",", "\n", ")", ".", "squeeze", "(", "0", ")", "\n", "\n", "predicted_span", "=", "tuple", "(", "best_span", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "start_char_offset", "=", "question_token_offsets", "[", "predicted_span", "[", "0", "]", "]", "[", "0", "]", "\n", "end_char_offset", "=", "question_token_offsets", "[", "predicted_span", "[", "1", "]", "]", "[", "1", "]", "\n", "predicted_answer", "=", "original_question", "[", "start_char_offset", ":", "end_char_offset", "]", "\n", "", "elif", "progtype", "==", "\"YearDifference\"", ":", "\n", "# Distribution over year_differences vector", "\n", "                        ", "year_differences_dist", "=", "denotation", ".", "_value", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "predicted_yeardiff_idx", "=", "np", ".", "argmax", "(", "year_differences_dist", ")", "\n", "# If not predicting year_diff = 0", "\n", "# if predicted_yeardiff_idx == 0 and len(instance_year_differences) > 1:", "\n", "#     predicted_yeardiff_idx = np.argmax(year_differences_dist[1:])", "\n", "#     predicted_yeardiff_idx += 1", "\n", "predicted_year_difference", "=", "instance_year_differences", "[", "predicted_yeardiff_idx", "]", "# int", "\n", "predicted_answer", "=", "str", "(", "predicted_year_difference", ")", "\n", "", "elif", "progtype", "==", "\"PassageNumber\"", ":", "\n", "                        ", "predicted_passagenum_idx", "=", "torch", ".", "argmax", "(", "denotation", ".", "_value", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "predicted_passage_number", "=", "instance_passage_numbers", "[", "predicted_passagenum_idx", "]", "# int/float", "\n", "predicted_passage_number", "=", "(", "\n", "int", "(", "predicted_passage_number", ")", "\n", "if", "int", "(", "predicted_passage_number", ")", "==", "predicted_passage_number", "\n", "else", "predicted_passage_number", "\n", ")", "\n", "predicted_answer", "=", "str", "(", "predicted_passage_number", ")", "\n", "", "elif", "progtype", "==", "\"ComposedNumber\"", ":", "\n", "                        ", "predicted_composednum_idx", "=", "torch", ".", "argmax", "(", "denotation", ".", "_value", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "predicted_composed_number", "=", "instance_composed_numbers", "[", "predicted_composednum_idx", "]", "# int/float", "\n", "predicted_composed_number", "=", "(", "\n", "int", "(", "predicted_composed_number", ")", "\n", "if", "int", "(", "predicted_composed_number", ")", "==", "predicted_composed_number", "\n", "else", "predicted_composed_number", "\n", ")", "\n", "predicted_answer", "=", "str", "(", "predicted_composed_number", ")", "\n", "", "elif", "progtype", "==", "\"CountNumber\"", ":", "\n", "                        ", "denotation", ":", "CountNumber", "=", "denotation", "\n", "count_idx", "=", "torch", ".", "argmax", "(", "denotation", ".", "_value", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "predicted_count_answer", "=", "instance_count_values", "[", "count_idx", "]", "\n", "predicted_count_answer", "=", "(", "\n", "int", "(", "predicted_count_answer", ")", "\n", "if", "int", "(", "predicted_count_answer", ")", "==", "predicted_count_answer", "\n", "else", "predicted_count_answer", "\n", ")", "\n", "predicted_answer", "=", "str", "(", "predicted_count_answer", ")", "\n", "\n", "", "else", ":", "\n", "                        ", "raise", "NotImplementedError", "\n", "\n", "", "all_instance_progs_predicted_answer_strs", ".", "append", "(", "predicted_answer", ")", "\n", "# If no program was found in beam-search", "\n", "", "if", "len", "(", "all_instance_progs_predicted_answer_strs", ")", "==", "0", ":", "\n", "                    ", "all_instance_progs_predicted_answer_strs", ".", "append", "(", "\"\"", ")", "\n", "# Since the programs are sorted by decreasing scores, we can directly take the first pred answer", "\n", "", "instance_predicted_answer", "=", "all_instance_progs_predicted_answer_strs", "[", "0", "]", "\n", "output_dict", "[", "\"predicted_answer\"", "]", ".", "append", "(", "instance_predicted_answer", ")", "\n", "output_dict", "[", "\"all_predicted_answers\"", "]", ".", "append", "(", "all_instance_progs_predicted_answer_strs", ")", "\n", "\n", "answer_annotations", "=", "metadata", "[", "i", "]", ".", "get", "(", "\"answer_annotations\"", ",", "[", "]", ")", "\n", "if", "answer_annotations", ":", "\n", "                    ", "self", ".", "_drop_metrics", "(", "instance_predicted_answer", ",", "answer_annotations", ")", "\n", "\n", "", "", "if", "not", "self", ".", "training", ":", "# and self._debug:", "\n", "                ", "output_dict", "[", "\"metadata\"", "]", "=", "metadata", "\n", "output_dict", "[", "\"passage_mask\"", "]", "=", "passage_mask", "\n", "output_dict", "[", "\"passage_token_idxs\"", "]", "=", "passage_token_idxs", "\n", "if", "answer_as_passage_spans", "is", "not", "None", ":", "\n", "                    ", "output_dict", "[", "\"answer_as_passage_spans\"", "]", "=", "answer_as_passage_spans", "\n", "", "output_dict", "[", "\"batch_action_seqs\"", "]", "=", "batch_actionseqs", "\n", "batch_logical_programs", "=", "[", "]", "\n", "for", "instance_idx", ",", "instance_actionseqs", "in", "enumerate", "(", "batch_actionseqs", ")", ":", "\n", "                    ", "instance_logical_progs", "=", "[", "]", "\n", "for", "ins_actionseq", "in", "instance_actionseqs", ":", "\n", "                        ", "logical_form", "=", "languages", "[", "instance_idx", "]", ".", "action_sequence_to_logical_form", "(", "ins_actionseq", ")", "\n", "instance_logical_progs", ".", "append", "(", "logical_form", ")", "\n", "", "batch_logical_programs", ".", "append", "(", "instance_logical_progs", ")", "\n", "\n", "", "output_dict", "[", "\"batch_logical_programs\"", "]", "=", "batch_logical_programs", "\n", "output_dict", "[", "\"batch_actionseq_logprobs\"", "]", "=", "batch_actionseq_logprobs", "\n", "output_dict", "[", "\"batch_actionseq_probs\"", "]", "=", "batch_actionseq_probs", "\n", "output_dict", "[", "\"batch_actionseq_sideargs\"", "]", "=", "batch_actionseq_sideargs", "\n", "output_dict", "[", "\"languages\"", "]", "=", "languages", "\n", "modules_debug_infos", "=", "[", "l", ".", "modules_debug_info", "for", "l", "in", "languages", "]", "\n", "output_dict", "[", "\"modules_debug_infos\"", "]", "=", "modules_debug_infos", "\n", "\n", "", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.compute_token_symbol_alignments": [[1015, 1055], ["passage_to_symbol_attention_params", "allennlp.masked_softmax", "passage_mask.unsqueeze", "passage_mask.unsqueeze", "passage_tokenidx2symbolidx_mask.unsqueeze", "passage_tokenidx2symbolidx_mask.unsqueeze"], "methods", ["None"], ["", "def", "compute_token_symbol_alignments", "(", "\n", "self", ",", "modeled_passage", ",", "passage_mask", ",", "passageidx2symbolidx", ",", "passage_to_symbol_attention_params", "\n", ")", ":", "\n", "        ", "\"\"\"Compute the passage_token-to-passage_date alignment matrix.\n\n        Args:\n        -----\n            modeled_passage: (batch_size, passage_length, hidden_dim)\n                Contextual passage repr.\n            passage_mask: (batch_size, passage_length)\n                Passage mask\n            passageidx2dateidx: (batch_size, passage_length)\n                For date-tokens, the index of the date-entity it belongs to, o/w masked with value = -1\n            passage_to_date_attention_params: Some matrix-attention parameterization for computing the alignment matrix\n\n        Returns:\n        --------\n            pasage_passage_token2symbol_aligment: (batch_size, passage_length, passage_length)\n                Alignment matrix from passage_token (dim=1) to passage_date (dim=2)\n                Should be masked in dim=2 for tokens that are not date-tokens\n        \"\"\"", "\n", "# ### Passage Token - Date Alignment", "\n", "# Shape: (batch_size, passage_length, passage_length)", "\n", "passage_passage_token2symbol_similarity", "=", "passage_to_symbol_attention_params", "(", "modeled_passage", ",", "modeled_passage", ")", "\n", "passage_passage_token2symbol_similarity", "=", "passage_passage_token2symbol_similarity", "*", "passage_mask", ".", "unsqueeze", "(", "1", ")", "\n", "passage_passage_token2symbol_similarity", "=", "passage_passage_token2symbol_similarity", "*", "passage_mask", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Shape: (batch_size, passage_length) -- masking for number tokens in the passage", "\n", "passage_tokenidx2symbolidx_mask", "=", "(", "passageidx2symbolidx", ">", "-", "1", ")", ".", "float", "(", ")", "\n", "# Shape: (batch_size, passage_length, passage_length)", "\n", "passage_passage_token2symbol_similarity", "=", "(", "\n", "passage_passage_token2symbol_similarity", "*", "passage_tokenidx2symbolidx_mask", ".", "unsqueeze", "(", "1", ")", "\n", ")", "\n", "# Shape: (batch_size, passage_length, passage_length)", "\n", "pasage_passage_token2symbol_aligment", "=", "allenutil", ".", "masked_softmax", "(", "\n", "passage_passage_token2symbol_similarity", ",", "\n", "mask", "=", "passage_tokenidx2symbolidx_mask", ".", "unsqueeze", "(", "1", ")", ",", "\n", "memory_efficient", "=", "True", ",", "\n", ")", "\n", "return", "pasage_passage_token2symbol_aligment", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.compute_avg_norm": [[1056, 1063], ["tensor.size", "tensor.size", "tensor.norm().sum", "tensor.norm"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "def", "compute_avg_norm", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "dim0_size", "=", "tensor", ".", "size", "(", ")", "[", "0", "]", "\n", "dim1_size", "=", "tensor", ".", "size", "(", ")", "[", "1", "]", "\n", "\n", "tensor_norm", "=", "tensor", ".", "norm", "(", "p", "=", "2", ",", "dim", "=", "2", ")", ".", "sum", "(", ")", "/", "(", "dim0_size", "*", "dim1_size", ")", "\n", "\n", "return", "tensor_norm", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.get_metrics": [[1064, 1085], ["drop_parser_bert.DROPParserBERT.modelloss_metric.get_metric", "drop_parser_bert.DROPParserBERT.excloss_metric.get_metric", "drop_parser_bert.DROPParserBERT.qattloss_metric.get_metric", "drop_parser_bert.DROPParserBERT.mmlloss_metric.get_metric", "drop_parser_bert.DROPParserBERT.auxwinloss_metric.get_metric", "drop_parser_bert.DROPParserBERT._drop_metrics.get_metric", "metric_dict.update"], "methods", ["None"], ["", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "metric_dict", "=", "{", "}", "\n", "model_loss", "=", "self", ".", "modelloss_metric", ".", "get_metric", "(", "reset", ")", "\n", "exec_loss", "=", "self", ".", "excloss_metric", ".", "get_metric", "(", "reset", ")", "\n", "qatt_loss", "=", "self", ".", "qattloss_metric", ".", "get_metric", "(", "reset", ")", "\n", "mml_loss", "=", "self", ".", "mmlloss_metric", ".", "get_metric", "(", "reset", ")", "\n", "winloss", "=", "self", ".", "auxwinloss_metric", ".", "get_metric", "(", "reset", ")", "\n", "exact_match", ",", "f1_score", "=", "self", ".", "_drop_metrics", ".", "get_metric", "(", "reset", ")", "\n", "metric_dict", ".", "update", "(", "\n", "{", "\n", "\"em\"", ":", "exact_match", ",", "\n", "\"f1\"", ":", "f1_score", ",", "\n", "\"ans\"", ":", "model_loss", ",", "\n", "\"exc\"", ":", "exec_loss", ",", "\n", "\"qatt\"", ":", "qatt_loss", ",", "\n", "\"mml\"", ":", "mml_loss", ",", "\n", "\"win\"", ":", "winloss", ",", "\n", "}", "\n", ")", "\n", "\n", "return", "metric_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT._get_span_answer_log_prob": [[1086, 1140], ["answer_as_spans.unsqueeze.unsqueeze.unsqueeze", "span_start_log_probs.unsqueeze.unsqueeze.unsqueeze", "span_end_log_probs.unsqueeze.unsqueeze.unsqueeze", "allennlp.replace_masked_values", "allennlp.replace_masked_values", "torch.gather", "torch.gather", "allennlp.replace_masked_values", "allennlp.logsumexp", "log_marginal_likelihood_for_span.squeeze.squeeze.squeeze"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_span_answer_log_prob", "(", "\n", "answer_as_spans", ":", "torch", ".", "LongTensor", ",", "span_log_probs", ":", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\" Compute the log_marginal_likelihood for the answer_spans given log_probs for start/end\n            Compute log_likelihood (product of start/end probs) of each ans_span\n            Sum the prob (logsumexp) for each span and return the log_likelihood\n\n        Parameters:\n        -----------\n        answer: ``torch.LongTensor`` Shape: (number_of_spans, 2)\n            These are the gold spans\n        span_log_probs: ``torch.FloatTensor``\n            2-Tuple with tensors of Shape: (length_of_sequence) for span_start/span_end log_probs\n\n        Returns:\n        log_marginal_likelihood_for_passage_span\n        \"\"\"", "\n", "\n", "# Unsqueezing dim=0 to make a batch_size of 1", "\n", "answer_as_spans", "=", "answer_as_spans", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "span_start_log_probs", ",", "span_end_log_probs", "=", "span_log_probs", "\n", "span_start_log_probs", "=", "span_start_log_probs", ".", "unsqueeze", "(", "0", ")", "\n", "span_end_log_probs", "=", "span_end_log_probs", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "# (batch_size, number_of_ans_spans)", "\n", "gold_passage_span_starts", "=", "answer_as_spans", "[", ":", ",", ":", ",", "0", "]", "\n", "gold_passage_span_ends", "=", "answer_as_spans", "[", ":", ",", ":", ",", "1", "]", "\n", "# Some spans are padded with index -1,", "\n", "# so we clamp those paddings to 0 and then mask after `torch.gather()`.", "\n", "gold_passage_span_mask", "=", "(", "gold_passage_span_starts", "!=", "-", "1", ")", ".", "long", "(", ")", "\n", "clamped_gold_passage_span_starts", "=", "allenutil", ".", "replace_masked_values", "(", "\n", "gold_passage_span_starts", ",", "gold_passage_span_mask", ",", "0", "\n", ")", "\n", "clamped_gold_passage_span_ends", "=", "allenutil", ".", "replace_masked_values", "(", "\n", "gold_passage_span_ends", ",", "gold_passage_span_mask", ",", "0", "\n", ")", "\n", "# Shape: (batch_size, # of answer spans)", "\n", "log_likelihood_for_span_starts", "=", "torch", ".", "gather", "(", "span_start_log_probs", ",", "1", ",", "clamped_gold_passage_span_starts", ")", "\n", "log_likelihood_for_span_ends", "=", "torch", ".", "gather", "(", "span_end_log_probs", ",", "1", ",", "clamped_gold_passage_span_ends", ")", "\n", "# Shape: (batch_size, # of answer spans)", "\n", "log_likelihood_for_spans", "=", "log_likelihood_for_span_starts", "+", "log_likelihood_for_span_ends", "\n", "# For those padded spans, we set their log probabilities to be very small negative value", "\n", "log_likelihood_for_spans", "=", "allenutil", ".", "replace_masked_values", "(", "\n", "log_likelihood_for_spans", ",", "gold_passage_span_mask", ",", "-", "1e7", "\n", ")", "\n", "# Shape: (batch_size, )", "\n", "log_marginal_likelihood_for_span", "=", "allenutil", ".", "logsumexp", "(", "log_likelihood_for_spans", ")", "\n", "\n", "# Squeezing the batch-size 1", "\n", "log_marginal_likelihood_for_span", "=", "log_marginal_likelihood_for_span", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "return", "log_marginal_likelihood_for_span", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.get_valid_start_actionids": [[1141, 1223], ["range", "len", "action_prefixes.append", "logging.error", "instance_prefix_sequences.append", "len", "instance_prefix_sequences.append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_valid_start_actionids", "(", "answer_types", ":", "List", "[", "List", "[", "str", "]", "]", ",", "action2actionidx", ":", "Dict", "[", "str", ",", "int", "]", ",", "\n", "valid_numcomposition_types", ":", "List", "[", "Set", "[", "str", "]", "]", "=", "None", ")", "->", "List", "[", "List", "[", "List", "[", "int", "]", "]", "]", ":", "\n", "        ", "\"\"\" For each instances, given answer_types as man-made strings, return the set of valid start action ids\n            that return an object of that type.\n            For example, given start_type as 'passage_span', '@start@ -> PassageSpanAnswer' is a valid start action\n            See the reader for all possible values of start_types\n\n            TODO(nitish): Instead of the reader passing in arbitrary strings and maintaining a mapping here,\n            TODO(nitish): we can pass in the names of the LanguageObjects directly and make the start-action-str\n                          in a programmatic manner.\n\n            This is used while *training* to run constrained-beam-search to only search through programs for which\n            the gold answer is known. These *** shouldn't *** beused during validation\n        Parameters:\n        -----------\n            composed_num_ans_types: ``List[Set[str]]``\n                For each instance, if the answer is of ComposedNumber type, then this tells valid compositions.\n                Can be used to limit search space\n        Returns:\n        --------\n        start_types: `List[Set[int]]`\n            For each instance, a set of possible start_types\n        \"\"\"", "\n", "\n", "# Map from string passed by reader to LanguageType class", "\n", "answer_type_to_start_action_mapping", "=", "{", "\n", "\"passage_span\"", ":", "\"@start@ -> PassageSpanAnswer\"", ",", "\n", "\"year_difference\"", ":", "\"@start@ -> YearDifference\"", ",", "\n", "\"passage_number\"", ":", "\"@start@ -> PassageNumber\"", ",", "\n", "\"question_span\"", ":", "\"@start@ -> QuestionSpanAnswer\"", ",", "\n", "\"count_number\"", ":", "\"@start@ -> CountNumber\"", ",", "\n", "\"composed_number\"", ":", "\"@start@ -> ComposedNumber\"", ",", "\n", "}", "\n", "\n", "# This is the next action to go from ComposedNumber to a function that could generate it", "\n", "composed_num_function_action", "=", "\"ComposedNumber -> [<PassageNumber,PassageNumber:ComposedNumber>, PassageNumber, PassageNumber]\"", "\n", "# These two actions satisfies the function signature in composed_num_function_action", "\n", "addition_function_action", "=", "\"<PassageNumber,PassageNumber:ComposedNumber> -> passagenumber_addition\"", "\n", "subtraction_function_action", "=", "\"<PassageNumber,PassageNumber:ComposedNumber> -> passagenumber_difference\"", "\n", "\n", "# This is the key that the reader sends in valid_numcomposition_types", "\n", "num_composition_type_action_mapping", "=", "{", "\n", "\"passage_num_addition\"", ":", "addition_function_action", ",", "\n", "\"passage_num_subtraction\"", ":", "subtraction_function_action", ",", "\n", "}", "\n", "\n", "# We are aiming to make a List[List[List[int]]] -- for each instance, a list of prefix sequences.", "\n", "#  Each prefix sequence is a list of action indices. The prefixes don't need to be of the same length.", "\n", "#  If an instance does not have any prefixes, its prefix_sequences_list can remain empty", "\n", "valid_action_prefixes", "=", "[", "]", "\n", "\n", "# valid_start_action_ids: List[Set[int]] = []", "\n", "action_prefixes", ":", "List", "[", "List", "[", "List", "[", "int", "]", "]", "]", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "answer_types", ")", ")", ":", "\n", "            ", "instance_answer_types", ":", "List", "[", "str", "]", "=", "answer_types", "[", "i", "]", "\n", "instance_prefix_sequences", ":", "List", "[", "List", "[", "int", "]", "]", "=", "[", "]", "\n", "for", "start_type", "in", "instance_answer_types", ":", "\n", "                ", "if", "start_type", "in", "answer_type_to_start_action_mapping", ":", "\n", "                    ", "start_action", "=", "answer_type_to_start_action_mapping", "[", "start_type", "]", "\n", "start_action_idx", "=", "action2actionidx", "[", "start_action", "]", "\n", "if", "start_type", "==", "\"composed_number\"", ":", "\n", "# Containing \"passage_num_addition\" and/or \"passage_num_subtraction\"", "\n", "                        ", "instance_numcomposition_types", ":", "Set", "[", "str", "]", "=", "valid_numcomposition_types", "[", "i", "]", "\n", "assert", "len", "(", "instance_numcomposition_types", ")", ">", "0", ",", "\"No composition type info given\"", "\n", "for", "composition_type", "in", "instance_numcomposition_types", ":", "\n", "                            ", "composition_function_action", "=", "num_composition_type_action_mapping", "[", "composition_type", "]", "\n", "composition_function_action_idx", "=", "action2actionidx", "[", "composition_function_action", "]", "\n", "composednumtype_to_funcsign_actionidx", "=", "action2actionidx", "[", "composed_num_function_action", "]", "\n", "prefix_sequence", "=", "[", "start_action_idx", ",", "composednumtype_to_funcsign_actionidx", ",", "\n", "composition_function_action_idx", "]", "\n", "instance_prefix_sequences", ".", "append", "(", "prefix_sequence", ")", "\n", "", "", "else", ":", "\n", "                        ", "prefix_sequence", "=", "[", "start_action_idx", "]", "\n", "instance_prefix_sequences", ".", "append", "(", "prefix_sequence", ")", "\n", "", "", "else", ":", "\n", "                    ", "logging", ".", "error", "(", "f\"StartType: {start_type} not present in {answer_type_to_start_action_mapping}\"", ")", "\n", "# valid_start_action_ids.append(start_action_ids)", "\n", "", "", "action_prefixes", ".", "append", "(", "instance_prefix_sequences", ")", "\n", "\n", "", "return", "action_prefixes", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.synthetic_num_grounding_loss": [[1224, 1256], ["allennlp.masked_softmax", "enumerate", "zip", "passageidx2numberidx_mask.unsqueeze", "torch.log"], "methods", ["None"], ["", "def", "synthetic_num_grounding_loss", "(", "\n", "self", ",", "qtypes", ",", "synthetic_numgrounding_metadata", ",", "passage_passage_num_similarity", ",", "passageidx2numberidx", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Parameters:\n        -----------\n        passage_passage_num_similarity: (B, P, P)\n        passage_tokenidx2dateidx: (B, P) containing non -1 values for number tokens. We'll use this for masking.\n        synthetic_numgrounding_metadata: For each instance, list of (token_idx, number_idx), i.e.\n            for row = token_idx, column = number_idx should be high\n        \"\"\"", "\n", "\n", "# (B, P)", "\n", "passageidx2numberidx_mask", "=", "(", "passageidx2numberidx", ">", "-", "1", ")", ".", "float", "(", ")", "\n", "\n", "# (B, P, P) -- with each row now normalized for number tokens", "\n", "passage_passage_num_attention", "=", "allenutil", ".", "masked_softmax", "(", "\n", "passage_passage_num_similarity", ",", "mask", "=", "passageidx2numberidx_mask", ".", "unsqueeze", "(", "1", ")", "\n", ")", "\n", "log_likelihood", "=", "0", "\n", "normalizer", "=", "0", "\n", "for", "idx", ",", "(", "qtype", ",", "token_number_idx_pairs", ")", "in", "enumerate", "(", "zip", "(", "qtypes", ",", "synthetic_numgrounding_metadata", ")", ")", ":", "\n", "            ", "if", "qtype", "==", "dropconstants", ".", "SYN_NUMGROUND_qtype", ":", "\n", "                ", "for", "token_idx", ",", "number_idx", "in", "token_number_idx_pairs", ":", "\n", "                    ", "log_likelihood", "+=", "torch", ".", "log", "(", "passage_passage_num_attention", "[", "idx", ",", "token_idx", ",", "number_idx", "]", "+", "1e-40", ")", "\n", "normalizer", "+=", "1", "\n", "", "", "", "if", "normalizer", ">", "0", ":", "\n", "            ", "log_likelihood", "=", "log_likelihood", "/", "normalizer", "\n", "\n", "", "loss", "=", "-", "1", "*", "log_likelihood", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT._ques_attention_loss": [[1257, 1395], ["range", "len", "len", "zip", "zip", "torch.sum", "torch.log", "torch.sum", "print"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "def", "_ques_attention_loss", "(", "\n", "self", ",", "\n", "batch_actionseqs", ":", "List", "[", "List", "[", "List", "[", "str", "]", "]", "]", ",", "\n", "batch_actionseq_sideargs", ":", "List", "[", "List", "[", "List", "[", "Dict", "]", "]", "]", ",", "\n", "qtypes", ":", "List", "[", "str", "]", ",", "\n", "qattn_supervised", ":", "List", "[", "bool", "]", ",", "\n", "qattn_supervision", ":", "torch", ".", "FloatTensor", ",", "\n", ")", ":", "\n", "\n", "        ", "\"\"\" Compute QAttn supervision loss for different kind of questions. Different question-types have diff.\n            gold-programs and can have different number of qattn-supervision for each instance.\n            There, the shape of qattn_supervision is (B, R, QLen) where R is the maximum number of attn-supervisions\n            provided for an instance in this batch. For instances with less number of relevant actions\n            the corresponding instance_slice will be padded with all zeros-tensors.\n\n            We hard-code the question-types supported, and for each qtype, the relevant actions for which the\n            qattn supervision will (should) be provided. For example, the gold program for date-comparison questions\n            contains two 'PassageAttention -> find_PassageAttention' actions which use the question_attention sidearg\n            for which the supervision is\n             provided. Hence, qtype2relevant_actions_list - contains the two actions for the\n            date-comparison question.\n\n            The loss computed is the negative-log of sum of relevant probabilities.\n\n            NOTE: This loss is only computed for instances that are marked as strongly-annotated and hence we don't\n            check if the qattns-supervision needs masking.\n        \"\"\"", "\n", "find_passage_attention", "=", "\"PassageAttention -> find_PassageAttention\"", "\n", "filter_passage_attention", "=", "\"<PassageAttention:PassageAttention> -> filter_PassageAttention\"", "\n", "relocate_passage_attention", "=", "\"<PassageAttention:PassageAttention_answer> -> relocate_PassageAttention\"", "\n", "\n", "single_find_passage_attention_list", "=", "[", "find_passage_attention", "]", "\n", "double_find_passage_attentions_list", "=", "[", "find_passage_attention", ",", "find_passage_attention", "]", "\n", "filter_find_passage_attention_list", "=", "[", "filter_passage_attention", ",", "find_passage_attention", "]", "\n", "relocate_find_passage_attention_list", "=", "[", "relocate_passage_attention", ",", "find_passage_attention", "]", "\n", "relocate_filterfind_passage_attention_list", "=", "[", "\n", "relocate_passage_attention", ",", "\n", "filter_passage_attention", ",", "\n", "find_passage_attention", ",", "\n", "]", "\n", "\n", "qtypes_w_findPA", "=", "[", "\n", "dropconstants", ".", "NUM_find_qtype", ",", "\n", "dropconstants", ".", "MAX_find_qtype", ",", "\n", "dropconstants", ".", "MIN_find_qtype", ",", "\n", "dropconstants", ".", "COUNT_find_qtype", ",", "\n", "dropconstants", ".", "YARDS_findnum_qtype", ",", "\n", "dropconstants", ".", "YARDS_longest_qtype", ",", "\n", "dropconstants", ".", "YARDS_shortest_qtype", ",", "\n", "]", "\n", "\n", "qtypes_w_filterfindPA", "=", "[", "\n", "dropconstants", ".", "NUM_filter_find_qtype", ",", "\n", "dropconstants", ".", "MAX_filter_find_qtype", ",", "\n", "dropconstants", ".", "MIN_filter_find_qtype", ",", "\n", "dropconstants", ".", "COUNT_filter_find_qtype", ",", "\n", "]", "\n", "\n", "qtypes_w_two_findPA", "=", "[", "dropconstants", ".", "DATECOMP_QTYPE", ",", "dropconstants", ".", "NUMCOMP_QTYPE", "]", "\n", "\n", "qtypes_w_relocatefindPA", "=", "[", "\n", "dropconstants", ".", "RELOC_find_qtype", ",", "\n", "dropconstants", ".", "RELOC_maxfind_qtype", ",", "\n", "dropconstants", ".", "RELOC_minfind_qtype", ",", "\n", "]", "\n", "qtypes_w_relocate_filterfindPA", "=", "[", "\n", "dropconstants", ".", "RELOC_filterfind_qtype", ",", "\n", "dropconstants", ".", "RELOC_maxfilterfind_qtype", ",", "\n", "dropconstants", ".", "RELOC_minfilterfind_qtype", ",", "\n", "]", "\n", "\n", "qtype2relevant_actions_list", "=", "{", "}", "\n", "\n", "for", "qtype", "in", "qtypes_w_findPA", ":", "\n", "            ", "qtype2relevant_actions_list", "[", "qtype", "]", "=", "single_find_passage_attention_list", "\n", "", "for", "qtype", "in", "qtypes_w_two_findPA", ":", "\n", "            ", "qtype2relevant_actions_list", "[", "qtype", "]", "=", "double_find_passage_attentions_list", "\n", "", "for", "qtype", "in", "qtypes_w_filterfindPA", ":", "\n", "            ", "qtype2relevant_actions_list", "[", "qtype", "]", "=", "filter_find_passage_attention_list", "\n", "", "for", "qtype", "in", "qtypes_w_relocatefindPA", ":", "\n", "            ", "qtype2relevant_actions_list", "[", "qtype", "]", "=", "relocate_find_passage_attention_list", "\n", "", "for", "qtype", "in", "qtypes_w_relocate_filterfindPA", ":", "\n", "            ", "qtype2relevant_actions_list", "[", "qtype", "]", "=", "relocate_filterfind_passage_attention_list", "\n", "\n", "", "loss", "=", "0.0", "\n", "normalizer", "=", "0", "\n", "\n", "for", "ins_idx", "in", "range", "(", "len", "(", "batch_actionseqs", ")", ")", ":", "\n", "            ", "qattn_supervised_instance", "=", "qattn_supervised", "[", "ins_idx", "]", "\n", "if", "not", "qattn_supervised_instance", ":", "\n", "# no point even bothering", "\n", "                ", "continue", "\n", "", "qtype", "=", "qtypes", "[", "ins_idx", "]", "\n", "if", "qtype", "not", "in", "qtype2relevant_actions_list", ":", "\n", "                ", "continue", "\n", "\n", "", "instance_programs", "=", "batch_actionseqs", "[", "ins_idx", "]", "\n", "instance_prog_sideargs", "=", "batch_actionseq_sideargs", "[", "ins_idx", "]", "\n", "# Shape: (R, question_length)", "\n", "instance_qattn_supervision", "=", "qattn_supervision", "[", "ins_idx", "]", "\n", "# These are the actions for which qattn_supervision should be provided.", "\n", "relevant_actions", "=", "qtype2relevant_actions_list", "[", "qtype", "]", "\n", "num_relevant_actions", "=", "len", "(", "relevant_actions", ")", "\n", "for", "program", ",", "side_args", "in", "zip", "(", "instance_programs", ",", "instance_prog_sideargs", ")", ":", "\n", "# Counter to keep a track of which relevant action we're looking for next", "\n", "                ", "relevant_action_idx", "=", "0", "\n", "relevant_action", "=", "relevant_actions", "[", "relevant_action_idx", "]", "\n", "gold_qattn", "=", "instance_qattn_supervision", "[", "relevant_action_idx", "]", "\n", "for", "action", ",", "side_arg", "in", "zip", "(", "program", ",", "side_args", ")", ":", "\n", "                    ", "if", "action", "==", "relevant_action", ":", "\n", "                        ", "question_attention", "=", "side_arg", "[", "\"question_attention\"", "]", "\n", "if", "torch", ".", "sum", "(", "gold_qattn", ")", "!=", "0.0", ":", "\n", "# Sum of probs -- model can distribute gold mass however it likes", "\n", "# l = torch.sum(question_attention * gold_qattn)", "\n", "# loss += torch.log(l)", "\n", "# Prod of probs -- forces model to evenly distribute mass on gold-attn", "\n", "                            ", "log_question_attention", "=", "torch", ".", "log", "(", "question_attention", "+", "1e-40", ")", "\n", "l", "=", "torch", ".", "sum", "(", "log_question_attention", "*", "gold_qattn", ")", "\n", "loss", "+=", "l", "\n", "normalizer", "+=", "1", "\n", "", "else", ":", "\n", "                            ", "print", "(", "\n", "f\"\\nGold attention sum == 0.0.\"", "\n", "f\"\\nQattnSupervised: {qattn_supervised_instance}\"", "\n", "f\"\\nQtype: {qtype}\"", "\n", ")", "\n", "", "relevant_action_idx", "+=", "1", "\n", "\n", "# All relevant actions for this instance in this program are found", "\n", "if", "relevant_action_idx", ">=", "num_relevant_actions", ":", "\n", "                            ", "break", "\n", "", "else", ":", "\n", "                            ", "relevant_action", "=", "relevant_actions", "[", "relevant_action_idx", "]", "\n", "gold_qattn", "=", "instance_qattn_supervision", "[", "relevant_action_idx", "]", "\n", "", "", "", "", "", "if", "normalizer", "==", "0", ":", "\n", "            ", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "-", "1", "*", "(", "loss", "/", "normalizer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT._get_best_spans": [[1396, 1472], ["range", "len", "zip", "batch_best_spans.append", "batch_predicted_answers.append", "allennlp.models.reading_comprehension.util.get_best_span().squeeze", "instance_best_spans.append", "tuple", "instance_predicted_ans.append", "allennlp.models.reading_comprehension.util.get_best_span().squeeze.detach().cpu().numpy", "allennlp.models.reading_comprehension.util.get_best_span", "allennlp.models.reading_comprehension.util.get_best_span().squeeze.detach().cpu", "print", "print", "print", "print", "print", "print", "print", "denotation._value[].unsqueeze", "denotation._value[].unsqueeze", "print", "print", "print", "print", "print", "allennlp.models.reading_comprehension.util.get_best_span().squeeze.detach", "question_mask_aslist[].size", "len", "len", "passage_mask_aslist[].size", "len", "len"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "_get_best_spans", "(", "\n", "batch_denotations", ",", "\n", "batch_denotation_types", ",", "\n", "question_char_offsets", ",", "\n", "question_strs", ",", "\n", "passage_char_offsets", ",", "\n", "passage_strs", ",", "\n", "*", "args", ",", "\n", ")", ":", "\n", "        ", "\"\"\" For all SpanType denotations, get the best span\n\n        Parameters:\n        ----------\n        batch_denotations: List[List[Any]]\n        batch_denotation_types: List[List[str]]\n        \"\"\"", "\n", "\n", "(", "question_mask_aslist", ",", "passage_mask_aslist", ")", "=", "args", "\n", "\n", "batch_best_spans", "=", "[", "]", "\n", "batch_predicted_answers", "=", "[", "]", "\n", "\n", "for", "instance_idx", "in", "range", "(", "len", "(", "batch_denotations", ")", ")", ":", "\n", "            ", "instance_prog_denotations", "=", "batch_denotations", "[", "instance_idx", "]", "\n", "instance_prog_types", "=", "batch_denotation_types", "[", "instance_idx", "]", "\n", "\n", "instance_best_spans", "=", "[", "]", "\n", "instance_predicted_ans", "=", "[", "]", "\n", "\n", "for", "denotation", ",", "progtype", "in", "zip", "(", "instance_prog_denotations", ",", "instance_prog_types", ")", ":", "\n", "# if progtype == \"QuestionSpanAnswwer\":", "\n", "# Distinction between QuestionSpanAnswer and PassageSpanAnswer is not needed currently,", "\n", "# since both classes store the start/end logits as a tuple", "\n", "# Shape: (2, )", "\n", "                ", "best_span", "=", "get_best_span", "(", "\n", "span_start_logits", "=", "denotation", ".", "_value", "[", "0", "]", ".", "unsqueeze", "(", "0", ")", ",", "\n", "span_end_logits", "=", "denotation", ".", "_value", "[", "1", "]", ".", "unsqueeze", "(", "0", ")", ",", "\n", ")", ".", "squeeze", "(", "0", ")", "\n", "instance_best_spans", ".", "append", "(", "best_span", ")", "\n", "\n", "predicted_span", "=", "tuple", "(", "best_span", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "if", "progtype", "==", "\"QuestionSpanAnswer\"", ":", "\n", "                    ", "try", ":", "\n", "                        ", "start_offset", "=", "question_char_offsets", "[", "instance_idx", "]", "[", "predicted_span", "[", "0", "]", "]", "[", "0", "]", "\n", "end_offset", "=", "question_char_offsets", "[", "instance_idx", "]", "[", "predicted_span", "[", "1", "]", "]", "[", "1", "]", "\n", "predicted_answer", "=", "question_strs", "[", "instance_idx", "]", "[", "start_offset", ":", "end_offset", "]", "\n", "", "except", ":", "\n", "                        ", "print", "(", ")", "\n", "print", "(", "f\"PredictedSpan: {predicted_span}\"", ")", "\n", "print", "(", "f\"QuesMaskLen: {question_mask_aslist[instance_idx].size()}\"", ")", "\n", "print", "(", "f\"StartLogProbs:{denotation._value[0]}\"", ")", "\n", "print", "(", "f\"EndLogProbs:{denotation._value[1]}\"", ")", "\n", "print", "(", "f\"LenofOffsets: {len(question_char_offsets[instance_idx])}\"", ")", "\n", "print", "(", "f\"QuesStrLen: {len(question_strs[instance_idx])}\"", ")", "\n", "\n", "", "", "elif", "progtype", "==", "\"PassageSpanAnswer\"", ":", "\n", "                    ", "try", ":", "\n", "                        ", "start_offset", "=", "passage_char_offsets", "[", "instance_idx", "]", "[", "predicted_span", "[", "0", "]", "]", "[", "0", "]", "\n", "end_offset", "=", "passage_char_offsets", "[", "instance_idx", "]", "[", "predicted_span", "[", "1", "]", "]", "[", "1", "]", "\n", "predicted_answer", "=", "passage_strs", "[", "instance_idx", "]", "[", "start_offset", ":", "end_offset", "]", "\n", "", "except", ":", "\n", "                        ", "print", "(", ")", "\n", "print", "(", "f\"PredictedSpan: {predicted_span}\"", ")", "\n", "print", "(", "f\"PassageMaskLen: {passage_mask_aslist[instance_idx].size()}\"", ")", "\n", "print", "(", "f\"LenofOffsets: {len(passage_char_offsets[instance_idx])}\"", ")", "\n", "print", "(", "f\"PassageStrLen: {len(passage_strs[instance_idx])}\"", ")", "\n", "", "", "else", ":", "\n", "                    ", "raise", "NotImplementedError", "\n", "\n", "", "instance_predicted_ans", ".", "append", "(", "predicted_answer", ")", "\n", "\n", "", "batch_best_spans", ".", "append", "(", "instance_best_spans", ")", "\n", "batch_predicted_answers", ".", "append", "(", "instance_predicted_ans", ")", "\n", "\n", "", "return", "batch_best_spans", ",", "batch_predicted_answers", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.passage_attention_to_sidearg": [[1473, 1497], ["range", "len", "zip", "zip"], "methods", ["None"], ["", "def", "passage_attention_to_sidearg", "(", "\n", "self", ",", "\n", "qtypes", ":", "List", "[", "str", "]", ",", "\n", "batch_actionseqs", ":", "List", "[", "List", "[", "List", "[", "str", "]", "]", "]", ",", "\n", "batch_actionseq_sideargs", ":", "List", "[", "List", "[", "List", "[", "Dict", "]", "]", "]", ",", "\n", "pattn_supervised", ":", "List", "[", "bool", "]", ",", "\n", "passage_attn_supervision", ":", "torch", ".", "FloatTensor", ",", "\n", "max_passage_len", ":", "int", ",", "\n", "device_id", ",", "\n", ")", ":", "\n", "        ", "\"\"\" If instance has passage attention supervision, add it to 'PassageAttention -> find_PassageAttention' \"\"\"", "\n", "\n", "relevant_action", "=", "\"PassageAttention -> find_PassageAttention\"", "\n", "for", "ins_idx", "in", "range", "(", "len", "(", "batch_actionseqs", ")", ")", ":", "\n", "            ", "instance_programs", "=", "batch_actionseqs", "[", "ins_idx", "]", "\n", "instance_prog_sideargs", "=", "batch_actionseq_sideargs", "[", "ins_idx", "]", "\n", "instance_pattn_supervised", "=", "pattn_supervised", "[", "ins_idx", "]", "\n", "pattn_supervision", "=", "passage_attn_supervision", "[", "ins_idx", "]", "\n", "if", "not", "instance_pattn_supervised", ":", "\n", "                ", "pattn_supervision", "=", "None", "\n", "", "for", "program", ",", "side_args", "in", "zip", "(", "instance_programs", ",", "instance_prog_sideargs", ")", ":", "\n", "                ", "for", "action", ",", "sidearg_dict", "in", "zip", "(", "program", ",", "side_args", ")", ":", "\n", "                    ", "if", "action", "==", "relevant_action", ":", "\n", "                        ", "sidearg_dict", "[", "\"passage_attention\"", "]", "=", "pattn_supervision", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.datecompare_eventdategr_to_sideargs": [[1498, 1528], ["drop_parser_bert.DROPParserBERT.get_gold_question_event_date_grounding", "range", "len", "zip", "zip"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.get_gold_question_event_date_grounding"], ["", "", "", "", "", "def", "datecompare_eventdategr_to_sideargs", "(", "\n", "self", ",", "\n", "qtypes", ":", "List", "[", "str", "]", ",", "\n", "batch_actionseqs", ":", "List", "[", "List", "[", "List", "[", "str", "]", "]", "]", ",", "\n", "batch_actionseq_sideargs", ":", "List", "[", "List", "[", "List", "[", "Dict", "]", "]", "]", ",", "\n", "datecomp_ques_event_date_groundings", ":", "List", "[", "Tuple", "[", "List", "[", "float", "]", ",", "List", "[", "float", "]", "]", "]", ",", "\n", "device_id", ",", "\n", ")", ":", "\n", "        ", "\"\"\" batch_event_date_groundings: For each question, a two-tuple containing the correct date-grounding for the\n            two events mentioned in the question.\n            These are in order of the annotation (order of events in question) but later the question attention\n            might be predicted in reverse order and these will then be the wrong (reverse) annotations. Take care later.\n        \"\"\"", "\n", "# List[Tuple[torch.Tensor, torch.Tensor]]", "\n", "q_event_date_groundings", "=", "self", ".", "get_gold_question_event_date_grounding", "(", "\n", "datecomp_ques_event_date_groundings", ",", "device_id", "\n", ")", "\n", "\n", "relevant_action1", "=", "\"<PassageAttention,PassageAttention:PassageAttention_answer> -> compare_date_greater_than\"", "\n", "relevant_action2", "=", "\"<PassageAttention,PassageAttention:PassageAttention_answer> -> compare_date_lesser_than\"", "\n", "relevant_actions", "=", "[", "relevant_action1", ",", "relevant_action2", "]", "\n", "\n", "for", "ins_idx", "in", "range", "(", "len", "(", "batch_actionseqs", ")", ")", ":", "\n", "            ", "instance_programs", "=", "batch_actionseqs", "[", "ins_idx", "]", "\n", "instance_prog_sideargs", "=", "batch_actionseq_sideargs", "[", "ins_idx", "]", "\n", "event_date_groundings", "=", "q_event_date_groundings", "[", "ins_idx", "]", "\n", "for", "program", ",", "side_args", "in", "zip", "(", "instance_programs", ",", "instance_prog_sideargs", ")", ":", "\n", "                ", "for", "action", ",", "sidearg_dict", "in", "zip", "(", "program", ",", "side_args", ")", ":", "\n", "                    ", "if", "action", "in", "relevant_actions", ":", "\n", "                        ", "sidearg_dict", "[", "\"event_date_groundings\"", "]", "=", "event_date_groundings", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.numcompare_eventnumgr_to_sideargs": [[1529, 1579], ["drop_parser_bert.DROPParserBERT.get_gold_question_event_date_grounding", "range", "len", "zip", "zip"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.get_gold_question_event_date_grounding"], ["", "", "", "", "", "def", "numcompare_eventnumgr_to_sideargs", "(", "\n", "self", ",", "\n", "qtypes", ",", "\n", "execution_supervised", ",", "\n", "batch_actionseqs", ":", "List", "[", "List", "[", "List", "[", "str", "]", "]", "]", ",", "\n", "batch_actionseq_sideargs", ":", "List", "[", "List", "[", "List", "[", "Dict", "]", "]", "]", ",", "\n", "numcomp_qspan_num_groundings", ":", "List", "[", "Tuple", "[", "List", "[", "float", "]", ",", "List", "[", "float", "]", "]", "]", ",", "\n", "device_id", ",", "\n", ")", ":", "\n", "        ", "\"\"\" UPDATE: function name suggest only numpcomp, but works for other questions also\n            numcomp_qspan_num_groundings - is a List of 1- or 2- or maybe n- tuple of number-grounding\n        \"\"\"", "\n", "\"\"\" batch_event_num_groundings: For each question, a 1- or 2--tuple containing the correct num-grounding for the\n            two events mentioned in the question.\n            \n            Currently, for each qtype, we only have supervision for one of the actions, hence this function works\n            (The tuple contains both groundings for the same action)\n            If we need somthing like qattn, where multiple supervisions are provided, things will have to change\n        \"\"\"", "\n", "# Reusing the function written for dates -- should work fine", "\n", "#", "\n", "q_event_num_groundings", "=", "self", ".", "get_gold_question_event_date_grounding", "(", "numcomp_qspan_num_groundings", ",", "device_id", ")", "\n", "numcomp_action_gt", "=", "\"<PassageAttention,PassageAttention:PassageAttention_answer> -> compare_num_greater_than\"", "\n", "numcomp_action_lt", "=", "\"<PassageAttention,PassageAttention:PassageAttention_answer> -> compare_num_greater_than\"", "\n", "findnum_action", "=", "\"<PassageAttention:PassageNumber> -> find_PassageNumber\"", "\n", "maxNumPattn_action", "=", "\"<PassageAttention:PassageAttention> -> maxNumPattn\"", "\n", "minNumPattn_action", "=", "\"<PassageAttention:PassageAttention> -> minNumPattn\"", "\n", "\n", "qtype2relevant_actions_list", "=", "{", "\n", "dropconstants", ".", "NUMCOMP_QTYPE", ":", "[", "numcomp_action_gt", ",", "numcomp_action_lt", "]", ",", "\n", "dropconstants", ".", "NUM_find_qtype", ":", "[", "findnum_action", "]", ",", "\n", "dropconstants", ".", "NUM_filter_find_qtype", ":", "[", "findnum_action", "]", ",", "\n", "dropconstants", ".", "MAX_find_qtype", ":", "[", "maxNumPattn_action", "]", ",", "\n", "dropconstants", ".", "MAX_filter_find_qtype", ":", "[", "maxNumPattn_action", "]", ",", "\n", "dropconstants", ".", "MIN_find_qtype", ":", "[", "minNumPattn_action", "]", ",", "\n", "dropconstants", ".", "MIN_filter_find_qtype", ":", "[", "minNumPattn_action", "]", ",", "\n", "}", "\n", "\n", "for", "ins_idx", "in", "range", "(", "len", "(", "batch_actionseqs", ")", ")", ":", "\n", "            ", "instance_programs", "=", "batch_actionseqs", "[", "ins_idx", "]", "\n", "instance_prog_sideargs", "=", "batch_actionseq_sideargs", "[", "ins_idx", "]", "\n", "event_num_groundings", "=", "q_event_num_groundings", "[", "ins_idx", "]", "\n", "qtype", "=", "qtypes", "[", "ins_idx", "]", "# Could be UNK", "\n", "if", "qtype", "not", "in", "qtype2relevant_actions_list", ":", "\n", "                ", "continue", "\n", "", "relevant_actions", "=", "qtype2relevant_actions_list", "[", "qtype", "]", "\n", "for", "program", ",", "side_args", "in", "zip", "(", "instance_programs", ",", "instance_prog_sideargs", ")", ":", "\n", "                ", "for", "action", ",", "sidearg_dict", "in", "zip", "(", "program", ",", "side_args", ")", ":", "\n", "                    ", "if", "action", "in", "relevant_actions", ":", "\n", "                        ", "sidearg_dict", "[", "\"event_num_groundings\"", "]", "=", "event_num_groundings", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.get_gold_question_event_date_grounding": [[1580, 1598], ["question_date_groundings.append", "allennlp.move_to_device", "groundings_tensors.append", "torch.FloatTensor"], "methods", ["None"], ["", "", "", "", "", "def", "get_gold_question_event_date_grounding", "(", "\n", "self", ",", "question_event_date_groundings", ":", "List", "[", "Tuple", "[", "List", "[", "int", "]", ",", "List", "[", "int", "]", "]", "]", ",", "device_id", ":", "int", "\n", ")", "->", "List", "[", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", "]", ":", "\n", "        ", "\"\"\" Converts input event date groundings (date-comparison) to FloatTensors \"\"\"", "\n", "question_date_groundings", "=", "[", "]", "\n", "# for grounding_1, grounding_2 in question_event_date_groundings:", "\n", "#     g1 = allenutil.move_to_device(torch.FloatTensor(grounding_1), device_id)", "\n", "#     g2 = allenutil.move_to_device(torch.FloatTensor(grounding_2), device_id)", "\n", "#     question_date_groundings.append((g1, g2))", "\n", "\n", "# Reader passes two groundings if not provided, hence all elements have tensors and no need to check for None", "\n", "for", "groundings", "in", "question_event_date_groundings", ":", "\n", "            ", "groundings_tensors", "=", "[", "]", "\n", "for", "grounding", "in", "groundings", ":", "\n", "                ", "g", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "FloatTensor", "(", "grounding", ")", ",", "device_id", ")", "\n", "groundings_tensors", ".", "append", "(", "g", ")", "\n", "", "question_date_groundings", ".", "append", "(", "groundings_tensors", ")", "\n", "", "return", "question_date_groundings", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.passageAnsSpan_to_PassageAttention": [[1599, 1648], ["answer_as_passage_spans.long", "span_starts.unsqueeze", "span_ends.unsqueeze", "allennlp.get_range_vector", "range_vector.unsqueeze().unsqueeze.unsqueeze().unsqueeze.unsqueeze().unsqueeze", "span_range_mask.sum", "passage_mask.size", "allennlp.get_device_of", "answers_mask.unsqueeze", "span_range_mask.sum.sum", "range_vector.unsqueeze().unsqueeze.unsqueeze().unsqueeze.unsqueeze", "range", "normalized_attention.size"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "def", "passageAnsSpan_to_PassageAttention", "(", "self", ",", "answer_as_passage_spans", ",", "passage_mask", ")", ":", "\n", "        ", "\"\"\" Convert answers as passage span into passage attention for model introspection\n\n        Parameters:\n        ----------\n        answer_as_passage_spans: `torch.Tensor`\n            Tensor of shape (batch_size, number_of_ans_spans, 2) containing start / end positions\n        passage_mask: `torch.FloatTensor`\n            Tensor of shape (batch_size, passage_length)\n\n        Returns:\n        --------\n        attention: `torch.FloatTensor`\n            List of (passage_length, ) shaped tensor containing normalized attention for gold spans\n        \"\"\"", "\n", "# Shape: (batch_size, number_of_ans_spans, 2)", "\n", "answer_as_spans", "=", "answer_as_passage_spans", ".", "long", "(", ")", "\n", "\n", "# TODO(nitish): ONLY USING FIRST CORRECT SPAN OUT OF MULTIPLE POSSIBLE", "\n", "# answer_as_spans = answer_as_spans[:, 0, :].unsqueeze(1)", "\n", "\n", "# Shape: (batch_size, number_of_ans_spans)", "\n", "span_starts", "=", "answer_as_spans", "[", ":", ",", ":", ",", "0", "]", "\n", "span_ends", "=", "answer_as_spans", "[", ":", ",", ":", ",", "1", "]", "\n", "answers_mask", "=", "(", "span_starts", ">=", "0", ")", ".", "float", "(", ")", "\n", "\n", "# Shape: (batch_size, 1, number_of_ans_spans)", "\n", "span_starts_ex", "=", "span_starts", ".", "unsqueeze", "(", "1", ")", "\n", "span_ends_ex", "=", "span_ends", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# Idea: Make a range vector from 0 <-> seq_len - 1 and convert into boolean with (val > start) and (val < end)", "\n", "# Such items in the sequence are within the span range", "\n", "# Shape: (passage_length, )", "\n", "range_vector", "=", "allenutil", ".", "get_range_vector", "(", "passage_mask", ".", "size", "(", "1", ")", ",", "allenutil", ".", "get_device_of", "(", "passage_mask", ")", ")", "\n", "\n", "# Shape: (1, passage_length, 1)", "\n", "range_vector", "=", "range_vector", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Shape: (batch_size, passage_length, number_of_ans_spans) - 1 as tokens in the span, 0 otherwise", "\n", "span_range_mask", "=", "(", "range_vector", ">=", "span_starts_ex", ")", ".", "float", "(", ")", "*", "(", "range_vector", "<=", "span_ends_ex", ")", ".", "float", "(", ")", "\n", "span_range_mask", "=", "span_range_mask", "*", "answers_mask", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# Shape: (batch_size, passage_length)", "\n", "unnormalized_attention", "=", "span_range_mask", ".", "sum", "(", "2", ")", "\n", "normalized_attention", "=", "unnormalized_attention", "/", "unnormalized_attention", ".", "sum", "(", "1", ",", "keepdim", "=", "True", ")", "\n", "\n", "attention_aslist", "=", "[", "normalized_attention", "[", "i", ",", ":", "]", "for", "i", "in", "range", "(", "normalized_attention", ".", "size", "(", "0", ")", ")", "]", "\n", "\n", "return", "attention_aslist", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.passageattn_to_startendlogits": [[1649, 1670], ["passage_attention.new_zeros", "passage_attention.new_zeros", "print", "allennlp.replace_masked_values", "allennlp.replace_masked_values", "passage_attention.size", "passage_attention.size"], "methods", ["None"], ["", "def", "passageattn_to_startendlogits", "(", "self", ",", "passage_attention", ",", "passage_mask", ")", ":", "\n", "        ", "span_start_logits", "=", "passage_attention", ".", "new_zeros", "(", "passage_attention", ".", "size", "(", ")", ")", "\n", "span_end_logits", "=", "passage_attention", ".", "new_zeros", "(", "passage_attention", ".", "size", "(", ")", ")", "\n", "\n", "nonzeroindcs", "=", "(", "passage_attention", ">", "0", ")", ".", "nonzero", "(", ")", "\n", "\n", "startidx", "=", "nonzeroindcs", "[", "0", "]", "\n", "endidx", "=", "nonzeroindcs", "[", "-", "1", "]", "\n", "\n", "print", "(", "f\"{startidx} {endidx}\"", ")", "\n", "\n", "span_start_logits", "[", "startidx", "]", "=", "2.0", "\n", "span_end_logits", "[", "endidx", "]", "=", "2.0", "\n", "\n", "span_start_logits", "=", "allenutil", ".", "replace_masked_values", "(", "span_start_logits", ",", "passage_mask", ",", "-", "1e32", ")", "\n", "span_end_logits", "=", "allenutil", ".", "replace_masked_values", "(", "span_end_logits", ",", "passage_mask", ",", "-", "1e32", ")", "\n", "\n", "span_start_logits", "+=", "1e-7", "\n", "span_end_logits", "+=", "1e-7", "\n", "\n", "return", "(", "span_start_logits", ",", "span_end_logits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.passage_ans_attn_to_sideargs": [[1671, 1689], ["range", "len", "zip", "zip"], "methods", ["None"], ["", "def", "passage_ans_attn_to_sideargs", "(", "\n", "self", ",", "\n", "batch_actionseqs", ":", "List", "[", "List", "[", "List", "[", "str", "]", "]", "]", ",", "\n", "batch_actionseq_sideargs", ":", "List", "[", "List", "[", "List", "[", "Dict", "]", "]", "]", ",", "\n", "batch_gold_attentions", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", ")", ":", "\n", "\n", "        ", "for", "ins_idx", "in", "range", "(", "len", "(", "batch_actionseqs", ")", ")", ":", "\n", "            ", "instance_programs", "=", "batch_actionseqs", "[", "ins_idx", "]", "\n", "instance_prog_sideargs", "=", "batch_actionseq_sideargs", "[", "ins_idx", "]", "\n", "instance_gold_attention", "=", "batch_gold_attentions", "[", "ins_idx", "]", "\n", "for", "program", ",", "side_args", "in", "zip", "(", "instance_programs", ",", "instance_prog_sideargs", ")", ":", "\n", "                ", "first_qattn", "=", "True", "# This tells model which qent attention to use", "\n", "# print(side_args)", "\n", "# print()", "\n", "for", "action", ",", "sidearg_dict", "in", "zip", "(", "program", ",", "side_args", ")", ":", "\n", "                    ", "if", "action", "==", "\"PassageSpanAnswer -> find_passageSpanAnswer\"", ":", "\n", "                        ", "sidearg_dict", "[", "\"passage_attention\"", "]", "=", "instance_gold_attention", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.getInitialDecoderState": [[1690, 1741], ["range", "drop_parser_bert.DROPParserBERT._get_initial_rnn_state", "allennlp_semparse.state_machines.states.GrammarBasedState", "encoded_question.new_zeros", "drop_parser_bert.DROPParserBERT._create_grammar_statelet", "initial_grammar_statelets.append", "batch_actionidx2actionstr.append", "batch_action2actionidx.append", "range", "range", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_base.DROPParserBase._get_initial_rnn_state", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_base.DROPParserBase._create_grammar_statelet"], ["", "", "", "", "", "def", "getInitialDecoderState", "(", "\n", "self", ",", "\n", "languages", ":", "List", "[", "DropLanguage", "]", ",", "\n", "actions", ":", "List", "[", "List", "[", "ProductionRule", "]", "]", ",", "\n", "encoded_question", ":", "torch", ".", "FloatTensor", ",", "\n", "question_mask", ":", "torch", ".", "FloatTensor", ",", "\n", "question_encoded_final_state", ":", "torch", ".", "FloatTensor", ",", "\n", "question_encoded_aslist", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", "question_mask_aslist", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", "batch_size", ":", "int", ",", "\n", ")", ":", "\n", "# List[torch.Tensor(0.0)] -- Initial log-score list for the decoding", "\n", "        ", "initial_score_list", "=", "[", "encoded_question", ".", "new_zeros", "(", "1", ",", "dtype", "=", "torch", ".", "float", ")", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "initial_grammar_statelets", "=", "[", "]", "\n", "batch_action2actionidx", ":", "List", "[", "Dict", "[", "str", ",", "int", "]", "]", "=", "[", "]", "\n", "# This is kind of useless, only needed for debugging in BasicTransitionFunction", "\n", "batch_actionidx2actionstr", ":", "List", "[", "List", "[", "str", "]", "]", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "(", "grammar_statelet", ",", "action2actionidx", ",", "actionidx2actionstr", ")", "=", "self", ".", "_create_grammar_statelet", "(", "\n", "languages", "[", "i", "]", ",", "actions", "[", "i", "]", "\n", ")", "\n", "\n", "initial_grammar_statelets", ".", "append", "(", "grammar_statelet", ")", "\n", "batch_actionidx2actionstr", ".", "append", "(", "actionidx2actionstr", ")", "\n", "batch_action2actionidx", ".", "append", "(", "action2actionidx", ")", "\n", "\n", "", "initial_rnn_states", "=", "self", ".", "_get_initial_rnn_state", "(", "\n", "question_encoded", "=", "encoded_question", ",", "\n", "question_mask", "=", "question_mask", ",", "\n", "question_encoded_finalstate", "=", "question_encoded_final_state", ",", "\n", "question_encoded_aslist", "=", "question_encoded_aslist", ",", "\n", "question_mask_aslist", "=", "question_mask_aslist", ",", "\n", ")", "\n", "\n", "initial_side_args", "=", "[", "[", "]", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "# Initial grammar state for the complete batch", "\n", "initial_state", "=", "GrammarBasedState", "(", "\n", "batch_indices", "=", "list", "(", "range", "(", "batch_size", ")", ")", ",", "\n", "action_history", "=", "[", "[", "]", "for", "_", "in", "range", "(", "batch_size", ")", "]", ",", "\n", "score", "=", "initial_score_list", ",", "\n", "rnn_state", "=", "initial_rnn_states", ",", "\n", "grammar_state", "=", "initial_grammar_statelets", ",", "\n", "possible_actions", "=", "actions", ",", "\n", "extras", "=", "batch_actionidx2actionstr", ",", "\n", "debug_info", "=", "initial_side_args", ",", "\n", ")", "\n", "\n", "return", "(", "initial_state", ",", "batch_action2actionidx", ",", "batch_actionidx2actionstr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT._select_indices_from_list": [[1742, 1745], ["None"], "methods", ["None"], ["", "def", "_select_indices_from_list", "(", "self", ",", "list", ",", "indices", ")", ":", "\n", "        ", "new_list", "=", "[", "list", "[", "i", "]", "for", "i", "in", "indices", "]", "\n", "return", "new_list", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.initialState_forInstanceIndices": [[1746, 1777], ["drop_parser_bert.DROPParserBERT._select_indices_from_list", "drop_parser_bert.DROPParserBERT._select_indices_from_list", "drop_parser_bert.DROPParserBERT._select_indices_from_list", "drop_parser_bert.DROPParserBERT._select_indices_from_list", "len", "drop_parser_bert.DROPParserBERT.getInitialDecoderState"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT._select_indices_from_list", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT._select_indices_from_list", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT._select_indices_from_list", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT._select_indices_from_list", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.getInitialDecoderState"], ["", "def", "initialState_forInstanceIndices", "(", "\n", "self", ",", "\n", "instances_list", ":", "List", "[", "int", "]", ",", "\n", "languages", ":", "List", "[", "DropLanguage", "]", ",", "\n", "actions", ":", "List", "[", "List", "[", "ProductionRule", "]", "]", ",", "\n", "encoded_question", ":", "torch", ".", "FloatTensor", ",", "\n", "question_mask", ":", "torch", ".", "FloatTensor", ",", "\n", "question_encoded_final_state", ":", "torch", ".", "FloatTensor", ",", "\n", "question_encoded_aslist", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", "question_mask_aslist", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", ")", ":", "\n", "\n", "        ", "s_languages", "=", "self", ".", "_select_indices_from_list", "(", "languages", ",", "instances_list", ")", "\n", "s_actions", "=", "self", ".", "_select_indices_from_list", "(", "actions", ",", "instances_list", ")", "\n", "s_encoded_question", "=", "encoded_question", "[", "instances_list", "]", "\n", "s_question_mask", "=", "question_mask", "[", "instances_list", "]", "\n", "s_question_encoded_final_state", "=", "question_encoded_final_state", "[", "instances_list", "]", "\n", "s_question_encoded_aslist", "=", "self", ".", "_select_indices_from_list", "(", "question_encoded_aslist", ",", "instances_list", ")", "\n", "s_question_mask_aslist", "=", "self", ".", "_select_indices_from_list", "(", "question_mask_aslist", ",", "instances_list", ")", "\n", "\n", "num_instances", "=", "len", "(", "instances_list", ")", "\n", "\n", "return", "self", ".", "getInitialDecoderState", "(", "\n", "s_languages", ",", "\n", "s_actions", ",", "\n", "s_encoded_question", ",", "\n", "s_question_mask", ",", "\n", "s_question_encoded_final_state", ",", "\n", "s_question_encoded_aslist", ",", "\n", "s_question_mask_aslist", ",", "\n", "num_instances", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.merge_final_states": [[1779, 1819], ["range", "len", "len", "len", "len", "supervised_instances.index", "unsupervised_instances.index"], "methods", ["None"], ["", "def", "merge_final_states", "(", "\n", "self", ",", "\n", "supervised_final_states", ",", "\n", "unsupervised_final_states", ",", "\n", "supervised_instances", ":", "List", "[", "int", "]", ",", "\n", "unsupervised_instances", ":", "List", "[", "int", "]", ",", "\n", ")", ":", "\n", "\n", "        ", "\"\"\" Supervised and unsupervised final_states are dicts with keys in order from 0 - len(dict)\n            The final keys are the instances' batch index which is stored in (un)supervised_instances list\n            i.e. index = supervised_instances[0] is the index of the instance in the original batch\n            whose final state is now in supervised_final_states[0].\n            Therefore the final_state should contain; final_state[index] = supervised_final_states[0]\n        \"\"\"", "\n", "\n", "if", "len", "(", "supervised_instances", ")", "==", "0", ":", "\n", "            ", "return", "unsupervised_final_states", "\n", "\n", "", "if", "len", "(", "unsupervised_instances", ")", "==", "0", ":", "\n", "            ", "return", "supervised_final_states", "\n", "\n", "", "batch_size", "=", "len", "(", "supervised_instances", ")", "+", "len", "(", "unsupervised_instances", ")", "\n", "\n", "final_states", "=", "{", "}", "\n", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "if", "i", "in", "supervised_instances", ":", "\n", "                ", "idx", "=", "supervised_instances", ".", "index", "(", "i", ")", "\n", "state_value", "=", "supervised_final_states", "[", "idx", "]", "\n", "final_states", "[", "i", "]", "=", "state_value", "\n", "", "else", ":", "\n", "                ", "idx", "=", "unsupervised_instances", ".", "index", "(", "i", ")", "\n", "# Unsupervised instances go through beam search and not always is a program found for them", "\n", "# If idx does not exist in unsupervised_final_states, don't add in final_states", "\n", "# Only add a instance_idx if it exists in final states -- not all beam-searches result in valid-paths", "\n", "if", "idx", "in", "unsupervised_final_states", ":", "\n", "                    ", "state_value", "=", "unsupervised_final_states", "[", "idx", "]", "\n", "final_states", "[", "i", "]", "=", "state_value", "\n", "\n", "", "", "", "return", "final_states", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.aux_count_loss": [[1820, 1856], ["torch.stack", "drop_parser_bert.DROPParserBERT._executor_parameters.passage_attention_to_count", "drop_parser_bert.DROPParserBERT._executor_parameters.passage_count_predictor", "torch.softmax", "torch.sum", "passage_attention.size", "answer_as_count.float.float.float", "torch.log", "torch.sum", "torch.argmax", "torch.argmax", "torch.sum().float", "count_mask.float", "count_mask.unsqueeze().float", "torch.sum", "count_mask.unsqueeze", "torch.sum", "torch.sum"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "def", "aux_count_loss", "(", "self", ",", "passage_attention", ",", "passage_mask", ",", "answer_as_count", ",", "count_mask", ")", ":", "\n", "        ", "if", "torch", ".", "sum", "(", "count_mask", ")", "==", "0", ":", "\n", "            ", "loss", ",", "accuracy", "=", "0.0", ",", "0.0", "\n", "return", "loss", ",", "accuracy", "\n", "\n", "", "batch_size", "=", "passage_attention", ".", "size", "(", ")", "[", "0", "]", "\n", "# List of (B, P) shaped tensors", "\n", "scaled_attentions", "=", "[", "passage_attention", "*", "sf", "for", "sf", "in", "self", ".", "_executor_parameters", ".", "passage_attention_scalingvals", "]", "\n", "# Shape: (B, passage_length, num_scaling_factors)", "\n", "scaled_passage_attentions", "=", "torch", ".", "stack", "(", "scaled_attentions", ",", "dim", "=", "2", ")", "\n", "# Shape: (B, hidden_dim)", "\n", "count_hidden_repr", "=", "self", ".", "_executor_parameters", ".", "passage_attention_to_count", "(", "\n", "scaled_passage_attentions", ",", "passage_mask", "\n", ")", "\n", "# Shape: (B, num_counts)", "\n", "passage_span_logits", "=", "self", ".", "_executor_parameters", ".", "passage_count_predictor", "(", "count_hidden_repr", ")", "\n", "count_distribution", "=", "torch", ".", "softmax", "(", "passage_span_logits", ",", "dim", "=", "1", ")", "\n", "\n", "loss", "=", "0", "\n", "accuracy", "=", "0", "\n", "if", "answer_as_count", "is", "not", "None", ":", "\n", "# (B, num_counts)", "\n", "            ", "answer_as_count", "=", "answer_as_count", ".", "float", "(", ")", "\n", "count_log_probs", "=", "torch", ".", "log", "(", "count_distribution", "+", "1e-40", ")", "\n", "log_likelihood", "=", "torch", ".", "sum", "(", "count_log_probs", "*", "answer_as_count", "*", "count_mask", ".", "unsqueeze", "(", "1", ")", ".", "float", "(", ")", ")", "\n", "\n", "loss", "=", "-", "1", "*", "log_likelihood", "\n", "loss", "=", "loss", "/", "torch", ".", "sum", "(", "count_mask", ")", ".", "float", "(", ")", "\n", "\n", "# List of predicted count idxs", "\n", "count_idx", "=", "torch", ".", "argmax", "(", "count_distribution", ",", "1", ")", "\n", "gold_count_idxs", "=", "torch", ".", "argmax", "(", "answer_as_count", ",", "1", ")", "\n", "correct_vec", "=", "(", "count_idx", "==", "gold_count_idxs", ")", ".", "float", "(", ")", "*", "count_mask", ".", "float", "(", ")", "\n", "accuracy", "=", "(", "torch", ".", "sum", "(", "correct_vec", ")", "/", "torch", ".", "sum", "(", "count_mask", ")", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "", "return", "loss", ",", "accuracy", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.masking_blockdiagonal": [[1857, 1883], ["torch.clamp", "torch.clamp", "torch.clamp.unsqueeze", "torch.clamp.unsqueeze", "allennlp.get_range_vector().unsqueeze", "allennlp.get_range_vector().unsqueeze", "allennlp.get_range_vector", "allennlp.get_range_vector", "allennlp.get_range_vector", "allennlp.get_range_vector"], "methods", ["None"], ["", "def", "masking_blockdiagonal", "(", "self", ",", "passage_length", ",", "window", ",", "device_id", ")", ":", "\n", "        ", "\"\"\" Make a (passage_length, passage_length) tensor M of 1 and -1 in which for each row x,\n            M[x, y] = -1 if y < x - window or y > x + window, else it is 1.\n            Basically for the x-th row, the [x-win, x+win] columns should be 1, and rest -1\n        \"\"\"", "\n", "\n", "# The lower and upper limit of token-idx that won't be masked for a given token", "\n", "lower", "=", "allenutil", ".", "get_range_vector", "(", "passage_length", ",", "device", "=", "device_id", ")", "-", "window", "\n", "upper", "=", "allenutil", ".", "get_range_vector", "(", "passage_length", ",", "device", "=", "device_id", ")", "+", "window", "\n", "lower", "=", "torch", ".", "clamp", "(", "lower", ",", "min", "=", "0", ",", "max", "=", "passage_length", "-", "1", ")", "\n", "upper", "=", "torch", ".", "clamp", "(", "upper", ",", "min", "=", "0", ",", "max", "=", "passage_length", "-", "1", ")", "\n", "lower_un", "=", "lower", ".", "unsqueeze", "(", "1", ")", "\n", "upper_un", "=", "upper", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# Range vector for each row", "\n", "lower_range_vector", "=", "allenutil", ".", "get_range_vector", "(", "passage_length", ",", "device", "=", "device_id", ")", ".", "unsqueeze", "(", "0", ")", "\n", "upper_range_vector", "=", "allenutil", ".", "get_range_vector", "(", "passage_length", ",", "device", "=", "device_id", ")", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "# Masks for lower and upper limits of the mask", "\n", "lower_mask", "=", "lower_range_vector", ">=", "lower_un", "\n", "upper_mask", "=", "upper_range_vector", "<=", "upper_un", "\n", "\n", "# Final-mask that we require", "\n", "inwindow_mask", "=", "(", "lower_mask", "==", "upper_mask", ")", ".", "float", "(", ")", "\n", "outwindow_mask", "=", "(", "lower_mask", "!=", "upper_mask", ")", ".", "float", "(", ")", "\n", "return", "inwindow_mask", ",", "outwindow_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.drop_parser_bert.DROPParserBERT.window_loss_numdate": [[1884, 1941], ["inwindow_probs.sum", "allennlp.replace_masked_values", "torch.sum", "allennlp.replace_masked_values", "torch.sum", "inwindow_mask.unsqueeze", "passage_tokenidx_mask.unsqueeze", "torch.log", "torch.sum", "outwindow_mask.unsqueeze", "passage_tokenidx_mask.unsqueeze", "torch.log", "torch.sum", "torch.sum", "torch.sum", "inwindow_mask.sum"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "def", "window_loss_numdate", "(", "self", ",", "passage_passage_alignment", ",", "passage_tokenidx_mask", ",", "inwindow_mask", ",", "outwindow_mask", ")", ":", "\n", "        ", "\"\"\"\n        The idea is to first softmax the similarity_scores to get a distribution over the date/num tokens from each\n        passage token.\n\n        For each passage_token,\n            -- increase the sum of prob for date/num tokens around it in a window (don't know which date/num is correct)\n            -- decrease the prod of prob for date/num tokens outside the window (know that all date/num are wrong)\n\n        Parameters:\n        -----------\n        passage_passage_similarity_scores: (batch_size, passage_length, passage_length)\n            For each passage_token, a similarity score to other passage_tokens for data/num\n            This should ideally, already be masked\n\n        passage_tokenidx_mask: (batch_size, passage_length)\n            Mask for tokens that are num/date\n\n        inwindow_mask: (passage_length, passage_length)\n            For row x, inwindow_mask[x, x-window : x+window] = 1 and 0 otherwise. Mask for a window around the token\n        outwindow_mask: (passage_length, passage_length)\n            Opposite of inwindow_mask. For each row x, the columns are 1 outside of a window around x\n        \"\"\"", "\n", "inwindow_mask", "=", "inwindow_mask", ".", "unsqueeze", "(", "0", ")", "*", "passage_tokenidx_mask", ".", "unsqueeze", "(", "1", ")", "\n", "inwindow_probs", "=", "passage_passage_alignment", "*", "inwindow_mask", "\n", "# This signifies that each token can distribute it's prob to nearby-date/num in anyway", "\n", "# Shape: (batch_size, passage_length)", "\n", "sum_inwindow_probs", "=", "inwindow_probs", ".", "sum", "(", "2", ")", "\n", "mask_sum", "=", "(", "inwindow_mask", ".", "sum", "(", "2", ")", ">", "0", ")", ".", "float", "(", ")", "\n", "# Image a row where mask = 0, there sum of probs will be zero and we need to compute masked_log", "\n", "masked_sum_inwindow_probs", "=", "allenutil", ".", "replace_masked_values", "(", "sum_inwindow_probs", ",", "mask_sum", ",", "replace_with", "=", "1e-40", ")", "\n", "log_sum_inwindow_probs", "=", "torch", ".", "log", "(", "masked_sum_inwindow_probs", "+", "1e-40", ")", "*", "mask_sum", "\n", "inwindow_likelihood", "=", "torch", ".", "sum", "(", "log_sum_inwindow_probs", ")", "\n", "if", "torch", ".", "sum", "(", "inwindow_mask", ")", ">", "0", ":", "\n", "            ", "inwindow_likelihood", "=", "inwindow_likelihood", "/", "torch", ".", "sum", "(", "inwindow_mask", ")", "\n", "", "else", ":", "\n", "            ", "inwindow_likelihood", "=", "0.0", "\n", "\n", "", "outwindow_mask", "=", "outwindow_mask", ".", "unsqueeze", "(", "0", ")", "*", "passage_tokenidx_mask", ".", "unsqueeze", "(", "1", ")", "\n", "# For tokens outside the window, increase entropy of the distribution. i.e. -\\sum p*log(p)", "\n", "# Since we'd like to distribute the weight equally to things outside the window", "\n", "# Shape: (batch_size, passage_length, passage_length)", "\n", "outwindow_probs", "=", "passage_passage_alignment", "*", "outwindow_mask", "\n", "\n", "masked_outwindow_probs", "=", "allenutil", ".", "replace_masked_values", "(", "outwindow_probs", ",", "outwindow_mask", ",", "replace_with", "=", "1e-40", ")", "\n", "outwindow_probs_log", "=", "torch", ".", "log", "(", "masked_outwindow_probs", "+", "1e-40", ")", "*", "outwindow_mask", "\n", "# Shape: (batch_length, passage_length)", "\n", "outwindow_negentropies", "=", "torch", ".", "sum", "(", "outwindow_probs", "*", "outwindow_probs_log", ")", "\n", "\n", "if", "torch", ".", "sum", "(", "outwindow_mask", ")", ">", "0", ":", "\n", "            ", "outwindow_negentropies", "=", "outwindow_negentropies", "/", "torch", ".", "sum", "(", "outwindow_mask", ")", "\n", "", "else", ":", "\n", "            ", "outwindow_negentropies", "=", "0.0", "\n", "\n", "# Increase inwindow likelihod and decrease outwindow-negative-entropy", "\n", "", "loss", "=", "-", "1", "*", "inwindow_likelihood", "+", "outwindow_negentropies", "\n", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.passage_attn_to_span.PassageAttnToSpan.__init__": [[23, 56], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "passage_attn_to_span.PassageAttnToSpan.passage_attention_to_span.get_output_dim", "torch.nn.Linear", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "initializers", "passage_attn_to_span.PassageAttnToSpan.passage_attention_to_span.get_output_dim", "torch.nn.Dropout", "len", "passage_attn_to_span.PassageAttnToSpan.passage_attention_to_span.get_input_dim"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "passage_attention_to_span", ":", "Seq2SeqEncoder", ",", "\n", "scaling", ":", "bool", "=", "False", ",", "\n", "dropout", ":", "float", "=", "0.0", ",", "\n", "initializers", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "super", "(", "PassageAttnToSpan", ",", "self", ")", ".", "__init__", "(", "vocab", "=", "vocab", ")", "\n", "\n", "self", ".", "_scaling", "=", "scaling", "\n", "self", ".", "scaling_vals", "=", "[", "1", ",", "2", ",", "5", ",", "10", "]", "\n", "\n", "self", ".", "passage_attention_to_span", "=", "passage_attention_to_span", "\n", "\n", "if", "self", ".", "_scaling", ":", "\n", "            ", "assert", "len", "(", "self", ".", "scaling_vals", ")", "==", "self", ".", "passage_attention_to_span", ".", "get_input_dim", "(", ")", "\n", "\n", "", "self", ".", "_span_rnn_hsize", "=", "self", ".", "passage_attention_to_span", ".", "get_output_dim", "(", ")", "\n", "\n", "self", ".", "passage_startend_predictor", "=", "torch", ".", "nn", ".", "Linear", "(", "self", ".", "passage_attention_to_span", ".", "get_output_dim", "(", ")", ",", "2", ")", "\n", "\n", "self", ".", "start_acc_metric", "=", "Average", "(", ")", "\n", "self", ".", "end_acc_metric", "=", "Average", "(", ")", "\n", "self", ".", "span_acc_metric", "=", "Average", "(", ")", "\n", "\n", "if", "dropout", ">", "0", ":", "\n", "            ", "self", ".", "_dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_dropout", "=", "lambda", "x", ":", "x", "\n", "\n", "", "initializers", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.passage_attn_to_span.PassageAttnToSpan.device_id": [[57, 59], ["allennlp.get_device_of"], "methods", ["None"], ["", "def", "device_id", "(", "self", ")", ":", "\n", "        ", "allenutil", ".", "get_device_of", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.passage_attn_to_span.PassageAttnToSpan.forward": [[60, 144], ["passage_attention.size", "passage_attention.new_zeros", "enumerate", "answer_as_passage_spans.long.long.long", "passage_attn_to_span.PassageAttnToSpan.passage_attention_to_span", "passage_attn_to_span.PassageAttnToSpan.passage_startend_predictor", "allennlp.replace_masked_values", "allennlp.replace_masked_values", "allennlp.masked_log_softmax", "allennlp.masked_log_softmax", "allennlp.replace_masked_values", "allennlp.replace_masked_values", "range", "torch.stack", "passage_attention.unsqueeze", "passage_attn_to_span.PassageAttnToSpan._get_span_answer_log_prob", "allennlp.models.reading_comprehension.util.get_best_span().squeeze", "passage_attn_to_span.PassageAttnToSpan.start_acc_metric", "passage_attn_to_span.PassageAttnToSpan.start_acc_metric", "passage_attn_to_span.PassageAttnToSpan.end_acc_metric", "passage_attn_to_span.PassageAttnToSpan.end_acc_metric", "passage_attn_to_span.PassageAttnToSpan.span_acc_metric", "passage_attn_to_span.PassageAttnToSpan.span_acc_metric", "allennlp.models.reading_comprehension.util.get_best_span", "span_start_log_probs[].unsqueeze", "span_end_log_probs[].unsqueeze"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.qanet.QANet._get_span_answer_log_prob"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "passage_attention", ":", "torch", ".", "Tensor", ",", "\n", "passage_lengths", ":", "List", "[", "int", "]", ",", "\n", "answer_as_passage_spans", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "batch_size", ",", "max_passage_length", "=", "passage_attention", ".", "size", "(", ")", "\n", "passage_mask", "=", "passage_attention", ".", "new_zeros", "(", "batch_size", ",", "max_passage_length", ")", "\n", "for", "i", ",", "passage_length", "in", "enumerate", "(", "passage_lengths", ")", ":", "\n", "            ", "passage_mask", "[", "i", ",", "0", ":", "passage_length", "]", "=", "1.0", "\n", "\n", "", "answer_as_passage_spans", "=", "answer_as_passage_spans", ".", "long", "(", ")", "\n", "\n", "passage_attention", "=", "passage_attention", "*", "passage_mask", "\n", "\n", "if", "self", ".", "_scaling", ":", "\n", "            ", "scaled_attentions", "=", "[", "passage_attention", "*", "sf", "for", "sf", "in", "self", ".", "scaling_vals", "]", "\n", "passage_attention_input", "=", "torch", ".", "stack", "(", "scaled_attentions", ",", "dim", "=", "2", ")", "\n", "", "else", ":", "\n", "            ", "passage_attention_input", "=", "passage_attention", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Shape: (batch_size, passage_length, span_rnn_hsize)", "\n", "", "passage_span_logits_repr", "=", "self", ".", "passage_attention_to_span", "(", "passage_attention_input", ",", "passage_mask", ")", "\n", "\n", "# Shape: (batch_size, passage_length, 2)", "\n", "passage_span_logits", "=", "self", ".", "passage_startend_predictor", "(", "passage_span_logits_repr", ")", "\n", "\n", "# Shape: (batch_size, passage_length)", "\n", "span_start_logits", "=", "passage_span_logits", "[", ":", ",", ":", ",", "0", "]", "\n", "span_end_logits", "=", "passage_span_logits", "[", ":", ",", ":", ",", "1", "]", "\n", "\n", "span_start_logits", "=", "allenutil", ".", "replace_masked_values", "(", "span_start_logits", ",", "passage_mask", ",", "-", "1e32", ")", "\n", "span_end_logits", "=", "allenutil", ".", "replace_masked_values", "(", "span_end_logits", ",", "passage_mask", ",", "-", "1e32", ")", "\n", "\n", "span_start_log_probs", "=", "allenutil", ".", "masked_log_softmax", "(", "span_start_logits", ",", "passage_mask", ")", "\n", "span_end_log_probs", "=", "allenutil", ".", "masked_log_softmax", "(", "span_end_logits", ",", "passage_mask", ")", "\n", "\n", "span_start_log_probs", "=", "allenutil", ".", "replace_masked_values", "(", "span_start_log_probs", ",", "passage_mask", ",", "-", "1e32", ")", "\n", "span_end_log_probs", "=", "allenutil", ".", "replace_masked_values", "(", "span_end_log_probs", ",", "passage_mask", ",", "-", "1e32", ")", "\n", "\n", "# Loss computation", "\n", "batch_likelihood", "=", "0", "\n", "output_dict", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "log_likelihood", "=", "self", ".", "_get_span_answer_log_prob", "(", "\n", "answer_as_spans", "=", "answer_as_passage_spans", "[", "i", "]", ",", "\n", "span_log_probs", "=", "(", "span_start_log_probs", "[", "i", "]", ",", "span_end_log_probs", "[", "i", "]", ")", ",", "\n", ")", "\n", "\n", "best_span", "=", "get_best_span", "(", "\n", "span_start_logits", "=", "span_start_log_probs", "[", "i", "]", ".", "unsqueeze", "(", "0", ")", ",", "\n", "span_end_logits", "=", "span_end_log_probs", "[", "i", "]", ".", "unsqueeze", "(", "0", ")", ",", "\n", ")", ".", "squeeze", "(", "0", ")", "\n", "\n", "correct_start", ",", "correct_end", "=", "False", ",", "False", "\n", "\n", "if", "best_span", "[", "0", "]", "==", "answer_as_passage_spans", "[", "i", "]", "[", "0", "]", "[", "0", "]", ":", "\n", "                ", "self", ".", "start_acc_metric", "(", "1", ")", "\n", "correct_start", "=", "True", "\n", "", "else", ":", "\n", "                ", "self", ".", "start_acc_metric", "(", "0", ")", "\n", "\n", "", "if", "best_span", "[", "1", "]", "==", "answer_as_passage_spans", "[", "i", "]", "[", "0", "]", "[", "1", "]", ":", "\n", "                ", "self", ".", "end_acc_metric", "(", "1", ")", "\n", "correct_end", "=", "True", "\n", "", "else", ":", "\n", "                ", "self", ".", "end_acc_metric", "(", "0", ")", "\n", "\n", "", "if", "correct_start", "and", "correct_end", ":", "\n", "                ", "self", ".", "span_acc_metric", "(", "1", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "span_acc_metric", "(", "0", ")", "\n", "\n", "", "batch_likelihood", "+=", "log_likelihood", "\n", "\n", "", "loss", "=", "-", "1.0", "*", "batch_likelihood", "\n", "\n", "batch_loss", "=", "loss", "/", "batch_size", "\n", "output_dict", "[", "\"loss\"", "]", "=", "batch_loss", "\n", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.passage_attn_to_span.PassageAttnToSpan.get_metrics": [[145, 153], ["passage_attn_to_span.PassageAttnToSpan.start_acc_metric.get_metric", "passage_attn_to_span.PassageAttnToSpan.end_acc_metric.get_metric", "passage_attn_to_span.PassageAttnToSpan.span_acc_metric.get_metric", "metric_dict.update"], "methods", ["None"], ["", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "metric_dict", "=", "{", "}", "\n", "start_acc", "=", "self", ".", "start_acc_metric", ".", "get_metric", "(", "reset", ")", "\n", "end_acc", "=", "self", ".", "end_acc_metric", ".", "get_metric", "(", "reset", ")", "\n", "span_acc", "=", "self", ".", "span_acc_metric", ".", "get_metric", "(", "reset", ")", "\n", "metric_dict", ".", "update", "(", "{", "\"st\"", ":", "start_acc", ",", "\"end\"", ":", "end_acc", ",", "\"span\"", ":", "span_acc", "}", ")", "\n", "\n", "return", "metric_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.passage_attn_to_span.PassageAttnToSpan._get_span_answer_log_prob": [[154, 205], ["answer_as_spans.unsqueeze.unsqueeze.unsqueeze", "span_start_log_probs.unsqueeze.unsqueeze.unsqueeze", "span_end_log_probs.unsqueeze.unsqueeze.unsqueeze", "allennlp.replace_masked_values", "allennlp.replace_masked_values", "torch.gather", "torch.gather", "allennlp.replace_masked_values", "allennlp.logsumexp"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_span_answer_log_prob", "(", "\n", "answer_as_spans", ":", "torch", ".", "LongTensor", ",", "span_log_probs", ":", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\" Compute the log_marginal_likelihood for the answer_spans given log_probs for start/end\n            Compute log_likelihood (product of start/end probs) of each ans_span\n            Sum the prob (logsumexp) for each span and return the log_likelihood\n\n        Parameters:\n        -----------\n        answer: ``torch.LongTensor`` Shape: (number_of_spans, 2)\n            These are the gold spans\n        span_log_probs: ``torch.FloatTensor``\n            2-Tuple with tensors of Shape: (length_of_sequence) for span_start/span_end log_probs\n\n        Returns:\n        log_marginal_likelihood_for_passage_span\n        \"\"\"", "\n", "\n", "# Unsqueezing dim=0 to make a batch_size of 1", "\n", "answer_as_spans", "=", "answer_as_spans", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "span_start_log_probs", ",", "span_end_log_probs", "=", "span_log_probs", "\n", "span_start_log_probs", "=", "span_start_log_probs", ".", "unsqueeze", "(", "0", ")", "\n", "span_end_log_probs", "=", "span_end_log_probs", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "# (batch_size, number_of_ans_spans)", "\n", "gold_passage_span_starts", "=", "answer_as_spans", "[", ":", ",", ":", ",", "0", "]", "\n", "gold_passage_span_ends", "=", "answer_as_spans", "[", ":", ",", ":", ",", "1", "]", "\n", "# Some spans are padded with index -1,", "\n", "# so we clamp those paddings to 0 and then mask after `torch.gather()`.", "\n", "gold_passage_span_mask", "=", "(", "gold_passage_span_starts", "!=", "-", "1", ")", ".", "long", "(", ")", "\n", "clamped_gold_passage_span_starts", "=", "allenutil", ".", "replace_masked_values", "(", "\n", "gold_passage_span_starts", ",", "gold_passage_span_mask", ",", "0", "\n", ")", "\n", "clamped_gold_passage_span_ends", "=", "allenutil", ".", "replace_masked_values", "(", "\n", "gold_passage_span_ends", ",", "gold_passage_span_mask", ",", "0", "\n", ")", "\n", "# Shape: (batch_size, # of answer spans)", "\n", "log_likelihood_for_span_starts", "=", "torch", ".", "gather", "(", "span_start_log_probs", ",", "1", ",", "clamped_gold_passage_span_starts", ")", "\n", "log_likelihood_for_span_ends", "=", "torch", ".", "gather", "(", "span_end_log_probs", ",", "1", ",", "clamped_gold_passage_span_ends", ")", "\n", "# Shape: (batch_size, # of answer spans)", "\n", "log_likelihood_for_spans", "=", "log_likelihood_for_span_starts", "+", "log_likelihood_for_span_ends", "\n", "# For those padded spans, we set their log probabilities to be very small negative value", "\n", "log_likelihood_for_spans", "=", "allenutil", ".", "replace_masked_values", "(", "\n", "log_likelihood_for_spans", ",", "gold_passage_span_mask", ",", "-", "1e7", "\n", ")", "\n", "# Shape: (batch_size, )", "\n", "log_marginal_likelihood_for_span", "=", "allenutil", ".", "logsumexp", "(", "log_likelihood_for_spans", ")", "\n", "\n", "return", "log_marginal_likelihood_for_span", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.passage_attn_to_span.PassageAttnToSpan._get_best_spans": [[206, 284], ["range", "len", "zip", "batch_best_spans.append", "batch_predicted_answers.append", "allennlp.models.reading_comprehension.util.get_best_span().squeeze", "instance_best_spans.append", "tuple", "instance_predicted_ans.append", "allennlp.models.reading_comprehension.util.get_best_span().squeeze.detach().cpu().numpy", "allennlp.models.reading_comprehension.util.get_best_span", "allennlp.models.reading_comprehension.util.get_best_span().squeeze.detach().cpu", "print", "print", "print", "print", "print", "print", "print", "print", "denotation._value[].unsqueeze", "denotation._value[].unsqueeze", "print", "print", "print", "print", "print", "print", "allennlp.models.reading_comprehension.util.get_best_span().squeeze.detach", "question_mask_aslist[].size", "len", "len", "passage_mask_aslist[].size", "len", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_best_spans", "(", "\n", "batch_denotations", ",", "\n", "batch_denotation_types", ",", "\n", "question_char_offsets", ",", "\n", "question_strs", ",", "\n", "passage_char_offsets", ",", "\n", "passage_strs", ",", "\n", "*", "args", ",", "\n", ")", ":", "\n", "        ", "\"\"\" For all SpanType denotations, get the best span\n\n        Parameters:\n        ----------\n        batch_denotations: List[List[Any]]\n        batch_denotation_types: List[List[str]]\n        \"\"\"", "\n", "\n", "(", "question_num_tokens", ",", "passage_num_tokens", ",", "question_mask_aslist", ",", "passage_mask_aslist", ")", "=", "args", "\n", "\n", "batch_best_spans", "=", "[", "]", "\n", "batch_predicted_answers", "=", "[", "]", "\n", "\n", "for", "instance_idx", "in", "range", "(", "len", "(", "batch_denotations", ")", ")", ":", "\n", "            ", "instance_prog_denotations", "=", "batch_denotations", "[", "instance_idx", "]", "\n", "instance_prog_types", "=", "batch_denotation_types", "[", "instance_idx", "]", "\n", "\n", "instance_best_spans", "=", "[", "]", "\n", "instance_predicted_ans", "=", "[", "]", "\n", "\n", "for", "denotation", ",", "progtype", "in", "zip", "(", "instance_prog_denotations", ",", "instance_prog_types", ")", ":", "\n", "# if progtype == \"QuestionSpanAnswwer\":", "\n", "# Distinction between QuestionSpanAnswer and PassageSpanAnswer is not needed currently,", "\n", "# since both classes store the start/end logits as a tuple", "\n", "# Shape: (2, )", "\n", "                ", "best_span", "=", "get_best_span", "(", "\n", "span_start_logits", "=", "denotation", ".", "_value", "[", "0", "]", ".", "unsqueeze", "(", "0", ")", ",", "\n", "span_end_logits", "=", "denotation", ".", "_value", "[", "1", "]", ".", "unsqueeze", "(", "0", ")", ",", "\n", ")", ".", "squeeze", "(", "0", ")", "\n", "instance_best_spans", ".", "append", "(", "best_span", ")", "\n", "\n", "predicted_span", "=", "tuple", "(", "best_span", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "if", "progtype", "==", "\"QuestionSpanAnswer\"", ":", "\n", "                    ", "try", ":", "\n", "                        ", "start_offset", "=", "question_char_offsets", "[", "instance_idx", "]", "[", "predicted_span", "[", "0", "]", "]", "[", "0", "]", "\n", "end_offset", "=", "question_char_offsets", "[", "instance_idx", "]", "[", "predicted_span", "[", "1", "]", "]", "[", "1", "]", "\n", "predicted_answer", "=", "question_strs", "[", "instance_idx", "]", "[", "start_offset", ":", "end_offset", "]", "\n", "", "except", ":", "\n", "                        ", "print", "(", ")", "\n", "print", "(", "f\"PredictedSpan: {predicted_span}\"", ")", "\n", "print", "(", "f\"Question numtoksn: {question_num_tokens[instance_idx]}\"", ")", "\n", "print", "(", "f\"QuesMaskLen: {question_mask_aslist[instance_idx].size()}\"", ")", "\n", "print", "(", "f\"StartLogProbs:{denotation._value[0]}\"", ")", "\n", "print", "(", "f\"EndLogProbs:{denotation._value[1]}\"", ")", "\n", "print", "(", "f\"LenofOffsets: {len(question_char_offsets[instance_idx])}\"", ")", "\n", "print", "(", "f\"QuesStrLen: {len(question_strs[instance_idx])}\"", ")", "\n", "\n", "", "", "elif", "progtype", "==", "\"PassageSpanAnswer\"", ":", "\n", "                    ", "try", ":", "\n", "                        ", "start_offset", "=", "passage_char_offsets", "[", "instance_idx", "]", "[", "predicted_span", "[", "0", "]", "]", "[", "0", "]", "\n", "end_offset", "=", "passage_char_offsets", "[", "instance_idx", "]", "[", "predicted_span", "[", "1", "]", "]", "[", "1", "]", "\n", "predicted_answer", "=", "passage_strs", "[", "instance_idx", "]", "[", "start_offset", ":", "end_offset", "]", "\n", "", "except", ":", "\n", "                        ", "print", "(", ")", "\n", "print", "(", "f\"PredictedSpan: {predicted_span}\"", ")", "\n", "print", "(", "f\"Passagenumtoksn: {passage_num_tokens[instance_idx]}\"", ")", "\n", "print", "(", "f\"PassageMaskLen: {passage_mask_aslist[instance_idx].size()}\"", ")", "\n", "print", "(", "f\"LenofOffsets: {len(passage_char_offsets[instance_idx])}\"", ")", "\n", "print", "(", "f\"PassageStrLen: {len(passage_strs[instance_idx])}\"", ")", "\n", "", "", "else", ":", "\n", "                    ", "raise", "NotImplementedError", "\n", "\n", "", "instance_predicted_ans", ".", "append", "(", "predicted_answer", ")", "\n", "\n", "", "batch_best_spans", ".", "append", "(", "instance_best_spans", ")", "\n", "batch_predicted_answers", ".", "append", "(", "instance_predicted_ans", ")", "\n", "\n", "", "return", "batch_best_spans", ",", "batch_predicted_answers", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.passage_attn_to_span.PassageAttnToSpan.passageattn_to_startendlogits": [[285, 306], ["passage_attention.new_zeros", "passage_attention.new_zeros", "print", "allennlp.replace_masked_values", "allennlp.replace_masked_values", "passage_attention.size", "passage_attention.size"], "methods", ["None"], ["", "def", "passageattn_to_startendlogits", "(", "self", ",", "passage_attention", ",", "passage_mask", ")", ":", "\n", "        ", "span_start_logits", "=", "passage_attention", ".", "new_zeros", "(", "passage_attention", ".", "size", "(", ")", ")", "\n", "span_end_logits", "=", "passage_attention", ".", "new_zeros", "(", "passage_attention", ".", "size", "(", ")", ")", "\n", "\n", "nonzeroindcs", "=", "(", "passage_attention", ">", "0", ")", ".", "nonzero", "(", ")", "\n", "\n", "startidx", "=", "nonzeroindcs", "[", "0", "]", "\n", "endidx", "=", "nonzeroindcs", "[", "-", "1", "]", "\n", "\n", "print", "(", "f\"{startidx} {endidx}\"", ")", "\n", "\n", "span_start_logits", "[", "startidx", "]", "=", "2.0", "\n", "span_end_logits", "[", "endidx", "]", "=", "2.0", "\n", "\n", "span_start_logits", "=", "allenutil", ".", "replace_masked_values", "(", "span_start_logits", ",", "passage_mask", ",", "-", "1e32", ")", "\n", "span_end_logits", "=", "allenutil", ".", "replace_masked_values", "(", "span_end_logits", ",", "passage_mask", ",", "-", "1e32", ")", "\n", "\n", "span_start_logits", "+=", "1e-7", "\n", "span_end_logits", "+=", "1e-7", "\n", "\n", "return", "(", "span_start_logits", ",", "span_end_logits", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.qanet.QANet.__init__": [[39, 88], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "phrase_layer.get_output_dim", "qanet.QANet._text_field_embedder.get_output_dim", "phrase_layer.get_input_dim", "phrase_layer.get_output_dim", "modeling_layer.get_input_dim", "modeling_layer.get_output_dim", "torch.nn.Linear", "allennlp.modules.Highway", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "allennlp.training.metrics.Average", "allennlp.training.metrics.DropEmAndF1", "initializer", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "text_field_embedder", ":", "TextFieldEmbedder", ",", "\n", "num_highway_layers", ":", "int", ",", "\n", "phrase_layer", ":", "Seq2SeqEncoder", ",", "\n", "matrix_attention_layer", ":", "MatrixAttention", ",", "\n", "modeling_layer", ":", "Seq2SeqEncoder", ",", "\n", "dropout", ":", "float", "=", "0.0", ",", "\n", "debug", ":", "bool", "=", "False", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", "regularizer", ":", "Optional", "[", "RegularizerApplicator", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "super", "(", "QANet", ",", "self", ")", ".", "__init__", "(", "vocab", "=", "vocab", ",", "regularizer", "=", "regularizer", ")", "\n", "\n", "question_encoding_dim", "=", "phrase_layer", ".", "get_output_dim", "(", ")", "\n", "self", ".", "_text_field_embedder", "=", "text_field_embedder", "\n", "\n", "text_embed_dim", "=", "self", ".", "_text_field_embedder", ".", "get_output_dim", "(", ")", "\n", "\n", "encoding_in_dim", "=", "phrase_layer", ".", "get_input_dim", "(", ")", "\n", "encoding_out_dim", "=", "phrase_layer", ".", "get_output_dim", "(", ")", "\n", "modeling_in_dim", "=", "modeling_layer", ".", "get_input_dim", "(", ")", "\n", "modeling_out_dim", "=", "modeling_layer", ".", "get_output_dim", "(", ")", "\n", "\n", "self", ".", "_embedding_proj_layer", "=", "torch", ".", "nn", ".", "Linear", "(", "text_embed_dim", ",", "encoding_in_dim", ")", "\n", "self", ".", "_highway_layer", "=", "Highway", "(", "encoding_in_dim", ",", "num_highway_layers", ")", "\n", "\n", "self", ".", "_encoding_proj_layer", "=", "torch", ".", "nn", ".", "Linear", "(", "encoding_in_dim", ",", "encoding_in_dim", ")", "\n", "self", ".", "_phrase_layer", "=", "phrase_layer", "\n", "\n", "self", ".", "_matrix_attention", "=", "matrix_attention_layer", "\n", "\n", "self", ".", "_modeling_proj_layer", "=", "torch", ".", "nn", ".", "Linear", "(", "encoding_out_dim", "*", "4", ",", "modeling_in_dim", ")", "\n", "self", ".", "_modeling_layer", "=", "modeling_layer", "\n", "\n", "self", ".", "_span_start_predictor", "=", "torch", ".", "nn", ".", "Linear", "(", "modeling_out_dim", "*", "2", ",", "1", ")", "\n", "self", ".", "_span_end_predictor", "=", "torch", ".", "nn", ".", "Linear", "(", "modeling_out_dim", "*", "2", ",", "1", ")", "\n", "\n", "if", "dropout", ">", "0", ":", "\n", "            ", "self", ".", "_dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_dropout", "=", "lambda", "x", ":", "x", "\n", "\n", "", "self", ".", "modelloss_metric", "=", "Average", "(", ")", "\n", "self", ".", "_drop_metrics", "=", "DropEmAndF1", "(", ")", "\n", "\n", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.qanet.QANet.forward": [[89, 233], ["len", "allennlp.get_text_field_mask().float", "allennlp.get_text_field_mask().float", "qanet.QANet._dropout", "qanet.QANet._dropout", "qanet.QANet._highway_layer", "qanet.QANet._highway_layer", "qanet.QANet._encoding_proj_layer", "qanet.QANet._encoding_proj_layer", "qanet.QANet._dropout", "qanet.QANet._dropout", "qanet.QANet._matrix_attention", "allennlp.masked_softmax", "allennlp.weighted_sum", "allennlp.masked_softmax", "torch.bmm", "allennlp.weighted_sum", "qanet.QANet._dropout", "range", "torch.cat", "qanet.QANet._span_start_predictor().squeeze", "torch.cat", "qanet.QANet._span_end_predictor().squeeze", "allennlp.replace_masked_values", "allennlp.replace_masked_values", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "allennlp.masked_log_softmax", "allennlp.masked_log_softmax", "allennlp.replace_masked_values", "allennlp.replace_masked_values", "allennlp.models.reading_comprehension.util.get_best_span", "output_dict.update", "qanet.QANet._text_field_embedder", "qanet.QANet._text_field_embedder", "qanet.QANet._embedding_proj_layer", "qanet.QANet._embedding_proj_layer", "qanet.QANet._phrase_layer", "qanet.QANet._phrase_layer", "qanet.QANet.transpose", "torch.cat", "qanet.QANet._modeling_proj_layer", "qanet.QANet._dropout", "modeled_passage_list.append", "range", "qanet.QANet.modelloss_metric", "range", "allennlp.get_text_field_mask", "allennlp.get_text_field_mask", "qanet.QANet._modeling_layer", "qanet.QANet._span_start_predictor", "qanet.QANet._span_end_predictor", "qanet.QANet._get_span_answer_log_prob", "question_tokens.append", "passage_tokens.append", "tuple", "output_dict[].append", "metadata[].get", "qanet.QANet._drop_metrics", "utils.tocpuNPList", "best_span[].detach().cpu().numpy", "best_span[].detach().cpu", "best_span[].detach"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.qanet.QANet._get_span_answer_log_prob", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "question", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", ",", "\n", "passage", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", ",", "\n", "passageidx2numberidx", ":", "torch", ".", "LongTensor", ",", "\n", "passage_number_values", ":", "List", "[", "int", "]", ",", "\n", "passageidx2dateidx", ":", "torch", ".", "LongTensor", ",", "\n", "passage_date_values", ":", "List", "[", "List", "[", "Date", "]", "]", ",", "\n", "actions", ":", "List", "[", "List", "[", "ProductionRule", "]", "]", ",", "\n", "datecomp_ques_event_date_groundings", ":", "List", "[", "Tuple", "[", "List", "[", "int", "]", ",", "List", "[", "int", "]", "]", "]", "=", "None", ",", "\n", "numcomp_qspan_num_groundings", ":", "List", "[", "Tuple", "[", "List", "[", "int", "]", ",", "List", "[", "int", "]", "]", "]", "=", "None", ",", "\n", "strongly_supervised", ":", "List", "[", "bool", "]", "=", "None", ",", "\n", "qtypes", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "qattn_supervision", ":", "torch", ".", "FloatTensor", "=", "None", ",", "\n", "answer_types", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "answer_as_passage_spans", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "answer_as_question_spans", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "epoch_num", ":", "List", "[", "int", "]", "=", "None", ",", "\n", "metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "batch_size", "=", "len", "(", "actions", ")", "\n", "\n", "if", "epoch_num", "is", "not", "None", ":", "\n", "# epoch_num in allennlp starts from 0", "\n", "            ", "epoch", "=", "epoch_num", "[", "0", "]", "+", "1", "\n", "", "else", ":", "\n", "            ", "epoch", "=", "None", "\n", "\n", "", "question_mask", "=", "allenutil", ".", "get_text_field_mask", "(", "question", ")", ".", "float", "(", ")", "\n", "passage_mask", "=", "allenutil", ".", "get_text_field_mask", "(", "passage", ")", ".", "float", "(", ")", "\n", "embedded_question", "=", "self", ".", "_dropout", "(", "self", ".", "_text_field_embedder", "(", "question", ")", ")", "\n", "embedded_passage", "=", "self", ".", "_dropout", "(", "self", ".", "_text_field_embedder", "(", "passage", ")", ")", "\n", "embedded_question", "=", "self", ".", "_highway_layer", "(", "self", ".", "_embedding_proj_layer", "(", "embedded_question", ")", ")", "\n", "embedded_passage", "=", "self", ".", "_highway_layer", "(", "self", ".", "_embedding_proj_layer", "(", "embedded_passage", ")", ")", "\n", "\n", "projected_embedded_question", "=", "self", ".", "_encoding_proj_layer", "(", "embedded_question", ")", "\n", "projected_embedded_passage", "=", "self", ".", "_encoding_proj_layer", "(", "embedded_passage", ")", "\n", "\n", "encoded_question", "=", "self", ".", "_dropout", "(", "self", ".", "_phrase_layer", "(", "projected_embedded_question", ",", "question_mask", ")", ")", "\n", "encoded_passage", "=", "self", ".", "_dropout", "(", "self", ".", "_phrase_layer", "(", "projected_embedded_passage", ",", "passage_mask", ")", ")", "\n", "\n", "# Shape: (batch_size, passage_length, question_length)", "\n", "passage_question_similarity", "=", "self", ".", "_matrix_attention", "(", "encoded_passage", ",", "encoded_question", ")", "\n", "# Shape: (batch_size, passage_length, question_length)", "\n", "passage_question_attention", "=", "allenutil", ".", "masked_softmax", "(", "\n", "passage_question_similarity", ",", "question_mask", ",", "memory_efficient", "=", "True", "\n", ")", "\n", "# Shape: (batch_size, passage_length, encoding_dim)", "\n", "passage_question_vectors", "=", "allenutil", ".", "weighted_sum", "(", "encoded_question", ",", "passage_question_attention", ")", "\n", "\n", "# Shape: (batch_size, question_length, passage_length)", "\n", "question_passage_attention", "=", "allenutil", ".", "masked_softmax", "(", "\n", "passage_question_similarity", ".", "transpose", "(", "1", ",", "2", ")", ",", "passage_mask", ",", "memory_efficient", "=", "True", "\n", ")", "\n", "# Shape: (batch_size, passage_length, passage_length)", "\n", "attention_over_attention", "=", "torch", ".", "bmm", "(", "passage_question_attention", ",", "question_passage_attention", ")", "\n", "# Shape: (batch_size, passage_length, encoding_dim)", "\n", "passage_passage_vectors", "=", "allenutil", ".", "weighted_sum", "(", "encoded_passage", ",", "attention_over_attention", ")", "\n", "\n", "# Shape: (batch_size, passage_length, encoding_dim * 4)", "\n", "merged_passage_attention_vectors", "=", "self", ".", "_dropout", "(", "\n", "torch", ".", "cat", "(", "\n", "[", "\n", "encoded_passage", ",", "\n", "passage_question_vectors", ",", "\n", "encoded_passage", "*", "passage_question_vectors", ",", "\n", "encoded_passage", "*", "passage_passage_vectors", ",", "\n", "]", ",", "\n", "dim", "=", "-", "1", ",", "\n", ")", "\n", ")", "\n", "\n", "modeled_passage_list", "=", "[", "self", ".", "_modeling_proj_layer", "(", "merged_passage_attention_vectors", ")", "]", "\n", "\n", "for", "_", "in", "range", "(", "3", ")", ":", "\n", "            ", "modeled_passage", "=", "self", ".", "_dropout", "(", "self", ".", "_modeling_layer", "(", "modeled_passage_list", "[", "-", "1", "]", ",", "passage_mask", ")", ")", "\n", "modeled_passage_list", ".", "append", "(", "modeled_passage", ")", "\n", "\n", "# Shape: (batch_size, passage_length, modeling_dim * 2))", "\n", "", "span_start_input", "=", "torch", ".", "cat", "(", "[", "modeled_passage_list", "[", "-", "3", "]", ",", "modeled_passage_list", "[", "-", "2", "]", "]", ",", "dim", "=", "-", "1", ")", "\n", "# Shape: (batch_size, passage_length)", "\n", "span_start_logits", "=", "self", ".", "_span_start_predictor", "(", "span_start_input", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "# Shape: (batch_size, passage_length, modeling_dim * 2)", "\n", "span_end_input", "=", "torch", ".", "cat", "(", "[", "modeled_passage_list", "[", "-", "3", "]", ",", "modeled_passage_list", "[", "-", "1", "]", "]", ",", "dim", "=", "-", "1", ")", "\n", "span_end_logits", "=", "self", ".", "_span_end_predictor", "(", "span_end_input", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "span_start_logits", "=", "allenutil", ".", "replace_masked_values", "(", "span_start_logits", ",", "passage_mask", ",", "-", "1e32", ")", "\n", "span_end_logits", "=", "allenutil", ".", "replace_masked_values", "(", "span_end_logits", ",", "passage_mask", ",", "-", "1e32", ")", "\n", "\n", "# Shape: (batch_size, passage_length)", "\n", "span_start_probs", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "span_start_logits", ",", "dim", "=", "-", "1", ")", "\n", "span_end_probs", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "span_end_logits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "span_start_logprob", "=", "allenutil", ".", "masked_log_softmax", "(", "span_start_logits", ",", "mask", "=", "passage_mask", ",", "dim", "=", "-", "1", ")", "\n", "span_end_logprob", "=", "allenutil", ".", "masked_log_softmax", "(", "span_end_logits", ",", "mask", "=", "passage_mask", ",", "dim", "=", "-", "1", ")", "\n", "span_start_logprob", "=", "allenutil", ".", "replace_masked_values", "(", "span_start_logprob", ",", "passage_mask", ",", "-", "1e32", ")", "\n", "span_end_logprob", "=", "allenutil", ".", "replace_masked_values", "(", "span_end_logprob", ",", "passage_mask", ",", "-", "1e32", ")", "\n", "\n", "best_span", "=", "get_best_span", "(", "span_start_logits", ",", "span_end_logits", ")", "\n", "\n", "output_dict", "=", "{", "\n", "\"passage_question_attention\"", ":", "passage_question_attention", ",", "\n", "\"span_start_logits\"", ":", "span_start_logits", ",", "\n", "\"span_start_probs\"", ":", "span_start_probs", ",", "\n", "\"span_end_logits\"", ":", "span_end_logits", ",", "\n", "\"span_end_probs\"", ":", "span_end_probs", ",", "\n", "\"best_span\"", ":", "best_span", ",", "\n", "}", "\n", "\n", "if", "answer_types", "is", "not", "None", ":", "\n", "            ", "loss", "=", "0", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "                ", "loss", "+=", "self", ".", "_get_span_answer_log_prob", "(", "\n", "answer_as_spans", "=", "answer_as_passage_spans", "[", "i", "]", ",", "\n", "span_log_probs", "=", "(", "span_start_logprob", "[", "i", "]", ",", "span_end_logprob", "[", "i", "]", ")", ",", "\n", ")", "\n", "\n", "", "loss", "=", "(", "-", "1.0", "*", "loss", ")", "/", "batch_size", "\n", "\n", "self", ".", "modelloss_metric", "(", "myutils", ".", "tocpuNPList", "(", "loss", ")", "[", "0", "]", ")", "\n", "output_dict", "[", "\"loss\"", "]", "=", "loss", "\n", "\n", "", "if", "metadata", "is", "not", "None", ":", "\n", "            ", "output_dict", "[", "\"best_span_str\"", "]", "=", "[", "]", "\n", "question_tokens", "=", "[", "]", "\n", "passage_tokens", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "                ", "question_tokens", ".", "append", "(", "metadata", "[", "i", "]", "[", "\"question_tokens\"", "]", ")", "\n", "passage_tokens", ".", "append", "(", "metadata", "[", "i", "]", "[", "\"passage_tokens\"", "]", ")", "\n", "passage_str", "=", "metadata", "[", "i", "]", "[", "\"original_passage\"", "]", "\n", "offsets", "=", "metadata", "[", "i", "]", "[", "\"passage_token_offsets\"", "]", "\n", "predicted_span", "=", "tuple", "(", "best_span", "[", "i", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "start_offset", "=", "offsets", "[", "predicted_span", "[", "0", "]", "]", "[", "0", "]", "\n", "end_offset", "=", "offsets", "[", "predicted_span", "[", "1", "]", "]", "[", "1", "]", "\n", "best_span_string", "=", "passage_str", "[", "start_offset", ":", "end_offset", "]", "\n", "output_dict", "[", "\"best_span_str\"", "]", ".", "append", "(", "best_span_string", ")", "\n", "answer_annotations", "=", "metadata", "[", "i", "]", ".", "get", "(", "\"answer_annotation\"", ")", "\n", "self", ".", "_drop_metrics", "(", "best_span_string", ",", "[", "answer_annotations", "]", ")", "\n", "\n", "", "", "output_dict", ".", "update", "(", "{", "\"metadata\"", ":", "metadata", "}", ")", "\n", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.qanet.QANet.get_metrics": [[234, 241], ["qanet.QANet.modelloss_metric.get_metric", "qanet.QANet._drop_metrics.get_metric", "metric_dict.update"], "methods", ["None"], ["", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "metric_dict", "=", "{", "}", "\n", "model_loss", "=", "self", ".", "modelloss_metric", ".", "get_metric", "(", "reset", ")", "\n", "exact_match", ",", "f1_score", "=", "self", ".", "_drop_metrics", ".", "get_metric", "(", "reset", ")", "\n", "metric_dict", ".", "update", "(", "{", "\"em\"", ":", "exact_match", ",", "\"f1\"", ":", "f1_score", ",", "\"model_loss\"", ":", "model_loss", "}", ")", "\n", "\n", "return", "metric_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.qanet.QANet._get_span_answer_log_prob": [[242, 295], ["answer_as_spans.unsqueeze.unsqueeze.unsqueeze", "span_start_log_probs.unsqueeze.unsqueeze.unsqueeze", "span_end_log_probs.unsqueeze.unsqueeze.unsqueeze", "allennlp.replace_masked_values", "allennlp.replace_masked_values", "torch.gather", "torch.gather", "allennlp.replace_masked_values", "allennlp.logsumexp"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_span_answer_log_prob", "(", "\n", "answer_as_spans", ":", "torch", ".", "LongTensor", ",", "span_log_probs", ":", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\" Compute the log_marginal_likelihood for the answer_spans given log_probs for start/end\n            Compute log_likelihood (product of start/end probs) of each ans_span\n            Sum the prob (logsumexp) for each span and return the log_likelihood\n\n        Parameters:\n        -----------\n        answer: ``torch.LongTensor`` Shape: (number_of_spans, 2)\n            These are the gold spans\n        span_log_probs: ``torch.FloatTensor``\n            2-Tuple with tensors of Shape: (length_of_sequence) for span_start/span_end log_probs\n\n        Returns:\n        log_marginal_likelihood_for_passage_span\n        \"\"\"", "\n", "# Unsqueezing dim=0 to make a batch_size of 1", "\n", "answer_as_spans", "=", "answer_as_spans", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "span_start_log_probs", ",", "span_end_log_probs", "=", "span_log_probs", "\n", "\n", "span_start_log_probs", "=", "span_start_log_probs", ".", "unsqueeze", "(", "0", ")", "\n", "span_end_log_probs", "=", "span_end_log_probs", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "# (batch_size, number_of_ans_spans)", "\n", "gold_passage_span_starts", "=", "answer_as_spans", "[", ":", ",", ":", ",", "0", "]", "\n", "gold_passage_span_ends", "=", "answer_as_spans", "[", ":", ",", ":", ",", "1", "]", "\n", "# Some spans are padded with index -1,", "\n", "# so we clamp those paddings to 0 and then mask after `torch.gather()`.", "\n", "gold_passage_span_mask", "=", "(", "gold_passage_span_starts", "!=", "-", "1", ")", ".", "long", "(", ")", "\n", "clamped_gold_passage_span_starts", "=", "allenutil", ".", "replace_masked_values", "(", "\n", "gold_passage_span_starts", ",", "gold_passage_span_mask", ",", "0", "\n", ")", "\n", "clamped_gold_passage_span_ends", "=", "allenutil", ".", "replace_masked_values", "(", "\n", "gold_passage_span_ends", ",", "gold_passage_span_mask", ",", "0", "\n", ")", "\n", "# Shape: (batch_size, # of answer spans)", "\n", "log_likelihood_for_span_starts", "=", "torch", ".", "gather", "(", "span_start_log_probs", ",", "1", ",", "clamped_gold_passage_span_starts", ")", "\n", "log_likelihood_for_span_ends", "=", "torch", ".", "gather", "(", "span_end_log_probs", ",", "1", ",", "clamped_gold_passage_span_ends", ")", "\n", "\n", "# Shape: (batch_size, # of answer spans)", "\n", "log_likelihood_for_spans", "=", "log_likelihood_for_span_starts", "+", "log_likelihood_for_span_ends", "\n", "# For those padded spans, we set their log probabilities to be very small negative value", "\n", "log_likelihood_for_spans", "=", "allenutil", ".", "replace_masked_values", "(", "\n", "log_likelihood_for_spans", ",", "gold_passage_span_mask", ",", "-", "1e7", "\n", ")", "\n", "\n", "# Shape: (batch_size, )", "\n", "log_marginal_likelihood_for_span", "=", "allenutil", ".", "logsumexp", "(", "log_likelihood_for_spans", ")", "\n", "\n", "return", "log_marginal_likelihood_for_span", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.qanet.QANet.find_valid_start_states": [[296, 324], ["range", "set", "range", "range", "range", "start_types[].add", "start_types[].add"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add"], ["", "@", "staticmethod", "\n", "def", "find_valid_start_states", "(", "\n", "answer_as_question_spans", ":", "torch", ".", "LongTensor", ",", "answer_as_passage_spans", ":", "torch", ".", "LongTensor", ",", "batch_size", ":", "int", "\n", ")", "->", "List", "[", "Set", "[", "Any", "]", "]", ":", "\n", "        ", "\"\"\" Firgure out valid start types based on gold answers\n            If answer as question (passage) span exist, QuestionSpanAnswer (PassageSpanAnswer) are valid start types\n\n        answer_as_question_spans: (B, N1, 2)\n        answer_as_passage_spans: (B, N2, 2)\n\n        Returns:\n        --------\n        start_types: `List[Set[Type]]`\n            For each instance, a set of possible start_types\n        \"\"\"", "\n", "\n", "# List containing 0/1 indicating whether a question / passage answer span is given", "\n", "passage_span_ans_bool", "=", "[", "(", "(", "answer_as_passage_spans", "[", "i", "]", "!=", "-", "1", ")", ".", "sum", "(", ")", ">", "0", ")", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "question_span_ans_bool", "=", "[", "(", "(", "answer_as_question_spans", "[", "i", "]", "!=", "-", "1", ")", ".", "sum", "(", ")", ">", "0", ")", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "start_types", "=", "[", "set", "(", ")", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "if", "passage_span_ans_bool", "[", "i", "]", ">", "0", ":", "\n", "                ", "start_types", "[", "i", "]", ".", "add", "(", "PassageSpanAnswer", ")", "\n", "", "if", "question_span_ans_bool", "[", "i", "]", ">", "0", ":", "\n", "                ", "start_types", "[", "i", "]", ".", "add", "(", "QuestionSpanAnswer", ")", "\n", "\n", "", "", "return", "start_types", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.qanet.QANet._get_best_spans": [[325, 403], ["range", "len", "zip", "batch_best_spans.append", "batch_predicted_answers.append", "allennlp.models.reading_comprehension.util.get_best_span().squeeze", "instance_best_spans.append", "tuple", "instance_predicted_ans.append", "allennlp.models.reading_comprehension.util.get_best_span().squeeze.detach().cpu().numpy", "allennlp.models.reading_comprehension.util.get_best_span", "allennlp.models.reading_comprehension.util.get_best_span().squeeze.detach().cpu", "print", "print", "print", "print", "print", "print", "print", "print", "denotation._value[].unsqueeze", "denotation._value[].unsqueeze", "print", "print", "print", "print", "print", "print", "allennlp.models.reading_comprehension.util.get_best_span().squeeze.detach", "question_mask_aslist[].size", "len", "len", "passage_mask_aslist[].size", "len", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_best_spans", "(", "\n", "batch_denotations", ",", "\n", "batch_denotation_types", ",", "\n", "question_char_offsets", ",", "\n", "question_strs", ",", "\n", "passage_char_offsets", ",", "\n", "passage_strs", ",", "\n", "*", "args", ",", "\n", ")", ":", "\n", "        ", "\"\"\" For all SpanType denotations, get the best span\n\n        Parameters:\n        ----------\n        batch_denotations: List[List[Any]]\n        batch_denotation_types: List[List[str]]\n        \"\"\"", "\n", "\n", "(", "question_num_tokens", ",", "passage_num_tokens", ",", "question_mask_aslist", ",", "passage_mask_aslist", ")", "=", "args", "\n", "\n", "batch_best_spans", "=", "[", "]", "\n", "batch_predicted_answers", "=", "[", "]", "\n", "\n", "for", "instance_idx", "in", "range", "(", "len", "(", "batch_denotations", ")", ")", ":", "\n", "            ", "instance_prog_denotations", "=", "batch_denotations", "[", "instance_idx", "]", "\n", "instance_prog_types", "=", "batch_denotation_types", "[", "instance_idx", "]", "\n", "\n", "instance_best_spans", "=", "[", "]", "\n", "instance_predicted_ans", "=", "[", "]", "\n", "\n", "for", "denotation", ",", "progtype", "in", "zip", "(", "instance_prog_denotations", ",", "instance_prog_types", ")", ":", "\n", "# if progtype == \"QuestionSpanAnswwer\":", "\n", "# Distinction between QuestionSpanAnswer and PassageSpanAnswer is not needed currently,", "\n", "# since both classes store the start/end logits as a tuple", "\n", "# Shape: (2, )", "\n", "                ", "best_span", "=", "get_best_span", "(", "\n", "span_end_logits", "=", "denotation", ".", "_value", "[", "0", "]", ".", "unsqueeze", "(", "0", ")", ",", "\n", "span_start_logits", "=", "denotation", ".", "_value", "[", "1", "]", ".", "unsqueeze", "(", "0", ")", ",", "\n", ")", ".", "squeeze", "(", "0", ")", "\n", "instance_best_spans", ".", "append", "(", "best_span", ")", "\n", "\n", "predicted_span", "=", "tuple", "(", "best_span", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "if", "progtype", "==", "\"QuestionSpanAnswer\"", ":", "\n", "                    ", "try", ":", "\n", "                        ", "start_offset", "=", "question_char_offsets", "[", "instance_idx", "]", "[", "predicted_span", "[", "0", "]", "]", "[", "0", "]", "\n", "end_offset", "=", "question_char_offsets", "[", "instance_idx", "]", "[", "predicted_span", "[", "1", "]", "]", "[", "1", "]", "\n", "predicted_answer", "=", "question_strs", "[", "instance_idx", "]", "[", "start_offset", ":", "end_offset", "]", "\n", "", "except", ":", "\n", "                        ", "print", "(", ")", "\n", "print", "(", "f\"PredictedSpan: {predicted_span}\"", ")", "\n", "print", "(", "f\"Question numtoksn: {question_num_tokens[instance_idx]}\"", ")", "\n", "print", "(", "f\"QuesMaskLen: {question_mask_aslist[instance_idx].size()}\"", ")", "\n", "print", "(", "f\"StartLogProbs:{denotation._value[0]}\"", ")", "\n", "print", "(", "f\"EndLogProbs:{denotation._value[1]}\"", ")", "\n", "print", "(", "f\"LenofOffsets: {len(question_char_offsets[instance_idx])}\"", ")", "\n", "print", "(", "f\"QuesStrLen: {len(question_strs[instance_idx])}\"", ")", "\n", "\n", "", "", "elif", "progtype", "==", "\"PassageSpanAnswer\"", ":", "\n", "                    ", "try", ":", "\n", "                        ", "start_offset", "=", "passage_char_offsets", "[", "instance_idx", "]", "[", "predicted_span", "[", "0", "]", "]", "[", "0", "]", "\n", "end_offset", "=", "passage_char_offsets", "[", "instance_idx", "]", "[", "predicted_span", "[", "1", "]", "]", "[", "1", "]", "\n", "predicted_answer", "=", "passage_strs", "[", "instance_idx", "]", "[", "start_offset", ":", "end_offset", "]", "\n", "", "except", ":", "\n", "                        ", "print", "(", ")", "\n", "print", "(", "f\"PredictedSpan: {predicted_span}\"", ")", "\n", "print", "(", "f\"Passagenumtoksn: {passage_num_tokens[instance_idx]}\"", ")", "\n", "print", "(", "f\"PassageMaskLen: {passage_mask_aslist[instance_idx].size()}\"", ")", "\n", "print", "(", "f\"LenofOffsets: {len(passage_char_offsets[instance_idx])}\"", ")", "\n", "print", "(", "f\"PassageStrLen: {len(passage_strs[instance_idx])}\"", ")", "\n", "", "", "else", ":", "\n", "                    ", "raise", "NotImplementedError", "\n", "\n", "", "instance_predicted_ans", ".", "append", "(", "predicted_answer", ")", "\n", "\n", "", "batch_best_spans", ".", "append", "(", "instance_best_spans", ")", "\n", "batch_predicted_answers", ".", "append", "(", "instance_predicted_ans", ")", "\n", "\n", "", "return", "batch_best_spans", ",", "batch_predicted_answers", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.qanet.QANet.datecompare_goldattn_to_sideargs": [[404, 426], ["range", "len", "zip", "zip"], "methods", ["None"], ["", "def", "datecompare_goldattn_to_sideargs", "(", "\n", "self", ",", "\n", "batch_actionseqs", ":", "List", "[", "List", "[", "List", "[", "str", "]", "]", "]", ",", "\n", "batch_actionseq_sideargs", ":", "List", "[", "List", "[", "List", "[", "Dict", "]", "]", "]", ",", "\n", "batch_gold_attentions", ":", "List", "[", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", "]", ",", "\n", ")", ":", "\n", "\n", "        ", "for", "ins_idx", "in", "range", "(", "len", "(", "batch_actionseqs", ")", ")", ":", "\n", "            ", "instance_programs", "=", "batch_actionseqs", "[", "ins_idx", "]", "\n", "instance_prog_sideargs", "=", "batch_actionseq_sideargs", "[", "ins_idx", "]", "\n", "instance_gold_attentions", "=", "batch_gold_attentions", "[", "ins_idx", "]", "\n", "for", "program", ",", "side_args", "in", "zip", "(", "instance_programs", ",", "instance_prog_sideargs", ")", ":", "\n", "                ", "first_qattn", "=", "True", "# This tells model which qent attention to use", "\n", "# print(side_args)", "\n", "# print()", "\n", "for", "action", ",", "sidearg_dict", "in", "zip", "(", "program", ",", "side_args", ")", ":", "\n", "                    ", "if", "action", "==", "\"PassageAttention -> find_PassageAttention\"", ":", "\n", "                        ", "if", "first_qattn", ":", "\n", "                            ", "sidearg_dict", "[", "\"question_attention\"", "]", "=", "instance_gold_attentions", "[", "0", "]", "\n", "first_qattn", "=", "False", "\n", "", "else", ":", "\n", "                            ", "sidearg_dict", "[", "\"question_attention\"", "]", "=", "instance_gold_attentions", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.qanet.QANet.get_date_compare_ques_attns": [[427, 482], ["qstr.split", "qstr.split", "torch.cuda.FloatTensor().fill_", "torch.cuda.FloatTensor().fill_", "qstr.split.index", "min", "len", "torch.cuda.FloatTensor().fill_.sum", "qstr.split.index", "qstr.split.index", "qstr.split.index", "torch.cuda.FloatTensor().fill_.sum", "torch.cuda.FloatTensor", "torch.cuda.FloatTensor", "qstr.split.index", "len", "qstr.split.index", "qstr.split.index", "qstr.split.index"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "", "", "", "", "", "def", "get_date_compare_ques_attns", "(", "self", ",", "qstr", ":", "str", ",", "masked_len", ":", "int", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\" Question only has one 'or'\n            Attn2 is after or until ?\n            Attn1 is after first ',' or ':' ('first', 'last', 'later') until the 'or'\n        \"\"\"", "\n", "\n", "or_split", "=", "qstr", ".", "split", "(", "\" or \"", ")", "\n", "assert", "len", "(", "or_split", ")", "==", "2", "\n", "\n", "tokens", "=", "qstr", ".", "split", "(", "\" \"", ")", "\n", "\n", "attn_1", "=", "torch", ".", "cuda", ".", "FloatTensor", "(", "masked_len", ",", "device", "=", "0", ")", ".", "fill_", "(", "0.0", ")", "\n", "attn_2", "=", "torch", ".", "cuda", ".", "FloatTensor", "(", "masked_len", ",", "device", "=", "0", ")", ".", "fill_", "(", "0.0", ")", "\n", "\n", "or_idx", "=", "tokens", ".", "index", "(", "\"or\"", ")", "\n", "# Last token is ? which we don't want to attend to", "\n", "attn_2", "[", "or_idx", "+", "1", ":", "len", "(", "tokens", ")", "-", "1", "]", "=", "1.0", "\n", "attn_2", "=", "attn_2", "/", "attn_2", ".", "sum", "(", ")", "\n", "\n", "# Gets first index of the item", "\n", "try", ":", "\n", "            ", "comma_idx", "=", "tokens", ".", "index", "(", "\",\"", ")", "\n", "", "except", ":", "\n", "            ", "comma_idx", "=", "100000", "\n", "", "try", ":", "\n", "            ", "colon_idx", "=", "tokens", ".", "index", "(", "\":\"", ")", "\n", "", "except", ":", "\n", "            ", "colon_idx", "=", "100000", "\n", "\n", "", "try", ":", "\n", "            ", "hyphen_idx", "=", "tokens", ".", "index", "(", "\"-\"", ")", "\n", "", "except", ":", "\n", "            ", "hyphen_idx", "=", "100000", "\n", "\n", "", "split_idx", "=", "min", "(", "comma_idx", ",", "colon_idx", ",", "hyphen_idx", ")", "\n", "\n", "if", "split_idx", "==", "100000", "or", "(", "or_idx", "-", "split_idx", "<=", "1", ")", ":", "\n", "# print(f\"{qstr} first_split:{split_idx} or:{or_idx}\")", "\n", "            ", "if", "\"first\"", "in", "tokens", ":", "\n", "                ", "split_idx", "=", "tokens", ".", "index", "(", "\"first\"", ")", "\n", "", "elif", "\"second\"", "in", "tokens", ":", "\n", "                ", "split_idx", "=", "tokens", ".", "index", "(", "\"second\"", ")", "\n", "", "elif", "\"last\"", "in", "tokens", ":", "\n", "                ", "split_idx", "=", "tokens", ".", "index", "(", "\"last\"", ")", "\n", "", "elif", "\"later\"", "in", "tokens", ":", "\n", "                ", "split_idx", "=", "tokens", ".", "index", "(", "\"later\"", ")", "\n", "", "else", ":", "\n", "                ", "split_idx", "=", "-", "1", "\n", "\n", "", "", "assert", "split_idx", "!=", "-", "1", ",", "f\"{qstr} {split_idx} {or_idx}\"", "\n", "\n", "attn_1", "[", "split_idx", "+", "1", ":", "or_idx", "]", "=", "1.0", "\n", "attn_1", "=", "attn_1", "/", "attn_1", ".", "sum", "(", ")", "\n", "\n", "return", "attn_1", ",", "attn_2", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.qanet.QANet.get_gold_quesattn_datecompare": [[483, 496], ["len", "range", "gold_ques_attns.append", "len", "len", "qanet.QANet.get_date_compare_ques_attns", "qstr.split"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.qanet.QANet.get_date_compare_ques_attns"], ["", "def", "get_gold_quesattn_datecompare", "(", "self", ",", "metadata", ",", "masked_len", ")", "->", "List", "[", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", "]", ":", "\n", "        ", "batch_size", "=", "len", "(", "metadata", ")", "\n", "\n", "gold_ques_attns", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "question_tokens", ":", "List", "[", "str", "]", "=", "metadata", "[", "i", "]", "[", "\"question_tokens\"", "]", "\n", "qstr", "=", "\" \"", ".", "join", "(", "question_tokens", ")", "\n", "assert", "len", "(", "qstr", ".", "split", "(", "\" \"", ")", ")", "==", "len", "(", "question_tokens", ")", "\n", "\n", "gold_ques_attns", ".", "append", "(", "self", ".", "get_date_compare_ques_attns", "(", "qstr", ",", "masked_len", ")", ")", "\n", "\n", "", "return", "gold_ques_attns", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.models.qanet.getGoldLF": [[31, 35], ["qent1_action.split", "qent2_action.split", "qstr_action.split"], "function", ["None"], ["def", "getGoldLF", "(", "qent1_action", ",", "qent2_action", ",", "qstr_action", ")", ":", "\n", "    ", "qent1", ",", "qent2", ",", "qstr", "=", "(", "qent1_action", ".", "split", "(", "\" -> \"", ")", "[", "1", "]", ",", "qent2_action", ".", "split", "(", "\" -> \"", ")", "[", "1", "]", ",", "qstr_action", ".", "split", "(", "\" -> \"", ")", "[", "1", "]", ")", "\n", "# These qent1, qent2, and qstr are actions", "\n", "return", "f\"{GOLD_BOOL_LF[0]} {qent1} {qstr}{GOLD_BOOL_LF[1]} {qent2} {qstr}{GOLD_BOOL_LF[2]}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.Date.__init__": [[22, 26], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "year", ":", "int", ",", "month", ":", "int", ",", "day", ":", "int", ")", "->", "None", ":", "\n", "        ", "self", ".", "year", "=", "year", "\n", "self", ".", "month", "=", "month", "\n", "self", ".", "day", "=", "day", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.Date.__eq__": [[27, 37], ["isinstance", "allennlp_semparse.common.ExecutionError"], "methods", ["None"], ["", "def", "__eq__", "(", "self", ",", "other", ")", "->", "bool", ":", "\n", "# Note that the logic below renders equality to be non-transitive. That is,", "\n", "# Date(2018, -1, -1) == Date(2018, 2, 3) and Date(2018, -1, -1) == Date(2018, 4, 5)", "\n", "# but Date(2018, 2, 3) != Date(2018, 4, 5).", "\n", "        ", "if", "not", "isinstance", "(", "other", ",", "Date", ")", ":", "\n", "            ", "raise", "ExecutionError", "(", "\"Can only compare Dates with Dates\"", ")", "\n", "", "year_is_same", "=", "self", ".", "year", "==", "-", "1", "or", "other", ".", "year", "==", "-", "1", "or", "self", ".", "year", "==", "other", ".", "year", "\n", "month_is_same", "=", "self", ".", "month", "==", "-", "1", "or", "other", ".", "month", "==", "-", "1", "or", "self", ".", "month", "==", "other", ".", "month", "\n", "day_is_same", "=", "self", ".", "day", "==", "-", "1", "or", "other", ".", "day", "==", "-", "1", "or", "self", ".", "day", "==", "other", ".", "day", "\n", "return", "year_is_same", "and", "month_is_same", "and", "day_is_same", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.Date.__gt__": [[38, 78], ["isinstance", "allennlp_semparse.common.ExecutionError"], "methods", ["None"], ["", "def", "__gt__", "(", "self", ",", "other", ")", "->", "bool", ":", "\n", "# pylint: disable=too-many-return-statements", "\n", "# The logic below is tricky, and is based on some assumptions we make about date comparison.", "\n", "# Year, month or day being -1 means that we do not know its value. In those cases, the", "\n", "# we consider the comparison to be undefined, and return False if all the fields that are", "\n", "# more significant than the field being compared are equal. However, when year is -1 for both", "\n", "# dates being compared, it is safe to assume that the year is not specified because it is", "\n", "# the same. So we make an exception just in that case. That is, we deem the comparison", "\n", "# undefined only when one of the year values is -1, but not both.", "\n", "        ", "if", "not", "isinstance", "(", "other", ",", "Date", ")", ":", "\n", "            ", "raise", "ExecutionError", "(", "\"Can only compare Dates with Dates\"", ")", "\n", "# We're doing an exclusive or below.", "\n", "", "if", "(", "self", ".", "year", "==", "-", "1", ")", "!=", "(", "other", ".", "year", "==", "-", "1", ")", ":", "\n", "# Penalize the underspecified date", "\n", "            ", "if", "self", ".", "year", "==", "-", "1", ":", "\n", "                ", "return", "False", "\n", "", "else", ":", "\n", "                ", "return", "True", "\n", "# return False  # comparison undefined", "\n", "# If both years are -1, we proceed.", "\n", "", "", "if", "self", ".", "year", "!=", "other", ".", "year", ":", "\n", "            ", "return", "self", ".", "year", ">", "other", ".", "year", "\n", "# The years are equal and not -1, or both are -1.", "\n", "\n", "", "if", "self", ".", "month", "==", "-", "1", "and", "other", ".", "month", "==", "-", "1", ":", "\n", "            ", "return", "False", "\n", "", "if", "self", ".", "month", "==", "-", "1", ":", "\n", "            ", "return", "False", "\n", "", "if", "other", ".", "month", "==", "-", "1", ":", "\n", "            ", "return", "True", "\n", "", "if", "self", ".", "month", "!=", "other", ".", "month", ":", "\n", "            ", "return", "self", ".", "month", ">", "other", ".", "month", "\n", "# The months and years are equal and not -1", "\n", "", "if", "self", ".", "day", "==", "-", "1", "and", "other", ".", "day", "==", "-", "1", ":", "\n", "            ", "return", "False", "\n", "", "if", "self", ".", "day", "==", "-", "1", ":", "\n", "            ", "return", "False", "\n", "", "if", "other", ".", "day", "==", "-", "1", ":", "\n", "            ", "return", "True", "\n", "", "return", "self", ".", "day", ">", "other", ".", "day", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.Date.__ge__": [[79, 83], ["isinstance", "allennlp_semparse.common.ExecutionError"], "methods", ["None"], ["", "def", "__ge__", "(", "self", ",", "other", ")", "->", "bool", ":", "\n", "        ", "if", "not", "isinstance", "(", "other", ",", "Date", ")", ":", "\n", "            ", "raise", "ExecutionError", "(", "\"Can only compare Dates with Dates\"", ")", "\n", "", "return", "self", ">", "other", "or", "self", "==", "other", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.Date.__str__": [[84, 86], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "f\"{self.year}/{self.month}/{self.day}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.Date.year_diff": [[87, 100], ["isinstance"], "methods", ["None"], ["", "def", "year_diff", "(", "self", ",", "other", ")", ":", "\n", "        ", "\"\"\" Returns the difference in years between two dates.\n            1. Either of the years is not defined, return -1\n            ~ ~ ~ ~ 2. The difference in self - other is negative ~ ~ ~ ~\n        \"\"\"", "\n", "assert", "isinstance", "(", "other", ",", "Date", ")", ",", "\"Given object is not a date instance\"", "\n", "other", ":", "Date", "=", "other", "\n", "year1", ",", "year2", "=", "self", ".", "year", ",", "other", ".", "year", "\n", "if", "year1", "==", "-", "1", "or", "year2", "==", "-", "1", ":", "\n", "            ", "return", "-", "1", "\n", "", "else", ":", "\n", "            ", "year_diff", "=", "year1", "-", "year2", "\n", "return", "year_diff", "\n", "# if year_diff > 0:", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.PassageSpanAnswer.__init__": [[107, 122], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "passage_span_start_log_probs", ":", "Tensor", ",", "\n", "passage_span_end_log_probs", ":", "Tensor", ",", "\n", "start_logits", ",", "\n", "end_logits", ",", "\n", "loss", "=", "0.0", ",", "\n", "debug_value", "=", "\"\"", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\" Tuple of start_log_probs and end_log_probs tensor \"\"\"", "\n", "self", ".", "start_logits", "=", "start_logits", "\n", "self", ".", "end_logits", "=", "end_logits", "\n", "self", ".", "_value", "=", "(", "passage_span_start_log_probs", ",", "passage_span_end_log_probs", ")", "\n", "self", ".", "loss", "=", "loss", "\n", "self", ".", "debug_value", "=", "debug_value", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.QuestionSpanAnswer.__init__": [[125, 140], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "question_span_start_log_probs", ":", "Tensor", ",", "\n", "question_span_end_log_probs", ":", "Tensor", ",", "\n", "start_logits", ",", "\n", "end_logits", ",", "\n", "loss", "=", "0.0", ",", "\n", "debug_value", "=", "\"\"", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\" Tuple of start_log_probs and end_log_probs tensor \"\"\"", "\n", "self", ".", "start_logits", "=", "start_logits", "\n", "self", ".", "end_logits", "=", "end_logits", "\n", "self", ".", "_value", "=", "(", "question_span_start_log_probs", ",", "question_span_end_log_probs", ")", "\n", "self", ".", "loss", "=", "loss", "\n", "self", ".", "debug_value", "=", "debug_value", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.QuestionAttention.__init__": [[143, 146], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "question_attention", ",", "debug_value", "=", "\"\"", ")", ":", "\n", "        ", "self", ".", "_value", "=", "question_attention", "\n", "self", ".", "debug_value", "=", "debug_value", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.PassageAttention.__init__": [[149, 153], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "passage_attention", ",", "loss", "=", "0.0", ",", "debug_value", "=", "\"\"", ")", ":", "\n", "        ", "self", ".", "_value", "=", "passage_attention", "\n", "self", ".", "loss", "=", "loss", "\n", "self", ".", "debug_value", "=", "debug_value", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.PassageAttention_answer.__init__": [[156, 160], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "passage_attention", ",", "loss", "=", "0.0", ",", "debug_value", "=", "\"\"", ")", ":", "\n", "        ", "self", ".", "_value", "=", "passage_attention", "\n", "self", ".", "loss", "=", "loss", "\n", "self", ".", "debug_value", "=", "debug_value", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.YearDifference.__init__": [[163, 167], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "year_difference_dist", ",", "loss", "=", "0.0", ",", "debug_value", "=", "\"\"", ")", ":", "\n", "        ", "self", ".", "_value", "=", "year_difference_dist", "\n", "self", ".", "loss", "=", "loss", "\n", "self", ".", "debug_value", "=", "debug_value", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.PassageNumber.__init__": [[170, 174], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "passage_number_dist", ",", "loss", "=", "0.0", ",", "debug_value", "=", "\"\"", ")", ":", "\n", "        ", "self", ".", "_value", "=", "passage_number_dist", "\n", "self", ".", "loss", "=", "loss", "\n", "self", ".", "debug_value", "=", "debug_value", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.ComposedNumber.__init__": [[177, 181], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "composed_number_dist", ",", "loss", "=", "0.0", ",", "debug_value", "=", "\"\"", ")", ":", "\n", "        ", "self", ".", "_value", "=", "composed_number_dist", "\n", "self", ".", "loss", "=", "loss", "\n", "self", ".", "debug_value", "=", "debug_value", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.CountNumber.__init__": [[184, 188], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "count_number_dist", ",", "loss", "=", "0.0", ",", "debug_value", "=", "\"\"", ")", ":", "\n", "        ", "self", ".", "_value", "=", "count_number_dist", "\n", "self", ".", "loss", "=", "loss", "\n", "self", ".", "debug_value", "=", "debug_value", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.__init__": [[253, 419], ["allennlp_semparse.domain_languages.DomainLanguage.__init__", "passage_tokenidx2dateidx.long", "passage_tokenidx2dateidx_mask.long", "passage_tokenidx2dateidx_mask.float", "len", "passage_tokenidx2numidx.long", "passage_tokenidx2numidx_mask.long", "passage_tokenidx2numidx_mask.float", "len", "len", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "allennlp.move_to_device", "add_num_combination_indices.long", "sub_num_combination_indices.long", "drop_language.DropLanguage.initialize", "allennlp.move_to_device", "allennlp.move_to_device", "drop_language.DropLanguage.passage_mask.size", "drop_language.DropLanguage.passage_sentboundary_mask.unsqueeze", "drop_language.DropLanguage.modeled_passage.size", "drop_language.DropLanguage.encoded_question.size", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "drop_language.DropLanguage.passage_num_values.index", "range"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.initialize"], ["def", "__init__", "(", "\n", "self", ",", "\n", "rawemb_question", ":", "Tensor", ",", "\n", "embedded_question", ":", "Tensor", ",", "\n", "encoded_question", ":", "Tensor", ",", "\n", "rawemb_passage", ":", "Tensor", ",", "\n", "embedded_passage", ":", "Tensor", ",", "\n", "encoded_passage", ":", "Tensor", ",", "\n", "question_mask", ":", "Tensor", ",", "\n", "passage_mask", ":", "Tensor", ",", "\n", "passage_sentence_boundaries", ":", "Tensor", ",", "\n", "passage_tokenidx2dateidx", ":", "torch", ".", "LongTensor", ",", "\n", "passage_date_values", ":", "List", "[", "Date", "]", ",", "\n", "passage_tokenidx2numidx", ":", "torch", ".", "LongTensor", ",", "\n", "passage_num_values", ":", "List", "[", "float", "]", ",", "\n", "composed_numbers", ":", "List", "[", "float", "]", ",", "\n", "passage_number_sortedtokenidxs", ":", "List", "[", "int", "]", ",", "\n", "add_num_combination_indices", ":", "Tensor", ",", "\n", "sub_num_combination_indices", ":", "Tensor", ",", "\n", "year_differences", ":", "List", "[", "int", "]", ",", "\n", "year_differences_mat", ":", "np", ".", "array", ",", "\n", "count_num_values", ":", "List", "[", "int", "]", ",", "\n", "question_passage_attention", ":", "Tensor", ",", "\n", "passage_question_attention", ":", "Tensor", ",", "\n", "passage_token2date_alignment", ":", "Tensor", ",", "\n", "passage_token2startdate_alignment", ":", "Tensor", ",", "\n", "passage_token2enddate_alignment", ":", "Tensor", ",", "\n", "passage_token2num_alignment", ":", "Tensor", ",", "\n", "parameters", ":", "ExecutorParameters", ",", "\n", "modeled_passage", ":", "Tensor", "=", "None", ",", "\n", "start_types", "=", "None", ",", "\n", "device_id", ":", "int", "=", "-", "1", ",", "\n", "max_samples", "=", "5", ",", "\n", "metadata", "=", "{", "}", ",", "\n", "debug", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "if", "start_types", "is", "None", ":", "\n", "            ", "start_types", "=", "{", "PassageSpanAnswer", ",", "YearDifference", ",", "PassageNumber", ",", "ComposedNumber", ",", "CountNumber", "}", "\n", "# QuestionSpanAnswer - could be one", "\n", "\n", "", "super", "(", ")", ".", "__init__", "(", "start_types", "=", "start_types", ")", "\n", "\n", "if", "embedded_question", "is", "None", ":", "\n", "            ", "return", "\n", "\n", "", "self", ".", "rawemb_question", "=", "rawemb_question", "\n", "self", ".", "embedded_question", "=", "embedded_question", "\n", "self", ".", "encoded_question", "=", "encoded_question", "\n", "self", ".", "question_mask", "=", "question_mask", "\n", "\n", "self", ".", "rawemb_passage", "=", "rawemb_passage", "\n", "self", ".", "embedded_passage", "=", "embedded_passage", "\n", "self", ".", "encoded_passage", "=", "encoded_passage", "\n", "self", ".", "modeled_passage", "=", "modeled_passage", "\n", "self", ".", "passage_mask", "=", "passage_mask", "\n", "self", ".", "passage_length", "=", "self", ".", "passage_mask", ".", "size", "(", ")", "[", "0", "]", "\n", "self", ".", "passage_sentence_boundaries", "=", "passage_sentence_boundaries", "# LongTensor of shape (numsent, 2)", "\n", "self", ".", "passage_sentboundary_mask", "=", "(", "self", ".", "passage_sentence_boundaries", "[", ":", ",", "0", "]", ">=", "0", ")", ".", "long", "(", ")", "# Shape: (num_sents)", "\n", "self", ".", "passage_sentence_boundaries_masked", "=", "(", "self", ".", "passage_sentence_boundaries", "*", "\n", "self", ".", "passage_sentboundary_mask", ".", "unsqueeze", "(", "1", ")", ")", "\n", "self", ".", "passage_sentence_starts_masked", "=", "self", ".", "passage_sentence_boundaries", "[", ":", ",", "0", "]", "*", "self", ".", "passage_sentboundary_mask", "\n", "self", ".", "passage_sentence_ends_masked", "=", "self", ".", "passage_sentence_boundaries", "[", ":", ",", "1", "]", "*", "self", ".", "passage_sentboundary_mask", "\n", "\n", "self", ".", "passage_encoding_dim", "=", "self", ".", "modeled_passage", ".", "size", "(", ")", "[", "-", "1", "]", "\n", "self", ".", "question_encoding_dim", "=", "self", ".", "encoded_question", ".", "size", "(", ")", "[", "-", "1", "]", "\n", "\n", "# Shape: (passage_length, )", "\n", "self", ".", "passage_tokenidx2dateidx", "=", "passage_tokenidx2dateidx", ".", "long", "(", ")", "\n", "passage_tokenidx2dateidx_mask", "=", "self", ".", "passage_tokenidx2dateidx", ">", "-", "1", "\n", "self", ".", "passage_datetokens_mask_long", "=", "passage_tokenidx2dateidx_mask", ".", "long", "(", ")", "\n", "self", ".", "passage_datetokens_mask_float", "=", "passage_tokenidx2dateidx_mask", ".", "float", "(", ")", "\n", "# List[Date] - number of unique dates in the passage", "\n", "self", ".", "passage_date_values", ":", "List", "[", "Date", "]", "=", "passage_date_values", "\n", "self", ".", "num_passage_dates", "=", "len", "(", "self", ".", "passage_date_values", ")", "\n", "\n", "# Shape: (passage_length, )", "\n", "self", ".", "passage_tokenidx2numidx", "=", "passage_tokenidx2numidx", ".", "long", "(", ")", "\n", "passage_tokenidx2numidx_mask", "=", "self", ".", "passage_tokenidx2numidx", ">", "-", "1", "\n", "self", ".", "passage_numtokens_mask_long", "=", "passage_tokenidx2numidx_mask", ".", "long", "(", ")", "\n", "self", ".", "passage_numtokens_mask_float", "=", "passage_tokenidx2numidx_mask", ".", "float", "(", ")", "\n", "# List[float] - number of unique numbers in the passage (includes implicit numbers)", "\n", "self", ".", "passage_num_values", ":", "List", "[", "float", "]", "=", "passage_num_values", "\n", "self", ".", "composed_numbers", ":", "List", "[", "float", "]", "=", "composed_numbers", "\n", "# List[int] - number-token-idxs in an order so their values are sorted. Needed to max/min pattn", "\n", "self", ".", "passage_number_sortedtokenidxs", "=", "passage_number_sortedtokenidxs", "\n", "self", ".", "num_passage_nums", "=", "len", "(", "self", ".", "passage_num_values", ")", "\n", "self", ".", "num_composed_nums", "=", "len", "(", "self", ".", "composed_numbers", ")", "\n", "\n", "# List[int ] -- Shape: (num_implicit_numbers)", "\n", "implicit_num_indices", "=", "torch", ".", "LongTensor", "(", "[", "self", ".", "passage_num_values", ".", "index", "(", "x", ")", "\n", "for", "x", "in", "DropLanguage", ".", "implicit_numbers", "]", ")", "\n", "self", ".", "implicit_num_indices", "=", "allenutil", ".", "move_to_device", "(", "implicit_num_indices", ",", "cuda_device", "=", "device_id", ")", "\n", "\n", "# Shape: (size_composed_numbers, max_num_combinations, 2) -- for each number in composed_nums (dim=0),", "\n", "#  indices for other passage_number combinations (dim=1) that lead to this number using the op.", "\n", "#  ComposedNum[i] = PassageNum(M[i,j,0]) OP PassageNum(M[i,j,1]) \\forall j", "\n", "# Since all numbers won't have same num of combinations, these indices are padded w/ -1", "\n", "self", ".", "add_num_combination_indices", "=", "add_num_combination_indices", ".", "long", "(", ")", "\n", "self", ".", "sub_num_combination_indices", "=", "sub_num_combination_indices", ".", "long", "(", ")", "\n", "self", ".", "add_num_combination_mask", "=", "(", "self", ".", "add_num_combination_indices", ">", "-", "1", ")", ".", "long", "(", ")", "\n", "self", ".", "sub_num_combination_mask", "=", "(", "self", ".", "sub_num_combination_indices", ">", "-", "1", ")", ".", "long", "(", ")", "\n", "\n", "self", ".", "parameters", "=", "parameters", "\n", "self", ".", "max_samples", "=", "max_samples", "\n", "\n", "self", ".", "metadata", "=", "metadata", "\n", "self", ".", "_debug", "=", "debug", "\n", "\n", "self", ".", "device_id", "=", "device_id", "\n", "\n", "# Shape: (question_length, passage_length)", "\n", "self", ".", "question_passage_attention", "=", "(", "\n", "question_passage_attention", "\n", ")", "# initialization_returns[\"question_passage_attention\"]", "\n", "# Shape: (passage_length, question_length)", "\n", "self", ".", "passage_question_attention", "=", "passage_question_attention", "\n", "# Shape: (passage_length, passage_length)", "\n", "self", ".", "passage_passage_token2date_alignment", "=", "passage_token2date_alignment", "\n", "self", ".", "passage_passage_token2startdate_alignment", "=", "passage_token2startdate_alignment", "\n", "self", ".", "passage_passage_token2enddate_alignment", "=", "passage_token2enddate_alignment", "\n", "self", ".", "passage_passage_token2num_alignment", "=", "passage_token2num_alignment", "\n", "initialization_returns", "=", "self", ".", "initialize", "(", ")", "\n", "self", ".", "date_lt_mat", "=", "initialization_returns", "[", "\"date_lt_mat\"", "]", "\n", "self", ".", "date_gt_mat", "=", "initialization_returns", "[", "\"date_gt_mat\"", "]", "\n", "# These matrices are for passage numbers", "\n", "self", ".", "num_lt_mat", "=", "initialization_returns", "[", "\"num_lt_mat\"", "]", "\n", "self", ".", "num_gt_mat", "=", "initialization_returns", "[", "\"num_gt_mat\"", "]", "\n", "\n", "# List[int]", "\n", "self", ".", "year_differences", "=", "year_differences", "\n", "# Shape: (num_passage_dates, num_passage_dates, num_of_year_differences)", "\n", "self", ".", "year_differences_mat", "=", "allenutil", ".", "move_to_device", "(", "\n", "torch", ".", "FloatTensor", "(", "year_differences_mat", ")", ",", "cuda_device", "=", "self", ".", "device_id", "\n", ")", "\n", "# List[int]", "\n", "self", ".", "count_num_values", "=", "count_num_values", "\n", "self", ".", "countvals", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "FloatTensor", "(", "range", "(", "0", ",", "10", ")", ")", ",", "cuda_device", "=", "self", ".", "device_id", ")", "\n", "\n", "# This is a list of list, where each list element corresponds to one program execution. Each element stores", "\n", "# details related to the module execution as added by the modules below.", "\n", "# drop_parse_base._get_denotations appends a new list to this before executing a program to start a new program", "\n", "# Each module below can then append info into self.modules_debug_info[-1] -- to add to the latest program, i.e.", "\n", "# the one getting executed.", "\n", "self", ".", "modules_debug_info", "=", "[", "]", "\n", "\n", "\"\"\"\n        if self._debug:\n            num_date_tokens = self.passage_datetokens_mask_float.sum()\n            plen = self.passage_mask.sum()\n            siml1norm = self.passage_passage_token2date_similarity.norm(p=1)/(num_date_tokens * plen)\n            sim_avgval = self.passage_passage_token2date_similarity.sum() / (num_date_tokens * plen)\n            if torch.isnan(sim_avgval):\n                print(\"Date fault\")\n                print(f\"{self.num_passage_dates} : {num_date_tokens}\")\n                print(self.passage_passage_token2date_similarity)\n            print(f\"Passage Token2Date sim, Avg L1 Norm: {siml1norm}. Avg Val: {sim_avgval}\")\n            num_numb_tokens = self.passage_numtokens_mask_float.sum()\n            plen = self.passage_mask.sum()\n            siml1norm = self.passage_passage_token2num_similarity.norm(p=1) / (num_numb_tokens * plen)\n            sim_avgval = self.passage_passage_token2num_similarity.sum() / (num_numb_tokens * plen)\n            if torch.isnan(sim_avgval):\n                print(\"Num fault\")\n                print(self.passage_passage_token2num_similarity)\n            print(f\"Passage Token2Num sim, Avg L1 Norm: {siml1norm}. Avg Val: {sim_avgval}\")\n        \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.initialize": [[420, 429], ["drop_language.DropLanguage.compute_date_comparison_matrices", "drop_language.DropLanguage.compute_item_comparison_matrices"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.compute_date_comparison_matrices", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.compute_item_comparison_matrices"], ["", "def", "initialize", "(", "self", ")", ":", "\n", "        ", "date_gt_mat", ",", "date_lt_mat", "=", "self", ".", "compute_date_comparison_matrices", "(", "self", ".", "passage_date_values", ",", "self", ".", "device_id", ")", "\n", "num_gt_mat", ",", "num_lt_mat", "=", "self", ".", "compute_item_comparison_matrices", "(", "self", ".", "passage_num_values", ",", "self", ".", "device_id", ")", "\n", "\n", "return", "{", "\n", "\"date_gt_mat\"", ":", "date_gt_mat", ",", "\n", "\"date_lt_mat\"", ":", "date_lt_mat", ",", "\n", "\"num_gt_mat\"", ":", "num_gt_mat", ",", "\n", "\"num_lt_mat\"", ":", "num_lt_mat", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.compute_date_comparison_matrices": [[431, 445], ["range", "allennlp.move_to_device", "allennlp.move_to_device", "len", "range", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "range", "range", "len", "range", "len", "range", "len", "len", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "compute_date_comparison_matrices", "(", "date_values", ":", "List", "[", "Date", "]", ",", "device_id", ":", "int", ")", ":", "\n", "        ", "date_gt_mat", "=", "[", "[", "0", "for", "_", "in", "range", "(", "len", "(", "date_values", ")", ")", "]", "for", "_", "in", "range", "(", "len", "(", "date_values", ")", ")", "]", "\n", "date_lt_mat", "=", "[", "[", "0", "for", "_", "in", "range", "(", "len", "(", "date_values", ")", ")", "]", "for", "_", "in", "range", "(", "len", "(", "date_values", ")", ")", "]", "\n", "# self.encoded_passage.new_zeros(self.num_passage_dates, self.num_passage_dates)", "\n", "for", "i", "in", "range", "(", "len", "(", "date_values", ")", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "len", "(", "date_values", ")", ")", ":", "\n", "                ", "date_gt_mat", "[", "i", "]", "[", "j", "]", "=", "1.0", "if", "date_values", "[", "i", "]", ">", "date_values", "[", "j", "]", "else", "0.0", "\n", "date_lt_mat", "[", "i", "]", "[", "j", "]", "=", "1.0", "if", "date_values", "[", "i", "]", "<", "date_values", "[", "j", "]", "else", "0.0", "\n", "# date_greater_than_mat[j][i] = 1.0 - date_greater_than_mat[i][j]", "\n", "", "", "date_gt_mat", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "FloatTensor", "(", "date_gt_mat", ")", ",", "device_id", ")", "\n", "date_lt_mat", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "FloatTensor", "(", "date_lt_mat", ")", ",", "device_id", ")", "\n", "\n", "return", "date_gt_mat", ",", "date_lt_mat", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.compute_item_comparison_matrices": [[446, 469], ["all", "len", "numpy.ones", "numpy.triu", "numpy.tril", "allennlp.move_to_device", "allennlp.move_to_device", "range", "allennlp.move_to_device", "allennlp.move_to_device", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "len", "range", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "range", "range", "range", "len", "range", "len", "range", "len", "len", "len", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "compute_item_comparison_matrices", "(", "values", ":", "List", "[", "Any", "]", ",", "device_id", ":", "int", ")", ":", "\n", "        ", "values_is_sorted", "=", "all", "(", "values", "[", "i", "]", "<=", "values", "[", "i", "+", "1", "]", "for", "i", "in", "range", "(", "0", ",", "len", "(", "values", ")", "-", "1", ")", ")", "\n", "if", "values_is_sorted", ":", "\n", "            ", "num_values", "=", "len", "(", "values", ")", "\n", "ones", "=", "np", ".", "ones", "(", "(", "num_values", ",", "num_values", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "upper_triu", "=", "np", ".", "triu", "(", "ones", ",", "k", "=", "1", ")", "# The k=1 ensures main diagonal is zero for strict lt_mat", "\n", "lower_triu", "=", "np", ".", "tril", "(", "ones", ",", "k", "=", "-", "1", ")", "# The k=1 ensures main diagonal is zero for strict gt_mat", "\n", "gt_mat", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "FloatTensor", "(", "lower_triu", ")", ",", "device_id", ")", "\n", "lt_mat", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "FloatTensor", "(", "upper_triu", ")", ",", "device_id", ")", "\n", "", "else", ":", "\n", "            ", "gt_mat", "=", "[", "[", "0", "for", "_", "in", "range", "(", "len", "(", "values", ")", ")", "]", "for", "_", "in", "range", "(", "len", "(", "values", ")", ")", "]", "\n", "lt_mat", "=", "[", "[", "0", "for", "_", "in", "range", "(", "len", "(", "values", ")", ")", "]", "for", "_", "in", "range", "(", "len", "(", "values", ")", ")", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "values", ")", ")", ":", "\n", "                ", "for", "j", "in", "range", "(", "len", "(", "values", ")", ")", ":", "\n", "                    ", "gt_mat", "[", "i", "]", "[", "j", "]", "=", "1.0", "if", "values", "[", "i", "]", ">", "values", "[", "j", "]", "else", "0.0", "\n", "lt_mat", "[", "i", "]", "[", "j", "]", "=", "1.0", "if", "values", "[", "i", "]", "<", "values", "[", "j", "]", "else", "0.0", "\n", "# date_greater_than_mat[j][i] = 1.0 - date_greater_than_mat[i][j]", "\n", "", "", "gt_mat", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "FloatTensor", "(", "gt_mat", ")", ",", "device_id", ")", "\n", "lt_mat", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "FloatTensor", "(", "lt_mat", ")", ",", "device_id", ")", "\n", "\n", "", "return", "gt_mat", ",", "lt_mat", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.find_PassageAttention": [[480, 512], ["allennlp_semparse.domain_languages.predicate_with_side_args", "drop_language.PassageAttention", "semqa.profiler.profile.Profile", "question_passage_attention.sum", "drop_language.clamp_distribution", "drop_language.PassageAttention", "question_attention.unsqueeze", "semqa.domain_languages.domain_language_utils.listTokensVis", "semqa.domain_languages.domain_language_utils.listTokensVis", "semqa.domain_languages.domain_language_utils.mostAttendedSpans", "drop_language.DropLanguage.modules_debug_info[].append"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.clamp_distribution", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.listTokensVis", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.listTokensVis", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.mostAttendedSpans"], ["", "@", "predicate_with_side_args", "(", "[", "\"question_attention\"", ",", "\"passage_attention\"", "]", ")", "\n", "def", "find_PassageAttention", "(", "self", ",", "question_attention", ":", "Tensor", ",", "passage_attention", ":", "Tensor", "=", "None", ")", "->", "PassageAttention", ":", "\n", "        ", "with", "Profile", "(", "\"find-pattn\"", ")", ":", "\n", "# The passage attention is only used as supervision for certain synthetic questions", "\n", "            ", "if", "passage_attention", "is", "not", "None", ":", "\n", "                ", "return", "PassageAttention", "(", "passage_attention", ",", "debug_value", "=", "\"Supervised-Pattn-Used\"", ")", "\n", "\n", "", "question_attention", "=", "question_attention", "*", "self", ".", "question_mask", "\n", "\n", "# Shape: (question_length, passage_length)", "\n", "question_passage_attention", "=", "self", ".", "question_passage_attention", "*", "question_attention", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "passage_attention", "=", "question_passage_attention", ".", "sum", "(", "0", ")", "\n", "passage_attention", "=", "clamp_distribution", "(", "passage_attention", ")", "\n", "\n", "debug_value", "=", "\"\"", "\n", "if", "self", ".", "_debug", ":", "\n", "                ", "qattn_vis_complete", ",", "qattn_vis_most", "=", "dlutils", ".", "listTokensVis", "(", "\n", "question_attention", ",", "self", ".", "metadata", "[", "\"question_tokens\"", "]", "\n", ")", "\n", "debug_value", "+=", "f\"Qattn: {qattn_vis_complete}\"", "\n", "pattn_vis_complete", ",", "pattn_vis_most", "=", "dlutils", ".", "listTokensVis", "(", "\n", "passage_attention", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", "\n", ")", "\n", "most_attended_spans", "=", "dlutils", ".", "mostAttendedSpans", "(", "passage_attention", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", ")", "\n", "debug_value", "+=", "f\"\\nPattn: {pattn_vis_complete}\"", "\n", "debug_value", "+=", "f\"\\nMostAttendedSpans: {most_attended_spans}\"", "\n", "debug_info_dict", "=", "{", "\"find\"", ":", "{", "\"passage\"", ":", "passage_attention", ",", "\n", "\"question\"", ":", "question_attention", "}", "}", "\n", "self", ".", "modules_debug_info", "[", "-", "1", "]", ".", "append", "(", "debug_info_dict", ")", "\n", "\n", "", "", "return", "PassageAttention", "(", "passage_attention", ",", "debug_value", "=", "debug_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.filter_PassageAttention": [[513, 602], ["allennlp_semparse.domain_languages.predicate_with_side_args", "drop_language.PassageAttention", "semqa.profiler.profile.Profile", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "drop_language.DropLanguage.parameters._endpoint_span_extractor", "drop_language.DropLanguage.parameters.filter_matrix_attention", "drop_language.DropLanguage.squeeze().squeeze", "allennlp.get_range_vector", "allennlp.get_range_vector.unsqueeze", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "drop_language.clamp_distribution", "torch.sum.unsqueeze().unsqueeze", "torch.sum.unsqueeze().unsqueeze", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "drop_language.DropLanguage.passage_sentboundary_mask.float", "drop_language.DropLanguage.passage_sentence_starts_masked.unsqueeze", "drop_language.DropLanguage.passage_sentence_ends_masked.unsqueeze", "drop_language.DropLanguage.passage_sentboundary_mask.unsqueeze().float", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "semqa.domain_languages.domain_language_utils.listTokensVis", "semqa.domain_languages.domain_language_utils.listTokensVis", "semqa.domain_languages.domain_language_utils.mostAttendedSpans", "semqa.domain_languages.domain_language_utils.listTokensVis", "semqa.domain_languages.domain_language_utils.mostAttendedSpans", "drop_language.DropLanguage.modules_debug_info[].append", "question_attention.unsqueeze", "passage_repr.unsqueeze", "drop_language.DropLanguage.passage_sentence_boundaries_masked.unsqueeze", "drop_language.DropLanguage.squeeze", "sentence_filter_prob.unsqueeze", "torch.sum.unsqueeze", "torch.sum.unsqueeze", "drop_language.DropLanguage.passage_sentboundary_mask.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.clamp_distribution", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.listTokensVis", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.listTokensVis", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.mostAttendedSpans", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.listTokensVis", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.mostAttendedSpans"], ["", "@", "predicate_with_side_args", "(", "[", "\"question_attention\"", "]", ")", "\n", "def", "filter_PassageAttention", "(", "\n", "self", ",", "passage_attention", ":", "PassageAttention", ",", "question_attention", ":", "Tensor", "\n", ")", "->", "PassageAttention", ":", "\n", "        ", "with", "Profile", "(", "\"filter-attn\"", ")", ":", "\n", "            ", "passage_attn", ":", "Tensor", "=", "passage_attention", ".", "_value", "\n", "\n", "question_attention", "=", "question_attention", "*", "self", ".", "question_mask", "\n", "# Shape: (ques_encoding_dim)", "\n", "weighted_question_vector", "=", "torch", ".", "sum", "(", "self", ".", "encoded_question", "*", "question_attention", ".", "unsqueeze", "(", "1", ")", ",", "0", ")", "\n", "\n", "# Shape: (passage_length, encoded_dim)", "\n", "passage_repr", "=", "self", ".", "modeled_passage", "if", "self", ".", "modeled_passage", "is", "not", "None", "else", "self", ".", "encoded_passage", "\n", "\n", "# Shape: (1, num_sents, 2 * passage_dim)", "\n", "sentence_start_end_repr", "=", "self", ".", "parameters", ".", "_endpoint_span_extractor", "(", "\n", "sequence_tensor", "=", "passage_repr", ".", "unsqueeze", "(", "0", ")", ",", "\n", "span_indices", "=", "self", ".", "passage_sentence_boundaries_masked", ".", "unsqueeze", "(", "0", ")", ")", "\n", "\n", "sentence_start_end_repr", "=", "(", "sentence_start_end_repr", "[", ":", ",", ":", ",", "0", ":", "self", ".", "passage_encoding_dim", "]", "+", "\n", "sentence_start_end_repr", "[", ":", ",", ":", ",", "self", ".", "passage_encoding_dim", ":", "]", ")", "\n", "\n", "# Shape: (1, 1, num_sents)", "\n", "sentence_logits_unsqueezed", "=", "self", ".", "parameters", ".", "filter_matrix_attention", "(", "\n", "weighted_question_vector", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "1", ")", ",", "sentence_start_end_repr", ")", "\n", "# Shape: (num_sents)", "\n", "sentence_logits", "=", "sentence_logits_unsqueezed", ".", "squeeze", "(", "0", ")", ".", "squeeze", "(", "0", ")", "\n", "sentence_filter_prob", "=", "torch", ".", "sigmoid", "(", "sentence_logits", ")", "*", "self", ".", "passage_sentboundary_mask", ".", "float", "(", ")", "\n", "# Shape: (passage_length)", "\n", "range_vec", "=", "allenutil", ".", "get_range_vector", "(", "self", ".", "passage_length", ",", "device", "=", "self", ".", "device_id", ")", "\n", "range_vec_unsq", "=", "range_vec", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "# Shape: (num_sents, passage_length)", "\n", "lower", "=", "range_vec_unsq", ">=", "self", ".", "passage_sentence_starts_masked", ".", "unsqueeze", "(", "1", ")", "\n", "upper", "=", "range_vec_unsq", "<=", "self", ".", "passage_sentence_ends_masked", ".", "unsqueeze", "(", "1", ")", "\n", "# (num_sents, passage_length)", "\n", "sentence_bool", "=", "(", "lower", "*", "upper", ")", ".", "float", "(", ")", "*", "self", ".", "passage_sentboundary_mask", ".", "unsqueeze", "(", "1", ")", ".", "float", "(", ")", "\n", "\n", "# Shape: (passage_length, )", "\n", "filter_attn", "=", "torch", ".", "sum", "(", "sentence_bool", "*", "sentence_filter_prob", ".", "unsqueeze", "(", "1", ")", ",", "dim", "=", "0", ")", "\n", "original_filter_attn", "=", "filter_attn", "*", "passage_attn", "\n", "filtered_passage_attention", "=", "original_filter_attn", "/", "torch", ".", "sum", "(", "original_filter_attn", ")", "\n", "filtered_passage_attention", "=", "clamp_distribution", "(", "filtered_passage_attention", ")", "\n", "\n", "\"\"\"\n            # Shape: (1, 1, passage_length)\n            passage_logits_unsqueezed = self.parameters.filter_matrix_attention(\n                weighted_question_vector.unsqueeze(0).unsqueeze(1), passage_repr.unsqueeze(0)\n            )\n\n            passage_logits = passage_logits_unsqueezed.squeeze(0).squeeze(0)\n\n            # filter_attn = allenutil.masked_softmax(passage_logits, mask=self.passage_mask, memory_efficient=True)\n            filter_attn = torch.sigmoid(passage_logits * self.passage_mask)\n\n            original_filter_attn = filter_attn * passage_attn\n\n            filtered_passage_attention = original_filter_attn / torch.sum(original_filter_attn)\n            filtered_passage_attention = clamp_distribution(filtered_passage_attention)\n            \"\"\"", "\n", "\n", "loss", "=", "passage_attention", ".", "loss", "\n", "\n", "debug_value", "=", "\"\"", "\n", "if", "self", ".", "_debug", ":", "\n", "                ", "qattn_vis_complete", ",", "qattn_vis_most", "=", "dlutils", ".", "listTokensVis", "(", "\n", "question_attention", ",", "self", ".", "metadata", "[", "\"question_tokens\"", "]", "\n", ")", "\n", "debug_value", "+=", "f\"Qattn: {qattn_vis_complete}\"", "\n", "\n", "f_attn_vis", ",", "_", "=", "dlutils", ".", "listTokensVis", "(", "filter_attn", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", ")", "\n", "most_attended_spans", "=", "dlutils", ".", "mostAttendedSpans", "(", "filter_attn", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", ")", "\n", "debug_value", "+=", "f\"\\nFilterAttn: {f_attn_vis}\"", "\n", "debug_value", "+=", "f\"\\nMostAttended: {most_attended_spans}\"", "\n", "\n", "pattn_vis_complete", ",", "pattn_vis_most", "=", "dlutils", ".", "listTokensVis", "(", "\n", "filtered_passage_attention", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", "\n", ")", "\n", "most_attended_spans", "=", "dlutils", ".", "mostAttendedSpans", "(", "filtered_passage_attention", ",", "\n", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", ")", "\n", "debug_value", "+=", "f\"\\nPattn: {pattn_vis_complete}\"", "\n", "debug_value", "+=", "f\"\\nMostAttended: {most_attended_spans}\"", "\n", "\n", "debug_info_dict", "=", "{", "\"filter\"", ":", "{", "\"passage\"", ":", "filtered_passage_attention", ",", "\n", "\"passage_input\"", ":", "passage_attn", ",", "\n", "\"question\"", ":", "question_attention", "}", "}", "\n", "self", ".", "modules_debug_info", "[", "-", "1", "]", ".", "append", "(", "debug_info_dict", ")", "\n", "\n", "", "", "return", "PassageAttention", "(", "filtered_passage_attention", ",", "loss", "=", "loss", ",", "debug_value", "=", "debug_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.relocate_PassageAttention": [[603, 662], ["allennlp_semparse.domain_languages.predicate_with_side_args", "drop_language.PassageAttention_answer", "semqa.profiler.profile.Profile", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "drop_language.DropLanguage.parameters.relocate_matrix_attention().squeeze", "allennlp.masked_softmax", "semqa.domain_languages.domain_language_utils.masking_blockdiagonal", "semqa.domain_languages.domain_language_utils.aux_window_loss", "drop_language.clamp_distribution", "torch.sum.unsqueeze", "torch.sum.unsqueeze", "drop_language.DropLanguage.passage_mask.unsqueeze", "passage_attn.size", "semqa.domain_languages.domain_language_utils.listTokensVis", "semqa.domain_languages.domain_language_utils.listTokensVis", "semqa.domain_languages.domain_language_utils.mostAttendedSpans", "drop_language.DropLanguage.modules_debug_info[].append", "question_attention.unsqueeze", "drop_language.DropLanguage.parameters.relocate_matrix_attention", "drop_language.DropLanguage.passage_mask.unsqueeze", "q_p_repr.unsqueeze", "passage_repr.unsqueeze", "passage_attn.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.masking_blockdiagonal", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.aux_window_loss", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.clamp_distribution", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.listTokensVis", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.listTokensVis", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.mostAttendedSpans"], ["", "@", "predicate_with_side_args", "(", "[", "\"question_attention\"", "]", ")", "\n", "def", "relocate_PassageAttention", "(", "\n", "self", ",", "passage_attention", ":", "PassageAttention", ",", "question_attention", ":", "Tensor", "\n", ")", "->", "PassageAttention_answer", ":", "\n", "        ", "with", "Profile", "(", "\"relocate-attn\"", ")", ":", "\n", "            ", "passage_attn", ":", "Tensor", "=", "passage_attention", ".", "_value", "\n", "passage_attn", "=", "passage_attn", "*", "self", ".", "passage_mask", "\n", "\n", "question_attention", "=", "question_attention", "*", "self", ".", "question_mask", "\n", "# Shape: (encoding_dim)", "\n", "weighted_question_vector", "=", "torch", ".", "sum", "(", "self", ".", "encoded_question", "*", "question_attention", ".", "unsqueeze", "(", "1", ")", ",", "0", ")", "\n", "\n", "# Shape: (passage_length, encoded_dim)", "\n", "passage_repr", "=", "self", ".", "modeled_passage", "if", "self", ".", "modeled_passage", "is", "not", "None", "else", "self", ".", "encoded_passage", "\n", "# Shape: (passage_length, encoded_dim)", "\n", "q_p_repr", "=", "weighted_question_vector", ".", "unsqueeze", "(", "0", ")", "+", "passage_repr", "\n", "\n", "# Shape: (passage_length, passage_length)", "\n", "passage_passage_relocate_similarity", "=", "self", ".", "parameters", ".", "relocate_matrix_attention", "(", "\n", "q_p_repr", ".", "unsqueeze", "(", "0", ")", ",", "passage_repr", ".", "unsqueeze", "(", "0", ")", "\n", ")", ".", "squeeze", "(", "0", ")", "\n", "# Shape: (passage_length, passage_length)", "\n", "p_to_p_relocate_attention", "=", "allenutil", ".", "masked_softmax", "(", "\n", "passage_passage_relocate_similarity", ",", "mask", "=", "self", ".", "passage_mask", ".", "unsqueeze", "(", "0", ")", ",", "dim", "=", "-", "1", "\n", ")", "\n", "p_to_p_relocate_attention", "=", "p_to_p_relocate_attention", "*", "self", ".", "passage_mask", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "passage_length", "=", "passage_attn", ".", "size", "(", ")", "[", "0", "]", "\n", "inwindow_mask", ",", "_", "=", "dlutils", ".", "masking_blockdiagonal", "(", "\n", "passage_length", "=", "passage_length", ",", "window", "=", "15", ",", "device_id", "=", "self", ".", "device_id", "\n", ")", "\n", "inwindow_aux_loss", "=", "dlutils", ".", "aux_window_loss", "(", "\n", "ptop_attention", "=", "p_to_p_relocate_attention", ",", "passage_mask", "=", "self", ".", "passage_mask", ",", "inwindow_mask", "=", "inwindow_mask", "\n", ")", "\n", "\n", "# Shape: (passage_length, )", "\n", "relocate_attn", "=", "(", "p_to_p_relocate_attention", "*", "passage_attn", ".", "unsqueeze", "(", "1", ")", ")", ".", "sum", "(", "0", ")", "\n", "relocate_attn", "=", "clamp_distribution", "(", "relocate_attn", ")", "\n", "\n", "loss", "=", "passage_attention", ".", "loss", "\n", "loss", "+=", "2.0", "*", "inwindow_aux_loss", "\n", "\n", "debug_value", "=", "\"\"", "\n", "if", "self", ".", "_debug", ":", "\n", "                ", "qattn_vis_complete", ",", "qattn_vis_most", "=", "dlutils", ".", "listTokensVis", "(", "\n", "question_attention", ",", "self", ".", "metadata", "[", "\"question_tokens\"", "]", "\n", ")", "\n", "debug_value", "+=", "f\"Qattn: {qattn_vis_complete}\"", "\n", "r_attn_vis", ",", "_", "=", "dlutils", ".", "listTokensVis", "(", "relocate_attn", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", ")", "\n", "most_attended_spans", "=", "dlutils", ".", "mostAttendedSpans", "(", "relocate_attn", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", ")", "\n", "debug_value", "+=", "f\"\\nRelocateAttn: {r_attn_vis}\"", "\n", "debug_value", "+=", "f\"\\nMostAttended: {most_attended_spans}\"", "\n", "\n", "debug_info_dict", "=", "{", "\"relocate\"", ":", "{", "\"passage\"", ":", "relocate_attn", ",", "\n", "\"passage_input\"", ":", "passage_attn", ",", "\n", "\"question\"", ":", "question_attention", "}", "}", "\n", "self", ".", "modules_debug_info", "[", "-", "1", "]", ".", "append", "(", "debug_info_dict", ")", "\n", "\n", "", "", "return", "PassageAttention_answer", "(", "relocate_attn", ",", "loss", "=", "loss", ",", "debug_value", "=", "debug_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.compute_date_scores": [[664, 732], ["semqa.profiler.profile.Profile", "attn_weighted_date_aligment_matrix.sum", "passage_attention.new_zeros", "clamp_distribution.scatter_add_", "drop_language.clamp_distribution", "passage_attention.unsqueeze", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.log", "torch.log", "torch.log", "torch.log"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.clamp_distribution", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "def", "compute_date_scores", "(", "self", ",", "passage_attention", ":", "Tensor", ",", "date_type", ":", "str", "=", "None", ")", ":", "\n", "        ", "\"\"\" Given a passage over passage token2date attention (normalized), and an additional passage attention\n            for token importance, compute a distribution over (unique) dates in the passage.\n\n            Using the token_attention from the passage_attention, find the expected input_token2date_token\n            distribution - weigh each row in passage_passage_token2date_attention and sum\n            Use this date_token attention as scores for the dates they refer to. Since each date can be referred\n            to in multiple places, we use scatter_add_ to get the total_score.\n            Softmax over these scores is the date dist.\n        \"\"\"", "\n", "with", "Profile", "(", "\"date-dist.\"", ")", ":", "\n", "# passage_date_alignment_matrix = allenutil.masked_softmax(self.passage_passage_token2date_similarity,", "\n", "#                                                          mask=self.passage_datetokens_mask_float.unsqueeze(0),", "\n", "#                                                          memory_efficient=True)", "\n", "\n", "            ", "if", "date_type", "is", "None", ":", "\n", "                ", "passage_date_alignment_matrix", "=", "self", ".", "passage_passage_token2date_alignment", "\n", "", "elif", "date_type", "==", "\"start\"", ":", "\n", "                ", "passage_date_alignment_matrix", "=", "self", ".", "passage_passage_token2startdate_alignment", "\n", "", "elif", "date_type", "==", "\"end\"", ":", "\n", "                ", "passage_date_alignment_matrix", "=", "self", ".", "passage_passage_token2enddate_alignment", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "\n", "\n", "", "attn_weighted_date_aligment_matrix", "=", "passage_date_alignment_matrix", "*", "passage_attention", ".", "unsqueeze", "(", "1", ")", "\n", "# Shape: (passage_length, )", "\n", "passage_date_token_probs", "=", "attn_weighted_date_aligment_matrix", ".", "sum", "(", "0", ")", "\n", "\n", "\"\"\"\n            if self._debug:\n                print('-------------------------')\n                print(self.metadata['question_tokens'])\n                passage_tokens = self.metadata['passage_tokens']\n                attn, topattn = dlutils.listTokensVis(passage_attention, passage_tokens)\n                print(f\"PassageAttention: Top: {topattn}\")\n                print(attn)\n                print()\n\n                print(\"Only showing 10 date-tokens ...\")\n                _, date_indices = torch.topk(self.passage_tokenidx2dateidx, k=5, dim=0)\n                date_indices = myutils.tocpuNPList(date_indices)\n                for number_idx in date_indices:\n                    token2datescores = passage_date_alignment_matrix[:, number_idx]\n                    _, top_tokens = torch.topk(token2datescores, 5, dim=0)\n                    top_tokens = myutils.tocpuNPList(top_tokens)\n                    print(f\"{passage_tokens[number_idx]}\")\n                    print([passage_tokens[x] for x in top_tokens])\n                    print(f\"Sum: {torch.sum(token2datescores)}\")\n                    compvis, _ = dlutils.listTokensVis(token2datescores, passage_tokens)\n                    print(compvis)\n\n                print(\"After passage attention; number-token-probs:\")\n                attn, _ = dlutils.listTokensVis(passage_date_token_probs, passage_tokens)\n                print(attn)\n                print()\n                print(\"-----------------------------------\")\n            \"\"\"", "\n", "\n", "masked_passage_tokenidx2dateidx", "=", "self", ".", "passage_datetokens_mask_long", "*", "self", ".", "passage_tokenidx2dateidx", "\n", "\n", "date_distribution", "=", "passage_attention", ".", "new_zeros", "(", "self", ".", "num_passage_dates", ")", "\n", "date_distribution", ".", "scatter_add_", "(", "0", ",", "masked_passage_tokenidx2dateidx", ",", "passage_date_token_probs", ")", "\n", "\n", "date_distribution", "=", "clamp_distribution", "(", "date_distribution", ")", "\n", "\n", "date_distribution_entropy", "=", "-", "1", "*", "torch", ".", "sum", "(", "date_distribution", "*", "torch", ".", "log", "(", "date_distribution", "+", "1e-40", ")", ")", "\n", "\n", "", "return", "date_distribution", ",", "passage_date_token_probs", ",", "date_distribution_entropy", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.compute_num_distribution": [[734, 794], ["semqa.profiler.profile.Profile", "attn_weighted_number_aligment_matrix.sum", "passage_attention.new_zeros", "clamp_distribution.scatter_add_", "drop_language.clamp_distribution", "passage_attention.unsqueeze", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.log", "torch.log", "torch.log", "torch.log"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.clamp_distribution", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "def", "compute_num_distribution", "(", "self", ",", "passage_attention", ":", "Tensor", ")", ":", "\n", "        ", "\"\"\" Given a passage over passage token2num attention (normalized), and an additional passage attention\n            for token importance, compute a distribution over (unique) nums in the passage.\n            See compute_date_distribution for details\n        \"\"\"", "\n", "\n", "with", "Profile", "(", "\"num-dist\"", ")", ":", "\n", "# Shape: (passage_length, passage_length) -- each row softmaxed over the number-tokens", "\n", "# passage_number_alignment_matrix = allenutil.masked_softmax(self.passage_passage_token2num_similarity,", "\n", "#                                                            mask=self.passage_numtokens_mask_float.unsqueeze(0),", "\n", "#                                                            memory_efficient=True)", "\n", "\n", "# Shape: (passage_length, passage_length) -- Each row is a masked-softmax over number-tokens", "\n", "            ", "passage_number_alignment_matrix", "=", "self", ".", "passage_passage_token2num_alignment", "\n", "\n", "# (passage_length, passage_length)", "\n", "attn_weighted_number_aligment_matrix", "=", "passage_number_alignment_matrix", "*", "passage_attention", ".", "unsqueeze", "(", "1", ")", "\n", "# Shape: (passage_length, )", "\n", "passage_number_token_probs", "=", "attn_weighted_number_aligment_matrix", ".", "sum", "(", "0", ")", "\n", "\n", "\"\"\"\n            if self._debug:\n                print('-------------------------')\n                print(self.metadata['question_tokens'])\n                passage_tokens = self.metadata['passage_tokens']\n                attn, topattn = dlutils.listTokensVis(passage_attention, passage_tokens)\n                print(f\"PassageAttention: Top: {topattn}\")\n                print(attn)\n                print()\n\n                print(\"Only showing 10 number-tokens ...\")\n                _, number_indices = torch.topk(self.passage_tokenidx2numidx, k=10, dim=0)\n                number_indices = myutils.tocpuNPList(number_indices)\n                for number_idx in number_indices:\n                    token2numberscores = passage_number_alignment_matrix[:, number_idx]\n                    _, top_tokens = torch.topk(token2numberscores, 5, dim=0)\n                    top_tokens = myutils.tocpuNPList(top_tokens)\n                    print(f\"{passage_tokens[number_idx]}\")\n                    print([passage_tokens[x] for x in top_tokens])\n                    print(f\"Sum: {torch.sum(token2numberscores)}\")\n                    compvis, _ = dlutils.listTokensVis(token2numberscores, passage_tokens)\n                    print(compvis)\n\n                print(\"After passage attention; number-token-probs:\")\n                attn, _ = dlutils.listTokensVis(passage_number_token_probs, passage_tokens)\n                print(attn)\n                print()\n                print(\"-----------------------------------\")\n            \"\"\"", "\n", "\n", "masked_passage_tokenidx2numidx", "=", "self", ".", "passage_numtokens_mask_long", "*", "self", ".", "passage_tokenidx2numidx", "\n", "\n", "\"\"\" normalized method with method 2 \"\"\"", "\n", "num_distribution", "=", "passage_attention", ".", "new_zeros", "(", "self", ".", "num_passage_nums", ")", "\n", "num_distribution", ".", "scatter_add_", "(", "0", ",", "masked_passage_tokenidx2numidx", ",", "passage_number_token_probs", ")", "\n", "\n", "num_distribution", "=", "clamp_distribution", "(", "num_distribution", ")", "\n", "\n", "num_distribution_entropy", "=", "-", "1", "*", "torch", ".", "sum", "(", "num_distribution", "*", "torch", ".", "log", "(", "num_distribution", "+", "1e-40", ")", ")", "\n", "", "return", "num_distribution", ",", "passage_number_token_probs", ",", "num_distribution_entropy", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.compute_implicitnum_distribution": [[795, 821], ["semqa.profiler.profile.Profile", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum.unsqueeze().unsqueeze", "torch.sum.unsqueeze().unsqueeze", "drop_language.DropLanguage.parameters.implicit_num_embeddings.unsqueeze", "drop_language.DropLanguage.parameters.implicitnum_bilinear_attention", "implicit_num_logits.squeeze().squeeze.squeeze().squeeze.squeeze().squeeze", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax.new_zeros", "torch.nn.functional.softmax.new_zeros", "clamp_distribution.scatter_add_", "drop_language.clamp_distribution", "question_attention.unsqueeze", "torch.sum.unsqueeze", "torch.sum.unsqueeze", "implicit_num_logits.squeeze().squeeze.squeeze().squeeze.squeeze"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.clamp_distribution"], ["", "def", "compute_implicitnum_distribution", "(", "self", ",", "question_attention", ":", "Tensor", ")", ":", "\n", "        ", "\"\"\" Given a question attention, compute a distribution over implicit numbers for this language.\n            See compute_date_distribution for details\n        \"\"\"", "\n", "with", "Profile", "(", "\"implicit-num\"", ")", ":", "\n", "            ", "question_attention", "=", "question_attention", "*", "self", ".", "question_mask", "\n", "# Shape: (encoding_dim)", "\n", "weighted_question_vector", "=", "torch", ".", "sum", "(", "self", ".", "encoded_question", "*", "question_attention", ".", "unsqueeze", "(", "1", ")", ",", "0", ")", "\n", "\n", "# unsqueeze -- one for num-of-vecs and one for batch", "\n", "weighted_question_vector_ex", "=", "weighted_question_vector", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "implicit_num_embeddings_ex", "=", "self", ".", "parameters", ".", "implicit_num_embeddings", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "# Shape: (1, 1, num_implicit_numbers)", "\n", "implicit_num_logits", "=", "self", ".", "parameters", ".", "implicitnum_bilinear_attention", "(", "weighted_question_vector_ex", ",", "\n", "implicit_num_embeddings_ex", ")", "\n", "# Shape: (num_implicit_numbers)", "\n", "implicit_num_logits", "=", "implicit_num_logits", ".", "squeeze", "(", "0", ")", ".", "squeeze", "(", "0", ")", "\n", "implicit_num_probs", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "implicit_num_logits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "num_distribution", "=", "implicit_num_probs", ".", "new_zeros", "(", "self", ".", "num_passage_nums", ")", "\n", "num_distribution", ".", "scatter_add_", "(", "0", ",", "self", ".", "implicit_num_indices", ",", "implicit_num_probs", ")", "\n", "\n", "num_distribution", "=", "clamp_distribution", "(", "num_distribution", ")", "\n", "", "return", "num_distribution", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.expected_number_addsub": [[822, 882], ["semqa.profiler.profile.Profile", "num_dist_1.unsqueeze().unsqueeze", "num_dist_2.unsqueeze().unsqueeze", "masked_num_combination_indices.unsqueeze", "allennlp.batched_index_select", "selected_d_1.squeeze().squeeze.squeeze().squeeze.squeeze().squeeze", "allennlp.batched_index_select", "selected_d_2.squeeze().squeeze.squeeze().squeeze.squeeze().squeeze", "drop_language.clamp_distribution", "num_combination_mask[].float", "num_combination_mask[].float", "num_dist_1.unsqueeze", "num_dist_2.unsqueeze", "selected_d_1.squeeze().squeeze.squeeze().squeeze.squeeze", "selected_d_2.squeeze().squeeze.squeeze().squeeze.squeeze"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.clamp_distribution"], ["", "def", "expected_number_addsub", "(", "self", ",", "num_dist_1", ":", "torch", ".", "FloatTensor", ",", "num_dist_2", ":", "torch", ".", "FloatTensor", ",", "operation", ":", "str", ")", ":", "\n", "        ", "\"\"\"Compute the expected number distribution for an addition/subtraction operation.\n\n        add_num_combination_indices / sum_num_combination_indices: (size_composed_numbers, num_combinations, 2)\n        Each combination is a tuple indexed into the passage numbers\n\n        Expected distribution over composed numbers is computed by; for each composed number,\n         1. extracting from num_dist_1, the probability of the first number in the combination\n         2. extracting from num_dist_2, the probability of the second number in the combination\n         3. Computing the marginalized joint probability by summing over the product  1. X 2.\n\n        Parameters:\n        -----------\n        num_dist_1: (num_passage_numbers) Passage number distribution\n        num_dist_2: (num_passage_numbers) Passage number distribution\n        \"\"\"", "\n", "with", "Profile", "(", "\"num-add-sub\"", ")", ":", "\n", "            ", "assert", "operation", "in", "[", "\"add\"", ",", "\"sub\"", "]", "\n", "if", "operation", "==", "\"add\"", ":", "\n", "                ", "num_combination_indices", "=", "self", ".", "add_num_combination_indices", "\n", "num_combination_mask", "=", "self", ".", "add_num_combination_mask", "\n", "", "elif", "operation", "==", "\"sub\"", ":", "\n", "                ", "num_combination_indices", "=", "self", ".", "sub_num_combination_indices", "\n", "num_combination_mask", "=", "self", ".", "sub_num_combination_mask", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "\n", "\n", "# num_combination_indices: (size_composed_numbers, max_num_combinations, 2) for each number in composed numbers,", "\n", "# these are combinations (indices of passasge numbers) that combine (in order) using the operation and result", "\n", "# into this number. For example, num_combination_indices[i, :, :] is a combs=[NC, 2] array where", "\n", "# composed_number[i] = PassageNumber(combs[j, 0]) OP PassageNumber(combs[j, 1]) for all j.", "\n", "# These combinations are padded with -1", "\n", "", "masked_num_combination_indices", "=", "num_combination_indices", "*", "num_combination_mask", "\n", "\n", "# Making (B=1, seq_len=passage_numbers, dim=1) for batch index selection", "\n", "num_dist_1_uns", "=", "num_dist_1", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "2", ")", "\n", "num_dist_2_uns", "=", "num_dist_2", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "2", ")", "\n", "# B=1 unsqueezing", "\n", "masked_num_combination_indices_uns", "=", "masked_num_combination_indices", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "# Indexing into num_dist_1 where indices are num_combination_indices[:, :, 0]", "\n", "selected_d_1", "=", "allenutil", ".", "batched_index_select", "(", "\n", "target", "=", "num_dist_1_uns", ",", "indices", "=", "masked_num_combination_indices_uns", "[", ":", ",", ":", ",", ":", ",", "0", "]", "\n", ")", "\n", "# Shape: (size_composed_numbers, max_num_combinations)", "\n", "selected_d_1", "=", "selected_d_1", ".", "squeeze", "(", "0", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "selected_d_1", "=", "selected_d_1", "*", "num_combination_mask", "[", ":", ",", ":", ",", "0", "]", ".", "float", "(", ")", "\n", "\n", "# Indexing into num_dist_2 where indices are num_combination_indices[:, :, 1]", "\n", "selected_d_2", "=", "allenutil", ".", "batched_index_select", "(", "\n", "target", "=", "num_dist_2_uns", ",", "indices", "=", "masked_num_combination_indices_uns", "[", ":", ",", ":", ",", ":", ",", "1", "]", "\n", ")", "\n", "# Shape: (size_composed_numbers, max_num_combinations)", "\n", "selected_d_2", "=", "selected_d_2", ".", "squeeze", "(", "0", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "selected_d_2", "=", "selected_d_2", "*", "num_combination_mask", "[", ":", ",", ":", ",", "1", "]", ".", "float", "(", ")", "\n", "\n", "# Shape: (number_support)", "\n", "expected_distribution", "=", "(", "selected_d_1", "*", "selected_d_2", ")", ".", "sum", "(", "dim", "=", "1", ")", "\n", "expected_distribution", "=", "clamp_distribution", "(", "expected_distribution", ")", "\n", "", "return", "expected_distribution", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.expected_date_year_difference": [[883, 910], ["semqa.profiler.profile.Profile", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "drop_language.clamp_distribution", "date_distribution_1.unsqueeze", "date_distribution_2.unsqueeze", "torch.matmul.unsqueeze", "torch.matmul.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.clamp_distribution"], ["", "def", "expected_date_year_difference", "(", "\n", "self", ",", "date_distribution_1", ":", "torch", ".", "FloatTensor", ",", "date_distribution_2", ":", "torch", ".", "FloatTensor", "\n", ")", ":", "\n", "        ", "\"\"\" Compute a distribution over possible year-differences by marginalizing over the year_differnces_mat.\n\n            Parameters:\n            -----------\n            date_distribution_1: ``torch.FloatTensor`` Shape: (self.num_passage_dates, )\n            date_distribution_2: ``torch.FloatTensor`` Shape: (self.num_passage_dates, )\n        \"\"\"", "\n", "with", "Profile", "(", "\"year-diff\"", ")", ":", "\n", "# Shape: (num_passage_dates, num_passage_dates)", "\n", "            ", "joint_dist", "=", "torch", ".", "matmul", "(", "date_distribution_1", ".", "unsqueeze", "(", "1", ")", ",", "date_distribution_2", ".", "unsqueeze", "(", "0", ")", ")", "\n", "\n", "# Shape: (number_year_differences, )", "\n", "year_differences_dist", "=", "torch", ".", "sum", "(", "self", ".", "year_differences_mat", "*", "joint_dist", ".", "unsqueeze", "(", "2", ")", ",", "dim", "=", "(", "0", ",", "1", ")", ")", "\n", "\n", "# if torch.sum(year_differences_dist) > 1.0:", "\n", "# print(\"year dist\")", "\n", "# print(f\"{date_distribution_1} {date_distribution_1.sum()}\")", "\n", "# print(f\"{date_distribution_2} {date_distribution_2.sum()}\")", "\n", "# print(f\"{year_differences_dist} {year_differences_dist.sum()}\")", "\n", "# print()", "\n", "\n", "year_differences_dist", "=", "clamp_distribution", "(", "year_differences_dist", ")", "\n", "\n", "", "return", "year_differences_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.expected_date_comparison": [[911, 934], ["semqa.profiler.profile.Profile", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "drop_language.clamp_distribution", "date_distribution_1.unsqueeze", "date_distribution_2.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.clamp_distribution"], ["", "def", "expected_date_comparison", "(", "self", ",", "date_distribution_1", ",", "date_distribution_2", ",", "comparison", ")", ":", "\n", "        ", "\"\"\" Compute the boolean probability that date_1 > date_2 given distributions over passage_dates for each\n\n        Parameters:\n        -----------\n        date_distribution_1: ``torch.FloatTensor`` Shape: (self.num_passage_dates, )\n        date_distribution_2: ``torch.FloatTensor`` Shape: (self.num_passage_dates, )\n        \"\"\"", "\n", "with", "Profile", "(", "\"date-comp\"", ")", ":", "\n", "# Shape: (num_passage_dates, num_passage_dates)", "\n", "            ", "joint_dist", "=", "torch", ".", "matmul", "(", "date_distribution_1", ".", "unsqueeze", "(", "1", ")", ",", "date_distribution_2", ".", "unsqueeze", "(", "0", ")", ")", "\n", "\n", "if", "comparison", "==", "\"greater\"", ":", "\n", "                ", "comparison_mat", "=", "self", ".", "date_gt_mat", "\n", "", "elif", "comparison", "==", "\"lesser\"", ":", "\n", "                ", "comparison_mat", "=", "self", ".", "date_lt_mat", "\n", "", "else", ":", "\n", "                ", "comparison_mat", "=", "None", "\n", "raise", "NotImplementedError", "\n", "\n", "", "expected_bool", "=", "(", "comparison_mat", "*", "joint_dist", ")", ".", "sum", "(", ")", "\n", "expected_bool", "=", "clamp_distribution", "(", "expected_bool", ")", "\n", "", "return", "expected_bool", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.expected_num_comparison": [[935, 958], ["semqa.profiler.profile.Profile", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "drop_language.clamp_distribution", "distribution_1.unsqueeze", "distribution_2.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.clamp_distribution"], ["", "def", "expected_num_comparison", "(", "self", ",", "distribution_1", ",", "distribution_2", ",", "comparison", ")", ":", "\n", "        ", "\"\"\" Compute the boolean probability that date_1 > date_2 given distributions over passage_dates for each\n\n        Parameters:\n        -----------\n        date_distribution_1: ``torch.FloatTensor`` Shape: (self.num_passage_dates, )\n        date_distribution_2: ``torch.FloatTensor`` Shape: (self.num_passage_dates, )\n        \"\"\"", "\n", "with", "Profile", "(", "\"num-comp\"", ")", ":", "\n", "# Shape: (num_passage_nums, num_passage_nums)", "\n", "            ", "joint_dist", "=", "torch", ".", "matmul", "(", "distribution_1", ".", "unsqueeze", "(", "1", ")", ",", "distribution_2", ".", "unsqueeze", "(", "0", ")", ")", "\n", "\n", "if", "comparison", "==", "\"greater\"", ":", "\n", "                ", "comparison_mat", "=", "self", ".", "num_gt_mat", "\n", "", "elif", "comparison", "==", "\"lesser\"", ":", "\n", "                ", "comparison_mat", "=", "self", ".", "num_lt_mat", "\n", "", "else", ":", "\n", "                ", "comparison_mat", "=", "None", "\n", "raise", "NotImplementedError", "\n", "\n", "", "expected_bool", "=", "(", "comparison_mat", "*", "joint_dist", ")", ".", "sum", "(", ")", "\n", "expected_bool", "=", "clamp_distribution", "(", "expected_bool", ")", "\n", "", "return", "expected_bool", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.date_comparison": [[959, 1007], ["drop_language.DropLanguage.compute_date_scores", "drop_language.DropLanguage.compute_date_scores", "drop_language.DropLanguage.expected_date_comparison", "drop_language.DropLanguage.expected_date_comparison", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.max", "torch.max", "torch.max", "torch.max", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.nll_loss", "torch.nll_loss", "date_grounding_loss.mean.mean.mean", "gold_date_grounding_1.unsqueeze", "gold_date_grounding_2.unsqueeze", "date_distribution_1.unsqueeze", "date_distribution_2.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.compute_date_scores", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.compute_date_scores", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.expected_date_comparison", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.expected_date_comparison"], ["", "def", "date_comparison", "(", "self", ",", "passage_attention_1", ",", "passage_attention_2", ",", "comparison", ":", "str", ",", "gold_date_groundings", "=", "None", ")", ":", "\n", "\n", "        ", "date_distribution_1", ",", "passage_datetoken_prob_1", ",", "d1_dist_entropy", "=", "self", ".", "compute_date_scores", "(", "passage_attention_1", ")", "\n", "date_distribution_2", ",", "passage_datetoken_prob_2", ",", "d2_dist_entropy", "=", "self", ".", "compute_date_scores", "(", "passage_attention_2", ")", "\n", "\n", "bool1", "=", "self", ".", "expected_date_comparison", "(", "date_distribution_1", ",", "date_distribution_2", ",", "comparison", ")", "\n", "bool2", "=", "self", ".", "expected_date_comparison", "(", "date_distribution_2", ",", "date_distribution_1", ",", "comparison", ")", "\n", "\n", "average_passage_distribution", "=", "bool1", "*", "passage_attention_1", "+", "bool2", "*", "passage_attention_2", "\n", "\n", "if", "gold_date_groundings", "is", "not", "None", ":", "\n", "# These are one-hot vectors the size of passage_dates", "\n", "# These can be zeros as well, indicating the date grounding is unknown, in which case they shouldn't be", "\n", "# used for computing the auxiliary date-grounding loss", "\n", "            ", "gold_date_grounding_1", ",", "gold_date_grounding_2", "=", "gold_date_groundings", "\n", "\n", "# Shape: (2, num_dates)", "\n", "gold_date_grounding_tensor", "=", "torch", ".", "cat", "(", "\n", "[", "gold_date_grounding_1", ".", "unsqueeze", "(", "0", ")", ",", "gold_date_grounding_2", ".", "unsqueeze", "(", "0", ")", "]", ",", "dim", "=", "0", "\n", ")", "\n", "\n", "# The groundings that contain a zero vector will yield max as zero, and hence mask = 0", "\n", "# For valid groundings, the max will be 1 (mask = 1) and will contain the correct idx from argmax", "\n", "date_grounding_mask", ",", "gold_date_idxs", "=", "torch", ".", "max", "(", "gold_date_grounding_tensor", ",", "dim", "=", "1", ")", "\n", "\n", "# Shape: [2, num_of_dates]", "\n", "stacked_date_distributions", "=", "torch", ".", "cat", "(", "\n", "[", "date_distribution_1", ".", "unsqueeze", "(", "0", ")", ",", "date_distribution_2", ".", "unsqueeze", "(", "0", ")", "]", ",", "dim", "=", "0", "\n", ")", "\n", "\n", "# Shape: (2) - Converting date_distribution to log-probabilties", "\n", "date_grounding_loss", "=", "F", ".", "nll_loss", "(", "\n", "input", "=", "(", "stacked_date_distributions", "+", "1e-40", ")", ".", "log", "(", ")", ",", "target", "=", "gold_date_idxs", ",", "reduction", "=", "\"none\"", "\n", ")", "\n", "date_grounding_loss", "=", "date_grounding_loss", "*", "date_grounding_mask", "\n", "date_grounding_loss", "=", "date_grounding_loss", ".", "mean", "(", ")", "\n", "\n", "", "else", ":", "\n", "            ", "date_grounding_loss", "=", "0.0", "\n", "\n", "# kl_div_neg_1_2 = -1 * F.kl_div(date_distribution_1, date_distribution_2, reduction='mean')", "\n", "# kl_div_neg_2_1 = -1 * F.kl_div(date_distribution_2, date_distribution_1, reduction='mean')", "\n", "\n", "", "aux_loss", "=", "date_grounding_loss", "\n", "# aux_loss += d1_dist_entropy + d2_dist_entropy", "\n", "\n", "return", "(", "date_distribution_1", ",", "date_distribution_2", ",", "bool1", ",", "bool2", ",", "\n", "passage_datetoken_prob_1", ",", "passage_datetoken_prob_2", ",", "average_passage_distribution", ",", "aux_loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.num_comparison": [[1008, 1056], ["drop_language.DropLanguage.compute_num_distribution", "drop_language.DropLanguage.compute_num_distribution", "drop_language.DropLanguage.expected_num_comparison", "drop_language.DropLanguage.expected_num_comparison", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.max", "torch.max", "torch.max", "torch.max", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.nll_loss", "torch.nll_loss", "num_grounding_loss.mean.mean.mean", "len", "gold_num_grounding_1.unsqueeze", "gold_num_grounding_2.unsqueeze", "num_distribution_1.unsqueeze", "num_distribution_2.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.compute_num_distribution", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.compute_num_distribution", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.expected_num_comparison", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.expected_num_comparison"], ["", "def", "num_comparison", "(", "self", ",", "passage_attention_1", ",", "passage_attention_2", ",", "comparison", ":", "str", ",", "gold_num_groundings", "=", "None", ")", ":", "\n", "\n", "        ", "num_distribution_1", ",", "passage_numtoken_prob_1", ",", "num1_entropy", "=", "self", ".", "compute_num_distribution", "(", "passage_attention_1", ")", "\n", "num_distribution_2", ",", "passage_numtoken_prob_2", ",", "num2_entropy", "=", "self", ".", "compute_num_distribution", "(", "passage_attention_2", ")", "\n", "\n", "bool1", "=", "self", ".", "expected_num_comparison", "(", "num_distribution_1", ",", "num_distribution_2", ",", "comparison", ")", "\n", "bool2", "=", "self", ".", "expected_num_comparison", "(", "num_distribution_2", ",", "num_distribution_1", ",", "comparison", ")", "\n", "\n", "average_passage_distribution", "=", "bool1", "*", "passage_attention_1", "+", "bool2", "*", "passage_attention_2", "\n", "\n", "if", "gold_num_groundings", "is", "not", "None", ":", "\n", "# These are one-hot vectors the size of passage_dates", "\n", "# These can be zeros as well, indicating the date grounding is unknown, in which case they shouldn't be", "\n", "# used for computing the auxiliary date-grounding loss", "\n", "# TODO(nitish): Sometimes in validation we only get one from other questions, for example findNumber", "\n", "            ", "if", "len", "(", "gold_num_groundings", ")", "==", "1", ":", "\n", "                ", "gold_num_groundings", "=", "[", "gold_num_groundings", "[", "0", "]", ",", "gold_num_groundings", "[", "0", "]", "]", "\n", "", "gold_num_grounding_1", ",", "gold_num_grounding_2", "=", "gold_num_groundings", "\n", "\n", "# Shape: (2, num_passage_nums)", "\n", "gold_num_grounding_tensor", "=", "torch", ".", "cat", "(", "\n", "[", "gold_num_grounding_1", ".", "unsqueeze", "(", "0", ")", ",", "gold_num_grounding_2", ".", "unsqueeze", "(", "0", ")", "]", ",", "dim", "=", "0", "\n", ")", "\n", "\n", "# The groundings that contain a zero vector will yield max as zero, and hence mask = 0", "\n", "# For valid groundings, the max will be 1 (mask = 1) and will contain the correct idx from argmax", "\n", "grounding_mask", ",", "gold_idxs", "=", "torch", ".", "max", "(", "gold_num_grounding_tensor", ",", "dim", "=", "1", ")", "\n", "\n", "# Shape: [2, num_of_dates]", "\n", "stacked_num_distributions", "=", "torch", ".", "cat", "(", "\n", "[", "num_distribution_1", ".", "unsqueeze", "(", "0", ")", ",", "num_distribution_2", ".", "unsqueeze", "(", "0", ")", "]", ",", "dim", "=", "0", "\n", ")", "\n", "# Shape: (2) - Converting date_distribution to log-probabilties", "\n", "num_grounding_loss", "=", "F", ".", "nll_loss", "(", "\n", "input", "=", "(", "stacked_num_distributions", "+", "1e-40", ")", ".", "log", "(", ")", ",", "target", "=", "gold_idxs", ",", "reduction", "=", "\"none\"", "\n", ")", "\n", "num_grounding_loss", "=", "num_grounding_loss", "*", "grounding_mask", "\n", "num_grounding_loss", "=", "num_grounding_loss", ".", "mean", "(", ")", "\n", "\n", "", "else", ":", "\n", "            ", "num_grounding_loss", "=", "0.0", "\n", "\n", "", "aux_loss", "=", "num_grounding_loss", "\n", "# aux_loss += num1_entropy + num2_entropy", "\n", "\n", "return", "(", "num_distribution_1", ",", "num_distribution_2", ",", "bool1", ",", "bool2", ",", "\n", "passage_numtoken_prob_1", ",", "passage_numtoken_prob_2", ",", "\n", "average_passage_distribution", ",", "aux_loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.compare_date_lesser_than": [[1058, 1116], ["allennlp_semparse.domain_languages.predicate_with_side_args", "drop_language.DropLanguage.date_comparison", "drop_language.clamp_distribution", "drop_language.PassageAttention_answer", "semqa.domain_languages.domain_language_utils.listTokensVis", "semqa.domain_languages.domain_language_utils.listTokensVis", "utils.round_all", "utils.round_all", "utils.round_all", "utils.round_all", "utils.round_all", "utils.round_all", "drop_language.DropLanguage.modules_debug_info[].append", "utils.round_all", "utils.round_all", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.date_comparison", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.clamp_distribution", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.listTokensVis", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.listTokensVis", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList"], ["", "@", "predicate_with_side_args", "(", "[", "\"event_date_groundings\"", "]", ")", "\n", "def", "compare_date_lesser_than", "(", "\n", "self", ",", "passage_attn_1", ":", "PassageAttention", ",", "passage_attn_2", ":", "PassageAttention", ",", "event_date_groundings", "=", "None", "\n", ")", "->", "PassageAttention_answer", ":", "\n", "\n", "        ", "passage_attention_1", "=", "passage_attn_1", ".", "_value", "*", "self", ".", "passage_mask", "\n", "passage_attention_2", "=", "passage_attn_2", ".", "_value", "*", "self", ".", "passage_mask", "\n", "\n", "(", "\n", "date_distribution_1", ",", "\n", "date_distribution_2", ",", "\n", "prob_date1_lesser", ",", "\n", "prob_date2_lesser", ",", "\n", "passage_datetoken_prob_1", ",", "\n", "passage_datetoken_prob_2", ",", "\n", "average_passage_distribution", ",", "\n", "aux_loss", ",", "\n", ")", "=", "self", ".", "date_comparison", "(", "passage_attention_1", ",", "passage_attention_2", ",", "\"lesser\"", ",", "event_date_groundings", ")", "\n", "\n", "average_passage_distribution", "=", "clamp_distribution", "(", "average_passage_distribution", ")", "\n", "loss", "=", "0.0", "\n", "loss", "+=", "aux_loss", "\n", "loss", "+=", "passage_attn_1", ".", "loss", "\n", "loss", "+=", "passage_attn_2", ".", "loss", "\n", "\n", "debug_value", "=", "\"\"", "\n", "if", "self", ".", "_debug", ":", "\n", "            ", "if", "event_date_groundings", ":", "\n", "                ", "gold_date_1", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "event_date_groundings", "[", "0", "]", ")", ",", "3", ")", "\n", "gold_date_2", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "event_date_groundings", "[", "1", "]", ")", ",", "3", ")", "\n", "", "else", ":", "\n", "                ", "gold_date_1", ",", "gold_date_2", "=", "None", ",", "None", "\n", "", "_", ",", "pattn_vis_most_1", "=", "dlutils", ".", "listTokensVis", "(", "passage_attention_1", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", ")", "\n", "_", ",", "pattn_vis_most_2", "=", "dlutils", ".", "listTokensVis", "(", "passage_attention_2", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", ")", "\n", "\n", "date1", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "date_distribution_1", ")", ",", "3", ")", "\n", "date2", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "date_distribution_2", ")", ",", "3", ")", "\n", "d1_lt_d2", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "prob_date1_lesser", ")", ",", "3", ")", "\n", "d2_lt_d1", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "prob_date2_lesser", ")", ",", "3", ")", "\n", "date1_token", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "passage_datetoken_prob_1", ")", ",", "3", ")", "\n", "date2_token", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "passage_datetoken_prob_2", ")", ",", "3", ")", "\n", "\n", "debug_value", "+=", "(", "\n", "f\"Pattn1: {pattn_vis_most_1}\\n Date1: {date1}\"", "\n", "+", "f\"\\nPattn2: {pattn_vis_most_2}\\n Date2: {date2}\"", "\n", "+", "f\"\\nP(D1 < D2): {d1_lt_d2}  P(D2 < D1): {d2_lt_d1}\"", "\n", ")", "\n", "if", "gold_date_1", ":", "\n", "                ", "debug_value", "+=", "f\"\\nGoldDates  Date1: {gold_date_1}  Date2: {gold_date_2}\"", "\n", "\n", "", "debug_info_dict", "=", "{", "\"compare-date-lt\"", ":", "{", "\"date_1\"", ":", "date_distribution_1", ",", "\n", "\"date_2\"", ":", "date_distribution_2", ",", "\n", "\"passage_date_1\"", ":", "date1_token", ",", "\n", "\"passage_date_2\"", ":", "date2_token", ",", "\n", "\"passage\"", ":", "average_passage_distribution", "}", "}", "\n", "self", ".", "modules_debug_info", "[", "-", "1", "]", ".", "append", "(", "debug_info_dict", ")", "\n", "\n", "", "return", "PassageAttention_answer", "(", "average_passage_distribution", ",", "loss", "=", "loss", ",", "debug_value", "=", "debug_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.compare_date_greater_than": [[1118, 1176], ["allennlp_semparse.domain_languages.predicate_with_side_args", "drop_language.DropLanguage.date_comparison", "drop_language.clamp_distribution", "drop_language.PassageAttention_answer", "semqa.domain_languages.domain_language_utils.listTokensVis", "semqa.domain_languages.domain_language_utils.listTokensVis", "utils.round_all", "utils.round_all", "utils.round_all", "utils.round_all", "utils.round_all", "utils.round_all", "drop_language.DropLanguage.modules_debug_info[].append", "utils.round_all", "utils.round_all", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.date_comparison", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.clamp_distribution", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.listTokensVis", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.listTokensVis", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList"], ["", "@", "predicate_with_side_args", "(", "[", "\"event_date_groundings\"", "]", ")", "\n", "def", "compare_date_greater_than", "(", "\n", "self", ",", "passage_attn_1", ":", "PassageAttention", ",", "passage_attn_2", ":", "PassageAttention", ",", "event_date_groundings", "=", "None", "\n", ")", "->", "PassageAttention_answer", ":", "\n", "        ", "\"\"\" In short; outputs PA_1 if D1 > D2 i.e. is PA_1 occurred after PA_2 \"\"\"", "\n", "\n", "passage_attention_1", "=", "passage_attn_1", ".", "_value", "*", "self", ".", "passage_mask", "\n", "passage_attention_2", "=", "passage_attn_2", ".", "_value", "*", "self", ".", "passage_mask", "\n", "\n", "(", "\n", "date_distribution_1", ",", "\n", "date_distribution_2", ",", "\n", "prob_date1_greater", ",", "\n", "prob_date2_greater", ",", "\n", "passage_datetoken_prob_1", ",", "\n", "passage_datetoken_prob_2", ",", "\n", "average_passage_distribution", ",", "\n", "aux_loss", ",", "\n", ")", "=", "self", ".", "date_comparison", "(", "passage_attention_1", ",", "passage_attention_2", ",", "\"greater\"", ",", "event_date_groundings", ")", "\n", "average_passage_distribution", "=", "clamp_distribution", "(", "average_passage_distribution", ")", "\n", "\n", "loss", "=", "0.0", "\n", "loss", "+=", "aux_loss", "\n", "loss", "+=", "passage_attn_1", ".", "loss", "\n", "loss", "+=", "passage_attn_2", ".", "loss", "\n", "\n", "debug_value", "=", "\"\"", "\n", "if", "self", ".", "_debug", ":", "\n", "            ", "if", "event_date_groundings", ":", "\n", "                ", "gold_date_1", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "event_date_groundings", "[", "0", "]", ")", ",", "3", ")", "\n", "gold_date_2", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "event_date_groundings", "[", "1", "]", ")", ",", "3", ")", "\n", "", "else", ":", "\n", "                ", "gold_date_1", ",", "gold_date_2", "=", "None", ",", "None", "\n", "", "_", ",", "pattn_vis_most_1", "=", "dlutils", ".", "listTokensVis", "(", "passage_attention_1", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", ")", "\n", "_", ",", "pattn_vis_most_2", "=", "dlutils", ".", "listTokensVis", "(", "passage_attention_2", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", ")", "\n", "\n", "date1", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "date_distribution_1", ")", ",", "3", ")", "\n", "date2", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "date_distribution_2", ")", ",", "3", ")", "\n", "d1_gt_d2", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "prob_date1_greater", ")", ",", "3", ")", "\n", "d2_gt_d1", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "prob_date2_greater", ")", ",", "3", ")", "\n", "date1_token", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "passage_datetoken_prob_1", ")", ",", "3", ")", "\n", "date2_token", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "passage_datetoken_prob_2", ")", ",", "3", ")", "\n", "\n", "debug_value", "+=", "(", "\n", "f\"Pattn1: {pattn_vis_most_1}\\n Date1: {date1}\"", "\n", "+", "f\"\\nPattn2: {pattn_vis_most_2}\\n Date2: {date2}\"", "\n", "+", "f\"\\nP(D1 > D2): {d1_gt_d2}  P(D2 > D1): {d2_gt_d1}\"", "\n", ")", "\n", "if", "gold_date_1", ":", "\n", "                ", "debug_value", "+=", "f\"\\nGoldDates  Date1: {gold_date_1}  Date2: {gold_date_2}\"", "\n", "", "debug_info_dict", "=", "{", "\"compare-date-gt\"", ":", "{", "\"date_1\"", ":", "date_distribution_1", ",", "\n", "\"date_2\"", ":", "date_distribution_2", ",", "\n", "\"passage_date_1\"", ":", "date1_token", ",", "\n", "\"passage_date_2\"", ":", "date2_token", ",", "\n", "\"passage\"", ":", "average_passage_distribution", "}", "}", "\n", "self", ".", "modules_debug_info", "[", "-", "1", "]", ".", "append", "(", "debug_info_dict", ")", "\n", "\n", "", "return", "PassageAttention_answer", "(", "average_passage_distribution", ",", "loss", "=", "loss", ",", "debug_value", "=", "debug_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.compare_num_lesser_than": [[1177, 1233], ["allennlp_semparse.domain_languages.predicate_with_side_args", "drop_language.DropLanguage.num_comparison", "drop_language.clamp_distribution", "drop_language.PassageAttention_answer", "semqa.domain_languages.domain_language_utils.listTokensVis", "semqa.domain_languages.domain_language_utils.listTokensVis", "utils.round_all", "utils.round_all", "utils.round_all", "utils.round_all", "utils.round_all", "utils.round_all", "drop_language.DropLanguage.modules_debug_info[].append", "utils.round_all", "utils.round_all", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.num_comparison", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.clamp_distribution", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.listTokensVis", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.listTokensVis", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList"], ["", "@", "predicate_with_side_args", "(", "[", "\"event_num_groundings\"", "]", ")", "\n", "def", "compare_num_lesser_than", "(", "\n", "self", ",", "passage_attn_1", ":", "PassageAttention", ",", "passage_attn_2", ":", "PassageAttention", ",", "event_num_groundings", "=", "None", "\n", ")", "->", "PassageAttention_answer", ":", "\n", "\n", "        ", "passage_attention_1", "=", "passage_attn_1", ".", "_value", "*", "self", ".", "passage_mask", "\n", "passage_attention_2", "=", "passage_attn_2", ".", "_value", "*", "self", ".", "passage_mask", "\n", "\n", "(", "\n", "num_distribution_1", ",", "\n", "num_distribution_2", ",", "\n", "prob_num1_lesser", ",", "\n", "prob_num2_lesser", ",", "\n", "passage_numtoken_prob_1", ",", "passage_numtoken_prob_2", ",", "\n", "average_passage_distribution", ",", "\n", "aux_loss", ",", "\n", ")", "=", "self", ".", "num_comparison", "(", "passage_attention_1", ",", "passage_attention_2", ",", "\"lesser\"", ",", "event_num_groundings", ")", "\n", "average_passage_distribution", "=", "clamp_distribution", "(", "average_passage_distribution", ")", "\n", "\n", "loss", "=", "0.0", "\n", "loss", "+=", "aux_loss", "\n", "loss", "+=", "passage_attn_1", ".", "loss", "\n", "loss", "+=", "passage_attn_2", ".", "loss", "\n", "\n", "debug_value", "=", "\"\"", "\n", "if", "self", ".", "_debug", ":", "\n", "            ", "if", "event_num_groundings", ":", "\n", "                ", "gold_num_1", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "event_num_groundings", "[", "0", "]", ")", ",", "3", ")", "\n", "gold_num_2", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "event_num_groundings", "[", "1", "]", ")", ",", "3", ")", "\n", "", "else", ":", "\n", "                ", "gold_num_1", ",", "gold_num_2", "=", "None", ",", "None", "\n", "", "_", ",", "pattn_vis_most_1", "=", "dlutils", ".", "listTokensVis", "(", "passage_attention_1", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", ")", "\n", "_", ",", "pattn_vis_most_2", "=", "dlutils", ".", "listTokensVis", "(", "passage_attention_2", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", ")", "\n", "\n", "num1", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "num_distribution_1", ")", ",", "3", ")", "\n", "num2", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "num_distribution_2", ")", ",", "3", ")", "\n", "d1_lt_d2", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "prob_num1_lesser", ")", ",", "3", ")", "\n", "d2_lt_d1", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "prob_num2_lesser", ")", ",", "3", ")", "\n", "num1_token", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "passage_numtoken_prob_1", ")", ",", "3", ")", "\n", "num2_token", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "passage_numtoken_prob_2", ")", ",", "3", ")", "\n", "\n", "debug_value", "+=", "(", "\n", "f\"Pattn1: {pattn_vis_most_1}\\n Num1: {num1}\"", "\n", "+", "f\"\\nPattn2: {pattn_vis_most_2}\\n Num2: {num2}\"", "\n", "+", "f\"\\nP(N1 < N2): {d1_lt_d2}  P(N2 < N1): {d2_lt_d1}\"", "\n", ")", "\n", "if", "gold_num_1", ":", "\n", "                ", "debug_value", "+=", "f\"\\nGoldNums Num1: {gold_num_1}  Num2: {gold_num_2}\"", "\n", "", "debug_info_dict", "=", "{", "\"compare-num-lt\"", ":", "{", "\"number_1\"", ":", "num_distribution_1", ",", "\n", "\"number_2\"", ":", "num_distribution_2", ",", "\n", "\"passage_number_1\"", ":", "num1_token", ",", "\n", "\"passage_number_2\"", ":", "num2_token", ",", "\n", "\"passage\"", ":", "average_passage_distribution", "}", "}", "\n", "self", ".", "modules_debug_info", "[", "-", "1", "]", ".", "append", "(", "debug_info_dict", ")", "\n", "\n", "", "return", "PassageAttention_answer", "(", "average_passage_distribution", ",", "loss", "=", "loss", ",", "debug_value", "=", "debug_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.compare_num_greater_than": [[1235, 1294], ["allennlp_semparse.domain_languages.predicate_with_side_args", "drop_language.DropLanguage.num_comparison", "drop_language.clamp_distribution", "drop_language.PassageAttention_answer", "semqa.domain_languages.domain_language_utils.listTokensVis", "semqa.domain_languages.domain_language_utils.listTokensVis", "utils.round_all", "utils.round_all", "utils.round_all", "utils.round_all", "utils.round_all", "utils.round_all", "drop_language.DropLanguage.modules_debug_info[].append", "utils.round_all", "utils.round_all", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.num_comparison", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.clamp_distribution", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.listTokensVis", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.listTokensVis", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList"], ["", "@", "predicate_with_side_args", "(", "[", "\"event_num_groundings\"", "]", ")", "\n", "def", "compare_num_greater_than", "(", "\n", "self", ",", "passage_attn_1", ":", "PassageAttention", ",", "passage_attn_2", ":", "PassageAttention", ",", "event_num_groundings", "=", "None", "\n", ")", "->", "PassageAttention_answer", ":", "\n", "        ", "\"\"\" In short; outputs PA_1 if D1 > D2 i.e. is PA_1 occurred after PA_2\n        \"\"\"", "\n", "\n", "passage_attention_1", "=", "passage_attn_1", ".", "_value", "*", "self", ".", "passage_mask", "\n", "passage_attention_2", "=", "passage_attn_2", ".", "_value", "*", "self", ".", "passage_mask", "\n", "\n", "(", "\n", "num_distribution_1", ",", "\n", "num_distribution_2", ",", "\n", "prob_num1_greater", ",", "\n", "prob_num2_greater", ",", "\n", "passage_numtoken_prob_1", ",", "passage_numtoken_prob_2", ",", "\n", "average_passage_distribution", ",", "\n", "aux_loss", ",", "\n", ")", "=", "self", ".", "num_comparison", "(", "passage_attention_1", ",", "passage_attention_2", ",", "\"greater\"", ",", "event_num_groundings", ")", "\n", "average_passage_distribution", "=", "clamp_distribution", "(", "average_passage_distribution", ")", "\n", "\n", "loss", "=", "0.0", "\n", "loss", "+=", "aux_loss", "\n", "loss", "+=", "passage_attn_1", ".", "loss", "\n", "loss", "+=", "passage_attn_2", ".", "loss", "\n", "\n", "debug_value", "=", "\"\"", "\n", "if", "self", ".", "_debug", ":", "\n", "            ", "if", "event_num_groundings", ":", "\n", "                ", "gold_num_1", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "event_num_groundings", "[", "0", "]", ")", ",", "3", ")", "\n", "gold_num_2", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "event_num_groundings", "[", "1", "]", ")", ",", "3", ")", "\n", "", "else", ":", "\n", "                ", "gold_num_1", ",", "gold_num_2", "=", "None", ",", "None", "\n", "", "_", ",", "pattn_vis_most_1", "=", "dlutils", ".", "listTokensVis", "(", "passage_attention_1", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", ")", "\n", "_", ",", "pattn_vis_most_2", "=", "dlutils", ".", "listTokensVis", "(", "passage_attention_2", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", ")", "\n", "\n", "num1", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "num_distribution_1", ")", ",", "3", ")", "\n", "num2", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "num_distribution_2", ")", ",", "3", ")", "\n", "d1_gt_d2", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "prob_num1_greater", ")", ",", "3", ")", "\n", "d2_gt_d1", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "prob_num2_greater", ")", ",", "3", ")", "\n", "num1_token", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "passage_numtoken_prob_1", ")", ",", "3", ")", "\n", "num2_token", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "passage_numtoken_prob_2", ")", ",", "3", ")", "\n", "\n", "debug_value", "+=", "(", "\n", "f\"Pattn1: {pattn_vis_most_1}\\n num1: {num1}\"", "\n", "+", "f\"\\nPattn2: {pattn_vis_most_2}\\n num2: {num2}\"", "\n", "+", "f\"\\nP(N1 > N2): {d1_gt_d2}  P(N2 > N1): {d2_gt_d1}\"", "\n", ")", "\n", "\n", "if", "gold_num_1", ":", "\n", "                ", "debug_value", "+=", "f\"\\nGoldNums  num1: {gold_num_1}  num2: {gold_num_2}\"", "\n", "", "debug_info_dict", "=", "{", "\"compare-num-gt\"", ":", "{", "\"number_1\"", ":", "num_distribution_1", ",", "\n", "\"number_2\"", ":", "num_distribution_2", ",", "\n", "\"passage_number_1\"", ":", "num1_token", ",", "\n", "\"passage_number_2\"", ":", "num2_token", ",", "\n", "\"passage\"", ":", "average_passage_distribution", "}", "}", "\n", "self", ".", "modules_debug_info", "[", "-", "1", "]", ".", "append", "(", "debug_info_dict", ")", "\n", "\n", "", "return", "PassageAttention_answer", "(", "average_passage_distribution", ",", "loss", "=", "loss", ",", "debug_value", "=", "debug_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.year_difference": [[1295, 1338], ["drop_language.DropLanguage.compute_date_scores", "drop_language.DropLanguage.compute_date_scores", "drop_language.DropLanguage.expected_date_year_difference", "drop_language.YearDifference", "semqa.domain_languages.domain_language_utils.listTokensVis", "semqa.domain_languages.domain_language_utils.listTokensVis", "utils.round_all", "utils.round_all", "utils.round_all", "utils.round_all", "utils.round_all", "drop_language.DropLanguage.modules_debug_info[].append", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.compute_date_scores", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.compute_date_scores", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.expected_date_year_difference", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.listTokensVis", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.listTokensVis", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList"], ["", "@", "predicate", "\n", "def", "year_difference", "(", "self", ",", "passage_attn_1", ":", "PassageAttention", ",", "passage_attn_2", ":", "PassageAttention", ")", "->", "YearDifference", ":", "\n", "        ", "\"\"\" Given two passage spans, ground them to dates, and then return the difference between their years \"\"\"", "\n", "\n", "passage_attention_1", "=", "passage_attn_1", ".", "_value", "*", "self", ".", "passage_mask", "\n", "passage_attention_2", "=", "passage_attn_2", ".", "_value", "*", "self", ".", "passage_mask", "\n", "\n", "date_distribution_1", ",", "passage_datetoken_prob_1", ",", "d1_dist_entropy", "=", "self", ".", "compute_date_scores", "(", "passage_attention_1", ")", "\n", "date_distribution_2", ",", "passage_datetoken_prob_2", ",", "d2_dist_entropy", "=", "self", ".", "compute_date_scores", "(", "passage_attention_2", ")", "\n", "\n", "# Shape: (number_of_year_differences, )", "\n", "year_difference_dist", "=", "self", ".", "expected_date_year_difference", "(", "date_distribution_1", ",", "date_distribution_2", ")", "\n", "\n", "loss", "=", "0.0", "\n", "loss", "+=", "passage_attn_1", ".", "loss", "\n", "loss", "+=", "passage_attn_2", ".", "loss", "\n", "# If we want to use an auxiliary entropy loss", "\n", "# loss += d1_dist_entropy + d2_dist_entropy", "\n", "\n", "debug_value", "=", "\"\"", "\n", "if", "self", ".", "_debug", ":", "\n", "            ", "_", ",", "pattn_vis_most_1", "=", "dlutils", ".", "listTokensVis", "(", "passage_attention_1", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", ")", "\n", "_", ",", "pattn_vis_most_2", "=", "dlutils", ".", "listTokensVis", "(", "passage_attention_2", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", ")", "\n", "\n", "date1", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "date_distribution_1", ")", ",", "3", ")", "\n", "date2", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "date_distribution_2", ")", ",", "3", ")", "\n", "year_diff_dist", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "year_difference_dist", ")", ",", "3", ")", "\n", "date1_token", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "passage_datetoken_prob_1", ")", ",", "3", ")", "\n", "date2_token", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "passage_datetoken_prob_2", ")", ",", "3", ")", "\n", "\n", "debug_value", "+=", "(", "\n", "f\"YearDiffDist: {year_diff_dist}\\n\"", "\n", "+", "f\"\\nPattn1: {pattn_vis_most_1}\\n Date1: {date1}\"", "\n", "+", "f\"\\nPattn2: {pattn_vis_most_2}\\n Date2: {date2}\"", "\n", ")", "\n", "debug_info_dict", "=", "{", "\"year-diff\"", ":", "{", "\"date_1\"", ":", "date_distribution_1", ",", "\n", "\"date_2\"", ":", "date_distribution_2", ",", "\n", "\"passage_date_1\"", ":", "date1_token", ",", "\n", "\"passage_date_2\"", ":", "date2_token", ",", "\n", "\"year-diff\"", ":", "year_difference_dist", "}", "}", "\n", "self", ".", "modules_debug_info", "[", "-", "1", "]", ".", "append", "(", "debug_info_dict", ")", "\n", "\n", "", "return", "YearDifference", "(", "year_difference_dist", "=", "year_difference_dist", ",", "loss", "=", "loss", ",", "debug_value", "=", "debug_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.year_difference_single_event": [[1339, 1384], ["drop_language.DropLanguage.compute_date_scores", "drop_language.DropLanguage.compute_date_scores", "drop_language.DropLanguage.expected_date_year_difference", "drop_language.YearDifference", "semqa.domain_languages.domain_language_utils.listTokensVis", "semqa.domain_languages.domain_language_utils.mostAttendedSpans", "utils.round_all", "utils.round_all", "utils.round_all", "utils.round_all", "utils.round_all", "drop_language.DropLanguage.modules_debug_info[].append", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList", "utils.tocpuNPList"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.compute_date_scores", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.compute_date_scores", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.expected_date_year_difference", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.listTokensVis", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.mostAttendedSpans", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList"], ["", "@", "predicate", "\n", "def", "year_difference_single_event", "(", "self", ",", "passage_attn", ":", "PassageAttention", ")", "->", "YearDifference", ":", "\n", "        ", "\"\"\" Given a single passage span, find its start and end dates, then return the difference in years \"\"\"", "\n", "\n", "passage_attention", "=", "passage_attn", ".", "_value", "*", "self", ".", "passage_mask", "\n", "\n", "# DATE_1 is end since the difference is computed as DATE_1 - DATE_2", "\n", "date_distribution_1", ",", "passage_datetoken_prob_1", ",", "d1_dist_entropy", "=", "self", ".", "compute_date_scores", "(", "passage_attention", ",", "\n", "date_type", "=", "\"end\"", ")", "\n", "date_distribution_2", ",", "passage_datetoken_prob_2", ",", "d2_dist_entropy", "=", "self", ".", "compute_date_scores", "(", "passage_attention", ",", "\n", "date_type", "=", "\"start\"", ")", "\n", "\n", "# Shape: (number_of_year_differences, )", "\n", "year_difference_dist", "=", "self", ".", "expected_date_year_difference", "(", "date_distribution_1", ",", "date_distribution_2", ")", "\n", "\n", "loss", "=", "0.0", "\n", "loss", "+=", "passage_attn", ".", "loss", "\n", "# If we want to use an auxiliary entropy loss", "\n", "# loss += d1_dist_entropy + d2_dist_entropy", "\n", "\n", "debug_value", "=", "\"\"", "\n", "if", "self", ".", "_debug", ":", "\n", "            ", "_", ",", "pattn_vis_most_1", "=", "dlutils", ".", "listTokensVis", "(", "passage_attention", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", ")", "\n", "most_attended_spans", "=", "dlutils", ".", "mostAttendedSpans", "(", "passage_attention", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", ")", "\n", "\n", "date1", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "date_distribution_1", ")", ",", "3", ")", "\n", "date2", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "date_distribution_2", ")", ",", "3", ")", "\n", "year_diff_dist", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "year_difference_dist", ")", ",", "3", ")", "\n", "date1_token", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "passage_datetoken_prob_1", ")", ",", "3", ")", "\n", "date2_token", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "passage_datetoken_prob_2", ")", ",", "3", ")", "\n", "\n", "debug_value", "+=", "(", "\n", "f\"YearDiffDist: {year_diff_dist}\\n\"", "\n", "+", "f\"\\nPattn1: {pattn_vis_most_1}\"", "\n", "+", "f\"\\nMostAttendedSpans: {most_attended_spans}\"", "\n", "+", "f\"\\n Date1: {date1}\\n Date2: {date2}\"", "\n", ")", "\n", "debug_info_dict", "=", "{", "\"year-diff\"", ":", "{", "\"date_1\"", ":", "date_distribution_1", ",", "\n", "\"date_2\"", ":", "date_distribution_2", ",", "\n", "\"passage_date_1\"", ":", "date1_token", ",", "\n", "\"passage_date_2\"", ":", "date2_token", ",", "\n", "\"year-diff\"", ":", "year_difference_dist", "}", "}", "\n", "self", ".", "modules_debug_info", "[", "-", "1", "]", ".", "append", "(", "debug_info_dict", ")", "\n", "\n", "", "return", "YearDifference", "(", "year_difference_dist", "=", "year_difference_dist", ",", "loss", "=", "loss", ",", "debug_value", "=", "debug_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.extractPassageSpanAnswer": [[1385, 1419], ["drop_language.PassageSpanAnswer", "semqa.profiler.profile.Profile", "drop_language.DropLanguage.parameters.oneshot_psa_startend_predictor", "allennlp.replace_masked_values", "allennlp.replace_masked_values", "allennlp.masked_log_softmax", "allennlp.masked_log_softmax", "allennlp.replace_masked_values", "allennlp.replace_masked_values"], "methods", ["None"], ["", "@", "predicate", "\n", "def", "extractPassageSpanAnswer", "(", "self", ")", "->", "PassageSpanAnswer", ":", "\n", "        ", "with", "Profile", "(", "\"pass-span-ans\"", ")", ":", "\n", "# Shape: (passage_length, encoded_dim)", "\n", "            ", "passage_repr", "=", "self", ".", "modeled_passage", "if", "self", ".", "modeled_passage", "is", "not", "None", "else", "self", ".", "encoded_passage", "\n", "\n", "# Shape: (passage_length, 2)", "\n", "passage_ans_startend_logits", "=", "self", ".", "parameters", ".", "oneshot_psa_startend_predictor", "(", "passage_repr", ")", "\n", "# Shape: (passage_length,)", "\n", "span_start_logits", "=", "passage_ans_startend_logits", "[", ":", ",", "0", "]", "\n", "span_end_logits", "=", "passage_ans_startend_logits", "[", ":", ",", "1", "]", "\n", "\n", "span_start_logits", "=", "allenutil", ".", "replace_masked_values", "(", "span_start_logits", ",", "self", ".", "passage_mask", ",", "-", "1e32", ")", "\n", "span_end_logits", "=", "allenutil", ".", "replace_masked_values", "(", "span_end_logits", ",", "self", ".", "passage_mask", ",", "-", "1e32", ")", "\n", "\n", "span_start_log_probs", "=", "allenutil", ".", "masked_log_softmax", "(", "span_start_logits", ",", "self", ".", "passage_mask", ")", "\n", "span_end_log_probs", "=", "allenutil", ".", "masked_log_softmax", "(", "span_end_logits", ",", "self", ".", "passage_mask", ")", "\n", "\n", "span_start_log_probs", "=", "allenutil", ".", "replace_masked_values", "(", "span_start_log_probs", ",", "self", ".", "passage_mask", ",", "-", "1e32", ")", "\n", "span_end_log_probs", "=", "allenutil", ".", "replace_masked_values", "(", "span_end_log_probs", ",", "self", ".", "passage_mask", ",", "-", "1e32", ")", "\n", "\n", "loss", "=", "0.0", "\n", "\n", "debug_value", "=", "\"\"", "\n", "if", "self", ".", "_debug", ":", "\n", "                ", "debug_value", "+=", "f\"OneShotPassageSpanAnswer extraction: nothing to visualize\"", "\n", "\n", "", "", "return", "PassageSpanAnswer", "(", "\n", "passage_span_start_log_probs", "=", "span_start_log_probs", ",", "\n", "passage_span_end_log_probs", "=", "span_end_log_probs", ",", "\n", "start_logits", "=", "span_start_logits", ",", "\n", "end_logits", "=", "span_end_logits", ",", "\n", "loss", "=", "loss", ",", "\n", "debug_value", "=", "debug_value", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.find_passageSpanAnswer": [[1421, 1476], ["drop_language.PassageSpanAnswer", "semqa.profiler.profile.Profile", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "drop_language.DropLanguage.parameters.passage_attention_to_span().squeeze", "drop_language.DropLanguage.parameters.passage_startend_predictor", "allennlp.replace_masked_values", "allennlp.replace_masked_values", "allennlp.masked_log_softmax", "allennlp.masked_log_softmax", "allennlp.replace_masked_values", "allennlp.replace_masked_values", "drop_language.DropLanguage.modules_debug_info[].append", "semqa.domain_languages.domain_language_utils.listTokensVis", "semqa.domain_languages.domain_language_utils.mostAttendedSpans", "drop_language.DropLanguage.parameters.passage_attention_to_span", "torch.stack.unsqueeze", "torch.stack.unsqueeze", "passage_mask.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.listTokensVis", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.mostAttendedSpans"], ["", "@", "predicate", "\n", "def", "find_passageSpanAnswer", "(", "self", ",", "passage_attention", ":", "PassageAttention_answer", ")", "->", "PassageSpanAnswer", ":", "\n", "        ", "with", "Profile", "(", "\"find-span-ans\"", ")", ":", "\n", "            ", "passage_attn", "=", "passage_attention", ".", "_value", "\n", "passage_mask", "=", "self", ".", "passage_mask", "\n", "\n", "# Shape: (passage_length)", "\n", "# passage_attn = passage_attn * self.passage_mask", "\n", "passage_attn", "=", "passage_attn", "*", "passage_mask", "\n", "\n", "scaled_attentions", "=", "[", "passage_attn", "*", "sf", "for", "sf", "in", "self", ".", "parameters", ".", "passage_attention_scalingvals", "]", "\n", "# Shape: (passage_length, num_scaling_factors)", "\n", "scaled_passage_attentions", "=", "torch", ".", "stack", "(", "scaled_attentions", ",", "dim", "=", "1", ")", "\n", "\n", "# Shape: (passage_lengths, hidden_dim)", "\n", "passage_span_hidden_reprs", "=", "self", ".", "parameters", ".", "passage_attention_to_span", "(", "\n", "scaled_passage_attentions", ".", "unsqueeze", "(", "0", ")", ",", "passage_mask", ".", "unsqueeze", "(", "0", ")", "\n", ")", ".", "squeeze", "(", "0", ")", "\n", "\n", "# Shape: (passage_lengths, 2)", "\n", "passage_span_logits", "=", "self", ".", "parameters", ".", "passage_startend_predictor", "(", "passage_span_hidden_reprs", ")", "\n", "\n", "# Shape: (passage_length)", "\n", "span_start_logits", "=", "passage_span_logits", "[", ":", ",", "0", "]", "\n", "span_end_logits", "=", "passage_span_logits", "[", ":", ",", "1", "]", "\n", "\n", "span_start_logits", "=", "allenutil", ".", "replace_masked_values", "(", "span_start_logits", ",", "passage_mask", ",", "-", "1e32", ")", "\n", "span_end_logits", "=", "allenutil", ".", "replace_masked_values", "(", "span_end_logits", ",", "passage_mask", ",", "-", "1e32", ")", "\n", "\n", "span_start_log_probs", "=", "allenutil", ".", "masked_log_softmax", "(", "span_start_logits", ",", "passage_mask", ")", "\n", "span_end_log_probs", "=", "allenutil", ".", "masked_log_softmax", "(", "span_end_logits", ",", "passage_mask", ")", "\n", "\n", "span_start_log_probs", "=", "allenutil", ".", "replace_masked_values", "(", "span_start_log_probs", ",", "passage_mask", ",", "-", "1e32", ")", "\n", "span_end_log_probs", "=", "allenutil", ".", "replace_masked_values", "(", "span_end_log_probs", ",", "passage_mask", ",", "-", "1e32", ")", "\n", "\n", "loss", "=", "passage_attention", ".", "loss", "\n", "\n", "debug_value", "=", "\"\"", "\n", "if", "self", ".", "_debug", ":", "\n", "                ", "debug_info_dict", "=", "{", "\"span\"", ":", "{", "\"span_start_logits\"", ":", "span_start_logits", ",", "\n", "\"span_end_logits\"", ":", "span_end_logits", ",", "\n", "\"passage_input\"", ":", "passage_attn", "}", "}", "\n", "self", ".", "modules_debug_info", "[", "-", "1", "]", ".", "append", "(", "debug_info_dict", ")", "\n", "_", ",", "pattn_vis_most", "=", "dlutils", ".", "listTokensVis", "(", "passage_attn", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", ")", "\n", "most_attended_spans", "=", "dlutils", ".", "mostAttendedSpans", "(", "passage_attn", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", ")", "\n", "# debug_value += f\"Pattn: {pattn_vis_most}\\n\"", "\n", "debug_value", "+=", "f\"MostAttendedSpans: {most_attended_spans}\"", "\n", "\n", "", "", "return", "PassageSpanAnswer", "(", "\n", "passage_span_start_log_probs", "=", "span_start_log_probs", ",", "\n", "passage_span_end_log_probs", "=", "span_end_log_probs", ",", "\n", "start_logits", "=", "span_start_logits", ",", "\n", "end_logits", "=", "span_end_logits", ",", "\n", "loss", "=", "loss", ",", "\n", "debug_value", "=", "debug_value", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.passageAttn2Count": [[1478, 1535], ["drop_language.CountNumber", "semqa.profiler.profile.Profile", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "drop_language.DropLanguage.parameters.passage_attention_to_count().squeeze", "drop_language.DropLanguage.parameters.passage_count_hidden2logits", "passage_token_logits.squeeze.squeeze.squeeze", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "drop_language.clamp_distribution", "torch.mse_loss", "torch.mse_loss", "logger.info", "allennlp.move_to_device", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "utils.round_all", "semqa.domain_languages.domain_language_utils.listTokensVis", "drop_language.DropLanguage.modules_debug_info[].append", "drop_language.DropLanguage.parameters.passage_attention_to_count", "allennlp.move_to_device", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "utils.tocpuNPList", "torch.stack.unsqueeze", "torch.stack.unsqueeze", "drop_language.DropLanguage.passage_mask.unsqueeze", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.clamp_distribution", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.listTokensVis", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList"], ["", "@", "predicate", "\n", "def", "passageAttn2Count", "(", "self", ",", "passage_attention", ":", "PassageAttention", ")", "->", "CountNumber", ":", "\n", "        ", "with", "Profile", "(", "\"count-mod\"", ")", ":", "\n", "            ", "passage_attn", "=", "passage_attention", ".", "_value", "\n", "\n", "# Shape: (passage_length)", "\n", "passage_attn", "=", "passage_attn", "*", "self", ".", "passage_mask", "\n", "\n", "scaled_attentions", "=", "[", "passage_attn", "*", "sf", "for", "sf", "in", "self", ".", "parameters", ".", "passage_attention_scalingvals", "]", "\n", "# Shape: (passage_length, num_scaling_factors)", "\n", "scaled_passage_attentions", "=", "torch", ".", "stack", "(", "scaled_attentions", ",", "dim", "=", "1", ")", "\n", "\n", "# Shape: (passage_length, hidden_dim)", "\n", "count_hidden_repr", "=", "self", ".", "parameters", ".", "passage_attention_to_count", "(", "\n", "scaled_passage_attentions", ".", "unsqueeze", "(", "0", ")", ",", "self", ".", "passage_mask", ".", "unsqueeze", "(", "0", ")", "\n", ")", ".", "squeeze", "(", "0", ")", "\n", "\n", "# Shape: (passage_length, 1)", "\n", "passage_token_logits", "=", "self", ".", "parameters", ".", "passage_count_hidden2logits", "(", "count_hidden_repr", ")", "\n", "# Shape: (passage_length)", "\n", "passage_token_logits", "=", "passage_token_logits", ".", "squeeze", "(", "1", ")", "\n", "\n", "passage_token_sigmoids", "=", "torch", ".", "sigmoid", "(", "passage_token_logits", ")", "\n", "passage_token_sigmoids", "=", "passage_token_sigmoids", "*", "self", ".", "passage_mask", "\n", "\n", "count_mean", "=", "torch", ".", "sum", "(", "passage_token_sigmoids", ")", "\n", "variance", "=", "0.5", "\n", "\n", "loss", "=", "0", "\n", "loss", "+=", "passage_attention", ".", "loss", "\n", "if", "count_mean", ">", "10.0", ":", "\n", "                ", "extra_loss", "=", "F", ".", "mse_loss", "(", "count_mean", ",", "\n", "allenutil", ".", "move_to_device", "(", "torch", ".", "tensor", "(", "9.0", ")", ",", "cuda_device", "=", "self", ".", "device_id", ")", ")", "\n", "loss", "+=", "extra_loss", "\n", "logger", ".", "info", "(", "f\"CountMean: {count_mean} ExtraLoss: {extra_loss}\"", ")", "\n", "count_mean", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "tensor", "(", "9.0", ")", ",", "cuda_device", "=", "self", ".", "device_id", ")", "\n", "\n", "# Shape: (num_count_values, )", "\n", "", "l2_by_vsquared", "=", "torch", ".", "pow", "(", "self", ".", "countvals", "-", "count_mean", ",", "2", ")", "/", "(", "2", "*", "variance", "*", "variance", ")", "\n", "exp_val", "=", "torch", ".", "exp", "(", "-", "1", "*", "l2_by_vsquared", ")", "+", "1e-30", "\n", "count_distribution", "=", "exp_val", "/", "(", "torch", ".", "sum", "(", "exp_val", ")", ")", "\n", "\n", "count_distribution", "=", "clamp_distribution", "(", "count_distribution", ")", "\n", "\n", "debug_value", "=", "\"\"", "\n", "if", "self", ".", "_debug", ":", "\n", "                ", "countdist", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "count_distribution", ")", ",", "3", ")", "\n", "psigms", ",", "pattn_vis_most", "=", "dlutils", ".", "listTokensVis", "(", "passage_token_sigmoids", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", ")", "\n", "debug_value", "+=", "f\"CountDist: {countdist}\"", "\n", "debug_value", "+=", "f\"CountMean: {count_mean}\"", "\n", "debug_value", "+=", "f\"\\nPSigms: {psigms}\"", "\n", "\n", "debug_info_dict", "=", "{", "\"count\"", ":", "{", "\"count\"", ":", "count_distribution", ",", "\n", "\"passage_input\"", ":", "passage_attn", "}", "}", "\n", "self", ".", "modules_debug_info", "[", "-", "1", "]", ".", "append", "(", "debug_info_dict", ")", "\n", "\n", "", "", "return", "CountNumber", "(", "count_number_dist", "=", "count_distribution", ",", "loss", "=", "loss", ",", "debug_value", "=", "debug_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.number_add_sub_module": [[1536, 1566], ["drop_language.DropLanguage.expected_number_addsub", "utils.round_all", "semqa.domain_languages.domain_language_utils.topProbMassElems", "semqa.domain_languages.domain_language_utils.topProbMassElems", "semqa.domain_languages.domain_language_utils.topProbMassElems", "utils.tocpuNPList"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.expected_number_addsub", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.topProbMassElems", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.topProbMassElems", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.topProbMassElems", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList"], ["", "def", "number_add_sub_module", "(", "self", ",", "number_1", ":", "PassageNumber", ",", "number_2", ":", "PassageNumber", ",", "add_sub", ":", "str", ")", ":", "\n", "        ", "assert", "add_sub", "in", "[", "\"add\"", ",", "\"sub\"", "]", "\n", "numberdist_1", "=", "number_1", ".", "_value", "\n", "numberdist_2", "=", "number_2", ".", "_value", "\n", "\n", "# Shape: (size_composed_numbers, )", "\n", "composed_number_dist", "=", "self", ".", "expected_number_addsub", "(", "\n", "num_dist_1", "=", "numberdist_1", ",", "num_dist_2", "=", "numberdist_2", ",", "operation", "=", "add_sub", "\n", ")", "\n", "\n", "loss", "=", "0.0", "\n", "loss", "+=", "number_1", ".", "loss", "+", "number_2", ".", "loss", "\n", "\n", "debug_value", "=", "\"\"", "\n", "if", "self", ".", "_debug", ":", "\n", "            ", "composed_number_dist_list", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "composed_number_dist", ")", ",", "3", ")", "\n", "topknumdiff", "=", "dlutils", ".", "topProbMassElems", "(", "\n", "attention", "=", "composed_number_dist", ",", "support", "=", "self", ".", "composed_numbers", ",", "k", "=", "5", "\n", ")", "\n", "topknum1", "=", "dlutils", ".", "topProbMassElems", "(", "attention", "=", "numberdist_1", ",", "support", "=", "self", ".", "passage_num_values", ",", "k", "=", "5", ")", "\n", "topknum2", "=", "dlutils", ".", "topProbMassElems", "(", "attention", "=", "numberdist_2", ",", "support", "=", "self", ".", "passage_num_values", ",", "k", "=", "5", ")", "\n", "\n", "debug_value", "+=", "(", "\n", "f\"ComposedNumberDist: {composed_number_dist_list}\\n\"", "\n", "+", "f\"Top-num-diff: {topknumdiff}\\n\"", "\n", "+", "f\"\\n Top-Num1: {topknum1}\"", "\n", "+", "f\"\\n Top-Num2: {topknum2}\"", "\n", ")", "\n", "\n", "", "return", "composed_number_dist", ",", "loss", ",", "debug_value", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.passagenumber_difference": [[1567, 1584], ["drop_language.DropLanguage.number_add_sub_module", "drop_language.ComposedNumber", "drop_language.DropLanguage.modules_debug_info[].append"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.number_add_sub_module"], ["", "@", "predicate", "\n", "def", "passagenumber_difference", "(", "\n", "self", ",", "passage_number_1", ":", "PassageNumber", ",", "passage_number_2", ":", "PassageNumber", "\n", ")", "->", "ComposedNumber", ":", "\n", "        ", "\"\"\" Find the expected difference between two number distributions. \"\"\"", "\n", "\n", "number_difference_dist", ",", "loss", ",", "debug_value", "=", "self", ".", "number_add_sub_module", "(", "\n", "number_1", "=", "passage_number_1", ",", "number_2", "=", "passage_number_2", ",", "add_sub", "=", "\"sub\"", "\n", ")", "\n", "\n", "if", "self", ".", "_debug", ":", "\n", "            ", "debug_info_dict", "=", "{", "\"number-difference\"", ":", "{", "\"difference_value\"", ":", "number_difference_dist", ",", "\n", "\"input_number_1\"", ":", "passage_number_1", ".", "_value", ",", "\n", "\"input_number_2\"", ":", "passage_number_2", ".", "_value", "}", "}", "\n", "self", ".", "modules_debug_info", "[", "-", "1", "]", ".", "append", "(", "debug_info_dict", ")", "\n", "\n", "", "return", "ComposedNumber", "(", "composed_number_dist", "=", "number_difference_dist", ",", "loss", "=", "loss", ",", "debug_value", "=", "debug_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.passagenumber_addition": [[1585, 1601], ["drop_language.DropLanguage.number_add_sub_module", "drop_language.ComposedNumber", "drop_language.DropLanguage.modules_debug_info[].append"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.number_add_sub_module"], ["", "@", "predicate", "\n", "def", "passagenumber_addition", "(", "\n", "self", ",", "passage_number_1", ":", "PassageNumber", ",", "passage_number_2", ":", "PassageNumber", "\n", ")", "->", "ComposedNumber", ":", "\n", "        ", "\"\"\" Find the expected sum of two number distributions. \"\"\"", "\n", "number_addition_dist", ",", "loss", ",", "debug_value", "=", "self", ".", "number_add_sub_module", "(", "\n", "number_1", "=", "passage_number_1", ",", "number_2", "=", "passage_number_2", ",", "add_sub", "=", "\"add\"", "\n", ")", "\n", "\n", "if", "self", ".", "_debug", ":", "\n", "            ", "debug_info_dict", "=", "{", "\"number-addition\"", ":", "{", "\"addition_value\"", ":", "number_addition_dist", ",", "\n", "\"input_number_1\"", ":", "passage_number_1", ".", "_value", ",", "\n", "\"input_number_2\"", ":", "passage_number_2", ".", "_value", "}", "}", "\n", "self", ".", "modules_debug_info", "[", "-", "1", "]", ".", "append", "(", "debug_info_dict", ")", "\n", "\n", "", "return", "ComposedNumber", "(", "composed_number_dist", "=", "number_addition_dist", ",", "loss", "=", "loss", ",", "debug_value", "=", "debug_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.max_number_distribution": [[1657, 1663], ["num_dist.cumsum", "drop_language.clamp_distribution", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "cum_dist_n.new_zeros"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.clamp_distribution"], ["def", "max_number_distribution", "(", "self", ",", "num_dist", ":", "torch", ".", "FloatTensor", ")", ":", "\n", "        ", "cum_dist", "=", "num_dist", ".", "cumsum", "(", "0", ")", "\n", "cum_dist_n", "=", "cum_dist", "**", "self", ".", "max_samples", "\n", "maximum_distribution", "=", "cum_dist_n", "-", "torch", ".", "cat", "(", "[", "cum_dist_n", ".", "new_zeros", "(", "1", ")", ",", "cum_dist_n", "[", ":", "-", "1", "]", "]", ")", "\n", "maximum_distribution", "=", "clamp_distribution", "(", "maximum_distribution", ")", "\n", "return", "maximum_distribution", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.min_number_distribution": [[1664, 1673], ["num_dist.cumsum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "drop_language.clamp_distribution", "inverse_cumdist_n.new_zeros"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.clamp_distribution"], ["", "def", "min_number_distribution", "(", "self", ",", "num_dist", ":", "torch", ".", "FloatTensor", ")", ":", "\n", "        ", "cumulative_distribution_function", "=", "num_dist", ".", "cumsum", "(", "0", ")", "\n", "# P(x>=i) = 1 - (P(x<=i) - P(x=i))", "\n", "inverse_cumdist", "=", "1", "-", "cumulative_distribution_function", "+", "num_dist", "\n", "inverse_cumdist_n", "=", "inverse_cumdist", "**", "self", ".", "max_samples", "\n", "inverse_cumdist_n_shifted", "=", "torch", ".", "cat", "(", "[", "inverse_cumdist_n", "[", "1", ":", "]", ",", "inverse_cumdist_n", ".", "new_zeros", "(", "1", ")", "]", ")", "\n", "minimum_distribution", "=", "inverse_cumdist_n", "-", "inverse_cumdist_n_shifted", "\n", "minimum_distribution", "=", "clamp_distribution", "(", "minimum_distribution", ")", "\n", "return", "minimum_distribution", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.find_PassageNumber": [[1674, 1721], ["allennlp_semparse.domain_languages.predicate_with_side_args", "drop_language.DropLanguage.compute_num_distribution", "drop_language.PassageNumber", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "utils.round_all", "semqa.domain_languages.domain_language_utils.topProbMassElems", "semqa.domain_languages.domain_language_utils.mostAttendedSpans", "drop_language.DropLanguage.modules_debug_info[].append", "torch.log", "torch.log", "torch.log", "torch.log", "utils.tocpuNPList", "utils.round_all", "utils.tocpuNPList", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.compute_num_distribution", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.topProbMassElems", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.mostAttendedSpans", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "@", "predicate_with_side_args", "(", "[", "\"event_num_groundings\"", "]", ")", "\n", "def", "find_PassageNumber", "(", "self", ",", "passage_attention", ":", "PassageAttention", ",", "event_num_groundings", "=", "None", ")", "->", "PassageNumber", ":", "\n", "# This comes as a list of groundings; even though for this action there's just one", "\n", "# This can be completely-zero indicating masked. In that case, don't compute loss", "\n", "        ", "if", "event_num_groundings", "is", "not", "None", ":", "\n", "            ", "number_grounding_supervision", "=", "event_num_groundings", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "number_grounding_supervision", "=", "None", "\n", "\n", "", "passage_attn", "=", "passage_attention", ".", "_value", "\n", "# Shape: (passage_length)", "\n", "passage_attn", "=", "passage_attn", "*", "self", ".", "passage_mask", "\n", "\n", "number_distribution", ",", "passage_numtoken_probs", ",", "num_dist_entropy", "=", "self", ".", "compute_num_distribution", "(", "\n", "passage_attention", "=", "passage_attn", ")", "\n", "\n", "loss", "=", "0.0", "\n", "grounding_loss", "=", "0.0", "\n", "if", "number_grounding_supervision", "is", "not", "None", ":", "\n", "            ", "grounding_mask", "=", "(", "torch", ".", "sum", "(", "number_grounding_supervision", ")", ">", "0", ")", ".", "float", "(", ")", "\n", "log_probs", "=", "torch", ".", "log", "(", "number_distribution", "+", "1e-40", ")", "*", "number_grounding_supervision", "\n", "log_likelihood", "=", "torch", ".", "sum", "(", "log_probs", ")", "# Want all grounded numbers to be high, hence prod of probs", "\n", "grounding_loss", "=", "-", "1", "*", "grounding_mask", "*", "log_likelihood", "\n", "\n", "", "loss", "+=", "grounding_loss", "\n", "loss", "+=", "passage_attention", ".", "loss", "\n", "\n", "debug_value", "=", "\"\"", "\n", "if", "self", ".", "_debug", ":", "\n", "            ", "number_dist", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "number_distribution", ")", ",", "3", ")", "\n", "topk_numdist", "=", "dlutils", ".", "topProbMassElems", "(", "attention", "=", "number_distribution", ",", "support", "=", "self", ".", "passage_num_values", ",", "k", "=", "5", ")", "\n", "if", "number_grounding_supervision", "is", "not", "None", ":", "\n", "                ", "num_grounding_sup", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "number_grounding_supervision", ")", ",", "3", ")", "\n", "", "else", ":", "\n", "                ", "num_grounding_sup", "=", "None", "\n", "# _, pattn_vis_most = dlutils.listTokensVis(passage_attn, self.metadata[\"passage_tokens\"])", "\n", "", "top_spans", "=", "dlutils", ".", "mostAttendedSpans", "(", "passage_attn", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", ")", "\n", "debug_value", "+=", "f\"PassageNumber: {number_dist}\"", "\n", "debug_value", "+=", "f\"\\ntopk-num-dist: {topk_numdist}\"", "\n", "debug_value", "+=", "f\"\\nGoldNum: {num_grounding_sup}\"", "\n", "debug_value", "+=", "f\"\\ninput-pattn-top-spans: {top_spans}\"", "\n", "\n", "debug_info_dict", "=", "{", "\"find-num\"", ":", "{", "\"number\"", ":", "number_distribution", ",", "\n", "\"passage_input\"", ":", "passage_attn", ",", "\n", "\"passage_number\"", ":", "passage_numtoken_probs", "}", "}", "\n", "self", ".", "modules_debug_info", "[", "-", "1", "]", ".", "append", "(", "debug_info_dict", ")", "\n", "", "return", "PassageNumber", "(", "passage_number_dist", "=", "number_distribution", ",", "loss", "=", "loss", ",", "debug_value", "=", "debug_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.ground_ImplicitNumber": [[1722, 1741], ["allennlp_semparse.domain_languages.predicate_with_side_args", "drop_language.DropLanguage.compute_implicitnum_distribution", "drop_language.PassageNumber", "utils.round_all", "semqa.domain_languages.domain_language_utils.topProbMassElems", "semqa.domain_languages.domain_language_utils.listTokensVis", "drop_language.DropLanguage.modules_debug_info[].append", "utils.tocpuNPList"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.compute_implicitnum_distribution", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.topProbMassElems", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.listTokensVis", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList"], ["", "@", "predicate_with_side_args", "(", "[", "\"question_attention\"", "]", ")", "\n", "def", "ground_ImplicitNumber", "(", "self", ",", "question_attention", ":", "Tensor", ")", "->", "PassageNumber", ":", "\n", "        ", "number_distribution", "=", "self", ".", "compute_implicitnum_distribution", "(", "question_attention", "=", "question_attention", ")", "\n", "loss", "=", "0.0", "\n", "debug_value", "=", "\"\"", "\n", "if", "self", ".", "_debug", ":", "\n", "            ", "number_dist", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "number_distribution", ")", ",", "3", ")", "\n", "topk_numdist", "=", "dlutils", ".", "topProbMassElems", "(", "attention", "=", "number_distribution", ",", "support", "=", "self", ".", "passage_num_values", ",", "k", "=", "5", ")", "\n", "debug_value", "+=", "f\"PassageNumber: {number_dist}\"", "\n", "debug_value", "+=", "f\"\\ntopk-num-dist: {topk_numdist}\"", "\n", "qattn_vis_complete", ",", "qattn_vis_most", "=", "dlutils", ".", "listTokensVis", "(", "\n", "question_attention", ",", "self", ".", "metadata", "[", "\"question_tokens\"", "]", "\n", ")", "\n", "debug_value", "+=", "f\"input-qattn: {qattn_vis_complete}\"", "\n", "\n", "debug_info_dict", "=", "{", "\"find-implicit-num\"", ":", "{", "\"number\"", ":", "number_distribution", ",", "\n", "\"question\"", ":", "question_attention", "}", "}", "\n", "self", ".", "modules_debug_info", "[", "-", "1", "]", ".", "append", "(", "debug_info_dict", ")", "\n", "", "return", "PassageNumber", "(", "passage_number_dist", "=", "number_distribution", ",", "loss", "=", "loss", ",", "debug_value", "=", "debug_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.minNumPattn": [[1742, 1755], ["allennlp_semparse.domain_languages.predicate_with_side_args", "drop_language.DropLanguage.minmaxNumPattn_module", "drop_language.PassageAttention", "drop_language.DropLanguage.modules_debug_info[].append"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.minmaxNumPattn_module"], ["", "@", "predicate_with_side_args", "(", "[", "\"event_num_groundings\"", "]", ")", "\n", "def", "minNumPattn", "(", "self", ",", "passage_attention", ":", "PassageAttention", ",", "event_num_groundings", "=", "None", ")", "->", "PassageAttention", ":", "\n", "        ", "(", "minmax_num_pattn", ",", "inputpattn_num_dist", ",", "inputpattn_numtoken_probs", ",", "minmax_numtoken_probs", ",", "\n", "loss", ",", "debug_value", ")", "=", "self", ".", "minmaxNumPattn_module", "(", "\n", "passage_attention", "=", "passage_attention", ",", "min_max_op", "=", "\"min\"", ",", "event_num_groundings", "=", "event_num_groundings", ")", "\n", "if", "self", ".", "_debug", ":", "\n", "            ", "debug_info_dict", "=", "{", "\"find-min-num\"", ":", "{", "\"passage\"", ":", "minmax_num_pattn", ",", "\n", "\"passage_input\"", ":", "passage_attention", ".", "_value", ",", "\n", "\"number_input\"", ":", "inputpattn_num_dist", ",", "\n", "\"passage_input_number\"", ":", "inputpattn_numtoken_probs", ",", "\n", "\"passage_minmax_number\"", ":", "minmax_numtoken_probs", "}", "}", "\n", "self", ".", "modules_debug_info", "[", "-", "1", "]", ".", "append", "(", "debug_info_dict", ")", "\n", "", "return", "PassageAttention", "(", "passage_attention", "=", "minmax_num_pattn", ",", "loss", "=", "loss", ",", "debug_value", "=", "debug_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.maxNumPattn": [[1756, 1769], ["allennlp_semparse.domain_languages.predicate_with_side_args", "drop_language.DropLanguage.minmaxNumPattn_module", "drop_language.PassageAttention", "drop_language.DropLanguage.modules_debug_info[].append"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.minmaxNumPattn_module"], ["", "@", "predicate_with_side_args", "(", "[", "\"event_num_groundings\"", "]", ")", "\n", "def", "maxNumPattn", "(", "self", ",", "passage_attention", ":", "PassageAttention", ",", "event_num_groundings", "=", "None", ")", "->", "PassageAttention", ":", "\n", "        ", "(", "minmax_num_pattn", ",", "inputpattn_num_dist", ",", "inputpattn_numtoken_probs", ",", "minmax_numtoken_probs", ",", "\n", "loss", ",", "debug_value", ")", "=", "self", ".", "minmaxNumPattn_module", "(", "\n", "passage_attention", "=", "passage_attention", ",", "min_max_op", "=", "\"max\"", ",", "event_num_groundings", "=", "event_num_groundings", ")", "\n", "if", "self", ".", "_debug", ":", "\n", "            ", "debug_info_dict", "=", "{", "\"find-max-num\"", ":", "{", "\"passage\"", ":", "minmax_num_pattn", ",", "\n", "\"passage_input\"", ":", "passage_attention", ".", "_value", ",", "\n", "\"number_input\"", ":", "inputpattn_num_dist", ",", "\n", "\"passage_input_number\"", ":", "inputpattn_numtoken_probs", ",", "\n", "\"passage_minmax_number\"", ":", "minmax_numtoken_probs", "}", "}", "\n", "self", ".", "modules_debug_info", "[", "-", "1", "]", ".", "append", "(", "debug_info_dict", ")", "\n", "", "return", "PassageAttention", "(", "passage_attention", "=", "minmax_num_pattn", ",", "loss", "=", "loss", ",", "debug_value", "=", "debug_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.minmaxNumPattn_module": [[1770, 1821], ["drop_language.DropLanguage.compute_num_distribution", "drop_language.DropLanguage.pattn_for_minmaxNum", "drop_language.DropLanguage.compute_num_distribution", "utils.round_all", "utils.round_all", "semqa.domain_languages.domain_language_utils.topProbMassElems", "semqa.domain_languages.domain_language_utils.topProbMassElems", "semqa.domain_languages.domain_language_utils.listTokensVis", "semqa.domain_languages.domain_language_utils.mostAttendedSpans", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "utils.tocpuNPList", "utils.tocpuNPList", "utils.round_all", "torch.log", "torch.log", "torch.log", "torch.log", "utils.tocpuNPList", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.compute_num_distribution", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.pattn_for_minmaxNum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.compute_num_distribution", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.topProbMassElems", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.topProbMassElems", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.listTokensVis", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.mostAttendedSpans", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "def", "minmaxNumPattn_module", "(", "self", ",", "passage_attention", ":", "PassageAttention", ",", "min_max_op", ":", "str", ",", "event_num_groundings", "=", "None", ")", ":", "\n", "        ", "assert", "min_max_op", "in", "[", "\"min\"", ",", "\"max\"", "]", "\n", "if", "event_num_groundings", "is", "not", "None", ":", "\n", "            ", "num_grounding_supervision", "=", "event_num_groundings", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "num_grounding_supervision", "=", "None", "\n", "\n", "", "pattn", "=", "passage_attention", ".", "_value", "*", "self", ".", "passage_mask", "\n", "# Computing for debugging and aux loss purposes", "\n", "inputpattn_num_dist", ",", "inputpnum_token_prob", ",", "_", "=", "self", ".", "compute_num_distribution", "(", "pattn", ")", "\n", "(", "minmax_num_pattn", ",", "input_numtoken_probs_only", ",", "minmax_numtoken_probs_only", ",", "\n", "minmaxnum_token_prob", ")", "=", "self", ".", "pattn_for_minmaxNum", "(", "pattn", "=", "pattn", ",", "max_min", "=", "min_max_op", ")", "\n", "\n", "loss", "=", "0.0", "\n", "if", "num_grounding_supervision", "is", "not", "None", ":", "\n", "            ", "grounding_mask", "=", "(", "torch", ".", "sum", "(", "num_grounding_supervision", ")", ">", "0", ")", ".", "float", "(", ")", "\n", "if", "grounding_mask", ">", "0", ":", "\n", "# Number distribution for input pattn", "\n", "                ", "log_probs", "=", "torch", ".", "log", "(", "inputpattn_num_dist", "+", "1e-40", ")", "*", "num_grounding_supervision", "\n", "log_likelihood", "=", "torch", ".", "sum", "(", "log_probs", ")", "# Want all grounded numbers to be high, hence prod of probs", "\n", "grounding_loss", "=", "-", "1", "*", "grounding_mask", "*", "log_likelihood", "\n", "loss", "+=", "grounding_loss", "\n", "", "", "loss", "+=", "passage_attention", ".", "loss", "\n", "\n", "minmax_num_distribution", "=", "None", "\n", "debug_value", "=", "\"\"", "\n", "inputpattn_numtoken_probs", "=", "None", "\n", "minmax_numtoken_probs", "=", "None", "\n", "if", "self", ".", "_debug", ":", "\n", "            ", "minmax_num_dist", ",", "_", ",", "_", "=", "self", ".", "compute_num_distribution", "(", "minmax_num_pattn", ")", "\n", "inputpattn_numtoken_probs", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "inputpnum_token_prob", ")", ",", "3", ")", "\n", "minmax_numtoken_probs", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "minmaxnum_token_prob", ")", ",", "3", ")", "\n", "# minmax_token_probs = myutils.round_all(myutils.tocpuNPList(minmax_numtoken_probs), 3)", "\n", "\n", "input_topknums", "=", "dlutils", ".", "topProbMassElems", "(", "attention", "=", "inputpattn_num_dist", ",", "support", "=", "self", ".", "passage_num_values", ")", "\n", "topknums", "=", "dlutils", ".", "topProbMassElems", "(", "attention", "=", "minmax_num_dist", ",", "support", "=", "self", ".", "passage_num_values", ")", "\n", "if", "num_grounding_supervision", "is", "not", "None", ":", "\n", "                ", "num_grounding_sup", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "num_grounding_supervision", ")", ",", "3", ")", "\n", "", "else", ":", "\n", "                ", "num_grounding_sup", "=", "None", "\n", "", "min_pattn_comp", ",", "_", "=", "dlutils", ".", "listTokensVis", "(", "minmax_num_pattn", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", ")", "\n", "topspans", "=", "dlutils", ".", "mostAttendedSpans", "(", "minmax_num_pattn", ",", "self", ".", "metadata", "[", "\"passage_tokens\"", "]", ")", "\n", "debug_value", "+=", "f\"{min_max_op}-pattn-module\"", "\n", "debug_value", "+=", "f\"\\noutput-{min_max_op}-num-pattn-most-attended-spans: {topspans}\"", "\n", "debug_value", "+=", "f\"\\ninput-numtoken-probs: {inputpattn_numtoken_probs}\"", "\n", "debug_value", "+=", "f\"\\ninput-num-dist: {input_topknums}\"", "\n", "# debug_value += f\"\\nminmax-numtoken-probs: {minmax_token_probs}\"", "\n", "debug_value", "+=", "f\"\\ntopk-input-num-dist: {topknums}\"", "\n", "debug_value", "+=", "f\"\\nGoldNum: {num_grounding_sup}\"", "\n", "", "return", "(", "minmax_num_pattn", ",", "inputpattn_num_dist", ",", "inputpattn_numtoken_probs", ",", "minmax_numtoken_probs", ",", "\n", "loss", ",", "debug_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.pattn_for_minmaxNum": [[1822, 1890], ["semqa.profiler.profile.Profile", "pattn_weighted_numbertoken_probs.sum", "pattn_weighted_numbertoken_probs.sum", "drop_language.clamp_distribution", "pattn.unsqueeze", "drop_language.DropLanguage.max_number_distribution", "drop_language.DropLanguage.unsqueeze", "allennlp.move_to_device", "pattn.new_zeros", "pattn.new_zeros.scatter_add_", "drop_language.DropLanguage.min_number_distribution", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.clamp_distribution", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.max_number_distribution", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.DropLanguage.min_number_distribution"], ["", "def", "pattn_for_minmaxNum", "(", "\n", "self", ",", "pattn", ":", "torch", ".", "FloatTensor", ",", "max_min", ":", "str", "\n", ")", "->", "Tuple", "[", "torch", ".", "FloatTensor", ",", "torch", ".", "FloatTensor", ",", "torch", ".", "FloatTensor", "]", ":", "\n", "        ", "\"\"\" Re-distribute the passage-attention based on a max number-token distribution\n            The idea here is to find a `max-number-token` passage attention, i.e. given a distribution over\n            passage-tokens, and for each token a distribution over numbers, find the passage-attention that contains,\n            for each token, the probability that the number-associated with that token is the max.\n\n            The output passage-distribution can be used compute an expected-number-distribution, which is the\n            expected value under the distribution over max-events. For eg, \"How many yards was the longest TD\"\n            Given the passage-attention for \"TD\", return the computed attention for P(\"longest TD\").\n\n            This is a little tricky to compute; the high level steps are:\n            1. Compute an expected token-number-distribution. (this is a masked passage-length attention)\n            2. Arrange the token-number-probs in an ascending order for the max\n            3. Compute the max-distribution and re-arrange the numbers in order\n            4. Re-map the token-number probs into a masked vector.\n            5. ....\n\n            If we know the number-grounding-supervision for the input passage-attention events, we can also compute\n            an auxiliary loss here.\n        \"\"\"", "\n", "with", "Profile", "(", "\"pattn-minmax\"", ")", ":", "\n", "# Shape: (passage_length, passage_length) -- each row is a number-token-distribution", "\n", "            ", "pattn_times_numbertokenprobs", "=", "self", ".", "passage_passage_token2num_alignment", "*", "pattn", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# Shape: (passage_length, num_of_number_tokens) -- For each token, a number-token distribution, where", "\n", "# 1. The underlying numbers are sorted in increasing order", "\n", "# 2. The probabiltiy is weighed by the token-prob in the input pattn", "\n", "pattn_weighted_numbertoken_probs", "=", "pattn_times_numbertokenprobs", "[", ":", ",", "self", ".", "passage_number_sortedtokenidxs", "]", "\n", "\n", "# Shape: (num_of_number_tokens, ) -- the probability of the number tokens in sorted order", "\n", "only_expected_numbertoken_probs", "=", "pattn_weighted_numbertoken_probs", ".", "sum", "(", "0", ")", "\n", "if", "max_min", "==", "\"max\"", ":", "\n", "                ", "only_numbertoken_minmaxprobs_sorted", "=", "self", ".", "max_number_distribution", "(", "only_expected_numbertoken_probs", ")", "\n", "", "elif", "max_min", "==", "\"min\"", ":", "\n", "                ", "only_numbertoken_minmaxprobs_sorted", "=", "self", ".", "min_number_distribution", "(", "only_expected_numbertoken_probs", ")", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "\n", "\n", "# For each (token i, number j), using pattn_weighted_numbertoken_probs[i, j] as the weight,", "\n", "# Total weight for numbertoken as pattn_weighted_numbertoken_probs.sum(0), redistribute the minmax-number-prob", "\n", "# to all tokens", "\n", "# Shape: (1, num_of_number_tokens)", "\n", "", "total_weight_to_numbertoken", "=", "pattn_weighted_numbertoken_probs", ".", "sum", "(", "0", ",", "keepdim", "=", "True", ")", "\n", "# Shape: (passage_length, num_of_number_tokens) - each entry here is (pattn * numberprob * number-max-prob)", "\n", "maxprob_times_pattn_numbertokenprob", "=", "(", "\n", "pattn_weighted_numbertoken_probs", "*", "only_numbertoken_minmaxprobs_sorted", ".", "unsqueeze", "(", "0", ")", "\n", ")", "\n", "# Divide each entry above by \\sum_tokens pattn * numberprob --", "\n", "# This is the new-distributed weight of number-max-prob on the i-th token, j-th number.", "\n", "# Now marginalize over numbers, to get the new-passage-attention", "\n", "new_pattn", "=", "(", "maxprob_times_pattn_numbertokenprob", "/", "total_weight_to_numbertoken", ")", ".", "sum", "(", "1", ")", "\n", "\n", "new_pattn", "=", "clamp_distribution", "(", "new_pattn", ")", "\n", "\n", "minmax_number_token_distribution", "=", "None", "\n", "if", "self", ".", "_debug", ":", "\n", "# These are the tokens ids for numbers s.t. numbers are sorted in increasing order", "\n", "                ", "number_tokenidxs_sorted", "=", "allenutil", ".", "move_to_device", "(", "\n", "torch", ".", "LongTensor", "(", "self", ".", "passage_number_sortedtokenidxs", ")", ",", "cuda_device", "=", "self", ".", "device_id", ")", "\n", "# Scattering min-max number distribution back to tokens", "\n", "minmax_number_token_distribution", "=", "pattn", ".", "new_zeros", "(", "self", ".", "passage_length", ")", "\n", "minmax_number_token_distribution", ".", "scatter_add_", "(", "0", ",", "number_tokenidxs_sorted", ",", "\n", "only_numbertoken_minmaxprobs_sorted", ")", "\n", "\n", "", "", "return", "(", "new_pattn", ",", "only_expected_numbertoken_probs", ",", "only_numbertoken_minmaxprobs_sorted", ",", "\n", "minmax_number_token_distribution", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.clamp_distribution": [[200, 202], ["torch.clamp", "torch.clamp"], "function", ["None"], ["", "def", "clamp_distribution", "(", "distribution", ")", ":", "\n", "    ", "return", "torch", ".", "clamp", "(", "distribution", ",", "min", "=", "1e-20", ",", "max", "=", "1", "-", "1e-20", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.get_empty_language_object": [[204, 237], ["drop_language.DropLanguage"], "function", ["None"], ["", "def", "get_empty_language_object", "(", ")", ":", "\n", "    ", "droplanguage", "=", "DropLanguage", "(", "\n", "rawemb_question", "=", "None", ",", "\n", "embedded_question", "=", "None", ",", "\n", "encoded_question", "=", "None", ",", "\n", "rawemb_passage", "=", "None", ",", "\n", "embedded_passage", "=", "None", ",", "\n", "encoded_passage", "=", "None", ",", "\n", "modeled_passage", "=", "None", ",", "\n", "question_mask", "=", "None", ",", "\n", "passage_mask", "=", "None", ",", "\n", "passage_sentence_boundaries", "=", "None", ",", "\n", "passage_tokenidx2dateidx", "=", "None", ",", "\n", "passage_date_values", "=", "None", ",", "\n", "passage_tokenidx2numidx", "=", "None", ",", "\n", "passage_num_values", "=", "None", ",", "\n", "composed_numbers", "=", "None", ",", "\n", "passage_number_sortedtokenidxs", "=", "None", ",", "\n", "add_num_combination_indices", "=", "None", ",", "\n", "sub_num_combination_indices", "=", "None", ",", "\n", "year_differences", "=", "None", ",", "\n", "year_differences_mat", "=", "None", ",", "\n", "count_num_values", "=", "None", ",", "\n", "question_passage_attention", "=", "None", ",", "\n", "passage_question_attention", "=", "None", ",", "\n", "passage_token2date_alignment", "=", "None", ",", "\n", "passage_token2startdate_alignment", "=", "None", ",", "\n", "passage_token2enddate_alignment", "=", "None", ",", "\n", "passage_token2num_alignment", "=", "None", ",", "\n", "parameters", "=", "None", ",", "\n", "start_types", "=", "None", ",", "\n", ")", "\n", "return", "droplanguage", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.execute_action_sequence": [[14, 41], ["first_action.split", "domain_language_utils._execute_sequence", "allennlp_semparse.common.ExecutionError"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils._execute_sequence"], ["def", "execute_action_sequence", "(", "language", ",", "action_sequence", ":", "List", "[", "str", "]", ",", "side_arguments", ":", "List", "[", "Dict", "]", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Executes the program defined by an action sequence directly, without needing the overhead\n    of translating to a logical form first.  For any given program, :func:`execute` and this\n    function are equivalent, they just take different representations of the program, so you\n    can use whichever is more efficient.\n\n    Also, if you have state or side arguments associated with particular production rules\n    (e.g., the decoder's attention on an input utterance when a predicate was predicted), you\n    `must` use this function to execute the logical form, instead of :func:`execute`, so that\n    we can match the side arguments with the right functions.\n    \"\"\"", "\n", "\n", "# We'll strip off the first action, because it doesn't matter for execution.", "\n", "first_action", "=", "action_sequence", "[", "0", "]", "\n", "left_side", ",", "right_side", "=", "first_action", ".", "split", "(", "\" -> \"", ")", "\n", "if", "left_side", "!=", "\"@start@\"", ":", "\n", "        ", "raise", "ExecutionError", "(", "\"invalid action sequence\"", ")", "\n", "", "remaining_side_args", "=", "side_arguments", "[", "1", ":", "]", "if", "side_arguments", "else", "None", "\n", "\n", "execution_vals", "=", "[", "]", "\n", "\n", "execution_value", ",", "_", ",", "_", ",", "execution_vals", "=", "_execute_sequence", "(", "\n", "language", ",", "action_sequence", "[", "1", ":", "]", ",", "remaining_side_args", ",", "execution_vals", "\n", ")", "\n", "\n", "return", "execution_value", ",", "execution_vals", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils._execute_sequence": [[43, 124], ["first_action.split", "isinstance", "right_side.split", "domain_language_utils._execute_sequence", "function", "args_exval_list.insert", "execution_vals.insert", "domain_language_utils._execute_sequence", "arguments.append", "args_exval_list.append", "inspect.signature", "function", "execution_vals.append", "non_kwargs.append", "function", "function", "execution_vals.append"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils._execute_sequence", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils._execute_sequence"], ["", "def", "_execute_sequence", "(", "\n", "language", ",", "action_sequence", ":", "List", "[", "str", "]", ",", "side_arguments", ":", "List", "[", "Dict", "]", ",", "execution_vals", ":", "List", "[", "Any", "]", "\n", ")", "->", "Tuple", "[", "Any", ",", "List", "[", "str", "]", ",", "List", "[", "Dict", "]", ",", "List", "[", "Any", "]", "]", ":", "\n", "    ", "\"\"\"\n    This does the bulk of the work of :func:`execute_action_sequence`, recursively executing\n    the functions it finds and trimming actions off of the action sequence.  The return value\n    is a tuple of (execution, remaining_actions), where the second value is necessary to handle\n    the recursion.\n    \"\"\"", "\n", "first_action", "=", "action_sequence", "[", "0", "]", "\n", "remaining_actions", "=", "action_sequence", "[", "1", ":", "]", "\n", "remaining_side_args", "=", "side_arguments", "[", "1", ":", "]", "if", "side_arguments", "else", "None", "\n", "left_side", ",", "right_side", "=", "first_action", ".", "split", "(", "\" -> \"", ")", "\n", "if", "right_side", "in", "language", ".", "_functions", ":", "\n", "        ", "function", "=", "language", ".", "_functions", "[", "right_side", "]", "\n", "# mypy doesn't like this check, saying that Callable isn't a reasonable thing to pass", "\n", "# here.  But it works just fine; I'm not sure why mypy complains about it.", "\n", "if", "isinstance", "(", "function", ",", "Callable", ")", ":", "# type: ignore", "\n", "            ", "function_arguments", "=", "inspect", ".", "signature", "(", "function", ")", ".", "parameters", "\n", "if", "not", "function_arguments", ":", "\n", "# This was a zero-argument function / constant that was registered as a lambda", "\n", "# function, for consistency of execution in `execute()`.", "\n", "                ", "execution_value", "=", "function", "(", ")", "\n", "execution_vals", ".", "append", "(", "[", "(", "function", ".", "__name__", ",", "execution_value", ".", "debug_value", ")", "]", ")", "\n", "", "elif", "side_arguments", ":", "\n", "                ", "kwargs", "=", "{", "}", "\n", "non_kwargs", "=", "[", "]", "\n", "for", "argument_name", "in", "function_arguments", ":", "\n", "                    ", "if", "argument_name", "in", "side_arguments", "[", "0", "]", ":", "\n", "                        ", "kwargs", "[", "argument_name", "]", "=", "side_arguments", "[", "0", "]", "[", "argument_name", "]", "\n", "", "else", ":", "\n", "                        ", "non_kwargs", ".", "append", "(", "argument_name", ")", "\n", "", "", "if", "kwargs", "and", "non_kwargs", ":", "\n", "# This is a function that has both side arguments and logical form", "\n", "# arguments - we curry the function so only the logical form arguments are", "\n", "# left.", "\n", "                    ", "def", "curried_function", "(", "*", "args", ")", ":", "\n", "                        ", "return", "function", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "", "execution_value", "=", "curried_function", "\n", "", "elif", "kwargs", ":", "\n", "# This is a function that _only_ has side arguments - we just call the", "\n", "# function and return a value.", "\n", "                    ", "execution_value", "=", "function", "(", "**", "kwargs", ")", "\n", "execution_vals", ".", "append", "(", "[", "(", "function", ".", "__name__", ",", "execution_value", ".", "debug_value", ")", "]", ")", "\n", "", "else", ":", "\n", "# This is a function that has logical form arguments, but no side arguments", "\n", "# that match what we were given - just return the function itself.", "\n", "                    ", "execution_value", "=", "function", "\n", "", "", "else", ":", "\n", "                ", "execution_value", "=", "function", "\n", "", "", "return", "execution_value", ",", "remaining_actions", ",", "remaining_side_args", ",", "execution_vals", "\n", "", "else", ":", "\n", "# This is a non-terminal expansion, like 'int -> [<int:int>, int, int]'.  We need to", "\n", "# get the function and its arguments, then call the function with its arguments.", "\n", "# Because we linearize the abstract syntax tree depth first, left-to-right, we can just", "\n", "# recursively call `_execute_sequence` for the function and all of its arguments, and", "\n", "# things will just work.", "\n", "        ", "right_side_parts", "=", "right_side", ".", "split", "(", "\", \"", ")", "\n", "\n", "# We don't really need to know what the types are, just how many of them there are, so", "\n", "# we recurse the right number of times.", "\n", "function", ",", "remaining_actions", ",", "remaining_side_args", ",", "execution_vals", "=", "_execute_sequence", "(", "\n", "language", ",", "remaining_actions", ",", "remaining_side_args", ",", "execution_vals", "\n", ")", "\n", "\n", "args_exval_list", "=", "[", "]", "\n", "arguments", "=", "[", "]", "\n", "for", "_", "in", "right_side_parts", "[", "1", ":", "]", ":", "\n", "            ", "argument", ",", "remaining_actions", ",", "remaining_side_args", ",", "args_exval_list_i", "=", "_execute_sequence", "(", "\n", "language", ",", "remaining_actions", ",", "remaining_side_args", ",", "[", "]", "\n", ")", "\n", "arguments", ".", "append", "(", "argument", ")", "\n", "args_exval_list", ".", "append", "(", "args_exval_list_i", "[", "0", "]", ")", "\n", "\n", "", "execution_value", "=", "function", "(", "*", "arguments", ")", "\n", "args_exval_list", ".", "insert", "(", "0", ",", "(", "function", ".", "__name__", ",", "execution_value", ".", "debug_value", ")", ")", "\n", "\n", "execution_vals", ".", "insert", "(", "0", ",", "args_exval_list", ")", "\n", "\n", "return", "execution_value", ",", "remaining_actions", ",", "remaining_side_args", ",", "execution_vals", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.masking_blockdiagonal": [[126, 155], ["allennlp.move_to_device", "allennlp.move_to_device", "allenutil.move_to_device.unsqueeze", "allenutil.move_to_device.unsqueeze", "allennlp.get_range_vector().unsqueeze", "allennlp.get_range_vector().unsqueeze", "max", "min", "torch.LongTensor", "torch.LongTensor", "range", "range", "allennlp.get_range_vector", "allennlp.get_range_vector"], "function", ["None"], ["", "", "def", "masking_blockdiagonal", "(", "passage_length", ",", "window", ",", "device_id", ")", ":", "\n", "    ", "\"\"\" Make a (passage_length, passage_length) tensor M of 1 and -1 in which for each row x,\n        M[:, x, y] = -1 if y < x - window or y > x + window, else it is 1.\n        Basically for the x-th row, the [x-win, x+win] columns should be 1, and rest -1\n    \"\"\"", "\n", "\n", "lower_limit", "=", "[", "max", "(", "0", ",", "i", "-", "window", ")", "for", "i", "in", "range", "(", "passage_length", ")", "]", "\n", "upper_limit", "=", "[", "min", "(", "passage_length", ",", "i", "+", "window", ")", "for", "i", "in", "range", "(", "passage_length", ")", "]", "\n", "\n", "# Tensors of lower and upper limits for each row", "\n", "lower", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "LongTensor", "(", "lower_limit", ")", ",", "cuda_device", "=", "device_id", ")", "\n", "upper", "=", "allenutil", ".", "move_to_device", "(", "torch", ".", "LongTensor", "(", "upper_limit", ")", ",", "cuda_device", "=", "device_id", ")", "\n", "lower_un", "=", "lower", ".", "unsqueeze", "(", "1", ")", "\n", "upper_un", "=", "upper", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# Range vector for each row", "\n", "lower_range_vector", "=", "allenutil", ".", "get_range_vector", "(", "passage_length", ",", "device", "=", "device_id", ")", ".", "unsqueeze", "(", "0", ")", "\n", "upper_range_vector", "=", "allenutil", ".", "get_range_vector", "(", "passage_length", ",", "device", "=", "device_id", ")", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "# Masks for lower and upper limits of the mask", "\n", "lower_mask", "=", "lower_range_vector", ">=", "lower_un", "\n", "upper_mask", "=", "upper_range_vector", "<=", "upper_un", "\n", "\n", "# Final-mask that we require", "\n", "# Shape: (passage_length, passage_length); (passage_length, passage_length)", "\n", "inwindow_mask", "=", "(", "lower_mask", "==", "upper_mask", ")", ".", "float", "(", ")", "\n", "outwindow_mask", "=", "(", "lower_mask", "!=", "upper_mask", ")", ".", "float", "(", ")", "\n", "\n", "return", "inwindow_mask", ",", "outwindow_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.aux_window_loss": [[157, 184], ["inwindow_probs.sum", "allennlp.replace_masked_values", "torch.sum", "passage_mask.unsqueeze", "passage_mask.unsqueeze", "torch.log", "torch.sum", "inwindow_mask.sum"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "def", "aux_window_loss", "(", "ptop_attention", ",", "passage_mask", ",", "inwindow_mask", ")", ":", "\n", "    ", "\"\"\"Auxiliary loss to encourage p-to-p attention to be within a certain window.\n\n    Args:\n        ptop_attention: (passage_length, passage_length)\n        passage_mask: (passage_length)\n        inwindow_mask: (passage_length, passage_length)\n\n    Returns:\n        inwindow_aux_loss: ()\n    \"\"\"", "\n", "inwindow_mask", "=", "inwindow_mask", "*", "passage_mask", ".", "unsqueeze", "(", "0", ")", "\n", "inwindow_mask", "=", "inwindow_mask", "*", "passage_mask", ".", "unsqueeze", "(", "1", ")", "\n", "inwindow_probs", "=", "ptop_attention", "*", "inwindow_mask", "\n", "# Sum inwindow_probs for each token, signifying the token can distribute its alignment prob in any way", "\n", "# Shape: (passage_length)", "\n", "sum_inwindow_probs", "=", "inwindow_probs", ".", "sum", "(", "1", ")", "\n", "# Shape: (passage_length) -- mask for tokens that have empty windows", "\n", "mask_sum", "=", "(", "inwindow_mask", ".", "sum", "(", "1", ")", ">", "0", ")", ".", "float", "(", ")", "\n", "masked_sum_inwindow_probs", "=", "allenutil", ".", "replace_masked_values", "(", "sum_inwindow_probs", ",", "mask_sum", ",", "replace_with", "=", "1e-40", ")", "\n", "log_sum_inwindow_probs", "=", "torch", ".", "log", "(", "masked_sum_inwindow_probs", "+", "1e-40", ")", "*", "mask_sum", "\n", "inwindow_likelihood", "=", "torch", ".", "sum", "(", "log_sum_inwindow_probs", ")", "\n", "inwindow_likelihood_avg", "=", "inwindow_likelihood", "/", "torch", ".", "sum", "(", "inwindow_mask", ")", "\n", "\n", "inwindow_aux_loss", "=", "-", "1.0", "*", "inwindow_likelihood_avg", "\n", "\n", "return", "inwindow_aux_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.mostAttendedSpans": [[186, 235], ["utils.round_all", "len", "range", "utils.sortDictByValue", "utils.round_all", "zip", "out_str.strip.strip", "utils.tocpuNPList", "sum", "len", "len", "utils.isSpanOverlap", "top_spans.append", "utils.round_all", "len"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.sortDictByValue", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.isSpanOverlap", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all"], ["", "def", "mostAttendedSpans", "(", "attention_vec", ":", "torch", ".", "FloatTensor", ",", "tokens", ":", "List", "[", "str", "]", ",", "span_length", "=", "5", ")", ":", "\n", "    ", "\"\"\" Visualize an attention vector for a list of tokens\n\n        Parameters:\n        ----------\n        attention_vec: Shape: (sequence_length, )\n            Padded vector containing attention over a sequence\n        question_tokens: List[str]\n            List of tokens in the sequence\n\n        Returns:\n        --------\n    \"\"\"", "\n", "\n", "attention_aslist", ":", "List", "[", "float", "]", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "attention_vec", ")", ",", "3", ")", "\n", "tokens_len", "=", "len", "(", "tokens", ")", "\n", "# To remove padded elements", "\n", "attention_aslist", ":", "List", "[", "float", "]", "=", "attention_aslist", "[", ":", "tokens_len", "]", "\n", "\n", "span2atten", "=", "{", "}", "\n", "for", "start", "in", "range", "(", "0", ",", "len", "(", "tokens", ")", "-", "span_length", "+", "1", ")", ":", "\n", "        ", "end", "=", "start", "+", "span_length", "\n", "attention_sum", "=", "sum", "(", "attention_aslist", "[", "start", ":", "end", "]", ")", "\n", "span2atten", "[", "(", "start", ",", "end", ")", "]", "=", "attention_sum", "\n", "\n", "", "sorted_spanattn", "=", "myutils", ".", "sortDictByValue", "(", "span2atten", ",", "decreasing", "=", "True", ")", "\n", "sorted_spanattn", "=", "myutils", ".", "round_all", "(", "sorted_spanattn", ",", "3", ")", "\n", "\n", "top_spans", "=", "[", "sorted_spanattn", "[", "0", "]", "[", "0", "]", "]", "\n", "idx", "=", "1", "\n", "while", "len", "(", "top_spans", ")", "<", "5", "and", "idx", "<", "len", "(", "sorted_spanattn", ")", ":", "\n", "        ", "span", "=", "sorted_spanattn", "[", "idx", "]", "[", "0", "]", "\n", "keep_span", "=", "True", "\n", "for", "in_span", "in", "top_spans", ":", "\n", "            ", "if", "myutils", ".", "isSpanOverlap", "(", "span", ",", "in_span", ")", ":", "\n", "                ", "keep_span", "=", "False", "\n", "\n", "", "", "if", "keep_span", ":", "\n", "            ", "top_spans", ".", "append", "(", "span", ")", "\n", "", "idx", "+=", "1", "\n", "\n", "", "most_attention_spans", "=", "[", "\" \"", ".", "join", "(", "tokens", "[", "start", ":", "end", "]", ")", "for", "(", "start", ",", "end", ")", "in", "top_spans", "]", "\n", "attention_values", "=", "[", "span2atten", "[", "span", "]", "for", "span", "in", "top_spans", "]", "\n", "out_str", "=", "\"\"", "\n", "for", "span", ",", "attn", "in", "zip", "(", "most_attention_spans", ",", "attention_values", ")", ":", "\n", "        ", "out_str", "+=", "\"{}:{} | \"", ".", "format", "(", "span", ",", "myutils", ".", "round_all", "(", "attn", ",", "3", ")", ")", "\n", "", "out_str", "=", "out_str", ".", "strip", "(", ")", "\n", "\n", "return", "out_str", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.listTokensVis": [[237, 270], ["utils.round_all", "len", "zip", "sorted", "utils.tocpuNPList", "complete_attention_vis.strip", "most_attended_vis.strip", "zip"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList"], ["", "def", "listTokensVis", "(", "attention_vec", ":", "torch", ".", "FloatTensor", ",", "tokens", ":", "List", "[", "str", "]", ")", ":", "\n", "    ", "\"\"\" Visualize an attention vector for a list of tokens\n\n        Parameters:\n        ----------\n        attention_vec: Shape: (sequence_length, )\n            Padded vector containing attention over a sequence\n        question_tokens: List[str]\n            List of tokens in the sequence\n\n        Returns:\n        --------\n        complete_attention_vis: str\n        most_attended_vis: String visualization of question attention\n    \"\"\"", "\n", "\n", "attention_aslist", ":", "List", "[", "float", "]", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "attention_vec", ")", ",", "3", ")", "\n", "tokens_len", "=", "len", "(", "tokens", ")", "\n", "# To remove padded elements", "\n", "attention_aslist", ":", "List", "[", "float", "]", "=", "attention_aslist", "[", ":", "tokens_len", "]", "\n", "\n", "complete_attention_vis", "=", "\"\"", "\n", "for", "token", ",", "attn", "in", "zip", "(", "tokens", ",", "attention_aslist", ")", ":", "\n", "        ", "complete_attention_vis", "+=", "f\"{token}|{attn} \"", "\n", "\n", "# List[(token, attn)]", "\n", "", "sorted_token_attn", "=", "sorted", "(", "[", "(", "x", ",", "y", ")", "for", "x", ",", "y", "in", "zip", "(", "tokens", ",", "attention_aslist", ")", "]", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "most_attended_token_attn", "=", "sorted_token_attn", "[", ":", "10", "]", "\n", "most_attended_vis", "=", "\"Most attended: \"", "\n", "for", "token", ",", "attn", "in", "most_attended_token_attn", ":", "\n", "        ", "most_attended_vis", "+=", "f\"{token}|{attn} \"", "\n", "\n", "", "return", "complete_attention_vis", ".", "strip", "(", ")", ",", "most_attended_vis", ".", "strip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.domain_language_utils.topProbMassElems": [[272, 295], ["sorted", "range", "out_str.strip", "utils.round_all", "zip", "min", "utils.tocpuNPList", "len", "len"], "function", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.tocpuNPList"], ["", "def", "topProbMassElems", "(", "attention", ":", "torch", ".", "Tensor", ",", "support", ":", "List", "[", "Any", "]", ",", "k", "=", "5", ")", ":", "\n", "    ", "\"\"\" Get the top attended elems.\n\n        Parameters:\n        ----------\n        attention: Shape: (padded_support_len)\n            Padded vector containing attention over a sequence\n        support: List[Any] List of len=support_len\n\n        Returns:\n        --------\n        complete_attention_vis: str\n        most_attended_vis: String visualization of question attention\n    \"\"\"", "\n", "\n", "attention_aslist", ":", "List", "[", "float", "]", "=", "myutils", ".", "round_all", "(", "myutils", ".", "tocpuNPList", "(", "attention", ")", ",", "3", ")", "[", ":", "len", "(", "support", ")", "]", "\n", "\n", "sorted_elem_attn", "=", "sorted", "(", "zip", "(", "support", ",", "attention_aslist", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "out_str", "=", "\"\"", "\n", "for", "i", "in", "range", "(", "min", "(", "k", ",", "len", "(", "sorted_elem_attn", ")", ")", ")", ":", "\n", "        ", "out_str", "+=", "f\"{sorted_elem_attn[i][0]}: {sorted_elem_attn[i][1]} || \"", "\n", "\n", "", "return", "out_str", ".", "strip", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_execution_parameters.ExecutorParameters.__init__": [[21, 106], ["super().__init__", "torch.nn.Linear", "torch.nn.Linear", "allennlp.modules.matrix_attention.DotProductMatrixAttention", "torch.nn.Parameter", "torch.nn.init.normal_", "allennlp.modules.matrix_attention.BilinearMatrixAttention", "allennlp.modules.matrix_attention.LinearMatrixAttention", "allennlp.modules.span_extractors.endpoint_span_extractor.EndpointSpanExtractor", "allennlp.modules.matrix_attention.LinearMatrixAttention", "allennlp.modules.matrix_attention.BilinearMatrixAttention", "allennlp.modules.matrix_attention.BilinearMatrixAttention", "allennlp.modules.matrix_attention.BilinearMatrixAttention", "allennlp.modules.matrix_attention.BilinearMatrixAttention", "drop_execution_parameters.ExecutorParameters.question_attention_to_span.get_output_dim", "torch.FloatTensor", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "question_encoding_dim", ":", "int", ",", "\n", "passage_encoding_dim", ":", "int", ",", "\n", "passage_attention_to_span", ":", "Seq2SeqEncoder", ",", "\n", "passage_startend_predictor", ",", "\n", "question_attention_to_span", ":", "Seq2SeqEncoder", ",", "\n", "passage_attention_to_count", ":", "Seq2SeqEncoder", ",", "\n", "num_implicit_nums", ":", "int", "=", "None", ",", "\n", "passage_count_predictor", "=", "None", ",", "\n", "passage_count_hidden2logits", "=", "None", ",", "\n", "dropout", ":", "float", "=", "0.0", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "num_counts", "=", "10", "\n", "\n", "self", ".", "passage_attention_scalingvals", "=", "[", "1", ",", "2", ",", "5", ",", "10", "]", "\n", "\n", "# Parameters for answer start/end prediction from PassageAttention", "\n", "self", ".", "passage_attention_to_span", "=", "passage_attention_to_span", "\n", "self", ".", "passage_startend_predictor", "=", "passage_startend_predictor", "# torch.nn.Linear(self.passage_attention_to_span.get_output_dim(), 2)", "\n", "\n", "# Parameters for answer start/end pred directly from passage encoding (direct PassageSpanAnswer from 1step prog)", "\n", "self", ".", "oneshot_psa_startend_predictor", "=", "torch", ".", "nn", ".", "Linear", "(", "passage_encoding_dim", ",", "2", ")", "\n", "\n", "self", ".", "question_attention_to_span", "=", "question_attention_to_span", "\n", "self", ".", "question_startend_predictor", "=", "torch", ".", "nn", ".", "Linear", "(", "self", ".", "question_attention_to_span", ".", "get_output_dim", "(", ")", ",", "2", ")", "\n", "\n", "self", ".", "passage_attention_to_count", "=", "passage_attention_to_count", "\n", "# self.passage_count_predictor = torch.nn.Linear(self.passage_attention_to_count.get_output_dim(),", "\n", "#                                                self.num_counts)", "\n", "self", ".", "passage_count_predictor", "=", "passage_count_predictor", "\n", "# Linear from self.passage_attention_to_count.output_dim --> 1", "\n", "self", ".", "passage_count_hidden2logits", "=", "passage_count_hidden2logits", "\n", "\n", "self", ".", "dotprod_matrix_attn", "=", "DotProductMatrixAttention", "(", ")", "\n", "\n", "self", ".", "implicit_num_embeddings", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "FloatTensor", "(", "num_implicit_nums", ",", "passage_encoding_dim", ")", ")", "\n", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "self", ".", "implicit_num_embeddings", ",", "mean", "=", "0.0", ",", "std", "=", "0.001", ")", "\n", "self", ".", "implicitnum_bilinear_attention", "=", "BilinearMatrixAttention", "(", "\n", "matrix_1_dim", "=", "passage_encoding_dim", ",", "matrix_2_dim", "=", "passage_encoding_dim", "\n", ")", "\n", "\n", "# self.filter_matrix_attention = LinearMatrixAttention(", "\n", "#     tensor_1_dim=question_encoding_dim, tensor_2_dim=passage_encoding_dim, combination=\"x,y,x*y\"", "\n", "# )", "\n", "\n", "self", ".", "filter_matrix_attention", "=", "LinearMatrixAttention", "(", "\n", "tensor_1_dim", "=", "question_encoding_dim", ",", "tensor_2_dim", "=", "passage_encoding_dim", ",", "combination", "=", "\"x,y,x*y\"", "\n", ")", "\n", "\n", "self", ".", "_endpoint_span_extractor", "=", "EndpointSpanExtractor", "(", "\n", "input_dim", "=", "passage_encoding_dim", ",", "\n", "combination", "=", "\"x,y\"", ")", "\n", "\n", "# We will sum the passage-token-repr to the weighted-q-repr - to use x*y combination", "\n", "self", ".", "relocate_matrix_attention", "=", "LinearMatrixAttention", "(", "\n", "tensor_1_dim", "=", "passage_encoding_dim", ",", "tensor_2_dim", "=", "passage_encoding_dim", ",", "combination", "=", "\"x,y,x*y\"", "\n", ")", "\n", "\n", "# This computes a passage_to_passage attention, hopefully, for each token, putting a weight on date tokens", "\n", "# that are related to it.", "\n", "self", ".", "passage_to_date_attention", ":", "MatrixAttention", "=", "BilinearMatrixAttention", "(", "\n", "matrix_1_dim", "=", "passage_encoding_dim", ",", "matrix_2_dim", "=", "passage_encoding_dim", "\n", ")", "\n", "\n", "self", ".", "passage_to_start_date_attention", ":", "MatrixAttention", "=", "BilinearMatrixAttention", "(", "\n", "matrix_1_dim", "=", "passage_encoding_dim", ",", "matrix_2_dim", "=", "passage_encoding_dim", "\n", ")", "\n", "\n", "self", ".", "passage_to_end_date_attention", ":", "MatrixAttention", "=", "BilinearMatrixAttention", "(", "\n", "matrix_1_dim", "=", "passage_encoding_dim", ",", "matrix_2_dim", "=", "passage_encoding_dim", "\n", ")", "\n", "\n", "# This computes a passage_to_passage attention, hopefully, for each token, putting a weight on date tokens", "\n", "# that are related to it.", "\n", "self", ".", "passage_to_num_attention", ":", "MatrixAttention", "=", "BilinearMatrixAttention", "(", "\n", "matrix_1_dim", "=", "passage_encoding_dim", ",", "matrix_2_dim", "=", "passage_encoding_dim", "\n", ")", "\n", "\n", "if", "dropout", ">", "0", ":", "\n", "            ", "self", ".", "_dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_dropout", "=", "lambda", "x", ":", "x", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.__init__": [[6, 29], ["allennlp_semparse.DomainLanguage.__init__"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "start_types", "=", "{", "int", "}", ",", "\n", "allowed_constants", "=", "{", "\n", "# We unfortunately have to explicitly enumerate all allowed constants in the", "\n", "# grammar.  Because we'll be inducing a grammar for this language for use with a", "\n", "# semantic parser, we need the grammar to be finite, which means we can't allow", "\n", "# arbitrary constants (you can't parameterize an infinite categorical", "\n", "# distribution).  So our Arithmetic language will have to only operate on simple", "\n", "# numbers.", "\n", "\"1\"", ":", "1", ",", "\n", "\"2\"", ":", "2", ",", "\n", "\"3\"", ":", "3", ",", "\n", "\"4\"", ":", "4", ",", "\n", "\"5\"", ":", "5", ",", "\n", "\"6\"", ":", "6", ",", "\n", "\"7\"", ":", "7", ",", "\n", "\"8\"", ":", "8", ",", "\n", "\"9\"", ":", "9", ",", "\n", "\"10\"", ":", "10", ",", "\n", "\"20\"", ":", "20", ",", "\n", "\"-5\"", ":", "-", "5", ",", "\n", "\"-2\"", ":", "-", "2", ",", "\n", "}", ",", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add": [[32, 35], ["None"], "methods", ["None"], ["", "@", "predicate", "\n", "def", "add", "(", "self", ",", "num1", ":", "int", ",", "num2", ":", "int", ")", "->", "int", ":", "\n", "        ", "return", "num1", "+", "num2", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum": [[36, 39], ["test_language.Arithmetic.sum"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "@", "predicate", "\n", "def", "sum", "(", "self", ",", "numbers", ":", "List", "[", "int", "]", ")", "->", "int", ":", "\n", "        ", "return", "sum", "(", "numbers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.list1": [[43, 46], ["None"], "methods", ["None"], ["", "@", "predicate", "\n", "def", "list1", "(", "self", ",", "num1", ":", "int", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "return", "[", "num1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.list2": [[47, 50], ["None"], "methods", ["None"], ["", "@", "predicate", "\n", "def", "list2", "(", "self", ",", "num1", ":", "int", ",", "num2", ":", "int", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "return", "[", "num1", ",", "num2", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.list3": [[51, 54], ["None"], "methods", ["None"], ["", "@", "predicate", "\n", "def", "list3", "(", "self", ",", "num1", ":", "int", ",", "num2", ":", "int", ",", "num3", ":", "int", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "return", "[", "num1", ",", "num2", ",", "num3", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.list4": [[55, 58], ["None"], "methods", ["None"], ["", "@", "predicate", "\n", "def", "list4", "(", "self", ",", "num1", ":", "int", ",", "num2", ":", "int", ",", "num3", ":", "int", ",", "num4", ":", "int", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "return", "[", "num1", ",", "num2", ",", "num3", ",", "num4", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.subtract": [[59, 62], ["None"], "methods", ["None"], ["", "@", "predicate", "\n", "def", "subtract", "(", "self", ",", "num1", ":", "int", ",", "num2", ":", "int", ")", "->", "int", ":", "\n", "        ", "return", "num1", "-", "num2", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.power": [[63, 66], ["None"], "methods", ["None"], ["", "@", "predicate", "\n", "def", "power", "(", "self", ",", "num1", ":", "int", ",", "num2", ":", "int", ")", "->", "int", ":", "\n", "        ", "return", "num1", "**", "num2", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.multiply": [[67, 70], ["None"], "methods", ["None"], ["", "@", "predicate", "\n", "def", "multiply", "(", "self", ",", "num1", ":", "int", ",", "num2", ":", "int", ")", "->", "int", ":", "\n", "        ", "return", "num1", "*", "num2", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.divide": [[71, 74], ["None"], "methods", ["None"], ["", "@", "predicate", "\n", "def", "divide", "(", "self", ",", "num1", ":", "int", ",", "num2", ":", "int", ")", "->", "int", ":", "\n", "        ", "return", "num1", "//", "num2", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.halve": [[75, 78], ["None"], "methods", ["None"], ["", "@", "predicate", "\n", "def", "halve", "(", "self", ",", "num1", ":", "int", ")", "->", "int", ":", "\n", "        ", "return", "num1", "//", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.three": [[79, 82], ["None"], "methods", ["None"], ["", "@", "predicate", "\n", "def", "three", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "3", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.three_less": [[83, 95], ["function"], "methods", ["None"], ["", "@", "predicate", "\n", "def", "three_less", "(", "self", ",", "function", ":", "Callable", "[", "[", "int", ",", "int", "]", ",", "int", "]", ")", "->", "Callable", "[", "[", "int", ",", "int", "]", ",", "int", "]", ":", "\n", "        ", "\"\"\"\n        Wraps a function into a new function that always returns three less than what the original\n        function would.  Totally senseless function that's just here to test higher-order\n        functions.\n        \"\"\"", "\n", "\n", "def", "new_function", "(", "num1", ":", "int", ",", "num2", ":", "int", ")", "->", "int", ":", "\n", "            ", "return", "function", "(", "num1", ",", "num2", ")", "-", "3", "\n", "\n", "", "return", "new_function", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.passage_attn2span.DROPReader.__init__": [[42, 60], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "lazy", ":", "bool", "=", "False", ",", "\n", "min_passage_length", "=", "200", ",", "\n", "max_passage_length", "=", "400", ",", "\n", "max_span_length", "=", "10", ",", "\n", "num_training_samples", "=", "2000", ",", "\n", "normalized", "=", "True", ",", "\n", "withnoise", "=", "True", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "lazy", ")", "\n", "\n", "# self._min_passage_length = min_passage_length", "\n", "# self._max_passage_length = max_passage_length", "\n", "# self._max_span_length = max_span_length", "\n", "# self._num_training_samples = num_training_samples", "\n", "self", ".", "_normalized", "=", "normalized", "\n", "self", ".", "_withnoise", "=", "withnoise", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.passage_attn2span.DROPReader._read": [[107, 161], ["logger.info", "json.load.items", "open", "json.load", "len", "passage_text.split", "allennlp.data.fields.ArrayField", "allennlp.data.fields.ArrayField", "allennlp.data.fields.MetadataField", "instances.append", "len", "print", "sum", "numpy.array", "numpy.array", "allennlp.data.instance.Instance", "range", "abs", "float", "random.gauss"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "# pylint: disable=logging-fstring-interpolation", "\n", "\n", "        ", "instances", ":", "List", "[", "Instance", "]", "=", "[", "]", "\n", "with", "open", "(", "file_path", ")", "as", "dataset_file", ":", "\n", "            ", "dataset", "=", "json", ".", "load", "(", "dataset_file", ")", "\n", "", "logger", ".", "info", "(", "f\"Reading the dataset from: {file_path}\"", ")", "\n", "\n", "for", "passage_id", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "            ", "passage_text", "=", "passage_info", "[", "constants", ".", "tokenized_passage", "]", "\n", "passage_length", "=", "len", "(", "passage_text", ".", "split", "(", "\" \"", ")", ")", "\n", "\n", "for", "question_answer", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "                ", "fields", "=", "{", "}", "\n", "\n", "if", "constants", ".", "answer_passage_spans", "in", "question_answer", ":", "\n", "                    ", "answer_passage_spans", "=", "question_answer", "[", "constants", ".", "answer_passage_spans", "]", "\n", "", "else", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "len", "(", "answer_passage_spans", ")", "==", "0", ":", "\n", "                    ", "print", "(", "\"NO PASSAGE SPAN AS ANS\"", ")", "\n", "continue", "\n", "\n", "# TODO(nitish): Only using first span as answer", "\n", "", "answer_span", "=", "answer_passage_spans", "[", "0", "]", "\n", "\n", "start_position", "=", "answer_span", "[", "0", "]", "\n", "end_position", "=", "answer_span", "[", "1", "]", "\n", "\n", "span_length", "=", "end_position", "-", "start_position", "+", "1", "\n", "\n", "attention", "=", "[", "0.0", "for", "_", "in", "range", "(", "passage_length", ")", "]", "\n", "\n", "attention", "[", "start_position", ":", "end_position", "+", "1", "]", "=", "[", "1.0", "]", "*", "span_length", "\n", "\n", "if", "self", ".", "_withnoise", ":", "\n", "                    ", "attention", "=", "[", "x", "+", "abs", "(", "random", ".", "gauss", "(", "0", ",", "0.001", ")", ")", "for", "x", "in", "attention", "]", "\n", "\n", "", "if", "self", ".", "_normalized", ":", "\n", "                    ", "attention_sum", "=", "sum", "(", "attention", ")", "\n", "attention", "=", "[", "float", "(", "x", ")", "/", "attention_sum", "for", "x", "in", "attention", "]", "\n", "\n", "", "passage_span_fields", "=", "ArrayField", "(", "np", ".", "array", "(", "[", "[", "start_position", ",", "end_position", "]", "]", ")", ",", "padding_value", "=", "-", "1", ")", "\n", "\n", "fields", "[", "\"passage_attention\"", "]", "=", "ArrayField", "(", "np", ".", "array", "(", "attention", ")", ",", "padding_value", "=", "0.0", ")", "\n", "\n", "fields", "[", "\"passage_lengths\"", "]", "=", "MetadataField", "(", "passage_length", ")", "\n", "\n", "fields", "[", "\"answer_as_passage_spans\"", "]", "=", "passage_span_fields", "\n", "\n", "instances", ".", "append", "(", "Instance", "(", "fields", ")", ")", "\n", "\n", "", "", "return", "instances", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.pattn2count_reader.PAttn2CountReader.__init__": [[41, 62], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "lazy", ":", "bool", "=", "False", ",", "\n", "min_passage_length", "=", "200", ",", "\n", "max_passage_length", "=", "400", ",", "\n", "min_span_length", "=", "5", ",", "\n", "max_span_length", "=", "15", ",", "\n", "samples_per_bucket_count", "=", "1000", ",", "\n", "normalized", "=", "True", ",", "\n", "withnoise", "=", "True", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "lazy", ")", "\n", "\n", "self", ".", "_min_passage_length", "=", "min_passage_length", "\n", "self", ".", "_max_passage_length", "=", "max_passage_length", "\n", "self", ".", "_min_span_length", "=", "min_span_length", "\n", "self", ".", "_max_span_length", "=", "max_span_length", "\n", "self", ".", "_normalized", "=", "True", "# normalized", "\n", "self", ".", "_withnoise", "=", "True", "# withnoise", "\n", "self", ".", "samples_per_bucket_count", "=", "samples_per_bucket_count", "\n", "self", ".", "num_instances", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.pattn2count_reader.PAttn2CountReader._read": [[90, 128], ["logger.info", "pattn2count_reader.PAttn2CountReader.make_data", "print", "print", "len", "allennlp.data.fields.ArrayField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.LabelField", "instances.append", "numpy.array", "allennlp.data.instance.Instance"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.pattn2count_reader.PAttn2CountReader.make_data"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "# pylint: disable=logging-fstring-interpolation", "\n", "        ", "logger", ".", "info", "(", "\n", "f\"Making training examples with:\\n\"", "\n", "f\"min_passage_len: {self._min_passage_length}\\n\"", "\n", "f\"max_passage_length: {self._max_passage_length}\\n\"", "\n", "f\"min / max span_len: {self._min_span_length} / {self._max_span_length}\\n\"", "\n", ")", "\n", "\n", "instances", ":", "List", "[", "Instance", "]", "=", "[", "]", "\n", "\n", "data_dicts", "=", "self", ".", "make_data", "(", "\n", "min_passage_length", "=", "self", ".", "_min_passage_length", ",", "\n", "max_passage_length", "=", "self", ".", "_max_passage_length", ",", "\n", "min_span_length", "=", "self", ".", "_min_span_length", ",", "\n", "max_span_length", "=", "self", ".", "_max_span_length", ",", "\n", "max_count_value", "=", "7", ",", "\n", "samples_per_bucket_count", "=", "self", ".", "samples_per_bucket_count", ",", "\n", ")", "\n", "\n", "for", "d", "in", "data_dicts", ":", "\n", "            ", "attention", "=", "d", "[", "\"attention\"", "]", "\n", "count_value", "=", "d", "[", "\"count_value\"", "]", "\n", "passage_length", "=", "len", "(", "attention", ")", "\n", "\n", "fields", "=", "{", "}", "\n", "fields", "[", "\"passage_attention\"", "]", "=", "ArrayField", "(", "np", ".", "array", "(", "attention", ")", ",", "padding_value", "=", "-", "1", ")", "\n", "fields", "[", "\"passage_lengths\"", "]", "=", "MetadataField", "(", "passage_length", ")", "\n", "fields", "[", "\"count_answer\"", "]", "=", "LabelField", "(", "count_value", ",", "skip_indexing", "=", "True", ")", "\n", "\n", "instances", ".", "append", "(", "Instance", "(", "fields", ")", ")", "\n", "self", ".", "num_instances", "+=", "1", "\n", "\n", "", "print", "(", "f\"SamplesPerBucketCount: {self.samples_per_bucket_count} TotalInstances: {self.num_instances}\"", ")", "\n", "print", "(", "f\"Data made!\"", ")", "\n", "\n", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.pattn2count_reader.PAttn2CountReader.make_data": [[129, 173], ["pattn2count_reader.PAttn2CountReader._get_length_buckets", "print", "collections.defaultdict", "print", "range", "print", "print", "range", "print", "pattn2count_reader.PAttn2CountReader.make_instance", "data_dicts.append", "collections.defaultdict"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.pattn2count_reader.PAttn2CountReader._get_length_buckets", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.pattn2count_reader.PAttn2CountReader.make_instance"], ["", "def", "make_data", "(", "\n", "self", ",", "\n", "min_passage_length", ",", "\n", "max_passage_length", ",", "\n", "min_span_length", ",", "\n", "max_span_length", ",", "\n", "samples_per_bucket_count", ":", "int", ",", "\n", "max_count_value", ":", "int", "=", "7", ",", "\n", ")", ":", "\n", "# For each 100 length bucket, and count value, generate 1000 examples in train mode, and 100 in val mode", "\n", "        ", "num_instances_per_bucket_per_count", "=", "samples_per_bucket_count", "\n", "\n", "# List of min and max passage", "\n", "minmax_passagelen_tuples", "=", "self", ".", "_get_length_buckets", "(", "min_passage_length", ",", "max_passage_length", ")", "\n", "\n", "data_dicts", "=", "[", "]", "\n", "\n", "print", "(", "f\"Making Data ... \"", ")", "\n", "\n", "lenbucket_count_dict", "=", "defaultdict", "(", ")", "\n", "print", "(", "f\"Passage Length Buckets: {minmax_passagelen_tuples}\"", ")", "\n", "\n", "for", "count_value", "in", "range", "(", "0", ",", "max_count_value", "+", "1", ")", ":", "\n", "            ", "print", "(", "f\"Count Value: {count_value}\"", ")", "\n", "for", "min_plen", ",", "max_plen", "in", "minmax_passagelen_tuples", ":", "\n", "                ", "instances_for_bucket", "=", "0", "\n", "for", "i", "in", "range", "(", "num_instances_per_bucket_per_count", ")", ":", "\n", "                    ", "attention", "=", "self", ".", "make_instance", "(", "\n", "min_passage_length", "=", "min_plen", ",", "\n", "max_passage_length", "=", "max_plen", ",", "\n", "min_span_length", "=", "min_span_length", ",", "\n", "max_span_length", "=", "max_span_length", ",", "\n", "count_value", "=", "count_value", ",", "\n", ")", "\n", "if", "attention", "is", "None", ":", "\n", "                        ", "continue", "\n", "", "if", "count_value", "not", "in", "lenbucket_count_dict", ":", "\n", "                        ", "lenbucket_count_dict", "[", "count_value", "]", "=", "defaultdict", "(", "int", ")", "\n", "", "lenbucket_count_dict", "[", "count_value", "]", "[", "(", "min_plen", ",", "max_plen", ")", "]", "+=", "1", "\n", "data_dicts", ".", "append", "(", "{", "\"attention\"", ":", "attention", ",", "\"count_value\"", ":", "count_value", "}", ")", "\n", "instances_for_bucket", "+=", "1", "\n", "", "print", "(", "f\"{min_plen}, {max_plen} :: {instances_for_bucket}\"", ")", "\n", "", "print", "(", "\"\\n\"", ")", "\n", "", "return", "data_dicts", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.pattn2count_reader.PAttn2CountReader.sample_spansfor_variablelength": [[174, 193], ["sum", "range", "enumerate", "sorted", "result.append", "random.sample"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "def", "sample_spansfor_variablelength", "(", "self", ",", "seqlen", ",", "num_spans", ",", "span_lengths", ":", "List", "[", "int", "]", ")", ":", "\n", "        ", "sum_lengths", "=", "sum", "(", "span_lengths", ")", "\n", "# We need a gap of atleast 1 token between two spans. Number of heads is computed based on longer spans (+1)", "\n", "# and offset is also up by +1", "\n", "# Range of Number of possible span starts", "\n", "num_heads", "=", "seqlen", "-", "(", "sum_lengths", "-", "num_spans", "+", "num_spans", ")", "\n", "if", "num_heads", "<", "num_spans", ":", "\n", "            ", "return", "None", "\n", "", "indices", "=", "range", "(", "seqlen", "-", "(", "sum_lengths", "-", "num_spans", ")", ")", "\n", "result", "=", "[", "]", "\n", "offset", "=", "0", "\n", "# Randomly sample n=num_spans heads", "\n", "for", "i", ",", "idx", "in", "enumerate", "(", "sorted", "(", "random", ".", "sample", "(", "indices", ",", "num_spans", ")", ")", ")", ":", "\n", "# These heads are 0-indexed, to this we add the offset we've covered in the seq", "\n", "            ", "idx", "+=", "offset", "\n", "span_length", "=", "span_lengths", "[", "i", "]", "\n", "result", ".", "append", "(", "(", "idx", ",", "idx", "+", "span_length", ")", ")", "\n", "offset", "+=", "span_length", "-", "1", "+", "1", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.pattn2count_reader.PAttn2CountReader.make_instance": [[194, 222], ["random.randint", "numpy.abs", "sum", "numpy.random.normal", "pattn2count_reader.PAttn2CountReader.sample_spansfor_variablelength", "random.randint", "range"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.pattn2count_reader.PAttn2CountReader.sample_spansfor_variablelength"], ["", "def", "make_instance", "(", "\n", "self", ",", "\n", "min_passage_length", ":", "int", ",", "\n", "max_passage_length", ":", "int", ",", "\n", "min_span_length", ":", "int", ",", "\n", "max_span_length", ":", "int", ",", "\n", "count_value", ":", "int", ",", "\n", ")", ":", "\n", "\n", "        ", "passage_length", "=", "random", ".", "randint", "(", "min_passage_length", ",", "max_passage_length", ")", "\n", "# Mean: 0, Std: 0.2, Size: PassageLength", "\n", "attention", "=", "np", ".", "abs", "(", "np", ".", "random", ".", "normal", "(", "0.0", ",", "0.1", ",", "passage_length", ")", ")", "\n", "\n", "if", "count_value", ">", "0", ":", "\n", "            ", "span_lengths", "=", "[", "random", ".", "randint", "(", "min_span_length", ",", "max_span_length", ")", "for", "_", "in", "range", "(", "count_value", ")", "]", "\n", "# Sample n=count_value spans of the same length. Ends are exclusive", "\n", "# sampled_spans = self.sample_spans(passage_length, count_value, span_length)", "\n", "sampled_spans", "=", "self", ".", "sample_spansfor_variablelength", "(", "passage_length", ",", "count_value", ",", "span_lengths", ")", "\n", "if", "sampled_spans", "is", "None", ":", "\n", "                ", "return", "None", "\n", "\n", "", "for", "(", "start", ",", "end", ")", "in", "sampled_spans", ":", "\n", "                ", "attention", "[", "start", ":", "end", "]", "+=", "1.0", "\n", "\n", "", "", "attention_sum", "=", "sum", "(", "attention", ")", "\n", "attention", "=", "attention", "/", "attention_sum", "\n", "\n", "return", "attention", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.pattn2count_reader.PAttn2CountReader._get_length_buckets": [[223, 245], ["list", "max_length_buckets.append", "zip", "max_length_buckets.append", "min_length_buckets.append", "len"], "methods", ["None"], ["", "def", "_get_length_buckets", "(", "self", ",", "min_passage_length", ",", "max_passage_length", ")", ":", "\n", "        ", "min_length_buckets", "=", "[", "min_passage_length", "]", "\n", "max_length_buckets", "=", "[", "]", "\n", "\n", "# Add start, end + 100 until end <= max_passage_length", "\n", "i", "=", "1", "\n", "while", "True", ":", "\n", "            ", "potential_max_len", "=", "i", "*", "100", "+", "min_passage_length", "\n", "if", "potential_max_len", "<=", "max_passage_length", ":", "\n", "                ", "max_length_buckets", ".", "append", "(", "potential_max_len", ")", "\n", "min_length_buckets", ".", "append", "(", "max_length_buckets", "[", "-", "1", "]", ")", "# Last end is next's start", "\n", "\n", "i", "+=", "1", "\n", "", "else", ":", "\n", "                ", "break", "\n", "", "", "if", "len", "(", "max_length_buckets", ")", "==", "0", "or", "max_length_buckets", "[", "-", "1", "]", "!=", "max_passage_length", ":", "# This was left out", "\n", "            ", "max_length_buckets", ".", "append", "(", "max_passage_length", ")", "\n", "\n", "", "if", "min_length_buckets", "[", "-", "1", "]", "==", "max_passage_length", ":", "\n", "            ", "min_length_buckets", "=", "min_length_buckets", "[", ":", "-", "1", "]", "\n", "\n", "", "return", "list", "(", "zip", "(", "min_length_buckets", ",", "max_length_buckets", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.numdist2count.NUmDist2CountReader.__init__": [[42, 63], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "lazy", ":", "bool", "=", "False", ",", "\n", "min_dist_length", "=", "6", ",", "\n", "max_dist_length", "=", "14", ",", "\n", "max_count", "=", "7", ",", "\n", "num_training_samples", "=", "2000", ",", "\n", "noise_std", "=", "0.05", ",", "\n", "normalized", "=", "True", ",", "\n", "withnoise", "=", "True", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "lazy", ")", "\n", "\n", "self", ".", "_min_dist_length", "=", "min_dist_length", "\n", "self", ".", "_max_dist_length", "=", "max_dist_length", "\n", "self", ".", "_max_count", "=", "max_count", "\n", "self", ".", "_num_training_samples", "=", "num_training_samples", "\n", "self", ".", "_normalized", "=", "normalized", "\n", "self", ".", "_withnoise", "=", "withnoise", "\n", "self", ".", "_noise_std", "=", "noise_std", "\n", "self", ".", "instances_made", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.numdist2count.NUmDist2CountReader._read": [[64, 109], ["logger.info", "range", "print", "random.randint", "random.randint", "utils.round_all", "print", "allennlp.data.fields.ArrayField", "allennlp.data.fields.LabelField", "instances.append", "min", "random.sample", "random.uniform", "sum", "numpy.array", "allennlp.data.instance.Instance", "range", "abs", "float", "random.gauss"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "# pylint: disable=logging-fstring-interpolation", "\n", "        ", "logger", ".", "info", "(", "\n", "f\"Making {self._num_training_samples} training examples with:\\n\"", "\n", "f\"max_pdist_length: {self._max_dist_length}\\n\"", "\n", "f\"min_dist_length: {self._min_dist_length}\\n\"", "\n", "f\"max_count:{self._max_count}\\n\"", "\n", ")", "\n", "\n", "instances", ":", "List", "[", "Instance", "]", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "_num_training_samples", ")", ":", "\n", "            ", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "\n", "dist_length", "=", "random", ".", "randint", "(", "self", ".", "_min_dist_length", ",", "self", ".", "_max_dist_length", ")", "\n", "count_value", "=", "random", ".", "randint", "(", "1", ",", "min", "(", "self", ".", "_max_count", ",", "dist_length", ")", ")", "\n", "\n", "number_distribution", "=", "[", "0.0", "]", "*", "dist_length", "\n", "if", "count_value", ">", "0", ":", "\n", "                ", "indices", "=", "random", ".", "sample", "(", "range", "(", "dist_length", ")", ",", "count_value", ")", "\n", "# Add 1.0 to all sampled indices", "\n", "for", "i", "in", "indices", ":", "\n", "                    ", "number_distribution", "[", "i", "]", "+=", "1.0", "\n", "\n", "", "", "if", "self", ".", "_withnoise", ":", "\n", "                ", "std_dev", "=", "random", ".", "uniform", "(", "0.01", ",", "0.1", ")", "\n", "number_distribution", "=", "[", "x", "+", "abs", "(", "random", ".", "gauss", "(", "0", ",", "std_dev", ")", ")", "for", "x", "in", "number_distribution", "]", "\n", "\n", "", "if", "self", ".", "_normalized", ":", "\n", "                ", "attention_sum", "=", "sum", "(", "number_distribution", ")", "\n", "number_distribution", "=", "[", "float", "(", "x", ")", "/", "attention_sum", "for", "x", "in", "number_distribution", "]", "\n", "\n", "", "number_distribution", "=", "myutil", ".", "round_all", "(", "number_distribution", ",", "3", ")", "\n", "\n", "print", "(", "f\"{number_distribution}   {count_value}\"", ")", "\n", "\n", "fields", "[", "\"number_dist\"", "]", "=", "ArrayField", "(", "np", ".", "array", "(", "number_distribution", ")", ",", "padding_value", "=", "-", "1", ")", "\n", "\n", "fields", "[", "\"count_answer\"", "]", "=", "LabelField", "(", "count_value", ",", "skip_indexing", "=", "True", ")", "\n", "\n", "instances", ".", "append", "(", "Instance", "(", "fields", ")", ")", "\n", "self", ".", "instances_made", "+=", "1", "\n", "\n", "", "print", "(", "f\"Instances made: {self.instances_made}\"", ")", "\n", "return", "instances", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.BertDropTokenIndexer.__init__": [[81, 90], ["pytorch_pretrained_bert.BertTokenizer.from_pretrained", "allennlp.data.token_indexers.wordpiece_indexer.WordpieceIndexer.__init__"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__"], ["    ", "def", "__init__", "(", "self", ",", "pretrained_model", ":", "str", ",", "max_pieces", ":", "int", "=", "512", ")", "->", "None", ":", "\n", "        ", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "pretrained_model", ")", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "vocab", "=", "bert_tokenizer", ".", "vocab", ",", "\n", "wordpiece_tokenizer", "=", "bert_tokenizer", ".", "wordpiece_tokenizer", ".", "tokenize", ",", "\n", "do_lowercase", "=", "True", ",", "\n", "max_pieces", "=", "max_pieces", ",", "\n", "namespace", "=", "\"bert\"", ",", "\n", "separator_token", "=", "\"[SEP]\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.__init__": [[95, 123], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "pytorch_pretrained_bert.BertTokenizer.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "lazy", ":", "bool", "=", "True", ",", "\n", "pretrained_model", ":", "str", "=", "None", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "relaxed_span_match", ":", "bool", "=", "True", ",", "\n", "do_augmentation", ":", "bool", "=", "True", ",", "\n", "question_length_limit", ":", "int", "=", "50", ",", "\n", "only_strongly_supervised", ":", "bool", "=", "False", ",", "\n", "skip_instances", "=", "False", ",", "\n", "skip_due_to_gold_programs", "=", "False", ",", "\n", "convert_spananswer_to_num", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "lazy", ")", "\n", "self", ".", "_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "pretrained_model", ")", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "\n", "self", ".", "_relaxed_span_match", "=", "relaxed_span_match", "\n", "self", ".", "_do_augmentation", "=", "do_augmentation", "\n", "self", ".", "question_length_limit", "=", "question_length_limit", "\n", "self", ".", "only_strongly_supervised", "=", "only_strongly_supervised", "\n", "self", ".", "skip_instances", "=", "skip_instances", "\n", "self", ".", "skip_due_to_gold_programs", "=", "skip_due_to_gold_programs", "\n", "self", ".", "convert_spananswer_to_num", "=", "convert_spananswer_to_num", "\n", "self", ".", "skip_count", "=", "0", "\n", "self", ".", "skip_due_to_gold_not_in_answer", "=", "0", "\n", "\n", "self", ".", "max_passage_nums", "=", "0", "\n", "self", ".", "max_composed_nums", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew._read": [[124, 285], ["allennlp.common.file_utils.cached_path", "logger.info", "logger.info", "json.load.items", "logger.info", "logger.info", "logger.info", "logger.info", "open", "json.load", "drop_reader_bert.DROPReaderNew.text_to_instance", "answer_annotations.append", "print", "print", "print", "exit"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "        ", "self", ".", "skip_count", "=", "0", "\n", "# pylint: disable=logging-fstring-interpolation", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "logger", ".", "info", "(", "\"Reading file at %s\"", ",", "file_path", ")", "\n", "with", "open", "(", "file_path", ")", "as", "dataset_file", ":", "\n", "            ", "dataset", "=", "json", ".", "load", "(", "dataset_file", ")", "\n", "", "logger", ".", "info", "(", "\"Reading the dataset\"", ")", "\n", "instances", ",", "skip_count", "=", "[", "]", ",", "0", "\n", "max_question_len", "=", "self", ".", "question_length_limit", "\n", "total_qas", "=", "0", "\n", "instances_read", "=", "0", "\n", "for", "passage_id", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "            ", "original_passage_text", "=", "passage_info", "[", "constants", ".", "cleaned_passage", "]", "\n", "passage_text", "=", "passage_info", "[", "constants", ".", "tokenized_passage", "]", "\n", "passage_charidxs", "=", "passage_info", "[", "constants", ".", "passage_charidxs", "]", "\n", "p_date_mens", ":", "List", "[", "Tuple", "[", "str", ",", "Tuple", "[", "int", ",", "int", "]", ",", "Tuple", "[", "int", ",", "int", ",", "int", "]", "]", "]", "=", "passage_info", "[", "\n", "constants", ".", "passage_date_mens", "\n", "]", "\n", "p_date_entidxs", ":", "List", "[", "int", "]", "=", "passage_info", "[", "constants", ".", "passage_date_entidx", "]", "\n", "p_date_normvals", ":", "List", "[", "Tuple", "[", "int", ",", "int", ",", "int", "]", "]", "=", "passage_info", "[", "constants", ".", "passage_date_normalized_values", "]", "\n", "\n", "p_num_mens", ":", "List", "[", "Tuple", "[", "str", ",", "int", ",", "int", "]", "]", "=", "passage_info", "[", "constants", ".", "passage_num_mens", "]", "\n", "p_num_entidxs", ":", "List", "[", "int", "]", "=", "passage_info", "[", "constants", ".", "passage_num_entidx", "]", "\n", "p_num_normvals", ":", "List", "[", "int", "]", "=", "passage_info", "[", "constants", ".", "passage_num_normalized_values", "]", "\n", "\n", "# start / end (exclusive) token offsets for sentences in the passage", "\n", "p_sent_boundaries", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "=", "passage_info", "[", "constants", ".", "passage_sent_idxs", "]", "\n", "\n", "for", "qa", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "                ", "total_qas", "+=", "1", "\n", "question_id", "=", "qa", "[", "constants", ".", "query_id", "]", "\n", "original_ques_text", "=", "qa", "[", "constants", ".", "cleaned_question", "]", "\n", "question_text", "=", "qa", "[", "constants", ".", "tokenized_question", "]", "\n", "question_charidxs", "=", "qa", "[", "constants", ".", "question_charidxs", "]", "\n", "\n", "if", "constants", ".", "answer_passage_spans", "in", "qa", ":", "\n", "                    ", "answer_passage_spans", "=", "qa", "[", "constants", ".", "answer_passage_spans", "]", "\n", "", "else", ":", "\n", "                    ", "answer_passage_spans", "=", "None", "\n", "", "if", "constants", ".", "answer_question_spans", "in", "qa", ":", "\n", "                    ", "answer_question_spans", "=", "qa", "[", "constants", ".", "answer_question_spans", "]", "\n", "", "else", ":", "\n", "                    ", "answer_question_spans", "=", "None", "\n", "", "answer_annotations", "=", "[", "]", "\n", "if", "\"answer\"", "in", "qa", ":", "\n", "                    ", "answer_annotations", ".", "append", "(", "qa", "[", "\"answer\"", "]", ")", "\n", "", "if", "\"validated_answers\"", "in", "qa", ":", "\n", "                    ", "answer_annotations", "+=", "qa", "[", "\"validated_answers\"", "]", "\n", "\n", "", "qtype", "=", "\"UNK\"", "\n", "if", "constants", ".", "qtype", "in", "qa", "and", "qa", "[", "constants", ".", "qtype", "]", "is", "not", "None", ":", "\n", "                    ", "qtype", "=", "qa", "[", "constants", ".", "qtype", "]", "\n", "", "program_supervised", "=", "False", "\n", "if", "constants", ".", "program_supervised", "in", "qa", ":", "\n", "                    ", "program_supervised", "=", "qa", "[", "constants", ".", "program_supervised", "]", "\n", "\n", "# If qtype is known and program_supervised = False OR", "\n", "# If qtype is unknown and program_supervision is True --- There's a problem, Houston!", "\n", "", "if", "(", "program_supervised", "and", "qtype", "==", "\"UNK\"", ")", "or", "(", "qtype", "!=", "\"UNK\"", "and", "program_supervised", "is", "False", ")", ":", "\n", "                    ", "print", "(", "original_ques_text", ")", "\n", "print", "(", "f\"Qtype: {qtype}\"", ")", "\n", "print", "(", "f\"Program supervised: {program_supervised}\"", ")", "\n", "exit", "(", ")", "\n", "\n", "", "ques_attn_supervision", "=", "None", "\n", "qattn_supervised", "=", "False", "\n", "if", "constants", ".", "qattn_supervised", "in", "qa", ":", "\n", "                    ", "qattn_supervised", "=", "qa", "[", "constants", ".", "qattn_supervised", "]", "\n", "if", "qattn_supervised", "is", "True", ":", "\n", "                        ", "ques_attn_supervision", "=", "qa", "[", "constants", ".", "ques_attention_supervision", "]", "\n", "\n", "", "", "date_grounding_supervision", "=", "None", "\n", "num_grounding_supervision", "=", "None", "\n", "execution_supervised", "=", "False", "\n", "if", "constants", ".", "exection_supervised", "in", "qa", ":", "\n", "                    ", "execution_supervised", "=", "qa", "[", "constants", ".", "exection_supervised", "]", "\n", "if", "qa", "[", "constants", ".", "exection_supervised", "]", "is", "True", ":", "\n", "# There can be multiple types of execution_supervision", "\n", "                        ", "if", "constants", ".", "qspan_dategrounding_supervision", "in", "qa", ":", "\n", "                            ", "date_grounding_supervision", "=", "qa", "[", "constants", ".", "qspan_dategrounding_supervision", "]", "\n", "", "if", "constants", ".", "qspan_numgrounding_supervision", "in", "qa", ":", "\n", "# This can be a 1- or 2- tuple of number groundings", "\n", "                            ", "num_grounding_supervision", "=", "qa", "[", "constants", ".", "qspan_numgrounding_supervision", "]", "\n", "\n", "# passage_att_supervision is probably never used", "\n", "", "", "", "passage_attn_supervision", "=", "None", "\n", "pattn_supervised", "=", "False", "\n", "if", "constants", ".", "pattn_supervised", "in", "qa", ":", "\n", "                    ", "pattn_supervised", "=", "qa", "[", "constants", ".", "pattn_supervised", "]", "\n", "if", "constants", ".", "passage_attn_supervision", "in", "qa", ":", "\n", "                        ", "passage_attn_supervision", "=", "qa", "[", "constants", ".", "passage_attn_supervision", "]", "\n", "\n", "", "", "synthetic_numground_metadata", "=", "None", "\n", "if", "constants", ".", "SYN_NUMGROUND_METADATA", "in", "qa", ":", "\n", "                    ", "synthetic_numground_metadata", "=", "qa", "[", "constants", ".", "SYN_NUMGROUND_METADATA", "]", "\n", "\n", "", "strongly_supervised", "=", "program_supervised", "and", "qattn_supervised", "and", "execution_supervised", "\n", "\n", "if", "qattn_supervised", "is", "True", ":", "\n", "                    ", "assert", "program_supervised", "is", "True", "and", "qtype", "is", "not", "\"UNK\"", "\n", "", "if", "execution_supervised", "is", "True", ":", "\n", "                    ", "assert", "qattn_supervised", "is", "True", "\n", "\n", "", "instance", "=", "self", ".", "text_to_instance", "(", "\n", "question_text", ",", "\n", "original_ques_text", ",", "\n", "question_charidxs", ",", "\n", "passage_text", ",", "\n", "original_passage_text", ",", "\n", "passage_charidxs", ",", "\n", "p_sent_boundaries", ",", "\n", "p_date_mens", ",", "\n", "p_date_entidxs", ",", "\n", "p_date_normvals", ",", "\n", "p_num_mens", ",", "\n", "p_num_entidxs", ",", "\n", "p_num_normvals", ",", "\n", "qtype", ",", "\n", "program_supervised", ",", "\n", "qattn_supervised", ",", "\n", "execution_supervised", ",", "\n", "pattn_supervised", ",", "\n", "strongly_supervised", ",", "\n", "ques_attn_supervision", ",", "\n", "date_grounding_supervision", ",", "\n", "num_grounding_supervision", ",", "\n", "passage_attn_supervision", ",", "\n", "synthetic_numground_metadata", ",", "\n", "answer_passage_spans", ",", "\n", "answer_question_spans", ",", "\n", "question_id", ",", "\n", "passage_id", ",", "\n", "answer_annotations", ",", "\n", "max_question_len", ",", "\n", ")", "\n", "\n", "if", "self", ".", "only_strongly_supervised", ":", "\n", "                    ", "if", "not", "strongly_supervised", ":", "\n", "                        ", "instance", "=", "None", "\n", "\n", "", "", "if", "instance", "is", "not", "None", ":", "\n", "                    ", "instances_read", "+=", "1", "\n", "# print(\"\\n\\n\")", "\n", "yield", "instance", "\n", "\n", "#         if instance is not None:", "\n", "#             instances.append(instance)", "\n", "#         else:", "\n", "#             skip_count += 1", "\n", "# logger.info(f\"Skipped {skip_count} questions, kept {len(instances)} questions.\")", "\n", "# return instances", "\n", "", "", "", "logger", ".", "info", "(", "f\"Total QAs: {total_qas}. Instances read: {instances_read}\"", ")", "\n", "logger", ".", "info", "(", "f\"Instances Skipped: {self.skip_count}\"", ")", "\n", "logger", ".", "info", "(", "\n", "f\"Instances skipped due to gold-answer not in gold_program_types: {self.skip_due_to_gold_not_in_answer}\"", "\n", ")", "\n", "logger", ".", "info", "(", "\"Max passage nums: {} Max composed nums : {} \"", ".", "format", "(", "self", ".", "max_passage_nums", ",", "\n", "self", ".", "max_composed_nums", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.text_to_instance": [[286, 771], ["semqa.domain_languages.drop_language.get_empty_language_object", "semqa.domain_languages.drop_language.get_empty_language_object.all_possible_productions", "allennlp.data.fields.ListField", "passage_text.split", "question_text.split", "drop_reader_bert.tokenize_bert", "drop_reader_bert.tokenize_bert", "len", "question_wps.extend", "q_wpidx2tokenidx.extend", "min", "passage_wps.append", "p_wpidx2tokenidx.append", "drop_reader_bert.DROPReaderNew.prune_for_passage_len", "drop_reader_bert.DROPReaderNew.prune_for_question_len", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "drop_reader_bert.DROPReaderNew.update_charoffsets", "drop_reader_bert.DROPReaderNew.update_charoffsets", "allennlp.data.fields.ListField", "set", "drop_reader_bert.DROPReaderNew.compute_number_support", "max", "max", "zip", "allennlp.data.fields.ArrayField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "drop_reader_bert.DROPReaderNew.make_addsub_combination_array", "drop_reader_bert.DROPReaderNew.make_addsub_combination_array", "allennlp.data.fields.ArrayField", "allennlp.data.fields.ArrayField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.ArrayField", "allennlp.data.fields.MetadataField", "drop_reader_bert.DROPReaderNew.get_year_difference_candidates", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "list", "allennlp.data.fields.MetadataField", "metadata.update", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "drop_reader_bert.DROPReaderNew.get_gold_action_seqs", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "allennlp_semparse.fields.ProductionRuleField", "production_rule_fields.append", "allennlp.data.tokenizers.Token", "len", "allennlp.data.tokenizers.Token", "len", "len", "len", "len", "passage_number_wpindices.append", "numpy.array", "min", "passage_date_spanidxs.append", "zip", "numpy.array", "str", "range", "allennlp.data.fields.ArrayField", "allennlp.data.fields.ArrayField", "allennlp.data.fields.ArrayField", "allennlp.data.fields.ArrayField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "len", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "metadata.update", "allennlp.data.fields.ListField", "allennlp.data.fields.ListField", "set", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "int", "range", "sorted", "range", "semqa.domain_languages.drop_language.Date", "semqa.domain_languages.drop_language.Date", "enumerate", "ques_attn_supervision_wp.append", "numpy.array", "len", "numpy.array", "numpy.array", "len", "numpy.array", "len", "grounding_sup.extend", "new_num_grounding_supervision.append", "len", "enumerate", "question_text.split", "answer_program_start_types.append", "metadata.update", "allennlp.data.fields.SpanField", "float", "len", "len", "len", "len", "allennlp.data.tokenizers.Token", "min", "p_sentboundary_wps.append", "int", "len", "len", "len", "len", "semqa.domain_languages.drop_language.get_empty_language_object.all_possible_productions", "min", "passage_span_fields.append", "allennlp.data.fields.SpanField", "int", "answer_program_start_types.append", "answer_program_start_types.append", "answer_program_start_types.append", "answer_program_start_types.append", "len", "allennlp.data.tokenizers.Token", "allennlp.data.fields.SpanField", "allennlp.data.fields.SpanField", "float", "span_answer_text.split", "int", "set.add", "set.add", "len", "len", "len", "len", "len", "len", "len", "int", "passage_number_values.index", "composed_numbers.index", "year_differences.index", "list.index", "new_answer_program_start_types.append", "float", "int"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.get_empty_language_object", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.tokenize_bert", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.tokenize_bert", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.prune_for_passage_len", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.prune_for_question_len", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.update_charoffsets", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.update_charoffsets", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.compute_number_support", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.make_addsub_combination_array", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.make_addsub_combination_array", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.get_year_difference_candidates", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.get_gold_action_seqs", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add"], ["", "@", "overrides", "\n", "def", "text_to_instance", "(", "\n", "self", ",", "\n", "question_text", ":", "str", ",", "\n", "original_ques_text", ":", "str", ",", "\n", "question_charidxs", ":", "List", "[", "int", "]", ",", "\n", "passage_text", ":", "str", ",", "\n", "original_passage_text", ":", "str", ",", "\n", "passage_charidxs", ":", "List", "[", "int", "]", ",", "\n", "p_sent_boundaries", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ",", "\n", "p_date_mens", ":", "List", "[", "Tuple", "[", "str", ",", "Tuple", "[", "int", ",", "int", "]", ",", "Tuple", "[", "int", ",", "int", ",", "int", "]", "]", "]", ",", "\n", "p_date_entidxs", ":", "List", "[", "int", "]", ",", "\n", "p_date_normvals", ":", "List", "[", "Tuple", "[", "int", ",", "int", ",", "int", "]", "]", ",", "\n", "p_num_mens", ":", "List", "[", "Tuple", "[", "str", ",", "int", ",", "int", "]", "]", ",", "\n", "p_num_entidxs", ":", "List", "[", "int", "]", ",", "\n", "p_num_normvals", ":", "List", "[", "int", "]", ",", "\n", "qtype", ":", "str", ",", "\n", "program_supervised", ":", "bool", ",", "\n", "qattn_supervised", ":", "bool", ",", "\n", "execution_supervised", ":", "bool", ",", "\n", "pattn_supervised", ":", "bool", ",", "\n", "strongly_supervised", ":", "bool", ",", "\n", "ques_attn_supervision", ":", "Tuple", "[", "List", "[", "float", "]", "]", ",", "\n", "date_grounding_supervision", ":", "Tuple", "[", "List", "[", "int", "]", ",", "List", "[", "int", "]", "]", ",", "\n", "num_grounding_supervision", ":", "Tuple", "[", "List", "[", "int", "]", ",", "List", "[", "int", "]", "]", ",", "\n", "passage_attn_supervision", ":", "List", "[", "float", "]", ",", "\n", "synthetic_numground_metadata", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ",", "\n", "answer_passage_spans", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ",", "\n", "answer_question_spans", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ",", "\n", "question_id", ":", "str", "=", "None", ",", "\n", "passage_id", ":", "str", "=", "None", ",", "\n", "answer_annotations", ":", "List", "[", "Dict", "[", "str", ",", "Union", "[", "str", ",", "Dict", ",", "List", "]", "]", "]", "=", "None", ",", "\n", "max_question_len", ":", "int", "=", "None", ",", "\n", ")", "->", "Union", "[", "Instance", ",", "None", "]", ":", "\n", "\n", "        ", "metadata", "=", "{", "\n", "\"original_passage\"", ":", "original_passage_text", ",", "\n", "\"original_question\"", ":", "original_ques_text", ",", "\n", "\"passage_id\"", ":", "passage_id", ",", "\n", "\"question_id\"", ":", "question_id", ",", "\n", "}", "\n", "\n", "language", "=", "get_empty_language_object", "(", ")", "\n", "\n", "production_rule_fields", ":", "List", "[", "Field", "]", "=", "[", "]", "\n", "for", "production_rule", "in", "language", ".", "all_possible_productions", "(", ")", ":", "\n", "            ", "field", "=", "ProductionRuleField", "(", "production_rule", ",", "is_global_rule", "=", "True", ")", "\n", "production_rule_fields", ".", "append", "(", "field", ")", "\n", "", "action_field", "=", "ListField", "(", "production_rule_fields", ")", "\n", "\n", "passage_tokens", ":", "List", "[", "str", "]", "=", "passage_text", ".", "split", "(", "\" \"", ")", "\n", "question_tokens", ":", "List", "[", "str", "]", "=", "question_text", ".", "split", "(", "\" \"", ")", "\n", "\n", "# Word-piece tokenize the tokens in Q and P. Get maps;", "\n", "#  tokenidx2wpidx: List[List[int]] map from token idx to list of word-piece indices that correspond to it", "\n", "#  wpidx2tokenidx: List[int] map from word-piece index to original token idx", "\n", "# List[str], List[List[int]], List[int]", "\n", "passage_wps", ",", "p_tokenidx2wpidx", ",", "p_wpidx2tokenidx", "=", "tokenize_bert", "(", "self", ".", "_tokenizer", ",", "passage_tokens", ")", "\n", "question_wps", ",", "q_tokenidx2wpidx", ",", "q_wpidx2tokenidx", "=", "tokenize_bert", "(", "self", ".", "_tokenizer", ",", "question_tokens", ")", "\n", "\n", "# Truncate question_wps and wpidx2tokenidx to maximum allowable length", "\n", "question_wps", "=", "question_wps", "[", "0", ":", "max_question_len", "]", "\n", "q_wpidx2tokenidx", "=", "q_wpidx2tokenidx", "[", "0", ":", "max_question_len", "]", "\n", "max_q_tokenidx", "=", "q_wpidx2tokenidx", "[", "-", "1", "]", "# The last token_idx after truncation", "\n", "max_q_token_len", "=", "max_q_tokenidx", "+", "1", "\n", "# NOTE: Last token's all word-pieces might not be included; take precaution later", "\n", "q_tokenidx2wpidx", "=", "q_tokenidx2wpidx", "[", "0", ":", "max_q_token_len", "]", "\n", "\n", "# Padding question to max length since it makes possible to separate question and passage in the model", "\n", "q_wp_len", "=", "len", "(", "question_wps", ")", "\n", "q_wp_pad_len", "=", "max_question_len", "-", "q_wp_len", "\n", "question_wps", ".", "extend", "(", "[", "\"[PAD]\"", "]", "*", "q_wp_pad_len", ")", "\n", "q_wpidx2tokenidx", ".", "extend", "(", "[", "-", "1", "]", "*", "q_wp_pad_len", ")", "\n", "\n", "question_wps_tokens", "=", "[", "Token", "(", "text", "=", "t", ")", "for", "t", "in", "question_wps", "]", "\n", "\n", "# Passage_len = Max seq len - CLS - SEP - SEP - Max_Qlen -- Questions will be padded to max length", "\n", "max_passage_len", "=", "min", "(", "512", "-", "3", "-", "max_question_len", ",", "len", "(", "passage_wps", ")", ")", "\n", "passage_wps", "=", "passage_wps", "[", "0", ":", "max_passage_len", "]", "\n", "p_wpidx2tokenidx", "=", "p_wpidx2tokenidx", "[", "0", ":", "max_passage_len", "]", "\n", "max_p_tokenidx", "=", "p_wpidx2tokenidx", "[", "-", "1", "]", "\n", "max_p_token_len", "=", "max_p_tokenidx", "+", "1", "\n", "p_tokenidx2wpidx", "=", "p_tokenidx2wpidx", "[", "0", ":", "max_p_token_len", "]", "\n", "\n", "passage_wps", ".", "append", "(", "\"[SEP]\"", ")", "\n", "p_wpidx2tokenidx", ".", "append", "(", "-", "1", ")", "\n", "\n", "passage_wps_tokens", "=", "[", "Token", "(", "text", "=", "t", ")", "for", "t", "in", "passage_wps", "]", "\n", "# This would be in the input to BERT", "\n", "question_passage_tokens", "=", "[", "Token", "(", "\"[CLS]\"", ")", "]", "+", "question_wps_tokens", "+", "[", "Token", "(", "\"[SEP]\"", ")", "]", "+", "passage_wps_tokens", "\n", "\n", "(", "\n", "p_date_mens", ",", "\n", "p_date_entidxs", ",", "\n", "p_date_normvals", ",", "\n", "p_num_mens", ",", "\n", "p_num_entidxs", ",", "\n", "p_num_normvals", ",", "\n", "answer_passage_spans", ",", "\n", "date_grounding_supervision", ",", "\n", "num_grounding_supervision", ",", "\n", "passage_attn_supervision", ",", "\n", ")", "=", "self", ".", "prune_for_passage_len", "(", "\n", "max_p_token_len", ",", "\n", "p_date_mens", ",", "\n", "p_date_entidxs", ",", "\n", "p_date_normvals", ",", "\n", "p_num_mens", ",", "\n", "p_num_entidxs", ",", "\n", "p_num_normvals", ",", "\n", "answer_passage_spans", ",", "\n", "date_grounding_supervision", ",", "\n", "num_grounding_supervision", ",", "\n", "passage_attn_supervision", ",", "\n", ")", "\n", "\n", "(", "answer_question_spans", ",", "ques_attn_supervision", ")", "=", "self", ".", "prune_for_question_len", "(", "\n", "max_q_token_len", ",", "answer_question_spans", ",", "ques_attn_supervision", "\n", ")", "\n", "\n", "fields", "=", "{", "}", "\n", "fields", "[", "\"actions\"", "]", "=", "action_field", "\n", "fields", "[", "\"question\"", "]", "=", "TextField", "(", "question_wps_tokens", ",", "self", ".", "_token_indexers", ")", "\n", "fields", "[", "\"passage\"", "]", "=", "TextField", "(", "passage_wps_tokens", ",", "self", ".", "_token_indexers", ")", "\n", "fields", "[", "\"question_passage\"", "]", "=", "TextField", "(", "question_passage_tokens", ",", "self", ".", "_token_indexers", ")", "\n", "# List of (start, end) char offsets for each passage and question word-piece", "\n", "passage_offsets", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "=", "self", ".", "update_charoffsets", "(", "\n", "wpidx2tokenidx", "=", "p_wpidx2tokenidx", ",", "tokens", "=", "passage_tokens", ",", "token_charidxs", "=", "passage_charidxs", "\n", ")", "\n", "question_offsets", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "=", "self", ".", "update_charoffsets", "(", "\n", "wpidx2tokenidx", "=", "q_wpidx2tokenidx", ",", "tokens", "=", "question_tokens", ",", "token_charidxs", "=", "question_charidxs", "\n", ")", "\n", "p_sentboundary_wps", ":", "List", "[", "SpanField", "]", "=", "[", "]", "\n", "for", "token_boundary", "in", "p_sent_boundaries", ":", "\n", "            ", "start", ",", "end", "=", "token_boundary", "\n", "end", "=", "end", "-", "1", "# making end inclusive", "\n", "if", "start", "<", "max_p_token_len", ":", "\n", "                ", "start_wpidx", "=", "p_tokenidx2wpidx", "[", "start", "]", "[", "0", "]", "\n", "if", "end", "<", "max_p_token_len", ":", "\n", "                    ", "end_wpidx", "=", "p_tokenidx2wpidx", "[", "end", "]", "[", "-", "1", "]", "\n", "# If all of last token's wps are not present then take last wp present.", "\n", "#  passage_wps_tokens - 2 since the last word piece is [CLS]", "\n", "end_wpidx", "=", "min", "(", "end_wpidx", ",", "len", "(", "passage_wps_tokens", ")", "-", "2", ")", "\n", "p_sentboundary_wps", ".", "append", "(", "SpanField", "(", "span_start", "=", "start_wpidx", ",", "span_end", "=", "end_wpidx", ",", "\n", "sequence_field", "=", "fields", "[", "\"passage\"", "]", ")", ")", "\n", "\n", "", "", "", "fields", "[", "\"p_sentboundary_wps\"", "]", "=", "ListField", "(", "field_list", "=", "p_sentboundary_wps", ")", "\n", "\n", "# Passage Number", "\n", "passage_number_values", "=", "[", "int", "(", "x", ")", "if", "int", "(", "x", ")", "==", "x", "else", "x", "for", "x", "in", "p_num_normvals", "]", "\n", "nums_from_passage", "=", "set", "(", "passage_number_values", ")", "\n", "# composed_numbers: List[int/float] is sorted", "\n", "# passage_number_values: now contains implicit numbers. Since they are added at the end,", "\n", "#  indexing should be fine.", "\n", "# compnumber2addcombinations (sub): Dict: {composed_number: List[(pass-num1, pass-num2)]} - mapping from", "\n", "#  composed number to list of passage-num-tuples that combine to form the number-key using the operation", "\n", "(", "composed_numbers", ",", "passage_number_values", ",", "\n", "compnumber2addcombinations", ",", "compnumber2subcombinations", ",", "\n", "nums_from_addition", ",", "nums_from_subtraction", ")", "=", "self", ".", "compute_number_support", "(", "\n", "numbers", "=", "passage_number_values", ",", "\n", "implicit_numbers", "=", "DropLanguage", ".", "implicit_numbers", ",", "\n", "max_number_of_numbers_to_consider", "=", "2", ",", "\n", ")", "\n", "\n", "self", ".", "max_passage_nums", "=", "max", "(", "len", "(", "passage_number_values", ")", ",", "self", ".", "max_passage_nums", ")", "\n", "self", ".", "max_composed_nums", "=", "max", "(", "len", "(", "composed_numbers", ")", ",", "self", ".", "max_composed_nums", ")", "\n", "if", "not", "passage_number_values", ":", "\n", "            ", "passage_number_values", "=", "[", "0", "]", "\n", "", "if", "not", "composed_numbers", ":", "\n", "            ", "composed_numbers", "=", "[", "0", "]", "\n", "# TODO(nitishg): Change this repr to (token_idx, value) repr.", "\n", "", "passage_number_entidxs", "=", "p_num_entidxs", "# Index of passage_num_tokens in passage_number_values list", "\n", "passage_number_tokenids", "=", "[", "tokenidx", "for", "(", "_", ",", "tokenidx", ",", "_", ")", "in", "p_num_mens", "]", "\n", "assert", "len", "(", "passage_number_entidxs", ")", "==", "len", "(", "passage_number_tokenids", ")", "\n", "passage_num_wpidx2entidx", "=", "[", "-", "1", "for", "_", "in", "range", "(", "len", "(", "passage_wps", ")", ")", "]", "# number_ent_idx for each token (pad=-1)", "\n", "passage_number_wpindices", "=", "[", "]", "# wp_idxs that are numbers", "\n", "for", "passage_num_tokenidx", ",", "number_ent_idx", "in", "zip", "(", "passage_number_tokenids", ",", "passage_number_entidxs", ")", ":", "\n", "            ", "wp_index", "=", "p_tokenidx2wpidx", "[", "passage_num_tokenidx", "]", "[", "0", "]", "# WP-idx of the number token", "\n", "passage_num_wpidx2entidx", "[", "wp_index", "]", "=", "number_ent_idx", "# Index of the num-value in passsge_num_values", "\n", "passage_number_wpindices", ".", "append", "(", "wp_index", ")", "\n", "", "if", "not", "passage_number_wpindices", ":", "# If no numbers in the passage, padding by faking the 0-th token as a number", "\n", "            ", "passage_num_wpidx2entidx", "[", "0", "]", "=", "0", "\n", "passage_number_wpindices", "=", "[", "0", "]", "\n", "\n", "# Making a list of wp_idxs so that their corresponding values are increasingly sorted for differentiable min/max", "\n", "", "numvals_wpidx", "=", "[", "(", "passage_number_values", "[", "passage_num_wpidx2entidx", "[", "wpidx", "]", "]", ",", "wpidx", ")", "\n", "for", "wpidx", "in", "passage_number_wpindices", "]", "# [(value, w_idx)]", "\n", "sorted_passage_number_wpindices", "=", "[", "x", "[", "1", "]", "for", "x", "in", "sorted", "(", "numvals_wpidx", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "]", "\n", "# print(\"sorted_passage_number_wpindices: {}\".format(sorted_passage_number_wpindices))", "\n", "\n", "fields", "[", "\"passageidx2numberidx\"", "]", "=", "ArrayField", "(", "np", ".", "array", "(", "passage_num_wpidx2entidx", ")", ",", "padding_value", "=", "-", "1", ")", "\n", "fields", "[", "\"passage_number_values\"", "]", "=", "MetadataField", "(", "passage_number_values", ")", "\n", "fields", "[", "\"composed_numbers\"", "]", "=", "MetadataField", "(", "composed_numbers", ")", "\n", "fields", "[", "\"passage_number_sortedtokenidxs\"", "]", "=", "MetadataField", "(", "sorted_passage_number_wpindices", ")", "\n", "\n", "# NP.array of shape: (size_of_number_support, max_num_combinations, 2)", "\n", "add_number_combinations_indices", ",", "max_num_add_combs", "=", "self", ".", "make_addsub_combination_array", "(", "\n", "composed_numbers", "=", "composed_numbers", ",", "passage_numbers", "=", "passage_number_values", ",", "\n", "compnumber2numcombinations", "=", "compnumber2addcombinations", "\n", ")", "\n", "sub_number_combinations_indices", ",", "max_num_sub_combs", "=", "self", ".", "make_addsub_combination_array", "(", "\n", "composed_numbers", "=", "composed_numbers", ",", "passage_numbers", "=", "passage_number_values", ",", "\n", "compnumber2numcombinations", "=", "compnumber2subcombinations", "\n", ")", "\n", "\n", "fields", "[", "\"add_number_combinations_indices\"", "]", "=", "ArrayField", "(", "\n", "array", "=", "add_number_combinations_indices", ",", "padding_value", "=", "-", "1", ",", "dtype", "=", "np", ".", "int32", "\n", ")", "\n", "fields", "[", "\"sub_number_combinations_indices\"", "]", "=", "ArrayField", "(", "\n", "array", "=", "sub_number_combinations_indices", ",", "padding_value", "=", "-", "1", ",", "dtype", "=", "np", ".", "int32", "\n", ")", "\n", "fields", "[", "\"max_num_add_combs\"", "]", "=", "MetadataField", "(", "max_num_add_combs", ")", "\n", "fields", "[", "\"max_num_sub_combs\"", "]", "=", "MetadataField", "(", "max_num_sub_combs", ")", "\n", "\n", "##  Passage Dates", "\n", "passage_date_entidxs", "=", "p_date_entidxs", "\n", "passage_date_values", "=", "p_date_normvals", "\n", "passage_date_spanidxs", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "=", "[", "]", "\n", "for", "(", "_", ",", "(", "start_token_idx", ",", "end_token_idx", ")", ",", "_", ")", "in", "p_date_mens", ":", "\n", "            ", "start_wp_idx", "=", "p_tokenidx2wpidx", "[", "start_token_idx", "]", "[", "0", "]", "\n", "# Even though the p_tokenidx2wpidx is truncated, the wps might overflow", "\n", "end_wp_idx", "=", "min", "(", "p_tokenidx2wpidx", "[", "end_token_idx", "]", "[", "-", "1", "]", ",", "len", "(", "passage_wps", ")", "-", "1", ")", "\n", "passage_date_spanidxs", ".", "append", "(", "(", "start_wp_idx", ",", "end_wp_idx", ")", ")", "\n", "\n", "", "passage_date_idx2dateidx", "=", "[", "-", "1", "for", "_", "in", "range", "(", "len", "(", "passage_wps", ")", ")", "]", "\n", "if", "passage_date_spanidxs", ":", "\n", "            ", "for", "passage_date_span", ",", "date_idx", "in", "zip", "(", "passage_date_spanidxs", ",", "passage_date_entidxs", ")", ":", "\n", "                ", "(", "s", ",", "e", ")", "=", "passage_date_span", "\n", "passage_date_idx2dateidx", "[", "s", ":", "e", "+", "1", "]", "=", "[", "date_idx", "]", "*", "(", "e", "+", "1", "-", "s", ")", "\n", "", "", "else", ":", "\n", "            ", "passage_date_idx2dateidx", "[", "0", "]", "=", "0", "\n", "", "if", "passage_date_values", ":", "\n", "            ", "passage_date_objs", "=", "[", "Date", "(", "day", "=", "d", ",", "month", "=", "m", ",", "year", "=", "y", ")", "for", "(", "d", ",", "m", ",", "y", ")", "in", "passage_date_values", "]", "\n", "", "else", ":", "\n", "            ", "passage_date_objs", "=", "[", "Date", "(", "day", "=", "-", "1", ",", "month", "=", "-", "1", ",", "year", "=", "-", "1", ")", "]", "\n", "", "fields", "[", "\"passageidx2dateidx\"", "]", "=", "ArrayField", "(", "np", ".", "array", "(", "passage_date_idx2dateidx", ")", ",", "padding_value", "=", "-", "1", ")", "\n", "fields", "[", "\"passage_date_values\"", "]", "=", "MetadataField", "(", "passage_date_objs", ")", "\n", "passage_date_strvals", "=", "[", "str", "(", "d", ")", "for", "d", "in", "passage_date_objs", "]", "\n", "\n", "# year_differences: List[int]", "\n", "year_differences", ",", "year_differences_mat", "=", "self", ".", "get_year_difference_candidates", "(", "passage_date_objs", ")", "\n", "fields", "[", "\"year_differences\"", "]", "=", "MetadataField", "(", "year_differences", ")", "\n", "fields", "[", "\"year_differences_mat\"", "]", "=", "MetadataField", "(", "year_differences_mat", ")", "\n", "\n", "count_values", "=", "list", "(", "range", "(", "10", ")", ")", "\n", "fields", "[", "\"count_values\"", "]", "=", "MetadataField", "(", "count_values", ")", "\n", "\n", "metadata", ".", "update", "(", "\n", "{", "\n", "\"passage_token_offsets\"", ":", "passage_offsets", ",", "\n", "\"question_token_offsets\"", ":", "question_offsets", ",", "\n", "\"passage_wpidx2tokenidx\"", ":", "p_wpidx2tokenidx", ",", "\n", "\"question_wpidx2tokenidx\"", ":", "q_wpidx2tokenidx", ",", "\n", "\"question_tokens\"", ":", "question_wps", ",", "\n", "\"passage_tokens\"", ":", "passage_wps", ",", "\n", "\"question_orig_tokens\"", ":", "question_tokens", ",", "\n", "\"passage_orig_tokens\"", ":", "passage_tokens", ",", "\n", "\"passage_date_values\"", ":", "passage_date_strvals", ",", "\n", "\"composed_numbers\"", ":", "composed_numbers", ",", "\n", "\"passage_number_values\"", ":", "passage_number_values", ",", "\n", "\"passage_year_diffs\"", ":", "year_differences", ",", "\n", "\"count_values\"", ":", "count_values", ",", "\n", "\"qtype\"", ":", "qtype", "\n", "}", "\n", ")", "\n", "\n", "# FIELDS FOR STRONG-SUPERVISION", "\n", "fields", "[", "\"strongly_supervised\"", "]", "=", "MetadataField", "(", "strongly_supervised", ")", "\n", "fields", "[", "\"program_supervised\"", "]", "=", "MetadataField", "(", "program_supervised", ")", "\n", "fields", "[", "\"qattn_supervised\"", "]", "=", "MetadataField", "(", "qattn_supervised", ")", "\n", "fields", "[", "\"execution_supervised\"", "]", "=", "MetadataField", "(", "execution_supervised", ")", "\n", "fields", "[", "\"pattn_supervised\"", "]", "=", "MetadataField", "(", "pattn_supervised", ")", "\n", "fields", "[", "\"qtypes\"", "]", "=", "MetadataField", "(", "qtype", ")", "\n", "fields", "[", "\"synthetic_numground_metadata\"", "]", "=", "MetadataField", "(", "synthetic_numground_metadata", ")", "\n", "\n", "# Question Attention Supervision", "\n", "if", "ques_attn_supervision", ":", "\n", "            ", "ques_attn_supervision_wp", "=", "[", "]", "\n", "for", "qattn", "in", "ques_attn_supervision", ":", "\n", "                ", "qattn_wp", "=", "[", "0.0", "]", "*", "len", "(", "question_wps", ")", "\n", "for", "tokenidx", ",", "attnval", "in", "enumerate", "(", "qattn", ")", ":", "\n", "                    ", "wp_idxs", "=", "q_tokenidx2wpidx", "[", "tokenidx", "]", "\n", "for", "wpidx", "in", "wp_idxs", ":", "\n", "                        ", "if", "wpidx", "<", "len", "(", "question_wps", ")", ":", "\n", "                            ", "qattn_wp", "[", "wpidx", "]", "=", "attnval", "\n", "", "", "", "ques_attn_supervision_wp", ".", "append", "(", "qattn_wp", ")", "\n", "\n", "", "fields", "[", "\"qattn_supervision\"", "]", "=", "ArrayField", "(", "np", ".", "array", "(", "ques_attn_supervision_wp", ")", ",", "padding_value", "=", "0", ")", "\n", "", "else", ":", "\n", "            ", "empty_question_attention", "=", "[", "0.0", "]", "*", "len", "(", "question_wps", ")", "\n", "empty_question_attention_tuple", "=", "[", "empty_question_attention", "]", "\n", "fields", "[", "\"qattn_supervision\"", "]", "=", "ArrayField", "(", "np", ".", "array", "(", "empty_question_attention_tuple", ")", ",", "padding_value", "=", "0", ")", "\n", "\n", "", "if", "passage_attn_supervision", ":", "\n", "            ", "fields", "[", "\"passage_attn_supervision\"", "]", "=", "ArrayField", "(", "np", ".", "array", "(", "passage_attn_supervision", ")", ",", "padding_value", "=", "0", ")", "\n", "", "else", ":", "\n", "            ", "empty_passage_attention", "=", "[", "0.0", "]", "*", "len", "(", "passage_wps", ")", "\n", "fields", "[", "\"passage_attn_supervision\"", "]", "=", "ArrayField", "(", "np", ".", "array", "(", "empty_passage_attention", ")", ",", "padding_value", "=", "0", ")", "\n", "\n", "# Date-comparison - Date Grounding Supervision", "\n", "", "if", "date_grounding_supervision", ":", "\n", "            ", "fields", "[", "\"datecomp_ques_event_date_groundings\"", "]", "=", "MetadataField", "(", "date_grounding_supervision", ")", "\n", "", "else", ":", "\n", "            ", "empty_date_grounding", "=", "[", "0.0", "]", "*", "len", "(", "passage_date_objs", ")", "\n", "empty_date_grounding_tuple", "=", "(", "empty_date_grounding", ",", "empty_date_grounding", ")", "\n", "fields", "[", "\"datecomp_ques_event_date_groundings\"", "]", "=", "MetadataField", "(", "empty_date_grounding_tuple", ")", "\n", "\n", "# Number Comparison - Passage Number Grounding Supervision", "\n", "", "if", "num_grounding_supervision", ":", "\n", "# print(\"Num Grounding Sup: {}\".format(num_grounding_supervision))", "\n", "# number groundings need to be updated by padding with 0 for we now added implicit_numbers in passage nums", "\n", "            ", "num_implicit_nums", "=", "len", "(", "DropLanguage", ".", "implicit_numbers", ")", "\n", "new_num_grounding_supervision", "=", "[", "]", "\n", "for", "grounding_sup", "in", "num_grounding_supervision", ":", "\n", "                ", "grounding_sup", ".", "extend", "(", "[", "0", "]", "*", "num_implicit_nums", ")", "\n", "new_num_grounding_supervision", ".", "append", "(", "grounding_sup", ")", "\n", "# print(\"New num Grounding Sup: {}\".format(new_num_grounding_supervision))", "\n", "", "fields", "[", "\"numcomp_qspan_num_groundings\"", "]", "=", "MetadataField", "(", "new_num_grounding_supervision", ")", "\n", "", "else", ":", "\n", "            ", "empty_passagenum_grounding", "=", "[", "0.0", "]", "*", "len", "(", "passage_number_values", ")", "\n", "empty_passagenum_grounding_tuple", "=", "(", "empty_passagenum_grounding", ",", "empty_passagenum_grounding", ")", "\n", "fields", "[", "\"numcomp_qspan_num_groundings\"", "]", "=", "MetadataField", "(", "empty_passagenum_grounding_tuple", ")", "\n", "\n", "# Get gold action_seqs for strongly_supervised questions", "\n", "", "action2idx_map", "=", "{", "rule", ":", "i", "for", "i", ",", "rule", "in", "enumerate", "(", "language", ".", "all_possible_productions", "(", ")", ")", "}", "\n", "\n", "# Tuple[List[List[int]], List[List[int]]]", "\n", "(", "\n", "gold_action_seqs", ",", "\n", "gold_actionseq_masks", ",", "\n", "gold_program_start_types", ",", "\n", "program_supervised", ",", "\n", ")", "=", "self", ".", "get_gold_action_seqs", "(", "\n", "program_supervised", "=", "program_supervised", ",", "\n", "qtype", "=", "qtype", ",", "\n", "question_tokens", "=", "question_text", ".", "split", "(", "\" \"", ")", ",", "\n", "language", "=", "language", ",", "\n", "action2idx_map", "=", "action2idx_map", ",", "\n", ")", "\n", "fields", "[", "\"program_supervised\"", "]", "=", "MetadataField", "(", "program_supervised", ")", "\n", "fields", "[", "\"gold_action_seqs\"", "]", "=", "MetadataField", "(", "(", "gold_action_seqs", ",", "gold_actionseq_masks", ")", ")", "\n", "\n", "########     ANSWER FIELDS      ###################", "\n", "if", "answer_annotations", ":", "\n", "            ", "metadata", ".", "update", "(", "{", "\"answer_annotations\"", ":", "answer_annotations", "}", ")", "\n", "# Using the first one supervision (training actually only has one)", "\n", "answer_annotation", "=", "answer_annotations", "[", "0", "]", "\n", "\n", "# This list contains the possible-start-types for programs that can yield the correct answer", "\n", "# For example, if the answer is a number but also in passage, this will contain two keys", "\n", "# If the answer is a number, we'll find which kind and that program-start-type will be added here", "\n", "answer_program_start_types", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "\n", "# We've pre-parsed the span types to passage / question spans", "\n", "\n", "# Passage-span answer", "\n", "if", "answer_passage_spans", ":", "\n", "                ", "answer_program_start_types", ".", "append", "(", "\"passage_span\"", ")", "\n", "passage_span_fields", "=", "[", "]", "\n", "for", "(", "start_token_idx", ",", "end_token_idx", ")", "in", "answer_passage_spans", ":", "\n", "                    ", "start_wp_idx", "=", "p_tokenidx2wpidx", "[", "start_token_idx", "]", "[", "0", "]", "\n", "# Even though the p_tokenidx2wpidx is truncated, the wps might overflow", "\n", "end_wp_idx", "=", "min", "(", "p_tokenidx2wpidx", "[", "end_token_idx", "]", "[", "-", "1", "]", ",", "len", "(", "passage_wps", ")", "-", "1", ")", "\n", "passage_span_fields", ".", "append", "(", "SpanField", "(", "start_wp_idx", ",", "end_wp_idx", ",", "fields", "[", "\"passage\"", "]", ")", ")", "\n", "", "metadata", ".", "update", "(", "{", "\"answer_passage_spans\"", ":", "answer_passage_spans", "}", ")", "\n", "", "else", ":", "\n", "                ", "passage_span_fields", "=", "[", "SpanField", "(", "-", "1", ",", "-", "1", ",", "fields", "[", "\"passage\"", "]", ")", "]", "\n", "", "fields", "[", "\"answer_as_passage_spans\"", "]", "=", "ListField", "(", "passage_span_fields", ")", "\n", "\n", "# if answer_question_spans:", "\n", "#     answer_program_start_types.append(\"question_span\")", "\n", "#     question_span_fields = \\", "\n", "#         [SpanField(span[0], span[1], fields[\"question\"]) for span in answer_question_spans]", "\n", "#     metadata.update({'answer_question_spans': answer_question_spans})", "\n", "# else:", "\n", "#     question_span_fields = [SpanField(-1, -1, fields[\"question\"])]", "\n", "# fields[\"answer_as_question_spans\"] = ListField(question_span_fields)", "\n", "\n", "# Question-span answer", "\n", "question_span_fields", "=", "[", "SpanField", "(", "-", "1", ",", "-", "1", ",", "fields", "[", "\"question\"", "]", ")", "]", "\n", "fields", "[", "\"answer_as_question_spans\"", "]", "=", "ListField", "(", "question_span_fields", ")", "\n", "\n", "# Number answers", "\n", "number_answer_str", "=", "answer_annotation", "[", "\"number\"", "]", "\n", "if", "not", "number_answer_str", ":", "\n", "# Answer as number string does not exist.", "\n", "                ", "if", "self", ".", "convert_spananswer_to_num", ":", "\n", "# Try to convert \"X\" or \"X-yard(s)\" into number(X)", "\n", "                    ", "span_answer_text", "=", "None", "\n", "try", ":", "\n", "                        ", "span_answer_text", "=", "answer_annotation", "[", "\"spans\"", "]", "[", "0", "]", "\n", "span_answer_number", "=", "float", "(", "span_answer_text", ")", "\n", "", "except", ":", "\n", "                        ", "span_answer_number", "=", "None", "\n", "", "if", "span_answer_number", "is", "None", "and", "span_answer_text", "is", "not", "None", ":", "\n", "                        ", "split_hyphen", "=", "span_answer_text", ".", "split", "(", "\"-\"", ")", "\n", "if", "len", "(", "split_hyphen", ")", "==", "2", ":", "\n", "                            ", "try", ":", "\n", "                                ", "span_answer_number", "=", "float", "(", "split_hyphen", "[", "0", "]", ")", "\n", "", "except", ":", "\n", "                                ", "span_answer_number", "=", "None", "\n", "", "", "else", ":", "\n", "                            ", "span_answer_number", "=", "None", "\n", "", "", "if", "span_answer_number", "is", "not", "None", ":", "\n", "                        ", "answer_number", "=", "(", "\n", "int", "(", "span_answer_number", ")", "\n", "if", "int", "(", "span_answer_number", ")", "==", "span_answer_number", "\n", "else", "span_answer_number", "\n", ")", "\n", "", "else", ":", "\n", "                        ", "answer_number", "=", "None", "\n", "", "", "else", ":", "\n", "                    ", "answer_number", "=", "None", "\n", "", "", "else", ":", "\n", "                ", "answer_number", "=", "float", "(", "number_answer_str", ")", "\n", "answer_number", "=", "int", "(", "answer_number", ")", "if", "int", "(", "answer_number", ")", "==", "answer_number", "else", "answer_number", "\n", "\n", "", "ans_as_passage_number", "=", "[", "0", "]", "*", "len", "(", "passage_number_values", ")", "\n", "ans_as_composed_number", "=", "[", "0", "]", "*", "len", "(", "composed_numbers", ")", "\n", "ans_as_year_difference", "=", "[", "0", "]", "*", "len", "(", "year_differences", ")", "\n", "answer_as_count", "=", "[", "0", "]", "*", "len", "(", "count_values", ")", "\n", "composed_num_ans_composition_types", "=", "set", "(", ")", "\n", "if", "answer_number", "is", "not", "None", ":", "\n", "# Passage-number answer", "\n", "                ", "if", "answer_number", "in", "passage_number_values", ":", "\n", "                    ", "answer_program_start_types", ".", "append", "(", "\"passage_number\"", ")", "\n", "ans_as_passage_number", "[", "passage_number_values", ".", "index", "(", "answer_number", ")", "]", "=", "1", "\n", "# Composed-number answer", "\n", "", "if", "answer_number", "in", "composed_numbers", ":", "\n", "                    ", "answer_program_start_types", ".", "append", "(", "\"composed_number\"", ")", "\n", "ans_as_composed_number", "[", "composed_numbers", ".", "index", "(", "answer_number", ")", "]", "=", "1", "\n", "if", "answer_number", "in", "nums_from_addition", ":", "\n", "                        ", "composed_num_ans_composition_types", ".", "add", "(", "\"passage_num_addition\"", ")", "\n", "", "if", "answer_number", "in", "nums_from_subtraction", ":", "\n", "                        ", "composed_num_ans_composition_types", ".", "add", "(", "\"passage_num_subtraction\"", ")", "\n", "", "assert", "len", "(", "composed_num_ans_composition_types", ")", "!=", "0", "\n", "# Year-difference answer", "\n", "", "if", "answer_number", "in", "year_differences", ":", "\n", "                    ", "answer_program_start_types", ".", "append", "(", "\"year_difference\"", ")", "\n", "ans_as_year_difference", "[", "year_differences", ".", "index", "(", "answer_number", ")", "]", "=", "1", "\n", "# Count answer", "\n", "", "if", "answer_number", "in", "count_values", ":", "\n", "                    ", "answer_program_start_types", ".", "append", "(", "\"count_number\"", ")", "\n", "answer_as_count", "[", "count_values", ".", "index", "(", "answer_number", ")", "]", "=", "1", "\n", "\n", "", "", "fields", "[", "\"answer_as_passage_number\"", "]", "=", "MetadataField", "(", "ans_as_passage_number", ")", "\n", "fields", "[", "\"answer_as_composed_number\"", "]", "=", "MetadataField", "(", "ans_as_composed_number", ")", "\n", "fields", "[", "\"answer_as_year_difference\"", "]", "=", "MetadataField", "(", "ans_as_year_difference", ")", "\n", "fields", "[", "\"answer_as_count\"", "]", "=", "MetadataField", "(", "answer_as_count", ")", "\n", "fields", "[", "\"composed_num_ans_composition_types\"", "]", "=", "MetadataField", "(", "composed_num_ans_composition_types", ")", "\n", "\n", "fields", "[", "\"answer_program_start_types\"", "]", "=", "MetadataField", "(", "answer_program_start_types", ")", "\n", "\n", "# If we already have gold program(s), removing program_start_types that don't come from these gold_programs", "\n", "# print(f\"AnswerTypes: {answer_program_start_types}\")", "\n", "# print(f\"New AnswerTypes: {new_answer_program_start_types}\")", "\n", "\n", "if", "self", ".", "skip_due_to_gold_programs", ":", "\n", "                ", "if", "program_supervised", ":", "\n", "                    ", "new_answer_program_start_types", "=", "[", "]", "\n", "for", "answer_program_type", "in", "answer_program_start_types", ":", "\n", "                        ", "if", "answer_program_type", "in", "gold_program_start_types", ":", "\n", "                            ", "new_answer_program_start_types", ".", "append", "(", "answer_program_type", ")", "\n", "", "", "", "else", ":", "\n", "                    ", "new_answer_program_start_types", "=", "answer_program_start_types", "\n", "# Answer exists as other programs but not for gold-program", "\n", "", "if", "len", "(", "answer_program_start_types", ")", "!=", "0", "and", "len", "(", "new_answer_program_start_types", ")", "==", "0", ":", "\n", "                    ", "self", ".", "skip_due_to_gold_not_in_answer", "+=", "1", "\n", "", "answer_program_start_types", "=", "new_answer_program_start_types", "\n", "\n", "", "if", "self", ".", "skip_instances", ":", "\n", "                ", "if", "len", "(", "answer_program_start_types", ")", "==", "0", ":", "\n", "                    ", "self", ".", "skip_count", "+=", "1", "\n", "# print(\"\\nNo answer grounding\")", "\n", "# print(original_ques_text)", "\n", "# print(original_passage_text)", "\n", "# print(answer_annotation)", "\n", "# print(answer_passage_spans)", "\n", "# print(answer_question_spans)", "\n", "# print(f\"NumSupport: {number_support}\")", "\n", "return", "None", "\n", "# End if answer-annotation", "\n", "\n", "", "", "", "fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "metadata", ")", "\n", "return", "Instance", "(", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.prune_for_passage_len": [[772, 876], ["zip", "enumerate", "zip", "enumerate", "pruned_date_mens.append", "pruned_old_dateidxs.append", "len", "new_date_values.append", "new_dategrounding_supervision.append", "pruned_num_mens.append", "pruned_old_numidxs.append", "len", "new_num_values.append", "new_numgrounding_supervision.append", "len", "range", "range", "len", "len"], "methods", ["None"], ["", "def", "prune_for_passage_len", "(", "\n", "self", ",", "\n", "max_passage_len", ":", "int", ",", "\n", "p_date_mens", ",", "\n", "p_date_entidxs", ",", "\n", "p_date_normvals", ",", "\n", "p_num_mens", ",", "\n", "p_num_entidxs", ",", "\n", "p_num_normvals", ",", "\n", "answer_passage_spans", ",", "\n", "date_grounding_supervision", ",", "\n", "num_grounding_supervision", ",", "\n", "passage_attn_supervision", ",", "\n", ")", ":", "\n", "\n", "        ", "\"\"\" Prunes the passage and related data for a maximum length\n\n            For the given max_passage_len, we first need to find out the pruned date and number mentions\n            Since these might remove some dates and numbers from the passage, we need to find the\n            pruned list of p_date_normvals (p_date_entidxs with the new date_entidxs)\n            pruned list of p_num_normvals (p_num_entidxs with new num_entidxs) -- make sure the numbers are still sorted\n\n            answer_passage_spans - only spans that are contained in the pruned passage\n\n            date_grounding_supervision, num_grounding_supervision -- both these are the length of original dates/nums\n            we need to find the new value by pruning and mapping old ent idxs to new ones.\n\n            passage_attn_supervision: if not None, is a list the length of the passage\n        \"\"\"", "\n", "pruned_date_mens", "=", "[", "]", "# New passage date mens", "\n", "pruned_old_dateidxs", "=", "[", "]", "\n", "for", "date_men", ",", "date_idx", "in", "zip", "(", "p_date_mens", ",", "p_date_entidxs", ")", ":", "\n", "            ", "_", ",", "(", "x", ",", "y", ")", ",", "_", "=", "date_men", "\n", "if", "y", "<", "max_passage_len", ":", "\n", "                ", "pruned_date_mens", ".", "append", "(", "date_men", ")", "\n", "pruned_old_dateidxs", ".", "append", "(", "date_idx", ")", "\n", "\n", "", "", "new_date_values", "=", "[", "]", "# New passage date values", "\n", "new2old_dateidx", "=", "{", "}", "\n", "old2new_dateidx", "=", "{", "}", "\n", "for", "old_date_idx", ",", "date_value", "in", "enumerate", "(", "p_date_normvals", ")", ":", "\n", "# Atleast one mention of this old_date_idx remains", "\n", "            ", "if", "old_date_idx", "in", "pruned_old_dateidxs", ":", "\n", "                ", "new_date_idx", "=", "len", "(", "new_date_values", ")", "\n", "new2old_dateidx", "[", "new_date_idx", "]", "=", "old_date_idx", "\n", "old2new_dateidx", "[", "old_date_idx", "]", "=", "new_date_idx", "\n", "new_date_values", ".", "append", "(", "date_value", ")", "\n", "\n", "", "", "new_date_entidxs", "=", "[", "old2new_dateidx", "[", "x", "]", "for", "x", "in", "pruned_old_dateidxs", "]", "# New passage date entidxs", "\n", "\n", "if", "date_grounding_supervision", "is", "not", "None", ":", "\n", "            ", "new_dategrounding_supervision", "=", "[", "]", "\n", "for", "date_grounding", "in", "date_grounding_supervision", ":", "\n", "                ", "new_grounding", "=", "[", "date_grounding", "[", "new2old_dateidx", "[", "newidx", "]", "]", "for", "newidx", "in", "range", "(", "len", "(", "new_date_values", ")", ")", "]", "\n", "new_dategrounding_supervision", ".", "append", "(", "new_grounding", ")", "\n", "", "", "else", ":", "\n", "            ", "new_dategrounding_supervision", "=", "None", "\n", "\n", "# Pruning numbers", "\n", "", "pruned_num_mens", ",", "pruned_old_numidxs", "=", "[", "]", ",", "[", "]", "\n", "for", "num_men", ",", "num_idx", "in", "zip", "(", "p_num_mens", ",", "p_num_entidxs", ")", ":", "\n", "            ", "_", ",", "tokenidx", ",", "_", "=", "num_men", "\n", "if", "tokenidx", "<", "max_passage_len", ":", "\n", "                ", "pruned_num_mens", ".", "append", "(", "num_men", ")", "\n", "pruned_old_numidxs", ".", "append", "(", "num_idx", ")", "\n", "", "", "new_num_values", "=", "[", "]", "\n", "old2new_numidx", ",", "new2old_numidx", "=", "{", "}", ",", "{", "}", "\n", "for", "old_num_idx", ",", "num_value", "in", "enumerate", "(", "p_num_normvals", ")", ":", "\n", "            ", "if", "old_num_idx", "in", "pruned_old_numidxs", ":", "\n", "                ", "new_num_idx", "=", "len", "(", "new_num_values", ")", "\n", "old2new_numidx", "[", "old_num_idx", "]", "=", "new_num_idx", "\n", "new2old_numidx", "[", "new_num_idx", "]", "=", "old_num_idx", "\n", "new_num_values", ".", "append", "(", "num_value", ")", "\n", "", "", "new_num_idxs", "=", "[", "old2new_numidx", "[", "x", "]", "for", "x", "in", "pruned_old_numidxs", "]", "\n", "\n", "if", "num_grounding_supervision", "is", "not", "None", ":", "\n", "            ", "new_numgrounding_supervision", "=", "[", "]", "\n", "for", "num_grounding", "in", "num_grounding_supervision", ":", "\n", "                ", "new_grounding", "=", "[", "num_grounding", "[", "new2old_numidx", "[", "newidx", "]", "]", "for", "newidx", "in", "range", "(", "len", "(", "new_num_values", ")", ")", "]", "\n", "new_numgrounding_supervision", ".", "append", "(", "new_grounding", ")", "\n", "", "", "else", ":", "\n", "            ", "new_numgrounding_supervision", "=", "None", "\n", "\n", "", "if", "answer_passage_spans", ":", "\n", "            ", "new_answer_passage_spans", "=", "[", "span", "for", "span", "in", "answer_passage_spans", "if", "span", "[", "1", "]", "<", "max_passage_len", "]", "\n", "", "else", ":", "\n", "            ", "new_answer_passage_spans", "=", "answer_passage_spans", "\n", "\n", "", "if", "passage_attn_supervision", "is", "not", "None", "and", "len", "(", "passage_attn_supervision", ")", ">", "max_passage_len", ":", "\n", "            ", "new_passage_attn_supervision", "=", "passage_attn_supervision", "[", "0", ":", "max_passage_len", "]", "\n", "", "else", ":", "\n", "            ", "new_passage_attn_supervision", "=", "passage_attn_supervision", "\n", "\n", "", "return", "(", "\n", "pruned_date_mens", ",", "\n", "new_date_entidxs", ",", "\n", "new_date_values", ",", "\n", "pruned_num_mens", ",", "\n", "new_num_idxs", ",", "\n", "new_num_values", ",", "\n", "new_answer_passage_spans", ",", "\n", "new_dategrounding_supervision", ",", "\n", "new_numgrounding_supervision", ",", "\n", "new_passage_attn_supervision", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.prune_for_question_len": [[878, 890], ["None"], "methods", ["None"], ["", "def", "prune_for_question_len", "(", "self", ",", "max_question_len", ",", "answer_question_spans", ",", "ques_attn_supervision", ")", ":", "\n", "        ", "if", "answer_question_spans", ":", "\n", "            ", "new_answer_question_spans", "=", "[", "span", "for", "span", "in", "answer_question_spans", "if", "span", "[", "1", "]", "<", "max_question_len", "]", "\n", "", "else", ":", "\n", "            ", "new_answer_question_spans", "=", "answer_question_spans", "\n", "\n", "", "if", "ques_attn_supervision", "is", "not", "None", ":", "\n", "            ", "new_qattn_supervision", "=", "[", "qattn", "[", "0", ":", "max_question_len", "]", "for", "qattn", "in", "ques_attn_supervision", "]", "\n", "", "else", ":", "\n", "            ", "new_qattn_supervision", "=", "None", "\n", "\n", "", "return", "(", "new_answer_question_spans", ",", "new_qattn_supervision", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.compute_number_support": [[891, 960], ["set", "collections.defaultdict", "collections.defaultdict", "set", "set", "range", "sorted", "passagenums_w_implicitnums.extend", "itertools.product", "list", "enumerate", "sum", "set.add", "zip", "compnumber2addcombinations[].add", "set.add", "compnumber2subcombinations[].add", "set.add"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.add"], ["", "@", "staticmethod", "\n", "def", "compute_number_support", "(", "\n", "numbers", ":", "List", "[", "Union", "[", "int", ",", "float", "]", "]", ",", "\n", "implicit_numbers", ":", "List", "[", "Union", "[", "int", ",", "float", "]", "]", "=", "None", ",", "\n", "max_number_of_numbers_to_consider", ":", "int", "=", "2", ",", "\n", ")", "->", "Tuple", "[", "List", "[", "Union", "[", "int", ",", "float", "]", "]", ",", "List", "[", "Union", "[", "int", ",", "float", "]", "]", ",", "Dict", ",", "Dict", "]", ":", "\n", "        ", "\"\"\"Compute the number support based on combinations of input numbers.\n        This function considers all possible addition/subtraction between all pairs of numbers (even self). This forms\n        the support of the possible answers. The output is a sorted list of number support.\n\n        Args:\n            numbers: input numbers -- usually passage numbers\n            implicit_numbers: Extra numbers not part of the passage, but added in language. E.g. 100, 0\n            max_number_of_numbers_to_consider: number of numbers to consider to combine\n        Returns:\n            composed_numbers: List of output composed numbers (also includes implicit numbers)\n            compnumber2addcombinations: Dict[composed_number, Set(Tuple[passage_number, passage_number])]\n            compnumber2subcombinations: Dict[composed_number, Set(Tuple[passage_number, passage_number])]\n                Map from number to set of number combinations that can create it using the addition/sub operator.\n                For example, {2: set((1,1), (0,2))} is a valid entry for addcombinations\n        \"\"\"", "\n", "if", "max_number_of_numbers_to_consider", ">", "2", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "", "passagenums_w_implicitnums", "=", "[", "x", "for", "x", "in", "numbers", "]", "\n", "# Adding implicit numbers here after checking if 0 is a part of original numbers so that we don't add tons of", "\n", "#  combinations of the kind x = x + 0 / x - 0", "\n", "zero_in_passage", "=", "True", "if", "0", "in", "numbers", "else", "False", "\n", "# Adding implicit-numbers to the input-numbers list since they can take part in composition with input-numbers.", "\n", "if", "implicit_numbers", ":", "\n", "            ", "passagenums_w_implicitnums", ".", "extend", "(", "implicit_numbers", ")", "\n", "\n", "", "composed_num_set", "=", "set", "(", ")", "\n", "# Map from composed-number to list of number-combination that lead to this number from the add/sub operation", "\n", "compnumber2subcombinations", "=", "defaultdict", "(", "set", ")", "\n", "compnumber2addcombinations", "=", "defaultdict", "(", "set", ")", "\n", "nums_from_addition", "=", "set", "(", ")", "\n", "nums_from_subtraction", "=", "set", "(", ")", "\n", "signs", "=", "[", "-", "1", ",", "1", "]", "\n", "# all_sign_combinations = list(itertools.product(signs, repeat=2))", "\n", "# Since our modules will only perform num1-num2 / num1+num2. Computation like -num1+num2 would not be done", "\n", "all_sign_combinations", "=", "[", "(", "1.0", ",", "-", "1.0", ")", ",", "(", "1.0", ",", "1.0", ")", "]", "\n", "for", "number_of_numbers_to_consider", "in", "range", "(", "2", ",", "max_number_of_numbers_to_consider", "+", "1", ")", ":", "\n", "# for number_combination in itertools.combinations(numbers, r=number_of_numbers_to_consider):", "\n", "            ", "for", "indexed_number_combination", "in", "itertools", ".", "product", "(", "\n", "enumerate", "(", "passagenums_w_implicitnums", ")", ",", "repeat", "=", "number_of_numbers_to_consider", "\n", ")", ":", "\n", "                ", "(", "(", "idx1", ",", "num1", ")", ",", "(", "idx2", ",", "num2", ")", ")", "=", "indexed_number_combination", "\n", "number_combination", "=", "(", "num1", ",", "num2", ")", "\n", "# if idx1 == idx2: continue     # Commented: 0 in support. Un-commented: 0 not in support", "\n", "# print(indexed_number_combination)", "\n", "for", "sign_combination", "in", "all_sign_combinations", ":", "\n", "                    ", "value", "=", "sum", "(", "[", "sign", "*", "num", "for", "(", "sign", ",", "num", ")", "in", "zip", "(", "sign_combination", ",", "number_combination", ")", "]", ")", "\n", "if", "value", ">=", "0", ":", "\n", "# If 0 was originally in numbers then allow its combinations, o/w don't to avoid the", "\n", "# combinations from getting bloated with x = x+0, 0+x, x-0", "\n", "                        ", "if", "(", "0", "in", "number_combination", "and", "zero_in_passage", ")", "or", "(", "0", "not", "in", "number_combination", ")", ":", "\n", "                            ", "composed_num_set", ".", "add", "(", "value", ")", "\n", "if", "sign_combination", "==", "(", "1", ",", "1", ")", ":", "\n", "                                ", "compnumber2addcombinations", "[", "value", "]", ".", "add", "(", "number_combination", ")", "\n", "nums_from_addition", ".", "add", "(", "value", ")", "\n", "", "else", ":", "#  sign_combination == [1, -1]:", "\n", "                                ", "compnumber2subcombinations", "[", "value", "]", ".", "add", "(", "number_combination", ")", "\n", "nums_from_subtraction", ".", "add", "(", "value", ")", "\n", "\n", "", "", "", "", "", "", "composed_numbers", "=", "sorted", "(", "list", "(", "composed_num_set", ")", ")", "\n", "\n", "return", "(", "composed_numbers", ",", "passagenums_w_implicitnums", ",", "compnumber2addcombinations", ",", "compnumber2subcombinations", ",", "\n", "nums_from_addition", ",", "nums_from_subtraction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.make_addsub_combination_array": [[962, 986], ["max", "compnumber2numcombinations.items", "numpy.ones", "composed_numbers.index", "enumerate", "len", "compnumber2numcombinations.items", "passage_numbers.index", "passage_numbers.index", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "make_addsub_combination_array", "(", "\n", "composed_numbers", ":", "List", "[", "Union", "[", "int", ",", "float", "]", "]", ",", "passage_numbers", ":", "List", "[", "Union", "[", "int", ",", "float", "]", "]", ",", "\n", "compnumber2numcombinations", ":", "Dict", "[", "Union", "[", "int", ",", "float", "]", ",", "List", "[", "Tuple", "]", "]", "\n", ")", ":", "\n", "        ", "\"\"\"Make a (size_of_composed_numbers, max_num_combinations, 2) sized numpy array which would contain indices into\n        the composed_numbers list.\n\n        Each entry (i, :) will be a list of tuples, where (i, j)-th = x tuple would signifiy that\n        composed_number[i] = passage_number[x[0] OP passage_number[x[1]]\n\n        dim=1 will be padded to the max num of combinations possible for a number for this instance.\n        Later on this will further be padded based on instances in the batch.\n        \"\"\"", "\n", "max_num_combinations", "=", "max", "(", "len", "(", "combinations", ")", "for", "(", "_", ",", "combinations", ")", "in", "compnumber2numcombinations", ".", "items", "(", ")", ")", "\n", "number_combinations_indices", "=", "-", "1", "*", "np", ".", "ones", "(", "shape", "=", "(", "len", "(", "composed_numbers", ")", ",", "max_num_combinations", ",", "2", ")", ",", "\n", "dtype", "=", "np", ".", "int32", ")", "\n", "\n", "for", "composed_num", ",", "combinations", "in", "compnumber2numcombinations", ".", "items", "(", ")", ":", "\n", "            ", "compnumber_idx", "=", "composed_numbers", ".", "index", "(", "composed_num", ")", "\n", "for", "combination_num", ",", "(", "num1", ",", "num2", ")", "in", "enumerate", "(", "combinations", ")", ":", "\n", "                ", "(", "passagenum1idx", ",", "passagenum2idx", ")", "=", "(", "passage_numbers", ".", "index", "(", "num1", ")", ",", "passage_numbers", ".", "index", "(", "num2", ")", ")", "\n", "number_combinations_indices", "[", "compnumber_idx", ",", "combination_num", ",", ":", "]", "=", "[", "passagenum1idx", ",", "passagenum2idx", "]", "\n", "", "", "return", "number_combinations_indices", ",", "max_num_combinations", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.update_charoffsets": [[987, 1007], ["len", "char_offsets.append", "char_offsets.append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "update_charoffsets", "(", "wpidx2tokenidx", ",", "tokens", ",", "token_charidxs", ")", "->", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ":", "\n", "        ", "\"\"\"Char start and end (exclusive) offset for each word-piece against the original text.\n        The offsets for a word-piece are the offsets for the original token containing it.\n        Therefore, if systemic becomes system, ##ic then the offsets for both word-pieces will be the same\n\n        Returns:\n            char_offsets: List[(start, end(ex)] same length as number of word-pieces\n        \"\"\"", "\n", "# List of (start, end) char offsets for each passage and question token. (end exclusive)", "\n", "char_offsets", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "=", "[", "]", "\n", "for", "token_idx", "in", "wpidx2tokenidx", ":", "\n", "            ", "if", "token_idx", ">=", "0", ":", "\n", "                ", "token_len", "=", "len", "(", "tokens", "[", "token_idx", "]", ")", "\n", "# This is the start char offset for this token_idx", "\n", "token_start_charidx", "=", "token_charidxs", "[", "token_idx", "]", "\n", "char_offsets", ".", "append", "(", "(", "token_start_charidx", ",", "token_start_charidx", "+", "token_len", ")", ")", "\n", "", "else", ":", "\n", "                ", "char_offsets", ".", "append", "(", "(", "0", ",", "0", ")", ")", "\n", "", "", "return", "char_offsets", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.get_year_difference_candidates": [[1008, 1044], ["len", "itertools.product", "len", "numpy.zeros", "itertools.product", "date1.year_diff", "enumerate", "date1.year_diff", "year_differences.index", "year_differences.append"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.Date.year_diff", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.Date.year_diff"], ["", "@", "staticmethod", "\n", "def", "get_year_difference_candidates", "(", "passage_date_objs", ":", "List", "[", "Date", "]", ")", "->", "Tuple", "[", "List", "[", "int", "]", ",", "np", ".", "array", "]", ":", "\n", "        ", "\"\"\" List of integers indicating all-possible year differences between the passage-dates\n            If year difference is not defined (year = -1) or negative, we don't consider such date-combinations\n\n            Returns the following:\n\n            Returns:\n            ---------\n            year_differences:\n                List[int] These are the possible year differences.\n            year_difference_mat: Binary np.array of shape (D, D, y_d)\n                Entry (i, j, k) == 1 denotes that D[i] - D[j] == year_differences[k]\n        \"\"\"", "\n", "num_date_objs", "=", "len", "(", "passage_date_objs", ")", "\n", "# Adding zero-first since it'll definitely be added and makes sanity-checking easy", "\n", "year_differences", ":", "List", "[", "int", "]", "=", "[", "0", "]", "\n", "\n", "# If any year is -1, we consider the year difference to be 0", "\n", "# If the year difference is negative, we consider the difference to be 0", "\n", "for", "(", "date1", ",", "date2", ")", "in", "itertools", ".", "product", "(", "passage_date_objs", ",", "repeat", "=", "2", ")", ":", "\n", "            ", "year_diff", "=", "date1", ".", "year_diff", "(", "date2", ")", "\n", "if", "year_diff", ">=", "0", ":", "\n", "                ", "if", "year_diff", "not", "in", "year_differences", ":", "\n", "                    ", "year_differences", ".", "append", "(", "year_diff", ")", "\n", "\n", "", "", "", "num_of_year_differences", "=", "len", "(", "year_differences", ")", "\n", "# Making year_difference_mat", "\n", "year_difference_mat", "=", "np", ".", "zeros", "(", "shape", "=", "(", "num_date_objs", ",", "num_date_objs", ",", "num_of_year_differences", ")", ",", "dtype", "=", "int", ")", "\n", "for", "(", "(", "date_idx1", ",", "date1", ")", ",", "(", "date_idx2", ",", "date2", ")", ")", "in", "itertools", ".", "product", "(", "enumerate", "(", "passage_date_objs", ")", ",", "repeat", "=", "2", ")", ":", "\n", "            ", "year_diff", "=", "date1", ".", "year_diff", "(", "date2", ")", "\n", "if", "year_diff", ">=", "0", ":", "\n", "                ", "year_diff_idx", "=", "year_differences", ".", "index", "(", "year_diff", ")", "# We know this will not fail", "\n", "year_difference_mat", "[", "date_idx1", ",", "date_idx2", ",", "year_diff_idx", "]", "=", "1", "\n", "\n", "", "", "return", "year_differences", ",", "year_difference_mat", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.get_gold_action_seqs": [[1045, 1105], ["gold_actionseq_idxs.append", "gold_actionseq_mask.append", "gold_start_types.append", "gold_actionseq_idxs.append", "gold_actionseq_mask.append", "gold_start_types.append", "logger.error", "len", "language.logical_form_to_action_sequence", "gold_actionseq_idxs.append", "gold_actionseq_mask.append", "range", "len"], "methods", ["None"], ["", "def", "get_gold_action_seqs", "(", "\n", "self", ",", "\n", "program_supervised", ":", "bool", ",", "\n", "qtype", ":", "str", ",", "\n", "question_tokens", ":", "List", "[", "str", "]", ",", "\n", "language", ":", "DropLanguage", ",", "\n", "action2idx_map", ":", "Dict", "[", "str", ",", "int", "]", ",", "\n", ")", "->", "Tuple", "[", "List", "[", "List", "[", "int", "]", "]", ",", "List", "[", "List", "[", "int", "]", "]", ",", "List", "[", "str", "]", ",", "bool", "]", ":", "\n", "\n", "        ", "qtype_to_lffunc", "=", "{", "\n", "constants", ".", "DATECOMP_QTYPE", ":", "self", ".", "datecomp_logicalforms", ",", "\n", "constants", ".", "NUMCOMP_QTYPE", ":", "self", ".", "numcomp_logicalforms", ",", "\n", "constants", ".", "NUM_find_qtype", ":", "self", ".", "findnum_logicalforms", ",", "\n", "constants", ".", "NUM_filter_find_qtype", ":", "self", ".", "filterfindnum_logicalforms", ",", "\n", "constants", ".", "MIN_find_qtype", ":", "self", ".", "minnum_find_logicalforms", ",", "\n", "constants", ".", "MIN_filter_find_qtype", ":", "self", ".", "minnum_filterfind_logicalforms", ",", "\n", "constants", ".", "MAX_find_qtype", ":", "self", ".", "maxnum_find_logicalforms", ",", "\n", "constants", ".", "MAX_filter_find_qtype", ":", "self", ".", "maxnum_filterfind_logicalforms", ",", "\n", "constants", ".", "COUNT_find_qtype", ":", "self", ".", "count_find_logicalforms", ",", "\n", "constants", ".", "COUNT_filter_find_qtype", ":", "self", ".", "count_filterfind_logicalforms", ",", "\n", "constants", ".", "RELOC_find_qtype", ":", "self", ".", "relocate_logicalforms", ",", "\n", "constants", ".", "RELOC_filterfind_qtype", ":", "self", ".", "relocate_logicalforms", ",", "\n", "constants", ".", "RELOC_maxfind_qtype", ":", "self", ".", "relocate_logicalforms", ",", "\n", "constants", ".", "RELOC_maxfilterfind_qtype", ":", "self", ".", "relocate_logicalforms", ",", "\n", "constants", ".", "RELOC_minfind_qtype", ":", "self", ".", "relocate_logicalforms", ",", "\n", "constants", ".", "RELOC_minfilterfind_qtype", ":", "self", ".", "relocate_logicalforms", ",", "\n", "constants", ".", "YEARDIFF_SE_qtype", ":", "self", ".", "yeardiff_singleevent_logicalforms", ",", "\n", "constants", ".", "YEARDIFF_TE_qtype", ":", "self", ".", "yeardiff_twoevent_logicalforms", ",", "\n", "}", "\n", "\n", "gold_actionseq_idxs", ":", "List", "[", "List", "[", "int", "]", "]", "=", "[", "]", "\n", "gold_actionseq_mask", ":", "List", "[", "List", "[", "int", "]", "]", "=", "[", "]", "\n", "gold_start_types", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "\n", "if", "not", "program_supervised", ":", "\n", "            ", "gold_actionseq_idxs", ".", "append", "(", "[", "0", "]", ")", "\n", "gold_actionseq_mask", ".", "append", "(", "[", "0", "]", ")", "\n", "gold_start_types", ".", "append", "(", "\"UNK\"", ")", "\n", "return", "(", "gold_actionseq_idxs", ",", "gold_actionseq_mask", ",", "gold_start_types", ",", "program_supervised", ")", "\n", "\n", "", "if", "qtype", "in", "qtype_to_lffunc", ":", "\n", "# Tuple[List[str], List[str]]", "\n", "            ", "(", "gold_logical_forms", ",", "gold_start_types", ")", "=", "qtype_to_lffunc", "[", "qtype", "]", "(", "\n", "question_tokens", "=", "question_tokens", ",", "language", "=", "language", ",", "qtype", "=", "qtype", "\n", ")", "\n", "assert", "len", "(", "gold_logical_forms", ")", ">=", "1", ",", "f\"No logical forms found for: {question_tokens}\"", "\n", "for", "logical_form", "in", "gold_logical_forms", ":", "\n", "                ", "gold_actions", ":", "List", "[", "str", "]", "=", "language", ".", "logical_form_to_action_sequence", "(", "logical_form", ")", "\n", "actionseq_idxs", ":", "List", "[", "int", "]", "=", "[", "action2idx_map", "[", "a", "]", "for", "a", "in", "gold_actions", "]", "\n", "actionseq_mask", ":", "List", "[", "int", "]", "=", "[", "1", "for", "_", "in", "range", "(", "len", "(", "actionseq_idxs", ")", ")", "]", "\n", "gold_actionseq_idxs", ".", "append", "(", "actionseq_idxs", ")", "\n", "gold_actionseq_mask", ".", "append", "(", "actionseq_mask", ")", "\n", "", "", "else", ":", "\n", "            ", "program_supervised", "=", "False", "\n", "gold_actionseq_idxs", ".", "append", "(", "[", "0", "]", ")", "\n", "gold_actionseq_mask", ".", "append", "(", "[", "0", "]", ")", "\n", "gold_start_types", ".", "append", "(", "\"UNK\"", ")", "\n", "logger", ".", "error", "(", "f\"Tried get gold logical form for: {qtype}\"", ")", "\n", "\n", "", "return", "(", "gold_actionseq_idxs", ",", "gold_actionseq_mask", ",", "gold_start_types", ",", "program_supervised", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.filter_passageattn_lf": [[1106, 1110], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "filter_passageattn_lf", "(", ")", "->", "str", ":", "\n", "        ", "gold_lf", "=", "\"(filter_PassageAttention find_PassageAttention)\"", "\n", "return", "gold_lf", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.findnum_logicalforms": [[1111, 1115], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "findnum_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "gold_lf", "=", "\"(find_PassageNumber find_PassageAttention)\"", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"passage_number\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.filterfindnum_logicalforms": [[1116, 1121], ["drop_reader_bert.DROPReaderNew.filter_passageattn_lf"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.filter_passageattn_lf"], ["", "@", "staticmethod", "\n", "def", "filterfindnum_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "filter_passage_attention_lf", "=", "DROPReaderNew", ".", "filter_passageattn_lf", "(", ")", "\n", "gold_lf", "=", "f\"(find_PassageNumber {filter_passage_attention_lf})\"", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"passage_number\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.minnum_find_logicalforms": [[1122, 1126], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "minnum_find_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "gold_lf", "=", "f\"(find_PassageNumber (minNumPattn find_PassageAttention))\"", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"passage_number\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.minnum_filterfind_logicalforms": [[1127, 1132], ["drop_reader_bert.DROPReaderNew.filter_passageattn_lf"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.filter_passageattn_lf"], ["", "@", "staticmethod", "\n", "def", "minnum_filterfind_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "filter_passage_attention_lf", "=", "DROPReaderNew", ".", "filter_passageattn_lf", "(", ")", "\n", "gold_lf", "=", "f\"(find_PassageNumber (minNumPattn {filter_passage_attention_lf}))\"", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"passage_number\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.maxnum_find_logicalforms": [[1133, 1137], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "maxnum_find_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "gold_lf", "=", "f\"(find_PassageNumber (maxNumPattn find_PassageAttention))\"", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"passage_number\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.maxnum_filterfind_logicalforms": [[1138, 1143], ["drop_reader_bert.DROPReaderNew.filter_passageattn_lf"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.filter_passageattn_lf"], ["", "@", "staticmethod", "\n", "def", "maxnum_filterfind_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "filter_passage_attention_lf", "=", "DROPReaderNew", ".", "filter_passageattn_lf", "(", ")", "\n", "gold_lf", "=", "f\"(find_PassageNumber (maxNumPattn {filter_passage_attention_lf}))\"", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"passage_number\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.count_find_logicalforms": [[1144, 1148], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "count_find_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "gold_lf", "=", "\"(passageAttn2Count find_PassageAttention)\"", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"count_number\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.count_filterfind_logicalforms": [[1149, 1154], ["drop_reader_bert.DROPReaderNew.filter_passageattn_lf"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.filter_passageattn_lf"], ["", "@", "staticmethod", "\n", "def", "count_filterfind_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "filter_passageattn_lf", "=", "DROPReaderNew", ".", "filter_passageattn_lf", "(", ")", "\n", "gold_lf", "=", "f\"(passageAttn2Count {filter_passageattn_lf})\"", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"count_number\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.relocate_logicalforms": [[1155, 1188], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "relocate_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "qtype", "=", "kwargs", "[", "\"qtype\"", "]", "\n", "# Could be one of", "\n", "# 'relocate_filterfind_qtype', 'relocate_minfind_qtype', 'relocate_maxfind_qtype',", "\n", "# 'relocate_maxfilterfind_qtype', 'relocate_find_qtype', 'relocate_minfilterfind_qtype'", "\n", "\n", "find", "=", "\"find_PassageAttention\"", "\n", "filterfind", "=", "\"(filter_PassageAttention find_PassageAttention)\"", "\n", "maxfind", "=", "\"(maxNumPattn find_PassageAttention)\"", "\n", "maxfilterfind", "=", "f\"(maxNumPattn {filterfind})\"", "\n", "minfind", "=", "\"(minNumPattn find_PassageAttention)\"", "\n", "minfilterfind", "=", "f\"(minNumPattn {filterfind})\"", "\n", "\n", "outer_leftside", "=", "\"(find_passageSpanAnswer (relocate_PassageAttention \"", "\n", "outer_rightside", "=", "\"))\"", "\n", "\n", "if", "qtype", "==", "constants", ".", "RELOC_find_qtype", ":", "\n", "            ", "gold_lf", "=", "outer_leftside", "+", "find", "+", "outer_rightside", "\n", "", "elif", "qtype", "==", "constants", ".", "RELOC_filterfind_qtype", ":", "\n", "            ", "gold_lf", "=", "outer_leftside", "+", "filterfind", "+", "outer_rightside", "\n", "", "elif", "qtype", "==", "constants", ".", "RELOC_maxfind_qtype", ":", "\n", "            ", "gold_lf", "=", "outer_leftside", "+", "maxfind", "+", "outer_rightside", "\n", "", "elif", "qtype", "==", "constants", ".", "RELOC_maxfilterfind_qtype", ":", "\n", "            ", "gold_lf", "=", "outer_leftside", "+", "maxfilterfind", "+", "outer_rightside", "\n", "", "elif", "qtype", "==", "constants", ".", "RELOC_minfind_qtype", ":", "\n", "            ", "gold_lf", "=", "outer_leftside", "+", "minfind", "+", "outer_rightside", "\n", "", "elif", "qtype", "==", "constants", ".", "RELOC_minfilterfind_qtype", ":", "\n", "            ", "gold_lf", "=", "outer_leftside", "+", "minfilterfind", "+", "outer_rightside", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "", "return", "[", "gold_lf", "]", ",", "[", "\"passage_span\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.yeardiff_singleevent_logicalforms": [[1189, 1195], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "yeardiff_singleevent_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "qtype", "=", "kwargs", "[", "\"qtype\"", "]", "\n", "gold_lf", "=", "\"(year_difference_single_event find_PassageAttention)\"", "\n", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"year_difference\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.yeardiff_twoevent_logicalforms": [[1196, 1202], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "yeardiff_twoevent_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "qtype", "=", "kwargs", "[", "\"qtype\"", "]", "\n", "gold_lf", "=", "\"(year_difference find_PassageAttention find_PassageAttention)\"", "\n", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"year_difference\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.numdiff_logicalforms": [[1203, 1237], ["qtype.split"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "numdiff_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "qtype", "=", "kwargs", "[", "\"qtype\"", "]", "\n", "# Qtype of form: diff_maxmin_qtype", "\n", "numtypes", "=", "qtype", ".", "split", "(", "\"_\"", ")", "[", "1", "]", "\n", "first_num", "=", "numtypes", "[", "0", ":", "3", "]", "# first 3 chars", "\n", "second_num", "=", "numtypes", "[", "3", ":", "6", "]", "# last 3 chars", "\n", "\n", "max_num_program", "=", "\"(max_PassageNumber (find_PassageNumber find_PassageAttention))\"", "\n", "min_num_program", "=", "\"(min_PassageNumber (find_PassageNumber find_PassageAttention))\"", "\n", "find_num_program", "=", "\"(find_PassageNumber find_PassageAttention)\"", "\n", "\n", "if", "first_num", "==", "\"max\"", ":", "\n", "            ", "first_num_prog", "=", "max_num_program", "\n", "", "elif", "first_num", "==", "\"min\"", ":", "\n", "            ", "first_num_prog", "=", "min_num_program", "\n", "", "elif", "first_num", "==", "\"num\"", ":", "\n", "            ", "first_num_prog", "=", "find_num_program", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "", "if", "second_num", "==", "\"max\"", ":", "\n", "            ", "second_num_prog", "=", "max_num_program", "\n", "", "elif", "second_num", "==", "\"min\"", ":", "\n", "            ", "second_num_prog", "=", "min_num_program", "\n", "", "elif", "second_num", "==", "\"num\"", ":", "\n", "            ", "second_num_prog", "=", "find_num_program", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "# \"(passagenumber_difference first_num_prog second_num_program)\"", "\n", "", "gold_lf", "=", "f\"(passagenumber_difference {first_num_prog} {second_num_prog})\"", "\n", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"passagenum_diff\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.yardsshortest_logicalforms": [[1238, 1242], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "yardsshortest_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "gold_lf", "=", "\"(min_PassageNumber (find_PassageNumber find_PassageAttention))\"", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"passage_number\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.yardslongest_logicalforms": [[1243, 1247], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "yardslongest_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "gold_lf", "=", "\"(max_PassageNumber (find_PassageNumber find_PassageAttention))\"", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"passage_number\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.datecomp_logicalforms": [[1248, 1290], ["language.all_possible_productions", "gold_logical_forms.append", "gold_start_types.append", "language.all_possible_productions", "gold_logical_forms.append", "gold_start_types.append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "datecomp_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "question_tokens", ":", "List", "[", "str", "]", "=", "kwargs", "[", "\"question_tokens\"", "]", "\n", "language", ":", "DropLanguage", "=", "kwargs", "[", "\"language\"", "]", "\n", "# \"(find_passageSpanAnswer (compare_date_greater_than find_PassageAttention find_PassageAttention))\"", "\n", "psa_start", "=", "\"(find_passageSpanAnswer (\"", "\n", "qsa_start", "=", "\"(find_questionSpanAnswer (\"", "\n", "# lf1 = \"(find_passageSpanAnswer (\"", "\n", "\n", "lf2", "=", "\" find_PassageAttention find_PassageAttention))\"", "\n", "greater_than", "=", "\"compare_date_greater_than\"", "\n", "lesser_than", "=", "\"compare_date_lesser_than\"", "\n", "\n", "# Correct if Attn1 is first event", "\n", "lesser_tokens", "=", "[", "\"first\"", ",", "\"earlier\"", ",", "\"forst\"", ",", "\"firts\"", "]", "\n", "greater_tokens", "=", "[", "\"later\"", ",", "\"last\"", ",", "\"second\"", "]", "\n", "\n", "operator_action", "=", "None", "\n", "\n", "for", "t", "in", "lesser_tokens", ":", "\n", "            ", "if", "t", "in", "question_tokens", ":", "\n", "                ", "operator_action", "=", "lesser_than", "\n", "break", "\n", "\n", "", "", "for", "t", "in", "greater_tokens", ":", "\n", "            ", "if", "t", "in", "question_tokens", ":", "\n", "                ", "operator_action", "=", "greater_than", "\n", "break", "\n", "\n", "", "", "if", "operator_action", "is", "None", ":", "\n", "            ", "operator_action", "=", "greater_than", "\n", "\n", "", "gold_logical_forms", "=", "[", "]", "\n", "gold_start_types", "=", "[", "]", "\n", "if", "\"@start@ -> PassageSpanAnswer\"", "in", "language", ".", "all_possible_productions", "(", ")", ":", "\n", "            ", "gold_logical_forms", ".", "append", "(", "f\"{psa_start}{operator_action}{lf2}\"", ")", "\n", "gold_start_types", ".", "append", "(", "\"passage_span\"", ")", "# from drop_parser.get_valid_start_actionids", "\n", "", "if", "\"@start@ -> QuestionSpanAnswer\"", "in", "language", ".", "all_possible_productions", "(", ")", ":", "\n", "            ", "gold_logical_forms", ".", "append", "(", "f\"{qsa_start}{operator_action}{lf2}\"", ")", "\n", "gold_start_types", ".", "append", "(", "\"question_span\"", ")", "# from drop_parser.get_valid_start_actionids", "\n", "\n", "", "return", "gold_logical_forms", ",", "gold_start_types", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.DROPReaderNew.numcomp_logicalforms": [[1291, 1334], ["language.all_possible_productions", "gold_logical_forms.append", "gold_start_types.append", "language.all_possible_productions", "gold_logical_forms.append", "gold_start_types.append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "numcomp_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "question_tokens", ":", "List", "[", "str", "]", "=", "kwargs", "[", "\"question_tokens\"", "]", "\n", "language", ":", "DropLanguage", "=", "kwargs", "[", "\"language\"", "]", "\n", "# \"(find_passageSpanAnswer (compare_date_greater_than find_PassageAttention find_PassageAttention))\"", "\n", "psa_start", "=", "\"(find_passageSpanAnswer (\"", "\n", "qsa_start", "=", "\"(find_questionSpanAnswer (\"", "\n", "\n", "lf2", "=", "\" find_PassageAttention find_PassageAttention))\"", "\n", "greater_than", "=", "\"compare_num_greater_than\"", "\n", "lesser_than", "=", "\"compare_num_lesser_than\"", "\n", "\n", "# Correct if Attn1 is first event", "\n", "greater_tokens", "=", "[", "\"larger\"", ",", "\"more\"", ",", "\"largest\"", ",", "\"bigger\"", ",", "\"higher\"", ",", "\"highest\"", ",", "\"most\"", ",", "\"greater\"", "]", "\n", "lesser_tokens", "=", "[", "\"smaller\"", ",", "\"fewer\"", ",", "\"lowest\"", ",", "\"smallest\"", ",", "\"less\"", ",", "\"least\"", ",", "\"fewest\"", ",", "\"lower\"", "]", "\n", "\n", "operator_action", "=", "None", "\n", "\n", "for", "t", "in", "lesser_tokens", ":", "\n", "            ", "if", "t", "in", "question_tokens", ":", "\n", "                ", "operator_action", "=", "lesser_than", "\n", "break", "\n", "# return [f\"{psa_start}{lesser_than}{lf2}\", f\"{qsa_start}{lesser_than}{lf2}\"]", "\n", "", "", "if", "operator_action", "is", "None", ":", "\n", "            ", "for", "t", "in", "greater_tokens", ":", "\n", "                ", "if", "t", "in", "question_tokens", ":", "\n", "                    ", "operator_action", "=", "greater_than", "\n", "break", "\n", "# return [f\"{psa_start}{greater_than}{lf2}\", f\"{qsa_start}{greater_than}{lf2}\"]", "\n", "\n", "", "", "", "if", "operator_action", "is", "None", ":", "\n", "            ", "operator_action", "=", "greater_than", "\n", "\n", "", "gold_logical_forms", "=", "[", "]", "\n", "gold_start_types", "=", "[", "]", "\n", "if", "\"@start@ -> PassageSpanAnswer\"", "in", "language", ".", "all_possible_productions", "(", ")", ":", "\n", "            ", "gold_logical_forms", ".", "append", "(", "f\"{psa_start}{operator_action}{lf2}\"", ")", "\n", "gold_start_types", ".", "append", "(", "\"passage_span\"", ")", "# from drop_parser.get_valid_start_actionids", "\n", "", "if", "\"@start@ -> QuestionSpanAnswer\"", "in", "language", ".", "all_possible_productions", "(", ")", ":", "\n", "            ", "gold_logical_forms", ".", "append", "(", "f\"{qsa_start}{operator_action}{lf2}\"", ")", "\n", "gold_start_types", ".", "append", "(", "\"question_span\"", ")", "# from drop_parser.get_valid_start_actionids", "\n", "\n", "", "return", "gold_logical_forms", ",", "gold_start_types", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader_bert.tokenize_bert": [[52, 77], ["enumerate", "len", "bert_tokenizer.tokenize", "wordpiece_tokens.extend", "wpidx2tokenidx.extend", "tokenidx2wpidxs.append", "len", "len", "len", "len", "len", "enumerate"], "function", ["None"], ["def", "tokenize_bert", "(", "bert_tokenizer", ":", "BertTokenizer", ",", "tokens", ":", "List", "[", "str", "]", ")", ":", "\n", "    ", "\"\"\"Word-piece tokenize input tokens.\n\n    Returns:\n        wordpiece_tokens: List[str] word-pieces\n        tokenidx2wpidxs: List[List[int]] For each original-tokenidx, a list of indices for its corresponding wps\n        wpidx2tokenidx: List[int] Same length as wordpiece_tokens; index of original\n\n    \"\"\"", "\n", "tokenidx2wpidxs", ":", "List", "[", "List", "[", "int", "]", "]", "=", "[", "]", "\n", "wpidx2tokenidx", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "wordpiece_tokens", "=", "[", "]", "\n", "for", "tokenidx", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "        ", "wp_idx", "=", "len", "(", "wordpiece_tokens", ")", "\n", "wps", "=", "bert_tokenizer", ".", "tokenize", "(", "token", ")", "\n", "wordpiece_tokens", ".", "extend", "(", "wps", ")", "\n", "wpidx2tokenidx", ".", "extend", "(", "[", "tokenidx", "]", "*", "len", "(", "wps", ")", ")", "\n", "# Word-piece idxs for this token, wp_idx is the idx for the first token, and +i for the number of wps", "\n", "wp_idxs", "=", "[", "wp_idx", "+", "i", "for", "i", ",", "_", "in", "enumerate", "(", "wps", ")", "]", "\n", "tokenidx2wpidxs", ".", "append", "(", "wp_idxs", ")", "\n", "\n", "", "assert", "len", "(", "wordpiece_tokens", ")", "==", "len", "(", "wpidx2tokenidx", ")", "\n", "assert", "len", "(", "tokens", ")", "==", "len", "(", "tokenidx2wpidxs", ")", "\n", "\n", "return", "wordpiece_tokens", ",", "tokenidx2wpidxs", ",", "wpidx2tokenidx", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.__init__": [[51, 78], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "lazy", ":", "bool", "=", "True", ",", "\n", "tokenizer", ":", "Tokenizer", "=", "None", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "relaxed_span_match", ":", "bool", "=", "True", ",", "\n", "do_augmentation", ":", "bool", "=", "True", ",", "\n", "passage_length_limit", ":", "int", "=", "None", ",", "\n", "question_length_limit", ":", "int", "=", "None", ",", "\n", "only_strongly_supervised", ":", "bool", "=", "False", ",", "\n", "skip_instances", "=", "False", ",", "\n", "skip_due_to_gold_programs", "=", "False", ",", "\n", "convert_spananswer_to_num", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "lazy", ")", "\n", "self", ".", "_tokenizer", "=", "tokenizer", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "_relaxed_span_match", "=", "relaxed_span_match", "\n", "self", ".", "_do_augmentation", "=", "do_augmentation", "\n", "self", ".", "passage_length_limit", "=", "passage_length_limit", "\n", "self", ".", "question_length_limit", "=", "question_length_limit", "\n", "self", ".", "only_strongly_supervised", "=", "only_strongly_supervised", "\n", "self", ".", "skip_instances", "=", "skip_instances", "\n", "self", ".", "skip_due_to_gold_programs", "=", "skip_due_to_gold_programs", "\n", "self", ".", "convert_spananswer_to_num", "=", "convert_spananswer_to_num", "\n", "self", ".", "skip_count", "=", "0", "\n", "self", ".", "skip_due_to_gold_not_in_answer", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew._read": [[79, 231], ["allennlp.common.file_utils.cached_path", "logger.info", "logger.info", "json.load.items", "logger.info", "logger.info", "logger.info", "open", "json.load", "drop_reader.DROPReaderNew.text_to_instance", "answer_annotations.append", "print", "print", "print", "exit"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "        ", "self", ".", "skip_count", "=", "0", "\n", "# pylint: disable=logging-fstring-interpolation", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "logger", ".", "info", "(", "\"Reading file at %s\"", ",", "file_path", ")", "\n", "with", "open", "(", "file_path", ")", "as", "dataset_file", ":", "\n", "            ", "dataset", "=", "json", ".", "load", "(", "dataset_file", ")", "\n", "", "logger", ".", "info", "(", "\"Reading the dataset\"", ")", "\n", "instances", ",", "skip_count", "=", "[", "]", ",", "0", "\n", "max_passage_len", "=", "self", ".", "passage_length_limit", "\n", "max_question_len", "=", "self", ".", "question_length_limit", "\n", "total_qas", "=", "0", "\n", "instances_read", "=", "0", "\n", "for", "passage_id", ",", "passage_info", "in", "dataset", ".", "items", "(", ")", ":", "\n", "            ", "original_passage_text", "=", "passage_info", "[", "constants", ".", "cleaned_passage", "]", "\n", "passage_text", "=", "passage_info", "[", "constants", ".", "tokenized_passage", "]", "\n", "passage_charidxs", "=", "passage_info", "[", "constants", ".", "passage_charidxs", "]", "\n", "p_date_mens", ":", "List", "[", "Tuple", "[", "str", ",", "Tuple", "[", "int", ",", "int", "]", ",", "Tuple", "[", "int", ",", "int", ",", "int", "]", "]", "]", "=", "passage_info", "[", "\n", "constants", ".", "passage_date_mens", "\n", "]", "\n", "p_date_entidxs", ":", "List", "[", "int", "]", "=", "passage_info", "[", "constants", ".", "passage_date_entidx", "]", "\n", "p_date_normvals", ":", "List", "[", "Tuple", "[", "int", ",", "int", ",", "int", "]", "]", "=", "passage_info", "[", "constants", ".", "passage_date_normalized_values", "]", "\n", "\n", "p_num_mens", ":", "List", "[", "Tuple", "[", "str", ",", "int", ",", "int", "]", "]", "=", "passage_info", "[", "constants", ".", "passage_num_mens", "]", "\n", "p_num_entidxs", ":", "List", "[", "int", "]", "=", "passage_info", "[", "constants", ".", "passage_num_entidx", "]", "\n", "p_num_normvals", ":", "List", "[", "int", "]", "=", "passage_info", "[", "constants", ".", "passage_num_normalized_values", "]", "\n", "\n", "for", "qa", "in", "passage_info", "[", "constants", ".", "qa_pairs", "]", ":", "\n", "                ", "total_qas", "+=", "1", "\n", "question_id", "=", "qa", "[", "constants", ".", "query_id", "]", "\n", "original_ques_text", "=", "qa", "[", "constants", ".", "cleaned_question", "]", "\n", "question_text", "=", "qa", "[", "constants", ".", "tokenized_question", "]", "\n", "question_charidxs", "=", "qa", "[", "constants", ".", "question_charidxs", "]", "\n", "\n", "answer_type", "=", "qa", "[", "constants", ".", "answer_type", "]", "\n", "answer_passage_spans", "=", "qa", "[", "constants", ".", "answer_passage_spans", "]", "\n", "answer_question_spans", "=", "qa", "[", "constants", ".", "answer_question_spans", "]", "\n", "answer_annotations", "=", "[", "]", "\n", "if", "\"answer\"", "in", "qa", ":", "\n", "                    ", "answer_annotations", ".", "append", "(", "qa", "[", "\"answer\"", "]", ")", "\n", "", "if", "\"validated_answers\"", "in", "qa", ":", "\n", "                    ", "answer_annotations", "+=", "qa", "[", "\"validated_answers\"", "]", "\n", "# answer_annotation = question_answer[\"answer\"] if \"answer\" in question_answer else None", "\n", "\n", "", "qtype", "=", "\"UNK\"", "\n", "if", "constants", ".", "qtype", "in", "qa", "and", "qa", "[", "constants", ".", "qtype", "]", "is", "not", "None", ":", "\n", "                    ", "qtype", "=", "qa", "[", "constants", ".", "qtype", "]", "\n", "", "program_supervised", "=", "False", "\n", "if", "constants", ".", "program_supervised", "in", "qa", ":", "\n", "                    ", "program_supervised", "=", "qa", "[", "constants", ".", "program_supervised", "]", "\n", "\n", "# If qtype is known and program_supervised = False OR", "\n", "# If qtype is unknown and program_supervision is True --- There's a problem, Houston!", "\n", "", "if", "(", "program_supervised", "and", "qtype", "==", "\"UNK\"", ")", "or", "(", "qtype", "!=", "\"UNK\"", "and", "program_supervised", "is", "False", ")", ":", "\n", "                    ", "print", "(", "original_ques_text", ")", "\n", "print", "(", "f\"Qtype: {qtype}\"", ")", "\n", "print", "(", "f\"Program supervised: {program_supervised}\"", ")", "\n", "exit", "(", ")", "\n", "\n", "", "ques_attn_supervision", "=", "None", "\n", "qattn_supervised", "=", "False", "\n", "if", "constants", ".", "qattn_supervised", "in", "qa", ":", "\n", "                    ", "qattn_supervised", "=", "qa", "[", "constants", ".", "qattn_supervised", "]", "\n", "if", "qattn_supervised", "is", "True", ":", "\n", "                        ", "ques_attn_supervision", "=", "qa", "[", "constants", ".", "ques_attention_supervision", "]", "\n", "\n", "", "", "date_grounding_supervision", "=", "None", "\n", "num_grounding_supervision", "=", "None", "\n", "execution_supervised", "=", "False", "\n", "if", "constants", ".", "exection_supervised", "in", "qa", ":", "\n", "                    ", "execution_supervised", "=", "qa", "[", "constants", ".", "exection_supervised", "]", "\n", "if", "qa", "[", "constants", ".", "exection_supervised", "]", "is", "True", ":", "\n", "# There can be multiple types of execution_supervision", "\n", "                        ", "if", "constants", ".", "qspan_dategrounding_supervision", "in", "qa", ":", "\n", "                            ", "date_grounding_supervision", "=", "qa", "[", "constants", ".", "qspan_dategrounding_supervision", "]", "\n", "", "if", "constants", ".", "qspan_numgrounding_supervision", "in", "qa", ":", "\n", "# This can be a 1- or 2- tuple of number groundings", "\n", "                            ", "num_grounding_supervision", "=", "qa", "[", "constants", ".", "qspan_numgrounding_supervision", "]", "\n", "\n", "# passage_att_supervision is probably never used", "\n", "", "", "", "passage_attn_supervision", "=", "None", "\n", "pattn_supervised", "=", "False", "\n", "if", "constants", ".", "pattn_supervised", "in", "qa", ":", "\n", "                    ", "pattn_supervised", "=", "qa", "[", "constants", ".", "pattn_supervised", "]", "\n", "if", "constants", ".", "passage_attn_supervision", "in", "qa", ":", "\n", "                        ", "passage_attn_supervision", "=", "qa", "[", "constants", ".", "passage_attn_supervision", "]", "\n", "\n", "", "", "synthetic_numground_metadata", "=", "None", "\n", "if", "constants", ".", "SYN_NUMGROUND_METADATA", "in", "qa", ":", "\n", "                    ", "synthetic_numground_metadata", "=", "qa", "[", "constants", ".", "SYN_NUMGROUND_METADATA", "]", "\n", "\n", "", "strongly_supervised", "=", "program_supervised", "and", "qattn_supervised", "and", "execution_supervised", "\n", "\n", "if", "qattn_supervised", "is", "True", ":", "\n", "                    ", "assert", "program_supervised", "is", "True", "and", "qtype", "is", "not", "\"UNK\"", "\n", "", "if", "execution_supervised", "is", "True", ":", "\n", "                    ", "assert", "qattn_supervised", "is", "True", "\n", "\n", "", "instance", "=", "self", ".", "text_to_instance", "(", "\n", "question_text", ",", "\n", "original_ques_text", ",", "\n", "question_charidxs", ",", "\n", "passage_text", ",", "\n", "original_passage_text", ",", "\n", "passage_charidxs", ",", "\n", "p_date_mens", ",", "\n", "p_date_entidxs", ",", "\n", "p_date_normvals", ",", "\n", "p_num_mens", ",", "\n", "p_num_entidxs", ",", "\n", "p_num_normvals", ",", "\n", "qtype", ",", "\n", "program_supervised", ",", "\n", "qattn_supervised", ",", "\n", "execution_supervised", ",", "\n", "pattn_supervised", ",", "\n", "strongly_supervised", ",", "\n", "ques_attn_supervision", ",", "\n", "date_grounding_supervision", ",", "\n", "num_grounding_supervision", ",", "\n", "passage_attn_supervision", ",", "\n", "synthetic_numground_metadata", ",", "\n", "answer_type", ",", "\n", "answer_passage_spans", ",", "\n", "answer_question_spans", ",", "\n", "question_id", ",", "\n", "passage_id", ",", "\n", "answer_annotations", ",", "\n", "max_passage_len", ",", "\n", "max_question_len", ",", "\n", ")", "\n", "\n", "if", "self", ".", "only_strongly_supervised", ":", "\n", "                    ", "if", "not", "strongly_supervised", ":", "\n", "                        ", "instance", "=", "None", "\n", "\n", "", "", "if", "instance", "is", "not", "None", ":", "\n", "                    ", "instances_read", "+=", "1", "\n", "yield", "instance", "\n", "\n", "#         if instance is not None:", "\n", "#             instances.append(instance)", "\n", "#         else:", "\n", "#             skip_count += 1", "\n", "# logger.info(f\"Skipped {skip_count} questions, kept {len(instances)} questions.\")", "\n", "# return instances", "\n", "", "", "", "logger", ".", "info", "(", "f\"Total QAs: {total_qas}. Instances read: {instances_read}\"", ")", "\n", "logger", ".", "info", "(", "f\"Instances Skipped: {self.skip_count}\"", ")", "\n", "logger", ".", "info", "(", "\n", "f\"Instances skipped due to gold-answer not in gold_program_types: {self.skip_due_to_gold_not_in_answer}\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.text_to_instance": [[233, 654], ["semqa.domain_languages.drop_language.get_empty_language_object", "semqa.domain_languages.drop_language.get_empty_language_object.all_possible_productions", "allennlp.data.fields.ListField", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "drop_reader.DROPReaderNew.get_numberindices_in_sorted_order", "allennlp.data.fields.ArrayField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.ArrayField", "allennlp.data.fields.MetadataField", "drop_reader.DROPReaderNew.get_year_difference_candidates", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "drop_reader.DROPReaderNew.get_passagenumber_difference_candidates", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "list", "allennlp.data.fields.MetadataField", "metadata.update", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "drop_reader.DROPReaderNew.get_gold_action_seqs", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "allennlp_semparse.fields.ProductionRuleField", "production_rule_fields.append", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token", "drop_reader.DROPReaderNew.prune_for_passage_len", "drop_reader.DROPReaderNew.prune_for_question_len", "zip", "passage_number_values.append", "numpy.array", "zip", "numpy.array", "str", "range", "allennlp.data.fields.ArrayField", "len", "allennlp.data.fields.ArrayField", "allennlp.data.fields.ArrayField", "allennlp.data.fields.ArrayField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "metadata.update", "allennlp.data.fields.ListField", "allennlp.data.fields.ListField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "zip", "zip", "int", "range", "range", "semqa.domain_languages.drop_language.Date", "semqa.domain_languages.drop_language.Date", "numpy.array", "numpy.array", "numpy.array", "len", "numpy.array", "len", "len", "enumerate", "question_text.split", "answer_program_start_types.append", "metadata.update", "allennlp.data.fields.SpanField", "float", "len", "len", "len", "len", "passage_text.split", "question_text.split", "len", "len", "int", "len", "len", "semqa.domain_languages.drop_language.get_empty_language_object.all_possible_productions", "allennlp.data.fields.SpanField", "allennlp.data.fields.SpanField", "int", "answer_program_start_types.append", "passage_number_values.index", "answer_program_start_types.append", "year_differences.index", "answer_program_start_types.append", "list.index", "len", "float", "span_answer_text.split", "int", "len", "len", "len", "int", "new_answer_program_start_types.append", "float", "int"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.get_empty_language_object", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.get_numberindices_in_sorted_order", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.get_year_difference_candidates", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.get_passagenumber_difference_candidates", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.get_gold_action_seqs", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.prune_for_passage_len", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.prune_for_question_len"], ["", "@", "overrides", "\n", "def", "text_to_instance", "(", "\n", "self", ",", "\n", "question_text", ":", "str", ",", "\n", "original_ques_text", ":", "str", ",", "\n", "question_charidxs", ":", "List", "[", "int", "]", ",", "\n", "passage_text", ":", "str", ",", "\n", "original_passage_text", ":", "str", ",", "\n", "passage_charidxs", ":", "List", "[", "int", "]", ",", "\n", "p_date_mens", ":", "List", "[", "Tuple", "[", "str", ",", "Tuple", "[", "int", ",", "int", "]", ",", "Tuple", "[", "int", ",", "int", ",", "int", "]", "]", "]", ",", "\n", "p_date_entidxs", ":", "List", "[", "int", "]", ",", "\n", "p_date_normvals", ":", "List", "[", "Tuple", "[", "int", ",", "int", ",", "int", "]", "]", ",", "\n", "p_num_mens", ":", "List", "[", "Tuple", "[", "str", ",", "int", ",", "int", "]", "]", ",", "\n", "p_num_entidxs", ":", "List", "[", "int", "]", ",", "\n", "p_num_normvals", ":", "List", "[", "int", "]", ",", "\n", "qtype", ":", "str", ",", "\n", "program_supervised", ":", "bool", ",", "\n", "qattn_supervised", ":", "bool", ",", "\n", "execution_supervised", ":", "bool", ",", "\n", "pattn_supervised", ":", "bool", ",", "\n", "strongly_supervised", ":", "bool", ",", "\n", "ques_attn_supervision", ":", "Tuple", "[", "List", "[", "float", "]", "]", ",", "\n", "date_grounding_supervision", ":", "Tuple", "[", "List", "[", "int", "]", ",", "List", "[", "int", "]", "]", ",", "\n", "num_grounding_supervision", ":", "Tuple", "[", "List", "[", "int", "]", ",", "List", "[", "int", "]", "]", ",", "\n", "passage_attn_supervision", ":", "List", "[", "float", "]", ",", "\n", "synthetic_numground_metadata", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ",", "\n", "answer_type", ":", "str", ",", "\n", "answer_passage_spans", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ",", "\n", "answer_question_spans", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ",", "\n", "question_id", ":", "str", "=", "None", ",", "\n", "passage_id", ":", "str", "=", "None", ",", "\n", "answer_annotations", ":", "List", "[", "Dict", "[", "str", ",", "Union", "[", "str", ",", "Dict", ",", "List", "]", "]", "]", "=", "None", ",", "\n", "max_passage_len", ":", "int", "=", "None", ",", "\n", "max_question_len", ":", "int", "=", "None", ",", "\n", ")", "->", "Union", "[", "Instance", ",", "None", "]", ":", "\n", "\n", "        ", "language", "=", "get_empty_language_object", "(", ")", "\n", "\n", "production_rule_fields", ":", "List", "[", "Field", "]", "=", "[", "]", "\n", "for", "production_rule", "in", "language", ".", "all_possible_productions", "(", ")", ":", "\n", "            ", "field", "=", "ProductionRuleField", "(", "production_rule", ",", "is_global_rule", "=", "True", ")", "\n", "production_rule_fields", ".", "append", "(", "field", ")", "\n", "", "action_field", "=", "ListField", "(", "production_rule_fields", ")", "\n", "\n", "# pylint: disable=arguments-differ", "\n", "passage_tokens", "=", "[", "\n", "Token", "(", "text", "=", "t", ",", "idx", "=", "t_charidx", ")", "for", "t", ",", "t_charidx", "in", "zip", "(", "passage_text", ".", "split", "(", "\" \"", ")", ",", "passage_charidxs", ")", "\n", "]", "\n", "\n", "question_tokens", "=", "[", "\n", "Token", "(", "text", "=", "t", ",", "idx", "=", "t_charidx", ")", "for", "t", ",", "t_charidx", "in", "zip", "(", "question_text", ".", "split", "(", "\" \"", ")", ",", "question_charidxs", ")", "\n", "]", "\n", "\n", "if", "max_passage_len", "is", "not", "None", ":", "\n", "            ", "passage_tokens", "=", "passage_tokens", "[", ":", "max_passage_len", "]", "\n", "(", "\n", "p_date_mens", ",", "\n", "p_date_entidxs", ",", "\n", "p_date_normvals", ",", "\n", "p_num_mens", ",", "\n", "p_num_entidxs", ",", "\n", "p_num_normvals", ",", "\n", "answer_passage_spans", ",", "\n", "date_grounding_supervision", ",", "\n", "num_grounding_supervision", ",", "\n", "passage_attn_supervision", ",", "\n", ")", "=", "self", ".", "prune_for_passage_len", "(", "\n", "max_passage_len", ",", "\n", "p_date_mens", ",", "\n", "p_date_entidxs", ",", "\n", "p_date_normvals", ",", "\n", "p_num_mens", ",", "\n", "p_num_entidxs", ",", "\n", "p_num_normvals", ",", "\n", "answer_passage_spans", ",", "\n", "date_grounding_supervision", ",", "\n", "num_grounding_supervision", ",", "\n", "passage_attn_supervision", ",", "\n", ")", "\n", "", "if", "max_question_len", "is", "not", "None", ":", "\n", "            ", "question_tokens", "=", "question_tokens", "[", ":", "max_question_len", "]", "\n", "(", "answer_question_spans", ",", "ques_attn_supervision", ")", "=", "self", ".", "prune_for_question_len", "(", "\n", "max_question_len", ",", "answer_question_spans", ",", "ques_attn_supervision", "\n", ")", "\n", "\n", "", "metadata", "=", "{", "\n", "\"original_passage\"", ":", "original_passage_text", ",", "\n", "\"original_question\"", ":", "original_ques_text", ",", "\n", "\"passage_id\"", ":", "passage_id", ",", "\n", "\"question_id\"", ":", "question_id", ",", "\n", "}", "\n", "\n", "fields", "=", "{", "}", "\n", "\n", "fields", "[", "\"actions\"", "]", "=", "action_field", "\n", "\n", "fields", "[", "\"passage\"", "]", "=", "TextField", "(", "passage_tokens", ",", "self", ".", "_token_indexers", ")", "\n", "fields", "[", "\"question\"", "]", "=", "TextField", "(", "question_tokens", ",", "self", ".", "_token_indexers", ")", "\n", "# List of (start, end) char offsets for each passage and question token", "\n", "passage_offsets", "=", "[", "(", "token", ".", "idx", ",", "token", ".", "idx", "+", "len", "(", "token", ".", "text", ")", ")", "for", "token", "in", "passage_tokens", "]", "\n", "question_offsets", "=", "[", "(", "token", ".", "idx", ",", "token", ".", "idx", "+", "len", "(", "token", ".", "text", ")", ")", "for", "token", "in", "question_tokens", "]", "\n", "\n", "##  Passage Number", "\n", "# The normalized values in processed dataset are floats even if the passage had ints. Converting them back ..", "\n", "p_num_normvals", "=", "[", "int", "(", "x", ")", "if", "int", "(", "x", ")", "==", "x", "else", "x", "for", "x", "in", "p_num_normvals", "]", "\n", "passage_number_entidxs", "=", "p_num_entidxs", "# same length as p_num_mens, containing num_grounding for the mens", "\n", "passage_number_values", "=", "p_num_normvals", "\n", "passage_number_indices", "=", "[", "tokenidx", "for", "(", "_", ",", "tokenidx", ",", "_", ")", "in", "p_num_mens", "]", "\n", "# These are number-token idxs in an order so that their values are sorted", "\n", "sorted_passagenumber_indices", "=", "self", ".", "get_numberindices_in_sorted_order", "(", "\n", "passage_number_values", ",", "passage_number_indices", ",", "passage_number_entidxs", "\n", ")", "\n", "\n", "# List of passage_len containing number_entidx for each token (-1 otherwise)", "\n", "passage_number_idx2entidx", "=", "[", "-", "1", "for", "_", "in", "range", "(", "len", "(", "passage_tokens", ")", ")", "]", "\n", "if", "passage_number_entidxs", ":", "\n", "            ", "for", "passage_num_tokenidx", ",", "number_idx", "in", "zip", "(", "passage_number_indices", ",", "passage_number_entidxs", ")", ":", "\n", "                ", "passage_number_idx2entidx", "[", "passage_num_tokenidx", "]", "=", "number_idx", "\n", "", "", "else", ":", "\n", "# No numbers found in the passage - making a fake number at the 0th token", "\n", "            ", "passage_number_idx2entidx", "[", "0", "]", "=", "0", "\n", "sorted_passagenumber_indices", "=", "[", "0", "]", "\n", "", "if", "not", "passage_number_values", ":", "\n", "            ", "passage_number_values", ".", "append", "(", "-", "1", ")", "\n", "", "fields", "[", "\"passageidx2numberidx\"", "]", "=", "ArrayField", "(", "np", ".", "array", "(", "passage_number_idx2entidx", ")", ",", "padding_value", "=", "-", "1", ")", "\n", "fields", "[", "\"passage_number_values\"", "]", "=", "MetadataField", "(", "passage_number_values", ")", "\n", "fields", "[", "\"passage_number_sortedtokenidxs\"", "]", "=", "MetadataField", "(", "sorted_passagenumber_indices", ")", "\n", "\n", "##  Passage Dates", "\n", "passage_date_entidxs", "=", "p_date_entidxs", "\n", "passage_date_values", "=", "p_date_normvals", "\n", "passage_date_spanidxs", "=", "[", "(", "x", ",", "y", ")", "for", "(", "_", ",", "(", "x", ",", "y", ")", ",", "_", ")", "in", "p_date_mens", "]", "\n", "\n", "passage_date_idx2dateidx", "=", "[", "-", "1", "for", "_", "in", "range", "(", "len", "(", "passage_tokens", ")", ")", "]", "\n", "if", "passage_date_spanidxs", ":", "\n", "            ", "for", "passage_date_span", ",", "date_idx", "in", "zip", "(", "passage_date_spanidxs", ",", "passage_date_entidxs", ")", ":", "\n", "                ", "(", "s", ",", "e", ")", "=", "passage_date_span", "\n", "passage_date_idx2dateidx", "[", "s", ":", "e", "+", "1", "]", "=", "[", "date_idx", "]", "*", "(", "e", "+", "1", "-", "s", ")", "\n", "", "", "else", ":", "\n", "            ", "passage_date_idx2dateidx", "[", "0", "]", "=", "0", "\n", "", "if", "passage_date_values", ":", "\n", "            ", "passage_date_objs", "=", "[", "Date", "(", "day", "=", "d", ",", "month", "=", "m", ",", "year", "=", "y", ")", "for", "(", "d", ",", "m", ",", "y", ")", "in", "passage_date_values", "]", "\n", "", "else", ":", "\n", "            ", "passage_date_objs", "=", "[", "Date", "(", "day", "=", "-", "1", ",", "month", "=", "-", "1", ",", "year", "=", "-", "1", ")", "]", "\n", "", "fields", "[", "\"passageidx2dateidx\"", "]", "=", "ArrayField", "(", "np", ".", "array", "(", "passage_date_idx2dateidx", ")", ",", "padding_value", "=", "-", "1", ")", "\n", "fields", "[", "\"passage_date_values\"", "]", "=", "MetadataField", "(", "passage_date_objs", ")", "\n", "passage_date_strvals", "=", "[", "str", "(", "d", ")", "for", "d", "in", "passage_date_objs", "]", "\n", "\n", "# year_differences: List[int]", "\n", "year_differences", ",", "year_differences_mat", "=", "self", ".", "get_year_difference_candidates", "(", "passage_date_objs", ")", "\n", "fields", "[", "\"year_differences\"", "]", "=", "MetadataField", "(", "year_differences", ")", "\n", "fields", "[", "\"year_differences_mat\"", "]", "=", "MetadataField", "(", "year_differences_mat", ")", "\n", "\n", "passage_number_differences", ",", "passage_number_diff_mat", "=", "self", ".", "get_passagenumber_difference_candidates", "(", "\n", "passage_number_values", "\n", ")", "\n", "fields", "[", "\"passagenumber_difference_values\"", "]", "=", "MetadataField", "(", "passage_number_differences", ")", "\n", "fields", "[", "\"passagenumber_differences_mat\"", "]", "=", "MetadataField", "(", "passage_number_diff_mat", ")", "\n", "\n", "count_values", "=", "list", "(", "range", "(", "10", ")", ")", "\n", "fields", "[", "\"count_values\"", "]", "=", "MetadataField", "(", "count_values", ")", "\n", "\n", "metadata", ".", "update", "(", "\n", "{", "\n", "\"passage_token_offsets\"", ":", "passage_offsets", ",", "\n", "\"question_token_offsets\"", ":", "question_offsets", ",", "\n", "\"question_tokens\"", ":", "[", "token", ".", "text", "for", "token", "in", "question_tokens", "]", ",", "\n", "\"passage_tokens\"", ":", "[", "token", ".", "text", "for", "token", "in", "passage_tokens", "]", ",", "\n", "\"passage_date_values\"", ":", "passage_date_strvals", ",", "\n", "\"passage_number_values\"", ":", "passage_number_values", ",", "\n", "\"passage_year_diffs\"", ":", "year_differences", ",", "\n", "\"passagenum_diffs\"", ":", "passage_number_differences", ",", "\n", "\"count_values\"", ":", "count_values", "\n", "# \"number_tokens\": [token.text for token in number_tokens],", "\n", "# \"number_indices\": number_indices", "\n", "}", "\n", ")", "\n", "\n", "# FIELDS FOR STRONG-SUPERVISION", "\n", "fields", "[", "\"strongly_supervised\"", "]", "=", "MetadataField", "(", "strongly_supervised", ")", "\n", "fields", "[", "\"program_supervised\"", "]", "=", "MetadataField", "(", "program_supervised", ")", "\n", "fields", "[", "\"qattn_supervised\"", "]", "=", "MetadataField", "(", "qattn_supervised", ")", "\n", "fields", "[", "\"execution_supervised\"", "]", "=", "MetadataField", "(", "execution_supervised", ")", "\n", "fields", "[", "\"pattn_supervised\"", "]", "=", "MetadataField", "(", "pattn_supervised", ")", "\n", "fields", "[", "\"qtypes\"", "]", "=", "MetadataField", "(", "qtype", ")", "\n", "fields", "[", "\"synthetic_numground_metadata\"", "]", "=", "MetadataField", "(", "synthetic_numground_metadata", ")", "\n", "\n", "# Question Attention Supervision", "\n", "if", "ques_attn_supervision", ":", "\n", "            ", "fields", "[", "\"qattn_supervision\"", "]", "=", "ArrayField", "(", "np", ".", "array", "(", "ques_attn_supervision", ")", ",", "padding_value", "=", "0", ")", "\n", "", "else", ":", "\n", "            ", "qlen", "=", "len", "(", "question_tokens", ")", "\n", "empty_question_attention", "=", "[", "0.0", "]", "*", "qlen", "\n", "empty_question_attention_tuple", "=", "[", "empty_question_attention", "]", "\n", "fields", "[", "\"qattn_supervision\"", "]", "=", "ArrayField", "(", "np", ".", "array", "(", "empty_question_attention_tuple", ")", ",", "padding_value", "=", "0", ")", "\n", "\n", "", "if", "passage_attn_supervision", ":", "\n", "            ", "fields", "[", "\"passage_attn_supervision\"", "]", "=", "ArrayField", "(", "np", ".", "array", "(", "passage_attn_supervision", ")", ",", "padding_value", "=", "0", ")", "\n", "", "else", ":", "\n", "            ", "empty_passage_attention", "=", "[", "0.0", "]", "*", "len", "(", "passage_tokens", ")", "\n", "fields", "[", "\"passage_attn_supervision\"", "]", "=", "ArrayField", "(", "np", ".", "array", "(", "empty_passage_attention", ")", ",", "padding_value", "=", "0", ")", "\n", "\n", "# Date-comparison - Date Grounding Supervision", "\n", "", "if", "date_grounding_supervision", ":", "\n", "            ", "fields", "[", "\"datecomp_ques_event_date_groundings\"", "]", "=", "MetadataField", "(", "date_grounding_supervision", ")", "\n", "", "else", ":", "\n", "            ", "empty_date_grounding", "=", "[", "0.0", "]", "*", "len", "(", "passage_date_objs", ")", "\n", "empty_date_grounding_tuple", "=", "(", "empty_date_grounding", ",", "empty_date_grounding", ")", "\n", "fields", "[", "\"datecomp_ques_event_date_groundings\"", "]", "=", "MetadataField", "(", "empty_date_grounding_tuple", ")", "\n", "\n", "# Number Comparison - Passage Number Grounding Supervision", "\n", "", "if", "num_grounding_supervision", ":", "\n", "            ", "fields", "[", "\"numcomp_qspan_num_groundings\"", "]", "=", "MetadataField", "(", "num_grounding_supervision", ")", "\n", "", "else", ":", "\n", "            ", "empty_passagenum_grounding", "=", "[", "0.0", "]", "*", "len", "(", "passage_number_values", ")", "\n", "empty_passagenum_grounding_tuple", "=", "(", "empty_passagenum_grounding", ",", "empty_passagenum_grounding", ")", "\n", "fields", "[", "\"numcomp_qspan_num_groundings\"", "]", "=", "MetadataField", "(", "empty_passagenum_grounding_tuple", ")", "\n", "\n", "# Get gold action_seqs for strongly_supervised questions", "\n", "", "action2idx_map", "=", "{", "rule", ":", "i", "for", "i", ",", "rule", "in", "enumerate", "(", "language", ".", "all_possible_productions", "(", ")", ")", "}", "\n", "\n", "# Tuple[List[List[int]], List[List[int]]]", "\n", "(", "\n", "gold_action_seqs", ",", "\n", "gold_actionseq_masks", ",", "\n", "gold_program_start_types", ",", "\n", "program_supervised", ",", "\n", ")", "=", "self", ".", "get_gold_action_seqs", "(", "\n", "program_supervised", "=", "program_supervised", ",", "\n", "qtype", "=", "qtype", ",", "\n", "question_tokens", "=", "question_text", ".", "split", "(", "\" \"", ")", ",", "\n", "language", "=", "language", ",", "\n", "action2idx_map", "=", "action2idx_map", ",", "\n", ")", "\n", "fields", "[", "\"program_supervised\"", "]", "=", "MetadataField", "(", "program_supervised", ")", "\n", "fields", "[", "\"gold_action_seqs\"", "]", "=", "MetadataField", "(", "(", "gold_action_seqs", ",", "gold_actionseq_masks", ")", ")", "\n", "\n", "########     ANSWER FIELDS      ###################", "\n", "\n", "if", "answer_annotations", ":", "\n", "            ", "metadata", ".", "update", "(", "{", "\"answer_annotations\"", ":", "answer_annotations", "}", ")", "\n", "\n", "# Using the first one for training (really, there's only one)", "\n", "answer_annotation", "=", "answer_annotations", "[", "0", "]", "\n", "\n", "# answer_type = \"UNK\"", "\n", "# if answer_annotation[\"spans\"]:", "\n", "#     answer_type = \"spans\"", "\n", "# elif answer_annotation[\"number\"]:", "\n", "#     answer_type = \"number\"", "\n", "# else:", "\n", "#     raise NotImplementedError", "\n", "\n", "# This list contains the possible-start-types for programs that can yield the correct answer", "\n", "# For example, if the answer is a number but also in passage, this will contain two keys", "\n", "# If the answer is a number, we'll find which kind and that program-start-type will be added here", "\n", "answer_program_start_types", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "\n", "# We've pre-parsed the span types to passage / question spans", "\n", "\n", "# Passage-span answer", "\n", "passage_span_fields", "=", "[", "]", "\n", "if", "answer_passage_spans", ":", "\n", "                ", "answer_program_start_types", ".", "append", "(", "\"passage_span\"", ")", "\n", "passage_span_fields", "=", "[", "SpanField", "(", "span", "[", "0", "]", ",", "span", "[", "1", "]", ",", "fields", "[", "\"passage\"", "]", ")", "for", "span", "in", "answer_passage_spans", "]", "\n", "metadata", ".", "update", "(", "{", "\"answer_passage_spans\"", ":", "answer_passage_spans", "}", ")", "\n", "", "else", ":", "\n", "                ", "passage_span_fields", "=", "[", "SpanField", "(", "-", "1", ",", "-", "1", ",", "fields", "[", "\"passage\"", "]", ")", "]", "\n", "", "fields", "[", "\"answer_as_passage_spans\"", "]", "=", "ListField", "(", "passage_span_fields", ")", "\n", "\n", "# if answer_question_spans:", "\n", "#     answer_program_start_types.append(\"question_span\")", "\n", "#     question_span_fields = \\", "\n", "#         [SpanField(span[0], span[1], fields[\"question\"]) for span in answer_question_spans]", "\n", "#     metadata.update({'answer_question_spans': answer_question_spans})", "\n", "# else:", "\n", "#     question_span_fields = [SpanField(-1, -1, fields[\"question\"])]", "\n", "# fields[\"answer_as_question_spans\"] = ListField(question_span_fields)", "\n", "\n", "# Question-span answer", "\n", "question_span_fields", "=", "[", "SpanField", "(", "-", "1", ",", "-", "1", ",", "fields", "[", "\"question\"", "]", ")", "]", "\n", "fields", "[", "\"answer_as_question_spans\"", "]", "=", "ListField", "(", "question_span_fields", ")", "\n", "\n", "# Number answers", "\n", "number_answer_str", "=", "answer_annotation", "[", "\"number\"", "]", "\n", "if", "not", "number_answer_str", ":", "\n", "# Answer as number string does not exist.", "\n", "                ", "if", "self", ".", "convert_spananswer_to_num", ":", "\n", "# Try to convert \"X\" or \"X-yard(s)\" into number(X)", "\n", "# span_answer_text = answer_annotation[\"spans\"][0]", "\n", "                    ", "try", ":", "\n", "                        ", "span_answer_text", "=", "answer_annotation", "[", "\"spans\"", "]", "[", "0", "]", "\n", "span_answer_number", "=", "float", "(", "span_answer_text", ")", "\n", "", "except", ":", "\n", "                        ", "span_answer_number", "=", "None", "\n", "", "if", "span_answer_number", "is", "None", ":", "\n", "                        ", "split_hyphen", "=", "span_answer_text", ".", "split", "(", "\"-\"", ")", "\n", "if", "len", "(", "split_hyphen", ")", "==", "2", ":", "\n", "                            ", "try", ":", "\n", "                                ", "span_answer_number", "=", "float", "(", "split_hyphen", "[", "0", "]", ")", "\n", "", "except", ":", "\n", "                                ", "span_answer_number", "=", "None", "\n", "", "", "else", ":", "\n", "                            ", "span_answer_number", "=", "None", "\n", "", "", "if", "span_answer_number", "is", "not", "None", ":", "\n", "                        ", "answer_number", "=", "(", "\n", "int", "(", "span_answer_number", ")", "\n", "if", "int", "(", "span_answer_number", ")", "==", "span_answer_number", "\n", "else", "span_answer_number", "\n", ")", "\n", "", "else", ":", "\n", "                        ", "answer_number", "=", "None", "\n", "", "", "else", ":", "\n", "                    ", "answer_number", "=", "None", "\n", "", "", "else", ":", "\n", "                ", "answer_number", "=", "float", "(", "number_answer_str", ")", "\n", "answer_number", "=", "int", "(", "answer_number", ")", "if", "int", "(", "answer_number", ")", "==", "answer_number", "else", "answer_number", "\n", "\n", "", "ans_as_passage_number", "=", "[", "0", "]", "*", "len", "(", "passage_number_values", ")", "\n", "ans_as_year_difference", "=", "[", "0", "]", "*", "len", "(", "year_differences", ")", "\n", "answer_as_passagenum_difference", "=", "[", "0", "]", "*", "len", "(", "passage_number_differences", ")", "\n", "answer_as_count", "=", "[", "0", "]", "*", "len", "(", "count_values", ")", "\n", "# if answer_type == \"number\":", "\n", "if", "answer_number", "is", "not", "None", ":", "\n", "# answer_text = answer_annotation[\"number\"]", "\n", "# answer_number = float(answer_text)", "\n", "# answer_number = int(answer_number) if int(answer_number) == answer_number else answer_number", "\n", "# Number lists below are mix of floats and ints. Type of answer_number doesn't matter", "\n", "\n", "# Passage-number answer", "\n", "                ", "if", "answer_number", "in", "passage_number_values", ":", "\n", "                    ", "answer_program_start_types", ".", "append", "(", "\"passage_number\"", ")", "\n", "ans_as_passage_number_idx", "=", "passage_number_values", ".", "index", "(", "answer_number", ")", "\n", "ans_as_passage_number", "[", "ans_as_passage_number_idx", "]", "=", "1", "\n", "\n", "# Year-difference answer", "\n", "", "if", "answer_number", "in", "year_differences", ":", "\n", "                    ", "answer_program_start_types", ".", "append", "(", "\"year_difference\"", ")", "\n", "ans_as_year_difference_idx", "=", "year_differences", ".", "index", "(", "answer_number", ")", "\n", "ans_as_year_difference", "[", "ans_as_year_difference_idx", "]", "=", "1", "\n", "\n", "", "\"\"\"\n                # PassageNum-difference Answer\n                if answer_number in passage_number_differences:\n                    answer_program_start_types.append(\"passagenum_diff\")\n                    ans_as_passagenum_diff_idx = passage_number_differences.index(answer_number)\n                    answer_as_passagenum_difference[ans_as_passagenum_diff_idx] = 1\n                \"\"\"", "\n", "\n", "# Count answer", "\n", "if", "answer_number", "in", "count_values", ":", "\n", "                    ", "answer_program_start_types", ".", "append", "(", "\"count_number\"", ")", "\n", "answer_count_idx", "=", "count_values", ".", "index", "(", "answer_number", ")", "\n", "answer_as_count", "[", "answer_count_idx", "]", "=", "1", "\n", "\n", "", "", "fields", "[", "\"answer_as_passage_number\"", "]", "=", "MetadataField", "(", "ans_as_passage_number", ")", "\n", "fields", "[", "\"answer_as_year_difference\"", "]", "=", "MetadataField", "(", "ans_as_year_difference", ")", "\n", "fields", "[", "\"answer_as_passagenum_difference\"", "]", "=", "MetadataField", "(", "answer_as_passagenum_difference", ")", "\n", "fields", "[", "\"answer_as_count\"", "]", "=", "MetadataField", "(", "answer_as_count", ")", "\n", "\n", "fields", "[", "\"answer_program_start_types\"", "]", "=", "MetadataField", "(", "answer_program_start_types", ")", "\n", "\n", "# If we already have gold program(s), removing program_start_types that don't come from these gold_programs", "\n", "# print(f\"AnswerTypes: {answer_program_start_types}\")", "\n", "# print(f\"New AnswerTypes: {new_answer_program_start_types}\")", "\n", "\n", "if", "self", ".", "skip_due_to_gold_programs", ":", "\n", "                ", "if", "program_supervised", ":", "\n", "                    ", "new_answer_program_start_types", "=", "[", "]", "\n", "for", "answer_program_type", "in", "answer_program_start_types", ":", "\n", "                        ", "if", "answer_program_type", "in", "gold_program_start_types", ":", "\n", "                            ", "new_answer_program_start_types", ".", "append", "(", "answer_program_type", ")", "\n", "", "", "", "else", ":", "\n", "                    ", "new_answer_program_start_types", "=", "answer_program_start_types", "\n", "# Answer exists as other programs but not for gold-program", "\n", "", "if", "len", "(", "answer_program_start_types", ")", "!=", "0", "and", "len", "(", "new_answer_program_start_types", ")", "==", "0", ":", "\n", "                    ", "self", ".", "skip_due_to_gold_not_in_answer", "+=", "1", "\n", "", "answer_program_start_types", "=", "new_answer_program_start_types", "\n", "\n", "# if len(answer_program_start_types) == 0:", "\n", "#     print(original_ques_text)", "\n", "#     print(original_passage_text)", "\n", "#     print(answer_annotation)", "\n", "#     print(f\"PassageNumVals:{passage_number_values}\")", "\n", "#     print(f\"PassageDates:{passage_date_strvals}\")", "\n", "#     print(f\"PassageNumDiffs: {passage_number_differences}\")", "\n", "#     print(f\"YearDiffs:{year_differences}\")", "\n", "\n", "", "if", "self", ".", "skip_instances", ":", "\n", "                ", "if", "len", "(", "answer_program_start_types", ")", "==", "0", ":", "\n", "                    ", "self", ".", "skip_count", "+=", "1", "\n", "# print(f\"Skipped instances: {self.skipped_instances}\")", "\n", "# print(\"\\nNo answer grounding\")", "\n", "# print(original_ques_text)", "\n", "# print(original_passage_text)", "\n", "# print(answer_annotation)", "\n", "# print(answer_passage_spans)", "\n", "# print(answer_question_spans)", "\n", "return", "None", "\n", "\n", "# TODO(nitish): Only using questions which have PassageSpan as answers", "\n", "", "", "", "\"\"\"\n        if not answer_passage_spans:\n            # print(\"Not dealing with empty passage answers\")\n            return None\n        \"\"\"", "\n", "\n", "\"\"\"\n        attention, count_answer, mask = self.make_count_instance(passage_text.split(' '))\n        attention = [x + abs(random.gauss(0, 0.001)) for x in attention]\n        attention_sum = sum(attention)\n        attention = [float(x) / attention_sum for x in attention]\n        count_answer_vec = [0] * 10\n        count_answer_vec[count_answer] = 1\n        fields[\"aux_passage_attention\"] = ArrayField(np.array(attention), padding_value=0.0)\n        fields[\"aux_answer_as_count\"] = ArrayField(np.array(count_answer_vec))\n        fields[\"aux_count_mask\"] = ArrayField(np.array(mask))\n        \"\"\"", "\n", "\n", "fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "metadata", ")", "\n", "return", "Instance", "(", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.prune_for_passage_len": [[655, 756], ["zip", "enumerate", "zip", "enumerate", "pruned_date_mens.append", "pruned_old_dateidxs.append", "len", "new_date_values.append", "new_dategrounding_supervision.append", "pruned_num_mens.append", "pruned_old_numidxs.append", "len", "new_num_values.append", "new_numgrounding_supervision.append", "len", "range", "range", "len", "len"], "methods", ["None"], ["", "def", "prune_for_passage_len", "(", "\n", "self", ",", "\n", "max_passage_len", ":", "int", ",", "\n", "p_date_mens", ",", "\n", "p_date_entidxs", ",", "\n", "p_date_normvals", ",", "\n", "p_num_mens", ",", "\n", "p_num_entidxs", ",", "\n", "p_num_normvals", ",", "\n", "answer_passage_spans", ",", "\n", "date_grounding_supervision", ",", "\n", "num_grounding_supervision", ",", "\n", "passage_attn_supervision", ",", "\n", ")", ":", "\n", "\n", "        ", "\"\"\" Prunes the passage and related data for a maximum length\n\n            For the given max_passage_len, we first need to find out the pruned date and number mentions\n            Since these might remove some dates and numbers from the passage, we need to find the\n            pruned list of p_date_normvals (p_date_entidxs with the new date_entidxs)\n            pruned list of p_num_normvals (p_num_entidxs with new num_entidxs) -- make sure the numbers are still sorted\n\n            answer_passage_spans - only spans that are contained in the pruned passage\n\n            date_grounding_supervision, num_grounding_supervision -- both these are the length of original dates/nums\n            we need to find the new value by pruning and mapping old ent idxs to new ones.\n\n            passage_attn_supervision: if not None, is a list the length of the passage\n        \"\"\"", "\n", "pruned_date_mens", "=", "[", "]", "# New passage date mens", "\n", "pruned_old_dateidxs", "=", "[", "]", "\n", "for", "date_men", ",", "date_idx", "in", "zip", "(", "p_date_mens", ",", "p_date_entidxs", ")", ":", "\n", "            ", "_", ",", "(", "x", ",", "y", ")", ",", "_", "=", "date_men", "\n", "if", "y", "<", "max_passage_len", ":", "\n", "                ", "pruned_date_mens", ".", "append", "(", "date_men", ")", "\n", "pruned_old_dateidxs", ".", "append", "(", "date_idx", ")", "\n", "\n", "", "", "new_date_values", "=", "[", "]", "# New passage date values", "\n", "new2old_dateidx", "=", "{", "}", "\n", "old2new_dateidx", "=", "{", "}", "\n", "for", "old_date_idx", ",", "date_value", "in", "enumerate", "(", "p_date_normvals", ")", ":", "\n", "# Atleast one mention of this old_date_idx remains", "\n", "            ", "if", "old_date_idx", "in", "pruned_old_dateidxs", ":", "\n", "                ", "new_date_idx", "=", "len", "(", "new_date_values", ")", "\n", "new2old_dateidx", "[", "new_date_idx", "]", "=", "old_date_idx", "\n", "old2new_dateidx", "[", "old_date_idx", "]", "=", "new_date_idx", "\n", "new_date_values", ".", "append", "(", "date_value", ")", "\n", "\n", "", "", "new_date_entidxs", "=", "[", "old2new_dateidx", "[", "x", "]", "for", "x", "in", "pruned_old_dateidxs", "]", "# New passage date entidxs", "\n", "\n", "if", "date_grounding_supervision", "is", "not", "None", ":", "\n", "            ", "new_dategrounding_supervision", "=", "[", "]", "\n", "for", "date_grounding", "in", "date_grounding_supervision", ":", "\n", "                ", "new_grounding", "=", "[", "date_grounding", "[", "new2old_dateidx", "[", "newidx", "]", "]", "for", "newidx", "in", "range", "(", "len", "(", "new_date_values", ")", ")", "]", "\n", "new_dategrounding_supervision", ".", "append", "(", "new_grounding", ")", "\n", "", "", "else", ":", "\n", "            ", "new_dategrounding_supervision", "=", "None", "\n", "\n", "# Pruning numbers", "\n", "", "pruned_num_mens", ",", "pruned_old_numidxs", "=", "[", "]", ",", "[", "]", "\n", "for", "num_men", ",", "num_idx", "in", "zip", "(", "p_num_mens", ",", "p_num_entidxs", ")", ":", "\n", "            ", "_", ",", "tokenidx", ",", "_", "=", "num_men", "\n", "if", "tokenidx", "<", "max_passage_len", ":", "\n", "                ", "pruned_num_mens", ".", "append", "(", "num_men", ")", "\n", "pruned_old_numidxs", ".", "append", "(", "num_idx", ")", "\n", "", "", "new_num_values", "=", "[", "]", "\n", "old2new_numidx", ",", "new2old_numidx", "=", "{", "}", ",", "{", "}", "\n", "for", "old_num_idx", ",", "num_value", "in", "enumerate", "(", "p_num_normvals", ")", ":", "\n", "            ", "if", "old_num_idx", "in", "pruned_old_numidxs", ":", "\n", "                ", "new_num_idx", "=", "len", "(", "new_num_values", ")", "\n", "old2new_numidx", "[", "old_num_idx", "]", "=", "new_num_idx", "\n", "new2old_numidx", "[", "new_num_idx", "]", "=", "old_num_idx", "\n", "new_num_values", ".", "append", "(", "num_value", ")", "\n", "", "", "new_num_idxs", "=", "[", "old2new_numidx", "[", "x", "]", "for", "x", "in", "pruned_old_numidxs", "]", "\n", "\n", "if", "num_grounding_supervision", "is", "not", "None", ":", "\n", "            ", "new_numgrounding_supervision", "=", "[", "]", "\n", "for", "num_grounding", "in", "num_grounding_supervision", ":", "\n", "                ", "new_grounding", "=", "[", "num_grounding", "[", "new2old_numidx", "[", "newidx", "]", "]", "for", "newidx", "in", "range", "(", "len", "(", "new_num_values", ")", ")", "]", "\n", "new_numgrounding_supervision", ".", "append", "(", "new_grounding", ")", "\n", "", "", "else", ":", "\n", "            ", "new_numgrounding_supervision", "=", "None", "\n", "\n", "", "new_answer_passage_spans", "=", "[", "span", "for", "span", "in", "answer_passage_spans", "if", "span", "[", "1", "]", "<", "max_passage_len", "]", "\n", "\n", "if", "passage_attn_supervision", "is", "not", "None", "and", "len", "(", "passage_attn_supervision", ")", ">", "max_passage_len", ":", "\n", "            ", "new_passage_attn_supervision", "=", "passage_attn_supervision", "[", "0", ":", "max_passage_len", "]", "\n", "", "else", ":", "\n", "            ", "new_passage_attn_supervision", "=", "passage_attn_supervision", "\n", "\n", "", "return", "(", "\n", "pruned_date_mens", ",", "\n", "new_date_entidxs", ",", "\n", "new_date_values", ",", "\n", "pruned_num_mens", ",", "\n", "new_num_idxs", ",", "\n", "new_num_values", ",", "\n", "new_answer_passage_spans", ",", "\n", "new_dategrounding_supervision", ",", "\n", "new_numgrounding_supervision", ",", "\n", "new_passage_attn_supervision", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.prune_for_question_len": [[758, 767], ["None"], "methods", ["None"], ["", "def", "prune_for_question_len", "(", "self", ",", "max_question_len", ",", "answer_question_spans", ",", "ques_attn_supervision", ")", ":", "\n", "        ", "new_answer_question_spans", "=", "[", "span", "for", "span", "in", "answer_question_spans", "if", "span", "[", "1", "]", "<", "max_question_len", "]", "\n", "\n", "if", "ques_attn_supervision", "is", "not", "None", ":", "\n", "            ", "new_qattn_supervision", "=", "[", "qattn", "[", "0", ":", "max_question_len", "]", "for", "qattn", "in", "ques_attn_supervision", "]", "\n", "", "else", ":", "\n", "            ", "new_qattn_supervision", "=", "None", "\n", "\n", "", "return", "(", "new_answer_question_spans", ",", "new_qattn_supervision", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.convert_string_to_int": [[802, 813], ["string.replace", "int"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "convert_string_to_int", "(", "string", ":", "str", ")", ":", "\n", "        ", "no_comma_string", "=", "string", ".", "replace", "(", "\",\"", ",", "\"\"", ")", "\n", "if", "no_comma_string", "in", "WORD_NUMBER_MAP", ":", "\n", "            ", "number", "=", "WORD_NUMBER_MAP", "[", "no_comma_string", "]", "\n", "", "else", ":", "\n", "            ", "try", ":", "\n", "                ", "number", "=", "int", "(", "no_comma_string", ")", "\n", "", "except", "ValueError", ":", "\n", "                ", "number", "=", "None", "\n", "", "", "return", "number", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.find_valid_spans": [[814, 841], ["collections.defaultdict", "enumerate", "token.text.lower().strip", "word_positions[].append", "answer_text.lower().strip().split", "len", "token.text.lower", "answer_text.lower().strip", "spans.append", "len", "answer_tokens[].strip", "answer_text.lower"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "find_valid_spans", "(", "passage_tokens", ":", "List", "[", "Token", "]", ",", "answer_texts", ":", "List", "[", "str", "]", ")", "->", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ":", "\n", "        ", "normalized_tokens", "=", "[", "token", ".", "text", ".", "lower", "(", ")", ".", "strip", "(", "STRIPPED_CHARACTERS", ")", "for", "token", "in", "passage_tokens", "]", "\n", "word_positions", ":", "Dict", "[", "str", ",", "List", "[", "int", "]", "]", "=", "defaultdict", "(", "list", ")", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "normalized_tokens", ")", ":", "\n", "            ", "word_positions", "[", "token", "]", ".", "append", "(", "i", ")", "\n", "", "spans", "=", "[", "]", "\n", "for", "answer_text", "in", "answer_texts", ":", "\n", "            ", "answer_tokens", "=", "answer_text", ".", "lower", "(", ")", ".", "strip", "(", "STRIPPED_CHARACTERS", ")", ".", "split", "(", ")", "\n", "num_answer_tokens", "=", "len", "(", "answer_tokens", ")", "\n", "if", "answer_tokens", "[", "0", "]", "not", "in", "word_positions", ":", "\n", "                ", "continue", "\n", "", "for", "span_start", "in", "word_positions", "[", "answer_tokens", "[", "0", "]", "]", ":", "\n", "                ", "span_end", "=", "span_start", "# span_end is _inclusive_", "\n", "answer_index", "=", "1", "\n", "while", "answer_index", "<", "num_answer_tokens", "and", "span_end", "+", "1", "<", "len", "(", "normalized_tokens", ")", ":", "\n", "                    ", "token", "=", "normalized_tokens", "[", "span_end", "+", "1", "]", "\n", "if", "answer_tokens", "[", "answer_index", "]", ".", "strip", "(", "STRIPPED_CHARACTERS", ")", "==", "token", ":", "\n", "                        ", "answer_index", "+=", "1", "\n", "span_end", "+=", "1", "\n", "", "elif", "token", "in", "IGNORED_TOKENS", ":", "\n", "                        ", "span_end", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "break", "\n", "", "", "if", "num_answer_tokens", "==", "answer_index", ":", "\n", "                    ", "spans", ".", "append", "(", "(", "span_start", ",", "span_end", ")", ")", "\n", "", "", "", "return", "spans", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.find_valid_plus_minus_combinations": [[842, 860], ["range", "list", "itertools.combinations", "itertools.product", "enumerate", "sum", "zip", "valid_combinations.append", "len", "zip"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.test_language.Arithmetic.sum"], ["", "@", "staticmethod", "\n", "def", "find_valid_plus_minus_combinations", "(", "\n", "numbers", ":", "List", "[", "int", "]", ",", "targets", ":", "List", "[", "int", "]", ",", "max_length_of_combinations", ":", "int", "=", "2", "\n", ")", "->", "List", "[", "List", "[", "int", "]", "]", ":", "\n", "        ", "valid_combinations", "=", "[", "]", "\n", "for", "combination_length", "in", "range", "(", "2", ",", "max_length_of_combinations", "+", "1", ")", ":", "\n", "            ", "possible_signs", "=", "list", "(", "itertools", ".", "product", "(", "(", "-", "1", ",", "1", ")", ",", "repeat", "=", "combination_length", ")", ")", "\n", "for", "combination", "in", "itertools", ".", "combinations", "(", "enumerate", "(", "numbers", ")", ",", "combination_length", ")", ":", "\n", "                ", "indices", "=", "[", "it", "[", "0", "]", "for", "it", "in", "combination", "]", "\n", "values", "=", "[", "it", "[", "1", "]", "for", "it", "in", "combination", "]", "\n", "for", "signs", "in", "possible_signs", ":", "\n", "                    ", "eval_value", "=", "sum", "(", "sign", "*", "value", "for", "sign", ",", "value", "in", "zip", "(", "signs", ",", "values", ")", ")", "\n", "if", "eval_value", "in", "targets", ":", "\n", "                        ", "labels_for_numbers", "=", "[", "0", "]", "*", "len", "(", "numbers", ")", "# 0 represents ``not included''.", "\n", "for", "index", ",", "sign", "in", "zip", "(", "indices", ",", "signs", ")", ":", "\n", "                            ", "labels_for_numbers", "[", "index", "]", "=", "1", "if", "sign", "==", "1", "else", "2", "# 1 for positive, 2 for negative", "\n", "", "valid_combinations", ".", "append", "(", "labels_for_numbers", ")", "\n", "", "", "", "", "return", "valid_combinations", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.find_valid_count": [[861, 868], ["enumerate", "valid_indices.append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "find_valid_count", "(", "count_numbers", ":", "List", "[", "int", "]", ",", "targets", ":", "List", "[", "int", "]", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "valid_indices", "=", "[", "]", "\n", "for", "index", ",", "number", "in", "enumerate", "(", "count_numbers", ")", ":", "\n", "            ", "if", "number", "in", "targets", ":", "\n", "                ", "valid_indices", ".", "append", "(", "index", ")", "\n", "", "", "return", "valid_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.convert_answer": [[869, 897], ["any", "answer_annotation[].values"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "convert_answer", "(", "answer_annotation", ":", "Dict", "[", "str", ",", "Union", "[", "str", ",", "Dict", ",", "List", "]", "]", ")", "->", "Tuple", "[", "str", ",", "List", "]", ":", "\n", "        ", "answer_type", "=", "None", "\n", "if", "answer_annotation", "[", "\"spans\"", "]", ":", "\n", "            ", "answer_type", "=", "\"spans\"", "\n", "", "elif", "answer_annotation", "[", "\"number\"", "]", ":", "\n", "            ", "answer_type", "=", "\"number\"", "\n", "", "elif", "any", "(", "answer_annotation", "[", "\"date\"", "]", ".", "values", "(", ")", ")", ":", "\n", "            ", "answer_type", "=", "\"date\"", "\n", "\n", "", "answer_content", "=", "answer_annotation", "[", "answer_type", "]", "if", "answer_type", "is", "not", "None", "else", "None", "\n", "\n", "answer_texts", "=", "[", "]", "\n", "if", "answer_type", "is", "None", ":", "# No answer", "\n", "            ", "pass", "\n", "", "elif", "answer_type", "==", "\"spans\"", ":", "\n", "# answer_content is a list of string in this case", "\n", "            ", "answer_texts", "=", "answer_content", "\n", "", "elif", "answer_type", "==", "\"date\"", ":", "\n", "# answer_content is a dict with \"month\", \"day\", \"year\" as the keys", "\n", "            ", "date_tokens", "=", "[", "\n", "answer_content", "[", "key", "]", "for", "key", "in", "[", "\"month\"", ",", "\"day\"", ",", "\"year\"", "]", "if", "key", "in", "answer_content", "and", "answer_content", "[", "key", "]", "\n", "]", "\n", "answer_texts", "=", "date_tokens", "\n", "", "elif", "answer_type", "==", "\"number\"", ":", "\n", "# answer_content is a string of number", "\n", "            ", "answer_texts", "=", "[", "answer_content", "]", "\n", "", "return", "answer_type", ",", "answer_texts", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.get_year_difference_candidates": [[898, 934], ["len", "itertools.product", "len", "numpy.zeros", "itertools.product", "date1.year_diff", "enumerate", "date1.year_diff", "year_differences.index", "year_differences.append"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.Date.year_diff", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.domain_languages.drop_language.Date.year_diff"], ["", "@", "staticmethod", "\n", "def", "get_year_difference_candidates", "(", "passage_date_objs", ":", "List", "[", "Date", "]", ")", "->", "Tuple", "[", "List", "[", "int", "]", ",", "np", ".", "array", "]", ":", "\n", "        ", "\"\"\" List of integers indicating all-possible year differences between the passage-dates\n            If year difference is not defined (year = -1) or negative, we don't consider such date-combinations\n\n            Returns the following:\n\n            Returns:\n            ---------\n            year_differences:\n                List[int] These are the possible year differences.\n            year_difference_mat: Binary np.array of shape (D, D, y_d)\n                Entry (i, j, k) == 1 denotes that D[i] - D[j] == year_differences[k]\n        \"\"\"", "\n", "num_date_objs", "=", "len", "(", "passage_date_objs", ")", "\n", "# Adding zero-first since it'll definitely be added and makes sanity-checking easy", "\n", "year_differences", ":", "List", "[", "int", "]", "=", "[", "0", "]", "\n", "\n", "# If any year is -1, we consider the year difference to be 0", "\n", "# If the year difference is negative, we consider the difference to be 0", "\n", "for", "(", "date1", ",", "date2", ")", "in", "itertools", ".", "product", "(", "passage_date_objs", ",", "repeat", "=", "2", ")", ":", "\n", "            ", "year_diff", "=", "date1", ".", "year_diff", "(", "date2", ")", "\n", "if", "year_diff", ">=", "0", ":", "\n", "                ", "if", "year_diff", "not", "in", "year_differences", ":", "\n", "                    ", "year_differences", ".", "append", "(", "year_diff", ")", "\n", "\n", "", "", "", "num_of_year_differences", "=", "len", "(", "year_differences", ")", "\n", "# Making year_difference_mat", "\n", "year_difference_mat", "=", "np", ".", "zeros", "(", "shape", "=", "(", "num_date_objs", ",", "num_date_objs", ",", "num_of_year_differences", ")", ",", "dtype", "=", "int", ")", "\n", "for", "(", "(", "date_idx1", ",", "date1", ")", ",", "(", "date_idx2", ",", "date2", ")", ")", "in", "itertools", ".", "product", "(", "enumerate", "(", "passage_date_objs", ")", ",", "repeat", "=", "2", ")", ":", "\n", "            ", "year_diff", "=", "date1", ".", "year_diff", "(", "date2", ")", "\n", "if", "year_diff", ">=", "0", ":", "\n", "                ", "year_diff_idx", "=", "year_differences", ".", "index", "(", "year_diff", ")", "# We know this will not fail", "\n", "year_difference_mat", "[", "date_idx1", ",", "date_idx2", ",", "year_diff_idx", "]", "=", "1", "\n", "\n", "", "", "return", "year_differences", ",", "year_difference_mat", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.get_passagenumber_difference_candidates": [[935, 972], ["len", "itertools.product", "len", "numpy.zeros", "itertools.product", "enumerate", "passage_number_differences.index", "passage_number_differences.append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_passagenumber_difference_candidates", "(", "passage_num_values", ":", "List", "[", "float", "]", ")", "->", "Tuple", "[", "List", "[", "float", "]", ",", "np", ".", "array", "]", ":", "\n", "        ", "\"\"\" List of numbers indicating all-possible non-negative subtractions between the passage-numbers\n\n            Parameters:\n            -----------\n            passage_num_values: List[float]\n                Sorted list of passage numbers\n            Returns:\n            --------\n            passage_number_differences:\n                List[float] These are the possible differences between passage numbers.\n            passagenumber_difference_mat: Binary np.array of shape (PN, PN, d)\n                Entry (i, j, k) == 1 denotes that PN[i] - PN[j] == passage_number_differences[k]\n        \"\"\"", "\n", "num_passage_numbers", "=", "len", "(", "passage_num_values", ")", "\n", "# Adding zero-first since it'll definitely be added and makes sanity-checking easy", "\n", "passage_number_differences", ":", "List", "[", "int", "]", "=", "[", "0", "]", "\n", "\n", "for", "(", "num1", ",", "num2", ")", "in", "itertools", ".", "product", "(", "passage_num_values", ",", "repeat", "=", "2", ")", ":", "\n", "            ", "number_diff", "=", "num1", "-", "num2", "\n", "if", "number_diff", ">=", "0", ":", "\n", "                ", "if", "number_diff", "not", "in", "passage_number_differences", ":", "\n", "                    ", "passage_number_differences", ".", "append", "(", "number_diff", ")", "\n", "\n", "", "", "", "num_of_passagenum_differences", "=", "len", "(", "passage_number_differences", ")", "\n", "# Making year_difference_mat", "\n", "passage_number_diff_mat", "=", "np", ".", "zeros", "(", "\n", "shape", "=", "(", "num_passage_numbers", ",", "num_passage_numbers", ",", "num_of_passagenum_differences", ")", ",", "dtype", "=", "int", "\n", ")", "\n", "for", "(", "(", "num_idx1", ",", "num1", ")", ",", "(", "num_idx2", ",", "num2", ")", ")", "in", "itertools", ".", "product", "(", "enumerate", "(", "passage_num_values", ")", ",", "repeat", "=", "2", ")", ":", "\n", "            ", "number_diff", "=", "num1", "-", "num2", "\n", "if", "number_diff", ">=", "0", ":", "\n", "                ", "num_diff_idx", "=", "passage_number_differences", ".", "index", "(", "number_diff", ")", "# We know this will not fail", "\n", "passage_number_diff_mat", "[", "num_idx1", ",", "num_idx2", ",", "num_diff_idx", "]", "=", "1", "\n", "\n", "", "", "return", "passage_number_differences", ",", "passage_number_diff_mat", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.get_numberindices_in_sorted_order": [[973, 994], ["list", "sorted", "zip", "zip"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_numberindices_in_sorted_order", "(", "number_values", ",", "number_token_indices", ",", "passage_number_entidxs", ")", ":", "\n", "        ", "\"\"\" Returns the number_token_indices in an order so that their values are sorted in increasing order.\n\n        For example,\n        number_values: [4, 1, 5]\n        number_token_indices: [10, 15, 20, 25]\n        passage_number_entidxs: [2, 0, 1, 0]\n\n        Underlying values for indices are: [5, 4, 1, 4]\n\n        Output sorted_number_indices: [20, 15, 25, 10]\n        with underlying values: [1, 4, 4, 5]\n        \"\"\"", "\n", "# These are the number-values the number-tokens", "\n", "number_token_numbervalues", "=", "[", "number_values", "[", "x", "]", "for", "x", "in", "passage_number_entidxs", "]", "\n", "\n", "number_tokenidx_values", "=", "list", "(", "zip", "(", "number_token_indices", ",", "number_token_numbervalues", ")", ")", "\n", "sorted_numberidx_value_tuples", "=", "sorted", "(", "number_tokenidx_values", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ")", "\n", "sorted_number_indices", ",", "_", "=", "zip", "(", "*", "sorted_numberidx_value_tuples", ")", "\n", "return", "sorted_number_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.get_candidate_additions": [[995, 1006], ["collections.defaultdict", "zip", "zip", "candidate_additions[].append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_candidate_additions", "(", "\n", "numbers_in_passage", ":", "List", "[", "int", "]", ",", "number_indices", ":", "List", "[", "int", "]", "\n", ")", "->", "Dict", "[", "int", ",", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", ":", "\n", "        ", "candidate_additions", "=", "defaultdict", "(", "list", ")", "\n", "\n", "for", "number_1", ",", "index_1", "in", "zip", "(", "numbers_in_passage", ",", "number_indices", ")", ":", "\n", "            ", "for", "number_2", ",", "index_2", "in", "zip", "(", "numbers_in_passage", ",", "number_indices", ")", ":", "\n", "                ", "result", "=", "number_1", "+", "number_2", "\n", "candidate_additions", "[", "result", "]", ".", "append", "(", "(", "index_1", ",", "index_2", ")", ")", "\n", "", "", "return", "candidate_additions", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.get_candidate_subtractions": [[1007, 1018], ["collections.defaultdict", "zip", "zip", "candidate_subtractions[].append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_candidate_subtractions", "(", "\n", "numbers_in_passage", ":", "List", "[", "int", "]", ",", "number_indices", ":", "List", "[", "int", "]", "\n", ")", "->", "Dict", "[", "int", ",", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", ":", "\n", "        ", "candidate_subtractions", "=", "defaultdict", "(", "list", ")", "\n", "\n", "for", "number_1", ",", "index_1", "in", "zip", "(", "numbers_in_passage", ",", "number_indices", ")", ":", "\n", "            ", "for", "number_2", ",", "index_2", "in", "zip", "(", "numbers_in_passage", ",", "number_indices", ")", ":", "\n", "                ", "result", "=", "number_1", "-", "number_2", "\n", "candidate_subtractions", "[", "result", "]", ".", "append", "(", "(", "index_1", ",", "index_2", ")", ")", "\n", "", "", "return", "candidate_subtractions", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.get_gold_action_seqs": [[1019, 1079], ["gold_actionseq_idxs.append", "gold_actionseq_mask.append", "gold_start_types.append", "gold_actionseq_idxs.append", "gold_actionseq_mask.append", "gold_start_types.append", "logger.error", "len", "language.logical_form_to_action_sequence", "gold_actionseq_idxs.append", "gold_actionseq_mask.append", "range", "len"], "methods", ["None"], ["", "def", "get_gold_action_seqs", "(", "\n", "self", ",", "\n", "program_supervised", ":", "bool", ",", "\n", "qtype", ":", "str", ",", "\n", "question_tokens", ":", "List", "[", "str", "]", ",", "\n", "language", ":", "DropLanguage", ",", "\n", "action2idx_map", ":", "Dict", "[", "str", ",", "int", "]", ",", "\n", ")", "->", "Tuple", "[", "List", "[", "List", "[", "int", "]", "]", ",", "List", "[", "List", "[", "int", "]", "]", ",", "List", "[", "str", "]", ",", "bool", "]", ":", "\n", "\n", "        ", "qtype_to_lffunc", "=", "{", "\n", "constants", ".", "DATECOMP_QTYPE", ":", "self", ".", "datecomp_logicalforms", ",", "\n", "constants", ".", "NUMCOMP_QTYPE", ":", "self", ".", "numcomp_logicalforms", ",", "\n", "constants", ".", "NUM_find_qtype", ":", "self", ".", "findnum_logicalforms", ",", "\n", "constants", ".", "NUM_filter_find_qtype", ":", "self", ".", "filterfindnum_logicalforms", ",", "\n", "constants", ".", "MIN_find_qtype", ":", "self", ".", "minnum_find_logicalforms", ",", "\n", "constants", ".", "MIN_filter_find_qtype", ":", "self", ".", "minnum_filterfind_logicalforms", ",", "\n", "constants", ".", "MAX_find_qtype", ":", "self", ".", "maxnum_find_logicalforms", ",", "\n", "constants", ".", "MAX_filter_find_qtype", ":", "self", ".", "maxnum_filterfind_logicalforms", ",", "\n", "constants", ".", "COUNT_find_qtype", ":", "self", ".", "count_find_logicalforms", ",", "\n", "constants", ".", "COUNT_filter_find_qtype", ":", "self", ".", "count_filterfind_logicalforms", ",", "\n", "constants", ".", "RELOC_find_qtype", ":", "self", ".", "relocate_logicalforms", ",", "\n", "constants", ".", "RELOC_filterfind_qtype", ":", "self", ".", "relocate_logicalforms", ",", "\n", "constants", ".", "RELOC_maxfind_qtype", ":", "self", ".", "relocate_logicalforms", ",", "\n", "constants", ".", "RELOC_maxfilterfind_qtype", ":", "self", ".", "relocate_logicalforms", ",", "\n", "constants", ".", "RELOC_minfind_qtype", ":", "self", ".", "relocate_logicalforms", ",", "\n", "constants", ".", "RELOC_minfilterfind_qtype", ":", "self", ".", "relocate_logicalforms", ",", "\n", "constants", ".", "YEARDIFF_SE_qtype", ":", "self", ".", "yeardiff_singleevent_logicalforms", ",", "\n", "constants", ".", "YEARDIFF_TE_qtype", ":", "self", ".", "yeardiff_twoevent_logicalforms", ",", "\n", "}", "\n", "\n", "gold_actionseq_idxs", ":", "List", "[", "List", "[", "int", "]", "]", "=", "[", "]", "\n", "gold_actionseq_mask", ":", "List", "[", "List", "[", "int", "]", "]", "=", "[", "]", "\n", "gold_start_types", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "\n", "if", "not", "program_supervised", ":", "\n", "            ", "gold_actionseq_idxs", ".", "append", "(", "[", "0", "]", ")", "\n", "gold_actionseq_mask", ".", "append", "(", "[", "0", "]", ")", "\n", "gold_start_types", ".", "append", "(", "\"UNK\"", ")", "\n", "return", "(", "gold_actionseq_idxs", ",", "gold_actionseq_mask", ",", "gold_start_types", ",", "program_supervised", ")", "\n", "\n", "", "if", "qtype", "in", "qtype_to_lffunc", ":", "\n", "# Tuple[List[str], List[str]]", "\n", "            ", "(", "gold_logical_forms", ",", "gold_start_types", ")", "=", "qtype_to_lffunc", "[", "qtype", "]", "(", "\n", "question_tokens", "=", "question_tokens", ",", "language", "=", "language", ",", "qtype", "=", "qtype", "\n", ")", "\n", "assert", "len", "(", "gold_logical_forms", ")", ">=", "1", ",", "f\"No logical forms found for: {question_tokens}\"", "\n", "for", "logical_form", "in", "gold_logical_forms", ":", "\n", "                ", "gold_actions", ":", "List", "[", "str", "]", "=", "language", ".", "logical_form_to_action_sequence", "(", "logical_form", ")", "\n", "actionseq_idxs", ":", "List", "[", "int", "]", "=", "[", "action2idx_map", "[", "a", "]", "for", "a", "in", "gold_actions", "]", "\n", "actionseq_mask", ":", "List", "[", "int", "]", "=", "[", "1", "for", "_", "in", "range", "(", "len", "(", "actionseq_idxs", ")", ")", "]", "\n", "gold_actionseq_idxs", ".", "append", "(", "actionseq_idxs", ")", "\n", "gold_actionseq_mask", ".", "append", "(", "actionseq_mask", ")", "\n", "", "", "else", ":", "\n", "            ", "program_supervised", "=", "False", "\n", "gold_actionseq_idxs", ".", "append", "(", "[", "0", "]", ")", "\n", "gold_actionseq_mask", ".", "append", "(", "[", "0", "]", ")", "\n", "gold_start_types", ".", "append", "(", "\"UNK\"", ")", "\n", "logger", ".", "error", "(", "f\"Tried get gold logical form for: {qtype}\"", ")", "\n", "\n", "", "return", "(", "gold_actionseq_idxs", ",", "gold_actionseq_mask", ",", "gold_start_types", ",", "program_supervised", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.filter_passageattn_lf": [[1080, 1084], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "filter_passageattn_lf", "(", ")", "->", "str", ":", "\n", "        ", "gold_lf", "=", "\"(filter_PassageAttention find_PassageAttention)\"", "\n", "return", "gold_lf", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.findnum_logicalforms": [[1085, 1089], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "findnum_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "gold_lf", "=", "\"(find_PassageNumber find_PassageAttention)\"", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"passage_number\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.filterfindnum_logicalforms": [[1090, 1095], ["drop_reader.DROPReaderNew.filter_passageattn_lf"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.filter_passageattn_lf"], ["", "@", "staticmethod", "\n", "def", "filterfindnum_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "filter_passage_attention_lf", "=", "DROPReaderNew", ".", "filter_passageattn_lf", "(", ")", "\n", "gold_lf", "=", "f\"(find_PassageNumber {filter_passage_attention_lf})\"", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"passage_number\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.minnum_find_logicalforms": [[1096, 1100], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "minnum_find_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "gold_lf", "=", "f\"(find_PassageNumber (minNumPattn find_PassageAttention))\"", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"passage_number\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.minnum_filterfind_logicalforms": [[1101, 1106], ["drop_reader.DROPReaderNew.filter_passageattn_lf"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.filter_passageattn_lf"], ["", "@", "staticmethod", "\n", "def", "minnum_filterfind_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "filter_passage_attention_lf", "=", "DROPReaderNew", ".", "filter_passageattn_lf", "(", ")", "\n", "gold_lf", "=", "f\"(find_PassageNumber (minNumPattn {filter_passage_attention_lf}))\"", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"passage_number\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.maxnum_find_logicalforms": [[1107, 1111], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "maxnum_find_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "gold_lf", "=", "f\"(find_PassageNumber (maxNumPattn find_PassageAttention))\"", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"passage_number\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.maxnum_filterfind_logicalforms": [[1112, 1117], ["drop_reader.DROPReaderNew.filter_passageattn_lf"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.filter_passageattn_lf"], ["", "@", "staticmethod", "\n", "def", "maxnum_filterfind_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "filter_passage_attention_lf", "=", "DROPReaderNew", ".", "filter_passageattn_lf", "(", ")", "\n", "gold_lf", "=", "f\"(find_PassageNumber (maxNumPattn {filter_passage_attention_lf}))\"", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"passage_number\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.count_find_logicalforms": [[1118, 1122], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "count_find_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "gold_lf", "=", "\"(passageAttn2Count find_PassageAttention)\"", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"count_number\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.count_filterfind_logicalforms": [[1123, 1128], ["drop_reader.DROPReaderNew.filter_passageattn_lf"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.filter_passageattn_lf"], ["", "@", "staticmethod", "\n", "def", "count_filterfind_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "filter_passageattn_lf", "=", "DROPReaderNew", ".", "filter_passageattn_lf", "(", ")", "\n", "gold_lf", "=", "f\"(passageAttn2Count {filter_passageattn_lf})\"", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"count_number\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.relocate_logicalforms": [[1129, 1162], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "relocate_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "qtype", "=", "kwargs", "[", "\"qtype\"", "]", "\n", "# Could be one of", "\n", "# 'relocate_filterfind_qtype', 'relocate_minfind_qtype', 'relocate_maxfind_qtype',", "\n", "# 'relocate_maxfilterfind_qtype', 'relocate_find_qtype', 'relocate_minfilterfind_qtype'", "\n", "\n", "find", "=", "\"find_PassageAttention\"", "\n", "filterfind", "=", "\"(filter_PassageAttention find_PassageAttention)\"", "\n", "maxfind", "=", "\"(maxNumPattn find_PassageAttention)\"", "\n", "maxfilterfind", "=", "f\"(maxNumPattn {filterfind})\"", "\n", "minfind", "=", "\"(minNumPattn find_PassageAttention)\"", "\n", "minfilterfind", "=", "f\"(minNumPattn {filterfind})\"", "\n", "\n", "outer_leftside", "=", "\"(find_passageSpanAnswer (relocate_PassageAttention \"", "\n", "outer_rightside", "=", "\"))\"", "\n", "\n", "if", "qtype", "==", "constants", ".", "RELOC_find_qtype", ":", "\n", "            ", "gold_lf", "=", "outer_leftside", "+", "find", "+", "outer_rightside", "\n", "", "elif", "qtype", "==", "constants", ".", "RELOC_filterfind_qtype", ":", "\n", "            ", "gold_lf", "=", "outer_leftside", "+", "filterfind", "+", "outer_rightside", "\n", "", "elif", "qtype", "==", "constants", ".", "RELOC_maxfind_qtype", ":", "\n", "            ", "gold_lf", "=", "outer_leftside", "+", "maxfind", "+", "outer_rightside", "\n", "", "elif", "qtype", "==", "constants", ".", "RELOC_maxfilterfind_qtype", ":", "\n", "            ", "gold_lf", "=", "outer_leftside", "+", "maxfilterfind", "+", "outer_rightside", "\n", "", "elif", "qtype", "==", "constants", ".", "RELOC_minfind_qtype", ":", "\n", "            ", "gold_lf", "=", "outer_leftside", "+", "minfind", "+", "outer_rightside", "\n", "", "elif", "qtype", "==", "constants", ".", "RELOC_minfilterfind_qtype", ":", "\n", "            ", "gold_lf", "=", "outer_leftside", "+", "minfilterfind", "+", "outer_rightside", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "", "return", "[", "gold_lf", "]", ",", "[", "\"passage_span\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.yeardiff_singleevent_logicalforms": [[1163, 1169], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "yeardiff_singleevent_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "qtype", "=", "kwargs", "[", "\"qtype\"", "]", "\n", "gold_lf", "=", "\"(year_difference_single_event find_PassageAttention)\"", "\n", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"year_difference\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.yeardiff_twoevent_logicalforms": [[1170, 1176], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "yeardiff_twoevent_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "qtype", "=", "kwargs", "[", "\"qtype\"", "]", "\n", "gold_lf", "=", "\"(year_difference find_PassageAttention find_PassageAttention)\"", "\n", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"year_difference\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.numdiff_logicalforms": [[1177, 1211], ["qtype.split"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "numdiff_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "qtype", "=", "kwargs", "[", "\"qtype\"", "]", "\n", "# Qtype of form: diff_maxmin_qtype", "\n", "numtypes", "=", "qtype", ".", "split", "(", "\"_\"", ")", "[", "1", "]", "\n", "first_num", "=", "numtypes", "[", "0", ":", "3", "]", "# first 3 chars", "\n", "second_num", "=", "numtypes", "[", "3", ":", "6", "]", "# last 3 chars", "\n", "\n", "max_num_program", "=", "\"(max_PassageNumber (find_PassageNumber find_PassageAttention))\"", "\n", "min_num_program", "=", "\"(min_PassageNumber (find_PassageNumber find_PassageAttention))\"", "\n", "find_num_program", "=", "\"(find_PassageNumber find_PassageAttention)\"", "\n", "\n", "if", "first_num", "==", "\"max\"", ":", "\n", "            ", "first_num_prog", "=", "max_num_program", "\n", "", "elif", "first_num", "==", "\"min\"", ":", "\n", "            ", "first_num_prog", "=", "min_num_program", "\n", "", "elif", "first_num", "==", "\"num\"", ":", "\n", "            ", "first_num_prog", "=", "find_num_program", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "", "if", "second_num", "==", "\"max\"", ":", "\n", "            ", "second_num_prog", "=", "max_num_program", "\n", "", "elif", "second_num", "==", "\"min\"", ":", "\n", "            ", "second_num_prog", "=", "min_num_program", "\n", "", "elif", "second_num", "==", "\"num\"", ":", "\n", "            ", "second_num_prog", "=", "find_num_program", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "# \"(passagenumber_difference first_num_prog second_num_program)\"", "\n", "", "gold_lf", "=", "f\"(passagenumber_difference {first_num_prog} {second_num_prog})\"", "\n", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"passagenum_diff\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.yardsshortest_logicalforms": [[1212, 1216], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "yardsshortest_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "gold_lf", "=", "\"(min_PassageNumber (find_PassageNumber find_PassageAttention))\"", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"passage_number\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.yardslongest_logicalforms": [[1217, 1221], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "yardslongest_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "gold_lf", "=", "\"(max_PassageNumber (find_PassageNumber find_PassageAttention))\"", "\n", "return", "[", "gold_lf", "]", ",", "[", "\"passage_number\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.datecomp_logicalforms": [[1222, 1264], ["language.all_possible_productions", "gold_logical_forms.append", "gold_start_types.append", "language.all_possible_productions", "gold_logical_forms.append", "gold_start_types.append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "datecomp_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "question_tokens", ":", "List", "[", "str", "]", "=", "kwargs", "[", "\"question_tokens\"", "]", "\n", "language", ":", "DropLanguage", "=", "kwargs", "[", "\"language\"", "]", "\n", "# \"(find_passageSpanAnswer (compare_date_greater_than find_PassageAttention find_PassageAttention))\"", "\n", "psa_start", "=", "\"(find_passageSpanAnswer (\"", "\n", "qsa_start", "=", "\"(find_questionSpanAnswer (\"", "\n", "# lf1 = \"(find_passageSpanAnswer (\"", "\n", "\n", "lf2", "=", "\" find_PassageAttention find_PassageAttention))\"", "\n", "greater_than", "=", "\"compare_date_greater_than\"", "\n", "lesser_than", "=", "\"compare_date_lesser_than\"", "\n", "\n", "# Correct if Attn1 is first event", "\n", "lesser_tokens", "=", "[", "\"first\"", ",", "\"earlier\"", ",", "\"forst\"", ",", "\"firts\"", "]", "\n", "greater_tokens", "=", "[", "\"later\"", ",", "\"last\"", ",", "\"second\"", "]", "\n", "\n", "operator_action", "=", "None", "\n", "\n", "for", "t", "in", "lesser_tokens", ":", "\n", "            ", "if", "t", "in", "question_tokens", ":", "\n", "                ", "operator_action", "=", "lesser_than", "\n", "break", "\n", "\n", "", "", "for", "t", "in", "greater_tokens", ":", "\n", "            ", "if", "t", "in", "question_tokens", ":", "\n", "                ", "operator_action", "=", "greater_than", "\n", "break", "\n", "\n", "", "", "if", "operator_action", "is", "None", ":", "\n", "            ", "operator_action", "=", "greater_than", "\n", "\n", "", "gold_logical_forms", "=", "[", "]", "\n", "gold_start_types", "=", "[", "]", "\n", "if", "\"@start@ -> PassageSpanAnswer\"", "in", "language", ".", "all_possible_productions", "(", ")", ":", "\n", "            ", "gold_logical_forms", ".", "append", "(", "f\"{psa_start}{operator_action}{lf2}\"", ")", "\n", "gold_start_types", ".", "append", "(", "\"passage_span\"", ")", "# from drop_parser.get_valid_start_actionids", "\n", "", "if", "\"@start@ -> QuestionSpanAnswer\"", "in", "language", ".", "all_possible_productions", "(", ")", ":", "\n", "            ", "gold_logical_forms", ".", "append", "(", "f\"{qsa_start}{operator_action}{lf2}\"", ")", "\n", "gold_start_types", ".", "append", "(", "\"question_span\"", ")", "# from drop_parser.get_valid_start_actionids", "\n", "\n", "", "return", "gold_logical_forms", ",", "gold_start_types", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.numcomp_logicalforms": [[1265, 1308], ["language.all_possible_productions", "gold_logical_forms.append", "gold_start_types.append", "language.all_possible_productions", "gold_logical_forms.append", "gold_start_types.append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "numcomp_logicalforms", "(", "**", "kwargs", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "question_tokens", ":", "List", "[", "str", "]", "=", "kwargs", "[", "\"question_tokens\"", "]", "\n", "language", ":", "DropLanguage", "=", "kwargs", "[", "\"language\"", "]", "\n", "# \"(find_passageSpanAnswer (compare_date_greater_than find_PassageAttention find_PassageAttention))\"", "\n", "psa_start", "=", "\"(find_passageSpanAnswer (\"", "\n", "qsa_start", "=", "\"(find_questionSpanAnswer (\"", "\n", "\n", "lf2", "=", "\" find_PassageAttention find_PassageAttention))\"", "\n", "greater_than", "=", "\"compare_num_greater_than\"", "\n", "lesser_than", "=", "\"compare_num_lesser_than\"", "\n", "\n", "# Correct if Attn1 is first event", "\n", "greater_tokens", "=", "[", "\"larger\"", ",", "\"more\"", ",", "\"largest\"", ",", "\"bigger\"", ",", "\"higher\"", ",", "\"highest\"", ",", "\"most\"", ",", "\"greater\"", "]", "\n", "lesser_tokens", "=", "[", "\"smaller\"", ",", "\"fewer\"", ",", "\"lowest\"", ",", "\"smallest\"", ",", "\"less\"", ",", "\"least\"", ",", "\"fewest\"", ",", "\"lower\"", "]", "\n", "\n", "operator_action", "=", "None", "\n", "\n", "for", "t", "in", "lesser_tokens", ":", "\n", "            ", "if", "t", "in", "question_tokens", ":", "\n", "                ", "operator_action", "=", "lesser_than", "\n", "break", "\n", "# return [f\"{psa_start}{lesser_than}{lf2}\", f\"{qsa_start}{lesser_than}{lf2}\"]", "\n", "", "", "if", "operator_action", "is", "None", ":", "\n", "            ", "for", "t", "in", "greater_tokens", ":", "\n", "                ", "if", "t", "in", "question_tokens", ":", "\n", "                    ", "operator_action", "=", "greater_than", "\n", "break", "\n", "# return [f\"{psa_start}{greater_than}{lf2}\", f\"{qsa_start}{greater_than}{lf2}\"]", "\n", "\n", "", "", "", "if", "operator_action", "is", "None", ":", "\n", "            ", "operator_action", "=", "greater_than", "\n", "\n", "", "gold_logical_forms", "=", "[", "]", "\n", "gold_start_types", "=", "[", "]", "\n", "if", "\"@start@ -> PassageSpanAnswer\"", "in", "language", ".", "all_possible_productions", "(", ")", ":", "\n", "            ", "gold_logical_forms", ".", "append", "(", "f\"{psa_start}{operator_action}{lf2}\"", ")", "\n", "gold_start_types", ".", "append", "(", "\"passage_span\"", ")", "# from drop_parser.get_valid_start_actionids", "\n", "", "if", "\"@start@ -> QuestionSpanAnswer\"", "in", "language", ".", "all_possible_productions", "(", ")", ":", "\n", "            ", "gold_logical_forms", ".", "append", "(", "f\"{qsa_start}{operator_action}{lf2}\"", ")", "\n", "gold_start_types", ".", "append", "(", "\"question_span\"", ")", "# from drop_parser.get_valid_start_actionids", "\n", "\n", "", "return", "gold_logical_forms", ",", "gold_start_types", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.make_count_instance": [[1309, 1371], ["len", "random.random", "len", "random.randint", "relevant_spans[].split", "drop_reader.DROPReaderNew.contains", "len", "len", "len", "len", "random.shuffle", "random.randint", "min", "random.randint", "len", "range", "len"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.contains"], ["", "def", "make_count_instance", "(", "self", ",", "passage_tokens", ":", "List", "[", "str", "]", ")", ":", "\n", "        ", "\"\"\" output an attention, count_answer, mask. Mask is when we don;t find relevant spans \"\"\"", "\n", "\n", "# We would like to count these spans", "\n", "relevant_spans", "=", "[", "\"TD pass\"", ",", "\"TD run\"", ",", "\"touchdown pass\"", ",", "\"field goal\"", ",", "\"touchdown run\"", "]", "\n", "num_relevant_spans", "=", "len", "(", "relevant_spans", ")", "\n", "\n", "attention", "=", "[", "0.0", "]", "*", "len", "(", "passage_tokens", ")", "\n", "\n", "# With 10% prob select no span", "\n", "count_zero_prob", "=", "random", ".", "random", "(", ")", "\n", "if", "count_zero_prob", "<", "0.1", ":", "\n", "            ", "return", "(", "attention", ",", "0", ",", "1", ")", "\n", "\n", "# Choose a particular type of span from relevant ones and find it's starting positions", "\n", "", "tries", "=", "0", "\n", "starting_positions_in_passage", "=", "[", "]", "\n", "while", "len", "(", "starting_positions_in_passage", ")", "==", "0", "and", "tries", "<", "5", ":", "\n", "            ", "choosen_span", "=", "random", ".", "randint", "(", "0", ",", "num_relevant_spans", "-", "1", ")", "\n", "span_tokens", "=", "relevant_spans", "[", "choosen_span", "]", ".", "split", "(", "\" \"", ")", "\n", "starting_positions_in_passage", "=", "self", ".", "contains", "(", "span_tokens", ",", "passage_tokens", ")", "\n", "tries", "+=", "1", "\n", "\n", "# even after 5 tries, span to count not found. Return masked attention", "\n", "", "if", "len", "(", "starting_positions_in_passage", ")", "==", "0", ":", "\n", "            ", "return", "attention", ",", "0", ",", "0", "\n", "\n", "# # TO save from infinite loop", "\n", "# count_zero_prob = random.random()", "\n", "# if count_zero_prob < 0.1:", "\n", "#     return attention, 0", "\n", "\n", "", "if", "len", "(", "starting_positions_in_passage", ")", "==", "1", ":", "\n", "            ", "count", "=", "len", "(", "starting_positions_in_passage", ")", "\n", "starting_position", "=", "starting_positions_in_passage", "[", "0", "]", "\n", "attention", "[", "starting_position", "]", "=", "1.0", "\n", "attention", "[", "starting_position", "+", "1", "]", "=", "1.0", "\n", "\n", "", "else", ":", "\n", "            ", "num_of_spans_found", "=", "len", "(", "starting_positions_in_passage", ")", "\n", "# Choose a subset of the starting_positions", "\n", "random", ".", "shuffle", "(", "starting_positions_in_passage", ")", "\n", "num_spans", "=", "random", ".", "randint", "(", "2", ",", "num_of_spans_found", ")", "\n", "num_spans", "=", "min", "(", "num_spans", ",", "9", ")", "\n", "\n", "count", "=", "num_spans", "\n", "\n", "spread_len", "=", "random", ".", "randint", "(", "1", ",", "3", ")", "\n", "\n", "chosen_starting_positions", "=", "starting_positions_in_passage", "[", "0", ":", "num_spans", "]", "\n", "for", "starting_position", "in", "chosen_starting_positions", ":", "\n", "                ", "attention", "[", "starting_position", "]", "=", "1.0", "\n", "attention", "[", "starting_position", "+", "1", "]", "=", "1.0", "\n", "for", "i", "in", "range", "(", "1", ",", "spread_len", "+", "1", ")", ":", "\n", "                    ", "prev_idx", "=", "starting_position", "-", "i", "\n", "if", "prev_idx", ">=", "0", ":", "\n", "                        ", "attention", "[", "prev_idx", "]", "=", "0.5", "\n", "", "next_idx", "=", "starting_position", "+", "1", "+", "i", "\n", "if", "next_idx", "<", "len", "(", "passage_tokens", ")", ":", "\n", "                        ", "attention", "[", "next_idx", "]", "=", "0.5", "\n", "\n", "", "", "", "", "return", "attention", ",", "count", ",", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.dataset_readers.drop_reader.DROPReaderNew.contains": [[1372, 1383], ["range", "range", "len", "starting_positions.append", "len", "len"], "methods", ["None"], ["", "def", "contains", "(", "self", ",", "small", ",", "big", ")", ":", "\n", "        ", "starting_positions", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "big", ")", "-", "len", "(", "small", ")", "+", "1", ")", ":", "\n", "            ", "start", "=", "True", "\n", "for", "j", "in", "range", "(", "len", "(", "small", ")", ")", ":", "\n", "                ", "if", "big", "[", "i", "+", "j", "]", "!=", "small", "[", "j", "]", ":", "\n", "                    ", "start", "=", "False", "\n", "break", "\n", "", "", "if", "start", ":", "\n", "                ", "starting_positions", ".", "append", "(", "i", ")", "\n", "", "", "return", "starting_positions", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.iterators.filter_iterator.DataFilterIterator.__init__": [[27, 54], ["allennlp.data.iterators.data_iterator.DataIterator.__init__"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "batch_size", ":", "int", "=", "32", ",", "\n", "instances_per_epoch", ":", "int", "=", "None", ",", "\n", "max_instances_in_memory", ":", "int", "=", "None", ",", "\n", "cache_instances", ":", "bool", "=", "False", ",", "\n", "track_epoch", ":", "bool", "=", "False", ",", "\n", "maximum_samples_per_batch", ":", "Tuple", "[", "str", ",", "int", "]", "=", "None", ",", "\n", "filter_key", ":", "str", "=", "\"strongly_supervised\"", ",", "\n", "supervision_keys", ":", "List", "[", "str", "]", "=", "[", "\"program_supervised\"", ",", "\"qattn_supervised\"", ",", "\"execution_supervised\"", "]", ",", "\n", "filter_instances", ":", "bool", "=", "False", ",", "\n", "filter_for_epochs", ":", "int", "=", "0", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", "DataFilterIterator", ",", "self", ")", ".", "__init__", "(", "\n", "batch_size", "=", "batch_size", ",", "\n", "instances_per_epoch", "=", "instances_per_epoch", ",", "\n", "max_instances_in_memory", "=", "max_instances_in_memory", ",", "\n", "cache_instances", "=", "cache_instances", ",", "\n", "track_epoch", "=", "track_epoch", ",", "\n", "maximum_samples_per_batch", "=", "maximum_samples_per_batch", ",", "\n", ")", "\n", "\n", "self", ".", "filter_instances", "=", "filter_instances", "\n", "# This is the field_name in the instances that contains filter-ing bool", "\n", "self", ".", "filter_key", "=", "filter_key", "\n", "self", ".", "filter_for_epochs", "=", "filter_for_epochs", "\n", "self", ".", "supervision_keys", "=", "supervision_keys", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.iterators.filter_iterator.DataFilterIterator._create_batches": [[55, 114], ["filter_iterator.DataFilterIterator._memory_sized_lists", "print", "list", "collections.defaultdict", "collections.defaultdict", "print", "print", "print", "iter", "collections.deque", "allennlp.common.util.lazy_groups_of", "filter_iterator.DataFilterIterator._epochs.values", "len", "random.shuffle", "filter_iterator.DataFilterIterator._ensure_batch_is_sufficiently_small", "allennlp.data.dataset.Batch", "allennlp.data.dataset.Batch", "len", "any", "filtered_instance_list.append", "len"], "methods", ["None"], ["", "def", "_create_batches", "(", "self", ",", "instances", ":", "Iterable", "[", "Instance", "]", ",", "shuffle", ":", "bool", ")", "->", "Iterable", "[", "Batch", "]", ":", "\n", "# First break the dataset into memory-sized lists:", "\n", "        ", "for", "instance_list", "in", "self", ".", "_memory_sized_lists", "(", "instances", ")", ":", "\n", "            ", "instances_w_epoch_num", "=", "0", "\n", "for", "instance", "in", "instances", ":", "\n", "                ", "if", "\"epoch_num\"", "in", "instance", ".", "fields", ":", "\n", "                    ", "instances_w_epoch_num", "+=", "1", "\n", "\n", "", "", "print", "(", "f\"\\nInstances: {len(instance_list)}\"", ")", "\n", "\n", "epochs_list", "=", "list", "(", "self", ".", "_epochs", ".", "values", "(", ")", ")", "\n", "assert", "len", "(", "epochs_list", ")", "==", "1", ",", "f\"Multiple epoch keys: {self._epochs}\"", "\n", "epoch_num", "=", "epochs_list", "[", "0", "]", "\n", "if", "self", ".", "_track_epoch", ":", "\n", "                ", "for", "instance", "in", "instance_list", ":", "\n", "                    ", "instance", ".", "fields", "[", "\"epoch_num\"", "]", "=", "epoch_num", "\n", "\n", "", "", "supervision_dict", "=", "defaultdict", "(", "int", ")", "\n", "qtype_dict", "=", "defaultdict", "(", "int", ")", "\n", "for", "instance", "in", "instance_list", ":", "\n", "                ", "for", "key", "in", "self", ".", "supervision_keys", ":", "\n", "                    ", "supervision_dict", "[", "key", "]", "+=", "1", "if", "instance", "[", "key", "]", ".", "metadata", "else", "0", "\n", "", "qtype_dict", "[", "instance", "[", "\"qtypes\"", "]", ".", "metadata", "]", "+=", "1", "\n", "\n", "", "print", "(", "f\"QType: {qtype_dict}\"", ")", "\n", "\n", "# These QType instances will not be kept in the first curriculum even if supervised", "\n", "NO_CURRICULUM", "=", "[", "\n", "constants", ".", "COUNT_filter_find_qtype", ",", "\n", "constants", ".", "MAX_filter_find_qtype", ",", "\n", "constants", ".", "MIN_filter_find_qtype", ",", "\n", "constants", ".", "NUM_filter_find_qtype", ",", "\n", "]", "\n", "\n", "filtered_instance_list", "=", "[", "]", "\n", "if", "self", ".", "filter_instances", "and", "epoch_num", "<", "self", ".", "filter_for_epochs", ":", "\n", "                ", "for", "instance", "in", "instance_list", ":", "\n", "                    ", "if", "(", "\n", "any", "(", "instance", "[", "key", "]", ".", "metadata", "is", "True", "for", "key", "in", "self", ".", "supervision_keys", ")", "\n", "and", "not", "instance", "[", "\"qtypes\"", "]", ".", "metadata", "in", "NO_CURRICULUM", "\n", ")", ":", "\n", "                        ", "filtered_instance_list", ".", "append", "(", "instance", ")", "\n", "", "", "", "else", ":", "\n", "                ", "filtered_instance_list", "=", "instance_list", "\n", "\n", "", "print", "(", "f\"SupervisionDict: {supervision_dict}\"", ")", "\n", "print", "(", "f\"Filtered Instances: {len(filtered_instance_list)}\"", ")", "\n", "\n", "if", "shuffle", ":", "\n", "                ", "random", ".", "shuffle", "(", "filtered_instance_list", ")", "\n", "", "iterator", "=", "iter", "(", "filtered_instance_list", ")", "\n", "excess", ":", "Deque", "[", "Instance", "]", "=", "deque", "(", ")", "\n", "# Then break each memory-sized list into batches.", "\n", "for", "batch_instances", "in", "lazy_groups_of", "(", "iterator", ",", "self", ".", "_batch_size", ")", ":", "\n", "                ", "for", "possibly_smaller_batches", "in", "self", ".", "_ensure_batch_is_sufficiently_small", "(", "batch_instances", ",", "excess", ")", ":", "\n", "                    ", "batch", "=", "Batch", "(", "possibly_smaller_batches", ")", "\n", "yield", "batch", "\n", "", "", "if", "excess", ":", "\n", "                ", "yield", "Batch", "(", "excess", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.state_machines.constrained_beam_search.FirstStepConstrainedBeamSearch.__init__": [[38, 41], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "beam_size", ":", "int", ",", "per_node_beam_size", ":", "int", "=", "None", ")", "->", "None", ":", "\n", "        ", "self", ".", "_beam_size", "=", "beam_size", "\n", "self", ".", "_per_node_beam_size", "=", "per_node_beam_size", "or", "beam_size", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.state_machines.constrained_beam_search.FirstStepConstrainedBeamSearch.search": [[42, 116], ["collections.defaultdict", "finished_states.items", "collections.defaultdict", "states[].combine_states", "next_states.items", "finished_to_sort.sort", "transition_function.take_step", "transition_function.take_step", "next_state.is_finished", "states.extend", "finished_states[].append", "next_states[].append", "finished_states[].append", "state.score[].item"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.allowed_prefix_beamsearch.SimpleState.combine_states", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.allowed_prefix_beamsearch.SimpleTransitionFunction.take_step", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.allowed_prefix_beamsearch.SimpleTransitionFunction.take_step", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.allowed_prefix_beamsearch.SimpleState.is_finished"], ["", "def", "search", "(", "\n", "self", ",", "\n", "num_steps", ":", "int", ",", "\n", "initial_state", ":", "StateType", ",", "\n", "transition_function", ":", "TransitionFunction", ",", "\n", "firststep_allowed_actions", ":", "List", "[", "Set", "[", "int", "]", "]", "=", "None", ",", "\n", "keep_final_unfinished_states", ":", "bool", "=", "True", ",", "\n", ")", "->", "Mapping", "[", "int", ",", "Sequence", "[", "StateType", "]", "]", ":", "\n", "        ", "\"\"\"\n        Parameters\n        ----------\n        num_steps : ``int``\n            How many steps should we take in our search?  This is an upper bound, as it's possible\n            for the search to run out of valid actions before hitting this number, or for all\n            states on the beam to finish.\n        initial_state : ``StateType``\n            The starting state of our search.  This is assumed to be `batched`, and our beam search\n            is batch-aware - we'll keep ``beam_size`` states around for each instance in the batch.\n        transition_function : ``TransitionFunction``\n            The ``TransitionFunction`` object that defines and scores transitions from one state to the\n            next.\n        firststep_allowed_actions : ``List[Set[int]]``\n            For each instance in the initial_state, a set of allowed_actions_idxs for the first decoding step.\n            This is useful in our case since we have supervision for the `type` of the action sequence\n            and only want the first action so that the action sequence results to that type.\n        keep_final_unfinished_states : ``bool``, optional (default=True)\n            If we run out of steps before a state is \"finished\", should we return that state in our\n            search results?\n\n        Returns\n        -------\n        best_states : ``Dict[int, List[StateType]]``\n            This is a mapping from batch index to the top states for that instance.\n        \"\"\"", "\n", "finished_states", ":", "Dict", "[", "int", ",", "List", "[", "StateType", "]", "]", "=", "defaultdict", "(", "list", ")", "\n", "states", "=", "[", "initial_state", "]", "\n", "step_num", "=", "1", "\n", "\n", "while", "states", "and", "step_num", "<=", "num_steps", ":", "\n", "            ", "next_states", ":", "Dict", "[", "int", ",", "List", "[", "StateType", "]", "]", "=", "defaultdict", "(", "list", ")", "\n", "grouped_state", "=", "states", "[", "0", "]", ".", "combine_states", "(", "states", ")", "\n", "if", "step_num", "==", "1", ":", "\n", "                ", "possible_next_states", "=", "transition_function", ".", "take_step", "(", "\n", "grouped_state", ",", "max_actions", "=", "self", ".", "_per_node_beam_size", ",", "allowed_actions", "=", "firststep_allowed_actions", "\n", ")", "\n", "", "else", ":", "\n", "                ", "possible_next_states", "=", "transition_function", ".", "take_step", "(", "\n", "grouped_state", ",", "max_actions", "=", "self", ".", "_per_node_beam_size", "\n", ")", "\n", "", "for", "next_state", "in", "possible_next_states", ":", "\n", "# NOTE: we're doing state.batch_indices[0] here (and similar things below),", "\n", "# hard-coding a group size of 1.  But, our use of `next_state.is_finished()`", "\n", "# already checks for that, as it crashes if the group size is not 1.", "\n", "                ", "batch_index", "=", "next_state", ".", "batch_indices", "[", "0", "]", "\n", "if", "next_state", ".", "is_finished", "(", ")", ":", "\n", "                    ", "finished_states", "[", "batch_index", "]", ".", "append", "(", "next_state", ")", "\n", "", "else", ":", "\n", "                    ", "if", "step_num", "==", "num_steps", "and", "keep_final_unfinished_states", ":", "\n", "                        ", "finished_states", "[", "batch_index", "]", ".", "append", "(", "next_state", ")", "\n", "", "next_states", "[", "batch_index", "]", ".", "append", "(", "next_state", ")", "\n", "", "", "states", "=", "[", "]", "\n", "for", "batch_index", ",", "batch_states", "in", "next_states", ".", "items", "(", ")", ":", "\n", "# The states from the generator are already sorted, so we can just take the first", "\n", "# ones here, without an additional sort.", "\n", "                ", "states", ".", "extend", "(", "batch_states", "[", ":", "self", ".", "_beam_size", "]", ")", "\n", "", "step_num", "+=", "1", "\n", "", "best_states", ":", "Dict", "[", "int", ",", "Sequence", "[", "StateType", "]", "]", "=", "{", "}", "\n", "for", "batch_index", ",", "batch_states", "in", "finished_states", ".", "items", "(", ")", ":", "\n", "# The time this sort takes is pretty negligible, no particular need to optimize this", "\n", "# yet.  Maybe with a larger beam size...", "\n", "            ", "finished_to_sort", "=", "[", "(", "-", "state", ".", "score", "[", "0", "]", ".", "item", "(", ")", ",", "state", ")", "for", "state", "in", "batch_states", "]", "\n", "finished_to_sort", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "best_states", "[", "batch_index", "]", "=", "[", "state", "[", "1", "]", "for", "state", "in", "finished_to_sort", "[", ":", "self", ".", "_beam_size", "]", "]", "\n", "", "return", "best_states", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.state_machines.prefixed_beam_search.PrefixedConstrainedBeamSearch.__init__": [[56, 72], ["allennlp_semparse.state_machines.util.construct_prefix_tree"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "beam_size", ":", "Optional", "[", "int", "]", ",", "\n", "all_action_indices", ":", "List", "[", "List", "[", "int", "]", "]", ",", "\n", "allowed_sequences", ":", "Union", "[", "torch", ".", "Tensor", ",", "List", "[", "List", "[", "List", "[", "int", "]", "]", "]", "]", ",", "\n", "allowed_sequence_mask", ":", "Optional", "[", "Union", "[", "torch", ".", "Tensor", ",", "List", "[", "List", "[", "List", "[", "int", "]", "]", "]", "]", "]", "=", "None", ",", "\n", "per_node_beam_size", ":", "int", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "_beam_size", "=", "beam_size", "\n", "self", ".", "_per_node_beam_size", "=", "per_node_beam_size", "or", "beam_size", "\n", "# This is a list of defaultdict (one for each batch instance) mapping action-prefix to allowed actions in the", "\n", "# next step", "\n", "self", ".", "_allowed_transitions", "=", "util", ".", "construct_prefix_tree", "(", "\n", "allowed_sequences", ",", "allowed_sequence_mask", "\n", ")", "\n", "self", ".", "_all_action_indices", "=", "all_action_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.state_machines.prefixed_beam_search.PrefixedConstrainedBeamSearch.search": [[73, 153], ["collections.defaultdict", "finished_states.items", "collections.defaultdict", "states[].combine_states", "zip", "transition_function.take_step", "next_states.items", "finished_to_sort.sort", "allowed_actions.append", "next_state.is_finished", "states.extend", "tuple", "finished_states[].append", "next_states[].append", "finished_states[].append", "state.score[].item", "tuple"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.allowed_prefix_beamsearch.SimpleState.combine_states", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.allowed_prefix_beamsearch.SimpleTransitionFunction.take_step", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.test.allowed_prefix_beamsearch.SimpleState.is_finished"], ["", "def", "search", "(", "\n", "self", ",", "\n", "initial_state", ":", "State", ",", "\n", "transition_function", ":", "TransitionFunction", ",", "\n", "num_steps", ":", "int", "=", "None", ",", "\n", "keep_final_unfinished_states", ":", "bool", "=", "True", ",", "\n", ")", "->", "Dict", "[", "int", ",", "List", "[", "State", "]", "]", ":", "\n", "        ", "\"\"\"\n        Parameters\n        ----------\n        initial_state : ``State``\n            The starting state of our search.  This is assumed to be `batched`, and our beam search\n            is batch-aware - we'll keep ``beam_size`` states around for each instance in the batch.\n        transition_function : ``TransitionFunction``\n            The ``TransitionFunction`` object that defines and scores transitions from one state to the\n            next.\n        num_steps : ``int``\n            How many steps should we take in our search?  This is an upper bound, as it's possible\n            for the search to run out of valid actions before hitting this number, or for all\n            states on the beam to finish.\n        keep_final_unfinished_states : ``bool``, optional (default=True)\n            If we run out of steps before a state is \"finished\", should we return that state in our\n            search results?\n\n        Returns\n        -------\n        best_states : ``Dict[int, List[State]]``\n            This is a mapping from batch index to the top states for that instance.\n        \"\"\"", "\n", "finished_states", ":", "Dict", "[", "int", ",", "List", "[", "State", "]", "]", "=", "defaultdict", "(", "list", ")", "\n", "states", "=", "[", "initial_state", "]", "\n", "step_num", "=", "0", "\n", "\n", "while", "states", ":", "\n", "            ", "step_num", "+=", "1", "\n", "next_states", ":", "Dict", "[", "int", ",", "List", "[", "State", "]", "]", "=", "defaultdict", "(", "list", ")", "\n", "grouped_state", "=", "states", "[", "0", "]", ".", "combine_states", "(", "states", ")", "\n", "allowed_actions", "=", "[", "]", "\n", "\n", "for", "batch_index", ",", "action_history", "in", "zip", "(", "\n", "grouped_state", ".", "batch_indices", ",", "grouped_state", ".", "action_history", "\n", ")", ":", "\n", "# This path being explored has a constrained-prefix so only allow actions from that path --", "\n", "                ", "if", "tuple", "(", "action_history", ")", "in", "self", ".", "_allowed_transitions", "[", "batch_index", "]", ":", "\n", "                    ", "actions_allowed_for_this_state", "=", "self", ".", "_allowed_transitions", "[", "batch_index", "]", "[", "tuple", "(", "action_history", ")", "]", "\n", "", "else", ":", "\n", "                    ", "actions_allowed_for_this_state", "=", "self", ".", "_all_action_indices", "[", "batch_index", "]", "\n", "\n", "", "allowed_actions", ".", "append", "(", "actions_allowed_for_this_state", ")", "\n", "\n", "", "for", "next_state", "in", "transition_function", ".", "take_step", "(", "\n", "grouped_state", ",", "max_actions", "=", "self", ".", "_per_node_beam_size", ",", "allowed_actions", "=", "allowed_actions", "\n", ")", ":", "\n", "# NOTE: we're doing state.batch_indices[0] here (and similar things below),", "\n", "# hard-coding a group size of 1.  But, our use of `next_state.is_finished()`", "\n", "# already checks for that, as it crashes if the group size is not 1.", "\n", "                ", "batch_index", "=", "next_state", ".", "batch_indices", "[", "0", "]", "\n", "if", "next_state", ".", "is_finished", "(", ")", ":", "\n", "                    ", "finished_states", "[", "batch_index", "]", ".", "append", "(", "next_state", ")", "\n", "", "else", ":", "\n", "                    ", "if", "num_steps", "and", "step_num", "==", "num_steps", "and", "keep_final_unfinished_states", ":", "\n", "                        ", "finished_states", "[", "batch_index", "]", ".", "append", "(", "next_state", ")", "\n", "", "next_states", "[", "batch_index", "]", ".", "append", "(", "next_state", ")", "\n", "", "", "states", "=", "[", "]", "\n", "for", "batch_index", ",", "batch_states", "in", "next_states", ".", "items", "(", ")", ":", "\n", "# The states from the generator are already sorted, so we can just take the first", "\n", "# ones here, without an additional sort.", "\n", "                ", "if", "self", ".", "_beam_size", ":", "\n", "                    ", "batch_states", "=", "batch_states", "[", ":", "self", ".", "_beam_size", "]", "\n", "", "states", ".", "extend", "(", "batch_states", ")", "\n", "", "if", "num_steps", "and", "step_num", ">=", "num_steps", ":", "\n", "                ", "break", "\n", "", "", "best_states", ":", "Dict", "[", "int", ",", "List", "[", "State", "]", "]", "=", "{", "}", "\n", "for", "batch_index", ",", "batch_states", "in", "finished_states", ".", "items", "(", ")", ":", "\n", "# The time this sort takes is pretty negligible, no particular need to optimize this", "\n", "# yet.  Maybe with a larger beam size...", "\n", "            ", "finished_to_sort", "=", "[", "(", "-", "state", ".", "score", "[", "0", "]", ".", "item", "(", ")", ",", "state", ")", "for", "state", "in", "batch_states", "]", "\n", "finished_to_sort", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "best_states", "[", "batch_index", "]", "=", "[", "state", "[", "1", "]", "for", "state", "in", "finished_to_sort", "[", ":", "self", ".", "_beam_size", "]", "]", "\n", "", "return", "best_states", "", "", "", ""]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.transition_functions.linking_transition_func_emb.LinkingTransitionFunctionEmbeddings.__init__": [[58, 95], ["allennlp_semparse.state_machines.transition_functions.BasicTransitionFunction.__init__", "allennlp.nn.Activation.by_name", "allennlp.common.checks.check_dimensions_match", "allennlp.common.checks.check_dimensions_match", "mixture_feedforward.get_input_dim", "mixture_feedforward.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "encoder_output_dim", ":", "int", ",", "\n", "action_embedding_dim", ":", "int", ",", "\n", "input_attention", ":", "Attention", ",", "\n", "input_attention_activation", ":", "Activation", "=", "None", ",", "\n", "activation", ":", "Activation", "=", "Activation", ".", "by_name", "(", "\"relu\"", ")", "(", ")", ",", "\n", "predict_start_type_separately", ":", "bool", "=", "True", ",", "\n", "num_start_types", ":", "int", "=", "None", ",", "\n", "add_action_bias", ":", "bool", "=", "True", ",", "\n", "mixture_feedforward", ":", "FeedForward", "=", "None", ",", "\n", "dropout", ":", "float", "=", "0.0", ",", "\n", "num_layers", ":", "int", "=", "1", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "encoder_output_dim", "=", "encoder_output_dim", ",", "\n", "action_embedding_dim", "=", "action_embedding_dim", ",", "\n", "input_attention", "=", "input_attention", ",", "\n", "input_attention_activation", "=", "input_attention_activation", ",", "\n", "num_start_types", "=", "num_start_types", ",", "\n", "activation", "=", "activation", ",", "\n", "predict_start_type_separately", "=", "predict_start_type_separately", ",", "\n", "add_action_bias", "=", "add_action_bias", ",", "\n", "dropout", "=", "dropout", ",", "\n", "num_layers", "=", "num_layers", ",", "\n", ")", "\n", "self", ".", "_mixture_feedforward", "=", "mixture_feedforward", "\n", "\n", "if", "mixture_feedforward", "is", "not", "None", ":", "\n", "            ", "check_dimensions_match", "(", "\n", "encoder_output_dim", ",", "\n", "mixture_feedforward", ".", "get_input_dim", "(", ")", ",", "\n", "\"hidden state embedding dim\"", ",", "\n", "\"mixture feedforward input dim\"", ",", "\n", ")", "\n", "check_dimensions_match", "(", "\n", "mixture_feedforward", ".", "get_output_dim", "(", ")", ",", "1", ",", "\"mixture feedforward output dim\"", ",", "\"dimension for scalar value\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.transition_functions.linking_transition_func_emb.LinkingTransitionFunctionEmbeddings._compute_action_probabilities": [[97, 191], ["len", "state.get_valid_actions", "collections.defaultdict", "range", "batch_results[].append", "action_embeddings.mm().squeeze", "linked_action_embeddings.mm().squeeze", "torch.nn.functional.log_softmax", "torch.cat", "linking_transition_func_emb.LinkingTransitionFunctionEmbeddings._mixture_feedforward", "torch.log", "torch.log", "torch.nn.functional.log_softmax", "action_embeddings.mm", "linked_action_embeddings.mm", "torch.nn.functional.log_softmax", "torch.cat", "torch.cat", "predicted_action_embedding.unsqueeze", "predicted_action_embedding.unsqueeze", "torch.nn.functional.log_softmax"], "methods", ["None"], ["", "", "@", "overrides", "\n", "def", "_compute_action_probabilities", "(", "\n", "self", ",", "\n", "state", ":", "GrammarBasedState", ",", "\n", "hidden_state", ":", "torch", ".", "Tensor", ",", "\n", "attention_weights", ":", "torch", ".", "Tensor", ",", "\n", "predicted_action_embeddings", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "Dict", "[", "int", ",", "List", "[", "Tuple", "[", "int", ",", "Any", ",", "Any", ",", "Any", ",", "List", "[", "int", "]", "]", "]", "]", ":", "\n", "# In this section we take our predicted action embedding and compare it to the available", "\n", "# actions in our current state (which might be different for each group element).  For", "\n", "# computing action scores, we'll forget about doing batched / grouped computation, as it", "\n", "# adds too much complexity and doesn't speed things up, anyway, with the operations we're", "\n", "# doing here.  This means we don't need any action masks, as we'll only get the right", "\n", "# lengths for what we're computing.", "\n", "\n", "        ", "group_size", "=", "len", "(", "state", ".", "batch_indices", ")", "\n", "actions", "=", "state", ".", "get_valid_actions", "(", ")", "\n", "\n", "# print(len(state.batch_indices))", "\n", "\n", "batch_results", ":", "Dict", "[", "int", ",", "List", "[", "Tuple", "[", "int", ",", "Any", ",", "Any", ",", "Any", ",", "List", "[", "int", "]", "]", "]", "]", "=", "defaultdict", "(", "list", ")", "\n", "for", "group_index", "in", "range", "(", "group_size", ")", ":", "\n", "            ", "batch_idx", "=", "state", ".", "batch_indices", "[", "group_index", "]", "\n", "# List[ProductionRule]", "\n", "possible_actions", "=", "state", ".", "possible_actions", "[", "batch_idx", "]", "\n", "\n", "instance_actions", "=", "actions", "[", "group_index", "]", "\n", "predicted_action_embedding", "=", "predicted_action_embeddings", "[", "group_index", "]", "\n", "embedded_actions", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "\n", "output_action_embeddings", "=", "None", "\n", "embedded_action_logits", "=", "None", "\n", "current_log_probs", "=", "None", "\n", "\n", "if", "\"global\"", "in", "instance_actions", ":", "\n", "                ", "action_embeddings", ",", "output_action_embeddings", ",", "embedded_actions", "=", "instance_actions", "[", "\"global\"", "]", "\n", "\n", "# This is just a matrix product between a (num_actions, embedding_dim) matrix and an", "\n", "# (embedding_dim, 1) matrix.", "\n", "embedded_action_logits", "=", "action_embeddings", ".", "mm", "(", "predicted_action_embedding", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "action_ids", "=", "embedded_actions", "\n", "\n", "", "if", "\"linked\"", "in", "instance_actions", ":", "\n", "                ", "linked_action_embeddings", ",", "type_embeddings", ",", "linked_actions", "=", "instance_actions", "[", "\"linked\"", "]", "\n", "\n", "linked_action_logits", "=", "linked_action_embeddings", ".", "mm", "(", "predicted_action_embedding", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "action_ids", "=", "embedded_actions", "+", "linked_actions", "\n", "# This is the old scoring function using linking_scores", "\n", "# linked_action_logits = linking_scores.mm(attention_weights[group_index].unsqueeze(-1)).squeeze(-1)", "\n", "\n", "# The `output_action_embeddings` tensor gets used later as the input to the next", "\n", "# decoder step.  For linked actions, we don't have any action embedding, so we use", "\n", "# the entity type instead.", "\n", "if", "output_action_embeddings", "is", "not", "None", ":", "\n", "                    ", "output_action_embeddings", "=", "torch", ".", "cat", "(", "[", "output_action_embeddings", ",", "type_embeddings", "]", ",", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "                    ", "output_action_embeddings", "=", "type_embeddings", "\n", "\n", "", "if", "self", ".", "_mixture_feedforward", "is", "not", "None", ":", "\n", "# The linked and global logits are combined with a mixture weight to prevent the", "\n", "# linked_action_logits from dominating the embedded_action_logits if a softmax", "\n", "# was applied on both together.", "\n", "                    ", "mixture_weight", "=", "self", ".", "_mixture_feedforward", "(", "hidden_state", "[", "group_index", "]", ")", "\n", "mix1", "=", "torch", ".", "log", "(", "mixture_weight", ")", "\n", "mix2", "=", "torch", ".", "log", "(", "1", "-", "mixture_weight", ")", "\n", "\n", "entity_action_probs", "=", "torch", ".", "nn", ".", "functional", ".", "log_softmax", "(", "linked_action_logits", ",", "dim", "=", "-", "1", ")", "+", "mix1", "\n", "if", "embedded_action_logits", "is", "not", "None", ":", "\n", "                        ", "embedded_action_probs", "=", "torch", ".", "nn", ".", "functional", ".", "log_softmax", "(", "embedded_action_logits", ",", "dim", "=", "-", "1", ")", "+", "mix2", "\n", "current_log_probs", "=", "torch", ".", "cat", "(", "[", "embedded_action_probs", ",", "entity_action_probs", "]", ",", "dim", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "                        ", "current_log_probs", "=", "entity_action_probs", "\n", "", "", "else", ":", "\n", "                    ", "if", "embedded_action_logits", "is", "not", "None", ":", "\n", "                        ", "action_logits", "=", "torch", ".", "cat", "(", "[", "embedded_action_logits", ",", "linked_action_logits", "]", ",", "dim", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "                        ", "action_logits", "=", "linked_action_logits", "\n", "", "current_log_probs", "=", "torch", ".", "nn", ".", "functional", ".", "log_softmax", "(", "action_logits", ",", "dim", "=", "-", "1", ")", "\n", "", "", "else", ":", "\n", "                ", "action_logits", "=", "embedded_action_logits", "\n", "current_log_probs", "=", "torch", ".", "nn", ".", "functional", ".", "log_softmax", "(", "action_logits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# This is now the total score for each state after taking each action.  We're going to", "\n", "# sort by this later, so it's important that this is the total score, not just the", "\n", "# score for the current action.", "\n", "# print(state.score[group_index])", "\n", "# print(current_log_probs)", "\n", "", "log_probs", "=", "state", ".", "score", "[", "group_index", "]", "+", "current_log_probs", "\n", "# print(f\"c: {current_log_probs} lp: {log_probs}\")", "\n", "batch_results", "[", "state", ".", "batch_indices", "[", "group_index", "]", "]", ".", "append", "(", "\n", "(", "group_index", ",", "log_probs", ",", "current_log_probs", ",", "output_action_embeddings", ",", "action_ids", ")", "\n", ")", "\n", "", "return", "batch_results", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__init__": [[9, 14], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "scope_name", ":", "str", ")", ":", "\n", "        ", "self", ".", "scope_name", "=", "scope_name", "\n", "self", ".", "start_time", "=", "None", "\n", "self", ".", "end_time", "=", "None", "\n", "self", ".", "total_time", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__enter__": [[15, 17], ["time.time"], "methods", ["None"], ["", "def", "__enter__", "(", "self", ")", ":", "\n", "        ", "self", ".", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.__exit__": [[18, 23], ["time.time"], "methods", ["None"], ["", "def", "__exit__", "(", "self", ",", "exc_type", ",", "exc_val", ",", "exc_tb", ")", ":", "\n", "        ", "self", ".", "end_time", "=", "time", ".", "time", "(", ")", "\n", "self", ".", "total_time", "=", "self", ".", "end_time", "-", "self", ".", "start_time", "\n", "Profile", ".", "timer_dict", "[", "self", ".", "scope_name", "]", "+=", "self", ".", "total_time", "\n", "Profile", ".", "num_calls", "[", "self", ".", "scope_name", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.Profile.to_string": [[24, 44], ["collections.defaultdict", "utils.util.round_all", "utils.util.round_all", "utils.util.round_all.items", "Profile.timer_dict.items"], "methods", ["home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all", "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.utils.util.round_all"], ["", "@", "staticmethod", "\n", "def", "to_string", "(", ")", ":", "\n", "        ", "perc_of_forward", "=", "collections", ".", "defaultdict", "(", "float", ")", "\n", "if", "\"forward\"", "in", "Profile", ".", "timer_dict", ":", "\n", "            ", "forward_time", "=", "Profile", ".", "timer_dict", "[", "\"forward\"", "]", "\n", "for", "k", ",", "v", "in", "Profile", ".", "timer_dict", ".", "items", "(", ")", ":", "\n", "                ", "perc", "=", "(", "v", "/", "forward_time", ")", "*", "100.0", "\n", "perc_of_forward", "[", "k", "]", "=", "perc", "\n", "\n", "", "", "timer_dict", "=", "util", ".", "round_all", "(", "Profile", ".", "timer_dict", ",", "prec", "=", "4", ")", "\n", "perc_of_forward", "=", "util", ".", "round_all", "(", "perc_of_forward", ",", "prec", "=", "4", ")", "\n", "\n", "s", "=", "\"\\n------------------------  Profiler Stats  ------------------------\\n\"", "\n", "s", "+=", "\"Scope \\t Num_Calls \\t TimeElapsed \\t Perc_of_Forward\\n\"", "\n", "for", "k", ",", "v", "in", "timer_dict", ".", "items", "(", ")", ":", "\n", "            ", "num_calls", "=", "Profile", ".", "num_calls", "[", "k", "]", "\n", "perc_forward", "=", "perc_of_forward", "[", "k", "]", "if", "k", "in", "perc_of_forward", "else", "0.0", "\n", "s", "+=", "\"{} \\t {} \\t {} seconds \\t {} % \\n\"", ".", "format", "(", "k", ",", "num_calls", ",", "v", ",", "perc_forward", ")", "\n", "", "s", "+=", "\"----------------------------------------------------------------\\n\"", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.nitishgupta_nmn-drop.profiler.profile.profile_func_decorator": [[46, 52], ["profile.Profile", "input_func"], "function", ["None"], ["", "", "def", "profile_func_decorator", "(", "input_func", ")", ":", "\n", "    ", "def", "timed_func", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "Profile", "(", "input_func", ".", "__name__", ")", ":", "\n", "            ", "result", "=", "input_func", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "", "return", "result", "\n", "", "return", "timed_func", "\n", "", ""]]}