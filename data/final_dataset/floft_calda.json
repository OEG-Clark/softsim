{"home.repos.pwc.inspect_result.floft_calda.None.models.FlipGradient.__init__": [[57, 61], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["def", "__init__", "(", "self", ",", "global_step", ",", "grl_schedule", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "global_step", "=", "global_step", "\n", "self", ".", "grl_schedule", "=", "grl_schedule", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.FlipGradient.call": [[62, 68], ["models.FlipGradient.grl_schedule", "models.flip_gradient"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.models.flip_gradient"], ["", "def", "call", "(", "self", ",", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\" Calculate grl_lambda first based on the current global step (a\n        variable) and then create the layer that does nothing except flip\n        the gradients \"\"\"", "\n", "grl_lambda", "=", "self", ".", "grl_schedule", "(", "self", ".", "global_step", ")", "\n", "return", "flip_gradient", "(", "inputs", ",", "grl_lambda", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.StopGradient.call": [[83, 85], ["tensorflow.stop_gradient"], "methods", ["None"], ["def", "call", "(", "self", ",", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "tf", ".", "stop_gradient", "(", "inputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase.__init__": [[89, 91], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase._get_trainable_variables_list": [[92, 100], ["None"], "methods", ["None"], ["", "def", "_get_trainable_variables_list", "(", "self", ",", "model_list", ")", ":", "\n", "        ", "\"\"\" Get all trainable variables if model is a list \"\"\"", "\n", "model_vars", "=", "[", "]", "\n", "\n", "for", "m", "in", "model_list", ":", "\n", "            ", "model_vars", "+=", "m", ".", "trainable_variables", "\n", "\n", "", "return", "model_vars", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase._get_trainable_variables": [[101, 107], ["isinstance", "models.ModelBase._get_trainable_variables_list"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase._get_trainable_variables_list"], ["", "def", "_get_trainable_variables", "(", "self", ",", "model", ")", ":", "\n", "        ", "\"\"\" Get trainable variables if model is a list or not \"\"\"", "\n", "if", "isinstance", "(", "model", ",", "list", ")", ":", "\n", "            ", "return", "self", ".", "_get_trainable_variables_list", "(", "model", ")", "\n", "\n", "", "return", "model", ".", "trainable_variables", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase.trainable_variables_fe": [[108, 111], ["models.ModelBase._get_trainable_variables"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase._get_trainable_variables"], ["", "@", "property", "\n", "def", "trainable_variables_fe", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_get_trainable_variables", "(", "self", ".", "feature_extractor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase.trainable_variables_task": [[112, 115], ["models.ModelBase._get_trainable_variables"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase._get_trainable_variables"], ["", "@", "property", "\n", "def", "trainable_variables_task", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_get_trainable_variables", "(", "self", ".", "task_classifier", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase.trainable_variables_domain": [[116, 119], ["models.ModelBase._get_trainable_variables"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase._get_trainable_variables"], ["", "@", "property", "\n", "def", "trainable_variables_domain", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_get_trainable_variables", "(", "self", ".", "domain_classifier", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase.trainable_variables_contrastive": [[120, 123], ["models.ModelBase._get_trainable_variables"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase._get_trainable_variables"], ["", "@", "property", "\n", "def", "trainable_variables_contrastive", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_get_trainable_variables", "(", "self", ".", "contrastive_head", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase.trainable_variables_task_fe": [[124, 133], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "trainable_variables_task_fe", "(", "self", ")", ":", "\n", "        ", "variables", "=", "self", ".", "trainable_variables_fe", "+", "self", ".", "trainable_variables_task", "\n", "\n", "if", "self", ".", "contrastive_head", "is", "not", "None", ":", "\n", "            ", "variables", "+=", "self", ".", "trainable_variables_contrastive", "\n", "\n", "", "return", "variables", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase.trainable_variables_task_fe_domain": [[134, 144], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "trainable_variables_task_fe_domain", "(", "self", ")", ":", "\n", "        ", "variables", "=", "self", ".", "trainable_variables_fe", "+", "self", ".", "trainable_variables_task", "+", "self", ".", "trainable_variables_domain", "\n", "\n", "if", "self", ".", "contrastive_head", "is", "not", "None", ":", "\n", "            ", "variables", "+=", "self", ".", "trainable_variables_contrastive", "\n", "\n", "", "return", "variables", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase.trainable_variables": [[145, 149], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "trainable_variables", "(", "self", ")", ":", "\n", "        ", "\"\"\" Returns all trainable variables in the model \"\"\"", "\n", "return", "self", ".", "trainable_variables_task_fe_domain", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase.call_feature_extractor": [[152, 160], ["models.ModelBase.feature_extractor", "isinstance", "isinstance"], "methods", ["None"], ["", "def", "call_feature_extractor", "(", "self", ",", "inputs", ",", "which_fe", "=", "None", ",", "which_tc", "=", "None", ",", "\n", "which_dc", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "which_fe", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "self", ".", "feature_extractor", ",", "list", ")", "\n", "return", "self", ".", "feature_extractor", "[", "which_fe", "]", "(", "inputs", ",", "**", "kwargs", ")", "\n", "\n", "", "assert", "not", "isinstance", "(", "self", ".", "feature_extractor", ",", "list", ")", "\n", "return", "self", ".", "feature_extractor", "(", "inputs", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase.call_task_classifier": [[161, 169], ["models.ModelBase.task_classifier", "isinstance", "isinstance"], "methods", ["None"], ["", "def", "call_task_classifier", "(", "self", ",", "fe", ",", "which_fe", "=", "None", ",", "which_tc", "=", "None", ",", "\n", "which_dc", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "which_tc", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "self", ".", "task_classifier", ",", "list", ")", "\n", "return", "self", ".", "task_classifier", "[", "which_tc", "]", "(", "fe", ",", "**", "kwargs", ")", "\n", "\n", "", "assert", "not", "isinstance", "(", "self", ".", "task_classifier", ",", "list", ")", "\n", "return", "self", ".", "task_classifier", "(", "fe", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase.call_domain_classifier": [[170, 178], ["models.ModelBase.domain_classifier", "isinstance", "isinstance"], "methods", ["None"], ["", "def", "call_domain_classifier", "(", "self", ",", "fe", ",", "task", ",", "which_fe", "=", "None", ",", "which_tc", "=", "None", ",", "\n", "which_dc", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "which_dc", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "self", ".", "domain_classifier", ",", "list", ")", "\n", "return", "self", ".", "domain_classifier", "[", "which_dc", "]", "(", "fe", ",", "**", "kwargs", ")", "\n", "\n", "", "assert", "not", "isinstance", "(", "self", ".", "domain_classifier", ",", "list", ")", "\n", "return", "self", ".", "domain_classifier", "(", "fe", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase.call_contrastive_head": [[179, 187], ["models.ModelBase.contrastive_head", "isinstance", "isinstance"], "methods", ["None"], ["", "def", "call_contrastive_head", "(", "self", ",", "fe", ",", "which_fe", "=", "None", ",", "which_tc", "=", "None", ",", "\n", "which_dc", "=", "None", ",", "which_ch", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "which_ch", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "self", ".", "contrastive_head", ",", "list", ")", "\n", "return", "self", ".", "contrastive_head", "[", "which_ch", "]", "(", "fe", ",", "**", "kwargs", ")", "\n", "\n", "", "assert", "not", "isinstance", "(", "self", ".", "contrastive_head", ",", "list", ")", "\n", "return", "self", ".", "contrastive_head", "(", "fe", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase.call": [[188, 257], ["isinstance", "isinstance", "tensorflow.concat", "models.ModelBase.call_task_classifier", "models.ModelBase.call_feature_extractor", "models.ModelBase.call_task_classifier", "models.ModelBase.call_domain_classifier", "len", "len", "models.ModelBase.call_feature_extractor", "models.ModelBase.call_contrastive_head", "len", "len", "len", "len", "models.ModelBase.call_domain_classifier", "models.ModelBase.call_contrastive_head", "range", "len", "str", "len"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase.call_task_classifier", "home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase.call_feature_extractor", "home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase.call_task_classifier", "home.repos.pwc.inspect_result.floft_calda.None.models.SleepModel.call_domain_classifier", "home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase.call_feature_extractor", "home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase.call_contrastive_head", "home.repos.pwc.inspect_result.floft_calda.None.models.SleepModel.call_domain_classifier", "home.repos.pwc.inspect_result.floft_calda.None.models.ModelBase.call_contrastive_head"], ["", "def", "call", "(", "self", ",", "inputs", ",", "training", "=", "None", ",", "input_is_list", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "# For backwards compatibility, the FE and DC aren't always lists, e.g.", "\n", "# for some methods that don't currently support multiple modalities.", "\n", "        ", "if", "isinstance", "(", "self", ".", "feature_extractor", ",", "list", ")", "and", "isinstance", "(", "self", ".", "domain_classifier", ",", "list", ")", ":", "\n", "# We have a separate feature extractor for each modality. We use the", "\n", "# lowest index ones first, i.e. if for example the source has 2 and the", "\n", "# target has 1, the lowest index source FE will be used for the target", "\n", "# modality (\"left to right\" of sorts). Change the ordering with", "\n", "# {source,target}_modality_subset if desired.", "\n", "            ", "assert", "len", "(", "inputs", ")", "<=", "len", "(", "self", ".", "feature_extractor", ")", ",", "\"need one feature extractor per modality\"", "\n", "\n", "fe", "=", "[", "\n", "self", ".", "call_feature_extractor", "(", "inputs", "[", "i", "]", ",", "which_fe", "=", "i", ",", "\n", "training", "=", "training", ",", "**", "kwargs", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", "\n", "]", "\n", "\n", "# Concatenate the shared modality FEs together for the task classifier", "\n", "# and contrastive head", "\n", "fe_concat", "=", "tf", ".", "concat", "(", "[", "fe", "[", "i", "]", "for", "i", "in", "self", ".", "shared_modalities", "]", ",", "axis", "=", "-", "1", ")", "\n", "task", "=", "self", ".", "call_task_classifier", "(", "fe_concat", ",", "\n", "training", "=", "training", ",", "**", "kwargs", ")", "\n", "\n", "if", "self", ".", "contrastive_head", "is", "not", "None", ":", "\n", "                ", "contrastive", "=", "self", ".", "call_contrastive_head", "(", "fe_concat", ",", "\n", "training", "=", "training", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "                ", "contrastive", "=", "0.0", "\n", "\n", "# Pass each FE to the corresponding domain classifier.", "\n", "", "assert", "len", "(", "self", ".", "shared_modalities", ")", "==", "len", "(", "self", ".", "domain_classifier", ")", ",", "\"need one domain classifier per shared modality\"", "\n", "assert", "len", "(", "self", ".", "shared_modalities", ")", "<=", "len", "(", "fe", ")", ",", "\"need at least the number of shared modalities as inputs\"", "\n", "\n", "domain", "=", "[", "\n", "self", ".", "call_domain_classifier", "(", "fe", "[", "i", "]", ",", "task", ",", "which_dc", "=", "i", ",", "\n", "training", "=", "training", ",", "**", "kwargs", ")", "\n", "for", "i", "in", "self", ".", "shared_modalities", "\n", "]", "\n", "", "else", ":", "\n", "# This is for ART which errors if the input is a list, i.e. if this is", "\n", "# false only single-modality is supported and when passing in x, don't", "\n", "# do [x] for single-modality.", "\n", "            ", "if", "input_is_list", ":", "\n", "                ", "assert", "len", "(", "inputs", ")", "==", "1", ",", "\"if more than one modality, must use multiple FE and DC \"", "\"input shape is \"", "+", "str", "(", "inputs", ".", "shape", ")", "\n", "\n", "# There's only one modality, so just use that one", "\n", "inputs", "=", "inputs", "[", "0", "]", "\n", "\n", "", "fe", "=", "self", ".", "call_feature_extractor", "(", "inputs", ",", "training", "=", "training", ",", "**", "kwargs", ")", "\n", "task", "=", "self", ".", "call_task_classifier", "(", "fe", ",", "training", "=", "training", ",", "**", "kwargs", ")", "\n", "domain", "=", "self", ".", "call_domain_classifier", "(", "fe", ",", "task", ",", "training", "=", "training", ",", "**", "kwargs", ")", "\n", "\n", "if", "self", ".", "contrastive_head", "is", "not", "None", ":", "\n", "                ", "contrastive", "=", "self", ".", "call_contrastive_head", "(", "fe", ",", "training", "=", "training", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "                ", "contrastive", "=", "0.0", "\n", "\n", "# Make consistent with the multi-modality case", "\n", "", "domain", "=", "[", "domain", "]", "\n", "fe", "=", "[", "fe", "]", "\n", "\n", "# Note: domain and fe are lists", "\n", "", "return", "task", ",", "domain", ",", "fe", ",", "contrastive", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.ModelMakerBase.__init__": [[270, 272], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.ModelMakerBase.make_feature_extractor": [[273, 275], ["NotImplementedError"], "methods", ["None"], ["", "def", "make_feature_extractor", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"must implement for ModelMaker class\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.ModelMakerBase.make_task_classifier": [[276, 278], ["NotImplementedError"], "methods", ["None"], ["", "def", "make_task_classifier", "(", "self", ",", "num_classes", ",", "**", "kwargs", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"must implement for ModelMaker class\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.ModelMakerBase.make_domain_classifier": [[279, 281], ["NotImplementedError"], "methods", ["None"], ["", "def", "make_domain_classifier", "(", "self", ",", "num_domains", ",", "**", "kwargs", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"must implement for ModelMaker class\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.ModelMakerBase.make_contrastive_head": [[282, 284], ["NotImplementedError"], "methods", ["None"], ["", "def", "make_contrastive_head", "(", "self", ",", "num_units", ",", "**", "kwargs", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"must implement for ModelMaker class\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.CodatsModelMakerBase.make_task_classifier": [[289, 292], ["tensorflow.keras.Sequential", "tensorflow.keras.layers.Dense"], "methods", ["None"], ["def", "make_task_classifier", "(", "self", ",", "num_classes", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "tf", ".", "keras", ".", "Sequential", "(", "[", "\n", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "num_classes", ")", ",", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.CodatsModelMakerBase.make_domain_classifier": [[294, 310], ["tensorflow.keras.Sequential", "tensorflow.keras.layers.Dense", "tensorflow.keras.layers.BatchNormalization", "tensorflow.keras.layers.Activation", "tensorflow.keras.layers.Dropout", "tensorflow.keras.layers.Dense", "tensorflow.keras.layers.BatchNormalization", "tensorflow.keras.layers.Activation", "tensorflow.keras.layers.Dropout", "tensorflow.keras.layers.Dense"], "methods", ["None"], ["", "def", "make_domain_classifier", "(", "self", ",", "num_domains", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "tf", ".", "keras", ".", "Sequential", "(", "[", "\n", "# Note: alternative is Dense(128, activation=\"tanh\") like used by", "\n", "# https://arxiv.org/pdf/1902.09820.pdf They say dropout of 0.7 but", "\n", "# I'm not sure if that means 1-0.7 = 0.3 or 0.7 itself.", "\n", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "500", ",", "use_bias", "=", "False", ")", ",", "\n", "tf", ".", "keras", ".", "layers", ".", "BatchNormalization", "(", ")", ",", "\n", "tf", ".", "keras", ".", "layers", ".", "Activation", "(", "\"relu\"", ")", ",", "\n", "tf", ".", "keras", ".", "layers", ".", "Dropout", "(", "0.3", ")", ",", "\n", "\n", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "500", ",", "use_bias", "=", "False", ")", ",", "\n", "tf", ".", "keras", ".", "layers", ".", "BatchNormalization", "(", ")", ",", "\n", "tf", ".", "keras", ".", "layers", ".", "Activation", "(", "\"relu\"", ")", ",", "\n", "tf", ".", "keras", ".", "layers", ".", "Dropout", "(", "0.3", ")", ",", "\n", "\n", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "num_domains", ")", ",", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.CodatsModelMakerBase.make_contrastive_head": [[312, 315], ["tensorflow.keras.Sequential", "tensorflow.keras.layers.Dense"], "methods", ["None"], ["", "def", "make_contrastive_head", "(", "self", ",", "num_units", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "tf", ".", "keras", ".", "Sequential", "(", "[", "\n", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "num_units", ")", ",", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.FcnModelMaker.make_feature_extractor": [[327, 356], ["tensorflow.keras.Sequential", "tensorflow.keras.Sequential", "tensorflow.keras.layers.Conv1D", "tensorflow.keras.layers.BatchNormalization", "tensorflow.keras.layers.Activation", "tensorflow.keras.layers.Conv1D", "tensorflow.keras.layers.BatchNormalization", "tensorflow.keras.layers.Activation", "tensorflow.keras.layers.Conv1D", "tensorflow.keras.layers.BatchNormalization", "tensorflow.keras.layers.Activation", "tensorflow.keras.layers.GlobalAveragePooling1D", "tensorflow.keras.layers.Conv1D"], "methods", ["None"], ["def", "make_feature_extractor", "(", "self", ",", "previous_model", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "# Make a new feature extractor if no previous feature extractor", "\n", "        ", "if", "previous_model", "is", "None", ":", "\n", "            ", "return", "tf", ".", "keras", ".", "Sequential", "(", "[", "\n", "tf", ".", "keras", ".", "layers", ".", "Conv1D", "(", "filters", "=", "128", ",", "kernel_size", "=", "8", ",", "padding", "=", "\"same\"", ",", "\n", "use_bias", "=", "False", ")", ",", "\n", "tf", ".", "keras", ".", "layers", ".", "BatchNormalization", "(", ")", ",", "\n", "tf", ".", "keras", ".", "layers", ".", "Activation", "(", "\"relu\"", ")", ",", "\n", "\n", "tf", ".", "keras", ".", "layers", ".", "Conv1D", "(", "filters", "=", "256", ",", "kernel_size", "=", "5", ",", "padding", "=", "\"same\"", ",", "\n", "use_bias", "=", "False", ")", ",", "\n", "tf", ".", "keras", ".", "layers", ".", "BatchNormalization", "(", ")", ",", "\n", "tf", ".", "keras", ".", "layers", ".", "Activation", "(", "\"relu\"", ")", ",", "\n", "\n", "tf", ".", "keras", ".", "layers", ".", "Conv1D", "(", "filters", "=", "128", ",", "kernel_size", "=", "3", ",", "padding", "=", "\"same\"", ",", "\n", "use_bias", "=", "False", ")", ",", "\n", "tf", ".", "keras", ".", "layers", ".", "BatchNormalization", "(", ")", ",", "\n", "tf", ".", "keras", ".", "layers", ".", "Activation", "(", "\"relu\"", ")", ",", "\n", "\n", "tf", ".", "keras", ".", "layers", ".", "GlobalAveragePooling1D", "(", ")", ",", "\n", "]", ")", "\n", "\n", "# Only totally separate layer is the first Conv1D layer since the", "\n", "# input shape may be different. The rest of the layers will be the", "\n", "# layers from the other model.", "\n", "", "return", "tf", ".", "keras", ".", "Sequential", "(", "[", "\n", "tf", ".", "keras", ".", "layers", ".", "Conv1D", "(", "filters", "=", "128", ",", "kernel_size", "=", "8", ",", "padding", "=", "\"same\"", ",", "\n", "use_bias", "=", "False", ")", "\n", "]", "+", "previous_model", ".", "layers", "[", "1", ":", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.CnnModelBase.__init__": [[365, 397], ["models.ModelBase.__init__", "models.get_model", "models.CnnModelBase._make_single_or_multiple", "models.CnnModelBase._make_single_or_multiple", "models.CnnModelBase._make_single_or_multiple", "models.CnnModelBase._make_single_or_multiple", "get_model.make_task_classifier", "get_model.make_domain_classifier", "get_model.make_contrastive_head"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__", "home.repos.pwc.inspect_result.floft_calda.None.models.get_model", "home.repos.pwc.inspect_result.floft_calda.None.models.CnnModelBase._make_single_or_multiple", "home.repos.pwc.inspect_result.floft_calda.None.models.CnnModelBase._make_single_or_multiple", "home.repos.pwc.inspect_result.floft_calda.None.models.CnnModelBase._make_single_or_multiple", "home.repos.pwc.inspect_result.floft_calda.None.models.CnnModelBase._make_single_or_multiple", "home.repos.pwc.inspect_result.floft_calda.None.models.CodatsModelMakerBase.make_task_classifier", "home.repos.pwc.inspect_result.floft_calda.None.models.CodatsModelMakerBase.make_domain_classifier", "home.repos.pwc.inspect_result.floft_calda.None.models.CodatsModelMakerBase.make_contrastive_head"], ["def", "__init__", "(", "self", ",", "num_classes", ",", "num_domains", ",", "model_name", ",", "\n", "num_feature_extractors", "=", "None", ",", "\n", "num_task_classifiers", "=", "None", ",", "\n", "num_domain_classifiers", "=", "None", ",", "\n", "shared_modalities", "=", "None", ",", "\n", "share_most_weights", "=", "False", ",", "\n", "num_contrastive_units", "=", "None", ",", "\n", "num_contrastive_heads", "=", "None", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "num_domains", "=", "num_domains", "\n", "self", ".", "shared_modalities", "=", "shared_modalities", "\n", "\n", "model_maker", "=", "get_model", "(", "model_name", ")", "\n", "\n", "self", ".", "feature_extractor", "=", "self", ".", "_make_single_or_multiple", "(", "\n", "model_maker", ".", "make_feature_extractor", ",", "\n", "num_feature_extractors", ",", "share_most_weights", ")", "\n", "self", ".", "task_classifier", "=", "self", ".", "_make_single_or_multiple", "(", "\n", "lambda", "**", "kwargs", ":", "model_maker", ".", "make_task_classifier", "(", "num_classes", ",", "**", "kwargs", ")", ",", "\n", "num_task_classifiers", ",", "share_most_weights", ")", "\n", "self", ".", "domain_classifier", "=", "self", ".", "_make_single_or_multiple", "(", "\n", "lambda", "**", "kwargs", ":", "model_maker", ".", "make_domain_classifier", "(", "num_domains", ",", "**", "kwargs", ")", ",", "\n", "num_domain_classifiers", ",", "share_most_weights", ")", "\n", "\n", "if", "num_contrastive_units", "is", "not", "None", ":", "\n", "            ", "self", ".", "contrastive_head", "=", "self", ".", "_make_single_or_multiple", "(", "\n", "lambda", "**", "kwargs", ":", "model_maker", ".", "make_contrastive_head", "(", "num_contrastive_units", ",", "**", "kwargs", ")", ",", "\n", "num_contrastive_heads", ",", "share_most_weights", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "contrastive_head", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.CnnModelBase._make_single_or_multiple": [[398, 418], ["f", "range", "results.append", "f", "len", "f", "range"], "methods", ["None"], ["", "", "def", "_make_single_or_multiple", "(", "self", ",", "f", ",", "num", ",", "share_most_weights", ")", ":", "\n", "        ", "if", "num", "is", "not", "None", ":", "\n", "            ", "if", "share_most_weights", ":", "\n", "# Share most weights via passing in the previous model", "\n", "# Note: only used for in feature extractor creation.", "\n", "                ", "results", "=", "[", "]", "\n", "\n", "for", "_", "in", "range", "(", "num", ")", ":", "\n", "                    ", "previous_model", "=", "None", "\n", "\n", "if", "len", "(", "results", ")", ">", "0", ":", "\n", "                        ", "previous_model", "=", "results", "[", "-", "1", "]", "\n", "\n", "", "results", ".", "append", "(", "f", "(", "previous_model", "=", "previous_model", ")", ")", "\n", "\n", "", "return", "results", "\n", "", "else", ":", "\n", "                ", "return", "[", "f", "(", ")", "for", "_", "in", "range", "(", "num", ")", "]", "\n", "\n", "", "", "return", "f", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.DannModelBase.__init__": [[432, 437], ["super().__init__", "models.DannGrlSchedule", "models.FlipGradient"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__", "home.repos.pwc.inspect_result.floft_calda.None.models.DannGrlSchedule"], ["def", "__init__", "(", "self", ",", "num_classes", ",", "num_domains", ",", "global_step", ",", "\n", "total_steps", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "num_classes", ",", "num_domains", ",", "**", "kwargs", ")", "\n", "grl_schedule", "=", "DannGrlSchedule", "(", "total_steps", ")", "\n", "self", ".", "flip_gradient", "=", "FlipGradient", "(", "global_step", ",", "grl_schedule", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.DannModelBase.call_domain_classifier": [[438, 442], ["models.DannModelBase.flip_gradient", "super().call_domain_classifier"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.models.flip_gradient", "home.repos.pwc.inspect_result.floft_calda.None.models.SleepModel.call_domain_classifier"], ["", "def", "call_domain_classifier", "(", "self", ",", "fe", ",", "task", ",", "**", "kwargs", ")", ":", "\n", "# Pass FE output through GRL then to DC", "\n", "        ", "grl_output", "=", "self", ".", "flip_gradient", "(", "fe", ",", "**", "kwargs", ")", "\n", "return", "super", "(", ")", ".", "call_domain_classifier", "(", "grl_output", ",", "task", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.SleepModel.__init__": [[452, 456], ["models.DannModelBase.__init__", "tensorflow.keras.layers.Concatenate", "models.StopGradient"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "concat", "=", "tf", ".", "keras", ".", "layers", ".", "Concatenate", "(", "axis", "=", "1", ")", "\n", "self", ".", "stop_gradient", "=", "StopGradient", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.SleepModel.call_domain_classifier": [[457, 468], ["models.SleepModel.flip_gradient", "models.SleepModel.stop_gradient", "models.SleepModel.concat", "models.SleepModel.domain_classifier", "isinstance"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.models.flip_gradient"], ["", "def", "call_domain_classifier", "(", "self", ",", "fe", ",", "task", ",", "**", "kwargs", ")", ":", "\n", "# We could support this but it's awkward since we want to call the super's", "\n", "# super's call_domain_classifier but not the super's version...", "\n", "        ", "assert", "not", "isinstance", "(", "self", ".", "domain_classifier", ",", "list", ")", ",", "\"currently do not support SleepModel with multiple domain classifiers\"", "\n", "\n", "# Pass FE output through GRL and append stop-gradient-ed task output too", "\n", "grl_output", "=", "self", ".", "flip_gradient", "(", "fe", ",", "**", "kwargs", ")", "\n", "task_stop_gradient", "=", "self", ".", "stop_gradient", "(", "task", ")", "\n", "domain_input", "=", "self", ".", "concat", "(", "[", "grl_output", ",", "task_stop_gradient", "]", ")", "\n", "return", "self", ".", "domain_classifier", "(", "domain_input", ",", "**", "kwargs", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.floft_calda.None.models.register_model": [[13, 23], ["None"], "function", ["None"], ["def", "register_model", "(", "name", ")", ":", "\n", "    ", "\"\"\" Add model to the list of models, e.g. add @register_model(\"name\")\n    before a class definition \"\"\"", "\n", "assert", "name", "not", "in", "models", ",", "\"duplicate model named \"", "+", "name", "\n", "\n", "def", "decorator", "(", "cls", ")", ":", "\n", "        ", "models", "[", "name", "]", "=", "cls", "\n", "return", "cls", "\n", "\n", "", "return", "decorator", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.get_model": [[25, 30], ["models.keys"], "function", ["None"], ["", "def", "get_model", "(", "name", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Based on the given name, call the correct model \"\"\"", "\n", "assert", "name", "in", "models", ".", "keys", "(", ")", ",", "\"Unknown model name \"", "+", "name", "\n", "return", "models", "[", "name", "]", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.list_models": [[32, 35], ["list", "models.keys"], "function", ["None"], ["", "def", "list_models", "(", ")", ":", "\n", "    ", "\"\"\" Returns list of all the available models \"\"\"", "\n", "return", "list", "(", "models", ".", "keys", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.flip_gradient": [[37, 47], ["tensorflow.cast", "tensorflow.ones_like", "tensorflow.negative"], "function", ["None"], ["", "@", "tf", ".", "custom_gradient", "\n", "def", "flip_gradient", "(", "x", ",", "grl_lambda", ")", ":", "\n", "    ", "\"\"\" Forward pass identity, backward pass negate gradient and multiply by  \"\"\"", "\n", "grl_lambda", "=", "tf", ".", "cast", "(", "grl_lambda", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "def", "grad", "(", "dy", ")", ":", "\n", "# the 0 is for grl_lambda, which doesn't have a gradient", "\n", "        ", "return", "tf", ".", "negative", "(", "dy", ")", "*", "grl_lambda", "*", "tf", ".", "ones_like", "(", "x", ")", ",", "0", "\n", "\n", "", "return", "x", ",", "grad", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.models.DannGrlSchedule": [[70, 79], ["tensorflow.cast", "tensorflow.cast", "tensorflow.exp"], "function", ["None"], ["", "", "def", "DannGrlSchedule", "(", "num_steps", ")", ":", "\n", "    ", "\"\"\" GRL schedule from DANN paper \"\"\"", "\n", "num_steps", "=", "tf", ".", "cast", "(", "num_steps", ",", "tf", ".", "float32", ")", "\n", "\n", "def", "schedule", "(", "step", ")", ":", "\n", "        ", "step", "=", "tf", ".", "cast", "(", "step", ",", "tf", ".", "float32", ")", "\n", "return", "2", "/", "(", "1", "+", "tf", ".", "exp", "(", "-", "10", "*", "(", "step", "/", "(", "num_steps", "+", "1", ")", ")", ")", ")", "-", "1", "\n", "\n", "", "return", "schedule", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.__init__": [[61, 135], ["len", "len", "methods.MethodBase.calculate_domain_outputs", "methods.MethodBase.create_iterators", "methods.MethodBase.create_losses", "enumerate", "enumerate", "isinstance", "methods.MethodBase.get_num_modalities", "methods.MethodBase.create_optimizers", "methods.MethodBase.create_model", "opt_dict.items", "len", "isinstance", "int", "range", "range", "NotImplementedError", "shared_modalities.split", "str", "str", "str"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDannDG.calculate_domain_outputs", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.create_iterators", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaBase.create_losses", "home.repos.pwc.inspect_result.floft_calda.None.load_datasets.get_num_modalities", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDann.create_optimizers", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodAflacDG.create_model"], ["    ", "def", "__init__", "(", "self", ",", "source_datasets", ",", "target_dataset", ",", "model_name", ",", "\n", "*", "args", ",", "ensemble_size", "=", "1", ",", "trainable", "=", "True", ",", "moving_average", "=", "False", ",", "\n", "shared_modalities", "=", "None", ",", "share_most_weights", "=", "False", ",", "\n", "dataset_name", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "source_datasets", "=", "source_datasets", "\n", "self", ".", "target_dataset", "=", "target_dataset", "\n", "self", ".", "moving_average", "=", "moving_average", "\n", "self", ".", "ensemble_size", "=", "ensemble_size", "\n", "assert", "ensemble_size", ">", "0", ",", "\"ensemble_size should be >= 1\"", "\n", "self", ".", "share_most_weights", "=", "share_most_weights", "# for HeterogeneousBase", "\n", "self", ".", "dataset_name", "=", "dataset_name", "\n", "\n", "# Support multiple targets when we add that functionality", "\n", "self", ".", "num_source_domains", "=", "len", "(", "source_datasets", ")", "\n", "self", ".", "num_domains", "=", "len", "(", "source_datasets", ")", "\n", "\n", "if", "target_dataset", "is", "not", "None", ":", "\n", "            ", "if", "isinstance", "(", "target_dataset", ",", "list", ")", ":", "\n", "                ", "self", ".", "num_domains", "+=", "len", "(", "target_dataset", ")", "\n", "", "elif", "isinstance", "(", "target_dataset", ",", "load_datasets", ".", "Dataset", ")", ":", "\n", "                ", "self", ".", "num_domains", "+=", "1", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "(", "\"target_dataset should be either one \"", "\n", "\"load_datasets.Dataset() or a list of them, \"", "\n", "\"but is \"", "+", "str", "(", "target_dataset", ")", ")", "\n", "\n", "", "", "if", "shared_modalities", "is", "not", "None", ":", "\n", "            ", "self", ".", "shared_modalities", "=", "[", "int", "(", "x", ")", "for", "x", "in", "shared_modalities", ".", "split", "(", "\",\"", ")", "]", "\n", "", "else", ":", "\n", "# Use first modality, assuming there's only one", "\n", "# Note: this of course also requires that there is a target domain.", "\n", "            ", "source_num_modalities", ",", "target_num_modalities", "=", "self", ".", "get_num_modalities", "(", ")", "\n", "assert", "source_num_modalities", "==", "1", ",", "\"if multiple modalities, set shared_modalities\"", "\n", "assert", "target_num_modalities", "==", "1", ",", "\"if multiple modalities, set shared_modalities\"", "\n", "\n", "self", ".", "shared_modalities", "=", "[", "0", "]", "\n", "\n", "# How to calculate the number of domain outputs", "\n", "", "self", ".", "domain_outputs", "=", "self", ".", "calculate_domain_outputs", "(", ")", "\n", "\n", "# We need to know the num_classes for creating the model", "\n", "# We'll just pick the first source since we have to have at least one", "\n", "# source and we've already verified they're all the same in load_da()", "\n", "self", ".", "num_classes", "=", "source_datasets", "[", "0", "]", ".", "num_classes", "\n", "\n", "# Needed for CoDATS label", "\n", "self", ".", "train_batch", "=", "source_datasets", "[", "0", "]", ".", "train_batch", "\n", "\n", "# What we want in the checkpoint", "\n", "self", ".", "checkpoint_variables", "=", "{", "}", "\n", "\n", "# Initialize components -- support ensemble, training all simultaneously", "\n", "# I think will be faster / more efficient overall time-wise", "\n", "self", ".", "create_iterators", "(", ")", "\n", "self", ".", "opt", "=", "[", "self", ".", "create_optimizers", "(", ")", "for", "_", "in", "range", "(", "ensemble_size", ")", "]", "\n", "self", ".", "model", "=", "[", "self", ".", "create_model", "(", "model_name", ")", "for", "_", "in", "range", "(", "ensemble_size", ")", "]", "\n", "self", ".", "create_losses", "(", ")", "\n", "\n", "# Checkpoint/save the model and optimizers", "\n", "for", "i", ",", "model", "in", "enumerate", "(", "self", ".", "model", ")", ":", "\n", "            ", "self", ".", "checkpoint_variables", "[", "\"model_\"", "+", "str", "(", "i", ")", "]", "=", "model", "\n", "\n", "", "for", "i", ",", "opt_dict", "in", "enumerate", "(", "self", ".", "opt", ")", ":", "\n", "            ", "for", "name", ",", "opt", "in", "opt_dict", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "checkpoint_variables", "[", "\"opt_\"", "+", "name", "+", "\"_\"", "+", "str", "(", "i", ")", "]", "=", "opt", "\n", "\n", "# Names of the losses returned in compute_losses", "\n", "", "", "self", ".", "loss_names", "=", "[", "\"total\"", "]", "\n", "\n", "# Should this method be trained (if not, then in main.py the config", "\n", "# is written and then it exits)", "\n", "self", ".", "trainable", "=", "trainable", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.get_num_modalities": [[136, 150], ["range", "len"], "methods", ["None"], ["", "def", "get_num_modalities", "(", "self", ")", ":", "\n", "        ", "\"\"\" Get the number of source/target modalities \"\"\"", "\n", "source_num_modalities", "=", "self", ".", "source_datasets", "[", "0", "]", ".", "num_modalities", "\n", "for", "other_source", "in", "range", "(", "1", ",", "len", "(", "self", ".", "source_datasets", ")", ")", ":", "\n", "            ", "assert", "self", ".", "source_datasets", "[", "other_source", "]", ".", "num_modalities", "==", "source_num_modalities", ",", "\"sources with different num_modalities not supported yet\"", "\n", "\n", "", "if", "self", ".", "target_dataset", "is", "not", "None", ":", "\n", "            ", "target_num_modalities", "=", "self", ".", "target_dataset", ".", "num_modalities", "\n", "", "else", ":", "\n", "            ", "target_num_modalities", "=", "None", "\n", "\n", "", "return", "source_num_modalities", ",", "target_num_modalities", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.calc_num_components": [[151, 177], ["methods.MethodBase.get_num_modalities", "len", "print", "print", "max"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.load_datasets.get_num_modalities"], ["", "def", "calc_num_components", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        We need a feature extractor for each modality. Take the max of the\n        source or target number of modalities. Note: if for example the source\n        has 2 and the target has 1, the lowest index source FE will be used\n        for the target modality (\"left to right\" of sorts). Change the\n        ordering with {source,target}_modality_subset if desired.\n\n        We can't use len(self.shared_modalities) here since we need a FE even\n        for the non-shared modalities.\n        \"\"\"", "\n", "source_num_modalities", ",", "target_num_modalities", "=", "self", ".", "get_num_modalities", "(", ")", "\n", "\n", "if", "target_num_modalities", "is", "not", "None", ":", "\n", "            ", "num_feature_extractors", "=", "max", "(", "source_num_modalities", ",", "\n", "target_num_modalities", ")", "\n", "", "else", ":", "\n", "            ", "num_feature_extractors", "=", "source_num_modalities", "\n", "\n", "# However, for the domain classifiers, we need one per shared modality.", "\n", "", "num_domain_classifiers", "=", "len", "(", "self", ".", "shared_modalities", ")", "\n", "\n", "print", "(", "\"Creating\"", ",", "num_feature_extractors", ",", "\"feature extractors\"", ")", "\n", "print", "(", "\"Creating\"", ",", "num_domain_classifiers", ",", "\"domain classifiers\"", ")", "\n", "\n", "return", "num_feature_extractors", ",", "num_domain_classifiers", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.calculate_domain_outputs": [[178, 184], ["None"], "methods", ["None"], ["", "def", "calculate_domain_outputs", "(", "self", ")", ":", "\n", "        ", "\"\"\" Calculate the number of outputs for the domain classifier. By\n        default it's the number of domains. However, for example, in domain\n        generalization we ignore the target, so it'll actually be the number of\n        source domains only, in which case override this function. \"\"\"", "\n", "return", "self", ".", "num_domains", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.create_iterators": [[185, 202], ["iter", "iter"], "methods", ["None"], ["", "def", "create_iterators", "(", "self", ")", ":", "\n", "        ", "\"\"\" Get the source/target train/eval datasets \"\"\"", "\n", "self", ".", "source_domain_ids", "=", "[", "x", ".", "domain_id", "for", "x", "in", "self", ".", "source_datasets", "]", "\n", "self", ".", "source_train_iterators", "=", "[", "iter", "(", "x", ".", "train", ")", "for", "x", "in", "self", ".", "source_datasets", "]", "\n", "self", ".", "source_train_eval_datasets", "=", "[", "x", ".", "train_evaluation", "for", "x", "in", "self", ".", "source_datasets", "]", "\n", "self", ".", "source_test_eval_datasets", "=", "[", "x", ".", "test_evaluation", "for", "x", "in", "self", ".", "source_datasets", "]", "\n", "\n", "if", "self", ".", "target_dataset", "is", "not", "None", ":", "\n", "            ", "self", ".", "target_domain_id", "=", "self", ".", "target_dataset", ".", "domain_id", "\n", "self", ".", "target_train_iterator", "=", "iter", "(", "self", ".", "target_dataset", ".", "train", ")", "\n", "self", ".", "target_train_eval_dataset", "=", "self", ".", "target_dataset", ".", "train_evaluation", "\n", "self", ".", "target_test_eval_dataset", "=", "self", ".", "target_dataset", ".", "test_evaluation", "\n", "", "else", ":", "\n", "            ", "self", ".", "target_domain_id", "=", "None", "\n", "self", ".", "target_train_iterator", "=", "None", "\n", "self", ".", "target_train_eval_dataset", "=", "None", "\n", "self", ".", "target_test_eval_dataset", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.create_optimizer": [[203, 211], ["tensorflow.keras.optimizers.Adam", "tensorflow_addons.optimizers.MovingAverage"], "methods", ["None"], ["", "", "def", "create_optimizer", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\" Create a single optimizer \"\"\"", "\n", "opt", "=", "tf", ".", "keras", ".", "optimizers", ".", "Adam", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "if", "self", ".", "moving_average", ":", "\n", "            ", "opt", "=", "tfa", ".", "optimizers", ".", "MovingAverage", "(", "opt", ")", "\n", "\n", "", "return", "opt", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.create_optimizers": [[212, 214], ["methods.MethodBase.create_optimizer"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.create_optimizer"], ["", "def", "create_optimizers", "(", "self", ")", ":", "\n", "        ", "return", "{", "\"opt\"", ":", "self", ".", "create_optimizer", "(", "learning_rate", "=", "FLAGS", ".", "lr", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.create_model": [[215, 218], ["models.BasicModel"], "methods", ["None"], ["", "def", "create_model", "(", "self", ",", "model_name", ")", ":", "\n", "        ", "return", "models", ".", "BasicModel", "(", "self", ".", "num_classes", ",", "self", ".", "domain_outputs", ",", "\n", "model_name", "=", "model_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.create_losses": [[219, 221], ["methods.make_loss"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.make_loss"], ["", "def", "create_losses", "(", "self", ")", ":", "\n", "        ", "self", ".", "task_loss", "=", "make_loss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.get_next_train_data": [[222, 231], ["methods.MethodBase.get_next_batch_both", "next", "next"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.get_next_batch_both"], ["", "@", "tf", ".", "function", "\n", "def", "get_next_train_data", "(", "self", ")", ":", "\n", "        ", "\"\"\" Get next batch of training data \"\"\"", "\n", "# Note we will use this same exact data in Metrics() as we use in", "\n", "# train_step()", "\n", "data_sources", "=", "[", "next", "(", "x", ")", "for", "x", "in", "self", ".", "source_train_iterators", "]", "\n", "data_target", "=", "next", "(", "self", ".", "target_train_iterator", ")", "if", "self", ".", "target_train_iterator", "is", "not", "None", "else", "None", "\n", "return", "self", ".", "get_next_batch_both", "(", "data_sources", ",", "data_target", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.domain_label": [[232, 244], ["None"], "methods", ["None"], ["", "def", "domain_label", "(", "self", ",", "index", ",", "is_target", ")", ":", "\n", "        ", "\"\"\" Default domain labeling. Indexes should be in [0,+inf) and integers.\n        0 = target\n        1 = source #0\n        2 = source #1\n        3 = source #2\n        ...\n        \"\"\"", "\n", "if", "is_target", ":", "\n", "            ", "return", "0", "\n", "", "else", ":", "\n", "            ", "return", "index", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.get_next_batch_both": [[245, 252], ["methods.MethodBase.get_next_batch_multiple", "methods.MethodBase.get_next_batch_single"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.get_next_batch_multiple", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.get_next_batch_single"], ["", "", "@", "tf", ".", "function", "\n", "def", "get_next_batch_both", "(", "self", ",", "data_sources", ",", "data_target", ")", ":", "\n", "        ", "\"\"\" Compile for training. Don't for evaluation (called directly,\n        not this _both function). \"\"\"", "\n", "data_sources", "=", "self", ".", "get_next_batch_multiple", "(", "data_sources", ",", "is_target", "=", "False", ")", "\n", "data_target", "=", "self", ".", "get_next_batch_single", "(", "data_target", ",", "is_target", "=", "True", ")", "\n", "return", "data_sources", ",", "data_target", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.get_next_batch_multiple": [[253, 283], ["enumerate", "xs.append", "ys.append", "ds.append", "example_ids.append", "len", "tensorflow.ones_like", "methods.MethodBase.domain_label"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDannDG.domain_label"], ["", "def", "get_next_batch_multiple", "(", "self", ",", "data", ",", "is_target", ")", ":", "\n", "        ", "\"\"\"\n        Get next set of training data. data should be a list of data (probably\n        something like [next(x) for x in iterators]).\n\n        Returns: (\n            [x_a1, x_a2, x_a3, ...],\n            [y_a1, y_a2, y_a3, ...],\n            [domain_a1, domain_a2, domain_a3, ...]\n        )\n        \"\"\"", "\n", "if", "data", "is", "None", ":", "\n", "            ", "return", "None", "\n", "\n", "", "assert", "not", "is_target", "or", "len", "(", "data", ")", "==", "1", ",", "\"only support one target at present\"", "\n", "\n", "xs", "=", "[", "]", "\n", "ys", "=", "[", "]", "\n", "ds", "=", "[", "]", "\n", "example_ids", "=", "[", "]", "\n", "\n", "for", "i", ",", "(", "x", ",", "y", ",", "example_id", ")", "in", "enumerate", "(", "data", ")", ":", "\n", "            ", "xs", ".", "append", "(", "x", ")", "\n", "ys", ".", "append", "(", "y", ")", "\n", "ds", ".", "append", "(", "tf", ".", "ones_like", "(", "y", ")", "*", "self", ".", "domain_label", "(", "index", "=", "i", ",", "\n", "is_target", "=", "is_target", ")", ")", "\n", "example_ids", ".", "append", "(", "example_id", ")", "\n", "\n", "", "return", "(", "xs", ",", "ys", ",", "ds", ",", "example_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.get_next_batch_single": [[284, 306], ["tensorflow.ones_like", "methods.MethodBase.domain_label"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDannDG.domain_label"], ["", "def", "get_next_batch_single", "(", "self", ",", "data", ",", "is_target", ",", "index", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        Get next set of training data. data should be a single batch (probably\n        something like next(iterator)). When processing target data, index\n        must be 0 since we only support one target at the moment. However,\n        during evaluation we evaluate each source's data individually so if\n        is_target is False, then index can be whichever source domain was\n        passed.\n\n        Returns: (x, y, domain)\n        \"\"\"", "\n", "if", "data", "is", "None", ":", "\n", "            ", "return", "None", "\n", "\n", "", "assert", "not", "is_target", "or", "index", "==", "0", ",", "\"only support one target at present\"", "\n", "\n", "x", ",", "y", ",", "example_id", "=", "data", "\n", "d", "=", "tf", ".", "ones_like", "(", "y", ")", "*", "self", ".", "domain_label", "(", "index", "=", "index", ",", "is_target", "=", "is_target", ")", "\n", "data_target", "=", "(", "x", ",", "y", ",", "d", ",", "example_id", ")", "\n", "\n", "return", "data_target", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.prepare_data": [[309, 344], ["methods.MethodBase.get_num_modalities", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "range"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.load_datasets.get_num_modalities"], ["", "@", "tf", ".", "function", "\n", "def", "prepare_data", "(", "self", ",", "data_sources", ",", "data_target", ",", "which_model", ")", ":", "\n", "        ", "\"\"\" Prepare the data for the model, e.g. by concatenating all sources\n        together. Note: do not put code in here that changes the domain labels\n        since you presumably want that during evaluation too. Put that in\n        domain_label()\n\n        Input xs shape: [num_domains, num_modalities, per-modality x shape...]\n        Output xs shape: [num_modalities, per-modality x shape...]\n\n        Note: compile by adding @tf.function decorator if the contents of this\n        (overrided) function is compilable\n        \"\"\"", "\n", "# By default (e.g. for no adaptation or domain generalization), ignore", "\n", "# the target data", "\n", "xs_a", ",", "y_a", ",", "domain_a", ",", "example_ids", "=", "data_sources", "\n", "\n", "# Concatenate all source domains' data", "\n", "#", "\n", "# xs is a list of domains, which is a tuple of modalities, which is", "\n", "# tensors, e.g. if one modality but two sources:", "\n", "#     [(s1 tensor,), (s2 tensor,)]", "\n", "# We want to concatenate the tensors from all domains separately for", "\n", "# each modality.", "\n", "source_num_modalities", ",", "target_num_modalities", "=", "self", ".", "get_num_modalities", "(", ")", "\n", "xs", "=", "[", "\n", "tf", ".", "concat", "(", "[", "x_a", "[", "i", "]", "for", "x_a", "in", "xs_a", "]", ",", "axis", "=", "0", ")", "\n", "for", "i", "in", "range", "(", "source_num_modalities", ")", "\n", "]", "\n", "\n", "task_y_true", "=", "tf", ".", "concat", "(", "y_a", ",", "axis", "=", "0", ")", "\n", "domain_y_true", "=", "tf", ".", "concat", "(", "domain_a", ",", "axis", "=", "0", ")", "\n", "auxiliary_data", "=", "None", "\n", "\n", "return", "xs", ",", "task_y_true", ",", "domain_y_true", ",", "auxiliary_data", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.prepare_data_eval": [[345, 374], ["isinstance", "isinstance", "methods.MethodBase.get_num_modalities", "tensorflow.concat", "tensorflow.concat", "isinstance", "tensorflow.concat", "range"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.load_datasets.get_num_modalities"], ["", "def", "prepare_data_eval", "(", "self", ",", "data", ",", "is_target", ")", ":", "\n", "        ", "\"\"\" Prepare the data for the model, e.g. by concatenating all sources\n        together. This is like prepare_data() but use during evaluation.\n\n        Input xs shape: [num_domains, num_modalities, per-modality x shape...]\n        Output xs shape: [num_modalities, per-modality x shape...]\n        \"\"\"", "\n", "xs", ",", "y", ",", "domain", ",", "example_ids", "=", "data", "\n", "\n", "for", "x", "in", "xs", ":", "\n", "            ", "assert", "isinstance", "(", "x", ",", "list", ")", ",", "\"Must pass xs=[[...],[...],...] even if only one domain for tf.function consistency\"", "\n", "", "assert", "isinstance", "(", "y", ",", "list", ")", ",", "\"Must pass y=[...] even if only one domain for tf.function consistency\"", "\n", "assert", "isinstance", "(", "domain", ",", "list", ")", ",", "\"Must pass domain=[...] even if only one domain for tf.function consistency\"", "\n", "\n", "# Concatenate all the data (e.g. if multiple source domains)", "\n", "source_num_modalities", ",", "target_num_modalities", "=", "self", ".", "get_num_modalities", "(", ")", "\n", "num_modalities", "=", "target_num_modalities", "if", "is_target", "else", "source_num_modalities", "\n", "xs", "=", "[", "\n", "tf", ".", "concat", "(", "[", "x", "[", "i", "]", "for", "x", "in", "xs", "]", ",", "axis", "=", "0", ")", "\n", "for", "i", "in", "range", "(", "num_modalities", ")", "\n", "]", "\n", "y", "=", "tf", ".", "concat", "(", "y", ",", "axis", "=", "0", ")", "\n", "domain", "=", "tf", ".", "concat", "(", "domain", ",", "axis", "=", "0", ")", "\n", "auxiliary_data", "=", "None", "\n", "\n", "return", "xs", ",", "y", ",", "domain", ",", "auxiliary_data", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.post_data_eval": [[375, 383], ["tensorflow.nn.softmax", "tensorflow.nn.softmax"], "methods", ["None"], ["", "def", "post_data_eval", "(", "self", ",", "task_y_true", ",", "task_y_pred", ",", "domain_y_true", ",", "\n", "domain_y_pred", ")", ":", "\n", "        ", "\"\"\" Optionally do something with the data after feeding through the\n        model. Since the model outputs logits, here we actually take the softmax\n        so that during evaluation we have probability distributions. \"\"\"", "\n", "task_y_pred", "=", "tf", ".", "nn", ".", "softmax", "(", "task_y_pred", ")", "\n", "domain_y_pred", "=", "[", "tf", ".", "nn", ".", "softmax", "(", "d", ")", "for", "d", "in", "domain_y_pred", "]", "\n", "return", "task_y_true", ",", "task_y_pred", ",", "domain_y_true", ",", "domain_y_pred", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.call_model": [[384, 386], ["None"], "methods", ["None"], ["", "def", "call_model", "(", "self", ",", "xs", ",", "which_model", ",", "is_target", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "model", "[", "which_model", "]", "(", "xs", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.compute_losses": [[387, 392], ["methods.MethodBase.task_loss"], "methods", ["None"], ["", "def", "compute_losses", "(", "self", ",", "xs", ",", "task_y_true", ",", "domain_y_true", ",", "task_y_pred", ",", "\n", "domain_y_pred", ",", "fe_output", ",", "contrastive_output", ",", "auxiliary_data", ",", "\n", "which_model", ",", "training", ")", ":", "\n", "# Maybe: regularization = sum(model.losses) and add to loss", "\n", "        ", "return", "self", ".", "task_loss", "(", "task_y_true", ",", "task_y_pred", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.compute_gradients": [[393, 396], ["tape.gradient"], "methods", ["None"], ["", "def", "compute_gradients", "(", "self", ",", "tape", ",", "loss", ",", "which_model", ")", ":", "\n", "        ", "return", "tape", ".", "gradient", "(", "loss", ",", "\n", "self", ".", "model", "[", "which_model", "]", ".", "trainable_variables_task_fe", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.apply_gradients": [[397, 400], ["[].apply_gradients", "zip"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodAflacDG.apply_gradients"], ["", "def", "apply_gradients", "(", "self", ",", "grad", ",", "which_model", ")", ":", "\n", "        ", "self", ".", "opt", "[", "which_model", "]", "[", "\"opt\"", "]", ".", "apply_gradients", "(", "zip", "(", "grad", ",", "\n", "self", ".", "model", "[", "which_model", "]", ".", "trainable_variables_task_fe", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.train_step": [[401, 451], ["range", "range", "methods.MethodBase.get_next_train_data", "all_data_sources.append", "all_data_target.append", "methods.MethodBase.prepare_data", "methods.MethodBase._update_model"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.get_next_train_data", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDannDG.prepare_data", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase._update_model"], ["", "def", "train_step", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Get batch of data, prepare data, run through model, compute losses,\n        apply the gradients\n\n        Override the individual parts with prepare_data(), call_model(),\n        compute_losses(), compute_gradients(), and apply_gradients()\n\n        We return the batch of data so we can use the exact same training batch\n        for the \"train\" evaluation metrics.\n        \"\"\"", "\n", "# TensorFlow errors constructing the graph (with tf.function, which", "\n", "# makes training faster) if we don't know the data size. Thus, first", "\n", "# load batches, then pass to compiled train step.", "\n", "all_data_sources", "=", "[", "]", "\n", "all_data_target", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "ensemble_size", ")", ":", "\n", "            ", "data_sources", ",", "data_target", "=", "self", ".", "get_next_train_data", "(", ")", "\n", "all_data_sources", ".", "append", "(", "data_sources", ")", "\n", "all_data_target", ".", "append", "(", "data_target", ")", "\n", "\n", "# If desired, use the same batch for each of the models.", "\n", "if", "FLAGS", ".", "ensemble_same_data", ":", "\n", "                ", "break", "\n", "\n", "", "", "for", "i", "in", "range", "(", "self", ".", "ensemble_size", ")", ":", "\n", "# Get random batch for this model in the ensemble (either same for", "\n", "# all or different for each)", "\n", "            ", "if", "FLAGS", ".", "ensemble_same_data", ":", "\n", "                ", "data_sources", "=", "all_data_sources", "[", "0", "]", "\n", "data_target", "=", "all_data_target", "[", "0", "]", "\n", "", "else", ":", "\n", "                ", "data_sources", "=", "all_data_sources", "[", "i", "]", "\n", "data_target", "=", "all_data_target", "[", "i", "]", "\n", "\n", "# Prepare", "\n", "", "xs", ",", "task_y_true", ",", "domain_y_true", ",", "auxiliary_data", "=", "self", ".", "prepare_data", "(", "\n", "data_sources", ",", "data_target", ",", "which_model", "=", "i", "\n", ")", "\n", "\n", "# We compile the entire model call, loss computation, and gradient", "\n", "# update/apply", "\n", "self", ".", "_update_model", "(", "\n", "xs", ",", "task_y_true", ",", "domain_y_true", ",", "auxiliary_data", ",", "which_model", "=", "i", "\n", ")", "\n", "\n", "# We return the first one since we don't really care about the \"train\"", "\n", "# evaluation metrics that much.", "\n", "", "return", "all_data_sources", "[", "0", "]", ",", "all_data_target", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase._update_model": [[452, 467], ["methods.MethodBase.compute_gradients", "methods.MethodBase.apply_gradients", "tensorflow.GradientTape", "methods.MethodBase.call_model", "methods.MethodBase.compute_losses"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodAflacDG.compute_gradients", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodAflacDG.apply_gradients", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.call_model", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodAflacDG.compute_losses"], ["", "@", "tf", ".", "function", "\n", "def", "_update_model", "(", "self", ",", "xs", ",", "task_y_true", ",", "domain_y_true", ",", "auxiliary_data", ",", "\n", "which_model", ")", ":", "\n", "# Run batch through the model and compute loss", "\n", "        ", "with", "tf", ".", "GradientTape", "(", "persistent", "=", "True", ")", "as", "tape", ":", "\n", "            ", "task_y_pred", ",", "domain_y_pred", ",", "fe_output", ",", "contrastive_output", "=", "self", ".", "call_model", "(", "xs", ",", "which_model", "=", "which_model", ",", "training", "=", "True", ")", "\n", "losses", "=", "self", ".", "compute_losses", "(", "xs", ",", "task_y_true", ",", "domain_y_true", ",", "\n", "task_y_pred", ",", "domain_y_pred", ",", "fe_output", ",", "contrastive_output", ",", "\n", "auxiliary_data", ",", "which_model", "=", "which_model", ",", "training", "=", "True", ")", "\n", "\n", "# Update model", "\n", "", "gradients", "=", "self", ".", "compute_gradients", "(", "tape", ",", "losses", ",", "which_model", "=", "which_model", ")", "\n", "del", "tape", "\n", "self", ".", "apply_gradients", "(", "gradients", ",", "which_model", "=", "which_model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.eval_step": [[468, 498], ["methods.MethodBase.eval_step_list", "isinstance", "xs_list.append", "isinstance", "isinstance", "isinstance", "list"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.eval_step_list"], ["", "def", "eval_step", "(", "self", ",", "data", ",", "is_target", ",", "is_single_domain", ")", ":", "\n", "        ", "\"\"\" Evaluate a batch of source or target data, called in metrics.py.\n        This preprocesses the data to have x, y, domain always be lists so\n        we can use the same compiled tf.function code in eval_step_list() for\n        both sources and target domains. \"\"\"", "\n", "xs", ",", "y", ",", "domain", ",", "example_ids", "=", "data", "\n", "\n", "# If it's a single domain,to make later code consistent, embed in a", "\n", "# single-item list.", "\n", "if", "is_single_domain", ":", "\n", "            ", "xs", "=", "[", "xs", "]", "\n", "\n", "# Convert any tuples to lists", "\n", "", "xs_list", "=", "[", "]", "\n", "for", "x", "in", "xs", ":", "\n", "            ", "if", "isinstance", "(", "x", ",", "tuple", ")", ":", "\n", "# convert the tuple from load_datasets.py (avoiding a TF error)", "\n", "# back to a list", "\n", "                ", "x", "=", "list", "(", "x", ")", "\n", "", "xs_list", ".", "append", "(", "x", ")", "\n", "", "xs", "=", "xs_list", "\n", "\n", "if", "not", "isinstance", "(", "y", ",", "list", ")", ":", "\n", "            ", "y", "=", "[", "y", "]", "\n", "", "if", "not", "isinstance", "(", "domain", ",", "list", ")", ":", "\n", "            ", "domain", "=", "[", "domain", "]", "\n", "", "if", "not", "isinstance", "(", "example_ids", ",", "list", ")", ":", "\n", "            ", "example_ids", "=", "[", "example_ids", "]", "\n", "\n", "", "return", "self", ".", "eval_step_list", "(", "(", "xs", ",", "y", ",", "domain", ",", "example_ids", ")", ",", "is_target", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.add_multiple_losses": [[499, 535], ["enumerate", "averaged_losses.append", "len", "len", "len"], "methods", ["None"], ["", "def", "add_multiple_losses", "(", "self", ",", "losses", ",", "average", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        losses = [\n            [total_loss1, task_loss1, ...],\n            [total_loss2, task_loss2, ...],\n            ...\n        ]\n\n        returns [total_loss, task_loss, ...] either the sum or average\n        \"\"\"", "\n", "losses_added", "=", "None", "\n", "\n", "for", "loss_list", "in", "losses", ":", "\n", "# If no losses yet, then just set to this", "\n", "            ", "if", "losses_added", "is", "None", ":", "\n", "                ", "losses_added", "=", "loss_list", "\n", "# Otherwise, add to the previous loss values", "\n", "", "else", ":", "\n", "                ", "assert", "len", "(", "losses_added", ")", "==", "len", "(", "loss_list", ")", ",", "\"subsequent losses have different length than the first\"", "\n", "\n", "for", "i", ",", "loss", "in", "enumerate", "(", "loss_list", ")", ":", "\n", "                    ", "losses_added", "[", "i", "]", "+=", "loss", "\n", "\n", "", "", "", "assert", "losses_added", "is", "not", "None", ",", "\"must return losses from at least one domain\"", "\n", "\n", "if", "average", ":", "\n", "            ", "averaged_losses", "=", "[", "]", "\n", "\n", "for", "loss", "in", "losses_added", ":", "\n", "                ", "averaged_losses", ".", "append", "(", "loss", "/", "len", "(", "losses", ")", ")", "\n", "\n", "", "return", "averaged_losses", "\n", "", "else", ":", "\n", "            ", "return", "losses_added", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.eval_step_list": [[537, 598], ["methods.MethodBase.prepare_data_eval", "range", "tensorflow.math.reduce_mean", "tensorflow.math.reduce_mean", "tensorflow.math.reduce_mean", "tensorflow.math.reduce_mean", "methods.MethodBase.add_multiple_losses", "methods.MethodBase.call_model", "methods.MethodBase.compute_losses", "losses_list.append", "methods.MethodBase.post_data_eval", "task_y_true_list.append", "task_y_pred_list.append", "domain_y_true_list.append", "isinstance"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDann.prepare_data_eval", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.add_multiple_losses", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.call_model", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodAflacDG.compute_losses", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.post_data_eval"], ["", "", "def", "eval_step_list", "(", "self", ",", "data", ",", "is_target", ")", ":", "\n", "        ", "\"\"\" Override preparation in prepare_data_eval() \"\"\"", "\n", "(", "\n", "xs", ",", "\n", "orig_task_y_true", ",", "\n", "orig_domain_y_true", ",", "\n", "auxiliary_data", ",", "\n", ")", "=", "self", ".", "prepare_data_eval", "(", "data", ",", "is_target", ")", "\n", "\n", "task_y_true_list", "=", "[", "]", "\n", "task_y_pred_list", "=", "[", "]", "\n", "domain_y_true_list", "=", "[", "]", "\n", "domain_y_pred_list", "=", "[", "]", "\n", "losses_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "ensemble_size", ")", ":", "\n", "# Run through model", "\n", "#", "\n", "# We don't need to handle both_domains_simultaneously here because", "\n", "# during evaluation we already pass the source(s) data through", "\n", "# first and the target data through separately second.", "\n", "            ", "task_y_pred", ",", "domain_y_pred", ",", "fe_output", ",", "contrastive_output", "=", "self", ".", "call_model", "(", "xs", ",", "which_model", "=", "i", ",", "is_target", "=", "is_target", ",", "\n", "training", "=", "False", ")", "\n", "\n", "# Calculate losses", "\n", "losses", "=", "self", ".", "compute_losses", "(", "xs", ",", "orig_task_y_true", ",", "\n", "orig_domain_y_true", ",", "task_y_pred", ",", "domain_y_pred", ",", "fe_output", ",", "\n", "contrastive_output", ",", "auxiliary_data", ",", "which_model", "=", "i", ",", "\n", "training", "=", "False", ")", "\n", "\n", "if", "not", "isinstance", "(", "losses", ",", "list", ")", ":", "\n", "                ", "losses", "=", "[", "losses", "]", "\n", "\n", "", "losses_list", ".", "append", "(", "losses", ")", "\n", "\n", "# Post-process data (e.g. compute softmax from logits)", "\n", "task_y_true", ",", "task_y_pred", ",", "domain_y_true", ",", "domain_y_pred", "=", "self", ".", "post_data_eval", "(", "orig_task_y_true", ",", "task_y_pred", ",", "\n", "orig_domain_y_true", ",", "domain_y_pred", ")", "\n", "\n", "task_y_true_list", ".", "append", "(", "task_y_true", ")", "\n", "task_y_pred_list", ".", "append", "(", "task_y_pred", ")", "\n", "domain_y_true_list", ".", "append", "(", "domain_y_true", ")", "\n", "domain_y_pred_list", "+=", "domain_y_pred", "# list from multiple modalities", "\n", "\n", "# Combine information from each model in the ensemble -- averaging.", "\n", "#", "\n", "# Note: this is how the ensemble predictions are made with InceptionTime", "\n", "# having an ensemble of 5 models -- they average the softmax outputs", "\n", "# over the ensemble (and we now have softmax after the post_data_eval()", "\n", "# call). See their code:", "\n", "# https://github.com/hfawaz/InceptionTime/blob/master/classifiers/nne.py", "\n", "", "task_y_true_avg", "=", "tf", ".", "math", ".", "reduce_mean", "(", "task_y_true_list", ",", "axis", "=", "0", ")", "\n", "task_y_pred_avg", "=", "tf", ".", "math", ".", "reduce_mean", "(", "task_y_pred_list", ",", "axis", "=", "0", ")", "\n", "domain_y_true_avg", "=", "tf", ".", "math", ".", "reduce_mean", "(", "domain_y_true_list", ",", "axis", "=", "0", ")", "\n", "domain_y_pred_avg", "=", "tf", ".", "math", ".", "reduce_mean", "(", "domain_y_pred_list", ",", "axis", "=", "0", ")", "\n", "losses_avg", "=", "self", ".", "add_multiple_losses", "(", "losses_list", ",", "average", "=", "True", ")", "\n", "\n", "return", "task_y_true_avg", ",", "task_y_pred_avg", ",", "domain_y_true_avg", ",", "domain_y_pred_avg", ",", "losses_avg", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodNone.create_model": [[606, 619], ["methods.MethodNone.calc_num_components", "models.BasicModel"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.calc_num_components"], ["    ", "def", "create_model", "(", "self", ",", "model_name", ")", ":", "\n", "# We need to create the right number of FE's and DC's otherwise the", "\n", "# closed method will error, though the open/partial will work (assuming", "\n", "# there's only 2 modalities, since then we only have 1 shared)", "\n", "        ", "num_feature_extractors", ",", "num_domain_classifiers", "=", "self", ".", "calc_num_components", "(", ")", "\n", "\n", "return", "models", ".", "BasicModel", "(", "self", ".", "num_classes", ",", "self", ".", "domain_outputs", ",", "\n", "model_name", "=", "model_name", ",", "\n", "num_feature_extractors", "=", "num_feature_extractors", ",", "\n", "num_domain_classifiers", "=", "num_domain_classifiers", ",", "\n", "shared_modalities", "=", "self", ".", "shared_modalities", ",", "\n", "share_most_weights", "=", "self", ".", "share_most_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodNone._keep_shared": [[620, 623], ["None"], "methods", ["None"], ["", "def", "_keep_shared", "(", "self", ",", "xs", ")", ":", "\n", "        ", "\"\"\" Keep only the domains set in --shared_modalities and the order \"\"\"", "\n", "return", "[", "xs", "[", "modality", "]", "for", "modality", "in", "self", ".", "shared_modalities", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodNone.prepare_data": [[624, 630], ["methods.MethodBase.prepare_data", "methods.MethodNone._keep_shared"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDannDG.prepare_data", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodNone._keep_shared"], ["", "@", "tf", ".", "function", "\n", "def", "prepare_data", "(", "self", ",", "data_sources", ",", "data_target", ",", "which_model", ")", ":", "\n", "        ", "xs", ",", "task_y_true", ",", "domain_y_true", ",", "auxiliary_data", "=", "super", "(", ")", ".", "prepare_data", "(", "\n", "data_sources", ",", "data_target", ",", "which_model", "\n", ")", "\n", "return", "self", ".", "_keep_shared", "(", "xs", ")", ",", "task_y_true", ",", "domain_y_true", ",", "auxiliary_data", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodNone.prepare_data_eval": [[631, 634], ["methods.MethodBase.prepare_data_eval", "methods.MethodNone._keep_shared"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDann.prepare_data_eval", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodNone._keep_shared"], ["", "def", "prepare_data_eval", "(", "self", ",", "data", ",", "is_target", ")", ":", "\n", "        ", "xs", ",", "y", ",", "domain", ",", "auxiliary_data", "=", "super", "(", ")", ".", "prepare_data_eval", "(", "data", ",", "is_target", ")", "\n", "return", "self", ".", "_keep_shared", "(", "xs", ")", ",", "y", ",", "domain", ",", "auxiliary_data", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDann.__init__": [[646, 652], ["methods.MethodBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["def", "__init__", "(", "self", ",", "source_datasets", ",", "target_dataset", ",", "\n", "global_step", ",", "total_steps", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "global_step", "=", "global_step", "# should be TF variable", "\n", "self", ".", "total_steps", "=", "total_steps", "\n", "super", "(", ")", ".", "__init__", "(", "source_datasets", ",", "target_dataset", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "loss_names", "+=", "[", "\"task\"", ",", "\"domain\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDann.create_model": [[653, 662], ["models.DannModel", "len"], "methods", ["None"], ["", "def", "create_model", "(", "self", ",", "model_name", ")", ":", "\n", "# By default we create only one FE, so make sure we only have one", "\n", "# shared modality. We drop the non-shared modalities in prepare_data()", "\n", "# and prepare_data_eval().", "\n", "        ", "assert", "len", "(", "self", ".", "shared_modalities", ")", "==", "1", ",", "\"DANN only supports one shared modality between domains\"", "\n", "\n", "return", "models", ".", "DannModel", "(", "self", ".", "num_classes", ",", "self", ".", "domain_outputs", ",", "\n", "self", ".", "global_step", ",", "self", ".", "total_steps", ",", "model_name", "=", "model_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDann.create_optimizers": [[663, 669], ["methods.MethodBase.create_optimizers", "methods.MethodDann.create_optimizer"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDann.create_optimizers", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.create_optimizer"], ["", "def", "create_optimizers", "(", "self", ")", ":", "\n", "        ", "opt", "=", "super", "(", ")", ".", "create_optimizers", "(", ")", "\n", "# We need an additional optimizer for DANN", "\n", "opt", "[", "\"d_opt\"", "]", "=", "self", ".", "create_optimizer", "(", "\n", "learning_rate", "=", "FLAGS", ".", "lr", "*", "FLAGS", ".", "lr_domain_mult", ")", "\n", "return", "opt", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDann.create_losses": [[670, 675], ["methods.MethodBase.create_losses", "methods.make_loss"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaBase.create_losses", "home.repos.pwc.inspect_result.floft_calda.None.methods.make_loss"], ["", "def", "create_losses", "(", "self", ")", ":", "\n", "# Note: at the moment these are the same, but if we go back to", "\n", "# single-source, then the domain classifier may be sigmoid not softmax", "\n", "        ", "super", "(", ")", ".", "create_losses", "(", ")", "\n", "self", ".", "domain_loss", "=", "make_loss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDann.prepare_data": [[676, 713], ["methods.MethodDann.get_num_modalities", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "range", "tensorflow.zeros_like"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.load_datasets.get_num_modalities"], ["", "@", "tf", ".", "function", "\n", "def", "prepare_data", "(", "self", ",", "data_sources", ",", "data_target", ",", "which_model", ")", ":", "\n", "        ", "assert", "data_target", "is", "not", "None", ",", "\"cannot run DANN without target\"", "\n", "xs_a", ",", "y_a", ",", "domain_a", ",", "example_ids_a", "=", "data_sources", "\n", "xs_b", ",", "y_b", ",", "domain_b", ",", "example_ids_b", "=", "data_target", "\n", "\n", "# Concatenate all source domains' data", "\n", "#", "\n", "# xs is a list of domains, which is a tuple of modalities, which is", "\n", "# tensors, e.g. if one modality but two sources:", "\n", "#     [(s1 tensor,), (s2 tensor,)]", "\n", "# We want to concatenate the tensors from all domains separately for", "\n", "# each modality.", "\n", "source_num_modalities", ",", "target_num_modalities", "=", "self", ".", "get_num_modalities", "(", ")", "\n", "xs_a", "=", "[", "\n", "tf", ".", "concat", "(", "[", "x_a", "[", "i", "]", "for", "x_a", "in", "xs_a", "]", ",", "axis", "=", "0", ")", "\n", "for", "i", "in", "range", "(", "source_num_modalities", ")", "\n", "]", "\n", "y_a", "=", "tf", ".", "concat", "(", "y_a", ",", "axis", "=", "0", ")", "\n", "domain_a", "=", "tf", ".", "concat", "(", "domain_a", ",", "axis", "=", "0", ")", "\n", "\n", "# Concatenate for adaptation - concatenate source labels with all-zero", "\n", "# labels for target since we can't use the target labels during", "\n", "# unsupervised domain adaptation", "\n", "#", "\n", "# We concatenate the shared modalities, dropping the non-shared", "\n", "# modalities. DANN by itself doesn't know how to handle changes in the", "\n", "# number of modalities between domains.", "\n", "xs", "=", "[", "\n", "tf", ".", "concat", "(", "(", "xs_a", "[", "modality", "]", ",", "xs_b", "[", "modality", "]", ")", ",", "axis", "=", "0", ")", "\n", "for", "modality", "in", "self", ".", "shared_modalities", "\n", "]", "\n", "task_y_true", "=", "tf", ".", "concat", "(", "(", "y_a", ",", "tf", ".", "zeros_like", "(", "y_b", ")", ")", ",", "axis", "=", "0", ")", "\n", "domain_y_true", "=", "tf", ".", "concat", "(", "(", "domain_a", ",", "domain_b", ")", ",", "axis", "=", "0", ")", "\n", "auxiliary_data", "=", "None", "\n", "\n", "return", "xs", ",", "task_y_true", ",", "domain_y_true", ",", "auxiliary_data", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDann.prepare_data_eval": [[714, 723], ["methods.MethodBase.prepare_data_eval"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDann.prepare_data_eval"], ["", "def", "prepare_data_eval", "(", "self", ",", "data", ",", "is_target", ")", ":", "\n", "        ", "\"\"\" Prepare the data for the model, e.g. by concatenating all sources\n        together. This is like prepare_data() but use during evaluation. \"\"\"", "\n", "xs", ",", "y", ",", "domain", ",", "auxiliary_data", "=", "super", "(", ")", ".", "prepare_data_eval", "(", "data", ",", "is_target", ")", "\n", "xs", "=", "[", "\n", "xs", "[", "modality", "]", "for", "modality", "in", "self", ".", "shared_modalities", "\n", "]", "\n", "\n", "return", "xs", ",", "y", ",", "domain", ",", "auxiliary_data", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDann.compute_losses": [[724, 738], ["tensorflow.where", "tensorflow.gather", "tensorflow.gather", "methods.MethodDann.task_loss", "tensorflow.not_equal", "sum", "len", "methods.MethodDann.domain_loss"], "methods", ["None"], ["", "def", "compute_losses", "(", "self", ",", "xs", ",", "task_y_true", ",", "domain_y_true", ",", "task_y_pred", ",", "\n", "domain_y_pred", ",", "fe_output", ",", "contrastive_output", ",", "auxiliary_data", ",", "\n", "which_model", ",", "training", ")", ":", "\n", "        ", "nontarget", "=", "tf", ".", "where", "(", "tf", ".", "not_equal", "(", "domain_y_true", ",", "0", ")", ")", "\n", "task_y_true", "=", "tf", ".", "gather", "(", "task_y_true", ",", "nontarget", ",", "axis", "=", "0", ")", "\n", "task_y_pred", "=", "tf", ".", "gather", "(", "task_y_pred", ",", "nontarget", ",", "axis", "=", "0", ")", "\n", "\n", "task_loss", "=", "self", ".", "task_loss", "(", "task_y_true", ",", "task_y_pred", ")", "\n", "d_loss", "=", "sum", "(", "[", "\n", "self", ".", "domain_loss", "(", "domain_y_true", ",", "d", ")", "\n", "for", "d", "in", "domain_y_pred", "\n", "]", ")", "/", "len", "(", "domain_y_pred", ")", "\n", "total_loss", "=", "task_loss", "+", "d_loss", "\n", "return", "[", "total_loss", ",", "task_loss", ",", "d_loss", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDann.compute_gradients": [[739, 746], ["tape.gradient", "tape.gradient"], "methods", ["None"], ["", "def", "compute_gradients", "(", "self", ",", "tape", ",", "losses", ",", "which_model", ")", ":", "\n", "        ", "total_loss", ",", "task_loss", ",", "d_loss", "=", "losses", "\n", "grad", "=", "tape", ".", "gradient", "(", "total_loss", ",", "\n", "self", ".", "model", "[", "which_model", "]", ".", "trainable_variables_task_fe_domain", ")", "\n", "d_grad", "=", "tape", ".", "gradient", "(", "d_loss", ",", "\n", "self", ".", "model", "[", "which_model", "]", ".", "trainable_variables_domain", ")", "\n", "return", "[", "grad", ",", "d_grad", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDann.apply_gradients": [[747, 754], ["[].apply_gradients", "[].apply_gradients", "zip", "zip"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodAflacDG.apply_gradients", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodAflacDG.apply_gradients"], ["", "def", "apply_gradients", "(", "self", ",", "gradients", ",", "which_model", ")", ":", "\n", "        ", "grad", ",", "d_grad", "=", "gradients", "\n", "self", ".", "opt", "[", "which_model", "]", "[", "\"opt\"", "]", ".", "apply_gradients", "(", "zip", "(", "grad", ",", "\n", "self", ".", "model", "[", "which_model", "]", ".", "trainable_variables_task_fe_domain", ")", ")", "\n", "# Update discriminator again", "\n", "self", ".", "opt", "[", "which_model", "]", "[", "\"d_opt\"", "]", ".", "apply_gradients", "(", "zip", "(", "d_grad", ",", "\n", "self", ".", "model", "[", "which_model", "]", ".", "trainable_variables_domain", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDawsBase.__init__": [[759, 767], ["methods.MethodDann.__init__", "methods.MethodDawsBase.compute_p_y"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDawsBase.compute_p_y"], ["def", "__init__", "(", "self", ",", "*", "args", ",", "noise_amount", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "if", "noise_amount", "is", "None", ":", "\n", "            ", "noise_amount", "=", "FLAGS", ".", "ws_noise", "\n", "\n", "", "self", ".", "loss_names", "+=", "[", "\"weak\"", "]", "\n", "self", ".", "compute_p_y", "(", "noise_amount", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDawsBase.compute_p_y": [[768, 840], ["class_balance.class_balance.class_balance", "print", "float", "print", "print", "print", "print", "print", "random.randint", "random.uniform", "sum", "sum", "sum", "print", "len", "abs", "abs", "range", "range", "len", "len"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.class_balance.class_balance"], ["", "def", "compute_p_y", "(", "self", ",", "noise_amount", "=", "None", ")", ":", "\n", "        ", "\"\"\" Compute P(y) (i.e. class balance) of the training target dataset\n\n        Note: we simulate the self-report label proportions from looking at\n        the target training labels (not validation or test sets). However, after\n        this function call, we don't use the labels themselves (outside of\n        computing evaluation accuracy), just the computed proportions for the\n        training.\n        \"\"\"", "\n", "# Compute proportion of each class", "\n", "# Note: we use the \"eval\" train dataset since it doesn't repeat infinitely", "\n", "# and we use \"train\" not test since we don't want to peak at the", "\n", "# validation data we use for model selection.", "\n", "self", ".", "p_y", "=", "class_balance", "(", "self", ".", "target_train_eval_dataset", ",", "self", ".", "num_classes", ")", "\n", "\n", "print", "(", "\"Correct proportions:\"", ",", "self", ".", "p_y", ")", "\n", "before", "=", "self", ".", "p_y", "*", "1.0", "\n", "total_noise", "=", "0", "\n", "\n", "# Add noise", "\n", "if", "noise_amount", "is", "not", "None", "and", "noise_amount", "!=", "0", ":", "\n", "# Make sure it's float", "\n", "            ", "noise_amount", "=", "float", "(", "noise_amount", ")", "\n", "\n", "while", "noise_amount", ">", "0", ":", "\n", "# Randomly pick a class", "\n", "                ", "label", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "self", ".", "p_y", ")", "-", "1", ")", "\n", "\n", "# Randomly pick some amount of our noise budget. Always positive", "\n", "# since otherwise we could do this loop forever.", "\n", "noise", "=", "random", ".", "uniform", "(", "0.000001", ",", "noise_amount", ")", "\n", "\n", "# We skip negatives since afterwards we normalize, so adding", "\n", "# to other classes results in a negative.", "\n", "#", "\n", "# Randomly pick whether to add/subtract this from the class", "\n", "# proportions (otherwise it's always additive noise)", "\n", "# positive = random.choice([True, False])", "\n", "# if not positive:", "\n", "#     noise *= -1", "\n", "\n", "# Update the class proportions", "\n", "new_value", "=", "self", ".", "p_y", "[", "label", "]", "+", "noise", "\n", "\n", "# Noise was too much, so break before we add it and go over.", "\n", "# if noise_amount - noise < 0:", "\n", "#     break", "\n", "\n", "# Skip and try again if we end up with a negative or zero", "\n", "# proportion. Also, make sure we don't add too much noise.", "\n", "if", "new_value", ">", "0", ":", "\n", "                    ", "print", "(", "\"{} {} {} label {}\"", ".", "format", "(", "\n", "\"Adding\"", "if", "noise", ">", "0", "else", "\"Subtracting\"", ",", "\n", "noise", ",", "\n", "\"to\"", "if", "noise", ">", "0", "else", "\"from\"", ",", "\n", "label", ",", "\n", ")", ")", "\n", "self", ".", "p_y", "[", "label", "]", "=", "new_value", "\n", "\n", "# Subtract the amount we used from the noise budget", "\n", "noise_amount", "-=", "noise", "\n", "# Debugging", "\n", "total_noise", "+=", "noise", "\n", "\n", "", "", "print", "(", "\"Noised proportions:\"", ",", "self", ".", "p_y", ")", "\n", "print", "(", "\"Sum difference:\"", ",", "sum", "(", "[", "abs", "(", "self", ".", "p_y", "[", "i", "]", "-", "before", "[", "i", "]", ")", "for", "i", "in", "range", "(", "len", "(", "self", ".", "p_y", ")", ")", "]", ")", ")", "\n", "print", "(", "\"Total noise:\"", ",", "total_noise", ")", "\n", "\n", "# Re-normalize so the sum still is 1", "\n", "self", ".", "p_y", "=", "self", ".", "p_y", "/", "sum", "(", "self", ".", "p_y", ")", "\n", "print", "(", "\"Normalized noised proportions:\"", ",", "self", ".", "p_y", ")", "\n", "print", "(", "\"Sum difference norm:\"", ",", "sum", "(", "[", "abs", "(", "self", ".", "p_y", "[", "i", "]", "-", "before", "[", "i", "]", ")", "for", "i", "in", "range", "(", "len", "(", "self", ".", "p_y", ")", ")", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDawsBase.compute_losses": [[841, 902], ["tensorflow.where", "tensorflow.gather", "tensorflow.gather", "methods.MethodDawsBase.task_loss", "tensorflow.where", "tensorflow.gather", "tensorflow.cast", "tensorflow.keras.losses.KLD", "tensorflow.not_equal", "sum", "len", "tensorflow.equal", "tensorflow.reduce_sum", "tensorflow.shape", "tensorflow.nn.softmax", "methods.MethodDawsBase.domain_loss"], "methods", ["None"], ["", "", "def", "compute_losses", "(", "self", ",", "xs", ",", "task_y_true", ",", "domain_y_true", ",", "task_y_pred", ",", "\n", "domain_y_pred", ",", "fe_output", ",", "contrastive_output", ",", "auxiliary_data", ",", "\n", "which_model", ",", "training", ")", ":", "\n", "# DANN losses", "\n", "        ", "nontarget", "=", "tf", ".", "where", "(", "tf", ".", "not_equal", "(", "domain_y_true", ",", "0", ")", ")", "\n", "task_y_true_nontarget", "=", "tf", ".", "gather", "(", "task_y_true", ",", "nontarget", ",", "axis", "=", "0", ")", "\n", "task_y_pred_nontarget", "=", "tf", ".", "gather", "(", "task_y_pred", ",", "nontarget", ",", "axis", "=", "0", ")", "\n", "\n", "task_loss", "=", "self", ".", "task_loss", "(", "task_y_true_nontarget", ",", "task_y_pred_nontarget", ")", "\n", "d_loss", "=", "sum", "(", "[", "\n", "self", ".", "domain_loss", "(", "domain_y_true", ",", "d", ")", "\n", "for", "d", "in", "domain_y_pred", "\n", "]", ")", "/", "len", "(", "domain_y_pred", ")", "\n", "\n", "# DA-WS regularizer", "\n", "#", "\n", "# Get predicted target-domain labels. We ignore label proportions for", "\n", "# the source domains since we train to predict the correct labels there.", "\n", "# We don't know the target-domain labels, so instead we try using this", "\n", "# additional P(y) label proportion information. Thus, we use it and the", "\n", "# adversarial domain-invariant FE objectives as sort of auxiliary", "\n", "# losses.", "\n", "target", "=", "tf", ".", "where", "(", "tf", ".", "equal", "(", "domain_y_true", ",", "0", ")", ")", "\n", "task_y_pred_target", "=", "tf", ".", "gather", "(", "task_y_pred", ",", "target", ",", "axis", "=", "0", ")", "\n", "\n", "# Idea:", "\n", "# argmax, one-hot, reduce_sum(..., axis=1), /= batch_size, KL with p_y", "\n", "# However, argmax yields essentially useless gradients (as far as I", "\n", "# understand it, e.g. we use cross entropy loss for classification not", "\n", "# the actual 0-1 loss or loss on the argmax of the softmax outputs)", "\n", "#", "\n", "# Thus, a soft version. Idea: softmax each, reduce sum vertically,", "\n", "#   /= batch_size, then KL", "\n", "# This is different than per-example-in-batch KLD because we average", "\n", "# over the softmax outputs across the batch before KLD. So, the", "\n", "# difference is whether averaging before or after KLD.", "\n", "#", "\n", "# Note: this depends on a large enough batch size. If you can't set it", "\n", "# >=64 or so (like what we use in SS-DA for the target data, i.e. half", "\n", "# the 128 batch size), then accumulate this gradient over multiple steps", "\n", "# and then apply.", "\n", "#", "\n", "# cast batch_size to float otherwise:", "\n", "# \"x and y must have the same dtype, got tf.float32 != tf.int32\"", "\n", "batch_size", "=", "tf", ".", "cast", "(", "tf", ".", "shape", "(", "task_y_pred_target", ")", "[", "0", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "p_y_batch", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "nn", ".", "softmax", "(", "task_y_pred_target", ")", ",", "axis", "=", "0", ")", "/", "batch_size", "\n", "daws_loss", "=", "tf", ".", "keras", ".", "losses", ".", "KLD", "(", "self", ".", "p_y", ",", "p_y_batch", ")", "\n", "\n", "# Sum up individual losses for the total", "\n", "#", "\n", "# Note: daws_loss doesn't have the DANN learning rate schedule because", "\n", "# it goes with the task_loss. We want to learn predictions for the task", "\n", "# classifier that both correctly predicts labels on the source data and", "\n", "# on the target data aligns with the correct label proportions.", "\n", "# Separately, we want the FE representation to also be domain invariant,", "\n", "# which we apply the learning rate schedule to, I think, to help the", "\n", "# adversarial part converge properly (recall GAN training instability", "\n", "# stuff).", "\n", "total_loss", "=", "task_loss", "+", "d_loss", "+", "daws_loss", "\n", "\n", "return", "[", "total_loss", ",", "task_loss", ",", "d_loss", ",", "daws_loss", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDawsBase.compute_gradients": [[903, 907], ["methods.MethodDann.compute_gradients"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodAflacDG.compute_gradients"], ["", "def", "compute_gradients", "(", "self", ",", "tape", ",", "losses", ",", "which_model", ")", ":", "\n", "# We only use daws_loss for plotting -- for computing gradients it's", "\n", "# included in the total loss", "\n", "        ", "return", "super", "(", ")", ".", "compute_gradients", "(", "tape", ",", "losses", "[", ":", "-", "1", "]", ",", "which_model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaBase.__init__": [[923, 942], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["def", "__init__", "(", "self", ",", "*", "args", ",", "num_contrastive_units", "=", "None", ",", "\n", "pseudo_label_target", "=", "False", ",", "hard", "=", "False", ",", "\n", "in_domain", "=", "False", ",", "any_domain", "=", "False", ",", "\n", "domain_generalization", "=", "False", ",", "weight_adversary", "=", "1", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "weight_adversary", "=", "weight_adversary", "\n", "self", ".", "weight_similarity", "=", "FLAGS", ".", "similarity_weight", "\n", "self", ".", "pseudo_label_target", "=", "pseudo_label_target", "\n", "self", ".", "hard", "=", "hard", "\n", "self", ".", "in_domain", "=", "in_domain", "\n", "self", ".", "any_domain", "=", "any_domain", "\n", "self", ".", "domain_generalization", "=", "domain_generalization", "\n", "\n", "if", "num_contrastive_units", "is", "None", ":", "\n", "            ", "self", ".", "num_contrastive_units", "=", "FLAGS", ".", "contrastive_units", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_contrastive_units", "=", "num_contrastive_units", "\n", "\n", "", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "loss_names", "+=", "[", "\"similarity\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaBase.create_losses": [[943, 950], ["super().create_losses", "methods.make_loss", "methods.make_loss"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaBase.create_losses", "home.repos.pwc.inspect_result.floft_calda.None.methods.make_loss", "home.repos.pwc.inspect_result.floft_calda.None.methods.make_loss"], ["", "def", "create_losses", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "create_losses", "(", ")", "\n", "# Used in non-compiled version", "\n", "self", ".", "cl_crossentropy_loss", "=", "make_loss", "(", ")", "\n", "# Used in hard pos/neg", "\n", "self", ".", "task_no_reduction_loss", "=", "make_loss", "(", "\n", "reduction", "=", "tf", ".", "keras", ".", "losses", ".", "Reduction", ".", "NONE", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaBase.create_model": [[951, 960], ["models.DannModel", "len"], "methods", ["None"], ["", "def", "create_model", "(", "self", ",", "model_name", ")", ":", "\n", "# Not multi-modal at the moment", "\n", "        ", "assert", "len", "(", "self", ".", "shared_modalities", ")", "==", "1", ",", "\"only supports one shared modality between domains\"", "\n", "\n", "return", "models", ".", "DannModel", "(", "self", ".", "num_classes", ",", "self", ".", "domain_outputs", ",", "\n", "self", ".", "global_step", ",", "self", ".", "total_steps", ",", "\n", "num_contrastive_units", "=", "self", ".", "num_contrastive_units", ",", "\n", "model_name", "=", "model_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaBase._cartesian_product": [[961, 972], ["tensorflow.tile", "tensorflow.expand_dims", "tensorflow.tile", "tensorflow.expand_dims", "tensorflow.concat", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.shape", "tensorflow.shape"], "methods", ["None"], ["", "def", "_cartesian_product", "(", "self", ",", "a", ",", "b", ")", ":", "\n", "        ", "\"\"\"\n        Assumes a and b are 1D\n        https://stackoverflow.com/a/47133461\n        \"\"\"", "\n", "tile_a", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "a", ",", "1", ")", ",", "[", "1", ",", "tf", ".", "shape", "(", "b", ")", "[", "0", "]", "]", ")", "\n", "tile_a", "=", "tf", ".", "expand_dims", "(", "tile_a", ",", "2", ")", "\n", "tile_b", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "b", ",", "0", ")", ",", "[", "tf", ".", "shape", "(", "a", ")", "[", "0", "]", ",", "1", "]", ")", "\n", "tile_b", "=", "tf", ".", "expand_dims", "(", "tile_b", ",", "2", ")", "\n", "cartesian_product", "=", "tf", ".", "concat", "(", "[", "tile_a", ",", "tile_b", "]", ",", "axis", "=", "2", ")", "\n", "return", "cartesian_product", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaBase._contrastive_get_examples": [[973, 1039], ["tensorflow.gather", "tensorflow.gather", "tensorflow.gather", "tensorflow.gather", "tensorflow.squeeze", "tensorflow.squeeze", "tensorflow.squeeze", "tensorflow.squeeze", "tensorflow.expand_dims", "tensorflow.where", "tensorflow.where", "tensorflow.gather", "tensorflow.cast", "tensorflow.gather", "tensorflow.gather", "tensorflow.gather", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "tensorflow.shape", "tensorflow.range", "tensorflow.not_equal", "tensorflow.equal", "tensorflow.argmax", "tensorflow.nn.softmax", "tensorflow.shape"], "methods", ["None"], ["", "def", "_contrastive_get_examples", "(", "self", ",", "task_y_true", ",", "domain_y_true", ",", "\n", "z_output", ",", "task_y_pred", ")", ":", "\n", "        ", "\"\"\"\n        Get y, d, z, and pred used in contrastive learning\n\n        Take the true domain and class labels and model outputs and get the\n        source-only or if pseudo labeling the source + pseudo-labeled target\n        \"\"\"", "\n", "# For domain generalization, d=0 is the first source domain not the", "\n", "# target domain since in domain generalization we don't have target", "\n", "# domain data. Thus, just use everything, but do range here so we don't", "\n", "# have to rewrite the rest of the code.", "\n", "if", "self", ".", "domain_generalization", ":", "\n", "            ", "nontarget", "=", "tf", ".", "expand_dims", "(", "tf", ".", "range", "(", "0", ",", "tf", ".", "shape", "(", "domain_y_true", ")", "[", "0", "]", ")", ",", "axis", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "# Anchors = the full batch, excluding the target data", "\n", "            ", "nontarget", "=", "tf", ".", "where", "(", "tf", ".", "not_equal", "(", "domain_y_true", ",", "0", ")", ")", "\n", "\n", "", "y", "=", "tf", ".", "gather", "(", "task_y_true", ",", "nontarget", ",", "axis", "=", "0", ")", "\n", "d", "=", "tf", ".", "gather", "(", "domain_y_true", ",", "nontarget", ",", "axis", "=", "0", ")", "\n", "z", "=", "tf", ".", "gather", "(", "z_output", ",", "nontarget", ",", "axis", "=", "0", ")", "\n", "pred", "=", "tf", ".", "gather", "(", "task_y_pred", ",", "nontarget", ",", "axis", "=", "0", ")", "\n", "\n", "# If we include the target domain, since we don't know the true labels,", "\n", "# we instead take the current task classifier predictions for the target", "\n", "# data as the true labels, i.e. pseudo labeling the target data. Then", "\n", "# concatenate with the non-target y/d/z we generated above.", "\n", "if", "self", ".", "pseudo_label_target", ":", "\n", "            ", "assert", "not", "self", ".", "domain_generalization", ",", "\"there's no target data to pseudo label in CALDG\"", "\n", "\n", "# Anchors = the full batch, excluding the target data", "\n", "target", "=", "tf", ".", "where", "(", "tf", ".", "equal", "(", "domain_y_true", ",", "0", ")", ")", "\n", "\n", "# Get y from task_y_pred rather than task_y_true", "\n", "target_y", "=", "tf", ".", "gather", "(", "task_y_pred", ",", "target", ",", "axis", "=", "0", ")", "\n", "# The true y's are sparse, so get class label integer from the", "\n", "# raw logits", "\n", "target_y", "=", "tf", ".", "cast", "(", "\n", "tf", ".", "argmax", "(", "tf", ".", "nn", ".", "softmax", "(", "target_y", ")", ",", "axis", "=", "-", "1", ")", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "# d and z as usual", "\n", "target_d", "=", "tf", ".", "gather", "(", "domain_y_true", ",", "target", ",", "axis", "=", "0", ")", "\n", "target_z", "=", "tf", ".", "gather", "(", "z_output", ",", "target", ",", "axis", "=", "0", ")", "\n", "target_pred", "=", "tf", ".", "gather", "(", "task_y_pred", ",", "target", ",", "axis", "=", "0", ")", "\n", "\n", "# Concatenate everything", "\n", "y", "=", "tf", ".", "concat", "(", "[", "y", ",", "target_y", "]", ",", "axis", "=", "0", ")", "\n", "d", "=", "tf", ".", "concat", "(", "[", "d", ",", "target_d", "]", ",", "axis", "=", "0", ")", "\n", "z", "=", "tf", ".", "concat", "(", "[", "z", ",", "target_z", "]", ",", "axis", "=", "0", ")", "\n", "pred", "=", "tf", ".", "concat", "(", "[", "pred", ",", "target_pred", "]", ",", "axis", "=", "0", ")", "\n", "\n", "# Get rid of extra \"1\" dimensions, otherwise these are of shapes", "\n", "# [num_total, 1], [num_total, 1], and", "\n", "# [num_total, 1, contrastive_units] respectively.", "\n", "#", "\n", "# Note: wrote code/examples without this extra dimension, so removing", "\n", "# to avoid rewriting everything.", "\n", "", "y", "=", "tf", ".", "squeeze", "(", "y", ",", "axis", "=", "1", ")", "\n", "d", "=", "tf", ".", "squeeze", "(", "d", ",", "axis", "=", "1", ")", "\n", "z", "=", "tf", ".", "squeeze", "(", "z", ",", "axis", "=", "1", ")", "\n", "pred", "=", "tf", ".", "squeeze", "(", "pred", ",", "axis", "=", "1", ")", "\n", "\n", "# Compute after we possibly append the target data", "\n", "num_total", "=", "tf", ".", "shape", "(", "y", ")", "[", "0", "]", "\n", "\n", "return", "y", ",", "d", ",", "z", ",", "pred", ",", "num_total", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaBase._contrastive_get_indices": [[1040, 1113], ["methods.MethodCaldaBase._cartesian_product", "tensorflow.gather_nd", "tensorflow.gather_nd", "tensorflow.gather_nd", "tensorflow.gather_nd", "tensorflow.logical_not", "tensorflow.gather_nd", "tensorflow.gather_nd", "tensorflow.range", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.cast", "tensorflow.logical_and", "tensorflow.logical_and", "tensorflow.where", "tensorflow.where", "tensorflow.gather_nd", "tensorflow.logical_and", "tensorflow.equal", "tensorflow.not_equal", "tensorflow.logical_and", "tensorflow.not_equal", "tensorflow.logical_and", "tensorflow.logical_and", "tensorflow.not_equal", "tensorflow.not_equal", "tensorflow.eye", "tensorflow.expand_dims", "tensorflow.equal", "tensorflow.equal", "tensorflow.equal", "tensorflow.not_equal", "tensorflow.equal", "tensorflow.not_equal", "tensorflow.not_equal", "tensorflow.cast", "tensorflow.cast"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaBase._cartesian_product"], ["", "def", "_contrastive_get_indices", "(", "self", ",", "y", ",", "d", ",", "anchors", ",", "num_total", ")", ":", "\n", "        ", "\"\"\"\n        Get the anchor, positives, and negatives indices into the y/d arrays\n        based on whether we want in/any/cross domain pairs for contrastive\n        learning\n        \"\"\"", "\n", "# Positive/negative masks", "\n", "prod_indices", "=", "self", ".", "_cartesian_product", "(", "anchors", ",", "tf", ".", "range", "(", "0", ",", "num_total", ")", ")", "\n", "# Note while these are called \"all\", if no pseudo-labeling then they", "\n", "# still exclude the target domain...", "\n", "anchor_d", "=", "tf", ".", "gather_nd", "(", "d", ",", "tf", ".", "expand_dims", "(", "prod_indices", "[", ":", ",", ":", ",", "0", "]", ",", "axis", "=", "-", "1", ")", ")", "\n", "all_d", "=", "tf", ".", "gather_nd", "(", "d", ",", "tf", ".", "expand_dims", "(", "prod_indices", "[", ":", ",", ":", ",", "1", "]", ",", "axis", "=", "-", "1", ")", ")", "\n", "anchor_y", "=", "tf", ".", "gather_nd", "(", "y", ",", "tf", ".", "expand_dims", "(", "prod_indices", "[", ":", ",", ":", ",", "0", "]", ",", "axis", "=", "-", "1", ")", ")", "\n", "all_y", "=", "tf", ".", "gather_nd", "(", "y", ",", "tf", ".", "expand_dims", "(", "prod_indices", "[", ":", ",", ":", ",", "1", "]", ",", "axis", "=", "-", "1", ")", ")", "\n", "# Whether the column corresponds to the anchor", "\n", "is_not_anchor", "=", "tf", ".", "logical_not", "(", "\n", "tf", ".", "cast", "(", "\n", "tf", ".", "gather_nd", "(", "tf", ".", "eye", "(", "num_total", ")", ",", "tf", ".", "expand_dims", "(", "anchors", ",", "axis", "=", "-", "1", ")", ")", ",", "\n", "dtype", "=", "tf", ".", "bool", "\n", ")", "\n", ")", "\n", "\n", "# Within-source label-contrastive learning", "\n", "#", "\n", "# Note: we need to exclude the anchor in the positives in this case. Not", "\n", "# the negatives though since it has a different label so is guaranteed", "\n", "# to not be the anchor.", "\n", "if", "self", ".", "in_domain", ":", "\n", "# Same domain, same label, not anchor", "\n", "            ", "positives", "=", "tf", ".", "logical_and", "(", "\n", "tf", ".", "logical_and", "(", "tf", ".", "equal", "(", "anchor_d", ",", "all_d", ")", ",", "tf", ".", "equal", "(", "anchor_y", ",", "all_y", ")", ")", ",", "\n", "is_not_anchor", "\n", ")", "\n", "# Same domain, different label", "\n", "negatives", "=", "tf", ".", "logical_and", "(", "\n", "tf", ".", "equal", "(", "anchor_d", ",", "all_d", ")", ",", "tf", ".", "not_equal", "(", "anchor_y", ",", "all_y", ")", "\n", ")", "\n", "# Label-contrastive learning (all sources grouped/lumped together)", "\n", "", "elif", "self", ".", "any_domain", ":", "\n", "# Any domain, same label, not anchor", "\n", "            ", "positives", "=", "tf", ".", "logical_and", "(", "tf", ".", "equal", "(", "anchor_y", ",", "all_y", ")", ",", "is_not_anchor", ")", "\n", "# Any domain, different label", "\n", "negatives", "=", "tf", ".", "not_equal", "(", "anchor_y", ",", "all_y", ")", "\n", "# Cross-source label-contrastive learning", "\n", "", "else", ":", "\n", "# Different domain, same label", "\n", "            ", "positives", "=", "tf", ".", "logical_and", "(", "\n", "tf", ".", "not_equal", "(", "anchor_d", ",", "all_d", ")", ",", "tf", ".", "equal", "(", "anchor_y", ",", "all_y", ")", "\n", ")", "\n", "# Different domain, different label", "\n", "negatives", "=", "tf", ".", "logical_and", "(", "\n", "tf", ".", "not_equal", "(", "anchor_d", ",", "all_d", ")", ",", "tf", ".", "not_equal", "(", "anchor_y", ",", "all_y", ")", "\n", ")", "\n", "\n", "# Positive/negative indices (w.r.t. z)", "\n", "#", "\n", "# Get indices of values with non-zero in mask, i.e. the real", "\n", "# positives/negatives. The tf.where returns the index into the mask", "\n", "# which since we take a subset isn't the index into z.", "\n", "", "positives_similarity_indices", "=", "tf", ".", "gather_nd", "(", "\n", "prod_indices", ",", "tf", ".", "where", "(", "tf", ".", "not_equal", "(", "tf", ".", "cast", "(", "positives", ",", "tf", ".", "int32", ")", ",", "0", ")", ")", "\n", ")", "\n", "negatives_similarity_indices", "=", "tf", ".", "gather_nd", "(", "\n", "prod_indices", ",", "tf", ".", "where", "(", "tf", ".", "not_equal", "(", "tf", ".", "cast", "(", "negatives", ",", "tf", ".", "int32", ")", ",", "0", ")", ")", "\n", ")", "\n", "\n", "# Also get the anchor indices", "\n", "anchors_indices", "=", "positives_similarity_indices", "[", ":", ",", "0", "]", "\n", "\n", "return", "(", "\n", "anchors_indices", ",", "\n", "positives_similarity_indices", ",", "\n", "negatives_similarity_indices", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaBase._contrastive_sampling": [[1115, 1223], ["methods.MethodCaldaBase.task_no_reduction_loss", "tensorflow.gather", "tensorflow.argsort", "tensorflow.gather", "tensorflow.gather", "tensorflow.gather_nd", "tensorflow.gather_nd", "methods.MethodCaldaBase.task_no_reduction_loss", "tensorflow.argsort", "tensorflow.gather", "tensorflow.shape", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.gather", "tensorflow.gather", "tensorflow.assert_equal", "tensorflow.random.shuffle", "tensorflow.random.shuffle", "tensorflow.range", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape"], "methods", ["None"], ["", "def", "_contrastive_sampling", "(", "self", ",", "y", ",", "pred", ",", "anchors_indices", ",", "\n", "positives_similarity_indices", ",", "negatives_similarity_indices", ",", "\n", "num_anchors", ")", ":", "\n", "        ", "\"\"\"\n        Subset of positives/negatives - random sampling or hard sampling\n        \"\"\"", "\n", "# These won't hold for each anchor individually like the non-vectorized", "\n", "# version. Instead, it's easier (and maybe better anyway?) to on average", "\n", "# have no more than max_positives positives, etc.", "\n", "if", "FLAGS", ".", "max_positives", "!=", "0", ":", "\n", "            ", "max_total_positives", "=", "num_anchors", "*", "FLAGS", ".", "max_positives", "\n", "", "else", ":", "\n", "            ", "max_total_positives", "=", "None", "\n", "", "if", "FLAGS", ".", "max_negatives", "!=", "0", ":", "\n", "            ", "max_total_negatives", "=", "num_anchors", "*", "FLAGS", ".", "max_negatives", "\n", "", "else", ":", "\n", "            ", "max_total_negatives", "=", "None", "\n", "\n", "# If desired, pick the hard positives/negatives (in terms of task loss)", "\n", "# rather than randomly", "\n", "", "if", "self", ".", "hard", ":", "\n", "            ", "assert", "FLAGS", ".", "max_positives", "!=", "0", "or", "FLAGS", ".", "max_negatives", "!=", "0", ",", "\"hard only makes sense if there's a limit on the number\"", "\"of positives or negatives. Otherwise, all of them are used, so\"", "\"sorting by hardness doesn't change anything.\"", "\n", "\n", "# Same as normal task loss, but this doesn't average over the batch.", "\n", "# Thus, we can get the per-example loss. Shape: [batch_size]", "\n", "hardness", "=", "self", ".", "task_no_reduction_loss", "(", "y", ",", "pred", ")", "\n", "\n", "# Get hardness of positives (note [:,0] is the anchor)", "\n", "pos_hardness", "=", "tf", ".", "gather", "(", "hardness", ",", "positives_similarity_indices", "[", ":", ",", "1", "]", ")", "\n", "\n", "# Indices ordered by hardness, with max pos/neg", "\n", "pos_hardsort", "=", "tf", ".", "argsort", "(", "pos_hardness", ",", "axis", "=", "-", "1", ",", "direction", "=", "'DESCENDING'", ",", "stable", "=", "False", ")", "\n", "\n", "if", "FLAGS", ".", "max_positives", "!=", "0", ":", "\n", "                ", "pos_hardsort", "=", "pos_hardsort", "[", ":", "max_total_positives", "]", "\n", "\n", "# Get the anchors/positives", "\n", "", "positives_similarity_indices", "=", "tf", ".", "gather", "(", "positives_similarity_indices", ",", "pos_hardsort", ")", "\n", "anchors_indices", "=", "tf", ".", "gather", "(", "anchors_indices", ",", "pos_hardsort", ")", "\n", "\n", "# How we define \"hard\" for negatives", "\n", "#", "\n", "# There's only one way for a positive to be right (disregarding", "\n", "# uncertainty of the softmax output). Thus, if the task loss is", "\n", "# high then the representation is (probably) different when it", "\n", "# should be similar.", "\n", "#", "\n", "# However, there's many ways to be wrong for the negatives. For", "\n", "# example, if the anchor's label is 0, one negative may have", "\n", "# high task loss if it should be label 1 but was 0 or if it", "\n", "# should be label 1 but was 2. The first case is primarily what", "\n", "# we're interested in: push apart the negatives that are", "\n", "# currently similar but should really be different.", "\n", "#", "\n", "# We do this via:", "\n", "# - Select the negatives", "\n", "# - Compute task loss, where true = anchor's label", "\n", "# - Take the *lowest* loss (rather than highest before): the", "\n", "#   examples that the classifier (incorrectly) most thinks are", "\n", "#   the same class as the anchor (probably currently similar in", "\n", "#   the representation space). We'll push those apart.", "\n", "\n", "# Select negatives", "\n", "negative_anchor_labels", "=", "tf", ".", "gather_nd", "(", "y", ",", "tf", ".", "expand_dims", "(", "negatives_similarity_indices", "[", ":", ",", "0", "]", ",", "axis", "=", "-", "1", ")", ")", "\n", "negative_predicted_labels", "=", "tf", ".", "gather_nd", "(", "pred", ",", "tf", ".", "expand_dims", "(", "negatives_similarity_indices", "[", ":", ",", "1", "]", ",", "axis", "=", "-", "1", ")", ")", "\n", "# Compute task loss - note the negative doesn't actually have", "\n", "# the anchor's label (\"true\" label) here", "\n", "neg_hardness", "=", "self", ".", "task_no_reduction_loss", "(", "negative_anchor_labels", ",", "negative_predicted_labels", ")", "\n", "# Sort ascending (not descending like for the positives)", "\n", "neg_hardsort", "=", "tf", ".", "argsort", "(", "neg_hardness", ",", "axis", "=", "-", "1", ",", "direction", "=", "'ASCENDING'", ",", "stable", "=", "False", ")", "\n", "\n", "# Subset negatives from the above definition of \"hard\"", "\n", "if", "FLAGS", ".", "max_negatives", "!=", "0", ":", "\n", "                ", "neg_hardsort", "=", "neg_hardsort", "[", ":", "max_total_negatives", "]", "\n", "\n", "", "negatives_similarity_indices", "=", "tf", ".", "gather", "(", "negatives_similarity_indices", ",", "neg_hardsort", ")", "\n", "", "else", ":", "\n", "            ", "if", "FLAGS", ".", "max_positives", "!=", "0", ":", "\n", "# Need to shuffle both the anchor indices and positive similarity", "\n", "# indices together, so shuffle the rows/indices then gather those to get", "\n", "# the shuffled results.", "\n", "                ", "shuffled_subset", "=", "tf", ".", "random", ".", "shuffle", "(", "\n", "tf", ".", "range", "(", "0", ",", "tf", ".", "shape", "(", "positives_similarity_indices", ")", "[", "0", "]", ")", "\n", ")", "[", ":", "max_total_positives", "]", "\n", "positives_similarity_indices", "=", "tf", ".", "gather", "(", "positives_similarity_indices", ",", "shuffled_subset", ")", "\n", "anchors_indices", "=", "tf", ".", "gather", "(", "anchors_indices", ",", "shuffled_subset", ")", "\n", "tf", ".", "assert_equal", "(", "tf", ".", "shape", "(", "anchors_indices", ")", "[", "0", "]", ",", "tf", ".", "shape", "(", "positives_similarity_indices", ")", "[", "0", "]", ")", "\n", "\n", "", "if", "FLAGS", ".", "max_negatives", "!=", "0", ":", "\n", "                ", "negatives_similarity_indices", "=", "tf", ".", "random", ".", "shuffle", "(", "negatives_similarity_indices", ")", "[", ":", "max_total_negatives", "]", "\n", "\n", "# Compute number of positives after we take the subset", "\n", "#", "\n", "# Note: includes dependency on number of anchors so we will only", "\n", "# normalize by this, not by num_anchors as well", "\n", "", "", "num_positives", "=", "tf", ".", "shape", "(", "positives_similarity_indices", ")", "[", "0", "]", "\n", "# num_negatives = tf.shape(negatives_similarity_indices)[0]", "\n", "# tf.print(\"Num positives\", num_positives)", "\n", "# tf.print(\"Num negatives\", num_negatives)", "\n", "\n", "return", "(", "\n", "anchors_indices", ",", "\n", "positives_similarity_indices", ",", "\n", "negatives_similarity_indices", ",", "\n", "num_positives", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaBase._cosine_similarity_from_indices": [[1225, 1244], ["tensorflow.gather_nd", "tensorflow.math.l2_normalize", "tensorflow.reduce_sum", "tensorflow.expand_dims", "tensorflow.reduce_prod"], "methods", ["None"], ["", "def", "_cosine_similarity_from_indices", "(", "self", ",", "vectors", ",", "indices", ")", ":", "\n", "        ", "\"\"\"\n        Compute cosine similarity between the desired pairs of\n        (anchor, positive/negative) z vectors.\n        \"\"\"", "\n", "# Using positives as an example...", "\n", "#", "\n", "# For each (anchor, positive) index pair, replace the index with the", "\n", "# actual z value. We get a tensor of shape", "\n", "# [num_positives, 2, contrastive_units]", "\n", "vectors", "=", "tf", ".", "gather_nd", "(", "vectors", ",", "tf", ".", "expand_dims", "(", "indices", ",", "axis", "=", "-", "1", ")", ")", "\n", "\n", "# Normalize, i.e. cosine similarity", "\n", "vectors", "=", "tf", ".", "math", ".", "l2_normalize", "(", "vectors", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# Compute cosine similarity across pairs. Multiply across the", "\n", "# second dimension (the pairs), sum across the components (last", "\n", "# dimension).", "\n", "return", "tf", ".", "reduce_sum", "(", "tf", ".", "reduce_prod", "(", "vectors", ",", "axis", "=", "1", ")", ",", "axis", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaBase._contrastive_infonce": [[1245, 1301], ["tensorflow.scatter_nd", "tensorflow.scatter_nd", "tensorflow.gather", "tensorflow.gather", "tensorflow.math.exp", "tensorflow.multiply", "tensorflow.reduce_sum", "tensorflow.print", "tensorflow.print", "tensorflow.print", "tensorflow.print", "tensorflow.print", "tensorflow.ones_like", "tensorflow.print", "tensorflow.print", "tensorflow.print", "tensorflow.math.exp", "tensorflow.print", "tensorflow.print", "tensorflow.print", "tensorflow.reduce_sum", "tensorflow.math.log", "tensorflow.cast", "tensorflow.reduce_sum"], "methods", ["None"], ["", "def", "_contrastive_infonce", "(", "self", ",", "positives_similarity", ",", "negatives_similarity", ",", "\n", "anchors_indices", ",", "positives_similarity_indices", ",", "\n", "negatives_similarity_indices", ",", "\n", "num_total", ",", "num_positives", ",", "training", ",", "debug", "=", "False", ")", ":", "\n", "# Temperature", "\n", "        ", "tau", "=", "FLAGS", ".", "temperature", "\n", "\n", "if", "debug", ":", "\n", "            ", "tf", ".", "print", "(", "\"negatives_similarity_indices\"", ",", "negatives_similarity_indices", ".", "shape", ")", "\n", "tf", ".", "print", "(", "\"negatives_similarity\"", ",", "negatives_similarity", ".", "shape", ")", "\n", "tf", ".", "print", "(", "\"num_total\"", ",", "num_total", ")", "\n", "tf", ".", "print", "(", "\"positives_similarity_indices\"", ",", "positives_similarity_indices", ".", "shape", ")", "\n", "tf", ".", "print", "(", "\"positives_similarity\"", ",", "positives_similarity", ".", "shape", ")", "\n", "\n", "#", "\n", "# Cross entropy loss", "\n", "#", "\n", "# Put back into the full matrix (since the indices are referenced to", "\n", "# z) and then take the subset of anchors we're looking at.", "\n", "#", "\n", "# Also generate mask to use in cross entropy loss since it's possible", "\n", "# the cosine similarity is actually zero, so we can't just check if = 0.", "\n", "", "negatives_square", "=", "tf", ".", "scatter_nd", "(", "negatives_similarity_indices", ",", "\n", "negatives_similarity", ",", "[", "num_total", ",", "num_total", "]", ")", "\n", "negatives_square_mask", "=", "tf", ".", "scatter_nd", "(", "negatives_similarity_indices", ",", "\n", "tf", ".", "ones_like", "(", "negatives_similarity", ")", ",", "[", "num_total", ",", "num_total", "]", ")", "\n", "negatives_for_anchors", "=", "tf", ".", "gather", "(", "negatives_square", ",", "anchors_indices", ")", "\n", "negatives_for_anchors_mask", "=", "tf", ".", "gather", "(", "negatives_square_mask", ",", "anchors_indices", ")", "\n", "\n", "if", "debug", ":", "\n", "            ", "tf", ".", "print", "(", "\"anchors_indices\"", ",", "anchors_indices", ".", "shape", ",", "anchors_indices", ")", "\n", "tf", ".", "print", "(", "\"negatives_square\"", ",", "negatives_square", ".", "shape", ")", "\n", "tf", ".", "print", "(", "\"negatives_for_anchors\"", ",", "negatives_for_anchors", ".", "shape", ")", "\n", "\n", "# We're not using the built-in cross entropy loss since we have to", "\n", "# handle the mask. Thus, we don't actually need to concatenate since", "\n", "# we know the true/correct value was positives_similarity.", "\n", "", "ce_positive", "=", "tf", ".", "math", ".", "exp", "(", "positives_similarity", "/", "tau", ")", "\n", "# Negatives with the mask, otherwise e^0 = 1, i.e. affects loss", "\n", "ce_negatives", "=", "tf", ".", "multiply", "(", "tf", ".", "math", ".", "exp", "(", "negatives_for_anchors", "/", "tau", ")", ",", "negatives_for_anchors_mask", ")", "\n", "\n", "if", "debug", ":", "\n", "            ", "tf", ".", "print", "(", "\"Positives:\"", ",", "ce_positive", ".", "shape", ")", "\n", "tf", ".", "print", "(", "\"Negatives:\"", ",", "ce_negatives", ".", "shape", ")", "\n", "tf", ".", "print", "(", "\"Negatives reduce sum:\"", ",", "tf", ".", "reduce_sum", "(", "ce_negatives", ",", "axis", "=", "-", "1", ")", ".", "shape", ")", "\n", "\n", "", "denominator", "=", "ce_positive", "+", "tf", ".", "reduce_sum", "(", "ce_negatives", ",", "axis", "=", "-", "1", ")", "\n", "cross_entropy_loss_from_logits", "=", "-", "tf", ".", "math", ".", "log", "(", "ce_positive", "/", "denominator", ")", "\n", "\n", "# Sum over all anchors", "\n", "similarity_loss", "=", "tf", ".", "reduce_sum", "(", "cross_entropy_loss_from_logits", ")", "\n", "\n", "# Normalize", "\n", "similarity_loss", "=", "similarity_loss", "/", "tf", ".", "cast", "(", "num_positives", ",", "tf", ".", "float32", ")", "\n", "\n", "return", "similarity_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaBase._contrastive_loss": [[1302, 1360], ["methods.MethodCaldaBase._contrastive_get_examples", "tensorflow.range", "methods.MethodCaldaBase._contrastive_get_indices", "methods.MethodCaldaBase._contrastive_sampling", "methods.MethodCaldaBase._cosine_similarity_from_indices", "methods.MethodCaldaBase._cosine_similarity_from_indices", "methods.MethodCaldaBase._contrastive_infonce", "tensorflow.shape", "tensorflow.random.shuffle"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaBase._contrastive_get_examples", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaBase._contrastive_get_indices", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaBase._contrastive_sampling", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaBase._cosine_similarity_from_indices", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaBase._cosine_similarity_from_indices", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaBase._contrastive_infonce"], ["", "def", "_contrastive_loss", "(", "self", ",", "task_y_true", ",", "domain_y_true", ",", "\n", "z_output", ",", "task_y_pred", ",", "auxiliary_data", ",", "training", ")", ":", "\n", "        ", "\"\"\" Cross-source similarity loss, based on InfoNCE\n\n        For pseudo code and Python examples, see contrastive_loss.md\n        \"\"\"", "\n", "# Get the label, domain, embedding, etc. arrays we'll use. We use only", "\n", "# the source domains unless pseudo labeling, in which case we add the", "\n", "# pseudo-labeled target domain data as well.", "\n", "y", ",", "d", ",", "z", ",", "pred", ",", "num_total", "=", "self", ".", "_contrastive_get_examples", "(", "task_y_true", ",", "\n", "domain_y_true", ",", "z_output", ",", "task_y_pred", ")", "\n", "\n", "# Since we have rearranged y/d/z and they no longer (necessarily)", "\n", "# correspond to the data in task_y_true, domain_y_true, ... we can just", "\n", "# make the anchors the indices into the entries of y/d/z", "\n", "anchors", "=", "tf", ".", "range", "(", "0", ",", "num_total", ")", "\n", "num_anchors", "=", "tf", ".", "shape", "(", "anchors", ")", "[", "0", "]", "\n", "\n", "# Limit number of anchors so this doesn't take forever, since", "\n", "# O(num_anchors^2). However, don't limit the positives/negatives to just", "\n", "# the anchor examples. Those can come from anywhere in the minibatch.", "\n", "if", "FLAGS", ".", "max_anchors", "!=", "0", ":", "\n", "            ", "anchors", "=", "tf", ".", "random", ".", "shuffle", "(", "anchors", ")", "[", ":", "FLAGS", ".", "max_anchors", "]", "\n", "\n", "# Get anchor, positive, and negative indices, which are one of", "\n", "# in/any/cross domain", "\n", "", "(", "\n", "anchors_indices", ",", "\n", "positives_similarity_indices", ",", "\n", "negatives_similarity_indices", ",", "\n", ")", "=", "self", ".", "_contrastive_get_indices", "(", "y", ",", "d", ",", "anchors", ",", "num_total", ")", "\n", "\n", "# Subset the anchors, positives, and negatives either randomly or hard", "\n", "# sampling", "\n", "(", "\n", "anchors_indices", ",", "\n", "positives_similarity_indices", ",", "\n", "negatives_similarity_indices", ",", "\n", "num_positives", ",", "\n", ")", "=", "self", ".", "_contrastive_sampling", "(", "y", ",", "pred", ",", "anchors_indices", ",", "\n", "positives_similarity_indices", ",", "negatives_similarity_indices", ",", "\n", "num_anchors", ")", "\n", "\n", "# Compute similarity (e.g. cosine similarity) between anchor-positive", "\n", "# and anchor-negative pairs", "\n", "positives_similarity", "=", "self", ".", "_cosine_similarity_from_indices", "(", "\n", "z", ",", "positives_similarity_indices", ")", "\n", "negatives_similarity", "=", "self", ".", "_cosine_similarity_from_indices", "(", "\n", "z", ",", "negatives_similarity_indices", ")", "\n", "\n", "# Compute the InfoNCE loss", "\n", "contrastive_loss", "=", "self", ".", "_contrastive_infonce", "(", "\n", "positives_similarity", ",", "negatives_similarity", ",", "\n", "anchors_indices", ",", "positives_similarity_indices", ",", "\n", "negatives_similarity_indices", ",", "\n", "num_total", ",", "num_positives", ",", "training", ")", "\n", "\n", "return", "contrastive_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaBase.compute_losses": [[1361, 1399], ["super().compute_losses", "methods.MethodCaldaBase._contrastive_loss", "len", "len", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodAflacDG.compute_losses", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaBase._contrastive_loss"], ["", "def", "compute_losses", "(", "self", ",", "xs", ",", "task_y_true", ",", "domain_y_true", ",", "task_y_pred", ",", "\n", "domain_y_pred", ",", "fe_output", ",", "contrastive_output", ",", "auxiliary_data", ",", "\n", "which_model", ",", "training", ")", ":", "\n", "# Normal task and domain classifier losses", "\n", "        ", "other_losses", "=", "super", "(", ")", ".", "compute_losses", "(", "\n", "xs", ",", "task_y_true", ",", "domain_y_true", ",", "task_y_pred", ",", "\n", "domain_y_pred", ",", "fe_output", ",", "contrastive_output", ",", "auxiliary_data", ",", "\n", "which_model", ",", "training", ")", "\n", "\n", "# No weak supervision", "\n", "if", "len", "(", "other_losses", ")", "==", "3", ":", "\n", "            ", "weak_supervision", "=", "False", "\n", "_", ",", "task_loss", ",", "d_loss", "=", "other_losses", "\n", "# Weak supervision", "\n", "", "elif", "len", "(", "other_losses", ")", "==", "4", ":", "\n", "            ", "weak_supervision", "=", "True", "\n", "_", ",", "task_loss", ",", "d_loss", ",", "daws_loss", "=", "other_losses", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"should be 3 or 4 losses\"", ")", "\n", "\n", "# Additional contrastive loss", "\n", "", "similarity_loss", "=", "self", ".", "_contrastive_loss", "(", "\n", "task_y_true", ",", "domain_y_true", ",", "contrastive_output", ",", "task_y_pred", ",", "\n", "auxiliary_data", ",", "training", ")", "\n", "\n", "# If weak supervision, include it in the total loss and also return it", "\n", "# for plotting in metrics", "\n", "if", "weak_supervision", ":", "\n", "            ", "total_loss", "=", "task_loss", "+", "self", ".", "weight_adversary", "*", "d_loss", "+", "self", ".", "weight_similarity", "*", "similarity_loss", "+", "daws_loss", "\n", "return", "[", "total_loss", ",", "task_loss", ",", "d_loss", ",", "daws_loss", ",", "similarity_loss", "]", "\n", "", "else", ":", "\n", "            ", "total_loss", "=", "task_loss", "+", "self", ".", "weight_adversary", "*", "d_loss", "+", "self", ".", "weight_similarity", "*", "similarity_loss", "\n", "return", "[", "total_loss", ",", "task_loss", ",", "d_loss", ",", "similarity_loss", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaBase.compute_gradients": [[1400, 1403], ["super().compute_gradients"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodAflacDG.compute_gradients"], ["", "", "def", "compute_gradients", "(", "self", ",", "tape", ",", "losses", ",", "which_model", ")", ":", "\n", "# similarity loss is included in the total loss, so ignore it here", "\n", "        ", "return", "super", "(", ")", ".", "compute_gradients", "(", "tape", ",", "losses", "[", ":", "-", "1", "]", ",", "which_model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaCrossR.__init__": [[1407, 1409], ["methods.MethodCaldaBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaInR.__init__": [[1413, 1415], ["methods.MethodCaldaBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "in_domain", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaAnyR.__init__": [[1419, 1421], ["methods.MethodCaldaBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "any_domain", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaCrossRP.__init__": [[1425, 1427], ["methods.MethodCaldaBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "pseudo_label_target", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaInRP.__init__": [[1431, 1433], ["methods.MethodCaldaBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "in_domain", "=", "True", ",", "pseudo_label_target", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaAnyRP.__init__": [[1437, 1439], ["methods.MethodCaldaBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "any_domain", "=", "True", ",", "pseudo_label_target", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaCrossH.__init__": [[1443, 1445], ["methods.MethodCaldaBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "hard", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaInH.__init__": [[1449, 1451], ["methods.MethodCaldaBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "hard", "=", "True", ",", "in_domain", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaAnyH.__init__": [[1455, 1457], ["methods.MethodCaldaBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "hard", "=", "True", ",", "any_domain", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaCrossHP.__init__": [[1461, 1463], ["methods.MethodCaldaBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "hard", "=", "True", ",", "pseudo_label_target", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaInHP.__init__": [[1467, 1469], ["methods.MethodCaldaBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "hard", "=", "True", ",", "in_domain", "=", "True", ",", "pseudo_label_target", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaAnyHP.__init__": [[1473, 1475], ["methods.MethodCaldaBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "hard", "=", "True", ",", "any_domain", "=", "True", ",", "pseudo_label_target", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaAnyRNoAdv.__init__": [[1489, 1492], ["methods.MethodCaldaBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "any_domain", "=", "True", ",", "weight_adversary", "=", "0", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldaCrossHNoAdv.__init__": [[1496, 1499], ["methods.MethodCaldaBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "hard", "=", "True", ",", "weight_adversary", "=", "0", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDannDG.calculate_domain_outputs": [[1515, 1541], ["None"], "methods", ["None"], ["def", "calculate_domain_outputs", "(", "self", ")", ":", "\n", "# We override it in load_datasets.py now", "\n", "# assert FLAGS.batch_division != \"all\", \\", "\n", "#     \"batch_division all doesn't make sense with DG, use --batch_division=sources\"", "\n", "\n", "# SparseCategoricalCrossentropy gives an error if there's only one class.", "\n", "# Thus, throw in an extra, unused class (so softmax output always has 2).", "\n", "# Really, why would anybody do DG if there's only one domain...", "\n", "#", "\n", "# a=tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94],", "\n", "#   [0.1, 0.6, 0.3]])", "\n", "# t=tf.constant([0,1,2,1])", "\n", "# cce(t,a)  ## works", "\n", "#", "\n", "# b=tf.constant([[1.0], [1.0], [1.0], [1.0]])", "\n", "# t=tf.constant([0,0,0,0])", "\n", "# cce(t,b)  ## errors:", "\n", "#   \"ValueError: Shape mismatch: The shape of labels (received (342,))", "\n", "#   should equal the shape of logits except for the last dimension", "\n", "#   (received (1, 4)).\"", "\n", "        ", "if", "self", ".", "num_source_domains", "==", "1", ":", "\n", "            ", "domain_outputs", "=", "2", "\n", "", "else", ":", "\n", "            ", "domain_outputs", "=", "self", ".", "num_source_domains", "\n", "\n", "", "return", "domain_outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDannDG.domain_label": [[1542, 1564], ["None"], "methods", ["None"], ["", "def", "domain_label", "(", "self", ",", "index", ",", "is_target", ")", ":", "\n", "        ", "\"\"\"\n        Shift down the domain labels so 0 is not source 1 since we don't have a\n        target domain.\n\n        Note: during evaluation, if target data is used, then the results will\n        be wrong since target=0 and source #1=0 for the domain label. However,\n        target data shouldn't be used. It may cause some issues in the metrics\n        computations though during training (see metrics.py), e.g. the\n        target-domain domain classifier accuracy won't make sense.\n\n        New domain labeling:\n        0 = target\n        0 = source #0\n        1 = source #1\n        2 = source #2\n        ...\n        \"\"\"", "\n", "if", "is_target", ":", "\n", "            ", "return", "0", "\n", "", "else", ":", "\n", "            ", "return", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDannDG.prepare_data": [[1565, 1589], ["methods.MethodDannDG.get_num_modalities", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "range"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.load_datasets.get_num_modalities"], ["", "", "@", "tf", ".", "function", "\n", "def", "prepare_data", "(", "self", ",", "data_sources", ",", "data_target", ",", "which_model", ")", ":", "\n", "# Ignore target domain data when doing domain generalization", "\n", "# (Copied from DANN's but throw out target data)", "\n", "        ", "xs_a", ",", "y_a", ",", "domain_a", ",", "example_ids_a", "=", "data_sources", "\n", "xs_b", ",", "y_b", ",", "domain_b", ",", "example_ids_b", "=", "data_target", "\n", "\n", "# Concatenate all source domains' data", "\n", "#", "\n", "# xs is a list of domains, which is a tuple of modalities, which is", "\n", "# tensors, e.g. if one modality but two sources:", "\n", "#     [(s1 tensor,), (s2 tensor,)]", "\n", "# We want to concatenate the tensors from all domains separately for", "\n", "# each modality.", "\n", "source_num_modalities", ",", "target_num_modalities", "=", "self", ".", "get_num_modalities", "(", ")", "\n", "xs_a", "=", "[", "\n", "tf", ".", "concat", "(", "[", "x_a", "[", "i", "]", "for", "x_a", "in", "xs_a", "]", ",", "axis", "=", "0", ")", "\n", "for", "i", "in", "range", "(", "source_num_modalities", ")", "\n", "]", "\n", "y_a", "=", "tf", ".", "concat", "(", "y_a", ",", "axis", "=", "0", ")", "\n", "domain_a", "=", "tf", ".", "concat", "(", "domain_a", ",", "axis", "=", "0", ")", "\n", "auxiliary_data", "=", "None", "\n", "\n", "return", "xs_a", ",", "y_a", ",", "domain_a", ",", "auxiliary_data", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodDannDG.compute_losses": [[1590, 1602], ["methods.MethodDannDG.task_loss", "sum", "len", "methods.MethodDannDG.domain_loss"], "methods", ["None"], ["", "def", "compute_losses", "(", "self", ",", "xs", ",", "task_y_true", ",", "domain_y_true", ",", "task_y_pred", ",", "\n", "domain_y_pred", ",", "fe_output", ",", "contrastive_output", ",", "auxiliary_data", ",", "\n", "which_model", ",", "training", ")", ":", "\n", "# Since we don't have target domain data, don't throw out anything like", "\n", "# we did in MethodDANN()", "\n", "        ", "task_loss", "=", "self", ".", "task_loss", "(", "task_y_true", ",", "task_y_pred", ")", "\n", "d_loss", "=", "sum", "(", "[", "\n", "self", ".", "domain_loss", "(", "domain_y_true", ",", "d", ")", "\n", "for", "d", "in", "domain_y_pred", "\n", "]", ")", "/", "len", "(", "domain_y_pred", ")", "\n", "total_loss", "=", "task_loss", "+", "d_loss", "\n", "return", "[", "total_loss", ",", "task_loss", ",", "d_loss", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodSleepDG.create_model": [[1608, 1611], ["models.SleepModel"], "methods", ["None"], ["def", "create_model", "(", "self", ",", "model_name", ")", ":", "\n", "        ", "return", "models", ".", "SleepModel", "(", "self", ".", "num_classes", ",", "self", ".", "domain_outputs", ",", "\n", "self", ".", "global_step", ",", "self", ".", "total_steps", ",", "model_name", "=", "model_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodAflacDG.__init__": [[1633, 1639], ["methods.MethodDann.__init__", "methods.MethodAflacDG.mle_for_p_d_given_y", "models.DannGrlSchedule"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodAflacDG.mle_for_p_d_given_y", "home.repos.pwc.inspect_result.floft_calda.None.models.DannGrlSchedule"], ["def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "loss_names", "=", "[", "\"fe_tc\"", ",", "\"domain\"", ",", "\"task\"", ",", "\"kl\"", "]", "\n", "self", ".", "mle_for_p_d_given_y", "(", ")", "\n", "# Not fed to the model, but used in the loss", "\n", "self", ".", "grl_schedule", "=", "models", ".", "DannGrlSchedule", "(", "self", ".", "total_steps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodAflacDG.mle_for_p_d_given_y": [[1640, 1699], ["enumerate", "tensorflow.cast", "tensorflow.cast", "y.numpy.numpy.numpy", "d.numpy.numpy.numpy", "len", "len", "numpy.zeros", "range", "tensorflow.constant", "tensorflow.reduce_sum", "tensorflow.transpose", "tensorflow.transpose", "tensorflow.squeeze", "tensorflow.squeeze", "numpy.unique", "numpy.unique", "numpy.where", "numpy.unique", "numpy.zeros", "tensorflow.math.abs", "ys.append", "ds.append", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.concat", "tensorflow.concat", "tensorflow.ones_like"], "methods", ["None"], ["", "def", "mle_for_p_d_given_y", "(", "self", ")", ":", "\n", "        ", "\"\"\" Compute P(d|y)\n        https://github.com/akuzeee/AFLAC/blob/master/AFLAC.py#L14\n\n        Note: doing this rather than mle_for_p_d() since default is \"dependent_y\"\n        in their code https://github.com/akuzeee/AFLAC/blob/master/run.py#L138\n        \"\"\"", "\n", "# Get lists of all labels and domains so we can compute how many there", "\n", "# are of each", "\n", "ys", "=", "[", "]", "\n", "ds", "=", "[", "]", "\n", "\n", "# The domain is 0 for source 0, 1 for source 1, etc.", "\n", "# Note: we use the \"eval\" train dataset since it doesn't repeat infinitely", "\n", "for", "d", ",", "dataset", "in", "enumerate", "(", "self", ".", "source_train_eval_datasets", ")", ":", "\n", "            ", "for", "_", ",", "y", ",", "_", "in", "dataset", ":", "\n", "                ", "ys", ".", "append", "(", "y", ")", "\n", "ds", ".", "append", "(", "tf", ".", "ones_like", "(", "y", ")", "*", "d", ")", "\n", "\n", "# Fix Tensorflow bug / problem: expand, transpose, concat, then squeeze.", "\n", "# What I wanted to do is just tf.concat(ys, axis=0)... since ys is an", "\n", "# array of 1D tensors. But, it gives an error:", "\n", "# \"Expected concatenating dimensions in the range [0, 0)\"", "\n", "", "", "ys", "=", "[", "tf", ".", "transpose", "(", "tf", ".", "expand_dims", "(", "x", ",", "axis", "=", "0", ")", ")", "for", "x", "in", "ys", "]", "\n", "ds", "=", "[", "tf", ".", "transpose", "(", "tf", ".", "expand_dims", "(", "x", ",", "axis", "=", "0", ")", ")", "for", "x", "in", "ds", "]", "\n", "y", "=", "tf", ".", "cast", "(", "tf", ".", "squeeze", "(", "tf", ".", "concat", "(", "ys", ",", "axis", "=", "0", ")", ")", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "d", "=", "tf", ".", "cast", "(", "tf", ".", "squeeze", "(", "tf", ".", "concat", "(", "ds", ",", "axis", "=", "0", ")", ")", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "# Convert to numpy to ease converting the AFLAC code", "\n", "y", "=", "y", ".", "numpy", "(", ")", "\n", "d", "=", "d", ".", "numpy", "(", ")", "\n", "\n", "num_y_keys", "=", "len", "(", "np", ".", "unique", "(", "y", ")", ")", "\n", "num_d_keys", "=", "len", "(", "np", ".", "unique", "(", "d", ")", ")", "\n", "# Note: do <= not == since sometimes a person doesn't perform any of", "\n", "# a certain class, so it may be less. Though, for domains it really", "\n", "# should be equal unless one of the domains has no data.", "\n", "assert", "num_y_keys", "<=", "self", ".", "num_classes", "\n", "assert", "num_d_keys", "<=", "self", ".", "num_source_domains", "\n", "\n", "# Note: using domain_outputs not num_source_domains, since we have an", "\n", "# extra domain label if there's only one source domain.", "\n", "p_d_given_y", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_classes", ",", "self", ".", "domain_outputs", ")", ",", "\n", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "# Classes are numbered 0, 1, ..., num_classes-1", "\n", "for", "y_key", "in", "range", "(", "self", ".", "num_classes", ")", ":", "\n", "            ", "indices", "=", "np", ".", "where", "(", "y", "==", "y_key", ")", "\n", "d_given_key", "=", "d", "[", "indices", "]", "\n", "d_keys", ",", "d_counts", "=", "np", ".", "unique", "(", "d_given_key", ",", "return_counts", "=", "True", ")", "\n", "p_d_given_key", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_source_domains", ",", ")", ")", "\n", "p_d_given_key", "[", "d_keys", "]", "=", "d_counts", "\n", "p_d_given_y", "[", "y_key", "]", "=", "p_d_given_key", "\n", "\n", "# Normalize so for each class, the domain counts sum to one", "\n", "", "p_d_given_y", "=", "tf", ".", "constant", "(", "p_d_given_y", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "p_d_given_y", "/=", "tf", ".", "reduce_sum", "(", "tf", ".", "math", ".", "abs", "(", "p_d_given_y", ")", ",", "axis", "=", "1", ",", "keepdims", "=", "True", ")", "\n", "\n", "self", ".", "p_d_given_y", "=", "p_d_given_y", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodAflacDG.create_model": [[1700, 1706], ["models.BasicModel", "len"], "methods", ["None"], ["", "def", "create_model", "(", "self", ",", "model_name", ")", ":", "\n", "        ", "assert", "len", "(", "self", ".", "shared_modalities", ")", "==", "1", ",", "\"AFLAC only supports one shared modality between domains\"", "\n", "\n", "return", "models", ".", "BasicModel", "(", "self", ".", "num_classes", ",", "self", ".", "domain_outputs", ",", "\n", "model_name", "=", "model_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodAflacDG.compute_losses": [[1707, 1761], ["methods.MethodAflacDG.task_loss", "tensorflow.gather", "tensorflow.keras.losses.KLD", "methods.MethodAflacDG.grl_schedule", "sum", "len", "tensorflow.cast", "tensorflow.nn.softmax", "methods.MethodAflacDG.domain_loss"], "methods", ["None"], ["", "def", "compute_losses", "(", "self", ",", "xs", ",", "task_y_true", ",", "domain_y_true", ",", "task_y_pred", ",", "\n", "domain_y_pred", ",", "fe_output", ",", "contrastive_output", ",", "auxiliary_data", ",", "\n", "which_model", ",", "training", ")", ":", "\n", "        ", "task_loss", "=", "self", ".", "task_loss", "(", "task_y_true", ",", "task_y_pred", ")", "\n", "d_loss", "=", "sum", "(", "[", "\n", "self", ".", "domain_loss", "(", "domain_y_true", ",", "d", ")", "\n", "for", "d", "in", "domain_y_pred", "\n", "]", ")", "/", "len", "(", "domain_y_pred", ")", "\n", "\n", "# Gather the P(d|y) for the true y's for each example.", "\n", "# Note: this doesn't leak target-domain label information since this", "\n", "# is DG not MS-DA, so we have no data (x or y) for the target domain.", "\n", "d_true", "=", "tf", ".", "gather", "(", "self", ".", "p_d_given_y", ",", "tf", ".", "cast", "(", "task_y_true", ",", "dtype", "=", "tf", ".", "int32", ")", ")", "\n", "\n", "# p_d_given_y (above, now d_true) is already normalized, but", "\n", "# domain_y_pred is just \"logits\" (no softmax in model), so pass the", "\n", "# domain_y_pred through softmax before computing KLD.", "\n", "#", "\n", "# Also, we could implement KL divergence as done in", "\n", "# https://github.com/akuzeee/AFLAC/blob/master/utils.py#L183 with", "\n", "# something like:", "\n", "#   cce = tf.keras.losses.CategoricalCrossentropy()", "\n", "#   kl_d = -cce(q, q) + cce(q, p)", "\n", "# However, it's equivalent to using the KLD function, so we'll just use", "\n", "# that.", "\n", "#", "\n", "# Pf: -cce(q,q) + cce(q,p)", "\n", "#   = sum_i^D q_i log q_i - sum_i^D q_i log p_i (for D domains)", "\n", "#   = sum_i^D q_i * (log q_i - log p_i)", "\n", "#   = sum_i^D q_i log(q_i/p_i)", "\n", "#   = D_KL(q||p)", "\n", "# (then of course, this is done for each example in the batch)", "\n", "#", "\n", "# See:", "\n", "# https://en.wikipedia.org/wiki/Cross_entropy", "\n", "# https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "\n", "kl_loss", "=", "tf", ".", "keras", ".", "losses", ".", "KLD", "(", "d_true", ",", "tf", ".", "nn", ".", "softmax", "(", "domain_y_pred", ")", ")", "\n", "\n", "# Looking at Figure 2 -- https://arxiv.org/pdf/1904.12543.pdf", "\n", "# They backpropagate the task and KL (weighted by alpha) losses to FE", "\n", "# (and task... but KL doesn't matter for updating the task classifier).", "\n", "# They backpropagate the domain loss for updating DC.", "\n", "#", "\n", "# Their code:", "\n", "# https://github.com/akuzeee/AFLAC/blob/master/AFLAC.py#L158", "\n", "# Note that y_optimizer only updates FE and TC and d_optimizer only", "\n", "# updates DC. Rather than putting in GradMultiplyLayerF into network,", "\n", "# I'll just calculate alpha here and weight the KL loss by it since", "\n", "# we're ignoring the gradient throughout DC anyway, don't need it to be", "\n", "# weighted only through part of the network.", "\n", "alpha", "=", "self", ".", "grl_schedule", "(", "self", ".", "global_step", ")", "\n", "fe_tc_loss", "=", "task_loss", "+", "alpha", "*", "kl_loss", "\n", "\n", "return", "[", "fe_tc_loss", ",", "d_loss", ",", "task_loss", ",", "kl_loss", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodAflacDG.compute_gradients": [[1762, 1769], ["tape.gradient", "tape.gradient"], "methods", ["None"], ["", "def", "compute_gradients", "(", "self", ",", "tape", ",", "losses", ",", "which_model", ")", ":", "\n", "        ", "fe_tc_loss", ",", "d_loss", ",", "_", ",", "_", "=", "losses", "\n", "grad", "=", "tape", ".", "gradient", "(", "fe_tc_loss", ",", "\n", "self", ".", "model", "[", "which_model", "]", ".", "trainable_variables_task_fe", ")", "\n", "d_grad", "=", "tape", ".", "gradient", "(", "d_loss", ",", "\n", "self", ".", "model", "[", "which_model", "]", ".", "trainable_variables_domain", ")", "\n", "return", "[", "grad", ",", "d_grad", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodAflacDG.apply_gradients": [[1770, 1776], ["[].apply_gradients", "[].apply_gradients", "zip", "zip"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodAflacDG.apply_gradients", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodAflacDG.apply_gradients"], ["", "def", "apply_gradients", "(", "self", ",", "gradients", ",", "which_model", ")", ":", "\n", "        ", "grad", ",", "d_grad", "=", "gradients", "\n", "self", ".", "opt", "[", "which_model", "]", "[", "\"opt\"", "]", ".", "apply_gradients", "(", "zip", "(", "grad", ",", "\n", "self", ".", "model", "[", "which_model", "]", ".", "trainable_variables_task_fe", ")", ")", "\n", "self", ".", "opt", "[", "which_model", "]", "[", "\"d_opt\"", "]", ".", "apply_gradients", "(", "zip", "(", "d_grad", ",", "\n", "self", ".", "model", "[", "which_model", "]", ".", "trainable_variables_domain", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldgCrossR.__init__": [[1786, 1789], ["methods.MethodCaldaBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "\n", "domain_generalization", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldgInR.__init__": [[1793, 1796], ["methods.MethodCaldaBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "in_domain", "=", "True", ",", "\n", "domain_generalization", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldgAnyR.__init__": [[1800, 1803], ["methods.MethodCaldaBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "any_domain", "=", "True", ",", "\n", "domain_generalization", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldgCrossH.__init__": [[1807, 1810], ["methods.MethodCaldaBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "hard", "=", "True", ",", "\n", "domain_generalization", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldgInH.__init__": [[1814, 1817], ["methods.MethodCaldaBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "hard", "=", "True", ",", "in_domain", "=", "True", ",", "\n", "domain_generalization", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodCaldgAnyH.__init__": [[1821, 1824], ["methods.MethodCaldaBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "hard", "=", "True", ",", "any_domain", "=", "True", ",", "\n", "domain_generalization", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.register_method": [[36, 46], ["None"], "function", ["None"], ["def", "register_method", "(", "name", ")", ":", "\n", "    ", "\"\"\" Add method to the list of methods, e.g. add @register_method(\"name\")\n    before a class definition \"\"\"", "\n", "assert", "name", "not", "in", "methods", ",", "\"duplicate method named \"", "+", "name", "\n", "\n", "def", "decorator", "(", "cls", ")", ":", "\n", "        ", "methods", "[", "name", "]", "=", "cls", "\n", "return", "cls", "\n", "\n", "", "return", "decorator", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.get_method": [[48, 53], ["methods.keys"], "function", ["None"], ["", "def", "get_method", "(", "name", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Based on the given name, call the correct method \"\"\"", "\n", "assert", "name", "in", "methods", ".", "keys", "(", ")", ",", "\"Unknown method name \"", "+", "name", "\n", "return", "methods", "[", "name", "]", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.list_methods": [[55, 58], ["list", "methods.keys"], "function", ["None"], ["", "def", "list_methods", "(", ")", ":", "\n", "    ", "\"\"\" Returns list of all the available methods \"\"\"", "\n", "return", "list", "(", "methods", ".", "keys", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.methods.make_loss": [[1826, 1837], ["tensorflow.keras.losses.SparseCategoricalCrossentropy", "tensorflow.keras.losses.SparseCategoricalCrossentropy", "tf.keras.losses.SparseCategoricalCrossentropy."], "function", ["None"], ["", "", "def", "make_loss", "(", "from_logits", "=", "True", ",", "reduction", "=", "None", ")", ":", "\n", "    ", "if", "reduction", "is", "None", ":", "\n", "        ", "cce", "=", "tf", ".", "keras", ".", "losses", ".", "SparseCategoricalCrossentropy", "(", "from_logits", "=", "from_logits", ")", "\n", "", "else", ":", "\n", "        ", "cce", "=", "tf", ".", "keras", ".", "losses", ".", "SparseCategoricalCrossentropy", "(", "\n", "from_logits", "=", "from_logits", ",", "reduction", "=", "reduction", ")", "\n", "\n", "", "def", "loss", "(", "y_true", ",", "y_pred", ")", ":", "\n", "        ", "return", "cce", "(", "y_true", ",", "y_pred", ")", "\n", "\n", "", "return", "loss", "\n", "", ""]], "home.repos.pwc.inspect_result.floft_calda.None.pool.run_job_pool": [[8, 55], ["multiprocessing.Pool.close", "multiprocessing.Pool.join", "multiprocessing.Pool", "multiprocessing.Pool", "processes.append", "pbar.close", "multiprocessing.cpu_count", "multiprocessing.Pool.apply_async", "tqdm.tqdm", "results.append", "results.append", "pbar.update", "process.get", "len", "process.get"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.close", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.close", "home.repos.pwc.inspect_result.floft_calda.None.hyperparameters.get", "home.repos.pwc.inspect_result.floft_calda.None.hyperparameters.get"], ["def", "run_job_pool", "(", "func", ",", "argsList", ",", "desc", "=", "None", ",", "cores", "=", "None", ",", "show_progress", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Processor pool to use multiple cores, with a progress bar\n\n    func = function to execute\n    argsList = array of tuples, each tuple is the arguments to pass to the function\n\n    Combination of:\n    https://stackoverflow.com/a/43714721/2698494\n    https://stackoverflow.com/a/45652769/2698494\n\n    Returns:\n    an array of the outputs from the function\n\n    Example:\n    # Define a function that'll be run a bunch of times\n    def f(a,b):\n        return a+b\n\n    # Array of arrays (or tuples) of the arguments for the function\n    commands = [[1,2],[3,4],[5,6],[7,8]]\n    results = run_job_pool(f, commands, desc=\"Addition\")\n    \"\"\"", "\n", "if", "cores", "is", "None", ":", "\n", "        ", "p", "=", "multiprocessing", ".", "Pool", "(", "multiprocessing", ".", "cpu_count", "(", ")", ")", "\n", "", "else", ":", "\n", "        ", "p", "=", "multiprocessing", ".", "Pool", "(", "cores", ")", "\n", "", "processes", "=", "[", "]", "\n", "results", "=", "[", "]", "\n", "\n", "for", "args", "in", "argsList", ":", "\n", "        ", "processes", ".", "append", "(", "p", ".", "apply_async", "(", "func", ",", "args", ")", ")", "\n", "\n", "", "if", "show_progress", ":", "\n", "        ", "with", "tqdm", ".", "tqdm", "(", "total", "=", "len", "(", "processes", ")", ",", "desc", "=", "desc", ")", "as", "pbar", ":", "\n", "            ", "for", "process", "in", "processes", ":", "\n", "                ", "results", ".", "append", "(", "process", ".", "get", "(", ")", ")", "\n", "pbar", ".", "update", "(", ")", "\n", "", "", "pbar", ".", "close", "(", ")", "\n", "", "else", ":", "\n", "        ", "for", "process", "in", "processes", ":", "\n", "            ", "results", ".", "append", "(", "process", ".", "get", "(", ")", ")", "\n", "\n", "", "", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n", "return", "results", "\n", "", ""]], "home.repos.pwc.inspect_result.floft_calda.None.main.get_directory_names": [[61, 92], ["print", "os.path.join", "os.path.join", "str", "file_utils.last_modified_number", "print", "os.path.join", "os.path.join", "str", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.file_utils.last_modified_number"], ["def", "get_directory_names", "(", ")", ":", "\n", "    ", "\"\"\" Figure out the log and model directory names \"\"\"", "\n", "prefix", "=", "FLAGS", ".", "dataset", "+", "\"-\"", "+", "FLAGS", ".", "uid", "+", "\"-\"", "+", "FLAGS", ".", "method", "\n", "\n", "# Use the number specified on the command line (higher precedence than --debug)", "\n", "if", "FLAGS", ".", "debugnum", ">=", "0", ":", "\n", "        ", "attempt", "=", "FLAGS", ".", "debugnum", "\n", "print", "(", "\"Debugging attempt:\"", ",", "attempt", ")", "\n", "\n", "prefix", "+=", "\"-\"", "+", "str", "(", "attempt", ")", "\n", "model_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "modeldir", ",", "prefix", ")", "\n", "log_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "logdir", ",", "prefix", ")", "\n", "# Find last one, increment number", "\n", "", "elif", "FLAGS", ".", "debug", ":", "\n", "        ", "attempt", "=", "file_utils", ".", "last_modified_number", "(", "FLAGS", ".", "logdir", ",", "prefix", "+", "\"*\"", ")", "\n", "attempt", "=", "attempt", "+", "1", "if", "attempt", "is", "not", "None", "else", "1", "\n", "print", "(", "\"Debugging attempt:\"", ",", "attempt", ")", "\n", "\n", "prefix", "+=", "\"-\"", "+", "str", "(", "attempt", ")", "\n", "model_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "modeldir", ",", "prefix", ")", "\n", "log_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "logdir", ",", "prefix", ")", "\n", "# If no debugging modes, use the model and log directory with only the \"prefix\"", "\n", "# (even though it's not actually a prefix in this case, it's the whole name)", "\n", "", "elif", "FLAGS", ".", "subdir", ":", "\n", "        ", "model_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "modeldir", ",", "prefix", ")", "\n", "log_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "logdir", ",", "prefix", ")", "\n", "", "else", ":", "\n", "        ", "model_dir", "=", "FLAGS", ".", "modeldir", "\n", "log_dir", "=", "FLAGS", ".", "logdir", "\n", "\n", "", "return", "model_dir", ",", "log_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.main.main": [[94, 238], ["gpu_memory.set_gpu_memory", "main.get_directory_names", "file_utils.write_config_from_args", "load_datasets.load_da", "tensorflow.Variable", "methods.get_method", "tensorflow.train.Checkpoint", "checkpoints.CheckpointManager", "checkpoints.CheckpointManager.restore_latest", "metrics.Metrics", "range", "file_utils.write_finished", "os.path.exists", "os.path.exists", "os.path.exists", "os.makedirs", "os.path.exists", "os.makedirs", "print", "int", "time.time", "methods.get_method.train_step", "tf.Variable.assign_add", "shutil.rmtree", "shutil.rmtree", "source_modality_subset.split", "time.time", "print", "print", "sys.stdout.flush", "metrics.Metrics.train", "metrics.Metrics.test", "checkpoints.CheckpointManager.save", "int", "new_modality_subset.append", "int", "int", "FLAGS.shared_modalities.split", "int"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.gpu_memory.set_gpu_memory", "home.repos.pwc.inspect_result.floft_calda.None.main.get_directory_names", "home.repos.pwc.inspect_result.floft_calda.None.file_utils.write_config_from_args", "home.repos.pwc.inspect_result.floft_calda.None.load_datasets.load_da", "home.repos.pwc.inspect_result.floft_calda.None.analysis.get_method", "home.repos.pwc.inspect_result.floft_calda.None.checkpoints.CheckpointManager.restore_latest", "home.repos.pwc.inspect_result.floft_calda.None.file_utils.write_finished", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.train_step", "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics.train", "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics.test", "home.repos.pwc.inspect_result.floft_calda.None.checkpoints.CheckpointManager.save"], ["", "def", "main", "(", "argv", ")", ":", "\n", "# Allow running multiple at once", "\n", "    ", "set_gpu_memory", "(", "FLAGS", ".", "gpumem", ")", "\n", "\n", "# Figure out the log and model directory filenames", "\n", "assert", "FLAGS", ".", "uid", "!=", "\"\"", ",", "\"uid cannot be an empty string\"", "\n", "model_dir", ",", "log_dir", "=", "get_directory_names", "(", ")", "\n", "\n", "if", "FLAGS", ".", "restart", ":", "\n", "        ", "if", "os", ".", "path", ".", "exists", "(", "model_dir", ")", ":", "\n", "            ", "shutil", ".", "rmtree", "(", "model_dir", ")", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "log_dir", ")", ":", "\n", "            ", "shutil", ".", "rmtree", "(", "log_dir", ")", "\n", "\n", "", "", "if", "not", "os", ".", "path", ".", "exists", "(", "model_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "model_dir", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "log_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "log_dir", ")", "\n", "\n", "# Write config file about what dataset we're using, sources, target, etc.", "\n", "", "file_utils", ".", "write_config_from_args", "(", "log_dir", ")", "\n", "\n", "# Changes for upper bound -- upper bound is actually method \"none\" but", "\n", "# without a target domain", "\n", "method_name", "=", "FLAGS", ".", "method", "\n", "\n", "if", "method_name", "==", "\"upper\"", ":", "\n", "        ", "method_name", "=", "\"none\"", "\n", "sources", "=", "FLAGS", ".", "target", "\n", "target", "=", "\"\"", "\n", "source_modality_subset", "=", "FLAGS", ".", "target_modality_subset", "\n", "target_modality_subset", "=", "\"\"", "\n", "", "else", ":", "\n", "        ", "sources", "=", "FLAGS", ".", "sources", "\n", "target", "=", "FLAGS", ".", "target", "\n", "source_modality_subset", "=", "FLAGS", ".", "source_modality_subset", "\n", "target_modality_subset", "=", "FLAGS", ".", "target_modality_subset", "\n", "\n", "# Remove unused modality since the no adaptation / upper bound will error", "\n", "", "if", "method_name", "==", "\"none\"", ":", "# or it was upper before the above if", "\n", "        ", "if", "source_modality_subset", "!=", "\"\"", ":", "\n", "# Fix \"Weights for model sequential_1 have not yet been created. Weights", "\n", "# are created when the Model is first called on inputs or `build()` is", "\n", "# called with an `input_shape`.\" e.g. when the above yields", "\n", "# source_modality_subset = \"1,0\" and shared_modalities=\"0\" we end up", "\n", "# never using the second modality's FE or DC. Thus, just throw out the", "\n", "# unused modality. For example, here this would end up just setting", "\n", "# source_modality_subset to \"1\".", "\n", "            ", "modality_subset_list", "=", "source_modality_subset", ".", "split", "(", "\",\"", ")", "# \"1\",\"0\"", "\n", "shared_modalities_list", "=", "[", "int", "(", "x", ")", "for", "x", "in", "FLAGS", ".", "shared_modalities", ".", "split", "(", "\",\"", ")", "]", "# 0", "\n", "new_modality_subset", "=", "[", "]", "\n", "\n", "for", "modality", "in", "shared_modalities_list", ":", "\n", "                ", "new_modality_subset", ".", "append", "(", "modality_subset_list", "[", "modality", "]", ")", "\n", "\n", "", "source_modality_subset", "=", "\",\"", ".", "join", "(", "new_modality_subset", ")", "\n", "\n", "# If using a domain generalization method, then split among sources not", "\n", "# sources and target. Same for weak supervision.", "\n", "# TODO keep this up to date with domain generalization method list", "\n", "", "", "domain_generalization", "=", "\"_dg\"", "in", "method_name", "or", "\"caldg\"", "in", "method_name", "\n", "weak_supervision", "=", "\"_ws\"", "in", "method_name", "\n", "override_batch_division", "=", "domain_generalization", "or", "weak_supervision", "\n", "\n", "# Load datasets", "\n", "source_datasets", ",", "target_dataset", "=", "load_datasets", ".", "load_da", "(", "FLAGS", ".", "dataset", ",", "\n", "sources", ",", "target", ",", "\n", "test", "=", "FLAGS", ".", "test", ",", "\n", "source_modality_subset", "=", "source_modality_subset", ",", "\n", "target_modality_subset", "=", "target_modality_subset", ",", "\n", "override_batch_division", "=", "override_batch_division", ")", "\n", "\n", "# Need to know which iteration for learning rate schedule", "\n", "global_step", "=", "tf", ".", "Variable", "(", "0", ",", "name", "=", "\"global_step\"", ",", "trainable", "=", "False", ")", "\n", "\n", "# Load the method, model, etc.", "\n", "method", "=", "methods", ".", "get_method", "(", "method_name", ",", "\n", "source_datasets", "=", "source_datasets", ",", "\n", "target_dataset", "=", "target_dataset", ",", "\n", "model_name", "=", "FLAGS", ".", "model", ",", "\n", "global_step", "=", "global_step", ",", "\n", "total_steps", "=", "FLAGS", ".", "steps", ",", "\n", "ensemble_size", "=", "FLAGS", ".", "ensemble", ",", "\n", "moving_average", "=", "FLAGS", ".", "moving_average", ",", "\n", "shared_modalities", "=", "FLAGS", ".", "shared_modalities", ",", "\n", "share_most_weights", "=", "FLAGS", ".", "share_most_weights", ",", "\n", "dataset_name", "=", "FLAGS", ".", "dataset", ")", "\n", "\n", "# Check that this method is supposed to be trainable. If not, we're done.", "\n", "# (Basically, we just wanted to write the config file for non-trainable", "\n", "# models.)", "\n", "if", "not", "method", ".", "trainable", ":", "\n", "        ", "print", "(", "\"Method not trainable. Exiting now.\"", ")", "\n", "return", "\n", "\n", "# Checkpoints", "\n", "", "checkpoint", "=", "tf", ".", "train", ".", "Checkpoint", "(", "\n", "global_step", "=", "global_step", ",", "**", "method", ".", "checkpoint_variables", ")", "\n", "checkpoint_manager", "=", "CheckpointManager", "(", "checkpoint", ",", "model_dir", ",", "log_dir", ")", "\n", "checkpoint_manager", ".", "restore_latest", "(", ")", "\n", "\n", "# Metrics", "\n", "has_target_domain", "=", "target_dataset", "is", "not", "None", "\n", "metrics", "=", "Metrics", "(", "log_dir", ",", "method", ",", "source_datasets", ",", "target_dataset", ",", "\n", "has_target_domain", ")", "\n", "\n", "# Start training", "\n", "for", "i", "in", "range", "(", "int", "(", "global_step", ")", ",", "FLAGS", ".", "steps", "+", "1", ")", ":", "\n", "        ", "t", "=", "time", ".", "time", "(", ")", "\n", "data_sources", ",", "data_target", "=", "method", ".", "train_step", "(", ")", "\n", "global_step", ".", "assign_add", "(", "1", ")", "\n", "t", "=", "time", ".", "time", "(", ")", "-", "t", "\n", "\n", "if", "FLAGS", ".", "time_training", ":", "\n", "            ", "print", "(", "int", "(", "global_step", ")", ",", "t", ",", "sep", "=", "\",\"", ")", "\n", "continue", "# skip evaluation, checkpointing, etc. when timing", "\n", "\n", "", "if", "i", "%", "1000", "==", "0", "or", "i", "<=", "10", ":", "\n", "            ", "print", "(", "\"step %d took %f seconds\"", "%", "(", "int", "(", "global_step", ")", ",", "t", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "# otherwise waits till the end to flush on Kamiak", "\n", "\n", "# Metrics on training/validation data", "\n", "", "if", "FLAGS", ".", "log_train_steps", "!=", "0", "and", "i", "%", "FLAGS", ".", "log_train_steps", "==", "0", ":", "\n", "            ", "metrics", ".", "train", "(", "data_sources", ",", "data_target", ",", "global_step", ",", "t", ")", "\n", "\n", "# Evaluate every log_val_steps but also at the last step", "\n", "", "validation_accuracy_source", "=", "None", "\n", "validation_accuracy_target", "=", "None", "\n", "if", "(", "FLAGS", ".", "log_val_steps", "!=", "0", "and", "i", "%", "FLAGS", ".", "log_val_steps", "==", "0", ")", "or", "i", "==", "FLAGS", ".", "steps", ":", "\n", "            ", "validation_accuracy_source", ",", "validation_accuracy_target", "=", "metrics", ".", "test", "(", "global_step", ")", "\n", "\n", "# Checkpoints -- Save either if at the right model step or if we found", "\n", "# a new validation accuracy. If this is better than the previous best", "\n", "# model, we need to make a new checkpoint so we can restore from this", "\n", "# step with the best accuracy.", "\n", "", "if", "(", "FLAGS", ".", "model_steps", "!=", "0", "and", "i", "%", "FLAGS", ".", "model_steps", "==", "0", ")", "or", "validation_accuracy_source", "is", "not", "None", ":", "\n", "            ", "checkpoint_manager", ".", "save", "(", "int", "(", "global_step", "-", "1", ")", ",", "\n", "validation_accuracy_source", ",", "validation_accuracy_target", ")", "\n", "\n", "# We're done -- used for hyperparameter tuning", "\n", "", "", "file_utils", ".", "write_finished", "(", "log_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.hyperparameter_tuning_analysis.get_average_accuracy": [[48, 82], ["analysis.get_results", "analysis.process_results", "analysis.process_results.items", "len", "analysis.pretty_dataset_name", "dataset_values.items", "sum", "len", "sum", "len", "accuracies.append", "stdevs.append", "print"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.sampling_analysis.get_results", "home.repos.pwc.inspect_result.floft_calda.None.analysis.process_results", "home.repos.pwc.inspect_result.floft_calda.None.analysis.pretty_dataset_name"], ["def", "get_average_accuracy", "(", "folder", ",", "runs", ",", "dataset", ",", "method", ",", "average_over_users", "=", "True", ",", "\n", "debug", "=", "False", ",", "show_progress", "=", "False", ",", "only_n", "=", "None", ")", ":", "\n", "# additional_match for speed", "\n", "    ", "results", "=", "get_results", "(", "runs", ",", "variant_match", "=", "\"*\"", ",", "\n", "source_feature_subset", "=", "None", ",", "target_feature_subset", "=", "None", ",", "tune", "=", "False", ",", "\n", "folder", "=", "folder", ",", "additional_match", "=", "\"{}-*-{}\"", ".", "format", "(", "dataset", ",", "method", ")", ",", "\n", "show_progress", "=", "show_progress", ")", "\n", "averages", "=", "process_results", "(", "results", ",", "average_over_users", "=", "average_over_users", ",", "\n", "ssda", "=", "False", ",", "upper_bound_offset", "=", "None", ",", "tune", "=", "False", ")", "\n", "\n", "accuracies", "=", "[", "]", "\n", "stdevs", "=", "[", "]", "\n", "\n", "for", "dataset_name", ",", "dataset_values", "in", "averages", ".", "items", "(", ")", ":", "\n", "# Skip all data for datasets other than the one we care about", "\n", "        ", "if", "pretty_dataset_name", "(", "dataset", ")", "==", "dataset_name", ":", "\n", "            ", "for", "method_name", ",", "method_values", "in", "dataset_values", ".", "items", "(", ")", ":", "\n", "# Skip all the data for methods other than the one we care about", "\n", "                ", "if", "method", "==", "method_name", ":", "\n", "                    ", "for", "n", ",", "accuracy", ",", "stdev", "in", "method_values", ":", "\n", "                        ", "if", "debug", ":", "\n", "                            ", "print", "(", "dataset_name", ",", "method_name", ",", "n", ",", "accuracy", ",", "stdev", ",", "\n", "file", "=", "sys", ".", "stderr", ")", "\n", "\n", "", "if", "only_n", "is", "not", "None", "and", "n", "!=", "only_n", ":", "\n", "                            ", "continue", "\n", "\n", "", "accuracies", ".", "append", "(", "accuracy", ")", "\n", "stdevs", ".", "append", "(", "stdev", ")", "\n", "\n", "", "", "", "", "", "if", "len", "(", "accuracies", ")", ">", "0", ":", "\n", "        ", "return", "sum", "(", "accuracies", ")", "/", "len", "(", "accuracies", ")", ",", "sum", "(", "stdevs", ")", "/", "len", "(", "stdevs", ")", "\n", "", "else", ":", "\n", "        ", "return", "-", "1", ",", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.hyperparameter_tuning_analysis.find_best_on_average": [[84, 111], ["max", "hyperparameter_tuning_analysis.get_average_accuracy", "range", "print", "range", "print", "len", "len", "print", "best_runs.append"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.hyperparameter_tuning_analysis.get_average_accuracy"], ["", "", "def", "find_best_on_average", "(", "folder", ",", "prefixes", ",", "runs", ",", "dataset", ",", "method", ",", "debug", "=", "True", ")", ":", "\n", "    ", "accuracies", "=", "[", "\n", "get_average_accuracy", "(", "folder", ",", "[", "\n", "prefix", "+", "\"_\"", "+", "run_name", "for", "prefix", "in", "prefixes", "\n", "]", ",", "dataset", ",", "method", ")", "\n", "for", "run_name", ",", "run_options", ",", "run_tuple", "in", "runs", "\n", "]", "\n", "accuracies", "=", "[", "a", "for", "a", ",", "_", "in", "accuracies", "]", "# throw out stdev", "\n", "best_average_accuracy", "=", "max", "(", "accuracies", ")", "\n", "\n", "# Return the list of the best since it's possible more than one has the", "\n", "# same accuracy", "\n", "best_runs", "=", "[", "]", "\n", "\n", "if", "best_average_accuracy", "!=", "-", "1", ":", "\n", "        ", "if", "debug", ":", "\n", "            ", "print", "(", "\"    Average accuracy for each run:\"", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "runs", ")", ")", ":", "\n", "# [0] is the folder, which is shorter than the full options in [1]", "\n", "                ", "print", "(", "\"      \"", ",", "runs", "[", "i", "]", "[", "0", "]", ",", "accuracies", "[", "i", "]", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "", "print", "(", "file", "=", "sys", ".", "stderr", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "runs", ")", ")", ":", "\n", "            ", "if", "accuracies", "[", "i", "]", "==", "best_average_accuracy", ":", "\n", "                ", "best_runs", ".", "append", "(", "runs", "[", "i", "]", ")", "\n", "\n", "", "", "", "return", "best_runs", ",", "best_average_accuracy", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.hyperparameter_tuning_analysis.main": [[113, 218], ["FLAGS.prefixes.split", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "print", "print_dictionary.print_dictionary", "print", "print_dictionary.print_dictionary", "print", "print_dictionary.print_dictionary", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "print", "print", "hyperparameter_tuning_analysis.find_best_on_average", "print", "print", "print", "print", "print", "len", "print", "len", "print"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.print_dictionary.print_dictionary", "home.repos.pwc.inspect_result.floft_calda.None.print_dictionary.print_dictionary", "home.repos.pwc.inspect_result.floft_calda.None.print_dictionary.print_dictionary", "home.repos.pwc.inspect_result.floft_calda.None.hyperparameter_tuning_analysis.find_best_on_average"], ["", "def", "main", "(", "argv", ")", ":", "\n", "    ", "assert", "FLAGS", ".", "prefixes", "!=", "\"\"", ",", "\"must pass prefixes\"", "\n", "prefixes", "=", "FLAGS", ".", "prefixes", ".", "split", "(", "\",\"", ")", "\n", "\n", "# Match that of hyperparameter_tuning_experiments.py probably", "\n", "datasets", "=", "[", "\n", "\"ucihar\"", ",", "\"ucihhar\"", ",", "\"wisdm_ar\"", ",", "\"wisdm_at\"", ",", "\n", "\"myo\"", ",", "\"ninapro_db5_like_myo_noshift\"", ",", "\n", "\"normal_n12_l3_inter2_intra1_5,0,0,0_sine\"", ",", "\n", "\"normal_n12_l3_inter2_intra1_0,0.5,0,0_sine\"", ",", "\n", "\"normal_n12_l3_inter1_intra2_0,0,5,0_sine\"", ",", "\n", "\"normal_n12_l3_inter1_intra2_0,0,0,0.5_sine\"", ",", "\n", "]", "\n", "methods", "=", "[", "\"none\"", ",", "\"codats\"", ",", "\"calda_xs_h\"", ",", "\"upper\"", ",", "\"can\"", "]", "\n", "\n", "# hyperparameters...[dataset][method] = \"\"", "\n", "hyperparameters_str", "=", "collections", ".", "defaultdict", "(", "\n", "lambda", ":", "collections", ".", "defaultdict", "(", "str", ")", "\n", ")", "\n", "hyperparameters_tuple", "=", "collections", ".", "defaultdict", "(", "\n", "lambda", ":", "collections", ".", "defaultdict", "(", "str", ")", "\n", ")", "\n", "hyperparameters_folder", "=", "collections", ".", "defaultdict", "(", "\n", "lambda", ":", "collections", ".", "defaultdict", "(", "str", ")", "\n", ")", "\n", "\n", "for", "dataset", "in", "datasets", ":", "\n", "        ", "for", "method", "in", "methods", ":", "\n", "            ", "method_name_tuning", "=", "method", "\n", "\n", "if", "(", "\n", "(", "\n", "dataset", "not", "in", "hyperparameter_tuning_experiments_list", "\n", "or", "method_name_tuning", "not", "in", "hyperparameter_tuning_experiments_list", "[", "dataset", "]", "\n", ")", "and", "(", "\n", "dataset", "not", "in", "hyperparameter_tuning_experiments_list_can", "\n", "or", "method_name_tuning", "not", "in", "hyperparameter_tuning_experiments_list_can", "[", "dataset", "]", "\n", ")", "\n", ")", ":", "\n", "                ", "print", "(", "\"Skipping\"", ",", "dataset", ",", "method", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "continue", "\n", "\n", "", "print", "(", "\"Dataset:\"", ",", "dataset", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "print", "(", "\"  Method:\"", ",", "method", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "\n", "# Get runs for this method", "\n", "if", "method_name_tuning", "==", "\"can\"", ":", "\n", "                ", "runs", "=", "hyperparameter_tuning_experiments_list_can", "[", "dataset", "]", "[", "method_name_tuning", "]", "\n", "", "else", ":", "\n", "                ", "runs", "=", "hyperparameter_tuning_experiments_list", "[", "dataset", "]", "[", "method_name_tuning", "]", "\n", "\n", "# Compute and output for debugging", "\n", "", "best_run", ",", "best_average_accuracy", "=", "find_best_on_average", "(", "\n", "\"results_tune\"", ",", "prefixes", ",", "runs", ",", "dataset", ",", "method", ")", "\n", "print", "(", "\"    Use hyperparameters from (valid set):\"", ",", "\", \"", ".", "join", "(", "[", "\n", "run_folder", "for", "run_folder", ",", "run_options", ",", "run_tuple", "in", "best_run", "\n", "]", ")", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "print", "(", "\"    which had average accuracy:\"", ",", "best_average_accuracy", ",", "\n", "file", "=", "sys", ".", "stderr", ")", "\n", "print", "(", "file", "=", "sys", ".", "stderr", ")", "\n", "print", "(", "file", "=", "sys", ".", "stderr", ")", "\n", "\n", "# Save hyperparameters", "\n", "if", "len", "(", "best_run", ")", ">", "0", ":", "\n", "                ", "if", "len", "(", "best_run", ")", ">", "1", ":", "\n", "                    ", "print", "(", "\"  Warning: more than one best run, selecting hyperparameters from the first\"", ",", "\n", "file", "=", "sys", ".", "stderr", ")", "\n", "\n", "", "if", "best_average_accuracy", "!=", "-", "1", ":", "\n", "                    ", "hyperparameters_str", "[", "dataset", "]", "[", "method_name_tuning", "]", "=", "best_run", "[", "0", "]", "[", "1", "]", "\n", "hyperparameters_folder", "[", "dataset", "]", "[", "method_name_tuning", "]", "=", "best_run", "[", "0", "]", "[", "0", "]", "\n", "hyperparameters_tuple", "[", "dataset", "]", "[", "method_name_tuning", "]", "=", "best_run", "[", "0", "]", "[", "2", "]", "\n", "", "", "else", ":", "\n", "                ", "print", "(", "\"  Warning: didn't find a best run\"", ",", "\n", "file", "=", "sys", ".", "stderr", ")", "\n", "\n", "# Output hyperparameters (to stdout)", "\n", "", "", "", "print", "(", "\"\"\"# Generated by hyperparameter_tuning_analysis.py. Changes will be overwritten.\ndef get(dataset, method, values, default_value=None):\n    if dataset in values and method in values[dataset]:\n        result = values[dataset][method]\n    else:\n        result = default_value\n\n    return result\n\n\ndef get_hyperparameters_str(dataset, method):\n    return get(dataset, method, hyperparameters_str, \"\")\n\n\ndef get_hyperparameters_tuple(dataset, method):\n    return get(dataset, method, hyperparameters_tuple, None)\n\n\ndef get_hyperparameters_folder(dataset, method):\n    return get(dataset, method, hyperparameters_folder, None)\n\n\"\"\"", ")", "\n", "\n", "print_dictionary", "(", "hyperparameters_str", ",", "\"hyperparameters_str\"", ")", "\n", "print", "(", ")", "\n", "print_dictionary", "(", "hyperparameters_tuple", ",", "\"hyperparameters_tuple\"", ")", "\n", "print", "(", ")", "\n", "print_dictionary", "(", "hyperparameters_folder", ",", "\"hyperparameters_folder\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.hyperparameters.get": [[2, 9], ["None"], "function", ["None"], ["def", "get", "(", "dataset", ",", "method", ",", "values", ",", "default_value", "=", "None", ")", ":", "\n", "    ", "if", "dataset", "in", "values", "and", "method", "in", "values", "[", "dataset", "]", ":", "\n", "        ", "result", "=", "values", "[", "dataset", "]", "[", "method", "]", "\n", "", "else", ":", "\n", "        ", "result", "=", "default_value", "\n", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.hyperparameters.get_hyperparameters_str": [[11, 13], ["hyperparameters.get"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.hyperparameters.get"], ["", "def", "get_hyperparameters_str", "(", "dataset", ",", "method", ")", ":", "\n", "    ", "return", "get", "(", "dataset", ",", "method", ",", "hyperparameters_str", ",", "\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.hyperparameters.get_hyperparameters_tuple": [[15, 17], ["hyperparameters.get"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.hyperparameters.get"], ["", "def", "get_hyperparameters_tuple", "(", "dataset", ",", "method", ")", ":", "\n", "    ", "return", "get", "(", "dataset", ",", "method", ",", "hyperparameters_tuple", ",", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.hyperparameters.get_hyperparameters_folder": [[19, 21], ["hyperparameters.get"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.hyperparameters.get"], ["", "def", "get_hyperparameters_folder", "(", "dataset", ",", "method", ")", ":", "\n", "    ", "return", "get", "(", "dataset", ",", "method", ",", "hyperparameters_folder", ",", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.print_dictionary._quote_if_string": [[6, 11], ["isinstance", "str"], "function", ["None"], ["def", "_quote_if_string", "(", "value", ")", ":", "\n", "    ", "if", "isinstance", "(", "value", ",", "str", ")", ":", "\n", "        ", "return", "\"\\\"\"", "+", "value", "+", "\"\\\"\"", "\n", "", "else", ":", "\n", "        ", "return", "str", "(", "value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.print_dictionary._prepend_if_multiline": [[13, 24], ["None"], "function", ["None"], ["", "", "def", "_prepend_if_multiline", "(", "s", ",", "prepend_spaces_count", ")", ":", "\n", "    ", "result", "=", "\"\"", "\n", "prepend", "=", "\" \"", "*", "prepend_spaces_count", "\n", "\n", "for", "c", "in", "s", ":", "\n", "        ", "if", "c", "==", "\"\\n\"", ":", "\n", "            ", "result", "+=", "c", "+", "prepend", "\n", "", "else", ":", "\n", "            ", "result", "+=", "c", "\n", "\n", "", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.print_dictionary._print_dictionary": [[26, 39], ["print", "d.keys", "print", "isinstance", "print_dictionary._print_dictionary", "print", "print_dictionary._quote_if_string", "print_dictionary._quote_if_string", "print_dictionary._prepend_if_multiline", "print_dictionary._quote_if_string", "len"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.print_dictionary._print_dictionary", "home.repos.pwc.inspect_result.floft_calda.None.print_dictionary._quote_if_string", "home.repos.pwc.inspect_result.floft_calda.None.print_dictionary._quote_if_string", "home.repos.pwc.inspect_result.floft_calda.None.print_dictionary._prepend_if_multiline", "home.repos.pwc.inspect_result.floft_calda.None.print_dictionary._quote_if_string"], ["", "def", "_print_dictionary", "(", "d", ",", "name", ",", "prepend", "=", "\"\"", ")", ":", "\n", "    ", "\"\"\" Recursively print dictionary \"\"\"", "\n", "print", "(", "prepend", "+", "_quote_if_string", "(", "name", ")", "+", "\": {\"", ")", "\n", "\n", "for", "k", "in", "d", ".", "keys", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "d", "[", "k", "]", ",", "dict", ")", ":", "\n", "            ", "_print_dictionary", "(", "d", "[", "k", "]", ",", "k", ",", "prepend", "=", "prepend", "+", "\"    \"", ")", "\n", "", "else", ":", "\n", "            ", "beginning", "=", "prepend", "+", "\"    \"", "+", "_quote_if_string", "(", "k", ")", "+", "\": \"", "\n", "print", "(", "beginning", "+", "_prepend_if_multiline", "(", "\n", "_quote_if_string", "(", "d", "[", "k", "]", ")", ",", "len", "(", "beginning", ")", ")", "+", "\",\"", ")", "\n", "\n", "", "", "print", "(", "prepend", "+", "\"},\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.print_dictionary.print_dictionary": [[41, 52], ["print", "d.keys", "print", "isinstance", "print_dictionary._print_dictionary", "print", "print_dictionary._quote_if_string", "print_dictionary._quote_if_string"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.print_dictionary._print_dictionary", "home.repos.pwc.inspect_result.floft_calda.None.print_dictionary._quote_if_string", "home.repos.pwc.inspect_result.floft_calda.None.print_dictionary._quote_if_string"], ["", "def", "print_dictionary", "(", "d", ",", "name", ")", ":", "\n", "    ", "\"\"\" Recursively print dictionary \"\"\"", "\n", "print", "(", "name", ",", "\"= {\"", ")", "\n", "\n", "for", "k", "in", "d", ".", "keys", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "d", "[", "k", "]", ",", "dict", ")", ":", "\n", "            ", "_print_dictionary", "(", "d", "[", "k", "]", ",", "k", ",", "prepend", "=", "\"    \"", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"    \"", "+", "_quote_if_string", "(", "k", ")", "+", "\": \"", "+", "_quote_if_string", "(", "d", "[", "k", "]", ")", "+", "\",\"", ")", "\n", "\n", "", "", "print", "(", "\"}\"", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics.__init__": [[51, 134], ["tensorflow.summary.create_file_writer", "range", "metrics.Metrics.source_datasets[].int_to_label", "tensorflow.keras.metrics.CategoricalAccuracy", "tensorflow.keras.metrics.CategoricalAccuracy", "tensorflow.keras.metrics.AUC", "tensorflow.keras.metrics.Precision", "tensorflow.keras.metrics.Recall", "tensorflow_addons.metrics.F1Score", "tensorflow_addons.metrics.F1Score", "tensorflow_addons.metrics.F1Score", "tensorflow.keras.metrics.Accuracy", "tensorflow.keras.metrics.TruePositives", "tensorflow.keras.metrics.FalsePositives", "tensorflow.keras.metrics.TrueNegatives", "tensorflow.keras.metrics.FalseNegatives"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.int_to_label"], ["def", "__init__", "(", "self", ",", "log_dir", ",", "method", ",", "source_datasets", ",", "target_dataset", ",", "\n", "target_domain", "=", "True", ")", ":", "\n", "        ", "self", ".", "writer", "=", "tf", ".", "summary", ".", "create_file_writer", "(", "log_dir", ")", "\n", "self", ".", "method", "=", "method", "\n", "self", ".", "source_datasets", "=", "source_datasets", "\n", "self", ".", "target_dataset", "=", "target_dataset", "\n", "# Just take from first one since they were checked earlier that they're", "\n", "# all the same", "\n", "self", ".", "num_classes", "=", "source_datasets", "[", "0", "]", ".", "num_classes", "\n", "# We could get the source dataset num_domains, but really we want the", "\n", "# number of softmax outputs of the method's domain classifier.", "\n", "self", ".", "domain_outputs", "=", "method", ".", "domain_outputs", "\n", "\n", "self", ".", "datasets", "=", "[", "\"training\"", ",", "\"validation\"", "]", "\n", "self", ".", "target_domain", "=", "target_domain", "# whether we have just source or both", "\n", "\n", "if", "not", "target_domain", ":", "\n", "            ", "self", ".", "domains", "=", "[", "\"source\"", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "domains", "=", "[", "\"source\"", ",", "\"target\"", "]", "\n", "\n", "", "self", ".", "classifiers", "=", "[", "\"task\"", "]", "\n", "\n", "# Create all entire-batch metrics", "\n", "self", ".", "batch_metrics", "=", "{", "dataset", ":", "{", "}", "for", "dataset", "in", "self", ".", "datasets", "}", "\n", "for", "domain", "in", "self", ".", "domains", ":", "\n", "            ", "for", "dataset", "in", "self", ".", "datasets", ":", "\n", "                ", "n", "=", "\"accuracy_domain/%s/%s\"", "%", "(", "domain", ",", "dataset", ")", "\n", "self", ".", "batch_metrics", "[", "dataset", "]", "[", "n", "]", "=", "tf", ".", "keras", ".", "metrics", ".", "CategoricalAccuracy", "(", "name", "=", "n", ")", "\n", "\n", "for", "name", "in", "self", ".", "classifiers", ":", "\n", "                    ", "n", "=", "\"accuracy_%s/%s/%s\"", "%", "(", "name", ",", "domain", ",", "dataset", ")", "\n", "self", ".", "batch_metrics", "[", "dataset", "]", "[", "n", "]", "=", "tf", ".", "keras", ".", "metrics", ".", "CategoricalAccuracy", "(", "name", "=", "n", ")", "\n", "\n", "", "", "", "for", "domain", "in", "self", ".", "domains", ":", "\n", "            ", "for", "dataset", "in", "self", ".", "datasets", ":", "\n", "                ", "for", "classifier", "in", "self", ".", "classifiers", ":", "\n", "                    ", "n", "=", "\"auc_%s/%s/%s\"", "%", "(", "classifier", ",", "domain", ",", "dataset", ")", "\n", "self", ".", "batch_metrics", "[", "dataset", "]", "[", "n", "]", "=", "tf", ".", "keras", ".", "metrics", ".", "AUC", "(", "name", "=", "n", ")", "\n", "\n", "n", "=", "\"precision_%s/%s/%s\"", "%", "(", "classifier", ",", "domain", ",", "dataset", ")", "\n", "self", ".", "batch_metrics", "[", "dataset", "]", "[", "n", "]", "=", "tf", ".", "keras", ".", "metrics", ".", "Precision", "(", "name", "=", "n", ")", "\n", "\n", "n", "=", "\"recall_%s/%s/%s\"", "%", "(", "classifier", ",", "domain", ",", "dataset", ")", "\n", "self", ".", "batch_metrics", "[", "dataset", "]", "[", "n", "]", "=", "tf", ".", "keras", ".", "metrics", ".", "Recall", "(", "name", "=", "n", ")", "\n", "\n", "n", "=", "\"f1score_micro_%s/%s/%s\"", "%", "(", "classifier", ",", "domain", ",", "dataset", ")", "\n", "self", ".", "batch_metrics", "[", "dataset", "]", "[", "n", "]", "=", "tfa", ".", "metrics", ".", "F1Score", "(", "\n", "self", ".", "num_classes", ",", "name", "=", "n", ",", "average", "=", "\"micro\"", ")", "\n", "\n", "n", "=", "\"f1score_macro_%s/%s/%s\"", "%", "(", "classifier", ",", "domain", ",", "dataset", ")", "\n", "self", ".", "batch_metrics", "[", "dataset", "]", "[", "n", "]", "=", "tfa", ".", "metrics", ".", "F1Score", "(", "\n", "self", ".", "num_classes", ",", "name", "=", "n", ",", "average", "=", "\"macro\"", ")", "\n", "\n", "n", "=", "\"f1score_weighted_%s/%s/%s\"", "%", "(", "classifier", ",", "domain", ",", "dataset", ")", "\n", "self", ".", "batch_metrics", "[", "dataset", "]", "[", "n", "]", "=", "tfa", ".", "metrics", ".", "F1Score", "(", "\n", "self", ".", "num_classes", ",", "name", "=", "n", ",", "average", "=", "\"weighted\"", ")", "\n", "\n", "# Create all per-class metrics", "\n", "", "", "", "self", ".", "per_class_metrics", "=", "{", "dataset", ":", "{", "}", "for", "dataset", "in", "self", ".", "datasets", "}", "\n", "for", "i", "in", "range", "(", "self", ".", "num_classes", ")", ":", "\n", "            ", "class_name", "=", "self", ".", "source_datasets", "[", "0", "]", ".", "int_to_label", "(", "i", ")", "\n", "\n", "for", "domain", "in", "self", ".", "domains", ":", "\n", "                ", "for", "dataset", "in", "self", ".", "datasets", ":", "\n", "                    ", "for", "classifier", "in", "self", ".", "classifiers", ":", "\n", "                        ", "n", "=", "\"accuracy_%s_class_%s/%s/%s\"", "%", "(", "classifier", ",", "class_name", ",", "domain", ",", "dataset", ")", "\n", "self", ".", "per_class_metrics", "[", "dataset", "]", "[", "n", "]", "=", "tf", ".", "keras", ".", "metrics", ".", "Accuracy", "(", "name", "=", "n", ")", "\n", "\n", "n", "=", "\"rates_%s_class_%s/TP/%s/%s\"", "%", "(", "classifier", ",", "class_name", ",", "domain", ",", "dataset", ")", "\n", "self", ".", "per_class_metrics", "[", "dataset", "]", "[", "n", "]", "=", "tf", ".", "keras", ".", "metrics", ".", "TruePositives", "(", "name", "=", "n", ")", "\n", "\n", "n", "=", "\"rates_%s_class_%s/FP/%s/%s\"", "%", "(", "classifier", ",", "class_name", ",", "domain", ",", "dataset", ")", "\n", "self", ".", "per_class_metrics", "[", "dataset", "]", "[", "n", "]", "=", "tf", ".", "keras", ".", "metrics", ".", "FalsePositives", "(", "name", "=", "n", ")", "\n", "\n", "n", "=", "\"rates_%s_class_%s/TN/%s/%s\"", "%", "(", "classifier", ",", "class_name", ",", "domain", ",", "dataset", ")", "\n", "self", ".", "per_class_metrics", "[", "dataset", "]", "[", "n", "]", "=", "tf", ".", "keras", ".", "metrics", ".", "TrueNegatives", "(", "name", "=", "n", ")", "\n", "\n", "n", "=", "\"rates_%s_class_%s/FN/%s/%s\"", "%", "(", "classifier", ",", "class_name", ",", "domain", ",", "dataset", ")", "\n", "self", ".", "per_class_metrics", "[", "dataset", "]", "[", "n", "]", "=", "tf", ".", "keras", ".", "metrics", ".", "FalseNegatives", "(", "name", "=", "n", ")", "\n", "\n", "# Variable number of losses", "\n", "", "", "", "", "self", ".", "losses", "=", "{", "dataset", ":", "{", "}", "for", "dataset", "in", "self", ".", "datasets", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics._reset_states": [[135, 145], ["metrics.Metrics.batch_metrics[].items", "metrics.Metrics.per_class_metrics[].items", "metrics.Metrics.losses[].items", "metric.reset_states", "metric.reset_states", "metric.reset_states"], "methods", ["None"], ["", "def", "_reset_states", "(", "self", ",", "dataset", ")", ":", "\n", "        ", "\"\"\" Reset states of all the Keras metrics \"\"\"", "\n", "for", "_", ",", "metric", "in", "self", ".", "batch_metrics", "[", "dataset", "]", ".", "items", "(", ")", ":", "\n", "            ", "metric", ".", "reset_states", "(", ")", "\n", "\n", "", "for", "_", ",", "metric", "in", "self", ".", "per_class_metrics", "[", "dataset", "]", ".", "items", "(", ")", ":", "\n", "            ", "metric", ".", "reset_states", "(", ")", "\n", "\n", "", "for", "_", ",", "metric", "in", "self", ".", "losses", "[", "dataset", "]", ".", "items", "(", ")", ":", "\n", "            ", "metric", ".", "reset_states", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics._process_batch": [[146, 177], ["tensorflow.one_hot", "tensorflow.one_hot", "tensorflow.cast", "tensorflow.cast"], "methods", ["None"], ["", "", "def", "_process_batch", "(", "self", ",", "results", ",", "classifier", ",", "domain", ",", "dataset", ")", ":", "\n", "        ", "\"\"\" Update metrics for accuracy over entire batch for domain-dataset \"\"\"", "\n", "task_y_true", ",", "task_y_pred", ",", "domain_y_true", ",", "domain_y_pred", ",", "losses", "=", "results", "\n", "\n", "# Since we are now using sparse", "\n", "domain_y_true", "=", "tf", ".", "one_hot", "(", "tf", ".", "cast", "(", "domain_y_true", ",", "tf", ".", "int32", ")", ",", "self", ".", "domain_outputs", ")", "\n", "\n", "domain_names", "=", "[", "\n", "\"accuracy_domain/%s/%s\"", ",", "\n", "]", "\n", "\n", "for", "n", "in", "domain_names", ":", "\n", "            ", "name", "=", "n", "%", "(", "domain", ",", "dataset", ")", "\n", "self", ".", "batch_metrics", "[", "dataset", "]", "[", "name", "]", "(", "domain_y_true", ",", "domain_y_pred", ")", "\n", "\n", "", "task_names", "=", "[", "\n", "\"accuracy_%s/%s/%s\"", ",", "\n", "\"auc_%s/%s/%s\"", ",", "\n", "\"precision_%s/%s/%s\"", ",", "\n", "\"recall_%s/%s/%s\"", ",", "\n", "\"f1score_micro_%s/%s/%s\"", ",", "\n", "\"f1score_macro_%s/%s/%s\"", ",", "\n", "\"f1score_weighted_%s/%s/%s\"", ",", "\n", "]", "\n", "\n", "# Since we are now using sparse", "\n", "task_y_true", "=", "tf", ".", "one_hot", "(", "tf", ".", "cast", "(", "task_y_true", ",", "tf", ".", "int32", ")", ",", "self", ".", "num_classes", ")", "\n", "\n", "for", "n", "in", "task_names", ":", "\n", "            ", "name", "=", "n", "%", "(", "classifier", ",", "domain", ",", "dataset", ")", "\n", "self", ".", "batch_metrics", "[", "dataset", "]", "[", "name", "]", "(", "task_y_true", ",", "task_y_pred", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics._process_per_class": [[178, 219], ["tensorflow.one_hot", "tensorflow.one_hot", "range", "tensorflow.shape", "tensorflow.cast", "tensorflow.argmax", "metrics.Metrics.source_datasets[].int_to_label", "tensorflow.slice", "tensorflow.slice", "tensorflow.where", "tensorflow.gather", "tensorflow.gather", "tensorflow.equal"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.int_to_label"], ["", "", "def", "_process_per_class", "(", "self", ",", "results", ",", "classifier", ",", "domain", ",", "dataset", ")", ":", "\n", "        ", "\"\"\" Update metrics for accuracy over per-class portions of batch for domain-dataset \"\"\"", "\n", "task_y_true", ",", "task_y_pred", ",", "domain_y_true", ",", "domain_y_pred", ",", "losses", "=", "results", "\n", "batch_size", "=", "tf", ".", "shape", "(", "task_y_true", ")", "[", "0", "]", "\n", "\n", "# Since we are now using sparse", "\n", "task_y_true", "=", "tf", ".", "one_hot", "(", "tf", ".", "cast", "(", "task_y_true", ",", "tf", ".", "int32", ")", ",", "self", ".", "num_classes", ")", "\n", "\n", "# If only predicting a single class (using softmax), then look for the", "\n", "# max value", "\n", "# e.g. [0.2 0.2 0.4 0.2] -> [0 0 1 0]", "\n", "per_class_predictions", "=", "tf", ".", "one_hot", "(", "\n", "tf", ".", "argmax", "(", "task_y_pred", ",", "axis", "=", "-", "1", ")", ",", "self", ".", "num_classes", ")", "\n", "\n", "# List of per-class task metrics to update", "\n", "task_names", "=", "[", "\n", "\"accuracy_%s_class_%s/%s/%s\"", ",", "\n", "\"rates_%s_class_%s/TP/%s/%s\"", ",", "\n", "\"rates_%s_class_%s/FP/%s/%s\"", ",", "\n", "\"rates_%s_class_%s/TN/%s/%s\"", ",", "\n", "\"rates_%s_class_%s/FN/%s/%s\"", ",", "\n", "]", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "num_classes", ")", ":", "\n", "            ", "class_name", "=", "self", ".", "source_datasets", "[", "0", "]", ".", "int_to_label", "(", "i", ")", "\n", "\n", "# Get ith column (all groundtruth/predictions for ith class)", "\n", "y_true", "=", "tf", ".", "slice", "(", "task_y_true", ",", "[", "0", ",", "i", "]", ",", "[", "batch_size", ",", "1", "]", ")", "# if not sparse", "\n", "y_pred", "=", "tf", ".", "slice", "(", "per_class_predictions", ",", "[", "0", ",", "i", "]", ",", "[", "batch_size", ",", "1", "]", ")", "\n", "\n", "# For single-class prediction, we want to first isolate which", "\n", "# examples in the batch were supposed to be class X. Then, of", "\n", "# those, calculate accuracy = correct / total.", "\n", "rows_of_class_y", "=", "tf", ".", "where", "(", "tf", ".", "equal", "(", "y_true", ",", "1", ")", ")", "# i.e. have 1", "\n", "acc_y_true", "=", "tf", ".", "gather", "(", "y_true", ",", "rows_of_class_y", ")", "\n", "acc_y_pred", "=", "tf", ".", "gather", "(", "y_pred", ",", "rows_of_class_y", ")", "\n", "\n", "# Update metrics", "\n", "for", "n", "in", "task_names", ":", "\n", "                ", "name", "=", "n", "%", "(", "classifier", ",", "class_name", ",", "domain", ",", "dataset", ")", "\n", "self", ".", "per_class_metrics", "[", "dataset", "]", "[", "name", "]", "(", "acc_y_true", ",", "acc_y_pred", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics._process_losses": [[220, 241], ["enumerate", "isinstance", "len", "len", "tensorflow.keras.metrics.Mean"], "methods", ["None"], ["", "", "", "def", "_process_losses", "(", "self", ",", "results", ",", "domain", ",", "dataset", ")", ":", "\n", "        ", "\"\"\" Update losses, but create if it hasn't been seen before. Create here\n        rather than in __init__ since different methods have different numbers\n        of losses, e.g. some have a domain loss and others don't \"\"\"", "\n", "task_y_true", ",", "task_y_pred", ",", "domain_y_true", ",", "domain_y_pred", ",", "losses", "=", "results", "\n", "\n", "# If only one loss, then make it a list so we can use the same code", "\n", "# for either case", "\n", "if", "not", "isinstance", "(", "losses", ",", "list", ")", ":", "\n", "            ", "losses", "=", "[", "losses", "]", "\n", "\n", "", "assert", "len", "(", "self", ".", "method", ".", "loss_names", ")", ">=", "len", "(", "losses", ")", ",", "\"not enough loss_names defined in method\"", "\n", "\n", "for", "i", ",", "loss", "in", "enumerate", "(", "losses", ")", ":", "\n", "            ", "name", "=", "\"loss/\"", "+", "self", ".", "method", ".", "loss_names", "[", "i", "]", "+", "\"/\"", "+", "domain", "+", "\"/\"", "+", "dataset", "\n", "\n", "if", "name", "not", "in", "self", ".", "losses", "[", "dataset", "]", ":", "\n", "                ", "self", ".", "losses", "[", "dataset", "]", "[", "name", "]", "=", "tf", ".", "keras", ".", "metrics", ".", "Mean", "(", "name", "=", "name", ")", "\n", "\n", "", "self", ".", "losses", "[", "dataset", "]", "[", "name", "]", "(", "loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics._write_data": [[242, 278], ["metrics.Metrics.writer.flush", "str", "metrics.Metrics.writer.as_default", "metrics.Metrics.batch_metrics[].items", "metrics.Metrics.per_class_metrics[].items", "metrics.Metrics.losses[].items", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "enumerate", "tensorflow.summary.scalar", "metric.result", "metric.result", "metric.result", "tensorflow.summary.scalar", "isinstance", "name.numpy().decode.numpy().decode.numpy().decode", "name.numpy().decode.numpy().decode.numpy"], "methods", ["None"], ["", "", "def", "_write_data", "(", "self", ",", "step", ",", "dataset", ",", "eval_time", ",", "train_time", "=", "None", ",", "\n", "additional_losses", "=", "None", ")", ":", "\n", "        ", "\"\"\" Write either the training or validation data \"\"\"", "\n", "assert", "dataset", "in", "self", ".", "datasets", ",", "\"unknown dataset \"", "+", "str", "(", "dataset", ")", "\n", "\n", "# Write all the values to the file", "\n", "with", "self", ".", "writer", ".", "as_default", "(", ")", ":", "\n", "            ", "for", "key", ",", "metric", "in", "self", ".", "batch_metrics", "[", "dataset", "]", ".", "items", "(", ")", ":", "\n", "                ", "tf", ".", "summary", ".", "scalar", "(", "key", ",", "metric", ".", "result", "(", ")", ",", "step", "=", "step", ")", "\n", "\n", "", "for", "key", ",", "metric", "in", "self", ".", "per_class_metrics", "[", "dataset", "]", ".", "items", "(", ")", ":", "\n", "                ", "tf", ".", "summary", ".", "scalar", "(", "key", ",", "metric", ".", "result", "(", ")", ",", "step", "=", "step", ")", "\n", "\n", "", "for", "key", ",", "metric", "in", "self", ".", "losses", "[", "dataset", "]", ".", "items", "(", ")", ":", "\n", "                ", "tf", ".", "summary", ".", "scalar", "(", "key", ",", "metric", ".", "result", "(", ")", ",", "step", "=", "step", ")", "\n", "\n", "# Any other losses", "\n", "", "if", "additional_losses", "is", "not", "None", ":", "\n", "                ", "names", ",", "values", "=", "additional_losses", "\n", "\n", "for", "i", ",", "name", "in", "enumerate", "(", "names", ")", ":", "\n", "# If TensorFlow string (when using tf.function), get the", "\n", "# value from it", "\n", "                    ", "if", "not", "isinstance", "(", "name", ",", "str", ")", ":", "\n", "                        ", "name", "=", "name", ".", "numpy", "(", ")", ".", "decode", "(", "\"utf-8\"", ")", "\n", "\n", "", "tf", ".", "summary", ".", "scalar", "(", "\"loss/%s\"", "%", "(", "name", ")", ",", "values", "[", "i", "]", ",", "step", "=", "step", ")", "\n", "\n", "# Regardless of mapping/task, log times", "\n", "", "", "tf", ".", "summary", ".", "scalar", "(", "\"step_time/metrics/%s\"", "%", "(", "dataset", ")", ",", "eval_time", ",", "step", "=", "step", ")", "\n", "\n", "if", "train_time", "is", "not", "None", ":", "\n", "                ", "tf", ".", "summary", ".", "scalar", "(", "\"step_time/%s\"", "%", "(", "dataset", ")", ",", "train_time", ",", "step", "=", "step", ")", "\n", "\n", "# Make sure we sync to disk", "\n", "", "", "self", ".", "writer", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics._run_dataset": [[279, 311], ["enumerate", "iter", "iter", "metrics.Metrics.method.get_next_batch_single", "metrics.Metrics._run_single_batch", "metrics.Metrics.method.get_next_batch_single", "metrics.Metrics._run_single_batch", "next", "next"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.get_next_batch_single", "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics._run_single_batch", "home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.get_next_batch_single", "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics._run_single_batch"], ["", "def", "_run_dataset", "(", "self", ",", "data_a", ",", "data_b", ",", "dataset", ")", ":", "\n", "        ", "\"\"\" Run all the data A/B through the model -- data_a and data_b\n        should both be of type tf.data.Dataset (with data_a a list of them) \"\"\"", "\n", "if", "data_a", "is", "not", "None", ":", "\n", "            ", "source_iterators", "=", "[", "iter", "(", "x", ")", "for", "x", "in", "data_a", "]", "\n", "\n", "# Do each source domain individually since they may have different", "\n", "# amounts of data and this loop will break as soon as the smallest", "\n", "# one has no data.", "\n", "for", "i", ",", "source_iter", "in", "enumerate", "(", "source_iterators", ")", ":", "\n", "                ", "while", "True", ":", "\n", "                    ", "try", ":", "\n", "                        ", "data", "=", "self", ".", "method", ".", "get_next_batch_single", "(", "\n", "next", "(", "source_iter", ")", ",", "is_target", "=", "False", ",", "index", "=", "i", ")", "\n", "self", ".", "_run_single_batch", "(", "data", ",", "dataset", ",", "\"source\"", ",", "\n", "is_single_domain", "=", "True", ")", "\n", "", "except", "StopIteration", ":", "\n", "                        ", "break", "\n", "\n", "", "", "", "", "if", "self", ".", "target_domain", "and", "data_b", "is", "not", "None", ":", "\n", "            ", "target_iter", "=", "iter", "(", "data_b", ")", "\n", "\n", "while", "True", ":", "\n", "                ", "try", ":", "\n", "                    ", "data", "=", "self", ".", "method", ".", "get_next_batch_single", "(", "\n", "next", "(", "target_iter", ")", ",", "is_target", "=", "True", ")", "\n", "# Target data is always a single domain since this is", "\n", "# single-target", "\n", "self", ".", "_run_single_batch", "(", "data", ",", "dataset", ",", "\"target\"", ",", "\n", "is_single_domain", "=", "True", ")", "\n", "", "except", "StopIteration", ":", "\n", "                    ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics._run_batch": [[312, 324], ["metrics.Metrics._run_single_batch", "metrics.Metrics._run_single_batch"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics._run_single_batch", "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics._run_single_batch"], ["", "", "", "", "def", "_run_batch", "(", "self", ",", "data_a", ",", "data_b", ",", "dataset", ")", ":", "\n", "        ", "\"\"\" Run a single batch of A/B data through the model -- data_a and data_b\n        should both be a tuple of (x, task_y_true, domain_y_true) \"\"\"", "\n", "if", "data_a", "is", "not", "None", ":", "\n", "# A training batch consists of data from multiple domains", "\n", "            ", "self", ".", "_run_single_batch", "(", "data_a", ",", "dataset", ",", "\"source\"", ",", "\n", "is_single_domain", "=", "False", ")", "\n", "\n", "", "if", "self", ".", "target_domain", "and", "data_b", "is", "not", "None", ":", "\n", "# Target data is always a single domain since this is single-target", "\n", "            ", "self", ".", "_run_single_batch", "(", "data_b", ",", "dataset", ",", "\"target\"", ",", "\n", "is_single_domain", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics._run_single_batch": [[325, 337], ["metrics.Metrics.method.eval_step", "metrics.Metrics._process_batch", "metrics.Metrics._process_per_class", "metrics.Metrics._process_losses", "str", "str"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.methods.MethodBase.eval_step", "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics._process_batch", "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics._process_per_class", "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics._process_losses"], ["", "", "def", "_run_single_batch", "(", "self", ",", "data", ",", "dataset_name", ",", "domain_name", ",", "is_single_domain", ")", ":", "\n", "        ", "\"\"\" Run a single batch of data through the model \"\"\"", "\n", "assert", "dataset_name", "in", "self", ".", "datasets", ",", "\"unknown dataset \"", "+", "str", "(", "dataset_name", ")", "\n", "assert", "domain_name", "in", "self", ".", "domains", ",", "\"unknown domain \"", "+", "str", "(", "domain_name", ")", "\n", "\n", "is_target", "=", "domain_name", "==", "\"target\"", "\n", "results", "=", "self", ".", "method", ".", "eval_step", "(", "data", ",", "is_target", ",", "is_single_domain", ")", "\n", "\n", "classifier", "=", "\"task\"", "# Which classifier's task_y_pred are we looking at?", "\n", "self", ".", "_process_batch", "(", "results", ",", "classifier", ",", "domain_name", ",", "dataset_name", ")", "\n", "self", ".", "_process_per_class", "(", "results", ",", "classifier", ",", "domain_name", ",", "dataset_name", ")", "\n", "self", ".", "_process_losses", "(", "results", ",", "domain_name", ",", "dataset_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics.train": [[338, 355], ["metrics.Metrics._reset_states", "time.time", "metrics.Metrics._run_batch", "int", "metrics.Metrics._write_data", "time.time"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics._reset_states", "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics._run_batch", "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics._write_data"], ["", "def", "train", "(", "self", ",", "data_a", ",", "data_b", ",", "step", ",", "train_time", ")", ":", "\n", "        ", "\"\"\"\n        Evaluate the model on a batch of training data (during training,\n        not at evaluation time -- use train_eval() for that)\n        \"\"\"", "\n", "dataset", "=", "\"training\"", "\n", "self", ".", "_reset_states", "(", "dataset", ")", "\n", "t", "=", "time", ".", "time", "(", ")", "\n", "\n", "if", "not", "self", ".", "target_domain", ":", "\n", "            ", "data_b", "=", "None", "\n", "\n", "", "self", ".", "_run_batch", "(", "data_a", ",", "data_b", ",", "dataset", ")", "\n", "\n", "t", "=", "time", ".", "time", "(", ")", "-", "t", "\n", "step", "=", "int", "(", "step", ")", "\n", "self", ".", "_write_data", "(", "step", ",", "dataset", ",", "t", ",", "train_time", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics.train_eval": [[356, 372], ["metrics.Metrics._reset_states", "metrics.Metrics._run_dataset"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics._reset_states", "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics._run_dataset"], ["", "def", "train_eval", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Evaluate the model on the entire training dataset, for use during\n        evaluation -- not at training time\n        \"\"\"", "\n", "dataset", "=", "\"training\"", "\n", "self", ".", "_reset_states", "(", "dataset", ")", "\n", "\n", "if", "self", ".", "target_domain", ":", "\n", "            ", "target_datasets", "=", "self", ".", "method", ".", "target_train_eval_dataset", "\n", "", "else", ":", "\n", "            ", "target_datasets", "=", "None", "\n", "\n", "# At evaluation time, use eval tf.data.Dataset", "\n", "", "self", ".", "_run_dataset", "(", "self", ".", "method", ".", "source_train_eval_datasets", ",", "\n", "target_datasets", ",", "dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics.test": [[373, 412], ["metrics.Metrics._reset_states", "time.time", "metrics.Metrics._run_dataset", "float", "acc_source.result", "float", "time.time", "int", "metrics.Metrics._write_data", "acc_target.result"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics._reset_states", "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics._run_dataset", "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics._write_data"], ["", "def", "test", "(", "self", ",", "step", "=", "None", ",", "evaluation", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Evaluate the model on domain A/B but batched to make sure we don't run\n        out of memory\n\n        Note: leave off step if evaluation=True\n\n        Returns: source task validation accuracy, target task validation accuracy\n        \"\"\"", "\n", "dataset", "=", "\"validation\"", "\n", "self", ".", "_reset_states", "(", "dataset", ")", "\n", "t", "=", "time", ".", "time", "(", ")", "\n", "\n", "if", "self", ".", "target_domain", ":", "\n", "            ", "target_datasets", "=", "self", ".", "method", ".", "target_test_eval_dataset", "\n", "", "else", ":", "\n", "            ", "target_datasets", "=", "None", "\n", "\n", "", "self", ".", "_run_dataset", "(", "self", ".", "method", ".", "source_test_eval_datasets", ",", "\n", "target_datasets", ",", "dataset", ")", "\n", "\n", "# We use the validation accuracy to save the best model", "\n", "acc_source", "=", "self", ".", "batch_metrics", "[", "\"validation\"", "]", "[", "\"accuracy_task/source/validation\"", "]", "\n", "validation_accuracy_source", "=", "float", "(", "acc_source", ".", "result", "(", ")", ")", "\n", "\n", "if", "self", ".", "target_domain", ":", "\n", "            ", "acc_target", "=", "self", ".", "batch_metrics", "[", "\"validation\"", "]", "[", "\"accuracy_task/target/validation\"", "]", "\n", "validation_accuracy_target", "=", "float", "(", "acc_target", ".", "result", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "validation_accuracy_target", "=", "None", "\n", "\n", "", "t", "=", "time", ".", "time", "(", ")", "-", "t", "\n", "\n", "if", "not", "evaluation", ":", "\n", "            ", "assert", "step", "is", "not", "None", ",", "\"Must pass step to test() if evaluation=False\"", "\n", "step", "=", "int", "(", "step", ")", "\n", "self", ".", "_write_data", "(", "step", ",", "dataset", ",", "t", ")", "\n", "\n", "", "return", "validation_accuracy_source", ",", "validation_accuracy_target", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics.results": [[413, 428], ["metrics.Metrics.batch_metrics[].items", "metrics.Metrics.per_class_metrics[].items", "metrics.Metrics.losses[].items", "float", "float", "float", "metric.result", "metric.result", "metric.result"], "methods", ["None"], ["", "def", "results", "(", "self", ")", ":", "\n", "        ", "\"\"\" Returns one dictionary of all the current metric results (floats) \"\"\"", "\n", "results", "=", "{", "}", "\n", "\n", "for", "dataset", "in", "self", ".", "datasets", ":", "\n", "            ", "for", "key", ",", "metric", "in", "self", ".", "batch_metrics", "[", "dataset", "]", ".", "items", "(", ")", ":", "\n", "                ", "results", "[", "key", "]", "=", "float", "(", "metric", ".", "result", "(", ")", ")", "\n", "\n", "", "for", "key", ",", "metric", "in", "self", ".", "per_class_metrics", "[", "dataset", "]", ".", "items", "(", ")", ":", "\n", "                ", "results", "[", "key", "]", "=", "float", "(", "metric", ".", "result", "(", ")", ")", "\n", "\n", "", "for", "key", ",", "metric", "in", "self", ".", "losses", "[", "dataset", "]", ".", "items", "(", ")", ":", "\n", "                ", "results", "[", "key", "]", "=", "float", "(", "metric", ".", "result", "(", ")", ")", "\n", "\n", "", "", "return", "results", "\n", "", "", ""]], "home.repos.pwc.inspect_result.floft_calda.None.analysis.get_tuning_files": [[125, 139], ["pathlib.Path().glob", "m.stem.replace", "str", "files.append", "pathlib.Path"], "function", ["None"], ["def", "get_tuning_files", "(", "dir_name", ",", "prefixes", ")", ":", "\n", "    ", "\"\"\" Get all the hyperparameter evaluation result files \"\"\"", "\n", "files", "=", "[", "]", "\n", "matching", "=", "[", "]", "\n", "\n", "for", "prefix", "in", "prefixes", ":", "\n", "        ", "matching", "+=", "pathlib", ".", "Path", "(", "dir_name", ")", ".", "glob", "(", "prefix", "+", "\".yaml\"", ")", "\n", "\n", "", "for", "m", "in", "matching", ":", "\n", "        ", "name", "=", "m", ".", "stem", ".", "replace", "(", "prefix", ",", "\"\"", ")", "\n", "file", "=", "str", "(", "m", ")", "\n", "files", ".", "append", "(", "(", "name", ",", "file", ")", ")", "\n", "\n", "", "return", "files", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.analysis.compute_average": [[141, 166], ["len", "numpy.array", "np.array.append", "print", "np.array.mean", "np.array.std", "str", "str"], "function", ["None"], ["", "def", "compute_average", "(", "name", ",", "data", ",", "metric", ",", "domain", ",", "train_or_valid", ")", ":", "\n", "    ", "results", "=", "[", "]", "\n", "\n", "for", "d", "in", "data", ":", "\n", "# Make sure this value exists in the evaluation results .yaml file", "\n", "        ", "assert", "\"results\"", "in", "d", ",", "\"No results in: \"", "+", "str", "(", "d", ")", "+", "\" for \"", "+", "name", "\n", "name_of_value", "=", "metric", "+", "\"_task/\"", "+", "domain", "+", "\"/\"", "+", "train_or_valid", "\n", "assert", "name_of_value", "in", "d", "[", "\"results\"", "]", ",", "\"No metric value \"", "+", "name_of_value", "+", "\" in: \"", "+", "str", "(", "d", "[", "\"results\"", "]", ")", "+", "\" for \"", "+", "name", "\n", "\n", "result", "=", "d", "[", "\"results\"", "]", "[", "name_of_value", "]", "\n", "results", ".", "append", "(", "result", ")", "\n", "\n", "# There should be 1 or 3 of each; if not, warn", "\n", "", "length", "=", "len", "(", "results", ")", "\n", "\n", "if", "length", "!=", "1", "and", "length", "!=", "3", ":", "\n", "        ", "print", "(", "\"Warning: number of runs \"", ",", "length", ",", "\"(not 1 or 3) for\"", ",", "name", ",", "\n", "file", "=", "sys", ".", "stderr", ")", "\n", "\n", "# ddof=0 is the numpy default, ddof=1 is Pandas' default", "\n", "", "results", "=", "np", ".", "array", "(", "results", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "return", "results", ".", "mean", "(", ")", ",", "results", ".", "std", "(", "ddof", "=", "0", ")", ",", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.analysis.get_method": [[168, 178], ["None"], "function", ["None"], ["", "def", "get_method", "(", "method", ",", "target", ")", ":", "\n", "    ", "\"\"\"\n    method=\"upper\" doesn't actually exist since it uses method=\"none\", but\n    our upper bound is method=\"none\" without any target domains, so set\n    appropriately.\n    \"\"\"", "\n", "if", "method", "==", "\"none\"", "and", "target", "==", "\"\"", ":", "\n", "        ", "method", "=", "\"upper\"", "\n", "\n", "", "return", "method", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.analysis._all_stats": [[180, 337], ["int", "analysis.compute_average", "pickle_data.load_pickle", "open", "yaml.load", "len", "print", "analysis.get_method", "int", "int.replace", "pickle_data.save_pickle", "str", "str", "str", "str", "str", "str", "str", "str", "str", "int", "int", "shared_modalities.split", "source_modality_subset.split", "target_modality_subset.split", "tuple", "tuple", "tuple"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.analysis.compute_average", "home.repos.pwc.inspect_result.floft_calda.None.pickle_data.load_pickle", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.load", "home.repos.pwc.inspect_result.floft_calda.None.analysis.get_method", "home.repos.pwc.inspect_result.floft_calda.None.pickle_data.save_pickle"], ["", "def", "_all_stats", "(", "name", ",", "filename", ",", "source_feature_subset", ",", "target_feature_subset", ",", "\n", "pickle", "=", "True", ")", ":", "\n", "# For speed, if we already loaded this and generated the pickle file,", "\n", "# load that instead", "\n", "    ", "if", "pickle", ":", "\n", "        ", "pickle_filename", "=", "\"{}.pickle\"", ".", "format", "(", "filename", ")", "\n", "results", "=", "load_pickle", "(", "pickle_filename", ")", "\n", "\n", "if", "results", "is", "not", "None", ":", "\n", "            ", "return", "results", "\n", "\n", "", "", "with", "open", "(", "filename", ")", "as", "f", ":", "\n", "# See: https://github.com/yaml/pyyaml/wiki/PyYAML-yaml.load(input)-Deprecation", "\n", "        ", "data", "=", "yaml", ".", "load", "(", "f", ",", "Loader", "=", "yaml", ".", "SafeLoader", ")", "\n", "\n", "# Get some of the config", "\n", "", "uid", "=", "None", "\n", "dataset", "=", "None", "\n", "method", "=", "None", "\n", "sources", "=", "None", "\n", "target", "=", "None", "\n", "source_modality_subset", "=", "None", "\n", "target_modality_subset", "=", "None", "\n", "shared_modalities", "=", "None", "\n", "similarity_weight", "=", "None", "\n", "has_results", "=", "False", "\n", "\n", "if", "len", "(", "data", ")", "==", "0", ":", "\n", "        ", "print", "(", "\"Warning: no data in file\"", ",", "filename", ")", "\n", "return", "{", "}", "\n", "\n", "", "for", "d", "in", "data", ":", "\n", "        ", "config", "=", "d", "[", "\"config\"", "]", "\n", "\n", "assert", "uid", "is", "None", "or", "config", "[", "\"uid\"", "]", "==", "uid", ",", "\"runs disagree on uid: \"", "+", "config", "[", "\"uid\"", "]", "+", "\" vs. \"", "+", "str", "(", "uid", ")", "\n", "uid", "=", "config", "[", "\"uid\"", "]", "\n", "\n", "assert", "dataset", "is", "None", "or", "config", "[", "\"dataset\"", "]", "==", "dataset", ",", "\"runs disagree on dataset: \"", "+", "config", "[", "\"dataset\"", "]", "+", "\" vs. \"", "+", "str", "(", "dataset", ")", "\n", "dataset", "=", "config", "[", "\"dataset\"", "]", "\n", "\n", "assert", "sources", "is", "None", "or", "config", "[", "\"sources\"", "]", "==", "sources", ",", "\"runs disagree on sources: \"", "+", "config", "[", "\"sources\"", "]", "+", "\" vs. \"", "+", "str", "(", "sources", ")", "\n", "sources", "=", "config", "[", "\"sources\"", "]", "\n", "\n", "assert", "target", "is", "None", "or", "config", "[", "\"target\"", "]", "==", "target", ",", "\"runs disagree on target: \"", "+", "config", "[", "\"target\"", "]", "+", "\" vs. \"", "+", "str", "(", "target", ")", "\n", "target", "=", "config", "[", "\"target\"", "]", "\n", "\n", "new_method", "=", "get_method", "(", "config", "[", "\"method\"", "]", ",", "target", ")", "\n", "assert", "method", "is", "None", "or", "new_method", "==", "method", ",", "\"runs disagree on method: \"", "+", "new_method", "+", "\" vs. \"", "+", "str", "(", "method", ")", "\n", "method", "=", "new_method", "\n", "\n", "assert", "source_modality_subset", "is", "None", "or", "config", "[", "\"source_modality_subset\"", "]", "==", "source_modality_subset", ",", "\"runs disagree on source_modality_subset: \"", "+", "config", "[", "\"source_modality_subset\"", "]", "+", "\" vs. \"", "+", "str", "(", "source_modality_subset", ")", "\n", "source_modality_subset", "=", "config", "[", "\"source_modality_subset\"", "]", "\n", "\n", "assert", "target_modality_subset", "is", "None", "or", "config", "[", "\"target_modality_subset\"", "]", "==", "target_modality_subset", ",", "\"runs disagree on target_modality_subset: \"", "+", "config", "[", "\"target_modality_subset\"", "]", "+", "\" vs. \"", "+", "str", "(", "target_modality_subset", ")", "\n", "target_modality_subset", "=", "config", "[", "\"target_modality_subset\"", "]", "\n", "\n", "assert", "shared_modalities", "is", "None", "or", "config", "[", "\"shared_modalities\"", "]", "==", "shared_modalities", ",", "\"runs disagree on shared_modalities: \"", "+", "config", "[", "\"shared_modalities\"", "]", "+", "\" vs. \"", "+", "str", "(", "shared_modalities", ")", "\n", "shared_modalities", "=", "config", "[", "\"shared_modalities\"", "]", "\n", "\n", "assert", "similarity_weight", "is", "None", "or", "config", "[", "\"similarity_weight\"", "]", "==", "similarity_weight", ",", "\"runs disagree on similarity_weight: \"", "+", "config", "[", "\"similarity_weight\"", "]", "+", "\" vs. \"", "+", "str", "(", "similarity_weight", ")", "\n", "similarity_weight", "=", "config", "[", "\"similarity_weight\"", "]", "\n", "\n", "# Skip if not the right source/target features", "\n", "current_source_feature_subset", "=", "config", "[", "\"source_feature_subset\"", "]", "\n", "current_target_feature_subset", "=", "config", "[", "\"target_feature_subset\"", "]", "\n", "\n", "if", "source_feature_subset", "is", "not", "None", "and", "source_feature_subset", "!=", "current_source_feature_subset", ":", "\n", "            ", "return", "{", "}", "\n", "\n", "", "if", "target_feature_subset", "is", "not", "None", "and", "target_feature_subset", "!=", "current_target_feature_subset", ":", "\n", "            ", "return", "{", "}", "\n", "\n", "", "if", "d", "[", "\"results\"", "]", "!=", "{", "}", ":", "\n", "            ", "has_results", "=", "True", "\n", "\n", "# Convert to lists of integers", "\n", "", "", "if", "source_modality_subset", "==", "\"\"", ":", "\n", "        ", "source_modality_subset", "=", "None", "\n", "", "else", ":", "\n", "        ", "source_modality_subset", "=", "[", "int", "(", "x", ")", "for", "x", "in", "source_modality_subset", ".", "split", "(", "\",\"", ")", "]", "\n", "\n", "", "if", "target_modality_subset", "==", "\"\"", ":", "\n", "        ", "target_modality_subset", "=", "None", "\n", "", "else", ":", "\n", "        ", "target_modality_subset", "=", "[", "int", "(", "x", ")", "for", "x", "in", "target_modality_subset", ".", "split", "(", "\",\"", ")", "]", "\n", "\n", "", "shared_modalities", "=", "[", "int", "(", "x", ")", "for", "x", "in", "shared_modalities", ".", "split", "(", "\",\"", ")", "]", "\n", "\n", "# Also replace u0, etc. with just \"0\"", "\n", "uid", "=", "int", "(", "uid", ".", "replace", "(", "\"u\"", ",", "\"\"", ")", ")", "\n", "\n", "# Identify problem based on modality subsets and shared modalities", "\n", "# Note: convert list to tuple so it's hashable in dictionary", "\n", "problem_name", "=", "problem_names", "[", "(", "\n", "tuple", "(", "source_modality_subset", ")", "if", "source_modality_subset", "is", "not", "None", "else", "None", ",", "\n", "tuple", "(", "target_modality_subset", ")", "if", "target_modality_subset", "is", "not", "None", "else", "None", ",", "\n", "tuple", "(", "shared_modalities", ")", "\n", ")", "]", "\n", "\n", "results", "=", "{", "\n", "\"name\"", ":", "name", ",", "\n", "\"problem\"", ":", "problem_name", ",", "\n", "\"dataset\"", ":", "dataset", ",", "\n", "\"method\"", ":", "method", ",", "\n", "\"sources\"", ":", "sources", ",", "\n", "\"target\"", ":", "target", ",", "\n", "\"similarity_weight\"", ":", "similarity_weight", ",", "\n", "\"uid\"", ":", "uid", ",", "\n", "# Full data if we need it", "\n", "\"data\"", ":", "data", ",", "\n", "}", "\n", "\n", "# For upper bound, there's no target, so instead use the \"source\" value", "\n", "# as the \"target\" value", "\n", "if", "method", "==", "\"upper\"", ":", "\n", "        ", "source_or_target", "=", "\"source\"", "\n", "", "else", ":", "\n", "        ", "source_or_target", "=", "\"target\"", "\n", "\n", "# results[\"results_source_train\"] = compute_average(name, data, FLAGS.metric, \"source\", \"training\")", "\n", "# results[\"results_source_test\"] = compute_average(name, data, FLAGS.metric, \"source\", \"validation\")", "\n", "# results[\"results_target_train\"] = compute_average(name, data, FLAGS.metric, \"target\", \"training\")", "\n", "\n", "# Would error if we tried computing average with no data", "\n", "", "if", "not", "has_results", ":", "\n", "        ", "return", "{", "}", "\n", "\n", "", "results", "[", "\"results_target_test\"", "]", "=", "compute_average", "(", "name", ",", "data", ",", "FLAGS", ".", "metric", ",", "source_or_target", ",", "\"validation\"", ")", "\n", "\n", "# Cache results", "\n", "if", "pickle", ":", "\n", "        ", "save_pickle", "(", "pickle_filename", ",", "results", ")", "\n", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.analysis.all_stats": [[339, 367], ["pool.run_job_pool.sort", "pool.run_job_pool", "pool.run_job_pool.append", "commands.append", "analysis._all_stats"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.pool.run_job_pool", "home.repos.pwc.inspect_result.floft_calda.None.analysis._all_stats"], ["", "def", "all_stats", "(", "files", ",", "source_feature_subset", ",", "target_feature_subset", ",", "\n", "show_progress", "=", "True", ")", ":", "\n", "    ", "\"\"\" Process all files, but since we may have many, many thousands, do it\n    with multiple cores by default \"\"\"", "\n", "if", "FLAGS", ".", "jobs", "==", "1", ":", "\n", "        ", "results", "=", "[", "]", "\n", "\n", "for", "name", ",", "filename", "in", "files", ":", "\n", "            ", "results", ".", "append", "(", "_all_stats", "(", "name", ",", "filename", ",", "\n", "source_feature_subset", ",", "target_feature_subset", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "commands", "=", "[", "]", "\n", "\n", "for", "name", ",", "filename", "in", "files", ":", "\n", "            ", "commands", ".", "append", "(", "(", "name", ",", "filename", ",", "source_feature_subset", ",", "\n", "target_feature_subset", ")", ")", "\n", "\n", "", "jobs", "=", "FLAGS", ".", "jobs", "if", "FLAGS", ".", "jobs", "!=", "0", "else", "None", "\n", "results", "=", "run_job_pool", "(", "_all_stats", ",", "commands", ",", "cores", "=", "jobs", ",", "\n", "show_progress", "=", "show_progress", ")", "\n", "\n", "# Remove empty dictionaries (the \"no data\" cases)", "\n", "", "results", "=", "[", "r", "for", "r", "in", "results", "if", "r", "!=", "{", "}", "]", "\n", "\n", "# Sort by name", "\n", "results", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "\"name\"", "]", ")", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.analysis.get_results": [[369, 424], ["analysis.get_tuning_files", "analysis.all_stats", "collections.defaultdict", "collections.defaultdict.items", "results_grouped[].append", "len", "max", "max_validation_accuracies.index", "tuned_results.append", "tuned_results.append", "max_validation_accuracies.append", "len", "print"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.analysis.get_tuning_files", "home.repos.pwc.inspect_result.floft_calda.None.analysis.all_stats"], ["", "def", "get_results", "(", "run_suffixes", ",", "variant_match", ",", "source_feature_subset", ",", "\n", "target_feature_subset", ",", "tune", ",", "folder", "=", "\"results\"", ",", "additional_match", "=", "\"*\"", ",", "\n", "show_progress", "=", "True", ")", ":", "\n", "    ", "\"\"\" Get the right result files and load them \"\"\"", "\n", "prefixes", "=", "[", "\n", "\"results_\"", "+", "run_suffix", "+", "\"_\"", "+", "variant_match", "+", "\"-\"", "+", "additional_match", "\n", "for", "run_suffix", "in", "run_suffixes", "\n", "]", "\n", "files", "=", "get_tuning_files", "(", "folder", ",", "prefixes", ")", "\n", "results", "=", "all_stats", "(", "files", ",", "source_feature_subset", ",", "target_feature_subset", ",", "\n", "show_progress", "=", "show_progress", ")", "\n", "\n", "# If there's multiple runs with different weights, we want to pick the", "\n", "# result from the one with the best validation results, i.e. a grid search", "\n", "# for hyperparameter tuning that variable", "\n", "if", "tune", ":", "\n", "        ", "results_grouped", "=", "collections", ".", "defaultdict", "(", "lambda", ":", "[", "]", ")", "\n", "\n", "for", "result", "in", "results", ":", "\n", "            ", "results_grouped", "[", "(", "\n", "result", "[", "\"problem\"", "]", ",", "\n", "result", "[", "\"dataset\"", "]", ",", "\n", "result", "[", "\"method\"", "]", ",", "\n", "result", "[", "\"uid\"", "]", ",", "\n", "# uid handles the unique sources/targets", "\n", "#result[\"sources\"],", "\n", "#result[\"target\"],", "\n", "# We don't include this since we want to group by this", "\n", "#result[\"similarity_weight\"]", "\n", ")", "]", ".", "append", "(", "result", ")", "\n", "\n", "", "tuned_results", "=", "[", "]", "\n", "\n", "for", "name", ",", "result", "in", "results_grouped", ".", "items", "(", ")", ":", "\n", "            ", "if", "len", "(", "result", ")", ">", "1", ":", "\n", "# Find the one that had the highest max_accuracy, i.e. on the", "\n", "# validation data performed the best", "\n", "                ", "max_validation_accuracies", "=", "[", "]", "\n", "\n", "for", "r", "in", "result", ":", "\n", "                    ", "if", "len", "(", "r", "[", "\"data\"", "]", ")", ">", "1", ":", "\n", "                        ", "print", "(", "\"Warning: found multiple runs for a single weight? using first\"", ")", "\n", "\n", "", "max_validation_accuracies", ".", "append", "(", "r", "[", "\"data\"", "]", "[", "0", "]", "[", "\"max_accuracy\"", "]", ")", "\n", "\n", "", "max_accuracy", "=", "max", "(", "max_validation_accuracies", ")", "\n", "index", "=", "max_validation_accuracies", ".", "index", "(", "max_accuracy", ")", "\n", "# We'll use the results from that one", "\n", "tuned_results", ".", "append", "(", "result", "[", "index", "]", ")", "\n", "", "else", ":", "\n", "                ", "tuned_results", ".", "append", "(", "result", "[", "0", "]", ")", "\n", "\n", "", "", "return", "tuned_results", "\n", "", "else", ":", "\n", "        ", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.analysis.gen_jitter": [[426, 436], ["range", "numpy.array", "x.append"], "function", ["None"], ["", "", "def", "gen_jitter", "(", "length", ",", "amount", "=", "0.04", ")", ":", "\n", "    ", "\"\"\" \"Dodge\" the points slightly on the x axis, so that they don't overlap \"\"\"", "\n", "x", "=", "[", "]", "\n", "value", "=", "-", "(", "amount", "/", "length", ")", "/", "2", "\n", "\n", "for", "i", "in", "range", "(", "length", ")", ":", "\n", "        ", "x", ".", "append", "(", "value", ")", "\n", "value", "+=", "amount", "\n", "\n", "", "return", "np", ".", "array", "(", "x", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.analysis.export_legend": [[438, 446], ["fig.canvas.draw", "legend.get_window_extent", "bbox.transformed.from_extents", "bbox.transformed.transformed", "fig.savefig", "fig.dpi_scale_trans.inverted", "os.path.join", "numpy.array"], "function", ["None"], ["", "def", "export_legend", "(", "legend", ",", "dir_name", "=", "\".\"", ",", "filename", "=", "\"key.pdf\"", ",", "expand", "=", "[", "-", "5", ",", "-", "5", ",", "5", ",", "5", "]", ")", ":", "\n", "    ", "\"\"\" See: https://stackoverflow.com/a/47749903 \"\"\"", "\n", "fig", "=", "legend", ".", "figure", "\n", "fig", ".", "canvas", ".", "draw", "(", ")", "\n", "bbox", "=", "legend", ".", "get_window_extent", "(", ")", "\n", "bbox", "=", "bbox", ".", "from_extents", "(", "*", "(", "bbox", ".", "extents", "+", "np", ".", "array", "(", "expand", ")", ")", ")", "\n", "bbox", "=", "bbox", ".", "transformed", "(", "fig", ".", "dpi_scale_trans", ".", "inverted", "(", ")", ")", "\n", "fig", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "dir_name", ",", "filename", ")", ",", "dpi", "=", "\"figure\"", ",", "bbox_inches", "=", "bbox", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.analysis.make_replacements": [[448, 457], ["s.replace.replace"], "function", ["None"], ["", "def", "make_replacements", "(", "s", ",", "replacements", ")", ":", "\n", "    ", "\"\"\" Make a bunch of replacements in a string \"\"\"", "\n", "if", "s", "is", "None", ":", "\n", "        ", "return", "s", "\n", "\n", "", "for", "before", ",", "after", "in", "replacements", ":", "\n", "        ", "s", "=", "s", ".", "replace", "(", "before", ",", "after", ")", "\n", "\n", "", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.analysis.pretty_dataset_name": [[459, 462], ["analysis.make_replacements"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.analysis.make_replacements"], ["", "def", "pretty_dataset_name", "(", "dataset_name", ")", ":", "\n", "    ", "\"\"\" Make dataset name look good for plots \"\"\"", "\n", "return", "make_replacements", "(", "dataset_name", ",", "dataset_replacements", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.analysis.average_over_n": [[464, 524], ["collections.defaultdict", "results.items", "v1.items", "collections.defaultdict", "v2.items", "new_values.sort", "numpy.array", "len", "numpy.array", "new_values.append", "new_values.append", "len", "numpy.array", "new_values.append", "NotImplementedError", "values[].mean", "values[].mean", "values[].mean", "values[].std"], "function", ["None"], ["", "def", "average_over_n", "(", "results", ",", "error_bars_over_runs_not_users", "=", "False", ")", ":", "\n", "    ", "\"\"\" Average over multiple runs (values of n, the number of source domains)\n\n    - Recompute mean/stdev for those that have multiple entries\n    - Get rid of the n-specific dictionary\n\n    i.e. we go from:\n        results[dataset_name][method][n] = [\n            (n, mean, std), ...\n        ]\n    to\n        averaged_results[dataset_name][method] = [\n            (n, mean, std), ...\n        ]\n    \"\"\"", "\n", "# averaged_results[dataset_name][method] = []", "\n", "averaged_results", "=", "collections", ".", "defaultdict", "(", "\n", "lambda", ":", "collections", ".", "defaultdict", "(", "list", ")", "\n", ")", "\n", "\n", "for", "dataset_name", ",", "v1", "in", "results", ".", "items", "(", ")", ":", "\n", "        ", "for", "method_name", ",", "v2", "in", "v1", ".", "items", "(", ")", ":", "\n", "            ", "new_values", "=", "[", "]", "\n", "\n", "for", "n", ",", "values", "in", "v2", ".", "items", "(", ")", ":", "\n", "# Average over the multiple values here and recompute", "\n", "# the standard deviation", "\n", "                ", "if", "len", "(", "values", ")", ">", "1", ":", "\n", "                    ", "values", "=", "np", ".", "array", "(", "values", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "if", "error_bars_over_runs_not_users", ":", "\n", "# Average the errors of the per-user standard deviation", "\n", "# over the multiple runs. This is instead of computing", "\n", "# standard deviation over all runs with both varying", "\n", "# target domain (user) and multiple runs -- which has", "\n", "# an \"error\" conflating two separate aspects.", "\n", "#", "\n", "# Note: this applies only if we're averaging over users...", "\n", "                        ", "new_values", ".", "append", "(", "(", "values", "[", "0", ",", "0", "]", ",", "values", "[", ":", ",", "1", "]", ".", "mean", "(", ")", ",", "\n", "values", "[", ":", ",", "2", "]", ".", "mean", "(", ")", ")", ")", "\n", "", "else", ":", "\n", "# All the 0th elements should be the same n", "\n", "# Then recompute the mean/stdev from the accuracy values", "\n", "# in 1th column", "\n", "                        ", "new_values", ".", "append", "(", "(", "values", "[", "0", ",", "0", "]", ",", "values", "[", ":", ",", "1", "]", ".", "mean", "(", ")", ",", "\n", "values", "[", ":", ",", "1", "]", ".", "std", "(", "ddof", "=", "0", ")", ")", ")", "\n", "", "", "elif", "len", "(", "values", ")", "==", "1", ":", "\n", "# Leave as is if there's only one", "\n", "                    ", "values", "=", "np", ".", "array", "(", "values", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "new_values", ".", "append", "(", "(", "values", "[", "0", ",", "0", "]", ",", "values", "[", "0", ",", "1", "]", ",", "\n", "values", "[", "0", ",", "2", "]", ")", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "NotImplementedError", "(", "\"must be several or one run\"", ")", "\n", "\n", "# Sort on n", "\n", "", "", "new_values", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "\n", "averaged_results", "[", "dataset_name", "]", "[", "method_name", "]", "=", "np", ".", "array", "(", "new_values", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "", "", "return", "averaged_results", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.analysis.process_results": [[526, 627], ["collections.defaultdict", "analysis.average_over_n", "analysis.pretty_dataset_name", "[].append", "[].sort", "collections.defaultdict", "average_over_n.items", "analysis.average_over_n", "collections.defaultdict", "values1.items", "collections.defaultdict", "collections.defaultdict", "len", "NotImplementedError", "[].append", "result[].split", "collections.defaultdict"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.analysis.average_over_n", "home.repos.pwc.inspect_result.floft_calda.None.analysis.pretty_dataset_name", "home.repos.pwc.inspect_result.floft_calda.None.analysis.average_over_n"], ["", "def", "process_results", "(", "results", ",", "average_over_users", ",", "ssda", ",", "upper_bound_offset", ",", "\n", "tune", ",", "average_over_runs_per_user", "=", "True", ")", ":", "\n", "    ", "\"\"\" Get results - get the test mean/std results indexed by:\n\n        if not average, not ssda (i.e. msda):\n            results[dataset_name + \" \" + target][method]\n        if not average, ssda:\n            results[(dataset_name, source(s), target)][method]\n        if average, not ssda (i.e. msda):\n            results[dataset_name][method]\n        if average, ssda:\n            results[dataset_name][method]\n\n    Note: for example, dataset_name=\"ucihar\", sources=\"1\", target=\"2\", and\n    method=\"dann\".\n    \"\"\"", "\n", "# results[dataset_name][method][n] = []", "\n", "# Note: at the end we average over the \"n\" dictionary", "\n", "processed_results", "=", "collections", ".", "defaultdict", "(", "\n", "lambda", ":", "collections", ".", "defaultdict", "(", "\n", "lambda", ":", "collections", ".", "defaultdict", "(", "list", ")", "\n", ")", "\n", ")", "\n", "\n", "for", "result", "in", "results", ":", "\n", "        ", "method_name", "=", "result", "[", "\"method\"", "]", "\n", "dataset_name", "=", "result", "[", "\"dataset\"", "]", "\n", "dataset_name", "=", "pretty_dataset_name", "(", "dataset_name", ")", "\n", "\n", "# For single-source domain adaptation, we create a table for each", "\n", "# source -> target pair, so we need index by that.", "\n", "if", "(", "ssda", "and", "not", "average_over_users", ")", "or", "(", "not", "average_over_users", "and", "not", "average_over_runs_per_user", ")", ":", "\n", "            ", "dataset_name", "=", "(", "dataset_name", ",", "result", "[", "\"sources\"", "]", ",", "result", "[", "\"target\"", "]", ")", "\n", "", "else", ":", "\n", "            ", "if", "method_name", "==", "\"upper\"", ":", "\n", "                ", "dataset_name", "=", "(", "dataset_name", ",", "result", "[", "\"sources\"", "]", ")", "\n", "", "else", ":", "\n", "                ", "dataset_name", "=", "(", "dataset_name", ",", "result", "[", "\"target\"", "]", ")", "\n", "\n", "# What we want the x axis to be...", "\n", "", "", "if", "FLAGS", ".", "xaxis", "==", "\"weight\"", ":", "\n", "            ", "n", "=", "result", "[", "\"similarity_weight\"", "]", "\n", "", "elif", "FLAGS", ".", "xaxis", "==", "\"uid\"", ":", "\n", "            ", "if", "upper_bound_offset", "is", "not", "None", "and", "result", "[", "\"method\"", "]", "==", "\"upper\"", ":", "\n", "                ", "n", "=", "result", "[", "\"uid\"", "]", "+", "upper_bound_offset", "\n", "", "else", ":", "\n", "                ", "n", "=", "result", "[", "\"uid\"", "]", "\n", "", "", "elif", "FLAGS", ".", "xaxis", "==", "\"domains\"", ":", "\n", "            ", "n", "=", "len", "(", "result", "[", "\"sources\"", "]", ".", "split", "(", "\",\"", ")", ")", "# number of source domains", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"xaxis value needs to be weight or uid\"", ")", "\n", "\n", "# We care about the target domain (note for the upper bound, we", "\n", "# replaced the \"target\" value with \"source\" in _all_stats())", "\n", "", "mean", ",", "std", ",", "all_values", "=", "result", "[", "\"results_target_test\"", "]", "\n", "\n", "processed_results", "[", "dataset_name", "]", "[", "method_name", "]", "[", "n", "]", ".", "append", "(", "\n", "(", "n", ",", "mean", ",", "std", ")", ")", "\n", "\n", "# Keep sorted by n", "\n", "processed_results", "[", "dataset_name", "]", "[", "method_name", "]", "[", "n", "]", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "\n", "# Get rid of the n dictionary and average over the multiple values (should", "\n", "# only be >1 if average_over_users==True)", "\n", "", "processed_results", "=", "average_over_n", "(", "processed_results", ")", "\n", "\n", "# How we compute error bars -- only applies if we're averaging over users", "\n", "if", "average_over_users", "and", "FLAGS", ".", "error_bars_over_runs_not_users", ":", "\n", "# Currently it's indexed by: results[(dataset_name, user)][method]", "\n", "# Now we want to average over users for each dataset to get:", "\n", "# results[dataset_name][method]", "\n", "#", "\n", "# We can easily do this (and reuse existing code) by converting to", "\n", "# results[dataset_name][method][n] = [", "\n", "#     (n,mean,std) for user 1,", "\n", "#     (n,mean,std) for user 2, etc.", "\n", "# ] and averaging over the users with average_over_n, but now setting", "\n", "# error_bars_over_runs_not_users=True so we average the error over users", "\n", "# rather than recomputing overall all users/runs.", "\n", "        ", "new_processed_results", "=", "collections", ".", "defaultdict", "(", "\n", "lambda", ":", "collections", ".", "defaultdict", "(", "\n", "lambda", ":", "collections", ".", "defaultdict", "(", "list", ")", "\n", ")", "\n", ")", "\n", "\n", "# Convert to results[(dataset_name,n)][method][user]", "\n", "for", "(", "dataset_name", ",", "user", ")", ",", "values1", "in", "processed_results", ".", "items", "(", ")", ":", "\n", "            ", "for", "method", ",", "values2", "in", "values1", ".", "items", "(", ")", ":", "\n", "# Actually, values2 is a numpy array, so we need to save", "\n", "# [values3] to make sure average_over_n has a 2d array (not 1d)", "\n", "                ", "for", "values3", "in", "values2", ":", "\n", "                    ", "n", "=", "values3", "[", "0", "]", "\n", "# we want all the user data in one array so that", "\n", "# average_over_n will average over that", "\n", "new_processed_results", "[", "dataset_name", "]", "[", "method", "]", "[", "n", "]", ".", "append", "(", "values3", ")", "\n", "\n", "# Average over users", "\n", "", "", "", "processed_results", "=", "average_over_n", "(", "new_processed_results", ",", "\n", "error_bars_over_runs_not_users", "=", "True", ")", "\n", "\n", "", "return", "processed_results", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.analysis.dictionary_sorted_keys": [[629, 633], ["list", "list.sort", "d.keys"], "function", ["None"], ["", "def", "dictionary_sorted_keys", "(", "d", ")", ":", "\n", "    ", "keys", "=", "list", "(", "d", ".", "keys", "(", ")", ")", "\n", "keys", ".", "sort", "(", ")", "\n", "return", "keys", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.analysis.generate_plots": [[635, 760], ["analysis.dictionary_sorted_keys", "len", "analysis.dictionary_sorted_keys", "range", "analysis.gen_jitter", "matplotlib.subplots", "range", "ax.set_ylabel", "matplotlib.show", "len", "numpy.array", "max", "min", "len", "ax.set_ylim", "ax.xaxis.set_major_locator", "len", "numpy.array", "matplotlib.title", "ax.set_xlabel", "ax.set_xlabel", "ax.get_position", "ax.set_position", "matplotlib.legend", "analysis.export_legend", "plt.legend.remove", "ax.get_position", "ax.set_position", "matplotlib.legend", "dataset_name.replace", "matplotlib.savefig", "matplotlib.close", "max", "min", "matplotlib.ticker.MaxNLocator", "matplotlib.errorbar", "matplotlib.plot", "ax.hlines", "os.path.join", "p[].get_color"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.analysis.dictionary_sorted_keys", "home.repos.pwc.inspect_result.floft_calda.None.analysis.dictionary_sorted_keys", "home.repos.pwc.inspect_result.floft_calda.None.analysis.gen_jitter", "home.repos.pwc.inspect_result.floft_calda.None.analysis.export_legend", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.close", "home.repos.pwc.inspect_result.floft_calda.None.sampling_analysis.plot"], ["", "def", "generate_plots", "(", "results", ",", "prefixes", ",", "save_plot", "=", "True", ",", "show_title", "=", "False", ",", "\n", "legend_separate", "=", "True", ",", "suffix", "=", "\"pdf\"", ",", "dir_name", "=", "\"result_plots\"", ",", "\n", "error_bars", "=", "True", ",", "figsize", "=", "(", "5", ",", "3", ")", ",", "skip", "=", "[", "]", ",", "yrange", "=", "None", ",", "\n", "integer_axis", "=", "False", ",", "ncol", "=", "1", ",", "jitter_amount", "=", "0.01", ",", "\n", "x_is_percentage", "=", "False", ",", "y_is_percentage", "=", "True", ",", "title_suffix", "=", "\"\"", ")", ":", "\n", "# See: https://matplotlib.org/3.1.1/api/markers_api.html", "\n", "    ", "markers", "=", "[", "\"o\"", ",", "\"v\"", ",", "\"^\"", ",", "\"<\"", ",", "\">\"", ",", "\"s\"", ",", "\"p\"", ",", "\"*\"", ",", "\"D\"", ",", "\"P\"", ",", "\"X\"", ",", "\"h\"", ",", "\n", "\"1\"", ",", "\"2\"", ",", "\"3\"", ",", "\"4\"", ",", "\"+\"", ",", "\"x\"", ",", "\"d\"", ",", "\"H\"", ",", "\"|\"", ",", "\"_\"", "]", "*", "2", "\n", "hollow", "=", "[", "False", "]", "*", "len", "(", "markers", ")", "\n", "\n", "# e.g. if \"baselines\" and \"modats1\", then \"baselines,modats1\" will be the", "\n", "# prefix", "\n", "prefix", "=", "\",\"", ".", "join", "(", "prefixes", ")", "\n", "\n", "# Do this sorted by name for a consistent ordering", "\n", "for", "dataset_name", "in", "dictionary_sorted_keys", "(", "results", ")", ":", "\n", "        ", "dataset_values", "=", "results", "[", "dataset_name", "]", "\n", "methods", "=", "dictionary_sorted_keys", "(", "dataset_values", ")", "\n", "\n", "# Get data in order of the sorted methods", "\n", "data", "=", "[", "dataset_values", "[", "m", "]", "for", "m", "in", "methods", "]", "\n", "\n", "# Find min/max x values for scaling the jittering appropriately", "\n", "max_x", "=", "-", "np", ".", "inf", "\n", "min_x", "=", "np", ".", "inf", "\n", "for", "i", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "            ", "method_data", "=", "np", ".", "array", "(", "data", "[", "i", "]", ")", "\n", "x", "=", "method_data", "[", ":", ",", "0", "]", "\n", "max_x", "=", "max", "(", "max", "(", "x", ")", ",", "max_x", ")", "\n", "min_x", "=", "min", "(", "min", "(", "x", ")", ",", "min_x", ")", "\n", "", "x_range", "=", "max_x", "-", "min_x", "\n", "\n", "# \"dodge\" points so they don't overlap", "\n", "jitter", "=", "gen_jitter", "(", "len", "(", "data", ")", ",", "amount", "=", "jitter_amount", "*", "x_range", ")", "\n", "\n", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "1", ",", "1", ",", "figsize", "=", "figsize", ",", "dpi", "=", "100", ")", "\n", "\n", "if", "yrange", "is", "not", "None", ":", "\n", "            ", "ax", ".", "set_ylim", "(", "yrange", ")", "\n", "\n", "# Only integers on x axis", "\n", "# https://stackoverflow.com/a/38096332", "\n", "", "if", "integer_axis", ":", "\n", "            ", "ax", ".", "xaxis", ".", "set_major_locator", "(", "MaxNLocator", "(", "integer", "=", "True", ")", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "            ", "method_data", "=", "np", ".", "array", "(", "data", "[", "i", "]", ")", "\n", "x", "=", "method_data", "[", ":", ",", "0", "]", "+", "jitter", "[", "i", "]", "\n", "y", "=", "method_data", "[", ":", ",", "1", "]", "\n", "std", "=", "method_data", "[", ":", ",", "2", "]", "\n", "\n", "if", "x_is_percentage", ":", "\n", "                ", "x", "*=", "100", "\n", "", "if", "y_is_percentage", ":", "\n", "                ", "y", "*=", "100", "\n", "std", "*=", "100", "\n", "\n", "", "if", "methods", "[", "i", "]", "in", "skip", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "methods", "[", "i", "]", "in", "nice_method_names", ":", "\n", "                ", "method_name", "=", "nice_method_names", "[", "methods", "[", "i", "]", "]", "\n", "", "else", ":", "\n", "                ", "method_name", "=", "methods", "[", "i", "]", "\n", "\n", "", "if", "methods", "[", "i", "]", "in", "method_lines", ":", "\n", "                ", "line_type", "=", "method_lines", "[", "methods", "[", "i", "]", "]", "\n", "", "else", ":", "\n", "                ", "line_type", "=", "\"-\"", "\n", "\n", "", "if", "hollow", "[", "i", "]", ":", "\n", "                ", "mfc", "=", "\"None\"", "\n", "", "else", ":", "\n", "                ", "mfc", "=", "None", "\n", "\n", "", "if", "error_bars", ":", "\n", "                ", "p", "=", "plt", ".", "errorbar", "(", "x", ",", "y", ",", "yerr", "=", "std", ",", "label", "=", "method_name", ",", "\n", "fmt", "=", "markers", "[", "i", "]", "+", "line_type", ",", "alpha", "=", "0.8", ",", "markerfacecolor", "=", "mfc", ")", "\n", "", "else", ":", "\n", "                ", "p", "=", "plt", ".", "plot", "(", "x", ",", "y", ",", "markers", "[", "i", "]", "+", "line_type", ",", "label", "=", "method_name", ",", "\n", "alpha", "=", "0.8", ",", "markerfacecolor", "=", "mfc", ")", "\n", "\n", "# Make a horizontal line at the upper bound since it doesn't matter", "\n", "# what \"n\" is for this method (ignores the sources, only trains", "\n", "# on target)", "\n", "", "if", "methods", "[", "i", "]", "==", "\"upper\"", "and", "FLAGS", ".", "xaxis", "!=", "\"uid\"", ":", "\n", "# xmin=1 since the upper bound is 1 source in a sense", "\n", "                ", "assert", "method_lines", "[", "methods", "[", "i", "]", "]", "==", "\"-.\"", ",", "\"change linestyles in hlines to match that of method_lines[\\\"upper\\\"]\"", "\n", "ax", ".", "hlines", "(", "y", "=", "y", ",", "xmin", "=", "1", ",", "xmax", "=", "max_x", ",", "colors", "=", "p", "[", "0", "]", ".", "get_color", "(", ")", ",", "\n", "linestyles", "=", "\"dashdot\"", ")", "\n", "\n", "", "", "if", "show_title", ":", "\n", "            ", "plt", ".", "title", "(", "\"Dataset: \"", "+", "dataset_name", "+", "title_suffix", ")", "\n", "\n", "", "if", "FLAGS", ".", "xaxis", "==", "\"domains\"", ":", "\n", "            ", "ax", ".", "set_xlabel", "(", "\"Number of source domains\"", ")", "\n", "", "else", ":", "\n", "            ", "ax", ".", "set_xlabel", "(", "FLAGS", ".", "xaxis", ")", "\n", "\n", "", "ax", ".", "set_ylabel", "(", "\"Target Domain \"", "+", "nice_metric_names", "[", "FLAGS", ".", "metric", "]", ")", "\n", "\n", "if", "legend_separate", ":", "\n", "            ", "box", "=", "ax", ".", "get_position", "(", ")", "\n", "ax", ".", "set_position", "(", "[", "box", ".", "x0", ",", "box", ".", "y0", ",", "box", ".", "width", "*", "0.8", ",", "box", ".", "height", "]", ")", "\n", "legend", "=", "plt", ".", "legend", "(", "loc", "=", "\"center left\"", ",", "bbox_to_anchor", "=", "(", "1", ",", "0.5", ")", ",", "ncol", "=", "ncol", ")", "\n", "export_legend", "(", "legend", ",", "dir_name", ",", "filename", "=", "prefix", "+", "\"_key.\"", "+", "suffix", ")", "\n", "legend", ".", "remove", "(", ")", "\n", "", "else", ":", "\n", "# Put legend outside the graph http://stackoverflow.com/a/4701285", "\n", "# Shrink current axis by 20%", "\n", "            ", "box", "=", "ax", ".", "get_position", "(", ")", "\n", "ax", ".", "set_position", "(", "[", "box", ".", "x0", ",", "box", ".", "y0", ",", "box", ".", "width", "*", "0.8", ",", "box", ".", "height", "]", ")", "\n", "plt", ".", "legend", "(", "loc", "=", "\"center left\"", ",", "bbox_to_anchor", "=", "(", "1", ",", "0.5", ")", ",", "ncol", "=", "ncol", ")", "\n", "\n", "", "if", "save_plot", ":", "\n", "            ", "save_dataset_name", "=", "dataset_name", ".", "replace", "(", "\" \"", ",", "\"_\"", ")", "\n", "filename", "=", "prefix", "+", "\"_\"", "+", "save_dataset_name", "+", "\"_\"", "+", "FLAGS", ".", "metric", "+", "\".\"", "+", "suffix", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "dir_name", ",", "filename", ")", ",", "\n", "bbox_inches", "=", "'tight'", ")", "\n", "plt", ".", "close", "(", ")", "\n", "\n", "", "", "if", "not", "save_plot", ":", "\n", "        ", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.analysis.get_float": [[762, 780], ["len", "value.split", "float", "len", "len", "parts[].replace", "parts[].replace", "parts[].replace", "parts[].replace"], "function", ["None"], ["", "", "def", "get_float", "(", "value", ")", ":", "\n", "    ", "\"\"\" Get float mean value (the part before plus/minus) from DDD.D $\\pm$ DDD.D \"\"\"", "\n", "float_value", "=", "None", "\n", "\n", "if", "len", "(", "value", ")", ">", "0", ":", "\n", "        ", "parts", "=", "value", ".", "split", "(", "\" $\\pm$ \"", ")", "\n", "\n", "if", "len", "(", "parts", ")", "==", "1", "or", "len", "(", "parts", ")", "==", "2", ":", "\n", "            ", "if", "\"underline{\"", "in", "parts", "[", "0", "]", ":", "\n", "                ", "parts", "[", "0", "]", "=", "parts", "[", "0", "]", ".", "replace", "(", "\"\\\\underline{\"", ",", "\"\"", ")", "\n", "parts", "[", "1", "]", "=", "parts", "[", "1", "]", ".", "replace", "(", "\"?\"", ",", "\"\"", ")", "\n", "", "elif", "\"textbf{\"", "in", "parts", "[", "0", "]", ":", "\n", "                ", "parts", "[", "0", "]", "=", "parts", "[", "0", "]", ".", "replace", "(", "\"\\\\textbf{\"", ",", "\"\"", ")", "\n", "parts", "[", "1", "]", "=", "parts", "[", "1", "]", ".", "replace", "(", "\"?\"", ",", "\"\"", ")", "\n", "\n", "", "float_value", "=", "float", "(", "parts", "[", "0", "]", ")", "\n", "\n", "", "", "return", "float_value", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.analysis.replace_highest_better": [[782, 805], ["enumerate", "reference_values.append", "len", "enumerate", "analysis.get_float", "analysis.get_float", "new_values.append", "new_values.append", "max"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.analysis.get_float", "home.repos.pwc.inspect_result.floft_calda.None.analysis.get_float"], ["", "def", "replace_highest_better", "(", "values", ",", "references", ",", "better_text", "=", "\"textbf\"", ")", ":", "\n", "    ", "\"\"\" Replace DDD.D $\\pm$ DDD.D with \\textbf{...} if higher than some column(s) \"\"\"", "\n", "# Get reference values first", "\n", "reference_values", "=", "[", "]", "\n", "\n", "for", "i", ",", "v", "in", "enumerate", "(", "references", ")", ":", "\n", "        ", "reference_values", ".", "append", "(", "get_float", "(", "v", ")", ")", "\n", "\n", "# Modify if better than (or equal to) all reference values", "\n", "", "if", "len", "(", "reference_values", ")", ">", "0", ":", "\n", "        ", "new_values", "=", "[", "]", "\n", "\n", "for", "i", ",", "v", "in", "enumerate", "(", "values", ")", ":", "\n", "            ", "float_value", "=", "get_float", "(", "v", ")", "\n", "\n", "if", "float_value", "is", "not", "None", "and", "float_value", ">=", "max", "(", "reference_values", ")", ":", "\n", "                ", "new_values", ".", "append", "(", "\"\\\\\"", "+", "better_text", "+", "\"{\"", "+", "v", "+", "\"}\"", ")", "\n", "", "else", ":", "\n", "                ", "new_values", ".", "append", "(", "v", ")", "\n", "\n", "", "", "return", "new_values", "\n", "", "else", ":", "\n", "        ", "return", "values", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.analysis.replace_highest_best": [[807, 834], ["enumerate", "analysis.get_float", "enumerate", "new_values.append", "new_values.append", "max_index.append"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.analysis.get_float"], ["", "", "def", "replace_highest_best", "(", "values", ",", "best_text", "=", "\"textbf\"", ")", ":", "\n", "    ", "\"\"\" Replace highest DDD.D $\\pm$ DDD.D with \\textbf{...} \"\"\"", "\n", "max_index", "=", "[", "]", "\n", "max_value", "=", "None", "\n", "\n", "for", "i", ",", "v", "in", "enumerate", "(", "values", ")", ":", "\n", "        ", "float_value", "=", "get_float", "(", "v", ")", "\n", "\n", "if", "float_value", "is", "not", "None", ":", "\n", "            ", "if", "max_value", "is", "None", "or", "float_value", ">", "max_value", ":", "\n", "                ", "max_value", "=", "float_value", "\n", "max_index", "=", "[", "i", "]", "\n", "", "elif", "float_value", "==", "max_value", ":", "\n", "                ", "max_index", ".", "append", "(", "i", ")", "\n", "\n", "", "", "", "if", "max_index", "is", "not", "None", ":", "\n", "        ", "new_values", "=", "[", "]", "\n", "\n", "for", "i", ",", "v", "in", "enumerate", "(", "values", ")", ":", "\n", "            ", "if", "i", "in", "max_index", ":", "\n", "                ", "new_values", ".", "append", "(", "\"\\\\\"", "+", "best_text", "+", "\"{\"", "+", "v", "+", "\"}\"", ")", "\n", "", "else", ":", "\n", "                ", "new_values", ".", "append", "(", "v", ")", "\n", "\n", "", "", "return", "new_values", "\n", "", "else", ":", "\n", "        ", "return", "values", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.analysis.write_table": [[836, 886], ["open", "max", "f.write", "f.write", "enumerate", "len", "len", "f.write", "f.write", "analysis.replace_highest_best", "analysis.replace_highest_better", "f.write", "f.write", "len"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.write", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.write", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.write", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.write", "home.repos.pwc.inspect_result.floft_calda.None.analysis.replace_highest_best", "home.repos.pwc.inspect_result.floft_calda.None.analysis.replace_highest_better", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.write", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.write"], ["", "", "def", "write_table", "(", "output_filename", ",", "table", ",", "replace_best", "=", "None", ",", "best_bold", "=", "False", ",", "\n", "replace_better", "=", "None", ",", "replace_better_ref", "=", "None", ",", "better_bold", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Write Latex table to file,\n    - underline highest row if replace_best=(row_start, row_end) inclusive\n    - bold rows replace_better if better than all in replace_better_ref\n    \"\"\"", "\n", "best_command", "=", "\"textbf\"", "if", "best_bold", "else", "\"underline\"", "\n", "better_command", "=", "\"textbf\"", "if", "better_bold", "else", "\"underline\"", "\n", "\n", "with", "open", "(", "output_filename", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "max_columns", "=", "max", "(", "[", "len", "(", "row", ")", "for", "row", "in", "table", "]", ")", "\n", "f", ".", "write", "(", "\"\\\\begin{tabular}{\"", "+", "\"c\"", "*", "max_columns", "+", "\"}\\n\"", ")", "\n", "\n", "for", "row", "in", "table", ":", "\n", "# \\hline's", "\n", "            ", "if", "len", "(", "row", ")", "==", "1", ":", "\n", "                ", "f", ".", "write", "(", "row", "[", "0", "]", "+", "\"\\n\"", ")", "\n", "continue", "\n", "\n", "# Identify best between columns if desired", "\n", "", "if", "replace_best", "is", "not", "None", ":", "\n", "                ", "try", ":", "\n", "                    ", "row_start", ",", "row_end", "=", "replace_best", "\n", "row", "[", "row_start", ":", "row_end", "+", "1", "]", "=", "replace_highest_best", "(", "\n", "row", "[", "row_start", ":", "row_end", "+", "1", "]", ",", "best_command", ")", "\n", "", "except", "ValueError", ":", "\n", "# If it's the header... ignore the error", "\n", "                    ", "pass", "\n", "\n", "", "", "if", "replace_better", "is", "not", "None", ":", "\n", "                ", "try", ":", "\n", "                    ", "row_start", ",", "row_end", "=", "replace_better", "\n", "row_start_ref", ",", "row_end_ref", "=", "replace_better_ref", "\n", "references", "=", "row", "[", "row_start_ref", ":", "row_end_ref", "+", "1", "]", "\n", "row", "[", "row_start", ":", "row_end", "+", "1", "]", "=", "replace_highest_better", "(", "\n", "row", "[", "row_start", ":", "row_end", "+", "1", "]", ",", "references", ",", "better_command", ")", "\n", "", "except", "ValueError", ":", "\n", "# If it's the header... ignore the error", "\n", "                    ", "pass", "\n", "\n", "", "", "for", "i", ",", "column", "in", "enumerate", "(", "row", ")", ":", "\n", "                ", "f", ".", "write", "(", "column", "+", "\" \"", ")", "\n", "\n", "if", "i", "==", "len", "(", "row", ")", "-", "1", ":", "\n", "                    ", "f", ".", "write", "(", "\"\\\\\\\\\\n\"", ")", "\n", "", "else", ":", "\n", "                    ", "f", ".", "write", "(", "\"& \"", ")", "\n", "\n", "", "", "", "f", ".", "write", "(", "\"\\\\end{tabular}\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.analysis.generate_table": [[888, 1106], ["collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "table.append", "table.append", "table.append", "enumerate", "table.append", "analysis.write_table", "analysis.dictionary_sorted_keys", "analysis.pretty_dataset_name", "analysis.dictionary_sorted_keys", "range", "table.append", "table.append", "len", "len", "collections.defaultdict", "collections.defaultdict", "len", "numpy.array", "collections.defaultdict.keys", "values1.items", "table.append", "table.append", "table.append", "len", "len", "enumerate", "enumerate", "table.append", "numpy.array", "thisrow.append", "thisrow.append", "len", "len", "len", "collections.defaultdict", "indexed_average_over_datasets[].append", "[].append", "indexed_average_over_datasets[].append", "thisrow.append", "len", "val[].mean", "val[].mean", "thisrow.append", "numpy.array", "val[].mean", "val[].mean", "thisrow.append", "thisrow.append", "thisrow.append", "thisrow.append", "str", "int"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.analysis.write_table", "home.repos.pwc.inspect_result.floft_calda.None.analysis.dictionary_sorted_keys", "home.repos.pwc.inspect_result.floft_calda.None.analysis.pretty_dataset_name", "home.repos.pwc.inspect_result.floft_calda.None.analysis.dictionary_sorted_keys"], ["", "", "def", "generate_table", "(", "results", ",", "prefixes", ",", "output_filename", ",", "x_is_percentage", "=", "False", ",", "\n", "y_is_percentage", "=", "True", ",", "skip", "=", "[", "]", ",", "list_of_methods", "=", "None", ",", "list_of_datasets", "=", "None", ",", "\n", "only_average", "=", "False", ",", "best_bold", "=", "False", ",", "better_bold", "=", "True", ",", "\n", "skip_best", "=", "False", ",", "skip_better", "=", "False", ",", "average", "=", "True", ",", "average_datasets", "=", "False", ")", ":", "\n", "# indexed[dataset_name][n][method] = \"\"", "\n", "    ", "indexed", "=", "collections", ".", "defaultdict", "(", "\n", "lambda", ":", "collections", ".", "defaultdict", "(", "\n", "lambda", ":", "collections", ".", "defaultdict", "(", "str", ")", "\n", ")", "\n", ")", "\n", "# indexed_averaged_over_n[dataset_name][method] = []", "\n", "# Note: exclude Train on Target since it's one value for all n -- would be", "\n", "# the same averaged", "\n", "indexed_averaged_over_n", "=", "collections", ".", "defaultdict", "(", "\n", "lambda", ":", "collections", ".", "defaultdict", "(", "list", ")", "\n", ")", "\n", "# Since Train on Target is one per dataset, do this separately", "\n", "# indexed_train_on_target[dataset_name] = \"\"", "\n", "indexed_train_on_target", "=", "collections", ".", "defaultdict", "(", "str", ")", "\n", "# indexed_average_over_datasets[method] = []", "\n", "indexed_average_over_datasets", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "\n", "# If list of datasets isn't provided, then get the list of all of them,", "\n", "# sorted for consistency", "\n", "if", "list_of_datasets", "is", "None", ":", "\n", "        ", "list_of_datasets", "=", "dictionary_sorted_keys", "(", "results", ")", "\n", "\n", "", "for", "dataset_name", "in", "list_of_datasets", ":", "\n", "        ", "dataset_values", "=", "results", "[", "dataset_name", "]", "\n", "dataset_name", "=", "pretty_dataset_name", "(", "dataset_name", ")", "\n", "methods", "=", "dictionary_sorted_keys", "(", "dataset_values", ")", "\n", "\n", "# Get data in order of the sorted methods", "\n", "data", "=", "[", "dataset_values", "[", "m", "]", "for", "m", "in", "methods", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "            ", "method_data", "=", "np", ".", "array", "(", "data", "[", "i", "]", ")", "\n", "x", "=", "method_data", "[", ":", ",", "0", "]", "\n", "y", "=", "method_data", "[", ":", ",", "1", "]", "\n", "std", "=", "method_data", "[", ":", ",", "2", "]", "\n", "\n", "if", "x_is_percentage", ":", "\n", "                ", "x", "*=", "100", "\n", "", "if", "y_is_percentage", ":", "\n", "                ", "y", "*=", "100", "\n", "std", "*=", "100", "\n", "\n", "", "if", "methods", "[", "i", "]", "in", "skip", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "methods", "[", "i", "]", "in", "nice_method_names", ":", "\n", "                ", "method_name", "=", "nice_method_names", "[", "methods", "[", "i", "]", "]", "\n", "", "else", ":", "\n", "                ", "method_name", "=", "methods", "[", "i", "]", "\n", "\n", "", "if", "method_name", "==", "\"Train on Target\"", ":", "\n", "                ", "for", "j", ",", "n", "in", "enumerate", "(", "x", ")", ":", "\n", "                    ", "assert", "j", "==", "0", "and", "n", "==", "1", ",", "\"should only be one Train on Target\"", "\n", "val", "=", "\"{:.1f} $\\\\pm$ {:.1f}\"", ".", "format", "(", "y", "[", "j", "]", ",", "std", "[", "j", "]", ")", "\n", "indexed_train_on_target", "[", "dataset_name", "]", "=", "val", "\n", "\n", "indexed_average_over_datasets", "[", "method_name", "]", ".", "append", "(", "[", "y", "[", "j", "]", ",", "std", "[", "j", "]", "]", ")", "\n", "", "", "else", ":", "\n", "                ", "for", "j", ",", "n", "in", "enumerate", "(", "x", ")", ":", "\n", "                    ", "val", "=", "\"{:.1f} $\\\\pm$ {:.1f}\"", ".", "format", "(", "y", "[", "j", "]", ",", "std", "[", "j", "]", ")", "\n", "indexed", "[", "dataset_name", "]", "[", "str", "(", "int", "(", "n", ")", ")", "]", "[", "method_name", "]", "=", "val", "\n", "\n", "indexed_averaged_over_n", "[", "dataset_name", "]", "[", "method_name", "]", ".", "append", "(", "\n", "[", "y", "[", "j", "]", ",", "std", "[", "j", "]", "]", ")", "\n", "\n", "indexed_average_over_datasets", "[", "method_name", "]", ".", "append", "(", "[", "y", "[", "j", "]", ",", "std", "[", "j", "]", "]", ")", "\n", "\n", "#", "\n", "# Create Latex table", "\n", "#", "\n", "", "", "", "", "if", "list_of_methods", "is", "None", ":", "\n", "        ", "columns", "=", "[", "\"No Adaptation\"", ",", "\"CoDATS\"", ",", "\"CALDA-Any,R\"", ",", "\"CALDA-XS,H\"", ",", "\"Train on Target\"", "]", "\n", "", "else", ":", "\n", "        ", "columns", "=", "list_of_methods", "\n", "\n", "", "prepend_columns", "=", "[", "\"Dataset\"", "]", "\n", "\n", "if", "not", "only_average", ":", "\n", "        ", "prepend_columns", "+=", "[", "\"$n$\"", "]", "\n", "\n", "", "fancy_columns", "=", "[", "\"\\\\textit{\"", "+", "c", "+", "\"}\"", "if", "\"CALDA\"", "in", "c", "else", "c", "for", "c", "in", "columns", "]", "\n", "\n", "# Create table", "\n", "table", "=", "[", "]", "\n", "table", ".", "append", "(", "[", "\"\\\\toprule\"", "]", ")", "\n", "table", ".", "append", "(", "prepend_columns", "+", "fancy_columns", ")", "\n", "table", ".", "append", "(", "[", "\"\\\\midrule\"", "]", ")", "\n", "\n", "# Which datasets we want to include", "\n", "dataset_names", "=", "[", "\n", "dataset_name", "for", "dataset_name", "in", "indexed", ".", "keys", "(", ")", "\n", "if", "(", "list_of_datasets", "is", "None", "or", "dataset_name", "in", "list_of_datasets", ")", "\n", "]", "\n", "\n", "for", "i", ",", "dataset_name", "in", "enumerate", "(", "dataset_names", ")", ":", "\n", "        ", "values1", "=", "indexed", "[", "dataset_name", "]", "\n", "\n", "if", "not", "only_average", ":", "\n", "# Row for each value of n", "\n", "            ", "for", "n", ",", "values2", "in", "values1", ".", "items", "(", ")", ":", "\n", "                ", "thisrow", "=", "[", "dataset_name", ",", "n", "]", "\n", "\n", "for", "method_name", "in", "columns", ":", "\n", "                    ", "if", "method_name", "==", "\"Train on Target\"", ":", "\n", "                        ", "thisrow", ".", "append", "(", "indexed_train_on_target", "[", "dataset_name", "]", ")", "\n", "", "else", ":", "\n", "                        ", "if", "method_name", "in", "values2", ":", "\n", "                            ", "thisrow", ".", "append", "(", "values2", "[", "method_name", "]", ")", "\n", "", "else", ":", "\n", "                            ", "thisrow", ".", "append", "(", "\"\"", ")", "\n", "\n", "", "", "", "table", ".", "append", "(", "thisrow", ")", "\n", "\n", "", "", "if", "average", "and", "not", "only_average", ":", "\n", "# Horizontal dashed line, avg of each method for this dataset,", "\n", "# horizontal solid line", "\n", "            ", "table", ".", "append", "(", "[", "\"\\\\hdashline[0.75pt/3pt]\"", "]", ")", "# https://tex.stackexchange.com/a/20141", "\n", "\n", "", "if", "average", ":", "\n", "            ", "thisrow", "=", "[", "dataset_name", "]", "\n", "\n", "if", "not", "only_average", ":", "\n", "                ", "thisrow", "+=", "[", "\"Avg\"", "]", "\n", "\n", "", "for", "method_name", "in", "columns", ":", "\n", "# Train on Target is the same for each value of n, so averaged it", "\n", "# is also the same", "\n", "                ", "if", "method_name", "==", "\"Train on Target\"", ":", "\n", "                    ", "thisrow", ".", "append", "(", "indexed_train_on_target", "[", "dataset_name", "]", ")", "\n", "", "else", ":", "\n", "                    ", "if", "dataset_name", "in", "indexed_averaged_over_n", "and", "method_name", "in", "indexed_averaged_over_n", "[", "dataset_name", "]", ":", "\n", "# Compute average", "\n", "                        ", "val", "=", "indexed_averaged_over_n", "[", "dataset_name", "]", "[", "method_name", "]", "\n", "val", "=", "np", ".", "array", "(", "val", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "# Format the same way and add to row", "\n", "mean", "=", "val", "[", ":", ",", "0", "]", ".", "mean", "(", ")", "\n", "std", "=", "val", "[", ":", ",", "1", "]", ".", "mean", "(", ")", "\n", "val", "=", "\"{:.1f} $\\\\pm$ {:.1f}\"", ".", "format", "(", "mean", ",", "std", ")", "\n", "thisrow", ".", "append", "(", "val", ")", "\n", "", "else", ":", "\n", "                        ", "thisrow", ".", "append", "(", "\"\"", ")", "\n", "\n", "", "", "", "table", ".", "append", "(", "thisrow", ")", "\n", "\n", "# Don't output for the last row, otherwise we have an extra line", "\n", "#", "\n", "# Also don't write if only-average, since then we have an hline for", "\n", "# every row.", "\n", "", "if", "i", "!=", "len", "(", "dataset_names", ")", "-", "1", "and", "not", "only_average", ":", "\n", "            ", "table", ".", "append", "(", "[", "\"\\\\hline\"", "]", ")", "\n", "\n", "", "", "if", "average_datasets", ":", "\n", "        ", "table", ".", "append", "(", "[", "\"\\\\hline\"", "]", ")", "\n", "\n", "thisrow", "=", "[", "\"Average\"", "]", "\n", "\n", "if", "not", "only_average", ":", "\n", "            ", "thisrow", "+=", "[", "\"\"", "]", "# don't have duplicate name", "\n", "\n", "", "for", "method_name", "in", "columns", ":", "\n", "            ", "if", "method_name", "in", "indexed_average_over_datasets", ":", "\n", "# Compute average", "\n", "                ", "val", "=", "indexed_average_over_datasets", "[", "method_name", "]", "\n", "val", "=", "np", ".", "array", "(", "val", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "# Format the same way and add to row", "\n", "val", "=", "\"{:.1f} $\\\\pm$ {:.1f}\"", ".", "format", "(", "val", "[", ":", ",", "0", "]", ".", "mean", "(", ")", ",", "val", "[", ":", ",", "1", "]", ".", "mean", "(", ")", ")", "\n", "thisrow", ".", "append", "(", "val", ")", "\n", "", "else", ":", "\n", "                ", "thisrow", ".", "append", "(", "\"\"", ")", "\n", "\n", "", "", "table", ".", "append", "(", "thisrow", ")", "\n", "\n", "", "table", ".", "append", "(", "[", "\"\\\\bottomrule\"", "]", ")", "\n", "\n", "if", "skip_best", ":", "\n", "        ", "replace_best", "=", "None", "\n", "", "else", ":", "\n", "# Print table, but underline the highest in each row excluding prepended", "\n", "# columns", "\n", "        ", "replace_best_start", "=", "len", "(", "prepend_columns", ")", "\n", "replace_best_end", "=", "len", "(", "prepend_columns", ")", "+", "len", "(", "columns", ")", "-", "1", "\n", "\n", "# Exclude train on target, if it's the last column (i.e., not doing the", "\n", "# ablation tables)", "\n", "if", "columns", "[", "-", "1", "]", "==", "\"Train on Target\"", ":", "\n", "            ", "replace_best_end", "-=", "1", "\n", "\n", "", "replace_best", "=", "(", "replace_best_start", ",", "replace_best_end", ")", "\n", "\n", "# Bold if better than CoDATS and No Adaptation", "\n", "", "if", "not", "skip_better", "and", "columns", "[", "0", "]", "==", "\"No Adaptation\"", "and", "\"CoDATS\"", "in", "columns", "[", "1", "]", ":", "\n", "# Reference is No Adaptation and CoDATS", "\n", "        ", "replace_better_ref_start", "=", "len", "(", "prepend_columns", ")", "# No Adaptation", "\n", "replace_better_ref_end", "=", "len", "(", "prepend_columns", ")", "+", "1", "# CoDATS", "\n", "replace_better_ref", "=", "(", "replace_better_ref_start", ",", "replace_better_ref_end", ")", "\n", "# End right before Train on Target (same as replace_best_end), but start", "\n", "# after CoDATS", "\n", "replace_better", "=", "(", "replace_better_ref_end", "+", "1", ",", "replace_best_end", ")", "\n", "", "elif", "not", "skip_better", "and", "columns", "[", "0", "]", "==", "\"No Adaptation\"", "and", "\"CAN\"", "in", "columns", "[", "1", "]", "and", "\"CoDATS\"", "in", "columns", "[", "2", "]", ":", "\n", "# Same as above but +2 for CoDATS since now we have CAN as well", "\n", "        ", "replace_better_ref_start", "=", "len", "(", "prepend_columns", ")", "# No Adaptation", "\n", "replace_better_ref_end", "=", "len", "(", "prepend_columns", ")", "+", "2", "# CoDATS - only difference", "\n", "replace_better_ref", "=", "(", "replace_better_ref_start", ",", "replace_better_ref_end", ")", "\n", "replace_better", "=", "(", "replace_better_ref_end", "+", "1", ",", "replace_best_end", ")", "\n", "", "else", ":", "\n", "        ", "replace_better", "=", "None", "\n", "replace_better_ref", "=", "None", "\n", "\n", "", "write_table", "(", "output_filename", "+", "\".tex\"", ",", "table", ",", "replace_best", "=", "replace_best", ",", "\n", "replace_better", "=", "replace_better", ",", "replace_better_ref", "=", "replace_better_ref", ",", "\n", "best_bold", "=", "best_bold", ",", "better_bold", "=", "better_bold", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.analysis.compute_significance": [[1108, 1215], ["collections.defaultdict", "results.items", "len", "analysis.pretty_dataset_name", "analysis.dictionary_sorted_keys", "range", "len", "len", "scipy.stats.ttest_rel", "scipy.stats.ttest_rel", "len", "numpy.array", "enumerate", "values[].append", "int"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.analysis.pretty_dataset_name", "home.repos.pwc.inspect_result.floft_calda.None.analysis.dictionary_sorted_keys"], ["", "def", "compute_significance", "(", "results_averaged", ",", "results_not_averaged_over_targets", ",", "\n", "results_not_averaged_over_targets_or_runs", ",", "prefixes", ",", "\n", "list_of_methods", "=", "None", ",", "list_of_datasets", "=", "None", ",", "\n", "x_is_percentage", "=", "False", ",", "y_is_percentage", "=", "True", ",", "\n", "which", "=", "\"no_tr\"", ",", "limit_n_for_datasets", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Determine if there's a significant difference between two methods, i.e.\n    len(list_of_methods) == 2\n\n    First part where we get all values is based on generate_table() code\n    \"\"\"", "\n", "assert", "len", "(", "list_of_methods", ")", "==", "2", ",", "\"significance() assumes comparison of two methods\"", "\n", "\n", "if", "which", "==", "\"no_t\"", ":", "\n", "        ", "results", "=", "results_not_averaged_over_targets", "\n", "", "elif", "which", "==", "\"no_tr\"", ":", "\n", "        ", "results", "=", "results_not_averaged_over_targets_or_runs", "\n", "", "else", ":", "\n", "        ", "results", "=", "results_averaged", "\n", "\n", "# values[method] = []", "\n", "", "values", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "\n", "for", "key", ",", "dataset_values", "in", "results", ".", "items", "(", ")", ":", "\n", "# Keys are different depending on if averaged or not", "\n", "        ", "if", "which", "==", "\"no_t\"", ":", "\n", "            ", "dataset_name", ",", "target", "=", "key", "\n", "", "elif", "which", "==", "\"no_tr\"", ":", "\n", "            ", "dataset_name", ",", "sources", ",", "target", "=", "key", "\n", "", "else", ":", "\n", "            ", "dataset_name", "=", "key", "\n", "\n", "", "dataset_name", "=", "pretty_dataset_name", "(", "dataset_name", ")", "\n", "methods", "=", "dictionary_sorted_keys", "(", "dataset_values", ")", "\n", "\n", "# Skip datasets we're not interested in", "\n", "if", "list_of_datasets", "is", "not", "None", "and", "dataset_name", "not", "in", "list_of_datasets", ":", "\n", "            ", "continue", "\n", "\n", "# Get data in order of the sorted methods", "\n", "", "data", "=", "[", "dataset_values", "[", "m", "]", "for", "m", "in", "methods", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "            ", "method_data", "=", "np", ".", "array", "(", "data", "[", "i", "]", ")", "\n", "x", "=", "method_data", "[", ":", ",", "0", "]", "\n", "y", "=", "method_data", "[", ":", ",", "1", "]", "\n", "std", "=", "method_data", "[", ":", ",", "2", "]", "\n", "\n", "if", "x_is_percentage", ":", "\n", "                ", "x", "*=", "100", "\n", "", "if", "y_is_percentage", ":", "\n", "                ", "y", "*=", "100", "\n", "std", "*=", "100", "\n", "\n", "", "if", "methods", "[", "i", "]", "in", "nice_method_names", ":", "\n", "                ", "method_name", "=", "nice_method_names", "[", "methods", "[", "i", "]", "]", "\n", "", "else", ":", "\n", "                ", "method_name", "=", "methods", "[", "i", "]", "\n", "\n", "", "if", "method_name", "in", "list_of_methods", ":", "\n", "                ", "for", "j", ",", "n", "in", "enumerate", "(", "x", ")", ":", "\n", "                    ", "assert", "n", "==", "int", "(", "x", "[", "j", "]", ")", ",", "\"x should be n but isn't\"", "\n", "\n", "if", "limit_n_for_datasets", "is", "not", "None", ":", "\n", "# All synthetic datasets have same values of n", "\n", "                        ", "if", "\"Synth \"", "in", "dataset_name", ":", "\n", "                            ", "n_dataset_name", "=", "\"synthetic\"", "\n", "", "else", ":", "\n", "                            ", "n_dataset_name", "=", "dataset_name", "\n", "\n", "", "assert", "n_dataset_name", "in", "limit_n_for_datasets", ",", "\"dataset \"", "+", "n_dataset_name", "+", "\" not in limit_n_for_datasets\"", "\n", "\n", "if", "n", "not", "in", "limit_n_for_datasets", "[", "n_dataset_name", "]", ":", "\n", "# Skip this value since it's not the right", "\n", "# value of n", "\n", "                            ", "continue", "\n", "\n", "", "", "if", "which", "==", "\"no_t\"", ":", "\n", "                        ", "name", "=", "\"{} t={} n={}\"", ".", "format", "(", "dataset_name", ",", "target", ",", "n", ")", "\n", "", "elif", "which", "==", "\"no_tr\"", ":", "\n", "                        ", "name", "=", "\"{} t={} s={} n={}\"", ".", "format", "(", "dataset_name", ",", "target", ",", "sources", ",", "n", ")", "\n", "", "else", ":", "\n", "                        ", "name", "=", "\"{} n={}\"", ".", "format", "(", "dataset_name", ",", "n", ")", "\n", "", "values", "[", "method_name", "]", ".", "append", "(", "(", "name", ",", "y", "[", "j", "]", ",", "std", "[", "j", "]", ")", ")", "\n", "\n", "", "", "", "", "method1", "=", "list_of_methods", "[", "0", "]", "\n", "method2", "=", "list_of_methods", "[", "1", "]", "\n", "\n", "# Make sure \"paired\" values truly are paired", "\n", "method1_names", "=", "[", "x", "[", "0", "]", "for", "x", "in", "values", "[", "method1", "]", "]", "\n", "method2_names", "=", "[", "x", "[", "0", "]", "for", "x", "in", "values", "[", "method2", "]", "]", "\n", "assert", "len", "(", "method1_names", ")", "==", "len", "(", "method2_names", ")", ",", "\"not the same number of values from each method\"", "\n", "assert", "method1_names", "==", "method2_names", ",", "\"methods do not correspond, so paired test will not work\"", "\n", "\n", "# Compute test", "\n", "method1_values", "=", "[", "x", "[", "1", "]", "for", "x", "in", "values", "[", "method1", "]", "]", "\n", "method2_values", "=", "[", "x", "[", "1", "]", "for", "x", "in", "values", "[", "method2", "]", "]", "\n", "\n", "test_notequal", "=", "stats", ".", "ttest_rel", "(", "method1_values", ",", "method2_values", ",", "alternative", "=", "\"two-sided\"", ")", ".", "pvalue", "\n", "test_lessthan", "=", "stats", ".", "ttest_rel", "(", "method1_values", ",", "method2_values", ",", "alternative", "=", "\"less\"", ")", ".", "pvalue", "\n", "\n", "return", "test_notequal", ",", "test_lessthan", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.analysis.compute_significance_all": [[1217, 1232], ["print", "analysis.compute_significance", "analysis.compute_significance", "analysis.compute_significance"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.analysis.compute_significance", "home.repos.pwc.inspect_result.floft_calda.None.analysis.compute_significance", "home.repos.pwc.inspect_result.floft_calda.None.analysis.compute_significance"], ["", "def", "compute_significance_all", "(", "*", "args", ",", "list_of_methods", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Compare how averaging affects significance tests \"\"\"", "\n", "# How much (if any) averaging to use -- compare them all", "\n", "p_avg", "=", "compute_significance", "(", "*", "args", ",", "list_of_methods", "=", "list_of_methods", ",", "**", "kwargs", ",", "which", "=", "\"avg\"", ")", "[", "0", "]", "\n", "p_no_t", "=", "compute_significance", "(", "*", "args", ",", "list_of_methods", "=", "list_of_methods", ",", "**", "kwargs", ",", "which", "=", "\"no_t\"", ")", "[", "0", "]", "\n", "p_no_tr", "=", "compute_significance", "(", "*", "args", ",", "list_of_methods", "=", "list_of_methods", ",", "**", "kwargs", ",", "which", "=", "\"no_tr\"", ")", "[", "0", "]", "\n", "\n", "method1", "=", "list_of_methods", "[", "0", "]", "\n", "method2", "=", "list_of_methods", "[", "1", "]", "\n", "\n", "print", "(", "\"p-value {} != {}: avg {}, no_t {}, no_tr {}\"", ".", "format", "(", "\n", "method1", ",", "method2", ",", "p_avg", ",", "p_no_t", ",", "p_no_tr", ")", ")", "\n", "\n", "# Return the one with no averaging", "\n", "return", "p_no_tr", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.analysis.compute_significance_eqlt": [[1234, 1252], ["analysis.compute_significance", "print"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.analysis.compute_significance"], ["", "def", "compute_significance_eqlt", "(", "*", "args", ",", "list_of_methods", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Compare != vs. < significance tests \"\"\"", "\n", "p_neq", ",", "p_lt", "=", "compute_significance", "(", "*", "args", ",", "list_of_methods", "=", "list_of_methods", ",", "**", "kwargs", ",", "which", "=", "\"no_tr\"", ")", "\n", "\n", "method1", "=", "list_of_methods", "[", "0", "]", "\n", "method2", "=", "list_of_methods", "[", "1", "]", "\n", "sig", "=", "\"\"", "\n", "\n", "if", "p_lt", "<", "0.01", ":", "\n", "        ", "sig", "=", "\", significant 0.01\"", "\n", "", "elif", "p_lt", "<", "0.05", ":", "\n", "        ", "sig", "=", "\", significant 0.05\"", "\n", "\n", "", "print", "(", "\"p-value {} != {}: {}, <: {}{}\"", ".", "format", "(", "\n", "method1", ",", "method2", ",", "p_neq", ",", "p_lt", ",", "sig", ")", ")", "\n", "\n", "# Return the not equal one", "\n", "return", "p_neq", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.analysis.make_plots_and_table": [[1254, 1579], ["analysis.get_results", "analysis.process_results", "analysis.process_results", "analysis.process_results", "analysis.generate_plots", "analysis.compute_significance_eqlt", "print", "print", "analysis.make_plots_and_table._sig"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.sampling_analysis.get_results", "home.repos.pwc.inspect_result.floft_calda.None.analysis.process_results", "home.repos.pwc.inspect_result.floft_calda.None.analysis.process_results", "home.repos.pwc.inspect_result.floft_calda.None.analysis.process_results", "home.repos.pwc.inspect_result.floft_calda.None.analysis.generate_plots", "home.repos.pwc.inspect_result.floft_calda.None.analysis.compute_significance_eqlt"], ["", "def", "make_plots_and_table", "(", "run_suffixes", ",", "variant_match", ",", "save_plot", "=", "True", ",", "\n", "show_title", "=", "False", ",", "legend_separate", "=", "True", ",", "ncol", "=", "4", ",", "suffix", "=", "\"pdf\"", ",", "\n", "skip", "=", "[", "]", ",", "figsize", "=", "(", "5", ",", "3", ")", ",", "dir_name", "=", "\"result_plots\"", ",", "\n", "jitter_amount", "=", "0.005", ",", "source_feature_subset", "=", "None", ",", "\n", "target_feature_subset", "=", "None", ",", "upper_bound_offset", "=", "None", ",", "title_suffix", "=", "\"\"", ",", "\n", "tune", "=", "False", ",", "table_output_prefix", "=", "None", ",", "weak_supervision", "=", "False", ",", "\n", "domain_generalization", "=", "False", ",", "only_synthetic", "=", "False", ")", ":", "\n", "    ", "\"\"\" Load files, process, save plots \"\"\"", "\n", "results", "=", "get_results", "(", "run_suffixes", ",", "variant_match", ",", "\n", "source_feature_subset", ",", "target_feature_subset", ",", "tune", ")", "\n", "averages", "=", "process_results", "(", "results", ",", "average_over_users", "=", "True", ",", "ssda", "=", "False", ",", "\n", "upper_bound_offset", "=", "upper_bound_offset", ",", "tune", "=", "tune", ")", "\n", "not_averaged_over_targets", "=", "process_results", "(", "results", ",", "average_over_users", "=", "False", ",", "ssda", "=", "False", ",", "\n", "upper_bound_offset", "=", "upper_bound_offset", ",", "tune", "=", "tune", ")", "\n", "not_averaged_over_targets_or_runs", "=", "process_results", "(", "results", ",", "average_over_users", "=", "False", ",", "ssda", "=", "False", ",", "\n", "upper_bound_offset", "=", "upper_bound_offset", ",", "tune", "=", "tune", ",", "average_over_runs_per_user", "=", "False", ")", "\n", "\n", "generate_plots", "(", "averages", ",", "run_suffixes", ",", "save_plot", ",", "\n", "show_title", ",", "legend_separate", ",", "suffix", ",", "ncol", "=", "ncol", ",", "skip", "=", "skip", ",", "\n", "figsize", "=", "figsize", ",", "dir_name", "=", "dir_name", ",", "jitter_amount", "=", "jitter_amount", ",", "\n", "title_suffix", "=", "title_suffix", ")", "\n", "\n", "datasets_to_use", "=", "[", "\n", "\"UCI HAR\"", ",", "\n", "\"UCI HHAR\"", ",", "\n", "\"WISDM AR\"", ",", "\n", "\"WISDM AT\"", ",", "\n", "\"Myo EMG\"", ",", "\n", "\"NinaPro Myo\"", ",", "\n", "]", "\n", "\n", "# Comparisons", "\n", "def", "_sig", "(", "m1", ",", "m2", ",", "list_of_datasets", ",", "**", "kwargs", ")", ":", "\n", "        ", "compute_significance_eqlt", "(", "\n", "averages", ",", "not_averaged_over_targets", ",", "not_averaged_over_targets_or_runs", ",", "run_suffixes", ",", "\n", "list_of_methods", "=", "[", "m1", ",", "m2", "]", ",", "list_of_datasets", "=", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "\n", "", "def", "_sig_all", "(", "list_of_datasets", ",", "**", "kwargs", ")", ":", "\n", "        ", "print", "(", "\"Datasets:\"", ",", "list_of_datasets", ")", "\n", "print", "(", "\"No Adversary - need adversary\"", ")", "\n", "_sig", "(", "\"CALDA-Any,R,NoAdv\"", ",", "\"CALDA-Any,R\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CALDA-XS,H,NoAdv\"", ",", "\"CALDA-XS,H\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "print", "(", "\"Pseudo labeling - no pseudo labeling better\"", ")", "\n", "_sig", "(", "\"CALDA-In,R,P\"", ",", "\"CALDA-In,R\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CALDA-In,H,P\"", ",", "\"CALDA-In,H\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CALDA-Any,R,P\"", ",", "\"CALDA-Any,R\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CALDA-Any,H,P\"", ",", "\"CALDA-Any,H\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CALDA-XS,R,P\"", ",", "\"CALDA-XS,R\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CALDA-XS,H,P\"", ",", "\"CALDA-XS,H\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "print", "(", "\"Selection - any/xs better than within\"", ")", "\n", "_sig", "(", "\"CALDA-In,R\"", ",", "\"CALDA-Any,R\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CALDA-In,R\"", ",", "\"CALDA-XS,R\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CALDA-In,H\"", ",", "\"CALDA-Any,R\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CALDA-In,H\"", ",", "\"CALDA-XS,R\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CALDA-In,R\"", ",", "\"CALDA-Any,H\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CALDA-In,R\"", ",", "\"CALDA-XS,H\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CALDA-In,H\"", ",", "\"CALDA-Any,H\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CALDA-In,H\"", ",", "\"CALDA-XS,H\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "print", "(", "\"Selection - any better than xs\"", ")", "\n", "_sig", "(", "\"CALDA-XS,R\"", ",", "\"CALDA-Any,H\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CALDA-XS,H\"", ",", "\"CALDA-Any,H\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CALDA-XS,R\"", ",", "\"CALDA-Any,R\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CALDA-XS,H\"", ",", "\"CALDA-Any,R\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "print", "(", "\"Hard selection - better to do hard sampling\"", ")", "\n", "_sig", "(", "\"CALDA-In,R\"", ",", "\"CALDA-In,H\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CALDA-Any,R\"", ",", "\"CALDA-Any,H\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CALDA-XS,R\"", ",", "\"CALDA-XS,H\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "print", "(", "\"Hard selection - better to do random sampling\"", ")", "\n", "_sig", "(", "\"CALDA-In,H\"", ",", "\"CALDA-In,R\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CALDA-Any,H\"", ",", "\"CALDA-Any,R\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CALDA-XS,H\"", ",", "\"CALDA-XS,R\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "print", "(", "\"Baselines\"", ")", "\n", "_sig", "(", "\"No Adaptation\"", ",", "\"CALDA-Any,R\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"No Adaptation\"", ",", "\"CALDA-XS,H\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CoDATS\"", ",", "\"CALDA-Any,R\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CoDATS\"", ",", "\"CALDA-XS,H\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CAN\"", ",", "\"CALDA-Any,R\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CAN\"", ",", "\"CALDA-XS,H\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "print", "(", ")", "\n", "\n", "", "def", "_sig_all_dg", "(", "list_of_datasets", ",", "**", "kwargs", ")", ":", "\n", "        ", "print", "(", "\"Datasets:\"", ",", "list_of_datasets", ")", "\n", "print", "(", "\"Domain Generalization - CALDG better\"", ")", "\n", "_sig", "(", "\"No Adaptation\"", ",", "\"CALDG-Any,R\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"No Adaptation\"", ",", "\"CALDG-XS,H\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CoDATS-DG\"", ",", "\"CALDG-Any,R\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CoDATS-DG\"", ",", "\"CALDG-XS,H\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"Sleep-DG\"", ",", "\"CALDG-Any,R\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"Sleep-DG\"", ",", "\"CALDG-XS,H\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"AFLAC-DG\"", ",", "\"CALDG-Any,R\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"AFLAC-DG\"", ",", "\"CALDG-XS,H\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "print", "(", "\"Domain Generalization - DA better than DG\"", ")", "\n", "_sig", "(", "\"CoDATS-DG\"", ",", "\"CoDATS\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CALDG-Any,R\"", ",", "\"CALDA-Any,R\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CALDG-XS,H\"", ",", "\"CALDA-XS,H\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "print", "(", ")", "\n", "\n", "", "def", "_sig_all_ws", "(", "list_of_datasets", ",", "**", "kwargs", ")", ":", "\n", "        ", "print", "(", "\"Datasets:\"", ",", "list_of_datasets", ")", "\n", "print", "(", "\"Weak supervision - CALDA better\"", ")", "\n", "_sig", "(", "\"No Adaptation\"", ",", "\"CALDA-Any,R,WS\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"No Adaptation\"", ",", "\"CALDA-XS,H,WS\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CoDATS-WS\"", ",", "\"CALDA-Any,R,WS\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CoDATS-WS\"", ",", "\"CALDA-XS,H,WS\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "print", "(", "\"Weak supervision - WS better than no-WS\"", ")", "\n", "_sig", "(", "\"CALDA-Any,R\"", ",", "\"CALDA-Any,R,WS\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CALDA-XS,H\"", ",", "\"CALDA-XS,H,WS\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "_sig", "(", "\"CoDATS\"", ",", "\"CoDATS-WS\"", ",", "list_of_datasets", ",", "**", "kwargs", ")", "\n", "print", "(", ")", "\n", "\n", "", "def", "_sig_just_n", "(", "list_of_datasets", ",", "f", ",", "which", ")", ":", "\n", "        ", "if", "which", "==", "\"low\"", ":", "\n", "            ", "limit_n_for_datasets", "=", "{", "\n", "\"UCI HAR\"", ":", "[", "2", "]", ",", "\n", "\"UCI HHAR\"", ":", "[", "2", "]", ",", "\n", "\"WISDM AR\"", ":", "[", "2", "]", ",", "\n", "\"WISDM AT\"", ":", "[", "2", "]", ",", "\n", "\"Myo EMG\"", ":", "[", "2", "]", ",", "\n", "\"NinaPro Myo\"", ":", "[", "2", "]", ",", "\n", "\"synthetic\"", ":", "[", "2", "]", ",", "\n", "}", "\n", "", "elif", "which", "==", "\"low2\"", ":", "\n", "            ", "limit_n_for_datasets", "=", "{", "\n", "\"UCI HAR\"", ":", "[", "2", ",", "8", "]", ",", "\n", "\"UCI HHAR\"", ":", "[", "2", ",", "3", "]", ",", "\n", "\"WISDM AR\"", ":", "[", "2", ",", "8", "]", ",", "\n", "\"WISDM AT\"", ":", "[", "2", ",", "12", "]", ",", "\n", "\"Myo EMG\"", ":", "[", "2", ",", "10", "]", ",", "\n", "\"NinaPro Myo\"", ":", "[", "2", ",", "4", "]", ",", "\n", "\"synthetic\"", ":", "[", "2", ",", "4", "]", ",", "\n", "}", "\n", "", "elif", "which", "==", "\"high\"", ":", "\n", "            ", "limit_n_for_datasets", "=", "{", "\n", "\"UCI HAR\"", ":", "[", "26", "]", ",", "\n", "\"UCI HHAR\"", ":", "[", "6", "]", ",", "\n", "\"WISDM AR\"", ":", "[", "26", "]", ",", "\n", "\"WISDM AT\"", ":", "[", "42", "]", ",", "\n", "\"Myo EMG\"", ":", "[", "34", "]", ",", "\n", "\"NinaPro Myo\"", ":", "[", "8", "]", ",", "\n", "\"synthetic\"", ":", "[", "10", "]", ",", "\n", "}", "\n", "", "elif", "which", "==", "\"high2\"", ":", "\n", "            ", "limit_n_for_datasets", "=", "{", "\n", "\"UCI HAR\"", ":", "[", "20", ",", "26", "]", ",", "\n", "\"UCI HHAR\"", ":", "[", "5", ",", "6", "]", ",", "\n", "\"WISDM AR\"", ":", "[", "20", ",", "26", "]", ",", "\n", "\"WISDM AT\"", ":", "[", "32", ",", "42", "]", ",", "\n", "\"Myo EMG\"", ":", "[", "26", ",", "34", "]", ",", "\n", "\"NinaPro Myo\"", ":", "[", "6", ",", "8", "]", ",", "\n", "\"synthetic\"", ":", "[", "8", ",", "10", "]", ",", "\n", "}", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"unknown which value passed\"", ")", "\n", "\n", "", "print", "(", "\"For\"", ",", "which", ",", "\"values of n\"", ")", "\n", "f", "(", "list_of_datasets", "=", "list_of_datasets", ",", "limit_n_for_datasets", "=", "limit_n_for_datasets", ")", "\n", "\n", "", "def", "_generate_table", "(", "averages", ",", "run_suffixes", ",", "output_filename", ",", "*", "args", ",", "average", "=", "None", ",", "only_average", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\" Generate both the normal table and the only-average table \"\"\"", "\n", "# Normal table -- keep average as passed in", "\n", "generate_table", "(", "averages", ",", "run_suffixes", ",", "output_filename", ",", "*", "args", ",", "average", "=", "average", ",", "**", "kwargs", ")", "\n", "# Only-average table -- ignore average and instead pass only_average", "\n", "generate_table", "(", "averages", ",", "run_suffixes", ",", "output_filename", "+", "\"_avg\"", ",", "*", "args", ",", "only_average", "=", "True", ",", "**", "kwargs", ")", "\n", "\n", "", "if", "table_output_prefix", "is", "not", "None", ":", "\n", "        ", "if", "not", "only_synthetic", ":", "\n", "            ", "if", "weak_supervision", ":", "\n", "                ", "_generate_table", "(", "averages", ",", "run_suffixes", ",", "table_output_prefix", "+", "\"_ws\"", ",", "\n", "skip", "=", "skip", ",", "list_of_methods", "=", "[", "\n", "\"No Adaptation\"", ",", "\n", "\"CoDATS-WS\"", ",", "\n", "\"CALDA-Any,R,WS\"", ",", "\n", "\"CALDA-XS,H,WS\"", ",", "\n", "\"Train on Target\"", "\n", "]", ",", "list_of_datasets", "=", "datasets_to_use", ",", "average", "=", "True", ",", "average_datasets", "=", "True", ")", "\n", "\n", "_sig_all_ws", "(", "datasets_to_use", ")", "\n", "\n", "", "if", "domain_generalization", ":", "\n", "                ", "_generate_table", "(", "averages", ",", "run_suffixes", ",", "table_output_prefix", "+", "\"_dg\"", ",", "\n", "skip", "=", "skip", ",", "list_of_methods", "=", "[", "\n", "\"No Adaptation\"", ",", "\n", "\"CoDATS-DG\"", ",", "\n", "\"Sleep-DG\"", ",", "\n", "\"AFLAC-DG\"", ",", "\n", "\"CALDG-Any,R\"", ",", "\n", "\"CALDG-XS,H\"", ",", "\n", "\"Train on Target\"", "\n", "]", ",", "list_of_datasets", "=", "datasets_to_use", ",", "average", "=", "True", ",", "average_datasets", "=", "True", ")", "\n", "\n", "_sig_all_dg", "(", "datasets_to_use", ")", "\n", "\n", "#", "\n", "# The full results", "\n", "#", "\n", "# Ablation", "\n", "", "_generate_table", "(", "averages", ",", "run_suffixes", ",", "table_output_prefix", "+", "\"_ablation_selection\"", ",", "\n", "skip", "=", "skip", ",", "list_of_methods", "=", "[", "\n", "# \"CAN\",", "\n", "\"CALDA-In,R\"", ",", "\n", "\"CALDA-In,H\"", ",", "\n", "\"CALDA-Any,R\"", ",", "\n", "\"CALDA-Any,H\"", ",", "\n", "\"CALDA-XS,R\"", ",", "\n", "\"CALDA-XS,H\"", ",", "\n", "]", ",", "list_of_datasets", "=", "datasets_to_use", ",", "average", "=", "True", ",", "best_bold", "=", "True", ",", "average_datasets", "=", "True", ")", "\n", "\n", "_generate_table", "(", "averages", ",", "run_suffixes", ",", "table_output_prefix", "+", "\"_ablation_pseudo\"", ",", "\n", "skip", "=", "skip", ",", "list_of_methods", "=", "[", "\n", "\"CALDA-In,R,P\"", ",", "\n", "\"CALDA-In,H,P\"", ",", "\n", "\"CALDA-Any,R,P\"", ",", "\n", "\"CALDA-Any,H,P\"", ",", "\n", "\"CALDA-XS,R,P\"", ",", "\n", "\"CALDA-XS,H,P\"", ",", "\n", "]", ",", "list_of_datasets", "=", "datasets_to_use", ",", "average", "=", "True", ",", "skip_best", "=", "True", ",", "average_datasets", "=", "True", ")", "\n", "\n", "# No adversary ablation", "\n", "_generate_table", "(", "averages", ",", "run_suffixes", ",", "table_output_prefix", "+", "\"_ablation_noadv\"", ",", "\n", "skip", "=", "skip", ",", "list_of_methods", "=", "[", "\n", "\"CALDA-Any,R,NoAdv\"", ",", "\n", "\"CALDA-XS,H,NoAdv\"", ",", "\n", "\"CALDA-Any,R\"", ",", "\n", "\"CALDA-XS,H\"", ",", "\n", "]", ",", "list_of_datasets", "=", "datasets_to_use", ",", "average", "=", "True", ",", "best_bold", "=", "True", ",", "average_datasets", "=", "True", ")", "\n", "\n", "_sig_all", "(", "datasets_to_use", ")", "\n", "\n", "# Full table", "\n", "_generate_table", "(", "averages", ",", "run_suffixes", ",", "table_output_prefix", "+", "\"_full\"", ",", "\n", "skip", "=", "skip", ",", "list_of_methods", "=", "[", "\n", "\"No Adaptation\"", ",", "\n", "\"CAN\"", ",", "\n", "\"CoDATS\"", ",", "\n", "\"CALDA-Any,R\"", ",", "\n", "\"CALDA-XS,H\"", ",", "\n", "\"Train on Target\"", "\n", "]", ",", "list_of_datasets", "=", "datasets_to_use", ",", "average", "=", "True", ",", "average_datasets", "=", "True", ")", "\n", "\n", "#", "\n", "# Synthetic data", "\n", "#", "\n", "", "synthetic_datasets", "=", "{", "\n", "\"noshift\"", ":", "[", "\n", "\"Synth InterT 0\"", ",", "\n", "\"Synth InterR 0\"", ",", "\n", "\"Synth IntraT 0\"", ",", "\n", "\"Synth IntraR 0\"", ",", "\n", "]", ",", "\n", "\"shift1\"", ":", "[", "\n", "\"Synth InterT 5\"", ",", "\n", "\"Synth InterR 0.5\"", ",", "\n", "\"Synth IntraT 5\"", ",", "\n", "\"Synth IntraR 0.5\"", ",", "\n", "]", ",", "\n", "\"shift2\"", ":", "[", "\n", "\"Synth InterT 10\"", ",", "\n", "\"Synth InterR 1.0\"", ",", "\n", "\"Synth IntraT 10\"", ",", "\n", "\"Synth IntraR 1.0\"", ",", "\n", "]", ",", "\n", "}", "\n", "\n", "for", "name", ",", "datasets_to_use_synthetic", "in", "synthetic_datasets", ".", "items", "(", ")", ":", "\n", "            ", "_generate_table", "(", "averages", ",", "run_suffixes", ",", "table_output_prefix", "+", "\"_ablation_selection_synthetic_\"", "+", "name", ",", "\n", "skip", "=", "skip", ",", "list_of_methods", "=", "[", "\n", "\"CALDA-In,R\"", ",", "\n", "\"CALDA-In,H\"", ",", "\n", "\"CALDA-Any,R\"", ",", "\n", "\"CALDA-Any,H\"", ",", "\n", "\"CALDA-XS,R\"", ",", "\n", "\"CALDA-XS,H\"", ",", "\n", "]", ",", "list_of_datasets", "=", "datasets_to_use_synthetic", ",", "average", "=", "True", ",", "best_bold", "=", "True", ",", "average_datasets", "=", "True", ")", "\n", "\n", "_generate_table", "(", "averages", ",", "run_suffixes", ",", "table_output_prefix", "+", "\"_ablation_pseudo_synthetic_\"", "+", "name", ",", "\n", "skip", "=", "skip", ",", "list_of_methods", "=", "[", "\n", "\"CALDA-In,R,P\"", ",", "\n", "\"CALDA-In,H,P\"", ",", "\n", "\"CALDA-Any,R,P\"", ",", "\n", "\"CALDA-Any,H,P\"", ",", "\n", "\"CALDA-XS,R,P\"", ",", "\n", "\"CALDA-XS,H,P\"", ",", "\n", "]", ",", "list_of_datasets", "=", "datasets_to_use_synthetic", ",", "average", "=", "True", ",", "skip_best", "=", "True", ",", "average_datasets", "=", "True", ")", "\n", "\n", "_generate_table", "(", "averages", ",", "run_suffixes", ",", "table_output_prefix", "+", "\"_full_synthetic_\"", "+", "name", ",", "\n", "skip", "=", "skip", ",", "list_of_methods", "=", "[", "\n", "\"No Adaptation\"", ",", "\n", "\"CAN\"", ",", "\n", "\"CoDATS\"", ",", "\n", "\"CALDA-Any,R\"", ",", "\n", "\"CALDA-XS,H\"", ",", "\n", "\"Train on Target\"", "\n", "]", ",", "list_of_datasets", "=", "datasets_to_use_synthetic", ",", "average", "=", "True", ",", "average_datasets", "=", "True", ")", ",", "\n", "\n", "_generate_table", "(", "averages", ",", "run_suffixes", ",", "table_output_prefix", "+", "\"_ablation_noadv_synthetic_\"", "+", "name", ",", "\n", "skip", "=", "skip", ",", "list_of_methods", "=", "[", "\n", "\"CALDA-Any,R,NoAdv\"", ",", "\n", "\"CALDA-XS,H,NoAdv\"", ",", "\n", "\"CALDA-Any,R\"", ",", "\n", "\"CALDA-XS,H\"", ",", "\n", "]", ",", "list_of_datasets", "=", "datasets_to_use_synthetic", ",", "average", "=", "True", ",", "best_bold", "=", "True", ",", "average_datasets", "=", "True", ")", "\n", "\n", "_generate_table", "(", "averages", ",", "run_suffixes", ",", "table_output_prefix", "+", "\"_ws_synthetic_\"", "+", "name", ",", "\n", "skip", "=", "skip", ",", "list_of_methods", "=", "[", "\n", "\"No Adaptation\"", ",", "\n", "\"CoDATS-WS\"", ",", "\n", "\"CALDA-Any,R,WS\"", ",", "\n", "\"CALDA-XS,H,WS\"", ",", "\n", "\"Train on Target\"", "\n", "]", ",", "list_of_datasets", "=", "datasets_to_use_synthetic", ",", "average", "=", "True", ",", "average_datasets", "=", "True", ")", "\n", "\n", "_generate_table", "(", "averages", ",", "run_suffixes", ",", "table_output_prefix", "+", "\"_dg_synthetic_\"", "+", "name", ",", "\n", "skip", "=", "skip", ",", "list_of_methods", "=", "[", "\n", "\"No Adaptation\"", ",", "\n", "\"CoDATS-DG\"", ",", "\n", "\"Sleep-DG\"", ",", "\n", "\"AFLAC-DG\"", ",", "\n", "\"CALDG-Any,R\"", ",", "\n", "\"CALDG-XS,H\"", ",", "\n", "\"Train on Target\"", "\n", "]", ",", "list_of_datasets", "=", "datasets_to_use_synthetic", ",", "average", "=", "True", ",", "average_datasets", "=", "True", ")", "\n", "\n", "_sig_all", "(", "datasets_to_use_synthetic", ")", "\n", "_sig_all_dg", "(", "datasets_to_use_synthetic", ")", "\n", "_sig_all_ws", "(", "datasets_to_use_synthetic", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.analysis.main": [[1581, 1630], ["matplotlib.rc", "matplotlib.rc", "analysis.main._make_plots_and_table"], "function", ["None"], ["", "", "", "def", "main", "(", "argv", ")", ":", "\n", "    ", "outdir", "=", "\"result_plots\"", "\n", "for_paper", "=", "FLAGS", ".", "paper", "\n", "skip", "=", "[", "]", "\n", "\n", "if", "for_paper", ":", "\n", "        ", "outdir", "+=", "\"_paper\"", "\n", "show_title", "=", "False", "\n", "legend_separate", "=", "True", "\n", "ncol", "=", "5", "\n", "suffix", "=", "\"pdf\"", "\n", "figsize", "=", "(", "5", ",", "3", ")", "\n", "jitter_amount", "=", "0.005", "\n", "", "else", ":", "\n", "        ", "show_title", "=", "True", "\n", "legend_separate", "=", "False", "\n", "ncol", "=", "1", "\n", "suffix", "=", "\"png\"", "\n", "figsize", "=", "(", "30", ",", "18", ")", "\n", "jitter_amount", "=", "0.005", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "outdir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "outdir", ")", "\n", "\n", "# ACM doesn't like Type 3 fonts", "\n", "# https://tex.stackexchange.com/q/18687", "\n", "", "plt", ".", "rc", "(", "'pdf'", ",", "fonttype", "=", "42", ")", "\n", "plt", ".", "rc", "(", "'ps'", ",", "fonttype", "=", "42", ")", "\n", "\n", "# We pass variant_match as \"*\" rather than best_target since for the upper", "\n", "# bound there isn't a \"target\" (since target is passed as source), but we", "\n", "# all the others we evaluate only with best_target, so we can match all to", "\n", "# get the best_source only for the upper bound.", "\n", "def", "_make_plots_and_table", "(", "run_suffixes", ",", "upper_bound_offset", "=", "None", ",", "\n", "title_suffix", "=", "\"\"", ",", "tune", "=", "False", ",", "table_output_prefix", "=", "None", ",", "\n", "weak_supervision", "=", "False", ",", "domain_generalization", "=", "False", ",", "\n", "only_synthetic", "=", "False", ")", ":", "\n", "        ", "make_plots_and_table", "(", "run_suffixes", ",", "\"*\"", ",", "\n", "show_title", "=", "show_title", ",", "legend_separate", "=", "legend_separate", ",", "ncol", "=", "ncol", ",", "\n", "suffix", "=", "suffix", ",", "skip", "=", "skip", ",", "figsize", "=", "figsize", ",", "dir_name", "=", "outdir", ",", "\n", "jitter_amount", "=", "jitter_amount", ",", "upper_bound_offset", "=", "upper_bound_offset", ",", "\n", "title_suffix", "=", "title_suffix", ",", "tune", "=", "tune", ",", "\n", "table_output_prefix", "=", "table_output_prefix", ",", "\n", "weak_supervision", "=", "weak_supervision", ",", "\n", "domain_generalization", "=", "domain_generalization", ",", "\n", "only_synthetic", "=", "only_synthetic", ")", "\n", "\n", "", "_make_plots_and_table", "(", "[", "\"experiments\"", "]", ",", "table_output_prefix", "=", "\"results\"", ",", "\n", "weak_supervision", "=", "True", ",", "domain_generalization", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.file_utils.get_last_int": [[18, 35], ["re.compile", "int", "len", "re.compile.findall", "len"], "function", ["None"], ["def", "get_last_int", "(", "s", ",", "only_one", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Get last integer in a string\n\n    If only_one==True, then assert there's only one number\n    \"\"\"", "\n", "regex", "=", "re", ".", "compile", "(", "r'\\d+'", ")", "\n", "numbers", "=", "[", "int", "(", "x", ")", "for", "x", "in", "regex", ".", "findall", "(", "s", ")", "]", "\n", "\n", "if", "only_one", ":", "\n", "        ", "assert", "len", "(", "numbers", ")", "==", "1", ",", "\"get_last_int() should not match more than one integer\"", "\n", "\n", "", "if", "len", "(", "numbers", ")", "==", "0", ":", "\n", "        ", "return", "None", "\n", "\n", "", "return", "numbers", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.file_utils.last_modified_number": [[37, 50], ["pathlib.Path().glob", "sorted", "len", "file_utils.get_last_int", "pathlib.Path", "str", "cp.stat"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.file_utils.get_last_int"], ["", "def", "last_modified_number", "(", "dir_name", ",", "glob", ")", ":", "\n", "    ", "\"\"\"\n    Looks in dir_name at all files matching glob and takes number\n    from the one last modified\n    \"\"\"", "\n", "files", "=", "pathlib", ".", "Path", "(", "dir_name", ")", ".", "glob", "(", "glob", ")", "\n", "files", "=", "sorted", "(", "files", ",", "key", "=", "lambda", "cp", ":", "cp", ".", "stat", "(", ")", ".", "st_mtime", ")", "\n", "\n", "if", "len", "(", "files", ")", ">", "0", ":", "\n", "# Get number from filename", "\n", "        ", "return", "get_last_int", "(", "str", "(", "files", "[", "-", "1", "]", ")", ",", "only_one", "=", "True", ")", "\n", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.file_utils.last_modified": [[52, 64], ["pathlib.Path().glob", "sorted", "len", "str", "pathlib.Path", "cp.stat"], "function", ["None"], ["", "def", "last_modified", "(", "dir_name", ",", "glob", ")", ":", "\n", "    ", "\"\"\"\n    Looks in dir_name at all files matching glob and returns the file last\n    modified\n    \"\"\"", "\n", "files", "=", "pathlib", ".", "Path", "(", "dir_name", ")", ".", "glob", "(", "glob", ")", "\n", "files", "=", "sorted", "(", "files", ",", "key", "=", "lambda", "cp", ":", "cp", ".", "stat", "(", ")", ".", "st_mtime", ")", "\n", "\n", "if", "len", "(", "files", ")", ">", "0", ":", "\n", "        ", "return", "str", "(", "files", "[", "-", "1", "]", ")", "\n", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.file_utils.get_best_valid": [[66, 83], ["os.path.join", "os.path.exists", "open", "float"], "function", ["None"], ["", "def", "get_best_valid", "(", "log_dir", ",", "filename", "=", "\"best_valid_accuracy.txt\"", ")", ":", "\n", "    ", "\"\"\"\n    Read in the best validation accuracy/mse/etc. from the\n    best_valid_accuracy.txt file in the log_dir, if it exists. If it doesn't,\n    return None.\n    \"\"\"", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "log_dir", ",", "filename", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "filename", ")", ":", "\n", "        ", "with", "open", "(", "filename", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "try", ":", "\n", "                    ", "return", "float", "(", "line", ")", "\n", "", "except", "ValueError", ":", "\n", "                    ", "pass", "\n", "\n", "", "", "", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.file_utils.write_best_valid": [[85, 92], ["os.path.join", "open", "f.write", "str"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.write"], ["", "def", "write_best_valid", "(", "log_dir", ",", "value", ",", "\n", "filename", "=", "\"best_valid_accuracy.txt\"", ")", ":", "\n", "    ", "\"\"\" Write the best validation accuracy/mse/etc. to a file \"\"\"", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "log_dir", ",", "filename", ")", "\n", "\n", "with", "open", "(", "filename", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "str", "(", "value", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.file_utils.get_finished": [[94, 98], ["os.path.join", "os.path.exists"], "function", ["None"], ["", "", "def", "get_finished", "(", "log_dir", ")", ":", "\n", "    ", "\"\"\" Does the file indicating completion exist? \"\"\"", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "log_dir", ",", "\"finished.txt\"", ")", "\n", "return", "os", ".", "path", ".", "exists", "(", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.file_utils.get_log_subdirs": [[100, 112], ["os.path.exists", "os.listdir", "os.path.join", "os.path.isdir", "dirs.append"], "function", ["None"], ["", "def", "get_log_subdirs", "(", "log_dir", ")", ":", "\n", "    ", "\"\"\" Get all the subdirectories of the log_dir \"\"\"", "\n", "dirs", "=", "[", "]", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "log_dir", ")", ":", "\n", "        ", "for", "d", "in", "os", ".", "listdir", "(", "log_dir", ")", ":", "\n", "            ", "subdir", "=", "os", ".", "path", ".", "join", "(", "log_dir", ",", "d", ")", "\n", "\n", "if", "os", ".", "path", ".", "isdir", "(", "subdir", ")", ":", "\n", "                ", "dirs", ".", "append", "(", "subdir", ")", "\n", "\n", "", "", "", "return", "dirs", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.file_utils.get_average_valid": [[114, 126], ["file_utils.get_log_subdirs", "numpy.array().mean", "file_utils.get_best_valid", "values.append", "numpy.array"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.file_utils.get_log_subdirs", "home.repos.pwc.inspect_result.floft_calda.None.file_utils.get_best_valid"], ["", "def", "get_average_valid", "(", "log_dir", ")", ":", "\n", "    ", "\"\"\" Get average of the best_valid_accuracy.txt files for all subdirs \"\"\"", "\n", "dirs", "=", "get_log_subdirs", "(", "log_dir", ")", "\n", "values", "=", "[", "]", "\n", "\n", "for", "d", "in", "dirs", ":", "\n", "        ", "value", "=", "get_best_valid", "(", "d", ")", "\n", "\n", "if", "value", "is", "not", "None", ":", "\n", "            ", "values", ".", "append", "(", "value", ")", "\n", "\n", "", "", "return", "np", ".", "array", "(", "values", ")", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.file_utils.get_num_finished": [[128, 143], ["file_utils.get_log_subdirs", "file_utils.get_finished"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.file_utils.get_log_subdirs", "home.repos.pwc.inspect_result.floft_calda.None.file_utils.get_finished"], ["", "def", "get_num_finished", "(", "log_dir", ")", ":", "\n", "    ", "\"\"\"\n    Count the number of files indicating completion in subdirectories\n    of the specified log directory\n    \"\"\"", "\n", "dirs", "=", "get_log_subdirs", "(", "log_dir", ")", "\n", "\n", "# Count the number that have finished.txt in them", "\n", "num_finished", "=", "0", "\n", "\n", "for", "d", "in", "dirs", ":", "\n", "        ", "if", "get_finished", "(", "d", ")", ":", "\n", "            ", "num_finished", "+=", "1", "\n", "\n", "", "", "return", "num_finished", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.file_utils.write_finished": [[145, 151], ["os.path.join", "open", "f.write"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.write"], ["", "def", "write_finished", "(", "log_dir", ")", ":", "\n", "    ", "\"\"\" Write the file indicating completion \"\"\"", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "log_dir", ",", "\"finished.txt\"", ")", "\n", "\n", "with", "open", "(", "filename", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.file_utils.get_config": [[153, 163], ["os.path.join", "os.path.exists", "open", "yaml.load"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.load"], ["", "", "def", "get_config", "(", "log_dir", ")", ":", "\n", "    ", "\"\"\" Get config file containing dataset name, sources, target, etc. \"\"\"", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "log_dir", ",", "\"config.yaml\"", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "filename", ")", ":", "\n", "        ", "return", "None", "\n", "\n", "", "with", "open", "(", "filename", ")", "as", "f", ":", "\n", "# See: https://github.com/yaml/pyyaml/wiki/PyYAML-yaml.load(input)-Deprecation", "\n", "        ", "return", "yaml", ".", "load", "(", "f", ",", "Loader", "=", "yaml", ".", "SafeLoader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.file_utils.write_config_from_args": [[165, 228], ["os.path.join", "os.path.join", "FLAGS.flags_by_module_dict().items", "file_utils.get_config", "get_config.items", "config.items", "open", "yaml.dump", "open", "f.write", "FLAGS.flags_by_module_dict", "FLAGS.flags_into_string", "getattr", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.file_utils.get_config", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.write"], ["", "", "def", "write_config_from_args", "(", "log_dir", ")", ":", "\n", "    ", "\"\"\" Save config file containing all flags \"\"\"", "\n", "config_filename", "=", "os", ".", "path", ".", "join", "(", "log_dir", ",", "\"config.yaml\"", ")", "\n", "flag_filename", "=", "os", ".", "path", ".", "join", "(", "log_dir", ",", "\"config.flags\"", ")", "\n", "\n", "# Rather than maintaining an ever-changing list of the flags from this", "\n", "# code, just get all of the ones that aren't absl or tensorflow related.", "\n", "config", "=", "{", "}", "\n", "\n", "# Note: we could use __flags_by_module directly, but there appears to be a", "\n", "# function flags_by_module_dict() that returns this, which hopefully is", "\n", "# less likely to change/disappear in the future.", "\n", "for", "module", ",", "module_flags", "in", "FLAGS", ".", "flags_by_module_dict", "(", ")", ".", "items", "(", ")", ":", "\n", "        ", "if", "\"absl\"", "not", "in", "module", "and", "\"tensorflow\"", "not", "in", "module", ":", "\n", "            ", "for", "module_flag", "in", "module_flags", ":", "\n", "                ", "name", "=", "module_flag", ".", "name", "\n", "# Note: this shouldn't change ever since they say it's always", "\n", "# FLAGS.name to access the flag.", "\n", "assert", "name", "not", "in", "config", ",", "\"duplicate flag name should not be possible\"", "\n", "config", "[", "name", "]", "=", "getattr", "(", "FLAGS", ",", "name", ")", "\n", "\n", "# If we're continuing training from a previous run (e.g. training was", "\n", "# preempted), check that the old config matches the new config. Otherwise,", "\n", "# we may accidentally have a uid conflict and mess up the trained model.", "\n", "# It's better to clearly error so we can fix the conflict.", "\n", "#", "\n", "# Note: we check for the three edit cases: insertion, deletion, edit", "\n", "", "", "", "old_config", "=", "get_config", "(", "log_dir", ")", "\n", "\n", "if", "old_config", "is", "not", "None", "and", "not", "FLAGS", ".", "ignore_old_config", ":", "\n", "        ", "for", "key", ",", "old_value", "in", "old_config", ".", "items", "(", ")", ":", "\n", "# Skip the flag that tells us whether or not to ignore these", "\n", "# differences. We want to be able to go back and forth on that.", "\n", "            ", "if", "key", "==", "\"ignore_old_config\"", ":", "\n", "                ", "continue", "\n", "\n", "# Check old key is in the new config", "\n", "", "assert", "key", "in", "config", ",", "\"mismatch in old/new config for key \\\"\"", "+", "str", "(", "key", ")", "+", "\"\\\": \"", "+", "\"missing key in new config; --ignore_old_config to ignore\"", "\n", "\n", "# Check old/new values are the same", "\n", "new_value", "=", "config", "[", "key", "]", "\n", "assert", "new_value", "==", "old_value", ",", "\"mismatch in old/new config for key \\\"\"", "+", "str", "(", "key", ")", "+", "\"\\\": \"", "+", "\"old = \"", "+", "str", "(", "old_value", ")", "+", "\", \"", "+", "\"new = \"", "+", "str", "(", "new_value", ")", "+", "\"; --ignore_old_config to ignore\"", "\n", "\n", "# Check new keys are in old config", "\n", "", "for", "key", ",", "new_value", "in", "config", ".", "items", "(", ")", ":", "\n", "            ", "assert", "key", "in", "old_config", ",", "\"mismatch in old/new config for key \\\"\"", "+", "str", "(", "key", ")", "+", "\"\\\": \"", "+", "\"missing key in old config; --ignore_old_config to ignore\"", "\n", "\n", "# Write the config file", "\n", "", "", "with", "open", "(", "config_filename", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "yaml", ".", "dump", "(", "config", ",", "f", ")", "\n", "\n", "# Also write out the flags into a file that can easily be loaded with", "\n", "# --flagfile=config.flags", "\n", "", "with", "open", "(", "flag_filename", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "FLAGS", ".", "flags_into_string", "(", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.floft_calda.None.hyperparameter_tuning_experiments_can.generate_hyperparameter_list": [[14, 87], ["enumerate", "runlist.append", "len", "random.Random().shuffle", "random.shuffle", "hyperparameter_tuning_experiments_can.generate_hyperparameter_list.format_f"], "function", ["None"], ["def", "generate_hyperparameter_list", "(", "max_experiments", "=", "None", ",", "seed", "=", "None", ")", ":", "\n", "    ", "base_lr_options", "=", "[", "0.0001", ",", "0.001", "]", "\n", "source_batch_size_options", "=", "[", "30", ",", "60", "]", "\n", "alpha_options", "=", "[", "0.0001", ",", "0.0005", ",", "0.001", "]", "\n", "beta_options", "=", "[", "0.75", ",", "1", ",", "1.5", ",", "2", ",", "2.25", ",", "2.5", ",", "2.75", ",", "3", "]", "\n", "loss_weight_options", "=", "[", "0.1", ",", "0.2", ",", "0.3", ",", "0.4", ",", "0.5", "]", "\n", "\n", "experiments", "=", "[", "]", "\n", "\n", "# All combinations of the above", "\n", "for", "base_lr", "in", "base_lr_options", ":", "\n", "        ", "for", "source_batch_size", "in", "source_batch_size_options", ":", "\n", "            ", "for", "alpha", "in", "alpha_options", ":", "\n", "                ", "for", "beta", "in", "beta_options", ":", "\n", "                    ", "for", "loss_weight", "in", "loss_weight_options", ":", "\n", "                        ", "experiments", ".", "append", "(", "[", "\n", "base_lr", ",", "\n", "source_batch_size", ",", "\n", "alpha", ",", "\n", "beta", ",", "\n", "loss_weight", ",", "\n", "]", ")", "\n", "\n", "# Limit number (if there's enough experiments)", "\n", "", "", "", "", "", "if", "max_experiments", "is", "not", "None", "and", "len", "(", "experiments", ")", ">", "max_experiments", ":", "\n", "        ", "if", "seed", "is", "not", "None", ":", "\n", "            ", "random", ".", "Random", "(", "seed", ")", ".", "shuffle", "(", "experiments", ")", "\n", "", "else", ":", "\n", "            ", "random", ".", "shuffle", "(", "experiments", ")", "\n", "\n", "", "experiments", "=", "experiments", "[", ":", "max_experiments", "]", "\n", "\n", "# Output list of hyperparameters for this method", "\n", "", "runlist", "=", "[", "]", "\n", "\n", "for", "i", ",", "the_tuple", "in", "enumerate", "(", "experiments", ")", ":", "\n", "        ", "(", "\n", "base_lr", ",", "\n", "source_batch_size", ",", "\n", "alpha", ",", "\n", "beta", ",", "\n", "loss_weight", ",", "\n", ")", "=", "the_tuple", "\n", "\n", "def", "format_f", "(", "f", ")", ":", "\n", "# Probably isn't always valid, but works for our numbers", "\n", "            ", "return", "\"{:f}\"", ".", "format", "(", "f", ")", ".", "rstrip", "(", "\"0\"", ")", ".", "rstrip", "(", "\".\"", ")", "\n", "\n", "# Folder for the logs/models", "\n", "", "folder", "=", "\"lr{lr}_sb{sb}_a{a}_b{b}_w{w}\"", ".", "format", "(", "\n", "lr", "=", "format_f", "(", "base_lr", ")", ",", "\n", "sb", "=", "source_batch_size", ",", "\n", "a", "=", "format_f", "(", "alpha", ")", ",", "\n", "b", "=", "format_f", "(", "beta", ")", ",", "\n", "w", "=", "format_f", "(", "loss_weight", ")", ",", "\n", ")", "\n", "\n", "# Args that set the hyperparameters", "\n", "options", "=", "\"--base_lr {lr} \"", "\"--train_source_batch_size {sb} \"", "\"--inv_alpha {a} \"", "\"--inv_beta {b} \"", "\"--loss_weight {w}\"", ".", "format", "(", "\n", "lr", "=", "format_f", "(", "base_lr", ")", ",", "\n", "sb", "=", "source_batch_size", ",", "\n", "a", "=", "format_f", "(", "alpha", ")", ",", "\n", "b", "=", "format_f", "(", "beta", ")", ",", "\n", "w", "=", "format_f", "(", "loss_weight", ")", ",", "\n", ")", "\n", "\n", "runlist", ".", "append", "(", "(", "folder", ",", "options", ",", "the_tuple", ")", ")", "\n", "\n", "", "return", "runlist", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.other_users": [[62, 71], ["new_users.append"], "function", ["None"], ["def", "other_users", "(", "users", ",", "skip_user", ")", ":", "\n", "    ", "\"\"\" From the list of users, throw out skip_user \"\"\"", "\n", "new_users", "=", "[", "]", "\n", "\n", "for", "user", "in", "users", ":", "\n", "        ", "if", "user", "!=", "skip_user", ":", "\n", "            ", "new_users", ".", "append", "(", "user", ")", "\n", "\n", "", "", "return", "new_users", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.generate_n_with_max": [[73, 81], ["list", "range"], "function", ["None"], ["", "def", "generate_n_with_max", "(", "num_users", ",", "max_num", ",", "start_with", "=", "1", ")", ":", "\n", "    ", "\"\"\" Generate [1,2,3,...,num_users] but max out at max_num and skip as close\n    to evenly to get there. For example, if num_users=30 and max_num=5, we get:\n    [1, 7, 13, 19, 25].\n\n    Note: above example assumes start_with=1\n    \"\"\"", "\n", "return", "list", "(", "range", "(", "start_with", ",", "num_users", ",", "num_users", "//", "max_num", ")", ")", "[", ":", "max_num", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.generate_multi_source": [[83, 140], ["random.sample", "len", "range", "pairs.append", "experiments_msda.other_users", "random.shuffle", "source_users.sort", "len", "tuple", "str", "str", "print", "tuple", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.other_users"], ["", "def", "generate_multi_source", "(", "dataset_name", ",", "users", ",", "target_users", ",", "n", ",", "repeat", ",", "max_users", ")", ":", "\n", "# Shrink the number of target users since otherwise we have >4000 adaptation", "\n", "# problems. That will take too long and won't fit in the paper's table", "\n", "# anyway.", "\n", "#", "\n", "# Take random set though, since IDs aren't necessarily randomized.", "\n", "# Note: not using random.shuffle() since that shuffles in-place", "\n", "    ", "shuffled_target_users", "=", "random", ".", "sample", "(", "target_users", ",", "len", "(", "target_users", ")", ")", "\n", "possible_target_users", "=", "shuffled_target_users", "[", ":", "max_users", "]", "\n", "\n", "# We'll generate multi-source options for each target user", "\n", "pairs", "=", "[", "]", "\n", "\n", "for", "target_user", "in", "possible_target_users", ":", "\n", "        ", "already_used_target", "=", "{", "}", "\n", "\n", "# We want several random subsets of each so we can get mean +/- stdev", "\n", "for", "i", "in", "range", "(", "repeat", ")", ":", "\n", "            ", "skip", "=", "False", "\n", "\n", "# Select random source domains excluding target, keep shuffling until", "\n", "# we find a source set we haven't already used. The point of \"repeat\"", "\n", "# is to get *different* subsets. If it's the same, then there's not", "\n", "# much point in re-running with the exact same data.", "\n", "j", "=", "0", "\n", "while", "True", ":", "\n", "                ", "others", "=", "other_users", "(", "users", ",", "target_user", ")", "\n", "random", ".", "shuffle", "(", "others", ")", "\n", "assert", "n", "<=", "len", "(", "others", ")", ",", "\"cannot choose n larger than len(users)-1\"", "\n", "source_users", "=", "others", "[", ":", "n", "]", "\n", "\n", "# Sort so if we ever use the same subset, we don't have to", "\n", "# regenerate the files. Also easier to read.", "\n", "source_users", ".", "sort", "(", ")", "\n", "\n", "if", "tuple", "(", "source_users", ")", "not", "in", "already_used_target", ":", "\n", "                    ", "already_used_target", "[", "tuple", "(", "source_users", ")", "]", "=", "None", "\n", "break", "\n", "", "elif", "j", ">", "1000", ":", "\n", "                    ", "print", "(", "\"Warning: couldn't pick different set of sources\"", ",", "\n", "\"than previously used,\"", ",", "\n", "\"dataset:\"", "+", "dataset_name", "+", "\",\"", ",", "\n", "\"n:\"", "+", "str", "(", "n", ")", "+", "\",\"", ",", "\n", "\"user:\"", "+", "str", "(", "target_user", ")", "+", "\",\"", ",", "\n", "\"repeat:\"", "+", "str", "(", "i", ")", ")", "\n", "skip", "=", "True", "\n", "break", "\n", "", "j", "+=", "1", "\n", "\n", "# Skip if this \"repeat\" would be the same as a previous one", "\n", "", "if", "skip", ":", "\n", "                ", "continue", "\n", "\n", "", "source_users", "=", "\",\"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "source_users", "]", ")", "\n", "pairs", ".", "append", "(", "(", "dataset_name", ",", "source_users", ",", "str", "(", "target_user", ")", ")", ")", "\n", "\n", "", "", "return", "pairs", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.generate_experiments_for_datasets": [[142, 209], ["datasets.get_dataset_users", "datasets.get_dataset_target_users", "enumerate", "experiments_msda.generate_n_with_max", "experiments_msda.generate_n_with_max", "random.seed", "experiments_msda.generate_multi_source", "enumerate", "len", "len", "uids.append", "uids.append", "str", "str"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.get_dataset_users", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.get_dataset_target_users", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.generate_n_with_max", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.generate_n_with_max", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.generate_multi_source"], ["", "def", "generate_experiments_for_datasets", "(", "dataset_names", ",", "tuning", ")", ":", "\n", "    ", "pairs", "=", "[", "]", "\n", "uids", "=", "[", "]", "\n", "\n", "for", "name", "in", "dataset_names", ":", "\n", "        ", "users", "=", "datasets", ".", "get_dataset_users", "(", "name", ")", "\n", "# Most of the time this is the same as users", "\n", "target_users", "=", "datasets", ".", "get_dataset_target_users", "(", "name", ")", "\n", "\n", "# Since sources-target aren't stored in filename anymore (too long), we", "\n", "# would run into folder name conflicts if we didn't append a unique ID", "\n", "# to each sources-target pair", "\n", "uid", "=", "0", "\n", "\n", "# For each value of n, from 1 (single-source domain adaptation) up to", "\n", "# the full number of users - 1 (since we have one for the target)", "\n", "# options = generate_n_with_max(len(users), 5)", "\n", "\n", "# Now we need at least two domains, so start_with=2", "\n", "if", "tuning", ":", "\n", "            ", "options", "=", "generate_n_with_max", "(", "len", "(", "users", ")", ",", "2", ",", "start_with", "=", "2", ")", "\n", "", "else", ":", "\n", "            ", "options", "=", "generate_n_with_max", "(", "len", "(", "users", ")", ",", "5", ",", "start_with", "=", "2", ")", "\n", "\n", "", "for", "i", ",", "n", "in", "enumerate", "(", "options", ")", ":", "\n", "# Make this repeatable even if we change which datasets, how many", "\n", "# n's we use, etc. Also nice since we end up using a subset of", "\n", "# n's source domains as (n-1)'s source domains. For example,", "\n", "# we get", "\n", "# (dataset_name, source_users, target_user) where each is a string", "\n", "# \"sleep\", \"17\", \"0\"", "\n", "# \"sleep\", \"17,13\", \"0\"", "\n", "# \"sleep\", \"17,13,10\", \"0\"", "\n", "# \"sleep\", \"17,13,10,20\", \"0\"", "\n", "            ", "random", ".", "seed", "(", "42", ")", "\n", "\n", "# Allows extra max_users for some datasets without changin uid's", "\n", "bonus_uid", "=", "0", "\n", "\n", "if", "tuning", ":", "\n", "                ", "max_users", "=", "5", "\n", "repeat", "=", "3", "\n", "", "else", ":", "\n", "                ", "max_users", "=", "10", "\n", "repeat", "=", "3", "\n", "\n", "", "curr_pairs", "=", "generate_multi_source", "(", "name", ",", "users", ",", "target_users", ",", "n", ",", "\n", "repeat", "=", "repeat", ",", "max_users", "=", "max_users", ")", "\n", "\n", "for", "i", ",", "(", "dataset_name", ",", "source_users", ",", "target_user", ")", "in", "enumerate", "(", "curr_pairs", ")", ":", "\n", "# We want to allow increasing the number of max_users for", "\n", "# wisdm_at and watch without changing the uid's of the 0-4", "\n", "# targets for backwards compatibility (otherwise we have to move", "\n", "# all the models around...)", "\n", "                ", "set_of_five", "=", "i", "//", "(", "5", "*", "repeat", ")", "\n", "\n", "# before we had 0-4 (or 1-5), so do as before", "\n", "if", "max_users", "==", "5", "or", "set_of_five", "==", "0", ":", "\n", "                    ", "uids", ".", "append", "(", "uid", ")", "\n", "uid", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "uids", ".", "append", "(", "str", "(", "uid", ")", "+", "\"_\"", "+", "str", "(", "bonus_uid", ")", ")", "\n", "bonus_uid", "+=", "1", "\n", "\n", "", "", "pairs", "+=", "curr_pairs", "\n", "\n", "", "", "return", "pairs", ",", "uids", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.print_experiments_list_debug": [[211, 216], ["print", "enumerate", "print", "print"], "function", ["None"], ["", "def", "print_experiments_list_debug", "(", "pairs", ",", "uids", ")", ":", "\n", "    ", "print", "(", "\"List of adaptations we'll perform:\"", ")", "\n", "for", "i", ",", "(", "dataset_name", ",", "source", ",", "target", ")", "in", "enumerate", "(", "pairs", ")", ":", "\n", "        ", "print", "(", "\"    \"", ",", "dataset_name", ",", "source", ",", "\"to\"", ",", "target", ",", "\"uid\"", ",", "uids", "[", "i", "]", ")", "\n", "", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.output_list_of_targets": [[218, 230], ["collections.defaultdict", "enumerate", "print_dictionary.print_dictionary", "list_of_targets[].append"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.print_dictionary.print_dictionary"], ["", "def", "output_list_of_targets", "(", "pairs", ",", "display", "=", "False", ")", ":", "\n", "# list_of_targets[dataset_name] = [target1, target2, , ...]", "\n", "    ", "list_of_targets", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "\n", "for", "i", ",", "(", "dataset_name", ",", "source", ",", "target", ")", "in", "enumerate", "(", "pairs", ")", ":", "\n", "        ", "if", "target", "not", "in", "list_of_targets", "[", "dataset_name", "]", ":", "\n", "            ", "list_of_targets", "[", "dataset_name", "]", ".", "append", "(", "target", ")", "\n", "\n", "", "", "if", "display", ":", "\n", "        ", "print_dictionary", "(", "list_of_targets", ",", "\"list_of_targets\"", ")", "\n", "\n", "", "return", "list_of_targets", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.atof": [[232, 239], ["float"], "function", ["None"], ["", "def", "atof", "(", "text", ")", ":", "\n", "    ", "\"\"\" https://stackoverflow.com/a/5967539 \"\"\"", "\n", "try", ":", "\n", "        ", "retval", "=", "float", "(", "text", ")", "\n", "", "except", "ValueError", ":", "\n", "        ", "retval", "=", "text", "\n", "", "return", "retval", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.natural_keys": [[241, 250], ["experiments_msda.atof", "re.split"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.atof"], ["", "def", "natural_keys", "(", "text", ")", ":", "\n", "    ", "\"\"\"\n    https://stackoverflow.com/a/5967539\n    http://nedbatchelder.com/blog/200712/human_sorting.html\n    (See Toothy's implementation in the comments)\n    float regex comes from https://stackoverflow.com/a/12643073/190597\n    \"\"\"", "\n", "text", "=", "text", "[", "0", "]", "+", "text", "[", "1", "]", "# we actually are sorting tuples of strings", "\n", "return", "[", "atof", "(", "c", ")", "for", "c", "in", "re", ".", "split", "(", "r'[+-]?([0-9]+(?:[.][0-9]*)?|[.][0-9]+)'", ",", "text", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.print_experiments_old": [[252, 301], ["print", "enumerate", "print", "print", "print", "print", "print", "print", "print", "list", "list.sort", "print", "print", "print", "print", "print", "print", "dataset_names.append", "print_uids.append", "sources.append", "targets.append", "len", "set", "len", "targets_unique_uids.append", "targets_unique_dataset.append", "targets_unique_target.append", "len", "str", "str", "dataset_target_pairs.values", "str"], "function", ["None"], ["", "def", "print_experiments_old", "(", "pairs", ",", "uids", ")", ":", "\n", "# Train/eval baselines/weight", "\n", "    ", "print", "(", "\"For kamiak_{train,eval}...\"", ")", "\n", "dataset_names", "=", "[", "]", "\n", "print_uids", "=", "[", "]", "\n", "sources", "=", "[", "]", "\n", "targets", "=", "[", "]", "\n", "dataset_target_pairs", "=", "{", "}", "# for upper bounds", "\n", "for", "i", ",", "(", "dataset_name", ",", "source", ",", "target", ")", "in", "enumerate", "(", "pairs", ")", ":", "\n", "        ", "dataset_names", ".", "append", "(", "\"\\\"\"", "+", "dataset_name", "+", "\"\\\"\"", ")", "\n", "print_uids", ".", "append", "(", "str", "(", "uids", "[", "i", "]", ")", ")", "\n", "sources", ".", "append", "(", "\"\\\"\"", "+", "source", "+", "\"\\\"\"", ")", "\n", "targets", ".", "append", "(", "\"\\\"\"", "+", "target", "+", "\"\\\"\"", ")", "\n", "\n", "# for upper bounds", "\n", "pair_name", "=", "(", "\"\\\"\"", "+", "dataset_name", "+", "\"\\\"\"", ",", "\"\\\"\"", "+", "target", "+", "\"\\\"\"", ")", "\n", "full_pair", "=", "(", "\"\\\"\"", "+", "dataset_name", "+", "\"\\\"\"", ",", "str", "(", "uids", "[", "i", "]", ")", ",", "\"\\\"\"", "+", "target", "+", "\"\\\"\"", ")", "\n", "if", "pair_name", "not", "in", "dataset_target_pairs", ":", "\n", "            ", "dataset_target_pairs", "[", "pair_name", "]", "=", "full_pair", "\n", "\n", "", "", "print", "(", "\"# number of adaptation problems =\"", ",", "len", "(", "sources", ")", ")", "\n", "print", "(", "\"uids=(\"", ",", "\" \"", ".", "join", "(", "print_uids", ")", ",", "\")\"", ",", "sep", "=", "\"\"", ")", "\n", "print", "(", "\"datasets=(\"", ",", "\" \"", ".", "join", "(", "dataset_names", ")", ",", "\")\"", ",", "sep", "=", "\"\"", ")", "\n", "print", "(", "\"sources=(\"", ",", "\" \"", ".", "join", "(", "sources", ")", ",", "\")\"", ",", "sep", "=", "\"\"", ")", "\n", "print", "(", "\"targets=(\"", ",", "\" \"", ".", "join", "(", "targets", ")", ",", "\")\"", ",", "sep", "=", "\"\"", ")", "\n", "print", "(", ")", "\n", "\n", "# Upper bound", "\n", "print", "(", "\"For kamiak_{train,eval}_upper\"", ")", "\n", "targets_unique", "=", "list", "(", "set", "(", "dataset_target_pairs", ".", "values", "(", ")", ")", ")", "\n", "targets_unique", ".", "sort", "(", "key", "=", "natural_keys", ")", "\n", "sources_blank", "=", "[", "\"\\\"\\\"\"", "]", "*", "len", "(", "targets_unique", ")", "\n", "\n", "targets_unique_uids", "=", "[", "]", "\n", "targets_unique_dataset", "=", "[", "]", "\n", "targets_unique_target", "=", "[", "]", "\n", "\n", "for", "dataset_name", ",", "uid", ",", "target", "in", "targets_unique", ":", "\n", "# Uses first uid from dataset_name-target", "\n", "        ", "targets_unique_uids", ".", "append", "(", "uid", ")", "\n", "targets_unique_dataset", ".", "append", "(", "dataset_name", ")", "\n", "targets_unique_target", ".", "append", "(", "target", ")", "\n", "\n", "", "print", "(", "\"# number of adaptation problems =\"", ",", "len", "(", "targets_unique", ")", ")", "\n", "print", "(", "\"uids=(\"", ",", "\" \"", ".", "join", "(", "[", "\"u\"", "+", "str", "(", "x", ")", "for", "x", "in", "targets_unique_uids", "]", ")", ",", "\")\"", ",", "sep", "=", "\"\"", ")", "\n", "print", "(", "\"datasets=(\"", ",", "\" \"", ".", "join", "(", "targets_unique_dataset", ")", ",", "\")\"", ",", "sep", "=", "\"\"", ")", "\n", "print", "(", "\"sources=(\"", ",", "\" \"", ".", "join", "(", "sources_blank", ")", ",", "\")\"", ",", "sep", "=", "\"\"", ")", "\n", "print", "(", "\"targets=(\"", ",", "\" \"", ".", "join", "(", "targets_unique_target", ")", ",", "\")\"", ",", "sep", "=", "\"\"", ")", "\n", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.fill_in_template": [[303, 311], ["replacements.items", "open", "f.read", "template.replace.replace", "str"], "function", ["None"], ["", "def", "fill_in_template", "(", "filename", ",", "replacements", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "template", "=", "f", ".", "read", "(", ")", "# .decode(\"utf-8\")?", "\n", "\n", "", "for", "name", ",", "replacement", "in", "replacements", ".", "items", "(", ")", ":", "\n", "        ", "template", "=", "template", ".", "replace", "(", "\"{{\"", "+", "name", "+", "\"}}\"", ",", "str", "(", "replacement", ")", ")", "\n", "\n", "", "return", "template", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.save_script": [[313, 316], ["open", "f.write"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.write"], ["", "def", "save_script", "(", "filename", ",", "contents", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "contents", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.upper_from_full": [[318, 340], ["enumerate", "list", "list.sort", "set", "upper_uids.append", "upper_pairs.append", "str", "dataset_target_pairs.values"], "function", ["None"], ["", "", "def", "upper_from_full", "(", "pairs", ",", "uids", ")", ":", "\n", "    ", "dataset_target_pairs", "=", "{", "}", "\n", "for", "i", ",", "(", "dataset_name", ",", "source", ",", "target", ")", "in", "enumerate", "(", "pairs", ")", ":", "\n", "        ", "pair_name", "=", "(", "dataset_name", ",", "target", ")", "\n", "full_pair", "=", "(", "dataset_name", ",", "str", "(", "uids", "[", "i", "]", ")", ",", "target", ")", "\n", "if", "pair_name", "not", "in", "dataset_target_pairs", ":", "\n", "            ", "dataset_target_pairs", "[", "pair_name", "]", "=", "full_pair", "\n", "\n", "", "", "targets_unique", "=", "list", "(", "set", "(", "dataset_target_pairs", ".", "values", "(", ")", ")", ")", "\n", "targets_unique", ".", "sort", "(", "key", "=", "natural_keys", ")", "\n", "\n", "upper_uids", "=", "[", "]", "\n", "upper_pairs", "=", "[", "]", "\n", "\n", "for", "dataset_name", ",", "uid", ",", "target", "in", "targets_unique", ":", "\n", "        ", "source", "=", "\"\"", "\n", "upper_uids", ".", "append", "(", "\"u{}\"", ".", "format", "(", "uid", ")", ")", "\n", "upper_pairs", ".", "append", "(", "(", "\n", "dataset_name", ",", "source", ",", "target", "\n", ")", ")", "\n", "\n", "", "return", "upper_pairs", ",", "upper_uids", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str": [[342, 344], ["None"], "function", ["None"], ["", "def", "list_to_str", "(", "values", ")", ":", "\n", "    ", "return", "\" \"", ".", "join", "(", "values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.get_cpu_or_gpu": [[346, 360], ["None"], "function", ["None"], ["", "def", "get_cpu_or_gpu", "(", "cpu", ")", ":", "\n", "    ", "if", "cpu", "==", "0", ":", "\n", "        ", "cpu_or_gpu", "=", "\"gpu\"", "\n", "partitions", "=", "\"cook,cahnrs_gpu,kamiak\"", "# for kamiak", "\n", "# partitions = \"cook\" # for my desktop", "\n", "gpus", "=", "1", "\n", "cpus", "=", "1", "\n", "", "else", ":", "\n", "        ", "cpu_or_gpu", "=", "\"cpu\"", "\n", "partitions", "=", "\"cook,vcea,cahnrs,kamiak\"", "\n", "gpus", "=", "0", "\n", "cpus", "=", "cpu", "\n", "\n", "", "return", "cpu_or_gpu", ",", "partitions", ",", "gpus", ",", "cpus", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.output_experiments": [[362, 720], ["experiments_msda.get_cpu_or_gpu", "experiments_msda.get_cpu_or_gpu", "len", "enumerate", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "experiments_msda.fill_in_template", "experiments_msda.fill_in_template", "print", "experiments_msda.save_script", "experiments_msda.save_script", "range", "experiments_msda.fill_in_template", "experiments_msda.fill_in_template", "print", "experiments_msda.save_script", "experiments_msda.save_script", "experiments_msda.fill_in_template", "experiments_msda.fill_in_template", "os.path.join", "print", "experiments_msda.save_script", "experiments_msda.save_script", "experiments_msda.list_to_str", "experiments_msda.list_to_str", "experiments_msda.list_to_str", "experiments_msda.list_to_str", "experiments_msda.list_to_str", "experiments_msda.list_to_str", "experiments_msda.list_to_str", "experiments_msda.list_to_str", "experiments_msda.list_to_str", "experiments_msda.list_to_str", "experiments_msda.list_to_str", "experiments_msda.list_to_str", "experiments_msda.list_to_str", "experiments_msda.list_to_str", "experiments_msda.list_to_str", "os.path.exists", "os.makedirs", "os.path.exists", "os.makedirs", "experiments_msda.list_to_str", "experiments_msda.list_to_str", "experiments_msda.list_to_str", "experiments_msda.list_to_str", "experiments_msda.list_to_str", "experiments_msda.list_to_str", "experiments_msda.list_to_str", "experiments_msda.list_to_str", "experiments_msda.list_to_str", "experiments_msda.list_to_str", "hyperparameters.get_hyperparameters_str", "methods_eval.append", "variants_eval.append", "uids_eval.append", "datasets_eval.append", "sources_eval.append", "targets_eval.append", "additional_suffixes_eval.append", "len", "len", "len", "len", "NotImplementedError", "methods_train.append", "debugnums_train.append", "uids_train.append", "datasets_train.append", "sources_train.append", "targets_train.append", "options_train.append", "additional_suffixes_train.append", "baseline_can_problem_names.append", "baseline_can_results_filenames.append", "baseline_can_source_names.append", "baseline_can_target_name.append", "baseline_can_test_name.append", "baseline_can_num_classes.append", "baseline_can_in_channels.append", "baseline_can_save_dir.append", "baseline_can_datasets.append", "baseline_can_sources.append", "baseline_can_targets.append", "baseline_can_uids.append", "baseline_can_options.append", "baseline_can_options.append", "NotImplementedError", "source.split"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.get_cpu_or_gpu", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.get_cpu_or_gpu", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.fill_in_template", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.fill_in_template", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.save_script", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.save_script", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.fill_in_template", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.fill_in_template", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.save_script", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.save_script", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.fill_in_template", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.fill_in_template", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.save_script", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.save_script", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.list_to_str", "home.repos.pwc.inspect_result.floft_calda.None.hyperparameters.get_hyperparameters_str"], ["", "def", "output_experiments", "(", "name", ",", "method_names", ",", "\n", "full_pairs", ",", "full_uids", ",", "upper_pairs", ",", "upper_uids", ",", "tuning", ",", "\n", "train_cpu", ",", "eval_cpu", ",", "eval_variant", "=", "\"best_target\"", ")", ":", "\n", "# Create experiments for each method. These are different for train/eval", "\n", "# only because we use multiple debugnums for the upper bound.", "\n", "    ", "methods_train", "=", "[", "]", "\n", "debugnums_train", "=", "[", "]", "\n", "uids_train", "=", "[", "]", "\n", "datasets_train", "=", "[", "]", "\n", "sources_train", "=", "[", "]", "\n", "targets_train", "=", "[", "]", "\n", "options_train", "=", "[", "]", "\n", "additional_suffixes_train", "=", "[", "]", "\n", "\n", "methods_eval", "=", "[", "]", "\n", "variants_eval", "=", "[", "]", "\n", "uids_eval", "=", "[", "]", "\n", "datasets_eval", "=", "[", "]", "\n", "sources_eval", "=", "[", "]", "\n", "targets_eval", "=", "[", "]", "\n", "additional_suffixes_eval", "=", "[", "]", "\n", "\n", "# For Baselines", "\n", "baseline_can_problem_names", "=", "[", "]", "\n", "baseline_can_source_names", "=", "[", "]", "\n", "baseline_can_target_name", "=", "[", "]", "\n", "baseline_can_test_name", "=", "[", "]", "\n", "baseline_can_num_classes", "=", "[", "]", "\n", "baseline_can_in_channels", "=", "[", "]", "\n", "baseline_can_save_dir", "=", "[", "]", "\n", "baseline_can_results_filenames", "=", "[", "]", "\n", "baseline_can_datasets", "=", "[", "]", "\n", "baseline_can_sources", "=", "[", "]", "\n", "baseline_can_targets", "=", "[", "]", "\n", "baseline_can_uids", "=", "[", "]", "\n", "baseline_can_options", "=", "[", "]", "\n", "\n", "# For each method", "\n", "for", "method_name", "in", "method_names", ":", "\n", "        ", "if", "method_name", "==", "\"upper\"", ":", "\n", "            ", "method_pairs", "=", "upper_pairs", "\n", "", "else", ":", "\n", "            ", "method_pairs", "=", "full_pairs", "\n", "\n", "# For each adaptation problem/experiment, e.g. adapt sources (1,2)", "\n", "# to 3 (of dataset D and method M).", "\n", "", "for", "i", ",", "(", "dataset_name", ",", "source", ",", "target", ")", "in", "enumerate", "(", "method_pairs", ")", ":", "\n", "# Whether this is a baseline or our method", "\n", "            ", "is_baseline", "=", "method_name", "in", "baseline_methods", "\n", "\n", "# If tuning, we iterate over possible hyperparameters", "\n", "if", "tuning", ":", "\n", "                ", "if", "is_baseline", ":", "\n", "                    ", "assert", "method_name", "==", "\"can\"", ",", "\"only CAN tuning supported for now\"", "\n", "\n", "if", "dataset_name", "in", "hyperparameter_tuning_experiments_list_can", "and", "method_name", "in", "hyperparameter_tuning_experiments_list_can", "[", "dataset_name", "]", ":", "\n", "                        ", "hyperparameter_set", "=", "hyperparameter_tuning_experiments_list_can", "[", "dataset_name", "]", "[", "method_name", "]", "\n", "", "else", ":", "\n", "# Skip this method since we aren't tuning it", "\n", "                        ", "continue", "\n", "", "", "else", ":", "\n", "                    ", "if", "dataset_name", "in", "hyperparameter_tuning_experiments_list", "and", "method_name", "in", "hyperparameter_tuning_experiments_list", "[", "dataset_name", "]", ":", "\n", "                        ", "hyperparameter_set", "=", "hyperparameter_tuning_experiments_list", "[", "dataset_name", "]", "[", "method_name", "]", "\n", "", "else", ":", "\n", "# Skip this method since we aren't tuning it", "\n", "                        ", "continue", "\n", "# Otherwise, just set to None so we go through the loop once (and never", "\n", "# use these values)", "\n", "", "", "", "else", ":", "\n", "                ", "hyperparameter_set", "=", "[", "(", "None", ",", "None", ",", "None", ")", "]", "\n", "additional_suffix", "=", "\"\"", "\n", "option", "=", "None", "\n", "\n", "# If tuning=False, then hyperparameter_set is just None's so we do this", "\n", "# loop once. Otherwise, we do it once per set of hyperparameters we want", "\n", "# to test.", "\n", "", "for", "hyperparam_folder", ",", "hyperparam_option", ",", "hyperparam_tuple", "in", "hyperparameter_set", ":", "\n", "# Initially, we don't know what hyperparameters to use. Thus, during", "\n", "# tuning we try a bunch of different ones and append a suffix based", "\n", "# on that set of parameters.", "\n", "                ", "if", "tuning", ":", "\n", "                    ", "additional_suffix", "=", "\"_\"", "+", "hyperparam_folder", "\n", "option", "=", "hyperparam_option", "\n", "", "else", ":", "\n", "                    ", "additional_suffix", "=", "\"\"", "\n", "\n", "# Hyperparameters if available -- sometimes use the parameters", "\n", "# from a different method for direct comparison.", "\n", "if", "\"calda\"", "in", "method_name", "or", "\"caldg\"", "in", "method_name", ":", "\n", "                        ", "hyp_method_name", "=", "\"calda_xs_h\"", "\n", "", "elif", "\"codats\"", "in", "method_name", "or", "method_name", "in", "[", "\"sleep_dg\"", ",", "\"aflac_dg\"", "]", ":", "\n", "                        ", "hyp_method_name", "=", "\"codats\"", "\n", "", "else", ":", "\n", "                        ", "hyp_method_name", "=", "method_name", "\n", "\n", "", "if", "dataset_name", "in", "[", "\"normal_n12_l3_inter0_intra1_5,0,0,0_sine\"", ",", "\"normal_n12_l3_inter1_intra1_5,0,0,0_sine\"", ",", "\"normal_n12_l3_inter2_intra1_5,0,0,0_sine\"", "]", ":", "\n", "                        ", "hyp_dataset_name", "=", "\"normal_n12_l3_inter2_intra1_5,0,0,0_sine\"", "\n", "", "elif", "dataset_name", "in", "[", "\"normal_n12_l3_inter0_intra1_0,0.5,0,0_sine\"", ",", "\"normal_n12_l3_inter1_intra1_0,0.5,0,0_sine\"", ",", "\"normal_n12_l3_inter2_intra1_0,0.5,0,0_sine\"", "]", ":", "\n", "                        ", "hyp_dataset_name", "=", "\"normal_n12_l3_inter2_intra1_0,0.5,0,0_sine\"", "\n", "", "elif", "dataset_name", "in", "[", "\"normal_n12_l3_inter1_intra0_0,0,5,0_sine\"", ",", "\"normal_n12_l3_inter1_intra1_0,0,5,0_sine\"", ",", "\"normal_n12_l3_inter1_intra2_0,0,5,0_sine\"", "]", ":", "\n", "                        ", "hyp_dataset_name", "=", "\"normal_n12_l3_inter1_intra2_0,0,5,0_sine\"", "\n", "", "elif", "dataset_name", "in", "[", "\"normal_n12_l3_inter1_intra0_0,0,0,0.5_sine\"", ",", "\"normal_n12_l3_inter1_intra1_0,0,0,0.5_sine\"", ",", "\"normal_n12_l3_inter1_intra2_0,0,0,0.5_sine\"", "]", ":", "\n", "                        ", "hyp_dataset_name", "=", "\"normal_n12_l3_inter1_intra2_0,0,0,0.5_sine\"", "\n", "", "elif", "\"normal_n\"", "in", "dataset_name", ":", "\n", "                        ", "raise", "NotImplementedError", "(", "\"Found unknown normal-synthetic dataset\"", ")", "\n", "", "else", ":", "\n", "                        ", "hyp_dataset_name", "=", "dataset_name", "\n", "\n", "", "option", "=", "get_hyperparameters_str", "(", "hyp_dataset_name", ",", "hyp_method_name", ")", "\n", "\n", "", "if", "method_name", "==", "\"upper\"", ":", "\n", "                    ", "uid", "=", "upper_uids", "[", "i", "]", "\n", "\n", "# Upper bound uses multiple debugnums for multiple runs rather", "\n", "# than multiple sets of source domains (since there is no", "\n", "# source domain)", "\n", "debugnums", "=", "[", "1", ",", "2", ",", "3", "]", "\n", "", "else", ":", "\n", "                    ", "uid", "=", "full_uids", "[", "i", "]", "\n", "\n", "# Only upper bound has more than one debugnum. The rest have the", "\n", "# multiple runs through multiple different sets of sources.", "\n", "debugnums", "=", "[", "1", "]", "\n", "\n", "", "if", "is_baseline", ":", "\n", "                    ", "if", "method_name", "==", "\"can\"", ":", "\n", "                        ", "for", "debugnum", "in", "debugnums", ":", "\n", "                            ", "baseline_can_problem_names", ".", "append", "(", "\n", "\"can_timeseries_{dataset_name}_{uid}_{debugnum}{suffix}\"", ".", "format", "(", "\n", "dataset_name", "=", "dataset_name", ",", "\n", "uid", "=", "uid", ",", "\n", "debugnum", "=", "debugnum", ",", "\n", "suffix", "=", "additional_suffix", ",", "\n", ")", "\n", ")", "\n", "baseline_can_results_filenames", ".", "append", "(", "\n", "\"results_{suffix}_best_target-{dataset_name}-{uid}-{method}\"", ".", "format", "(", "\n", "suffix", "=", "name", "+", "additional_suffix", ",", "\n", "dataset_name", "=", "dataset_name", ",", "\n", "uid", "=", "uid", ",", "\n", "method", "=", "method_name", ",", "\n", ")", "\n", ")", "\n", "baseline_can_source_names", ".", "append", "(", "[", "\n", "\"{dataset_name}_{domain}_train\"", ".", "format", "(", "\n", "dataset_name", "=", "dataset_name", ",", "\n", "domain", "=", "s", ",", "\n", ")", "\n", "for", "s", "in", "source", ".", "split", "(", "\",\"", ")", "\n", "]", ")", "\n", "baseline_can_target_name", ".", "append", "(", "\n", "\"{dataset_name}_{domain}_train\"", ".", "format", "(", "\n", "dataset_name", "=", "dataset_name", ",", "\n", "domain", "=", "target", ",", "\n", ")", "\n", ")", "\n", "baseline_can_test_name", ".", "append", "(", "\n", "\"{dataset_name}_{domain}_{test}\"", ".", "format", "(", "\n", "dataset_name", "=", "dataset_name", ",", "\n", "domain", "=", "target", ",", "\n", "# Don't look at real test set during tuning", "\n", "test", "=", "\"valid\"", "if", "tuning", "else", "\"test\"", "\n", ")", "\n", ")", "\n", "baseline_can_num_classes", ".", "append", "(", "\n", "num_classes_for_dataset", "[", "dataset_name", "]", "\n", ")", "\n", "baseline_can_in_channels", ".", "append", "(", "\n", "channels_for_dataset", "[", "dataset_name", "]", "\n", ")", "\n", "baseline_can_save_dir", ".", "append", "(", "\n", "\"./experiments/ckpt{debugnum}\"", ".", "format", "(", "\n", "debugnum", "=", "debugnum", ",", "\n", ")", "\n", ")", "\n", "baseline_can_datasets", ".", "append", "(", "\"\\\"{}\\\"\"", ".", "format", "(", "dataset_name", ")", ")", "\n", "baseline_can_sources", ".", "append", "(", "\"\\\"{}\\\"\"", ".", "format", "(", "source", ")", ")", "\n", "baseline_can_targets", ".", "append", "(", "\"\\\"{}\\\"\"", ".", "format", "(", "target", ")", ")", "\n", "baseline_can_uids", ".", "append", "(", "\"\\\"{}\\\"\"", ".", "format", "(", "uid", ")", ")", "\n", "if", "option", "is", "not", "None", ":", "\n", "                                ", "baseline_can_options", ".", "append", "(", "\"\\\"{}\\\"\"", ".", "format", "(", "option", ")", ")", "\n", "", "else", ":", "\n", "                                ", "baseline_can_options", ".", "append", "(", "\"\\\"\\\"\"", ")", "\n", "", "", "", "else", ":", "\n", "                        ", "raise", "NotImplementedError", "(", "\"baseline \"", "+", "method_name", ")", "\n", "", "", "else", ":", "\n", "                    ", "for", "debugnum", "in", "debugnums", ":", "\n", "# Train array values", "\n", "                        ", "methods_train", ".", "append", "(", "\"\\\"{}\\\"\"", ".", "format", "(", "method_name", ")", ")", "\n", "debugnums_train", ".", "append", "(", "\"\\\"{}\\\"\"", ".", "format", "(", "debugnum", ")", ")", "\n", "uids_train", ".", "append", "(", "\"\\\"{}\\\"\"", ".", "format", "(", "uid", ")", ")", "\n", "datasets_train", ".", "append", "(", "\"\\\"{}\\\"\"", ".", "format", "(", "dataset_name", ")", ")", "\n", "sources_train", ".", "append", "(", "\"\\\"{}\\\"\"", ".", "format", "(", "source", ")", ")", "\n", "targets_train", ".", "append", "(", "\"\\\"{}\\\"\"", ".", "format", "(", "target", ")", ")", "\n", "options_train", ".", "append", "(", "\"\\\"{}\\\"\"", ".", "format", "(", "option", ")", ")", "\n", "additional_suffixes_train", ".", "append", "(", "\"\\\"{}\\\"\"", ".", "format", "(", "additional_suffix", ")", ")", "\n", "\n", "# When doing the upper bound, we set the source to be the target and", "\n", "# have no target. Thus, we need to select based on the best source", "\n", "# instead.", "\n", "", "if", "method_name", "==", "\"upper\"", ":", "\n", "                        ", "variant", "=", "\"best_source\"", "\n", "", "else", ":", "\n", "                        ", "variant", "=", "eval_variant", "\n", "\n", "# Eval array values", "\n", "", "methods_eval", ".", "append", "(", "\"\\\"{}\\\"\"", ".", "format", "(", "method_name", ")", ")", "\n", "variants_eval", ".", "append", "(", "\"\\\"{}\\\"\"", ".", "format", "(", "variant", ")", ")", "\n", "uids_eval", ".", "append", "(", "\"\\\"{}\\\"\"", ".", "format", "(", "uid", ")", ")", "\n", "datasets_eval", ".", "append", "(", "\"\\\"{}\\\"\"", ".", "format", "(", "dataset_name", ")", ")", "\n", "sources_eval", ".", "append", "(", "\"\\\"{}\\\"\"", ".", "format", "(", "source", ")", ")", "\n", "targets_eval", ".", "append", "(", "\"\\\"{}\\\"\"", ".", "format", "(", "target", ")", ")", "\n", "additional_suffixes_eval", ".", "append", "(", "\"\\\"{}\\\"\"", ".", "format", "(", "additional_suffix", ")", ")", "\n", "\n", "# Sanity check (also checked in the .srun files)", "\n", "", "", "", "", "assert", "len", "(", "methods_train", ")", "==", "len", "(", "debugnums_train", ")", ",", "\"debugnums_train wrong length\"", "\n", "assert", "len", "(", "methods_train", ")", "==", "len", "(", "uids_train", ")", ",", "\"uids_train wrong length\"", "\n", "assert", "len", "(", "methods_train", ")", "==", "len", "(", "datasets_train", ")", ",", "\"datasets_train wrong length\"", "\n", "assert", "len", "(", "methods_train", ")", "==", "len", "(", "sources_train", ")", ",", "\"sources_train wrong length\"", "\n", "assert", "len", "(", "methods_train", ")", "==", "len", "(", "targets_train", ")", ",", "\"targets_train wrong length\"", "\n", "assert", "len", "(", "methods_train", ")", "==", "len", "(", "options_train", ")", ",", "\"options_train wrong length\"", "\n", "assert", "len", "(", "methods_train", ")", "==", "len", "(", "additional_suffixes_train", ")", ",", "\"additional_suffixes_train wrong length\"", "\n", "assert", "len", "(", "methods_eval", ")", "==", "len", "(", "variants_eval", ")", ",", "\"variants_eval wrong length\"", "\n", "assert", "len", "(", "methods_eval", ")", "==", "len", "(", "uids_eval", ")", ",", "\"uids_eval wrong length\"", "\n", "assert", "len", "(", "methods_eval", ")", "==", "len", "(", "datasets_eval", ")", ",", "\"datasets_eval wrong length\"", "\n", "assert", "len", "(", "methods_eval", ")", "==", "len", "(", "sources_eval", ")", ",", "\"sources_eval wrong length\"", "\n", "assert", "len", "(", "methods_eval", ")", "==", "len", "(", "targets_eval", ")", ",", "\"targets_eval wrong length\"", "\n", "assert", "len", "(", "methods_eval", ")", "==", "len", "(", "additional_suffixes_eval", ")", ",", "\"additional_suffixes_eval wrong length\"", "\n", "\n", "# Fill the values into the templates", "\n", "cpu_or_gpu_train", ",", "partitions_train", ",", "gpus_train", ",", "cpus_train", "=", "get_cpu_or_gpu", "(", "train_cpu", ")", "\n", "cpu_or_gpu_eval", ",", "partitions_eval", ",", "gpus_eval", ",", "cpus_eval", "=", "get_cpu_or_gpu", "(", "eval_cpu", ")", "\n", "\n", "# If we have any of our method (i.e. not just baselines)", "\n", "if", "len", "(", "methods_train", ")", ">", "0", ":", "\n", "        ", "if", "tuning", ":", "\n", "            ", "results_dir", "=", "\"results_tune\"", "\n", "# For hyperparameter tuning, we pass --notest so tuning only looks at", "\n", "# the validation set, never the real test set.", "\n", "additional_args", "=", "\"--notest\"", "\n", "", "else", ":", "\n", "            ", "results_dir", "=", "\"results\"", "\n", "additional_args", "=", "\"\"", "\n", "\n", "", "train_script", "=", "fill_in_template", "(", "\"kamiak_train.srun.template\"", ",", "{", "\n", "\"cpus\"", ":", "cpus_train", ",", "\n", "\"gpus\"", ":", "gpus_train", ",", "\n", "\"partitions\"", ":", "partitions_train", ",", "\n", "\"max_array\"", ":", "len", "(", "methods_train", ")", "-", "1", ",", "\n", "\"cpu_or_gpu\"", ":", "cpu_or_gpu_train", ",", "\n", "\"methods\"", ":", "list_to_str", "(", "methods_train", ")", ",", "\n", "\"debugnums\"", ":", "list_to_str", "(", "debugnums_train", ")", ",", "\n", "\"uids\"", ":", "list_to_str", "(", "uids_train", ")", ",", "\n", "\"datasets\"", ":", "list_to_str", "(", "datasets_train", ")", ",", "\n", "\"sources\"", ":", "list_to_str", "(", "sources_train", ")", ",", "\n", "\"targets\"", ":", "list_to_str", "(", "targets_train", ")", ",", "\n", "\"options\"", ":", "list_to_str", "(", "options_train", ")", ",", "\n", "\"additional_suffixes\"", ":", "list_to_str", "(", "additional_suffixes_train", ")", ",", "\n", "}", ")", "\n", "\n", "eval_script", "=", "fill_in_template", "(", "\"kamiak_eval.srun.template\"", ",", "{", "\n", "\"cpus\"", ":", "cpus_eval", ",", "\n", "\"gpus\"", ":", "gpus_eval", ",", "\n", "\"partitions\"", ":", "partitions_eval", ",", "\n", "\"max_array\"", ":", "len", "(", "methods_eval", ")", "-", "1", ",", "\n", "\"cpu_or_gpu\"", ":", "cpu_or_gpu_eval", ",", "\n", "\"results_dir\"", ":", "results_dir", ",", "\n", "\"methods\"", ":", "list_to_str", "(", "methods_eval", ")", ",", "\n", "\"variants\"", ":", "list_to_str", "(", "variants_eval", ")", ",", "# variants instead of debugnums", "\n", "\"uids\"", ":", "list_to_str", "(", "uids_eval", ")", ",", "\n", "\"datasets\"", ":", "list_to_str", "(", "datasets_eval", ")", ",", "\n", "\"sources\"", ":", "list_to_str", "(", "sources_eval", ")", ",", "\n", "\"targets\"", ":", "list_to_str", "(", "targets_eval", ")", ",", "\n", "# no options", "\n", "\"additional_suffixes\"", ":", "list_to_str", "(", "additional_suffixes_eval", ")", ",", "\n", "\"additional_args\"", ":", "additional_args", ",", "\n", "}", ")", "\n", "\n", "# Save scripts", "\n", "train_script_filename", "=", "\"kamiak_train_\"", "+", "name", "+", "\".srun\"", "\n", "eval_script_filename", "=", "\"kamiak_eval_\"", "+", "name", "+", "\".srun\"", "\n", "print", "(", "\"Writing\"", ",", "train_script_filename", ",", "eval_script_filename", ")", "\n", "save_script", "(", "train_script_filename", ",", "train_script", ")", "\n", "save_script", "(", "eval_script_filename", ",", "eval_script", ")", "\n", "\n", "# If we have baselines", "\n", "", "num_can_baselines", "=", "len", "(", "baseline_can_source_names", ")", "\n", "if", "num_can_baselines", ">", "0", ":", "\n", "# Generate config files for CAN", "\n", "        ", "for", "i", "in", "range", "(", "num_can_baselines", ")", ":", "\n", "            ", "train_script", "=", "fill_in_template", "(", "\"can_train.yml.template\"", ",", "{", "\n", "\"num_classes\"", ":", "baseline_can_num_classes", "[", "i", "]", ",", "\n", "\"sources_train\"", ":", "baseline_can_source_names", "[", "i", "]", ",", "\n", "\"target_train\"", ":", "baseline_can_target_name", "[", "i", "]", ",", "\n", "\"in_channels\"", ":", "baseline_can_in_channels", "[", "i", "]", ",", "\n", "\"target_test\"", ":", "baseline_can_test_name", "[", "i", "]", ",", "\n", "\"save_dir\"", ":", "baseline_can_save_dir", "[", "i", "]", ",", "\n", "}", ")", "\n", "\n", "eval_script", "=", "fill_in_template", "(", "\"can_eval.yml.template\"", ",", "{", "\n", "\"num_classes\"", ":", "baseline_can_num_classes", "[", "i", "]", ",", "\n", "\"in_channels\"", ":", "baseline_can_in_channels", "[", "i", "]", ",", "\n", "\"target_test\"", ":", "baseline_can_test_name", "[", "i", "]", ",", "\n", "\"save_dir\"", ":", "baseline_can_save_dir", "[", "i", "]", ",", "\n", "}", ")", "\n", "\n", "# Save scripts", "\n", "config_name", "=", "baseline_can_problem_names", "[", "i", "]", "\n", "base", "=", "\"../Contrastive-Adaptation-Network-for-Unsupervised-Domain-Adaptation/experiments/config/{name}\"", ".", "format", "(", "name", "=", "config_name", ")", "\n", "can_base", "=", "os", ".", "path", ".", "join", "(", "base", ",", "\"CAN\"", ")", "\n", "train_script_filename", "=", "\"{can_base}/{name}_train_train2val_cfg.yaml\"", ".", "format", "(", "can_base", "=", "can_base", ",", "name", "=", "config_name", ")", "\n", "eval_script_filename", "=", "\"{base}/{name}_test_val_cfg.yaml\"", ".", "format", "(", "base", "=", "base", ",", "name", "=", "config_name", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "base", ")", ":", "\n", "                ", "os", ".", "makedirs", "(", "base", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "can_base", ")", ":", "\n", "                ", "os", ".", "makedirs", "(", "can_base", ")", "\n", "\n", "", "print", "(", "\"Writing\"", ",", "train_script_filename", ",", "eval_script_filename", ")", "\n", "save_script", "(", "train_script_filename", ",", "train_script", ")", "\n", "save_script", "(", "eval_script_filename", ",", "eval_script", ")", "\n", "\n", "", "train_script", "=", "fill_in_template", "(", "\"kamiak_baseline_can_train.srun.template\"", ",", "{", "\n", "\"cpus\"", ":", "cpus_train", ",", "\n", "\"gpus\"", ":", "gpus_train", ",", "\n", "\"partitions\"", ":", "partitions_train", ",", "\n", "\"max_array\"", ":", "len", "(", "baseline_can_problem_names", ")", "-", "1", ",", "\n", "\"cpu_or_gpu\"", ":", "cpu_or_gpu_train", ",", "\n", "\"names\"", ":", "list_to_str", "(", "baseline_can_problem_names", ")", ",", "\n", "\"savedirs\"", ":", "list_to_str", "(", "baseline_can_save_dir", ")", ",", "\n", "\"options\"", ":", "list_to_str", "(", "baseline_can_options", ")", ",", "\n", "}", ")", "\n", "\n", "eval_script", "=", "fill_in_template", "(", "\"kamiak_baseline_can_eval.srun.template\"", ",", "{", "\n", "\"cpus\"", ":", "cpus_eval", ",", "\n", "\"gpus\"", ":", "gpus_eval", ",", "\n", "\"partitions\"", ":", "partitions_eval", ",", "\n", "\"max_array\"", ":", "len", "(", "baseline_can_problem_names", ")", "-", "1", ",", "\n", "\"cpu_or_gpu\"", ":", "cpu_or_gpu_eval", ",", "\n", "\"names\"", ":", "list_to_str", "(", "baseline_can_problem_names", ")", ",", "\n", "\"savedirs\"", ":", "list_to_str", "(", "baseline_can_save_dir", ")", ",", "\n", "\"output_filenames\"", ":", "list_to_str", "(", "baseline_can_results_filenames", ")", ",", "\n", "\"datasets\"", ":", "list_to_str", "(", "baseline_can_datasets", ")", ",", "\n", "\"sources\"", ":", "list_to_str", "(", "baseline_can_sources", ")", ",", "\n", "\"targets\"", ":", "list_to_str", "(", "baseline_can_targets", ")", ",", "\n", "\"uids\"", ":", "list_to_str", "(", "baseline_can_uids", ")", ",", "\n", "\"out_dir\"", ":", "\"results_tune\"", "if", "tuning", "else", "\"results\"", ",", "\n", "}", ")", "\n", "\n", "# Save scripts", "\n", "train_script_filename", "=", "\"kamiak_train_baseline_can_\"", "+", "name", "+", "\".srun\"", "\n", "eval_script_filename", "=", "\"kamiak_eval_baseline_can_\"", "+", "name", "+", "\".srun\"", "\n", "print", "(", "\"Writing\"", ",", "train_script_filename", ",", "eval_script_filename", ")", "\n", "save_script", "(", "train_script_filename", ",", "train_script", ")", "\n", "save_script", "(", "eval_script_filename", ",", "eval_script", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.main": [[722, 802], ["experiments_msda.generate_experiments_for_datasets", "experiments_msda.upper_from_full", "experiments_msda.output_list_of_targets", "experiments_msda.print_experiments_list_debug", "experiments_msda.output_experiments"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.generate_experiments_for_datasets", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.upper_from_full", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.output_list_of_targets", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.print_experiments_list_debug", "home.repos.pwc.inspect_result.floft_calda.None.experiments_msda.output_experiments"], ["", "", "def", "main", "(", "argv", ")", ":", "\n", "    ", "tuning", "=", "FLAGS", ".", "tune", "\n", "\n", "#", "\n", "# All methods we wish to use", "\n", "#", "\n", "# For which method name in the paper corresponds to which method name here,", "\n", "# see analysis.py", "\n", "#", "\n", "method_names", "=", "[", "]", "\n", "\n", "if", "FLAGS", ".", "can", ":", "\n", "        ", "method_names", "=", "[", "\"can\"", "]", "\n", "", "else", ":", "\n", "# Main results", "\n", "        ", "method_names", "+=", "[", "\"calda_xs_h\"", ",", "\"calda_any_r\"", ",", "\"codats\"", ",", "\"none\"", ",", "\"upper\"", "]", "\n", "\n", "if", "not", "tuning", ":", "\n", "# Weak supervision", "\n", "            ", "method_names", "+=", "[", "\"codats_ws\"", ",", "\"calda_xs_h_ws\"", ",", "\"calda_any_r_ws\"", "]", "\n", "# Domain generalization", "\n", "method_names", "+=", "[", "\"codats_dg\"", ",", "\"caldg_xs_h\"", ",", "\"caldg_any_r\"", ",", "\"sleep_dg\"", ",", "\"aflac_dg\"", "]", "\n", "# CALDA - only contrastive, no adversary", "\n", "method_names", "+=", "[", "\"calda_xs_h_noadv\"", ",", "\"calda_any_r_noadv\"", "]", "\n", "\n", "", "method_names", "+=", "[", "\"calda_xs_r\"", ",", "\"calda_in_r\"", ",", "\"calda_xs_r_p\"", ",", "\"calda_in_r_p\"", ",", "\"calda_any_r_p\"", ",", "\"calda_in_h\"", ",", "\"calda_any_h\"", ",", "\"calda_xs_h_p\"", ",", "\"calda_in_h_p\"", ",", "\"calda_any_h_p\"", "]", "\n", "\n", "#", "\n", "# All datasets we wish to use", "\n", "#", "\n", "", "dataset_names", "=", "[", "]", "\n", "# HAR datasets", "\n", "dataset_names", "+=", "[", "\"ucihar\"", ",", "\"ucihhar\"", ",", "\"wisdm_ar\"", ",", "\"wisdm_at\"", "]", "\n", "# EMG datasets", "\n", "dataset_names", "+=", "[", "\"myo\"", ",", "\"ninapro_db5_like_myo_noshift\"", "]", "\n", "# Synthetic datasets", "\n", "if", "tuning", ":", "\n", "        ", "dataset_names", "+=", "[", "\n", "\"normal_n12_l3_inter2_intra1_5,0,0,0_sine\"", ",", "\n", "\"normal_n12_l3_inter2_intra1_0,0.5,0,0_sine\"", ",", "\n", "\"normal_n12_l3_inter1_intra2_0,0,5,0_sine\"", ",", "\n", "\"normal_n12_l3_inter1_intra2_0,0,0,0.5_sine\"", ",", "\n", "]", "\n", "", "else", ":", "\n", "        ", "dataset_names", "+=", "[", "\n", "\"normal_n12_l3_inter0_intra1_5,0,0,0_sine\"", ",", "\n", "\"normal_n12_l3_inter1_intra1_5,0,0,0_sine\"", ",", "\n", "\"normal_n12_l3_inter2_intra1_5,0,0,0_sine\"", ",", "\n", "\"normal_n12_l3_inter0_intra1_0,0.5,0,0_sine\"", ",", "\n", "\"normal_n12_l3_inter1_intra1_0,0.5,0,0_sine\"", ",", "\n", "\"normal_n12_l3_inter2_intra1_0,0.5,0,0_sine\"", ",", "\n", "\"normal_n12_l3_inter1_intra0_0,0,5,0_sine\"", ",", "\n", "\"normal_n12_l3_inter1_intra1_0,0,5,0_sine\"", ",", "\n", "\"normal_n12_l3_inter1_intra2_0,0,5,0_sine\"", ",", "\n", "\"normal_n12_l3_inter1_intra0_0,0,0,0.5_sine\"", ",", "\n", "\"normal_n12_l3_inter1_intra1_0,0,0,0.5_sine\"", ",", "\n", "\"normal_n12_l3_inter1_intra2_0,0,0,0.5_sine\"", ",", "\n", "]", "\n", "\n", "# Generate full list of experiments", "\n", "", "full_pairs", ",", "full_uids", "=", "generate_experiments_for_datasets", "(", "dataset_names", ",", "tuning", ")", "\n", "upper_pairs", ",", "upper_uids", "=", "upper_from_full", "(", "full_pairs", ",", "full_uids", ")", "\n", "\n", "if", "FLAGS", ".", "output_targets", ":", "\n", "        ", "output_list_of_targets", "(", "full_pairs", ",", "display", "=", "True", ")", "\n", "", "else", ":", "\n", "# Check that these make sense by printing list", "\n", "        ", "print_experiments_list_debug", "(", "full_pairs", ",", "full_uids", ")", "\n", "\n", "if", "not", "FLAGS", ".", "only_print", ":", "\n", "            ", "assert", "FLAGS", ".", "name", "!=", "\"\"", ",", "\"Need to pass argument --name=<name>\"", "\n", "\n", "# Command line settings", "\n", "eval_variant", "=", "\"best_source\"", "if", "FLAGS", ".", "best_source", "else", "\"best_target\"", "\n", "\n", "# Create a train/eval script", "\n", "output_experiments", "(", "FLAGS", ".", "name", ",", "method_names", ",", "\n", "full_pairs", ",", "full_uids", ",", "upper_pairs", ",", "upper_uids", ",", "tuning", ",", "\n", "train_cpu", "=", "FLAGS", ".", "train_cpu", ",", "eval_cpu", "=", "FLAGS", ".", "eval_cpu", ",", "\n", "eval_variant", "=", "eval_variant", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.ws_analysis.get_csv": [[15, 83], ["open", "f.write", "collections.defaultdict", "results.keys", "results[].keys", "[].keys", "f.write", "csv_results[].append", "f.write", "ws_analysis.get_csv.f1"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.write", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.write", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.write"], ["def", "get_csv", "(", "results", ",", "output_filename", ",", "full", "=", "True", ")", ":", "\n", "# Output CSV rather than printing results", "\n", "    ", "def", "f1", "(", "v", ")", ":", "\n", "        ", "\"\"\" Format mean and stdev properly \"\"\"", "\n", "if", "full", ":", "\n", "            ", "if", "v", "[", "0", "]", "==", "-", "1", "and", "v", "[", "1", "]", "==", "-", "1", ":", "\n", "                ", "return", "\";\"", "\n", "\n", "", "return", "\"{};{}\"", ".", "format", "(", "v", "[", "0", "]", "*", "100", ",", "v", "[", "1", "]", "*", "100", ")", "\n", "", "else", ":", "\n", "            ", "if", "v", "[", "0", "]", "==", "-", "1", "and", "v", "[", "1", "]", "==", "-", "1", ":", "\n", "                ", "return", "\"\"", "\n", "\n", "", "return", "\"{:.1f} $\\\\pm$ {:.1f}\"", ".", "format", "(", "v", "[", "0", "]", "*", "100", ",", "v", "[", "1", "]", "*", "100", ")", "\n", "\n", "", "", "def", "f2", "(", "v", ")", ":", "\n", "        ", "\"\"\" Format single float properly \"\"\"", "\n", "if", "full", ":", "\n", "            ", "return", "\"{}\"", ".", "format", "(", "v", "*", "100", ")", "\n", "", "else", ":", "\n", "            ", "return", "\"{:.1f}\"", ".", "format", "(", "v", "*", "100", ")", "\n", "\n", "", "", "methods", "=", "[", "\n", "\"CoDATS\"", ",", "\n", "\"CALDA-XS,H\"", ",", "\n", "\"CALDA-Any,R\"", ",", "\n", "\"CoDATS-WS\"", ",", "\n", "\"CALDA-XS,H,WS\"", ",", "\n", "\"CALDA-Any,R,WS\"", ",", "\n", "]", "\n", "\n", "with", "open", "(", "output_filename", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "columns", "=", "[", "\"Dataset\"", ",", "\"n\"", ",", "\"Noise\"", "]", "\n", "\n", "# Stdev is separate column in full results", "\n", "if", "full", ":", "\n", "            ", "for", "m", "in", "methods", ":", "\n", "                ", "columns", "+=", "[", "m", ",", "\"+/-\"", "]", "\n", "", "", "else", ":", "\n", "            ", "columns", "+=", "methods", "\n", "\n", "", "columns", "=", "\";\"", ".", "join", "(", "columns", ")", "\n", "f", ".", "write", "(", "columns", "+", "\"\\n\"", ")", "\n", "\n", "csv_results", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "n", "in", "results", ".", "keys", "(", ")", ":", "\n", "            ", "for", "d", "in", "results", "[", "n", "]", ".", "keys", "(", ")", ":", "\n", "# key is the noise amount", "\n", "                ", "for", "key", "in", "results", "[", "n", "]", "[", "d", "]", ".", "keys", "(", ")", ":", "\n", "                    ", "method_results", "=", "[", "]", "\n", "\n", "for", "m", "in", "methods", ":", "\n", "                        ", "if", "m", "in", "results", "[", "n", "]", "[", "d", "]", "[", "key", "]", ":", "\n", "                            ", "method_results", "+=", "[", "results", "[", "n", "]", "[", "d", "]", "[", "key", "]", "[", "m", "]", "]", "\n", "", "else", ":", "\n", "                            ", "method_results", "+=", "[", "(", "-", "1", ",", "-", "1", ")", "]", "\n", "\n", "", "", "row", "=", "[", "d", ",", "n", ",", "key", "]", "+", "[", "f1", "(", "m", ")", "for", "m", "in", "method_results", "]", "\n", "row_str", "=", "\";\"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "row", "]", ")", "\n", "\n", "# Also keep raw data", "\n", "csv_results", "[", "n", "]", ".", "append", "(", "[", "d", ",", "n", ",", "key", "]", "+", "method_results", ")", "\n", "\n", "f", ".", "write", "(", "row_str", "+", "\"\\n\"", ")", "\n", "\n", "", "f", ".", "write", "(", "\";;;;;\\n\"", ")", "\n", "\n", "", "", "", "return", "csv_results", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.ws_analysis.all_results": [[85, 97], ["sampling_analysis.get_results", "sampling_analysis.get_results"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.sampling_analysis.get_results", "home.repos.pwc.inspect_result.floft_calda.None.sampling_analysis.get_results"], ["", "def", "all_results", "(", "prefixes", ",", "datasets", ",", "methods", ",", "n", ")", ":", "\n", "    ", "\"\"\" Get results for each value of n in addition to on average \"\"\"", "\n", "results", "=", "{", "\n", "\"Avg\"", ":", "get_results", "(", "datasets", ",", "methods", ",", "prefixes", ")", "\n", "}", "\n", "\n", "for", "dataset", "in", "datasets", ":", "\n", "        ", "for", "only_n", "in", "n", "[", "dataset", "]", ":", "\n", "# TODO handle more than one dataset", "\n", "            ", "results", "[", "only_n", "]", "=", "get_results", "(", "[", "dataset", "]", ",", "methods", ",", "prefixes", ",", "only_n", ")", "\n", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.ws_analysis.main": [[99, 148], ["ws_analysis.main.output_results"], "function", ["None"], ["", "def", "main", "(", "argv", ")", ":", "\n", "    ", "n", "=", "{", "\n", "\"ucihar\"", ":", "[", "2", ",", "8", ",", "14", ",", "20", ",", "26", "]", ",", "\n", "\"ucihhar\"", ":", "[", "2", ",", "3", ",", "4", ",", "5", ",", "6", "]", ",", "\n", "\"wisdm_ar\"", ":", "[", "2", ",", "8", ",", "14", ",", "20", ",", "26", "]", ",", "\n", "\"wisdm_at\"", ":", "[", "2", ",", "12", ",", "22", ",", "32", ",", "42", "]", ",", "\n", "}", "\n", "prefixes_ar", "=", "[", "\n", "(", "0", ",", "\"alltune1\"", ")", ",", "# for CALDA-XS,H on WISDM AR", "\n", "# (0, \"alltune1_bounds2\"),  # for upper/lower bounds on WISDM AR", "\n", "(", "0", ",", "\"alltune2\"", ")", ",", "# for CALDA-XS,H on WISDM AT", "\n", "(", "0", ",", "\"allin1\"", ")", ",", "# for CALDA-Any-R", "\n", "\n", "# Weak supervision experiments -- \"wsar\" for WISDM AR", "\n", "(", "0", ",", "\"weak2\"", ")", ",", "\n", "(", "0.05", ",", "\"wsar0.05\"", ")", ",", "\n", "(", "0.1", ",", "\"wsar0.1\"", ")", ",", "\n", "(", "0.2", ",", "\"wsar0.2\"", ")", ",", "\n", "(", "0.4", ",", "\"wsar0.4\"", ")", ",", "\n", "]", "\n", "prefixes_at", "=", "[", "\n", "(", "0", ",", "\"alltune1\"", ")", ",", "# for CALDA-XS,H on WISDM AR", "\n", "# (0, \"alltune1_bounds2\"),  # for upper/lower bounds on WISDM AR", "\n", "(", "0", ",", "\"alltune2\"", ")", ",", "# for CALDA-XS,H on WISDM AT", "\n", "(", "0", ",", "\"allin1\"", ")", ",", "# for CALDA-Any-R", "\n", "\n", "# Weak supervision experiments -- \"wsat\" for WISDM AT", "\n", "(", "0", ",", "\"weak2\"", ")", ",", "\n", "(", "0.05", ",", "\"wsat0.05\"", ")", ",", "\n", "(", "0.1", ",", "\"wsat0.1\"", ")", ",", "\n", "(", "0.2", ",", "\"wsat0.2\"", ")", ",", "\n", "(", "0.4", ",", "\"wsat0.4\"", ")", ",", "\n", "]", "\n", "\n", "non_ws_methods", "=", "[", "\n", "\"codats\"", ",", "\"calda_xs_h\"", ",", "\"calda_any_r\"", ",", "\n", "]", "\n", "\n", "methods", "=", "[", "\"codats_ws\"", ",", "\"calda_xs_h_ws\"", ",", "\"calda_any_r_ws\"", "]", "\n", "methods", "+=", "non_ws_methods", "\n", "\n", "def", "output_results", "(", "prefixes", ",", "output_filename_prefix", ",", "datasets", ")", ":", "\n", "        ", "\"\"\" Generate plot and CSV file \"\"\"", "\n", "results", "=", "all_results", "(", "prefixes", ",", "datasets", ",", "methods", ",", "n", ")", "\n", "csv_results", "=", "get_csv", "(", "results", ",", "output_filename_prefix", "+", "\".csv\"", ")", "\n", "return", "csv_results", "\n", "\n", "", "output_results", "(", "prefixes_ar", ",", "\"ws_analysis_ar\"", ",", "[", "\"wisdm_ar\"", "]", ")", "\n", "output_results", "(", "prefixes_at", ",", "\"ws_analysis_at\"", ",", "[", "\"wisdm_at\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.gpu_memory.set_gpu_memory": [[15, 36], ["tensorflow.config.experimental.list_physical_devices", "tensorflow.config.experimental.set_virtual_device_configuration", "print", "tensorflow.config.experimental.VirtualDeviceConfiguration"], "function", ["None"], ["def", "set_gpu_memory", "(", "gpumem", ")", ":", "\n", "    ", "\"\"\"\n    Set max GPU memory usage (if using a GPU), use 0 for all memory, i.e. don't\n    limit the usage.\n\n    Note: now gpumem is in MiB not a percentage of the total.\n    \"\"\"", "\n", "# Skip if we wish to use all the GPU memory", "\n", "if", "gpumem", "==", "0", ":", "\n", "        ", "return", "\n", "\n", "", "gpus", "=", "tf", ".", "config", ".", "experimental", ".", "list_physical_devices", "(", "\"GPU\"", ")", "\n", "\n", "if", "gpus", ":", "\n", "# Restrict TensorFlow to only allocate 1GB of memory on the first GPU", "\n", "        ", "try", ":", "\n", "            ", "tf", ".", "config", ".", "experimental", ".", "set_virtual_device_configuration", "(", "gpus", "[", "0", "]", ",", "\n", "[", "tf", ".", "config", ".", "experimental", ".", "VirtualDeviceConfiguration", "(", "memory_limit", "=", "gpumem", ")", "]", ")", "\n", "", "except", "RuntimeError", "as", "e", ":", "\n", "# Virtual devices must be set at program startup", "\n", "            ", "print", "(", "e", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.floft_calda.None.pickle_data.save_pickle": [[10, 30], ["os.path.exists", "absl.logging.debug", "absl.logging.debug", "os.path.exists", "os.rename", "open", "pickle.dump", "absl.logging.error"], "function", ["None"], ["def", "save_pickle", "(", "filename", ",", "data", ",", "overwrite", "=", "False", ",", "debug", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Pickle some data so we don't have to rerun everything\n\n    Usage:\n        savePickle(\"data.pickle\", (data1, data2))\n    \"\"\"", "\n", "if", "not", "overwrite", "and", "os", ".", "path", ".", "exists", "(", "filename", ")", ":", "\n", "        ", "logging", ".", "debug", "(", "\"Skipping, %s exists\"", ",", "filename", ")", "\n", "", "else", ":", "\n", "        ", "logging", ".", "debug", "(", "\"Pickling %s\"", ",", "filename", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "filename", ")", ":", "\n", "            ", "os", ".", "rename", "(", "filename", ",", "filename", "+", "\".bak\"", ")", "\n", "\n", "", "try", ":", "\n", "            ", "with", "open", "(", "filename", ",", "'wb'", ")", "as", "f", ":", "\n", "                ", "pickle", ".", "dump", "(", "data", ",", "f", ",", "pickle", ".", "HIGHEST_PROTOCOL", ")", "\n", "", "", "except", "Exception", "as", "error", ":", "\n", "            ", "logging", ".", "error", "(", "\"Error saving %s: %s\"", ",", "filename", ",", "error", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.pickle_data.load_pickle": [[32, 49], ["os.path.exists", "absl.logging.debug", "open", "pickle.load"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.load"], ["", "", "", "def", "load_pickle", "(", "filename", ",", "debug", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Load some data from a pickle file\n\n    Usage:\n        if os.path.exists(\"data.pickle\"):\n            data1, data2 = loadPickle(\"data.pickle\")\n    \"\"\"", "\n", "if", "os", ".", "path", ".", "exists", "(", "filename", ")", ":", "\n", "        ", "logging", ".", "debug", "(", "\"Loading %s\"", ",", "filename", ")", "\n", "\n", "with", "open", "(", "filename", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "data", "=", "pickle", ".", "load", "(", "f", ")", "\n", "\n", "", "return", "data", "\n", "\n", "", "return", "None", "\n", "", ""]], "home.repos.pwc.inspect_result.floft_calda.None.main_eval.get_gpus": [[43, 48], ["int", "os.getenv().split", "os.getenv"], "function", ["None"], ["def", "get_gpus", "(", ")", ":", "\n", "    ", "\"\"\"\n    Get the list of GPU ID's that SLURM is giving us\n    \"\"\"", "\n", "return", "[", "int", "(", "x", ")", "for", "x", "in", "os", ".", "getenv", "(", "\"SLURM_JOB_GPUS\"", ",", "\"\"", ")", ".", "split", "(", "\",\"", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.main_eval.get_pool_id": [[50, 59], ["multiprocessing.current_process"], "function", ["None"], ["", "def", "get_pool_id", "(", ")", ":", "\n", "    ", "\"\"\"\n    Get unique ID for this process in the job pool. It'll range from\n    1 to max_jobs. See: https://stackoverflow.com/a/10192611/2698494\n\n    Will return a number in [0,max_jobs)\n    \"\"\"", "\n", "current", "=", "multiprocessing", ".", "current_process", "(", ")", "\n", "return", "current", ".", "_identity", "[", "0", "]", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.main_eval.setup_gpu_for_process": [[61, 85], ["gpu_memory.set_gpu_memory", "main_eval.get_gpus", "main_eval.get_pool_id", "str"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.gpu_memory.set_gpu_memory", "home.repos.pwc.inspect_result.floft_calda.None.main_eval.get_gpus", "home.repos.pwc.inspect_result.floft_calda.None.main_eval.get_pool_id"], ["", "def", "setup_gpu_for_process", "(", "gpumem", ",", "multi_gpu", ")", ":", "\n", "    ", "\"\"\" Handle setting GPU memory or which GPU to use in each process \"\"\"", "\n", "# We need to do this in the process since otherwise TF can't access cuDNN", "\n", "# for some reason. But, we only need to do this the first time we create the", "\n", "# process. It'll error on any subsequent calls (since the pool re-uses", "\n", "# process).", "\n", "try", ":", "\n", "        ", "set_gpu_memory", "(", "gpumem", ")", "\n", "", "except", "RuntimeError", ":", "\n", "        ", "pass", "# Ignore: \"RuntimeError: GPU options must be set at program startup\"", "\n", "\n", "# Get what GPU to run this on, otherwise it'll default to whatever the", "\n", "# first one is", "\n", "", "if", "multi_gpu", ":", "\n", "# Get all GPUs SLURM gave to us and what process in the pool this is", "\n", "        ", "available_gpus", "=", "get_gpus", "(", ")", "\n", "pool_id", "=", "get_pool_id", "(", ")", "\n", "\n", "# Pick which one based on pool id", "\n", "gpu", "=", "available_gpus", "[", "pool_id", "]", "\n", "\n", "# Only let TensorFlow see this GPU. I tried tf.device, but somehow", "\n", "# each process still put some stuff into memory on every GPU.", "\n", "os", ".", "environ", "[", "\"CUDA_VISIBLE_DEVICES\"", "]", "=", "str", "(", "gpu", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.main_eval.get_models_to_evaluate": [[87, 107], ["pathlib.Path().glob", "file_utils.get_config", "os.path.join", "os.path.exists", "models_to_evaluate.append", "pathlib.Path", "str", "str"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.file_utils.get_config"], ["", "", "def", "get_models_to_evaluate", "(", ")", ":", "\n", "    ", "\"\"\"\n    Returns the models to evaluate based on what is in logdir and modeldir\n    specified as command line arguments. The matching pattern is specified by\n    the match argument.\n\n    Returns: [(log_dir, model_dir, config), ...]\n    \"\"\"", "\n", "files", "=", "pathlib", ".", "Path", "(", "FLAGS", ".", "logdir", ")", ".", "glob", "(", "FLAGS", ".", "match", ")", "\n", "models_to_evaluate", "=", "[", "]", "\n", "\n", "for", "log_dir", "in", "files", ":", "\n", "        ", "config", "=", "file_utils", ".", "get_config", "(", "log_dir", ")", "\n", "# Note: previously used .stem, but that excludes any suffix, i.e. it", "\n", "# breaks if there's a dot in log_dir.", "\n", "model_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "modeldir", ",", "log_dir", ".", "name", ")", "\n", "assert", "os", ".", "path", ".", "exists", "(", "model_dir", ")", ",", "\"Model does not exist \"", "+", "str", "(", "model_dir", ")", "\n", "models_to_evaluate", ".", "append", "(", "(", "str", "(", "log_dir", ")", ",", "model_dir", ",", "config", ")", ")", "\n", "\n", "", "return", "models_to_evaluate", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.main_eval.save_results": [[109, 128], ["all_process_results.append", "open", "yaml.dump"], "function", ["None"], ["", "def", "save_results", "(", "process_results", ",", "results_filename", ")", ":", "\n", "    ", "all_process_results", "=", "[", "]", "\n", "\n", "for", "pr", "in", "process_results", ":", "\n", "        ", "log_dir", ",", "model_dir", ",", "config", ",", "results", ",", "max_accuracy_step", ",", "max_accuracy", "=", "pr", "\n", "\n", "all_process_results", ".", "append", "(", "{", "\n", "\"logdir\"", ":", "log_dir", ",", "\n", "\"modeldir\"", ":", "model_dir", ",", "\n", "\"config\"", ":", "config", ",", "\n", "\"results\"", ":", "results", ",", "\n", "\"max_accuracy_step\"", ":", "max_accuracy_step", ",", "\n", "\"max_accuracy\"", ":", "max_accuracy", ",", "\n", "}", ")", "\n", "\n", "# Write the config file", "\n", "", "with", "open", "(", "results_filename", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "yaml", ".", "dump", "(", "all_process_results", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.main_eval.process_model": [[130, 248], ["main_eval.setup_gpu_for_process", "load_datasets.load_da", "methods.get_method", "metrics.Metrics", "metrics.Metrics.train_eval", "metrics.Metrics.test", "metrics.Metrics.results", "len", "tensorflow.train.Checkpoint", "checkpoints.CheckpointManager", "source_modality_subset.split", "checkpoints.CheckpointManager.restore_latest", "checkpoints.CheckpointManager.latest_step", "int", "new_modality_subset.append", "checkpoints.CheckpointManager.restore_best_source", "checkpoints.CheckpointManager.best_step_source", "shared_modalities.split", "checkpoints.CheckpointManager.restore_best_target", "checkpoints.CheckpointManager.best_step_target", "NotImplementedError"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.main_eval.setup_gpu_for_process", "home.repos.pwc.inspect_result.floft_calda.None.load_datasets.load_da", "home.repos.pwc.inspect_result.floft_calda.None.analysis.get_method", "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics.train_eval", "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics.test", "home.repos.pwc.inspect_result.floft_calda.None.metrics.Metrics.results", "home.repos.pwc.inspect_result.floft_calda.None.checkpoints.CheckpointManager.restore_latest", "home.repos.pwc.inspect_result.floft_calda.None.checkpoints.CheckpointManager.latest_step", "home.repos.pwc.inspect_result.floft_calda.None.checkpoints.CheckpointManager.restore_best_source", "home.repos.pwc.inspect_result.floft_calda.None.checkpoints.CheckpointManager.best_step_source", "home.repos.pwc.inspect_result.floft_calda.None.checkpoints.CheckpointManager.restore_best_target", "home.repos.pwc.inspect_result.floft_calda.None.checkpoints.CheckpointManager.best_step_target"], ["", "", "def", "process_model", "(", "log_dir", ",", "model_dir", ",", "config", ",", "gpumem", ",", "multi_gpu", ")", ":", "\n", "    ", "\"\"\" Evaluate a model on the train/test data and compute the results \"\"\"", "\n", "setup_gpu_for_process", "(", "gpumem", ",", "multi_gpu", ")", "\n", "\n", "dataset_name", "=", "config", "[", "\"dataset\"", "]", "\n", "method_name", "=", "config", "[", "\"method\"", "]", "\n", "model_name", "=", "config", "[", "\"model\"", "]", "\n", "sources", "=", "config", "[", "\"sources\"", "]", "\n", "target", "=", "config", "[", "\"target\"", "]", "\n", "moving_average", "=", "config", "[", "\"moving_average\"", "]", "\n", "ensemble_size", "=", "config", "[", "\"ensemble\"", "]", "\n", "shared_modalities", "=", "config", "[", "\"shared_modalities\"", "]", "\n", "source_modality_subset", "=", "config", "[", "\"source_modality_subset\"", "]", "\n", "target_modality_subset", "=", "config", "[", "\"target_modality_subset\"", "]", "\n", "share_most_weights", "=", "config", "[", "\"share_most_weights\"", "]", "\n", "\n", "# Changes for upper bound -- upper bound is actually method \"none\" but", "\n", "# without a target domain", "\n", "#", "\n", "# Note: copied from main.py but removed instances of \"FLAGS.\"", "\n", "if", "method_name", "==", "\"upper\"", ":", "\n", "        ", "method_name", "=", "\"none\"", "\n", "sources", "=", "target", "\n", "target", "=", "\"\"", "\n", "source_modality_subset", "=", "target_modality_subset", "\n", "target_modality_subset", "=", "\"\"", "\n", "\n", "# Remove unused modality since the no adaptation / upper bound will error", "\n", "", "if", "method_name", "==", "\"none\"", ":", "# or it was upper before the above if", "\n", "        ", "if", "source_modality_subset", "!=", "\"\"", ":", "\n", "# Fix \"Weights for model sequential_1 have not yet been created. Weights", "\n", "# are created when the Model is first called on inputs or `build()` is", "\n", "# called with an `input_shape`.\" e.g. when the above yields", "\n", "# source_modality_subset = \"1,0\" and shared_modalities=\"0\" we end up", "\n", "# never using the second modality's FE or DC. Thus, just throw out the", "\n", "# unused modality. For example, here this would end up just setting", "\n", "# source_modality_subset to \"1\".", "\n", "            ", "modality_subset_list", "=", "source_modality_subset", ".", "split", "(", "\",\"", ")", "# \"1\",\"0\"", "\n", "shared_modalities_list", "=", "[", "int", "(", "x", ")", "for", "x", "in", "shared_modalities", ".", "split", "(", "\",\"", ")", "]", "# 0", "\n", "new_modality_subset", "=", "[", "]", "\n", "\n", "for", "modality", "in", "shared_modalities_list", ":", "\n", "                ", "new_modality_subset", ".", "append", "(", "modality_subset_list", "[", "modality", "]", ")", "\n", "\n", "", "source_modality_subset", "=", "\",\"", ".", "join", "(", "new_modality_subset", ")", "\n", "\n", "# If using a domain generalization method, then split among sources not", "\n", "# sources and target. Same for weak supervision.", "\n", "# TODO keep this up to date with domain generalization method list", "\n", "", "", "domain_generalization", "=", "\"_dg\"", "in", "method_name", "or", "\"caldg\"", "in", "method_name", "\n", "weak_supervision", "=", "\"_ws\"", "in", "method_name", "\n", "override_batch_division", "=", "domain_generalization", "or", "weak_supervision", "\n", "\n", "# Load datasets", "\n", "source_datasets", ",", "target_dataset", "=", "load_datasets", ".", "load_da", "(", "dataset_name", ",", "\n", "sources", ",", "target", ",", "test", "=", "FLAGS", ".", "test", ",", "\n", "source_modality_subset", "=", "source_modality_subset", ",", "\n", "target_modality_subset", "=", "target_modality_subset", ",", "\n", "override_batch_division", "=", "override_batch_division", ")", "\n", "\n", "# Load the method, model, etc.", "\n", "# Note: {global,num}_step are for training, so it doesn't matter what", "\n", "# we set them to here", "\n", "method", "=", "methods", ".", "get_method", "(", "method_name", ",", "\n", "source_datasets", "=", "source_datasets", ",", "\n", "target_dataset", "=", "target_dataset", ",", "\n", "model_name", "=", "model_name", ",", "\n", "global_step", "=", "1", ",", "total_steps", "=", "1", ",", "\n", "moving_average", "=", "moving_average", ",", "\n", "ensemble_size", "=", "ensemble_size", ",", "\n", "shared_modalities", "=", "shared_modalities", ",", "\n", "share_most_weights", "=", "share_most_weights", ",", "\n", "dataset_name", "=", "dataset_name", ")", "\n", "\n", "# Load model from checkpoint (if there's anything in the checkpoint)", "\n", "if", "len", "(", "method", ".", "checkpoint_variables", ")", ">", "0", ":", "\n", "        ", "checkpoint", "=", "tf", ".", "train", ".", "Checkpoint", "(", "**", "method", ".", "checkpoint_variables", ")", "\n", "checkpoint_manager", "=", "CheckpointManager", "(", "checkpoint", ",", "model_dir", ",", "log_dir", ")", "\n", "\n", "if", "FLAGS", ".", "selection", "==", "\"last\"", ":", "\n", "            ", "checkpoint_manager", ".", "restore_latest", "(", ")", "\n", "max_accuracy_step", "=", "checkpoint_manager", ".", "latest_step", "(", ")", "\n", "max_accuracy", "=", "None", "# We don't really care...", "\n", "found", "=", "checkpoint_manager", ".", "found_last", "\n", "", "elif", "FLAGS", ".", "selection", "==", "\"best_source\"", ":", "\n", "            ", "checkpoint_manager", ".", "restore_best_source", "(", ")", "\n", "max_accuracy_step", "=", "checkpoint_manager", ".", "best_step_source", "(", ")", "\n", "max_accuracy", "=", "checkpoint_manager", ".", "best_validation_source", "\n", "found", "=", "checkpoint_manager", ".", "found_best_source", "\n", "", "elif", "FLAGS", ".", "selection", "==", "\"best_target\"", ":", "\n", "            ", "checkpoint_manager", ".", "restore_best_target", "(", ")", "\n", "max_accuracy_step", "=", "checkpoint_manager", ".", "best_step_target", "(", ")", "\n", "max_accuracy", "=", "checkpoint_manager", ".", "best_validation_target", "\n", "found", "=", "checkpoint_manager", ".", "found_best_target", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"unknown --selection argument\"", ")", "\n", "", "", "else", ":", "\n", "        ", "max_accuracy_step", "=", "None", "\n", "max_accuracy", "=", "None", "\n", "found", "=", "True", "\n", "\n", "# Metrics", "\n", "", "has_target_domain", "=", "target_dataset", "is", "not", "None", "\n", "metrics", "=", "Metrics", "(", "log_dir", ",", "method", ",", "source_datasets", ",", "target_dataset", ",", "\n", "has_target_domain", ")", "\n", "\n", "# If not found, give up", "\n", "if", "not", "found", ":", "\n", "        ", "return", "log_dir", ",", "model_dir", ",", "config", ",", "{", "}", ",", "None", ",", "None", "\n", "\n", "# Evaluate on both datasets", "\n", "", "metrics", ".", "train_eval", "(", ")", "\n", "metrics", ".", "test", "(", "evaluation", "=", "True", ")", "\n", "\n", "# Get results", "\n", "results", "=", "metrics", ".", "results", "(", ")", "\n", "\n", "return", "log_dir", ",", "model_dir", ",", "config", ",", "results", ",", "max_accuracy_step", ",", "max_accuracy", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.main_eval.main": [[250, 284], ["main_eval.get_models_to_evaluate", "main_eval.save_results", "commands.append", "pool.run_job_pool", "pool.run_job_pool.append", "main_eval.process_model"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.main_eval.get_models_to_evaluate", "home.repos.pwc.inspect_result.floft_calda.None.main_eval.save_results", "home.repos.pwc.inspect_result.floft_calda.None.pool.run_job_pool", "home.repos.pwc.inspect_result.floft_calda.None.main_eval.process_model"], ["", "def", "main", "(", "argv", ")", ":", "\n", "# If single GPU, then split memory between jobs. But, if multiple GPUs,", "\n", "# each GPU has its own memory, so don't divide it up.", "\n", "#", "\n", "# If multiple GPUs, the jobs are split by GPU not by the \"jobs\" argument, so", "\n", "# ignore it and just set jobs to the GPU count.", "\n", "    ", "if", "FLAGS", ".", "gpus", "==", "1", ":", "\n", "        ", "jobs", "=", "FLAGS", ".", "jobs", "\n", "gpumem", "=", "FLAGS", ".", "gpumem", "/", "jobs", "\n", "multi_gpu", "=", "False", "\n", "", "else", ":", "\n", "        ", "jobs", "=", "FLAGS", ".", "gpus", "\n", "gpumem", "=", "FLAGS", ".", "gpumem", "\n", "multi_gpu", "=", "True", "\n", "\n", "# Find models in the model/log directories", "\n", "", "models_to_evaluate", "=", "get_models_to_evaluate", "(", ")", "\n", "\n", "# Run in parallel", "\n", "commands", "=", "[", "]", "\n", "\n", "for", "model_params", "in", "models_to_evaluate", ":", "\n", "        ", "commands", ".", "append", "(", "(", "*", "model_params", ",", "gpumem", ",", "multi_gpu", ")", ")", "\n", "\n", "", "if", "jobs", "==", "1", ":", "# Eases debugging, printing even if it errors", "\n", "        ", "process_results", "=", "[", "]", "\n", "\n", "for", "c", "in", "commands", ":", "\n", "            ", "process_results", ".", "append", "(", "process_model", "(", "*", "c", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "process_results", "=", "run_job_pool", "(", "process_model", ",", "commands", ",", "cores", "=", "jobs", ")", "\n", "\n", "# Save results", "\n", "", "save_results", "(", "process_results", ",", "FLAGS", ".", "output_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.load_datasets.Dataset.__init__": [[36, 115], ["load_datasets.Dataset.load_dataset", "len", "len", "str", "str"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.load_datasets.Dataset.load_dataset"], ["def", "__init__", "(", "self", ",", "num_classes", ",", "class_labels", ",", "num_domains", ",", "num_modalities", ",", "\n", "train_filenames", ",", "test_filenames", ",", "domain_id", "=", "None", ",", "\n", "train_batch", "=", "None", ",", "eval_batch", "=", "None", ",", "\n", "shuffle_buffer", "=", "None", ",", "prefetch_buffer", "=", "None", ",", "\n", "eval_shuffle_seed", "=", "None", ",", "cache", "=", "None", ",", "\n", "train_max_examples", "=", "None", ",", "eval_max_examples", "=", "None", ",", "\n", "tune_num_parallel_calls", "=", "None", ",", "feature_subset", "=", "None", ",", "\n", "modality_subset", "=", "None", ",", "max_examples_percentage", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Initialize dataset\n\n        Must specify num_classes and class_labels (the names of the classes).\n        Other arguments if None are defaults from command line flags.\n\n        For example:\n            Dataset(num_classes=2, class_labels=[\"class1\", \"class2\"])\n        \"\"\"", "\n", "# Sanity checks", "\n", "assert", "num_classes", "==", "len", "(", "class_labels", ")", ",", "\"num_classes != len(class_labels)\"", "\n", "\n", "# Set parameters", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "class_labels", "=", "class_labels", "\n", "self", ".", "num_domains", "=", "num_domains", "\n", "self", ".", "num_modalities", "=", "num_modalities", "\n", "self", ".", "domain_id", "=", "domain_id", "\n", "self", ".", "train_batch", "=", "train_batch", "\n", "self", ".", "eval_batch", "=", "eval_batch", "\n", "self", ".", "shuffle_buffer", "=", "shuffle_buffer", "\n", "self", ".", "prefetch_buffer", "=", "prefetch_buffer", "\n", "self", ".", "eval_shuffle_seed", "=", "eval_shuffle_seed", "\n", "self", ".", "cache", "=", "cache", "\n", "self", ".", "eval_max_examples", "=", "eval_max_examples", "\n", "self", ".", "train_max_examples", "=", "train_max_examples", "\n", "self", ".", "tune_num_parallel_calls", "=", "tune_num_parallel_calls", "\n", "self", ".", "feature_subset", "=", "feature_subset", "\n", "self", ".", "modality_subset", "=", "modality_subset", "\n", "self", ".", "max_examples_percentage", "=", "max_examples_percentage", "\n", "\n", "# Unaffected by the modality subset since we need to know how many", "\n", "# modalities are in the tfrecord file when loading", "\n", "self", ".", "tfrecord_num_modalities", "=", "num_modalities", "\n", "\n", "# If we set a modality subset, then make sure they exist and then update", "\n", "# num_modalities to the subset we chose", "\n", "if", "modality_subset", "is", "not", "None", ":", "\n", "            ", "for", "modality", "in", "modality_subset", ":", "\n", "                ", "assert", "modality", "<", "num_modalities", ",", "str", "(", "modality", ")", "+", "\" is not < num_modalities = \"", "+", "str", "(", "num_modalities", ")", "\n", "\n", "", "self", ".", "num_modalities", "=", "len", "(", "modality_subset", ")", "\n", "\n", "# Set defaults if not specified", "\n", "", "if", "self", ".", "train_batch", "is", "None", ":", "\n", "            ", "self", ".", "train_batch", "=", "FLAGS", ".", "train_batch", "\n", "", "if", "self", ".", "eval_batch", "is", "None", ":", "\n", "            ", "self", ".", "eval_batch", "=", "FLAGS", ".", "eval_batch", "\n", "", "if", "self", ".", "shuffle_buffer", "is", "None", ":", "\n", "            ", "self", ".", "shuffle_buffer", "=", "FLAGS", ".", "shuffle_buffer", "\n", "", "if", "self", ".", "prefetch_buffer", "is", "None", ":", "\n", "            ", "self", ".", "prefetch_buffer", "=", "FLAGS", ".", "prefetch_buffer", "\n", "", "if", "self", ".", "eval_shuffle_seed", "is", "None", ":", "\n", "            ", "self", ".", "eval_shuffle_seed", "=", "FLAGS", ".", "eval_shuffle_seed", "\n", "", "if", "self", ".", "cache", "is", "None", ":", "\n", "            ", "self", ".", "cache", "=", "FLAGS", ".", "cache", "\n", "", "if", "self", ".", "eval_max_examples", "is", "None", ":", "\n", "            ", "self", ".", "eval_max_examples", "=", "FLAGS", ".", "eval_max_examples", "\n", "", "if", "self", ".", "train_max_examples", "is", "None", ":", "\n", "            ", "self", ".", "train_max_examples", "=", "FLAGS", ".", "train_max_examples", "\n", "", "if", "self", ".", "tune_num_parallel_calls", "is", "None", ":", "\n", "            ", "self", ".", "tune_num_parallel_calls", "=", "FLAGS", ".", "tune_num_parallel_calls", "\n", "", "if", "self", ".", "max_examples_percentage", "is", "None", ":", "\n", "            ", "self", ".", "max_examples_percentage", "=", "FLAGS", ".", "max_examples_percentage", "\n", "\n", "# Load the dataset", "\n", "", "self", ".", "train", ",", "self", ".", "train_evaluation", ",", "self", ".", "test_evaluation", "=", "self", ".", "load_dataset", "(", "train_filenames", ",", "test_filenames", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.load_datasets.Dataset.get_feature_description": [[116, 142], ["range", "modalities.append", "tensorflow.io.FixedLenFeature", "load_datasets.Dataset.get_feature_description._feature"], "methods", ["None"], ["", "def", "get_feature_description", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Create feature description based on the number of modalities\n\n        For example, if self.tfrecord_num_modalities = 2, we get:\n            x_0, x_1, y\n        \"\"\"", "\n", "modalities", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "tfrecord_num_modalities", ")", ":", "\n", "            ", "modalities", ".", "append", "(", "\"x_\"", "+", "str", "(", "i", ")", ")", "\n", "\n", "# All datasets have a label (y) \"feature\" and example id (id)", "\n", "", "modalities", "+=", "[", "\"y\"", ",", "\"id\"", "]", "\n", "\n", "# Create a description of the features", "\n", "# See: https://www.tensorflow.org/tutorials/load_data/tf-records", "\n", "def", "_feature", "(", ")", ":", "\n", "            ", "return", "tf", ".", "io", ".", "FixedLenFeature", "(", "[", "]", ",", "tf", ".", "string", ")", "\n", "\n", "", "feature_description", "=", "{", "}", "\n", "\n", "for", "modality", "in", "modalities", ":", "\n", "            ", "feature_description", "[", "modality", "]", "=", "_feature", "(", ")", "\n", "\n", "", "return", "feature_description", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.load_datasets.Dataset.load_tfrecords": [[143, 303], ["load_datasets.Dataset.get_feature_description", "tensorflow.data.Dataset.from_tensor_slices", "tensorflow.data.Dataset.from_tensor_slices.interleave", "dataset.shuffle().repeat.shuffle().repeat.batch", "dataset.shuffle().repeat.shuffle().repeat.prefetch", "len", "tensorflow.io.parse_single_example", "list", "list.sort", "tensorflow.io.parse_tensor", "tensorflow.io.parse_tensor", "dataset.shuffle().repeat.shuffle().repeat.take", "dataset.shuffle().repeat.shuffle().repeat.map", "dataset.shuffle().repeat.shuffle().repeat.cache", "dataset.shuffle().repeat.shuffle().repeat.map", "tensorflow.io.parse_single_example.keys", "tensorflow.io.parse_tensor", "xs.append", "tuple", "tensorflow.data.TFRecordDataset().prefetch", "len", "math.ceil", "print", "int", "int", "dataset.shuffle().repeat.shuffle().repeat.shuffle", "dataset.shuffle().repeat.shuffle().repeat.shuffle().repeat", "tensorflow.slice", "tensorflow.slice", "tensorflow.gather", "xs_subset.append", "tensorflow.data.TFRecordDataset", "dataset.shuffle().repeat.shuffle().repeat.shuffle", "tensorflow.minimum", "tensorflow.minimum", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.load_datasets.Dataset.get_feature_description"], ["", "def", "load_tfrecords", "(", "self", ",", "filenames", ",", "batch_size", ",", "count", "=", "False", ",", "evaluation", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Load data from .tfrecord files (requires less memory but more disk space)\n        \"\"\"", "\n", "if", "len", "(", "filenames", ")", "==", "0", ":", "\n", "            ", "return", "None", "\n", "\n", "# Compute feature description based on the number of modalities", "\n", "", "feature_description", "=", "self", ".", "get_feature_description", "(", ")", "\n", "\n", "def", "_parse_example_function", "(", "example_proto", ")", ":", "\n", "            ", "\"\"\"\n            Parse the input tf.Example proto using the dictionary above.\n            parse_single_example is without a batch, parse_example is with batches\n\n            What's parsed returns byte strings, but really we want to get the\n            tensors back that we encoded with tf.io.serialize_tensor() earlier,\n            so also run tf.io.parse_tensor\n            \"\"\"", "\n", "parsed", "=", "tf", ".", "io", ".", "parse_single_example", "(", "serialized", "=", "example_proto", ",", "\n", "features", "=", "feature_description", ")", "\n", "\n", "# Sort names for consistency", "\n", "names", "=", "list", "(", "parsed", ".", "keys", "(", ")", ")", "\n", "names", ".", "sort", "(", ")", "\n", "\n", "xs", "=", "[", "]", "\n", "\n", "for", "name", "in", "names", ":", "\n", "# Skip \"y\" and \"id\" since we know these are here", "\n", "                ", "if", "name", "==", "\"y\"", "or", "name", "==", "\"id\"", ":", "\n", "                    ", "continue", "\n", "\n", "# Parse", "\n", "", "x", "=", "tf", ".", "io", ".", "parse_tensor", "(", "parsed", "[", "name", "]", ",", "tf", ".", "float32", ")", "\n", "\n", "# Trim to certain time series length (note single example, not batch)", "\n", "# shape before: [time_steps, features]", "\n", "# shape after:  [min(time_steps, trim_time_steps), features]", "\n", "if", "FLAGS", ".", "trim_time_steps", "!=", "0", ":", "\n", "                    ", "x", "=", "tf", ".", "slice", "(", "x", ",", "[", "0", ",", "0", "]", ",", "\n", "[", "tf", ".", "minimum", "(", "tf", ".", "shape", "(", "x", ")", "[", "0", "]", ",", "FLAGS", ".", "trim_time_steps", ")", ",", "tf", ".", "shape", "(", "x", ")", "[", "1", "]", "]", ")", "\n", "\n", "# Trim to a certain number of features (the first n = trim_features)", "\n", "", "if", "FLAGS", ".", "trim_features", "!=", "0", ":", "\n", "                    ", "x", "=", "tf", ".", "slice", "(", "x", ",", "[", "0", ",", "0", "]", ",", "\n", "[", "tf", ".", "shape", "(", "x", ")", "[", "0", "]", ",", "tf", ".", "minimum", "(", "tf", ".", "shape", "(", "x", ")", "[", "1", "]", ",", "FLAGS", ".", "trim_features", ")", "]", ")", "\n", "\n", "# Select only the desired features, if specified", "\n", "", "if", "self", ".", "feature_subset", "is", "not", "None", ":", "\n", "                    ", "assert", "FLAGS", ".", "trim_features", "==", "0", ",", "\"cannot specify both {source,target}_feature_subset and trim_features\"", "\n", "# axis=-1 is the feature dimension", "\n", "x", "=", "tf", ".", "gather", "(", "x", ",", "self", ".", "feature_subset", ",", "axis", "=", "-", "1", ")", "\n", "\n", "", "xs", ".", "append", "(", "x", ")", "\n", "\n", "# Rearrange or take subset if desired", "\n", "", "if", "self", ".", "modality_subset", "is", "not", "None", ":", "\n", "                ", "xs_subset", "=", "[", "]", "\n", "\n", "for", "modality", "in", "self", ".", "modality_subset", ":", "\n", "                    ", "xs_subset", ".", "append", "(", "xs", "[", "modality", "]", ")", "\n", "\n", "", "xs", "=", "xs_subset", "\n", "\n", "# Label (y)", "\n", "", "y", "=", "tf", ".", "io", ".", "parse_tensor", "(", "parsed", "[", "\"y\"", "]", ",", "tf", ".", "float32", ")", "\n", "\n", "# Example ID - unique to domain/set (e.g. within source 1 train)", "\n", "example_id", "=", "tf", ".", "io", ".", "parse_tensor", "(", "parsed", "[", "\"id\"", "]", ",", "tf", ".", "int32", ")", "\n", "\n", "# TensorFlow gives weird problems when lists, probably wants to", "\n", "# convert lists to tensors, which means all the x's have to be the", "\n", "# same shape, which defeats the purpose of separating them here.", "\n", "return", "tuple", "(", "xs", ")", ",", "y", ",", "example_id", "\n", "\n", "# Interleave the tfrecord files", "\n", "", "files", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "filenames", ")", "\n", "dataset", "=", "files", ".", "interleave", "(", "\n", "lambda", "x", ":", "tf", ".", "data", ".", "TFRecordDataset", "(", "x", ",", "compression_type", "=", "'GZIP'", ")", ".", "prefetch", "(", "100", ")", ",", "\n", "cycle_length", "=", "len", "(", "filenames", ")", ",", "block_length", "=", "1", ")", "\n", "\n", "# If desired, take the first max_examples examples or percentage.", "\n", "#", "\n", "# Note: this is the first so-many examples, but we shuffled before", "\n", "# putting into the tfrecord file (train_test_split in datasets.py), so", "\n", "# it is a random set essentially. We do this so we consistently use the", "\n", "# same data between runs.", "\n", "examples", "=", "0", "\n", "\n", "if", "self", ".", "max_examples_percentage", ":", "\n", "# Take the first max_examples percentage of the total examples", "\n", "            ", "if", "evaluation", ":", "\n", "                ", "percentage", "=", "self", ".", "eval_max_examples", "\n", "", "else", ":", "\n", "                ", "percentage", "=", "self", ".", "train_max_examples", "\n", "\n", "# To save time, if the percentage is zero, just skip all of this", "\n", "# and set examples=0, i.e. use them all", "\n", "", "if", "percentage", ">", "0", ":", "\n", "# Calculate total number of examples", "\n", "                ", "total", "=", "0", "\n", "\n", "for", "x", "in", "dataset", ":", "\n", "                    ", "total", "+=", "1", "\n", "\n", "# Take percentage -- ceil so we'd end up with 1 not 0 if we", "\n", "# have very few examples", "\n", "", "examples", "=", "math", ".", "ceil", "(", "percentage", "*", "total", ")", "\n", "\n", "print", "(", "percentage", ",", "\"percent of\"", ",", "total", ",", "\"examples =\"", ",", "examples", ",", "\"for\"", ",", "filenames", ")", "\n", "", "else", ":", "\n", "                ", "examples", "=", "0", "\n", "", "", "else", ":", "\n", "# Take the first max_examples examples", "\n", "            ", "if", "evaluation", ":", "\n", "                ", "examples", "=", "int", "(", "self", ".", "eval_max_examples", ")", "\n", "", "else", ":", "\n", "                ", "examples", "=", "int", "(", "self", ".", "train_max_examples", ")", "\n", "\n", "", "", "if", "examples", ">", "0", ":", "\n", "            ", "dataset", "=", "dataset", ".", "take", "(", "examples", ")", "\n", "\n", "# Whether to do autotuning of prefetch or num_parallel_calls", "\n", "", "prefetch_buffer", "=", "self", ".", "prefetch_buffer", "\n", "num_parallel_calls", "=", "None", "\n", "if", "self", ".", "tune_num_parallel_calls", ":", "\n", "            ", "num_parallel_calls", "=", "tf", ".", "data", ".", "experimental", ".", "AUTOTUNE", "\n", "", "if", "self", ".", "prefetch_buffer", "==", "0", ":", "\n", "            ", "prefetch_buffer", "=", "tf", ".", "data", ".", "experimental", ".", "AUTOTUNE", "\n", "\n", "# Use .cache() or .cache(filename) to reduce loading over the network", "\n", "# https://www.tensorflow.org/guide/data_performance#map_and_cache", "\n", "# Example: https://www.tensorflow.org/tutorials/load_data/images", "\n", "", "if", "self", ".", "cache", ":", "\n", "# Map before caching so we don't have to keep doing this over and over", "\n", "# again -- drastically reduces CPU usage.", "\n", "            ", "dataset", "=", "dataset", ".", "map", "(", "_parse_example_function", ",", "\n", "num_parallel_calls", "=", "num_parallel_calls", ")", "\n", "\n", "dataset", "=", "dataset", ".", "cache", "(", ")", "\n", "\n", "", "if", "count", ":", "# only count, so no need to shuffle", "\n", "            ", "pass", "\n", "", "elif", "evaluation", ":", "# don't repeat since we want to evaluate entire set", "\n", "            ", "dataset", "=", "dataset", ".", "shuffle", "(", "self", ".", "shuffle_buffer", ",", "seed", "=", "self", ".", "eval_shuffle_seed", ")", "\n", "pass", "\n", "", "else", ":", "# repeat and shuffle", "\n", "            ", "dataset", "=", "dataset", ".", "shuffle", "(", "self", ".", "shuffle_buffer", ")", ".", "repeat", "(", ")", "\n", "\n", "# If not caching, then it's faster to map right next to batch", "\n", "", "if", "not", "self", ".", "cache", ":", "\n", "            ", "dataset", "=", "dataset", ".", "map", "(", "_parse_example_function", ",", "\n", "num_parallel_calls", "=", "num_parallel_calls", ")", "\n", "\n", "", "dataset", "=", "dataset", ".", "batch", "(", "batch_size", ")", "\n", "dataset", "=", "dataset", ".", "prefetch", "(", "prefetch_buffer", ")", "\n", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.load_datasets.Dataset.load_dataset": [[304, 316], ["load_datasets.Dataset.load_tfrecords", "load_datasets.Dataset.load_tfrecords", "load_datasets.Dataset.load_tfrecords"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.load_datasets.Dataset.load_tfrecords", "home.repos.pwc.inspect_result.floft_calda.None.load_datasets.Dataset.load_tfrecords", "home.repos.pwc.inspect_result.floft_calda.None.load_datasets.Dataset.load_tfrecords"], ["", "def", "load_dataset", "(", "self", ",", "train_filenames", ",", "test_filenames", ")", ":", "\n", "        ", "\"\"\"\n        Load the X dataset as a tf.data.Dataset from train/test tfrecord filenames\n        \"\"\"", "\n", "train_dataset", "=", "self", ".", "load_tfrecords", "(", "\n", "train_filenames", ",", "self", ".", "train_batch", ")", "\n", "eval_train_dataset", "=", "self", ".", "load_tfrecords", "(", "\n", "train_filenames", ",", "self", ".", "eval_batch", ",", "evaluation", "=", "True", ")", "\n", "eval_test_dataset", "=", "self", ".", "load_tfrecords", "(", "\n", "test_filenames", ",", "self", ".", "eval_batch", ",", "evaluation", "=", "True", ")", "\n", "\n", "return", "train_dataset", ",", "eval_train_dataset", ",", "eval_test_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.load_datasets.Dataset.label_to_int": [[317, 320], ["load_datasets.Dataset.class_labels.index"], "methods", ["None"], ["", "def", "label_to_int", "(", "self", ",", "label_name", ")", ":", "\n", "        ", "\"\"\" e.g. Bathe to 0 \"\"\"", "\n", "return", "self", ".", "class_labels", ".", "index", "(", "label_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.load_datasets.Dataset.int_to_label": [[321, 324], ["None"], "methods", ["None"], ["", "def", "int_to_label", "(", "self", ",", "label_index", ")", ":", "\n", "        ", "\"\"\" e.g. 0 to Bathe \"\"\"", "\n", "return", "self", ".", "class_labels", "[", "label_index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.load_datasets.load": [[326, 378], ["datasets.datasets.attributes", "load_datasets.load._path"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.attributes"], ["", "", "def", "load", "(", "dataset_name", ",", "num_domains", ",", "postfix", "=", "\"\"", ",", "test", "=", "False", ",", "\n", "train_on_everything", "=", "False", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Load a dataset (source and target). Names must be in datasets.names().\n\n    If test=True, then load real test set. Otherwise, load validation set as\n    the \"test\" data (for use during training and hyperparameter tuning).\n    \"\"\"", "\n", "# Sanity checks", "\n", "assert", "dataset_name", "in", "names", "(", ")", ",", "dataset_name", "+", "\" not a supported dataset\"", "\n", "\n", "# Get dataset information", "\n", "num_classes", ",", "class_labels", "=", "datasets", ".", "attributes", "(", "dataset_name", ")", "\n", "\n", "# Get dataset tfrecord filenames", "\n", "def", "_path", "(", "filename", ")", ":", "\n", "        ", "\"\"\" Files are in datasets/ subdirectory. If the file exists, return it\n        as an array since we may sometimes want more than one file for a\n        dataset. If it doesn't exist, ignore it (some datasets don't have a test\n        set for example).\"\"\"", "\n", "fn", "=", "os", ".", "path", ".", "join", "(", "\"datasets\"", ",", "\"tfrecords\"", ",", "filename", ")", "\n", "return", "[", "fn", "]", "if", "os", ".", "path", ".", "exists", "(", "fn", ")", "else", "[", "]", "\n", "\n", "", "if", "postfix", "!=", "\"\"", ":", "\n", "        ", "postfix", "=", "postfix", "+", "\"_\"", "\n", "\n", "", "train_filenames", "=", "_path", "(", "tfrecord_filename", "(", "dataset_name", ",", "postfix", "+", "\"train\"", ")", ")", "\n", "valid_filenames", "=", "_path", "(", "tfrecord_filename", "(", "dataset_name", ",", "postfix", "+", "\"valid\"", ")", ")", "\n", "test_filenames", "=", "_path", "(", "tfrecord_filename", "(", "dataset_name", ",", "postfix", "+", "\"test\"", ")", ")", "\n", "\n", "# This is used for some plots not actual training", "\n", "if", "train_on_everything", ":", "\n", "        ", "print", "(", "\"Warning: training dataset contains all train/valid/test data\"", ")", "\n", "train_filenames", "+=", "valid_filenames", "+", "test_filenames", "\n", "test_filenames", "=", "[", "]", "\n", "# By default use validation data as the \"test\" data, unless test=True", "\n", "", "elif", "not", "test", ":", "\n", "        ", "test_filenames", "=", "valid_filenames", "\n", "# If test=True, then make \"train\" consist of both training and validation", "\n", "# data to match the original dataset.", "\n", "", "else", ":", "\n", "        ", "train_filenames", "+=", "valid_filenames", "\n", "\n", "# Get number of modalities for this dataset", "\n", "", "num_modalities", "=", "get_num_modalities", "(", "dataset_name", ")", "\n", "\n", "# Create all the train, test, evaluation, ... tf.data.Dataset objects within", "\n", "# a Dataset() class that stores them", "\n", "dataset", "=", "Dataset", "(", "num_classes", ",", "class_labels", ",", "num_domains", ",", "num_modalities", ",", "\n", "train_filenames", ",", "test_filenames", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.load_datasets.load_da": [[380, 538], ["sources.split", "len", "load_datasets.names", "enumerate", "range", "target.split", "len", "int", "int", "max", "source_datasets.append", "len", "load_datasets.load", "datasets.datasets.get_dataset_target_users", "len", "str", "print", "max", "int", "int", "int", "int", "load_datasets.load", "int", "len", "FLAGS.source_feature_subset.split", "FLAGS.target_feature_subset.split", "source_modality_subset.split", "target_modality_subset.split", "str"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.names", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.load", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.get_dataset_target_users", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.load"], ["", "def", "load_da", "(", "dataset", ",", "sources", ",", "target", ",", "*", "args", ",", "\n", "source_modality_subset", "=", "\"\"", ",", "target_modality_subset", "=", "\"\"", ",", "\n", "override_batch_division", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    Load the source(s) and target domains\n\n    Input:\n        dataset - one of the dataset names (e.g. ucihar)\n        sources - comma-separated string of source domain numbers\n        target - string of target domain number\n\n    Returns:\n        [source1_dataset, source2_dataset, ...], target_dataset\n    \"\"\"", "\n", "# Allow target blank meaning None as well", "\n", "if", "target", "==", "\"\"", ":", "\n", "        ", "target", "=", "None", "\n", "\n", "# Get proper dataset names", "\n", "", "source_ids", "=", "sources", ".", "split", "(", "\",\"", ")", "\n", "sources", "=", "[", "dataset", "+", "\"_\"", "+", "x", "for", "x", "in", "source_ids", "]", "\n", "\n", "# Need to know how many domains for creating the proper-sized model, etc.", "\n", "num_domains", "=", "len", "(", "sources", ")", "\n", "\n", "if", "target", "is", "not", "None", ":", "\n", "# Probably 1, but still will work if we ever support multiple targets", "\n", "        ", "target_ids", "=", "target", ".", "split", "(", "\",\"", ")", "\n", "num_domains", "+=", "len", "(", "target_ids", ")", "\n", "\n", "original_target", "=", "target", "\n", "target", "=", "dataset", "+", "\"_\"", "+", "target", "\n", "\n", "# Integer ids", "\n", "", "if", "target", "is", "not", "None", ":", "\n", "        ", "assert", "len", "(", "target_ids", ")", "==", "1", ",", "\"only support one target domain for now\"", "\n", "target_id", "=", "int", "(", "target_ids", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "        ", "target_id", "=", "None", "\n", "\n", "", "source_ids", "=", "[", "int", "(", "source_id", ")", "for", "source_id", "in", "source_ids", "]", "\n", "\n", "# Check they're all valid", "\n", "valid_names", "=", "names", "(", ")", "\n", "\n", "for", "s", "in", "sources", ":", "\n", "        ", "assert", "s", "in", "valid_names", ",", "\"unknown source domain: \"", "+", "s", "+", "\" not in \"", "+", "str", "(", "valid_names", ")", "\n", "\n", "", "if", "target", "is", "not", "None", ":", "\n", "        ", "assert", "target", "in", "valid_names", ",", "\"unknown target domain: \"", "+", "target", "\n", "\n", "# Determine batch sizes", "\n", "", "source_train_batch", "=", "None", "\n", "target_train_batch", "=", "None", "\n", "\n", "# Divide among sources, so batch_size/num_sources. Keep target the normal", "\n", "# batch size. Though, we must at least have one sample from each domain,", "\n", "# so take max of the division and 1.", "\n", "#", "\n", "# Note: we don't need to change eval_batch since that data is fed in", "\n", "# per-domain anyway in metrics.py. Thus, there's no point in decreasing", "\n", "# the batch size since it's not affected by the number of domains.", "\n", "assert", "FLAGS", ".", "train_batch", ">", "0", ",", "\"must have positive train_batch size\"", "\n", "if", "FLAGS", ".", "batch_division", "==", "\"sources\"", "or", "override_batch_division", ":", "\n", "        ", "source_train_batch", "=", "max", "(", "FLAGS", ".", "train_batch", "//", "len", "(", "sources", ")", ",", "1", ")", "\n", "target_train_batch", "=", "FLAGS", ".", "train_batch", "\n", "\n", "if", "override_batch_division", ":", "\n", "            ", "print", "(", "\"Overriding: batch_division=sources\"", ")", "\n", "# Divide among all, so batch_size/num_domains. Set for both sources/target.", "\n", "", "", "elif", "FLAGS", ".", "batch_division", "==", "\"all\"", ":", "\n", "        ", "batch_size", "=", "max", "(", "FLAGS", ".", "train_batch", "//", "num_domains", ",", "1", ")", "\n", "source_train_batch", "=", "batch_size", "\n", "target_train_batch", "=", "batch_size", "\n", "", "else", ":", "\n", "        ", "source_train_batch", "=", "FLAGS", ".", "train_batch", "\n", "target_train_batch", "=", "FLAGS", ".", "train_batch", "\n", "\n", "#print(\"Source batch size:\", source_train_batch, \"for\", len(sources), \"sources\")", "\n", "#print(\"Target batch size:\", target_train_batch)", "\n", "\n", "# Which features from source/target to use (if not all of them)", "\n", "", "if", "FLAGS", ".", "source_feature_subset", "==", "\"\"", ":", "\n", "        ", "source_feature_subset", "=", "None", "\n", "", "else", ":", "\n", "        ", "source_feature_subset", "=", "[", "int", "(", "x", ")", "for", "x", "in", "FLAGS", ".", "source_feature_subset", ".", "split", "(", "\",\"", ")", "]", "\n", "\n", "", "if", "FLAGS", ".", "target_feature_subset", "==", "\"\"", ":", "\n", "        ", "target_feature_subset", "=", "None", "\n", "", "else", ":", "\n", "        ", "target_feature_subset", "=", "[", "int", "(", "x", ")", "for", "x", "in", "FLAGS", ".", "target_feature_subset", ".", "split", "(", "\",\"", ")", "]", "\n", "\n", "# Which modalities from source/target to use (if not all of them)", "\n", "", "if", "source_modality_subset", "==", "\"\"", ":", "\n", "        ", "source_modality_subset", "=", "None", "\n", "", "else", ":", "\n", "        ", "source_modality_subset", "=", "[", "int", "(", "x", ")", "for", "x", "in", "source_modality_subset", ".", "split", "(", "\",\"", ")", "]", "\n", "\n", "", "if", "target_modality_subset", "==", "\"\"", ":", "\n", "        ", "target_modality_subset", "=", "None", "\n", "", "else", ":", "\n", "        ", "target_modality_subset", "=", "[", "int", "(", "x", ")", "for", "x", "in", "target_modality_subset", ".", "split", "(", "\",\"", ")", "]", "\n", "\n", "# Load each source", "\n", "", "source_datasets", "=", "[", "]", "\n", "\n", "for", "i", ",", "s", "in", "enumerate", "(", "sources", ")", ":", "\n", "        ", "source_datasets", ".", "append", "(", "load", "(", "s", ",", "num_domains", ",", "*", "args", ",", "\n", "train_batch", "=", "source_train_batch", ",", "\n", "feature_subset", "=", "source_feature_subset", ",", "\n", "modality_subset", "=", "source_modality_subset", ",", "\n", "domain_id", "=", "source_ids", "[", "i", "]", ",", "\n", "**", "kwargs", ")", ")", "\n", "\n", "# Check that they all have the same number of classes as the first one", "\n", "", "for", "i", "in", "range", "(", "1", ",", "len", "(", "source_datasets", ")", ")", ":", "\n", "        ", "assert", "source_datasets", "[", "i", "]", ".", "num_classes", "==", "source_datasets", "[", "0", "]", ".", "num_classes", ",", "\"Source domain \"", "+", "str", "(", "i", ")", "+", "\" has different # of classes than source 0\"", "\n", "\n", "# Load target", "\n", "", "if", "target", "is", "not", "None", ":", "\n", "        ", "train_max_examples", "=", "None", "\n", "eval_max_examples", "=", "None", "\n", "\n", "# If desired, only use the a limited number of target examples for", "\n", "# training/evaluation", "\n", "if", "FLAGS", ".", "max_target_examples", "!=", "0", ":", "\n", "            ", "train_max_examples", "=", "FLAGS", ".", "max_target_examples", "\n", "\n", "# Note: for now don't limit eval examples. We want the best estimate", "\n", "# of evaluation performance. We just want to limit how much data", "\n", "# we have during training.", "\n", "#eval_max_examples = FLAGS.max_target_examples", "\n", "\n", "", "target_dataset", "=", "load", "(", "target", ",", "num_domains", ",", "*", "args", ",", "\n", "train_batch", "=", "target_train_batch", ",", "\n", "train_max_examples", "=", "train_max_examples", ",", "\n", "eval_max_examples", "=", "eval_max_examples", ",", "\n", "feature_subset", "=", "target_feature_subset", ",", "\n", "modality_subset", "=", "target_modality_subset", ",", "\n", "domain_id", "=", "target_id", ",", "\n", "**", "kwargs", ")", "\n", "\n", "# Check that the target has the same number of classes as the first", "\n", "# source (since we already verified all sources have the same)", "\n", "assert", "target_dataset", ".", "num_classes", "==", "source_datasets", "[", "0", "]", ".", "num_classes", ",", "\"Target has different # of classes than source 0\"", "\n", "\n", "# Check that the target is in the allowed set of target users", "\n", "allowed_target_users", "=", "datasets", ".", "get_dataset_target_users", "(", "dataset", ")", "\n", "assert", "int", "(", "original_target", ")", "in", "allowed_target_users", ",", "\"target {} not in list of allowed target users {}\"", ".", "format", "(", "\n", "original_target", ",", "allowed_target_users", ")", "\n", "", "else", ":", "\n", "        ", "target_dataset", "=", "None", "\n", "\n", "", "return", "source_datasets", ",", "target_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.load_datasets.names": [[540, 543], ["datasets.datasets.names"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.names"], ["", "def", "names", "(", ")", ":", "\n", "    ", "\"\"\" Returns list of all the available datasets to load \"\"\"", "\n", "return", "datasets", ".", "names", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.load_datasets.get_num_modalities": [[545, 561], ["dataset_name.split", "datasets.datasets.get_dataset_modalities", "len"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.get_dataset_modalities"], ["", "def", "get_num_modalities", "(", "dataset_name", ")", ":", "\n", "    ", "\"\"\" Get the number of modalities for each dataset \"\"\"", "\n", "# Different datasets have different sets of features", "\n", "# Format for the names is: watch_1 where \"watch\" is the dataset_name", "\n", "# and 1 is the user_id", "\n", "#", "\n", "# Note: this allows for the dataset to have _ in it by recombining the", "\n", "# multiple splits and ignoring the last one that's the user id.", "\n", "parts", "=", "dataset_name", ".", "split", "(", "\"_\"", ")", "\n", "assert", "len", "(", "parts", ")", ">=", "2", ",", "\"cannot split dataset into name_user\"", "\n", "dataset_name", "=", "\"_\"", ".", "join", "(", "parts", "[", ":", "-", "1", "]", ")", "# re-combine _-separated name", "\n", "# user_id = parts[-1]  # just the final user id", "\n", "\n", "num_modalities", "=", "datasets", ".", "get_dataset_modalities", "(", "dataset_name", ")", "\n", "\n", "return", "num_modalities", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.load_datasets.main": [[563, 586], ["print", "load_datasets.load_da", "print", "print", "enumerate", "load_datasets.names", "print", "print", "print", "print", "str", "str"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.load_datasets.load_da", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.names"], ["", "def", "main", "(", "argv", ")", ":", "\n", "    ", "print", "(", "\"Available datasets:\"", ",", "names", "(", ")", ")", "\n", "\n", "# Example showing that the sizes and number of channels are matched", "\n", "sources", ",", "target", "=", "load_da", "(", "\"ucihar\"", ",", "\"1,2\"", ",", "\"3\"", ")", "\n", "\n", "print", "(", "\"Source 0:\"", ",", "sources", "[", "0", "]", ".", "train", ")", "\n", "print", "(", "\"Target:\"", ",", "target", ".", "train", ")", "\n", "\n", "for", "i", ",", "source", "in", "enumerate", "(", "sources", ")", ":", "\n", "        ", "assert", "source", ".", "train", "is", "not", "None", ",", "\"dataset file probably doesn't exist\"", "\n", "\n", "for", "x", ",", "y", "in", "source", ".", "train", ":", "\n", "            ", "print", "(", "\"Source \"", "+", "str", "(", "i", ")", "+", "\" x shape:\"", ",", "x", ".", "shape", ")", "\n", "print", "(", "\"Source \"", "+", "str", "(", "i", ")", "+", "\" y shape:\"", ",", "y", ".", "shape", ")", "\n", "break", "\n", "\n", "", "", "assert", "target", ".", "train", "is", "not", "None", ",", "\"dataset file probably doesn't exist\"", "\n", "\n", "for", "x", ",", "y", "in", "target", ".", "train", ":", "\n", "        ", "print", "(", "\"Target x shape:\"", ",", "x", ".", "shape", ")", "\n", "print", "(", "\"Target y shape:\"", ",", "y", ".", "shape", ")", "\n", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.sampling_analysis.print_results": [[19, 31], ["print", "print", "print", "print", "hyperparameter_tuning_analysis.get_average_accuracy", "print"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.hyperparameter_tuning_analysis.get_average_accuracy"], ["def", "print_results", "(", "datasets", ",", "methods", ",", "prefixes", ",", "only_n", "=", "None", ")", ":", "\n", "    ", "for", "dataset", "in", "datasets", ":", "\n", "        ", "for", "method", "in", "methods", ":", "\n", "            ", "print", "(", "\"  Dataset:\"", ",", "dataset", ")", "\n", "print", "(", "\"    Method:\"", ",", "method", ")", "\n", "\n", "print", "(", "\"      Average accuracy for each run:\"", ")", "\n", "for", "prefix", "in", "prefixes", ":", "\n", "                ", "accuracy", ",", "stdev", "=", "get_average_accuracy", "(", "\"results\"", ",", "[", "prefix", "]", ",", "\n", "dataset", ",", "method", ",", "only_n", "=", "only_n", ")", "\n", "print", "(", "\"        \"", ",", "prefix", ",", "accuracy", ",", "stdev", ")", "\n", "", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.sampling_analysis.get_results": [[33, 59], ["collections.defaultdict", "analysis.pretty_dataset_name", "collections.defaultdict", "hyperparameter_tuning_analysis.get_average_accuracy", "collections.defaultdict", "str"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.analysis.pretty_dataset_name", "home.repos.pwc.inspect_result.floft_calda.None.hyperparameter_tuning_analysis.get_average_accuracy"], ["", "", "", "def", "get_results", "(", "datasets", ",", "methods", ",", "prefixes", ",", "only_n", "=", "None", ")", ":", "\n", "# results[dataset][key][method] = ()", "\n", "    ", "results", "=", "collections", ".", "defaultdict", "(", "\n", "lambda", ":", "collections", ".", "defaultdict", "(", "\n", "lambda", ":", "collections", ".", "defaultdict", "(", "tuple", ")", "\n", ")", "\n", ")", "\n", "\n", "for", "dataset", "in", "datasets", ":", "\n", "        ", "dataset_name", "=", "pretty_dataset_name", "(", "dataset", ")", "\n", "\n", "for", "method", "in", "methods", ":", "\n", "            ", "method_name", "=", "nice_method_names", "[", "method", "]", "\n", "\n", "for", "key", ",", "prefix", "in", "prefixes", ":", "\n", "                ", "accuracy", ",", "stdev", "=", "get_average_accuracy", "(", "\"results\"", ",", "[", "prefix", "]", ",", "\n", "dataset", ",", "method", ",", "only_n", "=", "only_n", ")", "\n", "\n", "# Don't add if it wasn't found", "\n", "if", "accuracy", "!=", "-", "1", "and", "stdev", "!=", "-", "1", ":", "\n", "# Save with nice names", "\n", "                    ", "assert", "method_name", "not", "in", "results", "[", "dataset_name", "]", "[", "key", "]", ",", "\"duplicate found: \"", "+", "method_name", "+", "\" already in \"", "+", "str", "(", "results", "[", "dataset_name", "]", "[", "key", "]", ")", "\n", "results", "[", "dataset_name", "]", "[", "key", "]", "[", "method_name", "]", "=", "(", "accuracy", ",", "stdev", ")", "\n", "\n", "", "", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.sampling_analysis.get_csv": [[61, 92], ["open", "f.write", "collections.defaultdict", "results.keys", "results[].keys", "[].keys", "f.write", "csv_results[].append", "f.write", "sampling_analysis.get_csv.f1"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.write", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.write", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.write"], ["", "def", "get_csv", "(", "results", ",", "output_filename", ")", ":", "\n", "# Output CSV rather than printing results", "\n", "    ", "def", "f1", "(", "v", ")", ":", "\n", "        ", "\"\"\" Format mean and stdev properly \"\"\"", "\n", "return", "\"{:.1f} $\\\\pm$ {:.1f}\"", ".", "format", "(", "v", "[", "0", "]", "*", "100", ",", "v", "[", "1", "]", "*", "100", ")", "\n", "\n", "", "def", "f2", "(", "v", ")", ":", "\n", "        ", "\"\"\" Format single float properly \"\"\"", "\n", "return", "\"{:.1f}\"", ".", "format", "(", "v", "*", "100", ")", "\n", "\n", "", "with", "open", "(", "output_filename", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "\"Dataset;n;P&N Multiplier;CALDA-XS,R;CALDA-XS,H;Gap\\n\"", ")", "\n", "csv_results", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "n", "in", "results", ".", "keys", "(", ")", ":", "\n", "            ", "for", "d", "in", "results", "[", "n", "]", ".", "keys", "(", ")", ":", "\n", "                ", "for", "multiplier", "in", "results", "[", "n", "]", "[", "d", "]", ".", "keys", "(", ")", ":", "\n", "                    ", "r", "=", "results", "[", "n", "]", "[", "d", "]", "[", "multiplier", "]", "[", "\"CALDA-XS,R\"", "]", "\n", "h", "=", "results", "[", "n", "]", "[", "d", "]", "[", "multiplier", "]", "[", "\"CALDA-XS,H\"", "]", "\n", "gap", "=", "h", "[", "0", "]", "-", "r", "[", "0", "]", "\n", "\n", "row", "=", "[", "d", ",", "n", ",", "multiplier", ",", "f1", "(", "r", ")", ",", "f1", "(", "h", ")", ",", "f2", "(", "gap", ")", "]", "\n", "row_str", "=", "\";\"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "row", "]", ")", "\n", "\n", "# Also keep raw data", "\n", "csv_results", "[", "n", "]", ".", "append", "(", "[", "d", ",", "n", ",", "multiplier", ",", "r", ",", "h", ",", "gap", "]", ")", "\n", "\n", "f", ".", "write", "(", "row_str", "+", "\"\\n\"", ")", "\n", "\n", "", "f", ".", "write", "(", "\";;;;;\\n\"", ")", "\n", "\n", "", "", "", "return", "csv_results", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.sampling_analysis.best_fit": [[94, 102], ["numpy.polynomial.polynomial.Polynomial.fit", "numpy.linspace", "bestfit", "matplotlib.plot", "min", "max"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.sampling_analysis.plot"], ["", "def", "best_fit", "(", "x", ",", "y", ",", "label", "=", "\"Least Sq.\"", ",", "alpha", "=", "1.0", ")", ":", "\n", "# Best-fit line", "\n", "    ", "bestfit", ",", "stats", "=", "np", ".", "polynomial", ".", "polynomial", ".", "Polynomial", ".", "fit", "(", "x", ",", "y", ",", "deg", "=", "1", ",", "full", "=", "True", ")", "\n", "# resid, rank, sv, rcond = stats", "\n", "\n", "x", "=", "np", ".", "linspace", "(", "min", "(", "x", ")", ",", "max", "(", "x", ")", ")", "\n", "y", "=", "bestfit", "(", "x", ")", "\n", "plt", ".", "plot", "(", "x", ",", "y", ",", "\"-\"", ",", "label", "=", "label", ",", "alpha", "=", "alpha", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.sampling_analysis.plot": [[104, 158], ["matplotlib.subplots", "ax.xaxis.set_major_locator", "analysis.gen_jitter", "enumerate", "sampling_analysis.best_fit", "ax.set_xlabel", "ax.set_ylabel", "ax.get_position", "ax.set_position", "matplotlib.legend", "matplotlib.ticker.MaxNLocator", "len", "results.keys", "matplotlib.savefig", "matplotlib.close", "matplotlib.show", "matplotlib.scatter", "range", "len"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.analysis.gen_jitter", "home.repos.pwc.inspect_result.floft_calda.None.sampling_analysis.best_fit", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.close"], ["", "def", "plot", "(", "results", ",", "save_plot_filename", "=", "None", ",", "figsize", "=", "(", "5", ",", "3", ")", ",", "ncol", "=", "1", ")", ":", "\n", "    ", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "1", ",", "1", ",", "figsize", "=", "figsize", ",", "dpi", "=", "100", ")", "\n", "\n", "# ax.set_ylim(yrange)", "\n", "ax", ".", "xaxis", ".", "set_major_locator", "(", "MaxNLocator", "(", "integer", "=", "True", ")", ")", "\n", "\n", "markers", "=", "[", "\"o\"", ",", "\"v\"", ",", "\"^\"", ",", "\"<\"", ",", "\">\"", ",", "\"s\"", ",", "\"p\"", ",", "\"*\"", ",", "\"D\"", ",", "\"P\"", ",", "\"X\"", ",", "\"h\"", ",", "\n", "\"1\"", ",", "\"2\"", ",", "\"3\"", ",", "\"4\"", ",", "\"+\"", ",", "\"x\"", ",", "\"d\"", ",", "\"H\"", ",", "\"|\"", ",", "\"_\"", "]", "\n", "\n", "xs", "=", "[", "]", "\n", "ys", "=", "[", "]", "\n", "\n", "jitter", "=", "gen_jitter", "(", "len", "(", "results", ")", ",", "amount", "=", "0.05", ")", "\n", "\n", "for", "i", ",", "n", "in", "enumerate", "(", "results", ".", "keys", "(", ")", ")", ":", "\n", "        ", "avg", "=", "n", "==", "\"Avg\"", "\n", "\n", "# Get multiplier for x and gap for y", "\n", "x", "=", "[", "v", "[", "2", "]", "for", "v", "in", "results", "[", "n", "]", "]", "\n", "y", "=", "[", "v", "[", "5", "]", "*", "100", "for", "v", "in", "results", "[", "n", "]", "]", "# accuracy", "\n", "\n", "# Jitter slightly", "\n", "x_jittered", "=", "[", "x", "[", "j", "]", "+", "jitter", "[", "i", "]", "for", "j", "in", "range", "(", "len", "(", "x", ")", ")", "]", "\n", "\n", "# line_type = \"-\" if avg else \"--\"", "\n", "label", "=", "\"Avg\"", "if", "avg", "else", "\"$n={}$\"", ".", "format", "(", "n", ")", "\n", "# plt.plot(x, y, markers[i]+line_type, label=label, alpha=0.8)", "\n", "\n", "# Exclude average", "\n", "if", "not", "avg", ":", "\n", "            ", "plt", ".", "scatter", "(", "x_jittered", ",", "y", ",", "label", "=", "label", ",", "marker", "=", "markers", "[", "i", "]", ")", "\n", "\n", "# For best-fit line", "\n", "xs", "+=", "x", "\n", "ys", "+=", "y", "\n", "\n", "# best_fit(x, y, label)", "\n", "\n", "", "", "best_fit", "(", "xs", ",", "ys", ")", "\n", "\n", "ax", ".", "set_xlabel", "(", "\"P&N Multiplier\"", ")", "\n", "ax", ".", "set_ylabel", "(", "\"Hard Sampling Accuracy Gain ($H - R$ %)\"", ")", "\n", "\n", "# Put legend outside the graph http://stackoverflow.com/a/4701285", "\n", "# Shrink current axis by 20%", "\n", "box", "=", "ax", ".", "get_position", "(", ")", "\n", "ax", ".", "set_position", "(", "[", "box", ".", "x0", ",", "box", ".", "y0", ",", "box", ".", "width", "*", "0.8", ",", "box", ".", "height", "]", ")", "\n", "plt", ".", "legend", "(", "loc", "=", "\"center left\"", ",", "bbox_to_anchor", "=", "(", "1", ",", "0.5", ")", ",", "ncol", "=", "ncol", ")", "\n", "\n", "if", "save_plot_filename", "is", "not", "None", ":", "\n", "        ", "plt", ".", "savefig", "(", "save_plot_filename", ",", "bbox_inches", "=", "'tight'", ")", "\n", "plt", ".", "close", "(", ")", "\n", "", "else", ":", "\n", "        ", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.sampling_analysis.all_results": [[160, 170], ["sampling_analysis.get_results", "sampling_analysis.get_results"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.sampling_analysis.get_results", "home.repos.pwc.inspect_result.floft_calda.None.sampling_analysis.get_results"], ["", "", "def", "all_results", "(", "prefixes", ",", "datasets", ",", "methods", ",", "n", ")", ":", "\n", "    ", "\"\"\" Get results for each value of n in addition to on average \"\"\"", "\n", "results", "=", "{", "\n", "\"Avg\"", ":", "get_results", "(", "datasets", ",", "methods", ",", "prefixes", ")", "\n", "}", "\n", "\n", "for", "only_n", "in", "n", ":", "\n", "        ", "results", "[", "only_n", "]", "=", "get_results", "(", "datasets", ",", "methods", ",", "prefixes", ",", "only_n", ")", "\n", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.sampling_analysis.main": [[172, 202], ["sampling_analysis.main.output_results"], "function", ["None"], ["", "def", "main", "(", "argv", ")", ":", "\n", "    ", "n", "=", "[", "2", ",", "8", ",", "14", ",", "20", ",", "26", "]", "# for WISDM AR", "\n", "\n", "prefixes_b128", "=", "[", "\n", "(", "1", ",", "\"sample-b128-p5-n10\"", ")", ",", "\n", "(", "2", ",", "\"sample-b128-p10-n20\"", ")", ",", "\n", "(", "4", ",", "\"sample-b128-p20-n40\"", ")", ",", "\n", "(", "6", ",", "\"sample-b128-p30-n60\"", ")", ",", "\n", "(", "8", ",", "\"sample-b128-p40-n80\"", ")", ",", "\n", "]", "\n", "\n", "prefixes_b64", "=", "[", "\n", "(", "1", ",", "\"sample-b64-p5-n10\"", ")", ",", "\n", "(", "2", ",", "\"sample-b64-p10-n20\"", ")", ",", "\n", "(", "4", ",", "\"sample-b64-p20-n40\"", ")", ",", "\n", "(", "6", ",", "\"sample-b64-p30-n60\"", ")", ",", "\n", "(", "8", ",", "\"sample-b64-p40-n80\"", ")", ",", "\n", "]", "\n", "\n", "datasets", "=", "[", "\"wisdm_ar\"", "]", "\n", "methods", "=", "[", "\"calda_xs_r\"", ",", "\"calda_xs_h\"", "]", "\n", "\n", "def", "output_results", "(", "prefixes", ",", "output_filename_prefix", ")", ":", "\n", "        ", "\"\"\" Generate plot and CSV file \"\"\"", "\n", "results", "=", "all_results", "(", "prefixes", ",", "datasets", ",", "methods", ",", "n", ")", "\n", "csv_results", "=", "get_csv", "(", "results", ",", "output_filename_prefix", "+", "\".csv\"", ")", "\n", "plot", "(", "csv_results", ",", "output_filename_prefix", "+", "\".pdf\"", ")", "\n", "\n", "", "output_results", "(", "prefixes_b64", ",", "\"sampling_analysis_b64\"", ")", "\n", "output_results", "(", "prefixes_b128", ",", "\"sampling_analysis_b128\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.class_balance.get_labels": [[24, 32], ["numpy.hstack", "ys.append", "y.numpy"], "function", ["None"], ["def", "get_labels", "(", "dataset", ")", ":", "\n", "    ", "\"\"\" Count training examples for all the sources datasets \"\"\"", "\n", "ys", "=", "[", "]", "\n", "\n", "for", "_", ",", "y", ",", "_", "in", "dataset", ":", "\n", "        ", "ys", ".", "append", "(", "y", ".", "numpy", "(", ")", ")", "\n", "\n", "", "return", "np", ".", "hstack", "(", "ys", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.class_balance.calc_class_balance": [[34, 51], ["range", "sum", "sum"], "function", ["None"], ["", "def", "calc_class_balance", "(", "labels", ",", "num_classes", ")", ":", "\n", "    ", "\"\"\" Count number of labels from each class in the dataset\n\n    (Copied from methods.py)\n    \"\"\"", "\n", "p_y", "=", "[", "0", "]", "*", "num_classes", "\n", "\n", "for", "class_num", "in", "range", "(", "0", ",", "num_classes", ")", ":", "\n", "# Count instances of this class", "\n", "        ", "this_class_count", "=", "sum", "(", "labels", "==", "class_num", ")", "\n", "p_y", "[", "class_num", "]", "=", "this_class_count", "\n", "\n", "# Normalize to make P(y) sum to one like a proper probability", "\n", "# distribution", "\n", "", "p_y", "=", "p_y", "/", "sum", "(", "p_y", ")", "\n", "\n", "return", "p_y", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.class_balance.class_balance": [[53, 56], ["class_balance.calc_class_balance", "class_balance.get_labels"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.class_balance.calc_class_balance", "home.repos.pwc.inspect_result.floft_calda.None.class_balance.get_labels"], ["", "def", "class_balance", "(", "dataset", ",", "num_classes", ")", ":", "\n", "    ", "\"\"\" First get the labels as a numpy array, then calculate label proportions \"\"\"", "\n", "return", "calc_class_balance", "(", "get_labels", "(", "dataset", ")", ",", "num_classes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.class_balance.print_table": [[58, 64], ["print", "classes.items", "print", "print"], "function", ["None"], ["", "def", "print_table", "(", "title", ",", "classes", ",", "total_width", "=", "5", ")", ":", "\n", "    ", "\"\"\" Print the table of dataset and then each of the classes \"\"\"", "\n", "print", "(", "title", ")", "\n", "for", "n", ",", "v", "in", "classes", ".", "items", "(", ")", ":", "\n", "        ", "print", "(", "n", ",", "*", "[", "\"{:.1f}\"", ".", "format", "(", "x", "*", "100", ")", ".", "rjust", "(", "total_width", ")", "for", "x", "in", "v", "]", ",", "sep", "=", "\" \"", ")", "\n", "", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.class_balance.print_class_balances": [[66, 88], ["class_balance.print_table", "class_balance.print_table", "class_balance.class_balance", "class_balance.class_balance", "str"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.class_balance.print_table", "home.repos.pwc.inspect_result.floft_calda.None.class_balance.print_table", "home.repos.pwc.inspect_result.floft_calda.None.class_balance.class_balance", "home.repos.pwc.inspect_result.floft_calda.None.class_balance.class_balance"], ["", "def", "print_class_balances", "(", "dataset_name", ",", "user_source_pairs", ")", ":", "\n", "    ", "classes_train", "=", "{", "}", "\n", "classes_test", "=", "{", "}", "\n", "\n", "for", "user", ",", "source", "in", "user_source_pairs", ":", "\n", "        ", "train", "=", "None", "\n", "test", "=", "None", "\n", "\n", "if", "source", ".", "train_evaluation", "is", "not", "None", ":", "\n", "            ", "train", "=", "class_balance", "(", "source", ".", "train_evaluation", ",", "source", ".", "num_classes", ")", "\n", "", "if", "source", ".", "test_evaluation", "is", "not", "None", ":", "\n", "            ", "test", "=", "class_balance", "(", "source", ".", "test_evaluation", ",", "source", ".", "num_classes", ")", "\n", "\n", "", "name", "=", "dataset_name", "+", "\"_\"", "+", "str", "(", "user", ")", "\n", "\n", "if", "train", "is", "not", "None", ":", "\n", "            ", "classes_train", "[", "name", "]", "=", "train", "\n", "", "if", "test", "is", "not", "None", ":", "\n", "            ", "classes_test", "[", "name", "]", "=", "test", "\n", "\n", "", "", "print_table", "(", "dataset_name", "+", "\" (train)\"", ",", "classes_train", ")", "\n", "print_table", "(", "dataset_name", "+", "\" (test)\"", ",", "classes_test", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.class_balance.main": [[90, 114], ["datasets.datasets.list_datasets", "datasets.datasets.get_dataset_users", "class_balance.print_class_balances", "load_datasets.load_da", "user_source_pairs.append", "str", "len"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.list_datasets", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.get_dataset_users", "home.repos.pwc.inspect_result.floft_calda.None.class_balance.print_class_balances", "home.repos.pwc.inspect_result.floft_calda.None.load_datasets.load_da"], ["", "def", "main", "(", "argv", ")", ":", "\n", "# Don't bother using the GPU for this", "\n", "    ", "os", ".", "environ", "[", "\"CUDA_VISIBLE_DEVICES\"", "]", "=", "\"\"", "\n", "\n", "for", "dataset_name", "in", "datasets", ".", "list_datasets", "(", ")", ":", "\n", "        ", "user_source_pairs", "=", "[", "]", "\n", "\n", "# We run out of memory...", "\n", "if", "\"casas\"", "in", "dataset_name", ":", "\n", "            ", "continue", "\n", "\n", "", "for", "user", "in", "datasets", ".", "get_dataset_users", "(", "dataset_name", ")", ":", "\n", "# Note: test=False so we only look at the training samples, where", "\n", "# train=80% of training set, test=20% of training set, i.e. the", "\n", "# validation set", "\n", "            ", "sources", ",", "_", "=", "load_da", "(", "dataset_name", ",", "str", "(", "user", ")", ",", "\"\"", ",", "test", "=", "False", ")", "\n", "\n", "# We load them one at a time", "\n", "assert", "len", "(", "sources", ")", "==", "1", "\n", "source", "=", "sources", "[", "0", "]", "\n", "\n", "user_source_pairs", ".", "append", "(", "(", "user", ",", "source", ")", ")", "\n", "\n", "", "print_class_balances", "(", "dataset_name", ",", "user_source_pairs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.hyperparameter_tuning_experiments.generate_hyperparameter_list": [[14, 82], ["enumerate", "runlist.append", "experiments.append", "len", "random.Random().shuffle", "random.shuffle", "hyperparameter_tuning_experiments.generate_hyperparameter_list.format_f"], "function", ["None"], ["def", "generate_hyperparameter_list", "(", "max_experiments", "=", "None", ",", "seed", "=", "None", ",", "\n", "baseline", "=", "False", ",", "dataset", "=", "None", ",", "method", "=", "None", ")", ":", "\n", "    ", "lr_options", "=", "[", "0.00001", ",", "0.0001", ",", "0.001", "]", "# one power of 10 bigger/smaller than current", "\n", "similarity_weight_options", "=", "[", "1.0", ",", "10.0", ",", "100.0", "]", "\n", "max_positives_options", "=", "[", "5", ",", "10", "]", "\n", "neg_pos_ratio_options", "=", "[", "2", ",", "4", "]", "# how many negatives for each positive", "\n", "temperature_options", "=", "[", "0.01", ",", "0.05", ",", "0.1", ",", "0.5", "]", "\n", "\n", "experiments", "=", "[", "]", "\n", "\n", "if", "baseline", ":", "\n", "# Baselines only use learning rate", "\n", "        ", "for", "lr", "in", "lr_options", ":", "\n", "# Other values don't matter, so just set to 0 or 1", "\n", "            ", "experiments", ".", "append", "(", "[", "lr", ",", "0", ",", "1", ",", "1", ",", "0", "]", ")", "\n", "", "", "else", ":", "\n", "# All combinations of the above", "\n", "        ", "for", "lr", "in", "lr_options", ":", "\n", "            ", "for", "similarity_weight", "in", "similarity_weight_options", ":", "\n", "                ", "for", "max_positives", "in", "max_positives_options", ":", "\n", "                    ", "for", "neg_pos_ratio", "in", "neg_pos_ratio_options", ":", "\n", "                        ", "max_negatives", "=", "neg_pos_ratio", "*", "max_positives", "\n", "\n", "for", "temperature", "in", "temperature_options", ":", "\n", "                            ", "experiments", ".", "append", "(", "[", "\n", "lr", ",", "\n", "similarity_weight", ",", "\n", "max_positives", ",", "\n", "max_negatives", ",", "\n", "temperature", ",", "\n", "]", ")", "\n", "\n", "# Limit number (if there's enough experiments)", "\n", "", "", "", "", "", "", "if", "max_experiments", "is", "not", "None", "and", "len", "(", "experiments", ")", ">", "max_experiments", ":", "\n", "        ", "if", "seed", "is", "not", "None", ":", "\n", "            ", "random", ".", "Random", "(", "seed", ")", ".", "shuffle", "(", "experiments", ")", "\n", "", "else", ":", "\n", "            ", "random", ".", "shuffle", "(", "experiments", ")", "\n", "\n", "", "experiments", "=", "experiments", "[", ":", "max_experiments", "]", "\n", "\n", "# Output list of hyperparameters for this method", "\n", "", "runlist", "=", "[", "]", "\n", "\n", "for", "i", ",", "the_tuple", "in", "enumerate", "(", "experiments", ")", ":", "\n", "        ", "lr", ",", "w", ",", "p", ",", "n", ",", "t", "=", "the_tuple", "\n", "\n", "def", "format_f", "(", "f", ")", ":", "\n", "# Probably isn't always valid, but works for our numbers", "\n", "            ", "return", "\"{:f}\"", ".", "format", "(", "f", ")", ".", "rstrip", "(", "\"0\"", ")", ".", "rstrip", "(", "\".\"", ")", "\n", "\n", "# Folder for the logs/models", "\n", "", "folder", "=", "\"lr{lr}_w{w}_p{p}_n{n}_t{t}\"", ".", "format", "(", "\n", "lr", "=", "format_f", "(", "lr", ")", ",", "w", "=", "format_f", "(", "w", ")", ",", "p", "=", "p", ",", "n", "=", "n", ",", "t", "=", "format_f", "(", "t", ")", ",", "\n", ")", "\n", "\n", "# Args that set the hyperparameters", "\n", "options", "=", "\"--lr={lr} \"", "\"--similarity_weight={w} \"", "\"--max_positives={p} \"", "\"--max_negatives={n} \"", "\"--temperature={t}\"", ".", "format", "(", "\n", "lr", "=", "format_f", "(", "lr", ")", ",", "w", "=", "format_f", "(", "w", ")", ",", "p", "=", "p", ",", "n", "=", "n", ",", "t", "=", "format_f", "(", "t", ")", ",", "\n", ")", "\n", "\n", "runlist", ".", "append", "(", "(", "folder", ",", "options", ",", "the_tuple", ")", ")", "\n", "\n", "", "return", "runlist", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.checkpoints.CheckpointManager.__init__": [[41, 81], ["tensorflow.train.CheckpointManager", "os.path.join", "tensorflow.train.CheckpointManager", "os.path.join", "tensorflow.train.CheckpointManager", "file_utils.get_best_valid", "file_utils.get_best_valid", "len"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.file_utils.get_best_valid", "home.repos.pwc.inspect_result.floft_calda.None.file_utils.get_best_valid"], ["def", "__init__", "(", "self", ",", "checkpoint", ",", "model_dir", ",", "log_dir", ")", ":", "\n", "        ", "self", ".", "checkpoint", "=", "checkpoint", "\n", "self", ".", "log_dir", "=", "log_dir", "\n", "\n", "# Keep track of the latest for restoring interrupted training", "\n", "self", ".", "latest_manager", "=", "tf", ".", "train", ".", "CheckpointManager", "(", "\n", "checkpoint", ",", "directory", "=", "model_dir", ",", "max_to_keep", "=", "FLAGS", ".", "latest_checkpoints", ")", "\n", "\n", "# Keeps track of our best model for use after training", "\n", "best_model_dir_source", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "\"best_source\"", ")", "\n", "\n", "self", ".", "best_manager_source", "=", "tf", ".", "train", ".", "CheckpointManager", "(", "\n", "checkpoint", ",", "directory", "=", "best_model_dir_source", ",", "\n", "max_to_keep", "=", "FLAGS", ".", "best_checkpoints", ")", "\n", "\n", "best_model_dir_target", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "\"best_target\"", ")", "\n", "self", ".", "best_manager_target", "=", "tf", ".", "train", ".", "CheckpointManager", "(", "\n", "checkpoint", ",", "directory", "=", "best_model_dir_target", ",", "\n", "max_to_keep", "=", "FLAGS", ".", "best_checkpoints", ")", "\n", "\n", "# Restore best from file or if no file yet, set it to zero", "\n", "self", ".", "best_validation_source", "=", "get_best_valid", "(", "self", ".", "log_dir", ",", "\n", "filename", "=", "\"best_valid_accuracy_source.txt\"", ")", "\n", "\n", "self", ".", "best_validation_target", "=", "get_best_valid", "(", "self", ".", "log_dir", ",", "\n", "filename", "=", "\"best_valid_accuracy_target.txt\"", ")", "\n", "\n", "# Do we have these checkpoints -- used to verify we were able to load", "\n", "# the previous or best checkpoint during evaluation", "\n", "self", ".", "found_last", "=", "len", "(", "self", ".", "latest_manager", ".", "checkpoints", ")", "!=", "0", "\n", "self", ".", "found_best_source", "=", "True", "\n", "self", ".", "found_best_target", "=", "True", "\n", "\n", "if", "self", ".", "best_validation_source", "is", "None", ":", "\n", "            ", "self", ".", "found_best_source", "=", "False", "\n", "self", ".", "best_validation_source", "=", "0.0", "\n", "\n", "", "if", "self", ".", "best_validation_target", "is", "None", ":", "\n", "            ", "self", ".", "found_best_target", "=", "False", "\n", "self", ".", "best_validation_target", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.checkpoints.CheckpointManager.restore_latest": [[82, 85], ["checkpoints.CheckpointManager.checkpoint.restore().expect_partial", "checkpoints.CheckpointManager.checkpoint.restore"], "methods", ["None"], ["", "", "def", "restore_latest", "(", "self", ")", ":", "\n", "        ", "\"\"\" Restore the checkpoint from the latest one \"\"\"", "\n", "self", ".", "checkpoint", ".", "restore", "(", "self", ".", "latest_manager", ".", "latest_checkpoint", ")", ".", "expect_partial", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.checkpoints.CheckpointManager.restore_best_source": [[86, 91], ["checkpoints.CheckpointManager.checkpoint.restore().expect_partial", "checkpoints.CheckpointManager.checkpoint.restore"], "methods", ["None"], ["", "def", "restore_best_source", "(", "self", ")", ":", "\n", "        ", "\"\"\" Restore the checkpoint from the best one on the source valid data \"\"\"", "\n", "# Note: using expect_partial() so we don't get warnings about loading", "\n", "# only some of the weights", "\n", "self", ".", "checkpoint", ".", "restore", "(", "self", ".", "best_manager_source", ".", "latest_checkpoint", ")", ".", "expect_partial", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.checkpoints.CheckpointManager.restore_best_target": [[92, 97], ["checkpoints.CheckpointManager.checkpoint.restore().expect_partial", "checkpoints.CheckpointManager.checkpoint.restore"], "methods", ["None"], ["", "def", "restore_best_target", "(", "self", ")", ":", "\n", "        ", "\"\"\" Restore the checkpoint from the best one on the target valid data \"\"\"", "\n", "# Note: using expect_partial() so we don't get warnings about loading", "\n", "# only some of the weights", "\n", "self", ".", "checkpoint", ".", "restore", "(", "self", ".", "best_manager_target", ".", "latest_checkpoint", ")", ".", "expect_partial", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.checkpoints.CheckpointManager.latest_step": [[98, 102], ["checkpoints.CheckpointManager._get_step_from_manager"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.checkpoints.CheckpointManager._get_step_from_manager"], ["", "def", "latest_step", "(", "self", ")", ":", "\n", "        ", "\"\"\" Return the step number from the latest checkpoint. Returns None if\n        no checkpoints. \"\"\"", "\n", "return", "self", ".", "_get_step_from_manager", "(", "self", ".", "latest_manager", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.checkpoints.CheckpointManager.best_step_source": [[103, 107], ["checkpoints.CheckpointManager._get_step_from_manager"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.checkpoints.CheckpointManager._get_step_from_manager"], ["", "def", "best_step_source", "(", "self", ")", ":", "\n", "        ", "\"\"\" Return the step number from the best source checkpoint. Returns None\n        if no checkpoints. \"\"\"", "\n", "return", "self", ".", "_get_step_from_manager", "(", "self", ".", "best_manager_source", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.checkpoints.CheckpointManager.best_step_target": [[108, 112], ["checkpoints.CheckpointManager._get_step_from_manager"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.checkpoints.CheckpointManager._get_step_from_manager"], ["", "def", "best_step_target", "(", "self", ")", ":", "\n", "        ", "\"\"\" Return the step number from the best target checkpoint. Returns None\n        if no checkpoints. \"\"\"", "\n", "return", "self", ".", "_get_step_from_manager", "(", "self", ".", "best_manager_target", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.checkpoints.CheckpointManager._get_step_from_manager": [[113, 126], ["os.path.basename", "file_utils.get_last_int", "len"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.file_utils.get_last_int"], ["", "def", "_get_step_from_manager", "(", "self", ",", "manager", ")", ":", "\n", "# If no checkpoints found", "\n", "        ", "if", "len", "(", "manager", ".", "checkpoints", ")", "==", "0", ":", "\n", "            ", "return", "None", "\n", "\n", "# If one is found, the last checkpoint will be a string like", "\n", "#   \"models/target-foldX-model-debugnum/ckpt-100'", "\n", "# and we want to step number at the end, e.g. 100 in this example", "\n", "", "last", "=", "manager", ".", "checkpoints", "[", "-", "1", "]", "# sorted oldest to newest", "\n", "name", "=", "os", ".", "path", ".", "basename", "(", "last", ")", "\n", "step", "=", "get_last_int", "(", "name", ",", "only_one", "=", "True", ")", "\n", "\n", "return", "step", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.None.checkpoints.CheckpointManager.save": [[127, 152], ["checkpoints.CheckpointManager.latest_manager.save", "checkpoints.CheckpointManager.best_manager_source.save", "file_utils.write_best_valid", "checkpoints.CheckpointManager.best_manager_target.save", "file_utils.write_best_valid"], "methods", ["home.repos.pwc.inspect_result.floft_calda.None.checkpoints.CheckpointManager.save", "home.repos.pwc.inspect_result.floft_calda.None.checkpoints.CheckpointManager.save", "home.repos.pwc.inspect_result.floft_calda.None.file_utils.write_best_valid", "home.repos.pwc.inspect_result.floft_calda.None.checkpoints.CheckpointManager.save", "home.repos.pwc.inspect_result.floft_calda.None.file_utils.write_best_valid"], ["", "def", "save", "(", "self", ",", "step", ",", "validation_accuracy_source", "=", "None", ",", "\n", "validation_accuracy_target", "=", "None", ")", ":", "\n", "        ", "\"\"\" Save the latest model. If validation_accuracy_* specified and higher\n        than the previous best, also save this model as the new best one. \"\"\"", "\n", "# Always save the latest", "\n", "self", ".", "latest_manager", ".", "save", "(", "checkpoint_number", "=", "step", ")", "\n", "\n", "# Only save the \"best\" if it's better than the previous best", "\n", "if", "validation_accuracy_source", "is", "not", "None", ":", "\n", "            ", "if", "validation_accuracy_source", ">", "self", ".", "best_validation_source", "or", "not", "self", ".", "found_best_source", ":", "\n", "                ", "self", ".", "best_manager_source", ".", "save", "(", "checkpoint_number", "=", "step", ")", "\n", "self", ".", "best_validation_source", "=", "validation_accuracy_source", "\n", "write_best_valid", "(", "self", ".", "log_dir", ",", "\n", "self", ".", "best_validation_source", ",", "\n", "filename", "=", "\"best_valid_accuracy_source.txt\"", ")", "\n", "\n", "", "", "if", "validation_accuracy_target", "is", "not", "None", ":", "\n", "            ", "if", "validation_accuracy_target", ">", "self", ".", "best_validation_target", "or", "not", "self", ".", "found_best_target", ":", "\n", "                ", "self", ".", "best_manager_target", ".", "save", "(", "checkpoint_number", "=", "step", ")", "\n", "self", ".", "best_validation_target", "=", "validation_accuracy_target", "\n", "write_best_valid", "(", "self", ".", "log_dir", ",", "\n", "self", ".", "best_validation_target", ",", "\n", "filename", "=", "\"best_valid_accuracy_target.txt\"", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.floft_calda.datasets.main_as_images.write": [[41, 49], ["os.path.exists", "datasets.tfrecord.write_tfrecord", "print", "print"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.write_tfrecord"], ["def", "write", "(", "filename", ",", "x", ",", "y", ")", ":", "\n", "    ", "if", "x", "is", "not", "None", "and", "y", "is", "not", "None", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "filename", ")", ":", "\n", "            ", "write_tfrecord", "(", "filename", ",", "x", ",", "y", ")", "\n", "", "elif", "FLAGS", ".", "debug", ":", "\n", "            ", "print", "(", "\"Skipping:\"", ",", "filename", ",", "\"(already exists)\"", ")", "\n", "", "", "elif", "FLAGS", ".", "debug", ":", "\n", "        ", "print", "(", "\"Skipping:\"", ",", "filename", ",", "\"(no data)\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.main_as_images.write_modality": [[51, 60], ["os.path.exists", "datasets.tfrecord.write_tfrecord_modality", "print", "print"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.write_tfrecord_modality"], ["", "", "def", "write_modality", "(", "filename", ",", "xs", ",", "y", ")", ":", "\n", "    ", "\"\"\" Same as write() except calls the multi-modality version \"\"\"", "\n", "if", "xs", "is", "not", "None", "and", "y", "is", "not", "None", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "filename", ")", ":", "\n", "            ", "write_tfrecord_modality", "(", "filename", ",", "xs", ",", "y", ")", "\n", "", "elif", "FLAGS", ".", "debug", ":", "\n", "            ", "print", "(", "\"Skipping:\"", ",", "filename", ",", "\"(already exists)\"", ")", "\n", "", "", "elif", "FLAGS", ".", "debug", ":", "\n", "        ", "print", "(", "\"Skipping:\"", ",", "filename", ",", "\"(no data)\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.main_as_images.write_modality_as_images_v1": [[62, 99], ["os.path.exists", "len", "print", "range", "print", "len", "len", "len", "os.path.join", "os.path.join", "numpy.transpose", "numpy.save", "str", "os.path.exists", "os.makedirs", "len", "range", "int"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.checkpoints.CheckpointManager.save"], ["", "", "def", "write_modality_as_images_v1", "(", "folder", ",", "xs", ",", "y", ")", ":", "\n", "    ", "\"\"\" Write as individual files - turns out this is very very slow over a\n    network file system. Use v2 instead. \"\"\"", "\n", "if", "xs", "is", "not", "None", "and", "y", "is", "not", "None", ":", "\n", "        ", "if", "os", ".", "path", ".", "exists", "(", "folder", ")", ":", "\n", "# write to files", "\n", "            ", "num_modalities", "=", "len", "(", "xs", ")", "\n", "assert", "num_modalities", "==", "1", ",", "\"for now only support 1 modality for the image-based methods\"", "\n", "for", "x", "in", "xs", ":", "\n", "                ", "assert", "len", "(", "x", ")", "==", "len", "(", "y", ")", ",", "\"each modality must have same length as y\"", "\n", "for", "i", "in", "range", "(", "len", "(", "y", ")", ")", ":", "\n", "# Get the x for each of this example's modality along with the", "\n", "# corresponding y value", "\n", "                    ", "out_x", "=", "[", "xs", "[", "m", "]", "[", "i", "]", "for", "m", "in", "range", "(", "num_modalities", ")", "]", "\n", "out_folder", "=", "os", ".", "path", ".", "join", "(", "folder", ",", "str", "(", "int", "(", "y", "[", "i", "]", ")", ")", ")", "\n", "out_filename", "=", "os", ".", "path", ".", "join", "(", "out_folder", ",", "\"{}.npy\"", ".", "format", "(", "i", ")", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "out_folder", ")", ":", "\n", "                        ", "os", ".", "makedirs", "(", "out_folder", ")", "\n", "\n", "# limiting to one modality for now", "\n", "", "out_x", "=", "out_x", "[", "0", "]", "\n", "\n", "# torch is (num features [channels], time steps) rather than", "\n", "# (time steps, num features)", "\n", "assert", "len", "(", "out_x", ".", "shape", ")", "==", "2", ",", "\"expecting 2 dimensions\"", "\n", "out_x", "=", "np", ".", "transpose", "(", "out_x", ",", "axes", "=", "[", "1", ",", "0", "]", ")", "\n", "\n", "np", ".", "save", "(", "out_filename", ",", "out_x", ",", "allow_pickle", "=", "False", ")", "\n", "\n", "#for i, modality in enumerate(modalities):", "\n", "#    feature[modality] = _feature(xs[i])", "\n", "#feature[\"y\"] = _feature(y)", "\n", "", "", "", "elif", "FLAGS", ".", "debug", ":", "\n", "            ", "print", "(", "\"Skipping:\"", ",", "folder", ",", "\"(folder does not exists)\"", ")", "\n", "", "", "elif", "FLAGS", ".", "debug", ":", "\n", "        ", "print", "(", "\"Skipping:\"", ",", "folder", ",", "\"(no data)\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.main_as_images.write_modality_as_images_v2": [[101, 134], ["len", "pickle_data.save_pickle", "range", "print", "len", "len", "len", "str", "numpy.transpose", "output[].append", "int", "len", "range"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.pickle_data.save_pickle"], ["", "", "def", "write_modality_as_images_v2", "(", "filename", ",", "xs", ",", "y", ")", ":", "\n", "    ", "\"\"\" Create one pickle file for each domain of each dataset \"\"\"", "\n", "if", "xs", "is", "not", "None", "and", "y", "is", "not", "None", ":", "\n", "        ", "output", "=", "{", "}", "\n", "\n", "num_modalities", "=", "len", "(", "xs", ")", "\n", "assert", "num_modalities", "==", "1", ",", "\"for now only support 1 modality for the image-based methods\"", "\n", "for", "x", "in", "xs", ":", "\n", "            ", "assert", "len", "(", "x", ")", "==", "len", "(", "y", ")", ",", "\"each modality must have same length as y\"", "\n", "for", "i", "in", "range", "(", "len", "(", "y", ")", ")", ":", "\n", "# Get the x for each of this example's modality along with the", "\n", "# corresponding y value", "\n", "                ", "out_x", "=", "[", "xs", "[", "m", "]", "[", "i", "]", "for", "m", "in", "range", "(", "num_modalities", ")", "]", "\n", "out_y", "=", "str", "(", "int", "(", "y", "[", "i", "]", ")", ")", "\n", "\n", "# limiting to one modality for now", "\n", "out_x", "=", "out_x", "[", "0", "]", "\n", "\n", "# torch is (num features [channels], time steps) rather than", "\n", "# (time steps, num features)", "\n", "assert", "len", "(", "out_x", ".", "shape", ")", "==", "2", ",", "\"expecting 2 dimensions\"", "\n", "out_x", "=", "np", ".", "transpose", "(", "out_x", ",", "axes", "=", "[", "1", ",", "0", "]", ")", "\n", "\n", "# Save", "\n", "if", "out_y", "not", "in", "output", ":", "\n", "                    ", "output", "[", "out_y", "]", "=", "[", "]", "\n", "\n", "", "output", "[", "out_y", "]", ".", "append", "(", "out_x", ")", "\n", "\n", "", "", "save_pickle", "(", "filename", ",", "output", ")", "\n", "\n", "", "elif", "FLAGS", ".", "debug", ":", "\n", "        ", "print", "(", "\"Skipping:\"", ",", "filename", ",", "\"(no data)\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.main_as_images.shuffle_together_calc": [[136, 141], ["numpy.random.RandomState", "np.random.RandomState.permutation"], "function", ["None"], ["", "", "def", "shuffle_together_calc", "(", "length", ",", "seed", "=", "None", ")", ":", "\n", "    ", "\"\"\" Generate indices of numpy array shuffling, then do x[p] \"\"\"", "\n", "rand", "=", "np", ".", "random", ".", "RandomState", "(", "seed", ")", "\n", "p", "=", "rand", ".", "permutation", "(", "length", ")", "\n", "return", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.main_as_images.to_numpy": [[143, 148], ["isinstance", "type", "value.numpy.numpy", "tensorflow.constant"], "function", ["None"], ["", "def", "to_numpy", "(", "value", ")", ":", "\n", "    ", "\"\"\" Make sure value is numpy array \"\"\"", "\n", "if", "isinstance", "(", "value", ",", "type", "(", "tf", ".", "constant", "(", "0", ")", ")", ")", ":", "\n", "        ", "value", "=", "value", ".", "numpy", "(", ")", "\n", "", "return", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.main_as_images.valid_split": [[150, 167], ["int", "sklearn.model_selection.train_test_split", "len", "print"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.train_test_split"], ["", "def", "valid_split", "(", "data", ",", "labels", ",", "seed", "=", "None", ",", "validation_size", "=", "1000", ")", ":", "\n", "    ", "\"\"\" (Stratified) split training data into train/valid as is commonly done,\n    taking 1000 random (stratified) (labeled, even if target domain) samples for\n    a validation set \"\"\"", "\n", "percentage_size", "=", "int", "(", "0.2", "*", "len", "(", "data", ")", ")", "\n", "if", "percentage_size", ">", "validation_size", ":", "\n", "        ", "test_size", "=", "validation_size", "\n", "", "else", ":", "\n", "        ", "if", "FLAGS", ".", "debug", ":", "\n", "            ", "print", "(", "\"Warning: using smaller validation set size\"", ",", "percentage_size", ")", "\n", "", "test_size", "=", "0.2", "# 20% maximum", "\n", "\n", "", "x_train", ",", "x_valid", ",", "y_train", ",", "y_valid", "=", "train_test_split", "(", "data", ",", "labels", ",", "test_size", "=", "test_size", ",", "\n", "stratify", "=", "labels", ",", "random_state", "=", "seed", ")", "\n", "\n", "return", "x_train", ",", "y_train", ",", "x_valid", ",", "y_valid", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.main_as_images.valid_split_modality": [[169, 198], ["int", "sklearn.model_selection.train_test_split", "len", "print", "len"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.train_test_split"], ["", "def", "valid_split_modality", "(", "xs", ",", "y", ",", "seed", "=", "None", ",", "validation_size", "=", "1000", ")", ":", "\n", "    ", "\"\"\" (Stratified) split training data into train/valid as is commonly done,\n    taking 1000 random (stratified) (labeled, even if target domain) samples for\n    a validation set \"\"\"", "\n", "percentage_size", "=", "int", "(", "0.2", "*", "len", "(", "xs", "[", "0", "]", ")", ")", "\n", "if", "percentage_size", ">", "validation_size", ":", "\n", "        ", "test_size", "=", "validation_size", "\n", "", "else", ":", "\n", "        ", "if", "FLAGS", ".", "debug", ":", "\n", "            ", "print", "(", "\"Warning: using smaller validation set size\"", ",", "percentage_size", ")", "\n", "", "test_size", "=", "0.2", "# 20% maximum", "\n", "\n", "# Returns train/test for each input passed in, so: xs1_train, xs1_test,", "\n", "# xs2_train, xs2_test, ...", "\n", "", "results", "=", "train_test_split", "(", "*", "xs", ",", "y", ",", "test_size", "=", "test_size", ",", "\n", "stratify", "=", "y", ",", "random_state", "=", "seed", ")", "\n", "\n", "# Train is evens (starting at position 0), test is odds", "\n", "assert", "len", "(", "results", ")", "%", "2", "==", "0", ",", "\"should get even number of splits\"", "\n", "train", "=", "results", "[", "0", ":", ":", "2", "]", "\n", "valid", "=", "results", "[", "1", ":", ":", "2", "]", "\n", "\n", "# y is at the end, xs is everything else", "\n", "xs_train", "=", "train", "[", ":", "-", "1", "]", "\n", "xs_valid", "=", "valid", "[", ":", "-", "1", "]", "\n", "y_train", "=", "train", "[", "-", "1", "]", "\n", "y_valid", "=", "valid", "[", "-", "1", "]", "\n", "\n", "return", "xs_train", ",", "y_train", ",", "xs_valid", ",", "y_valid", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.main_as_images.save_dataset": [[200, 274], ["os.path.join", "os.path.join", "os.path.join", "datasets.datasets.load", "main_as_images.valid_split_modality", "main_as_images.write_modality_as_images_v2", "main_as_images.write_modality_as_images_v2", "main_as_images.write_modality_as_images_v2", "datasets.tfrecord.tfrecord_as_images_folder", "datasets.tfrecord.tfrecord_as_images_folder", "datasets.tfrecord.tfrecord_as_images_folder", "os.path.exists", "os.path.exists", "os.path.exists", "print", "sys.stdout.flush", "datasets.normalization.calc_normalization_modality", "datasets.normalization.apply_normalization_modality", "datasets.normalization.apply_normalization_modality", "datasets.normalization.apply_normalization_modality", "print"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.load", "home.repos.pwc.inspect_result.floft_calda.datasets.main.valid_split_modality", "home.repos.pwc.inspect_result.floft_calda.datasets.main_as_images.write_modality_as_images_v2", "home.repos.pwc.inspect_result.floft_calda.datasets.main_as_images.write_modality_as_images_v2", "home.repos.pwc.inspect_result.floft_calda.datasets.main_as_images.write_modality_as_images_v2", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.tfrecord_as_images_folder", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.tfrecord_as_images_folder", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.tfrecord_as_images_folder", "home.repos.pwc.inspect_result.floft_calda.datasets.normalization.calc_normalization_modality", "home.repos.pwc.inspect_result.floft_calda.datasets.normalization.apply_normalization_modality", "home.repos.pwc.inspect_result.floft_calda.datasets.normalization.apply_normalization_modality", "home.repos.pwc.inspect_result.floft_calda.datasets.normalization.apply_normalization_modality"], ["", "def", "save_dataset", "(", "dataset_name", ",", "output_dir", ",", "seed", "=", "0", ")", ":", "\n", "    ", "\"\"\" Save single dataset \"\"\"", "\n", "# For v1", "\n", "# train_folder = os.path.join(output_dir,", "\n", "#     tfrecord_as_images_folder(dataset_name, \"train\"))", "\n", "# valid_folder = os.path.join(output_dir,", "\n", "#     tfrecord_as_images_folder(dataset_name, \"valid\"))", "\n", "# test_folder = os.path.join(output_dir,", "\n", "#     tfrecord_as_images_folder(dataset_name, \"test\"))", "\n", "\n", "# # Skip if they already exist", "\n", "# if os.path.exists(train_folder) \\", "\n", "#         and os.path.exists(valid_folder) \\", "\n", "#         and os.path.exists(test_folder):", "\n", "#     if FLAGS.debug:", "\n", "#         print(\"Skipping:\", train_folder, valid_folder, test_folder,", "\n", "#            \"already exist\")", "\n", "#     return", "\n", "# else:", "\n", "#     if not os.path.exists(train_folder):", "\n", "#         os.makedirs(train_folder)", "\n", "#     if not os.path.exists(valid_folder):", "\n", "#         os.makedirs(valid_folder)", "\n", "#     if not os.path.exists(test_folder):", "\n", "#         os.makedirs(test_folder)", "\n", "\n", "train_filename", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\n", "tfrecord_as_images_folder", "(", "dataset_name", ",", "\"train.pickle\"", ")", ")", "\n", "valid_filename", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\n", "tfrecord_as_images_folder", "(", "dataset_name", ",", "\"valid.pickle\"", ")", ")", "\n", "test_filename", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\n", "tfrecord_as_images_folder", "(", "dataset_name", ",", "\"test.pickle\"", ")", ")", "\n", "\n", "# Skip if they already exist", "\n", "if", "os", ".", "path", ".", "exists", "(", "train_filename", ")", "and", "os", ".", "path", ".", "exists", "(", "valid_filename", ")", "and", "os", ".", "path", ".", "exists", "(", "test_filename", ")", ":", "\n", "        ", "if", "FLAGS", ".", "debug", ":", "\n", "            ", "print", "(", "\"Skipping:\"", ",", "train_filename", ",", "valid_filename", ",", "test_filename", ",", "\n", "\"already exist\"", ")", "\n", "", "return", "\n", "\n", "", "if", "FLAGS", ".", "debug", ":", "\n", "        ", "print", "(", "\"Saving dataset\"", ",", "dataset_name", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "", "dataset", ",", "dataset_class", "=", "datasets", ".", "load", "(", "dataset_name", ")", "\n", "\n", "# Skip if already normalized/bounded, e.g. UCI HAR datasets", "\n", "already_normalized", "=", "dataset_class", ".", "already_normalized", "\n", "\n", "# Split into training/valid datasets", "\n", "train_data", ",", "train_labels", ",", "valid_data", ",", "valid_labels", "=", "valid_split_modality", "(", "dataset", ".", "train_data", ",", "dataset", ".", "train_labels", ",", "seed", "=", "seed", ")", "\n", "\n", "# The multiple modality data is stored like (e.g. where xs = train_data):", "\n", "# xs = [(example 1 modality 1, example 2 modality 1, ...),", "\n", "#       (example 1 modality 2, example 2 modality 2, ...)]", "\n", "\n", "# Calculate normalization only on the training data", "\n", "if", "FLAGS", ".", "normalize", "!=", "\"none\"", "and", "not", "already_normalized", ":", "\n", "        ", "normalization", "=", "calc_normalization_modality", "(", "train_data", ",", "FLAGS", ".", "normalize", ")", "\n", "\n", "# Apply the normalization to the training, validation, and testing data", "\n", "train_data", "=", "apply_normalization_modality", "(", "train_data", ",", "normalization", ")", "\n", "valid_data", "=", "apply_normalization_modality", "(", "valid_data", ",", "normalization", ")", "\n", "test_data", "=", "apply_normalization_modality", "(", "dataset", ".", "test_data", ",", "normalization", ")", "\n", "", "else", ":", "\n", "        ", "test_data", "=", "dataset", ".", "test_data", "\n", "\n", "# Saving", "\n", "", "write_modality_as_images_v2", "(", "train_filename", ",", "train_data", ",", "train_labels", ")", "\n", "write_modality_as_images_v2", "(", "valid_filename", ",", "valid_data", ",", "valid_labels", ")", "\n", "write_modality_as_images_v2", "(", "test_filename", ",", "test_data", ",", "dataset", ".", "test_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.main_as_images.main": [[276, 308], ["os.path.join", "datasets.datasets.names", "os.path.exists", "os.makedirs", "pool.run_job_pool", "main_as_images.save_dataset"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.names", "home.repos.pwc.inspect_result.floft_calda.None.pool.run_job_pool", "home.repos.pwc.inspect_result.floft_calda.datasets.main.save_dataset"], ["", "def", "main", "(", "argv", ")", ":", "\n", "    ", "output_dir", "=", "os", ".", "path", ".", "join", "(", "\"datasets\"", ",", "\"as_images\"", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "output_dir", ")", "\n", "\n", "# Get all possible datasets we can generate, but only the single-modality", "\n", "# ones. We'll generate the multi-modality ones later.", "\n", "", "adaptation_problems", "=", "datasets", ".", "names", "(", "single_modality", "=", "True", ")", "\n", "\n", "# Subset if desired", "\n", "# adaptation_problems = [", "\n", "#     a for a in adaptation_problems if \"uci\" in a or \"wisdm\" in a", "\n", "# ]", "\n", "\n", "# Save tfrecord files for each of the adaptation problems", "\n", "if", "FLAGS", ".", "parallel", "and", "FLAGS", ".", "jobs", "!=", "1", ":", "\n", "# TensorFlow will error from all processes trying to use ~90% of the", "\n", "# GPU memory on all parallel jobs, which will fail, so do this on the", "\n", "# CPU.", "\n", "        ", "os", ".", "environ", "[", "\"CUDA_VISIBLE_DEVICES\"", "]", "=", "\"\"", "\n", "\n", "if", "FLAGS", ".", "jobs", "==", "0", ":", "\n", "            ", "cores", "=", "None", "\n", "", "else", ":", "\n", "            ", "cores", "=", "FLAGS", ".", "jobs", "\n", "\n", "", "run_job_pool", "(", "save_dataset", ",", "\n", "[", "(", "d", ",", "output_dir", ")", "for", "d", "in", "adaptation_problems", "]", ",", "cores", "=", "cores", ")", "\n", "", "else", ":", "\n", "        ", "for", "dataset_name", "in", "adaptation_problems", ":", "\n", "            ", "save_dataset", "(", "dataset_name", ",", "output_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.main.write": [[36, 44], ["os.path.exists", "datasets.tfrecord.write_tfrecord", "print", "print"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.write_tfrecord"], ["flags", ".", "DEFINE_integer", "(", "\"steps\"", ",", "30000", ",", "\"Number of training steps to run\"", ")", "\n", "flags", ".", "DEFINE_float", "(", "\"gpumem\"", ",", "2300", ",", "\"GPU memory to let TensorFlow use, in MiB (0 for all)\"", ")", "\n", "flags", ".", "DEFINE_integer", "(", "\"model_steps\"", ",", "0", ",", "\"Save the model every so many steps (0 for only when log_val_steps)\"", ")", "\n", "flags", ".", "DEFINE_integer", "(", "\"log_train_steps\"", ",", "500", ",", "\"Log training information every so many steps (0 for never)\"", ")", "\n", "flags", ".", "DEFINE_integer", "(", "\"log_val_steps\"", ",", "4000", ",", "\"Log validation information every so many steps (also saves model, 0 for only at end)\"", ")", "\n", "flags", ".", "DEFINE_boolean", "(", "\"test\"", ",", "False", ",", "\"Use real test set for evaluation rather than validation set\"", ")", "\n", "flags", ".", "DEFINE_boolean", "(", "\"subdir\"", ",", "True", ",", "\"Save models/logs in subdirectory of prefix\"", ")", "\n", "flags", ".", "DEFINE_boolean", "(", "\"debug\"", ",", "False", ",", "\"Start new log/model/images rather than continuing from previous run\"", ")", "\n", "flags", ".", "DEFINE_boolean", "(", "\"time_training\"", ",", "False", ",", "\"Print how long each step takes, instead of every 100 steps\"", ")", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.main.write_modality": [[46, 55], ["os.path.exists", "datasets.tfrecord.write_tfrecord_modality", "print", "print"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.write_tfrecord_modality"], ["flags", ".", "DEFINE_boolean", "(", "\"share_most_weights\"", ",", "False", ",", "\"Instead of regularizing weights in heterogeneous domain adaptation, share same-shape weights\"", ")", "\n", "flags", ".", "DEFINE_integer", "(", "\"debugnum\"", ",", "-", "1", ",", "\"Specify exact log/model/images number to use rather than incrementing from last. (Don't pass both this and --debug at the same time.)\"", ")", "\n", "flags", ".", "DEFINE_boolean", "(", "\"restart\"", ",", "False", ",", "\"Restart training, i.e. delete old checkpoints and start training from scratch\"", ")", "\n", "\n", "# By default we only use the first modality (the datasets we're interested in only have one modality anyway)", "\n", "flags", ".", "DEFINE_string", "(", "\"source_modality_subset\"", ",", "\"0\"", ",", "\"List of source modalities to use and in what order (e.g. \\\"0\\\" or \\\"0,1\\\"), others are ignored\"", ")", "\n", "flags", ".", "DEFINE_string", "(", "\"target_modality_subset\"", ",", "\"0\"", ",", "\"List of target modalities to use and in what order (e.g. \\\"0\\\" or \\\"0,1\\\"), others are ignored\"", ")", "\n", "flags", ".", "DEFINE_string", "(", "\"shared_modalities\"", ",", "\"0\"", ",", "\"List of modalities shared between source and target (e.g. \\\"0\\\" or \\\"0,1\\\"), i.e. which modalities' feature representations are [concatenated then] fed to the task classifier. Note: this is after --{source,target}_modality_subset possibly rearranges modalities.\"", ")", "\n", "\n", "flags", ".", "mark_flag_as_required", "(", "\"method\"", ")", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.main.shuffle_together_calc": [[57, 62], ["numpy.random.RandomState", "np.random.RandomState.permutation"], "function", ["None"], ["flags", ".", "mark_flag_as_required", "(", "\"sources\"", ")", "\n", "flags", ".", "mark_flag_as_required", "(", "\"uid\"", ")", "\n", "\n", "\n", "def", "get_directory_names", "(", ")", ":", "\n", "    ", "\"\"\" Figure out the log and model directory names \"\"\"", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.main.to_numpy": [[64, 69], ["isinstance", "type", "value.numpy.numpy", "tensorflow.constant"], "function", ["None"], ["\n", "# Use the number specified on the command line (higher precedence than --debug)", "\n", "if", "FLAGS", ".", "debugnum", ">=", "0", ":", "\n", "        ", "attempt", "=", "FLAGS", ".", "debugnum", "\n", "print", "(", "\"Debugging attempt:\"", ",", "attempt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.main.valid_split": [[71, 88], ["int", "sklearn.model_selection.train_test_split", "len", "print"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.train_test_split"], ["model_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "modeldir", ",", "prefix", ")", "\n", "log_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "logdir", ",", "prefix", ")", "\n", "# Find last one, increment number", "\n", "", "elif", "FLAGS", ".", "debug", ":", "\n", "        ", "attempt", "=", "file_utils", ".", "last_modified_number", "(", "FLAGS", ".", "logdir", ",", "prefix", "+", "\"*\"", ")", "\n", "attempt", "=", "attempt", "+", "1", "if", "attempt", "is", "not", "None", "else", "1", "\n", "print", "(", "\"Debugging attempt:\"", ",", "attempt", ")", "\n", "\n", "prefix", "+=", "\"-\"", "+", "str", "(", "attempt", ")", "\n", "model_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "modeldir", ",", "prefix", ")", "\n", "log_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "logdir", ",", "prefix", ")", "\n", "# If no debugging modes, use the model and log directory with only the \"prefix\"", "\n", "# (even though it's not actually a prefix in this case, it's the whole name)", "\n", "", "elif", "FLAGS", ".", "subdir", ":", "\n", "        ", "model_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "modeldir", ",", "prefix", ")", "\n", "log_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "logdir", ",", "prefix", ")", "\n", "", "else", ":", "\n", "        ", "model_dir", "=", "FLAGS", ".", "modeldir", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.main.valid_split_modality": [[90, 119], ["int", "sklearn.model_selection.train_test_split", "len", "print", "len"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.train_test_split"], ["\n", "", "return", "model_dir", ",", "log_dir", "\n", "\n", "\n", "", "def", "main", "(", "argv", ")", ":", "\n", "# Allow running multiple at once", "\n", "    ", "set_gpu_memory", "(", "FLAGS", ".", "gpumem", ")", "\n", "\n", "# Figure out the log and model directory filenames", "\n", "assert", "FLAGS", ".", "uid", "!=", "\"\"", ",", "\"uid cannot be an empty string\"", "\n", "model_dir", ",", "log_dir", "=", "get_directory_names", "(", ")", "\n", "\n", "if", "FLAGS", ".", "restart", ":", "\n", "        ", "if", "os", ".", "path", ".", "exists", "(", "model_dir", ")", ":", "\n", "            ", "shutil", ".", "rmtree", "(", "model_dir", ")", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "log_dir", ")", ":", "\n", "            ", "shutil", ".", "rmtree", "(", "log_dir", ")", "\n", "\n", "", "", "if", "not", "os", ".", "path", ".", "exists", "(", "model_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "model_dir", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "log_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "log_dir", ")", "\n", "\n", "# Write config file about what dataset we're using, sources, target, etc.", "\n", "", "file_utils", ".", "write_config_from_args", "(", "log_dir", ")", "\n", "\n", "# Changes for upper bound -- upper bound is actually method \"none\" but", "\n", "# without a target domain", "\n", "method_name", "=", "FLAGS", ".", "method", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.main.save_dataset": [[121, 171], ["os.path.join", "os.path.join", "os.path.join", "datasets.datasets.load", "main.valid_split_modality", "main.write_modality", "main.write_modality", "main.write_modality", "datasets.tfrecord.tfrecord_filename", "datasets.tfrecord.tfrecord_filename", "datasets.tfrecord.tfrecord_filename", "os.path.exists", "os.path.exists", "os.path.exists", "print", "sys.stdout.flush", "datasets.normalization.calc_normalization_modality", "datasets.normalization.apply_normalization_modality", "datasets.normalization.apply_normalization_modality", "datasets.normalization.apply_normalization_modality", "print"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.load", "home.repos.pwc.inspect_result.floft_calda.datasets.main.valid_split_modality", "home.repos.pwc.inspect_result.floft_calda.datasets.main.write_modality", "home.repos.pwc.inspect_result.floft_calda.datasets.main.write_modality", "home.repos.pwc.inspect_result.floft_calda.datasets.main.write_modality", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.tfrecord_filename", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.tfrecord_filename", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.tfrecord_filename", "home.repos.pwc.inspect_result.floft_calda.datasets.normalization.calc_normalization_modality", "home.repos.pwc.inspect_result.floft_calda.datasets.normalization.apply_normalization_modality", "home.repos.pwc.inspect_result.floft_calda.datasets.normalization.apply_normalization_modality", "home.repos.pwc.inspect_result.floft_calda.datasets.normalization.apply_normalization_modality"], ["        ", "method_name", "=", "\"none\"", "\n", "sources", "=", "FLAGS", ".", "target", "\n", "target", "=", "\"\"", "\n", "source_modality_subset", "=", "FLAGS", ".", "target_modality_subset", "\n", "target_modality_subset", "=", "\"\"", "\n", "", "else", ":", "\n", "        ", "sources", "=", "FLAGS", ".", "sources", "\n", "target", "=", "FLAGS", ".", "target", "\n", "source_modality_subset", "=", "FLAGS", ".", "source_modality_subset", "\n", "target_modality_subset", "=", "FLAGS", ".", "target_modality_subset", "\n", "\n", "# Remove unused modality since the no adaptation / upper bound will error", "\n", "", "if", "method_name", "==", "\"none\"", ":", "# or it was upper before the above if", "\n", "        ", "if", "source_modality_subset", "!=", "\"\"", ":", "\n", "# Fix \"Weights for model sequential_1 have not yet been created. Weights", "\n", "# are created when the Model is first called on inputs or `build()` is", "\n", "# called with an `input_shape`.\" e.g. when the above yields", "\n", "# source_modality_subset = \"1,0\" and shared_modalities=\"0\" we end up", "\n", "# never using the second modality's FE or DC. Thus, just throw out the", "\n", "# unused modality. For example, here this would end up just setting", "\n", "# source_modality_subset to \"1\".", "\n", "            ", "modality_subset_list", "=", "source_modality_subset", ".", "split", "(", "\",\"", ")", "# \"1\",\"0\"", "\n", "shared_modalities_list", "=", "[", "int", "(", "x", ")", "for", "x", "in", "FLAGS", ".", "shared_modalities", ".", "split", "(", "\",\"", ")", "]", "# 0", "\n", "new_modality_subset", "=", "[", "]", "\n", "\n", "for", "modality", "in", "shared_modalities_list", ":", "\n", "                ", "new_modality_subset", ".", "append", "(", "modality_subset_list", "[", "modality", "]", ")", "\n", "\n", "", "source_modality_subset", "=", "\",\"", ".", "join", "(", "new_modality_subset", ")", "\n", "\n", "# If using a domain generalization method, then split among sources not", "\n", "# sources and target. Same for weak supervision.", "\n", "# TODO keep this up to date with domain generalization method list", "\n", "", "", "domain_generalization", "=", "\"_dg\"", "in", "method_name", "or", "\"caldg\"", "in", "method_name", "\n", "weak_supervision", "=", "\"_ws\"", "in", "method_name", "\n", "override_batch_division", "=", "domain_generalization", "or", "weak_supervision", "\n", "\n", "# Load datasets", "\n", "source_datasets", ",", "target_dataset", "=", "load_datasets", ".", "load_da", "(", "FLAGS", ".", "dataset", ",", "\n", "sources", ",", "target", ",", "\n", "test", "=", "FLAGS", ".", "test", ",", "\n", "source_modality_subset", "=", "source_modality_subset", ",", "\n", "target_modality_subset", "=", "target_modality_subset", ",", "\n", "override_batch_division", "=", "override_batch_division", ")", "\n", "\n", "# Need to know which iteration for learning rate schedule", "\n", "global_step", "=", "tf", ".", "Variable", "(", "0", ",", "name", "=", "\"global_step\"", ",", "trainable", "=", "False", ")", "\n", "\n", "# Load the method, model, etc.", "\n", "method", "=", "methods", ".", "get_method", "(", "method_name", ",", "\n", "source_datasets", "=", "source_datasets", ",", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.main.main": [[173, 200], ["os.path.join", "datasets.datasets.names", "os.path.exists", "os.makedirs", "pool.run_job_pool", "main.save_dataset"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.names", "home.repos.pwc.inspect_result.floft_calda.None.pool.run_job_pool", "home.repos.pwc.inspect_result.floft_calda.datasets.main.save_dataset"], ["model_name", "=", "FLAGS", ".", "model", ",", "\n", "global_step", "=", "global_step", ",", "\n", "total_steps", "=", "FLAGS", ".", "steps", ",", "\n", "ensemble_size", "=", "FLAGS", ".", "ensemble", ",", "\n", "moving_average", "=", "FLAGS", ".", "moving_average", ",", "\n", "shared_modalities", "=", "FLAGS", ".", "shared_modalities", ",", "\n", "share_most_weights", "=", "FLAGS", ".", "share_most_weights", ",", "\n", "dataset_name", "=", "FLAGS", ".", "dataset", ")", "\n", "\n", "# Check that this method is supposed to be trainable. If not, we're done.", "\n", "# (Basically, we just wanted to write the config file for non-trainable", "\n", "# models.)", "\n", "if", "not", "method", ".", "trainable", ":", "\n", "        ", "print", "(", "\"Method not trainable. Exiting now.\"", ")", "\n", "return", "\n", "\n", "# Checkpoints", "\n", "", "checkpoint", "=", "tf", ".", "train", ".", "Checkpoint", "(", "\n", "global_step", "=", "global_step", ",", "**", "method", ".", "checkpoint_variables", ")", "\n", "checkpoint_manager", "=", "CheckpointManager", "(", "checkpoint", ",", "model_dir", ",", "log_dir", ")", "\n", "checkpoint_manager", ".", "restore_latest", "(", ")", "\n", "\n", "# Metrics", "\n", "has_target_domain", "=", "target_dataset", "is", "not", "None", "\n", "metrics", "=", "Metrics", "(", "log_dir", ",", "method", ",", "source_datasets", ",", "target_dataset", ",", "\n", "has_target_domain", ")", "\n", "\n", "# Start training", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.normalization.calc_normalization": [[7, 33], ["x.reshape.reshape", "numpy.mean", "numpy.std", "NotImplementedError", "numpy.min", "numpy.max"], "function", ["None"], ["def", "calc_normalization", "(", "x", ",", "method", ")", ":", "\n", "    ", "\"\"\"\n    Calculate zero mean unit variance normalization statistics\n\n    We calculate separate mean/std or min/max statistics for each\n    feature/channel, the default (-1) for BatchNormalization in TensorFlow and\n    I think makes the most sense. If we set axis=0, then we end up with a\n    separate statistic for each time step and feature, and then we can get odd\n    jumps between time steps. Though, we get shape problems when setting axis=2\n    in numpy, so instead we reshape/transpose.\n    \"\"\"", "\n", "# from (10000,100,1) to (1,100,10000)", "\n", "x", "=", "x", ".", "T", "\n", "# from (1,100,10000) to (1,100*10000)", "\n", "x", "=", "x", ".", "reshape", "(", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ")", ")", "\n", "# then we compute statistics over axis=1, i.e. along 100*10000 and end up", "\n", "# with 1 statistic per channel (in this example only one)", "\n", "\n", "if", "method", "==", "\"meanstd\"", ":", "\n", "        ", "values", "=", "(", "np", ".", "mean", "(", "x", ",", "axis", "=", "1", ")", ",", "np", ".", "std", "(", "x", ",", "axis", "=", "1", ")", ")", "\n", "", "elif", "method", "==", "\"minmax\"", ":", "\n", "        ", "values", "=", "(", "np", ".", "min", "(", "x", ",", "axis", "=", "1", ")", ",", "np", ".", "max", "(", "x", ",", "axis", "=", "1", ")", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"unsupported normalization method\"", ")", "\n", "\n", "", "return", "method", ",", "values", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.normalization.calc_normalization_modality": [[35, 41], ["normalization.calc_normalization"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.normalization.calc_normalization"], ["", "def", "calc_normalization_modality", "(", "xs", ",", "method", ")", ":", "\n", "    ", "\"\"\" Calculate normalization for each modality separately\n    xs = [(example 1 modality 1, example 2 modality 1, ...),\n          (example 1 modality 2, example 2 modality 2, ...)]\n    \"\"\"", "\n", "return", "[", "calc_normalization", "(", "x", ",", "method", ")", "for", "x", "in", "xs", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.normalization.is_numpy": [[43, 47], ["type"], "function", ["None"], ["", "def", "is_numpy", "(", "x", ")", ":", "\n", "# Though, could probably use isinstance(x, np.ndarray) ?", "\n", "# https://stackoverflow.com/a/12570040", "\n", "    ", "return", "type", "(", "x", ")", ".", "__module__", "==", "np", ".", "__name__", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.normalization.to_numpy_if_not": [[49, 55], ["normalization.is_numpy", "numpy.array"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.normalization.is_numpy"], ["", "def", "to_numpy_if_not", "(", "x", ",", "dtype", "=", "np", ".", "float32", ")", ":", "\n", "# Create a numpy array, if not one already", "\n", "    ", "if", "not", "is_numpy", "(", "x", ")", ":", "\n", "        ", "x", "=", "np", ".", "array", "(", "x", ",", "dtype", "=", "dtype", ")", "\n", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.normalization.calc_normalization_jagged": [[57, 108], ["len", "print", "enumerate", "range", "numpy.array", "numpy.array", "NotImplementedError", "len", "str", "numpy.concatenate", "numpy.array", "numpy.array", "numpy.mean", "numpy.std", "numpy.min", "numpy.max"], "function", ["None"], ["", "def", "calc_normalization_jagged", "(", "x", ",", "method", ")", ":", "\n", "    ", "\"\"\" Same as calc_normalization() except works for arrays of varying-length\n    numpy arrays\n\n    x should be: [\n        np.array([example 1 time steps, example 1 features]),\n        np.array([example 2 time steps, example 2 features]),\n        ...\n    ] where the # time steps can differ between examples.\n    \"\"\"", "\n", "assert", "len", "(", "x", ")", ">", "0", ",", "\"x cannot be zero-length\"", "\n", "\n", "# No data, e.g. GPS sometimes has no values in the window, so find a window", "\n", "# that does have data to get shape -- otherwise we can't normalize since", "\n", "# there is no data, so skip by returning (None, None)", "\n", "found", "=", "False", "\n", "for", "example", "in", "x", ":", "\n", "        ", "if", "example", ".", "shape", "!=", "(", "0", ",", ")", ":", "\n", "            ", "assert", "len", "(", "example", ".", "shape", ")", ">", "1", ",", "\"shape should be [time steps, features] but is \"", "+", "str", "(", "example", ".", "shape", ")", "\n", "num_features", "=", "example", ".", "shape", "[", "1", "]", "\n", "found", "=", "True", "\n", "break", "\n", "\n", "", "", "if", "not", "found", ":", "\n", "        ", "print", "(", "\"Warning: no data found, so skipping normalization\"", ")", "\n", "return", "(", "None", ",", "None", ")", "\n", "\n", "", "features", "=", "[", "None", "for", "x", "in", "range", "(", "num_features", ")", "]", "\n", "\n", "for", "example", "in", "x", ":", "\n", "# example = to_numpy_if_not(example)", "\n", "        ", "transpose", "=", "example", ".", "T", "\n", "\n", "for", "i", ",", "feature_values", "in", "enumerate", "(", "transpose", ")", ":", "\n", "            ", "if", "features", "[", "i", "]", "is", "None", ":", "\n", "                ", "features", "[", "i", "]", "=", "feature_values", "\n", "", "else", ":", "\n", "                ", "features", "[", "i", "]", "=", "np", ".", "concatenate", "(", "[", "features", "[", "i", "]", ",", "feature_values", "]", ",", "axis", "=", "0", ")", "\n", "\n", "", "", "", "if", "method", "==", "\"meanstd\"", ":", "\n", "        ", "values", "=", "(", "np", ".", "array", "(", "[", "np", ".", "mean", "(", "x", ")", "for", "x", "in", "features", "]", ",", "dtype", "=", "np", ".", "float32", ")", ",", "\n", "np", ".", "array", "(", "[", "np", ".", "std", "(", "x", ")", "for", "x", "in", "features", "]", ",", "dtype", "=", "np", ".", "float32", ")", ")", "\n", "", "elif", "method", "==", "\"minmax\"", ":", "\n", "        ", "values", "=", "(", "np", ".", "array", "(", "[", "np", ".", "min", "(", "x", ")", "for", "x", "in", "features", "]", ",", "dtype", "=", "np", ".", "float32", ")", ",", "\n", "np", ".", "array", "(", "[", "np", ".", "max", "(", "x", ")", "for", "x", "in", "features", "]", ",", "dtype", "=", "np", ".", "float32", ")", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"unsupported normalization method\"", ")", "\n", "\n", "", "return", "method", ",", "values", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.normalization.calc_normalization_jagged_modality": [[110, 116], ["normalization.calc_normalization_jagged"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.normalization.calc_normalization_jagged"], ["", "def", "calc_normalization_jagged_modality", "(", "xs", ",", "method", ")", ":", "\n", "    ", "\"\"\" Calculate normalization for each modality separately\n    xs = [(example 1 modality 1, example 2 modality 1, ...),\n          (example 1 modality 2, example 2 modality 2, ...)]\n    \"\"\"", "\n", "return", "[", "calc_normalization_jagged", "(", "x", ",", "method", ")", "for", "x", "in", "xs", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.normalization.apply_normalization": [[118, 136], ["len", "numpy.isnan"], "function", ["None"], ["", "def", "apply_normalization", "(", "x", ",", "normalization", ",", "epsilon", "=", "1e-5", ")", ":", "\n", "    ", "\"\"\" Apply zero mean unit variance normalization statistics \"\"\"", "\n", "# Don't do anything if it's zero-length", "\n", "if", "len", "(", "x", ")", "==", "0", ":", "\n", "        ", "return", "x", "\n", "\n", "", "method", ",", "values", "=", "normalization", "\n", "\n", "if", "method", "==", "\"meanstd\"", ":", "\n", "        ", "mean", ",", "std", "=", "values", "\n", "x", "=", "(", "x", "-", "mean", ")", "/", "(", "std", "+", "epsilon", ")", "\n", "", "elif", "method", "==", "\"minmax\"", ":", "\n", "        ", "minx", ",", "maxx", "=", "values", "\n", "x", "=", "(", "x", "-", "minx", ")", "/", "(", "maxx", "-", "minx", "+", "epsilon", ")", "-", "0.5", "\n", "\n", "", "x", "[", "np", ".", "isnan", "(", "x", ")", "]", "=", "0", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.normalization.apply_normalization_modality": [[138, 147], ["len", "len", "normalization.apply_normalization", "range", "len"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.normalization.apply_normalization"], ["", "def", "apply_normalization_modality", "(", "xs", ",", "normalization", ",", "epsilon", "=", "1e-5", ")", ":", "\n", "    ", "\"\"\" Apply normalization for each modality separately\n    xs = [(example 1 modality 1, example 2 modality 1, ...),\n          (example 1 modality 2, example 2 modality 2, ...)]\n    normalization = [norm. for modality 1, norm. for modality 2, ...]\n    \"\"\"", "\n", "assert", "len", "(", "xs", ")", "==", "len", "(", "normalization", ")", "\n", "return", "[", "apply_normalization", "(", "xs", "[", "i", "]", ",", "normalization", "[", "i", "]", ",", "epsilon", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "xs", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.normalization.apply_normalization_jagged": [[149, 162], ["normalized.append", "normalization.apply_normalization"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.normalization.apply_normalization"], ["", "def", "apply_normalization_jagged", "(", "x", ",", "normalization", ",", "epsilon", "=", "1e-5", ")", ":", "\n", "    ", "\"\"\" Same as apply_normalization() except works for arrays of varying-length\n    numpy arrays \"\"\"", "\n", "# Can't normalize if there was no normalization statistics computed", "\n", "if", "normalization", "[", "0", "]", "is", "None", "or", "normalization", "[", "1", "]", "is", "None", ":", "\n", "        ", "return", "x", "\n", "\n", "", "normalized", "=", "[", "]", "\n", "\n", "for", "example", "in", "x", ":", "\n", "        ", "normalized", ".", "append", "(", "apply_normalization", "(", "example", ",", "normalization", ",", "epsilon", ")", ")", "\n", "\n", "", "return", "normalized", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.normalization.apply_normalization_jagged_modality": [[164, 173], ["len", "len", "normalization.apply_normalization_jagged", "range", "len"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.normalization.apply_normalization_jagged"], ["", "def", "apply_normalization_jagged_modality", "(", "xs", ",", "normalization", ",", "epsilon", "=", "1e-5", ")", ":", "\n", "    ", "\"\"\" Apply normalization for each modality separately\n    xs = [(example 1 modality 1, example 2 modality 1, ...),\n          (example 1 modality 2, example 2 modality 2, ...)]\n    normalization = [norm. for modality 1, norm. for modality 2, ...]\n    \"\"\"", "\n", "assert", "len", "(", "xs", ")", "==", "len", "(", "normalization", ")", "\n", "return", "[", "apply_normalization_jagged", "(", "xs", "[", "i", "]", ",", "normalization", "[", "i", "]", ",", "epsilon", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "xs", ")", ")", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.floft_calda.datasets.view_datasets.display": [[36, 77], ["enumerate", "matplotlib.subplots", "fig.suptitle", "range", "datasets.normalization.apply_normalization", "ax.plot", "ax.set_ylabel", "ax.set_ylim", "str", "datasets.normalization.calc_normalization", "numpy.tile", "numpy.arange", "len", "str", "str", "str", "len", "str"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.normalization.apply_normalization", "home.repos.pwc.inspect_result.floft_calda.None.sampling_analysis.plot", "home.repos.pwc.inspect_result.floft_calda.datasets.normalization.calc_normalization"], ["def", "display", "(", "name", ",", "data", ",", "feature_names", ")", ":", "\n", "    ", "for", "modality", ",", "modality_data", "in", "enumerate", "(", "data", ")", ":", "\n", "        ", "modality_feature_names", "=", "feature_names", "[", "modality", "]", "\n", "\n", "# Shape: examples, time steps, features", "\n", "num_examples", ",", "num_samples", ",", "num_features", "=", "modality_data", ".", "shape", "\n", "\n", "fig", ",", "axes", "=", "plt", ".", "subplots", "(", "nrows", "=", "num_features", ",", "ncols", "=", "1", ",", "\n", "sharex", "=", "True", ",", "sharey", "=", "False", ")", "\n", "fig", ".", "suptitle", "(", "name", "+", "\" Modality \"", "+", "str", "(", "modality", ")", ")", "\n", "\n", "# Normalize", "\n", "if", "FLAGS", ".", "normalize", "!=", "\"none\"", ":", "\n", "            ", "modality_data", "=", "apply_normalization", "(", "modality_data", ",", "\n", "calc_normalization", "(", "modality_data", ",", "FLAGS", ".", "normalize", ")", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "num_features", ")", ":", "\n", "            ", "if", "num_features", "==", "1", ":", "\n", "                ", "ax", "=", "axes", "\n", "", "else", ":", "\n", "                ", "ax", "=", "axes", "[", "i", "]", "\n", "\n", "# Set x to 0..num_samples. We care about the data values not the time", "\n", "# scale.", "\n", "", "x_list", "=", "np", ".", "tile", "(", "np", ".", "arange", "(", "0", ",", "num_samples", ")", ",", "\n", "(", "FLAGS", ".", "maxexample", "-", "FLAGS", ".", "minexample", ",", "1", ")", ")", ".", "T", "\n", "values", "=", "modality_data", "[", "FLAGS", ".", "minexample", ":", "FLAGS", ".", "maxexample", ",", ":", ",", "i", "]", ".", "T", "\n", "\n", "if", "modality_feature_names", "is", "not", "None", ":", "\n", "                ", "assert", "len", "(", "modality_feature_names", ")", ">", "i", ",", "\"not enough features, looking for feature \"", "+", "str", "(", "i", ")", "+", "\" (zero indexed), but only \"", "+", "str", "(", "len", "(", "modality_feature_names", ")", ")", "+", "\" features: \"", "+", "str", "(", "modality_feature_names", ")", "\n", "label", "=", "modality_feature_names", "[", "i", "]", "\n", "", "else", ":", "\n", "                ", "label", "=", "\"#\"", "+", "str", "(", "i", ")", "\n", "\n", "", "ax", ".", "plot", "(", "x_list", ",", "values", ")", "\n", "ax", ".", "set_ylabel", "(", "label", ")", "\n", "ax", ".", "set_ylim", "(", "auto", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.view_datasets.main": [[79, 110], ["datasets.datasets.load", "view_datasets.display", "matplotlib.show", "datasets.datasets.load", "view_datasets.display"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.load", "home.repos.pwc.inspect_result.floft_calda.datasets.view_datasets.display", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.load", "home.repos.pwc.inspect_result.floft_calda.datasets.view_datasets.display"], ["", "", "", "def", "main", "(", "argv", ")", ":", "\n", "# Don't bother using the GPU for this", "\n", "    ", "os", ".", "environ", "[", "\"CUDA_VISIBLE_DEVICES\"", "]", "=", "\"\"", "\n", "\n", "# Input data", "\n", "source_dataset", ",", "_", "=", "datasets", ".", "load", "(", "FLAGS", ".", "source", ")", "\n", "\n", "if", "FLAGS", ".", "target", "!=", "\"\"", ":", "\n", "        ", "target_dataset", ",", "_", "=", "datasets", ".", "load", "(", "FLAGS", ".", "target", ")", "\n", "", "else", ":", "\n", "        ", "target_dataset", "=", "None", "\n", "\n", "", "if", "not", "FLAGS", ".", "test", ":", "\n", "        ", "source_data", "=", "source_dataset", ".", "train_data", "\n", "target_data", "=", "target_dataset", ".", "train_data", "if", "target_dataset", "is", "not", "None", "else", "None", "\n", "", "else", ":", "\n", "        ", "source_data", "=", "source_dataset", ".", "test_data", "\n", "target_data", "=", "target_dataset", ".", "test_data", "if", "target_dataset", "is", "not", "None", "else", "None", "\n", "\n", "", "source_feature_names", "=", "source_dataset", ".", "feature_names", "\n", "target_feature_names", "=", "target_dataset", ".", "feature_names", "if", "target_dataset", "is", "not", "None", "else", "None", "\n", "\n", "display", "(", "\"Source\"", ",", "source_data", ",", "source_feature_names", ")", "\n", "\n", "if", "target_dataset", "is", "not", "None", ":", "\n", "        ", "display", "(", "\"Target\"", ",", "target_data", ",", "target_feature_names", ")", "\n", "\n", "", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.__init__": [[62, 66], ["tensorflow.io.TFRecordOptions", "tensorflow.io.TFRecordWriter"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "filename", ")", ":", "\n", "        ", "options", "=", "tf", ".", "io", ".", "TFRecordOptions", "(", "compression_type", "=", "\"GZIP\"", ")", "\n", "self", ".", "writer", "=", "tf", ".", "io", ".", "TFRecordWriter", "(", "filename", ",", "options", "=", "options", ")", "\n", "self", ".", "example_id", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.write": [[67, 86], ["tensorflow.squeeze", "tfrecord.create_tf_example_modality", "tfrecord.MultiModalityTFRecord.writer.write", "len", "str", "str", "create_tf_example_modality.SerializeToString"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.create_tf_example_modality", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.write"], ["", "def", "write", "(", "self", ",", "xs", ",", "epochs", ")", ":", "\n", "# To match the tfrecord format of the non-multi-modal datasets, split", "\n", "# the last xs (the response) \"modality\" into a separate y. We also", "\n", "# throw out the epochs at the moment since we don't use them.", "\n", "        ", "y", "=", "xs", "[", "-", "1", "]", "\n", "xs", "=", "xs", "[", ":", "-", "1", "]", "\n", "\n", "# If we don't \"squeeze\" y, then we end up with (n,1) when loading y", "\n", "# from the tfrecords. We instead want (n,). Though, each individual", "\n", "# y changes from being shape (1,) to ().", "\n", "assert", "len", "(", "y", ".", "shape", ")", "==", "1", ",", "\"y.shape not of length 1 but \"", "+", "str", "(", "y", ".", "shape", ")", "\n", "assert", "y", ".", "shape", "[", "0", "]", "==", "1", ",", "\"expecting y to be shape (1,) for each examples but is \"", "+", "str", "(", "y", ".", "shape", ")", "\n", "y", "=", "tf", ".", "squeeze", "(", "y", ")", "\n", "\n", "tf_example", "=", "create_tf_example_modality", "(", "xs", ",", "y", ",", "self", ".", "example_id", ")", "\n", "self", ".", "writer", ".", "write", "(", "tf_example", ".", "SerializeToString", "(", ")", ")", "\n", "self", ".", "example_id", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.close": [[87, 90], ["tfrecord.MultiModalityTFRecord.writer.close"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "# Normally the tf.io.TFRecordWriter is used in a with block", "\n", "        ", "self", ".", "writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord._bytes_feature": [[7, 12], ["isinstance", "tensorflow.train.Feature", "type", "value.numpy.numpy", "tensorflow.constant", "tensorflow.train.BytesList"], "function", ["None"], ["def", "_bytes_feature", "(", "value", ")", ":", "\n", "    ", "\"\"\" Returns a bytes_list from a string / byte. \"\"\"", "\n", "if", "isinstance", "(", "value", ",", "type", "(", "tf", ".", "constant", "(", "0", ")", ")", ")", ":", "\n", "        ", "value", "=", "value", ".", "numpy", "(", ")", "# BytesList won't unpack a string from an EagerTensor.", "\n", "", "return", "tf", ".", "train", ".", "Feature", "(", "bytes_list", "=", "tf", ".", "train", ".", "BytesList", "(", "value", "=", "[", "value", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.create_tf_example": [[14, 21], ["tensorflow.train.Example", "tensorflow.train.Features", "tfrecord._bytes_feature", "tfrecord._bytes_feature", "tfrecord._bytes_feature", "tensorflow.io.serialize_tensor", "tensorflow.io.serialize_tensor", "tensorflow.io.serialize_tensor"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord._bytes_feature", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord._bytes_feature", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord._bytes_feature"], ["", "def", "create_tf_example", "(", "x", ",", "y", ",", "example_id", ")", ":", "\n", "    ", "tf_example", "=", "tf", ".", "train", ".", "Example", "(", "features", "=", "tf", ".", "train", ".", "Features", "(", "feature", "=", "{", "\n", "'x'", ":", "_bytes_feature", "(", "tf", ".", "io", ".", "serialize_tensor", "(", "x", ")", ")", ",", "\n", "'y'", ":", "_bytes_feature", "(", "tf", ".", "io", ".", "serialize_tensor", "(", "y", ")", ")", ",", "\n", "'id'", ":", "_bytes_feature", "(", "tf", ".", "io", ".", "serialize_tensor", "(", "example_id", ")", ")", ",", "\n", "}", ")", ")", "\n", "return", "tf_example", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.create_tf_example_modality": [[23, 56], ["len", "range", "enumerate", "tfrecord.create_tf_example_modality._feature"], "function", ["None"], ["", "def", "create_tf_example_modality", "(", "xs", ",", "y", ",", "example_id", ")", ":", "\n", "    ", "\"\"\"\n    Create TF example based on the number of modalities\n\n    For example, if self.num_modalities = 2, we get features:\n        x_0, x_1, y\n    \"\"\"", "\n", "# Xs", "\n", "num_modalities", "=", "len", "(", "xs", ")", "\n", "modalities", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "num_modalities", ")", ":", "\n", "        ", "modalities", ".", "append", "(", "\"x_\"", "+", "str", "(", "i", ")", ")", "\n", "\n", "", "def", "_feature", "(", "x", ")", ":", "\n", "        ", "return", "_bytes_feature", "(", "tf", ".", "io", ".", "serialize_tensor", "(", "x", ")", ")", "\n", "\n", "", "feature", "=", "{", "}", "\n", "\n", "for", "i", ",", "modality", "in", "enumerate", "(", "modalities", ")", ":", "\n", "        ", "feature", "[", "modality", "]", "=", "_feature", "(", "xs", "[", "i", "]", ")", "\n", "\n", "# y", "\n", "", "feature", "[", "\"y\"", "]", "=", "_feature", "(", "y", ")", "\n", "\n", "# example id", "\n", "feature", "[", "\"id\"", "]", "=", "_feature", "(", "example_id", ")", "\n", "\n", "# example containing both xs and y", "\n", "tf_example", "=", "tf", ".", "train", ".", "Example", "(", "features", "=", "tf", ".", "train", ".", "Features", "(", "\n", "feature", "=", "feature", ")", ")", "\n", "\n", "return", "tf_example", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.write_tfrecord": [[92, 101], ["tensorflow.io.TFRecordOptions", "len", "len", "tensorflow.io.TFRecordWriter", "range", "len", "tfrecord.create_tf_example", "writer.write", "create_tf_example.SerializeToString"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.create_tf_example", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.write"], ["", "", "def", "write_tfrecord", "(", "filename", ",", "x", ",", "y", ")", ":", "\n", "    ", "\"\"\" Output to TF record file \"\"\"", "\n", "assert", "len", "(", "x", ")", "==", "len", "(", "y", ")", "\n", "options", "=", "tf", ".", "io", ".", "TFRecordOptions", "(", "compression_type", "=", "\"GZIP\"", ")", "\n", "\n", "with", "tf", ".", "io", ".", "TFRecordWriter", "(", "filename", ",", "options", "=", "options", ")", "as", "writer", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "x", ")", ")", ":", "\n", "            ", "tf_example", "=", "create_tf_example", "(", "x", "[", "i", "]", ",", "y", "[", "i", "]", ",", "i", ")", "\n", "writer", ".", "write", "(", "tf_example", ".", "SerializeToString", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.write_tfrecord_modality": [[103, 126], ["len", "tensorflow.io.TFRecordOptions", "tensorflow.io.TFRecordWriter", "range", "len", "len", "len", "tfrecord.create_tf_example_modality", "writer.write", "create_tf_example_modality.SerializeToString", "range"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.create_tf_example_modality", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.write"], ["", "", "", "def", "write_tfrecord_modality", "(", "filename", ",", "xs", ",", "y", ")", ":", "\n", "    ", "\"\"\" Output to TF record file, multi-modality version\n\n    xs = [(example 1 modality 1, example 2 modality 1, ...),\n          (example 1 modality 2, example 2 modality 2, ...)]\n\n    Note: write_tfrecord_modality_by_example() has a different format for x.\n    This version is more convenient to create, but basically converts to the\n    format that write_tfrecord_modality_by_example() uses since that is the\n    format needed to write it to disk.\n    \"\"\"", "\n", "num_modalities", "=", "len", "(", "xs", ")", "\n", "for", "x", "in", "xs", ":", "\n", "        ", "assert", "len", "(", "x", ")", "==", "len", "(", "y", ")", ",", "\"each modality must have same length as y\"", "\n", "", "options", "=", "tf", ".", "io", ".", "TFRecordOptions", "(", "compression_type", "=", "\"GZIP\"", ")", "\n", "\n", "with", "tf", ".", "io", ".", "TFRecordWriter", "(", "filename", ",", "options", "=", "options", ")", "as", "writer", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "y", ")", ")", ":", "\n", "# Get the x for each of this example's modality along with the", "\n", "# corresponding y value", "\n", "            ", "tf_example", "=", "create_tf_example_modality", "(", "\n", "[", "xs", "[", "m", "]", "[", "i", "]", "for", "m", "in", "range", "(", "num_modalities", ")", "]", ",", "y", "[", "i", "]", ",", "i", ")", "\n", "writer", ".", "write", "(", "tf_example", ".", "SerializeToString", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.write_tfrecord_modality_by_example": [[128, 141], ["tensorflow.io.TFRecordOptions", "len", "len", "tensorflow.io.TFRecordWriter", "range", "len", "tfrecord.create_tf_example_modality", "writer.write", "create_tf_example_modality.SerializeToString"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.create_tf_example_modality", "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.write"], ["", "", "", "def", "write_tfrecord_modality_by_example", "(", "filename", ",", "x", ",", "y", ")", ":", "\n", "    ", "\"\"\" Output to TF record file -- exactly the same as write_tfrecord except\n    this calls the multi-modality version of create_tf_example\n\n    x = [(example 1 modality 1, example 1 modality 2, ...), ...]\n    \"\"\"", "\n", "assert", "len", "(", "x", ")", "==", "len", "(", "y", ")", "\n", "options", "=", "tf", ".", "io", ".", "TFRecordOptions", "(", "compression_type", "=", "\"GZIP\"", ")", "\n", "\n", "with", "tf", ".", "io", ".", "TFRecordWriter", "(", "filename", ",", "options", "=", "options", ")", "as", "writer", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "x", ")", ")", ":", "\n", "            ", "tf_example", "=", "create_tf_example_modality", "(", "x", "[", "i", "]", ",", "y", "[", "i", "]", ",", "i", ")", "\n", "writer", ".", "write", "(", "tf_example", ".", "SerializeToString", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.tfrecord_filename": [[143, 146], ["None"], "function", ["None"], ["", "", "", "def", "tfrecord_filename", "(", "dataset_name", ",", "postfix", ")", ":", "\n", "    ", "\"\"\" Filename for tfrecord files, e.g. ucihar_1_train.tfrecord \"\"\"", "\n", "return", "\"%s_%s.tfrecord\"", "%", "(", "dataset_name", ",", "postfix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.tfrecord_as_images_folder": [[148, 151], ["None"], "function", ["None"], ["", "def", "tfrecord_as_images_folder", "(", "dataset_name", ",", "postfix", ")", ":", "\n", "    ", "\"\"\" Filename for tfrecord files, e.g. ucihar_1_train/ \"\"\"", "\n", "return", "\"%s_%s\"", "%", "(", "dataset_name", ",", "postfix", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.confidence_ellipse": [[28, 113], ["numpy.sqrt", "numpy.sqrt", "matplotlib.patches.Ellipse", "matplotlib.Affine2D().rotate_deg().scale().translate", "matplotlib.patches.Ellipse.set_transform", "ax.add_patch", "numpy.cov", "numpy.sqrt", "numpy.sqrt", "numpy.mean", "numpy.sqrt", "numpy.mean", "ValueError", "ValueError", "matplotlib.Affine2D().rotate_deg().scale", "ValueError", "len", "len", "matplotlib.Affine2D().rotate_deg", "matplotlib.Affine2D"], "function", ["None"], ["def", "confidence_ellipse", "(", "ax", ",", "mean", "=", "None", ",", "cov", "=", "None", ",", "x", "=", "None", ",", "y", "=", "None", ",", "n_std", "=", "2.0", ",", "\n", "facecolor", "=", "'none'", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    Create a plot of the covariance confidence ellipse of *x* and *y*. Or,\n    optionally from mean/cov instead (if known/available) of estimated from\n    samples drawn from these distributions.\n\n    Based on:\n\n    https://matplotlib.org/stable/gallery/statistics/confidence_ellipse.html\n    https://carstenschelp.github.io/2018/09/14/Plot_Confidence_Ellipse_001.html\n\n    Parameters\n    ----------\n    x, y : array-like, shape (n, )\n        Input data.\n\n    ax : matplotlib.axes.Axes\n        The axes object to draw the ellipse into.\n\n    n_std : float\n        The number of standard deviations to determine the ellipse's radiuses.\n\n    **kwargs\n        Forwarded to `~matplotlib.patches.Ellipse`\n\n    Returns\n    -------\n    matplotlib.patches.Ellipse\n    \"\"\"", "\n", "# Sanity checks", "\n", "if", "x", "is", "not", "None", "and", "y", "is", "not", "None", ":", "\n", "        ", "based_on_xy", "=", "True", "\n", "\n", "if", "x", ".", "size", "!=", "y", ".", "size", ":", "\n", "            ", "raise", "ValueError", "(", "\"x and y must be the same size\"", ")", "\n", "", "", "elif", "mean", "is", "not", "None", "and", "cov", "is", "not", "None", ":", "\n", "        ", "based_on_xy", "=", "False", "\n", "\n", "if", "len", "(", "cov", ")", "!=", "2", "or", "len", "(", "mean", ")", "!=", "2", ":", "\n", "            ", "raise", "ValueError", "(", "\"mean/cov must be 2D\"", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Pass either mean/cov or x/y\"", ")", "\n", "\n", "# Estimate covariance from data, if not given", "\n", "", "if", "based_on_xy", ":", "\n", "        ", "cov", "=", "np", ".", "cov", "(", "x", ",", "y", ")", "\n", "\n", "# Calculate needed parameter", "\n", "", "pearson", "=", "cov", "[", "0", ",", "1", "]", "/", "np", ".", "sqrt", "(", "cov", "[", "0", ",", "0", "]", "*", "cov", "[", "1", ",", "1", "]", ")", "\n", "\n", "# Using a special case to obtain the eigenvalues of this", "\n", "# two-dimensional dataset.", "\n", "ell_radius_x", "=", "np", ".", "sqrt", "(", "1", "+", "pearson", ")", "\n", "ell_radius_y", "=", "np", ".", "sqrt", "(", "1", "-", "pearson", ")", "\n", "ellipse", "=", "Ellipse", "(", "(", "0", ",", "0", ")", ",", "width", "=", "ell_radius_x", "*", "2", ",", "height", "=", "ell_radius_y", "*", "2", ",", "\n", "facecolor", "=", "facecolor", ",", "**", "kwargs", ")", "\n", "\n", "# Calculating the standard deviation of x from the squareroot of the", "\n", "# variance and multiplying with the given number of standard deviations.", "\n", "scale_x", "=", "np", ".", "sqrt", "(", "cov", "[", "0", ",", "0", "]", ")", "*", "n_std", "\n", "\n", "# Calculate mean from the data if not given", "\n", "if", "based_on_xy", ":", "\n", "        ", "mean_x", "=", "np", ".", "mean", "(", "x", ")", "\n", "", "else", ":", "\n", "        ", "mean_x", "=", "mean", "[", "0", "]", "\n", "\n", "# Same for y", "\n", "", "scale_y", "=", "np", ".", "sqrt", "(", "cov", "[", "1", ",", "1", "]", ")", "*", "n_std", "\n", "\n", "if", "based_on_xy", ":", "\n", "        ", "mean_y", "=", "np", ".", "mean", "(", "y", ")", "\n", "", "else", ":", "\n", "        ", "mean_y", "=", "mean", "[", "1", "]", "\n", "\n", "# Compute the ellipse", "\n", "", "transf", "=", "transforms", ".", "Affine2D", "(", ")", ".", "rotate_deg", "(", "45", ")", ".", "scale", "(", "scale_x", ",", "scale_y", ")", ".", "translate", "(", "mean_x", ",", "mean_y", ")", "\n", "\n", "ellipse", ".", "set_transform", "(", "transf", "+", "ax", ".", "transData", ")", "\n", "\n", "return", "ax", ".", "add_patch", "(", "ellipse", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.get_translate_rotate": [[115, 135], ["numpy.random.uniform", "numpy.random.uniform", "numpy.array", "numpy.cos", "numpy.sin", "numpy.cos", "numpy.sin"], "function", ["None"], ["", "def", "get_translate_rotate", "(", "translate", ",", "rotate", ",", "dimensions", ",", "rotation_per_class", ")", ":", "\n", "    ", "\"\"\" Generate how much / where to translate and rotate and the corresponding\n    covariance matrix for the rotated multivariate normal distributions \"\"\"", "\n", "translate_amount", "=", "translate", "/", "2", "# half since +/-", "\n", "translation", "=", "np", ".", "random", ".", "uniform", "(", "\n", "-", "translate_amount", ",", "translate_amount", ",", "dimensions", ")", "\n", "\n", "rotate_amount", "=", "rotate", "/", "2", "*", "rotation_per_class", "\n", "rotation", "=", "np", ".", "random", ".", "uniform", "(", "-", "rotate_amount", ",", "rotate_amount", ")", "\n", "\n", "# No rotation of the Gaussian", "\n", "# rotation_conv = np.diag(np.ones((dimensions,)))", "\n", "# Simple rotation matrix to rotate the Gaussian distribution (if not", "\n", "# isotropic)", "\n", "rotation_conv", "=", "np", ".", "array", "(", "[", "\n", "[", "np", ".", "cos", "(", "rotation", ")", ",", "-", "np", ".", "sin", "(", "rotation", ")", "]", ",", "\n", "[", "np", ".", "sin", "(", "rotation", ")", ",", "np", ".", "cos", "(", "rotation", ")", "]", "\n", "]", ")", "\n", "\n", "return", "translation", ",", "rotation", ",", "rotation_conv", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.get_near_psd": [[137, 147], ["numpy.linalg.eig", "eigvec.dot().dot", "eigvec.dot", "numpy.diag"], "function", ["None"], ["", "def", "get_near_psd", "(", "A", ")", ":", "\n", "    ", "\"\"\"\n    Gets rid of \"covariance is not positive-semidefinite\" warning\n    From: https://stackoverflow.com/a/63131309\n    \"\"\"", "\n", "C", "=", "(", "A", "+", "A", ".", "T", ")", "/", "2", "\n", "eigval", ",", "eigvec", "=", "np", ".", "linalg", ".", "eig", "(", "C", ")", "\n", "eigval", "[", "eigval", "<", "0", "]", "=", "0", "\n", "\n", "return", "eigvec", ".", "dot", "(", "np", ".", "diag", "(", "eigval", ")", ")", ".", "dot", "(", "eigvec", ".", "T", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.make_domain": [[149, 209], ["synthetic_datasets_normal.get_translate_rotate", "range", "numpy.diag", "synthetic_datasets_normal.get_translate_rotate", "numpy.array", "results.append", "numpy.ones", "numpy.cos", "numpy.sin"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.get_translate_rotate", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.get_translate_rotate"], ["", "def", "make_domain", "(", "num_classes", ",", "center", ",", "radius", ",", "dimensions", ",", "\n", "inter_domain_translate", "=", "0", ",", "inter_domain_rotate", "=", "0", ",", "\n", "intra_domain_translate", "=", "0", ",", "intra_domain_rotate", "=", "0", ",", "\n", "initial_cov", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Make a domain, i.e. generate a list of mean/cov for each class\n\n    Returns:\n        [(class1 mean, class1 cov), (class2 mean, class2 cov), ...]\n    \"\"\"", "\n", "results", "=", "[", "]", "\n", "\n", "# For now we only support 2D. There's some ways to do this in 3D, but", "\n", "# it looks like higher dimensions get a bit more complicated. For now", "\n", "# we'll start with 2D only.", "\n", "assert", "dimensions", "==", "2", ",", "\">2D not yet implemented\"", "\n", "\n", "# Evenly distribute classes around a circle", "\n", "rotation_per_class", "=", "2", "*", "np", ".", "pi", "/", "num_classes", "\n", "\n", "# Calculate inter-domain shift", "\n", "inter_translate", ",", "inter_rotate", ",", "inter_rotate_conv", "=", "get_translate_rotate", "(", "\n", "inter_domain_translate", ",", "inter_domain_rotate", ",", "dimensions", ",", "rotation_per_class", ")", "\n", "\n", "# Start with a particular \"shaped\" Gaussian - for now, isotropic", "\n", "if", "initial_cov", "is", "None", ":", "\n", "        ", "initial_cov", "=", "np", ".", "diag", "(", "np", ".", "ones", "(", "(", "dimensions", ",", ")", ")", ")", "\n", "\n", "# Create distribution for each class", "\n", "", "for", "i", "in", "range", "(", "num_classes", ")", ":", "\n", "# Intra-domain shift, i.e. shift each class differently", "\n", "        ", "intra_translate", ",", "intra_rotate", ",", "intra_rotate_conv", "=", "get_translate_rotate", "(", "\n", "intra_domain_translate", ",", "intra_domain_rotate", ",", "dimensions", ",", "rotation_per_class", ")", "\n", "\n", "# Adjust mean of distributions around the circle", "\n", "theta", "=", "i", "*", "rotation_per_class", "+", "inter_rotate", "+", "intra_rotate", "\n", "mean", "=", "np", ".", "array", "(", "[", "\n", "center", "[", "0", "]", "+", "radius", "*", "np", ".", "cos", "(", "theta", ")", ",", "\n", "center", "[", "1", "]", "+", "radius", "*", "np", ".", "sin", "(", "theta", ")", ",", "\n", "]", ")", "\n", "\n", "# Initial covariance before rotation", "\n", "cov", "=", "initial_cov", "\n", "\n", "# Inter-domain shift, i.e. shift all classes the same", "\n", "mean", "+=", "inter_translate", "+", "intra_translate", "\n", "\n", "# It's isotropic for now, so we don't need to rotate.", "\n", "# cov = inter_rotate_conv.dot(cov)", "\n", "# cov = intra_rotate_conv.dot(cov)", "\n", "\n", "# Get close positive-semidefinite to this matrix - needed because of", "\n", "# the rotation matrices", "\n", "# print(\"Before:\", cov)", "\n", "# print(\"After:\", get_near_psd(cov))", "\n", "# cov = get_near_psd(cov)", "\n", "\n", "results", ".", "append", "(", "(", "mean", ",", "cov", ")", ")", "\n", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.get_distribution_data": [[211, 227], ["enumerate", "scipy.stats.multivariate_normal", "dists.append", "data.append", "labels.append", "stats.multivariate_normal.rvs"], "function", ["None"], ["", "def", "get_distribution_data", "(", "domain", ",", "num_points", ")", ":", "\n", "    ", "\"\"\" Create the multivariate normal distribution and draw a number of samples\n    from the distribution \"\"\"", "\n", "dists", "=", "[", "]", "\n", "data", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "\n", "for", "i", ",", "(", "mean", ",", "cov", ")", "in", "enumerate", "(", "domain", ")", ":", "\n", "        ", "dist", "=", "stats", ".", "multivariate_normal", "(", "mean", ",", "cov", ")", "\n", "dists", ".", "append", "(", "dist", ")", "\n", "\n", "if", "num_points", ">", "0", ":", "\n", "            ", "data", ".", "append", "(", "dist", ".", "rvs", "(", "num_points", ")", ")", "\n", "labels", ".", "append", "(", "[", "i", "]", "*", "num_points", ")", "\n", "\n", "", "", "return", "dists", ",", "data", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.plot_domains": [[229, 337], ["enumerate", "ax.legend", "matplotlib.subplots", "ax.axvline", "ax.axhline", "numpy.array", "numpy.mean", "synthetic_datasets_normal.get_distribution_data", "enumerate", "ax.set_title", "matplotlib.tight_layout", "numpy.vstack", "datasets.normalization.calc_normalization", "numpy.mean", "numpy.array", "numpy.array", "ax.annotate", "normalized_data.append", "numpy.mean", "ax.scatter", "synthetic_datasets_normal.confidence_ellipse", "synthetic_datasets_normal.confidence_ellipse", "ax.plot", "datasets.normalization.apply_normalization", "len", "len", "len", "str"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.get_distribution_data", "home.repos.pwc.inspect_result.floft_calda.datasets.normalization.calc_normalization", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.confidence_ellipse", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.confidence_ellipse", "home.repos.pwc.inspect_result.floft_calda.None.sampling_analysis.plot", "home.repos.pwc.inspect_result.floft_calda.datasets.normalization.apply_normalization"], ["", "def", "plot_domains", "(", "domains", ",", "num_points", ",", "draw_lines", ",", "from_data", ",", "normalized", ",", "\n", "fig", "=", "None", ",", "ax", "=", "None", ",", "title", "=", "None", ")", ":", "\n", "    ", "\"\"\" Visualize a set of multivariate normal distributions, one for each\n    class of each domain. Optionally estimate from a sample of points drawn\n    from these distributions rather from the mean/covariance directly, and\n    optionally from the normalized samples instead. Optionally draw lines\n    from each domain's class distributions to the center/mean of that domain\n    for added clarity. \"\"\"", "\n", "if", "fig", "is", "None", "and", "ax", "is", "None", ":", "\n", "        ", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "figsize", "=", "(", "5", ",", "5", ")", ")", "\n", "", "if", "not", "FLAGS", ".", "plots_for_paper", ":", "\n", "        ", "ax", ".", "axvline", "(", "c", "=", "'grey'", ",", "lw", "=", "1", ")", "\n", "ax", ".", "axhline", "(", "c", "=", "'grey'", ",", "lw", "=", "1", ")", "\n", "\n", "# https://matplotlib.org/stable/tutorials/colors/colors.html", "\n", "", "colors", "=", "[", "'C0'", ",", "'C1'", ",", "'C2'", ",", "'C3'", ",", "'C4'", ",", "'C5'", ",", "'C6'", ",", "'C7'", ",", "'C8'", ",", "'C9'", "]", "\n", "\n", "# For each domain", "\n", "for", "i", ",", "domain", "in", "enumerate", "(", "domains", ")", ":", "\n", "        ", "means", "=", "np", ".", "array", "(", "[", "mean", "for", "mean", ",", "cov", "in", "domain", "]", ")", "\n", "domain_mean", "=", "np", ".", "mean", "(", "means", ",", "axis", "=", "0", ")", "\n", "\n", "# Create distributions and draw samples (if desired) from these", "\n", "# distributions", "\n", "_", ",", "data", ",", "_", "=", "get_distribution_data", "(", "domain", ",", "num_points", ")", "\n", "\n", "# If desired, normalize the points", "\n", "if", "normalized", ":", "\n", "# Compute stats over entire data, not over just one class", "\n", "            ", "all_data", "=", "np", ".", "vstack", "(", "data", ")", "\n", "norm", "=", "calc_normalization", "(", "all_data", ",", "\"meanstd\"", ")", "\n", "\n", "# Then, normalize each class's data", "\n", "normalized_data", "=", "[", "]", "\n", "\n", "for", "d", "in", "data", ":", "\n", "                ", "normalized_data", ".", "append", "(", "apply_normalization", "(", "d", ",", "norm", ")", ")", "\n", "\n", "", "data", "=", "normalized_data", "\n", "\n", "# Calculate means from data if desired", "\n", "", "if", "from_data", ":", "\n", "            ", "means", "=", "[", "np", ".", "mean", "(", "d", ",", "axis", "=", "0", ")", "for", "d", "in", "data", "]", "\n", "domain_mean", "=", "np", ".", "mean", "(", "means", ",", "axis", "=", "0", ")", "\n", "\n", "# For each class in that domain", "\n", "", "for", "j", ",", "(", "mean", ",", "cov", ")", "in", "enumerate", "(", "domain", ")", ":", "\n", "# Get class color", "\n", "            ", "color", "=", "colors", "[", "j", "%", "len", "(", "colors", ")", "]", "\n", "\n", "# Label for each class", "\n", "if", "i", "==", "0", ":", "\n", "                ", "label", "=", "\"Class {}\"", ".", "format", "(", "j", ")", "\n", "", "else", ":", "\n", "                ", "label", "=", "None", "\n", "\n", "# Plot some data from the distribution", "\n", "", "if", "num_points", ">", "0", ":", "\n", "                ", "x1", "=", "data", "[", "j", "]", "[", ":", ",", "0", "]", "\n", "x2", "=", "data", "[", "j", "]", "[", ":", ",", "1", "]", "\n", "ax", ".", "scatter", "(", "x1", ",", "x2", ",", "s", "=", "1.0", ",", "alpha", "=", "0.5", ",", "c", "=", "color", ")", "\n", "\n", "# Make sure they're numpy arrays", "\n", "", "mean", "=", "np", ".", "array", "(", "mean", ")", "\n", "cov", "=", "np", ".", "array", "(", "cov", ")", "\n", "\n", "# Plot the ellipse for the multivariate normal distribution", "\n", "if", "from_data", ":", "\n", "                ", "assert", "num_points", ">", "0", ",", "\"if from_data, set num_points > 0\"", "\n", "confidence_ellipse", "(", "ax", ",", "x", "=", "x1", ",", "y", "=", "x2", ",", "\n", "edgecolor", "=", "color", ",", "alpha", "=", "0.5", ",", "facecolor", "=", "color", ",", "\n", "label", "=", "label", ")", "\n", "\n", "# Also overwrite mean, so we use the means from the data", "\n", "mean", "=", "means", "[", "j", "]", "\n", "", "else", ":", "\n", "                ", "confidence_ellipse", "(", "ax", ",", "mean", "=", "mean", ",", "cov", "=", "cov", ",", "\n", "edgecolor", "=", "color", ",", "alpha", "=", "0.5", ",", "facecolor", "=", "color", ",", "\n", "label", "=", "label", ")", "\n", "\n", "# Annotate each center by which source/target it's from, note", "\n", "# target is at the end", "\n", "", "txt", "=", "\"$T$\"", "if", "i", "==", "len", "(", "domains", ")", "-", "1", "else", "\"$S_{\"", "+", "str", "(", "i", ")", "+", "\"}$\"", "\n", "ax", ".", "annotate", "(", "txt", ",", "(", "mean", "[", "0", "]", ",", "mean", "[", "1", "]", ")", ")", "\n", "\n", "if", "draw_lines", ":", "\n", "# Line from each class center to the domain mean - makes it easier", "\n", "# to see how domains are translated/rotated", "\n", "                ", "ax", ".", "plot", "(", "[", "domain_mean", "[", "0", "]", ",", "mean", "[", "0", "]", "]", ",", "[", "domain_mean", "[", "1", "]", ",", "mean", "[", "1", "]", "]", ",", "\n", "'r-'", ",", "alpha", "=", "0.5", ")", "\n", "\n", "", "", "", "if", "not", "FLAGS", ".", "plots_for_paper", ":", "\n", "        ", "if", "title", "is", "None", ":", "\n", "            ", "title", "=", "\"MS-DA Distributions, n={}\"", ".", "format", "(", "len", "(", "domains", ")", "-", "1", ")", "\n", "\n", "", "if", "from_data", "and", "normalized", ":", "\n", "            ", "title", "+=", "\", from normalized data\"", "\n", "", "elif", "from_data", ":", "\n", "            ", "title", "+=", "\", from data\"", "\n", "", "elif", "normalized", ":", "\n", "            ", "title", "+=", "\", normalized\"", "\n", "\n", "", "ax", ".", "set_title", "(", "title", ")", "\n", "\n", "", "ax", ".", "legend", "(", ")", "\n", "\n", "if", "fig", "is", "None", "and", "ax", "is", "None", ":", "\n", "        ", "plt", ".", "tight_layout", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.generate_msda_problems": [[339, 367], ["random.seed", "numpy.random.seed", "synthetic_datasets_normal.make_domain", "range", "synthetic_datasets_normal.plot_domains", "sources.append", "synthetic_datasets_normal.make_domain"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.make_domain", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.plot_domains", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.make_domain"], ["", "", "def", "generate_msda_problems", "(", "num_sources", ",", "num_classes", ",", "\n", "center", "=", "(", "0", ",", "0", ")", ",", "radius", "=", "5", ",", "inter_domain_translate", "=", "10", ",", "inter_domain_rotate", "=", "1", ",", "\n", "intra_domain_translate", "=", "5", ",", "intra_domain_rotate", "=", "0.5", ",", "\n", "dimensions", "=", "2", ",", "num_points", "=", "0", ",", "seed", "=", "42", ",", "draw_lines", "=", "False", ",", "\n", "from_data", "=", "False", ",", "normalize", "=", "False", ",", "fig", "=", "None", ",", "ax", "=", "None", ",", "title", "=", "None", ")", ":", "\n", "    ", "\"\"\" Generate a single MS-DA problem, for a given value of n, L, radius,\n    amount of translation/rotation for inter/intra-domain shifts, etc. Plot\n    the result. \"\"\"", "\n", "# Make repeatable", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "\n", "# Create domains", "\n", "target", "=", "make_domain", "(", "num_classes", ",", "center", ",", "radius", ",", "dimensions", ")", "\n", "sources", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "num_sources", ")", ":", "\n", "        ", "sources", ".", "append", "(", "make_domain", "(", "num_classes", ",", "center", ",", "radius", ",", "dimensions", ",", "\n", "inter_domain_translate", ",", "inter_domain_rotate", ",", "\n", "intra_domain_translate", ",", "intra_domain_rotate", ")", ")", "\n", "\n", "# Plot domains", "\n", "", "assert", "dimensions", "==", "2", ",", "\"Can only plot domains for 2D data at the moment\"", "\n", "# Note: we depend on the target being last for labeling", "\n", "plot_domains", "(", "sources", "+", "[", "target", "]", ",", "num_points", ",", "draw_lines", ",", "from_data", ",", "\n", "normalize", ",", "fig", ",", "ax", ",", "title", ")", "\n", "\n", "return", "sources", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.save_plot": [[369, 376], ["matplotlib.savefig", "matplotlib.close", "os.path.exists", "os.makedirs", "os.path.join"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.close"], ["", "def", "save_plot", "(", "name", ",", "out", "=", "\"normal_plots\"", ",", "extension", "=", "\"png\"", ")", ":", "\n", "    ", "\"\"\" Rather than displaying the plot, save it to a file \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "out", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "out", ")", "\n", "\n", "", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "out", ",", "name", "+", "\".\"", "+", "extension", ")", ",", "bbox_inches", "=", "'tight'", ")", "\n", "plt", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.all_problems": [[378, 513], ["len", "len", "matplotlib.subplots", "fig.suptitle", "fig.tight_layout", "matplotlib.subplots_adjust", "synthetic_datasets_normal.save_plot", "synthetic_datasets_normal.generate_msda_problems", "len", "synthetic_datasets_normal.generate_msda_problems", "synthetic_datasets_normal.save_plot", "synthetic_datasets_normal.save_data", "synthetic_datasets_normal.save_data"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.save_plot", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.generate_msda_problems", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.generate_msda_problems", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.save_plot", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.save_data", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.save_data"], ["", "def", "all_problems", "(", "num_sources", ",", "num_classes", ",", "draw_lines", "=", "False", ",", "center", "=", "(", "25", ",", "25", ")", ",", "\n", "inter_scaling", "=", "[", "1", "]", ",", "intra_scaling", "=", "[", "1", "]", ",", "raw", "=", "False", ",", "\n", "base_params", "=", "None", ",", "display", "=", "False", ",", "suffix", "=", "None", ")", ":", "\n", "    ", "\"\"\" Generate all the MS-DA problems for a given value of n and L, varying\n    inter-domain and/or intra-domain scaling, putting all of these results into\n    one plot. Both the theoretical distribution (exact from mean/covariance) and\n    the distribution estimated from 1k normalized points are displayed, to make\n    clear what may happen when we normalize the data. \"\"\"", "\n", "subplot_index", "=", "0", "\n", "rows", "=", "len", "(", "inter_scaling", ")", "*", "len", "(", "intra_scaling", ")", "\n", "cols", "=", "2", "\n", "# num_plots = rows * cols", "\n", "\n", "if", "FLAGS", ".", "plots_for_paper", ":", "\n", "        ", "subplots", "=", "False", "\n", "plot_ext", "=", "\"pdf\"", "\n", "", "else", ":", "\n", "        ", "subplots", "=", "True", "\n", "plot_ext", "=", "\"png\"", "\n", "\n", "", "if", "subplots", ":", "\n", "        ", "fig", ",", "axs", "=", "plt", ".", "subplots", "(", "rows", ",", "cols", ",", "figsize", "=", "(", "10", ",", "5", "*", "rows", ")", ")", "\n", "fig", ".", "suptitle", "(", "\"MS-DA Distributions, n={}\"", ".", "format", "(", "num_sources", ")", ")", "\n", "\n", "# For some reason Matplotlib doesn't include the extra dimension if", "\n", "# it's only 1 row. To make the code below work either way, add dim back.", "\n", "if", "rows", "==", "1", ":", "\n", "            ", "axs", "=", "[", "axs", "]", "\n", "\n", "# Save space", "\n", "", "fig", ".", "tight_layout", "(", ")", "\n", "\n", "# top - make title above plots", "\n", "# wspace - between columns", "\n", "# hspace - between rows", "\n", "plt", ".", "subplots_adjust", "(", "top", "=", "0.93", ",", "wspace", "=", "0.05", "*", "cols", ",", "hspace", "=", "0.07", "*", "rows", ")", "\n", "\n", "# Which are we varying?", "\n", "", "if", "inter_scaling", "==", "[", "1", "]", "and", "intra_scaling", "==", "[", "1", "]", ":", "\n", "        ", "variation", "=", "\"none\"", "\n", "", "elif", "inter_scaling", "==", "[", "1", "]", ":", "\n", "        ", "variation", "=", "\"intra\"", "\n", "", "elif", "intra_scaling", "==", "[", "1", "]", ":", "\n", "        ", "variation", "=", "\"inter\"", "\n", "", "else", ":", "\n", "        ", "variation", "=", "\"both\"", "\n", "\n", "# Output suffix", "\n", "", "if", "suffix", "is", "None", ":", "\n", "        ", "suffix", "=", "\"\"", "\n", "", "else", ":", "\n", "        ", "suffix", "=", "\"_\"", "+", "suffix", "\n", "\n", "# Base inter/intra-domain shifts", "\n", "", "if", "base_params", "is", "None", ":", "\n", "        ", "base_params", "=", "[", "10", ",", "1", ",", "5", ",", "0.5", "]", "\n", "\n", "", "for", "inter_scale", "in", "inter_scaling", ":", "\n", "        ", "for", "intra_scale", "in", "intra_scaling", ":", "\n", "            ", "assert", "len", "(", "base_params", ")", "==", "4", ",", "\"wrong number of base_params\"", "\n", "inter_domain_translate", "=", "base_params", "[", "0", "]", "*", "inter_scale", "\n", "inter_domain_rotate", "=", "base_params", "[", "1", "]", "*", "inter_scale", "\n", "intra_domain_translate", "=", "base_params", "[", "2", "]", "*", "intra_scale", "\n", "intra_domain_rotate", "=", "base_params", "[", "3", "]", "*", "intra_scale", "\n", "\n", "# Get title", "\n", "if", "variation", "==", "\"none\"", ":", "\n", "                ", "title", "=", "\"\"", "\n", "", "elif", "variation", "==", "\"intra\"", ":", "\n", "                ", "title", "=", "\"intra-scale {}\"", ".", "format", "(", "intra_scale", ")", "\n", "", "elif", "variation", "==", "\"inter\"", ":", "\n", "                ", "title", "=", "\"inter-scale {}\"", ".", "format", "(", "inter_scale", ")", "\n", "", "else", ":", "\n", "                ", "title", "=", "\"inter-scale {}, intra-scale {}\"", ".", "format", "(", "inter_scale", ",", "intra_scale", ")", "\n", "\n", "", "name", "=", "\"normal_n{}_l{}_inter{}_intra{}{}\"", ".", "format", "(", "\n", "num_sources", ",", "num_classes", ",", "inter_scale", ",", "intra_scale", ",", "\n", "suffix", ")", "\n", "\n", "# The left column is the un-normalized", "\n", "if", "subplots", ":", "\n", "                ", "ax", "=", "axs", "[", "subplot_index", "//", "cols", "]", "[", "subplot_index", "%", "cols", "]", "\n", "subplot_index", "+=", "1", "\n", "", "else", ":", "\n", "                ", "fig", "=", "None", "\n", "ax", "=", "None", "\n", "\n", "", "generate_msda_problems", "(", "num_sources", ",", "num_classes", ",", "\n", "draw_lines", "=", "draw_lines", ",", "num_points", "=", "100", ",", "center", "=", "center", ",", "\n", "inter_domain_translate", "=", "inter_domain_translate", ",", "\n", "inter_domain_rotate", "=", "inter_domain_rotate", ",", "\n", "intra_domain_translate", "=", "intra_domain_translate", ",", "\n", "intra_domain_rotate", "=", "intra_domain_rotate", ",", "\n", "fig", "=", "fig", ",", "ax", "=", "ax", ",", "title", "=", "title", ")", "\n", "\n", "# Check that from data looks the same", "\n", "# generate_msda_problems(num_sources, num_classes,", "\n", "#     draw_lines=draw_lines, num_points=1000, from_data=True)", "\n", "\n", "# The right column is the from_data/normalized", "\n", "if", "subplots", ":", "\n", "                ", "ax", "=", "axs", "[", "subplot_index", "//", "cols", "]", "[", "subplot_index", "%", "cols", "]", "\n", "subplot_index", "+=", "1", "\n", "", "else", ":", "\n", "                ", "fig", "=", "None", "\n", "ax", "=", "None", "\n", "\n", "# We only want the normalized plots when we generate subfigures", "\n", "", "if", "subplots", ":", "\n", "# Also, check results if we normalize - note only applies when we use this", "\n", "# data directly rather than generating sines of two frequencies (x1 and x2).", "\n", "                ", "sources", ",", "target", "=", "generate_msda_problems", "(", "num_sources", ",", "num_classes", ",", "\n", "draw_lines", "=", "draw_lines", ",", "num_points", "=", "1000", ",", "from_data", "=", "True", ",", "normalize", "=", "True", ",", "\n", "center", "=", "center", ",", "\n", "inter_domain_translate", "=", "inter_domain_translate", ",", "\n", "inter_domain_rotate", "=", "inter_domain_rotate", ",", "\n", "intra_domain_translate", "=", "intra_domain_translate", ",", "\n", "intra_domain_rotate", "=", "intra_domain_rotate", ",", "\n", "fig", "=", "fig", ",", "ax", "=", "ax", ",", "title", "=", "title", ")", "\n", "\n", "# Generate actual data", "\n", "if", "not", "FLAGS", ".", "plots_for_paper", ":", "\n", "                    ", "if", "raw", ":", "\n", "                        ", "save_data", "(", "sources", ",", "target", ",", "num_classes", ",", "name", ",", "raw", "=", "True", ",", "\n", "display", "=", "display", ")", "\n", "", "else", ":", "\n", "                        ", "save_data", "(", "sources", ",", "target", ",", "num_classes", ",", "name", "+", "\"_sine\"", ",", "\n", "display", "=", "display", ")", "\n", "", "", "", "else", ":", "\n", "                ", "save_plot", "(", "name", ",", "extension", "=", "plot_ext", ")", "\n", "\n", "# Save plot", "\n", "", "", "", "if", "subplots", ":", "\n", "        ", "save_plot", "(", "\"normal_n{}_l{}_{}{}\"", ".", "format", "(", "num_sources", ",", "num_classes", ",", "\n", "variation", ",", "suffix", ")", ",", "extension", "=", "plot_ext", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.sanity_check_freqs": [[515, 523], ["enumerate", "enumerate", "ValueError"], "function", ["None"], ["", "", "def", "sanity_check_freqs", "(", "freqs", ",", "sample_freq", ")", ":", "\n", "    ", "\"\"\" Check that no frequencies are negative or over Nyquist sampling rate \"\"\"", "\n", "for", "i", ",", "freq", "in", "enumerate", "(", "freqs", ")", ":", "\n", "        ", "for", "j", ",", "f", "in", "enumerate", "(", "freq", ")", ":", "\n", "            ", "if", "f", "<", "0", "or", "f", ">", "sample_freq", "/", "2", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"Found frequency {}, which is either <0 or >{}\"", ".", "format", "(", "\n", "f", ",", "sample_freq", "/", "2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.generate_sine": [[525, 555], ["synthetic_datasets_normal.sanity_check_freqs", "datasets.synthetic_datasets.sine", "numpy.random.normal", "len"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.sanity_check_freqs", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.sine"], ["", "", "", "", "def", "generate_sine", "(", "data", ")", ":", "\n", "    ", "\"\"\" Generate a bunch of univariate time series signals (length matches that\n    of data) with two sine waves of particular frequencies each\n\n    Input format: [(ex1 freq1, ex1 freq2), (ex2 freq1, ex2 freq2), ...]\n    Outputs: x, y\n        - x is the time dimension, i.e. from 0 to duration (in seconds)\n        - y is the amplitude at each point in time, i.e. the signal to be used\n          for classification, having shape [time_steps, num_examples] where\n          time_steps = duration * sample_freq\n    \"\"\"", "\n", "# Config", "\n", "duration", "=", "0.2", "# seconds", "\n", "sample_freq", "=", "250", "# Hz", "\n", "amp_noise", "=", "0.0", "# slight amplitude noise", "\n", "freq_noise", "=", "0.0", "# slight frequency noise", "\n", "phase_shift", "=", "0.0", "# random phase shift", "\n", "\n", "# Generate time series data", "\n", "freqs", "=", "data", "# [num examples, 2 frequencies]", "\n", "sanity_check_freqs", "(", "freqs", ",", "sample_freq", ")", "\n", "amps", "=", "[", "1.0", ",", "1.0", "]", "\n", "x", ",", "y", "=", "sine", "(", "f", "=", "freqs", ",", "amps", "=", "amps", ",", "maxt", "=", "duration", ",", "\n", "length", "=", "duration", "*", "sample_freq", ",", "\n", "freq_noise", "=", "freq_noise", ",", "phase_shift", "=", "phase_shift", ")", "\n", "\n", "if", "amp_noise", "is", "not", "None", ":", "\n", "        ", "y", "+=", "np", ".", "random", ".", "normal", "(", "0.0", ",", "amp_noise", ",", "(", "y", ".", "shape", "[", "0", "]", ",", "len", "(", "data", ")", ")", ")", "\n", "\n", "", "return", "x", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.save_single_domain_data": [[557, 593], ["synthetic_datasets_normal.get_distribution_data", "numpy.vstack", "numpy.hstack", "datasets.synthetic_datasets.save_data_file", "synthetic_datasets_normal.generate_sine", "numpy.expand_dims", "range", "matplotlib.show", "numpy.array", "len", "synthetic_datasets_normal.generate_sine", "datasets.synthetic_datasets.display_xy", "print", "matplotlib.title"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.get_distribution_data", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.save_data_file", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.generate_sine", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.generate_sine", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.display_xy"], ["", "def", "save_single_domain_data", "(", "domain", ",", "num_points", ",", "filename", ",", "raw", ",", "display", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Generate either raw or time series data for each one domain and save that\n    data (and optionally display a sample)\n\n    If raw=True, then write the raw data (2D points) rather than the time series\n    signals. Num points indicates how many examples to generate.\n    \"\"\"", "\n", "_", ",", "data_by_class", ",", "labels_by_class", "=", "get_distribution_data", "(", "domain", ",", "num_points", ")", "\n", "data", "=", "np", ".", "vstack", "(", "data_by_class", ")", "\n", "labels", "=", "np", ".", "hstack", "(", "labels_by_class", ")", "\n", "\n", "# Convert to sine waves", "\n", "if", "not", "raw", ":", "\n", "# Debugging", "\n", "        ", "if", "display", ":", "\n", "            ", "num", "=", "10", "# how many from each class to display", "\n", "for", "i", "in", "range", "(", "len", "(", "data_by_class", ")", ")", ":", "\n", "                ", "x", ",", "y", "=", "generate_sine", "(", "data_by_class", "[", "i", "]", ")", "\n", "display_xy", "(", "x", "[", ":", ",", ":", "num", "]", ",", "y", "[", ":", ",", ":", "num", "]", ",", "show", "=", "False", ")", "\n", "title", "=", "\"Class {}\"", ".", "format", "(", "i", ")", "\n", "print", "(", "title", ",", "data_by_class", "[", "i", "]", "[", ":", "num", "]", ")", "\n", "plt", ".", "title", "(", "title", ")", "\n", "", "plt", ".", "show", "(", ")", "\n", "\n", "# Generate data for all classes combined", "\n", "", "x", ",", "y", "=", "generate_sine", "(", "data", ")", "\n", "\n", "# Transpose so we have [examples, time_steps]", "\n", "data", "=", "np", ".", "array", "(", "y", ",", "dtype", "=", "np", ".", "float32", ")", ".", "T", "\n", "\n", "# Expand so we have 1 feature: [examples, time_steps, num_features]", "\n", "data", "=", "np", ".", "expand_dims", "(", "data", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# Write to file", "\n", "", "save_data_file", "(", "data", ",", "labels", ",", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.save_data": [[595, 616], ["enumerate", "synthetic_datasets_normal.save_single_domain_data", "synthetic_datasets_normal.save_single_domain_data", "os.path.exists", "os.makedirs", "synthetic_datasets_normal.save_single_domain_data", "synthetic_datasets_normal.save_single_domain_data"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.save_single_domain_data", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.save_single_domain_data", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.save_single_domain_data", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.save_single_domain_data"], ["", "def", "save_data", "(", "sources", ",", "target", ",", "num_classes", ",", "name", ",", "\n", "num_train_points", "=", "10000", ",", "num_test_points", "=", "1000", ",", "\n", "out", "=", "\"datasets/synthetic\"", ",", "raw", "=", "False", ",", "display", "=", "False", ")", ":", "\n", "    ", "\"\"\" Save data for each source domain and the target domain \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "out", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "out", ")", "\n", "\n", "# We want this many points total, but they're generated per-class", "\n", "", "num_train_points", "=", "num_train_points", "//", "num_classes", "\n", "num_test_points", "=", "num_test_points", "//", "num_classes", "\n", "\n", "for", "i", ",", "source", "in", "enumerate", "(", "sources", ")", ":", "\n", "        ", "save_single_domain_data", "(", "source", ",", "num_train_points", ",", "\n", "'{}/{}_s{}_TRAIN'", ".", "format", "(", "out", ",", "name", ",", "i", ")", ",", "raw", ",", "display", ")", "\n", "save_single_domain_data", "(", "source", ",", "num_test_points", ",", "\n", "'{}/{}_s{}_TEST'", ".", "format", "(", "out", ",", "name", ",", "i", ")", ",", "raw", ",", "display", ")", "\n", "\n", "", "save_single_domain_data", "(", "target", ",", "num_train_points", ",", "\n", "'{}/{}_t_TRAIN'", ".", "format", "(", "out", ",", "name", ")", ",", "raw", ",", "display", ")", "\n", "save_single_domain_data", "(", "target", ",", "num_test_points", ",", "\n", "'{}/{}_t_TEST'", ".", "format", "(", "out", ",", "name", ")", ",", "raw", ",", "display", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.main": [[618, 647], ["synthetic_datasets_normal.all_problems", "synthetic_datasets_normal.all_problems", "synthetic_datasets_normal.all_problems", "synthetic_datasets_normal.all_problems", "synthetic_datasets_normal.all_problems", "synthetic_datasets_normal.all_problems", "synthetic_datasets_normal.all_problems", "synthetic_datasets_normal.all_problems"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.all_problems", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.all_problems", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.all_problems", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.all_problems", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.all_problems", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.all_problems", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.all_problems", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets_normal.all_problems"], ["", "def", "main", "(", "argv", ")", ":", "\n", "# Test inter/intra translate/rotate each separately", "\n", "# n=4, L=3", "\n", "    ", "all_problems", "(", "num_sources", "=", "4", ",", "num_classes", "=", "3", ",", "draw_lines", "=", "True", ",", "\n", "base_params", "=", "[", "5", ",", "0", ",", "0", ",", "0", "]", ",", "inter_scaling", "=", "[", "0", ",", "1", ",", "2", "]", ",", "\n", "suffix", "=", "\"5,0,0,0\"", ")", "\n", "all_problems", "(", "num_sources", "=", "4", ",", "num_classes", "=", "3", ",", "draw_lines", "=", "True", ",", "\n", "base_params", "=", "[", "0", ",", "0.5", ",", "0", ",", "0", "]", ",", "inter_scaling", "=", "[", "0", ",", "1", ",", "2", "]", ",", "\n", "suffix", "=", "\"0,0.5,0,0\"", ")", "\n", "all_problems", "(", "num_sources", "=", "4", ",", "num_classes", "=", "3", ",", "draw_lines", "=", "True", ",", "\n", "base_params", "=", "[", "0", ",", "0", ",", "5", ",", "0", "]", ",", "intra_scaling", "=", "[", "0", ",", "1", ",", "2", "]", ",", "\n", "suffix", "=", "\"0,0,5,0\"", ")", "\n", "all_problems", "(", "num_sources", "=", "4", ",", "num_classes", "=", "3", ",", "draw_lines", "=", "True", ",", "\n", "base_params", "=", "[", "0", ",", "0", ",", "0", ",", "0.5", "]", ",", "intra_scaling", "=", "[", "0", ",", "1", ",", "2", "]", ",", "\n", "suffix", "=", "\"0,0,0,0.5\"", ")", "\n", "\n", "# n=12, L=3 -- so we have n=2, 4, 6, 8, 10 and 10 has 3 diff. sets available", "\n", "all_problems", "(", "num_sources", "=", "12", ",", "num_classes", "=", "3", ",", "draw_lines", "=", "True", ",", "\n", "base_params", "=", "[", "5", ",", "0", ",", "0", ",", "0", "]", ",", "inter_scaling", "=", "[", "0", ",", "1", ",", "2", "]", ",", "\n", "suffix", "=", "\"5,0,0,0\"", ")", "\n", "all_problems", "(", "num_sources", "=", "12", ",", "num_classes", "=", "3", ",", "draw_lines", "=", "True", ",", "\n", "base_params", "=", "[", "0", ",", "0.5", ",", "0", ",", "0", "]", ",", "inter_scaling", "=", "[", "0", ",", "1", ",", "2", "]", ",", "\n", "suffix", "=", "\"0,0.5,0,0\"", ")", "\n", "all_problems", "(", "num_sources", "=", "12", ",", "num_classes", "=", "3", ",", "draw_lines", "=", "True", ",", "\n", "base_params", "=", "[", "0", ",", "0", ",", "5", ",", "0", "]", ",", "intra_scaling", "=", "[", "0", ",", "1", ",", "2", "]", ",", "\n", "suffix", "=", "\"0,0,5,0\"", ")", "\n", "all_problems", "(", "num_sources", "=", "12", ",", "num_classes", "=", "3", ",", "draw_lines", "=", "True", ",", "\n", "base_params", "=", "[", "0", ",", "0", ",", "0", ",", "0.5", "]", ",", "intra_scaling", "=", "[", "0", ",", "1", ",", "2", "]", ",", "\n", "suffix", "=", "\"0,0,0,0.5\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.linear": [[21, 25], ["numpy.arange().reshape().astype", "numpy.arange().reshape", "numpy.arange"], "function", ["None"], ["def", "linear", "(", "m", ",", "b", ",", "length", "=", "100", ",", "minvalue", "=", "0", ",", "maxvalue", "=", "2", ")", ":", "\n", "    ", "x", "=", "np", ".", "arange", "(", "minvalue", ",", "maxvalue", ",", "(", "maxvalue", "-", "minvalue", ")", "/", "length", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "y", "=", "m", "*", "x", "+", "b", "\n", "return", "x", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.display_xy": [[27, 34], ["matplotlib.figure", "range", "matplotlib.plot", "matplotlib.show"], "function", ["home.repos.pwc.inspect_result.floft_calda.None.sampling_analysis.plot"], ["", "def", "display_xy", "(", "x", ",", "y", ",", "show", "=", "True", ")", ":", "\n", "    ", "plt", ".", "figure", "(", ")", "\n", "for", "i", "in", "range", "(", "y", ".", "shape", "[", "1", "]", ")", ":", "\n", "        ", "plt", ".", "plot", "(", "x", ",", "y", "[", ":", ",", "i", "]", ")", "\n", "\n", "", "if", "show", ":", "\n", "        ", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.to_pandas": [[36, 43], ["pandas.DataFrame", "pd.DataFrame.insert", "pandas.Series", "numpy.squeeze().astype", "numpy.squeeze"], "function", ["None"], ["", "", "def", "to_pandas", "(", "y", ",", "labels", ")", ":", "\n", "    ", "\"\"\"\n    Note: y is y-axis but actually the data, i.e. \"x\" in (x,y) ML terminology\n    \"\"\"", "\n", "df", "=", "pd", ".", "DataFrame", "(", "y", ".", "T", ")", "\n", "df", ".", "insert", "(", "0", ",", "'class'", ",", "pd", ".", "Series", "(", "np", ".", "squeeze", "(", "labels", ")", ".", "astype", "(", "np", ".", "int32", ")", "+", "1", ",", "index", "=", "df", ".", "index", ")", ")", "\n", "return", "df", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.generate_positive_slope_data": [[45, 61], ["numpy.random.uniform", "numpy.random.uniform", "synthetic_datasets.linear", "numpy.random.normal", "synthetic_datasets.display_xy"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.linear", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.display_xy"], ["", "def", "generate_positive_slope_data", "(", "n", ",", "display", "=", "False", ",", "add_noise", "=", "False", ",", "\n", "bmin", "=", "0.0", ",", "bmax", "=", "5.0", ",", "mmin", "=", "-", "1.0", ",", "mmax", "=", "1.0", ")", ":", "\n", "    ", "\"\"\" Positive or negative slope lines \"\"\"", "\n", "m", "=", "np", ".", "random", ".", "uniform", "(", "mmin", ",", "mmax", ",", "(", "1", ",", "n", ")", ")", "\n", "b", "=", "np", ".", "random", ".", "uniform", "(", "bmin", ",", "bmax", ",", "(", "1", ",", "n", ")", ")", "\n", "x", ",", "y", "=", "linear", "(", "m", ",", "b", ")", "\n", "labels", "=", "m", ">", "0", "\n", "\n", "if", "add_noise", ":", "\n", "        ", "noise", "=", "np", ".", "random", ".", "normal", "(", "0.0", ",", "0.25", ",", "(", "y", ".", "shape", "[", "0", "]", ",", "n", ")", ")", "\n", "y", "+=", "noise", "\n", "\n", "", "if", "display", ":", "\n", "        ", "display_xy", "(", "x", ",", "y", ")", "\n", "\n", "", "return", "y", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.sine": [[63, 146], ["numpy.arange().reshape", "isinstance", "isinstance", "numpy.array", "numpy.array", "numpy.array", "numpy.tile", "numpy.expand_dims", "numpy.random.normal", "numpy.sin", "numpy.array", "numpy.array", "numpy.random.normal", "numpy.arange", "isinstance", "numpy.sum", "isinstance", "len", "NotImplementedError"], "function", ["None"], ["", "def", "sine", "(", "m", "=", "1.0", ",", "b", "=", "0.0", ",", "f", "=", "None", ",", "amps", "=", "None", ",", "freq_noise", "=", "1.0", ",", "phase_shift", "=", "5.0", ",", "\n", "length", "=", "100", ",", "mint", "=", "0", ",", "maxt", "=", "2", ")", ":", "\n", "    ", "\"\"\"\n    Generate a single or multiple sine waves (multiple if f is list)\n\n    m - scale horizontally\n    b - offset vertically\n    f - either one frequency, a list of frequencies for each example (see below)\n    amps - if specified, the amplitude of each specified frequency\n    freq_noise - if not None, frequencies noisy about what is given in f\n    phase_shift - how much to randomly offset in time (per frequency, so\n        even with freq_noise=None, the two samples of f=[[1,2], [1,2]] will\n        look different)\n    length - number of samples to generate between mint and maxt\n    mint - starting time\n    maxt - stopping time\n\n    100-length samples, one of 1 Hz and one of 2 Hz:\n        sine(f=[[1], [2]], maxt=1, length=100, freq_noise=None, phase_shift=None)\n\n    One 100-length sample with 1 and 2 Hz frequency components:\n        sine(f=[[1,2]], maxt=1, length=100, freq_noise=None, phase_shift=None)\n\n    Same frequency but different phase:\n        sine(f=[[1]]*5, maxt=1, length=100, freq_noise=None, phase_shift=1.0)\n    \"\"\"", "\n", "multi_freq", "=", "isinstance", "(", "f", ",", "list", ")", "or", "isinstance", "(", "f", ",", "np", ".", "ndarray", ")", "\n", "\n", "# Set frequency if desired", "\n", "if", "f", "is", "None", ":", "\n", "        ", "s", "=", "np", ".", "array", "(", "1.0", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "amps", "=", "np", ".", "array", "(", "1.0", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "", "else", ":", "\n", "        ", "if", "amps", "is", "not", "None", ":", "\n", "            ", "amps", "=", "np", ".", "array", "(", "amps", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "", "else", ":", "\n", "            ", "amps", "=", "np", ".", "array", "(", "[", "1.0", "]", "*", "len", "(", "f", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "", "f", "=", "np", ".", "array", "(", "f", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "if", "freq_noise", "is", "not", "None", ":", "\n", "            ", "f", "+=", "np", ".", "random", ".", "normal", "(", "0.0", ",", "freq_noise", ",", "f", ".", "shape", ")", "\n", "\n", "", "s", "=", "2.0", "*", "np", ".", "pi", "*", "f", "\n", "\n", "", "x_orig", "=", "np", ".", "arange", "(", "mint", ",", "maxt", ",", "(", "maxt", "-", "mint", ")", "/", "length", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "x", "=", "x_orig", "\n", "\n", "if", "multi_freq", ":", "\n", "        ", "x_tile", "=", "np", ".", "tile", "(", "x", ",", "f", ".", "shape", "[", "0", "]", ")", "# e.g. (100,1) to (100,3) if 100 time steps, 3 frequencies", "\n", "x_newdim", "=", "np", ".", "expand_dims", "(", "x_tile", ",", "2", ")", "# now (100,3,1) so broadcasting works below", "\n", "x", "=", "x_newdim", "\n", "\n", "", "if", "phase_shift", "is", "None", ":", "\n", "        ", "phase_shift", "=", "0", "\n", "", "else", ":", "\n", "        ", "if", "f", "is", "None", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "np", ".", "ndarray", ")", ":", "\n", "                ", "shape", "=", "m", ".", "shape", "\n", "", "elif", "isinstance", "(", "b", ",", "np", ".", "ndarray", ")", ":", "\n", "                ", "shape", "=", "b", ".", "shape", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "(", "\"When using phase shift one of m, b, \"", "\n", "\"or f must be np.ndarray\"", ")", "\n", "", "", "else", ":", "\n", "# shape = (num_freq, num_examples)", "\n", "            ", "shape", "=", "f", ".", "shape", "\n", "\n", "", "phase_shift", "=", "np", ".", "random", ".", "normal", "(", "0.0", ",", "phase_shift", ",", "shape", ")", "\n", "\n", "", "y", "=", "m", "*", "amps", "*", "np", ".", "sin", "(", "s", "*", "(", "x", "+", "phase_shift", ")", ")", "\n", "\n", "# Sum the extra dimension for multiple frequencies, e.g. from (100,3,2)", "\n", "# back to (100,3) if 2 frequencies each", "\n", "#", "\n", "# Note: make sure we don't add b in y above before summing, since otherwise", "\n", "# it'll be shifted much more than b", "\n", "if", "multi_freq", ":", "\n", "        ", "y", "=", "np", ".", "sum", "(", "y", ",", "axis", "=", "2", ")", "+", "b", "\n", "", "else", ":", "\n", "        ", "y", "+=", "b", "\n", "\n", "", "return", "x_orig", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.generate_positive_sine_data": [[148, 164], ["numpy.random.uniform", "numpy.random.uniform", "synthetic_datasets.sine", "synthetic_datasets.to_pandas", "numpy.random.normal", "synthetic_datasets.display_xy"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.sine", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.to_pandas", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.display_xy"], ["", "def", "generate_positive_sine_data", "(", "n", ",", "display", "=", "False", ",", "add_noise", "=", "False", ",", "\n", "bmin", "=", "0.0", ",", "bmax", "=", "5.0", ",", "mmin", "=", "-", "1.0", ",", "mmax", "=", "1.0", ",", "f", "=", "None", ")", ":", "\n", "    ", "\"\"\" Sine wave multiplied by positive or negative number and offset some \"\"\"", "\n", "m", "=", "np", ".", "random", ".", "uniform", "(", "mmin", ",", "mmax", ",", "(", "1", ",", "n", ")", ")", "\n", "b", "=", "np", ".", "random", ".", "uniform", "(", "bmin", ",", "bmax", ",", "(", "1", ",", "n", ")", ")", "\n", "x", ",", "y", "=", "sine", "(", "m", "=", "m", ",", "b", "=", "b", ",", "f", "=", "f", ",", "freq_noise", "=", "None", ",", "phase_shift", "=", "None", ")", "\n", "labels", "=", "m", ">", "0", "\n", "\n", "if", "add_noise", ":", "\n", "        ", "noise", "=", "np", ".", "random", ".", "normal", "(", "0.0", ",", "0.1", ",", "(", "y", ".", "shape", "[", "0", "]", ",", "n", ")", ")", "\n", "y", "+=", "noise", "\n", "\n", "", "if", "display", ":", "\n", "        ", "display_xy", "(", "x", ",", "y", ")", "\n", "\n", "", "return", "to_pandas", "(", "y", ",", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.generate_freq": [[166, 186], ["numpy.random.uniform", "synthetic_datasets.sine", "numpy.random.normal", "numpy.random.normal", "synthetic_datasets.display_xy"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.sine", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.display_xy"], ["", "def", "generate_freq", "(", "n", ",", "display", "=", "False", ",", "amp_noise", "=", "False", ",", "freq_noise", "=", "False", ",", "\n", "fmin", "=", "1.0", ",", "fmax", "=", "2.0", ")", ":", "\n", "    ", "\"\"\" Sine wave multiplied by positive or negative number and offset some\n    Warning: probably recall Nyquist when setting fmax\n    \"\"\"", "\n", "freq", "=", "np", ".", "random", ".", "uniform", "(", "fmin", ",", "fmax", ",", "(", "n", ",", "1", ")", ")", "\n", "\n", "if", "freq_noise", ":", "\n", "        ", "freq", "+=", "np", ".", "random", ".", "normal", "(", "0.0", ",", "1.0", ",", "(", "n", ",", "1", ")", ")", "# on order of freq diffs", "\n", "\n", "", "x", ",", "y", "=", "sine", "(", "f", "=", "freq", ",", "maxt", "=", "2", ",", "length", "=", "2", "*", "50", ")", "\n", "labels", "=", "freq", ">", "(", "fmax", "-", "fmin", ")", "/", "2", "\n", "\n", "if", "amp_noise", ":", "\n", "        ", "y", "+=", "np", ".", "random", ".", "normal", "(", "0.0", ",", "0.1", ",", "(", "y", ".", "shape", "[", "0", "]", ",", "n", ")", ")", "\n", "\n", "", "if", "display", ":", "\n", "        ", "display_xy", "(", "x", ",", "y", ")", "\n", "\n", "", "return", "y", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.generate_multi_freq": [[188, 293], ["numpy.random.randint", "range", "len", "numpy.array", "numpy.array", "isinstance", "isinstance", "numpy.array", "numpy.array", "synthetic_datasets.sine", "data.append", "numpy.array", "len", "len", "len", "len", "len", "len", "len", "len", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "len", "len", "numpy.pad", "numpy.pad", "np.array.append", "np.array.append", "numpy.random.normal", "synthetic_datasets.display_xy", "len", "len", "len", "len", "len", "len", "len", "len", "numpy.pad", "numpy.pad", "numpy.isnan", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.sine", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.display_xy"], ["", "def", "generate_multi_freq", "(", "n", ",", "pos_f_all", ",", "neg_f_all", ",", "\n", "pos_amp_all", "=", "None", ",", "neg_amp_all", "=", "None", ",", "\n", "display", "=", "False", ",", "\n", "amp_noise", "=", "0.1", ",", "freq_noise", "=", "1.0", ",", "phase_shift", "=", "5.0", ",", "\n", "sample_freq", "=", "50", ",", "duration", "=", "2", ",", "b", "=", "0.0", ")", ":", "\n", "    ", "\"\"\"\n    Generate data with different sets of frequencies for +/- classes\n\n    Optionally specify different amplitudes for each of the frequencies. If not,\n    then they all have the same amplitude (possibly with amplitude noise if\n    amp_noise is not None).\n\n    Note: {pos,neg}_{f,amp} is a 2D list to split frequencies/amplitudes across\n    multiple channels. If you only want one channel, then do\n        generate_multi_freq(x, [run_f], [walk_f], [run_amp], [walk_amp], ...)\n    \"\"\"", "\n", "assert", "pos_amp_all", "is", "None", "or", "len", "(", "pos_amp_all", ")", "==", "len", "(", "pos_f_all", ")", ",", "\"pos_amp_all must be same length as pos_f_all\"", "\n", "assert", "neg_amp_all", "is", "None", "or", "len", "(", "neg_amp_all", ")", "==", "len", "(", "neg_f_all", ")", ",", "\"neg_amp_all must be same length as neg_f_all\"", "\n", "\n", "# Generate the labels, ~1/2 from + and 1/2 from - classes", "\n", "labels", "=", "np", ".", "random", ".", "randint", "(", "2", ",", "size", "=", "n", ")", "\n", "\n", "# Multi-dimensional data", "\n", "data", "=", "[", "]", "\n", "\n", "# For each channel", "\n", "for", "channel", "in", "range", "(", "len", "(", "pos_f_all", ")", ")", ":", "\n", "        ", "pos_f", "=", "np", ".", "array", "(", "pos_f_all", "[", "channel", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "neg_f", "=", "np", ".", "array", "(", "neg_f_all", "[", "channel", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "assert", "isinstance", "(", "freq_noise", ",", "list", ")", ",", "\"freq_noise should be list [noise ch1, noise ch2, ...]\"", "\n", "assert", "isinstance", "(", "amp_noise", ",", "list", ")", ",", "\"amp_noise should be list [noise ch1, noise ch2, ...]\"", "\n", "freq_noise_channel", "=", "freq_noise", "[", "channel", "]", "\n", "amp_noise_channel", "=", "amp_noise", "[", "channel", "]", "\n", "\n", "# Check that we don't accidentally expect multiple output channels for", "\n", "# instance but only create one", "\n", "assert", "len", "(", "freq_noise", ")", "==", "len", "(", "pos_f_all", ")", ",", "\"more noise channels than frequency channels\"", "\n", "assert", "len", "(", "amp_noise", ")", "==", "len", "(", "pos_f_all", ")", ",", "\"more noise channels than frequency channels\"", "\n", "\n", "if", "pos_amp_all", "is", "not", "None", ":", "\n", "            ", "pos_amp", "=", "np", ".", "array", "(", "pos_amp_all", "[", "channel", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "", "else", ":", "\n", "            ", "pos_amp", "=", "np", ".", "array", "(", "[", "1.0", "]", "*", "len", "(", "pos_f_all", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "", "if", "neg_amp_all", "is", "not", "None", ":", "\n", "            ", "neg_amp", "=", "np", ".", "array", "(", "neg_amp_all", "[", "channel", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "", "else", ":", "\n", "            ", "neg_amp", "=", "np", ".", "array", "(", "[", "1.0", "]", "*", "len", "(", "neg_f_all", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "", "assert", "pos_amp", "is", "None", "or", "len", "(", "pos_amp", ")", "==", "len", "(", "pos_f", ")", ",", "\"pos_amp must be same length as pos_f\"", "\n", "assert", "neg_amp", "is", "None", "or", "len", "(", "neg_amp", ")", "==", "len", "(", "neg_f", ")", ",", "\"neg_amp must be same length as neg_f\"", "\n", "\n", "# Match sizes by zero padding, otherwise we can't convert to single matrix", "\n", "if", "len", "(", "pos_f", ")", ">", "len", "(", "neg_f", ")", ":", "\n", "            ", "padding", "=", "len", "(", "pos_f", ")", "-", "len", "(", "neg_f", ")", "\n", "neg_f", "=", "np", ".", "pad", "(", "neg_f", ",", "(", "0", ",", "padding", ")", ",", "'constant'", ",", "constant_values", "=", "(", "0.0", ",", "0.0", ")", ")", "\n", "neg_amp", "=", "np", ".", "pad", "(", "neg_amp", ",", "(", "0", ",", "padding", ")", ",", "'constant'", ",", "constant_values", "=", "(", "0.0", ",", "0.0", ")", ")", "\n", "", "elif", "len", "(", "neg_f", ")", ">", "len", "(", "pos_f", ")", ":", "\n", "            ", "padding", "=", "len", "(", "neg_f", ")", "-", "len", "(", "pos_f", ")", "\n", "pos_f", "=", "np", ".", "pad", "(", "pos_f", ",", "(", "0", ",", "padding", ")", ",", "'constant'", ",", "constant_values", "=", "(", "0.0", ",", "0.0", ")", ")", "\n", "pos_amp", "=", "np", ".", "pad", "(", "pos_amp", ",", "(", "0", ",", "padding", ")", ",", "'constant'", ",", "constant_values", "=", "(", "0.0", ",", "0.0", ")", ")", "\n", "\n", "# Get approximately the pos_f/neg_f frequencies for each", "\n", "", "freqs", "=", "[", "]", "\n", "amps", "=", "[", "]", "\n", "\n", "for", "label", "in", "labels", ":", "\n", "            ", "f", "=", "neg_f", "if", "label", "==", "0", "else", "pos_f", "\n", "amp", "=", "neg_amp", "if", "label", "==", "0", "else", "pos_amp", "\n", "freqs", ".", "append", "(", "f", ")", "\n", "amps", ".", "append", "(", "amp", ")", "\n", "\n", "", "freqs", "=", "np", ".", "array", "(", "freqs", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "amps", "=", "np", ".", "array", "(", "amps", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "amps", "[", "np", ".", "isnan", "(", "amps", ")", "]", "=", "1.0", "# None is NaN, and so just set to 1.0 amplitude", "\n", "\n", "# Generate time series data", "\n", "x", ",", "y", "=", "sine", "(", "b", "=", "b", ",", "f", "=", "freqs", ",", "amps", "=", "amps", ",", "maxt", "=", "duration", ",", "length", "=", "duration", "*", "sample_freq", ",", "\n", "freq_noise", "=", "freq_noise_channel", ",", "phase_shift", "=", "phase_shift", ")", "\n", "\n", "if", "amp_noise", "is", "not", "None", ":", "\n", "            ", "y", "+=", "np", ".", "random", ".", "normal", "(", "0.0", ",", "amp_noise_channel", ",", "(", "y", ".", "shape", "[", "0", "]", ",", "n", ")", ")", "\n", "\n", "", "if", "display", ":", "\n", "            ", "display_xy", "(", "x", ",", "y", ")", "\n", "\n", "", "data", ".", "append", "(", "y", ")", "\n", "\n", "# Transpose from [features, time_steps, examples] to", "\n", "# [examples, time_steps, features]", "\n", "", "data", "=", "np", ".", "array", "(", "data", ",", "dtype", "=", "np", ".", "float32", ")", ".", "T", "\n", "\n", "# Make labels 1-indexed", "\n", "labels", "+=", "1", "\n", "\n", "return", "data", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.rotate2d": [[295, 318], ["numpy.radians", "numpy.cos", "numpy.sin", "numpy.array", "numpy.dot"], "function", ["None"], ["", "def", "rotate2d", "(", "x", ",", "degrees", ")", ":", "\n", "    ", "\"\"\"\n    Rotate the 2D data in x a certain number of degrees (clockwise, if each\n    point in x is (x,y))\n\n    If x is a single point (i.e. x is something like (x,y)) then it's only\n    rotates that point. However, more useful is if x is a time series, where the\n    values to be rotated (i.e. the feature dimension) is last. For example, pass\n    x with shape: [examples, time_steps, num_features] where num_features = 2\n    (since this is a 2D rotation matrix).\n\n    Note: if you want counterclockwise, do a left-multiply instead of a\n    right-multiply\n\n    See:\n    https://en.wikipedia.org/wiki/Rotation_matrix\n    https://scipython.com/book/chapter-6-numpy/examples/creating-a-rotation-matrix-in-numpy/\n    \"\"\"", "\n", "theta", "=", "np", ".", "radians", "(", "degrees", ")", "\n", "c", "=", "np", ".", "cos", "(", "theta", ")", "\n", "s", "=", "np", ".", "sin", "(", "theta", ")", "\n", "rotation_matrix", "=", "np", ".", "array", "(", "(", "(", "c", ",", "-", "s", ")", ",", "(", "s", ",", "c", ")", ")", ")", "\n", "return", "np", ".", "dot", "(", "x", ",", "rotation_matrix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.rotate2d_data": [[320, 323], ["synthetic_datasets.rotate2d"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.rotate2d"], ["", "def", "rotate2d_data", "(", "data", ",", "labels", ",", "degrees", ")", ":", "\n", "    ", "\"\"\" rotate2d but only rotates data and directly passes through labels \"\"\"", "\n", "return", "rotate2d", "(", "data", ",", "degrees", ")", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.save_data_file": [[325, 356], ["open", "enumerate", "f.write", "str", "len", "len", "enumerate", "NotImplementedError", "str", "str", "len"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.tfrecord.MultiModalityTFRecord.write"], ["", "def", "save_data_file", "(", "values", ",", "labels", ",", "filename", ")", ":", "\n", "    ", "\"\"\"\n    To be compatible with UCR dataset format with 1D data (univariate), commas\n    separate label first then all the data with one example on each line.\n    However, to support multivariate data, features for each time step are\n    delimitated by semicolons.\n\n    Example:\n      univariate: label,timestep1,timestep2,timestep3,...\n      multivariate: label,ts1f1;ts1f2;ts1f3,fs2f1;fs2f2;ts2f3,...\n    \"\"\"", "\n", "with", "open", "(", "filename", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "for", "i", ",", "x", "in", "enumerate", "(", "values", ")", ":", "\n", "            ", "y", "=", "labels", "[", "i", "]", "\n", "s", "=", "str", "(", "y", ")", "+", "\",\"", "\n", "\n", "# If only one feature, we don't have the extra dimension", "\n", "if", "len", "(", "x", ".", "shape", ")", "==", "1", ":", "\n", "                ", "s", "+=", "\",\"", ".", "join", "(", "[", "str", "(", "v", ")", "for", "v", "in", "x", "]", ")", "\n", "", "elif", "len", "(", "x", ".", "shape", ")", "==", "2", ":", "\n", "                ", "for", "j", ",", "time_step", "in", "enumerate", "(", "x", ")", ":", "\n", "                    ", "s", "+=", "\";\"", ".", "join", "(", "[", "str", "(", "v", ")", "for", "v", "in", "time_step", "]", ")", "\n", "\n", "if", "j", "!=", "len", "(", "x", ")", "-", "1", ":", "\n", "                        ", "s", "+=", "\",\"", "\n", "", "", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "(", "\n", "\"only support shapes [examples, time_steps]\"", "\n", "\" or [examples, time_steps, features]\"", ")", "\n", "\n", "", "f", ".", "write", "(", "s", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.save_data": [[358, 363], ["print", "synthetic_datasets.save_data_file", "synthetic_datasets.save_data_file", "func", "func"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.save_data_file", "home.repos.pwc.inspect_result.floft_calda.datasets.synthetic_datasets.save_data_file"], ["", "", "", "def", "save_data", "(", "func", ",", "dataset_name", ",", "display", "=", "False", ")", ":", "\n", "    ", "\"\"\" Use func to create examples that are saved to ..._TRAIN and ..._TEST \"\"\"", "\n", "print", "(", "dataset_name", ")", "\n", "save_data_file", "(", "*", "func", "(", "50000", ",", "False", ")", ",", "'datasets/synthetic/'", "+", "dataset_name", "+", "'_TRAIN'", ")", "\n", "save_data_file", "(", "*", "func", "(", "5000", ",", "display", ")", ",", "'datasets/synthetic/'", "+", "dataset_name", "+", "'_TEST'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.__init__": [[135, 183], ["datasets.Dataset.load", "len", "len", "datasets.Dataset.process", "datasets.Dataset.process"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.load", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.UciHarBase.process", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.UciHarBase.process"], ["def", "__init__", "(", "self", ",", "num_classes", ",", "class_labels", ",", "num_modalities", ",", "\n", "window_size", ",", "window_overlap", ",", "\n", "feature_names", "=", "None", ",", "test_percent", "=", "0.2", ")", ":", "\n", "        ", "\"\"\"\n        Initialize dataset\n\n        Must specify num_classes and class_labels (the names of the classes).\n\n        For example,\n            Dataset(num_classes=2, class_labels=[\"class1\", \"class2\"])\n\n        This calls load() to get the data, process() to normalize, convert to\n        float, etc.\n\n        At the end, look at dataset.{train,test}_{data,labels}\n        \"\"\"", "\n", "# Sanity checks", "\n", "assert", "num_classes", "==", "len", "(", "class_labels", ")", ",", "\"num_classes != len(class_labels)\"", "\n", "\n", "# Set parameters", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "class_labels", "=", "class_labels", "\n", "self", ".", "num_modalities", "=", "num_modalities", "\n", "self", ".", "window_size", "=", "window_size", "\n", "self", ".", "window_overlap", "=", "window_overlap", "\n", "self", ".", "feature_names", "=", "feature_names", "\n", "self", ".", "test_percent", "=", "test_percent", "\n", "\n", "assert", "len", "(", "self", ".", "feature_names", ")", "==", "self", ".", "num_modalities", ",", "\"feature_names should be same length as num_modalities\"", "\n", "\n", "# Load the dataset", "\n", "train_data", ",", "train_labels", ",", "test_data", ",", "test_labels", "=", "self", ".", "load", "(", ")", "\n", "\n", "if", "train_data", "is", "not", "None", "and", "train_labels", "is", "not", "None", ":", "\n", "            ", "self", ".", "train_data", ",", "self", ".", "train_labels", "=", "self", ".", "process", "(", "train_data", ",", "train_labels", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "train_data", "=", "None", "\n", "self", ".", "train_labels", "=", "None", "\n", "\n", "", "if", "test_data", "is", "not", "None", "and", "test_labels", "is", "not", "None", ":", "\n", "            ", "self", ".", "test_data", ",", "self", ".", "test_labels", "=", "self", ".", "process", "(", "test_data", ",", "test_labels", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "test_data", "=", "None", "\n", "self", ".", "test_labels", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.load": [[184, 186], ["NotImplementedError"], "methods", ["None"], ["", "", "def", "load", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"must implement load() for Dataset class\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.download_dataset": [[187, 208], ["isinstance", "downloaded_files.append", "tensorflow.keras.utils.get_file"], "methods", ["None"], ["", "def", "download_dataset", "(", "self", ",", "files_to_download", ",", "url", ")", ":", "\n", "        ", "\"\"\"\n        Download url/file for file in files_to_download\n        Returns: the downloaded filenames for each of the files given\n\n        Note: files_to_download can either be a list of individual filenames\n        or if desired a list of tuples:\n            [(filename used when saving, path to download), ...]\n        \"\"\"", "\n", "downloaded_files", "=", "[", "]", "\n", "\n", "for", "f", "in", "files_to_download", ":", "\n", "            ", "if", "isinstance", "(", "f", ",", "tuple", ")", ":", "\n", "                ", "save_f", ",", "f", "=", "f", "\n", "", "else", ":", "\n", "                ", "save_f", "=", "f", "\n", "\n", "", "downloaded_files", ".", "append", "(", "tf", ".", "keras", ".", "utils", ".", "get_file", "(", "\n", "fname", "=", "save_f", ",", "origin", "=", "url", "+", "\"/\"", "+", "f", ")", ")", "\n", "\n", "", "return", "downloaded_files", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.process": [[209, 214], ["None"], "methods", ["None"], ["", "def", "process", "(", "self", ",", "data", ",", "labels", ")", ":", "\n", "        ", "\"\"\" Perform conversions, etc. If you override,\n        you should `return super().process(data, labels)` to make sure these\n        options are handled. \"\"\"", "\n", "return", "data", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.train_test_split": [[215, 226], ["sklearn.model_selection.train_test_split"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.train_test_split"], ["", "def", "train_test_split", "(", "self", ",", "x", ",", "y", ",", "random_state", "=", "42", ")", ":", "\n", "        ", "\"\"\"\n        Split x and y data into train/test sets\n\n        Warning: train_test_split() is from sklearn but self.train_test_split()\n        is this function, which is what you should use.\n        \"\"\"", "\n", "x_train", ",", "x_test", ",", "y_train", ",", "y_test", "=", "train_test_split", "(", "x", ",", "y", ",", "test_size", "=", "self", ".", "test_percent", ",", "\n", "stratify", "=", "y", ",", "random_state", "=", "random_state", ")", "\n", "return", "x_train", ",", "y_train", ",", "x_test", ",", "y_test", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.train_test_split_modality": [[227, 252], ["sklearn.model_selection.train_test_split", "len"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.train_test_split"], ["", "def", "train_test_split_modality", "(", "self", ",", "xs", ",", "y", ",", "random_state", "=", "42", ")", ":", "\n", "        ", "\"\"\"\n        Split x and y data into train/test sets. Assumes xs is a list of the\n        modalities in the format\n\n        xs = [(example 1 modality 1, example 2 modality 1, ...),\n              (example 1 modality 2, example 2 modality 2, ...)]\n        \"\"\"", "\n", "# Returns train/test for each input passed in, so: xs1_train, xs1_test,", "\n", "# xs2_train, xs2_test, ...", "\n", "results", "=", "train_test_split", "(", "*", "xs", ",", "y", ",", "test_size", "=", "self", ".", "test_percent", ",", "\n", "stratify", "=", "y", ",", "random_state", "=", "random_state", ")", "\n", "\n", "# Train is evens (starting at position 0), test is odds", "\n", "assert", "len", "(", "results", ")", "%", "2", "==", "0", ",", "\"should get even number of splits\"", "\n", "train", "=", "results", "[", "0", ":", ":", "2", "]", "\n", "test", "=", "results", "[", "1", ":", ":", "2", "]", "\n", "\n", "# y is at the end, xs is everything else", "\n", "xs_train", "=", "train", "[", ":", "-", "1", "]", "\n", "xs_test", "=", "test", "[", ":", "-", "1", "]", "\n", "y_train", "=", "train", "[", "-", "1", "]", "\n", "y_test", "=", "test", "[", "-", "1", "]", "\n", "\n", "return", "xs_train", ",", "y_train", ",", "xs_test", ",", "y_test", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.get_file_in_archive": [[253, 258], ["archive.open", "fp.read"], "methods", ["None"], ["", "def", "get_file_in_archive", "(", "self", ",", "archive", ",", "filename", ")", ":", "\n", "        ", "\"\"\" Read one file out of the already-open zip/rar file \"\"\"", "\n", "with", "archive", ".", "open", "(", "filename", ")", "as", "fp", ":", "\n", "            ", "contents", "=", "fp", ".", "read", "(", ")", "\n", "", "return", "contents", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.window_next_i": [[259, 272], ["isinstance", "NotImplementedError"], "methods", ["None"], ["", "def", "window_next_i", "(", "self", ",", "i", ",", "overlap", ",", "window_size", ")", ":", "\n", "        ", "\"\"\" Where to start the next window \"\"\"", "\n", "if", "overlap", "is", "not", "False", ":", "\n", "            ", "if", "overlap", "is", "True", ":", "\n", "                ", "i", "+=", "1", "\n", "", "elif", "isinstance", "(", "overlap", ",", "int", ")", ":", "\n", "                ", "i", "+=", "overlap", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "(", "\"overlap should be True/False or integer\"", ")", "\n", "", "", "else", ":", "\n", "            ", "i", "+=", "window_size", "\n", "\n", "", "return", "i", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.create_windows_x": [[273, 306], ["numpy.expand_dims", "numpy.vstack", "numpy.expand_dims", "windows_x.append", "datasets.Dataset.window_next_i", "len", "numpy.concatenate"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.window_next_i"], ["", "def", "create_windows_x", "(", "self", ",", "x", ",", "window_size", ",", "overlap", ")", ":", "\n", "        ", "\"\"\"\n        Concatenate along dim-1 to meet the desired window_size. We'll skip any\n        windows that reach beyond the end. Only process x (saves memory).\n\n        Three options (examples for window_size=5):\n            Overlap - e.g. window 0 will be a list of examples 0,1,2,3,4 and the\n                label of example 4; and window 1 will be 1,2,3,4,5 and the label of\n                example 5\n            No overlap - e.g. window 0 will be a list of examples 0,1,2,3,4 and the\n                label of example 4; and window 1 will be 5,6,7,8,9 and the label of\n                example 9\n            Overlap as integer rather than True/False - e.g. if overlap=2 then\n                window 0 will be examples 0,1,2,3,4 and then window 1 will be\n                2,3,4,5,6, etc.\n        \"\"\"", "\n", "x", "=", "np", ".", "expand_dims", "(", "x", ",", "axis", "=", "1", ")", "\n", "\n", "# No work required if the window size is 1, only part required is", "\n", "# the above expand dims", "\n", "if", "window_size", "==", "1", ":", "\n", "            ", "return", "x", "\n", "\n", "", "windows_x", "=", "[", "]", "\n", "i", "=", "0", "\n", "\n", "while", "i", "<", "len", "(", "x", ")", "-", "window_size", ":", "\n", "            ", "window_x", "=", "np", ".", "expand_dims", "(", "np", ".", "concatenate", "(", "x", "[", "i", ":", "i", "+", "window_size", "]", ",", "axis", "=", "0", ")", ",", "axis", "=", "0", ")", "\n", "windows_x", ".", "append", "(", "window_x", ")", "\n", "# Where to start the next window", "\n", "i", "=", "self", ".", "window_next_i", "(", "i", ",", "overlap", ",", "window_size", ")", "\n", "\n", "", "return", "np", ".", "vstack", "(", "windows_x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.create_windows_y": [[307, 337], ["numpy.hstack", "windows_y.append", "datasets.Dataset.window_next_i", "len"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.window_next_i"], ["", "def", "create_windows_y", "(", "self", ",", "y", ",", "window_size", ",", "overlap", ")", ":", "\n", "        ", "\"\"\"\n        Concatenate along dim-1 to meet the desired window_size. We'll skip any\n        windows that reach beyond the end. Only process y (saves memory).\n\n        Two options (examples for window_size=5):\n            Overlap - e.g. window 0 will be a list of examples 0,1,2,3,4 and the\n                label of example 4; and window 1 will be 1,2,3,4,5 and the label of\n                example 5\n            No overlap - e.g. window 0 will be a list of examples 0,1,2,3,4 and the\n                label of example 4; and window 1 will be 5,6,7,8,9 and the label of\n                example 9\n            Overlap as integer rather than True/False - e.g. if overlap=2 then\n                window 0 will be examples 0,1,2,3,4 and then window 1 will be\n                2,3,4,5,6, etc.\n        \"\"\"", "\n", "# No work required if the window size is 1", "\n", "if", "window_size", "==", "1", ":", "\n", "            ", "return", "y", "\n", "\n", "", "windows_y", "=", "[", "]", "\n", "i", "=", "0", "\n", "\n", "while", "i", "<", "len", "(", "y", ")", "-", "window_size", ":", "\n", "            ", "window_y", "=", "y", "[", "i", "+", "window_size", "-", "1", "]", "\n", "windows_y", ".", "append", "(", "window_y", ")", "\n", "# Where to start the next window", "\n", "i", "=", "self", ".", "window_next_i", "(", "i", ",", "overlap", ",", "window_size", ")", "\n", "\n", "", "return", "np", ".", "hstack", "(", "windows_y", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.create_windows": [[338, 343], ["datasets.Dataset.create_windows_x", "datasets.Dataset.create_windows_y"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.create_windows_x", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.create_windows_y"], ["", "def", "create_windows", "(", "self", ",", "x", ",", "y", ",", "window_size", ",", "overlap", ")", ":", "\n", "        ", "\"\"\" Split time-series data into windows \"\"\"", "\n", "x", "=", "self", ".", "create_windows_x", "(", "x", ",", "window_size", ",", "overlap", ")", "\n", "y", "=", "self", ".", "create_windows_y", "(", "y", ",", "window_size", ",", "overlap", ")", "\n", "return", "x", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.pad_to": [[344, 371], ["len", "numpy.pad", "str", "len", "numpy.pad", "NotImplementedError", "str", "str", "str"], "methods", ["None"], ["", "def", "pad_to", "(", "self", ",", "data", ",", "desired_length", ")", ":", "\n", "        ", "\"\"\"\n        Pad the number of time steps to the desired length\n\n        Accepts data in one of two formats:\n            - shape: (time_steps, features) -> (desired_length, features)\n            - shape: (batch_size, time_steps, features) ->\n                (batch_size, desired_length, features)\n        \"\"\"", "\n", "if", "len", "(", "data", ".", "shape", ")", "==", "2", ":", "\n", "            ", "current_length", "=", "data", ".", "shape", "[", "0", "]", "\n", "assert", "current_length", "<=", "desired_length", ",", "\"Cannot shrink size by padding, current length \"", "+", "str", "(", "current_length", ")", "+", "\" vs. desired_length \"", "+", "str", "(", "desired_length", ")", "\n", "return", "np", ".", "pad", "(", "data", ",", "[", "(", "0", ",", "desired_length", "-", "current_length", ")", ",", "(", "0", ",", "0", ")", "]", ",", "\n", "mode", "=", "\"constant\"", ",", "constant_values", "=", "0", ")", "\n", "", "elif", "len", "(", "data", ".", "shape", ")", "==", "3", ":", "\n", "            ", "current_length", "=", "data", ".", "shape", "[", "1", "]", "\n", "assert", "current_length", "<=", "desired_length", ",", "\"Cannot shrink size by padding, current length \"", "+", "str", "(", "current_length", ")", "+", "\" vs. desired_length \"", "+", "str", "(", "desired_length", ")", "\n", "return", "np", ".", "pad", "(", "data", ",", "[", "(", "0", ",", "0", ")", ",", "(", "0", ",", "desired_length", "-", "current_length", ")", ",", "(", "0", ",", "0", ")", "]", ",", "\n", "mode", "=", "\"constant\"", ",", "constant_values", "=", "0", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"pad_to requires 2 or 3-dim data\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.label_to_int": [[372, 375], ["datasets.Dataset.class_labels.index"], "methods", ["None"], ["", "", "def", "label_to_int", "(", "self", ",", "label_name", ")", ":", "\n", "        ", "\"\"\" e.g. Bathe to 0 \"\"\"", "\n", "return", "self", ".", "class_labels", ".", "index", "(", "label_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.int_to_label": [[376, 379], ["None"], "methods", ["None"], ["", "def", "int_to_label", "(", "self", ",", "label_index", ")", ":", "\n", "        ", "\"\"\" e.g. Bathe to 0 \"\"\"", "\n", "return", "self", ".", "class_labels", "[", "label_index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.UciHarBase.__init__": [[403, 408], ["datasets.Dataset.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["def", "__init__", "(", "self", ",", "users", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "users", "=", "users", "\n", "super", "(", ")", ".", "__init__", "(", "UciHarBase", ".", "num_classes", ",", "UciHarBase", ".", "class_labels", ",", "\n", "UciHarBase", ".", "num_modalities", ",", "\n", "None", ",", "None", ",", "UciHarBase", ".", "feature_names", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.UciHarBase.download": [[409, 413], ["datasets.UciHarBase.download_dataset"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.download_dataset"], ["", "def", "download", "(", "self", ")", ":", "\n", "        ", "(", "dataset_fp", ",", ")", "=", "self", ".", "download_dataset", "(", "[", "\"UCI%20HAR%20Dataset.zip\"", "]", ",", "\n", "\"https://archive.ics.uci.edu/ml/machine-learning-databases/00240\"", ")", "\n", "return", "dataset_fp", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.UciHarBase.get_feature": [[414, 427], ["content.decode().strip().split", "features.append", "content.decode().strip", "float", "content.decode", "line.strip().split", "line.strip"], "methods", ["None"], ["", "def", "get_feature", "(", "self", ",", "content", ")", ":", "\n", "        ", "\"\"\"\n        Read the space-separated, example on each line file\n\n        Returns 2D array with dimensions: [num_examples, num_time_steps]\n        \"\"\"", "\n", "lines", "=", "content", ".", "decode", "(", "\"utf-8\"", ")", ".", "strip", "(", ")", ".", "split", "(", "\"\\n\"", ")", "\n", "features", "=", "[", "]", "\n", "\n", "for", "line", "in", "lines", ":", "\n", "            ", "features", ".", "append", "(", "[", "float", "(", "v", ")", "for", "v", "in", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "]", ")", "\n", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.UciHarBase.get_data": [[428, 460], ["datasets.UciHarBase.get_data.get_data_single"], "methods", ["None"], ["", "def", "get_data", "(", "self", ",", "archive", ",", "name", ")", ":", "\n", "        ", "\"\"\" To shorten duplicate code for name=train or name=test cases \"\"\"", "\n", "def", "get_data_single", "(", "f", ")", ":", "\n", "            ", "return", "self", ".", "get_feature", "(", "self", ".", "get_file_in_archive", "(", "archive", ",", "\n", "\"UCI HAR Dataset/\"", "+", "f", ")", ")", "\n", "\n", "", "data", "=", "[", "\n", "get_data_single", "(", "name", "+", "\"/Inertial Signals/body_acc_x_\"", "+", "name", "+", "\".txt\"", ")", ",", "\n", "get_data_single", "(", "name", "+", "\"/Inertial Signals/body_acc_y_\"", "+", "name", "+", "\".txt\"", ")", ",", "\n", "get_data_single", "(", "name", "+", "\"/Inertial Signals/body_acc_z_\"", "+", "name", "+", "\".txt\"", ")", ",", "\n", "get_data_single", "(", "name", "+", "\"/Inertial Signals/body_gyro_x_\"", "+", "name", "+", "\".txt\"", ")", ",", "\n", "get_data_single", "(", "name", "+", "\"/Inertial Signals/body_gyro_y_\"", "+", "name", "+", "\".txt\"", ")", ",", "\n", "get_data_single", "(", "name", "+", "\"/Inertial Signals/body_gyro_z_\"", "+", "name", "+", "\".txt\"", ")", ",", "\n", "get_data_single", "(", "name", "+", "\"/Inertial Signals/total_acc_x_\"", "+", "name", "+", "\".txt\"", ")", ",", "\n", "get_data_single", "(", "name", "+", "\"/Inertial Signals/total_acc_y_\"", "+", "name", "+", "\".txt\"", ")", ",", "\n", "get_data_single", "(", "name", "+", "\"/Inertial Signals/total_acc_z_\"", "+", "name", "+", "\".txt\"", ")", ",", "\n", "]", "\n", "\n", "labels", "=", "get_data_single", "(", "name", "+", "\"/y_\"", "+", "name", "+", "\".txt\"", ")", "\n", "\n", "subjects", "=", "get_data_single", "(", "name", "+", "\"/subject_\"", "+", "name", "+", "\".txt\"", ")", "\n", "\n", "data", "=", "np", ".", "array", "(", "data", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "labels", "=", "np", ".", "squeeze", "(", "np", ".", "array", "(", "labels", ",", "dtype", "=", "np", ".", "float32", ")", ")", "\n", "# Squeeze so we can easily do selection on this later on", "\n", "subjects", "=", "np", ".", "squeeze", "(", "np", ".", "array", "(", "subjects", ",", "dtype", "=", "np", ".", "float32", ")", ")", "\n", "\n", "# Transpose from [features, examples, time_steps] to", "\n", "# [examples, time_steps (128), features (9)]", "\n", "data", "=", "np", ".", "transpose", "(", "data", ",", "axes", "=", "[", "1", ",", "2", ",", "0", "]", ")", "\n", "\n", "return", "data", ",", "labels", ",", "subjects", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.UciHarBase.load_file": [[461, 492], ["numpy.vstack().astype", "numpy.hstack().astype", "numpy.hstack().astype", "numpy.vstack().astype", "numpy.hstack().astype", "zipfile.ZipFile", "datasets.UciHarBase.get_data", "datasets.UciHarBase.get_data", "data.append", "labels.append", "numpy.vstack", "numpy.hstack", "numpy.hstack", "numpy.vstack", "numpy.hstack"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProBase.get_data", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProBase.get_data"], ["", "def", "load_file", "(", "self", ",", "filename", ")", ":", "\n", "        ", "\"\"\" Load ZIP file containing all the .txt files \"\"\"", "\n", "with", "zipfile", ".", "ZipFile", "(", "filename", ",", "\"r\"", ")", "as", "archive", ":", "\n", "            ", "train_data", ",", "train_labels", ",", "train_subjects", "=", "self", ".", "get_data", "(", "archive", ",", "\"train\"", ")", "\n", "test_data", ",", "test_labels", ",", "test_subjects", "=", "self", ".", "get_data", "(", "archive", ",", "\"test\"", ")", "\n", "\n", "", "all_data", "=", "np", ".", "vstack", "(", "[", "train_data", ",", "test_data", "]", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "all_labels", "=", "np", ".", "hstack", "(", "[", "train_labels", ",", "test_labels", "]", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "all_subjects", "=", "np", ".", "hstack", "(", "[", "train_subjects", ",", "test_subjects", "]", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n", "# All data if no selection", "\n", "if", "self", ".", "users", "is", "None", ":", "\n", "            ", "return", "all_data", ",", "all_labels", "\n", "\n", "# Otherwise, select based on the desired users", "\n", "", "data", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "\n", "for", "user", "in", "self", ".", "users", ":", "\n", "            ", "selection", "=", "all_subjects", "==", "user", "\n", "data", ".", "append", "(", "all_data", "[", "selection", "]", ")", "\n", "current_labels", "=", "all_labels", "[", "selection", "]", "\n", "labels", ".", "append", "(", "current_labels", ")", "\n", "\n", "", "x", "=", "np", ".", "vstack", "(", "data", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "y", "=", "np", ".", "hstack", "(", "labels", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n", "# print(\"Selected data:\", self.users)", "\n", "# print(x.shape, y.shape)", "\n", "\n", "return", "x", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.UciHarBase.load": [[493, 501], ["datasets.UciHarBase.download", "datasets.UciHarBase.load_file", "datasets.UciHarBase.train_test_split"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProDB5BaseLower.download", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.load_file", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.train_test_split"], ["", "def", "load", "(", "self", ")", ":", "\n", "        ", "dataset_fp", "=", "self", ".", "download", "(", ")", "\n", "x", ",", "y", "=", "self", ".", "load_file", "(", "dataset_fp", ")", "\n", "train_data", ",", "train_labels", ",", "test_data", ",", "test_labels", "=", "self", ".", "train_test_split", "(", "x", ",", "y", ")", "\n", "\n", "# Only one modality", "\n", "return", "[", "train_data", "]", ",", "train_labels", ",", "[", "test_data", "]", ",", "test_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.UciHarBase.process": [[502, 506], ["datasets.Dataset.process"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.UciHarBase.process"], ["", "def", "process", "(", "self", ",", "data", ",", "labels", ")", ":", "\n", "# Index one", "\n", "        ", "labels", "=", "labels", "-", "1", "\n", "return", "super", "(", ")", ".", "process", "(", "data", ",", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.UciHHarBase.__init__": [[527, 533], ["datasets.Dataset.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["def", "__init__", "(", "self", ",", "users", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "users", "=", "users", "\n", "super", "(", ")", ".", "__init__", "(", "UciHHarBase", ".", "num_classes", ",", "UciHHarBase", ".", "class_labels", ",", "\n", "UciHHarBase", ".", "num_modalities", ",", "\n", "UciHHarBase", ".", "window_size", ",", "UciHHarBase", ".", "window_overlap", ",", "\n", "UciHHarBase", ".", "feature_names", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.UciHHarBase.download": [[534, 538], ["datasets.UciHHarBase.download_dataset"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.download_dataset"], ["", "def", "download", "(", "self", ")", ":", "\n", "        ", "(", "dataset_fp", ",", ")", "=", "self", ".", "download_dataset", "(", "[", "\"Activity%20recognition%20exp.zip\"", "]", ",", "\n", "\"https://archive.ics.uci.edu/ml/machine-learning-databases/00344/\"", ")", "\n", "return", "dataset_fp", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.UciHHarBase.read_file": [[539, 576], ["content.decode().strip().split", "numpy.array", "numpy.array", "numpy.array", "line.strip().split", "users.index", "content.decode().strip", "float", "float", "float", "datasets.UciHHarBase.class_labels.index", "numpy.array.append", "numpy.array.append", "numpy.array.append", "line.strip", "content.decode"], "methods", ["None"], ["", "def", "read_file", "(", "self", ",", "content", ")", ":", "\n", "        ", "\"\"\" Read the CSV file \"\"\"", "\n", "lines", "=", "content", ".", "decode", "(", "\"utf-8\"", ")", ".", "strip", "(", ")", ".", "split", "(", "\"\\n\"", ")", "\n", "data_x", "=", "[", "]", "\n", "data_label", "=", "[", "]", "\n", "data_subject", "=", "[", "]", "\n", "users", "=", "[", "\"a\"", ",", "\"b\"", ",", "\"c\"", ",", "\"d\"", ",", "\"e\"", ",", "\"f\"", ",", "\"g\"", ",", "\"h\"", ",", "\"i\"", "]", "\n", "\n", "for", "line", "in", "lines", ":", "\n", "            ", "index", ",", "arrival", ",", "creation", ",", "x", ",", "y", ",", "z", ",", "user", ",", "model", ",", "device", ",", "label", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "\",\"", ")", "\n", "\n", "# Skip the header (can't determine user if invalid)", "\n", "if", "index", "==", "\"Index\"", ":", "\n", "                ", "continue", "\n", "\n", "", "user", "=", "users", ".", "index", "(", "user", ")", "# letter --> number", "\n", "\n", "# Skip users we don't care about and data without a label", "\n", "if", "user", "in", "self", ".", "users", "and", "label", "!=", "\"null\"", ":", "\n", "#index = int(index)", "\n", "#arrival = float(arrival)", "\n", "#creation = float(creation)", "\n", "                ", "x", "=", "float", "(", "x", ")", "\n", "y", "=", "float", "(", "y", ")", "\n", "z", "=", "float", "(", "z", ")", "\n", "label", "=", "self", ".", "class_labels", ".", "index", "(", "label", ")", "# name --> number", "\n", "\n", "data_x", ".", "append", "(", "(", "x", ",", "y", ",", "z", ")", ")", "\n", "data_label", ".", "append", "(", "label", ")", "\n", "data_subject", ".", "append", "(", "user", ")", "\n", "\n", "", "", "data_x", "=", "np", ".", "array", "(", "data_x", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "data_label", "=", "np", ".", "array", "(", "data_label", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "data_subject", "=", "np", ".", "array", "(", "data_subject", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "return", "data_x", ",", "data_label", ",", "data_subject", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.UciHHarBase.get_data": [[577, 583], ["datasets.UciHHarBase.read_file", "datasets.UciHHarBase.get_file_in_archive"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.UciHHarBase.read_file", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.get_file_in_archive"], ["", "def", "get_data", "(", "self", ",", "archive", ",", "name", ")", ":", "\n", "# In their paper, looks like they only did either accelerometer or", "\n", "# gyroscope, not aligning them by the creation timestamp. For them the", "\n", "# accelerometer data worked better, so we'll just use that for now.", "\n", "        ", "return", "self", ".", "read_file", "(", "self", ".", "get_file_in_archive", "(", "archive", ",", "\n", "\"Activity recognition exp/\"", "+", "name", "+", "\"_accelerometer.csv\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.UciHHarBase.load_file": [[584, 623], ["numpy.vstack().astype", "numpy.hstack().astype", "zipfile.ZipFile", "datasets.UciHHarBase.get_data", "datasets.UciHHarBase.create_windows", "data.append", "labels.append", "len", "str", "numpy.vstack", "numpy.hstack"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProBase.get_data", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.create_windows"], ["", "def", "load_file", "(", "self", ",", "filename", ")", ":", "\n", "        ", "\"\"\" Load ZIP file containing all the .txt files \"\"\"", "\n", "with", "zipfile", ".", "ZipFile", "(", "filename", ",", "\"r\"", ")", "as", "archive", ":", "\n", "# For now just use phone data since the positions may differ too much", "\n", "            ", "all_data", ",", "all_labels", ",", "all_subjects", "=", "self", ".", "get_data", "(", "archive", ",", "\"Phones\"", ")", "\n", "\n", "# phone_data, phone_labels, phone_subjects = self.get_data(archive, \"Phone\")", "\n", "# watch_data, watch_labels, watch_subjects = self.get_data(archive, \"Watch\")", "\n", "\n", "# all_data = np.vstack([phone_data, watch_data]).astype(np.float32)", "\n", "# all_labels = np.hstack([phone_labels, watch_labels]).astype(np.float32)", "\n", "# all_subjects = np.hstack([phone_subjects, watch_subjects]).astype(np.float32)", "\n", "\n", "# Otherwise, select based on the desired users", "\n", "", "data", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "\n", "for", "user", "in", "self", ".", "users", ":", "\n", "# Load this user's data", "\n", "            ", "selection", "=", "all_subjects", "==", "user", "\n", "current_data", "=", "all_data", "[", "selection", "]", "\n", "current_labels", "=", "all_labels", "[", "selection", "]", "\n", "assert", "len", "(", "current_labels", ")", ">", "0", ",", "\"Error: no data for user \"", "+", "str", "(", "user", ")", "\n", "\n", "# Split into windows", "\n", "current_data", ",", "current_labels", "=", "self", ".", "create_windows", "(", "current_data", ",", "\n", "current_labels", ",", "self", ".", "window_size", ",", "self", ".", "window_overlap", ")", "\n", "\n", "# Save", "\n", "data", ".", "append", "(", "current_data", ")", "\n", "labels", ".", "append", "(", "current_labels", ")", "\n", "\n", "", "x", "=", "np", ".", "vstack", "(", "data", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "y", "=", "np", ".", "hstack", "(", "labels", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n", "# print(\"Selected data:\", self.users)", "\n", "# print(x.shape, y.shape)", "\n", "\n", "return", "x", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.UciHHarBase.load": [[624, 632], ["datasets.UciHHarBase.download", "datasets.UciHHarBase.load_file", "datasets.UciHHarBase.train_test_split"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProDB5BaseLower.download", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.load_file", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.train_test_split"], ["", "def", "load", "(", "self", ")", ":", "\n", "        ", "dataset_fp", "=", "self", ".", "download", "(", ")", "\n", "x", ",", "y", "=", "self", ".", "load_file", "(", "dataset_fp", ")", "\n", "train_data", ",", "train_labels", ",", "test_data", ",", "test_labels", "=", "self", ".", "train_test_split", "(", "x", ",", "y", ")", "\n", "\n", "# Only one modality", "\n", "return", "[", "train_data", "]", ",", "train_labels", ",", "[", "test_data", "]", ",", "test_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.WisdmBase.__init__": [[645, 652], ["datasets.Dataset.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["def", "__init__", "(", "self", ",", "users", ",", "num_classes", ",", "class_labels", ",", "num_modalities", ",", "\n", "*", "args", ",", "class_labels_map", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "users", "=", "users", "\n", "self", ".", "class_labels_map", "=", "class_labels_map", "\n", "super", "(", ")", ".", "__init__", "(", "num_classes", ",", "class_labels", ",", "num_modalities", ",", "\n", "WisdmBase", ".", "window_size", ",", "WisdmBase", ".", "window_overlap", ",", "\n", "WisdmBase", ".", "feature_names", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "# Override and set these", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.WisdmBase.download": [[656, 660], ["datasets.WisdmBase.download_dataset"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.download_dataset"], ["", "def", "download", "(", "self", ")", ":", "\n", "        ", "(", "dataset_fp", ",", ")", "=", "self", ".", "download_dataset", "(", "[", "self", ".", "download_filename", "]", ",", "\n", "\"http://www.cis.fordham.edu/wisdm/includes/datasets/latest/\"", ")", "\n", "return", "dataset_fp", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.WisdmBase.read_data": [[661, 706], ["numpy.array", "numpy.array", "numpy.array", "line.strip().replace().split", "int", "user_list.index", "line.strip().replace", "len", "len", "float", "float", "float", "datasets.WisdmBase.class_labels.index", "numpy.array.append", "numpy.array.append", "numpy.array.append", "line.strip"], "methods", ["None"], ["", "def", "read_data", "(", "self", ",", "lines", ",", "user_list", ")", ":", "\n", "        ", "\"\"\" Read the raw data CSV file \"\"\"", "\n", "data_x", "=", "[", "]", "\n", "data_label", "=", "[", "]", "\n", "data_subject", "=", "[", "]", "\n", "\n", "for", "line", "in", "lines", ":", "\n", "            ", "parts", "=", "line", ".", "strip", "(", ")", ".", "replace", "(", "\";\"", ",", "\"\"", ")", ".", "split", "(", "\",\"", ")", "\n", "\n", "# For some reason there's blank rows in the data, e.g.", "\n", "# a bunch of lines like \"577,,;\"", "\n", "# Though, allow 7 since sometimes there's an extra comma at the end:", "\n", "# \"21,Jogging,117687701514000,3.17,9,1.23,;\"", "\n", "if", "len", "(", "parts", ")", "!=", "6", "and", "len", "(", "parts", ")", "!=", "7", ":", "\n", "                ", "continue", "\n", "\n", "# Skip if x, y, or z is blank", "\n", "", "if", "parts", "[", "3", "]", "==", "\"\"", "or", "parts", "[", "4", "]", "==", "\"\"", "or", "parts", "[", "5", "]", "==", "\"\"", ":", "\n", "                ", "continue", "\n", "\n", "", "user", "=", "int", "(", "parts", "[", "0", "]", ")", "\n", "\n", "# Skip users that may not have enough data", "\n", "if", "user", "in", "user_list", ":", "\n", "                ", "user", "=", "user_list", ".", "index", "(", "user", ")", "# non-consecutive to consecutive", "\n", "\n", "# Skip users we don't care about", "\n", "if", "user", "in", "self", ".", "users", ":", "\n", "                    ", "x", "=", "float", "(", "parts", "[", "3", "]", ")", "\n", "y", "=", "float", "(", "parts", "[", "4", "]", ")", "\n", "z", "=", "float", "(", "parts", "[", "5", "]", ")", "\n", "label", "=", "parts", "[", "1", "]", "\n", "if", "self", ".", "class_labels_map", "is", "not", "None", ":", "\n", "                        ", "label", "=", "self", ".", "class_labels_map", "[", "label", "]", "\n", "", "label", "=", "self", ".", "class_labels", ".", "index", "(", "label", ")", "# name --> number", "\n", "\n", "data_x", ".", "append", "(", "(", "x", ",", "y", ",", "z", ")", ")", "\n", "data_label", ".", "append", "(", "label", ")", "\n", "data_subject", ".", "append", "(", "user", ")", "\n", "\n", "", "", "", "data_x", "=", "np", ".", "array", "(", "data_x", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "data_label", "=", "np", ".", "array", "(", "data_label", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "data_subject", "=", "np", ".", "array", "(", "data_subject", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "return", "data_x", ",", "data_label", ",", "data_subject", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.WisdmBase.read_user_list": [[707, 753], ["int", "user_sample_count.items", "user_list.sort", "line.strip().split", "int", "user_list.append", "line.strip", "len", "len"], "methods", ["None"], ["", "def", "read_user_list", "(", "self", ",", "lines", ",", "min_test_samples", "=", "30", ")", ":", "\n", "        ", "\"\"\" Read first column of the CSV file to get a unique list of uid's\n        Also, skip users with too few samples \"\"\"", "\n", "user_sample_count", "=", "{", "}", "\n", "\n", "for", "line", "in", "lines", ":", "\n", "            ", "parts", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "\",\"", ")", "\n", "\n", "# There's some lines without the right number of parts, e.g. blank", "\n", "if", "len", "(", "parts", ")", "!=", "6", "and", "len", "(", "parts", ")", "!=", "7", ":", "\n", "                ", "continue", "\n", "\n", "# Skip if x, y, or z is blank", "\n", "", "if", "parts", "[", "3", "]", "==", "\"\"", "or", "parts", "[", "4", "]", "==", "\"\"", "or", "parts", "[", "5", "]", "==", "\"\"", ":", "\n", "                ", "continue", "\n", "\n", "", "uid", "=", "int", "(", "parts", "[", "0", "]", ")", "\n", "\n", "# There are duplicates in the file for some reason (so, either the", "\n", "# same person or it's not truly unique)", "\n", "if", "uid", "not", "in", "user_sample_count", ":", "\n", "                ", "user_sample_count", "[", "uid", "]", "=", "0", "\n", "", "else", ":", "\n", "                ", "user_sample_count", "[", "uid", "]", "+=", "1", "\n", "\n", "# Remove users with too few samples", "\n", "", "", "user_list", "=", "[", "]", "\n", "\n", "# How many samples we need: to stratify the sklearn function says", "\n", "# The test_size = A should be greater or equal to the number of classes = B", "\n", "# x/128*.2 > 6 classes", "\n", "# x > 6*128/.2", "\n", "# Though, actually, just set the minimum test samples. It's probably not", "\n", "# enough to have only 7...", "\n", "test_percentage", "=", "0.20", "# default", "\n", "#min_samples = int(len(self.class_labels)*self.window_size/test_percentage)", "\n", "min_samples", "=", "int", "(", "min_test_samples", "*", "self", ".", "window_size", "/", "test_percentage", ")", "\n", "\n", "for", "user", ",", "count", "in", "user_sample_count", ".", "items", "(", ")", ":", "\n", "            ", "if", "count", ">", "min_samples", ":", "\n", "                ", "user_list", ".", "append", "(", "user", ")", "\n", "\n", "# Data isn't sorted by user in the file", "\n", "", "", "user_list", ".", "sort", "(", ")", "\n", "\n", "return", "user_list", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.WisdmBase.get_lines": [[754, 762], ["archive.extractfile", "archive.extractfile.read().decode().strip().split", "archive.extractfile.read().decode().strip", "archive.extractfile.read().decode", "archive.extractfile.read"], "methods", ["None"], ["", "def", "get_lines", "(", "self", ",", "archive", ",", "name", ")", ":", "\n", "        ", "\"\"\" Open and load file in tar file, get lines from file \"\"\"", "\n", "f", "=", "archive", ".", "extractfile", "(", "self", ".", "filename_prefix", "+", "name", ")", "\n", "\n", "if", "f", "is", "None", ":", "\n", "            ", "return", "None", "\n", "\n", "", "return", "f", ".", "read", "(", ")", ".", "decode", "(", "\"utf-8\"", ")", ".", "strip", "(", ")", ".", "split", "(", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.WisdmBase.load_file": [[763, 806], ["datasets.WisdmBase.read_user_list", "datasets.WisdmBase.read_data", "numpy.vstack().astype", "numpy.hstack().astype", "tarfile.open", "datasets.WisdmBase.get_lines", "datasets.WisdmBase.create_windows", "data.append", "labels.append", "len", "str", "numpy.vstack", "numpy.hstack"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.WisdmBase.read_user_list", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.WisdmBase.read_data", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.WisdmBase.get_lines", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.create_windows"], ["", "def", "load_file", "(", "self", ",", "filename", ")", ":", "\n", "        ", "\"\"\" Load desired participants' data \"\"\"", "\n", "# Get data", "\n", "with", "tarfile", ".", "open", "(", "filename", ",", "\"r\"", ")", "as", "archive", ":", "\n", "            ", "raw_data", "=", "self", ".", "get_lines", "(", "archive", ",", "\"raw.txt\"", ")", "\n", "\n", "# Some of the data doesn't have a uid in the demographics file? So,", "\n", "# instead just get the user list from the raw data. Also, one person", "\n", "# have very little data, so skip them (e.g. one person only has 25", "\n", "# samples, which is only 0.5 seconds of data -- not useful).", "\n", "", "user_list", "=", "self", ".", "read_user_list", "(", "raw_data", ")", "\n", "\n", "#print(\"Number of users:\", len(user_list))", "\n", "\n", "# For now just use phone data since the positions may differ too much", "\n", "all_data", ",", "all_labels", ",", "all_subjects", "=", "self", ".", "read_data", "(", "raw_data", ",", "user_list", ")", "\n", "\n", "# Otherwise, select based on the desired users", "\n", "data", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "\n", "for", "user", "in", "self", ".", "users", ":", "\n", "# Load this user's data", "\n", "            ", "selection", "=", "all_subjects", "==", "user", "\n", "current_data", "=", "all_data", "[", "selection", "]", "\n", "current_labels", "=", "all_labels", "[", "selection", "]", "\n", "assert", "len", "(", "current_labels", ")", ">", "0", ",", "\"Error: no data for user \"", "+", "str", "(", "user", ")", "\n", "\n", "# Split into windows", "\n", "current_data", ",", "current_labels", "=", "self", ".", "create_windows", "(", "current_data", ",", "\n", "current_labels", ",", "self", ".", "window_size", ",", "self", ".", "window_overlap", ")", "\n", "\n", "# Save", "\n", "data", ".", "append", "(", "current_data", ")", "\n", "labels", ".", "append", "(", "current_labels", ")", "\n", "\n", "", "x", "=", "np", ".", "vstack", "(", "data", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "y", "=", "np", ".", "hstack", "(", "labels", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n", "# print(\"Selected data:\", self.users)", "\n", "# print(x.shape, y.shape)", "\n", "\n", "return", "x", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.WisdmBase.load": [[807, 815], ["datasets.WisdmBase.download", "datasets.WisdmBase.load_file", "datasets.WisdmBase.train_test_split"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProDB5BaseLower.download", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.load_file", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.train_test_split"], ["", "def", "load", "(", "self", ")", ":", "\n", "        ", "dataset_fp", "=", "self", ".", "download", "(", ")", "\n", "x", ",", "y", "=", "self", ".", "load_file", "(", "dataset_fp", ")", "\n", "train_data", ",", "train_labels", ",", "test_data", ",", "test_labels", "=", "self", ".", "train_test_split", "(", "x", ",", "y", ")", "\n", "\n", "# Only one modality", "\n", "return", "[", "train_data", "]", ",", "train_labels", ",", "[", "test_data", "]", ",", "test_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.WisdmAtBase.__init__": [[831, 837], ["datasets.WisdmBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["def", "__init__", "(", "self", ",", "users", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "filename_prefix", "=", "\"home/share/data/public_sets/WISDM_at_v2.0/WISDM_at_v2.0_\"", "\n", "self", ".", "download_filename", "=", "\"WISDM_at_latest.tar.gz\"", "\n", "super", "(", ")", ".", "__init__", "(", "users", ",", "\n", "WisdmAtBase", ".", "num_classes", ",", "WisdmAtBase", ".", "class_labels", ",", "\n", "WisdmAtBase", ".", "num_modalities", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.WisdmArBase.__init__": [[852, 858], ["datasets.WisdmBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["def", "__init__", "(", "self", ",", "users", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "filename_prefix", "=", "\"WISDM_ar_v1.1/WISDM_ar_v1.1_\"", "\n", "self", ".", "download_filename", "=", "\"WISDM_ar_latest.tar.gz\"", "\n", "super", "(", ")", ".", "__init__", "(", "users", ",", "\n", "WisdmArBase", ".", "num_classes", ",", "WisdmArBase", ".", "class_labels", ",", "\n", "WisdmArBase", ".", "num_modalities", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Myo.__init__": [[886, 894], ["datasets.Dataset.__init__", "len"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["def", "__init__", "(", "self", ",", "users", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "users", "=", "users", "\n", "super", "(", ")", ".", "__init__", "(", "Myo", ".", "num_classes", ",", "Myo", ".", "class_labels", ",", "\n", "Myo", ".", "num_modalities", ",", "Myo", ".", "window_size", ",", "Myo", ".", "window_overlap", ",", "\n", "Myo", ".", "feature_names", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "assert", "Myo", ".", "num_classes", "==", "load_myo_data", ".", "number_of_classes", "\n", "assert", "len", "(", "Myo", ".", "class_labels", ")", "==", "Myo", ".", "num_classes", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Myo.get_data": [[895, 929], ["range", "load_myo_data.shift_electrodes", "numpy.vstack().astype", "numpy.hstack().astype", "numpy.squeeze", "numpy.transpose", "os.path.join", "numpy.fromfile", "numpy.array", "load_myo_data.format_data_to_train", "numpy.transpose.append", "numpy.hstack().astype.append", "numpy.zeros", "numpy.vstack", "numpy.hstack"], "methods", ["None"], ["", "def", "get_data", "(", "self", ",", "folder", ",", "data_type", ")", ":", "\n", "        ", "\"\"\" Load one user's data, based on load_myo_data.read_data() \"\"\"", "\n", "user_examples", "=", "[", "]", "\n", "user_labels", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "load_myo_data", ".", "number_of_classes", "*", "4", ")", ":", "\n", "            ", "data_file", "=", "os", ".", "path", ".", "join", "(", "folder", ",", "data_type", ",", "\"classe_{}.dat\"", ".", "format", "(", "i", ")", ")", "\n", "data_read_from_file", "=", "np", ".", "fromfile", "(", "data_file", ",", "dtype", "=", "np", ".", "int16", ")", "\n", "data_read_from_file", "=", "np", ".", "array", "(", "data_read_from_file", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "dataset_example", "=", "load_myo_data", ".", "format_data_to_train", "(", "data_read_from_file", ")", "\n", "labels", "=", "(", "i", "%", "load_myo_data", ".", "number_of_classes", ")", "+", "np", ".", "zeros", "(", "dataset_example", ".", "shape", "[", "0", "]", ")", "\n", "\n", "# print(\"{}:\".format(i),", "\n", "#     \"class\", i%load_myo_data.number_of_classes,", "\n", "#     \"iter\", i//load_myo_data.number_of_classes,", "\n", "#     \"-\", dataset_example.shape, labels.shape)", "\n", "\n", "user_examples", ".", "append", "(", "dataset_example", ")", "\n", "user_labels", ".", "append", "(", "labels", ")", "\n", "\n", "", "user_examples", ",", "user_labels", "=", "load_myo_data", ".", "shift_electrodes", "(", "user_examples", ",", "user_labels", ")", "\n", "\n", "# Convert from list of examples to one matrix", "\n", "user_examples", "=", "np", ".", "vstack", "(", "user_examples", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "user_labels", "=", "np", ".", "hstack", "(", "user_labels", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n", "# Remove extra 1 dimension, e.g. [examples, 1, features, time_steps]", "\n", "user_examples", "=", "np", ".", "squeeze", "(", "user_examples", ",", "axis", "=", "1", ")", "\n", "\n", "# Transpose from [examples, features, time_steps] to", "\n", "# [examples, time_steps, features]", "\n", "user_examples", "=", "np", ".", "transpose", "(", "user_examples", ",", "axes", "=", "[", "0", ",", "2", ",", "1", "]", ")", "\n", "\n", "return", "user_examples", ",", "user_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Myo.load": [[930, 1024], ["os.path.join", "os.path.join", "os.path.join", "range", "range", "range", "range", "numpy.vstack().astype", "numpy.hstack().astype", "datasets.Myo.get_data", "data_train.append", "labels_train.append", "numpy.vstack().astype", "numpy.hstack().astype", "numpy.array", "numpy.array", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "datasets.Myo.get_data", "datasets.Myo.get_data", "numpy.vstack().astype", "numpy.hstack().astype", "data_test.append", "labels_test.append", "numpy.vstack", "numpy.hstack", "len", "len", "numpy.vstack", "numpy.hstack", "numpy.vstack", "numpy.hstack"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProBase.get_data", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProBase.get_data", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProBase.get_data"], ["", "def", "load", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Load desired participants' data\n\n        We combine pre-training and evaluation datasets since we're doing\n        unsupervised domain adaptation rather than supervised domain adaptation\n        (i.e., pre-training and then fine-tuning with some target-domain labeled\n        data like they did)\n\n        Numbering:\n            0 - PreTrainingDataset/Female0\n            1 - PreTrainingDataset/Female1\n            ...\n            9 - PreTrainingDataset/Female9\n\n            10 - PreTrainingDataset/Male0\n            11 - PreTrainingDataset/Male1\n            ...\n            21 - PreTrainingDataset/Male11\n\n            22 - EvaluationDataset/Female0\n            23 - EvaluationDataset/Female1\n\n            24 - EvaluationDataset/Male0\n            25 - EvaluationDataset/Male1\n            ...\n            39 - EvaluationDataset/Male15\n\n        All the pre-training datasets are included in training data, i.e. those\n        test sets are empty. For evaluation datasets, the \"training0\" data is\n        the train set and the \"Test0\" and \"Test1\" are combined as the test set.\n\n        Each training set is later further split into train/valid when creating\n        the tfrecord files.\n\n        Note: only use 22-39 for the target domains!\n        \"\"\"", "\n", "# Locations of data, within git submodule", "\n", "data_folder", "=", "os", ".", "path", ".", "join", "(", "\"datasets\"", ",", "\"MyoArmbandDataset\"", ")", "\n", "pre_folder", "=", "os", ".", "path", ".", "join", "(", "data_folder", ",", "\"PreTrainingDataset\"", ")", "\n", "eval_folder", "=", "os", ".", "path", ".", "join", "(", "data_folder", ",", "\"EvaluationDataset\"", ")", "\n", "\n", "# Get (folder, is_evaluation) pair from the user id", "\n", "folder_map", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "0", ",", "9", "+", "1", ")", ":", "\n", "            ", "folder_map", "[", "i", "]", "=", "(", "os", ".", "path", ".", "join", "(", "pre_folder", ",", "\"Female{}\"", ".", "format", "(", "i", ")", ")", ",", "False", ")", "\n", "", "for", "i", "in", "range", "(", "10", ",", "21", "+", "1", ")", ":", "\n", "            ", "folder_map", "[", "i", "]", "=", "(", "os", ".", "path", ".", "join", "(", "pre_folder", ",", "\"Male{}\"", ".", "format", "(", "i", "-", "10", ")", ")", ",", "False", ")", "\n", "", "for", "i", "in", "range", "(", "22", ",", "23", "+", "1", ")", ":", "\n", "            ", "folder_map", "[", "i", "]", "=", "(", "os", ".", "path", ".", "join", "(", "eval_folder", ",", "\"Female{}\"", ".", "format", "(", "i", "-", "22", ")", ")", ",", "True", ")", "\n", "", "for", "i", "in", "range", "(", "24", ",", "39", "+", "1", ")", ":", "\n", "            ", "folder_map", "[", "i", "]", "=", "(", "os", ".", "path", ".", "join", "(", "eval_folder", ",", "\"Male{}\"", ".", "format", "(", "i", "-", "24", ")", ")", ",", "True", ")", "\n", "\n", "", "data_train", "=", "[", "]", "\n", "labels_train", "=", "[", "]", "\n", "data_test", "=", "[", "]", "\n", "labels_test", "=", "[", "]", "\n", "\n", "for", "user", "in", "self", ".", "users", ":", "\n", "            ", "folder", ",", "is_evaluation", "=", "folder_map", "[", "user", "]", "\n", "user_data_train", ",", "user_labels_train", "=", "self", ".", "get_data", "(", "folder", ",", "\"training0\"", ")", "\n", "\n", "user_data_test", "=", "None", "\n", "user_labels_test", "=", "None", "\n", "\n", "if", "is_evaluation", ":", "\n", "                ", "user_data_test0", ",", "user_labels_test0", "=", "self", ".", "get_data", "(", "folder", ",", "\"Test0\"", ")", "\n", "user_data_test1", ",", "user_labels_test1", "=", "self", ".", "get_data", "(", "folder", ",", "\"Test1\"", ")", "\n", "user_data_test", "=", "np", ".", "vstack", "(", "[", "user_data_test0", ",", "user_data_test1", "]", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "user_labels_test", "=", "np", ".", "hstack", "(", "[", "user_labels_test0", ",", "user_labels_test1", "]", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n", "", "data_train", ".", "append", "(", "user_data_train", ")", "\n", "labels_train", ".", "append", "(", "user_labels_train", ")", "\n", "\n", "if", "user_data_test", "is", "not", "None", ":", "\n", "                ", "data_test", ".", "append", "(", "user_data_test", ")", "\n", "labels_test", ".", "append", "(", "user_labels_test", ")", "\n", "\n", "", "", "x_train", "=", "np", ".", "vstack", "(", "data_train", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "y_train", "=", "np", ".", "hstack", "(", "labels_train", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n", "if", "len", "(", "data_test", ")", ">", "0", "and", "len", "(", "labels_test", ")", ">", "0", ":", "\n", "            ", "x_test", "=", "np", ".", "vstack", "(", "data_test", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "y_test", "=", "np", ".", "hstack", "(", "labels_test", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "", "else", ":", "\n", "            ", "x_test", "=", "np", ".", "array", "(", "[", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "y_test", "=", "np", ".", "array", "(", "[", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "# print(\"Selected data:\", self.users)", "\n", "# print(x_train.shape, y_train.shape)", "\n", "# print(x_test.shape, y_test.shape)", "\n", "\n", "# Only one modality", "\n", "", "return", "[", "x_train", "]", ",", "y_train", ",", "[", "x_test", "]", ",", "y_test", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProBase.__init__": [[1046, 1062], ["datasets.Dataset.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["def", "__init__", "(", "self", ",", "users", ",", "num_classes", ",", "class_labels", ",", "feature_names", ",", "\n", "window_size", ",", "window_overlap", ",", "which_db", ",", "which_ex", ",", "\n", "include_rest", "=", "False", ",", "label_subset", "=", "None", ",", "channel_subset", "=", "None", ",", "\n", "balance_rest", "=", "True", ",", "shift_electrodes", "=", "False", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "users", "=", "users", "\n", "self", ".", "which_db", "=", "which_db", "\n", "self", ".", "which_ex", "=", "which_ex", "\n", "self", ".", "include_rest", "=", "include_rest", "\n", "self", ".", "label_subset", "=", "label_subset", "\n", "self", ".", "channel_subset", "=", "channel_subset", "\n", "self", ".", "balance_rest", "=", "balance_rest", "\n", "self", ".", "shift_electrodes", "=", "shift_electrodes", "\n", "super", "(", ")", ".", "__init__", "(", "num_classes", ",", "class_labels", ",", "\n", "NinaProBase", ".", "num_modalities", ",", "\n", "window_size", ",", "window_overlap", ",", "\n", "feature_names", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProBase.download": [[1063, 1065], ["NotImplementedError"], "methods", ["None"], ["", "def", "download", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"must override download()\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProBase.archive_path": [[1066, 1082], ["template.format", "NotImplementedError"], "methods", ["None"], ["", "def", "archive_path", "(", "self", ",", "subject", ")", ":", "\n", "        ", "\"\"\" Get file path of the .mat file in the zip file for this user \"\"\"", "\n", "db", "=", "self", ".", "which_db", "\n", "ex", "=", "self", ".", "which_ex", "\n", "\n", "# The filenames are indexed starting at 1 not zero", "\n", "subject", "+=", "1", "\n", "\n", "if", "db", "==", "1", ":", "\n", "            ", "template", "=", "\"DB1_s{subject}/S{subject}_A1_E{exercise}.mat\"", "\n", "", "elif", "db", "==", "5", ":", "\n", "            ", "template", "=", "\"s{subject}/S{subject}_E{exercise}_A1.mat\"", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"currently only support NinaPro DB1 and DB5\"", ")", "\n", "\n", "", "return", "template", ".", "format", "(", "subject", "=", "subject", ",", "exercise", "=", "ex", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProBase.db_repetitions": [[1083, 1097], ["NotImplementedError"], "methods", ["None"], ["", "def", "db_repetitions", "(", "self", ")", ":", "\n", "        ", "db", "=", "self", ".", "which_db", "\n", "\n", "# Try to get around 80% / 20% to match what we do for the other datasets", "\n", "if", "db", "==", "1", ":", "\n", "            ", "train", "=", "[", "1", ",", "2", ",", "3", ",", "4", ",", "5", ",", "6", ",", "7", ",", "8", "]", "\n", "test", "=", "[", "9", ",", "10", "]", "\n", "", "elif", "db", "==", "5", ":", "\n", "            ", "train", "=", "[", "1", ",", "2", ",", "3", ",", "4", ",", "5", "]", "\n", "test", "=", "[", "6", "]", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"currently only support NinaPro DB1 and DB5\"", ")", "\n", "\n", "", "return", "train", ",", "test", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProBase.get_data": [[1098, 1269], ["enumerate", "numpy.vstack().astype", "numpy.hstack().astype", "archive.open", "scipy.io.loadmat", "numpy.unique", "isinstance", "all", "list", "numpy.mean().astype", "numpy.vstack().astype", "numpy.hstack().astype", "numpy.random.RandomState().permutation", "numpy.vstack().astype.append", "numpy.hstack().astype.append", "range", "load_myo_data.shift_electrodes", "range", "range", "numpy.where", "datasets.NinaProBase.create_windows_x", "len", "windows_per_label.append", "len", "len", "len", "len", "numpy.transpose", "numpy.expand_dims", "data_for_shift.append", "labels_for_shift.append", "len", "len", "len", "len", "len", "numpy.squeeze", "numpy.transpose", "numpy.vstack().astype.append", "numpy.hstack().astype.append", "numpy.vstack", "numpy.hstack", "isinstance", "numpy.squeeze", "len", "numpy.vstack().astype.append", "numpy.hstack().astype.append", "numpy.vstack().astype.append", "numpy.hstack().astype.append", "numpy.mean", "numpy.vstack", "numpy.hstack", "numpy.random.RandomState", "len", "numpy.logical_and"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.create_windows_x"], ["", "def", "get_data", "(", "self", ",", "archive", ",", "filename", ",", "repetitions", ",", "random_seed", "=", "42", ")", ":", "\n", "        ", "\"\"\" Open .mat file in zip file, then load contents, get desired data,\n        split into multiple sliding windows \"\"\"", "\n", "# Get data from .mat inside the .zip file", "\n", "with", "archive", ".", "open", "(", "filename", ")", "as", "fp", ":", "\n", "            ", "mat", "=", "scipy", ".", "io", ".", "loadmat", "(", "fp", ")", "\n", "\n", "xs", "=", "mat", "[", "\"emg\"", "]", "\n", "\n", "# Channel subset", "\n", "if", "self", ".", "channel_subset", "is", "not", "None", ":", "\n", "                ", "assert", "isinstance", "(", "self", ".", "channel_subset", ",", "tuple", ")", "and", "len", "(", "self", ".", "channel_subset", ")", "==", "2", ",", "\"channel_subset should be of the form (start_ch, end_ch)\"", "\n", "\n", "channel_start", ",", "channel_end", "=", "self", ".", "channel_subset", "\n", "# Select start to end, inclusive, thus end+1", "\n", "xs", "=", "xs", "[", ":", ",", "channel_start", ":", "channel_end", "+", "1", "]", "\n", "\n", "# They say restimulus is the \"the corrected stimulus, processed with", "\n", "# movement detection algorithms\" whereas stimulus is \"the original", "\n", "# label of the movement repeated by the subject.\" Thus, it sounds", "\n", "# like we should use restimulus since it's the \"correct\" one.", "\n", "", "ys", "=", "mat", "[", "\"restimulus\"", "]", "\n", "\n", "# Repetition, but use rerepetition similar to why we use restimulus", "\n", "reps", "=", "mat", "[", "\"rerepetition\"", "]", "\n", "\n", "# Set of labels - to verify our label_subset consists of valid", "\n", "# labels (if specified)", "\n", "label_set", "=", "np", ".", "unique", "(", "mat", "[", "\"restimulus\"", "]", ")", "\n", "\n", "# Load the x/y data for the desired set of repetitions", "\n", "", "data", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "windows_per_label", "=", "[", "]", "\n", "\n", "# We handle rest differently because otherwise there's way more \"rest\"", "\n", "# than the other classes", "\n", "data_rest", "=", "[", "]", "\n", "labels_rest", "=", "[", "]", "\n", "\n", "if", "self", ".", "label_subset", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "self", ".", "label_subset", ",", "list", ")", ",", "\"label_subset should be a list\"", "\n", "assert", "all", "(", "[", "\n", "label", "in", "label_set", "for", "label", "in", "self", ".", "label_subset", "\n", "]", ")", ",", "\"not all labels in label_subset are found in the file\"", "\n", "\n", "all_classes", "=", "self", ".", "label_subset", "\n", "", "else", ":", "\n", "            ", "all_classes", "=", "list", "(", "range", "(", "self", ".", "num_classes", ")", ")", "\n", "\n", "", "if", "self", ".", "include_rest", ":", "\n", "            ", "label_offset", "=", "0", "\n", "", "else", ":", "\n", "# If we're excluding rest, then make sure label_subset does not", "\n", "# contain 0", "\n", "            ", "assert", "self", ".", "label_subset", "is", "None", "or", "0", "not", "in", "self", ".", "label_subset", ",", "\"label_subset should not contain label 0 (rest) if include_rest=False\"", "\n", "\n", "label_offset", "=", "1", "\n", "\n", "# We use label to get from the file, but output with label_index", "\n", "# so our final labels are 0, 1, ..., num_classes-1 even if we take a", "\n", "# subset of the labels.", "\n", "", "for", "label_index", ",", "label", "in", "enumerate", "(", "all_classes", ")", ":", "\n", "            ", "windows_per_this_label", "=", "0", "\n", "\n", "for", "rep", "in", "repetitions", ":", "\n", "# Get just the data with this label for this repetition", "\n", "#", "\n", "# Note: we skip rest, so label 0 is the first movement not rest,", "\n", "# if not include_rest.", "\n", "                ", "wh", "=", "np", ".", "where", "(", "np", ".", "squeeze", "(", "np", ".", "logical_and", "(", "\n", "ys", "==", "label", "+", "label_offset", ",", "reps", "==", "rep", ")", ",", "axis", "=", "1", ")", ")", "\n", "x", "=", "xs", "[", "wh", "]", "\n", "\n", "# Within each repetition for each class, split into", "\n", "# overlapping windows. We don't have to worry about", "\n", "# train/test overlap since they use non-overlapping", "\n", "# repetitions.", "\n", "x", "=", "self", ".", "create_windows_x", "(", "x", ",", "self", ".", "window_size", ",", "self", ".", "window_overlap", ")", "\n", "y", "=", "[", "label_index", "]", "*", "len", "(", "x", ")", "\n", "\n", "windows_per_this_label", "+=", "len", "(", "x", ")", "\n", "\n", "if", "self", ".", "include_rest", "and", "self", ".", "balance_rest", "and", "label", "==", "0", ":", "\n", "                    ", "data_rest", ".", "append", "(", "x", ")", "\n", "labels_rest", ".", "append", "(", "y", ")", "\n", "", "else", ":", "\n", "                    ", "data", ".", "append", "(", "x", ")", "\n", "labels", ".", "append", "(", "y", ")", "\n", "\n", "# Exclude \"rest\" from this average, since we're using this to", "\n", "# balance the \"rest\" class data", "\n", "", "", "if", "self", ".", "include_rest", "and", "self", ".", "balance_rest", "and", "label", "!=", "0", ":", "\n", "                ", "windows_per_label", ".", "append", "(", "windows_per_this_label", ")", "\n", "\n", "# Take a subset of \"rest\" if balance_rest", "\n", "", "", "if", "self", ".", "include_rest", "and", "self", ".", "balance_rest", ":", "\n", "            ", "avg_windows_others", "=", "np", ".", "mean", "(", "windows_per_label", ")", ".", "astype", "(", "np", ".", "int32", ")", "\n", "# print(\"Taking only first\", avg_windows_others, \"of rest\")", "\n", "\n", "# Concat all labels/reps, otherwise if we take the subset first we", "\n", "# essentially get all the data", "\n", "data_rest", "=", "np", ".", "vstack", "(", "data_rest", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "labels_rest", "=", "np", ".", "hstack", "(", "labels_rest", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n", "# Shuffle both together; we don't want to always get just the first", "\n", "# \"rest\" instances. Also, make this repeatable.", "\n", "p", "=", "np", ".", "random", ".", "RandomState", "(", "seed", "=", "random_seed", ")", ".", "permutation", "(", "len", "(", "data_rest", ")", ")", "\n", "data_rest", "=", "data_rest", "[", "p", "]", "\n", "labels_rest", "=", "labels_rest", "[", "p", "]", "\n", "\n", "# Limit the number, also put at the beginning in case", "\n", "# shift_electrodes", "\n", "data", ".", "append", "(", "data_rest", "[", ":", "avg_windows_others", "]", ")", "\n", "labels", ".", "append", "(", "labels_rest", "[", ":", "avg_windows_others", "]", ")", "\n", "\n", "# Shift electrodes if desired -- *must* be the like-Myo dataset since", "\n", "# this assumes Myo dataset labels and channels", "\n", "", "if", "self", ".", "shift_electrodes", ":", "\n", "# Put into the format that load_myo_data.shift_electrodes assumes", "\n", "            ", "data_for_shift", "=", "[", "]", "\n", "labels_for_shift", "=", "[", "]", "\n", "assert", "len", "(", "data", ")", "==", "len", "(", "labels", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "# Transpose from [examples, time_steps, features] to", "\n", "# [examples, features, time_steps]", "\n", "                ", "x", "=", "np", ".", "transpose", "(", "data", "[", "i", "]", ",", "axes", "=", "[", "0", ",", "2", ",", "1", "]", ")", "\n", "\n", "# Add extra dimension: [examples, 1, features, time_steps]", "\n", "x", "=", "np", ".", "expand_dims", "(", "x", ",", "axis", "=", "1", ")", "\n", "\n", "data_for_shift", ".", "append", "(", "x", ")", "\n", "labels_for_shift", ".", "append", "(", "labels", "[", "i", "]", ")", "\n", "\n", "# print(\"{}:\".format(i), \"-\", x.shape, np.array(labels[i]).shape)", "\n", "\n", "# Perform electrode shift", "\n", "", "shifted_data", ",", "shifted_labels", "=", "load_myo_data", ".", "shift_electrodes", "(", "data_for_shift", ",", "labels_for_shift", ")", "\n", "\n", "# Put back in our format", "\n", "assert", "len", "(", "shifted_data", ")", "==", "len", "(", "shifted_labels", ")", "\n", "assert", "len", "(", "shifted_data", ")", "==", "len", "(", "data", ")", "\n", "data", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "shifted_data", ")", ")", ":", "\n", "# Remove extra 1 dimension, e.g. [examples, 1, features, time_steps]", "\n", "                ", "x", "=", "np", ".", "squeeze", "(", "shifted_data", "[", "i", "]", ",", "axis", "=", "1", ")", "\n", "\n", "# Transpose from [examples, features, time_steps] to", "\n", "# [examples, time_steps, features]", "\n", "x", "=", "np", ".", "transpose", "(", "x", ",", "axes", "=", "[", "0", ",", "2", ",", "1", "]", ")", "\n", "\n", "data", ".", "append", "(", "x", ")", "\n", "labels", ".", "append", "(", "shifted_labels", "[", "i", "]", ")", "\n", "\n", "", "", "data", "=", "np", ".", "vstack", "(", "data", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "labels", "=", "np", ".", "hstack", "(", "labels", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n", "# Check balancing", "\n", "# print(\"Balance\")", "\n", "# for i in range(self.num_classes):", "\n", "#     print(\"  Label {}:\".format(i), sum(labels == i))", "\n", "# print()", "\n", "\n", "return", "data", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProBase.get_data_set": [[1270, 1285], ["numpy.vstack().astype", "numpy.hstack().astype", "zipfile.ZipFile", "datasets.NinaProBase.get_data", "numpy.vstack().astype.append", "numpy.hstack().astype.append", "numpy.vstack", "numpy.hstack", "datasets.NinaProBase.archive_path"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProBase.get_data", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProBase.archive_path"], ["", "def", "get_data_set", "(", "self", ",", "files", ",", "repetitions", ")", ":", "\n", "        ", "data", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "\n", "for", "user", ",", "f", "in", "files", ":", "\n", "            ", "with", "zipfile", ".", "ZipFile", "(", "f", ",", "\"r\"", ")", "as", "archive", ":", "\n", "                ", "current_data", ",", "current_labels", "=", "self", ".", "get_data", "(", "archive", ",", "\n", "self", ".", "archive_path", "(", "user", ")", ",", "repetitions", ")", "\n", "data", ".", "append", "(", "current_data", ")", "\n", "labels", ".", "append", "(", "current_labels", ")", "\n", "\n", "", "", "data", "=", "np", ".", "vstack", "(", "data", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "labels", "=", "np", ".", "hstack", "(", "labels", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n", "return", "data", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProBase.load": [[1286, 1300], ["datasets.NinaProBase.download", "datasets.NinaProBase.db_repetitions", "datasets.NinaProBase.get_data_set", "datasets.NinaProBase.get_data_set"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProDB5BaseLower.download", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProBase.db_repetitions", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProBase.get_data_set", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProBase.get_data_set"], ["", "def", "load", "(", "self", ")", ":", "\n", "        ", "dataset_fp", "=", "self", ".", "download", "(", ")", "\n", "files", "=", "[", "(", "i", ",", "dataset_fp", "[", "i", "]", ")", "for", "i", "in", "self", ".", "users", "]", "\n", "\n", "# Get data from the .mat files for the desired user(s)", "\n", "train_repetitions", ",", "test_repetitions", "=", "self", ".", "db_repetitions", "(", ")", "\n", "train_data", ",", "train_labels", "=", "self", ".", "get_data_set", "(", "files", ",", "train_repetitions", ")", "\n", "test_data", ",", "test_labels", "=", "self", ".", "get_data_set", "(", "files", ",", "test_repetitions", ")", "\n", "\n", "# print(\"Train:\", train_data.shape, train_labels.shape)", "\n", "# print(\"Test:\", test_data.shape, test_labels.shape)", "\n", "\n", "# Only one modality", "\n", "return", "[", "train_data", "]", ",", "train_labels", ",", "[", "test_data", "]", ",", "test_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProExercise2Base.__init__": [[1331, 1338], ["datasets.NinaProBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["def", "__init__", "(", "self", ",", "users", ",", "feature_names", ",", "window_size", ",", "window_overlap", ",", "\n", "which_db", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "users", ",", "\n", "NinaProExercise2Base", ".", "num_classes", ",", "\n", "NinaProExercise2Base", ".", "class_labels", ",", "\n", "feature_names", ",", "window_size", ",", "window_overlap", ",", "which_db", ",", "\n", "which_ex", "=", "2", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProExercise2LikeMyo.__init__": [[1362, 1379], ["datasets.NinaProBase.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["def", "__init__", "(", "self", ",", "users", ",", "feature_names", ",", "window_size", ",", "window_overlap", ",", "\n", "which_db", ",", "shift_electrodes", "=", "True", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "# Note: for each exercise, the labels are given in Figure 2 on", "\n", "# https://www.nature.com/articles/sdata201453", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "users", ",", "\n", "NinaProExercise2LikeMyo", ".", "num_classes", ",", "\n", "NinaProExercise2LikeMyo", ".", "class_labels", ",", "\n", "feature_names", ",", "window_size", ",", "window_overlap", ",", "which_db", ",", "\n", "which_ex", "=", "2", ",", "include_rest", "=", "True", ",", "label_subset", "=", "[", "\n", "0", ",", "# Neutral", "\n", "15", ",", "# \"Wrist radial deviation\"", "\n", "13", ",", "# \"Wrist flexion\"", "\n", "16", ",", "# \"Wrist ulnar deviation\"", "\n", "14", ",", "# \"Wrist extension\"", "\n", "6", ",", "# \"Fingers flexed together in fist\" (hand closed?)", "\n", "5", ",", "# \"Abduction of all fingers\" (hand open?)", "\n", "]", ",", "shift_electrodes", "=", "shift_electrodes", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProDB5Base.__init__": [[1482, 1486], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["def", "__init__", "(", "self", ",", "users", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "users", ",", "NinaProDB5Base", ".", "feature_names", ",", "\n", "NinaProDB5Base", ".", "window_size", ",", "NinaProDB5Base", ".", "window_overlap", ",", "\n", "which_db", "=", "5", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProDB5Base.download": [[1487, 1493], ["datasets.NinaProDB5Base.download_dataset", "range"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.download_dataset"], ["", "def", "download", "(", "self", ")", ":", "\n", "        ", "dataset_fps", "=", "self", ".", "download_dataset", "(", "[", "\n", "(", "\"NinaPro_DB5_s{}.zip\"", ".", "format", "(", "i", ")", ",", "\"s{}.zip?download=1\"", ".", "format", "(", "i", ")", ")", "\n", "for", "i", "in", "range", "(", "1", ",", "10", "+", "1", ")", "\n", "]", ",", "\"https://zenodo.org/record/1000116/files/\"", ")", "\n", "return", "dataset_fps", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProDB5BaseLower.__init__": [[1511, 1516], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["def", "__init__", "(", "self", ",", "users", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "users", ",", "NinaProDB5BaseLower", ".", "feature_names", ",", "\n", "NinaProDB5BaseLower", ".", "window_size", ",", "NinaProDB5BaseLower", ".", "window_overlap", ",", "\n", "which_db", "=", "5", ",", "channel_subset", "=", "(", "8", ",", "15", ")", ",", "\n", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProDB5BaseLower.download": [[1517, 1523], ["datasets.NinaProDB5BaseLower.download_dataset", "range"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.Dataset.download_dataset"], ["", "def", "download", "(", "self", ")", ":", "\n", "        ", "dataset_fps", "=", "self", ".", "download_dataset", "(", "[", "\n", "(", "\"NinaPro_DB5_s{}.zip\"", ".", "format", "(", "i", ")", ",", "\"s{}.zip?download=1\"", ".", "format", "(", "i", ")", ")", "\n", "for", "i", "in", "range", "(", "1", ",", "10", "+", "1", ")", "\n", "]", ",", "\"https://zenodo.org/record/1000116/files/\"", ")", "\n", "return", "dataset_fps", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.NinaProDB5LikeMyoNoShift.__init__": [[1561, 1563], ["datasets.NinaProDB5BaseLower.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["    ", "def", "__init__", "(", "self", ",", "users", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "users", ",", "shift_electrodes", "=", "False", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__": [[1575, 1586], ["datasets.Dataset.__init__"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["def", "__init__", "(", "self", ",", "users", ",", "num_classes", ",", "class_labels", ",", "features", ",", "\n", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "users", "=", "users", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "num_classes", ",", "\n", "class_labels", ",", "\n", "MultivariateNormalSyntheticBase", ".", "num_modalities", ",", "\n", "MultivariateNormalSyntheticBase", ".", "window_size", ",", "\n", "MultivariateNormalSyntheticBase", ".", "window_overlap", ",", "\n", "features", ",", "\n", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.load_file": [[1587, 1631], ["numpy.array", "numpy.array", "all", "open", "len", "line.split", "int", "numpy.array.append", "numpy.array.append", "len", "value.split", "values.append", "values.append", "float", "float"], "methods", ["None"], ["", "def", "load_file", "(", "self", ",", "filename", ",", "time_series", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Load CSV files in UCR time-series data format but with semicolons\n        delimiting the features\n        Returns:\n            data - numpy array with data of shape (num_examples, time_steps, num_features)\n            labels - numpy array with labels of shape: (num_examples, 1)\n\n        Note: based on SyntheticDataBase()\n        \"\"\"", "\n", "with", "open", "(", "filename", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "data", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "\n", "for", "line", "in", "f", ":", "\n", "                ", "parts", "=", "line", ".", "split", "(", "\",\"", ")", "\n", "assert", "len", "(", "parts", ")", ">=", "2", ",", "\"must be at least a label and a data value\"", "\n", "label", "=", "int", "(", "parts", "[", "0", "]", ")", "\n", "values_str", "=", "parts", "[", "1", ":", "]", "\n", "values", "=", "[", "]", "\n", "\n", "for", "value", "in", "values_str", ":", "\n", "# If a time series, further split", "\n", "                    ", "if", "time_series", ":", "\n", "                        ", "features_str", "=", "value", ".", "split", "(", "\";\"", ")", "\n", "features", "=", "[", "float", "(", "v", ")", "for", "v", "in", "features_str", "]", "\n", "values", ".", "append", "(", "features", ")", "\n", "# Otherwise, there's just a few data values (i.e., x1 and x2)", "\n", "", "else", ":", "\n", "# If you want to visualize with", "\n", "# python -m datasets.view_datasets --source=mvn_n4_l3_inter1_intra1_0", "\n", "# values.append([float(value)])", "\n", "# For actual experiments", "\n", "                        ", "values", ".", "append", "(", "float", "(", "value", ")", ")", "\n", "\n", "", "", "labels", ".", "append", "(", "label", ")", "\n", "data", ".", "append", "(", "values", ")", "\n", "\n", "", "", "data", "=", "np", ".", "array", "(", "data", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "labels", "=", "np", ".", "array", "(", "labels", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "assert", "len", "(", "labels", ".", "shape", ")", "==", "1", ",", "\"incorrect label shape: not (n,)\"", "\n", "assert", "all", "(", "labels", ">=", "0", ")", ",", "\"labels should all be >= 0\"", "\n", "\n", "return", "data", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.load": [[1632, 1664], ["os.path.join", "os.path.join", "datasets.MultivariateNormalSyntheticBase.load_file", "datasets.MultivariateNormalSyntheticBase.load_file", "len"], "methods", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.load_file", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.load_file"], ["", "def", "load", "(", "self", ")", ":", "\n", "# Get filename", "\n", "        ", "assert", "len", "(", "self", ".", "users", ")", "==", "1", ",", "\"currently only support one user\"", "\n", "user", "=", "self", ".", "users", "[", "0", "]", "\n", "\n", "# Which synthetic data to use", "\n", "filename_base", "=", "\"normal_n{}_l{}_inter{}_intra{}{}\"", ".", "format", "(", "\n", "self", ".", "num_sources", ",", "self", ".", "num_classes", ",", "self", ".", "inter", ",", "self", ".", "intra", ",", "\n", "\"_\"", "+", "self", ".", "suffix", "if", "self", ".", "suffix", "is", "not", "None", "else", "\"\"", ")", "\n", "\n", "if", "self", ".", "raw", ":", "\n", "            ", "time_series", "=", "False", "\n", "", "else", ":", "\n", "            ", "filename_base", "+=", "\"_sine\"", "\n", "time_series", "=", "True", "\n", "\n", "# We set target to be user 0, since there's only one target", "\n", "", "if", "user", "==", "0", ":", "\n", "            ", "filename", "=", "filename_base", "+", "\"_t\"", "\n", "", "else", ":", "\n", "            ", "filename", "=", "filename_base", "+", "\"_s{}\"", ".", "format", "(", "user", "-", "1", ")", "\n", "\n", "", "data_path", "=", "\"datasets/synthetic\"", "# see synthetic_datasets.py", "\n", "train_filename", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "filename", "+", "\"_TRAIN\"", ")", "\n", "test_filename", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "filename", "+", "\"_TEST\"", ")", "\n", "\n", "# Load CSV-like files", "\n", "train_data", ",", "train_labels", "=", "self", ".", "load_file", "(", "train_filename", ",", "time_series", ")", "\n", "test_data", ",", "test_labels", "=", "self", ".", "load_file", "(", "test_filename", ",", "time_series", ")", "\n", "\n", "# Only one modality", "\n", "return", "[", "train_data", "]", ",", "train_labels", ",", "[", "test_data", "]", ",", "test_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.register_dataset": [[31, 41], ["None"], "function", ["None"], ["def", "register_dataset", "(", "name", ")", ":", "\n", "    ", "\"\"\" Add dataset to the list of datsets, e.g. add @register_dataset(\"name\")\n    before a class definition \"\"\"", "\n", "assert", "name", "not", "in", "list_of_datasets", ",", "\"duplicate dataset named \"", "+", "name", "\n", "\n", "def", "decorator", "(", "cls", ")", ":", "\n", "        ", "list_of_datasets", "[", "name", "]", "=", "cls", "\n", "return", "cls", "\n", "\n", "", "return", "decorator", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.get_dataset": [[43, 57], ["name.replace.replace", "NotImplementedError", "list_of_datasets.keys", "str", "datasets.list_datasets"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.list_datasets"], ["", "def", "get_dataset", "(", "name", ")", ":", "\n", "    ", "\"\"\" Based on the given name, get the correct dataset processor\n\n    Note: if not in this file of single-modality datasets, try the multi-modal\n    datasets to see if it's there. If not, error.\n    \"\"\"", "\n", "# Check single-modality datasets", "\n", "if", "name", "in", "list_of_datasets", ".", "keys", "(", ")", ":", "\n", "        ", "return", "list_of_datasets", "[", "name", "]", "\n", "\n", "# Check multi-modality datasets", "\n", "", "name", "=", "name", ".", "replace", "(", "\"mm_\"", ",", "\"\"", ")", "# denotes multi-modal", "\n", "raise", "NotImplementedError", "(", "\"Unknown dataset name \"", "+", "name", "\n", "+", "\", not in \"", "+", "str", "(", "list_datasets", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.get_dataset_users": [[59, 62], ["datasets.get_dataset"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.get_dataset"], ["", "def", "get_dataset_users", "(", "name", ")", ":", "\n", "    ", "\"\"\" Get list of users for a dataset \"\"\"", "\n", "return", "get_dataset", "(", "name", ")", ".", "users", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.get_dataset_target_users": [[64, 73], ["datasets.get_dataset", "hasattr"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.get_dataset"], ["", "def", "get_dataset_target_users", "(", "name", ")", ":", "\n", "    ", "\"\"\" Get list of target users for a dataset, if target_users doesn't exist\n    in the class, then it's the same as get_dataset_users() \"\"\"", "\n", "d", "=", "get_dataset", "(", "name", ")", "\n", "\n", "if", "hasattr", "(", "d", ",", "\"target_users\"", ")", "and", "d", ".", "target_users", "is", "not", "None", ":", "\n", "        ", "return", "d", ".", "target_users", "\n", "\n", "", "return", "d", ".", "users", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.get_dataset_modalities": [[75, 78], ["datasets.get_dataset"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.get_dataset"], ["", "def", "get_dataset_modalities", "(", "name", ")", ":", "\n", "    ", "\"\"\" Get number of modalities for a dataset \"\"\"", "\n", "return", "get_dataset", "(", "name", ")", ".", "num_modalities", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.call_dataset": [[80, 83], ["datasets.get_dataset"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.get_dataset"], ["", "def", "call_dataset", "(", "name", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Based on the given name, call the correct dataset processor \"\"\"", "\n", "return", "get_dataset", "(", "name", ")", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.list_datasets": [[85, 89], ["list", "list_of_datasets.keys"], "function", ["None"], ["", "def", "list_datasets", "(", ")", ":", "\n", "    ", "\"\"\" Returns list of all the available datasets -- both single-modality\n    and multi-modality\"\"\"", "\n", "return", "list", "(", "list_of_datasets", ".", "keys", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.list_datasets_single_modality": [[91, 94], ["list", "list_of_datasets.keys"], "function", ["None"], ["", "def", "list_datasets_single_modality", "(", ")", ":", "\n", "    ", "\"\"\" Returns list of all the available datasets (only single-modality) \"\"\"", "\n", "return", "list", "(", "list_of_datasets", ".", "keys", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.zero_to_n": [[96, 99], ["list", "range"], "function", ["None"], ["", "def", "zero_to_n", "(", "n", ")", ":", "\n", "    ", "\"\"\" Return [0, 1, 2, ..., n] \"\"\"", "\n", "return", "list", "(", "range", "(", "0", ",", "n", "+", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.one_to_n": [[101, 104], ["list", "range"], "function", ["None"], ["", "def", "one_to_n", "(", "n", ")", ":", "\n", "    ", "\"\"\" Return [1, 2, 3, ..., n] \"\"\"", "\n", "return", "list", "(", "range", "(", "1", ",", "n", "+", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.make_synthetic_datasets": [[1666, 1697], ["datasets.zero_to_n", "datasets.register_dataset", "super().__init__", "range"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.zero_to_n", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.register_dataset", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.MultivariateNormalSyntheticBase.__init__"], ["", "", "def", "make_synthetic_datasets", "(", "num_sources", ",", "num_labels", ",", "inter", ",", "intra", ",", "suffix", ",", "raw", "=", "False", ")", ":", "\n", "    ", "\"\"\" Reduces duplicate code, just with slight changes \"\"\"", "\n", "\n", "class", "MultivariateNormalSyntheticDataset", "(", "MultivariateNormalSyntheticBase", ")", ":", "\n", "# Changes", "\n", "        ", "num_classes", "=", "num_labels", "\n", "users", "=", "zero_to_n", "(", "num_sources", ")", "# 0 is target, 1..n is sources", "\n", "\n", "# Same for all variations", "\n", "feature_names", "=", "[", "[", "\"x\"", "]", "]", "\n", "class_labels", "=", "[", "\"class{}\"", ".", "format", "(", "i", ")", "for", "i", "in", "range", "(", "num_classes", ")", "]", "\n", "target_users", "=", "[", "0", "]", "# only allow this one target", "\n", "\n", "def", "__init__", "(", "self", ",", "users", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "            ", "self", ".", "num_sources", "=", "num_sources", "# same as above, exclude target", "\n", "self", ".", "inter", "=", "inter", "\n", "self", ".", "intra", "=", "intra", "\n", "self", ".", "suffix", "=", "suffix", "\n", "self", ".", "raw", "=", "raw", "\n", "super", "(", ")", ".", "__init__", "(", "users", ",", "\n", "MultivariateNormalSyntheticDataset", ".", "num_classes", ",", "\n", "MultivariateNormalSyntheticDataset", ".", "class_labels", ",", "\n", "MultivariateNormalSyntheticDataset", ".", "feature_names", ",", "\n", "**", "kwargs", ")", "\n", "\n", "", "", "dataset_name_suffix", "=", "\"\"", "if", "raw", "else", "\"_sine\"", "\n", "dataset", "=", "register_dataset", "(", "\"normal_n{}_l{}_inter{}_intra{}_{}{}\"", ".", "format", "(", "\n", "num_sources", ",", "num_labels", ",", "inter", ",", "intra", ",", "suffix", ",", "dataset_name_suffix", "\n", ")", ")", "(", "MultivariateNormalSyntheticDataset", ")", "\n", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.load": [[1717, 1737], ["datasets.list_datasets", "datasets.get_dataset_users", "NotImplementedError", "str", "datasets.get_dataset", "datasets.call_dataset"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.list_datasets", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.get_dataset_users", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.get_dataset", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.call_dataset"], ["", "def", "load", "(", "dataset_name_to_load", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Load a dataset based on the name (must be one of datasets.names()) \"\"\"", "\n", "dataset_class", "=", "None", "\n", "dataset_object", "=", "None", "\n", "\n", "# Go through list of valid datasets, create the one this matches", "\n", "for", "name", "in", "list_datasets", "(", ")", ":", "\n", "        ", "for", "user", "in", "get_dataset_users", "(", "name", ")", ":", "\n", "            ", "dataset_name", "=", "name", "+", "\"_\"", "+", "str", "(", "user", ")", "\n", "\n", "if", "dataset_name_to_load", "==", "dataset_name", ":", "\n", "                ", "dataset_class", "=", "get_dataset", "(", "name", ")", "\n", "dataset_object", "=", "call_dataset", "(", "name", ",", "users", "=", "[", "user", "]", ",", "\n", "*", "args", ",", "**", "kwargs", ")", "\n", "break", "\n", "\n", "", "", "", "if", "dataset_object", "is", "None", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"unknown dataset \"", "+", "dataset_name_to_load", ")", "\n", "\n", "", "return", "dataset_object", ",", "dataset_class", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.attributes": [[1740, 1757], ["datasets.list_datasets", "datasets.get_dataset_users", "str", "datasets.get_dataset"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.list_datasets", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.get_dataset_users", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.get_dataset"], ["", "def", "attributes", "(", "dataset_name_to_load", ")", ":", "\n", "    ", "\"\"\" Get num_classes, class_labels for dataset (must be one of datasets.names()) \"\"\"", "\n", "num_classes", "=", "None", "\n", "class_labels", "=", "None", "\n", "\n", "# Go through list of valid datasets, load attributes of the one this matches", "\n", "for", "name", "in", "list_datasets", "(", ")", ":", "\n", "        ", "for", "user", "in", "get_dataset_users", "(", "name", ")", ":", "\n", "            ", "dataset_name", "=", "name", "+", "\"_\"", "+", "str", "(", "user", ")", "\n", "\n", "if", "dataset_name_to_load", "==", "dataset_name", ":", "\n", "                ", "d", "=", "get_dataset", "(", "name", ")", "\n", "num_classes", "=", "d", ".", "num_classes", "\n", "class_labels", "=", "d", ".", "class_labels", "\n", "break", "\n", "\n", "", "", "", "return", "num_classes", ",", "class_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.names": [[1760, 1775], ["datasets.list_datasets_single_modality", "datasets.list_datasets", "datasets.get_dataset_users", "datasets.append", "str"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.list_datasets_single_modality", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.list_datasets", "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.get_dataset_users"], ["", "def", "names", "(", "single_modality", "=", "False", ")", ":", "\n", "    ", "\"\"\" Returns list of all the available datasets to load with\n    datasets.load(name) \"\"\"", "\n", "datasets", "=", "[", "]", "\n", "\n", "if", "single_modality", ":", "\n", "        ", "dataset_list", "=", "list_datasets_single_modality", "(", ")", "\n", "", "else", ":", "\n", "        ", "dataset_list", "=", "list_datasets", "(", ")", "\n", "\n", "", "for", "name", "in", "dataset_list", ":", "\n", "        ", "for", "user", "in", "get_dataset_users", "(", "name", ")", ":", "\n", "            ", "datasets", ".", "append", "(", "name", "+", "\"_\"", "+", "str", "(", "user", ")", ")", "\n", "\n", "", "", "return", "datasets", "\n", "\n"]], "home.repos.pwc.inspect_result.floft_calda.datasets.datasets.main": [[1777, 1785], ["datasets.load", "print", "print", "print", "print", "print"], "function", ["home.repos.pwc.inspect_result.floft_calda.datasets.datasets.load"], ["", "def", "main", "(", "argv", ")", ":", "\n", "    ", "sd", "=", "load", "(", "\"ucihar_1\"", ")", "\n", "\n", "print", "(", "\"Source dataset\"", ")", "\n", "print", "(", "sd", ".", "train_data", ",", "sd", ".", "train_labels", ")", "\n", "print", "(", "sd", ".", "train_data", ".", "shape", ",", "sd", ".", "train_labels", ".", "shape", ")", "\n", "print", "(", "sd", ".", "test_data", ",", "sd", ".", "test_labels", ")", "\n", "print", "(", "sd", ".", "test_data", ".", "shape", ",", "sd", ".", "test_labels", ".", "shape", ")", "\n", "\n"]]}