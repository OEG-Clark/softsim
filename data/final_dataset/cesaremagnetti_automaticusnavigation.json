{"home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.make_XCAT_volumes_realistic.ContentLoss.__init__": [[28, 38], ["torch.Module.__init__", "isinstance", "target.detach", "t.detach"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "target", ")", ":", "\n", "        ", "super", "(", "ContentLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# we 'detach' the target content from the tree used", "\n", "# to dynamically compute the gradient: this is a stated value,", "\n", "# not a variable. Otherwise the forward method of the criterion", "\n", "# will throw an error.", "\n", "if", "isinstance", "(", "target", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "self", ".", "targets", "=", "[", "t", ".", "detach", "(", ")", "for", "t", "in", "target", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "targets", "=", "target", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.make_XCAT_volumes_realistic.ContentLoss.forward": [[39, 47], ["isinstance", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "self", ".", "loss", "=", "0", "\n", "if", "isinstance", "(", "self", ".", "targets", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "for", "target", "in", "self", ".", "targets", ":", "\n", "                ", "self", ".", "loss", "+=", "F", ".", "mse_loss", "(", "input", ",", "target", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "loss", "=", "F", ".", "mse_loss", "(", "input", ",", "self", ".", "targets", ")", "\n", "", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.make_XCAT_volumes_realistic.StyleLoss.__init__": [[59, 62], ["torch.Module.__init__", "gram_matrix().detach", "make_XCAT_volumes_realistic.gram_matrix"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.make_XCAT_volumes_realistic.gram_matrix"], ["    ", "def", "__init__", "(", "self", ",", "target_features", ")", ":", "\n", "        ", "super", "(", "StyleLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "targets", "=", "[", "gram_matrix", "(", "target_feature", ")", ".", "detach", "(", ")", "for", "target_feature", "in", "target_features", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.make_XCAT_volumes_realistic.StyleLoss.forward": [[63, 69], ["make_XCAT_volumes_realistic.gram_matrix", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.make_XCAT_volumes_realistic.gram_matrix"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "G", "=", "gram_matrix", "(", "input", ")", "\n", "self", ".", "loss", "=", "0", "\n", "for", "target", "in", "self", ".", "targets", ":", "\n", "            ", "self", ".", "loss", "+=", "F", ".", "mse_loss", "(", "G", ",", "target", ")", "\n", "", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.make_XCAT_volumes_realistic.Normalization.__init__": [[77, 84], ["torch.Module.__init__", "mean.detach().clone().view", "std.detach().clone().view", "mean.detach().clone", "std.detach().clone", "mean.detach", "std.detach"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "mean", ",", "std", ")", ":", "\n", "        ", "super", "(", "Normalization", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# .view the mean and std to make them [C x 1 x 1] so that they can", "\n", "# directly work with image Tensor of shape [B x C x H x W].", "\n", "# B is batch size. C is number of channels. H is height and W is width.", "\n", "self", ".", "mean", "=", "mean", ".", "detach", "(", ")", ".", "clone", "(", ")", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", "\n", "self", ".", "std", "=", "std", ".", "detach", "(", ")", ".", "clone", "(", ")", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.make_XCAT_volumes_realistic.Normalization.forward": [[85, 88], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "img", ")", ":", "\n", "# normalize img", "\n", "        ", "return", "(", "img", "-", "self", ".", "mean", ")", "/", "self", ".", "std", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.make_XCAT_volumes_realistic.AddGaussianNoise.__init__": [[240, 243], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "mean", "=", "0.", ",", "std", "=", "1.", ")", ":", "\n", "        ", "self", ".", "std", "=", "std", "\n", "self", ".", "mean", "=", "mean", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.make_XCAT_volumes_realistic.AddGaussianNoise.__call__": [[244, 246], ["torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "tensor.size"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "return", "tensor", "+", "torch", ".", "randn", "(", "tensor", ".", "size", "(", ")", ")", "*", "self", ".", "std", "+", "self", ".", "mean", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.make_XCAT_volumes_realistic.AddGaussianNoise.__repr__": [[247, 249], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'(mean={0}, std={1})'", ".", "format", "(", "self", ".", "mean", ",", "self", ".", "std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.make_XCAT_volumes_realistic.gram_matrix": [[48, 57], ["input.size", "input.view", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm.div", "input.view.t"], "function", ["None"], ["", "", "def", "gram_matrix", "(", "input", ")", ":", "\n", "    ", "a", ",", "b", ",", "c", ",", "d", "=", "input", ".", "size", "(", ")", "# a=batch size(=1)", "\n", "# b=number of feature maps", "\n", "# (c,d)=dimensions of a f. map (N=c*d)", "\n", "features", "=", "input", ".", "view", "(", "a", "*", "b", ",", "c", "*", "d", ")", "# resise F_XL into \\hat F_XL", "\n", "G", "=", "torch", ".", "mm", "(", "features", ",", "features", ".", "t", "(", ")", ")", "# compute the gram product", "\n", "# we 'normalize' the values of the gram matrix", "\n", "# by dividing by the number of element in each feature maps.", "\n", "return", "G", ".", "div", "(", "a", "*", "b", "*", "c", "*", "d", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.make_XCAT_volumes_realistic.get_style_model_and_losses": [[93, 151], ["copy.deepcopy", "Normalization().to", "torch.Sequential", "copy.deepcopy.children", "range", "isinstance", "nn.Sequential.add_module", "make_XCAT_volumes_realistic.Normalization", "isinstance", "make_XCAT_volumes_realistic.ContentLoss", "nn.Sequential.add_module", "content_losses.append", "make_XCAT_volumes_realistic.StyleLoss", "nn.Sequential.add_module", "style_losses.append", "len", "isinstance", "isinstance", "torch.ReLU", "isinstance", "nn.Sequential.detach", "nn.Sequential.detach", "isinstance", "RuntimeError", "nn.Sequential.", "nn.Sequential."], "function", ["None"], ["def", "get_style_model_and_losses", "(", "cnn", ",", "normalization_mean", ",", "normalization_std", ",", "style_imgs", ",", "content_imgs", ",", "content_layers", "=", "content_layers_default", ",", "style_layers", "=", "style_layers_default", ")", ":", "\n", "    ", "cnn", "=", "copy", ".", "deepcopy", "(", "cnn", ")", "\n", "\n", "# normalization module", "\n", "normalization", "=", "Normalization", "(", "normalization_mean", ",", "normalization_std", ")", ".", "to", "(", "device", ")", "\n", "\n", "# just in order to have an iterable access to or list of content/syle", "\n", "# losses", "\n", "content_losses", "=", "[", "]", "\n", "style_losses", "=", "[", "]", "\n", "\n", "# assuming that cnn is a nn.Sequential, so we make a new nn.Sequential", "\n", "# to put in modules that are supposed to be activated sequentially", "\n", "model", "=", "nn", ".", "Sequential", "(", "normalization", ")", "\n", "\n", "i", "=", "0", "# increment every time we see a conv", "\n", "for", "layer", "in", "cnn", ".", "children", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "layer", ",", "nn", ".", "Conv2d", ")", ":", "\n", "            ", "i", "+=", "1", "\n", "name", "=", "'conv_{}'", ".", "format", "(", "i", ")", "\n", "", "elif", "isinstance", "(", "layer", ",", "nn", ".", "ReLU", ")", ":", "\n", "            ", "name", "=", "'relu_{}'", ".", "format", "(", "i", ")", "\n", "# The in-place version doesn't play very nicely with the ContentLoss", "\n", "# and StyleLoss we insert below. So we replace with out-of-place", "\n", "# ones here.", "\n", "layer", "=", "nn", ".", "ReLU", "(", "inplace", "=", "False", ")", "\n", "", "elif", "isinstance", "(", "layer", ",", "nn", ".", "MaxPool2d", ")", ":", "\n", "            ", "name", "=", "'pool_{}'", ".", "format", "(", "i", ")", "\n", "", "elif", "isinstance", "(", "layer", ",", "nn", ".", "BatchNorm2d", ")", ":", "\n", "            ", "name", "=", "'bn_{}'", ".", "format", "(", "i", ")", "\n", "", "else", ":", "\n", "            ", "raise", "RuntimeError", "(", "'Unrecognized layer: {}'", ".", "format", "(", "layer", ".", "__class__", ".", "__name__", ")", ")", "\n", "\n", "", "model", ".", "add_module", "(", "name", ",", "layer", ")", "\n", "\n", "if", "name", "in", "content_layers", ":", "\n", "# add content loss:", "\n", "            ", "target", "=", "[", "model", "(", "content_img", ")", ".", "detach", "(", ")", "for", "content_img", "in", "content_imgs", "]", "\n", "#target = model(content_img).detach(", "\n", "content_loss", "=", "ContentLoss", "(", "target", ")", "\n", "model", ".", "add_module", "(", "\"content_loss_{}\"", ".", "format", "(", "i", ")", ",", "content_loss", ")", "\n", "content_losses", ".", "append", "(", "content_loss", ")", "\n", "\n", "", "if", "name", "in", "style_layers", ":", "\n", "# add style loss:", "\n", "            ", "target_features", "=", "[", "model", "(", "style_img", ")", ".", "detach", "(", ")", "for", "style_img", "in", "style_imgs", "]", "\n", "style_loss", "=", "StyleLoss", "(", "target_features", ")", "\n", "model", ".", "add_module", "(", "\"style_loss_{}\"", ".", "format", "(", "i", ")", ",", "style_loss", ")", "\n", "style_losses", ".", "append", "(", "style_loss", ")", "\n", "\n", "# now we trim off the layers after the last content and style losses", "\n", "", "", "for", "i", "in", "range", "(", "len", "(", "model", ")", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "        ", "if", "isinstance", "(", "model", "[", "i", "]", ",", "ContentLoss", ")", "or", "isinstance", "(", "model", "[", "i", "]", ",", "StyleLoss", ")", ":", "\n", "            ", "break", "\n", "\n", "", "", "model", "=", "model", "[", ":", "(", "i", "+", "1", ")", "]", "\n", "\n", "return", "model", ",", "style_losses", ",", "content_losses", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.make_XCAT_volumes_realistic.get_input_optimizer": [[152, 156], ["torch.LBFGS", "input_img.requires_grad_"], "function", ["None"], ["", "def", "get_input_optimizer", "(", "input_img", ")", ":", "\n", "# this line to show that input is a parameter that requires a gradient", "\n", "    ", "optimizer", "=", "optim", ".", "LBFGS", "(", "[", "input_img", ".", "requires_grad_", "(", ")", "]", ")", "\n", "return", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.make_XCAT_volumes_realistic.run_style_transfer": [[157, 202], ["make_XCAT_volumes_realistic.get_style_model_and_losses", "make_XCAT_volumes_realistic.get_input_optimizer", "input_img.data.clamp_", "get_input_optimizer.step", "input_img.data.clamp_", "get_input_optimizer.zero_grad", "model", "loss.backward"], "function", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.make_XCAT_volumes_realistic.get_style_model_and_losses", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.make_XCAT_volumes_realistic.get_input_optimizer", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.trainers.PrioritizedDoubleDeepQLearning.step"], ["", "def", "run_style_transfer", "(", "cnn", ",", "normalization_mean", ",", "normalization_std", ",", "content_img", ",", "style_img", ",", "input_img", ",", "num_steps", "=", "300", ",", "style_weight", "=", "100000", ",", "content_weight", "=", "1", ")", ":", "\n", "    ", "\"\"\"Run the style transfer.\"\"\"", "\n", "\n", "model", ",", "style_losses", ",", "content_losses", "=", "get_style_model_and_losses", "(", "cnn", ",", "normalization_mean", ",", "normalization_std", ",", "style_img", ",", "content_img", ")", "\n", "\n", "optimizer", "=", "get_input_optimizer", "(", "input_img", ")", "\n", "\n", "run", "=", "[", "0", "]", "\n", "while", "run", "[", "0", "]", "<=", "num_steps", ":", "\n", "\n", "        ", "def", "closure", "(", ")", ":", "\n", "# correct the values of updated input image", "\n", "            ", "input_img", ".", "data", ".", "clamp_", "(", "0", ",", "1", ")", "\n", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "model", "(", "input_img", ")", "\n", "style_score", "=", "0", "\n", "content_score", "=", "0", "\n", "\n", "for", "sl", "in", "style_losses", ":", "\n", "                ", "style_score", "+=", "sl", ".", "loss", "\n", "", "for", "cl", "in", "content_losses", ":", "\n", "                ", "content_score", "+=", "cl", ".", "loss", "\n", "\n", "", "style_score", "*=", "style_weight", "\n", "content_score", "*=", "content_weight", "\n", "\n", "loss", "=", "style_score", "+", "content_score", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "run", "[", "0", "]", "+=", "1", "\n", "# if run[0] % 50 == 0:", "\n", "#     print(\"run {}:\".format(run))", "\n", "#     print('Style Loss : {:4f} Content Loss: {:4f}'.format(", "\n", "#         style_score.item(), content_score.item()))", "\n", "#     print()", "\n", "\n", "return", "style_score", "+", "content_score", "\n", "\n", "", "optimizer", ".", "step", "(", "closure", ")", "\n", "\n", "# a last correction...", "\n", "", "input_img", ".", "data", ".", "clamp_", "(", "0", ",", "1", ")", "\n", "\n", "return", "input_img", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.make_XCAT_volumes_realistic.intensity_scaling": [[205, 216], ["ndarr.min", "ndarr.max"], "function", ["None"], ["", "def", "intensity_scaling", "(", "ndarr", ",", "pmin", "=", "None", ",", "pmax", "=", "None", ",", "nmin", "=", "None", ",", "nmax", "=", "None", ")", ":", "\n", "    ", "pmin", "=", "pmin", "if", "pmin", "!=", "None", "else", "ndarr", ".", "min", "(", ")", "\n", "pmax", "=", "pmax", "if", "pmax", "!=", "None", "else", "ndarr", ".", "max", "(", ")", "\n", "nmin", "=", "nmin", "if", "nmin", "!=", "None", "else", "pmin", "\n", "nmax", "=", "nmax", "if", "nmax", "!=", "None", "else", "pmax", "\n", "\n", "ndarr", "[", "ndarr", "<", "pmin", "]", "=", "pmin", "\n", "ndarr", "[", "ndarr", ">", "pmax", "]", "=", "pmax", "\n", "ndarr", "=", "(", "ndarr", "-", "pmin", ")", "/", "(", "pmax", "-", "pmin", ")", "\n", "ndarr", "=", "ndarr", "*", "(", "nmax", "-", "nmin", ")", "+", "nmin", "\n", "return", "ndarr", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.make_XCAT_volumes_realistic.load": [[217, 237], ["SimpleITK.ReadImage", "sitk.ReadImage.GetSpacing", "SimpleITK.GetArrayFromImage", "make_XCAT_volumes_realistic.intensity_scaling", "Volume.astype.astype", "SimpleITK.ReadImage", "SimpleITK.GetArrayFromImage", "Volume.astype.max", "Volume[].flatten", "Volume[].flatten"], "function", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.intensity_scaling"], ["", "def", "load", "(", "filename", ",", "pmin", ",", "pmax", ",", "nmin", ",", "nmax", ")", ":", "\n", "# load CT volume", "\n", "    ", "itkVolume", "=", "sitk", ".", "ReadImage", "(", "filename", "+", "\"_1_CT.nii.gz\"", ")", "\n", "Spacing", "=", "itkVolume", ".", "GetSpacing", "(", ")", "\n", "Volume", "=", "sitk", ".", "GetArrayFromImage", "(", "itkVolume", ")", "\n", "# preprocess volume", "\n", "Volume", "=", "Volume", "/", "Volume", ".", "max", "(", ")", "*", "255", "\n", "Volume", "=", "intensity_scaling", "(", "Volume", ",", "pmin", "=", "pmin", ",", "pmax", "=", "pmax", ",", "nmin", "=", "nmin", ",", "nmax", "=", "nmax", ")", "\n", "Volume", "=", "Volume", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "\n", "# load queried CT segmentation", "\n", "itkSegmentation", "=", "sitk", ".", "ReadImage", "(", "filename", "+", "\"_1_SEG.nii.gz\"", ")", "\n", "Seg", "=", "sitk", ".", "GetArrayFromImage", "(", "itkSegmentation", ")", "\n", "\n", "# dampen myocardium", "\n", "m_int", "=", "(", "Volume", "[", "Seg", "==", "2884", "]", ".", "flatten", "(", ")", "[", "0", "]", "+", "Volume", "[", "Seg", "==", "2889", "]", ".", "flatten", "(", ")", "[", "0", "]", ")", "/", "2", "\n", "Volume", "[", "Seg", "==", "2884", "]", "=", "m_int", "#bg", "\n", "Volume", "[", "Seg", "==", "2889", "]", "=", "m_int", "#mc", "\n", "Volume", "[", "Seg", "==", "4", "]", "=", "m_int", "#mc", "\n", "return", "Volume", ",", "Spacing", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.make_XCAT_volumes_realistic.NST_Volume": [[254, 292], ["tqdm.tqdm", "range", "make_XCAT_volumes_realistic.run_style_transfer", "output.clamp().cpu().detach().numpy().squeeze.clamp().cpu().detach().numpy().squeeze", "planes[].squeeze", "numpy.hstack", "matplotlib.imsave", "noise1().unsqueeze().to", "noise().unsqueeze().to", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "os.path.exists", "os.makedirs", "planes[].data.size", "output.clamp().cpu().detach().numpy().squeeze.clamp().cpu().detach().numpy", "range", "noise1().unsqueeze", "noise().unsqueeze", "range", "output.clamp().cpu().detach().numpy().squeeze.clamp().cpu().detach", "min", "range", "noise1", "noise", "max", "min", "totensor", "totensor", "output.clamp().cpu().detach().numpy().squeeze.clamp().cpu", "max", "min", "max", "output.clamp().cpu().detach().numpy().squeeze.clamp"], "function", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.make_XCAT_volumes_realistic.run_style_transfer"], ["def", "NST_Volume", "(", "Volume", ",", "style_img", ",", "main_ax", ",", "init", "=", "\"content\"", ",", "window", "=", "2", ")", ":", "\n", "# 1. roll volume axis as queried (to slice volume along x,y or z)", "\n", "    ", "sx", ",", "sy", ",", "sz", "=", "Volume", ".", "shape", "\n", "# 2. loop through all slices along main_Axis, transfer the style and update slices in the Volume", "\n", "for", "i", "in", "tqdm", "(", "range", "(", "sx", ")", ",", "\"transfering volume ...\"", ")", ":", "\n", "        ", "if", "main_ax", "==", "0", ":", "\n", "            ", "planes", "=", "[", "Volume", "[", "min", "(", "max", "(", "i", "+", "w", ",", "0", ")", ",", "sx", "-", "1", ")", ",", ":", ",", ":", "]", "for", "w", "in", "range", "(", "-", "window", ",", "window", "+", "1", ")", "]", "\n", "", "elif", "main_ax", "==", "1", ":", "\n", "            ", "planes", "=", "[", "Volume", "[", ":", ",", "min", "(", "max", "(", "i", "+", "w", ",", "0", ")", ",", "sy", "-", "1", ")", ",", ":", "]", "for", "w", "in", "range", "(", "-", "window", ",", "window", "+", "1", ")", "]", "\n", "", "elif", "main_ax", "==", "2", ":", "\n", "            ", "planes", "=", "[", "Volume", "[", ":", ",", ":", ",", "min", "(", "max", "(", "i", "+", "w", ",", "0", ")", ",", "sz", "-", "1", ")", "]", "for", "w", "in", "range", "(", "-", "window", ",", "window", "+", "1", ")", "]", "\n", "# 2.1 setup content and input images to tensor already sets image in (0,1) range", "\n", "", "content_img", "=", "[", "noise1", "(", "totensor", "(", "p", ")", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "device", ",", "torch", ".", "float", ")", "for", "p", "in", "planes", "]", "\n", "#content_img = noise1(totensor(current_plane)).unsqueeze(0).to(device, torch.float)", "\n", "if", "init", "==", "\"content\"", ":", "\n", "            ", "input_img", "=", "noise", "(", "totensor", "(", "planes", "[", "window", "]", ")", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "device", ",", "torch", ".", "float", ")", "\n", "", "else", ":", "\n", "            ", "input_img", "=", "torch", ".", "randn", "(", "planes", "[", "window", "]", ".", "data", ".", "size", "(", ")", ",", "device", "=", "device", ")", "\n", "# 2.2 run style transfer on this plane", "\n", "", "output", "=", "run_style_transfer", "(", "cnn", ",", "cnn_normalization_mean", ",", "cnn_normalization_std", ",", "content_img", ",", "style_img", ",", "input_img", ")", "\n", "output", "=", "output", ".", "clamp", "(", "0", ",", "1", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ".", "squeeze", "(", ")", "\n", "output", "=", "(", "output", "*", "255", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "img", "=", "planes", "[", "window", "]", ".", "squeeze", "(", ")", "\n", "img", "=", "np", ".", "hstack", "(", "[", "img", ",", "output", "]", ")", "\n", "# save as we go to inspect (DELETE AFTER)", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "\"./temp_transferred_slices\"", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "\"./temp_transferred_slices\"", ")", "\n", "", "plt", ".", "imsave", "(", "\"./temp_transferred_slices/sample{}_ax{}.png\"", ".", "format", "(", "i", ",", "main_ax", ")", ",", "img", ",", "cmap", "=", "\"Greys_r\"", ")", "\n", "# 2.3 store output image in the volume (as type uint8)", "\n", "if", "main_ax", "==", "0", ":", "\n", "            ", "Volume", "[", "i", ",", ":", ",", ":", "]", "=", "output", "\n", "", "elif", "main_ax", "==", "1", ":", "\n", "            ", "Volume", "[", ":", ",", "i", ",", ":", "]", "=", "output", "\n", "", "elif", "main_ax", "==", "2", ":", "\n", "            ", "Volume", "[", ":", ",", ":", ",", "i", "]", "=", "output", "\n", "\n", "# 3. roll the axis back to its origin form", "\n", "", "", "return", "Volume", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.test_old.HandlerLineImage.__init__": [[18, 23], ["matplotlib.legend_handler.HandlerBase.__init__"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "img_arr", ",", "space", "=", "15", ",", "offset", "=", "10", ")", ":", "\n", "        ", "self", ".", "space", "=", "space", "\n", "self", ".", "offset", "=", "offset", "\n", "self", ".", "image_data", "=", "img_arr", "\n", "super", "(", "HandlerLineImage", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.test_old.HandlerLineImage.create_artists": [[24, 45], ["matplotlib.lines.Line2D", "matplotlib.lines.Line2D.update_from", "matplotlib.lines.Line2D.set_clip_on", "matplotlib.lines.Line2D.set_transform", "matplotlib.transforms.Bbox.from_bounds", "matplotlib.transforms.TransformedBbox", "matplotlib.image.BboxImage", "matplotlib.image.BboxImage.set_data", "matplotlib.image.BboxImage.set_cmap", "test_old.HandlerLineImage.update_prop"], "methods", ["None"], ["", "def", "create_artists", "(", "self", ",", "legend", ",", "orig_handle", ",", "\n", "xdescent", ",", "ydescent", ",", "width", ",", "height", ",", "fontsize", ",", "trans", ")", ":", "\n", "\n", "        ", "l", "=", "matplotlib", ".", "lines", ".", "Line2D", "(", "[", "xdescent", "+", "self", ".", "offset", ",", "xdescent", "+", "(", "width", "-", "self", ".", "space", ")", "/", "3.", "+", "self", ".", "offset", "]", ",", "\n", "[", "ydescent", "+", "height", "/", "2.", ",", "ydescent", "+", "height", "/", "2.", "]", ")", "\n", "l", ".", "update_from", "(", "orig_handle", ")", "\n", "l", ".", "set_clip_on", "(", "False", ")", "\n", "l", ".", "set_transform", "(", "trans", ")", "\n", "\n", "bb", "=", "Bbox", ".", "from_bounds", "(", "xdescent", "+", "(", "width", "+", "self", ".", "space", ")", "/", "3.", "+", "self", ".", "offset", ",", "\n", "ydescent", ",", "\n", "height", "*", "self", ".", "image_data", ".", "shape", "[", "1", "]", "/", "self", ".", "image_data", ".", "shape", "[", "0", "]", ",", "\n", "height", ")", "\n", "\n", "tbb", "=", "TransformedBbox", "(", "bb", ",", "trans", ")", "\n", "image", "=", "BboxImage", "(", "tbb", ")", "\n", "image", ".", "set_data", "(", "self", ".", "image_data", ")", "\n", "image", ".", "set_cmap", "(", "\"Greys_r\"", ")", "\n", "\n", "self", ".", "update_prop", "(", "image", ",", "orig_handle", ",", "legend", ")", "\n", "return", "[", "l", ",", "image", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.utils.train": [[15, 94], ["utils.setup_environment", "agent.agent.MultiVolumeAgent", "utils.setup_optimizer", "utils.setup_criterion", "utils.setup_buffers", "visualisation.visualizers.Visualizer", "wandb.init", "wandb.watch", "tqdm.tqdm", "torch.rand", "torch.rand", "torch.rand", "torch.onnx.export", "torch.onnx.export", "torch.onnx.export", "wandb.save", "enumerate", "wandb.login", "range", "agent.agent.MultiVolumeAgent.play_episode", "agent.agent.MultiVolumeAgent.train", "local_model.save", "torch.save", "torch.save", "torch.save", "setup_buffers.items", "torch.rand.float().to", "os.path.join", "os.path.join", "zip", "print", "env.random_walk", "wandb.Settings", "wandb.log", "os.path.join", "setup_optimizer.state_dict", "os.path.join", "buffer.save", "numpy.random.choice", "agent.agent.MultiVolumeAgent.test_agent", "agent.test_agent.items", "wandb.save", "setup_environment.values", "setup_buffers.values", "int", "int", "max", "os.path.join", "print", "local_model.save", "torch.save", "torch.save", "torch.save", "setup_buffers.items", "config.volume_ids.split", "wandb.log", "os.path.join", "torch.rand.float", "len", "int", "max", "os.path.join", "setup_optimizer.state_dict", "os.path.join", "buffer.save", "visualisation.visualizers.Visualizer.render_frames", "visualisation.visualizers.Visualizer.render_frames", "len", "int", "os.path.join", "len"], "function", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.utils.setup_environment", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.utils.setup_optimizer", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.utils.setup_criterion", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.utils.setup_buffers", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.save", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.baseAgent.BaseAgent.play_episode", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.baseAgent.BaseAgent.train", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.save", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.save", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.save", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.save", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.random_walk", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.save", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.agent.MultiVolumeAgent.test_agent", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.save", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.save", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.save", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.save", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.save", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.save", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.visualisation.visualizers.Visualizer.render_frames", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.visualisation.visualizers.Visualizer.render_frames"], ["def", "train", "(", "config", ",", "local_model", ",", "target_model", ",", "name", ",", "wandb_entity", "=", "\"us_navigation\"", ",", "sweep", "=", "False", ")", ":", "\n", "        ", "\"\"\" Trains an agent on an input environment, given networks/optimizers and training criterions.\n        Params:\n        ==========\n                config (argparse object): configuration with all training options. (see options/options.py)\n                local_model (PyTorch model): pytorch network that will be trained using a particular training routine (i.e. DQN)\n                                             (if more processes, the Qnetwork is shared)\n                target_model (PyTorch model): pytorch network that will be used as a target to estimate future Qvalues. \n                                              (it is a hard copy or a running average of the local model, helps against diverging)\n                name (str): experiments name\n                wandb_entuty (str): which wandb workspace to save logs to. (if unsure use your main workspace i.e. your-user-name)\n                sweep (bool): flag if we are performing a sweep, in which case we will not be saving checkpoints as that will occupy too much memory.\n                              However we will still save the final model in .onnx format (only intermediate .pth checkpoints are not saved)\n        \"\"\"", "\n", "# ==== instanciate useful classes ==== ", "\n", "# 1. instanciate environment(s)", "\n", "envs", "=", "setup_environment", "(", "config", ")", "\n", "# 2. instanciate agent", "\n", "agent", "=", "MultiVolumeAgent", "(", "config", ")", "\n", "# 3. instanciate optimizer for local_network", "\n", "optimizer", "=", "setup_optimizer", "(", "config", ",", "local_model", ")", "\n", "# 4. instanciate criterion", "\n", "criterion", "=", "setup_criterion", "(", "config", ")", "\n", "# 5. instanciate replay buffer(s) (one per environment)", "\n", "buffers", "=", "setup_buffers", "(", "config", ")", "\n", "# 6. instanciate visualizer", "\n", "visualizer", "=", "Visualizer", "(", "agent", ".", "results_dir", ")", "\n", "\n", "# ==== LAUNCH TRAINING ====", "\n", "# 1. launch exploring steps if needed", "\n", "if", "agent", ".", "config", ".", "exploring_steps", ">", "0", ":", "\n", "                ", "for", "idx", ",", "(", "env", ",", "buffer", ")", "in", "enumerate", "(", "zip", "(", "envs", ".", "values", "(", ")", ",", "buffers", ".", "values", "(", ")", ")", ",", "1", ")", ":", "\n", "                    ", "print", "(", "\"[{}]/[{}] random walk to collect experience...\"", ".", "format", "(", "idx", ",", "len", "(", "envs", ")", ")", ")", "\n", "env", ".", "random_walk", "(", "int", "(", "config", ".", "exploring_steps", "/", "len", "(", "envs", ")", ")", ",", "buffer", ")", "\n", "# 2. initialize wandb for logging purposes", "\n", "", "", "if", "config", ".", "wandb", "in", "[", "\"online\"", ",", "\"offline\"", "]", ":", "\n", "                ", "wandb", ".", "login", "(", ")", "\n", "", "wandb", ".", "init", "(", "entity", "=", "wandb_entity", ",", "config", "=", "config", ",", "mode", "=", "config", ".", "wandb", ",", "name", "=", "config", ".", "name", ",", "settings", "=", "wandb", ".", "Settings", "(", "start_method", "=", "\"fork\"", ")", ")", "\n", "config", "=", "wandb", ".", "config", "# oddly this ensures wandb works smoothly", "\n", "# 3. tell wandb to watch what the model gets up to: gradients, weights, and loss", "\n", "wandb", ".", "watch", "(", "local_model", ",", "criterion", ",", "log", "=", "\"all\"", ",", "log_freq", "=", "config", ".", "log_freq", ")", "\n", "# 4. start training", "\n", "for", "episode", "in", "tqdm", "(", "range", "(", "config", ".", "starting_episode", "+", "1", ",", "config", ".", "n_episodes", "+", "1", ")", ",", "desc", "=", "\"playing episode...\"", ")", ":", "\n", "                ", "logs", "=", "agent", ".", "play_episode", "(", "envs", ",", "local_model", ",", "buffers", ")", "\n", "logs", "[", "\"loss\"", "]", "=", "agent", ".", "train", "(", "envs", ",", "local_model", ",", "target_model", ",", "optimizer", ",", "criterion", ",", "buffers", ",", "\n", "n_iter", "=", "int", "(", "config", ".", "n_steps_per_episode", "/", "config", ".", "update_every", ")", ")", "\n", "# send logs to weights and biases", "\n", "if", "episode", "%", "max", "(", "1", ",", "int", "(", "config", ".", "log_freq", "/", "len", "(", "envs", ")", ")", ")", "==", "0", ":", "\n", "                        ", "wandb", ".", "log", "(", "logs", ",", "commit", "=", "True", ")", "\n", "# save agent locally and test its current greedy policy", "\n", "", "local_model", ".", "save", "(", "os", ".", "path", ".", "join", "(", "agent", ".", "checkpoints_dir", ",", "\"latest.pth\"", ")", ")", "\n", "torch", ".", "save", "(", "optimizer", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "agent", ".", "checkpoints_dir", ",", "\"latest_optimizer.pth\"", ")", ")", "\n", "for", "vol_id", ",", "buffer", "in", "buffers", ".", "items", "(", ")", ":", "\n", "                    ", "buffer", ".", "save", "(", "os", ".", "path", ".", "join", "(", "config", ".", "checkpoints_dir", ",", "config", ".", "name", ",", "\"latest_{}_\"", ".", "format", "(", "vol_id", ")", ")", ")", "\n", "", "if", "episode", "%", "max", "(", "1", ",", "int", "(", "config", ".", "save_freq", ")", ")", "==", "0", "or", "episode", "==", "1", ":", "\n", "                        ", "if", "not", "sweep", ":", "\n", "                            ", "print", "(", "\"saving model, optimizer and buffer...\"", ")", "\n", "local_model", ".", "save", "(", "os", ".", "path", ".", "join", "(", "agent", ".", "checkpoints_dir", ",", "\"episode%d.pth\"", "%", "episode", ")", ")", "\n", "torch", ".", "save", "(", "optimizer", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "agent", ".", "checkpoints_dir", ",", "\"episode%d_optimizer.pth\"", "%", "episode", ")", ")", "\n", "for", "vol_id", ",", "buffer", "in", "buffers", ".", "items", "(", ")", ":", "\n", "                                ", "buffer", ".", "save", "(", "os", ".", "path", ".", "join", "(", "config", ".", "checkpoints_dir", ",", "config", ".", "name", ",", "\"episode{}_{}_\"", ".", "format", "(", "episode", ",", "vol_id", ")", ")", ")", "\n", "# test the greedy policy on a random environment and send logs to wandb", "\n", "", "", "test_env_id", "=", "np", ".", "random", ".", "choice", "(", "config", ".", "volume_ids", ".", "split", "(", "\",\"", ")", ")", "\n", "out", "=", "agent", ".", "test_agent", "(", "config", ".", "n_steps_per_episode", ",", "{", "test_env_id", ":", "envs", "[", "test_env_id", "]", "}", ",", "local_model", ")", "\n", "for", "_", ",", "log", "in", "out", ".", "items", "(", ")", ":", "\n", "                            ", "wandb", ".", "log", "(", "log", "[", "\"wandb\"", "]", ",", "commit", "=", "True", ")", "\n", "# animate the trajectory followed by the agent in the current episode", "\n", "if", "agent", ".", "config", ".", "CT2US", ":", "\n", "                                ", "visualizer", ".", "render_frames", "(", "log", "[", "\"planes\"", "]", ",", "log", "[", "\"planesCT\"", "]", ",", "n_rows", "=", "2", "if", "agent", ".", "config", ".", "location_aware", "else", "1", ",", "fname", "=", "\"episode%d.gif\"", "%", "episode", ")", "\n", "", "else", ":", "\n", "                                ", "visualizer", ".", "render_frames", "(", "log", "[", "\"planes\"", "]", ",", "n_rows", "=", "2", "if", "agent", ".", "config", ".", "location_aware", "else", "1", ",", "fname", "=", "\"episode%d.gif\"", "%", "episode", ")", "\n", "# upload file to wandb", "\n", "", "", "wandb", ".", "save", "(", "os", ".", "path", ".", "join", "(", "visualizer", ".", "savedir", ",", "\"episode%d.gif\"", "%", "episode", ")", ")", "\n", "# at the end of the training session save the model as .onnx to improve the open sourceness and exchange-ability amongst different ML frameworks", "\n", "", "", "nchannels", "=", "1", "if", "not", "config", ".", "location_aware", "else", "4", "\n", "sample_inputs", "=", "torch", ".", "rand", "(", "agent", ".", "config", ".", "batch_size", ",", "nchannels", ",", "agent", ".", "config", ".", "load_size", ",", "agent", ".", "config", ".", "load_size", ")", "\n", "torch", ".", "onnx", ".", "export", "(", "local_model", ",", "sample_inputs", ".", "float", "(", ")", ".", "to", "(", "agent", ".", "config", ".", "device", ")", ",", "os", ".", "path", ".", "join", "(", "agent", ".", "checkpoints_dir", ",", "\"DQN.onnx\"", ")", ")", "\n", "# upload file to wandb", "\n", "wandb", ".", "save", "(", "os", ".", "path", ".", "join", "(", "agent", ".", "checkpoints_dir", ",", "\"DQN.onnx\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.utils.setup_environment": [[95, 110], ["config.volume_ids.split", "envs[].set_reward", "environment.xcatEnvironment.SingleVolumeEnvironment", "environment.CT2USenvironment.CT2USSingleVolumeEnvironment", "environment.xcatEnvironment.LocationAwareSingleVolumeEnvironment", "environment.CT2USenvironment.LocationAwareCT2USSingleVolumeEnvironment"], "function", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.SingleVolumeEnvironment.set_reward"], ["", "def", "setup_environment", "(", "config", ")", ":", "\n", "    ", "envs", "=", "{", "}", "\n", "for", "vol_id", "in", "config", ".", "volume_ids", ".", "split", "(", "','", ")", ":", "\n", "        ", "if", "not", "config", ".", "location_aware", "and", "not", "config", ".", "CT2US", ":", "\n", "            ", "envs", "[", "vol_id", "]", "=", "SingleVolumeEnvironment", "(", "config", ",", "vol_id", "=", "vol_id", ")", "\n", "", "elif", "not", "config", ".", "location_aware", "and", "config", ".", "CT2US", ":", "\n", "            ", "envs", "[", "vol_id", "]", "=", "CT2USSingleVolumeEnvironment", "(", "config", ",", "vol_id", "=", "vol_id", ")", "\n", "", "elif", "config", ".", "location_aware", "and", "not", "config", ".", "CT2US", ":", "\n", "            ", "envs", "[", "vol_id", "]", "=", "LocationAwareSingleVolumeEnvironment", "(", "config", ",", "vol_id", "=", "vol_id", ")", "\n", "", "else", ":", "\n", "            ", "envs", "[", "vol_id", "]", "=", "LocationAwareCT2USSingleVolumeEnvironment", "(", "config", ",", "vol_id", "=", "vol_id", ")", "\n", "# start reward function of each agent based on the input config parsed", "\n", "", "", "for", "key", "in", "envs", ":", "\n", "        ", "envs", "[", "key", "]", ".", "set_reward", "(", ")", "\n", "", "return", "envs", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.utils.setup_optimizer": [[111, 123], ["torch.Adam", "local_model.parameters", "print", "torch.load", "torch.load", "torch.load", "optim.Adam.load_state_dict", "os.path.join"], "function", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.load", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.load", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.load"], ["", "def", "setup_optimizer", "(", "config", ",", "local_model", ")", ":", "\n", "    ", "optimizer", "=", "optim", ".", "Adam", "(", "local_model", ".", "parameters", "(", ")", ",", "lr", "=", "config", ".", "learning_rate", ")", "\n", "# load optimizer if needed", "\n", "if", "config", ".", "load", "is", "not", "None", ":", "\n", "        ", "if", "config", ".", "load_name", "is", "not", "None", ":", "\n", "            ", "load_name", "=", "config", ".", "load_name", "\n", "", "else", ":", "\n", "            ", "load_name", "=", "config", ".", "name", "\n", "", "print", "(", "\"loading {}/{} optimizer ...\"", ".", "format", "(", "load_name", ",", "config", ".", "load", ")", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "config", ".", "checkpoints_dir", ",", "load_name", ",", "config", ".", "load", "+", "\"_optimizer.pth\"", ")", ",", "map_location", "=", "'cpu'", ")", "\n", "optimizer", ".", "load_state_dict", "(", "state_dict", ")", "\n", "", "return", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.utils.setup_buffers": [[124, 142], ["config.volume_ids.split", "print", "buffers.items", "buffer.buffer.RecurrentPrioritizedReplayBuffer", "buffer.buffer.PrioritizedReplayBuffer", "buffer.load", "os.path.join"], "function", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.load"], ["", "def", "setup_buffers", "(", "config", ")", ":", "\n", "# instanciate buffers", "\n", "    ", "buffers", "=", "{", "}", "\n", "for", "vol_id", "in", "config", ".", "volume_ids", ".", "split", "(", "','", ")", ":", "\n", "        ", "if", "config", ".", "recurrent", ":", "\n", "            ", "buffers", "[", "vol_id", "]", "=", "RecurrentPrioritizedReplayBuffer", "(", "config", ".", "buffer_size", ",", "config", ".", "batch_size", ",", "config", ".", "alpha", ",", "config", ".", "recurrent_history_len", ")", "\n", "", "else", ":", "\n", "            ", "buffers", "[", "vol_id", "]", "=", "PrioritizedReplayBuffer", "(", "config", ".", "buffer_size", ",", "config", ".", "batch_size", ",", "config", ".", "alpha", ")", "\n", "# load buffers if needed", "\n", "", "", "if", "config", ".", "load", "is", "not", "None", ":", "\n", "        ", "if", "config", ".", "load_name", "is", "not", "None", ":", "\n", "            ", "load_name", "=", "config", ".", "load_name", "\n", "", "else", ":", "\n", "            ", "load_name", "=", "config", ".", "name", "\n", "", "print", "(", "\"loading {}/{} buffers ...\"", ".", "format", "(", "load_name", ",", "config", ".", "load", ")", ")", "\n", "for", "vol_id", ",", "buffer", "in", "buffers", ".", "items", "(", ")", ":", "\n", "            ", "buffer", ".", "load", "(", "os", ".", "path", ".", "join", "(", "config", ".", "checkpoints_dir", ",", "load_name", ",", "\"{}_{}_\"", ".", "format", "(", "config", ".", "load", ",", "vol_id", ")", ")", ")", "\n", "", "", "return", "buffers", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.None.utils.setup_criterion": [[143, 151], ["config.loss.lower", "torch.MSELoss", "config.loss.lower", "torch.SmoothL1Loss", "ValueError"], "function", ["None"], ["", "def", "setup_criterion", "(", "config", ")", ":", "\n", "    ", "if", "\"mse\"", "in", "config", ".", "loss", ".", "lower", "(", ")", ":", "\n", "        ", "criterion", "=", "nn", ".", "MSELoss", "(", "reduction", "=", "'none'", ")", "\n", "", "elif", "\"smooth\"", "in", "config", ".", "loss", ".", "lower", "(", ")", ":", "\n", "        ", "criterion", "=", "nn", ".", "SmoothL1Loss", "(", "reduction", "=", "'none'", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", ")", "\n", "", "return", "criterion", "", "", ""]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.ReplayBuffer.__init__": [[8, 19], ["collections.deque"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "buffer_size", ",", "batch_size", ")", ":", "\n", "        ", "\"\"\"Initialize a ReplayBuffer object.\n        Params\n        ======\n            buffer_size (int): capacity of the replay buffer\n            batch_size (int): batch size sampled each time we call self.sample()\n\n        \"\"\"", "\n", "self", ".", "buffer_size", "=", "buffer_size", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "memory", "=", "deque", "(", "maxlen", "=", "buffer_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.ReplayBuffer.add": [[20, 23], ["buffer.ReplayBuffer.memory.append"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "transition", ")", ":", "\n", "        ", "\"\"\"Add a new experience to memory.\"\"\"", "\n", "self", ".", "memory", ".", "append", "(", "transition", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.ReplayBuffer.sample": [[24, 33], ["numpy.random.choice", "zip", "len"], "methods", ["None"], ["", "def", "sample", "(", "self", ")", ":", "\n", "        ", "\"\"\"Randomly sample a batch of experiences from memory.\"\"\"", "\n", "# 2. sample experiences", "\n", "indices", "=", "np", ".", "random", ".", "choice", "(", "len", "(", "self", ".", "memory", ")", ",", "size", "=", "self", ".", "batch_size", ",", "replace", "=", "True", ")", "\n", "experiences", "=", "[", "self", ".", "memory", "[", "i", "]", "for", "i", "in", "indices", "]", "# random lookup in a deque is more efficient", "\n", "# experiences = np.random.choice(self.memory, size=self.batch_size, replace=False)", "\n", "# reorganize batch", "\n", "batch", "=", "zip", "(", "*", "experiences", ")", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.ReplayBuffer.save": [[34, 36], ["pickle.dump", "open"], "methods", ["None"], ["", "def", "save", "(", "self", ",", "fname", ")", ":", "\n", "        ", "pickle", ".", "dump", "(", "self", ".", "memory", ",", "open", "(", "'{}_buffer.pkl'", ".", "format", "(", "fname", ")", ",", "'wb'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.ReplayBuffer.load": [[37, 39], ["pickle.load", "open"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.load"], ["", "def", "load", "(", "self", ",", "fname", ")", ":", "\n", "        ", "self", ".", "memory", "=", "pickle", ".", "load", "(", "open", "(", "'{}_buffer.pkl'", ".", "format", "(", "fname", ")", ",", "'rb'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.ReplayBuffer.__len__": [[40, 43], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the current size of internal memory.\"\"\"", "\n", "return", "len", "(", "self", ".", "memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.PrioritizedReplayBuffer.__init__": [[46, 50], ["buffer.ReplayBuffer.__init__", "collections.deque"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "buffer_size", ",", "batch_size", ",", "prob_alpha", "=", "0.6", ")", ":", "\n", "        ", "super", "(", "PrioritizedReplayBuffer", ",", "self", ")", ".", "__init__", "(", "buffer_size", ",", "batch_size", ")", "\n", "self", ".", "prob_alpha", "=", "prob_alpha", "\n", "self", ".", "priorities", "=", "deque", "(", "maxlen", "=", "buffer_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.PrioritizedReplayBuffer.add": [[51, 57], ["buffer.ReplayBuffer.add", "buffer.PrioritizedReplayBuffer.priorities.append", "max", "len"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.RecurrentPrioritizedReplayBuffer.add"], ["", "def", "add", "(", "self", ",", "transition", ")", ":", "\n", "# 1. add to experience", "\n", "        ", "super", "(", "PrioritizedReplayBuffer", ",", "self", ")", ".", "add", "(", "transition", ")", "\n", "# 2. add to buffer with max probability to incentivize exploration of new transitions", "\n", "max_prio", "=", "max", "(", "self", ".", "priorities", ")", "if", "len", "(", "self", ".", "priorities", ")", ">", "0", "else", "1.0", "\n", "self", ".", "priorities", ".", "append", "(", "max_prio", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.PrioritizedReplayBuffer.sample": [[58, 73], ["numpy.array", "numpy.array.sum", "numpy.random.choice", "zip", "len", "len", "numpy.array.min"], "methods", ["None"], ["", "def", "sample", "(", "self", ",", "beta", "=", "0.4", ")", ":", "\n", "# 1. get prioritized distribution", "\n", "        ", "probs", "=", "np", ".", "array", "(", "self", ".", "priorities", ")", "\n", "probs", "=", "probs", "**", "self", ".", "prob_alpha", "\n", "probs", "/=", "probs", ".", "sum", "(", ")", "\n", "# 2. sample experiences", "\n", "indices", "=", "np", ".", "random", ".", "choice", "(", "len", "(", "self", ".", "memory", ")", ",", "size", "=", "self", ".", "batch_size", ",", "replace", "=", "True", ",", "p", "=", "probs", ")", "\n", "experiences", "=", "[", "self", ".", "memory", "[", "i", "]", "for", "i", "in", "indices", "]", "# random lookup in a deque is more efficient", "\n", "# 3. reorganize batch", "\n", "batch", "=", "zip", "(", "*", "experiences", ")", "\n", "# 3. sample weights for bias correction of the gradient", "\n", "N", "=", "len", "(", "self", ".", "memory", ")", "\n", "max_weight", "=", "(", "probs", ".", "min", "(", ")", "*", "N", ")", "**", "(", "-", "beta", ")", "\n", "weights", "=", "(", "(", "N", "*", "probs", "[", "indices", "]", ")", "**", "(", "-", "beta", ")", ")", "/", "max_weight", "\n", "return", "batch", ",", "weights", ",", "indices", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.PrioritizedReplayBuffer.update_priorities": [[74, 77], ["zip"], "methods", ["None"], ["", "def", "update_priorities", "(", "self", ",", "batch_indices", ",", "batch_priorities", ",", "eps", "=", "10e-5", ")", ":", "\n", "        ", "for", "idx", ",", "prio", "in", "zip", "(", "batch_indices", ",", "batch_priorities", ")", ":", "\n", "            ", "self", ".", "priorities", "[", "idx", "]", "=", "prio", "+", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.PrioritizedReplayBuffer.save": [[78, 81], ["pickle.dump", "pickle.dump", "open", "open"], "methods", ["None"], ["", "", "def", "save", "(", "self", ",", "fname", ")", ":", "\n", "        ", "pickle", ".", "dump", "(", "self", ".", "memory", ",", "open", "(", "'{}_buffer.pkl'", ".", "format", "(", "fname", ")", ",", "'wb'", ")", ")", "\n", "pickle", ".", "dump", "(", "self", ".", "priorities", ",", "open", "(", "'{}_priorities.pkl'", ".", "format", "(", "fname", ")", ",", "'wb'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.PrioritizedReplayBuffer.load": [[82, 85], ["pickle.load", "pickle.load", "open", "open"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.load", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.load"], ["", "def", "load", "(", "self", ",", "fname", ")", ":", "\n", "        ", "self", ".", "memory", "=", "pickle", ".", "load", "(", "open", "(", "'{}_buffer.pkl'", ".", "format", "(", "fname", ")", ",", "'rb'", ")", ")", "\n", "self", ".", "priorities", "=", "pickle", ".", "load", "(", "open", "(", "'{}_priorities.pkl'", ".", "format", "(", "fname", ")", ",", "'rb'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.RecurrentPrioritizedReplayBuffer.__init__": [[92, 99], ["buffer.PrioritizedReplayBuffer.__init__", "collections.deque"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__"], ["def", "__init__", "(", "self", ",", "buffer_size", ",", "batch_size", ",", "prob_alpha", "=", "0.6", ",", "history_length", "=", "10", ")", ":", "\n", "        ", "super", "(", "RecurrentPrioritizedReplayBuffer", ",", "self", ")", ".", "__init__", "(", "buffer_size", ",", "batch_size", ",", "prob_alpha", ")", "\n", "self", ".", "history_length", "=", "history_length", "-", "1", "#we want the full input to be of length history length, so we subtract 1", "\n", "# lookback will guide you to a particular entry in self.memory to recover useful information (state and next_state in our case)", "\n", "self", ".", "lookback", "=", "deque", "(", "maxlen", "=", "buffer_size", ")", "\n", "# we use the flag -1 to indicate a first time-step (episode changed) we do not want to lookback overlapping episodes", "\n", "self", ".", "lookup_index", "=", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.RecurrentPrioritizedReplayBuffer.add": [[100, 107], ["buffer.PrioritizedReplayBuffer.add", "buffer.RecurrentPrioritizedReplayBuffer.lookback.append"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.RecurrentPrioritizedReplayBuffer.add"], ["", "def", "add", "(", "self", ",", "transition", ",", "is_first_time_step", ")", ":", "\n", "        ", "super", "(", "RecurrentPrioritizedReplayBuffer", ",", "self", ")", ".", "add", "(", "transition", ")", "\n", "# add lookback index (signal first time-step with a -1 so we don't cross over episodes)", "\n", "if", "is_first_time_step", ":", "\n", "            ", "self", ".", "lookup_index", "=", "-", "1", "\n", "", "self", ".", "lookback", ".", "append", "(", "self", ".", "lookup_index", ")", "\n", "self", ".", "lookup_index", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.RecurrentPrioritizedReplayBuffer.sample": [[108, 132], ["buffer.PrioritizedReplayBuffer.sample", "enumerate", "range"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.RecurrentPrioritizedReplayBuffer.sample"], ["", "def", "sample", "(", "self", ",", "beta", "=", "0.4", ")", ":", "\n", "# sample normally", "\n", "        ", "batch", ",", "weights", ",", "indices", "=", "super", "(", "RecurrentPrioritizedReplayBuffer", ",", "self", ")", ".", "sample", "(", "beta", ")", "\n", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "dones", "=", "batch", "\n", "# lookup to previous time step for the history of states and next_states for each batch index", "\n", "batch_states", ",", "batch_next_states", "=", "[", "]", ",", "[", "]", "\n", "for", "i", ",", "index", "in", "enumerate", "(", "indices", ")", ":", "\n", "# initialize state next_state history with the sampled state and next state", "\n", "            ", "states_new", ",", "next_states_new", "=", "[", "states", "[", "i", "]", "]", ",", "[", "next_states", "[", "i", "]", "]", "\n", "# perform n lookback steps to get previous states and next states", "\n", "lookback_index", "=", "index", "\n", "for", "t", "in", "range", "(", "self", ".", "history_length", ")", ":", "\n", "# change the lookback index if we are not at the start of a new episode", "\n", "                ", "if", "lookback_index", "!=", "-", "1", ":", "\n", "                    ", "lookback_index", "=", "self", ".", "lookback", "[", "lookback_index", "]", "\n", "# add new state and next_states, if we are at the start it will pad with the same state and next_state (sequences will be padded to the left this way)", "\n", "", "states_new", "=", "[", "self", ".", "memory", "[", "lookback_index", "]", "[", "0", "]", "]", "+", "states_new", "\n", "next_states_new", "=", "[", "self", ".", "memory", "[", "lookback_index", "]", "[", "0", "]", "]", "+", "next_states_new", "\n", "# replace the original single state and next_state entry in the batch with these sequential ones", "\n", "", "batch_states", "+=", "states_new", "# B*L x 3 x 3", "\n", "batch_next_states", "+=", "next_states_new", "# B*L x 3 x 3", "\n", "# reform batch", "\n", "", "batch", "=", "batch_states", ",", "actions", ",", "rewards", ",", "batch_next_states", ",", "dones", "\n", "return", "batch", ",", "weights", ",", "indices", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.RecurrentPrioritizedReplayBuffer.save": [[133, 136], ["buffer.PrioritizedReplayBuffer.save", "pickle.dump", "open"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.save"], ["", "def", "save", "(", "self", ",", "fname", ")", ":", "\n", "        ", "super", "(", "RecurrentPrioritizedReplayBuffer", ",", "self", ")", ".", "save", "(", "fname", ")", "\n", "pickle", ".", "dump", "(", "self", ".", "lookback", ",", "open", "(", "'{}_lookback.pkl'", ".", "format", "(", "fname", ")", ",", "'wb'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.RecurrentPrioritizedReplayBuffer.load": [[137, 140], ["buffer.PrioritizedReplayBuffer.load", "pickle.load", "open"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.load", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.load"], ["", "def", "load", "(", "self", ",", "fname", ")", ":", "\n", "        ", "super", "(", "RecurrentPrioritizedReplayBuffer", ",", "self", ")", ".", "load", "(", "fname", ")", "\n", "self", ".", "lookback", "=", "pickle", ".", "load", "(", "open", "(", "'{}_lookback.pkl'", ".", "format", "(", "fname", ")", ",", "'rb'", ")", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.CT2USSingleVolumeEnvironment.__init__": [[10, 14], ["SingleVolumeEnvironment.__init__", "get_model().to", "CT2USenvironment.get_model"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.get_model"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "vol_id", "=", "0", ")", ":", "\n", "        ", "SingleVolumeEnvironment", ".", "__init__", "(", "self", ",", "config", ",", "vol_id", ")", "\n", "# load the queried CT2US model", "\n", "self", ".", "CT2USmodel", "=", "get_model", "(", "config", ".", "ct2us_model_name", ")", ".", "to", "(", "config", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.CT2USSingleVolumeEnvironment.sample_plane": [[16, 47], ["super().sample_plane", "preprocessCT().float().to", "sample[].detach().cpu().numpy", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "CT2USenvironment.CT2USSingleVolumeEnvironment.CT2USmodel().clamp().cpu().numpy", "CT2USenvironment.mask_array", "preprocessCT().float", "sample[].detach().cpu", "sample[].squeeze", "sample[].squeeze", "CT2USenvironment.CT2USSingleVolumeEnvironment.CT2USmodel().clamp().cpu", "CT2USenvironment.preprocessCT", "sample[].detach", "CT2USenvironment.CT2USSingleVolumeEnvironment.CT2USmodel().clamp", "CT2USenvironment.CT2USSingleVolumeEnvironment.CT2USmodel"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.sample_plane", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.mask_array", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.preprocessCT"], ["", "def", "sample_plane", "(", "self", ",", "state", ",", "return_seg", "=", "False", ",", "oob_black", "=", "True", ",", "preprocess", "=", "False", ",", "return_ct", "=", "False", ")", ":", "\n", "        ", "\"\"\" function to sample a plane from 3 3D points (state) and convert it to US\n        Params:\n        ==========\n            state (np.ndarray of shape (3,3)): v-stacked 3D points that will define a particular plane in the CT volume.\n            return_seg (bool): flag if we wish to return the corresponding segmentation map. (default=False)\n            oob_black (bool): flack if we wish to mask out of volume pixels to black. (default=True)\n            preprocess (bool): if to preprocess the plane before returning it (unsqueeze to BxCxHxW, normalize and/or add positional binary maps) \n            returns -> plane (torch.tensor of shape (1, 1, self.sy, self.sx)): corresponding plane sampled from the CT volume transformed to US\n                       seg (optional, np.ndarray of shape (self.sy, self.sx)): segmentation map of the sampled plane\n        \"\"\"", "\n", "# call environemnt sample_plane function, do not preprocess as we will have to do it ourselves anyways", "\n", "sample", "=", "super", "(", ")", ".", "sample_plane", "(", "state", "=", "state", ",", "return_seg", "=", "return_seg", ",", "oob_black", "=", "oob_black", ",", "preprocess", "=", "False", ")", "\n", "# we need to preprocess the CT slice before ", "\n", "sample", "[", "\"planeCT\"", "]", "=", "preprocessCT", "(", "sample", "[", "\"plane\"", "]", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "config", ".", "device", ")", "\n", "# transform the image to US (do not store gradients)", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "plane", "=", "self", ".", "CT2USmodel", "(", "sample", "[", "\"planeCT\"", "]", ")", ".", "clamp", "(", "-", "1.0", ",", "1.0", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "# the cyclegan uses tanh, hence output is in (-1,1), need to bring it to (0,1)", "\n", "# sample[\"planeUS\"] = (plane +1)/2 # now its in (0,1)", "\n", "sample", "[", "\"plane\"", "]", "=", "(", "plane", "+", "1", ")", "/", "2", "# now its in (0,1)", "\n", "sample", "[", "\"plane\"", "]", "=", "mask_array", "(", "sample", "[", "\"plane\"", "]", ")", "# apply US mask", "\n", "# remove planeCT from GPU and convert to numpy", "\n", "", "sample", "[", "\"planeCT\"", "]", "=", "sample", "[", "\"planeCT\"", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "# both planes are already unsqueezed and normalized, if we did not want to preprocess the image then squeeze and multiply by 255 to undo preprocessing", "\n", "# sample[\"plane\"] = sample[\"plane\"][np.newaxis, np.newaxis, ...]/255", "\n", "if", "not", "preprocess", ":", "\n", "# sample[\"planeUS\"] = sample[\"planeUS\"].squeeze()*255", "\n", "            ", "sample", "[", "\"plane\"", "]", "=", "sample", "[", "\"plane\"", "]", ".", "squeeze", "(", ")", "*", "255", "\n", "sample", "[", "\"planeCT\"", "]", "=", "sample", "[", "\"planeCT\"", "]", ".", "squeeze", "(", ")", "*", "255", "\n", "", "return", "sample", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.LocationAwareCT2USSingleVolumeEnvironment.__init__": [[49, 53], ["LocationAwareSingleVolumeEnvironment.__init__", "get_model().to", "CT2USenvironment.get_model"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.get_model"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "vol_id", "=", "0", ")", ":", "\n", "        ", "LocationAwareSingleVolumeEnvironment", ".", "__init__", "(", "self", ",", "config", ",", "vol_id", ")", "\n", "# load the queried CT2US model", "\n", "self", ".", "CT2USmodel", "=", "get_model", "(", "config", ".", "ct2us_model_name", ")", ".", "to", "(", "config", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.LocationAwareCT2USSingleVolumeEnvironment.sample_plane": [[55, 86], ["super().sample_plane", "preprocessCT().float().to", "sample[].detach().cpu().numpy", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "CT2USenvironment.LocationAwareCT2USSingleVolumeEnvironment.CT2USmodel().clamp().cpu().numpy", "np.concatenate", "preprocessCT().float", "sample[].detach().cpu", "sample[].squeeze", "sample[].squeeze", "CT2USenvironment.LocationAwareCT2USSingleVolumeEnvironment.CT2USmodel().clamp().cpu", "CT2USenvironment.preprocessCT", "sample[].detach", "CT2USenvironment.LocationAwareCT2USSingleVolumeEnvironment.CT2USmodel().clamp", "CT2USenvironment.LocationAwareCT2USSingleVolumeEnvironment.CT2USmodel"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.sample_plane", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.preprocessCT"], ["", "def", "sample_plane", "(", "self", ",", "state", ",", "return_seg", "=", "False", ",", "oob_black", "=", "True", ",", "preprocess", "=", "False", ",", "return_ct", "=", "False", ")", ":", "\n", "        ", "\"\"\" function to sample a plane from 3 3D points (state) and convert it to US\n        Params:\n        ==========\n            state (np.ndarray of shape (3,3)): v-stacked 3D points that will define a particular plane in the CT volume.\n            return_seg (bool): flag if we wish to return the corresponding segmentation map. (default=False)\n            oob_black (bool): flack if we wish to mask out of volume pixels to black. (default=True)\n            preprocess (bool): if to preprocess the plane before returning it (unsqueeze to BxCxHxW, normalize and/or add positional binary maps) \n            returns -> plane (torch.tensor of shape (1, 1, self.sy, self.sx)): corresponding plane sampled from the CT volume transformed to US\n                       seg (optional, np.ndarray of shape (self.sy, self.sx)): segmentation map of the sampled plane\n        \"\"\"", "\n", "# call environemnt sample_plane function, do not preprocess as we will have to do it ourselves anyways", "\n", "sample", "=", "super", "(", ")", ".", "sample_plane", "(", "state", "=", "state", ",", "return_seg", "=", "return_seg", ",", "oob_black", "=", "oob_black", ",", "preprocess", "=", "False", ")", "\n", "# we need to preprocess the CT slice before ", "\n", "sample", "[", "\"planeCT\"", "]", "=", "preprocessCT", "(", "sample", "[", "\"plane\"", "]", "[", "0", ",", "...", "]", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "config", ".", "device", ")", "\n", "# transform the image to US (do not store gradients)", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "plane", "=", "self", ".", "CT2USmodel", "(", "sample", "[", "\"planeCT\"", "]", ")", ".", "clamp", "(", "-", "1.0", ",", "1.0", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "# the cyclegan uses tanh, hence output is in (-1,1), need to bring it to (0,1)", "\n", "plane", "=", "(", "plane", "+", "1", ")", "/", "2", "# now its in (0,1)", "\n", "# concatenate new plane with the positional embeddings", "\n", "#pos = [p/255 for p in np.split(sample[\"plane\"], sample[\"plane\"].shape[0], axis=0)]", "\n", "pos", "=", "sample", "[", "\"plane\"", "]", "[", "1", ":", ",", "...", "]", "/", "255", "\n", "sample", "[", "\"plane\"", "]", "=", "np", ".", "concatenate", "(", "[", "plane", ",", "+", "pos", "[", "np", ".", "newaxis", ",", "...", "]", "]", ",", "axis", "=", "1", ")", "\n", "# remove planeCT from GPU and convert to numpy", "\n", "", "sample", "[", "\"planeCT\"", "]", "=", "sample", "[", "\"planeCT\"", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "# both planes are already unsqueezed and normalized, if we did not want to preprocess the image then squeeze and multiply by 255 to undo preprocessing", "\n", "if", "not", "preprocess", ":", "\n", "            ", "sample", "[", "\"plane\"", "]", "=", "sample", "[", "\"plane\"", "]", ".", "squeeze", "(", ")", "*", "255", "\n", "sample", "[", "\"planeCT\"", "]", "=", "sample", "[", "\"planeCT\"", "]", ".", "squeeze", "(", ")", "*", "255", "\n", "", "return", "sample", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.ResnetGenerator.__init__": [[150, 215], ["torch.Module.__init__", "range", "range", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "type", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Tanh", "torch.Tanh", "torch.Tanh", "CT2USenvironment.ResnetBlock", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "CT2USenvironment.Downsample", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "CT2USenvironment.Upsample", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "int", "int", "int", "int"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__"], ["def", "__init__", "(", "self", ",", "input_nc", ",", "output_nc", ",", "ngf", "=", "64", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "use_dropout", "=", "False", ",", "n_blocks", "=", "6", ",", "padding_type", "=", "'reflect'", ",", "no_antialias", "=", "False", ",", "no_antialias_up", "=", "False", ",", "opt", "=", "None", ")", ":", "\n", "        ", "\"\"\"Construct a Resnet-based generator\n\n        Parameters:\n            input_nc (int)      -- the number of channels in input images\n            output_nc (int)     -- the number of channels in output images\n            ngf (int)           -- the number of filters in the last conv layer\n            norm_layer          -- normalization layer\n            use_dropout (bool)  -- if use dropout layers\n            n_blocks (int)      -- the number of ResNet blocks\n            padding_type (str)  -- the name of padding layer in conv layers: reflect | replicate | zero\n        \"\"\"", "\n", "assert", "(", "n_blocks", ">=", "0", ")", "\n", "super", "(", "ResnetGenerator", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "opt", "=", "opt", "\n", "if", "type", "(", "norm_layer", ")", "==", "functools", ".", "partial", ":", "\n", "            ", "use_bias", "=", "norm_layer", ".", "func", "==", "nn", ".", "InstanceNorm2d", "\n", "", "else", ":", "\n", "            ", "use_bias", "=", "norm_layer", "==", "nn", ".", "InstanceNorm2d", "\n", "\n", "", "model", "=", "[", "nn", ".", "ReflectionPad2d", "(", "3", ")", ",", "\n", "nn", ".", "Conv2d", "(", "input_nc", ",", "ngf", ",", "kernel_size", "=", "7", ",", "padding", "=", "0", ",", "bias", "=", "use_bias", ")", ",", "\n", "norm_layer", "(", "ngf", ")", ",", "\n", "nn", ".", "ReLU", "(", "True", ")", "]", "\n", "\n", "n_downsampling", "=", "2", "\n", "for", "i", "in", "range", "(", "n_downsampling", ")", ":", "# add downsampling layers", "\n", "            ", "mult", "=", "2", "**", "i", "\n", "if", "(", "no_antialias", ")", ":", "\n", "                ", "model", "+=", "[", "nn", ".", "Conv2d", "(", "ngf", "*", "mult", ",", "ngf", "*", "mult", "*", "2", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ",", "bias", "=", "use_bias", ")", ",", "\n", "norm_layer", "(", "ngf", "*", "mult", "*", "2", ")", ",", "\n", "nn", ".", "ReLU", "(", "True", ")", "]", "\n", "", "else", ":", "\n", "                ", "model", "+=", "[", "nn", ".", "Conv2d", "(", "ngf", "*", "mult", ",", "ngf", "*", "mult", "*", "2", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ",", "bias", "=", "use_bias", ")", ",", "\n", "norm_layer", "(", "ngf", "*", "mult", "*", "2", ")", ",", "\n", "nn", ".", "ReLU", "(", "True", ")", ",", "\n", "Downsample", "(", "ngf", "*", "mult", "*", "2", ")", "]", "\n", "\n", "", "", "mult", "=", "2", "**", "n_downsampling", "\n", "for", "i", "in", "range", "(", "n_blocks", ")", ":", "# add ResNet blocks", "\n", "\n", "            ", "model", "+=", "[", "ResnetBlock", "(", "ngf", "*", "mult", ",", "padding_type", "=", "padding_type", ",", "norm_layer", "=", "norm_layer", ",", "use_dropout", "=", "use_dropout", ",", "use_bias", "=", "use_bias", ")", "]", "\n", "\n", "", "for", "i", "in", "range", "(", "n_downsampling", ")", ":", "# add upsampling layers", "\n", "            ", "mult", "=", "2", "**", "(", "n_downsampling", "-", "i", ")", "\n", "if", "no_antialias_up", ":", "\n", "                ", "model", "+=", "[", "nn", ".", "ConvTranspose2d", "(", "ngf", "*", "mult", ",", "int", "(", "ngf", "*", "mult", "/", "2", ")", ",", "\n", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "\n", "padding", "=", "1", ",", "output_padding", "=", "1", ",", "\n", "bias", "=", "use_bias", ")", ",", "\n", "norm_layer", "(", "int", "(", "ngf", "*", "mult", "/", "2", ")", ")", ",", "\n", "nn", ".", "ReLU", "(", "True", ")", "]", "\n", "", "else", ":", "\n", "                ", "model", "+=", "[", "Upsample", "(", "ngf", "*", "mult", ")", ",", "\n", "nn", ".", "Conv2d", "(", "ngf", "*", "mult", ",", "int", "(", "ngf", "*", "mult", "/", "2", ")", ",", "\n", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "\n", "padding", "=", "1", ",", "# output_padding=1,", "\n", "bias", "=", "use_bias", ")", ",", "\n", "norm_layer", "(", "int", "(", "ngf", "*", "mult", "/", "2", ")", ")", ",", "\n", "nn", ".", "ReLU", "(", "True", ")", "]", "\n", "", "", "model", "+=", "[", "nn", ".", "ReflectionPad2d", "(", "3", ")", "]", "\n", "model", "+=", "[", "nn", ".", "Conv2d", "(", "ngf", ",", "output_nc", ",", "kernel_size", "=", "7", ",", "padding", "=", "0", ")", "]", "\n", "model", "+=", "[", "nn", ".", "Tanh", "(", ")", "]", "\n", "\n", "self", ".", "model", "=", "nn", ".", "Sequential", "(", "*", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.ResnetGenerator.forward": [[216, 240], ["layers.append", "len", "enumerate", "CT2USenvironment.ResnetGenerator.model", "len", "layer", "feats.append"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ",", "layers", "=", "[", "]", ",", "encode_only", "=", "False", ")", ":", "\n", "        ", "if", "-", "1", "in", "layers", ":", "\n", "            ", "layers", ".", "append", "(", "len", "(", "self", ".", "model", ")", ")", "\n", "", "if", "len", "(", "layers", ")", ">", "0", ":", "\n", "            ", "feat", "=", "input", "\n", "feats", "=", "[", "]", "\n", "for", "layer_id", ",", "layer", "in", "enumerate", "(", "self", ".", "model", ")", ":", "\n", "# print(layer_id, layer)", "\n", "                ", "feat", "=", "layer", "(", "feat", ")", "\n", "if", "layer_id", "in", "layers", ":", "\n", "# print(\"%d: adding the output of %s %d\" % (layer_id, layer.__class__.__name__, feat.size(1)))", "\n", "                    ", "feats", ".", "append", "(", "feat", ")", "\n", "", "else", ":", "\n", "# print(\"%d: skipping %s %d\" % (layer_id, layer.__class__.__name__, feat.size(1)))", "\n", "                    ", "pass", "\n", "", "if", "layer_id", "==", "layers", "[", "-", "1", "]", "and", "encode_only", ":", "\n", "# print('encoder only return features')", "\n", "                    ", "return", "feats", "# return intermediate features alone; stop in the last layers", "\n", "\n", "", "", "return", "feat", ",", "feats", "# return both output and intermediate features", "\n", "", "else", ":", "\n", "            ", "\"\"\"Standard forward\"\"\"", "\n", "fake", "=", "self", ".", "model", "(", "input", ")", "\n", "return", "fake", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.Downsample.__init__": [[330, 344], ["torch.Module.__init__", "int", "CT2USenvironment.get_filter", "CT2USenvironment.Downsample.register_buffer", "int", "int", "int", "int", "filt[].repeat", "CT2USenvironment.get_pad_layer", "np.ceil", "np.ceil"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.get_filter", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.get_pad_layer"], ["    ", "def", "__init__", "(", "self", ",", "channels", ",", "pad_type", "=", "'reflect'", ",", "filt_size", "=", "3", ",", "stride", "=", "2", ",", "pad_off", "=", "0", ")", ":", "\n", "        ", "super", "(", "Downsample", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "filt_size", "=", "filt_size", "\n", "self", ".", "pad_off", "=", "pad_off", "\n", "self", ".", "pad_sizes", "=", "[", "int", "(", "1.", "*", "(", "filt_size", "-", "1", ")", "/", "2", ")", ",", "int", "(", "np", ".", "ceil", "(", "1.", "*", "(", "filt_size", "-", "1", ")", "/", "2", ")", ")", ",", "int", "(", "1.", "*", "(", "filt_size", "-", "1", ")", "/", "2", ")", ",", "int", "(", "np", ".", "ceil", "(", "1.", "*", "(", "filt_size", "-", "1", ")", "/", "2", ")", ")", "]", "\n", "self", ".", "pad_sizes", "=", "[", "pad_size", "+", "pad_off", "for", "pad_size", "in", "self", ".", "pad_sizes", "]", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "off", "=", "int", "(", "(", "self", ".", "stride", "-", "1", ")", "/", "2.", ")", "\n", "self", ".", "channels", "=", "channels", "\n", "\n", "filt", "=", "get_filter", "(", "filt_size", "=", "self", ".", "filt_size", ")", "\n", "self", ".", "register_buffer", "(", "'filt'", ",", "filt", "[", "None", ",", "None", ",", ":", ",", ":", "]", ".", "repeat", "(", "(", "self", ".", "channels", ",", "1", ",", "1", ",", "1", ")", ")", ")", "\n", "\n", "self", ".", "pad", "=", "get_pad_layer", "(", "pad_type", ")", "(", "self", ".", "pad_sizes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.Downsample.forward": [[345, 353], ["torch.conv2d", "torch.conv2d", "torch.conv2d", "CT2USenvironment.Downsample.pad", "CT2USenvironment.Downsample.pad"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inp", ")", ":", "\n", "        ", "if", "(", "self", ".", "filt_size", "==", "1", ")", ":", "\n", "            ", "if", "(", "self", ".", "pad_off", "==", "0", ")", ":", "\n", "                ", "return", "inp", "[", ":", ",", ":", ",", ":", ":", "self", ".", "stride", ",", ":", ":", "self", ".", "stride", "]", "\n", "", "else", ":", "\n", "                ", "return", "self", ".", "pad", "(", "inp", ")", "[", ":", ",", ":", ",", ":", ":", "self", ".", "stride", ",", ":", ":", "self", ".", "stride", "]", "\n", "", "", "else", ":", "\n", "            ", "return", "F", ".", "conv2d", "(", "self", ".", "pad", "(", "inp", ")", ",", "self", ".", "filt", ",", "stride", "=", "self", ".", "stride", ",", "groups", "=", "inp", ".", "shape", "[", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.Upsample2.__init__": [[356, 360], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "scale_factor", ",", "mode", "=", "'nearest'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "factor", "=", "scale_factor", "\n", "self", ".", "mode", "=", "mode", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.Upsample2.forward": [[361, 363], ["torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "torch.nn.functional.interpolate"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "torch", ".", "nn", ".", "functional", ".", "interpolate", "(", "x", ",", "scale_factor", "=", "self", ".", "factor", ",", "mode", "=", "self", ".", "mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.Upsample.__init__": [[365, 378], ["torch.Module.__init__", "int", "int", "CT2USenvironment.Upsample.register_buffer", "np.mod", "CT2USenvironment.get_filter", "filt[].repeat", "CT2USenvironment.get_pad_layer"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.get_filter", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.get_pad_layer"], ["    ", "def", "__init__", "(", "self", ",", "channels", ",", "pad_type", "=", "'repl'", ",", "filt_size", "=", "4", ",", "stride", "=", "2", ")", ":", "\n", "        ", "super", "(", "Upsample", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "filt_size", "=", "filt_size", "\n", "self", ".", "filt_odd", "=", "np", ".", "mod", "(", "filt_size", ",", "2", ")", "==", "1", "\n", "self", ".", "pad_size", "=", "int", "(", "(", "filt_size", "-", "1", ")", "/", "2", ")", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "off", "=", "int", "(", "(", "self", ".", "stride", "-", "1", ")", "/", "2.", ")", "\n", "self", ".", "channels", "=", "channels", "\n", "\n", "filt", "=", "get_filter", "(", "filt_size", "=", "self", ".", "filt_size", ")", "*", "(", "stride", "**", "2", ")", "\n", "self", ".", "register_buffer", "(", "'filt'", ",", "filt", "[", "None", ",", "None", ",", ":", ",", ":", "]", ".", "repeat", "(", "(", "self", ".", "channels", ",", "1", ",", "1", ",", "1", ")", ")", ")", "\n", "\n", "self", ".", "pad", "=", "get_pad_layer", "(", "pad_type", ")", "(", "[", "1", ",", "1", ",", "1", ",", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.Upsample.forward": [[379, 385], ["torch.conv_transpose2d", "torch.conv_transpose2d", "torch.conv_transpose2d", "CT2USenvironment.Upsample.pad"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inp", ")", ":", "\n", "        ", "ret_val", "=", "F", ".", "conv_transpose2d", "(", "self", ".", "pad", "(", "inp", ")", ",", "self", ".", "filt", ",", "stride", "=", "self", ".", "stride", ",", "padding", "=", "1", "+", "self", ".", "pad_size", ",", "groups", "=", "inp", ".", "shape", "[", "1", "]", ")", "[", ":", ",", ":", ",", "1", ":", ",", "1", ":", "]", "\n", "if", "(", "self", ".", "filt_odd", ")", ":", "\n", "            ", "return", "ret_val", "\n", "", "else", ":", "\n", "            ", "return", "ret_val", "[", ":", ",", ":", ",", ":", "-", "1", ",", ":", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.Identity.forward": [[400, 402], ["None"], "methods", ["None"], ["    ", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.ResnetBlock.__init__": [[426, 435], ["torch.Module.__init__", "CT2USenvironment.ResnetBlock.build_conv_block"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.ResnetBlock.build_conv_block"], ["def", "__init__", "(", "self", ",", "dim", ",", "padding_type", ",", "norm_layer", ",", "use_dropout", ",", "use_bias", ")", ":", "\n", "        ", "\"\"\"Initialize the Resnet block\n        A resnet block is a conv block with skip connections\n        We construct a conv block with build_conv_block function,\n        and implement skip connections in <forward> function.\n        Original Resnet paper: https://arxiv.org/pdf/1512.03385.pdf\n        \"\"\"", "\n", "super", "(", "ResnetBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv_block", "=", "self", ".", "build_conv_block", "(", "dim", ",", "padding_type", ",", "norm_layer", ",", "use_dropout", ",", "use_bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.ResnetBlock.build_conv_block": [[436, 473], ["torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.ReplicationPad2d", "torch.ReplicationPad2d", "torch.ReplicationPad2d", "NotImplementedError", "torch.ReplicationPad2d", "torch.ReplicationPad2d", "torch.ReplicationPad2d", "NotImplementedError"], "methods", ["None"], ["", "def", "build_conv_block", "(", "self", ",", "dim", ",", "padding_type", ",", "norm_layer", ",", "use_dropout", ",", "use_bias", ")", ":", "\n", "        ", "\"\"\"Construct a convolutional block.\n        Parameters:\n            dim (int)           -- the number of channels in the conv layer.\n            padding_type (str)  -- the name of padding layer: reflect | replicate | zero\n            norm_layer          -- normalization layer\n            use_dropout (bool)  -- if use dropout layers.\n            use_bias (bool)     -- if the conv layer uses bias or not\n        Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU))\n        \"\"\"", "\n", "conv_block", "=", "[", "]", "\n", "p", "=", "0", "\n", "if", "padding_type", "==", "'reflect'", ":", "\n", "            ", "conv_block", "+=", "[", "nn", ".", "ReflectionPad2d", "(", "1", ")", "]", "\n", "", "elif", "padding_type", "==", "'replicate'", ":", "\n", "            ", "conv_block", "+=", "[", "nn", ".", "ReplicationPad2d", "(", "1", ")", "]", "\n", "", "elif", "padding_type", "==", "'zero'", ":", "\n", "            ", "p", "=", "1", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'padding [%s] is not implemented'", "%", "padding_type", ")", "\n", "\n", "", "conv_block", "+=", "[", "nn", ".", "Conv2d", "(", "dim", ",", "dim", ",", "kernel_size", "=", "3", ",", "padding", "=", "p", ",", "bias", "=", "use_bias", ")", ",", "norm_layer", "(", "dim", ")", ",", "nn", ".", "ReLU", "(", "True", ")", "]", "\n", "if", "use_dropout", ":", "\n", "            ", "conv_block", "+=", "[", "nn", ".", "Dropout", "(", "0.5", ")", "]", "\n", "\n", "", "p", "=", "0", "\n", "if", "padding_type", "==", "'reflect'", ":", "\n", "            ", "conv_block", "+=", "[", "nn", ".", "ReflectionPad2d", "(", "1", ")", "]", "\n", "", "elif", "padding_type", "==", "'replicate'", ":", "\n", "            ", "conv_block", "+=", "[", "nn", ".", "ReplicationPad2d", "(", "1", ")", "]", "\n", "", "elif", "padding_type", "==", "'zero'", ":", "\n", "            ", "p", "=", "1", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'padding [%s] is not implemented'", "%", "padding_type", ")", "\n", "", "conv_block", "+=", "[", "nn", ".", "Conv2d", "(", "dim", ",", "dim", ",", "kernel_size", "=", "3", ",", "padding", "=", "p", ",", "bias", "=", "use_bias", ")", ",", "norm_layer", "(", "dim", ")", "]", "\n", "\n", "return", "nn", ".", "Sequential", "(", "*", "conv_block", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.ResnetBlock.forward": [[474, 478], ["CT2USenvironment.ResnetBlock.conv_block"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Forward function (with skip connections)\"\"\"", "\n", "out", "=", "x", "+", "self", ".", "conv_block", "(", "x", ")", "# add skip connections", "\n", "return", "out", "", "", "", ""]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.preprocessCT": [[88, 105], ["torch.from_numpy().unsqueeze().unsqueeze", "torch.from_numpy().unsqueeze().unsqueeze", "torch.from_numpy().unsqueeze().unsqueeze", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "function", ["None"], ["", "", "def", "preprocessCT", "(", "CT", ")", ":", "\n", "# # normalize and mask", "\n", "# CT = mask_array(CT/255)", "\n", "# # convert to tensor and unsqueeze", "\n", "# CT = torch.from_numpy(CT).unsqueeze(0).unsqueeze(0)", "\n", "# # apply salt-pepper noise", "\n", "# noise = torch.rand(*CT.shape)", "\n", "# proba = 0.1", "\n", "# disrupt = 1.5", "\n", "# CT[noise < proba] = CT[noise < proba]/disrupt", "\n", "# CT[noise > 1-proba] = CT[noise > 1-proba]*disrupt", "\n", "\n", "# normalize and invert intensities", "\n", "    ", "CT", "=", "(", "255", "-", "CT", ")", "/", "255", "\n", "# convert to tensor and unsqueeze", "\n", "CT", "=", "torch", ".", "from_numpy", "(", "CT", ")", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", "\n", "return", "CT", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.mask_array": [[107, 130], ["np.ones", "skimage.draw.circle", "skimage.draw.polygon", "skimage.draw.polygon", "int"], "function", ["None"], ["", "def", "mask_array", "(", "arr", ")", ":", "\n", "#assert arr.ndim == 2 # on 1 channel images broadcasting will work", "\n", "    ", "_", ",", "_", ",", "sx", ",", "sy", "=", "arr", ".", "shape", "# H, W", "\n", "mask", "=", "np", ".", "ones", "(", "(", "sx", ",", "sy", ")", ")", "\n", "# Bottom circular shape", "\n", "rr", ",", "cc", "=", "circle", "(", "0", ",", "int", "(", "sy", "//", "2", ")", ",", "sy", "-", "1", ")", "\n", "rr", "[", "rr", ">=", "sy", "]", "=", "sy", "-", "1", "\n", "cc", "[", "cc", ">=", "sy", "]", "=", "sy", "-", "1", "\n", "rr", "[", "rr", "<", "0", "]", "=", "0", "\n", "cc", "[", "cc", "<", "0", "]", "=", "0", "\n", "mask", "[", "rr", ",", "cc", "]", "=", "0", "\n", "# Left triangle", "\n", "r", "=", "(", "0", ",", "0", ",", "sx", "*", "5", "/", "9", ")", "\n", "c", "=", "(", "0", ",", "sy", "//", "2", ",", "0", ")", "\n", "rr", ",", "cc", "=", "polygon", "(", "r", ",", "c", ")", "\n", "mask", "[", "rr", ",", "cc", "]", "=", "1", "\n", "# Right triangle", "\n", "r", "=", "(", "0", ",", "0", ",", "sx", "*", "5", "/", "9", ")", "\n", "c", "=", "(", "sy", "//", "2", ",", "sy", "-", "1", ",", "sy", "-", "1", ")", "\n", "rr", ",", "cc", "=", "polygon", "(", "r", ",", "c", ")", "\n", "mask", "[", "rr", ",", "cc", "]", "=", "1", "\n", "mask", "=", "(", "1", "-", "mask", ")", ".", "astype", "(", "np", ".", "bool", ")", "\n", "return", "mask", "*", "arr", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.get_model": [[133, 141], ["CT2USenvironment.ResnetGenerator", "torch.load", "torch.load", "torch.load", "print", "ResnetGenerator.load_state_dict", "os.path.join", "os.getcwd", "os.path.join", "os.getcwd"], "function", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.load", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.load", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.load"], ["", "def", "get_model", "(", "name", ",", "use_cuda", "=", "False", ")", ":", "\n", "# instanciate cyclegan architecture used in CT2UStransfer (this is also the default architecture recommended by the authors)", "\n", "    ", "model", "=", "ResnetGenerator", "(", "input_nc", "=", "1", ",", "output_nc", "=", "1", ",", "ngf", "=", "64", ",", "norm_layer", "=", "nn", ".", "InstanceNorm2d", ",", "use_dropout", "=", "True", ",", "n_blocks", "=", "9", ",", "\n", "padding_type", "=", "'reflect'", ",", "no_antialias", "=", "True", ",", "no_antialias_up", "=", "False", ",", "opt", "=", "None", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "os", ".", "getcwd", "(", ")", ",", "\"environment\"", ",", "\"CT2USmodels\"", ",", "\"%s.pth\"", "%", "name", ")", ",", "map_location", "=", "'cpu'", ")", "\n", "print", "(", "\"loading: {} ...\"", ".", "format", "(", "os", ".", "path", ".", "join", "(", "os", ".", "getcwd", "(", ")", ",", "\"environment\"", ",", "\"CT2USmodels\"", ",", "\"%s.pth\"", "%", "name", ")", ")", ")", "\n", "model", ".", "load_state_dict", "(", "state_dict", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.get_filter": [[308, 328], ["torch.Tensor", "torch.Tensor", "torch.Tensor", "np.array", "torch.sum", "torch.sum", "torch.sum", "np.array", "np.array", "np.array", "np.array", "np.array", "np.array"], "function", ["None"], ["", "", "", "def", "get_filter", "(", "filt_size", "=", "3", ")", ":", "\n", "    ", "if", "(", "filt_size", "==", "1", ")", ":", "\n", "        ", "a", "=", "np", ".", "array", "(", "[", "1.", ",", "]", ")", "\n", "", "elif", "(", "filt_size", "==", "2", ")", ":", "\n", "        ", "a", "=", "np", ".", "array", "(", "[", "1.", ",", "1.", "]", ")", "\n", "", "elif", "(", "filt_size", "==", "3", ")", ":", "\n", "        ", "a", "=", "np", ".", "array", "(", "[", "1.", ",", "2.", ",", "1.", "]", ")", "\n", "", "elif", "(", "filt_size", "==", "4", ")", ":", "\n", "        ", "a", "=", "np", ".", "array", "(", "[", "1.", ",", "3.", ",", "3.", ",", "1.", "]", ")", "\n", "", "elif", "(", "filt_size", "==", "5", ")", ":", "\n", "        ", "a", "=", "np", ".", "array", "(", "[", "1.", ",", "4.", ",", "6.", ",", "4.", ",", "1.", "]", ")", "\n", "", "elif", "(", "filt_size", "==", "6", ")", ":", "\n", "        ", "a", "=", "np", ".", "array", "(", "[", "1.", ",", "5.", ",", "10.", ",", "10.", ",", "5.", ",", "1.", "]", ")", "\n", "", "elif", "(", "filt_size", "==", "7", ")", ":", "\n", "        ", "a", "=", "np", ".", "array", "(", "[", "1.", ",", "6.", ",", "15.", ",", "20.", ",", "15.", ",", "6.", ",", "1.", "]", ")", "\n", "\n", "", "filt", "=", "torch", ".", "Tensor", "(", "a", "[", ":", ",", "None", "]", "*", "a", "[", "None", ",", ":", "]", ")", "\n", "filt", "=", "filt", "/", "torch", ".", "sum", "(", "filt", ")", "\n", "\n", "return", "filt", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.get_pad_layer": [[387, 397], ["print"], "function", ["None"], ["", "", "", "def", "get_pad_layer", "(", "pad_type", ")", ":", "\n", "    ", "if", "(", "pad_type", "in", "[", "'refl'", ",", "'reflect'", "]", ")", ":", "\n", "        ", "PadLayer", "=", "nn", ".", "ReflectionPad2d", "\n", "", "elif", "(", "pad_type", "in", "[", "'repl'", ",", "'replicate'", "]", ")", ":", "\n", "        ", "PadLayer", "=", "nn", ".", "ReplicationPad2d", "\n", "", "elif", "(", "pad_type", "==", "'zero'", ")", ":", "\n", "        ", "PadLayer", "=", "nn", ".", "ZeroPad2d", "\n", "", "else", ":", "\n", "        ", "print", "(", "'Pad type [%s] not recognized'", "%", "pad_type", ")", "\n", "", "return", "PadLayer", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.CT2USenvironment.get_norm_layer": [[403, 422], ["functools.partial", "functools.partial", "NotImplementedError", "CT2USenvironment.Identity"], "function", ["None"], ["", "", "def", "get_norm_layer", "(", "norm_type", "=", "'instance'", ")", ":", "\n", "    ", "\"\"\"Return a normalization layer\n\n    Parameters:\n        norm_type (str) -- the name of the normalization layer: batch | instance | none\n\n    For BatchNorm, we use learnable affine parameters and track running statistics (mean/stddev).\n    For InstanceNorm, we do not use learnable affine parameters. We do not track running statistics.\n    \"\"\"", "\n", "if", "norm_type", "==", "'batch'", ":", "\n", "        ", "norm_layer", "=", "functools", ".", "partial", "(", "nn", ".", "BatchNorm2d", ",", "affine", "=", "True", ",", "track_running_stats", "=", "True", ")", "\n", "", "elif", "norm_type", "==", "'instance'", ":", "\n", "        ", "norm_layer", "=", "functools", ".", "partial", "(", "nn", ".", "InstanceNorm2d", ",", "affine", "=", "False", ",", "track_running_stats", "=", "False", ")", "\n", "", "elif", "norm_type", "==", "'none'", ":", "\n", "        ", "def", "norm_layer", "(", "x", ")", ":", "\n", "            ", "return", "Identity", "(", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "'normalization layer [%s] is not found'", "%", "norm_type", ")", "\n", "", "return", "norm_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.SingleVolumeEnvironment.__init__": [[9, 57], ["environment.baseEnvironment.BaseEnvironment.__init__", "SimpleITK.ReadImage", "SimpleITK.GetArrayFromImage", "intensity_scaling.astype", "SimpleITK.ReadImage", "SimpleITK.GetArrayFromImage", "xcatEnvironment.SingleVolumeEnvironment.set_goal", "xcatEnvironment.SingleVolumeEnvironment.reset", "os.path.join", "xcatEnvironment.intensity_scaling", "os.path.join", "xcatEnvironment.RandIntensity", "Oscillate", "int", "intensity_scaling.max", "config.goal_centroids.split"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.SingleVolumeEnvironment.set_goal", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.reset", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.intensity_scaling"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "vol_id", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        Initialize an Environment object, the environment will focus on a single XCAT/CT volume.\n    \n        Params that we will use:\n        ======\n            config (argparse object): contains all options. see ./options/options.py for more details\n            vol_id (int): specifies which volume we want to load. Specifically ``config.volume_ids`` may contain\n                          all queried volumes by the main.py script. (i.e. parser.volume_ids = 'samp0,samp1,samp2,samp3,samp4,samp5,samp6,samp7')\n                          we will load the volume at position ``vol_id`` (default=0)               \n        \"\"\"", "\n", "\n", "# initialize the base environment", "\n", "BaseEnvironment", ".", "__init__", "(", "self", ",", "config", ")", "\n", "# identify the volume used by this environment instance", "\n", "self", ".", "vol_id", "=", "vol_id", "\n", "\n", "# load queried CT volume", "\n", "itkVolume", "=", "sitk", ".", "ReadImage", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataroot", ",", "self", ".", "vol_id", "+", "\"_1_CT.nii.gz\"", ")", ")", "\n", "Volume", "=", "sitk", ".", "GetArrayFromImage", "(", "itkVolume", ")", "\n", "# preprocess volume if queried (we do this on XCAT images, but not on CT and fakeCT images)", "\n", "if", "not", "config", ".", "no_preprocess", ":", "\n", "            ", "Volume", "=", "Volume", "/", "Volume", ".", "max", "(", ")", "*", "255", "\n", "Volume", "=", "intensity_scaling", "(", "Volume", ",", "pmin", "=", "config", ".", "pmin", ",", "pmax", "=", "config", ".", "pmax", ",", "nmin", "=", "config", ".", "nmin", ",", "nmax", "=", "config", ".", "nmax", ")", "\n", "", "self", ".", "Volume", "=", "Volume", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "self", ".", "sx", ",", "self", ".", "sy", ",", "self", ".", "sz", "=", "self", ".", "Volume", ".", "shape", "\n", "\n", "# load queried CT segmentation", "\n", "itkSegmentation", "=", "sitk", ".", "ReadImage", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataroot", ",", "self", ".", "vol_id", "+", "\"_1_SEG.nii.gz\"", ")", ")", "\n", "Segmentation", "=", "sitk", ".", "GetArrayFromImage", "(", "itkSegmentation", ")", "\n", "self", ".", "Segmentation", "=", "Segmentation", "\n", "\n", "# if we are performing navigation on fakeCTs, we randomize the intensities at each episode", "\n", "if", "config", ".", "randomize_intensities", ":", "\n", "            ", "self", ".", "random_intensity_scaling", "=", "RandIntensity", "(", ")", "# see defaults value at bottom of file", "\n", "\n", "# get an approximated location for the 4-chamber slice using the centroids", "\n", "", "self", ".", "set_goal", "(", "[", "int", "(", "i", ")", "for", "i", "in", "config", ".", "goal_centroids", ".", "split", "(", "\",\"", ")", "]", ")", "\n", "\n", "# monitor oscillations for termination", "\n", "if", "config", ".", "termination", "==", "\"oscillate\"", ":", "\n", "            ", "self", ".", "oscillates", "=", "Oscillate", "(", "history_length", "=", "config", ".", "termination_history_len", ",", "stop_freq", "=", "config", ".", "termination_oscillation_freq", ")", "\n", "\n", "# initiating empty containers for reward shaping", "\n", "", "self", ".", "logged_rewards", ",", "self", ".", "rewards", "=", "[", "]", ",", "{", "}", "\n", "\n", "# get starting configuration and reset environment for a new episode", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.SingleVolumeEnvironment.set_goal": [[58, 69], ["xcatEnvironment.get_centroid", "xcatEnvironment.get_centroid", "xcatEnvironment.get_centroid", "numpy.vstack", "xcatEnvironment.SingleVolumeEnvironment.get_plane_coefs"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.get_centroid", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.get_centroid", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.get_centroid", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.get_plane_coefs"], ["", "def", "set_goal", "(", "self", ",", "centroid_ids", ")", ":", "\n", "# set the current centroids ids", "\n", "        ", "self", ".", "centroid_ids", "=", "centroid_ids", "\n", "# get the centroids coordinates", "\n", "centroid1", "=", "get_centroid", "(", "self", ".", "Segmentation", ",", "centroid_ids", "[", "0", "]", ")", "\n", "centroid2", "=", "get_centroid", "(", "self", ".", "Segmentation", ",", "centroid_ids", "[", "1", "]", ")", "\n", "centroid3", "=", "get_centroid", "(", "self", ".", "Segmentation", ",", "centroid_ids", "[", "2", "]", ")", "\n", "# set goal state", "\n", "self", ".", "goal_state", "=", "np", ".", "vstack", "(", "[", "centroid1", ",", "centroid2", ",", "centroid3", "]", ")", "\n", "# set the corresponding plane coefficients", "\n", "self", ".", "goal_plane", "=", "self", ".", "get_plane_coefs", "(", "centroid1", ",", "centroid2", ",", "centroid3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.SingleVolumeEnvironment.set_reward": [[70, 102], ["xcatEnvironment.SingleVolumeEnvironment.logged_rewards.append", "PlaneDistanceReward", "xcatEnvironment.SingleVolumeEnvironment.logged_rewards.append", "AnatomyReward", "abs", "xcatEnvironment.SingleVolumeEnvironment.logged_rewards.append", "AreaReward", "abs", "range", "OutOfBoundaryReward", "abs", "xcatEnvironment.SingleVolumeEnvironment.logged_rewards.append", "StopReward", "xcatEnvironment.SingleVolumeEnvironment.logged_rewards.append", "AnatomyReward", "xcatEnvironment.SingleVolumeEnvironment.logged_rewards.append", "xcatEnvironment.SingleVolumeEnvironment.rewards[].get_anatomy_reward", "xcatEnvironment.SingleVolumeEnvironment.sample_plane"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.rewards.rewards.AnatomyReward.get_anatomy_reward", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.sample_plane"], ["", "def", "set_reward", "(", "self", ")", ":", "\n", "# add plane distance reward", "\n", "        ", "if", "self", ".", "config", ".", "planeDistanceRewardWeight", ">", "0", ":", "\n", "            ", "self", ".", "logged_rewards", ".", "append", "(", "\"planeDistanceReward\"", ")", "\n", "self", ".", "rewards", "[", "\"planeDistanceReward\"", "]", "=", "PlaneDistanceReward", "(", "self", ".", "goal_plane", ",", "weight", "=", "self", ".", "config", ".", "planeDistanceRewardWeight", ")", "\n", "# add anatomy reward", "\n", "", "if", "self", ".", "config", ".", "anatomyRewardWeight", ">", "0", ":", "\n", "            ", "self", ".", "logged_rewards", ".", "append", "(", "\"anatomyReward\"", ")", "\n", "self", ".", "rewards", "[", "\"anatomyReward\"", "]", "=", "AnatomyReward", "(", "self", ".", "config", ".", "anatomyRewardIDs", ",", "\n", "incremental", "=", "self", ".", "config", ".", "incrementalAnatomyReward", ",", "\n", "weight", "=", "self", ".", "config", ".", "anatomyRewardWeight", ")", "\n", "# add area reward", "\n", "", "if", "abs", "(", "self", ".", "config", ".", "areaRewardWeight", ")", ">", "0", ":", "\n", "            ", "self", ".", "logged_rewards", ".", "append", "(", "\"areaReward\"", ")", "\n", "self", ".", "rewards", "[", "\"areaReward\"", "]", "=", "AreaReward", "(", "self", ".", "config", ".", "areaRewardWeight", ",", "self", ".", "sx", "*", "self", ".", "sy", ")", "\n", "# add oob reward", "\n", "", "if", "abs", "(", "self", ".", "config", ".", "oobReward", ")", ">", "0", ":", "\n", "            ", "for", "i", "in", "range", "(", "self", ".", "config", ".", "n_agents", ")", ":", "\n", "                ", "self", ".", "logged_rewards", ".", "append", "(", "\"oobReward_%d\"", "%", "(", "i", "+", "1", ")", ")", "\n", "", "self", ".", "rewards", "[", "\"oobReward\"", "]", "=", "OutOfBoundaryReward", "(", "self", ".", "config", ".", "oobReward", ",", "self", ".", "sx", ",", "self", ".", "sy", ",", "self", ".", "sz", ")", "\n", "# add stop reward", "\n", "", "if", "abs", "(", "self", ".", "config", ".", "stopReward", ")", ">", "0", ":", "\n", "            ", "assert", "\"anatomyReward\"", "in", "self", ".", "rewards", ",", "\"stopReward only implemented when using anatomyReward.\"", "\n", "assert", "self", ".", "config", ".", "termination", "==", "\"learned\"", ",", "\"stopReward is only meaningful when learning how to stop (action_size = 7, termination = learned)\"", "\n", "self", ".", "logged_rewards", ".", "append", "(", "\"stopReward\"", ")", "\n", "self", ".", "rewards", "[", "\"stopReward\"", "]", "=", "StopReward", "(", "self", ".", "config", ".", "stopReward", ",", "\n", "goal_reward", "=", "self", ".", "rewards", "[", "\"anatomyReward\"", "]", ".", "get_anatomy_reward", "(", "self", ".", "sample_plane", "(", "self", ".", "goal_state", ",", "\n", "return_seg", "=", "True", ")", "[", "\"seg\"", "]", ")", ")", "\n", "# add oob pixels reward", "\n", "", "if", "self", ".", "config", ".", "penalize_oob_pixels", ":", "\n", "            ", "self", ".", "logged_rewards", ".", "append", "(", "\"oobPixelsReward\"", ")", "\n", "self", ".", "rewards", "[", "\"oobPixelsReward\"", "]", "=", "AnatomyReward", "(", "\"0\"", ",", "is_penalty", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.SingleVolumeEnvironment.sample_plane": [[103, 140], ["xcatEnvironment.SingleVolumeEnvironment.get_plane_from_points", "xcatEnvironment.SingleVolumeEnvironment.random_intensity_scaling"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.get_plane_from_points"], ["", "", "def", "sample_plane", "(", "self", ",", "state", ",", "return_seg", "=", "False", ",", "oob_black", "=", "True", ",", "preprocess", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\" function to sample a plane from 3 3D points (state)\n        Params:\n        ==========\n            state (np.ndarray of shape (3,3)): v-stacked 3D points that will define a particular plane in the CT volume.\n            return_seg (bool): flag if we wish to return the corresponding segmentation map. (default=False)\n            oob_black (bool): flack if we wish to mask out of volume pixels to black. (default=True)\n            preprocess (bool): if to preprocess the plane before returning it (unsqueeze to BxCxHxW, normalize) \n            returns -> plane (torch.tensor of shape (1, 1, self.sy, self.sx)): corresponding plane sampled from the CT volume\n                       seg (optional, np.ndarray of shape (self.sy, self.sx)): segmentation map of the sampled plane\n        \"\"\"", "\n", "out", "=", "{", "}", "\n", "# 1. extract plane specs", "\n", "XYZ", ",", "P", ",", "S", "=", "self", ".", "get_plane_from_points", "(", "state", ",", "(", "self", ".", "sx", ",", "self", ".", "sy", ",", "self", ".", "sz", ")", ")", "\n", "# 2. sample plane from the current volume", "\n", "X", ",", "Y", ",", "Z", "=", "XYZ", "\n", "plane", "=", "self", ".", "Volume", "[", "X", ",", "Y", ",", "Z", "]", "\n", "# mask out of boundary pixels to black", "\n", "if", "oob_black", "==", "True", ":", "\n", "            ", "plane", "[", "P", "<", "0", "]", "=", "0", "\n", "plane", "[", "P", ">", "S", "]", "=", "0", "\n", "# if needed randomize intensities", "\n", "", "if", "self", ".", "config", ".", "randomize_intensities", ":", "\n", "            ", "plane", "=", "self", ".", "random_intensity_scaling", "(", "plane", ")", "\n", "# normalize and unsqueeze array if needed.", "\n", "", "if", "preprocess", ":", "\n", "            ", "plane", "=", "plane", "[", "np", ".", "newaxis", ",", "np", ".", "newaxis", ",", "...", "]", "/", "255", "\n", "# add to output", "\n", "", "out", "[", "\"plane\"", "]", "=", "plane", "\n", "# 3. sample the segmentation if needed", "\n", "if", "return_seg", ":", "\n", "            ", "seg", "=", "self", ".", "Segmentation", "[", "X", ",", "Y", ",", "Z", "]", "\n", "if", "oob_black", "==", "True", ":", "\n", "                ", "seg", "[", "P", "<", "0", "]", "=", "0", "\n", "seg", "[", "P", ">", "S", "]", "=", "0", "\n", "", "out", "[", "\"seg\"", "]", "=", "seg", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.SingleVolumeEnvironment.get_reward": [[141, 187], ["xcatEnvironment.SingleVolumeEnvironment.rewards.items", "numpy.array", "shared_rewards.update", "total_rewards[].astype", "range", "func", "sum", "xcatEnvironment.SingleVolumeEnvironment.get_plane_coefs", "func", "shared_rewards.values", "func", "func", "xcatEnvironment.SingleVolumeEnvironment.rewards[].get_anatomy_reward", "func", "enumerate", "func"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.get_plane_coefs", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.rewards.rewards.AnatomyReward.get_anatomy_reward"], ["", "def", "get_reward", "(", "self", ",", "seg", ",", "state", "=", "None", ",", "increment", "=", "None", ")", ":", "\n", "        ", "\"\"\"Calculates the corresponding reward of stepping into a state given its segmentation map.\n        Params:\n        ==========\n            seg (np.ndarray of shape (self.sy, self.sx)): segmentation map from which to extract the reward.\n            state (np.ndarray): 3 stacked 3D arrays representing the coordinates of the agent.\n            increment (np.ndarray): 3 stacked 3D arrays containing the increment action chosen by each agent.\n                                    we use this to penalize the agents if they choose to stop on a bad frame.\n        Returns -> rewards (dict): the corresponding rewards collected by the agents.\n        \"\"\"", "\n", "shared_rewards", "=", "{", "}", "\n", "single_rewards", "=", "{", "}", "\n", "for", "key", ",", "func", "in", "self", ".", "rewards", ".", "items", "(", ")", ":", "\n", "            ", "if", "key", "==", "\"planeDistanceReward\"", ":", "\n", "# this will contain a single reward for all agents", "\n", "                ", "shared_rewards", "[", "\"planeDistanceReward\"", "]", "=", "func", "(", "self", ".", "get_plane_coefs", "(", "*", "state", ")", ")", "\n", "", "elif", "key", "==", "\"anatomyReward\"", ":", "\n", "# this will contain a single reward for all agents", "\n", "                ", "shared_rewards", "[", "\"anatomyReward\"", "]", "=", "func", "(", "seg", ")", "\n", "", "elif", "key", "==", "\"areaReward\"", ":", "\n", "# this will contain a single reward for all agents", "\n", "                ", "shared_rewards", "[", "\"areaReward\"", "]", "=", "func", "(", "state", ")", "\n", "", "elif", "key", "==", "\"stopReward\"", ":", "\n", "# this will contain a single reward for all agents, only really applicable when we are using anatomyReward", "\n", "                ", "shared_rewards", "[", "\"stopReward\"", "]", "=", "func", "(", "increment", ",", "self", ".", "rewards", "[", "\"anatomyReward\"", "]", ".", "get_anatomy_reward", "(", "seg", ")", ")", "\n", "", "elif", "key", "==", "\"oobPixelsReward\"", ":", "\n", "# this will contain a single reward for all agents", "\n", "                ", "shared_rewards", "[", "\"oobPixelsReward\"", "]", "=", "func", "(", "seg", ")", "\n", "", "elif", "key", "==", "\"oobReward\"", ":", "\n", "# this will contain a reward for each agent", "\n", "                ", "for", "i", ",", "point", "in", "enumerate", "(", "state", ")", ":", "\n", "                    ", "single_rewards", "[", "\"oobReward_%d\"", "%", "(", "i", "+", "1", ")", "]", "=", "func", "(", "point", ")", "\n", "\n", "# extract total rewards of each agent", "\n", "", "", "", "total_rewards", "=", "[", "sum", "(", "shared_rewards", ".", "values", "(", ")", ")", "]", "*", "self", ".", "config", ".", "n_agents", "\n", "if", "\"oobReward\"", "in", "self", ".", "rewards", ":", "\n", "            ", "for", "i", "in", "range", "(", "self", ".", "config", ".", "n_agents", ")", ":", "\n", "                ", "total_rewards", "[", "i", "]", "+=", "single_rewards", "[", "\"oobReward_%d\"", "%", "(", "i", "+", "1", ")", "]", "\n", "", "", "total_rewards", "=", "np", ".", "array", "(", "total_rewards", ")", "\n", "\n", "# log these rewards to the current episode count", "\n", "shared_rewards", ".", "update", "(", "single_rewards", ")", "\n", "for", "r", "in", "self", ".", "logged_rewards", ":", "\n", "            ", "self", ".", "current_logs", "[", "r", "]", "=", "shared_rewards", "[", "r", "]", "\n", "self", ".", "logs", "[", "r", "]", "+=", "shared_rewards", "[", "r", "]", "\n", "", "return", "total_rewards", "[", "...", ",", "np", ".", "newaxis", "]", ".", "astype", "(", "np", ".", "float", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.SingleVolumeEnvironment.step": [[188, 218], ["numpy.vstack", "xcatEnvironment.SingleVolumeEnvironment.sample_plane", "xcatEnvironment.SingleVolumeEnvironment.get_reward", "xcatEnvironment.SingleVolumeEnvironment.oscillates", "xcatEnvironment.SingleVolumeEnvironment.mapActionToIncrement", "tuple", "ValueError", "xcatEnvironment.SingleVolumeEnvironment.get_plane_coefs", "numpy.vstack.any"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.sample_plane", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.get_reward", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.mapActionToIncrement", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.get_plane_coefs"], ["", "def", "step", "(", "self", ",", "action", ",", "preprocess", "=", "False", ")", ":", "\n", "        ", "\"\"\"Perform an input action (discrete action), observe the next state and reward.\n        Params:\n        ==========\n            action (int): discrete action (see baseEnvironment.mapActionToIncrement())\n            preprocess (bool): if to preprocess the plane before returning it (unsqueeze to BxCxHxW, normalize)\n        \"\"\"", "\n", "# get the increment corresponding to this action", "\n", "increment", "=", "np", ".", "vstack", "(", "[", "self", ".", "mapActionToIncrement", "(", "act", ")", "for", "act", "in", "action", "]", ")", "\n", "# step into the next state", "\n", "state", "=", "self", ".", "state", "\n", "next_state", "=", "state", "+", "increment", "\n", "# observe the next plane and get the reward from segmentation map", "\n", "sample", "=", "self", ".", "sample_plane", "(", "state", "=", "next_state", ",", "return_seg", "=", "True", ",", "preprocess", "=", "preprocess", ")", "\n", "# get rewards", "\n", "rewards", "=", "self", ".", "get_reward", "(", "sample", "[", "\"seg\"", "]", ",", "next_state", ",", "increment", ")", "\n", "# update the current state", "\n", "self", ".", "state", "=", "next_state", "\n", "# episode termination, when done is True, the agent will consider only immediate rewards (no bootstrapping)", "\n", "if", "self", ".", "config", ".", "termination", "==", "\"oscillate\"", ":", "\n", "# check if oscillating for episode termination", "\n", "            ", "done", "=", "self", ".", "oscillates", "(", "tuple", "(", "self", ".", "get_plane_coefs", "(", "*", "next_state", ")", ")", ")", "\n", "", "elif", "self", ".", "config", ".", "termination", "==", "\"learned\"", ":", "\n", "# check if all agents chose to stop (increment will be all zero)", "\n", "            ", "done", "=", "not", "increment", ".", "any", "(", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'unknown termination method: {}'", ".", "format", "(", "self", ".", "config", ".", "termination", ")", ")", "\n", "\n", "# return transition and the sample", "\n", "", "return", "(", "state", ",", "action", ",", "rewards", ",", "next_state", ",", "done", ")", ",", "sample", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.SingleVolumeEnvironment.reset": [[219, 260], ["numpy.random.shuffle", "numpy.random.randint", "numpy.array", "numpy.array", "numpy.array", "numpy.vstack().astype", "xcatEnvironment.SingleVolumeEnvironment.get_plane_coefs", "xcatEnvironment.SingleVolumeEnvironment.oscillates.history.clear", "xcatEnvironment.SingleVolumeEnvironment.random_intensity_scaling.reset_intensities", "numpy.vstack", "numpy.random.uniform", "numpy.random.uniform", "numpy.random.uniform", "numpy.random.uniform", "numpy.random.uniform", "numpy.random.uniform", "numpy.random.uniform", "numpy.random.uniform", "numpy.random.uniform"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.get_plane_coefs", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.RandIntensity.reset_intensities"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "config", ".", "easy_objective", ":", "\n", "# shuffle rows of the goal state", "\n", "            ", "np", ".", "random", ".", "shuffle", "(", "self", ".", "goal_state", ")", "\n", "# add a random increment of +/- 10 pixels to this goal state", "\n", "noise", "=", "np", ".", "random", ".", "randint", "(", "low", "=", "-", "20", ",", "high", "=", "20", ",", "size", "=", "(", "3", ",", "3", ")", ")", "\n", "self", ".", "state", "=", "self", ".", "goal_state", "+", "noise", "\n", "", "else", ":", "\n", "# sample a random plane (defined by 3 points) to start the episode from", "\n", "            ", "pointA", "=", "np", ".", "array", "(", "[", "np", ".", "random", ".", "uniform", "(", "low", "=", "0.", ",", "high", "=", "1", ")", "*", "self", ".", "sx", "-", "1", ",", "\n", "np", ".", "random", ".", "uniform", "(", "low", "=", "0.", ",", "high", "=", "1", ")", "*", "self", ".", "sy", "-", "1", ",", "\n", "np", ".", "random", ".", "uniform", "(", "low", "=", "0.", ",", "high", "=", "1.", ")", "*", "self", ".", "sz", "-", "1", "]", ")", "\n", "\n", "pointB", "=", "np", ".", "array", "(", "[", "np", ".", "random", ".", "uniform", "(", "low", "=", "0.", ",", "high", "=", "1.", ")", "*", "self", ".", "sx", "-", "1", ",", "\n", "np", ".", "random", ".", "uniform", "(", "low", "=", "0.", ",", "high", "=", "1", ")", "*", "self", ".", "sy", "-", "1", ",", "\n", "np", ".", "random", ".", "uniform", "(", "low", "=", "0.", ",", "high", "=", "1.", ")", "*", "self", ".", "sz", "-", "1", "]", ")", "\n", "\n", "pointC", "=", "np", ".", "array", "(", "[", "np", ".", "random", ".", "uniform", "(", "low", "=", "0.", ",", "high", "=", "1.", ")", "*", "self", ".", "sx", "-", "1", ",", "\n", "np", ".", "random", ".", "uniform", "(", "low", "=", "0.", ",", "high", "=", "1", ")", "*", "self", ".", "sy", "-", "1", ",", "\n", "np", ".", "random", ".", "uniform", "(", "low", "=", "0.", ",", "high", "=", "1.", ")", "*", "self", ".", "sz", "-", "1", "]", ")", "\n", "# stack points to define the state", "\n", "self", ".", "state", "=", "np", ".", "vstack", "(", "[", "pointA", ",", "pointB", ",", "pointC", "]", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "\n", "# reset the logged rewards for this episode", "\n", "", "if", "self", ".", "logged_rewards", ":", "\n", "            ", "self", ".", "logs", "=", "{", "r", ":", "0", "for", "r", "in", "self", ".", "logged_rewards", "}", "\n", "self", ".", "current_logs", "=", "{", "r", ":", "0", "for", "r", "in", "self", ".", "logged_rewards", "}", "\n", "# set the previous plane attribute in the planeDistanceReward if present", "\n", "", "if", "\"planeDistanceReward\"", "in", "self", ".", "rewards", ":", "\n", "            ", "self", ".", "rewards", "[", "\"planeDistanceReward\"", "]", ".", "previous_plane", "=", "self", ".", "get_plane_coefs", "(", "*", "self", ".", "state", ")", "\n", "# set the previous anatomyReward if we are using incremental anatomy reward", "\n", "", "if", "\"anatomyReward\"", "in", "self", ".", "rewards", "and", "self", ".", "config", ".", "incrementalAnatomyReward", ":", "\n", "#sample = self.sample_plane(self.state, return_seg=True)", "\n", "#self.rewards[\"anatomyReward\"].previous_reward = self.rewards[\"anatomyReward\"].get_anatomy_reward(sample[\"seg\"])", "\n", "            ", "self", ".", "rewards", "[", "\"anatomyReward\"", "]", ".", "previous_reward", "=", "0", "\n", "# reset the oscillation monitoring", "\n", "", "if", "self", ".", "config", ".", "termination", "==", "\"oscillate\"", ":", "\n", "            ", "self", ".", "oscillates", ".", "history", ".", "clear", "(", ")", "\n", "# if we are using randomized intensitities (for navigation in fakeCT) reset the intensity ranges", "\n", "", "if", "self", ".", "config", ".", "randomize_intensities", ":", "\n", "            ", "self", ".", "random_intensity_scaling", ".", "reset_intensities", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.LocationAwareSingleVolumeEnvironment.__init__": [[270, 287], ["xcatEnvironment.SingleVolumeEnvironment.__init__", "numpy.zeros_like", "xcatEnvironment.LocationAwareSingleVolumeEnvironment.reset"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.reset"], ["def", "__init__", "(", "self", ",", "config", ",", "vol_id", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        Initialize an Environment object, the environment will focus on a single XCAT/CT volume.\n    \n        Params that we will use:\n        ======\n            config (argparse object): contains all options. see ./options/options.py for more details\n            vol_id (int): specifies which volume we want to load. Specifically ``config.volume_ids`` may contain\n                          all queried volumes by the main.py script. (i.e. parser.volume_ids = 'samp0,samp1,samp2,samp3,samp4,samp5,samp6,samp7')\n                          we will load the volume at position ``vol_id`` (default=0)               \n        \"\"\"", "\n", "# initialize environment", "\n", "SingleVolumeEnvironment", ".", "__init__", "(", "self", ",", "config", ",", "vol_id", ")", "\n", "# generate cube for agent position retrieval", "\n", "self", ".", "agents_cube", "=", "np", ".", "zeros_like", "(", "self", ".", "Volume", ")", "\n", "# reset the agent to get starting configuration", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.LocationAwareSingleVolumeEnvironment.sample_agents_position": [[289, 340], ["xcatEnvironment.is_in_volume", "xcatEnvironment.is_in_volume", "xcatEnvironment.is_in_volume", "numpy.stack", "xcatEnvironment.is_in_volume", "xcatEnvironment.is_in_volume", "xcatEnvironment.is_in_volume", "numpy.where", "numpy.where", "numpy.where", "numpy.meshgrid", "numpy.arange", "numpy.arange", "numpy.stack", "maps.append", "numpy.stack.max", "maps.append", "abs", "abs", "numpy.zeros_like"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.is_in_volume", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.is_in_volume", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.is_in_volume", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.is_in_volume", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.is_in_volume", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.is_in_volume"], ["", "def", "sample_agents_position", "(", "self", ",", "state", ",", "X", ",", "Y", ",", "Z", ")", ":", "\n", "\n", "        ", "\"\"\" function to get the agents postion on the sampled plane\n        Params:\n        ==========\n            state (np.ndarray of shape (3,3)): v-stacked 3D points that will define a particular plane in the CT volume.\n            X (np.ndarray ): X coordinates to sample from the volume (output of ``get_plane_from_points()``)\n            Y (np.ndarray ): Y coordinates to sample from the volume (output of ``get_plane_from_points()``)\n            Z (np.ndarray ): Z coordinates to sample from the volume (output of ``get_plane_from_points()``)\n\n            returns -> plane (np.array of shape (3, self.sx, self.sy)): 3 planes each containing one white dot representing \n                        the position of the corresponding agent\n        \"\"\"", "\n", "# Retrieve agents positions and empty volume", "\n", "A", ",", "B", ",", "C", "=", "state", "\n", "\n", "#Identify pixels where the agents are with unique values", "\n", "if", "is_in_volume", "(", "self", ".", "Volume", ",", "A", ")", ":", "\n", "            ", "self", ".", "agents_cube", "[", "A", "[", "0", "]", ",", "A", "[", "1", "]", ",", "A", "[", "2", "]", "]", "=", "1", "\n", "", "if", "is_in_volume", "(", "self", ".", "Volume", ",", "B", ")", ":", "\n", "            ", "self", ".", "agents_cube", "[", "B", "[", "0", "]", ",", "B", "[", "1", "]", ",", "B", "[", "2", "]", "]", "=", "2", "\n", "", "if", "is_in_volume", "(", "self", ".", "Volume", ",", "C", ")", ":", "\n", "            ", "self", ".", "agents_cube", "[", "C", "[", "0", "]", ",", "C", "[", "1", "]", ",", "C", "[", "2", "]", "]", "=", "3", "\n", "\n", "# Sample plane defined by the sample_plane function in the empty volume", "\n", "", "plane", "=", "self", ".", "agents_cube", "[", "X", ",", "Y", ",", "Z", "]", "\n", "# extract the 2D location of the agents", "\n", "loc1", ",", "loc2", ",", "loc3", "=", "np", ".", "where", "(", "plane", "==", "1", ")", ",", "np", ".", "where", "(", "plane", "==", "2", ")", ",", "np", ".", "where", "(", "plane", "==", "3", ")", "\n", "# make each map a smooth (distances from the white dot)", "\n", "maps", "=", "[", "]", "\n", "for", "loc", "in", "[", "loc1", ",", "loc2", ",", "loc3", "]", ":", "\n", "            ", "rows", ",", "cols", "=", "np", ".", "meshgrid", "(", "np", ".", "arange", "(", "self", ".", "sx", ")", ",", "np", ".", "arange", "(", "self", ".", "sy", ")", ")", "\n", "try", ":", "\n", "                ", "rows", "-=", "loc", "[", "0", "]", "\n", "cols", "-=", "loc", "[", "1", "]", "\n", "arr", "=", "np", ".", "stack", "(", "[", "abs", "(", "rows", ")", ",", "abs", "(", "cols", ")", "]", ")", "\n", "maps", ".", "append", "(", "arr", ".", "max", "(", "0", ")", ")", "\n", "", "except", ":", "\n", "                ", "maps", ".", "append", "(", "np", ".", "zeros_like", "(", "plane", ")", ")", "\n", "# Separate each agent into it's own channel, set them as white (255 since we use uint8)", "\n", "#plane = np.stack((plane == 1, plane == 2, plane == 3)).astype(np.uint8)*255", "\n", "", "", "plane", "=", "np", ".", "stack", "(", "maps", ")", "\n", "# reset the modified pixels to black", "\n", "if", "is_in_volume", "(", "self", ".", "Volume", ",", "A", ")", ":", "\n", "            ", "self", ".", "agents_cube", "[", "A", "[", "0", "]", ",", "A", "[", "1", "]", ",", "A", "[", "2", "]", "]", "=", "0", "\n", "", "if", "is_in_volume", "(", "self", ".", "Volume", ",", "B", ")", ":", "\n", "            ", "self", ".", "agents_cube", "[", "B", "[", "0", "]", ",", "B", "[", "1", "]", ",", "B", "[", "2", "]", "]", "=", "0", "\n", "", "if", "is_in_volume", "(", "self", ".", "Volume", ",", "C", ")", ":", "\n", "            ", "self", ".", "agents_cube", "[", "C", "[", "0", "]", ",", "C", "[", "1", "]", ",", "C", "[", "2", "]", "]", "=", "0", "\n", "\n", "", "return", "plane", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.LocationAwareSingleVolumeEnvironment.sample_plane": [[341, 381], ["xcatEnvironment.LocationAwareSingleVolumeEnvironment.get_plane_from_points", "xcatEnvironment.LocationAwareSingleVolumeEnvironment.sample_agents_position", "numpy.concatenate", "xcatEnvironment.LocationAwareSingleVolumeEnvironment.random_intensity_scaling"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.get_plane_from_points", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.LocationAwareSingleVolumeEnvironment.sample_agents_position"], ["", "def", "sample_plane", "(", "self", ",", "state", ",", "return_seg", "=", "False", ",", "oob_black", "=", "True", ",", "preprocess", "=", "False", ")", ":", "\n", "        ", "\"\"\" function to sample a plane from 3 3D points (state)\n        Params:\n        ==========\n            state (np.ndarray of shape (3,3)): v-stacked 3D points that will define a particular plane in the CT volume.\n            return_seg (bool): flag if we wish to return the corresponding segmentation map. (default=False)\n            oob_black (bool): flack if we wish to mask out of volume pixels to black. (default=True)\n            preprocess (bool): if to preprocess the plane before returning it (unsqueeze to BxCxHxW, normalize and/or add positional binary maps) \n            returns -> plane (torch.tensor of shape (1, 1, self.sy, self.sx)): corresponding plane sampled from the CT volume\n                       seg (optional, np.ndarray of shape (self.sy, self.sx)): segmentation map of the sampled plane\n        \"\"\"", "\n", "out", "=", "{", "}", "\n", "# 1. extract plane specs", "\n", "XYZ", ",", "P", ",", "S", "=", "self", ".", "get_plane_from_points", "(", "state", ",", "(", "self", ".", "sx", ",", "self", ".", "sy", ",", "self", ".", "sz", ")", ")", "\n", "# 2. sample plane from the current volume", "\n", "X", ",", "Y", ",", "Z", "=", "XYZ", "\n", "plane", "=", "self", ".", "Volume", "[", "X", ",", "Y", ",", "Z", "]", "\n", "# mask out of boundary pixels to black", "\n", "if", "oob_black", "==", "True", ":", "\n", "            ", "plane", "[", "P", "<", "0", "]", "=", "0", "\n", "plane", "[", "P", ">", "S", "]", "=", "0", "\n", "# concatenate binary location maps along channel diension", "\n", "", "pos", "=", "self", ".", "sample_agents_position", "(", "state", ",", "X", ",", "Y", ",", "Z", ")", "\n", "plane", "=", "np", ".", "concatenate", "(", "(", "plane", "[", "np", ".", "newaxis", ",", "...", "]", ",", "pos", ")", ",", "axis", "=", "0", ")", "\n", "# if needed randomize intensities", "\n", "if", "self", ".", "config", ".", "randomize_intensities", ":", "\n", "            ", "plane", "=", "self", ".", "random_intensity_scaling", "(", "plane", ")", "\n", "# normalize and unsqueeze array if needed.  if necessary.", "\n", "", "if", "preprocess", ":", "\n", "            ", "plane", "=", "plane", "[", "np", ".", "newaxis", ",", "...", "]", "/", "255", "\n", "# add to output", "\n", "", "out", "[", "\"plane\"", "]", "=", "plane", "\n", "# 3. sample the segmentation if needed", "\n", "if", "return_seg", ":", "\n", "            ", "seg", "=", "self", ".", "Segmentation", "[", "X", ",", "Y", ",", "Z", "]", "\n", "if", "oob_black", "==", "True", ":", "\n", "                ", "seg", "[", "P", "<", "0", "]", "=", "0", "\n", "seg", "[", "P", ">", "S", "]", "=", "0", "\n", "", "out", "[", "\"seg\"", "]", "=", "seg", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.RandIntensity.__init__": [[386, 392], ["xcatEnvironment.RandIntensity.reset_intensities"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.RandIntensity.reset_intensities"], ["def", "__init__", "(", "self", ",", "min_range", "=", "100", ",", "max_range", "=", "200", ",", "imin", "=", "0", ",", "imax", "=", "255", ")", ":", "\n", "        ", "self", ".", "rmin", "=", "min_range", "\n", "self", ".", "rmax", "=", "max_range", "\n", "self", ".", "imin", "=", "imin", "\n", "self", ".", "imax", "=", "imax", "\n", "self", ".", "reset_intensities", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.RandIntensity.reset_intensities": [[393, 396], ["int", "int", "numpy.random.randint", "numpy.random.randint"], "methods", ["None"], ["", "def", "reset_intensities", "(", "self", ")", ":", "\n", "        ", "self", ".", "trange", "=", "int", "(", "np", ".", "random", ".", "randint", "(", "low", "=", "self", ".", "rmin", ",", "high", "=", "self", ".", "rmax", ",", "size", "=", "(", "1", ",", ")", ")", ")", "\n", "self", ".", "tmin", "=", "int", "(", "np", ".", "random", ".", "randint", "(", "low", "=", "self", ".", "imin", ",", "high", "=", "self", ".", "imax", "-", "self", ".", "trange", ",", "size", "=", "(", "1", ",", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.RandIntensity.__call__": [[397, 403], ["arr.min", "arr.max", "arr.min"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "arr", ")", ":", "\n", "#set to 0-1 range", "\n", "        ", "arr", "=", "(", "arr", "-", "arr", ".", "min", "(", ")", ")", "/", "(", "arr", ".", "max", "(", ")", "-", "arr", ".", "min", "(", ")", ")", "\n", "#set to new range (0-255)", "\n", "arr", "=", "arr", "*", "self", ".", "trange", "+", "self", ".", "tmin", "\n", "return", "arr", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.get_centroid": [[404, 410], ["numpy.array", "scipy.measurements.center_of_mass"], "function", ["None"], ["", "", "def", "get_centroid", "(", "data", ",", "seg_id", ",", "norm", "=", "False", ")", ":", "\n", "    ", "volume", "=", "data", "==", "seg_id", "# Get binary map for the specified seg id", "\n", "values", "=", "np", ".", "array", "(", "ndimage", ".", "measurements", ".", "center_of_mass", "(", "volume", ")", ",", "dtype", "=", "np", ".", "int", ")", "\n", "if", "norm", ":", "\n", "        ", "values", "=", "values", "/", "data", ".", "shape", "\n", "", "return", "values", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.intensity_scaling": [[411, 422], ["ndarr.min", "ndarr.max"], "function", ["None"], ["", "def", "intensity_scaling", "(", "ndarr", ",", "pmin", "=", "None", ",", "pmax", "=", "None", ",", "nmin", "=", "None", ",", "nmax", "=", "None", ")", ":", "\n", "    ", "pmin", "=", "pmin", "if", "pmin", "!=", "None", "else", "ndarr", ".", "min", "(", ")", "\n", "pmax", "=", "pmax", "if", "pmax", "!=", "None", "else", "ndarr", ".", "max", "(", ")", "\n", "nmin", "=", "nmin", "if", "nmin", "!=", "None", "else", "pmin", "\n", "nmax", "=", "nmax", "if", "nmax", "!=", "None", "else", "pmax", "\n", "\n", "ndarr", "[", "ndarr", "<", "pmin", "]", "=", "pmin", "\n", "ndarr", "[", "ndarr", ">", "pmax", "]", "=", "pmax", "\n", "ndarr", "=", "(", "ndarr", "-", "pmin", ")", "/", "(", "pmax", "-", "pmin", ")", "\n", "ndarr", "=", "ndarr", "*", "(", "nmax", "-", "nmin", ")", "+", "nmin", "\n", "return", "ndarr", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.draw_sphere": [[423, 428], ["numpy.indices", "numpy.sqrt"], "function", ["None"], ["", "def", "draw_sphere", "(", "arr", ",", "point", ",", "r", "=", "3", ",", "color", "=", "255", ")", ":", "\n", "    ", "i", ",", "j", ",", "k", "=", "np", ".", "indices", "(", "arr", ".", "shape", ")", "\n", "dist", "=", "np", ".", "sqrt", "(", "(", "point", "[", "0", "]", "-", "i", ")", "**", "2", "+", "(", "point", "[", "1", "]", "-", "j", ")", "**", "2", "+", "(", "point", "[", "2", "]", "-", "k", ")", "**", "2", ")", "\n", "arr", "[", "dist", "<", "r", "]", "=", "color", "\n", "return", "arr", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.is_in_volume": [[429, 432], ["None"], "function", ["None"], ["", "def", "is_in_volume", "(", "volume", ",", "point", ")", ":", "\n", "    ", "sx", ",", "sy", ",", "sz", "=", "volume", ".", "shape", "\n", "return", "(", "point", "[", "0", "]", ">=", "0", "and", "point", "[", "0", "]", "<", "sx", ")", "and", "(", "point", "[", "1", "]", ">=", "0", "and", "point", "[", "1", "]", "<", "sy", ")", "and", "(", "point", "[", "2", "]", ">=", "0", "and", "point", "[", "2", "]", "<", "sz", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.xcatEnvironment.doNothing": [[434, 436], ["None"], "function", ["None"], ["", "def", "doNothing", "(", ")", ":", "\n", "    ", "return", "None", "", "", ""]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.realCTenvironment.realCTtestEnvironment.__init__": [[8, 37], ["environment.baseEnvironment.BaseEnvironment.__init__", "SimpleITK.ReadImage", "SimpleITK.GetArrayFromImage", "SimpleITK.GetArrayFromImage.astype", "realCTenvironment.realCTtestEnvironment.reset", "os.path.join", "Oscillate"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.reset"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "vol_id", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        Initialize an Environment object, the environment will focus on a single real CT volume.\n    \n        Params that we will use:\n        ======\n            config (argparse object): contains all options. see ./options/options.py for more details\n            vol_id (int): specifies which volume we want to load. Specifically ``config.volume_ids`` may contain\n                          all queried volumes by the main.py script. (i.e. parser.volume_ids = 'samp0,samp1,samp2,samp3,samp4,samp5,samp6,samp7')\n                          we will load the volume at position ``vol_id`` (default=0)               \n        \"\"\"", "\n", "\n", "# initialize the base environment", "\n", "BaseEnvironment", ".", "__init__", "(", "self", ",", "config", ")", "\n", "# identify the volume used by this environment instance", "\n", "self", ".", "vol_id", "=", "vol_id", "\n", "\n", "# load queried CT volume", "\n", "itkVolume", "=", "sitk", ".", "ReadImage", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataroot", ",", "self", ".", "vol_id", "+", "\".nii.gz\"", ")", ")", "\n", "Volume", "=", "sitk", ".", "GetArrayFromImage", "(", "itkVolume", ")", "\n", "self", ".", "Volume", "=", "Volume", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "self", ".", "sx", ",", "self", ".", "sy", ",", "self", ".", "sz", "=", "self", ".", "Volume", ".", "shape", "\n", "\n", "# monitor oscillations for termination", "\n", "if", "config", ".", "termination", "==", "\"oscillate\"", ":", "\n", "            ", "self", ".", "oscillates", "=", "Oscillate", "(", "history_length", "=", "config", ".", "termination_history_len", ",", "stop_freq", "=", "config", ".", "termination_oscillation_freq", ")", "\n", "\n", "# get starting configuration and reset environment for a new episode", "\n", "", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.realCTenvironment.realCTtestEnvironment.sample_plane": [[38, 69], ["realCTenvironment.realCTtestEnvironment.get_plane_from_points", "realCTenvironment.realCTtestEnvironment.random_intensity_scaling"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.get_plane_from_points"], ["", "def", "sample_plane", "(", "self", ",", "state", ",", "return_seg", "=", "False", ",", "oob_black", "=", "True", ",", "preprocess", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\" function to sample a plane from 3 3D points (state)\n        Params:\n        ==========\n            state (np.ndarray of shape (3,3)): v-stacked 3D points that will define a particular plane in the CT volume.\n            return_seg (bool): flag if we wish to return the corresponding segmentation map. (default=False)\n            oob_black (bool): flack if we wish to mask out of volume pixels to black. (default=True)\n            preprocess (bool): if to preprocess the plane before returning it (unsqueeze to BxCxHxW, normalize) \n\n            returns -> \n                plane (torch.tensor of shape (1, 1, self.sy, self.sx)): corresponding plane sampled from the CT volume\n        \"\"\"", "\n", "out", "=", "{", "}", "\n", "# 1. extract plane specs", "\n", "XYZ", ",", "P", ",", "S", "=", "self", ".", "get_plane_from_points", "(", "state", ",", "(", "self", ".", "sx", ",", "self", ".", "sy", ",", "self", ".", "sz", ")", ")", "\n", "# 2. sample plane from the current volume", "\n", "X", ",", "Y", ",", "Z", "=", "XYZ", "\n", "plane", "=", "self", ".", "Volume", "[", "X", ",", "Y", ",", "Z", "]", "\n", "# mask out of boundary pixels to black", "\n", "if", "oob_black", "==", "True", ":", "\n", "            ", "plane", "[", "P", "<", "0", "]", "=", "0", "\n", "plane", "[", "P", ">", "S", "]", "=", "0", "\n", "# if needed randomize intensities", "\n", "", "if", "self", ".", "config", ".", "randomize_intensities", ":", "\n", "            ", "plane", "=", "self", ".", "random_intensity_scaling", "(", "plane", ")", "\n", "# normalize and unsqueeze array if needed.", "\n", "", "if", "preprocess", ":", "\n", "            ", "plane", "=", "plane", "[", "np", ".", "newaxis", ",", "np", ".", "newaxis", ",", "...", "]", "/", "255", "\n", "# add to output", "\n", "", "out", "[", "\"plane\"", "]", "=", "plane", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.realCTenvironment.realCTtestEnvironment.step": [[70, 98], ["numpy.vstack", "realCTenvironment.realCTtestEnvironment.sample_plane", "realCTenvironment.realCTtestEnvironment.oscillates", "realCTenvironment.realCTtestEnvironment.mapActionToIncrement", "tuple", "ValueError", "realCTenvironment.realCTtestEnvironment.get_plane_coefs", "numpy.vstack.any"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.sample_plane", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.mapActionToIncrement", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.get_plane_coefs"], ["", "def", "step", "(", "self", ",", "action", ",", "preprocess", "=", "False", ")", ":", "\n", "        ", "\"\"\"Perform an input action (discrete action), observe the next state and reward.\n        Params:\n        ==========\n            action (int): discrete action (see baseEnvironment.mapActionToIncrement())\n            preprocess (bool): if to preprocess the plane before returning it (unsqueeze to BxCxHxW, normalize)\n        \"\"\"", "\n", "# get the increment corresponding to this action", "\n", "increment", "=", "np", ".", "vstack", "(", "[", "self", ".", "mapActionToIncrement", "(", "act", ")", "for", "act", "in", "action", "]", ")", "\n", "# step into the next state", "\n", "state", "=", "self", ".", "state", "\n", "next_state", "=", "state", "+", "increment", "\n", "# observe the next plane and get the reward from segmentation map", "\n", "sample", "=", "self", ".", "sample_plane", "(", "state", "=", "next_state", ",", "return_seg", "=", "True", ",", "preprocess", "=", "preprocess", ")", "\n", "# update the current state", "\n", "self", ".", "state", "=", "next_state", "\n", "# episode termination, when done is True, the agent will consider only immediate rewards (no bootstrapping)", "\n", "if", "self", ".", "config", ".", "termination", "==", "\"oscillate\"", ":", "\n", "# check if oscillating for episode termination", "\n", "            ", "done", "=", "self", ".", "oscillates", "(", "tuple", "(", "self", ".", "get_plane_coefs", "(", "*", "next_state", ")", ")", ")", "\n", "", "elif", "self", ".", "config", ".", "termination", "==", "\"learned\"", ":", "\n", "# check if all agents chose to stop (increment will be all zero)", "\n", "            ", "done", "=", "not", "increment", ".", "any", "(", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'unknown termination method: {}'", ".", "format", "(", "self", ".", "config", ".", "termination", ")", ")", "\n", "\n", "# return transition and the sample", "\n", "", "return", "(", "state", ",", "action", ",", "next_state", ",", "done", ")", ",", "sample", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.realCTenvironment.realCTtestEnvironment.reset": [[99, 125], ["numpy.random.shuffle", "numpy.random.randint", "numpy.array", "numpy.array", "numpy.array", "numpy.vstack().astype", "realCTenvironment.realCTtestEnvironment.oscillates.history.clear", "numpy.vstack", "numpy.random.uniform", "numpy.random.uniform", "numpy.random.uniform", "numpy.random.uniform", "numpy.random.uniform", "numpy.random.uniform", "numpy.random.uniform", "numpy.random.uniform", "numpy.random.uniform"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "config", ".", "easy_objective", ":", "\n", "# shuffle rows of the goal state", "\n", "            ", "np", ".", "random", ".", "shuffle", "(", "self", ".", "goal_state", ")", "\n", "# add a random increment of +/- 10 pixels to this goal state", "\n", "noise", "=", "np", ".", "random", ".", "randint", "(", "low", "=", "-", "20", ",", "high", "=", "20", ",", "size", "=", "(", "3", ",", "3", ")", ")", "\n", "self", ".", "state", "=", "self", ".", "goal_state", "+", "noise", "\n", "", "else", ":", "\n", "# sample a random plane (defined by 3 points) to start the episode from", "\n", "            ", "pointA", "=", "np", ".", "array", "(", "[", "np", ".", "random", ".", "uniform", "(", "low", "=", "0.", ",", "high", "=", "1", ")", "*", "self", ".", "sx", "-", "1", ",", "\n", "np", ".", "random", ".", "uniform", "(", "low", "=", "0.", ",", "high", "=", "1", ")", "*", "self", ".", "sy", "-", "1", ",", "\n", "np", ".", "random", ".", "uniform", "(", "low", "=", "0.", ",", "high", "=", "1.", ")", "*", "self", ".", "sz", "-", "1", "]", ")", "\n", "\n", "pointB", "=", "np", ".", "array", "(", "[", "np", ".", "random", ".", "uniform", "(", "low", "=", "0.", ",", "high", "=", "1.", ")", "*", "self", ".", "sx", "-", "1", ",", "\n", "np", ".", "random", ".", "uniform", "(", "low", "=", "0.", ",", "high", "=", "1", ")", "*", "self", ".", "sy", "-", "1", ",", "\n", "np", ".", "random", ".", "uniform", "(", "low", "=", "0.", ",", "high", "=", "1.", ")", "*", "self", ".", "sz", "-", "1", "]", ")", "\n", "\n", "pointC", "=", "np", ".", "array", "(", "[", "np", ".", "random", ".", "uniform", "(", "low", "=", "0.", ",", "high", "=", "1.", ")", "*", "self", ".", "sx", "-", "1", ",", "\n", "np", ".", "random", ".", "uniform", "(", "low", "=", "0.", ",", "high", "=", "1", ")", "*", "self", ".", "sy", "-", "1", ",", "\n", "np", ".", "random", ".", "uniform", "(", "low", "=", "0.", ",", "high", "=", "1.", ")", "*", "self", ".", "sz", "-", "1", "]", ")", "\n", "# stack points to define the state", "\n", "self", ".", "state", "=", "np", ".", "vstack", "(", "[", "pointA", ",", "pointB", ",", "pointC", "]", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "\n", "# reset the oscillation monitoring", "\n", "", "if", "self", ".", "config", ".", "termination", "==", "\"oscillate\"", ":", "\n", "            ", "self", ".", "oscillates", ".", "history", ".", "clear", "(", ")", "", "", "", "", ""]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.__init__": [[11, 28], ["os.path.join", "os.path.join", "os.path.exists", "os.makedirs", "os.path.exists", "os.makedirs"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "\"\"\" Initialize environment object\n        Params:\n        =========\n            config (argparse object): all useful options to build the environment (see options/options.py for details)\n        \"\"\"", "\n", "\n", "# setup data, checkpoints and results dirs for any logging/ input output", "\n", "self", ".", "dataroot", "=", "config", ".", "dataroot", "\n", "self", ".", "checkpoints_dir", "=", "os", ".", "path", ".", "join", "(", "config", ".", "checkpoints_dir", ",", "config", ".", "name", ")", "\n", "self", ".", "results_dir", "=", "os", ".", "path", ".", "join", "(", "config", ".", "results_dir", ",", "config", ".", "name", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "checkpoints_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "checkpoints_dir", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "results_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "results_dir", ")", "\n", "# save the config for any options we might need", "\n", "", "self", ".", "config", "=", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.sample_plane": [[29, 33], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "sample_plane", "(", "self", ",", "state", ",", "return_seg", "=", "False", ",", "oob_black", "=", "True", ")", ":", "\n", "        ", "\"\"\" Universal function to sample a plane from 3 3D points\n        \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.step": [[34, 39], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "step", "(", "self", ",", "action", ",", "buffer", ")", ":", "\n", "        ", "\"\"\"Perform an input action, observe the next state and reward.\n        Automatically stores the tuple (state, action, reward, next_state) to the replay buffer.\n        \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.reset": [[40, 45], ["NotImplementedError"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"Reset the environment at the end of an episode. It will set a random initial state for the next episode\n        and reset all the logs that the environment is keeping.\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.get_reward": [[46, 50], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_reward", "(", "self", ")", ":", "\n", "        ", "\"\"\"Calculates the corresponding reward of stepping into a new state.\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.sample_planes": [[51, 73], ["concurrent.futures.ThreadPoolExecutor", "samples.append", "executor.submit", "f.result"], "methods", ["None"], ["", "def", "sample_planes", "(", "self", ",", "states", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Sample multiple queried planes launching multiple threads in parallel. This function is useful if the self.sample_plane()\n        function is time consuming and we whish to query several planes. And example of this scenario is when we sample a batch from\n        the replay buffer: it is impractical to store full image frames in the replay buffer, it is more efficient to only store the\n        coordinates of the 3 points we need in order to sample the corresponding plane. This means that every time we train the Qnetwork\n        we need to sample planes for a batch of states and next states. If sampling a single plane is costly, this will lead to an \n        extremely slow training.\n\n        Params\n        ==========\n        states (list/tuple), all states that we wish to sample.\n\n        returns -> planes (list), a list containing all sampled planes\n        \"\"\"", "\n", "# sample planes using multi-thread", "\n", "samples", "=", "[", "]", "\n", "with", "concurrent", ".", "futures", ".", "ThreadPoolExecutor", "(", ")", "as", "executor", ":", "\n", "            ", "futures", "=", "[", "executor", ".", "submit", "(", "self", ".", "sample_plane", ",", "state", ",", "**", "kwargs", ")", "for", "state", "in", "states", "]", "\n", "", "[", "samples", ".", "append", "(", "f", ".", "result", "(", ")", ")", "for", "f", "in", "futures", "]", "\n", "# convert list of dicts to dict of lists (assumes all dicts have same keys, which is our case)", "\n", "samples", "=", "{", "k", ":", "[", "dic", "[", "k", "]", "for", "dic", "in", "samples", "]", "for", "k", "in", "samples", "[", "0", "]", "}", "\n", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.random_walk": [[74, 110], ["baseEnvironment.BaseEnvironment.reset", "range", "numpy.vstack", "baseEnvironment.BaseEnvironment.step", "trajectory.append", "baseEnvironment.BaseEnvironment.reset", "random.choice", "buffer.add", "buffer.add", "numpy.arange", "range"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.reset", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.trainers.PrioritizedDoubleDeepQLearning.step", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.reset", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.RecurrentPrioritizedReplayBuffer.add", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.RecurrentPrioritizedReplayBuffer.add"], ["", "def", "random_walk", "(", "self", ",", "n_random_steps", ",", "buffer", "=", "None", ",", "return_trajectory", "=", "False", ")", ":", "\n", "        ", "\"\"\" Starts a random walk to gather observations (s, a, r, s').\n        Will return the number of unique states reached by each agent to quantify the amount of exploration\n        Params:\n        ==========\n            n_random_steps (int): number of steps for which we want the random walk to continue.\n            buffer (buffer/* instance): if passed, a ReplayBuffer instance to collect memory.\n            return_trajectory (bool): if True we return the trajectory followed by the agent.\n        \"\"\"", "\n", "# get the trajectory if needed", "\n", "if", "return_trajectory", ":", "\n", "            ", "trajectory", "=", "[", "]", "\n", "# start the random walk", "\n", "", "self", ".", "reset", "(", ")", "\n", "is_first_time_step", "=", "True", "\n", "for", "step", "in", "range", "(", "1", ",", "n_random_steps", "+", "1", ")", ":", "\n", "# random action", "\n", "            ", "action", "=", "np", ".", "vstack", "(", "[", "random", ".", "choice", "(", "np", ".", "arange", "(", "self", ".", "config", ".", "action_size", ")", ")", "for", "_", "in", "range", "(", "self", ".", "config", ".", "n_agents", ")", "]", ")", "\n", "# step the environment according to this random action", "\n", "transition", ",", "_", "=", "self", ".", "step", "(", "action", ")", "# do not need the next_slice in random walk (only random actions)", "\n", "# add (state, action, reward, next_state) to buffer", "\n", "if", "buffer", "is", "not", "None", ":", "\n", "                ", "if", "self", ".", "config", ".", "recurrent", ":", "\n", "                    ", "buffer", ".", "add", "(", "transition", ",", "is_first_time_step", "=", "is_first_time_step", ")", "\n", "is_first_time_step", "=", "False", "\n", "", "else", ":", "\n", "                    ", "buffer", ".", "add", "(", "transition", ")", "\n", "# get the visual if needed", "\n", "", "", "if", "return_trajectory", ":", "\n", "                ", "trajectory", ".", "append", "(", "self", ".", "state", ")", "\n", "# restart environment after max steps per episode are reached", "\n", "", "if", "step", "%", "self", ".", "config", ".", "n_steps_per_episode", "==", "0", ":", "\n", "                ", "self", ".", "reset", "(", ")", "\n", "is_first_time_step", "=", "True", "\n", "", "", "if", "return_trajectory", ":", "\n", "            ", "return", "trajectory", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.get_plane_from_points": [[111, 164], ["baseEnvironment.BaseEnvironment.get_plane_coefs", "numpy.argmax", "numpy.meshgrid", "X.round().astype.round().astype.round().astype", "X.round().astype.round().astype.copy", "abs", "abs", "abs", "numpy.arange", "numpy.arange", "numpy.meshgrid", "Y.round().astype.round().astype.round().astype", "Y.round().astype.round().astype.copy", "X.round().astype.round().astype.round", "numpy.arange", "numpy.arange", "numpy.meshgrid", "Z.round().astype.round().astype.round().astype", "Z.round().astype.round().astype.copy", "Y.round().astype.round().astype.round", "numpy.arange", "numpy.arange", "Z.round().astype.round().astype.round"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.get_plane_coefs"], ["", "", "def", "get_plane_from_points", "(", "self", ",", "points", ",", "shape", ")", ":", "\n", "            ", "\"\"\" function to sample a plane from 3 3D points (state)\n            Params:\n            ==========\n                points (np.ndarray of shape (3,3)): v-stacked 3D points that will define a particular plane in the volume.\n                shape (np.ndarry): shape of the 3D volume to sample plane from\n\n                returns -> (X,Y,Z) ndarrays that will index Volume in order to extract a specific plane.\n            \"\"\"", "\n", "# get plane coefs", "\n", "a", ",", "b", ",", "c", ",", "d", "=", "self", ".", "get_plane_coefs", "(", "*", "points", ")", "\n", "# get volume shape", "\n", "sx", ",", "sy", ",", "sz", "=", "shape", "\n", "# extract corresponding slice", "\n", "main_ax", "=", "np", ".", "argmax", "(", "[", "abs", "(", "a", ")", ",", "abs", "(", "b", ")", ",", "abs", "(", "c", ")", "]", ")", "\n", "# if easy objective then force the main_Ax to be zero", "\n", "if", "self", ".", "config", ".", "easy_objective", ":", "\n", "                ", "main_ax", "=", "0", "\n", "\n", "", "if", "main_ax", "==", "0", ":", "\n", "                ", "Y", ",", "Z", "=", "np", ".", "meshgrid", "(", "np", ".", "arange", "(", "sy", ")", ",", "np", ".", "arange", "(", "sz", ")", ",", "indexing", "=", "'ij'", ")", "\n", "X", "=", "(", "d", "-", "b", "*", "Y", "-", "c", "*", "Z", ")", "/", "a", "\n", "\n", "X", "=", "X", ".", "round", "(", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "P", "=", "X", ".", "copy", "(", ")", "\n", "S", "=", "sx", "-", "1", "\n", "\n", "X", "[", "X", "<=", "0", "]", "=", "0", "\n", "X", "[", "X", ">=", "sx", "]", "=", "sx", "-", "1", "\n", "\n", "", "elif", "main_ax", "==", "1", ":", "\n", "                ", "X", ",", "Z", "=", "np", ".", "meshgrid", "(", "np", ".", "arange", "(", "sx", ")", ",", "np", ".", "arange", "(", "sz", ")", ",", "indexing", "=", "'ij'", ")", "\n", "Y", "=", "(", "d", "-", "a", "*", "X", "-", "c", "*", "Z", ")", "/", "b", "\n", "\n", "Y", "=", "Y", ".", "round", "(", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "P", "=", "Y", ".", "copy", "(", ")", "\n", "S", "=", "sy", "-", "1", "\n", "\n", "Y", "[", "Y", "<=", "0", "]", "=", "0", "\n", "Y", "[", "Y", ">=", "sy", "]", "=", "sy", "-", "1", "\n", "\n", "", "elif", "main_ax", "==", "2", ":", "\n", "                ", "X", ",", "Y", "=", "np", ".", "meshgrid", "(", "np", ".", "arange", "(", "sx", ")", ",", "np", ".", "arange", "(", "sy", ")", ",", "indexing", "=", "'ij'", ")", "\n", "Z", "=", "(", "d", "-", "a", "*", "X", "-", "b", "*", "Y", ")", "/", "c", "\n", "\n", "Z", "=", "Z", ".", "round", "(", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "P", "=", "Z", ".", "copy", "(", ")", "\n", "S", "=", "sz", "-", "1", "\n", "\n", "Z", "[", "Z", "<=", "0", "]", "=", "0", "\n", "Z", "[", "Z", ">=", "sz", "]", "=", "sz", "-", "1", "\n", "\n", "", "return", "(", "X", ",", "Y", ",", "Z", ")", ",", "P", ",", "S", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.get_plane_coefs": [[165, 195], ["numpy.cross", "numpy.dot", "numpy.sum", "numpy.array", "abs", "abs", "abs", "abs"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_plane_coefs", "(", "p1", ",", "p2", ",", "p3", ")", ":", "\n", "        ", "\"\"\" Gets the coefficients of a 3D plane given the coordinates of 3 3D points\n        \"\"\"", "\n", "# These two vectors are in the plane", "\n", "v1", "=", "p3", "-", "p1", "\n", "v2", "=", "p2", "-", "p1", "\n", "# the cross product is a vector normal to the plane", "\n", "cp", "=", "np", ".", "cross", "(", "v1", ",", "v2", ")", "\n", "a", ",", "b", ",", "c", "=", "cp", "\n", "# This evaluates a * x3 + b * y3 + c * z3 which equals d", "\n", "d", "=", "np", ".", "dot", "(", "cp", ",", "p3", ")", "\n", "\n", "# normalize the coeffs (they would still define the same plane)", "\n", "norm", "=", "np", ".", "sum", "(", "[", "abs", "(", "a", ")", ",", "abs", "(", "b", ")", ",", "abs", "(", "c", ")", ",", "abs", "(", "d", ")", "]", ")", "\n", "if", "norm", ">", "0", ":", "\n", "            ", "a", "/=", "norm", "\n", "b", "/=", "norm", "\n", "c", "/=", "norm", "\n", "d", "/=", "norm", "\n", "\n", "# # ensure a is always positive to have consistency in the distance metric (a plane where all coeff are multiplied by -1 is still the same plane,", "\n", "# # but if you take the distance between a plane and a plane*-1 you are going to get a large number)", "\n", "# if a<0:", "\n", "#     a*=-1", "\n", "#     b*=-1", "\n", "#     c*=-1", "\n", "#     d*=-1", "\n", "\n", "", "return", "np", ".", "array", "(", "[", "a", ",", "b", ",", "c", ",", "d", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.mapActionToIncrement": [[196, 230], ["numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "ValueError"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "mapActionToIncrement", "(", "action", ")", ":", "\n", "        ", "\"\"\" Maps a discrete action to a specific increment that will be added to the state in self.step() in order\n        to move towards the next state.\n\n        It uses the following convention for 3D navigation:\n        0: +1 in x coordinate\n        1: -1 in x coordinate\n        2: +1 in y coordinate\n        3: -1 in y coordinate\n        4: +1 in z coordinate\n        5: -1 in z coordinate\n        6: stays still/does not lead to an increment\n\n        returns -> incr (np.ndarray) of shape: (3,). It contains a unit increment in a particular 3D direction.\n        \"\"\"", "\n", "if", "action", "==", "0", ":", "\n", "            ", "incr", "=", "np", ".", "array", "(", "[", "1", ",", "0", ",", "0", "]", ")", "\n", "", "elif", "action", "==", "1", ":", "\n", "            ", "incr", "=", "np", ".", "array", "(", "[", "-", "1", ",", "0", ",", "0", "]", ")", "\n", "", "elif", "action", "==", "2", ":", "\n", "            ", "incr", "=", "np", ".", "array", "(", "[", "0", ",", "1", ",", "0", "]", ")", "\n", "", "elif", "action", "==", "3", ":", "\n", "            ", "incr", "=", "np", ".", "array", "(", "[", "0", ",", "-", "1", ",", "0", "]", ")", "\n", "", "elif", "action", "==", "4", ":", "\n", "            ", "incr", "=", "np", ".", "array", "(", "[", "0", ",", "0", ",", "1", "]", ")", "\n", "", "elif", "action", "==", "5", ":", "\n", "            ", "incr", "=", "np", ".", "array", "(", "[", "0", ",", "0", ",", "-", "1", "]", ")", "\n", "", "elif", "action", "==", "6", ":", "\n", "            ", "incr", "=", "np", ".", "array", "(", "[", "0", ",", "0", ",", "0", "]", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'unknown action: %d, legal actions: [0, 1, 2, 3, 4, 5]'", "%", "action", ")", "\n", "\n", "", "return", "incr", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.mapIncrementToAction": [[231, 265], ["ValueError"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "mapIncrementToAction", "(", "incr", ")", ":", "\n", "        ", "\"\"\" Maps a specific increment to a discrete action, performing the inverse mapping to self.mapActionToIncrement().\n        This function is useful if we want to store discrete actions rather than the corresponding increments in the replay buffer\n        Since in the QLearning update we need such discrete form of the action.\n\n        It uses the following convention:\n        0: +1 in x coordinate\n        1: -1 in x coordinate\n        2: +1 in y coordinate\n        3: -1 in y coordinate\n        4: +1 in z coordinate\n        5: -1 in z coordinate\n        6: stays still/does not lead to an increment\n\n        returns -> action (int). It contains the discrete action of a particular 3D unit movement.\n        \"\"\"", "\n", "if", "incr", "[", "0", "]", "==", "1", ":", "\n", "            ", "action", "=", "0", "\n", "", "elif", "incr", "[", "0", "]", "==", "-", "1", ":", "\n", "            ", "action", "=", "1", "\n", "", "elif", "incr", "[", "1", "]", "==", "1", ":", "\n", "            ", "action", "=", "2", "\n", "", "elif", "incr", "[", "1", "]", "==", "-", "1", ":", "\n", "            ", "action", "=", "3", "\n", "", "elif", "incr", "[", "2", "]", "==", "1", ":", "\n", "            ", "action", "=", "4", "\n", "", "elif", "incr", "[", "2", "]", "==", "-", "1", ":", "\n", "            ", "action", "=", "5", "\n", "", "elif", "incr", "[", "0", "]", "==", "0", "and", "incr", "[", "1", "]", "==", "0", "and", "incr", "[", "2", "]", "==", "0", ":", "\n", "            ", "action", "=", "6", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'unknown increment: {}.'", ".", "format", "(", "incr", ")", ")", "\n", "", "return", "action", "", "", "", ""]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.visualisation.visualizers.Visualizer.__init__": [[21, 25], ["os.path.join", "os.path.exists", "os.makedirs"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "savedir", ")", ":", "\n", "        ", "self", ".", "savedir", "=", "os", ".", "path", ".", "join", "(", "savedir", ",", "\"visuals\"", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "savedir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "savedir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.visualisation.visualizers.Visualizer.preprocess_inputs": [[26, 40], ["numpy.array", "len", "numpy.split", "new_args.extend", "numpy.array", "new_args.append", "numpy.array", "a.squeeze", "a.max"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "preprocess_inputs", "(", "*", "args", ")", ":", "\n", "        ", "new_args", "=", "[", "]", "\n", "for", "arg", "in", "args", ":", "\n", "            ", "arg", "=", "np", ".", "array", "(", "arg", ")", "\n", "if", "len", "(", "arg", ".", "shape", ")", ">", "3", ":", "\n", "# convert to list of 1channel trajectories, normalize and append to new args", "\n", "                ", "arg", "=", "np", ".", "split", "(", "arg", ",", "arg", ".", "shape", "[", "1", "]", ",", "axis", "=", "1", ")", "\n", "new_args", ".", "extend", "(", "[", "a", ".", "squeeze", "(", ")", "/", "a", ".", "max", "(", ")", "for", "a", "in", "arg", "]", ")", "\n", "", "else", ":", "\n", "# trajectory was already 1 channel (still normalize it)", "\n", "                ", "arg", "=", "np", ".", "array", "(", "arg", ")", "\n", "new_args", ".", "append", "(", "np", ".", "array", "(", "arg", ")", ")", "\n", "", "", "return", "new_args", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.visualisation.visualizers.Visualizer.stack_trajectories": [[41, 54], ["len", "numpy.concatenate", "int", "int", "range", "numpy.concatenate", "numpy.ones_like", "range"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "stack_trajectories", "(", "*", "args", ",", "n_rows", ")", ":", "\n", "# 1. get grid specs", "\n", "        ", "N", "=", "len", "(", "args", ")", "\n", "n_cols", "=", "int", "(", "N", "/", "n_rows", ")", "+", "int", "(", "not", "N", "%", "n_rows", "==", "0", ")", "\n", "n_padding", "=", "n_rows", "*", "n_cols", "-", "N", "\n", "# add padding trajectories", "\n", "if", "n_padding", ">", "0", ":", "\n", "            ", "for", "i", "in", "range", "(", "n_padding", ")", ":", "\n", "                ", "args", "+=", "(", "np", ".", "ones_like", "(", "args", "[", "0", "]", ")", ",", ")", "\n", "# 2. stack arrays accordingly", "\n", "", "", "frames", "=", "np", ".", "concatenate", "(", "[", "np", ".", "concatenate", "(", "args", "[", "row", "*", "n_cols", ":", "(", "row", "+", "1", ")", "*", "n_cols", "]", ",", "axis", "=", "-", "1", ")", "for", "row", "in", "range", "(", "n_rows", ")", "]", ",", "axis", "=", "1", ")", "\n", "return", "frames", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.visualisation.visualizers.Visualizer.render_frames": [[55, 68], ["visualizers.Visualizer.preprocess_inputs", "visualizers.Visualizer.stack_trajectories", "moviepy.editor.ImageSequenceClip", "moviepy.editor.ImageSequenceClip.write_gif", "f.squeeze", "os.path.join", "numpy.ones", "numpy.split"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.visualisation.visualizers.Visualizer.preprocess_inputs", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.visualisation.visualizers.Visualizer.stack_trajectories"], ["", "def", "render_frames", "(", "self", ",", "*", "args", ",", "fname", "=", "\"trajectory.gif\"", ",", "n_rows", "=", "1", ",", "fps", "=", "10", ")", ":", "\n", "        ", "\"\"\"Render frames from multiple inputs, assuming equal number of frames per input and equal frames shape per input.\n        \"\"\"", "\n", "# 1. process inputs to be 1 channel trajectories", "\n", "args", "=", "self", ".", "preprocess_inputs", "(", "*", "args", ")", "\n", "# 2. stack all trajectories as queried", "\n", "frames", "=", "self", ".", "stack_trajectories", "(", "*", "args", ",", "n_rows", "=", "n_rows", ")", "\n", "# 3. to make this work with ImageSequenceClip we need to make RGB (we assumed 1channel trajectories), we also need to unnormalize to uint8", "\n", "frames", "=", "frames", "[", "...", ",", "np", ".", "newaxis", "]", "*", "np", ".", "ones", "(", "3", ")", "*", "255", "\n", "# 4. generate the gif (note that ImageSequenceClip wants a list as an input)", "\n", "frames", "=", "[", "f", ".", "squeeze", "(", ")", "for", "f", "in", "np", ".", "split", "(", "frames", ",", "frames", ".", "shape", "[", "0", "]", ",", "axis", "=", "0", ")", "]", "\n", "clip", "=", "ImageSequenceClip", "(", "frames", ",", "fps", "=", "fps", ")", "\n", "clip", ".", "write_gif", "(", "os", ".", "path", ".", "join", "(", "self", ".", "savedir", ",", "fname", ")", ",", "fps", "=", "fps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.visualisation.visualizers.Visualizer.render_full": [[69, 150], ["numpy.vstack", "visualizers.Visualizer.preprocess_inputs", "visualizers.Visualizer.stack_trajectories", "matplotlib.figure", "matplotlib.figure", "matplotlib.figure.add_subplot", "matplotlib.figure.add_subplot", "matplotlib.figure.add_subplot", "plt.figure.add_subplot.set_xlim", "plt.figure.add_subplot.set_ylim", "plt.figure.add_subplot.set_zlim", "plt.figure.add_subplot.set_xlabel", "plt.figure.add_subplot.set_ylabel", "plt.figure.add_subplot.set_zlabel", "plt.figure.add_subplot.legend", "plt.figure.add_subplot.set_xlim", "plt.figure.add_subplot.set_ylim", "plt.figure.add_subplot.set_title", "plt.figure.add_subplot.set_xlabel", "plt.figure.add_subplot.set_ylabel", "matplotlib.FuncAnimation", "matplotlib.FuncAnimation", "matplotlib.FuncAnimation.save", "plot_objects[].set_data", "plot_objects[].set_3d_properties", "plot_objects[].set_text", "plot_objects[].set_text", "plot_objects[].set_text", "plot_objects[].set_text", "plot_objects[].set_data", "plot_objects[].set_text", "zip", "numpy.array", "main_rewards.append", "main_rewards.append", "plt.figure.add_subplot.scatter", "plt.figure.add_subplot.text2D", "plt.figure.add_subplot.text2D", "plt.figure.add_subplot.text2D", "plt.figure.add_subplot.text2D", "plt.figure.add_subplot.imshow", "plt.figure.add_subplot.text", "len", "numpy.append", "numpy.append", "numpy.append", "main_rewards_text.format", "logs.values", "plot.set_data", "int", "plt.figure.add_subplot.plot", "visualizers.plot_linear_cube", "range", "numpy.sqrt", "numpy.append", "numpy.append", "numpy.append", "plt.figure.add_subplot.plot", "len", "len", "abs().max", "key.lower().split", "abs", "key.lower"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.visualisation.visualizers.Visualizer.preprocess_inputs", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.visualisation.visualizers.Visualizer.stack_trajectories", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.save", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.visualisation.visualizers.plot_linear_cube"], ["", "def", "render_full", "(", "self", ",", "out", ",", "fname", ",", "fps", "=", "10", ")", ":", "\n", "\n", "            ", "def", "update", "(", "num", ",", "states", ",", "frames", ",", "logs", ",", "plot_objects", ")", ":", "\n", "                ", "plot_objects", "[", "0", "]", ".", "set_data", "(", "np", ".", "append", "(", "states", "[", "num", ",", ":", ",", "0", "]", ",", "states", "[", "num", ",", "0", ",", "0", "]", ")", ",", "np", ".", "append", "(", "states", "[", "num", ",", ":", ",", "1", "]", ",", "states", "[", "num", ",", "0", ",", "1", "]", ")", ")", "\n", "plot_objects", "[", "0", "]", ".", "set_3d_properties", "(", "np", ".", "append", "(", "states", "[", "num", ",", ":", ",", "2", "]", ",", "states", "[", "num", ",", "0", ",", "2", "]", ")", ")", "\n", "plot_objects", "[", "1", "]", ".", "_offsets3d", "=", "(", "states", "[", "num", ",", ":", ",", "0", "]", ",", "states", "[", "num", ",", ":", ",", "1", "]", ",", "states", "[", "num", ",", ":", ",", "2", "]", ")", "\n", "plot_objects", "[", "2", "]", ".", "set_text", "(", "\"oob: {:.2f}\"", ".", "format", "(", "logs", "[", "\"oobReward_1\"", "]", "[", "num", "]", ")", ")", "\n", "plot_objects", "[", "3", "]", ".", "set_text", "(", "\"oob: {:.2f}\"", ".", "format", "(", "logs", "[", "\"oobReward_2\"", "]", "[", "num", "]", ")", ")", "\n", "plot_objects", "[", "4", "]", ".", "set_text", "(", "\"oob: {:.2f}\"", ".", "format", "(", "logs", "[", "\"oobReward_3\"", "]", "[", "num", "]", ")", ")", "\n", "plot_objects", "[", "5", "]", ".", "set_text", "(", "\"area reward: {:.4f}\"", ".", "format", "(", "logs", "[", "\"areaReward\"", "]", "[", "num", "]", ")", ")", "\n", "plot_objects", "[", "7", "]", ".", "set_data", "(", "frames", "[", "num", "]", ")", "\n", "plot_objects", "[", "8", "]", ".", "set_text", "(", "main_rewards_text", ".", "format", "(", "*", "[", "logs", "[", "key", "]", "[", "num", "]", "for", "key", "in", "main_rewards", "]", ")", ")", "\n", "for", "plot", ",", "log", "in", "zip", "(", "plot_objects", "[", "9", "]", ",", "logs", ".", "values", "(", ")", ")", ":", "\n", "                    ", "plot", ".", "set_data", "(", "range", "(", "len", "(", "log", "[", ":", "num", "]", ")", ")", ",", "log", "[", ":", "num", "]", "/", "(", "abs", "(", "log", ")", ".", "max", "(", ")", "+", "10e-6", ")", ")", "# normalize for ease of visualization", "\n", "", "return", "plot_objects", "\n", "\n", "# gather useful information", "\n", "", "logs", "=", "out", "[", "\"logs\"", "]", "\n", "for", "key", "in", "logs", ":", "\n", "                ", "logs", "[", "key", "]", "=", "np", ".", "array", "(", "logs", "[", "key", "]", ")", "\n", "", "main_rewards", ",", "main_rewards_text", "=", "[", "]", ",", "\"\"", "\n", "if", "\"anatomyReward\"", "in", "logs", ":", "\n", "                ", "main_rewards", ".", "append", "(", "\"anatomyReward\"", ")", "\n", "main_rewards_text", "+=", "\"anatomyReward: {:.4f}\"", "\n", "", "if", "\"planeDistanceReward\"", "in", "logs", ":", "\n", "                ", "if", "\"anatomyReward\"", "in", "logs", ":", "\n", "                    ", "main_rewards_text", "+=", "\"\\n\"", "\n", "", "main_rewards", ".", "append", "(", "\"planeDistanceReward\"", ")", "\n", "main_rewards_text", "+=", "\"planeDistanceReward: {:.4f}\"", "\n", "\n", "# 2. stack the states in a single numpy array", "\n", "", "states", "=", "np", ".", "vstack", "(", "[", "state", "[", "np", ".", "newaxis", ",", "...", "]", "for", "state", "in", "out", "[", "\"states\"", "]", "]", ")", "\n", "# 3. process the frames", "\n", "frames", "=", "self", ".", "preprocess_inputs", "(", "out", "[", "\"planes\"", "]", ")", "\n", "frames", "=", "self", ".", "stack_trajectories", "(", "*", "frames", ",", "n_rows", "=", "int", "(", "np", ".", "sqrt", "(", "len", "(", "frames", ")", ")", ")", ")", "\n", "\n", "# Attaching 3D axis to the figure", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "14", ",", "4", ")", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", "131", ",", "projection", "=", "'3d'", ")", "\n", "ax1", "=", "fig", ".", "add_subplot", "(", "132", ")", "\n", "ax2", "=", "fig", ".", "add_subplot", "(", "133", ")", "\n", "# instanciate a list of plot objects that will be dynamically updated to create an animation", "\n", "plot_objects", "=", "[", "# 1. render the current position of the agents", "\n", "ax", ".", "plot", "(", "np", ".", "append", "(", "states", "[", "0", ",", ":", ",", "0", "]", ",", "states", "[", "0", ",", "0", ",", "0", "]", ")", ",", "\n", "np", ".", "append", "(", "states", "[", "0", ",", ":", ",", "1", "]", ",", "states", "[", "0", ",", "0", ",", "1", "]", ")", ",", "\n", "np", ".", "append", "(", "states", "[", "0", ",", ":", ",", "2", "]", ",", "states", "[", "0", ",", "0", ",", "2", "]", ")", ",", "c", "=", "'k'", ",", "ls", "=", "\"--\"", ",", "alpha", "=", "0.5", ")", "[", "0", "]", ",", "\n", "ax", ".", "scatter", "(", "states", "[", "0", ",", ":", ",", "0", "]", ",", "states", "[", "0", ",", ":", ",", "1", "]", ",", "states", "[", "0", ",", ":", ",", "2", "]", ",", "color", "=", "[", "'r'", ",", "'g'", ",", "'b'", "]", ")", ",", "\n", "# 2. render a legend with the current rewards of the agents for being within the volume boundaries", "\n", "ax", ".", "text2D", "(", "0.2", ",", "0.95", ",", "\"oob: \"", ",", "color", "=", "'red'", ",", "transform", "=", "ax", ".", "transAxes", ")", ",", "\n", "ax", ".", "text2D", "(", "0.5", ",", "0.95", ",", "\"oob: \"", ",", "color", "=", "'green'", ",", "transform", "=", "ax", ".", "transAxes", ")", ",", "\n", "ax", ".", "text2D", "(", "0.8", ",", "0.95", ",", "\"oob: \"", ",", "color", "=", "'blue'", ",", "transform", "=", "ax", ".", "transAxes", ")", ",", "\n", "# add the current reward the agents receive for distancing from each other (not clustering at a point)", "\n", "ax", ".", "text2D", "(", "0.3", ",", "1", ",", "main_rewards_text", ",", "color", "=", "'black'", ",", "transform", "=", "ax", ".", "transAxes", ")", ",", "\n", "# 3. plot the boundaries of the volume as a reference", "\n", "plot_linear_cube", "(", "ax", ",", "0", ",", "0", ",", "0", ",", "256", ",", "256", ",", "256", ")", "[", "0", "]", ",", "\n", "# 4. plot the current imaged slice on the second subplot", "\n", "ax1", ".", "imshow", "(", "frames", "[", "0", "]", ",", "cmap", "=", "\"Greys_r\"", ")", ",", "\n", "# add the current reward given the context anatomy contained in the slice", "\n", "ax1", ".", "text", "(", "0.25", ",", "1", ",", "\"anatomy reward: \"", ",", "color", "=", "'black'", ",", "transform", "=", "ax1", ".", "transAxes", ")", ",", "\n", "# 5. running plots for all losses", "\n", "[", "ax2", ".", "plot", "(", "[", "]", ",", "[", "]", ",", "label", "=", "''", ".", "join", "(", "key", ".", "lower", "(", ")", ".", "split", "(", "\"reward\"", ")", ")", ")", "[", "0", "]", "for", "key", "in", "logs", "]", ",", "\n", "]", "\n", "\n", "# set the limits for the axes (slightly larger than the volume to observe oob points)", "\n", "ax", ".", "set_xlim", "(", "-", "50", ",", "300", ")", "\n", "ax", ".", "set_ylim", "(", "-", "50", ",", "300", ")", "\n", "ax", ".", "set_zlim", "(", "-", "50", ",", "300", ")", "\n", "# label the axis", "\n", "ax", ".", "set_xlabel", "(", "\"x\"", ")", "\n", "ax", ".", "set_ylabel", "(", "\"y\"", ")", "\n", "ax", ".", "set_zlabel", "(", "\"z\"", ")", "\n", "# set the legend for the lineplots", "\n", "ax2", ".", "legend", "(", "bbox_to_anchor", "=", "(", "1.4", ",", "1", ")", ",", "ncol", "=", "1", ",", "fontsize", "=", "10", ")", "\n", "ax2", ".", "set_xlim", "(", "0", ",", "states", ".", "shape", "[", "0", "]", ")", "\n", "ax2", ".", "set_ylim", "(", "-", "1", ",", "1", ")", "\n", "ax2", ".", "set_title", "(", "\"collected rewards\"", ")", "\n", "ax2", ".", "set_xlabel", "(", "\"steps\"", ")", "\n", "ax2", ".", "set_ylabel", "(", "\"normalized reward\"", ")", "\n", "plt", ".", "rcParams", "[", "'animation.html'", "]", "=", "'html5'", "\n", "line_ani", "=", "animation", ".", "FuncAnimation", "(", "fig", ",", "update", ",", "len", "(", "states", ")", ",", "fargs", "=", "(", "states", ",", "frames", ",", "logs", ",", "plot_objects", ")", ")", "\n", "line_ani", ".", "save", "(", "fname", ",", "fps", "=", "fps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.visualisation.visualizers.Visualizer.render_frames_double": [[151, 158], ["moviepy.editor.ImageSequenceClip", "print", "moviepy.editor.ImageSequenceClip.write_gif", "numpy.hstack", "os.path.join", "zip", "numpy.ones"], "methods", ["None"], ["", "def", "render_frames_double", "(", "self", ",", "first", ",", "second", ",", "fname", ",", "fps", "=", "10", ")", ":", "\n", "        ", "frames", "=", "[", "np", ".", "hstack", "(", "[", "f", ",", "s", "]", ")", "for", "f", ",", "s", "in", "zip", "(", "first", ",", "second", ")", "]", "\n", "frames", "=", "[", "elem", "[", "...", ",", "np", ".", "newaxis", "]", "*", "np", ".", "ones", "(", "3", ")", "*", "255", "for", "elem", "in", "frames", "]", "\n", "# generate the gif", "\n", "clip", "=", "ImageSequenceClip", "(", "frames", ",", "fps", "=", "fps", ")", "\n", "print", "(", "os", ".", "path", ".", "join", "(", "self", ".", "savedir", ",", "fname", ")", ")", "\n", "clip", ".", "write_gif", "(", "fname", ",", "fps", "=", "fps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.visualisation.visualizers.plot_linear_cube": [[9, 19], ["ax.plot3D", "ax.plot3D", "ax.plot3D", "ax.plot3D", "ax.plot3D", "ax.plot3D"], "function", ["None"], ["def", "plot_linear_cube", "(", "ax", ",", "x", ",", "y", ",", "z", ",", "dx", ",", "dy", ",", "dz", ",", "color", "=", "'black'", ")", ":", "\n", "    ", "xx", "=", "[", "x", ",", "x", ",", "x", "+", "dx", ",", "x", "+", "dx", ",", "x", "]", "\n", "yy", "=", "[", "y", ",", "y", "+", "dy", ",", "y", "+", "dy", ",", "y", ",", "y", "]", "\n", "kwargs", "=", "{", "'alpha'", ":", "1", ",", "'color'", ":", "color", "}", "\n", "ax", ".", "plot3D", "(", "xx", ",", "yy", ",", "[", "z", "]", "*", "5", ",", "**", "kwargs", ")", "\n", "ax", ".", "plot3D", "(", "xx", ",", "yy", ",", "[", "z", "+", "dz", "]", "*", "5", ",", "**", "kwargs", ")", "\n", "ax", ".", "plot3D", "(", "[", "x", ",", "x", "]", ",", "[", "y", ",", "y", "]", ",", "[", "z", ",", "z", "+", "dz", "]", ",", "**", "kwargs", ")", "\n", "ax", ".", "plot3D", "(", "[", "x", ",", "x", "]", ",", "[", "y", "+", "dy", ",", "y", "+", "dy", "]", ",", "[", "z", ",", "z", "+", "dz", "]", ",", "**", "kwargs", ")", "\n", "ax", ".", "plot3D", "(", "[", "x", "+", "dx", ",", "x", "+", "dx", "]", ",", "[", "y", "+", "dy", ",", "y", "+", "dy", "]", ",", "[", "z", ",", "z", "+", "dz", "]", ",", "**", "kwargs", ")", "\n", "return", "ax", ".", "plot3D", "(", "[", "x", "+", "dx", ",", "x", "+", "dx", "]", ",", "[", "y", ",", "y", "]", ",", "[", "z", ",", "z", "+", "dz", "]", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.ConvBlock.__init__": [[59, 72], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inChannels", ",", "outChannels", ",", "kernel_size", ",", "stride", ",", "padding", ",", "dropout", ",", "batchnorm", ")", ":", "\n", "        ", "super", "(", "ConvBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# conv layer", "\n", "block", "=", "[", "nn", ".", "Conv2d", "(", "inChannels", ",", "outChannels", ",", "kernel_size", ",", "stride", ",", "padding", ")", "]", "\n", "# batchnorm layer", "\n", "if", "batchnorm", ":", "\n", "            ", "block", "+=", "[", "nn", ".", "BatchNorm2d", "(", "outChannels", ")", "]", "\n", "# activation", "\n", "", "block", "+=", "[", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "]", "\n", "# dropout", "\n", "if", "dropout", ":", "\n", "            ", "block", "+=", "[", "nn", ".", "Dropout", "(", "0.5", ")", "]", "\n", "", "self", ".", "block", "=", "nn", ".", "Sequential", "(", "*", "block", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.ConvBlock.forward": [[73, 75], ["Qnetworks.ConvBlock.block"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "block", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.HeadBlock.__init__": [[77, 92], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inFeatures", ",", "actionSize", ",", "dropout", ",", "batchnorm", ")", ":", "\n", "        ", "super", "(", "HeadBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# linear layer", "\n", "block", "=", "[", "nn", ".", "Linear", "(", "inFeatures", ",", "inFeatures", "//", "4", ")", "]", "\n", "# batchnorm layer", "\n", "if", "batchnorm", ":", "\n", "            ", "block", "+=", "[", "nn", ".", "BatchNorm1d", "(", "inFeatures", "//", "4", ")", "]", "\n", "# activation", "\n", "", "block", "+=", "[", "nn", ".", "ReLU", "(", ")", "]", "\n", "# dropout", "\n", "if", "dropout", ":", "\n", "            ", "block", "+=", "[", "nn", ".", "Dropout", "(", "0.5", ")", "]", "\n", "# output layer", "\n", "", "block", "+=", "[", "nn", ".", "Linear", "(", "inFeatures", "//", "4", ",", "actionSize", ")", "]", "\n", "self", ".", "block", "=", "nn", ".", "Sequential", "(", "*", "block", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.HeadBlock.forward": [[93, 95], ["Qnetworks.HeadBlock.block"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "block", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.__init__": [[102, 137], ["torch.Module.__init__", "range", "cnn.append", "torch.Sequential", "torch.Sequential", "Qnetworks.SimpleQNetwork.cnn_out_dim", "range", "torch.ModuleList", "torch.ModuleList", "Qnetworks.ConvBlock", "cnn.append", "torch.Conv2d", "torch.Conv2d", "heads.append", "Qnetworks.ConvBlock", "Qnetworks.HeadBlock"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.cnn_out_dim"], ["def", "__init__", "(", "self", ",", "state_size", ",", "action_size", ",", "Nheads", ",", "Nblocks", "=", "6", ",", "downsampling", "=", "2", ",", "num_features", "=", "4", ",", "dropout", "=", "True", ",", "batchnorm", "=", "True", ",", "first_downsampling", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        params\n        ======\n            state_size (list, tuple): shape of the input as CxHxW\n            Nblocks (int): number of convolutional blocks to use in the backbone\n            Nheads (int): number of agents sharing the convolutional backbone\n            action_size (int): number of actions each agent has to take (i.e. +/- movement in each of 3 dimensions -> 6 actions)\n            downsampling (int): downsampling factor of each convolutional bock\n            num_features (int): number of filters in the first convolutional block, each following block\n                                will go from num_features*2**i  -->  num_features*2**(i+1)\n            dropout (bool): if you want to use dropout (p=0.5)\n            batchnorm (bool): if you want to use batch normalization\n            first_downsampling (int): downsampling factor of the first convolutional bock (if None than downsampling is used)\n                                 \n        \"\"\"", "\n", "super", "(", "SimpleQNetwork", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# build convolutional backbone", "\n", "if", "first_downsampling", "is", "None", ":", "\n", "            ", "first_downsampling", "=", "downsampling", "\n", "", "cnn", "=", "[", "ConvBlock", "(", "state_size", "[", "0", "]", ",", "num_features", ",", "3", ",", "first_downsampling", ",", "1", ",", "dropout", ",", "batchnorm", ")", "]", "\n", "for", "i", "in", "range", "(", "Nblocks", "-", "1", ")", ":", "\n", "            ", "cnn", ".", "append", "(", "ConvBlock", "(", "num_features", "*", "2", "**", "i", ",", "num_features", "*", "2", "**", "(", "i", "+", "1", ")", ",", "3", ",", "downsampling", ",", "1", ",", "dropout", ",", "batchnorm", ")", ")", "\n", "", "cnn", ".", "append", "(", "nn", ".", "Conv2d", "(", "num_features", "*", "2", "**", "(", "i", "+", "1", ")", ",", "num_features", "*", "2", "**", "(", "i", "+", "2", ")", ",", "3", ",", "downsampling", ",", "1", ")", ")", "\n", "self", ".", "cnn", "=", "nn", ".", "Sequential", "(", "*", "cnn", ")", "\n", "\n", "# get the shape after the conv layers", "\n", "self", ".", "num_linear_features", "=", "self", ".", "cnn_out_dim", "(", "state_size", ")", "\n", "\n", "# build N linear heads, one for each agent", "\n", "heads", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "Nheads", ")", ":", "\n", "            ", "heads", ".", "append", "(", "HeadBlock", "(", "self", ".", "num_linear_features", ",", "action_size", ",", "dropout", ",", "batchnorm", ")", ")", "\n", "", "self", ".", "heads", "=", "nn", ".", "ModuleList", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.forward": [[138, 148], ["Qnetworks.SimpleQNetwork.cnn", "y.reshape.reshape.reshape", "outs.append", "head"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "y", "=", "self", ".", "cnn", "(", "x", ")", "\n", "y", "=", "y", ".", "reshape", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "\n", "# get outputs for each head", "\n", "outs", "=", "[", "]", "\n", "for", "head", "in", "self", ".", "heads", ":", "\n", "            ", "outs", ".", "append", "(", "head", "(", "y", ")", ")", "\n", "\n", "", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.cnn_out_dim": [[149, 151], ["Qnetworks.SimpleQNetwork.cnn().flatten", "Qnetworks.SimpleQNetwork.cnn", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "cnn_out_dim", "(", "self", ",", "input_dim", ")", ":", "\n", "        ", "return", "self", ".", "cnn", "(", "torch", ".", "zeros", "(", "1", ",", "*", "input_dim", ")", ")", ".", "flatten", "(", ")", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.save": [[152, 154], ["torch.save", "torch.save", "torch.save", "torch.save", "Qnetworks.SimpleQNetwork.state_dict"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.save", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.save", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.save", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.save"], ["", "def", "save", "(", "self", ",", "savepath", ")", ":", "\n", "        ", "torch", ".", "save", "(", "self", ".", "state_dict", "(", ")", ",", "savepath", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.load": [[155, 158], ["torch.load", "torch.load", "torch.load", "torch.load", "Qnetworks.SimpleQNetwork.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.load", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.load", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.load", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.load"], ["", "def", "load", "(", "self", ",", "savepath", ")", ":", "\n", "        ", "state_dict", "=", "torch", ".", "load", "(", "savepath", ",", "map_location", "=", "'cpu'", ")", "\n", "self", ".", "load_state_dict", "(", "state_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.count_parameters": [[159, 161], ["sum", "p.numel", "Qnetworks.SimpleQNetwork.parameters"], "methods", ["None"], ["", "def", "count_parameters", "(", "self", ")", ":", "\n", "        ", "return", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "self", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.DuelingQNetwork.__init__": [[165, 171], ["Qnetworks.SimpleQNetwork.__init__", "Qnetworks.HeadBlock"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__"], ["def", "__init__", "(", "self", ",", "state_size", ",", "action_size", ",", "Nheads", ",", "Nblocks", "=", "6", ",", "downsampling", "=", "2", ",", "num_features", "=", "4", ",", "dropout", "=", "True", ",", "batchnorm", "=", "True", ",", "first_downsampling", "=", "None", ")", ":", "\n", "# initialize Q-network", "\n", "        ", "super", "(", "DuelingQNetwork", ",", "self", ")", ".", "__init__", "(", "state_size", ",", "action_size", ",", "Nheads", ",", "Nblocks", ",", "downsampling", ",", "num_features", ",", "dropout", ",", "batchnorm", ",", "first_downsampling", ")", "\n", "\n", "# set the value stream as a linear head on top of the parent convolutional block", "\n", "self", ".", "value_stream", "=", "HeadBlock", "(", "self", ".", "num_linear_features", ",", "1", ",", "dropout", ",", "batchnorm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.DuelingQNetwork.forward": [[172, 190], ["Qnetworks.DuelingQNetwork.cnn", "y.reshape.reshape.reshape", "Qnetworks.DuelingQNetwork.value_stream", "advantages.append", "outs.append", "head", "adv.mean"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "y", "=", "self", ".", "cnn", "(", "x", ")", "\n", "y", "=", "y", ".", "reshape", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "\n", "# get advantages for each head", "\n", "advantages", "=", "[", "]", "\n", "for", "head", "in", "self", ".", "heads", ":", "\n", "            ", "advantages", ".", "append", "(", "head", "(", "y", ")", ")", "\n", "\n", "# get value stream output", "\n", "", "values", "=", "self", ".", "value_stream", "(", "y", ")", "\n", "\n", "# aggregate the advantages and value to get the Q values", "\n", "outs", "=", "[", "]", "\n", "for", "adv", "in", "advantages", ":", "\n", "            ", "outs", ".", "append", "(", "values", "+", "(", "adv", "-", "adv", ".", "mean", "(", ")", ")", ")", "# could also use .max() but original paper uses mean", "\n", "\n", "", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.RecurrentQnetwork.__init__": [[193, 200], ["Qnetworks.SimpleQNetwork.__init__", "torch.LSTM", "torch.LSTM"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__"], ["def", "__init__", "(", "self", ",", "state_size", ",", "action_size", ",", "Nheads", ",", "Nblocks", "=", "6", ",", "downsampling", "=", "2", ",", "num_features", "=", "4", ",", "dropout", "=", "True", ",", "batchnorm", "=", "True", ",", "first_downsampling", "=", "None", ",", "history_length", "=", "10", ")", ":", "\n", "# initialize Q-network", "\n", "        ", "super", "(", "RecurrentQnetwork", ",", "self", ")", ".", "__init__", "(", "state_size", ",", "action_size", ",", "Nheads", ",", "Nblocks", ",", "downsampling", ",", "num_features", ",", "dropout", ",", "batchnorm", ",", "first_downsampling", ")", "\n", "\n", "# initialize the recurrent layer", "\n", "self", ".", "history_length", "=", "history_length", "\n", "self", ".", "recurrent_layer", "=", "nn", ".", "LSTM", "(", "self", ".", "num_linear_features", ",", "self", ".", "num_linear_features", ",", "batch_first", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.RecurrentQnetwork.forward": [[201, 227], ["int", "Qnetworks.RecurrentQnetwork.cnn", "y.reshape.reshape.reshape", "Qnetworks.RecurrentQnetwork.recurrent_layer", "outs.append", "head", "h_n.squeeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\" forward pass through the network\n        Params:\n        ==========\n            x (tensor[B*L x C x H x W]): a sequence of batched image frames (sequence and batch are along the same dimension)\n        Outputs:\n        ==========\n            out (list[tensor[B x action_size]]): Q values for each agent for the current frame \n        \"\"\"", "\n", "#print(\"x:\", x.shape)", "\n", "# get batch size of inputs", "\n", "B", "=", "int", "(", "x", ".", "shape", "[", "0", "]", "/", "self", ".", "history_length", ")", "\n", "#print(B)", "\n", "# pass all frames through convolutional backbone and reshape for LSTM ", "\n", "y", "=", "self", ".", "cnn", "(", "x", ")", "# B*L x self.num_linear_features (B*L can be BIG: on 15 envs with B=64 and L=10 -> 15*64*10 = 9600)", "\n", "y", "=", "y", ".", "reshape", "(", "B", ",", "self", ".", "history_length", ",", "-", "1", ")", "# B x L x self.num_linear_features", "\n", "#print(\"y:\", y.shape)", "\n", "# pass these sequential features to the recurrent layer using default (h, c) initialized as zeros", "\n", "_", ",", "(", "h_n", ",", "_", ")", "=", "self", ".", "recurrent_layer", "(", "y", ")", "\n", "#print(\"h_n:\", h_n.shape)", "\n", "# get outputs for each head", "\n", "outs", "=", "[", "]", "\n", "for", "head", "in", "self", ".", "heads", ":", "\n", "            ", "outs", ".", "append", "(", "head", "(", "h_n", ".", "squeeze", "(", "0", ")", ")", ")", "\n", "#print([\"out_i: {}\".format(outs[i].shape) for i in range(len(outs))])", "\n", "", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.RecurrentDuelingQNetwork.__init__": [[230, 237], ["Qnetworks.DuelingQNetwork.__init__", "torch.LSTM", "torch.LSTM"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__"], ["def", "__init__", "(", "self", ",", "state_size", ",", "action_size", ",", "Nheads", ",", "Nblocks", "=", "6", ",", "downsampling", "=", "2", ",", "num_features", "=", "4", ",", "dropout", "=", "True", ",", "batchnorm", "=", "True", ",", "first_downsampling", "=", "None", ",", "history_length", "=", "10", ")", ":", "\n", "# initialize Q-network", "\n", "        ", "super", "(", "RecurrentDuelingQNetwork", ",", "self", ")", ".", "__init__", "(", "state_size", ",", "action_size", ",", "Nheads", ",", "Nblocks", ",", "downsampling", ",", "num_features", ",", "dropout", ",", "batchnorm", ",", "first_downsampling", ")", "\n", "\n", "# initialize the recurrent layer", "\n", "self", ".", "history_length", "=", "history_length", "\n", "self", ".", "recurrent_layer", "=", "nn", ".", "LSTM", "(", "self", ".", "num_linear_features", ",", "self", ".", "num_linear_features", ",", "batch_first", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.RecurrentDuelingQNetwork.forward": [[238, 271], ["int", "Qnetworks.RecurrentDuelingQNetwork.cnn", "y.reshape.reshape.reshape", "Qnetworks.RecurrentDuelingQNetwork.recurrent_layer", "Qnetworks.RecurrentDuelingQNetwork.value_stream", "advantages.append", "h_n.squeeze", "outs.append", "head", "h_n.squeeze", "adv.mean"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\" forward pass through the network\n        Params:\n        ==========\n            x (tensor[Seq_len x B x C x H x W]): a sequence of batched image frames\n        Outputs:\n        ==========\n            out (list[tensor[B x action_size]]): Q values for each agent for the current frame \n        \"\"\"", "\n", "# print(\"x:\", x.shape)", "\n", "# get batch size of inputs", "\n", "B", "=", "int", "(", "x", ".", "shape", "[", "0", "]", "/", "self", ".", "history_length", ")", "\n", "# pass all frames through convolutional backbone and reshape for LSTM ", "\n", "y", "=", "self", ".", "cnn", "(", "x", ")", "# B*L x self.num_linear_features (B*L can be BIG: on 15 envs with B=64 and L=10 -> 15*64*10 = 9600)", "\n", "y", "=", "y", ".", "reshape", "(", "B", ",", "self", ".", "history_length", ",", "-", "1", ")", "# B x L x self.num_linear_features", "\n", "# print(\"y:\", y.shape)", "\n", "# pass these sequential features to the recurrent layer, using default (h, c) initialized as zeros", "\n", "_", ",", "(", "h_n", ",", "_", ")", "=", "self", ".", "recurrent_layer", "(", "y", ")", "\n", "# print(\"h_n:\", h_n.shape)", "\n", "# get advantages for each head", "\n", "advantages", "=", "[", "]", "\n", "for", "head", "in", "self", ".", "heads", ":", "\n", "            ", "advantages", ".", "append", "(", "head", "(", "h_n", ".", "squeeze", "(", "0", ")", ")", ")", "\n", "\n", "# get value stream output", "\n", "", "values", "=", "self", ".", "value_stream", "(", "h_n", ".", "squeeze", "(", "0", ")", ")", "\n", "\n", "# aggregate the advantages and value to get the Q values", "\n", "outs", "=", "[", "]", "\n", "for", "adv", "in", "advantages", ":", "\n", "            ", "outs", ".", "append", "(", "values", "+", "(", "adv", "-", "adv", ".", "mean", "(", ")", ")", ")", "# could also use .max() but original paper uses mean", "\n", "#print([\"out_i: {}\".format(outs[i].shape) for i in range(len(outs))])", "\n", "", "return", "outs", "", "", "", ""]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.setup_networks": [[4, 54], ["SimpleQNetwork().to.eval", "SimpleQNetwork().to.eval", "print", "config.default_Q.lower", "print", "SimpleQNetwork().to.load", "SimpleQNetwork().to.load", "config.default_Q.lower", "RecurrentDuelingQNetwork().to", "RecurrentDuelingQNetwork().to", "DuelingQNetwork().to", "DuelingQNetwork().to", "RecurrentQnetwork().to", "RecurrentQnetwork().to", "SimpleQNetwork().to", "SimpleQNetwork().to", "SimpleQNetwork().to.count_parameters", "os.path.join", "os.path.join", "ValueError", "Qnetworks.RecurrentDuelingQNetwork", "Qnetworks.RecurrentDuelingQNetwork", "Qnetworks.DuelingQNetwork", "Qnetworks.DuelingQNetwork", "Qnetworks.RecurrentQnetwork", "Qnetworks.RecurrentQnetwork", "Qnetworks.SimpleQNetwork", "Qnetworks.SimpleQNetwork"], "function", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.load", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.load", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.networks.Qnetworks.SimpleQNetwork.count_parameters"], ["def", "setup_networks", "(", "config", ")", ":", "\n", "# setup correct channels if we are location aware", "\n", "    ", "nchannels", "=", "1", "if", "not", "config", ".", "location_aware", "else", "4", "\n", "# 1. instanciate the Qnetworks", "\n", "if", "config", ".", "default_Q", ".", "lower", "(", ")", "==", "\"small\"", ":", "\n", "        ", "if", "config", ".", "load_size", "==", "128", ":", "\n", "            ", "first_downsampling", "=", "2", "\n", "", "elif", "config", ".", "load_size", "==", "256", ":", "\n", "            ", "first_downsampling", "=", "4", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'small Q-network was only thought for load_size == 128 or load_size == 256'", ")", "\n", "", "params", "=", "[", "(", "nchannels", ",", "config", ".", "load_size", ",", "config", ".", "load_size", ")", ",", "config", ".", "action_size", ",", "config", ".", "n_agents", ",", "3", ",", "\n", "4", ",", "32", ",", "not", "config", ".", "no_dropout_Q", ",", "not", "config", ".", "no_batchnorm_Q", ",", "first_downsampling", "]", "\n", "", "elif", "config", ".", "default_Q", ".", "lower", "(", ")", "==", "\"large\"", ":", "\n", "        ", "params", "=", "[", "(", "nchannels", ",", "config", ".", "load_size", ",", "config", ".", "load_size", ")", ",", "config", ".", "action_size", ",", "config", ".", "n_agents", ",", "6", ",", "\n", "2", ",", "4", ",", "not", "config", ".", "no_dropout_Q", ",", "not", "config", ".", "no_batchnorm_Q", "]", "\n", "", "else", ":", "\n", "        ", "params", "=", "[", "(", "nchannels", ",", "config", ".", "load_size", ",", "config", ".", "load_size", ")", ",", "config", ".", "action_size", ",", "config", ".", "n_agents", ",", "config", ".", "n_blocks_Q", ",", "\n", "config", ".", "downsampling_Q", ",", "config", ".", "n_features_Q", ",", "not", "config", ".", "no_dropout_Q", ",", "not", "config", ".", "no_batchnorm_Q", "]", "\n", "# instanciate and send to gpu", "\n", "", "if", "config", ".", "dueling", ":", "\n", "        ", "if", "config", ".", "recurrent", ":", "\n", "            ", "params", "+=", "[", "config", ".", "recurrent_history_len", "]", "\n", "qnetwork_local", "=", "RecurrentDuelingQNetwork", "(", "*", "params", ")", ".", "to", "(", "config", ".", "device", ")", "\n", "qnetwork_target", "=", "RecurrentDuelingQNetwork", "(", "*", "params", ")", ".", "to", "(", "config", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "qnetwork_local", "=", "DuelingQNetwork", "(", "*", "params", ")", ".", "to", "(", "config", ".", "device", ")", "\n", "qnetwork_target", "=", "DuelingQNetwork", "(", "*", "params", ")", ".", "to", "(", "config", ".", "device", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "config", ".", "recurrent", ":", "\n", "            ", "params", "+=", "[", "config", ".", "recurrent_history_len", "]", "\n", "qnetwork_local", "=", "RecurrentQnetwork", "(", "*", "params", ")", ".", "to", "(", "config", ".", "device", ")", "\n", "qnetwork_target", "=", "RecurrentQnetwork", "(", "*", "params", ")", ".", "to", "(", "config", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "qnetwork_local", "=", "SimpleQNetwork", "(", "*", "params", ")", ".", "to", "(", "config", ".", "device", ")", "\n", "qnetwork_target", "=", "SimpleQNetwork", "(", "*", "params", ")", ".", "to", "(", "config", ".", "device", ")", "\n", "# we keep networks in evaluation mode at all times, when training is needed, .train() will be called on the local network", "\n", "", "", "qnetwork_local", ".", "eval", "(", ")", "\n", "qnetwork_target", ".", "eval", "(", ")", "\n", "print", "(", "\"Qnetwork instanciated: {} params.\\n\"", ".", "format", "(", "qnetwork_local", ".", "count_parameters", "(", ")", ")", ",", "qnetwork_local", ")", "\n", "# 2. load from checkpoint if needed", "\n", "if", "config", ".", "load", "is", "not", "None", ":", "\n", "        ", "if", "config", ".", "load_name", "is", "not", "None", ":", "\n", "            ", "load_name", "=", "config", ".", "load_name", "\n", "", "else", ":", "\n", "            ", "load_name", "=", "config", ".", "name", "\n", "", "print", "(", "\"loading: {}/{} model ...\"", ".", "format", "(", "load_name", ",", "config", ".", "load", ")", ")", "\n", "qnetwork_local", ".", "load", "(", "os", ".", "path", ".", "join", "(", "config", ".", "checkpoints_dir", ",", "load_name", ",", "config", ".", "load", "+", "\".pth\"", ")", ")", "\n", "qnetwork_target", ".", "load", "(", "os", ".", "path", ".", "join", "(", "config", ".", "checkpoints_dir", ",", "load_name", ",", "config", ".", "load", "+", "\".pth\"", ")", ")", "\n", "", "return", "qnetwork_local", ",", "qnetwork_target", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.trainers.PrioritizedDeepQLearning.__init__": [[5, 13], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "gamma", ")", ":", "\n", "        ", "\"\"\"Initializes the trainer class\n        Params:\n        ==========\n            gamma (float): discount factor for dqn.\n        \"\"\"", "\n", "# batch size and discount factor", "\n", "self", ".", "gamma", "=", "gamma", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.trainers.PrioritizedDeepQLearning.step": [[14, 41], ["local_model", "target_model", "sum", "sum", "concurrent.futures.ThreadPoolExecutor", "executor.submit", "zip", "f.result", "f.result"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "batch", ",", "local_model", ",", "target_model", ",", "criterion", ")", ":", "\n", "        ", "\"\"\"Update qbetwork parameters using given batch of experience tuples.\n        Params\n        ======\n            batch (dict): contains all training inputs\n            local_model (PyTorch model): local Q network (the one we update)\n            target_model (PyTorch model): target Q network (the one we use to bootstrap future Q values)\n            criterion (torch.nn.Module): the loss we use to update the network parameters\n        Returns\n        ======\n            loss (torch.tensor): final TD error loss tensor to compute gradients from.\n        \"\"\"", "\n", "# 1. split batch", "\n", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "dones", ",", "weights", "=", "batch", "[", "\"states\"", "]", ",", "batch", "[", "\"actions\"", "]", ",", "batch", "[", "\"rewards\"", "]", ",", "batch", "[", "\"next_states\"", "]", ",", "batch", "[", "\"dones\"", "]", ",", "batch", "[", "\"weights\"", "]", "\n", "# 2. get our value estimates Q for the current state, and our target values estimates (Qtarget) for the next state", "\n", "# note that the qnetworks have multiple heads, returned as a list, we will need to process each head separately,", "\n", "# and aggregate the individual losses", "\n", "Q", "=", "local_model", "(", "states", ")", "\n", "Qtarget", "=", "target_model", "(", "next_states", ")", "\n", "# 3. evaluate the loss (TDerror) for each head, launch in parallel for efficiency, aggregate each loss by summation", "\n", "with", "concurrent", ".", "futures", ".", "ThreadPoolExecutor", "(", ")", "as", "executor", ":", "\n", "            ", "futures", "=", "[", "executor", ".", "submit", "(", "self", ".", "single_head_loss", ",", "q", ",", "q_target", ",", "act", ",", "r", ",", "dones", ",", "criterion", ",", "weights", ")", "for", "q", ",", "q_target", ",", "act", ",", "r", "in", "zip", "(", "Q", ",", "Qtarget", ",", "actions", ",", "rewards", ")", "]", "\n", "# 4. aggregate the deltas of each head and update buffer priorities", "\n", "", "deltas", "=", "sum", "(", "[", "f", ".", "result", "(", ")", "[", "1", "]", "for", "f", "in", "futures", "]", ")", "\n", "# 5. aggregate the losses of each head and average across batch", "\n", "loss", "=", "sum", "(", "[", "f", ".", "result", "(", ")", "[", "0", "]", "for", "f", "in", "futures", "]", ")", "\n", "return", "loss", ",", "deltas", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.trainers.PrioritizedDeepQLearning.single_head_loss": [[42, 54], ["Q.gather().squeeze", "[].detach", "torch.abs", "rewards.squeeze", "criterion", "loss.mean", "Q.gather", "Qtarget.max", "dones.int"], "methods", ["None"], ["", "def", "single_head_loss", "(", "self", ",", "Q", ",", "Qtarget", ",", "actions", ",", "rewards", ",", "dones", ",", "criterion", ",", "weights", ")", ":", "\n", "# 1. gather the values of the action taken", "\n", "        ", "Qa", "=", "Q", ".", "gather", "(", "1", ",", "actions", ")", ".", "squeeze", "(", ")", "\n", "# 2. get the target value of the greedy action at the next state", "\n", "MaxQ", "=", "Qtarget", ".", "max", "(", "1", ")", "[", "0", "]", ".", "detach", "(", ")", "\n", "# 3. backup the expected value of this action by bootstrapping on the greedy value of the next state", "\n", "Qhat", "=", "rewards", ".", "squeeze", "(", ")", "+", "(", "1", "-", "dones", ".", "int", "(", ")", ")", "*", "self", ".", "gamma", "*", "MaxQ", "\n", "# 4. evalauate TD error as a fit function for the netwrok", "\n", "loss", "=", "criterion", "(", "Qa", ",", "Qhat", ")", "*", "weights", "\n", "# 5. deltas to update priorities", "\n", "deltas", "=", "torch", ".", "abs", "(", "Qa", "-", "Qhat", ")", "\n", "return", "(", "loss", ".", "mean", "(", ")", ",", "deltas", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.trainers.PrioritizedDoubleDeepQLearning.__init__": [[56, 64], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "gamma", ")", ":", "\n", "        ", "\"\"\"Initializes the trainer class\n        Params:\n        ==========\n            gamma (float): discount factor for dqn.\n        \"\"\"", "\n", "# batch size and discount factor", "\n", "self", ".", "gamma", "=", "gamma", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.trainers.PrioritizedDoubleDeepQLearning.step": [[65, 94], ["local_model", "local_model", "target_model", "sum", "sum", "concurrent.futures.ThreadPoolExecutor", "executor.submit", "zip", "f.result", "f.result"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "batch", ",", "local_model", ",", "target_model", ",", "criterion", ")", ":", "\n", "        ", "\"\"\"Update qbetwork parameters using given batch of experience tuples.\n        Params\n        ======\n            batch (Tuple[torch.Tensor]): tuple of (s, a, r, s') tuples\n            local_model (PyTorch model): local Q network (the one we update)\n            target_model (PyTorch model): target Q network (the one we use to bootstrap future Q values)\n            criterion (torch.nn.Module): the loss we use to update the network parameters\n        Returns\n        ======\n            loss (torch.tensor): final TD error loss tensor to compute gradients from. \n        \"\"\"", "\n", "# 1. split batch", "\n", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "dones", ",", "weights", "=", "batch", "[", "\"states\"", "]", ",", "batch", "[", "\"actions\"", "]", ",", "batch", "[", "\"rewards\"", "]", ",", "batch", "[", "\"next_states\"", "]", ",", "batch", "[", "\"dones\"", "]", ",", "batch", "[", "\"weights\"", "]", "\n", "# 2. get our value estimates Q for the current state, and our target values estimates (Qtarget) for the next state", "\n", "# note that the qnetworks have multiple heads, returned as a list, we will need to process each head separately,", "\n", "# and aggregate the individual losses", "\n", "Q", "=", "local_model", "(", "states", ")", "\n", "Q_next", "=", "local_model", "(", "next_states", ")", "\n", "Qtarget_next", "=", "target_model", "(", "next_states", ")", "\n", "# 3. evaluate the loss (TDerror) for each head, launch in parallel for efficiency, aggregate each loss by summation.", "\n", "# each head will have its Q values, its actions and its rewards. Each head will share the same optimizing criterion.", "\n", "with", "concurrent", ".", "futures", ".", "ThreadPoolExecutor", "(", ")", "as", "executor", ":", "\n", "            ", "futures", "=", "[", "executor", ".", "submit", "(", "self", ".", "single_head_loss", ",", "q", ",", "q_next", ",", "q_target_next", ",", "act", ",", "r", ",", "dones", ",", "criterion", ",", "weights", ")", "for", "q", ",", "q_next", ",", "q_target_next", ",", "act", ",", "r", "in", "zip", "(", "Q", ",", "Q_next", ",", "Qtarget_next", ",", "actions", ",", "rewards", ")", "]", "\n", "# 4. aggregate the deltas of each head and update buffer priorities", "\n", "", "deltas", "=", "sum", "(", "[", "f", ".", "result", "(", ")", "[", "1", "]", "for", "f", "in", "futures", "]", ")", "\n", "# 5. aggregate the losses of each head and average across batch", "\n", "loss", "=", "sum", "(", "[", "f", ".", "result", "(", ")", "[", "0", "]", "for", "f", "in", "futures", "]", ")", "\n", "return", "loss", ",", "deltas", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.trainers.PrioritizedDoubleDeepQLearning.single_head_loss": [[95, 108], ["Q.gather().squeeze", "[].unsqueeze", "Qtarget_next.gather().detach().squeeze", "torch.abs", "rewards.squeeze", "criterion", "loss.mean", "Q.gather", "Qtarget_next.gather().detach", "Q_next.max", "Qtarget_next.gather", "dones.int"], "methods", ["None"], ["", "def", "single_head_loss", "(", "self", ",", "Q", ",", "Q_next", ",", "Qtarget_next", ",", "actions", ",", "rewards", ",", "dones", ",", "criterion", ",", "weights", ")", ":", "\n", "# 1. gather Q values for the actions taken at the current state", "\n", "        ", "Qa", "=", "Q", ".", "gather", "(", "1", ",", "actions", ")", ".", "squeeze", "(", ")", "\n", "# 2. get the discrete action that maximizes the target value at the next state", "\n", "a_next", "=", "Q_next", ".", "max", "(", "1", ")", "[", "1", "]", ".", "unsqueeze", "(", "-", "1", ")", "\n", "Qa_next", "=", "Qtarget_next", ".", "gather", "(", "1", ",", "a_next", ")", ".", "detach", "(", ")", ".", "squeeze", "(", ")", "\n", "# 3. backup the expected value of this action by bootstrapping on the greedy value of the next state", "\n", "Qhat", "=", "rewards", ".", "squeeze", "(", ")", "+", "(", "1", "-", "dones", ".", "int", "(", ")", ")", "*", "self", ".", "gamma", "*", "Qa_next", "\n", "# 4. evalauate TD error as a fit function for the network", "\n", "loss", "=", "criterion", "(", "Qa", ",", "Qhat", ")", "*", "weights", "\n", "# 5. deltas to update priorities", "\n", "deltas", "=", "torch", ".", "abs", "(", "Qa", "-", "Qhat", ")", "\n", "return", "(", "loss", ".", "mean", "(", ")", ",", "deltas", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.agent.SingleVolumeAgent.__init__": [[10, 18], ["agent.baseAgent.BaseAgent.__init__"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "\"\"\"Initialize an Agent object.\n        Params:\n        ======\n            config (argparse object): parser with all training options (see options/options.py)\n        \"\"\"", "\n", "# Initialize the base class", "\n", "BaseAgent", ".", "__init__", "(", "self", ",", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.agent.SingleVolumeAgent.play_episode": [[19, 67], ["env.reset", "env.sample_plane", "logs.update", "collections.deque", "torch.no_grad", "range", "agent.SingleVolumeAgent.act", "env.step", "collections.deque.append", "buffer.add", "buffer.add", "len", "numpy.concatenate", "numpy.concatenate", "len", "list", "list"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.reset", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.sample_plane", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.baseAgent.BaseAgent.act", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.trainers.PrioritizedDoubleDeepQLearning.step", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.RecurrentPrioritizedReplayBuffer.add", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.RecurrentPrioritizedReplayBuffer.add"], ["", "def", "play_episode", "(", "self", ",", "env", ",", "local_model", ",", "buffer", ")", ":", "\n", "        ", "\"\"\" Plays one episode on an input environment.\n        Params:\n        ==========\n            env (environment/* instance): the environment the agent will interact with while training.\n            local_model (PyTorch model): pytorch network that will be trained using a particular training routine (i.e. DQN)\n            buffer (buffer/* object): replay buffer shared amongst processes (each process pushes to the same memory.)\n        Returns logs (dict): all relevant logs acquired throughout the episode.\n        \"\"\"", "\n", "self", ".", "episode", "+=", "1", "\n", "env", ".", "reset", "(", ")", "\n", "sample", "=", "env", ".", "sample_plane", "(", "env", ".", "state", ",", "preprocess", "=", "True", ")", "\n", "if", "self", ".", "config", ".", "recurrent", ":", "\n", "            ", "plane_history", "=", "deque", "(", "maxlen", "=", "self", ".", "config", ".", "recurrent_history_len", ")", "# instanciate history at beginning of each episode", "\n", "# play episode (stores transition tuples to the buffer)", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "i", "in", "range", "(", "self", ".", "config", ".", "n_steps_per_episode", ")", ":", "\n", "# if recurrent add plane to history", "\n", "                ", "if", "self", ".", "config", ".", "recurrent", ":", "\n", "                    ", "plane_history", ".", "append", "(", "sample", "[", "\"plane\"", "]", ")", "\n", "# if less than config.recurrent_history_len planes, then pad to the left with oldest plane in history", "\n", "if", "len", "(", "plane_history", ")", "<", "self", ".", "config", ".", "recurrent_history_len", ":", "\n", "                        ", "n_pad", "=", "self", ".", "config", ".", "recurrent_history_len", "-", "len", "(", "plane_history", ")", "\n", "tensor_pad", "=", "plane_history", "[", "0", "]", "\n", "# concatenate the history to 1*LxCxHxW", "\n", "plane", "=", "np", ".", "concatenate", "(", "[", "tensor_pad", "]", "*", "n_pad", "+", "list", "(", "plane_history", ")", ",", "axis", "=", "0", ")", "\n", "", "else", ":", "\n", "                        ", "plane", "=", "np", ".", "concatenate", "(", "list", "(", "plane_history", ")", ",", "axis", "=", "0", ")", "\n", "# else the current plane is passed through the Qnetwork", "\n", "", "", "else", ":", "\n", "                    ", "plane", "=", "sample", "[", "\"plane\"", "]", "\n", "", "self", ".", "t_step", "+=", "1", "\n", "# get action from current state", "\n", "actions", "=", "self", ".", "act", "(", "plane", ",", "local_model", ",", "self", ".", "eps", ")", "\n", "# step the environment to return a transitiony  ", "\n", "transition", ",", "next_sample", "=", "env", ".", "step", "(", "actions", ",", "preprocess", "=", "True", ")", "\n", "# add (state, action, reward, next_state) to buffer", "\n", "if", "self", ".", "config", ".", "recurrent", ":", "buffer", ".", "add", "(", "transition", ",", "is_first_time_step", "=", "i", "==", "0", ")", "\n", "else", ":", "buffer", ".", "add", "(", "transition", ")", "\n", "# set sample to next sample", "\n", "sample", "=", "next_sample", "\n", "# if done, end episode early", "\n", "if", "transition", "[", "-", "1", "]", ":", "\n", "                    ", "break", "\n", "# return episode logs", "\n", "", "", "", "logs", "=", "env", ".", "logs", "\n", "logs", ".", "update", "(", "{", "\"epsilon\"", ":", "self", ".", "eps", ",", "\"beta\"", ":", "self", ".", "beta", "}", ")", "\n", "return", "logs", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.agent.SingleVolumeAgent.test_agent": [[68, 130], ["env.reset", "env.sample_plane", "out.update", "collections.deque", "torch.no_grad", "range", "agent.SingleVolumeAgent.act", "env.step", "out[].append", "out[].append", "numpy.mean", "collections.deque.append", "sample[].squeeze", "out[].append", "out[].append", "out[].append", "env.logs.items", "out[].items", "len", "numpy.concatenate", "numpy.concatenate", "sample[].squeeze", "sample[].squeeze", "len", "list", "list", "env.current_logs.items"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.reset", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.sample_plane", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.baseAgent.BaseAgent.act", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.trainers.PrioritizedDoubleDeepQLearning.step"], ["", "def", "test_agent", "(", "self", ",", "steps", ",", "env", ",", "local_model", ")", ":", "\n", "        ", "\"\"\"Test the greedy policy learned by the agent and returns a dict with useful metrics/logs.\n        Params:\n        ==========\n            steps (int): number of steps to test the agent for.\n            env (environment/* instance): the environment the agent will interact with while testing.\n            local_model (PyTorch model): pytorch network that will be tested.\n        \"\"\"", "\n", "out", "=", "{", "\"planes\"", ":", "[", "]", ",", "\"segs\"", ":", "[", "]", ",", "\"states\"", ":", "[", "]", ",", "\"logs\"", ":", "[", "]", "}", "\n", "if", "self", ".", "config", ".", "CT2US", ":", "\n", "            ", "out", ".", "update", "(", "{", "\"planesCT\"", ":", "[", "]", "}", ")", "\n", "# reset env to a random initial slice", "\n", "", "env", ".", "reset", "(", ")", "\n", "sample", "=", "env", ".", "sample_plane", "(", "env", ".", "state", ",", "preprocess", "=", "True", ",", "return_seg", "=", "True", ")", "\n", "if", "self", ".", "config", ".", "recurrent", ":", "\n", "            ", "plane_history", "=", "deque", "(", "maxlen", "=", "self", ".", "config", ".", "recurrent_history_len", ")", "# instanciate history at beginning of each episode", "\n", "# play an episode greedily", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "_", "in", "range", "(", "1", ",", "steps", "+", "1", ")", ":", "\n", "# if recurrent add plane to history", "\n", "                ", "if", "self", ".", "config", ".", "recurrent", ":", "\n", "                    ", "plane_history", ".", "append", "(", "sample", "[", "\"plane\"", "]", ")", "\n", "# if less than config.recurrent_history_len planes, then pad to the left with oldest plane in history", "\n", "if", "len", "(", "plane_history", ")", "<", "self", ".", "config", ".", "recurrent_history_len", ":", "\n", "                        ", "n_pad", "=", "self", ".", "config", ".", "recurrent_history_len", "-", "len", "(", "plane_history", ")", "\n", "tensor_pad", "=", "plane_history", "[", "0", "]", "\n", "# concatenate the history to 1*LxCxHxW", "\n", "plane", "=", "np", ".", "concatenate", "(", "[", "tensor_pad", "]", "*", "n_pad", "+", "list", "(", "plane_history", ")", ",", "axis", "=", "0", ")", "\n", "", "else", ":", "\n", "                        ", "plane", "=", "np", ".", "concatenate", "(", "list", "(", "plane_history", ")", ",", "axis", "=", "0", ")", "\n", "# else the current plane is passed through the Qnetwork", "\n", "", "", "else", ":", "\n", "                    ", "plane", "=", "sample", "[", "\"plane\"", "]", "\n", "# get action from current state", "\n", "", "actions", "=", "self", ".", "act", "(", "plane", ",", "local_model", ")", "\n", "# observe transition and next_slice", "\n", "transition", ",", "next_sample", "=", "env", ".", "step", "(", "actions", ",", "preprocess", "=", "True", ")", "\n", "# set slice to next slice", "\n", "sample", "=", "next_sample", "\n", "# add logs to output dict  ", "\n", "out", "[", "\"planes\"", "]", ".", "append", "(", "sample", "[", "\"plane\"", "]", ".", "squeeze", "(", ")", ")", "\n", "if", "not", "self", ".", "config", ".", "realCT", ":", "\n", "                    ", "out", "[", "\"segs\"", "]", ".", "append", "(", "sample", "[", "\"seg\"", "]", ".", "squeeze", "(", ")", ")", "\n", "", "if", "self", ".", "config", ".", "CT2US", ":", "\n", "                    ", "out", "[", "\"planesCT\"", "]", ".", "append", "(", "sample", "[", "\"planeCT\"", "]", ".", "squeeze", "(", ")", ")", "\n", "", "out", "[", "\"states\"", "]", ".", "append", "(", "env", ".", "state", ")", "\n", "# when we test on clinical data we do not know the rewards we get", "\n", "if", "not", "self", ".", "config", ".", "realCT", ":", "\n", "#out[\"logs\"].append({log: r for log,r in env.logs.items()}) # cumulative logs", "\n", "                    ", "out", "[", "\"logs\"", "]", ".", "append", "(", "{", "log", ":", "r", "for", "log", ",", "r", "in", "env", ".", "current_logs", ".", "items", "(", ")", "}", ")", "\n", "# if done, end episode early and pad logs with terminal state", "\n", "", "if", "transition", "[", "-", "1", "]", ":", "\n", "                    ", "break", "\n", "# when we test on clinical data we do not know the rewards we get", "\n", "", "", "", "if", "not", "self", ".", "config", ".", "realCT", ":", "\n", "# add logs for wandb to out", "\n", "            ", "out", "[", "\"wandb\"", "]", "=", "{", "log", "+", "\"_test\"", ":", "r", "for", "log", ",", "r", "in", "env", ".", "logs", ".", "items", "(", ")", "}", "\n", "# re-arrange the logs key so that it goes from list of dicts to dict of lists", "\n", "out", "[", "\"logs\"", "]", "=", "{", "k", ":", "[", "dic", "[", "k", "]", "for", "dic", "in", "out", "[", "\"logs\"", "]", "]", "for", "k", "in", "out", "[", "\"logs\"", "]", "[", "0", "]", "}", "\n", "# get the mean rewards collected by the agents in the episode", "\n", "out", "[", "\"logs_mean\"", "]", "=", "{", "key", ":", "np", ".", "mean", "(", "val", ")", "for", "key", ",", "val", "in", "out", "[", "\"logs\"", "]", ".", "items", "(", ")", "}", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.agent.SingleVolumeAgent.train": [[131, 153], ["local_model.train", "range", "local_model.eval", "max", "min", "agent.SingleVolumeAgent.prepare_batch", "agent.SingleVolumeAgent.learn", "buffer.update_priorities", "loss.item", "deltas.cpu().detach().numpy().squeeze", "batch[].to", "deltas.cpu().detach().numpy", "deltas.cpu().detach", "deltas.cpu"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.baseAgent.BaseAgent.train", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.agent.MultiVolumeAgent.prepare_batch", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.baseAgent.BaseAgent.learn", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.PrioritizedReplayBuffer.update_priorities"], ["", "def", "train", "(", "self", ",", "env", ",", "buffer", ",", "local_model", ",", "target_model", ",", "optimizer", ",", "criterion", ",", "n_iter", "=", "1", ")", ":", "\n", "# train the q_network for a number of iterations", "\n", "        ", "local_model", ".", "train", "(", ")", "\n", "total_loss", "=", "0", "\n", "for", "i", "in", "range", "(", "n_iter", ")", ":", "\n", "# 1. sample batch and send to GPU", "\n", "            ", "batch", "=", "self", ".", "prepare_batch", "(", "env", ",", "buffer", ")", "\n", "for", "key", "in", "batch", ":", "\n", "                ", "if", "key", "!=", "\"indices\"", ":", "\n", "                    ", "batch", "[", "key", "]", "=", "batch", "[", "key", "]", ".", "to", "(", "self", ".", "config", ".", "device", ")", "\n", "# 2. take a training step", "\n", "", "", "loss", ",", "deltas", "=", "self", ".", "learn", "(", "batch", ",", "local_model", ",", "target_model", ",", "optimizer", ",", "criterion", ")", "\n", "# 3. update priorities", "\n", "buffer", ".", "update_priorities", "(", "batch", "[", "\"indices\"", "]", ",", "deltas", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ".", "squeeze", "(", ")", ")", "\n", "# 4. add to total loss", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "# set back to eval mode as we will only be training inside this function", "\n", "", "local_model", ".", "eval", "(", ")", "\n", "# decrease eps", "\n", "self", ".", "eps", "=", "max", "(", "self", ".", "eps", "*", "self", ".", "EPS_DECAY_FACTOR", ",", "self", ".", "config", ".", "eps_end", ")", "\n", "self", ".", "beta", "=", "min", "(", "self", ".", "beta", "*", "self", ".", "BETA_DECAY_FACTOR", ",", "self", ".", "config", ".", "beta_end", ")", "\n", "return", "loss", "/", "n_iter", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.agent.SingleVolumeAgent.prepare_batch": [[154, 179], ["buffer.sample", "torch.from_numpy().float().squeeze", "env.sample_planes", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().unsqueeze().float", "torch.from_numpy().unsqueeze().long", "torch.tensor().unsqueeze().bool", "torch.from_numpy().float", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "torch.tensor().unsqueeze", "numpy.vstack", "numpy.vstack", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.tensor", "numpy.hstack", "numpy.hstack"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.buffer.buffer.RecurrentPrioritizedReplayBuffer.sample", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.environment.baseEnvironment.BaseEnvironment.sample_planes"], ["", "def", "prepare_batch", "(", "self", ",", "env", ",", "buffer", ")", ":", "\n", "        ", "\"\"\"arranges a training batch for the learn function\n        Params:\n        ==========\n            env (environment/* instance): the environment the agent will interact with while training.\n            buffer (buffer/* object): replay buffer shared amongst processes (each process pushes to the same memory.)\n        Returns: batch (tuple): preprocessed transitions\n        \"\"\"", "\n", "# 1. sample transitions, weights and indices (for prioritization)", "\n", "batch", ",", "weights", ",", "indices", "=", "buffer", ".", "sample", "(", "beta", "=", "self", ".", "beta", ")", "\n", "weights", "=", "torch", ".", "from_numpy", "(", "weights", ")", ".", "float", "(", ")", ".", "squeeze", "(", ")", "\n", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "dones", "=", "batch", "\n", "# this is used when recurrent Q network is used, states and next_states will be B*L x 3 x 3", "\n", "L", "=", "self", ".", "config", ".", "recurrent_history_len", "if", "self", ".", "config", ".", "recurrent", "else", "1", "\n", "# 2. sample planes and next_planes using multiple threads", "\n", "sample", "=", "env", ".", "sample_planes", "(", "states", "+", "next_states", ",", "preprocess", "=", "True", ")", "\n", "# 3. preprocess each item", "\n", "states", "=", "torch", ".", "from_numpy", "(", "np", ".", "vstack", "(", "sample", "[", "\"plane\"", "]", "[", ":", "self", ".", "config", ".", "batch_size", "*", "L", "]", ")", ")", ".", "float", "(", ")", "\n", "next_states", "=", "torch", ".", "from_numpy", "(", "np", ".", "vstack", "(", "sample", "[", "\"plane\"", "]", "[", "self", ".", "config", ".", "batch_size", "*", "L", ":", "]", ")", ")", ".", "float", "(", ")", "\n", "rewards", "=", "torch", ".", "from_numpy", "(", "np", ".", "hstack", "(", "rewards", ")", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "float", "(", ")", "\n", "actions", "=", "torch", ".", "from_numpy", "(", "np", ".", "hstack", "(", "actions", ")", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "long", "(", ")", "\n", "dones", "=", "torch", ".", "tensor", "(", "dones", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "bool", "(", ")", "\n", "# organize batch and return", "\n", "batch", "=", "{", "\"states\"", ":", "states", ",", "\"actions\"", ":", "actions", ",", "\"rewards\"", ":", "rewards", ",", "\"next_states\"", ":", "next_states", ",", "\"dones\"", ":", "dones", ",", "\"weights\"", ":", "weights", ",", "\"indices\"", ":", "indices", "}", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.agent.SingleVolumeAgent.learn": [[180, 204], ["optimizer.zero_grad", "agent.SingleVolumeAgent.trainer.step", "loss.backward", "optimizer.step", "agent.SingleVolumeAgent.config.target_update.lower", "agent.SingleVolumeAgent.soft_update", "agent.SingleVolumeAgent.config.target_update.lower", "agent.SingleVolumeAgent.hard_update", "ValueError"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.trainers.PrioritizedDoubleDeepQLearning.step", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.trainers.PrioritizedDoubleDeepQLearning.step", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.baseAgent.BaseAgent.soft_update", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.baseAgent.BaseAgent.hard_update"], ["", "def", "learn", "(", "self", ",", "batch", ",", "local_model", ",", "target_model", ",", "optimizer", ",", "criterion", ")", ":", "\n", "        ", "\"\"\" Update value parameters using given batch of experience tuples.\n        Params:\n        ==========\n            batch (dict): contains all training inputs (see self.prepare_batch())\n            local_model (PyTorch model): pytorch network that will be trained using a particular training routine (i.e. DQN)\n            target_model (PyTorch model): pytorch network that will be used as a target to estimate future Qvalues. \n                                          (it is a hard copy or a running average of the local model, helps against diverging)\n            optimizer (PyTorch optimizer): optimizer to update the local network weights.\n            criterion (PyTorch Module): loss to minimize in order to train the local network.\n        \"\"\"", "\n", "# 1. take a training step (retain graph because we will backprop multiple times through the backbone cnn)", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ",", "deltas", "=", "self", ".", "trainer", ".", "step", "(", "batch", ",", "local_model", ",", "target_model", ",", "criterion", ")", "\n", "loss", ".", "backward", "(", "retain_graph", "=", "True", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "# 2. update target network", "\n", "if", "self", ".", "config", ".", "target_update", ".", "lower", "(", ")", "==", "\"soft\"", ":", "\n", "            ", "self", ".", "soft_update", "(", "local_model", ",", "target_model", ",", "self", ".", "config", ".", "tau", ")", "\n", "", "elif", "self", ".", "config", ".", "target_update", ".", "lower", "(", ")", "==", "\"hard\"", ":", "\n", "            ", "self", ".", "hard_update", "(", "local_model", ",", "target_model", ",", "self", ".", "config", ".", "delay_steps", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'unknown ``self.target_update``: {}. possible options: [hard, soft]'", ".", "format", "(", "self", ".", "config", ".", "target_update", ")", ")", "\n", "", "return", "loss", ",", "deltas", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.agent.MultiVolumeAgent.__init__": [[208, 218], ["agent.SingleVolumeAgent.__init__", "len", "config.volume_ids.split"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "\"\"\"Initialize an Agent object.\n        Params:\n        ======\n            config (argparse object): parser with all training options (see options/options.py)\n        \"\"\"", "\n", "# Initialize the base class", "\n", "SingleVolumeAgent", ".", "__init__", "(", "self", ",", "config", ")", "\n", "# number of environments that we are training on", "\n", "self", ".", "n_envs", "=", "len", "(", "config", ".", "volume_ids", ".", "split", "(", "','", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.agent.MultiVolumeAgent.play_episode": [[220, 236], ["concurrent.futures.ThreadPoolExecutor", "f.result", "executor.submit", "futures.items"], "methods", ["None"], ["", "def", "play_episode", "(", "self", ",", "envs", ",", "local_model", ",", "buffers", ")", ":", "\n", "        ", "\"\"\" Plays one episode on an input environment.\n        Params:\n        ==========\n            envs dict[environment/* instance]: dict of environments the agent will interact with while training.\n            local_model (PyTorch model): pytorch network that will be trained using a particular training routine (i.e. DQN)\n            buffers dict[buffer/* object]: dict of replay buffers (one per environment, same keys)\n        Returns logs (dict): all relevant logs acquired throughout the episode.\n        \"\"\"", "\n", "# play episode in each environment using multi-threading", "\n", "with", "concurrent", ".", "futures", ".", "ThreadPoolExecutor", "(", ")", "as", "executor", ":", "\n", "            ", "futures", "=", "{", "vol_id", ":", "executor", ".", "submit", "(", "super", "(", "MultiVolumeAgent", ",", "self", ")", ".", "play_episode", ",", "envs", "[", "vol_id", "]", ",", "\n", "local_model", ",", "\n", "buffers", "[", "vol_id", "]", ")", "for", "vol_id", "in", "envs", "}", "\n", "", "logs", "=", "{", "vol_id", ":", "f", ".", "result", "(", ")", "for", "vol_id", ",", "f", "in", "futures", ".", "items", "(", ")", "}", "\n", "return", "logs", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.agent.MultiVolumeAgent.prepare_batch": [[238, 254], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "numpy.stack", "concurrent.futures.ThreadPoolExecutor", "f.result", "executor.submit", "futures.values"], "methods", ["None"], ["", "def", "prepare_batch", "(", "self", ",", "envs", ",", "buffers", ")", ":", "\n", "# 1. sample batches in parallel from all buffers", "\n", "        ", "with", "concurrent", ".", "futures", ".", "ThreadPoolExecutor", "(", ")", "as", "executor", ":", "\n", "            ", "futures", "=", "{", "vol_id", ":", "executor", ".", "submit", "(", "super", "(", "MultiVolumeAgent", ",", "self", ")", ".", "prepare_batch", ",", "envs", "[", "vol_id", "]", ",", "buffers", "[", "vol_id", "]", ")", "for", "vol_id", "in", "envs", "}", "\n", "", "batches", "=", "[", "f", ".", "result", "(", ")", "for", "f", "in", "futures", ".", "values", "(", ")", "]", "\n", "# 2. concatenate states, actions, rewards, next_states, weights and indices from all buffers into a single batch", "\n", "states", "=", "torch", ".", "cat", "(", "[", "batch", "[", "\"states\"", "]", "for", "batch", "in", "batches", "]", ",", "dim", "=", "0", ")", "\n", "actions", "=", "torch", ".", "cat", "(", "[", "batch", "[", "\"actions\"", "]", "for", "batch", "in", "batches", "]", ",", "dim", "=", "1", ")", "\n", "rewards", "=", "torch", ".", "cat", "(", "[", "batch", "[", "\"rewards\"", "]", "for", "batch", "in", "batches", "]", ",", "dim", "=", "1", ")", "\n", "next_states", "=", "torch", ".", "cat", "(", "[", "batch", "[", "\"next_states\"", "]", "for", "batch", "in", "batches", "]", ",", "dim", "=", "0", ")", "\n", "dones", "=", "torch", ".", "cat", "(", "[", "batch", "[", "\"dones\"", "]", "for", "batch", "in", "batches", "]", ",", "dim", "=", "0", ")", "\n", "weights", "=", "torch", ".", "cat", "(", "[", "batch", "[", "\"weights\"", "]", "for", "batch", "in", "batches", "]", ",", "dim", "=", "0", ")", "\n", "indices", "=", "np", ".", "stack", "(", "[", "batch", "[", "\"indices\"", "]", "for", "batch", "in", "batches", "]", ")", "\n", "# organize batch and return", "\n", "batch", "=", "{", "\"states\"", ":", "states", ",", "\"actions\"", ":", "actions", ",", "\"rewards\"", ":", "rewards", ",", "\"next_states\"", ":", "next_states", ",", "\"dones\"", ":", "dones", ",", "\"weights\"", ":", "weights", ",", "\"indices\"", ":", "indices", "}", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.agent.MultiVolumeAgent.train": [[256, 280], ["local_model.train", "tqdm.tqdm.tqdm", "local_model.eval", "max", "min", "range", "agent.MultiVolumeAgent.prepare_batch", "agent.MultiVolumeAgent.learn", "deltas.cpu().detach().numpy().squeeze().reshape.cpu().detach().numpy().squeeze().reshape.cpu().detach().numpy().squeeze().reshape", "loss.item", "concurrent.futures.ThreadPoolExecutor", "batch[].to", "deltas.cpu().detach().numpy().squeeze().reshape.cpu().detach().numpy().squeeze().reshape.cpu().detach().numpy().squeeze", "executor.submit", "zip", "deltas.cpu().detach().numpy().squeeze().reshape.cpu().detach().numpy().squeeze().reshape.cpu().detach().numpy", "buffers.values", "deltas.cpu().detach().numpy().squeeze().reshape.cpu().detach().numpy().squeeze().reshape.cpu().detach", "deltas.cpu().detach().numpy().squeeze().reshape.cpu().detach().numpy().squeeze().reshape.cpu"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.baseAgent.BaseAgent.train", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.agent.MultiVolumeAgent.prepare_batch", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.baseAgent.BaseAgent.learn"], ["", "def", "train", "(", "self", ",", "envs", ",", "local_model", ",", "target_model", ",", "optimizer", ",", "criterion", ",", "buffers", ",", "n_iter", "=", "1", ")", ":", "\n", "# train the q_network for a number of iterations", "\n", "        ", "local_model", ".", "train", "(", ")", "\n", "total_loss", "=", "0", "\n", "for", "_", "in", "tqdm", "(", "range", "(", "n_iter", ")", ",", "desc", "=", "\"training Qnetwork...\"", ")", ":", "\n", "# 1. sample batch and send to GPU", "\n", "            ", "batch", "=", "self", ".", "prepare_batch", "(", "envs", ",", "buffers", ")", "\n", "for", "key", "in", "batch", ":", "\n", "                ", "if", "key", "!=", "\"indices\"", ":", "\n", "                    ", "batch", "[", "key", "]", "=", "batch", "[", "key", "]", ".", "to", "(", "self", ".", "config", ".", "device", ")", "\n", "# 2. take a training step", "\n", "", "", "loss", ",", "deltas", "=", "self", ".", "learn", "(", "batch", ",", "local_model", ",", "target_model", ",", "optimizer", ",", "criterion", ")", "\n", "# 3. update priorities for each buffer separately (do this in parallel)", "\n", "deltas", "=", "deltas", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ".", "squeeze", "(", ")", ".", "reshape", "(", "self", ".", "n_envs", ",", "-", "1", ")", "\n", "with", "concurrent", ".", "futures", ".", "ThreadPoolExecutor", "(", ")", "as", "executor", ":", "\n", "                ", "futures", "=", "[", "executor", ".", "submit", "(", "buffer", ".", "update_priorities", ",", "ind", ",", "delt", ")", "for", "buffer", ",", "ind", ",", "delt", "in", "zip", "(", "buffers", ".", "values", "(", ")", ",", "batch", "[", "\"indices\"", "]", ",", "deltas", ")", "]", "\n", "# 4. add to total loss", "\n", "", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "# set back to eval mode as we will only be training inside this function", "\n", "", "local_model", ".", "eval", "(", ")", "\n", "# decrease eps", "\n", "self", ".", "eps", "=", "max", "(", "self", ".", "eps", "*", "self", ".", "EPS_DECAY_FACTOR", ",", "self", ".", "config", ".", "eps_end", ")", "\n", "self", ".", "beta", "=", "min", "(", "self", ".", "beta", "*", "self", ".", "BETA_DECAY_FACTOR", ",", "self", ".", "config", ".", "beta_end", ")", "\n", "return", "loss", "/", "n_iter", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.agent.MultiVolumeAgent.test_agent": [[282, 295], ["concurrent.futures.ThreadPoolExecutor", "f.result", "executor.submit", "futures.items"], "methods", ["None"], ["", "def", "test_agent", "(", "self", ",", "steps", ",", "envs", ",", "local_model", ")", ":", "\n", "        ", "\"\"\"Test the greedy policy learned by the agent on some test environments and returns a dict with useful metrics/logs.\n        Params:\n        ==========\n            steps (int): number of steps to test the agent for.\n            envs dict[environment/* instance]: dict of environments.\n            local_model (PyTorch model): pytorch network that will be tested.\n        \"\"\"", "\n", "# test agent on each environment using multi-threading", "\n", "with", "concurrent", ".", "futures", ".", "ThreadPoolExecutor", "(", ")", "as", "executor", ":", "\n", "            ", "futures", "=", "{", "vol_id", ":", "executor", ".", "submit", "(", "super", "(", "MultiVolumeAgent", ",", "self", ")", ".", "test_agent", ",", "steps", ",", "envs", "[", "vol_id", "]", ",", "local_model", ")", "for", "vol_id", "in", "envs", "}", "\n", "", "logs", "=", "{", "vol_id", ":", "f", ".", "result", "(", ")", "for", "vol_id", ",", "f", "in", "futures", ".", "items", "(", ")", "}", "\n", "return", "logs", "", "", "", ""]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.baseAgent.BaseAgent.__init__": [[11, 52], ["os.path.join", "os.path.join", "os.path.exists", "os.makedirs", "os.path.exists", "os.makedirs", "config.trainer.lower", "PrioritizedDeepQLearning", "config.trainer.lower", "PrioritizedDoubleDeepQLearning", "NotImplementedError", "int", "int"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "\"\"\" Initialize environment object\n        Params:\n        =========\n            config (argparse object): all useful options to build the environment (see options/options.py for details)\n        \"\"\"", "\n", "\n", "# setup checkpoints and results dirs for any logging/ input output", "\n", "self", ".", "checkpoints_dir", "=", "os", ".", "path", ".", "join", "(", "config", ".", "checkpoints_dir", ",", "config", ".", "name", ")", "\n", "self", ".", "results_dir", "=", "os", ".", "path", ".", "join", "(", "config", ".", "results_dir", ",", "config", ".", "name", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "checkpoints_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "checkpoints_dir", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "results_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "results_dir", ")", "\n", "\n", "# setup the action size and the number of agents", "\n", "", "if", "config", ".", "termination", "==", "\"learned\"", ":", "\n", "            ", "assert", "config", ".", "action_size", "==", "7", ",", "\"action_size must be set to 7 when termination is learned.\"", "\n", "", "self", ".", "n_agents", ",", "self", ".", "action_size", "=", "config", ".", "n_agents", ",", "config", ".", "action_size", "\n", "\n", "# place holder for steps and episode counts", "\n", "self", ".", "t_step", ",", "self", ".", "episode", "=", "0", ",", "0", "\n", "# starting epsilon value for exploration/exploitation trade off", "\n", "self", ".", "eps", "=", "config", ".", "eps_start", "\n", "# formulate a suitable decay factor for epsilon given the queried options.", "\n", "self", ".", "EPS_DECAY_FACTOR", "=", "(", "config", ".", "eps_end", "/", "config", ".", "eps_start", ")", "**", "(", "1", "/", "(", "int", "(", "config", ".", "stop_decay", "*", "config", ".", "n_episodes", ")", "-", "config", ".", "starting_episode", ")", ")", "\n", "# starting beta value for bias correction in prioritized experience replay", "\n", "self", ".", "beta", "=", "config", ".", "beta_start", "\n", "# formulate a suitable decay factor for beta given the queried options. (since beta_end>beta_start, this will actually be an increase factor)", "\n", "# annealiate beta to 1 (or beta_end) as we go further in the episode (original P.E.R paper reccommends this)", "\n", "self", ".", "BETA_DECAY_FACTOR", "=", "(", "config", ".", "beta_end", "/", "config", ".", "beta_start", ")", "**", "(", "1", "/", "(", "int", "(", "config", ".", "stop_decay", "*", "config", ".", "n_episodes", ")", "-", "config", ".", "starting_episode", ")", ")", "\n", "# set the trainer algorithm", "\n", "if", "config", ".", "trainer", ".", "lower", "(", ")", "in", "[", "\"deepqlearning\"", ",", "\"qlearning\"", ",", "\"dqn\"", "]", ":", "\n", "            ", "self", ".", "trainer", "=", "PrioritizedDeepQLearning", "(", "gamma", "=", "config", ".", "gamma", ")", "\n", "", "elif", "config", ".", "trainer", ".", "lower", "(", ")", "in", "[", "\"doubledeepqlearning\"", ",", "\"doubleqlearning\"", ",", "\"doubledqn\"", ",", "\"ddqn\"", "]", ":", "\n", "            ", "self", ".", "trainer", "=", "PrioritizedDoubleDeepQLearning", "(", "gamma", "=", "config", ".", "gamma", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'unknown ``trainer`` configuration: {}. available options: [DQN, DoubleDQN]'", ".", "format", "(", "config", ".", "trainer", ")", ")", "\n", "\n", "# save the config for any options we might need", "\n", "", "self", ".", "config", "=", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.baseAgent.BaseAgent.random_action": [[53, 57], ["numpy.vstack", "random.choice", "numpy.arange", "range"], "methods", ["None"], ["", "def", "random_action", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return a random discrete action for each agent, stack them vertically.\n        \"\"\"", "\n", "return", "np", ".", "vstack", "(", "[", "random", ".", "choice", "(", "np", ".", "arange", "(", "self", ".", "action_size", ")", ")", "for", "_", "in", "range", "(", "self", ".", "n_agents", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.baseAgent.BaseAgent.greedy_action": [[58, 73], ["torch.from_numpy().float().to", "numpy.vstack", "torch.no_grad", "local_model", "torch.from_numpy().float", "torch.argmax().item", "torch.from_numpy", "torch.argmax"], "methods", ["None"], ["", "def", "greedy_action", "(", "self", ",", "slice", ",", "local_model", ")", ":", "\n", "        ", "\"\"\"Returns the discrete actions for which the Q values of each agent are maximized, stacked vertically.\n        Params:\n        ==========\n            slice (np.ndarray of shape (H, W)): 2D slice of anatomy.\n            local_model (PyTorch model): takes input the slice and outputs action values.\n\n        returns:\n            np.ndarray (contains an action for each agent)\n        \"\"\"", "\n", "# convert to tensor, normalize and unsqueeze to pass through qnetwork", "\n", "slice", "=", "torch", ".", "from_numpy", "(", "slice", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "config", ".", "device", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "Qs", "=", "local_model", "(", "slice", ")", "\n", "", "return", "np", ".", "vstack", "(", "[", "torch", ".", "argmax", "(", "Q", ",", "dim", "=", "1", ")", ".", "item", "(", ")", "for", "Q", "in", "Qs", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.baseAgent.BaseAgent.act": [[74, 86], ["random.random", "baseAgent.BaseAgent.greedy_action", "baseAgent.BaseAgent.random_action"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.baseAgent.BaseAgent.greedy_action", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.baseAgent.BaseAgent.random_action"], ["", "def", "act", "(", "self", ",", "plane", ",", "local_model", ",", "eps", "=", "0.", ")", ":", "\n", "        ", "\"\"\"Generate an action given some input.\n        Params:\n        ==========\n            plane (np.ndarray of shape (H, W)): 2D slice of anatomy.\n            local_model (PyTorch model): takes input the slice and outputs action values\n            eps (float): epsilon parameter governing exploration/exploitation trade-off\n        \"\"\"", "\n", "if", "random", ".", "random", "(", ")", ">", "eps", ":", "\n", "            ", "return", "self", ".", "greedy_action", "(", "plane", ",", "local_model", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "random_action", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.baseAgent.BaseAgent.learn": [[87, 91], ["None"], "methods", ["None"], ["", "", "@", "abstractmethod", "\n", "def", "learn", "(", "self", ")", ":", "\n", "        ", "\"\"\"update the Q network through some routine.\n        \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.baseAgent.BaseAgent.play_episode": [[92, 96], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "play_episode", "(", "self", ")", ":", "\n", "        ", "\"\"\"Make the agent play a full episode.\n        \"\"\"", "\n", "", "def", "train", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.baseAgent.BaseAgent.train": [[96, 98], ["NotImplementedError"], "methods", ["None"], ["", "def", "train", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.baseAgent.BaseAgent.soft_update": [[99, 110], ["zip", "target_model.parameters", "local_model.parameters", "target_param.data.copy_"], "methods", ["None"], ["", "def", "soft_update", "(", "self", ",", "local_model", ",", "target_model", ",", "tau", ")", ":", "\n", "        ", "\"\"\"Soft update model parameters.\n        \u03b8_target = \u03c4*\u03b8_local + (1 - \u03c4)*\u03b8_target\n        Params\n        ======\n            local_model (PyTorch model): weights will be copied from\n            target_model (PyTorch model): weights will be copied to\n            tau (float): interpolation parameter \n        \"\"\"", "\n", "for", "target_param", ",", "local_param", "in", "zip", "(", "target_model", ".", "parameters", "(", ")", ",", "local_model", ".", "parameters", "(", ")", ")", ":", "\n", "            ", "target_param", ".", "data", ".", "copy_", "(", "tau", "*", "local_param", ".", "data", "+", "(", "1.0", "-", "tau", ")", "*", "target_param", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.agent.baseAgent.BaseAgent.hard_update": [[111, 123], ["zip", "target_model.parameters", "local_model.parameters", "target_param.data.copy_"], "methods", ["None"], ["", "", "def", "hard_update", "(", "self", ",", "local_model", ",", "target_model", ",", "N", ")", ":", "\n", "        ", "\"\"\"hard update model parameters.\n        \u03b8_target = \u03b8_local every N steps.\n        Params\n        ======\n            local_model (PyTorch model): weights will be copied from\n            target_model (PyTorch model): weights will be copied to\n            N (flintoat): number of steps after which hard update takes place \n        \"\"\"", "\n", "if", "self", ".", "t_step", "%", "N", "==", "0", ":", "\n", "            ", "for", "target_param", ",", "local_param", "in", "zip", "(", "target_model", ".", "parameters", "(", ")", ",", "local_model", ".", "parameters", "(", ")", ")", ":", "\n", "                ", "target_param", ".", "data", ".", "copy_", "(", "local_param", ".", "data", ")", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.rewards.rewards.PlaneDistanceReward.__init__": [[8, 12], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "goal", ",", "weight", "=", "1", ")", ":", "\n", "        ", "self", ".", "goal", "=", "goal", "\n", "self", ".", "weight", "=", "weight", "\n", "self", ".", "previous_plane", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.rewards.rewards.PlaneDistanceReward.get_distance_from_goal": [[13, 15], ["None"], "methods", ["None"], ["", "def", "get_distance_from_goal", "(", "self", ",", "coefs", ")", ":", "\n", "        ", "return", "(", "(", "coefs", "-", "self", ".", "goal", ")", "**", "2", ")", ".", "sum", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.rewards.rewards.PlaneDistanceReward.__call__": [[16, 28], ["rewards.PlaneDistanceReward.get_distance_from_goal", "rewards.PlaneDistanceReward.get_distance_from_goal", "ValueError", "numpy.sign"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.rewards.rewards.PlaneDistanceReward.get_distance_from_goal", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.rewards.rewards.PlaneDistanceReward.get_distance_from_goal"], ["", "def", "__call__", "(", "self", ",", "coefs", ")", ":", "\n", "# if previous plane is none (first step) set it equal to the current step -> reward of zero at the beginning", "\n", "        ", "if", "self", ".", "previous_plane", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "'self.previous_plane not defined. Overwrite it as ``self.previous_plane = self.get_plane_coefs(pointA, pointB, pointC)``'", ")", "\n", "# calculate euclidean distance between current plane and the goal", "\n", "", "D1", "=", "self", ".", "get_distance_from_goal", "(", "coefs", ")", "\n", "# calculate distance between previous plane and goal", "\n", "D2", "=", "self", ".", "get_distance_from_goal", "(", "self", ".", "previous_plane", ")", "\n", "# store plane as the new previous plane", "\n", "self", ".", "previous_plane", "=", "coefs", "\n", "# return sign function of distance improvement (D1 should be smaller than D2 if we are getting closer -> +1 if closer, -1 if further, 0 if same distance)", "\n", "return", "self", ".", "weight", "*", "np", ".", "sign", "(", "D2", "-", "D1", ")", "# scale by weight", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.rewards.rewards.Oscillate.__init__": [[30, 33], ["collections.deque"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "history_length", ",", "stop_freq", ")", ":", "\n", "        ", "self", ".", "history", "=", "deque", "(", "maxlen", "=", "history_length", ")", "\n", "self", ".", "stop_freq", "=", "stop_freq", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.rewards.rewards.Oscillate.__call__": [[34, 45], ["rewards.Oscillate.history.append", "collections.Counter", "collections.Counter.most_common"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "state", ")", ":", "\n", "# append new state", "\n", "        ", "self", ".", "history", ".", "append", "(", "state", ")", "\n", "# get frequency of seen states in hystory", "\n", "counter", "=", "Counter", "(", "self", ".", "history", ")", "\n", "freq", "=", "counter", ".", "most_common", "(", ")", "\n", "# if most common plane seen more than stop_freq, then stop", "\n", "if", "freq", "[", "0", "]", "[", "1", "]", ">", "self", ".", "stop_freq", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.rewards.rewards.AnatomyReward.__init__": [[51, 59], ["int", "rewardIDs.split"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "rewardIDs", ",", "is_penalty", "=", "False", ",", "incremental", "=", "False", ",", "weight", "=", "1", ")", ":", "\n", "# if more IDs are passed store in an array", "\n", "        ", "self", ".", "IDs", "=", "[", "int", "(", "ID", ")", "for", "ID", "in", "rewardIDs", ".", "split", "(", "\",\"", ")", "]", "\n", "self", ".", "is_penalty", "=", "is_penalty", "\n", "self", ".", "incremental", "=", "incremental", "\n", "self", ".", "weight", "=", "weight", "\n", "if", "incremental", ":", "\n", "            ", "self", ".", "previous_reward", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.rewards.rewards.AnatomyReward.get_anatomy_reward": [[60, 68], ["numpy.prod"], "methods", ["None"], ["", "", "def", "get_anatomy_reward", "(", "self", ",", "seg", ")", ":", "\n", "        ", "rewardAnatomy", "=", "0", "\n", "for", "ID", "in", "self", ".", "IDs", ":", "\n", "            ", "rewardAnatomy", "+=", "(", "seg", "==", "ID", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "", "rewardAnatomy", "/=", "np", ".", "prod", "(", "seg", ".", "shape", ")", "\n", "if", "self", ".", "is_penalty", ":", "\n", "            ", "rewardAnatomy", "*=", "-", "1", "\n", "", "return", "rewardAnatomy", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.rewards.rewards.AnatomyReward.__call__": [[69, 84], ["rewards.AnatomyReward.get_anatomy_reward", "rewards.AnatomyReward.get_anatomy_reward", "numpy.sign"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.rewards.rewards.AnatomyReward.get_anatomy_reward", "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.rewards.rewards.AnatomyReward.get_anatomy_reward"], ["", "def", "__call__", "(", "self", ",", "seg", ")", ":", "\n", "        ", "\"\"\" evaluates the anotical reward as the ratio of pixels containing structures of interest in the segmented slice.\n        Params:\n        ==========\n            seg (np.ndarray): segmented slice of the current state.\n        returns -> float, ratio of pixels of interest wrt slice pixel count.\n        \"\"\"", "\n", "if", "not", "self", ".", "incremental", ":", "\n", "            ", "return", "self", ".", "get_anatomy_reward", "(", "seg", ")", "\n", "", "else", ":", "\n", "# the current slice should have higher anatomical reward if we are getting closer -> +1 if more anatomical content, -1 if less, 0 if same", "\n", "            ", "current_reward", "=", "self", ".", "get_anatomy_reward", "(", "seg", ")", "\n", "reward", "=", "np", ".", "sign", "(", "current_reward", "-", "self", ".", "previous_reward", ")", "\n", "self", ".", "previous_reward", "=", "current_reward", "\n", "return", "self", ".", "weight", "*", "reward", "# scale by weight", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.rewards.rewards.SteppingReward.__init__": [[90, 92], ["abs"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "penalty", ")", ":", "\n", "        ", "self", ".", "penalty", "=", "-", "abs", "(", "penalty", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.rewards.rewards.SteppingReward.__call__": [[93, 106], ["None"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "give_penalty", ")", ":", "\n", "        ", "\"\"\" Give a static penalty after each step if give_penalty flag is True, else do not reward nor penalyze.\n        This should incentivize the agent to move towards the goal quicker as when the agent is close to the goal, the give_penalty\n        flag will be set to False.\n        Params:\n        =========\n            give_penalty (bool): flag if penalize or not\n        returns -> float, the amount of penalty the agent will receive.\n        \"\"\"", "\n", "if", "give_penalty", ":", "\n", "            ", "return", "self", ".", "penalty", "\n", "", "else", ":", "\n", "            ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.rewards.rewards.AreaReward.__init__": [[112, 115], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "weight", ",", "max_area", ")", ":", "\n", "        ", "self", ".", "weight", "=", "weight", "\n", "self", ".", "max_area", "=", "max_area", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.rewards.rewards.AreaReward.__call__": [[116, 126], ["rewards.AreaReward.get_traingle_area"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.rewards.rewards.AreaReward.get_traingle_area"], ["", "def", "__call__", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\" Takes the current state (3 stacked 3D points), and evaluates the corresponding reward (proportional to area of triangle)\n        Params:\n        ==========\n            state (np.ndarray): 3 stacked 3D points organized in a 2D np array.\n        return -> float, corresponding reward\n        \"\"\"", "\n", "area", "=", "self", ".", "get_traingle_area", "(", "*", "state", ")", "\n", "area", "/=", "self", ".", "max_area", "\n", "return", "self", ".", "weight", "*", "area", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.rewards.rewards.AreaReward.get_traingle_area": [[127, 130], ["numpy.linalg.norm", "numpy.cross"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_traingle_area", "(", "a", ",", "b", ",", "c", ")", ":", "\n", "        ", "return", "0.5", "*", "np", ".", "linalg", ".", "norm", "(", "np", ".", "cross", "(", "b", "-", "a", ",", "c", "-", "a", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.rewards.rewards.OutOfBoundaryReward.__init__": [[135, 138], ["abs"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "penalty", ",", "sx", ",", "sy", ",", "sz", ")", ":", "\n", "        ", "self", ".", "penalty", "=", "-", "abs", "(", "penalty", ")", "\n", "self", ".", "sx", ",", "self", ".", "sy", ",", "self", ".", "sz", "=", "sx", ",", "sy", ",", "sz", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.rewards.rewards.OutOfBoundaryReward.__call__": [[139, 157], ["zip", "max", "abs"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "point", ")", ":", "\n", "        ", "\"\"\" give a penalty proportional to how many pixels OOB the agent is. If the agent is within the volume then do not give any penalty.\n        Params:\n        ==========\n            point (np.ndarray): the current location of an agent (3 element array).\n        returns -> float, corresponding oob reward for the agent\n        \"\"\"", "\n", "total_penalty", "=", "0", "\n", "for", "coord", ",", "s", "in", "zip", "(", "point", ",", "[", "self", ".", "sx", ",", "self", ".", "sy", ",", "self", ".", "sz", "]", ")", ":", "\n", "# the origin of the volume is at (0,0,0), hence the agent is out of boundary", "\n", "# if its position is >= than the resolution of the volume OR if its position is", "\n", "# negative. We count the number of pixels out of boundary for each dimension as a penalty,", "\n", "# scaled by self.penalty", "\n", "            ", "if", "coord", ">=", "0", ":", "\n", "                ", "total_penalty", "+=", "self", ".", "penalty", "*", "max", "(", "0", ",", "coord", "-", "s", ")", "\n", "", "else", ":", "\n", "                ", "total_penalty", "+=", "self", ".", "penalty", "*", "abs", "(", "coord", ")", "\n", "", "", "return", "total_penalty", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.rewards.rewards.StopReward.__init__": [[163, 166], ["abs"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "scale", ",", "goal_reward", ")", ":", "\n", "        ", "self", ".", "scale", "=", "abs", "(", "scale", ")", "\n", "self", ".", "goal_reward", "=", "goal_reward", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.rewards.rewards.StopReward.__call__": [[167, 177], ["increment.any"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "increment", ",", "current_reward", ")", ":", "\n", "# if all agents choose to not move, then increment will be an all-zero array,", "\n", "# in this case increment.any() will return False and we give the penalty.", "\n", "        ", "if", "increment", ".", "any", "(", ")", ":", "\n", "            ", "return", "0", "\n", "", "else", ":", "\n", "# get difference in reward from current step and the goal reward", "\n", "            ", "D", "=", "current_reward", "-", "self", ".", "goal_reward", "# negative if doing worse than goal, positive if doing better than goal ", "\n", "# scale reward as queried", "\n", "return", "self", ".", "scale", "*", "D", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.options.options.gather_options": [[6, 124], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "ValueError"], "function", ["None"], ["def", "gather_options", "(", "phase", "=", "\"train\"", ")", ":", "\n", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'train/test scripts to launch navigation experiments.'", ")", "\n", "# I/O directories and data", "\n", "parser", ".", "add_argument", "(", "'--name'", ",", "'-n'", ",", "type", "=", "str", ",", "help", "=", "'name of the experiment.'", ")", "\n", "parser", ".", "add_argument", "(", "'--dataroot'", ",", "'-r'", ",", "type", "=", "str", ",", "default", "=", "\"/vol/biomedic3/hjr119/DATA/XCAT_VOLUMES/\"", ",", "help", "=", "'path to the XCAT CT volumes.'", ")", "\n", "parser", ".", "add_argument", "(", "'--volume_ids'", ",", "'-vol_ids'", ",", "type", "=", "str", ",", "default", "=", "'samp0'", ",", "help", "=", "'filename(s) of the CT volume(s) comma separated.'", ")", "\n", "parser", ".", "add_argument", "(", "'--ct2us_model_name'", ",", "'-model'", ",", "type", "=", "str", ",", "default", "=", "\"bestCT2US\"", ",", "\n", "help", "=", "'filename for the state dict of the ct2us model (.pth) file.\\navailable models can be found at ./models'", ")", "\n", "parser", ".", "add_argument", "(", "'--results_dir'", ",", "type", "=", "str", ",", "default", "=", "'./results/'", ",", "help", "=", "'where to save the trajectory.'", ")", "\n", "parser", ".", "add_argument", "(", "'--checkpoints_dir'", ",", "type", "=", "str", ",", "default", "=", "'./checkpoints/'", ",", "help", "=", "'where to save the trajectory.'", ")", "\n", "parser", ".", "add_argument", "(", "'--load'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "help", "=", "'which model to load from.'", ")", "\n", "parser", ".", "add_argument", "(", "'--load_name'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "help", "=", "'which experiment to load model from.'", ")", "\n", "\n", "# logs and checkpointing ", "\n", "parser", ".", "add_argument", "(", "'--wandb'", ",", "type", "=", "str", ",", "default", "=", "'online'", ",", "help", "=", "'handles weights and biases interface.\\n'", "'online: launches online interface.\\n'", "'offline: writes all data to disk for later syncing to a server\\n'", "'disabled: completely shuts off wandb. (default = online)'", ")", "\n", "# preprocessing", "\n", "parser", ".", "add_argument", "(", "'--load_size'", ",", "type", "=", "int", ",", "default", "=", "256", ",", "help", "=", "\"resolution to load the data. By default 256 isotropic resolution.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--no_preprocess'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"If you do not want to preprocess the CT volume. (set to uint8 and scale intensities)\"", ")", "\n", "parser", ".", "add_argument", "(", "'--pmin'", ",", "type", "=", "int", ",", "default", "=", "150", ",", "help", "=", "\"pmin value for xcatEnvironment/intensity_scaling() function.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--pmax'", ",", "type", "=", "int", ",", "default", "=", "200", ",", "help", "=", "\"pmax value for xcatEnvironment/intensity_scaling() function.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--nmin'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "\"nmin value for xcatEnvironment/intensity_scaling() function.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--nmax'", ",", "type", "=", "int", ",", "default", "=", "255", ",", "help", "=", "\"nmax value for xcatEnvironment/intensity_scaling() function.\"", ")", "\n", "\n", "# Qnetwork specs", "\n", "parser", ".", "add_argument", "(", "'--default_Q'", ",", "type", "=", "str", ",", "default", "=", "\"small\"", ",", "help", "=", "\"give a standard architecure: small -> 3 blocks with stride 4. large -> 6 blocks with stride 2.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--n_blocks_Q'", ",", "type", "=", "int", ",", "default", "=", "6", ",", "help", "=", "\"number of convolutional blocks in the Qnetwork.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--downsampling_Q'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "help", "=", "\"downsampling factor of each convolutional layer of the Qnetwork.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--n_features_Q'", ",", "type", "=", "int", ",", "default", "=", "4", ",", "help", "=", "\"number of features in the first convolutional layer of the Qnetwork.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--no_dropout_Q'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"if use dropout in the Qnetwork (p=0.5 after conv layers and p=0.1 after linear layers).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--no_batchnorm_Q'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"if use batch normalization in the Qnetwork.\"", ")", "\n", "\n", "# agent specs", "\n", "parser", ".", "add_argument", "(", "'--action_size'", ",", "type", "=", "int", ",", "default", "=", "6", ",", "help", "=", "\"how many action can a single agent perform.\\n(i.e. up/down,left/right,forward/backwards,do nothing = 7 in a 3D volume).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--n_agents'", ",", "type", "=", "int", ",", "default", "=", "3", ",", "help", "=", "\"how many RL agents (heads) will share the same CNN backbone.\"", ")", "\n", "\n", "# termination specs", "\n", "parser", ".", "add_argument", "(", "'--termination'", ",", "type", "=", "str", ",", "default", "=", "\"oscillate\"", ",", "help", "=", "\"options: <oscillate, learned> whether we terminate the episode when the agent starts to oscillate or if we learn termination with an extra action.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--termination_history_len'", ",", "type", "=", "int", ",", "default", "=", "20", ",", "help", "=", "\"number of history frames to check oscillations on termination.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--termination_oscillation_freq'", ",", "type", "=", "int", ",", "default", "=", "3", ",", "help", "=", "\"if in the last ``termination_history_len`` steps there are more than this number of equal planes, terminate the episode for oscillation.\"", ")", "\n", "\n", "# reward signal shaping", "\n", "parser", ".", "add_argument", "(", "'--goal_centroids'", ",", "type", "=", "str", ",", "default", "=", "\"2885,2897,2895\"", ",", "help", "=", "\"centroids of these anatomical tissues will define the goal plane.\\n\"", "\"(default: LV+RV+RA: 2885,2897,2895). if multiple IDs separate by comma.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--planeDistanceRewardWeight'", ",", "type", "=", "float", ",", "default", "=", "1.", ",", "help", "=", "'relative weight of the plane distance reward if present, see rewards/rewards.py for more info.\\n'", ")", "\n", "parser", ".", "add_argument", "(", "'--anatomyRewardWeight'", ",", "type", "=", "float", ",", "default", "=", "1.", ",", "help", "=", "'relative weight of the anatomy reward if present, see rewards/rewards.py for more info.\\n'", ")", "\n", "parser", ".", "add_argument", "(", "'--anatomyRewardIDs'", ",", "type", "=", "str", ",", "default", "=", "\"2885,2897,2895\"", ",", "help", "=", "\"segmentation IDs for the anatomical reward, see rewards/rewards.py for more info.\\n\"", "\"(default: LV+RV+RA: 2885,2897,2895). if multiple IDs separate by comma.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--incrementalAnatomyReward'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "\"whether the agent is rewarded on the improvement of anatomical content or on the current anatomical content.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--oobReward'", ",", "type", "=", "float", ",", "default", "=", "0.01", ",", "help", "=", "'penalize each agent if it steps outside the boundaries of the volume, see rewards/rewards.py for more info.'", ")", "\n", "parser", ".", "add_argument", "(", "'--areaRewardWeight'", ",", "type", "=", "float", ",", "default", "=", "0.01", ",", "help", "=", "'reward the agents if they stay far apart from each other (measuring area of spanned triangle), see rewards/rewards.py for more info.\\n'", "'This is to prevent them from clustering together, which will yield rough transitions.'", ")", "\n", "parser", ".", "add_argument", "(", "'--stopReward'", ",", "type", "=", "float", ",", "default", "=", "0.", ",", "help", "=", "\"penalize the agents when they to stop on a bad frame, see rewards/rewards.py for more info.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--penalize_oob_pixels'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"penalize the agents when they sample slices significantly out of boundary, see rewards/rewards.py for more info.\\n\"", "\"It will give a penalty equal to the ratio of oob pixels in a sampled slice.\"", ")", "\n", "\n", "# random seed for reproducibility", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "\"random seed for reproducibility.\"", ")", "\n", "# gpu device", "\n", "parser", ".", "add_argument", "(", "'--gpu_device'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "\"gpu ID if multiple devices available. (only single device training supported)\"", ")", "\n", "# flag for easier objective", "\n", "parser", ".", "add_argument", "(", "'--easy_objective'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"starts the agent in a plane that should be close to a 4-chamber view.\"", ")", "\n", "# flag for location aware environment (it will give agents information about their location concatenating binary location maps to the input image through the channel dimension)", "\n", "parser", ".", "add_argument", "(", "'--location_aware'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"will emebed the agents with information regarding their relative positions in the sampled slice.\"", "\"achieved by concatenating binary location maps along the channels dimension of the input image to the qnetwork.\"", ")", "\n", "# flag for CT2US pipeline (will convert slices to US before feeding them to RL agents)", "\n", "parser", ".", "add_argument", "(", "'--CT2US'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"will launch full pipeline navigating in US domain. Else navigation will take place in the CT/XCAT volume.\"", ")", "\n", "# flag for realCT inputs. We can evaluate our agents on actual clinical data", "\n", "parser", ".", "add_argument", "(", "'--realCT'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"tells the framework we are using actual clinical data (we have no segmentations available etc.)\"", ")", "\n", "# flag if randomizing image instensities on an episode basis, useful when navigatin in fakeCT to ensure diverse intensity range for generalization", "\n", "parser", ".", "add_argument", "(", "'--randomize_intensities'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "\"whether to randomize intensities each time we reset the environment class, usefull for generalization in fakeCT navigation.\"", ")", "\n", "\n", "# training options (general)", "\n", "parser", ".", "add_argument", "(", "'--starting_episode'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "\"what episode we start training from.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--n_episodes'", ",", "type", "=", "int", ",", "default", "=", "2000", ",", "help", "=", "\"number of episodes to train the agents for.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--n_steps_per_episode'", ",", "type", "=", "int", ",", "default", "=", "250", ",", "help", "=", "\"number of steps in each episode.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "64", ",", "help", "=", "\"batch size for the replay buffer.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--buffer_size'", ",", "type", "=", "int", ",", "default", "=", "50000", ",", "help", "=", "\"capacity of the replay buffer.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gamma'", ",", "type", "=", "int", ",", "default", "=", "0.999", ",", "help", "=", "\"discount factor.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--learning_rate'", ",", "'-lr'", ",", "type", "=", "float", ",", "default", "=", "0.0001", ",", "help", "=", "\"learning rate for the q network.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--loss'", ",", "type", "=", "str", ",", "default", "=", "\"SmoothL1\"", ",", "help", "=", "'which loss to use to optimize the Qnetwork (MSE, SmoothL1).'", ")", "\n", "parser", ".", "add_argument", "(", "'--trainer'", ",", "type", "=", "str", ",", "default", "=", "\"DoubleDeepQLearning\"", ",", "help", "=", "'which training routine to use (DeepQLearning, DoubleDeepQLearning...).'", ")", "\n", "parser", ".", "add_argument", "(", "'--dueling'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"instanciates a dueling q-network architecture, everything else is the same.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--recurrent'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"instanciates a recurrent q-network architecture that keeps account of a history of frames, everything else is the same.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--recurrent_history_len'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "help", "=", "\"number of lookup steps when using a recurrent q network.\"", ")", "\n", "\n", "# training options (specific)", "\n", "parser", ".", "add_argument", "(", "'--stop_decay'", ",", "type", "=", "float", ",", "default", "=", "0.9", ",", "help", "=", "\"after what fraction of episodes we want to have eps = --eps_end or beta = --beta_end.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--eps_start'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "help", "=", "\"epsilon factor for egreedy policy, starting value.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--eps_end'", ",", "type", "=", "float", ",", "default", "=", "0.005", ",", "help", "=", "\"epsilon factor for egreedy policy, starting value.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--alpha'", ",", "type", "=", "float", ",", "default", "=", "0.6", ",", "help", "=", "\"alpha factor for prioritization contribution.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--beta_start'", ",", "type", "=", "float", ",", "default", "=", "0.4", ",", "help", "=", "\"starting beta factor for bias correction when using a priotizied buffer.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--beta_end'", ",", "type", "=", "float", ",", "default", "=", "1.", ",", "help", "=", "\"ending beta factor for bias correction when using a priotizied buffer.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--update_every'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "help", "=", "\"how often to update the network, in steps.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--exploring_steps'", ",", "type", "=", "int", ",", "default", "=", "25000", ",", "help", "=", "\"number of purely exploring steps at the beginning.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--target_update'", ",", "type", "=", "str", ",", "default", "=", "\"soft\"", ",", "help", "=", "\"hard or soft update for target network. If hard specify --delay_steps. If soft specify --tau.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--tau'", ",", "type", "=", "float", ",", "default", "=", "1e-2", ",", "help", "=", "\"weight for soft update of target parameters.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--delay_steps'", ",", "type", "=", "int", ",", "default", "=", "10000", ",", "help", "=", "\"delay with which a hard update of the target network is conducted.\"", ")", "\n", "\n", "if", "phase", "==", "\"train\"", ":", "\n", "        ", "parser", ".", "add_argument", "(", "'--train'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "\"training flag set to true.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--save_freq'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "help", "=", "\"save Qnetworks every n episodes. Also tests the agent greedily for logs.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--log_freq'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "help", "=", "\"frequency (in episodes) with wich we store logs to weights and biases.\"", ")", "\n", "", "elif", "phase", "==", "\"test\"", ":", "\n", "        ", "parser", ".", "add_argument", "(", "'--train'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "\"training flag set to False.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--option_file'", ",", "type", "=", "str", ",", "default", "=", "\"train_options.txt\"", ",", "help", "=", "\".txt file from which we load options.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--n_runs'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "help", "=", "\"number of test runs to do\"", ")", "\n", "parser", ".", "add_argument", "(", "'--n_steps'", ",", "type", "=", "int", ",", "default", "=", "250", ",", "help", "=", "\"number of steps to test the agent for.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fname'", ",", "type", "=", "str", ",", "default", "=", "\"sample\"", ",", "help", "=", "\"name of the file to save (gif).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--render'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"if rendering test trajectories (WARNING: takes a while).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--no_load'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "\"don't load any options when testing.\"", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'unknown parameter phase: {}. expected: (\"train\"/\"test\").'", ".", "format", "(", "phase", ")", ")", "\n", "\n", "", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.options.options.print_options": [[125, 153], ["sorted", "print", "vars().items", "parser.get_default", "os.path.join", "str", "str", "os.path.exists", "os.makedirs", "os.path.join", "os.path.join", "open", "opt_file.write", "opt_file.write", "vars", "str"], "function", ["None"], ["", "def", "print_options", "(", "opt", ",", "parser", ",", "save", "=", "True", ")", ":", "\n", "    ", "\"\"\"Print and save options\n    It will print both current options and default values(if different).\n    It will save options into a text file / [checkpoints_dir] / opt.txt\n    \"\"\"", "\n", "message", "=", "''", "\n", "message", "+=", "'----------------- Options ---------------\\n'", "\n", "for", "k", ",", "v", "in", "sorted", "(", "vars", "(", "opt", ")", ".", "items", "(", ")", ")", ":", "\n", "        ", "comment", "=", "''", "\n", "default", "=", "parser", ".", "get_default", "(", "k", ")", "\n", "if", "v", "!=", "default", ":", "\n", "            ", "comment", "=", "'\\t[default: %s]'", "%", "str", "(", "default", ")", "\n", "", "message", "+=", "'{:>25}: {:<30}{}\\n'", ".", "format", "(", "str", "(", "k", ")", ",", "str", "(", "v", ")", ",", "comment", ")", "\n", "", "message", "+=", "'----------------- End -------------------'", "\n", "print", "(", "message", ")", "\n", "\n", "if", "save", ":", "\n", "# save to the disk", "\n", "        ", "expr_dir", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "checkpoints_dir", ",", "opt", ".", "name", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "expr_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "expr_dir", ")", "\n", "", "if", "opt", ".", "train", ":", "\n", "            ", "file_name", "=", "os", ".", "path", ".", "join", "(", "expr_dir", ",", "'train_options.txt'", ")", "\n", "", "else", ":", "\n", "            ", "file_name", "=", "os", ".", "path", ".", "join", "(", "expr_dir", ",", "'test_options.txt'", ")", "\n", "", "with", "open", "(", "file_name", ",", "'wt'", ")", "as", "opt_file", ":", "\n", "            ", "opt_file", ".", "write", "(", "message", ")", "\n", "opt_file", ".", "write", "(", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.options.options.load_options": [[154, 201], ["print", "vars", "argparse.Namespace", "os.path.join", "open", "opt_file.readlines", "[].strip", "[].strip", "line.split", "[].strip", "[].strip.lower", "type", "line.split", "[].strip.split"], "function", ["None"], ["", "", "", "def", "load_options", "(", "opt", ",", "load_filename", "=", "None", ")", ":", "\n", "    ", "\"\"\" loads and overrides options in opt with an existing option .txt output\"\"\"", "\n", "if", "opt", ".", "load_name", "is", "None", ":", "\n", "        ", "load_name", "=", "opt", ".", "name", "\n", "", "else", ":", "\n", "        ", "load_name", "=", "opt", ".", "load_name", "\n", "", "if", "load_filename", "is", "None", ":", "\n", "        ", "load_filename", "=", "'train_options.txt'", "\n", "", "if", "\"/\"", "in", "load_filename", ":", "# a path was given", "\n", "        ", "load_path", "=", "load_filename", "\n", "", "else", ":", "# a filename was given", "\n", "        ", "load_path", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "checkpoints_dir", ",", "load_name", ",", "load_filename", ")", "\n", "\n", "", "print", "(", "\"loading options from: {} ...\"", ".", "format", "(", "load_path", ")", ")", "\n", "with", "open", "(", "load_path", ",", "'r'", ")", "as", "opt_file", ":", "\n", "        ", "lines", "=", "opt_file", ".", "readlines", "(", ")", "\n", "\n", "# read options from the txt file ", "\n", "", "opt", "=", "vars", "(", "opt", ")", "\n", "load_opt", "=", "{", "}", "\n", "for", "line", "in", "lines", ":", "\n", "        ", "if", "':'", "in", "line", ":", "\n", "            ", "key", "=", "line", ".", "split", "(", "':'", ")", "[", "0", "]", ".", "strip", "(", ")", "\n", "if", "key", "in", "opt", ":", "\n", "# specify which keys we do NOT want to overwrite", "\n", "                ", "if", "key", "not", "in", "[", "\"load\"", ",", "\"load_name\"", ",", "\"wandb\"", ",", "\"dataroot\"", ",", "\"load_size\"", ",", "\"volume_ids\"", ",", "\"n_runs\"", ",", "\"n_steps\"", ",", "\"fname\"", ",", "\"render\"", ",", "\"option_file\"", ",", "\"easy_objective\"", ",", "\"train\"", ",", "\"name\"", ",", "\"randomize_intensities\"", ",", "\"no_preprocess\"", "]", ":", "\n", "# get the str version of the value", "\n", "                    ", "value", "=", "line", ".", "split", "(", "':'", ")", "[", "1", "]", ".", "strip", "(", ")", "\n", "if", "\"default\"", "in", "value", ":", "\n", "                        ", "value", "=", "value", ".", "split", "(", "\"[default\"", ")", "[", "0", "]", ".", "strip", "(", ")", "\n", "# if instance of bool handle explicitely to cast str to bool", "\n", "", "if", "value", ".", "lower", "(", ")", "in", "[", "\"true\"", ",", "\"false\"", "]", ":", "\n", "                        ", "value", "=", "value", "==", "\"True\"", "\n", "# otherwise cast implicitely with type()()", "\n", "", "else", ":", "\n", "#print(opt[key], value)", "\n", "                        ", "value", "=", "type", "(", "opt", "[", "key", "]", ")", "(", "value", ")", "\n", "", "load_opt", "[", "key", "]", "=", "value", "\n", "\n", "# overwrite overlapping opt entries with entries from load_opt", "\n", "", "", "", "", "for", "key", "in", "load_opt", ":", "\n", "        ", "opt", "[", "key", "]", "=", "load_opt", "[", "key", "]", "\n", "\n", "# reconvert opt to be a Namespace", "\n", "", "opt", "=", "Namespace", "(", "**", "opt", ")", "\n", "\n", "return", "opt", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__init__": [[22, 25], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "log", "=", "None", ")", ":", "\n", "        ", "self", ".", "_start_time", "=", "None", "\n", "self", ".", "log", "=", "log", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.start": [[26, 28], ["time.perf_counter"], "methods", ["None"], ["", "def", "start", "(", "self", ")", ":", "\n", "        ", "self", ".", "_start_time", "=", "time", ".", "perf_counter", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.stop": [[29, 40], ["timer.Timer.timers[].append", "ValueError", "time.perf_counter"], "methods", ["None"], ["", "def", "stop", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_start_time", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "'stopped timer without starting it'", ")", "\n", "\n", "", "elapsed_time", "=", "time", ".", "perf_counter", "(", ")", "-", "self", ".", "_start_time", "\n", "self", ".", "_start_time", "=", "None", "\n", "\n", "if", "not", "self", ".", "log", "in", "self", ".", "timers", ":", "\n", "            ", "self", ".", "timers", "[", "self", ".", "log", "]", "=", "[", "]", "\n", "\n", "", "self", ".", "timers", "[", "self", ".", "log", "]", ".", "append", "(", "elapsed_time", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__enter__": [[41, 46], ["timer.Timer.start"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.start"], ["", "def", "__enter__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Start a new timer as a context manager\"\"\"", "\n", "assert", "self", ".", "log", ",", "\"must specify what to log if using Timer() as a context manager\"", "\n", "self", ".", "start", "(", ")", "\n", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.__exit__": [[47, 50], ["timer.Timer.stop"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.stop"], ["", "def", "__exit__", "(", "self", ",", "*", "exc_info", ")", ":", "\n", "        ", "\"\"\"Stop the context manager timer\"\"\"", "\n", "self", ".", "stop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.summary": [[51, 59], ["dict", "dict", "dict", "timer.Timer.timers.items", "numpy.mean", "numpy.std", "len"], "methods", ["None"], ["", "def", "summary", "(", "self", ")", ":", "\n", "        ", "self", ".", "means", "=", "dict", "(", ")", "\n", "self", ".", "stds", "=", "dict", "(", ")", "\n", "self", ".", "counts", "=", "dict", "(", ")", "\n", "for", "key", ",", "value", "in", "self", ".", "timers", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "means", "[", "key", "]", "=", "np", ".", "mean", "(", "value", ")", "\n", "self", ".", "stds", "[", "key", "]", "=", "np", ".", "std", "(", "value", ")", "\n", "self", ".", "counts", "[", "key", "]", "=", "len", "(", "value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.save": [[60, 65], ["timer.Timer.summary", "open", "fp.write"], "methods", ["home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.timer.Timer.summary"], ["", "", "def", "save", "(", "self", ",", "savedir", ")", ":", "\n", "        ", "self", ".", "summary", "(", ")", "\n", "with", "open", "(", "savedir", ",", "\"w\"", ")", "as", "fp", ":", "\n", "            ", "for", "key", "in", "self", ".", "timers", ":", "\n", "                ", "fp", ".", "write", "(", "\"{}: {:.5f} +/- {:.5f}. counts: {}\\n\"", ".", "format", "(", "key", ",", "self", ".", "means", "[", "key", "]", ",", "self", ".", "stds", "[", "key", "]", ",", "self", ".", "counts", "[", "key", "]", ")", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.cesaremagnetti_automaticusnavigation.timer.test.func1": [[3, 8], ["range"], "function", ["None"], ["\n"]]}