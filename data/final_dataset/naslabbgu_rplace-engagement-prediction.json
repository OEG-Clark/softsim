{"home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.data_loaders.general_loader.general_loader": [[37, 144], ["os.getcwd", "datetime.datetime.now", "sorted", "sorted", "enumerate", "enumerate", "os.listdir", "re.match", "os.listdir", "re.match", "cur_submission_file.endswith", "enumerate", "open", "submissions[].keys", "print", "cur_comments_file.endswith", "enumerate", "open", "comments[].keys", "print", "bz2.BZ2File", "lzma.open", "str", "submissions.append", "csv.DictWriter", "csv.DictWriter.writeheader", "csv.DictWriter.writerows", "bz2.BZ2File", "lzma.open", "pandas.to_datetime", "comments.append", "csv.DictWriter", "csv.DictWriter.writeheader", "csv.DictWriter.writerows", "json.loads", "pandas.to_datetime", "dict", "dict", "datetime.datetime.now", "len", "json.loads", "dict", "dict", "datetime.datetime.now", "len", "line.decode", "line.decode"], "function", ["None"], ["def", "general_loader", "(", "data_path", ",", "sr_to_include", "=", "None", ",", "saving_path", "=", "os", ".", "getcwd", "(", ")", ",", "load_only_columns_subset", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    loading reddit data, based on the zipped files from here - https://files.pushshift.io/reddit/\n    the process converts this files into csv, after filtering some columns (if load_only_columns_subset=True) and\n    adding a date columns\n\n    :param data_path: str\n        location of the data\n    :param sr_to_include: list or None, default: NOne\n        placeholder for future use - to be used when only subset of SRs are needed\n    :param saving_path: str, default: existing location of the python code\n        location to save the files into\n    :param load_only_columns_subset: bool, default: False\n    :return: None\n        only prints to screen and saving csv files into the saving_path location\n\n    Example\n    -------\n    >>> general_loader(data_path, saving_path=data_path + 'place_classifier_csvs', load_only_columns_subset=True)\n    \"\"\"", "\n", "\n", "start_time", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "# finding all the relevant zip files in the 'data_path' directory", "\n", "submission_files_path", "=", "data_path", "+", "'submissions/'", "if", "sys", ".", "platform", "==", "'linux'", "else", "data_path", "+", "'submissions\\\\'", "\n", "comments_files_path", "=", "data_path", "+", "'comments/'", "if", "sys", ".", "platform", "==", "'linux'", "else", "data_path", "+", "'comments\\\\'", "\n", "submission_files", "=", "[", "f", "for", "f", "in", "os", ".", "listdir", "(", "submission_files_path", ")", "if", "re", ".", "match", "(", "r'RS.*\\.bz2|RS.*\\.xz'", ",", "f", ")", "]", "\n", "comments_files", "=", "[", "f", "for", "f", "in", "os", ".", "listdir", "(", "comments_files_path", ")", "if", "re", ".", "match", "(", "r'RC.*\\.bz2|RC.*\\.xz'", ",", "f", ")", "]", "\n", "# taking only files which are in the 'included_years' subset", "\n", "submission_files", "=", "[", "i", "for", "i", "in", "submission_files", "if", "'RS_2017-04.bz2'", "<=", "i", "<=", "'RS_2017-04.bz2'", "]", "\n", "comments_files", "=", "[", "i", "for", "i", "in", "comments_files", "if", "'RC_2017-04.bz2'", "<=", "i", "<=", "'RC_2017-04.bz2'", "]", "\n", "submission_files", "=", "sorted", "(", "submission_files", ")", "\n", "comments_files", "=", "sorted", "(", "comments_files", ")", "\n", "submissions_interesting_col", "=", "[", "\"created_utc_as_date\"", ",", "\"author\"", ",", "\"subreddit\"", ",", "\"title\"", ",", "\"selftext\"", ",", "\"num_comments\"", ",", "\n", "\"permalink\"", ",", "\"score\"", ",", "\"id\"", ",", "\"thumbnail\"", "]", "\n", "comments_interesting_col", "=", "[", "\"created_utc_as_date\"", ",", "\"author\"", ",", "\"subreddit\"", ",", "\"body\"", ",", "\"score\"", ",", "\"id\"", ",", "\n", "\"link_id\"", ",", "\"parent_id\"", ",", "\"thumbnail\"", "]", "\n", "# looping over each file of the submission/comment and handling it", "\n", "if", "data_to_process", "==", "'submission'", "or", "data_to_process", "==", "'both'", ":", "\n", "# for cur_submission_file in submission_files[0]:", "\n", "        ", "for", "subm_idx", ",", "cur_submission_file", "in", "enumerate", "(", "submission_files", ")", ":", "\n", "            ", "if", "cur_submission_file", ".", "endswith", "(", "'bz2'", ")", ":", "\n", "                ", "zipped_submission", "=", "bz2", ".", "BZ2File", "(", "submission_files_path", "+", "cur_submission_file", ",", "'r'", ")", "\n", "", "else", ":", "\n", "                ", "zipped_submission", "=", "lzma", ".", "open", "(", "submission_files_path", "+", "cur_submission_file", ",", "mode", "=", "'r'", ")", "\n", "# looping over each row in the submission data", "\n", "", "submissions", "=", "[", "]", "\n", "for", "inner_idx", ",", "line", "in", "enumerate", "(", "zipped_submission", ")", ":", "\n", "                ", "try", ":", "\n", "                    ", "cur_line", "=", "json", ".", "loads", "(", "line", ".", "decode", "(", "'UTF-8'", ")", ")", "\n", "", "except", "json", ".", "decoder", ".", "JSONDecodeError", ":", "\n", "                    ", "continue", "\n", "", "cur_line", "[", "'created_utc_as_date'", "]", "=", "str", "(", "pd", ".", "to_datetime", "(", "cur_line", "[", "'created_utc'", "]", ",", "unit", "=", "'s'", ")", ")", "\n", "if", "load_only_columns_subset", ":", "\n", "                    ", "line_shrinked", "=", "dict", "(", "(", "k", ",", "cur_line", "[", "k", "]", ")", "if", "k", "in", "cur_line", "else", "(", "k", ",", "None", ")", "for", "k", "in", "submissions_interesting_col", ")", "\n", "# we still define this 'line_shrinked' also in cases when we want to have all columns, since in some", "\n", "# cases there are redundant columns appear in the zip original files", "\n", "", "else", ":", "\n", "                    ", "line_shrinked", "=", "dict", "(", "(", "k", ",", "cur_line", "[", "k", "]", ")", "if", "k", "in", "cur_line", "else", "(", "k", ",", "None", ")", "for", "k", "in", "submission_columns", ")", "\n", "", "submissions", ".", "append", "(", "line_shrinked", ")", "\n", "#if inner_idx > 100000:", "\n", "#    break", "\n", "# saving the file to disk. Currently it is as a csv format (found it as the most useful one)", "\n", "", "full_file_name", "=", "saving_path", "+", "'/'", "if", "sys", ".", "platform", "==", "'linux'", "else", "saving_path", "+", "'\\\\'", "\n", "f", "=", "open", "(", "full_file_name", "+", "cur_submission_file", "[", ":", "-", "4", "]", "+", "'.csv'", ",", "mode", "=", "'a'", ",", "encoding", "=", "\"utf-8\"", ")", "\n", "keys", "=", "submissions", "[", "0", "]", ".", "keys", "(", ")", "\n", "with", "f", "as", "output_file", ":", "\n", "                ", "dict_writer", "=", "csv", ".", "DictWriter", "(", "output_file", ",", "keys", ")", "\n", "dict_writer", ".", "writeheader", "(", ")", "\n", "dict_writer", ".", "writerows", "(", "submissions", ")", "\n", "", "duration", "=", "(", "datetime", ".", "datetime", ".", "now", "(", ")", "-", "start_time", ")", ".", "seconds", "\n", "print", "(", "\"Finished handling the {}'th submission file called {}. Took us up to now: {} seconds. \"", "\n", "\"Current submission size is {}\"", ".", "format", "(", "subm_idx", "+", "1", ",", "cur_submission_file", ",", "duration", ",", "len", "(", "submissions", ")", ")", ")", "\n", "# handling the comments data - exactly same logic as the submission data handling", "\n", "", "", "if", "data_to_process", "==", "'comments'", "or", "data_to_process", "==", "'both'", ":", "\n", "        ", "for", "comm_idx", ",", "cur_comments_file", "in", "enumerate", "(", "comments_files", ")", ":", "\n", "            ", "if", "cur_comments_file", ".", "endswith", "(", "'bz2'", ")", ":", "\n", "                ", "zipped_comments", "=", "bz2", ".", "BZ2File", "(", "comments_files_path", "+", "cur_comments_file", ",", "'r'", ")", "\n", "", "else", ":", "\n", "                ", "zipped_comments", "=", "lzma", ".", "open", "(", "comments_files_path", "+", "cur_comments_file", ",", "mode", "=", "'r'", ")", "\n", "# looping over each row in the comments data", "\n", "", "comments", "=", "[", "]", "\n", "for", "inner_idx", ",", "line", "in", "enumerate", "(", "zipped_comments", ")", ":", "\n", "                ", "try", ":", "\n", "                    ", "cur_line", "=", "json", ".", "loads", "(", "line", ".", "decode", "(", "'UTF-8'", ")", ")", "\n", "", "except", "json", ".", "decoder", ".", "JSONDecodeError", ":", "\n", "                    ", "continue", "\n", "", "cur_line", "[", "'created_utc_as_date'", "]", "=", "pd", ".", "to_datetime", "(", "cur_line", "[", "'created_utc'", "]", ",", "unit", "=", "'s'", ")", "\n", "if", "load_only_columns_subset", ":", "\n", "                    ", "line_shrinked", "=", "dict", "(", "(", "k", ",", "cur_line", "[", "k", "]", ")", "for", "k", "in", "comments_interesting_col", "if", "k", "in", "cur_line", ")", "\n", "# we still define this 'line_shrinked' also in cases when we want to have all columns, since in some", "\n", "# cases there are redundant columns appear in the zip original files", "\n", "", "else", ":", "\n", "                    ", "line_shrinked", "=", "dict", "(", "(", "k", ",", "cur_line", "[", "k", "]", ")", "if", "k", "in", "cur_line", "else", "(", "k", ",", "None", ")", "for", "k", "in", "comments_columns", ")", "\n", "", "comments", ".", "append", "(", "line_shrinked", ")", "\n", "#if inner_idx > 100000:", "\n", "#    break", "\n", "# saving the file to disk", "\n", "", "full_file_name", "=", "saving_path", "+", "'/'", "if", "sys", ".", "platform", "==", "'linux'", "else", "saving_path", "+", "'\\\\'", "\n", "f", "=", "open", "(", "full_file_name", "+", "cur_comments_file", "[", ":", "-", "4", "]", "+", "'.csv'", ",", "mode", "=", "'a'", ",", "encoding", "=", "\"utf-8\"", ")", "\n", "keys", "=", "comments", "[", "0", "]", ".", "keys", "(", ")", "\n", "with", "f", "as", "output_file", ":", "\n", "                ", "dict_writer", "=", "csv", ".", "DictWriter", "(", "output_file", ",", "keys", ")", "\n", "dict_writer", ".", "writeheader", "(", ")", "\n", "dict_writer", ".", "writerows", "(", "comments", ")", "\n", "", "duration", "=", "(", "datetime", ".", "datetime", ".", "now", "(", ")", "-", "start_time", ")", ".", "seconds", "\n", "print", "(", "\"Finished handling the {}'th comments file. Took us up to now: {} seconds. \"", "\n", "\"Current comments size is {}\"", ".", "format", "(", "comm_idx", "+", "1", ",", "duration", ",", "len", "(", "comments", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.data_loaders.general_loader.sr_sample_based_subscribers": [[146, 274], ["pandas.to_numeric", "pd.read_csv.assign", "pd.read_csv.sort_values", "pandas.read_excel", "collections.defaultdict", "pandas.DataFrame.from_dict", "pd.read_csv.reset_index", "pandas.read_csv", "pickle.load", "set", "set", "set", "set", "set", "type", "sample_size.split", "int", "general_loader._sample_srs_based_size", "numpy.random.seed", "set", "open", "enumerate", "pandas.to_datetime", "open", "drawing_srs_df.set_index().to_dict", "not_drawing_srs_df.set_index().to_dict", "int", "int", "numpy.random.choice", "str().lower", "chosen_srs_full_info.itertuples", "str().lower", "chosen_srs_full_info2.itertuples", "json.loads", "str().lower", "str().lower", "str().lower", "str().lower", "str().lower", "str().lower", "len", "type", "int", "print", "drawing_srs_dict.items", "not_drawing_srs_dict.items", "enumerate", "sr_basic[].str.lower().isin", "place_related_srs[].isin", "pickle.load.keys", "type", "sr_basic[].str.lower().isin", "sr_basic[].str.lower().isin", "drawing_srs_df.set_index", "not_drawing_srs_df.set_index", "range", "str", "str", "str", "max", "sr_basic[].isnull", "sr_basic[].isna", "str", "str", "str().lower", "str", "str().lower", "str", "str", "float", "len", "sr_basic[].str.lower", "float", "sr_basic[].str.lower", "sr_basic[].str.lower", "len", "str", "str"], "function", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.data_loaders.general_loader._sample_srs_based_size"], ["", "", "", "def", "sr_sample_based_subscribers", "(", "data_path", ",", "sample_size", ",", "threshold_to_define_as_drawing", ",", "internal_sr_metadata", "=", "True", ",", "\n", "sr_statistics_usage", "=", "True", ",", "balanced_sampling_based_sr_size", "=", "False", ",", "seed", "=", "1984", ")", ":", "\n", "    ", "\"\"\"\n    sampling group of SRs to be later used for modeling. This group is in most cases the 'not-drawing' teams. We will\n    use meta-data for sampling purposes, either internal meta-data or external one\n    :param data_path: str\n        location of all data needed for the algorithm\n    :param sample_size: int or string of float\n        how many SRs should be sampled. Can be represents as simple in (then, it should be in\n        most cases >= len(not_drawing_SRs)). Can be float, and then it represents the % of SRs to choose. Can be string\n        in the form of 'a:b' and then it represnts ratio between drawing and not-drawing team (e.g '2:1' means we will\n        have not-drawing teams X2 compared to the drawing teams. '1:1' means pure balance situation)\n    :param threshold_to_define_as_drawing: float\n        a value to be used to define a SR as drawing, according to the value in the excel file. The value in the excel\n        file represents our probability that a certain SR was drawing\n    :param sr_statistics_usage: bool, default: True\n        whether to use statisics we computer regarding SRs and thier uasge in reddit in the past. It allows us not to\n        sample SRs which didn't submit any submission along the last 6 months\n    :param internal_sr_metadata: bool, default: True\n        whether to use internal or extrnal meta-data as supportive source of information.\n        Internal is the summary of SRs mata-data we created, which is based on the submissions data and reddit API (can\n        be seen in the crawl_srs_metadata.py file). External is the data coming from here -\n        https://files.pushshift.io/reddit/subreddits/ (which can be not updated and missing some SRs we need)\n    :param balanced_sampling_based_sr_size: bool, default: False\n         whether or not to apply an algorithm which will do \"smart\" sampling and not random. It will balance the\n         SRs between drawing/not-drawing in terms of SR size (so distribution of the two will be almost similar)\n    :param sr_metadata_usage: bool, default: True\n        whether or not to use meta-data for sampling purposes. This can indeed lead to better sampling (e.g., SRs\n        without any submission data will not be chosen)\n    :param seed: int, default: 1984\n        the seed to be used for randomization\n    :return: list of tuples\n        list where each item is a SRs and contains some information about it (represented in a tuple):\n        [0] contains its name, [1] contains num_of_users, [2] contains 'creation_utc', [3] contains string ('drawing'\n        or 'not_drawing')\n    \"\"\"", "\n", "# in case we wish to use the data we crawled from reddit with explicit information about SRs writing along 2016-2017", "\n", "if", "internal_sr_metadata", ":", "\n", "        ", "srs_meta_data", "=", "defaultdict", "(", "list", ")", "\n", "with", "open", "(", "data_path", "+", "'srs_meta_data_102016_to_032017.json'", ")", "as", "f", ":", "\n", "            ", "for", "idx", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "                ", "cur_line", "=", "json", ".", "loads", "(", "line", ")", "\n", "cur_sr_name", "=", "str", "(", "cur_line", "[", "'display_name'", "]", ")", ".", "lower", "(", ")", "\n", "# case we already 'met' this sr, we'll take the max out of all subscribers amount we see", "\n", "if", "cur_sr_name", "in", "srs_meta_data", ":", "\n", "                    ", "srs_meta_data", "[", "cur_sr_name", "]", "=", "[", "max", "(", "cur_line", "[", "'subscribers'", "]", ",", "srs_meta_data", "[", "cur_sr_name", "]", "[", "0", "]", ")", ",", "\n", "cur_line", "[", "'created'", "]", "]", "\n", "\n", "", "elif", "cur_line", "[", "'subscribers'", "]", "is", "not", "None", ":", "\n", "                    ", "srs_meta_data", "[", "cur_sr_name", "]", "=", "[", "cur_line", "[", "'subscribers'", "]", ",", "cur_line", "[", "'created'", "]", "]", "\n", "", "", "", "sr_basic", "=", "pd", ".", "DataFrame", ".", "from_dict", "(", "data", "=", "srs_meta_data", ",", "orient", "=", "'index'", ")", "\n", "sr_basic", ".", "reset_index", "(", "inplace", "=", "True", ")", "\n", "sr_basic", ".", "columns", "=", "[", "'subreddit_name'", ",", "'number_of_subscribers'", ",", "'creation_epoch'", "]", "\n", "\n", "# if we don't want to use the metadata information we crawled, we will use other source of information related to", "\n", "# the meta-data of subreddits (coming from https://files.pushshift.io/reddit/subreddits/)", "\n", "", "else", ":", "\n", "# this is the big file, including ALL the SRs exists", "\n", "        ", "sr_basic", "=", "pd", ".", "read_csv", "(", "filepath_or_buffer", "=", "data_path", "+", "'subreddits/subreddits_basic.csv'", "if", "sys", ".", "platform", "==", "'linux'", "\n", "else", "data_path", "+", "'subreddits_basic.csv'", ",", "\n", "header", "=", "0", ",", "index_col", "=", "False", ")", "\n", "# removing from sr_basic rows we cannot handle (None, nulls, 'None') and", "\n", "# converting an important columns to numeric", "\n", "sr_basic", "=", "sr_basic", "[", "~", "(", "(", "sr_basic", "[", "'number_of_subscribers'", "]", ".", "isnull", "(", ")", ")", "|", "(", "sr_basic", "[", "'number_of_subscribers'", "]", ".", "isna", "(", ")", ")", ")", "]", "\n", "sr_basic", "=", "sr_basic", ".", "loc", "[", "sr_basic", "[", "'number_of_subscribers'", "]", "!=", "'None'", "]", "\n", "", "sr_basic", "[", "'number_of_subscribers'", "]", "=", "pd", ".", "to_numeric", "(", "sr_basic", "[", "'number_of_subscribers'", "]", ")", "\n", "\n", "# adding explicit timestamp to the data and filtering SRs which were created before r/place started", "\n", "sr_basic", "=", "sr_basic", ".", "assign", "(", "created_utc_as_date", "=", "pd", ".", "to_datetime", "(", "sr_basic", "[", "'creation_epoch'", "]", ",", "unit", "=", "'s'", ")", ")", "\n", "sr_basic", "=", "sr_basic", "[", "sr_basic", "[", "'created_utc_as_date'", "]", "<", "'2017-03-31 00:00:00'", "]", "\n", "sr_basic", ".", "sort_values", "(", "by", "=", "'created_utc_as_date'", ",", "inplace", "=", "True", ")", "\n", "# this is the excel file we created, including only SRs related in some way to the r/place experiment", "\n", "place_related_srs", "=", "pd", ".", "read_excel", "(", "io", "=", "data_path", "+", "'subreddits_revealed/'", "\n", "'all_subredditts_based_atlas_and_submissions.xlsx'", "\n", "if", "sys", ".", "platform", "==", "'linux'", "else", "data_path", "+", "'sr_relations\\\\all_subredditts_based'", "\n", "'_atlas_and_submissions.xlsx'", ",", "\n", "sheet_name", "=", "'Detailed list'", ")", "\n", "drawing_srs", "=", "place_related_srs", "[", "(", "place_related_srs", "[", "'trying_to_draw'", "]", "==", "'Yes'", ")", "|", "\n", "(", "place_related_srs", "[", "'models_prediction'", "]", ">", "threshold_to_define_as_drawing", ")", "]", "[", "'SR'", "]", "\n", "# flag which determines whether we filter SRs based on the fact no submission was done by them in the last 6 months", "\n", "if", "sr_statistics_usage", ":", "\n", "        ", "submission_stats", "=", "pickle", ".", "load", "(", "open", "(", "data_path", "+", "\n", "\"place_classifier_csvs/submission_stats_102016_to_032017.p\"", ",", "\"rb\"", ")", ")", "\n", "active_srs", "=", "set", "(", "str", "(", "key", ")", ".", "lower", "(", ")", "for", "key", "in", "submission_stats", ".", "keys", "(", ")", ")", "\n", "drawing_srs", "=", "set", "(", "[", "str", "(", "name", ")", ".", "lower", "(", ")", "for", "name", "in", "drawing_srs", "if", "str", "(", "name", ")", ".", "lower", "(", ")", "in", "active_srs", "]", ")", "\n", "sr_basic_names", "=", "set", "(", "[", "str", "(", "name", ")", ".", "lower", "(", ")", "for", "name", "in", "sr_basic", "[", "'subreddit_name'", "]", "if", "str", "(", "name", ")", ".", "lower", "(", ")", "in", "active_srs", "]", ")", "\n", "", "else", ":", "\n", "        ", "drawing_srs", "=", "set", "(", "[", "str", "(", "name", ")", ".", "lower", "(", ")", "for", "name", "in", "drawing_srs", "]", ")", "\n", "sr_basic_names", "=", "set", "(", "[", "str", "(", "name", ")", ".", "lower", "(", ")", "for", "name", "in", "sr_basic", "[", "'subreddit_name'", "]", "]", ")", "\n", "", "not_drawing_srs", "=", "{", "name", "for", "name", "in", "sr_basic_names", "if", "name", "not", "in", "drawing_srs", "}", "\n", "# handling the sample size parameter", "\n", "if", "type", "(", "sample_size", ")", "is", "str", ":", "\n", "        ", "ratio", "=", "sample_size", ".", "split", "(", "':'", ")", "\n", "sample_amount", "=", "int", "(", "float", "(", "ratio", "[", "0", "]", ")", "*", "1.0", "/", "float", "(", "ratio", "[", "1", "]", ")", "*", "1.0", "*", "len", "(", "drawing_srs", ")", ")", "\n", "", "elif", "type", "(", "sample_size", ")", "is", "int", "and", "sample_size", ">", "1", ":", "\n", "        ", "sample_amount", "=", "sample_size", "\n", "", "elif", "type", "(", "sample_size", ")", "is", "float", "and", "0", "<", "sample_size", "<", "1", ":", "\n", "        ", "sample_amount", "=", "int", "(", "sample_size", "*", "len", "(", "not_drawing_srs", ")", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Current parameter type is not supported yet\"", ")", "\n", "return", "1", "\n", "\n", "# now sampling the 'not_drawing_srs' population", "\n", "", "if", "balanced_sampling_based_sr_size", ":", "\n", "        ", "drawing_srs_df", "=", "sr_basic", "[", "sr_basic", "[", "'subreddit_name'", "]", ".", "str", ".", "lower", "(", ")", ".", "isin", "(", "drawing_srs", ")", "]", "[", "\n", "[", "'subreddit_name'", ",", "'number_of_subscribers'", "]", "]", "\n", "not_drawing_srs_df", "=", "sr_basic", "[", "sr_basic", "[", "'subreddit_name'", "]", ".", "str", ".", "lower", "(", ")", ".", "isin", "(", "not_drawing_srs", ")", "]", "[", "\n", "[", "'subreddit_name'", ",", "'number_of_subscribers'", "]", "]", "\n", "drawing_srs_dict", "=", "drawing_srs_df", ".", "set_index", "(", "'subreddit_name'", ")", ".", "to_dict", "(", ")", "[", "'number_of_subscribers'", "]", "\n", "not_drawing_srs_dict", "=", "not_drawing_srs_df", ".", "set_index", "(", "'subreddit_name'", ")", ".", "to_dict", "(", ")", "[", "'number_of_subscribers'", "]", "\n", "drawing_srs_dict", "=", "{", "k", ":", "int", "(", "v", ")", "for", "k", ",", "v", "in", "drawing_srs_dict", ".", "items", "(", ")", "}", "\n", "not_drawing_srs_dict", "=", "{", "k", ":", "int", "(", "v", ")", "for", "k", ",", "v", "in", "not_drawing_srs_dict", ".", "items", "(", ")", "}", "\n", "chosen_srs", ",", "diffs", "=", "_sample_srs_based_size", "(", "drawing_srs_data", "=", "drawing_srs_dict", ",", "\n", "not_drawing_srs_data", "=", "not_drawing_srs_dict", ",", "size", "=", "sample_amount", ")", "\n", "", "else", ":", "\n", "        ", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "sampling_idx", "=", "set", "(", "np", ".", "random", ".", "choice", "(", "a", "=", "range", "(", "0", ",", "len", "(", "not_drawing_srs", ")", "-", "1", ")", ",", "size", "=", "sample_amount", ",", "replace", "=", "False", ")", ")", "\n", "chosen_srs", "=", "[", "name", "for", "idx", ",", "name", "in", "enumerate", "(", "not_drawing_srs", ")", "if", "idx", "in", "sampling_idx", "]", "\n", "", "chosen_srs_full_info", "=", "sr_basic", "[", "sr_basic", "[", "'subreddit_name'", "]", ".", "str", ".", "lower", "(", ")", ".", "isin", "(", "chosen_srs", ")", "]", "[", "[", "'subreddit_name'", ",", "\n", "'number_of_subscribers'", ",", "\n", "'created_utc_as_date'", "]", "]", "\n", "# returning results in a list format - each item is a tuple of 3. First is the name in lower-case letters,", "\n", "# second is the # of users in this SR and third is the creation date", "\n", "results", "=", "[", "(", "str", "(", "x", "[", "1", "]", ")", ".", "lower", "(", ")", ",", "x", "[", "2", "]", ",", "x", "[", "3", "]", ",", "'not_drawing'", ")", "for", "x", "in", "chosen_srs_full_info", ".", "itertuples", "(", ")", "]", "\n", "# adding the drawing teams to the party and sending results", "\n", "chosen_srs_full_info2", "=", "place_related_srs", "[", "place_related_srs", "[", "'SR'", "]", ".", "isin", "(", "drawing_srs", ")", "]", "[", "[", "'SR'", ",", "'num_of_users'", ",", "'creation_utc'", "]", "]", "\n", "results2", "=", "[", "(", "str", "(", "x", "[", "1", "]", ")", ".", "lower", "(", ")", ",", "x", "[", "2", "]", ",", "x", "[", "3", "]", ",", "'drawing'", ")", "for", "x", "in", "chosen_srs_full_info2", ".", "itertuples", "(", ")", "]", "\n", "return", "results2", "+", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.data_loaders.general_loader.sr_sample_based_submissions": [[276, 389], ["datetime.datetime.now", "os.path.join", "sorted", "collections.Counter", "enumerate", "collections.OrderedDict", "pandas.read_excel", "set", "set", "general_loader._sample_srs_based_size", "collections.defaultdict", "pandas.DataFrame.from_dict", "sr_basic.assign.reset_index", "pandas.to_numeric", "sr_basic.assign.assign", "print", "pandas.read_csv", "cur_submission_df[].str.lower", "collections.Counter", "sorted", "type", "sample_size.split", "int", "str().lower", "int", "str().lower", "int", "open", "enumerate", "os.listdir", "re.match", "collections.Counter.items", "os.path.join", "str().lower", "str().lower", "collections.OrderedDict.items", "collections.OrderedDict.items", "os.path.join", "json.loads", "str().lower", "pandas.to_datetime", "str().lower", "chosen_srs_full_info.itertuples", "str().lower", "chosen_srs_full_info2.itertuples", "datetime.datetime.now", "len", "len", "os.path.join", "collections.OrderedDict.keys", "len", "type", "int", "print", "str", "str", "sr_basic[].str.lower().isin", "place_related_srs[].isin", "str", "str().lower", "collections.OrderedDict.keys", "str", "type", "str", "max", "str", "str", "float", "len", "sr_basic[].str.lower", "str", "float"], "function", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.data_loaders.general_loader._sample_srs_based_size"], ["", "def", "sr_sample_based_submissions", "(", "data_path", ",", "sample_size", ",", "threshold_to_define_as_drawing", ",", "\n", "start_period", "=", "'2017-01'", ",", "end_period", "=", "'2017-03'", ",", "seed", "=", "1984", ")", ":", "\n", "    ", "\"\"\"\n    sampling group of SRs to be later used for modeling. This group is in most cases the 'not-drawing' teams. We will\n    use meta-data for sampling purposes, either internal meta-data or external one\n\n    :param data_path: str\n        location of all data needed for the algorithm\n    :param sample_size: int or string of float\n        how many SRs should be sampled. Can be represents as simple in (then, it should be in\n        most cases >= len(not_drawing_SRs)). Can be float, and then it represents the % of SRs to choose. Can be string\n        in the form of 'a:b' and then it represnts ratio between drawing and not-drawing team (e.g '2:1' means we will\n        have not-drawing teams X2 compared to the drawing teams. '1:1' means pure balance situation)\n    :param threshold_to_define_as_drawing: float\n        a value to be used to define a SR as drawing, according to the value in the excel file. The value in the excel\n        file represents our probability that a certain SR was drawing\n    :param start_period: str\n        the starting month of the data (format: YYYY-MM)\n    :param end_period: str\n        the ending month of the data (format: YYYY-MM)\n    :param seed: int\n        random seed to be used for the sampling (currently not being used)\n    :return: list of tuples\n        list where each item is a SRs and contains some information about it (represented in a tuple):\n        [0] contains its name, [1] contains num_of_users, [2] contains 'creation_utc', [3] contains string ('drawing'\n        or 'not_drawing')\n    \"\"\"", "\n", "start_time", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "\n", "# location of the csv", "\n", "csv_path", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "'place_classifier_csvs'", ")", "\n", "# finding all the relevant zip files in the 'data_path' directory", "\n", "submission_files", "=", "[", "f", "for", "f", "in", "os", ".", "listdir", "(", "csv_path", ")", "if", "re", ".", "match", "(", "r'RS.*\\.csv'", ",", "f", ")", "]", "\n", "# taking only the submissions files from 10-2016 to 03-2017 by default", "\n", "submission_files", "=", "[", "i", "for", "i", "in", "submission_files", "if", "'RS_'", "+", "start_period", "+", "'.csv'", "<=", "i", "<=", "'RS_'", "+", "end_period", "+", "'.csv'", "]", "\n", "submission_files", "=", "sorted", "(", "submission_files", ")", "\n", "tot_srs_counter", "=", "collections", ".", "Counter", "(", ")", "\n", "# iterating over each submission file", "\n", "for", "subm_idx", ",", "cur_submission_file", "in", "enumerate", "(", "submission_files", ")", ":", "\n", "        ", "cur_submission_df", "=", "pd", ".", "read_csv", "(", "filepath_or_buffer", "=", "os", ".", "path", ".", "join", "(", "csv_path", ",", "cur_submission_file", ")", ",", "encoding", "=", "'utf-8'", ")", "\n", "# making sure the the date is not after r/place started", "\n", "cur_submission_df", "=", "cur_submission_df", "[", "cur_submission_df", "[", "'created_utc_as_date'", "]", "<", "'2017-03-29 00:00:00'", "]", "\n", "cur_sr_submissions", "=", "cur_submission_df", "[", "\"subreddit\"", "]", ".", "str", ".", "lower", "(", ")", "\n", "cur_srs_counter", "=", "collections", ".", "Counter", "(", "cur_sr_submissions", ")", "\n", "tot_srs_counter", "=", "tot_srs_counter", "+", "cur_srs_counter", "\n", "", "srs_counter_ordered", "=", "collections", ".", "OrderedDict", "(", "sorted", "(", "tot_srs_counter", ".", "items", "(", ")", ",", "key", "=", "lambda", "t", ":", "t", "[", "1", "]", ",", "reverse", "=", "True", ")", ")", "\n", "# in order to see the first (and highest SR) - list(srs_counter_ordered.items())[0]", "\n", "# pulling out drawing srs information", "\n", "place_related_srs", "=", "pd", ".", "read_excel", "(", "io", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "'subreddits_revealed'", ",", "\n", "'all_subredditts_based_atlas_and_submissions.xlsx'", ")", ",", "\n", "sheet_name", "=", "'Detailed list'", ")", "\n", "drawing_srs", "=", "place_related_srs", "[", "(", "place_related_srs", "[", "'trying_to_draw'", "]", "==", "'Yes'", ")", "|", "\n", "(", "place_related_srs", "[", "'models_prediction'", "]", ">", "threshold_to_define_as_drawing", ")", "]", "[", "'SR'", "]", "\n", "# creating sets for the drawing/not-drawing teams", "\n", "drawing_srs", "=", "set", "(", "[", "str", "(", "name", ")", ".", "lower", "(", ")", "for", "name", "in", "drawing_srs", "\n", "if", "str", "(", "name", ")", ".", "lower", "(", ")", "in", "srs_counter_ordered", ".", "keys", "(", ")", "]", ")", "\n", "sr_basic_names", "=", "set", "(", "[", "str", "(", "name", ")", ".", "lower", "(", ")", "for", "name", "in", "srs_counter_ordered", ".", "keys", "(", ")", "]", ")", "\n", "not_drawing_srs", "=", "{", "name", "for", "name", "in", "sr_basic_names", "if", "name", "not", "in", "drawing_srs", "}", "\n", "# handling the sample size parameter", "\n", "if", "type", "(", "sample_size", ")", "is", "str", ":", "\n", "        ", "ratio", "=", "sample_size", ".", "split", "(", "':'", ")", "\n", "sample_amount", "=", "int", "(", "float", "(", "ratio", "[", "0", "]", ")", "*", "1.0", "/", "float", "(", "ratio", "[", "1", "]", ")", "*", "1.0", "*", "len", "(", "drawing_srs", ")", ")", "\n", "", "elif", "type", "(", "sample_size", ")", "is", "int", "and", "sample_size", ">", "1", ":", "\n", "        ", "sample_amount", "=", "sample_size", "\n", "", "elif", "type", "(", "sample_size", ")", "is", "float", "and", "0", "<", "sample_size", "<", "1", ":", "\n", "        ", "sample_amount", "=", "int", "(", "sample_size", "*", "len", "(", "not_drawing_srs", ")", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Current parameter type is not supported yet\"", ")", "\n", "return", "1", "\n", "# now sampling the 'not_drawing_srs' population", "\n", "\n", "", "drawing_srs_dict", "=", "{", "str", "(", "k", ")", ".", "lower", "(", ")", ":", "int", "(", "v", ")", "for", "k", ",", "v", "in", "srs_counter_ordered", ".", "items", "(", ")", "if", "k", "in", "drawing_srs", "}", "\n", "not_drawing_srs_dict", "=", "{", "str", "(", "k", ")", ".", "lower", "(", ")", ":", "int", "(", "v", ")", "for", "k", ",", "v", "in", "srs_counter_ordered", ".", "items", "(", ")", "if", "k", "in", "not_drawing_srs", "}", "\n", "chosen_srs", ",", "diffs", "=", "_sample_srs_based_size", "(", "drawing_srs_data", "=", "drawing_srs_dict", ",", "\n", "not_drawing_srs_data", "=", "not_drawing_srs_dict", ",", "size", "=", "sample_amount", ")", "\n", "\n", "# loading file with meta data about SRs, in order to return additional information about the returned SRs", "\n", "# this will include the SR size (users) and date of creation", "\n", "srs_meta_data", "=", "defaultdict", "(", "list", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'srs_meta_data_102016_to_032017.json'", ")", ")", "as", "f", ":", "\n", "        ", "for", "idx", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "            ", "cur_line", "=", "json", ".", "loads", "(", "line", ")", "\n", "cur_sr_name", "=", "str", "(", "cur_line", "[", "'display_name'", "]", ")", ".", "lower", "(", ")", "\n", "# case we already 'met' this sr, we'll take the max out of all subscribers amount we see", "\n", "if", "cur_sr_name", "in", "srs_meta_data", ":", "\n", "                ", "srs_meta_data", "[", "cur_sr_name", "]", "=", "[", "max", "(", "cur_line", "[", "'subscribers'", "]", ",", "srs_meta_data", "[", "cur_sr_name", "]", "[", "0", "]", ")", ",", "\n", "cur_line", "[", "'created'", "]", "]", "\n", "\n", "", "elif", "cur_line", "[", "'subscribers'", "]", "is", "not", "None", ":", "\n", "                ", "srs_meta_data", "[", "cur_sr_name", "]", "=", "[", "cur_line", "[", "'subscribers'", "]", ",", "cur_line", "[", "'created'", "]", "]", "\n", "", "", "", "sr_basic", "=", "pd", ".", "DataFrame", ".", "from_dict", "(", "data", "=", "srs_meta_data", ",", "orient", "=", "'index'", ")", "\n", "sr_basic", ".", "reset_index", "(", "inplace", "=", "True", ")", "\n", "sr_basic", ".", "columns", "=", "[", "'subreddit_name'", ",", "'number_of_subscribers'", ",", "'creation_epoch'", "]", "\n", "sr_basic", "[", "'number_of_subscribers'", "]", "=", "pd", ".", "to_numeric", "(", "sr_basic", "[", "'number_of_subscribers'", "]", ")", "\n", "\n", "# adding explicit timestamp to the data and filtering SRs which were created before r/place started", "\n", "sr_basic", "=", "sr_basic", ".", "assign", "(", "created_utc_as_date", "=", "pd", ".", "to_datetime", "(", "sr_basic", "[", "'creation_epoch'", "]", ",", "unit", "=", "'s'", ")", ")", "\n", "sr_basic", "=", "sr_basic", "[", "sr_basic", "[", "'created_utc_as_date'", "]", "<", "'2017-03-31 00:00:00'", "]", "\n", "\n", "# using the data we pulled out, we will connect between the two and return the needed information", "\n", "chosen_srs_full_info", "=", "sr_basic", "[", "sr_basic", "[", "'subreddit_name'", "]", ".", "str", ".", "lower", "(", ")", ".", "isin", "(", "chosen_srs", ")", "]", "[", "[", "'subreddit_name'", ",", "\n", "'number_of_subscribers'", ",", "\n", "'created_utc_as_date'", "]", "]", "\n", "# returning results in a list format - each item is a tuple of 3. First is the name in lower-case letters,", "\n", "# second is the # of users in this SR and third is the creation date", "\n", "results", "=", "[", "(", "str", "(", "x", "[", "1", "]", ")", ".", "lower", "(", ")", ",", "x", "[", "2", "]", ",", "x", "[", "3", "]", ",", "'not_drawing'", ")", "for", "x", "in", "chosen_srs_full_info", ".", "itertuples", "(", ")", "]", "\n", "# adding the drawing teams to the party and sending results", "\n", "chosen_srs_full_info2", "=", "place_related_srs", "[", "place_related_srs", "[", "'SR'", "]", ".", "isin", "(", "drawing_srs", ")", "]", "[", "[", "'SR'", ",", "'num_of_users'", ",", "'creation_utc'", "]", "]", "\n", "results2", "=", "[", "(", "str", "(", "x", "[", "1", "]", ")", ".", "lower", "(", ")", ",", "x", "[", "2", "]", ",", "x", "[", "3", "]", ",", "'drawing'", ")", "for", "x", "in", "chosen_srs_full_info2", ".", "itertuples", "(", ")", "]", "\n", "duration", "=", "(", "datetime", ".", "datetime", ".", "now", "(", ")", "-", "start_time", ")", ".", "seconds", "\n", "print", "(", "\"'sr_sample_based_submissions' function has ended, total of {} not drawing teams and {} of drawing teams were\"", "\n", "\" suggested to be used. Took us {} seconds\"", ".", "format", "(", "len", "(", "results", ")", ",", "len", "(", "results2", ")", ",", "duration", ")", ")", "\n", "return", "results2", "+", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.data_loaders.general_loader._sample_srs_based_size": [[391, 467], ["list", "list", "print", "len", "print", "collections.OrderedDict().items", "collections.OrderedDict().items", "len", "chosen_srs.append", "diffs.append", "list.pop", "max", "abs", "int", "len", "sum", "sr.lower", "collections.OrderedDict", "collections.OrderedDict", "len", "sorted", "sorted", "print", "abs", "abs", "drawing_srs_data.items", "not_drawing_srs_data.items", "len"], "function", ["None"], ["", "def", "_sample_srs_based_size", "(", "drawing_srs_data", ",", "not_drawing_srs_data", ",", "size", ")", ":", "\n", "    ", "\"\"\"\n    Sample observations in a smart way. The sampling tries to return the closets distribution possible to the original\n    data got. This is a support function to the meta sampling functions (e.g., sr_sample_based_submissions)\n    :param drawing_srs_data: list\n        list of values related to the distribution values of the drawing team (i.e., list of the gorup size values\n        of all the drawing teams in r/place)\n    :param not_drawing_srs_data:  list\n        similar list as the drawing_srs_data one, but here it is related to the not_drawing teams. Expected to be a\n        much larger list than the drawing_srs_data one\n    :param size: int\n        how many instances to sample out of the not_drawing_srs_data group\n    :return: set (of integers)\n        set of int values, related to the indices of the not_drawing_srs_data (representing the chosen indices)\n\n    Examples:\n    >>> drawing_data = {'a': 5,'b': 10, 'c': 8, 'd': 3, 'e': 22}\n    >>> not_drawing_data = {'z': 40, 'x': 13, 'w': 22, 'v': 45, 'q': 8, 'r': 9, 's': 16, 't': 45, 'm': 98, 'n': 104}\n    >>> _sample_srs_based_size(drawing_srs_data=drawing_data, not_drawing_srs_data=not_drawing_data, size=len(drawing_data))\n    \"\"\"", "\n", "if", "size", ">", "len", "(", "not_drawing_srs_data", ")", ":", "\n", "        ", "print", "(", "\"Not possible to sample a subset larger than the origianl group size. 'size' must be smaller than the\"", "\n", "\"size of not_drawing_srs_data gropu. Try again please\"", ")", "\n", "return", "-", "1", "\n", "# first value is the original index, second one is the value. It will be ordered lowest to highest", "\n", "#drawing_srs_mapping = [(idx, value) for idx, value in enumerate(drawing_srs_data)]", "\n", "#not_drawing_srs_mapping = [(idx, value) for idx, value in enumerate(not_drawing_srs_data)]", "\n", "", "drawing_srs_mapping", "=", "list", "(", "OrderedDict", "(", "sorted", "(", "drawing_srs_data", ".", "items", "(", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "False", ")", ")", ".", "items", "(", ")", ")", "\n", "not_drawing_srs_mapping", "=", "list", "(", "OrderedDict", "(", "sorted", "(", "not_drawing_srs_data", ".", "items", "(", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "False", ")", ")", ".", "items", "(", ")", ")", "\n", "# some variables to handle the loop and the lookup process", "\n", "drawing_cur_idx", "=", "0", "\n", "not_drawing_cur_idx", "=", "0", "\n", "diffs", "=", "[", "]", "\n", "chosen_srs", "=", "[", "]", "\n", "# looping over and over till the list of indices we return is big enough", "\n", "while", "len", "(", "chosen_srs", ")", "<", "size", ":", "\n", "        ", "drawing_cur_value", "=", "drawing_srs_mapping", "[", "drawing_cur_idx", "]", "[", "1", "]", "\n", "not_drawing_cur_value", "=", "not_drawing_srs_mapping", "[", "not_drawing_cur_idx", "]", "[", "1", "]", "\n", "highest_value_limit_flag", "=", "False", "\n", "# inner loop, which looks for the best fit for the current candidate value", "\n", "while", "not_drawing_cur_value", "<", "drawing_cur_value", "and", "not", "highest_value_limit_flag", ":", "\n", "            ", "not_drawing_cur_idx", "+=", "1", "\n", "try", ":", "\n", "                ", "not_drawing_cur_value", "=", "not_drawing_srs_mapping", "[", "not_drawing_cur_idx", "]", "[", "1", "]", "\n", "# case the highest value in the not_drawing_list is lower than the value we look for", "\n", "", "except", "IndexError", ":", "\n", "                ", "not_drawing_cur_idx", "-=", "1", "\n", "not_drawing_cur_value", "=", "not_drawing_srs_mapping", "[", "not_drawing_cur_idx", "]", "[", "1", "]", "\n", "highest_value_limit_flag", "=", "True", "\n", "print", "(", "\"Along the _sample_srs_based_size function, highest_value_limit_flag turned to True once\"", ")", "\n", "# deciding which side to take (higher/lower than the the comparison to", "\n", "", "", "if", "not_drawing_cur_idx", ">", "0", ":", "\n", "            ", "lower_option", "=", "not_drawing_srs_mapping", "[", "not_drawing_cur_idx", "-", "1", "]", "[", "1", "]", "\n", "upper_option", "=", "not_drawing_cur_value", "\n", "chosen_idx", "=", "(", "not_drawing_cur_idx", "-", "1", ")", "if", "abs", "(", "drawing_cur_value", "-", "lower_option", ")", "<=", "abs", "(", "drawing_cur_value", "-", "upper_option", ")", "else", "not_drawing_cur_idx", "\n", "", "else", ":", "\n", "            ", "chosen_idx", "=", "0", "\n", "# adding the relevant name to the list of chosen indices", "\n", "", "chosen_srs", ".", "append", "(", "not_drawing_srs_mapping", "[", "chosen_idx", "]", "[", "0", "]", ")", "\n", "diffs", ".", "append", "(", "abs", "(", "drawing_cur_value", "-", "not_drawing_srs_mapping", "[", "chosen_idx", "]", "[", "1", "]", ")", ")", "\n", "# taking the instance we chose of of the big list, since we do not want to have duplications", "\n", "not_drawing_srs_mapping", ".", "pop", "(", "chosen_idx", ")", "\n", "# since we took out an instance, we need to update the index of the not_drawing group", "\n", "not_drawing_cur_idx", "=", "max", "(", "0", ",", "not_drawing_cur_idx", "-", "1", ")", "\n", "# case we haven't reached the end of the drawing group, we will continue iterating over it", "\n", "if", "drawing_cur_idx", "<", "len", "(", "drawing_srs_data", ")", "-", "1", ":", "\n", "            ", "drawing_cur_idx", "+=", "1", "\n", "# case we did reach the end of this group, we will start a new \"cycle\" of sampling, but not from the begining,", "\n", "# since then our sample would be biased towrds small submissions amount SRs", "\n", "", "else", ":", "\n", "            ", "drawing_cur_idx", "=", "int", "(", "len", "(", "drawing_srs_mapping", ")", "*", "1.0", "/", "2.0", ")", "\n", "not_drawing_cur_idx", "=", "0", "\n", "", "", "print", "(", "\"Summary of the _sample_srs_based_size function: we sampled {} SRs.\"", "\n", "\"Total difference according to the data is: {}\"", ".", "format", "(", "len", "(", "chosen_srs", ")", ",", "sum", "(", "diffs", ")", ")", ")", "\n", "return", "[", "sr", ".", "lower", "(", ")", "for", "sr", "in", "chosen_srs", "]", ",", "diffs", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.data_loaders.torch_loader.TorchLoader.__init__": [[21, 116], ["os.path.join", "datetime.datetime.now", "torch.utils.data.DataLoader.__init__", "pickle.load", "sr_classifier.reddit_data_preprocessing.RedditDataPrep", "max", "enumerate", "eval", "gc.collect", "sklearn.model_selection.StratifiedKFold", "list", "open", "eval", "eval", "eval", "cur_sr.meta_features_handler", "sklearn.model_selection.StratifiedKFold.split", "print", "cur_sr.subsample_submissions_data", "full_tok_text.append", "submission_tokens_as_one_list.append", "os.path.join", "os.path.join", "torchtext.data.Example.fromlist", "torchtext.data.Example.fromlist", "sr_classifier.reddit_data_preprocessing.RedditDataPrep.tokenize_text", "len", "submission_tokens_as_one_list.append", "datetime.datetime.now", "type", "type", "type", "sr_classifier.reddit_data_preprocessing.RedditDataPrep.tokenize_text", "type", "sr_classifier.reddit_data_preprocessing.RedditDataPrep.tokenize_text"], "methods", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.single_lstm.SinglelLstm.__init__"], ["def", "__init__", "(", "self", ",", "text_field", ",", "label_field", ",", "sr_name_field", ",", "meta_data_field", ",", "config_dict", ",", "examples", "=", "None", ",", "\n", "verbose", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Create an MR dataset instance given a path and fields.\n\n        Arguments:\n            text_field: The field that will be used for text data.\n            label_field: The field that will be used for label data.\n            path: Path to the data file.\n            examples: The examples contain all the data.\n            Remaining keyword arguments: Passed to the constructor of\n                data.Dataset.\n        \"\"\"", "\n", "machine", "=", "config_dict", "[", "'machine'", "]", "\n", "self", ".", "data_path", "=", "config_dict", "[", "'data_dir'", "]", "[", "machine", "]", "\n", "self", ".", "sr_objects_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_path", ",", "'sr_objects'", ",", "config_dict", "[", "'srs_obj_file'", "]", "[", "machine", "]", ")", "\n", "self", ".", "config_dict", "=", "config_dict", "\n", "\n", "# now we start loading (this is part of the init!)", "\n", "#text_field.preprocessing = data.Pipeline(clean_str)", "\n", "start_time", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "data_path", "=", "config_dict", "[", "'data_dir'", "]", "[", "machine", "]", "\n", "fields", "=", "[", "(", "'text'", ",", "text_field", ")", ",", "(", "'label'", ",", "label_field", ")", ",", "\n", "(", "'sr_name'", ",", "sr_name_field", ")", ",", "(", "'meta_data'", ",", "meta_data_field", ")", "]", "\n", "if", "examples", "is", "None", ":", "\n", "            ", "examples", "=", "[", "]", "\n", "sr_objs", "=", "pickle", ".", "load", "(", "open", "(", "self", ".", "sr_objects_path", ",", "\"rb\"", ")", ")", "\n", "#sr_objs = sr_objs[0:100]", "\n", "# creating tokenized information to each SR, if it doesn't exist", "\n", "dp_obj", "=", "RedditDataPrep", "(", "is_submission_data", "=", "True", ",", "remove_stop_words", "=", "False", ",", "most_have_regex", "=", "None", ")", "\n", "min_sent_length", "=", "max", "(", "config_dict", "[", "'kernel_sizes'", "]", ")", "\n", "for", "loop_idx", ",", "cur_sr", "in", "enumerate", "(", "sr_objs", ")", ":", "\n", "                ", "if", "verbose", "and", "(", "loop_idx", "%", "400", "==", "0", ")", "and", "loop_idx", "!=", "0", ":", "\n", "                    ", "duration", "=", "(", "datetime", ".", "datetime", ".", "now", "(", ")", "-", "start_time", ")", ".", "seconds", "\n", "print", "(", "\"Finished loading {} objects. Took us up to now: {} sec\"", ".", "format", "(", "loop_idx", ",", "duration", ")", ")", "\n", "# submission data under sampling", "\n", "", "sampling_dict", "=", "config_dict", "[", "'submissions_sampling'", "]", "\n", "if", "eval", "(", "sampling_dict", "[", "'should_sample'", "]", ")", ":", "\n", "                    ", "cur_sr", ".", "subsample_submissions_data", "(", "subsample_logic", "=", "sampling_dict", "[", "'sampling_logic'", "]", ",", "\n", "percentage", "=", "sampling_dict", "[", "'percentage'", "]", ",", "\n", "maximum_submissions", "=", "sampling_dict", "[", "'max_subm'", "]", ",", "\n", "seed", "=", "config_dict", "[", "'random_seed'", "]", ")", "\n", "\n", "", "full_tok_text", "=", "[", "]", "\n", "for", "st", "in", "cur_sr", ".", "submissions_as_list", ":", "\n", "# case the self-text is not none (in case it is none, we'll just take the header or the self text)", "\n", "                    ", "if", "type", "(", "st", "[", "2", "]", ")", "is", "str", "and", "type", "(", "st", "[", "1", "]", ")", "is", "str", ":", "\n", "                        ", "cur_tok_words", "=", "dp_obj", ".", "tokenize_text", "(", "sample", "=", "st", "[", "1", "]", "+", "'. '", "+", "st", "[", "2", "]", ",", "convert_to_lemmas", "=", "False", ")", "\n", "", "elif", "type", "(", "st", "[", "1", "]", ")", "is", "str", ":", "\n", "                        ", "cur_tok_words", "=", "dp_obj", ".", "tokenize_text", "(", "sample", "=", "st", "[", "1", "]", ",", "convert_to_lemmas", "=", "False", ")", "\n", "", "elif", "type", "(", "st", "[", "2", "]", ")", "is", "str", ":", "\n", "                        ", "cur_tok_words", "=", "dp_obj", ".", "tokenize_text", "(", "sample", "=", "st", "[", "2", "]", ",", "convert_to_lemmas", "=", "False", ")", "\n", "", "else", ":", "\n", "                        ", "continue", "\n", "", "full_tok_text", ".", "append", "(", "cur_tok_words", ")", "\n", "# after going over all submissions, we add it to the object itself", "\n", "", "submission_tokens_as_one_list", "=", "[", "]", "\n", "for", "sent", "in", "full_tok_text", ":", "\n", "# filter out sentences which are too short (shorter than minimum kernel size)", "\n", "                    ", "if", "len", "(", "sent", ")", "<=", "min_sent_length", ":", "\n", "                        ", "continue", "\n", "", "for", "w", "in", "sent", ":", "\n", "                        ", "submission_tokens_as_one_list", ".", "append", "(", "w", ")", "\n", "", "submission_tokens_as_one_list", ".", "append", "(", "'<SENT_ENDS>'", ")", "\n", "\n", "", "cur_sr", ".", "submission_tokens_as_one_list", "=", "submission_tokens_as_one_list", "\n", "\n", "# data prep to the explanatory features (adding the network features/com2vec_algorithm if needed)", "\n", "if", "eval", "(", "config_dict", "[", "'meta_data_usage'", "]", "[", "'use_network'", "]", ")", ":", "\n", "                    ", "net_file_path", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "config_dict", "[", "'meta_data_usage'", "]", "[", "'network_file_path'", "]", "[", "machine", "]", ")", "\n", "", "else", ":", "\n", "                    ", "net_file_path", "=", "None", "\n", "", "if", "eval", "(", "config_dict", "[", "'meta_data_usage'", "]", "[", "'use_communities_overlap'", "]", ")", ":", "\n", "                    ", "com_overlap_file_path", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "config_dict", "[", "'meta_data_usage'", "]", "[", "'communities_overlap_file_path'", "]", "[", "machine", "]", ")", "\n", "", "else", ":", "\n", "                    ", "com_overlap_file_path", "=", "None", "\n", "", "cur_sr", ".", "meta_features_handler", "(", "net_feat_file", "=", "net_file_path", ",", "com_overlap_file", "=", "com_overlap_file_path", ")", "\n", "\n", "# case we wish to use explanatory features as part of the modeling", "\n", "", "if", "eval", "(", "config_dict", "[", "'meta_data_usage'", "]", "[", "'use_meta'", "]", ")", ":", "\n", "                ", "examples", "+=", "[", "data", ".", "Example", ".", "fromlist", "(", "[", "sr", ".", "submission_tokens_as_one_list", ",", "sr", ".", "trying_to_draw", ",", "\n", "sr", ".", "name", ",", "sr", ".", "explanatory_features", "]", ",", "fields", ")", "for", "sr", "in", "sr_objs", "]", "\n", "# case explanatory features are not taking part - it will be None in each example", "\n", "", "else", ":", "\n", "                ", "examples", "+=", "[", "data", ".", "Example", ".", "fromlist", "(", "[", "sr", ".", "submission_tokens_as_one_list", ",", "sr", ".", "trying_to_draw", ",", "\n", "sr", ".", "name", ",", "None", "]", ",", "fields", ")", "for", "sr", "in", "sr_objs", "]", "\n", "\n", "", "del", "sr_objs", "\n", "gc", ".", "collect", "(", ")", "\n", "# case cv_splits were not initialized before - we'll do it here for the first time", "\n", "", "if", "TorchLoader", ".", "cv_splits", "is", "None", ":", "\n", "            ", "cv_obj", "=", "StratifiedKFold", "(", "n_splits", "=", "config_dict", "[", "'cv'", "]", "[", "'folds'", "]", ",", "\n", "random_state", "=", "config_dict", "[", "'random_seed'", "]", ")", "\n", "y_data", "=", "[", "ex", ".", "label", "for", "ex", "in", "examples", "]", "\n", "TorchLoader", ".", "cv_splits", "=", "list", "(", "cv_obj", ".", "split", "(", "examples", ",", "y_data", ")", ")", "\n", "", "super", "(", "TorchLoader", ",", "self", ")", ".", "__init__", "(", "examples", ",", "fields", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.data_loaders.torch_loader.TorchLoader.sort_key": [[117, 120], ["len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "sort_key", "(", "ex", ")", ":", "\n", "        ", "return", "len", "(", "ex", ".", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.data_loaders.torch_loader.TorchLoader.clean_str": [[121, 142], ["re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub.strip"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "clean_str", "(", "string", ")", ":", "\n", "        ", "\"\"\"\n        AVRAHAMI - change this to call spacy tokenizer\n        Tokenization/string cleaning for all datasets except for SST.\n        Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n        \"\"\"", "\n", "string", "=", "re", ".", "sub", "(", "r\"[^A-Za-z0-9(),!?\\'\\`]\"", ",", "\" \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'s\"", ",", "\" \\'s\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'ve\"", ",", "\" \\'ve\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"n\\'t\"", ",", "\" n\\'t\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'re\"", ",", "\" \\'re\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'d\"", ",", "\" \\'d\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'ll\"", ",", "\" \\'ll\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\",\"", ",", "\" , \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"!\"", ",", "\" ! \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\(\"", ",", "\" \\( \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\)\"", ",", "\" \\) \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\?\"", ",", "\" \\? \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\s{2,}\"", ",", "\" \"", ",", "string", ")", "\n", "return", "string", ".", "strip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.data_loaders.torch_loader.TorchLoader.splits": [[143, 186], ["cls", "cls", "cls", "cls", "random.Random().shuffle", "int", "random.Random", "len"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "splits", "(", "cls", ",", "text_field", ",", "label_field", ",", "sr_name_field", ",", "meta_data_field", ",", "config_dict", ",", "fold_number", ",", "\n", "dev_ratio", "=", ".1", ",", "dev_shuffle", "=", "True", ",", "root", "=", "'.'", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Create dataset objects for splits of the subreddits dataset.\n\n        Arguments:\n            text_field: The field that will be used for the sentence.\n            label_field: The field that will be used for label data.\n            dev_ratio: The ratio that will be used to get split validation dataset.\n            shuffle: Whether to shuffle the data before split.\n            root: The root directory that the dataset's zip archive will be\n                expanded into; therefore the directory in whose trees\n                subdirectory the data files will be stored.\n            train: The filename of the train data. Default: 'train.txt'.\n            Remaining keyword arguments: Passed to the splits method of\n                Dataset.\n        \"\"\"", "\n", "# note that although cls is a class call to a constructor, a list of examples is returned", "\n", "# (because of the .examples at the end of the construction call)", "\n", "examples", "=", "cls", "(", "text_field", "=", "text_field", ",", "label_field", "=", "label_field", ",", "sr_name_field", "=", "sr_name_field", ",", "\n", "meta_data_field", "=", "meta_data_field", ",", "config_dict", "=", "config_dict", ",", "**", "kwargs", ")", ".", "examples", "\n", "# creating CV splits of the data (usually into 5 folds)", "\n", "\n", "#cv_obj = StratifiedKFold(n_splits=config_dict['cv']['folds'], random_state=config_dict['random_seed'])", "\n", "#y_data = [ex.label for ex in examples]", "\n", "#cv_splits = list(cv_obj.split(examples, y_data))", "\n", "train_index", ",", "test_index", "=", "TorchLoader", ".", "cv_splits", "[", "fold_number", "]", "#list(cv_splits)[fold_number]", "\n", "cur_train_examples", "=", "[", "examples", "[", "i", "]", "for", "i", "in", "train_index", "]", "\n", "cur_test_examples", "=", "[", "examples", "[", "i", "]", "for", "i", "in", "test_index", "]", "\n", "if", "dev_shuffle", ":", "\n", "            ", "random", ".", "Random", "(", "x", "=", "config_dict", "[", "'random_seed'", "]", ")", ".", "shuffle", "(", "cur_train_examples", ")", "\n", "", "cur_dev_index", "=", "-", "1", "*", "int", "(", "dev_ratio", "*", "len", "(", "cur_train_examples", ")", ")", "\n", "\n", "cur_train_class", "=", "cls", "(", "text_field", "=", "text_field", ",", "label_field", "=", "label_field", ",", "sr_name_field", "=", "sr_name_field", ",", "\n", "meta_data_field", "=", "meta_data_field", ",", "config_dict", "=", "config_dict", ",", "\n", "examples", "=", "cur_train_examples", "[", ":", "cur_dev_index", "]", ")", "\n", "cur_dev_class", "=", "cls", "(", "text_field", "=", "text_field", ",", "label_field", "=", "label_field", ",", "sr_name_field", "=", "sr_name_field", ",", "\n", "meta_data_field", "=", "meta_data_field", ",", "config_dict", "=", "config_dict", ",", "\n", "examples", "=", "cur_train_examples", "[", "cur_dev_index", ":", "]", ")", "\n", "cur_test_class", "=", "cls", "(", "text_field", "=", "text_field", ",", "label_field", "=", "label_field", ",", "sr_name_field", "=", "sr_name_field", ",", "\n", "meta_data_field", "=", "meta_data_field", ",", "config_dict", "=", "config_dict", ",", "\n", "examples", "=", "cur_test_examples", ")", "\n", "return", "cur_train_class", ",", "cur_dev_class", ",", "cur_test_class", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.data_loaders.regex_based_loader.regex_based_loader": [[34, 178], ["os.getcwd", "datetime.datetime.now", "sorted", "sorted", "open", "open.close", "csv.DictWriter", "csv.DictWriter.writeheader", "enumerate", "enumerate", "os.listdir", "re.match", "os.listdir", "re.match", "any", "any", "cur_submission_file.endswith", "enumerate", "open", "open", "open.close", "print", "cur_comments_file.endswith", "enumerate", "open", "open", "open.close", "print", "bz2.BZ2File", "lzma.open", "re.search", "re.search", "str", "submissions.append", "len", "submissions[].keys", "open.close", "csv.DictWriter", "csv.DictWriter.writerow", "bz2.BZ2File", "lzma.open", "re.search", "pandas.to_datetime", "comments.append", "len", "comments[].keys", "open.close", "csv.DictWriter", "csv.DictWriter.writerow", "json.loads", "pandas.to_datetime", "dict", "dict", "csv.DictWriter", "csv.DictWriter.writerows", "datetime.datetime.now", "len", "json.loads", "int", "dict", "dict", "csv.DictWriter", "csv.DictWriter.writerows", "datetime.datetime.now", "len", "str", "str", "line.decode", "int", "csv.DictWriter.writeheader", "len", "line.decode", "csv.DictWriter.writeheader", "len", "os.stat", "os.stat"], "function", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_bert.bert_service.BertService.close", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_bert.bert_service.BertService.close", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_bert.bert_service.BertService.close", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_bert.bert_service.BertService.close", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_bert.bert_service.BertService.close"], ["def", "regex_based_loader", "(", "data_path", ",", "sr_to_include", "=", "None", ",", "saving_path", "=", "os", ".", "getcwd", "(", ")", ",", "load_only_columns_subset", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    loading reddit data, based on the zipped files from here - https://files.pushshift.io/reddit/\n    the process converts this files into csv, after filtering some columns (if load_only_columns_subset=True),\n    adding a date column and filtering a regex given. Logic of this function is almost identical to the one in the\n    'general_loader' function\n\n    :param data_path: str\n        location of the data\n    :param sr_to_include: list or None, default: NOne\n        placeholder for future use - to be used when only subset of SRs are needed\n    :param saving_path: str, default: existing location of the python code\n        location to save the files into\n    :param load_only_columns_subset: bool, default: False\n    :return: None\n        only prints to screen and saving csv files into the saving_path location\n\n    Example\n    -------\n    >>> general_loader(data_path, saving_path=data_path + 'place_classifier_csvs', load_only_columns_subset=True)\n    \"\"\"", "\n", "start_time", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "# finding all the relevant zip files in the 'data_path' directory", "\n", "submission_files_path", "=", "data_path", "+", "'submissions/'", "if", "sys", ".", "platform", "==", "'linux'", "else", "data_path", "+", "'submissions\\\\'", "\n", "comments_files_path", "=", "data_path", "+", "'comments/'", "if", "sys", ".", "platform", "==", "'linux'", "else", "data_path", "+", "'comments\\\\'", "\n", "submission_files", "=", "[", "f", "for", "f", "in", "os", ".", "listdir", "(", "submission_files_path", ")", "if", "re", ".", "match", "(", "r'RS.*\\.bz2|RS.*\\.xz'", ",", "f", ")", "]", "\n", "comments_files", "=", "[", "f", "for", "f", "in", "os", ".", "listdir", "(", "comments_files_path", ")", "if", "re", ".", "match", "(", "r'RC.*\\.bz2|RC.*\\.xz'", ",", "f", ")", "]", "\n", "# taking only files which are in the 'included_years' subset", "\n", "submission_files", "=", "[", "sf", "for", "sf", "in", "submission_files", "if", "any", "(", "str", "(", "year", ")", "in", "sf", "for", "year", "in", "included_years", ")", "]", "\n", "comments_files", "=", "[", "cf", "for", "cf", "in", "comments_files", "if", "any", "(", "str", "(", "year", ")", "in", "cf", "for", "year", "in", "included_years", ")", "]", "\n", "submission_files", "=", "sorted", "(", "submission_files", ")", "\n", "comments_files", "=", "sorted", "(", "comments_files", ")", "\n", "submissions_interesting_col", "=", "[", "\"created_utc_as_date\"", ",", "\"author\"", ",", "\"subreddit\"", ",", "\"title\"", ",", "\"selftext\"", ",", "\"num_comments\"", ",", "\n", "\"permalink\"", ",", "\"score\"", ",", "\"id\"", ",", "\"thumbnail\"", "]", "\n", "comments_interesting_col", "=", "[", "\"created_utc_as_date\"", ",", "\"author\"", ",", "\"subreddit\"", ",", "\"body\"", ",", "\"score\"", ",", "\"id\"", ",", "\n", "\"link_id\"", ",", "\"parent_id\"", ",", "\"thumbnail\"", "]", "\n", "meta_data_file", "=", "saving_path", "+", "'/'", "if", "sys", ".", "platform", "==", "'linux'", "else", "saving_path", "+", "'\\\\'", "\n", "meta_data_file", "+=", "'meta_data_desired_regex_filtered.csv'", "\n", "# writing the header for the meta data file", "\n", "m", "=", "open", "(", "meta_data_file", ",", "'a'", ")", "\n", "with", "m", "as", "output_file", ":", "\n", "        ", "dict_writer", "=", "csv", ".", "DictWriter", "(", "output_file", ",", "fieldnames", "=", "[", "'file'", ",", "'tot_lines'", ",", "'relvent_lines'", "]", ")", "\n", "dict_writer", ".", "writeheader", "(", ")", "\n", "", "m", ".", "close", "(", ")", "\n", "# looping over each file of the submission/comment and handling it", "\n", "if", "data_to_process", "==", "'submission'", "or", "data_to_process", "==", "'both'", ":", "\n", "# for cur_submission_file in submission_files[0]:", "\n", "        ", "for", "subm_idx", ",", "cur_submission_file", "in", "enumerate", "(", "submission_files", ")", ":", "\n", "# we expect to see inside the zip, a file called exactly the same as the zip one, besides the ending 'bz2'", "\n", "# or 'xz", "\n", "            ", "if", "cur_submission_file", ".", "endswith", "(", "'bz2'", ")", ":", "\n", "                ", "zipped_submission", "=", "bz2", ".", "BZ2File", "(", "submission_files_path", "+", "cur_submission_file", ",", "'r'", ")", "\n", "", "else", ":", "\n", "                ", "zipped_submission", "=", "lzma", ".", "open", "(", "submission_files_path", "+", "cur_submission_file", ",", "mode", "=", "'r'", ")", "\n", "# looping over each row in the submission data", "\n", "", "submissions", "=", "[", "]", "\n", "for", "inner_idx", ",", "line", "in", "enumerate", "(", "zipped_submission", ")", ":", "\n", "                ", "try", ":", "\n", "                    ", "cur_line", "=", "json", ".", "loads", "(", "line", ".", "decode", "(", "'UTF-8'", ")", ")", "\n", "", "except", "json", ".", "decoder", ".", "JSONDecodeError", ":", "\n", "                    ", "continue", "\n", "", "cur_title", "=", "cur_line", "[", "'title'", "]", "\n", "cur_selftext", "=", "cur_line", "[", "'selftext'", "]", "\n", "title_res", "=", "re", ".", "search", "(", "regex_required", ",", "cur_title", ",", "re", ".", "IGNORECASE", ")", "\n", "selftext_res", "=", "re", ".", "search", "(", "regex_required", ",", "cur_selftext", ",", "re", ".", "IGNORECASE", ")", "\n", "if", "title_res", "is", "None", "and", "selftext_res", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "cur_line", "[", "'created_utc_as_date'", "]", "=", "str", "(", "pd", ".", "to_datetime", "(", "int", "(", "cur_line", "[", "'created_utc'", "]", ")", ",", "unit", "=", "'s'", ")", ")", "\n", "if", "load_only_columns_subset", ":", "\n", "                    ", "line_shrinked", "=", "dict", "(", "(", "k", ",", "cur_line", "[", "k", "]", ")", "if", "k", "in", "cur_line", "else", "(", "k", ",", "None", ")", "for", "k", "in", "submissions_interesting_col", ")", "\n", "# we still define this 'line_shrinked' also in cases when we want to have all columns, since in some", "\n", "# cases there are redundant columns appear in the zip original files", "\n", "", "else", ":", "\n", "                    ", "line_shrinked", "=", "dict", "(", "(", "k", ",", "cur_line", "[", "k", "]", ")", "if", "k", "in", "cur_line", "else", "(", "k", ",", "None", ")", "for", "k", "in", "submission_columns", ")", "\n", "", "submissions", ".", "append", "(", "line_shrinked", ")", "\n", "# saving the data to a file. Currently it is as a csv format (found it as the most useful one)", "\n", "", "full_file_name", "=", "saving_path", "+", "'/'", "if", "sys", ".", "platform", "==", "'linux'", "else", "saving_path", "+", "'\\\\'", "\n", "full_file_name", "+=", "cur_submission_file", "[", "0", ":", "7", "]", "+", "'_desired_regex_filtered.csv'", "\n", "f", "=", "open", "(", "full_file_name", ",", "'a'", ")", "\n", "# only in case we found some relevant post - we'll add it to the file", "\n", "if", "len", "(", "submissions", ")", ">", "0", ":", "\n", "                ", "keys", "=", "submissions", "[", "0", "]", ".", "keys", "(", ")", "\n", "with", "f", "as", "output_file", ":", "\n", "                    ", "dict_writer", "=", "csv", ".", "DictWriter", "(", "output_file", ",", "keys", ")", "\n", "# adding a header only if the file is a new one", "\n", "if", "os", ".", "stat", "(", "full_file_name", ")", ".", "st_size", "==", "0", ":", "\n", "                        ", "dict_writer", ".", "writeheader", "(", ")", "\n", "", "dict_writer", ".", "writerows", "(", "submissions", ")", "\n", "", "f", ".", "close", "(", ")", "\n", "", "m", "=", "open", "(", "meta_data_file", ",", "'a'", ")", "\n", "with", "m", "as", "output_file", ":", "\n", "                ", "dict_writer", "=", "csv", ".", "DictWriter", "(", "output_file", ",", "fieldnames", "=", "[", "'file'", ",", "'tot_lines'", ",", "'relvent_lines'", "]", ")", "\n", "dict_writer", ".", "writerow", "(", "{", "'file'", ":", "cur_submission_file", ",", "'tot_lines'", ":", "inner_idx", ",", "'relvent_lines'", ":", "len", "(", "submissions", ")", "}", ")", "\n", "", "m", ".", "close", "(", ")", "\n", "duration", "=", "(", "datetime", ".", "datetime", ".", "now", "(", ")", "-", "start_time", ")", ".", "seconds", "\n", "print", "(", "\"Finished handling the {}'th submission file called {}. Took us up to now: {} seconds. \"", "\n", "\"Current submission size is {}\"", ".", "format", "(", "subm_idx", "+", "1", ",", "cur_submission_file", ",", "duration", ",", "len", "(", "submissions", ")", ")", ")", "\n", "", "", "if", "data_to_process", "==", "'comments'", "or", "data_to_process", "==", "'both'", ":", "\n", "        ", "for", "comm_idx", ",", "cur_comments_file", "in", "enumerate", "(", "comments_files", ")", ":", "\n", "            ", "if", "cur_comments_file", ".", "endswith", "(", "'bz2'", ")", ":", "\n", "                ", "zipped_comment", "=", "bz2", ".", "BZ2File", "(", "comments_files_path", "+", "cur_comments_file", ",", "'r'", ")", "\n", "", "else", ":", "\n", "                ", "zipped_comment", "=", "lzma", ".", "open", "(", "comments_files_path", "+", "cur_comments_file", ",", "mode", "=", "'r'", ")", "\n", "# looping over each row in the comments data", "\n", "", "comments", "=", "[", "]", "\n", "for", "inner_idx", ",", "line", "in", "enumerate", "(", "zipped_comment", ")", ":", "\n", "                ", "try", ":", "\n", "                    ", "cur_line", "=", "json", ".", "loads", "(", "line", ".", "decode", "(", "'UTF-8'", ")", ")", "\n", "", "except", "json", ".", "decoder", ".", "JSONDecodeError", ":", "\n", "                    ", "continue", "\n", "", "cur_body", "=", "cur_line", "[", "'body'", "]", "\n", "body_res", "=", "re", ".", "search", "(", "regex_required", ",", "cur_body", ",", "re", ".", "IGNORECASE", ")", "\n", "if", "body_res", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "cur_line", "[", "'created_utc_as_date'", "]", "=", "pd", ".", "to_datetime", "(", "int", "(", "cur_line", "[", "'created_utc'", "]", ")", ",", "unit", "=", "'s'", ")", "\n", "if", "load_only_columns_subset", ":", "\n", "                    ", "line_shrinked", "=", "dict", "(", "(", "k", ",", "cur_line", "[", "k", "]", ")", "if", "k", "in", "cur_line", "else", "(", "k", ",", "None", ")", "for", "k", "in", "comments_interesting_col", ")", "\n", "# we still define this 'line_shrinked' also in cases when we want to have all columns, since in some", "\n", "# cases there are redundant columns appear in the zip original files", "\n", "", "else", ":", "\n", "                    ", "line_shrinked", "=", "dict", "(", "(", "k", ",", "cur_line", "[", "k", "]", ")", "if", "k", "in", "cur_line", "else", "(", "k", ",", "None", ")", "for", "k", "in", "comments_columns", ")", "\n", "", "comments", ".", "append", "(", "line_shrinked", ")", "\n", "# saving the data to a file. Currently it is as a csv format (found it as the most useful one)", "\n", "", "full_file_name", "=", "saving_path", "+", "'/'", "if", "sys", ".", "platform", "==", "'linux'", "else", "saving_path", "+", "'\\\\'", "\n", "full_file_name", "+=", "cur_comments_file", "[", "0", ":", "7", "]", "+", "'_desired_regex_filtered.csv'", "\n", "f", "=", "open", "(", "full_file_name", ",", "'a'", ")", "\n", "if", "len", "(", "comments", ")", ">", "0", ":", "\n", "                ", "keys", "=", "comments", "[", "0", "]", ".", "keys", "(", ")", "\n", "with", "f", "as", "output_file", ":", "\n", "                    ", "dict_writer", "=", "csv", ".", "DictWriter", "(", "output_file", ",", "keys", ")", "\n", "# adding a header only if the file is a new one", "\n", "if", "os", ".", "stat", "(", "full_file_name", ")", ".", "st_size", "==", "0", ":", "\n", "                        ", "dict_writer", ".", "writeheader", "(", ")", "\n", "", "dict_writer", ".", "writerows", "(", "comments", ")", "\n", "", "f", ".", "close", "(", ")", "\n", "", "m", "=", "open", "(", "meta_data_file", ",", "'a'", ")", "\n", "with", "m", "as", "output_file", ":", "\n", "                ", "dict_writer", "=", "csv", ".", "DictWriter", "(", "output_file", ",", "fieldnames", "=", "[", "'file'", ",", "'tot_lines'", ",", "'relvent_lines'", "]", ")", "\n", "dict_writer", ".", "writerow", "(", "{", "'file'", ":", "cur_comments_file", ",", "\n", "'tot_lines'", ":", "inner_idx", ",", "'relvent_lines'", ":", "len", "(", "comments", ")", "}", ")", "\n", "", "m", ".", "close", "(", ")", "\n", "duration", "=", "(", "datetime", ".", "datetime", ".", "now", "(", ")", "-", "start_time", ")", ".", "seconds", "\n", "print", "(", "\"Finished handling the {}'th comments file. Took us up to now: {} seconds. \"", "\n", "\"Current comments size is {}\"", ".", "format", "(", "comm_idx", "+", "1", ",", "duration", ",", "len", "(", "comments", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.r_place_drawing_classifier.main_pytorch_bert._extract_sr_info": [[31, 45], ["pickle.load", "pickle.load.meta_features_handler", "open", "print", "eval", "dict", "dict", "os.path.join"], "function", ["None"], ["def", "_extract_sr_info", "(", "idx", ",", "sr_obj_file", ",", "data_path", ",", "net_feat_file", ")", ":", "\n", "    ", "cur_sr", "=", "pickle", ".", "load", "(", "open", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'sr_objects'", ",", "sr_obj_file", ")", ",", "\"rb\"", ")", ")", "\n", "# case the language of the current SR is French/Italian/Greek...", "\n", "if", "not", "(", "cur_sr", ".", "lang", "==", "'en'", "or", "cur_sr", ".", "lang", "is", "None", ")", ":", "\n", "        ", "print", "(", "\"SR {} was found with a foreign language \"", "\n", "\"(target={}), we skip it\"", ".", "format", "(", "cur_sr", ".", "name", ",", "cur_sr", ".", "trying_to_draw", ")", ")", "\n", "return", "None", "\n", "", "res", "=", "cur_sr", ".", "meta_features_handler", "(", "smooth_zero_features", "=", "True", ",", "\n", "net_feat_file", "=", "net_feat_file", ",", "\n", "features_to_exclude", "=", "None", ")", "\n", "y_value", "=", "1", "if", "cur_sr", ".", "trying_to_draw", "==", "1", "else", "0", "\n", "other_explanatory_features", "=", "dict", "(", "cur_sr", ".", "explanatory_features", ")", "if", "eval", "(", "config_dict", "[", "\"meta_data_usage\"", "]", "[", "\"use_meta\"", "]", ")", "else", "dict", "(", ")", "\n", "return", "{", "'sr_name'", ":", "cur_sr", ".", "name", ",", "'label'", ":", "y_value", ",", "'other_explanatory_features'", ":", "other_explanatory_features", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.r_place_drawing_classifier.main_srs_creation._sr_creation": [[36, 160], ["print", "datetime.datetime.now", "r_place_drawing_classifier.utils.get_submissions_subset", "enumerate", "print", "gc.collect", "eval", "eval", "r_place_drawing_classifier.utils.get_comments_subset", "sr_classifier.sub_reddit.SubReddit", "submission_dp_obj.data_pre_process", "min", "submission_dp_obj.detect_lang", "gc.collect", "eval", "eval", "sr_classifier.sub_reddit.SubReddit.update_words_dicts", "os.path.join", "gc.collect", "os.path.join", "len", "full_tok_text.append", "eval", "eval", "set", "comments_dp_obj.data_pre_process", "sr_classifier.sub_reddit.SubReddit.create_explanatory_features", "sr_classifier.sub_reddit.SubReddit.create_explanatory_features", "str", "os.path.exists", "eval", "eval", "warnings.warn", "pickle.dump", "print", "datetime.datetime.now", "len", "os.path.join", "submission_dp_obj.tokenize_text", "open", "eval", "pickle.dump", "submission_data[].str.lower", "type", "type", "type", "submission_dp_obj.tokenize_text", "cur_sr_comments[].isin", "type", "comments_dp_obj.tokenize_text", "full_tok_text.append", "os.path.exists", "open", "datetime.datetime.now", "type", "type", "submission_dp_obj.mark_urls", "submission_dp_obj.mark_urls", "type", "submission_dp_obj.tokenize_text", "comments_data[].str.lower", "comments_dp_obj.mark_urls", "submission_dp_obj.mark_urls", "submission_dp_obj.mark_urls"], "function", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.r_place_drawing_classifier.utils.get_submissions_subset", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.r_place_drawing_classifier.utils.get_comments_subset"], ["def", "_sr_creation", "(", "srs_mapping", ",", "submission_dp_obj", ",", "comments_dp_obj", ",", "srs_to_create", ",", "pool_number", ")", ":", "\n", "    ", "print", "(", "\"Pool # {} has started running the _sr_creation function\"", ".", "format", "(", "pool_number", ")", ")", "\n", "start_time", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "empty_srs", "=", "0", "\n", "submission_data", "=", "get_submissions_subset", "(", "\n", "files_path", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "'place_classifier_csvs'", ")", ",", "srs_to_include", "=", "srs_to_create", ",", "\n", "start_month", "=", "config_dict", "[", "'data_period'", "]", "[", "'start_month'", "]", ",", "end_month", "=", "config_dict", "[", "'data_period'", "]", "[", "'end_month'", "]", ",", "\n", "min_utc", "=", "None", ",", "max_utc", "=", "'2017-03-29 00:00:00'", ")", "\n", "\n", "# same thing for the comments data", "\n", "if", "eval", "(", "config_dict", "[", "'comments_usage'", "]", "[", "'meta_data'", "]", ")", "or", "eval", "(", "config_dict", "[", "'comments_usage'", "]", "[", "'corpus'", "]", ")", ":", "\n", "        ", "comments_data", "=", "get_comments_subset", "(", "files_path", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "'place_classifier_csvs'", ")", ",", "\n", "srs_to_include", "=", "srs_to_create", ",", "\n", "start_month", "=", "config_dict", "[", "'data_period'", "]", "[", "'start_month'", "]", ",", "\n", "end_month", "=", "config_dict", "[", "'data_period'", "]", "[", "'end_month'", "]", ",", "\n", "min_utc", "=", "None", ",", "max_utc", "=", "'2017-03-29 00:00:00'", ")", "\n", "", "else", ":", "\n", "        ", "comments_data", "=", "None", "\n", "\n", "", "for", "idx", ",", "sr_name", "in", "enumerate", "(", "srs_to_create", ")", ":", "\n", "        ", "cur_sr_submission", "=", "submission_data", "[", "submission_data", "[", "'subreddit'", "]", ".", "str", ".", "lower", "(", ")", "==", "sr_name", "]", "\n", "# case there are no relevant submissions to this sr", "\n", "if", "cur_sr_submission", ".", "shape", "[", "0", "]", "==", "0", ":", "\n", "            ", "empty_srs", "+=", "1", "\n", "continue", "\n", "# pulling out the meta data about the SR (the iloc[0] is used only to convert it into Series)", "\n", "", "sr_meta_data", "=", "{", "'SR'", ":", "sr_name", ",", "'creation_utc'", ":", "srs_mapping", "[", "sr_name", "]", "[", "1", "]", ",", "'end_state'", ":", "None", ",", "\n", "'num_of_users'", ":", "srs_mapping", "[", "sr_name", "]", "[", "0", "]", ",", "\n", "'trying_to_draw'", ":", "'Yes'", "if", "srs_mapping", "[", "sr_name", "]", "[", "2", "]", "==", "'drawing'", "else", "'No'", ",", "'models_prediction'", ":", "-", "1", "}", "\n", "cur_sr_obj", "=", "SubReddit", "(", "meta_data", "=", "sr_meta_data", ")", "\n", "cur_sr_submission_after_dp", ",", "submission_text", "=", "submission_dp_obj", ".", "data_pre_process", "(", "reddit_df", "=", "cur_sr_submission", ")", "\n", "# updating the object with the list of submissions", "\n", "cur_sr_obj", ".", "submissions_as_list", "=", "submission_text", "\n", "# detecting language of the text", "\n", "subm_to_take", "=", "min", "(", "len", "(", "cur_sr_obj", ".", "submissions_as_list", ")", ",", "1000", ")", "\n", "data_for_lang_detector", "=", "[", "sal", "[", "1", "]", "+", "'. '", "+", "sal", "[", "2", "]", "for", "sal", "in", "cur_sr_obj", ".", "submissions_as_list", "[", "0", ":", "subm_to_take", "]", "\n", "if", "type", "(", "sal", "[", "2", "]", ")", "is", "str", "and", "type", "(", "sal", "[", "1", "]", ")", "is", "str", "]", "\n", "chosen_lang", "=", "submission_dp_obj", ".", "detect_lang", "(", "text_list", "=", "data_for_lang_detector", ",", "min_score_per_sent", "=", "0.9", ",", "\n", "min_agg_score", "=", "0.7", ",", "sr_name", "=", "cur_sr_obj", ".", "name", ",", "verbose", "=", "False", ")", "\n", "cur_sr_obj", ".", "lang", "=", "chosen_lang", "\n", "del", "cur_sr_submission", "\n", "del", "data_for_lang_detector", "\n", "gc", ".", "collect", "(", ")", "\n", "# case we wish to tokenize the submission data, we'll do it now", "\n", "full_tok_text", "=", "[", "]", "\n", "for", "s", "in", "submission_text", ":", "\n", "# case the self-text is not none (in case it is none, we'll just take the header or the self text)", "\n", "            ", "if", "type", "(", "s", "[", "2", "]", ")", "is", "str", "and", "type", "(", "s", "[", "1", "]", ")", "is", "str", ":", "\n", "                ", "sample_for_tokenizer", "=", "submission_dp_obj", ".", "mark_urls", "(", "s", "[", "1", "]", ",", "marking_method", "=", "'tag'", ")", "[", "0", "]", "+", "'. '", "+", "submission_dp_obj", ".", "mark_urls", "(", "s", "[", "2", "]", ",", "marking_method", "=", "'tag'", ")", "[", "0", "]", "\n", "cur_tok_words", "=", "submission_dp_obj", ".", "tokenize_text", "(", "sample_for_tokenizer", ",", "convert_to_lemmas", "=", "False", ",", "\n", "break_to_sents", "=", "True", ")", "\n", "", "elif", "type", "(", "s", "[", "1", "]", ")", "is", "str", ":", "\n", "                ", "sample_for_tokenizer", "=", "submission_dp_obj", ".", "mark_urls", "(", "s", "[", "1", "]", ",", "marking_method", "=", "'tag'", ")", "[", "0", "]", "\n", "cur_tok_words", "=", "submission_dp_obj", ".", "tokenize_text", "(", "sample", "=", "sample_for_tokenizer", ",", "convert_to_lemmas", "=", "False", ",", "\n", "break_to_sents", "=", "True", ")", "\n", "", "elif", "type", "(", "s", "[", "2", "]", ")", "is", "str", ":", "\n", "                ", "sample_for_tokenizer", "=", "submission_dp_obj", ".", "mark_urls", "(", "s", "[", "2", "]", ",", "marking_method", "=", "'tag'", ")", "[", "0", "]", "\n", "cur_tok_words", "=", "submission_dp_obj", ".", "tokenize_text", "(", "sample", "=", "sample_for_tokenizer", ",", "convert_to_lemmas", "=", "False", ",", "\n", "break_to_sents", "=", "True", ")", "\n", "", "else", ":", "\n", "                ", "continue", "\n", "", "full_tok_text", ".", "append", "(", "cur_tok_words", ")", "\n", "", "cur_sr_obj", ".", "submissions_as_tokens", "=", "full_tok_text", "\n", "del", "full_tok_text", "\n", "\n", "# pulling out the comments data - case we want to use it. There are a few option of comments usage", "\n", "# case we want to use comments data for either creation of meta-data and as part of the corpus (or both)", "\n", "if", "eval", "(", "config_dict", "[", "'comments_usage'", "]", "[", "'meta_data'", "]", ")", "or", "eval", "(", "config_dict", "[", "'comments_usage'", "]", "[", "'corpus'", "]", ")", ":", "\n", "# first, we filter the comments, so only ones in the current sr we work with will appear", "\n", "            ", "cur_sr_comments", "=", "comments_data", "[", "comments_data", "[", "'subreddit'", "]", ".", "str", ".", "lower", "(", ")", "==", "sr_name", "]", "\n", "submission_ids", "=", "set", "(", "[", "'t3_'", "+", "sub_id", "for", "sub_id", "in", "cur_sr_submission_after_dp", "[", "'id'", "]", "]", ")", "\n", "# second, we filter the comments, so only ones which are relevant to the submissions dataset will appear", "\n", "# (this is due to the fact that we have already filtered a lot of submissions in pre step)", "\n", "cur_sr_comments", "=", "cur_sr_comments", "[", "cur_sr_comments", "[", "'link_id'", "]", ".", "isin", "(", "submission_ids", ")", "]", "\n", "cur_sr_comments_after_dp", ",", "comments_text", "=", "comments_dp_obj", ".", "data_pre_process", "(", "reddit_df", "=", "cur_sr_comments", ")", "\n", "del", "cur_sr_comments", "\n", "# case we want to use comments data for meta-features creation (very logical to be used)", "\n", "", "if", "eval", "(", "config_dict", "[", "'comments_usage'", "]", "[", "'meta_data'", "]", ")", ":", "\n", "            ", "cur_sr_obj", ".", "create_explanatory_features", "(", "submission_data", "=", "cur_sr_submission_after_dp", ",", "\n", "comments_data", "=", "cur_sr_comments_after_dp", ")", "\n", "# case we want to use only submission data for meta-data creation", "\n", "", "else", ":", "\n", "            ", "cur_sr_obj", ".", "create_explanatory_features", "(", "submission_data", "=", "cur_sr_submission_after_dp", ",", "comments_data", "=", "None", ")", "\n", "# case we want to use comments data as part of the corpus creation (most of the times not the case)", "\n", "", "if", "eval", "(", "config_dict", "[", "'comments_usage'", "]", "[", "'corpus'", "]", ")", ":", "\n", "            ", "cur_sr_obj", ".", "comments_as_list", "=", "comments_text", "\n", "\n", "# case we wish to tokenize the comments data, we'll do it now", "\n", "full_tok_text", "=", "[", "]", "\n", "for", "s", "in", "comments_text", ":", "\n", "                ", "if", "type", "(", "s", "[", "1", "]", ")", "is", "str", ":", "\n", "                    ", "sample_for_tokenizer", "=", "comments_dp_obj", ".", "mark_urls", "(", "s", "[", "1", "]", ",", "marking_method", "=", "'tag'", ")", "[", "0", "]", "\n", "cur_tok_words", "=", "comments_dp_obj", ".", "tokenize_text", "(", "sample", "=", "sample_for_tokenizer", ",", "convert_to_lemmas", "=", "False", ",", "\n", "break_to_sents", "=", "True", ")", "\n", "full_tok_text", ".", "append", "(", "cur_tok_words", ")", "\n", "", "", "cur_sr_obj", ".", "comments_as_tokens", "=", "full_tok_text", "\n", "del", "full_tok_text", "\n", "# updating the object with dictionaries of both submissions and comments", "\n", "", "cur_sr_obj", ".", "update_words_dicts", "(", "update_only_submissions_dict", "=", "False", ")", "\n", "# saving a pickle file of the object", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "#'sr_objects',", "\n", "str", "(", "'sr_obj_'", "+", "sr_name", "+", "'_'", "+", "config_dict", "[", "'saving_options'", "]", "[", "'file_name_suffix'", "]", "+", "'.p'", ")", ")", "\n", "# case the file name exists, we will raise a warning about it and will replace it", "\n", "if", "os", ".", "path", ".", "exists", "(", "file_name", ")", "and", "eval", "(", "config_dict", "[", "'saving_options'", "]", "[", "'override_existing_files'", "]", ")", "and", "eval", "(", "config_dict", "[", "'saving_options'", "]", "[", "'save_obj'", "]", ")", ":", "\n", "            ", "warnings", ".", "warn", "(", "\"sr_obj file for sr {} was found, will be replaced by a new one\"", ".", "format", "(", "sr_name", ")", ")", "\n", "pickle", ".", "dump", "(", "cur_sr_obj", ",", "open", "(", "file_name", ",", "\"wb\"", ")", ")", "\n", "", "elif", "not", "os", ".", "path", ".", "exists", "(", "file_name", ")", "and", "eval", "(", "config_dict", "[", "'saving_options'", "]", "[", "'save_obj'", "]", ")", ":", "\n", "            ", "pickle", ".", "dump", "(", "cur_sr_obj", ",", "open", "(", "file_name", ",", "\"ab\"", ")", ")", "\n", "\n", "", "gc", ".", "collect", "(", ")", "\n", "# printing progress", "\n", "if", "idx", "%", "1", "==", "0", "and", "idx", "!=", "0", ":", "\n", "            ", "duration", "=", "(", "datetime", ".", "datetime", ".", "now", "(", ")", "-", "start_time", ")", ".", "seconds", "\n", "print", "(", "\"Pool # {} reporting finish of the {} iteration. Took this pool {} seconds, \"", "\n", "\"moving forward\"", ".", "format", "(", "pool_number", ",", "idx", ",", "duration", ")", ")", "\n", "", "", "duration", "=", "(", "datetime", ".", "datetime", ".", "now", "(", ")", "-", "start_time", ")", ".", "seconds", "\n", "print", "(", "\"Pool # {} reporting finished passing over all SRs. Took him up to now {} seconds and he created {} SRs \"", "\n", "\"objects (some where empty, so weren't created)\"", ".", "format", "(", "pool_number", ",", "duration", ",", "len", "(", "srs_to_create", ")", ")", ")", "\n", "del", "submission_data", "\n", "del", "comments_data", "\n", "gc", ".", "collect", "(", ")", "\n", "return", "0", "\n", "# return sr_objects", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.r_place_drawing_classifier.utils.get_submissions_subset": [[16, 65], ["datetime.datetime.now", "sorted", "enumerate", "pandas.concat", "print", "pandas.read_csv", "submission_dfs.append", "len", "IOError", "os.listdir", "min", "max", "datetime.datetime.now", "re.match", "os.path.join", "cur_submission_df[].str.lower().isin", "cur_submission_df[].str.lower"], "function", ["None"], ["def", "get_submissions_subset", "(", "files_path", ",", "srs_to_include", ",", "start_month", "=", "'2016-10'", ",", "end_month", "=", "'2017-03'", ",", "\n", "min_utc", "=", "None", ",", "max_utc", "=", "'2017-03-29 00:00:00'", ")", ":", "\n", "    ", "\"\"\"\n    pulling our subset of the submission data, which is related to the list of SRs given as input. This is very\n    simple \"where\" statement\n    :param files_path: string\n        location of the files to be used (.csv ones)\n    :param srs_to_include: list (maybe set will also work here)\n        list with SR names to be included in the returned dataset. Expected to be lowe-case ones.\n        If None is given as input, then ALL subrddits are included\n    :param start_month: string, default: '2016-10'\n        the starting month in YYYY-MM format which data should be taken from\n    :param end_month: string, default: '2017-03'\n        the ending month in YYYY-MM format which data should be taken from\n    :param min_utc: string, default: None\n        the minimum timestamp to take into account. If None - no minimum time limitation is taken into account\n    :param max_utc: string, default: '2017-03-29 00:00:00' (a day before the start time of r/place experiment)\n        the maximum timestamp to take into account. If None - no minimum time limitation is taken into account\n    :return: pandas data-frame\n        df including all relevant submission, related to the SRs given as input\n    \"\"\"", "\n", "start_time", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "# finding all the relevant zip files in the 'data_path' directory", "\n", "submission_files", "=", "[", "f", "for", "f", "in", "os", ".", "listdir", "(", "files_path", ")", "if", "re", ".", "match", "(", "r'RS.*\\.csv'", ",", "f", ")", "and", "'sample'", "not", "in", "f", "]", "\n", "# taking only the submissions files from 10-2016 to 03-2017", "\n", "submission_files", "=", "[", "i", "for", "i", "in", "submission_files", "if", "\n", "''", ".", "join", "(", "[", "'RS_'", ",", "start_month", ",", "'.csv'", "]", ")", "<=", "i", "<=", "''", ".", "join", "(", "[", "'RS_'", ",", "end_month", ",", "'.csv'", "]", ")", "]", "\n", "submission_files", "=", "sorted", "(", "submission_files", ")", "\n", "submission_dfs", "=", "[", "]", "\n", "# iterating over each submission file", "\n", "for", "subm_idx", ",", "cur_submission_file", "in", "enumerate", "(", "submission_files", ")", ":", "\n", "        ", "cur_submission_df", "=", "pd", ".", "read_csv", "(", "filepath_or_buffer", "=", "os", ".", "path", ".", "join", "(", "files_path", ",", "cur_submission_file", ")", ",", "encoding", "=", "'utf-8'", ")", "\n", "# filtering the data-frame based on the list of SRs we want to include and the date (before r/place started)", "\n", "if", "srs_to_include", "is", "not", "None", ":", "\n", "            ", "cur_submission_df", "=", "cur_submission_df", "[", "cur_submission_df", "[", "\"subreddit\"", "]", ".", "str", ".", "lower", "(", ")", ".", "isin", "(", "srs_to_include", ")", "]", "\n", "# filtering based on min/max date", "\n", "", "cur_min_utc", "=", "min", "(", "cur_submission_df", "[", "'created_utc_as_date'", "]", ")", "if", "min_utc", "is", "None", "else", "min_utc", "\n", "cur_max_utc", "=", "max", "(", "cur_submission_df", "[", "'created_utc_as_date'", "]", ")", "if", "max_utc", "is", "None", "else", "max_utc", "\n", "cur_submission_df", "=", "cur_submission_df", "[", "(", "cur_submission_df", "[", "'created_utc_as_date'", "]", ">=", "cur_min_utc", ")", "&", "\n", "(", "cur_submission_df", "[", "'created_utc_as_date'", "]", "<=", "cur_max_utc", ")", "]", "\n", "submission_dfs", ".", "append", "(", "cur_submission_df", ")", "\n", "", "if", "len", "(", "submission_dfs", ")", "==", "0", ":", "\n", "        ", "raise", "IOError", "(", "\"No submission file was found\"", ")", "\n", "\n", "", "full_submissions_df", "=", "pd", ".", "concat", "(", "submission_dfs", ")", "\n", "duration", "=", "(", "datetime", ".", "datetime", ".", "now", "(", ")", "-", "start_time", ")", ".", "seconds", "\n", "print", "(", "f\"Function 'get_submission_subset_dataset' has ended. Took us : {duration} seconds. \"", "\n", "f\"Submission data-frame shape created is {full_submissions_df.shape}\"", ",", "flush", "=", "True", ")", "\n", "return", "full_submissions_df", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.r_place_drawing_classifier.utils.get_comments_subset": [[67, 116], ["datetime.datetime.now", "sorted", "enumerate", "pandas.concat", "print", "comments_dfs.append", "len", "IOError", "os.listdir", "pandas.read_csv", "pandas.read_csv", "min", "max", "datetime.datetime.now", "re.match", "os.path.join", "os.path.join", "cur_comments_df[].str.lower().isin", "cur_comments_df[].str.lower"], "function", ["None"], ["", "def", "get_comments_subset", "(", "files_path", ",", "srs_to_include", ",", "start_month", "=", "'2016-10'", ",", "end_month", "=", "'2017-03'", ",", "\n", "min_utc", "=", "None", ",", "max_utc", "=", "'2017-03-29 00:00:00'", ")", ":", "\n", "    ", "\"\"\"\n    pulling our subset of the commnets data, which is related to the list of SRs given as input. This is very\n    simple \"where\" statement\n    :param files_path: string\n        location of the files to be used (.csv ones)\n    :param srs_to_include: list (maybe set will also work here)\n        list with SR names to be included in the returned dataset. Expected to be lowe-case ones\n        If None is given as input, then ALL subrddits are included\n    :param start_month: string\n        the starting month in YYYY-MM format which data should be taken from\n    :param end_month: string\n        the ending month in YYYY-MM format which data should be taken from\n    :param min_utc: string, default: None\n        the minimum timestamp to take into account. If None - no minimum time limitation is taken into account\n    :param max_utc: string, default: '2017-03-29 00:00:00' (a day before the start time of r/place experiment)\n        the maximum timestamp to take into account. If None - no minimum time limitation is taken into account\n    :return: pandas data-frame\n        df including all relevant submission, related to the SRs given as input\n    \"\"\"", "\n", "start_time", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "# pulling out all comment files in the desired range of months", "\n", "comments_files", "=", "[", "f", "for", "f", "in", "os", ".", "listdir", "(", "files_path", ")", "if", "re", ".", "match", "(", "r'RC.*\\.csv'", ",", "f", ")", "and", "'sample'", "not", "in", "f", "]", "\n", "comments_files", "=", "[", "i", "for", "i", "in", "comments_files", "if", "\n", "''", ".", "join", "(", "[", "'RC_'", ",", "start_month", ",", "'.csv'", "]", ")", "<=", "i", "<=", "''", ".", "join", "(", "[", "'RC_'", ",", "end_month", ",", "'.csv'", "]", ")", "]", "\n", "comments_files", "=", "sorted", "(", "comments_files", ")", "\n", "comments_dfs", "=", "[", "]", "\n", "# looping over each file", "\n", "for", "comm_idx", ",", "cur_comments_file", "in", "enumerate", "(", "comments_files", ")", ":", "\n", "        ", "if", "sys", ".", "platform", "==", "'linux'", ":", "\n", "            ", "cur_comments_df", "=", "pd", ".", "read_csv", "(", "filepath_or_buffer", "=", "os", ".", "path", ".", "join", "(", "files_path", ",", "cur_comments_file", ")", ",", "encoding", "=", "'latin-1'", ")", "\n", "", "else", ":", "\n", "            ", "cur_comments_df", "=", "pd", ".", "read_csv", "(", "filepath_or_buffer", "=", "os", ".", "path", ".", "join", "(", "files_path", ",", "cur_comments_file", ")", ",", "encoding", "=", "'latin-1'", ")", "\n", "", "if", "srs_to_include", "is", "not", "None", ":", "\n", "            ", "cur_comments_df", "=", "cur_comments_df", "[", "cur_comments_df", "[", "\"subreddit\"", "]", ".", "str", ".", "lower", "(", ")", ".", "isin", "(", "srs_to_include", ")", "]", "\n", "# filtering based on min/max date", "\n", "", "cur_min_utc", "=", "min", "(", "cur_comments_df", "[", "'created_utc_as_date'", "]", ")", "if", "min_utc", "is", "None", "else", "min_utc", "\n", "cur_max_utc", "=", "max", "(", "cur_comments_df", "[", "'created_utc_as_date'", "]", ")", "if", "max_utc", "is", "None", "else", "max_utc", "\n", "cur_comments_df", "=", "cur_comments_df", "[", "(", "cur_comments_df", "[", "'created_utc_as_date'", "]", ">=", "cur_min_utc", ")", "&", "\n", "(", "cur_comments_df", "[", "'created_utc_as_date'", "]", "<=", "cur_max_utc", ")", "]", "\n", "comments_dfs", ".", "append", "(", "cur_comments_df", ")", "\n", "", "if", "len", "(", "comments_dfs", ")", "==", "0", ":", "\n", "        ", "raise", "IOError", "(", "\"No comments file were found\"", ")", "\n", "", "full_comments_df", "=", "pd", ".", "concat", "(", "comments_dfs", ")", "\n", "duration", "=", "(", "datetime", ".", "datetime", ".", "now", "(", ")", "-", "start_time", ")", ".", "seconds", "\n", "print", "(", "f\"Function 'get_comments_subset' has ended. Took us : {duration} seconds. \"", "\n", "f\"Comments data-frame shape created is {full_comments_df.shape}\"", ",", "flush", "=", "True", ")", "\n", "return", "full_comments_df", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.r_place_drawing_classifier.utils.calc_sr_statistics": [[118, 151], ["os.getcwd", "datetime.datetime.now", "sorted", "collections.Counter", "enumerate", "pickle.dump", "print", "pandas.read_csv", "collections.Counter", "print", "open", "os.listdir", "re.match", "any", "datetime.datetime.now", "len", "os.path.join", "datetime.datetime.now", "str"], "function", ["None"], ["", "def", "calc_sr_statistics", "(", "files_path", ",", "included_years", ",", "saving_res_path", "=", "os", ".", "getcwd", "(", ")", ")", ":", "\n", "    ", "\"\"\"\n    calculating relevant statistics to each sr found in the files given as input. This will be later used in order\n    to filter SRs with no submission/very small amount of submissions\n    :param files_path: string\n        location of the files to be used (.csv ones)\n    :param included_years: list\n        list of years to include in the analysis\n    :param saving_res_path: string\n        location where to save results\n    :return: dictionary\n        dictionary with statistics to each SR. The function also saves the results as a pickle file\n    \"\"\"", "\n", "start_time", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "submission_files", "=", "[", "f", "for", "f", "in", "os", ".", "listdir", "(", "files_path", ")", "if", "re", ".", "match", "(", "r'RS.*\\.csv'", ",", "f", ")", "]", "\n", "# taking only files which are in the 'included_years' subset", "\n", "submission_files", "=", "[", "sf", "for", "sf", "in", "submission_files", "if", "any", "(", "str", "(", "year", ")", "in", "sf", "for", "year", "in", "included_years", ")", "]", "\n", "# comments_files = [cf for cf in comments_files if any(str(year) in cf for year in included_years)]", "\n", "submission_files", "=", "sorted", "(", "submission_files", ")", "\n", "sr_statistics", "=", "collections", ".", "Counter", "(", ")", "\n", "for", "subm_idx", ",", "cur_submission_file", "in", "enumerate", "(", "submission_files", ")", ":", "\n", "        ", "cur_submission_df", "=", "pd", ".", "read_csv", "(", "filepath_or_buffer", "=", "os", ".", "path", ".", "join", "(", "files_path", ",", "cur_submission_file", ")", ")", "\n", "cur_sr_statistics", "=", "collections", ".", "Counter", "(", "cur_submission_df", "[", "\"subreddit\"", "]", ")", "\n", "sr_statistics", "+=", "cur_sr_statistics", "\n", "# writing status to screen", "\n", "duration", "=", "(", "datetime", ".", "datetime", ".", "now", "(", ")", "-", "start_time", ")", ".", "seconds", "\n", "print", "(", "\"Ended loop # {}, up to now took us {} seconds\"", ".", "format", "(", "subm_idx", ",", "duration", ")", ")", "\n", "# saving the stats to a file", "\n", "", "pickle", ".", "dump", "(", "sr_statistics", ",", "open", "(", "saving_res_path", "+", "\"submission_stats_102016_to_032017.p\"", ",", "\"wb\"", ")", ")", "\n", "duration", "=", "(", "datetime", ".", "datetime", ".", "now", "(", ")", "-", "start_time", ")", ".", "seconds", "\n", "print", "(", "\"Function 'calc_sr_statistics' has ended. Took us : {} seconds. \"", "\n", "\"Final dictionary size is {}\"", ".", "format", "(", "duration", ",", "len", "(", "sr_statistics", ")", ")", ")", "\n", "return", "sr_statistics", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.r_place_drawing_classifier.utils.save_results_to_csv": [[153, 188], ["os.path.isfile", "open", "open.close", "csv.DictWriter", "csv.DictWriter.writerow", "csv.DictWriter.writeheader", "datetime.datetime.now", "len"], "function", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_bert.bert_service.BertService.close"], ["", "def", "save_results_to_csv", "(", "results_file", ",", "start_time", ",", "objects_amount", ",", "config_dict", ",", "results", ")", ":", "\n", "    ", "\"\"\"\n    given inputs regarding a final run results - write these results into a file\n    :param results_file: str\n        file of the csv where results should be placed\n    :param start_time: datetime\n        time when the current result run started\n    :param objects_amount: int\n        amount of objects in the run was based on, usually it is between 1000-2500\n    :param config_dict: dict\n        dictionary holding all the configuration of the run, the one we get as input json\n    :param results: dict\n        dictionary with all results. Currently it should contain the following keys: 'accuracy', 'precision', 'recall'\n    :return: None\n        Nothing is returned, only saving to the file is being done\n    \"\"\"", "\n", "\n", "file_exists", "=", "os", ".", "path", ".", "isfile", "(", "results_file", ")", "\n", "rf", "=", "open", "(", "results_file", ",", "'a'", ",", "newline", "=", "''", ")", "\n", "with", "rf", "as", "output_file", ":", "\n", "        ", "dict_writer", "=", "csv", ".", "DictWriter", "(", "output_file", ",", "\n", "fieldnames", "=", "[", "'timestamp'", ",", "'start_time'", ",", "'machine'", ",", "'SRs_amount'", ",", "'cv_folds'", ",", "\n", "'configurations'", ",", "'accuracy'", ",", "'precision'", ",", "'recall'", ",", "'auc'", "]", ")", "\n", "# only in case the file doesn't exist, we'll add a header", "\n", "if", "not", "file_exists", ":", "\n", "            ", "dict_writer", ".", "writeheader", "(", ")", "\n", "", "try", ":", "\n", "            ", "host_name", "=", "os", ".", "environ", "[", "'HOSTNAME'", "]", "if", "sys", ".", "platform", "==", "'linux'", "else", "os", ".", "environ", "[", "'COMPUTERNAME'", "]", "\n", "", "except", "KeyError", ":", "\n", "            ", "host_name", "=", "'pycharm with this ssh: '", "+", "os", ".", "environ", "[", "'SSH_CONNECTION'", "]", "\n", "", "dict_writer", ".", "writerow", "(", "{", "'timestamp'", ":", "datetime", ".", "datetime", ".", "now", "(", ")", ",", "'start_time'", ":", "start_time", ",", "\n", "'SRs_amount'", ":", "objects_amount", ",", "'machine'", ":", "host_name", ",", "'cv_folds'", ":", "len", "(", "results", "[", "'accuracy'", "]", ")", ",", "\n", "'configurations'", ":", "config_dict", ",", "'accuracy'", ":", "results", "[", "'accuracy'", "]", ",", "\n", "'precision'", ":", "results", "[", "'precision'", "]", ",", "'recall'", ":", "results", "[", "'recall'", "]", ",", "'auc'", ":", "results", "[", "'auc'", "]", "}", ")", "\n", "", "rf", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.r_place_drawing_classifier.utils.examine_word": [[190, 242], ["os.getcwd", "datetime.datetime.now", "print", "os.path.exists", "open", "text_file.write", "print", "tokenizer", "tokenizer", "set", "text_file.write", "datetime.datetime.now", "str"], "function", ["None"], ["", "def", "examine_word", "(", "sr_object", ",", "regex_required", ",", "tokenizer", ",", "saving_file", "=", "os", ".", "getcwd", "(", ")", ")", ":", "\n", "    ", "\"\"\"\n    analysis function to see where a specific word is being used in the submissions corpus. This is useful in order to\n    see how different communities/users use different words in Reddit\n\n    :param sr_object: SubReddit object\n        an object (which can be seen in sr_classifier.sub_reddit) containing information about a community\n    :param regex_required: str\n        the regex we want to examine - can be a single word/bi-gram or any other regex\n    :param tokenizer: tokenizer object\n        function which can be used to tokenize the data\n        Example how it can be defined:\n        >>>submission_dp_obj = RedditDataPrep(is_submission_data=True, remove_stop_words=False, most_have_regex=None)\n        >>>reddit_tokenizer = submission_dp_obj.tokenize_text\n    :param saving_file: str\n        the full path to the file (including its name) where results should be saved in\n    :return: nothing\n    \"\"\"", "\n", "start_time", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "print", "(", "\"examine_word function has started\"", ")", "\n", "tot_cnt", "=", "0", "\n", "if", "sys", ".", "platform", "==", "'linux'", ":", "\n", "        ", "explicit_file_name", "=", "saving_file", "+", "'/'", "+", "'examine_word_res_regex_'", "+", "regex_required", "+", "'.txt'", "\n", "", "else", ":", "\n", "        ", "explicit_file_name", "=", "saving_file", "+", "'\\\\'", "+", "'examine_word_res_regex_'", "+", "regex_required", "+", "'.txt'", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "explicit_file_name", ")", ":", "\n", "        ", "append_write", "=", "'a'", "# append if already exists", "\n", "", "else", ":", "\n", "        ", "append_write", "=", "'w'", "# make a new file if not", "\n", "", "with", "open", "(", "explicit_file_name", ",", "append_write", ",", "encoding", "=", "\"utf-8\"", ")", "as", "text_file", ":", "\n", "        ", "text_file", ".", "write", "(", "\"\\n\\nHere are the relevant sentences with the regex {} in SR named {}. This sr is labeled as: \"", "\n", "\"{}\"", ".", "format", "(", "regex_required", ",", "sr_object", ".", "name", ",", "\n", "'not drawing'", "if", "sr_object", ".", "trying_to_draw", "==", "-", "1", "else", "'drawing'", ")", ")", "\n", "\n", "for", "subm", "in", "sr_object", ".", "submissions_as_list", ":", "\n", "            ", "normalized_text", "=", "[", "]", "\n", "try", ":", "\n", "                ", "tokenized_txt_title", "=", "tokenizer", "(", "subm", "[", "1", "]", ")", "\n", "normalized_text", "+=", "tokenized_txt_title", "\n", "", "except", "TypeError", ":", "\n", "                ", "pass", "\n", "", "try", ":", "\n", "                ", "tokenized_txt_selftext", "=", "tokenizer", "(", "subm", "[", "2", "]", ")", "\n", "normalized_text", "+=", "tokenized_txt_selftext", "\n", "", "except", "TypeError", ":", "\n", "                ", "pass", "\n", "", "if", "regex_required", "in", "set", "(", "normalized_text", ")", ":", "\n", "                ", "text_file", ".", "write", "(", "'\\n'", "+", "str", "(", "subm", ")", ")", "\n", "tot_cnt", "+=", "1", "\n", "", "", "duration", "=", "(", "datetime", ".", "datetime", ".", "now", "(", ")", "-", "start_time", ")", ".", "seconds", "\n", "print", "(", "\"examine_word has ended, took us {} seconds.\"", "\n", "\"Total of {} rows were written to a text file\"", ".", "format", "(", "duration", ",", "tot_cnt", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.r_place_drawing_classifier.utils.remove_huge_srs": [[244, 265], ["srs_summary.sort", "int", "sum", "set", "print", "len", "enumerate", "len", "len"], "function", ["None"], ["", "", "def", "remove_huge_srs", "(", "sr_objects", ",", "quantile", "=", "0.01", ")", ":", "\n", "    ", "\"\"\"\n    removed the largest sr objects - in order not to handle big srs with lots of submissions/comments\n    :param sr_objects: list\n        list of sr objects\n    :param quantile: float, default=0.01\n        the % of srs required to remove\n    :return: list\n        the list of srs after removing the huge ones from it\n    \"\"\"", "\n", "srs_summary", "=", "[", "(", "idx", ",", "cur_sr", ".", "name", ",", "cur_sr", ".", "trying_to_draw", ",", "len", "(", "cur_sr", ".", "submissions_as_list", ")", ")", "\n", "for", "idx", ",", "cur_sr", "in", "enumerate", "(", "sr_objects", ")", "]", "\n", "srs_summary", ".", "sort", "(", "key", "=", "lambda", "tup", ":", "tup", "[", "3", "]", ",", "reverse", "=", "True", ")", "# sorts in place according to the # of submissions", "\n", "amount_of_srs_to_remove", "=", "int", "(", "len", "(", "sr_objects", ")", "*", "quantile", ")", "\n", "srs_to_remove_summary", "=", "srs_summary", "[", "0", ":", "amount_of_srs_to_remove", "]", "\n", "drawing_removed_srs", "=", "sum", "(", "[", "sr", "[", "2", "]", "for", "sr", "in", "srs_to_remove_summary", "if", "sr", "[", "2", "]", "==", "1", "]", ")", "\n", "srs_to_remove", "=", "set", "(", "[", "sr", "[", "1", "]", "for", "sr", "in", "srs_to_remove_summary", "]", ")", "\n", "returned_list", "=", "[", "sr", "for", "sr", "in", "sr_objects", "if", "sr", ".", "name", "not", "in", "srs_to_remove", "]", "\n", "print", "(", "\"remove_huge_srs function has ended, {} srs have been removed,\"", "\n", "\" {} out of them are from class 1 (drawing)\"", ".", "format", "(", "len", "(", "srs_to_remove", ")", ",", "drawing_removed_srs", ")", ")", "\n", "return", "returned_list", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.r_place_drawing_classifier.utils.check_input_validity": [[267, 293], ["os.path.join", "os.path.exists", "IOError", "print", "str", "random.randint"], "function", ["None"], ["", "def", "check_input_validity", "(", "config_dict", ",", "machine", ")", ":", "\n", "    ", "\"\"\"\n    Set of tests to make sure the input configuration we have is valid.\n    If not - we either fix it or change it to be valid\n    :param config_dict: dict\n        the configuration dictionary input. This is read as a json in the main file\n    :param machine: str\n        the machine (computer) we work on \n    :return: dict\n        the updated config_dict (dictionary) or an error\n    \"\"\"", "\n", "# making sure the model we get as input is valid", "\n", "if", "config_dict", "[", "'class_model'", "]", "[", "'model_type'", "]", "not", "in", "[", "'clf_meta_only'", ",", "'bow'", ",", "'mlp'", ",", "\n", "'single_lstm'", ",", "'parallel_lstm'", ",", "'cnn_max_pooling'", "]", ":", "\n", "        ", "raise", "IOError", "(", "'Model name input is invalid. Must be one out of the following: '", "\n", "'[\"clf_meta_only\", \"bow\", \"mlp\", \"single_lstm\", \"parallel_lstm\", \"cnn_max_pooling]. '", "\n", "'Please fix and try again'", ")", "\n", "\n", "# checking if a folder with the model name exists already - if it exists, we will change name of the model", "\n", "", "cur_folder_name", "=", "os", ".", "path", ".", "join", "(", "config_dict", "[", "'results_dir'", "]", "[", "machine", "]", ",", "\"model_\"", "+", "config_dict", "[", "'model_version'", "]", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "cur_folder_name", ")", ":", "\n", "        ", "new_model_version", "=", "config_dict", "[", "'model_version'", "]", "+", "'.'", "+", "str", "(", "random", ".", "randint", "(", "1", ",", "100000000", ")", ")", "\n", "config_dict", "[", "'model_version'", "]", "=", "new_model_version", "\n", "print", "(", "\"model_version has been changed to {}, \"", "\n", "\"since the original model_version was used in the past\"", ".", "format", "(", "new_model_version", ")", ")", "\n", "", "return", "config_dict", "\n", "", ""]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_bert.bert_service.BertService.__init__": [[18, 30], ["bert_serving.server.BertServer", "bert_service.BertService.server.start", "time.sleep", "bert_serving.client.BertClient", "len", "bert_service.BertService.bc.encode"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "server_args", ",", "config_dict", ")", ":", "\n", "        ", "self", ".", "servers_args", "=", "server_args", "\n", "self", ".", "config_dict", "=", "config_dict", "\n", "self", ".", "server", "=", "BertServer", "(", "self", ".", "servers_args", ")", "\n", "self", ".", "server", ".", "start", "(", ")", "\n", "time", ".", "sleep", "(", "120", ")", "\n", "self", ".", "server", ".", "max_seq_len", "=", "1000", "\n", "# allowing the server to wake up and be ready", "\n", "self", ".", "bc", "=", "BertClient", "(", "timeout", "=", "config_dict", "[", "'bert_config'", "]", "[", "'timeout_in_milliseconds'", "]", ",", "# 2 minutes time out", "\n", "port", "=", "config_dict", "[", "'bert_config'", "]", "[", "'bert_server_params'", "]", "[", "'port'", "]", ",", "\n", "port_out", "=", "config_dict", "[", "'bert_config'", "]", "[", "'bert_server_params'", "]", "[", "'port_out'", "]", ")", "\n", "self", ".", "bert_model_dim", "=", "len", "(", "self", ".", "bc", ".", "encode", "(", "[", "'hello world'", "]", ")", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_bert.bert_service.BertService.get_sr_representation": [[31, 125], ["eval", "sr_obj.subsample_submissions_data", "bert_service.BertService._sentences_prep", "range", "numpy.vstack", "numpy.multiply", "numpy.mean", "numpy.zeros", "enumerate", "bert_service.BertService.bc.encode", "all_sentences_embed.append", "tuple", "print", "type", "type", "print", "len", "nlp", "nlp", "bert_service.BertService.bc.encode().mean", "nlp", "bert_service.BertService.bc.encode().mean", "bert_serving.server.BertServer", "bert_serving.server.BertServer.start", "time.sleep", "int", "bert_service.BertService.bc.encode", "nlp", "bert_service.BertService.bc.encode().mean", "bert_service.BertService.server.is_ready.is_set", "bert_service.BertService.server.is_alive", "bert_service.BertService.bc.encode", "bert_service.BertService.bc.encode", "itertools.chain", "sen.text.isspace", "sen.text.isspace", "sen.text.isspace"], "methods", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_bert.bert_service.BertService._sentences_prep"], ["", "def", "get_sr_representation", "(", "self", ",", "sr_obj", ",", "use_sr_obj_tokens", "=", "True", ")", ":", "\n", "        ", "request_max_size", "=", "self", ".", "config_dict", "[", "'bert_config'", "]", "[", "'request_max_size'", "]", "\n", "# looping over all files found in the directory", "\n", "# sorting data according to some logic", "\n", "if", "eval", "(", "self", ".", "config_dict", "[", "\"submissions_sampling\"", "]", "[", "\"should_sample\"", "]", ")", ":", "\n", "            ", "sampling_dict", "=", "self", ".", "config_dict", "[", "'submissions_sampling'", "]", "\n", "subsample_res", "=", "sr_obj", ".", "subsample_submissions_data", "(", "subsample_logic", "=", "sampling_dict", "[", "'sampling_logic'", "]", ",", "\n", "percentage", "=", "sampling_dict", "[", "'percentage'", "]", ",", "\n", "maximum_submissions", "=", "sampling_dict", "[", "'max_subm'", "]", ",", "\n", "seed", "=", "self", ".", "config_dict", "[", "'random_seed'", "]", ",", "apply_to_tokens", "=", "True", ")", "\n", "if", "subsample_res", "<", "0", ":", "\n", "                ", "return", "None", "\n", "\n", "", "", "if", "use_sr_obj_tokens", ":", "\n", "# since we want to call BERT as a service with a flat list of sentences, we need to know how", "\n", "# many sentences each submissions holds", "\n", "            ", "all_sentences", ",", "sent_weight", "=", "self", ".", "_sentences_prep", "(", "sentences_list", "=", "sr_obj", ".", "submissions_as_tokens", ",", "return_weights", "=", "True", ")", "\n", "'''\n            subm_sent_amount = [len(sat) for sat in sr_obj.submissions_as_tokens for sent in sat ]\n            # now we decide the weight of each sentence, according to the amount of senteces \"his submission\" hols\n            # this is useful in order to normalize the weight of each submission (so submissions with lots of sentences\n            # will not get too much weight\n            sent_weight = [[1/ssa] * ssa for ssa in subm_sent_amount if ssa > 0]\n            sent_weight = np.array([item for sublist in sent_weight for item in sublist if item!=[]])\n            all_sentences = [sent for submission in sr_obj.submissions_as_tokens for sent in submission if sent != []]\n            '''", "\n", "# calling the bert module along with breaking all sentences into chunks", "\n", "all_sentences_embed", "=", "[", "]", "\n", "loop_chunks", "=", "(", "len", "(", "all_sentences", ")", "-", "1", ")", "//", "request_max_size", "+", "1", "\n", "for", "i", "in", "range", "(", "loop_chunks", ")", ":", "\n", "                ", "cur_subm_embd", "=", "self", ".", "bc", ".", "encode", "(", "all_sentences", "[", "i", "*", "request_max_size", ":", "(", "i", "+", "1", ")", "*", "request_max_size", "]", ",", "is_tokenized", "=", "True", ")", "\n", "all_sentences_embed", ".", "append", "(", "cur_subm_embd", ")", "\n", "# concatenating all the embeddings into one big array", "\n", "", "full_subm_embd", "=", "np", ".", "vstack", "(", "tuple", "(", "all_sentences_embed", ")", ")", "\n", "# multiple the matrix by the weight (one for each sentence)", "\n", "full_subm_embd", "=", "np", ".", "multiply", "(", "full_subm_embd", ",", "sent_weight", "[", ":", ",", "np", ".", "newaxis", "]", ")", "\n", "# averaging over the columns since we want to have fixed representation per SR (and not per sentence)", "\n", "return", "np", ".", "mean", "(", "full_subm_embd", ",", "axis", "=", "0", ")", "\n", "\n", "", "else", ":", "\n", "            ", "cur_sr_subm_embd_avg", "=", "np", ".", "zeros", "(", "self", ".", "bert_model_dim", ")", "\n", "valid_subm_counter", "=", "0", "\n", "# looping over each submission in the SR", "\n", "for", "subm_idx", ",", "cur_subm", "in", "enumerate", "(", "sr_obj", ".", "submissions_as_list", ")", ":", "\n", "                ", "header_is_valid", "=", "type", "(", "cur_subm", "[", "1", "]", ")", "is", "str", "\n", "selftext_is_valid", "=", "type", "(", "cur_subm", "[", "2", "]", ")", "is", "str", "\n", "# case both header and selftext are not valid - skipping the row", "\n", "if", "not", "header_is_valid", "and", "not", "selftext_is_valid", ":", "\n", "                    ", "continue", "\n", "", "got_response", "=", "False", "\n", "num_tries", "=", "0", "\n", "# looping till we get an answer", "\n", "while", "not", "got_response", "and", "num_tries", "<", "10", ":", "\n", "                    ", "try", ":", "\n", "# both header and selftext are valid", "\n", "                        ", "if", "header_is_valid", "and", "selftext_is_valid", ":", "\n", "                            ", "cur_header", "=", "nlp", "(", "cur_subm", "[", "1", "]", ")", "\n", "cur_selftext", "=", "nlp", "(", "cur_subm", "[", "2", "]", ")", "\n", "cur_subm_embd", "=", "self", ".", "bc", ".", "encode", "(", "[", "sen", ".", "text", "for", "sen", "in", "chain", "(", "cur_header", ".", "sents", ",", "cur_selftext", ".", "sents", ")", "\n", "if", "sen", ".", "text", "and", "not", "sen", ".", "text", ".", "isspace", "(", ")", "]", ")", ".", "mean", "(", "axis", "=", "0", ")", "\n", "got_response", "=", "True", "\n", "# only header is valid", "\n", "", "elif", "header_is_valid", ":", "\n", "                            ", "cur_header", "=", "nlp", "(", "cur_subm", "[", "1", "]", ")", "\n", "cur_subm_embd", "=", "self", ".", "bc", ".", "encode", "(", "[", "sen", ".", "text", "for", "sen", "in", "cur_header", ".", "sents", "\n", "if", "sen", ".", "text", "and", "not", "sen", ".", "text", ".", "isspace", "(", ")", "]", ")", ".", "mean", "(", "axis", "=", "0", ")", "\n", "got_response", "=", "True", "\n", "# only selftext is valid", "\n", "", "elif", "selftext_is_valid", ":", "\n", "                            ", "cur_selftext", "=", "nlp", "(", "cur_subm", "[", "2", "]", ")", "\n", "cur_subm_embd", "=", "self", ".", "bc", ".", "encode", "(", "[", "sen", ".", "text", "for", "sen", "in", "cur_selftext", ".", "sents", "\n", "if", "sen", ".", "text", "and", "not", "sen", ".", "text", ".", "isspace", "(", ")", "]", ")", ".", "mean", "(", "axis", "=", "0", ")", "\n", "got_response", "=", "True", "\n", "", "", "except", "TimeoutError", ":", "\n", "# restarting the server, only in case is is not responsive", "\n", "                        ", "if", "not", "self", ".", "server", ".", "is_ready", ".", "is_set", "(", ")", "or", "not", "self", ".", "server", ".", "is_alive", "(", ")", ":", "\n", "                            ", "server", "=", "BertServer", "(", "self", ".", "args", ")", "\n", "server", ".", "start", "(", ")", "\n", "time", ".", "sleep", "(", "180", ")", "# allowing the server to wake up and be ready", "\n", "", "else", ":", "\n", "                            ", "self", ".", "bc", ".", "timeout", "=", "int", "(", "self", ".", "bc", ".", "timeout", "*", "1.5", ")", "\n", "# case we have tried 3 times and still no response from the server, we give up and print the error", "\n", "", "", "", "if", "not", "got_response", "and", "num_tries", ">=", "1", ":", "\n", "                    ", "print", "(", "\"While handling sr {} with submission index {}, \"", "\n", "\"the BERT server failed to provide a response\"", ".", "format", "(", "sr_obj", ".", "name", ",", "subm_idx", ")", ")", "\n", "\n", "", "cur_sr_subm_embd_avg", "+=", "cur_subm_embd", "\n", "valid_subm_counter", "+=", "1", "\n", "self", ".", "bc", ".", "timeout", "=", "120000", "\n", "print", "(", "\"Passed through {} submission\"", ".", "format", "(", "subm_idx", ")", ")", "\n", "# since we need the avg, we will divide the vector by the amount of elements we have added,", "\n", "# and then will return this information", "\n", "", "cur_sr_subm_embd_avg", "/=", "valid_subm_counter", "\n", "return", "cur_sr_subm_embd_avg", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_bert.bert_service.BertService.close": [[126, 128], ["bert_service.BertService.server.close"], "methods", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_bert.bert_service.BertService.close"], ["", "", "def", "close", "(", "self", ")", ":", "\n", "        ", "self", ".", "server", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_bert.bert_service.BertService._sentences_prep": [[129, 151], ["numpy.array", "sent_amount_in_each_batch.append", "all_sentences.append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_sentences_prep", "(", "sentences_list", ",", "return_weights", "=", "True", ")", ":", "\n", "        ", "sent_amount_in_each_batch", "=", "[", "]", "\n", "all_sentences", "=", "[", "]", "\n", "for", "current_batch", "in", "sentences_list", ":", "\n", "            ", "valid_sentences", "=", "0", "\n", "for", "sentence", "in", "current_batch", ":", "\n", "                ", "if", "sentence", ":", "\n", "                    ", "valid_sentences", "+=", "1", "\n", "all_sentences", ".", "append", "(", "sentence", ")", "\n", "", "", "if", "valid_sentences", ":", "\n", "                ", "sent_amount_in_each_batch", ".", "append", "(", "valid_sentences", ")", "\n", "# now we decide the weight of each sentence, according to the amount of senteces \"his submission\" hols", "\n", "# this is useful in order to normalize the weight of each submission (so submissions with lots of sentences", "\n", "# will not get too much weight", "\n", "", "", "sent_weight", "=", "[", "[", "1", "/", "ssb", "]", "*", "ssb", "for", "ssb", "in", "sent_amount_in_each_batch", "]", "\n", "sent_weight", "=", "np", ".", "array", "(", "[", "item", "for", "sublist", "in", "sent_weight", "for", "item", "in", "sublist", "]", ")", "\n", "\n", "if", "return_weights", ":", "\n", "            ", "return", "all_sentences", ",", "sent_weight", "\n", "", "else", ":", "\n", "            ", "return", "all_sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_bert.bert_nn_model.MyModule.__init__": [[12, 19], ["super().__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.single_lstm.SinglelLstm.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_dim", ",", "hid_size", "=", "100", ",", "dropout_perc", "=", "0.5", ",", "nonlin", "=", "F", ".", "relu", ")", ":", "\n", "        ", "super", "(", "MyModule", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense0", "=", "torch", ".", "nn", ".", "Linear", "(", "input_dim", ",", "hid_size", ")", "\n", "self", ".", "nonlin", "=", "nonlin", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout_perc", ")", "\n", "self", ".", "dense1", "=", "torch", ".", "nn", ".", "Linear", "(", "hid_size", ",", "10", ")", "\n", "self", ".", "output", "=", "torch", ".", "nn", ".", "Linear", "(", "10", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_bert.bert_nn_model.MyModule.forward": [[20, 26], ["bert_nn_model.MyModule.nonlin", "bert_nn_model.MyModule.dropout", "torch.relu", "torch.relu", "torch.softmax", "torch.softmax", "bert_nn_model.MyModule.dense0", "bert_nn_model.MyModule.dense1", "bert_nn_model.MyModule.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "X", ",", "**", "kwargs", ")", ":", "\n", "        ", "X", "=", "self", ".", "nonlin", "(", "self", ".", "dense0", "(", "X", ")", ")", "\n", "X", "=", "self", ".", "dropout", "(", "X", ")", "\n", "X", "=", "F", ".", "relu", "(", "self", ".", "dense1", "(", "X", ")", ")", "\n", "X", "=", "F", ".", "softmax", "(", "input", "=", "self", ".", "output", "(", "X", ")", ")", "\n", "return", "X", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_bert.bert_nn_model.BertNNModel.__init__": [[29, 36], ["neural_net.nn_classifier.NNClassifier.__init__", "bert_nn_model.MyModule"], "methods", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.single_lstm.SinglelLstm.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_dim", ",", "model", ",", "eval_measures", ",", "epochs", "=", "10", ",", "hid_size", "=", "100", ",", "dropout_perc", "=", "0.5", ",", "\n", "nonlin", "=", "F", ".", "relu", ",", "layers_amount", "=", "2", ",", "use_meta_features", "=", "True", ",", "seed", "=", "1984", ")", ":", "\n", "        ", "super", "(", "BertNNModel", ",", "self", ")", ".", "__init__", "(", "model", "=", "model", ",", "eval_measures", "=", "eval_measures", ",", "hid_size", "=", "hid_size", ",", "\n", "epochs", "=", "epochs", ",", "use_meta_features", "=", "use_meta_features", ",", "seed", "=", "seed", ")", "\n", "self", ".", "input_dim", "=", "input_dim", "\n", "self", ".", "my_module", "=", "MyModule", "(", "input_dim", "=", "self", ".", "input_dim", ",", "hid_size", "=", "hid_size", ",", "dropout_perc", "=", "dropout_perc", ")", "\n", "self", ".", "layers_amount", "=", "layers_amount", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_bert.bert_nn_model.BertNNModel.build_model": [[37, 41], ["skorch.NeuralNetClassifier"], "methods", ["None"], ["", "def", "build_model", "(", "self", ",", "lr", "=", "0.1", ",", "suffle_train", "=", "True", ")", ":", "\n", "        ", "net", "=", "NeuralNetClassifier", "(", "self", ".", "my_module", ",", "max_epochs", "=", "self", ".", "epochs", ",", "lr", "=", "lr", ",", "\n", "iterator_train__shuffle", "=", "suffle_train", ")", "\n", "return", "net", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_bert.utils.build_modeling_df": [[9, 29], ["pandas.DataFrame.from_dict", "pandas.DataFrame.from_dict", "pandas.merge", "sklearn.preprocessing.StandardScaler", "pandas.DataFrame", "pandas.DataFrame", "pd.merge.fillna", "sklearn.preprocessing.StandardScaler.fit_transform", "explanatory_features.keys"], "function", ["None"], ["import", "collections", "\n", "import", "pickle", "\n", "import", "sys", "\n", "import", "csv", "\n", "import", "random", "\n", "\n", "\n", "def", "get_submissions_subset", "(", "files_path", ",", "srs_to_include", ",", "start_month", "=", "'2016-10'", ",", "end_month", "=", "'2017-03'", ",", "\n", "min_utc", "=", "None", ",", "max_utc", "=", "'2017-03-29 00:00:00'", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.mydatasets.TarDataset.download_or_unzip": [[23, 35], ["os.path.join", "os.path.join", "os.path.isdir", "os.path.join", "os.path.isfile", "print", "urllib.request.urlretrieve", "tarfile.open", "print", "tfile.extractall"], "methods", ["None"], ["@", "classmethod", "\n", "def", "download_or_unzip", "(", "cls", ",", "root", ")", ":", "\n", "        ", "path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "cls", ".", "dirname", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "path", ")", ":", "\n", "            ", "tpath", "=", "os", ".", "path", ".", "join", "(", "root", ",", "cls", ".", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "tpath", ")", ":", "\n", "                ", "print", "(", "'downloading'", ")", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "cls", ".", "url", ",", "tpath", ")", "\n", "", "with", "tarfile", ".", "open", "(", "tpath", ",", "'r'", ")", "as", "tfile", ":", "\n", "                ", "print", "(", "'extracting'", ")", "\n", "tfile", ".", "extractall", "(", "root", ")", "\n", "", "", "return", "os", ".", "path", ".", "join", "(", "path", ",", "''", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.mydatasets.MR.sort_key": [[43, 46], ["len"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "sort_key", "(", "ex", ")", ":", "\n", "        ", "return", "len", "(", "ex", ".", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.mydatasets.MR.__init__": [[47, 91], ["torchtext.data.Pipeline", "torchtext.data.Dataset.__init__", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub.strip", "open", "open", "os.path.join", "torchtext.data.Example.fromlist", "os.path.join", "torchtext.data.Example.fromlist"], "methods", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.single_lstm.SinglelLstm.__init__"], ["", "def", "__init__", "(", "self", ",", "text_field", ",", "label_field", ",", "path", "=", "None", ",", "examples", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Create an MR dataset instance given a path and fields.\n\n        Arguments:\n            text_field: The field that will be used for text data.\n            label_field: The field that will be used for label data.\n            path: Path to the data file.\n            examples: The examples contain all the data.\n            Remaining keyword arguments: Passed to the constructor of\n                data.Dataset.\n        \"\"\"", "\n", "def", "clean_str", "(", "string", ")", ":", "\n", "            ", "\"\"\"\n            Tokenization/string cleaning for all datasets except for SST.\n            Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n            \"\"\"", "\n", "string", "=", "re", ".", "sub", "(", "r\"[^A-Za-z0-9(),!?\\'\\`]\"", ",", "\" \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'s\"", ",", "\" \\'s\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'ve\"", ",", "\" \\'ve\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"n\\'t\"", ",", "\" n\\'t\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'re\"", ",", "\" \\'re\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'d\"", ",", "\" \\'d\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'ll\"", ",", "\" \\'ll\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\",\"", ",", "\" , \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"!\"", ",", "\" ! \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\(\"", ",", "\" \\( \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\)\"", ",", "\" \\) \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\?\"", ",", "\" \\? \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\s{2,}\"", ",", "\" \"", ",", "string", ")", "\n", "return", "string", ".", "strip", "(", ")", "\n", "\n", "", "text_field", ".", "preprocessing", "=", "data", ".", "Pipeline", "(", "clean_str", ")", "\n", "fields", "=", "[", "(", "'text'", ",", "text_field", ")", ",", "(", "'label'", ",", "label_field", ")", "]", "\n", "\n", "if", "examples", "is", "None", ":", "\n", "            ", "path", "=", "self", ".", "dirname", "if", "path", "is", "None", "else", "path", "\n", "examples", "=", "[", "]", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "path", ",", "'rt-polarity.neg'", ")", ",", "errors", "=", "'ignore'", ")", "as", "f", ":", "\n", "                ", "examples", "+=", "[", "\n", "data", ".", "Example", ".", "fromlist", "(", "[", "line", ",", "'negative'", "]", ",", "fields", ")", "for", "line", "in", "f", "]", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "path", ",", "'rt-polarity.pos'", ")", ",", "errors", "=", "'ignore'", ")", "as", "f", ":", "\n", "                ", "examples", "+=", "[", "\n", "data", ".", "Example", ".", "fromlist", "(", "[", "line", ",", "'positive'", "]", ",", "fields", ")", "for", "line", "in", "f", "]", "\n", "", "", "super", "(", "MR", ",", "self", ")", ".", "__init__", "(", "examples", ",", "fields", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.mydatasets.MR.splits": [[92, 115], ["cls.download_or_unzip", "cls", "random.shuffle", "int", "cls", "cls", "len"], "methods", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.mydatasets.TarDataset.download_or_unzip"], ["", "@", "classmethod", "\n", "def", "splits", "(", "cls", ",", "text_field", ",", "label_field", ",", "dev_ratio", "=", ".1", ",", "shuffle", "=", "True", ",", "root", "=", "'.'", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Create dataset objects for splits of the MR dataset.\n\n        Arguments:\n            text_field: The field that will be used for the sentence.\n            label_field: The field that will be used for label data.\n            dev_ratio: The ratio that will be used to get split validation dataset.\n            shuffle: Whether to shuffle the data before split.\n            root: The root directory that the dataset's zip archive will be\n                expanded into; therefore the directory in whose trees\n                subdirectory the data files will be stored.\n            train: The filename of the train data. Default: 'train.txt'.\n            Remaining keyword arguments: Passed to the splits method of\n                Dataset.\n        \"\"\"", "\n", "path", "=", "cls", ".", "download_or_unzip", "(", "root", ")", "\n", "examples", "=", "cls", "(", "text_field", ",", "label_field", ",", "path", "=", "path", ",", "**", "kwargs", ")", ".", "examples", "\n", "if", "shuffle", ":", "random", ".", "shuffle", "(", "examples", ")", "\n", "dev_index", "=", "-", "1", "*", "int", "(", "dev_ratio", "*", "len", "(", "examples", ")", ")", "\n", "\n", "return", "(", "cls", "(", "text_field", ",", "label_field", ",", "examples", "=", "examples", "[", ":", "dev_index", "]", ")", ",", "\n", "cls", "(", "text_field", ",", "label_field", ",", "examples", "=", "examples", "[", "dev_index", ":", "]", ")", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.train.train": [[15, 99], ["torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "max", "os.path.join", "datetime.datetime.now", "range", "best_model.load_state_dict", "train.evaluation", "model.cuda", "model.parameters", "set", "model.train", "enumerate", "print", "print", "torch.load", "torch.load", "torch.load", "r_place_drawing_classifier.pytorch_cnn.utils.submissions_separation", "gc.collect", "torch.optim.Adam.zero_grad", "model", "model.unsqueeze_", "torch.cross_entropy", "F.cross_entropy.backward", "torch.optim.Adam.step", "gc.collect", "model.eval", "train.evaluation", "os.path.join", "len", "set.add", "feature.data.t_", "target.data.sub_", "train.evaluation", "train.evaluation", "print", "datetime.datetime.now", "len", "eval", "train.save", "int", "r_place_drawing_classifier.pytorch_cnn.utils.submissions_separation.cuda", "target.cuda", "train.save", "eval", "print", "datetime.datetime.now", "str", "[].view", "target.size", "torch.max", "torch.max", "torch.max"], "function", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.train.evaluation", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.train.train", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.utils.submissions_separation", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.train.evaluation", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.train.evaluation", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.train.evaluation", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.train.save", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.train.save"], ["def", "train", "(", "train_data", ",", "dev_data", ",", "test_data", ",", "model", ",", "config_dict", ",", "fold", ")", ":", "\n", "    ", "if", "config_dict", "[", "'cuda'", "]", ":", "\n", "        ", "model", ".", "cuda", "(", ")", "\n", "", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "config_dict", "[", "'class_model'", "]", "[", "'nn_params'", "]", "[", "'lr'", "]", ")", "\n", "\n", "steps", "=", "0", "\n", "best_acc", "=", "0", "\n", "last_step", "=", "0", "\n", "min_sent_length", "=", "max", "(", "model", ".", "config_dict", "[", "'kernel_sizes'", "]", ")", "\n", "# building the saving path location according to info in the config file", "\n", "save_dir", "=", "os", ".", "path", ".", "join", "(", "config_dict", "[", "'results_dir'", "]", "[", "config_dict", "[", "'machine'", "]", "]", ",", "config_dict", "[", "'model_version'", "]", ")", "\n", "start_time", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "# setting the model to train mode (pytorch requires this)", "\n", "for", "epoch", "in", "range", "(", "1", ",", "config_dict", "[", "'class_model'", "]", "[", "'nn_params'", "]", "[", "'epochs'", "]", "+", "1", ")", ":", "\n", "        ", "corrects", "=", "0", "\n", "steps", "=", "0", "\n", "empty_text_srs", "=", "set", "(", ")", "\n", "# looping over each instance in the train data - we do online learning and not batch learning at all", "\n", "# note that the iterator of train data converts the text to int \"on the fly\" - so no text here, only ints", "\n", "model", ".", "train", "(", ")", "\n", "for", "example_idx", ",", "cur_example", "in", "enumerate", "(", "train_data", ")", ":", "\n", "            ", "feature", ",", "target", ",", "sr_idx", ",", "explanatory_meta_features", "=", "cur_example", ".", "text", ",", "cur_example", ".", "label", ",", "cur_example", ".", "sr_name", ",", "cur_example", ".", "meta_data", "\n", "sr_name", "=", "cur_example", ".", "dataset", ".", "fields", "[", "'sr_name'", "]", ".", "vocab", ".", "itos", "[", "int", "(", "sr_idx", "[", "0", "]", ")", "]", "\n", "if", "len", "(", "feature", ")", "==", "0", ":", "\n", "                ", "empty_text_srs", ".", "add", "(", "sr_name", ")", "\n", "continue", "\n", "# converting the meta-feature to be explicit None and not list of None (since the iterator of the loop", "\n", "# looks at the data as batches, it converts all to be list of values, but here it is only a None value)", "\n", "", "if", "explanatory_meta_features", "==", "[", "None", "]", ":", "\n", "                ", "explanatory_meta_features", "=", "None", "\n", "# pulling out the name of the SR in order to use it in the near future", "\n", "\n", "", "feature", ".", "data", ".", "t_", "(", ")", ",", "target", ".", "data", ".", "sub_", "(", "1", ")", "# batch first, index align", "\n", "# breaking the long sentence we created into the actual submissions it comes from", "\n", "feature_separated", "=", "submissions_separation", "(", "input_tensor", "=", "feature", ",", "\n", "separator_int", "=", "cur_example", ".", "dataset", ".", "fields", "[", "'text'", "]", ".", "vocab", ".", "stoi", "[", "'<sent_ends>'", "]", ",", "\n", "padding_int", "=", "cur_example", ".", "dataset", ".", "fields", "[", "'text'", "]", ".", "vocab", ".", "stoi", "[", "'<pad>'", "]", ")", "\n", "del", "feature", "\n", "gc", ".", "collect", "(", ")", "\n", "if", "config_dict", "[", "'cuda'", "]", ":", "\n", "                ", "feature_separated", ",", "explanatory_meta_features", ",", "target", "=", "feature_separated", ".", "cuda", "(", ")", ",", "explanatory_meta_features", ",", "target", ".", "cuda", "(", ")", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "logit", "=", "model", "(", "x", "=", "feature_separated", ",", "explanatory_meta_features", "=", "explanatory_meta_features", ")", "\n", "logit", ".", "unsqueeze_", "(", "0", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "logit", ",", "target", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "del", "feature_separated", "\n", "gc", ".", "collect", "(", ")", "\n", "steps", "+=", "1", "\n", "corrects", "+=", "(", "torch", ".", "max", "(", "logit", ",", "1", ")", "[", "1", "]", ".", "view", "(", "target", ".", "size", "(", ")", ")", ".", "data", "==", "target", ".", "data", ")", ".", "sum", "(", ")", "\n", "if", "example_idx", "%", "500", "==", "0", "and", "example_idx", ">", "0", ":", "\n", "                ", "dev_acc", "=", "evaluation", "(", "dev_data", ",", "model", ",", "config_dict", ",", "dataset", "=", "'dev'", ",", "return_proba", "=", "False", ")", "\n", "test_acc", "=", "evaluation", "(", "test_data", ",", "model", ",", "config_dict", ",", "dataset", "=", "'test'", ",", "return_proba", "=", "False", ")", "\n", "duration", "=", "(", "datetime", ".", "datetime", ".", "now", "(", ")", "-", "start_time", ")", ".", "seconds", "\n", "print", "(", "\"Finished handling {} objects in {} sec.\"", "\n", "\"Dev accuracy: {:.4f}. Test accuracy: {:.4f}\"", ".", "format", "(", "example_idx", ",", "duration", ",", "dev_acc", ",", "test_acc", ")", ")", "\n", "", "", "accuracy", "=", "100.0", "*", "corrects", "/", "steps", "\n", "duration", "=", "(", "datetime", ".", "datetime", ".", "now", "(", ")", "-", "start_time", ")", ".", "seconds", "\n", "print", "(", "\"Along training phase, {} srs weren't included (empty input).\"", ".", "format", "(", "len", "(", "empty_text_srs", ")", ")", ")", "\n", "print", "(", "'\\rEnd of epoch [{}] - train accuracy: {:.4f}%({}/{}). '", "\n", "'Took us up to now {} sec'", ".", "format", "(", "epoch", ",", "accuracy", ",", "corrects", ",", "steps", ",", "duration", ")", ",", "flush", "=", "True", ")", "\n", "if", "epoch", "%", "config_dict", "[", "'mode'", "]", "[", "'test_interval'", "]", "==", "0", ":", "\n", "            ", "model", ".", "eval", "(", ")", "\n", "dev_acc", "=", "evaluation", "(", "dev_data", ",", "model", ",", "config_dict", ",", "dataset", "=", "'dev'", ",", "return_proba", "=", "False", ")", "\n", "if", "dev_acc", ">", "best_acc", ":", "\n", "                ", "best_acc", "=", "dev_acc", "\n", "last_step", "=", "steps", "\n", "if", "eval", "(", "config_dict", "[", "'saving_options'", "]", "[", "'save_best'", "]", ")", ":", "\n", "                    ", "save", "(", "model", ",", "save_dir", ",", "'best'", ",", "fold", ")", "\n", "", "", "else", ":", "\n", "                ", "if", "eval", "(", "config_dict", "[", "'class_model'", "]", "[", "'nn_params'", "]", "[", "'early_stopping'", "]", ")", "and", "steps", "-", "last_step", ">=", "config_dict", "[", "'class_model'", "]", "[", "'nn_params'", "]", "[", "'early_stop_steps'", "]", ":", "\n", "                    ", "print", "(", "'early stop by {} steps.'", ".", "format", "(", "config_dict", "[", "'class_model'", "]", "[", "'nn_params'", "]", "[", "'early_stop_steps'", "]", ")", ")", "\n", "", "", "", "elif", "epoch", "%", "config_dict", "[", "'saving_options'", "]", "[", "'save_interval'", "]", "==", "0", ":", "\n", "            ", "save", "(", "model", ",", "save_dir", ",", "'snapshot'", ",", "fold", ")", "\n", "\n", "# at the end of all epochs, we will evaluate the test set based on the best model we have", "\n", "", "", "best_model", "=", "model", "\n", "best_model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "save_dir", ",", "'best_fold_'", "+", "str", "(", "fold", ")", "+", "\".pt\"", ")", ")", ")", "\n", "test_proba", "=", "evaluation", "(", "data_iter", "=", "test_data", ",", "model", "=", "best_model", ",", "config_dict", "=", "config_dict", ",", "\n", "dataset", "=", "'test'", ",", "return_proba", "=", "True", ")", "\n", "return", "test_proba", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.train.evaluation": [[101, 150], ["model.eval", "max", "dict", "set", "enumerate", "print", "print", "gc.collect", "r_place_drawing_classifier.pytorch_cnn.utils.submissions_separation", "model", "model.unsqueeze_", "torch.nn.functional.softmax().data[].tolist", "torch.nn.functional.softmax().data[].tolist", "torch.nn.functional.softmax().data[].tolist", "torch.cross_entropy", "len", "set.add", "feature.data.t_", "target.data.sub_", "len", "target.data[].tolist", "len", "int", "feature.cuda", "target.cuda", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "[].view", "target.size", "torch.max", "torch.max", "torch.max"], "function", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.utils.submissions_separation"], ["", "def", "evaluation", "(", "data_iter", ",", "model", ",", "config_dict", ",", "dataset", "=", "'unknown'", ",", "return_proba", "=", "False", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "corrects", ",", "avg_loss", "=", "0", ",", "0", "\n", "min_sent_length", "=", "max", "(", "model", ".", "config_dict", "[", "'kernel_sizes'", "]", ")", "\n", "proba_res", "=", "dict", "(", ")", "\n", "size", "=", "0", "\n", "empty_text_srs", "=", "set", "(", ")", "\n", "for", "cur_idx", ",", "cur_example", "in", "enumerate", "(", "data_iter", ")", ":", "\n", "        ", "feature", ",", "target", ",", "sr_idx", ",", "explanatory_meta_features", "=", "cur_example", ".", "text", ",", "cur_example", ".", "label", ",", "cur_example", ".", "sr_name", ",", "cur_example", ".", "meta_data", "\n", "sr_name", "=", "cur_example", ".", "dataset", ".", "fields", "[", "'sr_name'", "]", ".", "vocab", ".", "itos", "[", "int", "(", "sr_idx", "[", "0", "]", ")", "]", "\n", "if", "len", "(", "feature", ")", "==", "0", ":", "\n", "            ", "empty_text_srs", ".", "add", "(", "sr_name", ")", "\n", "continue", "\n", "# converting the meta-feature to be explicit None and not list of None (since the iterator of the loop", "\n", "# looks at the data as batches, it converts all to be list of values, but here it is only a None value)", "\n", "", "if", "explanatory_meta_features", "==", "[", "None", "]", ":", "\n", "            ", "explanatory_meta_features", "=", "None", "\n", "", "feature", ".", "data", ".", "t_", "(", ")", ",", "target", ".", "data", ".", "sub_", "(", "1", ")", "# batch first, index align", "\n", "if", "config_dict", "[", "'cuda'", "]", ":", "\n", "            ", "feature", ",", "target", "=", "feature", ".", "cuda", "(", ")", ",", "target", ".", "cuda", "(", ")", "\n", "", "if", "len", "(", "feature", ".", "data", "[", "0", "]", ")", "<", "min_sent_length", ":", "\n", "            ", "continue", "\n", "# breaking the long sentence we created into the actual submissions it comes from", "\n", "", "feature_separated", "=", "submissions_separation", "(", "input_tensor", "=", "feature", ",", "\n", "separator_int", "=", "cur_example", ".", "dataset", ".", "fields", "[", "'text'", "]", ".", "vocab", ".", "stoi", "[", "\n", "'<sent_ends>'", "]", ",", "\n", "padding_int", "=", "cur_example", ".", "dataset", ".", "fields", "[", "'text'", "]", ".", "vocab", ".", "stoi", "[", "'<pad>'", "]", ")", "\n", "\n", "# the actual learning phase", "\n", "logit", "=", "model", "(", "x", "=", "feature_separated", ",", "explanatory_meta_features", "=", "explanatory_meta_features", ")", "\n", "logit", ".", "unsqueeze_", "(", "0", ")", "\n", "cur_proba", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "logit", ",", "dim", "=", "1", ")", ".", "data", "[", "0", "]", ".", "tolist", "(", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "logit", ",", "target", ",", "size_average", "=", "False", ")", "\n", "avg_loss", "+=", "loss", ".", "data", "\n", "corrects", "+=", "(", "torch", ".", "max", "(", "logit", ",", "1", ")", "\n", "[", "1", "]", ".", "view", "(", "target", ".", "size", "(", ")", ")", ".", "data", "==", "target", ".", "data", ")", ".", "sum", "(", ")", "\n", "proba_res", "[", "sr_name", "]", "=", "(", "cur_proba", ",", "target", ".", "data", "[", "0", "]", ".", "tolist", "(", ")", ")", "\n", "size", "+=", "1", "\n", "\n", "", "avg_loss", "/=", "size", "\n", "accuracy", "=", "100.0", "*", "corrects", "/", "size", "\n", "print", "(", "\"Along evaluation phase, {} srs weren't included (empty input)\"", ".", "format", "(", "len", "(", "empty_text_srs", ")", ")", ")", "\n", "print", "(", "'\\nEvaluation ({} dataset)- loss: {:.6f}  acc: {:.4f}%({}/{})'", ".", "format", "(", "dataset", ",", "avg_loss", ",", "accuracy", ",", "\n", "corrects", ",", "size", ")", ")", "\n", "gc", ".", "collect", "(", ")", "\n", "if", "return_proba", ":", "\n", "        ", "return", "proba_res", "\n", "", "else", ":", "\n", "        ", "return", "accuracy", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.train.predict": [[152, 167], ["isinstance", "model.eval", "text_field.preprocess", "torch.tensor", "torch.tensor", "torch.tensor", "torch.Variable", "print", "model", "torch.max", "torch.max", "torch.max", "x.cuda.cuda"], "function", ["None"], ["", "", "def", "predict", "(", "text", ",", "model", ",", "text_field", ",", "label_feild", ",", "cuda_flag", ")", ":", "\n", "    ", "assert", "isinstance", "(", "text", ",", "str", ")", "\n", "model", ".", "eval", "(", ")", "\n", "# text = text_field.tokenize(text)", "\n", "text", "=", "text_field", ".", "preprocess", "(", "text", ")", "\n", "text", "=", "[", "[", "text_field", ".", "vocab", ".", "stoi", "[", "x", "]", "for", "x", "in", "text", "]", "]", "\n", "x", "=", "torch", ".", "tensor", "(", "text", ")", "\n", "x", "=", "autograd", ".", "Variable", "(", "x", ")", "\n", "if", "cuda_flag", ":", "\n", "        ", "x", "=", "x", ".", "cuda", "(", ")", "\n", "", "print", "(", "x", ")", "\n", "output", "=", "model", "(", "x", ")", "\n", "_", ",", "predicted", "=", "torch", ".", "max", "(", "output", ",", "1", ")", "\n", "#return label_feild.vocab.itos[predicted.data[0][0]+1]", "\n", "return", "label_feild", ".", "vocab", ".", "itos", "[", "predicted", ".", "data", "[", "0", "]", "+", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.train.save": [[169, 175], ["os.path.join", "torch.save", "torch.save", "torch.save", "os.path.isdir", "os.makedirs", "model.state_dict"], "function", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.train.save", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.train.save", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.train.save"], ["", "def", "save", "(", "model", ",", "save_dir", ",", "save_prefix", ",", "fold", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "isdir", "(", "save_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "save_dir", ")", "\n", "", "save_prefix", "=", "os", ".", "path", ".", "join", "(", "save_dir", ",", "save_prefix", ")", "\n", "save_path", "=", "'{}_fold_{}.pt'", ".", "format", "(", "save_prefix", ",", "fold", ")", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "save_path", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.model.CNN_Text.__init__": [[18, 59], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "collections.defaultdict", "eval", "torch.Embedding", "torch.Embedding", "torch.Embedding", "r_place_drawing_classifier.pytorch_cnn.utils.build_embedding_matrix", "model.CNN_Text.embed.weight.data.copy_", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "eval", "allennlp.commands.elmo.ElmoEmbedder", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "int", "int", "len"], "methods", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.single_lstm.SinglelLstm.__init__", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.build_embedding_matrix"], ["    ", "def", "__init__", "(", "self", ",", "config_dict", ",", "text_field", ",", "embedding_file", ",", "eval_measures", ")", ":", "\n", "        ", "super", "(", "CNN_Text", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config_dict", "=", "config_dict", "\n", "\n", "V", "=", "config_dict", "[", "'embed_num'", "]", "\n", "D", "=", "config_dict", "[", "'embedding'", "]", "[", "'emb_size'", "]", "\n", "C", "=", "config_dict", "[", "'class_num'", "]", "\n", "Ci", "=", "1", "\n", "Co", "=", "config_dict", "[", "'class_model'", "]", "[", "'cnn_max_pooling_parmas'", "]", "[", "'kernel_num'", "]", "\n", "Ks", "=", "config_dict", "[", "'kernel_sizes'", "]", "\n", "H", "=", "config_dict", "[", "'class_model'", "]", "[", "'cnn_max_pooling_parmas'", "]", "[", "'last_mlp_dim'", "]", "\n", "\n", "# Handling embedding component - either we use Elmo model / glove like pre-trained model / none of these", "\n", "# glove/w2vec option", "\n", "if", "eval", "(", "config_dict", "[", "'embedding'", "]", "[", "'use_pretrained'", "]", ")", "and", "config_dict", "[", "'embedding'", "]", "[", "'model_type'", "]", "!=", "'elmo'", ":", "\n", "            ", "self", ".", "embed", "=", "nn", ".", "Embedding", "(", "V", ",", "D", ")", "\n", "pre_trained_embedding", "=", "build_embedding_matrix", "(", "embedding_file", ",", "text_field", ",", "\n", "emb_size", "=", "config_dict", "[", "'embedding'", "]", "[", "'emb_size'", "]", ")", "\n", "self", ".", "embed", ".", "weight", ".", "data", ".", "copy_", "(", "torch", ".", "from_numpy", "(", "pre_trained_embedding", ")", ")", "\n", "# elmo option", "\n", "", "elif", "eval", "(", "config_dict", "[", "'embedding'", "]", "[", "'use_pretrained'", "]", ")", "and", "config_dict", "[", "'embedding'", "]", "[", "'model_type'", "]", "==", "'elmo'", ":", "\n", "            ", "options_file", "=", "config_dict", "[", "'embedding'", "]", "[", "'elmo_options_file'", "]", "\n", "weight_file", "=", "config_dict", "[", "'embedding'", "]", "[", "'elmo_weight_file'", "]", "\n", "self", ".", "embed", "=", "ElmoEmbedder", "(", "options_file", ",", "weight_file", ")", "\n", "\n", "# none of these (just random constant values)", "\n", "", "else", ":", "\n", "            ", "self", ".", "embed", "=", "nn", ".", "Embedding", "(", "V", ",", "D", ")", "\n", "", "self", ".", "convs1", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Conv2d", "(", "Ci", ",", "Co", ",", "(", "K", ",", "D", ")", ")", "for", "K", "in", "Ks", "]", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config_dict", "[", "'class_model'", "]", "[", "'nn_params'", "]", "[", "'dropout'", "]", ")", "\n", "# case the model should end up using meta features data", "\n", "#if eval(config_dict['meta_data_usage']['use_meta']):", "\n", "self", ".", "fc1", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "torch", ".", "nn", ".", "Linear", "(", "len", "(", "Ks", ")", "*", "Co", "+", "config_dict", "[", "'meta_features_dim'", "]", ",", "H", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "H", ",", "int", "(", "H", "/", "2", ")", ")", ",", "\n", "#torch.nn.ReLU(),", "\n", "torch", ".", "nn", ".", "Linear", "(", "int", "(", "H", "/", "2", ")", ",", "C", ")", ",", "\n", ")", "\n", "self", ".", "eval_measures", "=", "eval_measures", "\n", "self", ".", "eval_results", "=", "defaultdict", "(", "list", ")", "\n", "self", ".", "text_field", "=", "text_field", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.model.CNN_Text.conv_and_pool": [[60, 64], ["torch.relu().squeeze", "torch.relu().squeeze", "torch.relu().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.relu", "torch.relu", "torch.relu", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "conv", "torch.max_pool1d().squeeze.size"], "methods", ["None"], ["", "def", "conv_and_pool", "(", "self", ",", "x", ",", "conv", ")", ":", "\n", "        ", "x", "=", "F", ".", "relu", "(", "conv", "(", "x", ")", ")", ".", "squeeze", "(", "3", ")", "# (N, Co, W)", "\n", "x", "=", "F", ".", "max_pool1d", "(", "x", ",", "x", ".", "size", "(", "2", ")", ")", ".", "squeeze", "(", "2", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.model.CNN_Text.forward": [[65, 107], ["max", "any", "model.CNN_Text.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "model.CNN_Text.fc1", "gc.collect", "IOError", "eval", "list", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "model.CNN_Text.embed", "torch.relu().squeeze", "torch.relu().squeeze", "torch.relu().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.CNN_Text.dropout", "model.CNN_Text.dropout", "model.CNN_Text.embed.embed_sentences", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.relu", "torch.relu", "torch.relu", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "sorted", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "conv", "i.size", "explanatory_meta_features[].items", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "explanatory_meta_features", ")", ":", "\n", "# case the length of the sent is too short (compared to the kernel defined), we skip it", "\n", "        ", "min_sent_length", "=", "max", "(", "[", "ker", ".", "kernel_size", "[", "0", "]", "for", "ker", "in", "self", ".", "convs1", "]", ")", "\n", "if", "any", "(", "[", "1", "if", "len", "(", "cur_sent", ")", "<", "min_sent_length", "else", "0", "for", "cur_sent", "in", "x", "]", ")", ":", "\n", "            ", "raise", "IOError", "(", "\"Length of sentence is too short compared to kernel sizes given! Please fix\"", ")", "\n", "\n", "", "if", "eval", "(", "self", ".", "config_dict", "[", "'embedding'", "]", "[", "'use_pretrained'", "]", ")", "and", "self", ".", "config_dict", "[", "'embedding'", "]", "[", "'model_type'", "]", "==", "'elmo'", ":", "\n", "            ", "self", ".", "embed", ".", "training", "=", "False", "\n", "x_as_text", "=", "[", "[", "self", ".", "text_field", ".", "vocab", ".", "itos", "[", "cur_idx", "]", "for", "cur_idx", "in", "cur_sent", "]", "for", "cur_sent", "in", "x", "]", "\n", "embeddings", "=", "list", "(", "self", ".", "embed", ".", "embed_sentences", "(", "x_as_text", ")", ")", "\n", "x_embedded", "=", "torch", ".", "Tensor", "(", "[", "e", "[", "2", "]", "for", "e", "in", "embeddings", "]", ")", "\n", "", "else", ":", "\n", "#start_time = datetime.datetime.now()", "\n", "            ", "x_embedded", "=", "self", ".", "embed", "(", "x", ")", "# (N, W, D)", "\n", "#duration = (datetime.datetime.now() - start_time).seconds", "\n", "#print(\"embed model loading time: {} sec\".format(duration))", "\n", "", "x_embedded_unsqueezed", "=", "x_embedded", ".", "unsqueeze", "(", "1", ")", "# (N, Ci, W, D)", "\n", "# now converting x to list of 3 Tensors (one for each convolution)", "\n", "x_convultioned", "=", "[", "F", ".", "relu", "(", "conv", "(", "x_embedded_unsqueezed", ")", ")", ".", "squeeze", "(", "3", ")", "for", "conv", "in", "self", ".", "convs1", "]", "# [(N, Co, W), ...]*len(Ks)", "\n", "x_max_pooled", "=", "[", "F", ".", "max_pool1d", "(", "i", ",", "i", ".", "size", "(", "2", ")", ")", ".", "squeeze", "(", "2", ")", "for", "i", "in", "x_convultioned", "]", "# [(N, Co), ...]*len(Ks)", "\n", "# concatenate all kernel-sizes (by default there are 3 such ones)", "\n", "x_concat", "=", "torch", ".", "cat", "(", "x_max_pooled", ",", "1", ")", "\n", "# calculating the average per column over all instances", "\n", "x_concat_avg", "=", "torch", ".", "mean", "(", "input", "=", "x_concat", ",", "dim", "=", "0", ")", "\n", "# adding the explanatory meta features - sorting them by name and then concatinating it to the NN output", "\n", "if", "explanatory_meta_features", "is", "not", "None", ":", "\n", "            ", "meta_features_sorted", "=", "[", "value", "for", "(", "key", ",", "value", ")", "in", "sorted", "(", "explanatory_meta_features", "[", "0", "]", ".", "items", "(", ")", ")", "]", "\n", "# case we run on the GPU, we'll convert the data to the GPU", "\n", "if", "self", ".", "config_dict", "[", "'cuda'", "]", ":", "\n", "                ", "meta_features_sorted", "=", "torch", ".", "FloatTensor", "(", "meta_features_sorted", ")", ".", "cuda", "(", ")", "\n", "", "x_concat_avg_with_meta", "=", "torch", ".", "cat", "(", "[", "x_concat_avg", ",", "torch", ".", "tensor", "(", "meta_features_sorted", ")", "]", ")", "\n", "x_dropped_out", "=", "self", ".", "dropout", "(", "x_concat_avg_with_meta", ")", "# (N, len(Ks)*Co)", "\n", "", "else", ":", "\n", "            ", "x_dropped_out", "=", "self", ".", "dropout", "(", "x_concat_avg", ")", "# (N, len(Ks)*Co)", "\n", "# x1 = self.conv_and_pool(x,self.conv13) #(N,Co)", "\n", "# x2 = self.conv_and_pool(x,self.conv14) #(N,Co)", "\n", "# x3 = self.conv_and_pool(x,self.conv15) #(N,Co)", "\n", "# x = torch.cat((x1, x2, x3), 1) # (N,len(Ks)*Co)", "\n", "# this step doesn't change the shape of x_concat, only adds a probability not to take some weights into account", "\n", "", "logit", "=", "self", ".", "fc1", "(", "x_dropped_out", ")", "# (N, C)", "\n", "gc", ".", "collect", "(", ")", "\n", "return", "logit", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.model.CNN_Text.calc_eval_measures": [[108, 129], ["model.CNN_Text.eval_measures.items", "model.CNN_Text.eval_results[].append", "func"], "methods", ["None"], ["", "def", "calc_eval_measures", "(", "self", ",", "y_true", ",", "y_pred", ",", "nomalize_y", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        calculation of the evaluation measures for a given prediciton vector and the y_true vector\n        :param y_true: list of ints\n            list containing the true values of y. Any value > 0 is considered as 1 (drawing),\n            all others are 0 (not drawing)\n        :param y_pred: list of floats\n            list containing prediction values for each sr. It represnts the probability of the sr to be a drawing one\n        :param nomalize_y: boolean. default: True\n            whether or not to normalize the y_true and the predictions\n        :return: dict\n            dictionary with all the evalution measures calculated\n        \"\"\"", "\n", "if", "nomalize_y", ":", "\n", "            ", "y_true", "=", "[", "1", "if", "y", ">", "0", "else", "0", "for", "y", "in", "y_true", "]", "\n", "binary_y_pred", "=", "[", "1", "if", "p", ">", "0.5", "else", "0", "for", "p", "in", "y_pred", "]", "\n", "", "else", ":", "\n", "            ", "binary_y_pred", "=", "[", "1", "if", "p", ">", "0.5", "else", "-", "1", "for", "p", "in", "y_pred", "]", "\n", "", "for", "name", ",", "func", "in", "self", ".", "eval_measures", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "eval_results", "[", "name", "]", ".", "append", "(", "func", "(", "y_true", ",", "binary_y_pred", ")", ")", "\n", "", "return", "self", ".", "eval_results", "", "", "", ""]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.utils.build_embedding_matrix": [[22, 79], ["datetime.datetime.now", "len", "numpy.random.normal", "dict", "embedding_file.endswith", "print", "embedding_file.endswith", "open", "enumerate", "infile.close", "gensim.models.KeyedVectors.load_word2vec_format", "set", "enumerate", "IOError", "datetime.datetime.now", "line.split", "KeyedVectors.load_word2vec_format.vocab.keys", "numpy.asarray", "numpy.asarray"], "function", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_bert.bert_service.BertService.close"], ["\n", "start_time", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "# finding all the relevant zip files in the 'data_path' directory", "\n", "submission_files", "=", "[", "f", "for", "f", "in", "os", ".", "listdir", "(", "files_path", ")", "if", "re", ".", "match", "(", "r'RS.*\\.csv'", ",", "f", ")", "and", "'sample'", "not", "in", "f", "]", "\n", "# taking only the submissions files from 10-2016 to 03-2017", "\n", "submission_files", "=", "[", "i", "for", "i", "in", "submission_files", "if", "\n", "''", ".", "join", "(", "[", "'RS_'", ",", "start_month", ",", "'.csv'", "]", ")", "<=", "i", "<=", "''", ".", "join", "(", "[", "'RS_'", ",", "end_month", ",", "'.csv'", "]", ")", "]", "\n", "submission_files", "=", "sorted", "(", "submission_files", ")", "\n", "submission_dfs", "=", "[", "]", "\n", "# iterating over each submission file", "\n", "for", "subm_idx", ",", "cur_submission_file", "in", "enumerate", "(", "submission_files", ")", ":", "\n", "        ", "cur_submission_df", "=", "pd", ".", "read_csv", "(", "filepath_or_buffer", "=", "os", ".", "path", ".", "join", "(", "files_path", ",", "cur_submission_file", ")", ",", "encoding", "=", "'utf-8'", ")", "\n", "# filtering the data-frame based on the list of SRs we want to include and the date (before r/place started)", "\n", "if", "srs_to_include", "is", "not", "None", ":", "\n", "            ", "cur_submission_df", "=", "cur_submission_df", "[", "cur_submission_df", "[", "\"subreddit\"", "]", ".", "str", ".", "lower", "(", ")", ".", "isin", "(", "srs_to_include", ")", "]", "\n", "# filtering based on min/max date", "\n", "", "cur_min_utc", "=", "min", "(", "cur_submission_df", "[", "'created_utc_as_date'", "]", ")", "if", "min_utc", "is", "None", "else", "min_utc", "\n", "cur_max_utc", "=", "max", "(", "cur_submission_df", "[", "'created_utc_as_date'", "]", ")", "if", "max_utc", "is", "None", "else", "max_utc", "\n", "cur_submission_df", "=", "cur_submission_df", "[", "(", "cur_submission_df", "[", "'created_utc_as_date'", "]", ">=", "cur_min_utc", ")", "&", "\n", "(", "cur_submission_df", "[", "'created_utc_as_date'", "]", "<=", "cur_max_utc", ")", "]", "\n", "submission_dfs", ".", "append", "(", "cur_submission_df", ")", "\n", "", "if", "len", "(", "submission_dfs", ")", "==", "0", ":", "\n", "        ", "raise", "IOError", "(", "\"No submission file was found\"", ")", "\n", "\n", "", "full_submissions_df", "=", "pd", ".", "concat", "(", "submission_dfs", ")", "\n", "duration", "=", "(", "datetime", ".", "datetime", ".", "now", "(", ")", "-", "start_time", ")", ".", "seconds", "\n", "print", "(", "f\"Function 'get_submission_subset_dataset' has ended. Took us : {duration} seconds. \"", "\n", "f\"Submission data-frame shape created is {full_submissions_df.shape}\"", ",", "flush", "=", "True", ")", "\n", "return", "full_submissions_df", "\n", "\n", "\n", "", "def", "get_comments_subset", "(", "files_path", ",", "srs_to_include", ",", "start_month", "=", "'2016-10'", ",", "end_month", "=", "'2017-03'", ",", "\n", "min_utc", "=", "None", ",", "max_utc", "=", "'2017-03-29 00:00:00'", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.utils.pull_out_text_data": [[81, 105], ["enumerate", "cur_sr_sentences.append", "sr_classifier.reddit_data_preprocessing.RedditDataPrep.mark_urls", "type", "type", "type", "cur_sr_sentences.append"], "function", ["None"], ["\n", "start_time", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "# pulling out all comment files in the desired range of months", "\n", "comments_files", "=", "[", "f", "for", "f", "in", "os", ".", "listdir", "(", "files_path", ")", "if", "re", ".", "match", "(", "r'RC.*\\.csv'", ",", "f", ")", "and", "'sample'", "not", "in", "f", "]", "\n", "comments_files", "=", "[", "i", "for", "i", "in", "comments_files", "if", "\n", "''", ".", "join", "(", "[", "'RC_'", ",", "start_month", ",", "'.csv'", "]", ")", "<=", "i", "<=", "''", ".", "join", "(", "[", "'RC_'", ",", "end_month", ",", "'.csv'", "]", ")", "]", "\n", "comments_files", "=", "sorted", "(", "comments_files", ")", "\n", "comments_dfs", "=", "[", "]", "\n", "# looping over each file", "\n", "for", "comm_idx", ",", "cur_comments_file", "in", "enumerate", "(", "comments_files", ")", ":", "\n", "        ", "if", "sys", ".", "platform", "==", "'linux'", ":", "\n", "            ", "cur_comments_df", "=", "pd", ".", "read_csv", "(", "filepath_or_buffer", "=", "os", ".", "path", ".", "join", "(", "files_path", ",", "cur_comments_file", ")", ",", "encoding", "=", "'latin-1'", ")", "\n", "", "else", ":", "\n", "            ", "cur_comments_df", "=", "pd", ".", "read_csv", "(", "filepath_or_buffer", "=", "os", ".", "path", ".", "join", "(", "files_path", ",", "cur_comments_file", ")", ",", "encoding", "=", "'latin-1'", ")", "\n", "", "if", "srs_to_include", "is", "not", "None", ":", "\n", "            ", "cur_comments_df", "=", "cur_comments_df", "[", "cur_comments_df", "[", "\"subreddit\"", "]", ".", "str", ".", "lower", "(", ")", ".", "isin", "(", "srs_to_include", ")", "]", "\n", "# filtering based on min/max date", "\n", "", "cur_min_utc", "=", "min", "(", "cur_comments_df", "[", "'created_utc_as_date'", "]", ")", "if", "min_utc", "is", "None", "else", "min_utc", "\n", "cur_max_utc", "=", "max", "(", "cur_comments_df", "[", "'created_utc_as_date'", "]", ")", "if", "max_utc", "is", "None", "else", "max_utc", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.utils.submissions_separation": [[107, 129], ["len", "max", "torch.narrow", "torch.nn.ConstantPad1d", "torch.nn.ConstantPad1d.", "new_tensors_list.append", "torch.cat", "torch.narrow", "enumerate", "zip", "zip", "torch.narrow", "torch.nn.ConstantPad1d", "torch.nn.ConstantPad1d.", "new_tensors_list.append", "len", "range", "len"], "function", ["None"], ["(", "cur_comments_df", "[", "'created_utc_as_date'", "]", "<=", "cur_max_utc", ")", "]", "\n", "comments_dfs", ".", "append", "(", "cur_comments_df", ")", "\n", "", "if", "len", "(", "comments_dfs", ")", "==", "0", ":", "\n", "        ", "raise", "IOError", "(", "\"No comments file were found\"", ")", "\n", "", "full_comments_df", "=", "pd", ".", "concat", "(", "comments_dfs", ")", "\n", "duration", "=", "(", "datetime", ".", "datetime", ".", "now", "(", ")", "-", "start_time", ")", ".", "seconds", "\n", "print", "(", "f\"Function 'get_comments_subset' has ended. Took us : {duration} seconds. \"", "\n", "f\"Comments data-frame shape created is {full_comments_df.shape}\"", ",", "flush", "=", "True", ")", "\n", "return", "full_comments_df", "\n", "\n", "\n", "", "def", "calc_sr_statistics", "(", "files_path", ",", "included_years", ",", "saving_res_path", "=", "os", ".", "getcwd", "(", ")", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.utils.data_prep_meta_features": [[131, 204], ["pandas.DataFrame.from_dict", "sklearn.impute.SimpleImputer", "pandas.DataFrame", "enumerate", "sklearn.preprocessing.StandardScaler", "numpy.concatenate", "pandas.DataFrame", "enumerate", "pd.DataFrame.to_dict", "enumerate", "enumerate", "test_meta_features.append", "test_as_df.append", "sklearn.impute.SimpleImputer.fit_transform", "pandas.DataFrame", "numpy.concatenate", "pandas.DataFrame", "pd.DataFrame.fillna", "enumerate", "test_meta_features_imputed.append", "collections.defaultdict", "updated_meta_features_dict_test.append", "enumerate", "pandas.DataFrame.from_dict", "sklearn.impute.SimpleImputer.transform", "col.startswith", "sklearn.preprocessing.StandardScaler.fit_transform", "test_as_df[].fillna", "cur_test_as_df.to_dict", "train_as_df.to_dict.items", "col.startswith", "sklearn.preprocessing.StandardScaler.fit_transform", "collections.defaultdict", "cur_test_meta_features_imputed.items"], "function", ["None"], ["start_time", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "submission_files", "=", "[", "f", "for", "f", "in", "os", ".", "listdir", "(", "files_path", ")", "if", "re", ".", "match", "(", "r'RS.*\\.csv'", ",", "f", ")", "]", "\n", "# taking only files which are in the 'included_years' subset", "\n", "submission_files", "=", "[", "sf", "for", "sf", "in", "submission_files", "if", "any", "(", "str", "(", "year", ")", "in", "sf", "for", "year", "in", "included_years", ")", "]", "\n", "# comments_files = [cf for cf in comments_files if any(str(year) in cf for year in included_years)]", "\n", "submission_files", "=", "sorted", "(", "submission_files", ")", "\n", "sr_statistics", "=", "collections", ".", "Counter", "(", ")", "\n", "for", "subm_idx", ",", "cur_submission_file", "in", "enumerate", "(", "submission_files", ")", ":", "\n", "        ", "cur_submission_df", "=", "pd", ".", "read_csv", "(", "filepath_or_buffer", "=", "os", ".", "path", ".", "join", "(", "files_path", ",", "cur_submission_file", ")", ")", "\n", "cur_sr_statistics", "=", "collections", ".", "Counter", "(", "cur_submission_df", "[", "\"subreddit\"", "]", ")", "\n", "sr_statistics", "+=", "cur_sr_statistics", "\n", "# writing status to screen", "\n", "duration", "=", "(", "datetime", ".", "datetime", ".", "now", "(", ")", "-", "start_time", ")", ".", "seconds", "\n", "print", "(", "\"Ended loop # {}, up to now took us {} seconds\"", ".", "format", "(", "subm_idx", ",", "duration", ")", ")", "\n", "# saving the stats to a file", "\n", "", "pickle", ".", "dump", "(", "sr_statistics", ",", "open", "(", "saving_res_path", "+", "\"submission_stats_102016_to_032017.p\"", ",", "\"wb\"", ")", ")", "\n", "duration", "=", "(", "datetime", ".", "datetime", ".", "now", "(", ")", "-", "start_time", ")", ".", "seconds", "\n", "print", "(", "\"Function 'calc_sr_statistics' has ended. Took us : {} seconds. \"", "\n", "\"Final dictionary size is {}\"", ".", "format", "(", "duration", ",", "len", "(", "sr_statistics", ")", ")", ")", "\n", "return", "sr_statistics", "\n", "\n", "\n", "", "def", "save_results_to_csv", "(", "results_file", ",", "start_time", ",", "objects_amount", ",", "config_dict", ",", "results", ")", ":", "\n", "    ", "\"\"\"\n    given inputs regarding a final run results - write these results into a file\n    :param results_file: str\n        file of the csv where results should be placed\n    :param start_time: datetime\n        time when the current result run started\n    :param objects_amount: int\n        amount of objects in the run was based on, usually it is between 1000-2500\n    :param config_dict: dict\n        dictionary holding all the configuration of the run, the one we get as input json\n    :param results: dict\n        dictionary with all results. Currently it should contain the following keys: 'accuracy', 'precision', 'recall'\n    :return: None\n        Nothing is returned, only saving to the file is being done\n    \"\"\"", "\n", "\n", "file_exists", "=", "os", ".", "path", ".", "isfile", "(", "results_file", ")", "\n", "rf", "=", "open", "(", "results_file", ",", "'a'", ",", "newline", "=", "''", ")", "\n", "with", "rf", "as", "output_file", ":", "\n", "        ", "dict_writer", "=", "csv", ".", "DictWriter", "(", "output_file", ",", "\n", "fieldnames", "=", "[", "'timestamp'", ",", "'start_time'", ",", "'machine'", ",", "'SRs_amount'", ",", "'cv_folds'", ",", "\n", "'configurations'", ",", "'accuracy'", ",", "'precision'", ",", "'recall'", ",", "'auc'", "]", ")", "\n", "# only in case the file doesn't exist, we'll add a header", "\n", "if", "not", "file_exists", ":", "\n", "            ", "dict_writer", ".", "writeheader", "(", ")", "\n", "", "try", ":", "\n", "            ", "host_name", "=", "os", ".", "environ", "[", "'HOSTNAME'", "]", "if", "sys", ".", "platform", "==", "'linux'", "else", "os", ".", "environ", "[", "'COMPUTERNAME'", "]", "\n", "", "except", "KeyError", ":", "\n", "            ", "host_name", "=", "'pycharm with this ssh: '", "+", "os", ".", "environ", "[", "'SSH_CONNECTION'", "]", "\n", "", "dict_writer", ".", "writerow", "(", "{", "'timestamp'", ":", "datetime", ".", "datetime", ".", "now", "(", ")", ",", "'start_time'", ":", "start_time", ",", "\n", "'SRs_amount'", ":", "objects_amount", ",", "'machine'", ":", "host_name", ",", "'cv_folds'", ":", "len", "(", "results", "[", "'accuracy'", "]", ")", ",", "\n", "'configurations'", ":", "config_dict", ",", "'accuracy'", ":", "results", "[", "'accuracy'", "]", ",", "\n", "'precision'", ":", "results", "[", "'precision'", "]", ",", "'recall'", ":", "results", "[", "'recall'", "]", ",", "'auc'", ":", "results", "[", "'auc'", "]", "}", ")", "\n", "", "rf", ".", "close", "(", ")", "\n", "\n", "\n", "", "def", "examine_word", "(", "sr_object", ",", "regex_required", ",", "tokenizer", ",", "saving_file", "=", "os", ".", "getcwd", "(", ")", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.utils.mem_report": [[206, 260], ["print", "gc.get_objects", "print", "utils.mem_report._mem_report"], "function", ["None"], ["\n", "start_time", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "print", "(", "\"examine_word function has started\"", ")", "\n", "tot_cnt", "=", "0", "\n", "if", "sys", ".", "platform", "==", "'linux'", ":", "\n", "        ", "explicit_file_name", "=", "saving_file", "+", "'/'", "+", "'examine_word_res_regex_'", "+", "regex_required", "+", "'.txt'", "\n", "", "else", ":", "\n", "        ", "explicit_file_name", "=", "saving_file", "+", "'\\\\'", "+", "'examine_word_res_regex_'", "+", "regex_required", "+", "'.txt'", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "explicit_file_name", ")", ":", "\n", "        ", "append_write", "=", "'a'", "# append if already exists", "\n", "", "else", ":", "\n", "        ", "append_write", "=", "'w'", "# make a new file if not", "\n", "", "with", "open", "(", "explicit_file_name", ",", "append_write", ",", "encoding", "=", "\"utf-8\"", ")", "as", "text_file", ":", "\n", "        ", "text_file", ".", "write", "(", "\"\\n\\nHere are the relevant sentences with the regex {} in SR named {}. This sr is labeled as: \"", "\n", "\"{}\"", ".", "format", "(", "regex_required", ",", "sr_object", ".", "name", ",", "\n", "'not drawing'", "if", "sr_object", ".", "trying_to_draw", "==", "-", "1", "else", "'drawing'", ")", ")", "\n", "\n", "for", "subm", "in", "sr_object", ".", "submissions_as_list", ":", "\n", "            ", "normalized_text", "=", "[", "]", "\n", "try", ":", "\n", "                ", "tokenized_txt_title", "=", "tokenizer", "(", "subm", "[", "1", "]", ")", "\n", "normalized_text", "+=", "tokenized_txt_title", "\n", "", "except", "TypeError", ":", "\n", "                ", "pass", "\n", "", "try", ":", "\n", "                ", "tokenized_txt_selftext", "=", "tokenizer", "(", "subm", "[", "2", "]", ")", "\n", "normalized_text", "+=", "tokenized_txt_selftext", "\n", "", "except", "TypeError", ":", "\n", "                ", "pass", "\n", "", "if", "regex_required", "in", "set", "(", "normalized_text", ")", ":", "\n", "                ", "text_file", ".", "write", "(", "'\\n'", "+", "str", "(", "subm", ")", ")", "\n", "tot_cnt", "+=", "1", "\n", "", "", "duration", "=", "(", "datetime", ".", "datetime", ".", "now", "(", ")", "-", "start_time", ")", ".", "seconds", "\n", "print", "(", "\"examine_word has ended, took us {} seconds.\"", "\n", "\"Total of {} rows were written to a text file\"", ".", "format", "(", "duration", ",", "tot_cnt", ")", ")", "\n", "\n", "\n", "", "", "def", "remove_huge_srs", "(", "sr_objects", ",", "quantile", "=", "0.01", ")", ":", "\n", "    ", "\"\"\"\n    removed the largest sr objects - in order not to handle big srs with lots of submissions/comments\n    :param sr_objects: list\n        list of sr objects\n    :param quantile: float, default=0.01\n        the % of srs required to remove\n    :return: list\n        the list of srs after removing the huge ones from it\n    \"\"\"", "\n", "srs_summary", "=", "[", "(", "idx", ",", "cur_sr", ".", "name", ",", "cur_sr", ".", "trying_to_draw", ",", "len", "(", "cur_sr", ".", "submissions_as_list", ")", ")", "\n", "for", "idx", ",", "cur_sr", "in", "enumerate", "(", "sr_objects", ")", "]", "\n", "srs_summary", ".", "sort", "(", "key", "=", "lambda", "tup", ":", "tup", "[", "3", "]", ",", "reverse", "=", "True", ")", "# sorts in place according to the # of submissions", "\n", "amount_of_srs_to_remove", "=", "int", "(", "len", "(", "sr_objects", ")", "*", "quantile", ")", "\n", "srs_to_remove_summary", "=", "srs_summary", "[", "0", ":", "amount_of_srs_to_remove", "]", "\n", "drawing_removed_srs", "=", "sum", "(", "[", "sr", "[", "2", "]", "for", "sr", "in", "srs_to_remove_summary", "if", "sr", "[", "2", "]", "==", "1", "]", ")", "\n", "srs_to_remove", "=", "set", "(", "[", "sr", "[", "1", "]", "for", "sr", "in", "srs_to_remove_summary", "]", ")", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.utils.place_data": [[263, 284], ["data_loaders.TorchLoader.splits", "text_field.build_vocab", "label_field.build_vocab", "sr_name_field.build_vocab", "eval", "torchtext.Iterator.splits", "utils.data_prep_meta_features", "len"], "function", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.mydatasets.MR.splits", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.mydatasets.MR.splits", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.data_prep_meta_features"], ["\" {} out of them are from class 1 (drawing)\"", ".", "format", "(", "len", "(", "srs_to_remove", ")", ",", "drawing_removed_srs", ")", ")", "\n", "return", "returned_list", "\n", "\n", "\n", "", "def", "check_input_validity", "(", "config_dict", ",", "machine", ")", ":", "\n", "    ", "\"\"\"\n    Set of tests to make sure the input configuration we have is valid.\n    If not - we either fix it or change it to be valid\n    :param config_dict: dict\n        the configuration dictionary input. This is read as a json in the main file\n    :param machine: str\n        the machine (computer) we work on \n    :return: dict\n        the updated config_dict (dictionary) or an error\n    \"\"\"", "\n", "# making sure the model we get as input is valid", "\n", "if", "config_dict", "[", "'class_model'", "]", "[", "'model_type'", "]", "not", "in", "[", "'clf_meta_only'", ",", "'bow'", ",", "'mlp'", ",", "\n", "'single_lstm'", ",", "'parallel_lstm'", ",", "'cnn_max_pooling'", "]", ":", "\n", "        ", "raise", "IOError", "(", "'Model name input is invalid. Must be one out of the following: '", "\n", "'[\"clf_meta_only\", \"bow\", \"mlp\", \"single_lstm\", \"parallel_lstm\", \"cnn_max_pooling]. '", "\n", "'Please fix and try again'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.utils.set_random_seed": [[287, 295], ["torch.manual_seed", "random.seed", "torch.cuda.manual_seed_all", "numpy.random.seed", "str"], "function", ["None"], ["if", "os", ".", "path", ".", "exists", "(", "cur_folder_name", ")", ":", "\n", "        ", "new_model_version", "=", "config_dict", "[", "'model_version'", "]", "+", "'.'", "+", "str", "(", "random", ".", "randint", "(", "1", ",", "100000000", ")", ")", "\n", "config_dict", "[", "'model_version'", "]", "=", "new_model_version", "\n", "print", "(", "\"model_version has been changed to {}, \"", "\n", "\"since the original model_version was used in the past\"", ".", "format", "(", "new_model_version", ")", ")", "\n", "", "return", "config_dict", "\n", "", ""]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.__init__": [[62, 82], ["random.seed", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "len", "len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "model", ",", "eval_measures", ",", "tokenizer", "=", "None", ",", "emb_size", "=", "100", ",", "hid_size", "=", "100", ",", "early_stopping", "=", "True", ",", "\n", "epochs", "=", "10", ",", "use_meta_features", "=", "True", ",", "batch_size", "=", "10", ",", "seed", "=", "1984", ")", ":", "\n", "        ", "self", ".", "model", "=", "model", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "eval_measures", "=", "eval_measures", "\n", "self", ".", "emb_size", "=", "emb_size", "\n", "self", ".", "hid_size", "=", "hid_size", "\n", "self", ".", "early_stopping", "=", "early_stopping", "\n", "self", ".", "epochs", "=", "epochs", "\n", "self", ".", "use_meta_features", "=", "use_meta_features", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "seed", "=", "seed", "\n", "random", ".", "seed", "(", "self", ".", "seed", ")", "\n", "# Functions to read in the corpus", "\n", "self", ".", "w2i", "=", "defaultdict", "(", "lambda", ":", "len", "(", "self", ".", "w2i", ")", ")", "\n", "self", ".", "t2i", "=", "defaultdict", "(", "lambda", ":", "len", "(", "self", ".", "t2i", ")", ")", "\n", "self", ".", "w2i", "[", "\"<unk>\"", "]", "\n", "self", ".", "nwords", "=", "None", "\n", "self", ".", "ntags", "=", "None", "\n", "self", ".", "eval_results", "=", "defaultdict", "(", "list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.get_reddit_sentences": [[83, 124], ["len", "len", "enumerate", "nn_classifier.NNClassifier.tokenizer", "cur_sr_sentences.append", "sr_classifier.reddit_data_preprocessing.RedditDataPrep.mark_urls", "len", "cur_sr_sentences_as_int.append", "type", "type", "type", "cur_sr_sentences.append"], "methods", ["None"], ["", "def", "get_reddit_sentences", "(", "self", ",", "sr_objects", ")", ":", "\n", "        ", "\"\"\"\n        pulls out the sentences related to each sr_object in the list provided\n        :param sr_objects: list (of sr_objects)\n            list containing sr_objects\n        :return: generator\n            a generator containing a tuple with 3 variables:\n            1. embedded sentences - list of lists, containing in each place the embedded sentences\n                (e.g. [[2,4,5], [234, 455, 1]])\n            2.  tag - the tag of the sr, in the 1 / 0 format (1 means drawing)\n            3. name - string holding the sr name\n        \"\"\"", "\n", "# looping over all sr objects", "\n", "for", "cur_sr", "in", "sr_objects", ":", "\n", "# pulling out the tag of the current sr", "\n", "            ", "tag", "=", "cur_sr", ".", "trying_to_draw", "\n", "cur_sr_sentences", "=", "[", "]", "\n", "# looping over each submission in the list of submissions", "\n", "for", "idx", ",", "i", "in", "enumerate", "(", "cur_sr", ".", "submissions_as_list", ")", ":", "\n", "# case both (submission header + body are string)", "\n", "                ", "if", "type", "(", "i", "[", "1", "]", ")", "is", "str", "and", "type", "(", "i", "[", "2", "]", ")", "is", "str", ":", "\n", "                    ", "cur_sr_sentences", ".", "append", "(", "i", "[", "1", "]", "+", "' '", "+", "i", "[", "2", "]", ")", "\n", "continue", "\n", "# case only the submissions header is a string", "\n", "", "elif", "type", "(", "i", "[", "1", "]", ")", "is", "str", ":", "\n", "                    ", "cur_sr_sentences", ".", "append", "(", "i", "[", "1", "]", ")", "\n", "continue", "\n", "\n", "# before we tokenize the data, we remove the links and replace them with <link>", "\n", "", "", "cur_sr_sentences", "=", "[", "RedditDataPrep", ".", "mark_urls", "(", "sen", ",", "marking_method", "=", "'tag'", ")", "[", "0", "]", "for", "sen", "in", "cur_sr_sentences", "]", "\n", "cur_sr_sentences_as_int", "=", "[", "]", "\n", "# converting the words into embeddings and returning the tuple for each sr (note it is a generator)", "\n", "for", "sen", "in", "cur_sr_sentences", ":", "\n", "                ", "sen_tokenized", "=", "self", ".", "tokenizer", "(", "sen", ")", "\n", "if", "len", "(", "sen_tokenized", ")", ">", "0", ":", "\n", "                    ", "cur_sr_sentences_as_int", ".", "append", "(", "[", "self", ".", "w2i", "[", "x", "]", "for", "x", "in", "sen_tokenized", "]", ")", "\n", "", "", "yield", "(", "cur_sr_sentences_as_int", ",", "self", ".", "t2i", "[", "tag", "]", ",", "cur_sr", ".", "name", ")", "\n", "\n", "# updating the # of words and tags we found", "\n", "", "self", ".", "nwords", "=", "len", "(", "self", ".", "w2i", ")", "\n", "self", ".", "ntags", "=", "len", "(", "self", ".", "t2i", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.data_prep_meta_features": [[125, 174], ["pandas.DataFrame.from_dict", "pandas.DataFrame.from_dict", "sklearn.impute.SimpleImputer", "pandas.DataFrame", "pandas.DataFrame", "sklearn.preprocessing.StandardScaler", "pandas.DataFrame", "pandas.DataFrame", "pandas.DataFrame.to_dict", "pandas.DataFrame.to_dict", "sklearn.impute.SimpleImputer.fit_transform", "sklearn.impute.SimpleImputer.transform", "sklearn.preprocessing.StandardScaler.fit_transform", "sklearn.preprocessing.StandardScaler.transform", "collections.defaultdict", "collections.defaultdict", "pd.DataFrame.to_dict.items", "pd.DataFrame.to_dict.items"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "data_prep_meta_features", "(", "train_data", ",", "test_data", ",", "update_objects", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        function to handle and prepare meta features for the classification model\n        :param train_data: list\n            list of sr objects to be used as train set\n        :param test_data: list\n            list of sr objects to be used as train set\n        :param update_objects: boolean. default: True\n            whether or not to update the sr objects \"on the fly\" along the function. If False, the objects are not\n            updated and the updated dictionaries are returned by the function\n        :return: int or dicts\n            in case the update_objects=True should just return 0.\n            If it is False, 2 dictionaries containing the meta features are returned\n        \"\"\"", "\n", "# pulling out the meta features of each object and converting it as pandas df", "\n", "train_meta_features", "=", "{", "sr_obj", ".", "name", ":", "sr_obj", ".", "explanatory_features", "for", "sr_obj", "in", "train_data", "}", "\n", "test_meta_features", "=", "{", "sr_obj", ".", "name", ":", "sr_obj", ".", "explanatory_features", "for", "sr_obj", "in", "test_data", "}", "\n", "\n", "train_as_df", "=", "pd", ".", "DataFrame", ".", "from_dict", "(", "train_meta_features", ",", "orient", "=", "'index'", ")", "\n", "test_as_df", "=", "pd", ".", "DataFrame", ".", "from_dict", "(", "test_meta_features", ",", "orient", "=", "'index'", ")", "\n", "\n", "# imputation phase", "\n", "imp_obj", "=", "SimpleImputer", "(", "strategy", "=", "'mean'", ",", "copy", "=", "False", ")", "\n", "train_as_df", "=", "pd", ".", "DataFrame", "(", "imp_obj", ".", "fit_transform", "(", "train_as_df", ")", ",", "columns", "=", "train_as_df", ".", "columns", ",", "index", "=", "train_as_df", ".", "index", ")", "\n", "test_as_df", "=", "pd", ".", "DataFrame", "(", "imp_obj", ".", "transform", "(", "test_as_df", ")", ",", "columns", "=", "test_as_df", ".", "columns", ",", "index", "=", "test_as_df", ".", "index", ")", "\n", "\n", "# normalization phase", "\n", "normalize_obj", "=", "StandardScaler", "(", ")", "\n", "train_as_df", "=", "pd", ".", "DataFrame", "(", "normalize_obj", ".", "fit_transform", "(", "train_as_df", ")", ",", "columns", "=", "train_as_df", ".", "columns", ",", "index", "=", "train_as_df", ".", "index", ")", "\n", "test_as_df", "=", "pd", ".", "DataFrame", "(", "normalize_obj", ".", "transform", "(", "test_as_df", ")", ",", "columns", "=", "test_as_df", ".", "columns", ",", "index", "=", "test_as_df", ".", "index", ")", "\n", "\n", "# creating dicts to be used for replacing the meta features (or return them as is - depends on  the", "\n", "# 'update_objects' input param", "\n", "train_meta_features_imputed", "=", "train_as_df", ".", "to_dict", "(", "'index'", ")", "\n", "test_meta_features_imputed", "=", "test_as_df", ".", "to_dict", "(", "'index'", ")", "\n", "updated_meta_features_dict_train", "=", "{", "key", ":", "collections", ".", "defaultdict", "(", "None", ",", "value", ")", "for", "key", ",", "value", "in", "train_meta_features_imputed", ".", "items", "(", ")", "}", "\n", "updated_meta_features_dict_test", "=", "{", "key", ":", "collections", ".", "defaultdict", "(", "None", ",", "value", ")", "for", "key", ",", "value", "in", "test_meta_features_imputed", ".", "items", "(", ")", "}", "\n", "if", "update_objects", ":", "\n", "# looping over all srs object and updating the meta data features", "\n", "            ", "for", "cur_sr", "in", "train_data", ":", "\n", "                ", "cur_sr", ".", "explanatory_features", "=", "updated_meta_features_dict_train", "[", "cur_sr", ".", "name", "]", "\n", "", "for", "cur_sr", "in", "test_data", ":", "\n", "                ", "cur_sr", ".", "explanatory_features", "=", "updated_meta_features_dict_test", "[", "cur_sr", ".", "name", "]", "\n", "", "return", "0", "\n", "", "else", ":", "\n", "            ", "return", "updated_meta_features_dict_train", ",", "updated_meta_features_dict_test", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.build_embedding_matrix": [[175, 234], ["datetime.datetime.now", "dict", "embedding_file.endswith", "print", "numpy.random.normal", "numpy.random.normal", "embedding_file.endswith", "open", "enumerate", "infile.close", "gensim.models.KeyedVectors.load_word2vec_format", "set", "enumerate", "IOError", "datetime.datetime.now", "line.split", "gensim.models.KeyedVectors.load_word2vec_format.vocab.keys", "numpy.asarray", "numpy.asarray"], "methods", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_bert.bert_service.BertService.close"], ["", "", "def", "build_embedding_matrix", "(", "self", ",", "embedding_file", ",", "add_extra_row", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        building an embedding matrix based on a given external file. Such matrix is a combination of words we\n        identify in the exteranl file and words that do not appear there and will be initialize with a random\n        embedding vector\n        :param embedding_file: str\n            the path to the exact embedding file to be used. This should be a txt file, each row represents\n            a word and it's embedding (separated by whitespace). Example can be taken from 'glove' pre-trained models\n        :return: numpy matrix\n            the embedding matrix built\n        \"\"\"", "\n", "start_time", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "# building the full embedding matrix, with random normal values. Later we'll replace known words", "\n", "if", "add_extra_row", ":", "\n", "            ", "embedding_matrix", "=", "np", ".", "random", ".", "normal", "(", "loc", "=", "0.0", ",", "scale", "=", "1.0", ",", "size", "=", "(", "self", ".", "nwords", "+", "1", ",", "self", ".", "emb_size", ")", ")", "\n", "", "else", ":", "\n", "            ", "embedding_matrix", "=", "np", ".", "random", ".", "normal", "(", "loc", "=", "0.0", ",", "scale", "=", "1.0", ",", "size", "=", "(", "self", ".", "nwords", ",", "self", ".", "emb_size", ")", ")", "\n", "", "embeddings_index", "=", "dict", "(", ")", "\n", "found_words", "=", "0", "\n", "\n", "# passing over the pretrained embedding matrix", "\n", "if", "embedding_file", ".", "endswith", "(", "'.txt'", ")", ":", "\n", "            ", "with", "open", "(", "embedding_file", ")", "as", "infile", ":", "\n", "                ", "for", "idx", ",", "line", "in", "enumerate", "(", "infile", ")", ":", "\n", "                    ", "values", "=", "line", ".", "split", "(", ")", "\n", "word", "=", "values", "[", "0", "]", "\n", "try", ":", "\n", "                        ", "coefs", "=", "np", ".", "asarray", "(", "values", "[", "1", ":", "]", ",", "dtype", "=", "'float32'", ")", "\n", "", "except", "ValueError", ":", "\n", "                        ", "continue", "\n", "", "embeddings_index", "[", "word", "]", "=", "coefs", "\n", "# updating the embedding matrix according to the w2i dictionary (if is it found)", "\n", "if", "word", "in", "self", ".", "w2i", ":", "\n", "                        ", "embedding_matrix", "[", "self", ".", "w2i", "[", "word", "]", "]", "=", "coefs", "\n", "found_words", "+=", "1", "\n", "", "", "infile", ".", "close", "(", ")", "\n", "\n", "", "", "elif", "embedding_file", ".", "endswith", "(", "'.vec'", ")", ":", "\n", "            ", "w2v_model", "=", "KeyedVectors", ".", "load_word2vec_format", "(", "embedding_file", ")", "\n", "models_dict", "=", "set", "(", "w2v_model", ".", "vocab", ".", "keys", "(", ")", ")", "\n", "for", "idx", ",", "word", "in", "enumerate", "(", "models_dict", ")", ":", "\n", "                ", "values", "=", "w2v_model", "[", "word", "]", "\n", "try", ":", "\n", "                    ", "coefs", "=", "np", ".", "asarray", "(", "values", ",", "dtype", "=", "'float32'", ")", "\n", "", "except", "ValueError", ":", "\n", "                    ", "continue", "\n", "", "embeddings_index", "[", "word", "]", "=", "coefs", "\n", "# updating the embedding matrix according to the w2i dictionary (if is it found)", "\n", "if", "word", "in", "self", ".", "w2i", ":", "\n", "                    ", "embedding_matrix", "[", "self", ".", "w2i", "[", "word", "]", "]", "=", "coefs", "\n", "found_words", "+=", "1", "\n", "\n", "", "", "", "else", ":", "\n", "            ", "raise", "IOError", "(", "\"Embedding file format not recognized\"", ")", "\n", "", "duration", "=", "(", "datetime", ".", "datetime", ".", "now", "(", ")", "-", "start_time", ")", ".", "seconds", "\n", "print", "(", "\"We have finished running the 'build_embedding_matrix' function. Took us {0:.2f} seconds. \"", "\n", "\"We have found {1:.1f}% of matching words \"", "\n", "\"compared to the embedding matrix.\"", ".", "format", "(", "duration", ",", "found_words", "*", "100.0", "/", "self", ".", "nwords", ")", ")", "\n", "return", "embedding_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.calc_eval_measures": [[235, 256], ["nn_classifier.NNClassifier.eval_measures.items", "nn_classifier.NNClassifier.eval_results[].append", "func"], "methods", ["None"], ["", "def", "calc_eval_measures", "(", "self", ",", "y_true", ",", "y_pred", ",", "nomalize_y", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        calculation of the evaluation measures for a given prediciton vector and the y_true vector\n        :param y_true: list of ints\n            list containing the true values of y. Any value > 0 is considered as 1 (drawing),\n            all others are 0 (not drawing)\n        :param y_pred: list of floats\n            list containing prediction values for each sr. It represnts the probability of the sr to be a drawing one\n        :param nomalize_y: boolean. default: True\n            whether or not to normalize the y_true and the predictions\n        :return: dict\n            dictionary with all the evalution measures calculated\n        \"\"\"", "\n", "if", "nomalize_y", ":", "\n", "            ", "y_true", "=", "[", "1", "if", "y", ">", "0", "else", "0", "for", "y", "in", "y_true", "]", "\n", "binary_y_pred", "=", "[", "1", "if", "p", ">", "0.5", "else", "0", "for", "p", "in", "y_pred", "]", "\n", "", "else", ":", "\n", "            ", "binary_y_pred", "=", "[", "1", "if", "p", ">", "0.5", "else", "-", "1", "for", "p", "in", "y_pred", "]", "\n", "", "for", "name", ",", "func", "in", "self", ".", "eval_measures", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "eval_results", "[", "name", "]", ".", "append", "(", "func", "(", "y_true", ",", "binary_y_pred", ")", ")", "\n", "", "return", "self", ".", "eval_results", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.from_spec": [[257, 260], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "from_spec", "(", "spec", ",", "model", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.cnn_max_pooling.CnnMaxPooling.__init__": [[80, 97], ["nn_classifier.NNClassifier.__init__"], "methods", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.single_lstm.SinglelLstm.__init__"], ["def", "__init__", "(", "self", ",", "model", ",", "tokenizer", ",", "eval_measures", ",", "emb_size", "=", "100", ",", "early_stopping", "=", "True", ",", "\n", "epochs", "=", "10", ",", "use_meta_features", "=", "True", ",", "batch_size", "=", "10", ",", "seed", "=", "1984", ",", "filter_size", "=", "8", ",", "win_size", "=", "3", ")", ":", "\n", "        ", "super", "(", "CnnMaxPooling", ",", "self", ")", ".", "__init__", "(", "model", "=", "model", ",", "tokenizer", "=", "tokenizer", ",", "eval_measures", "=", "eval_measures", ",", "emb_size", "=", "emb_size", ",", "\n", "early_stopping", "=", "early_stopping", ",", "epochs", "=", "epochs", ",", "\n", "use_meta_features", "=", "use_meta_features", ",", "batch_size", "=", "batch_size", ",", "seed", "=", "seed", ")", "\n", "self", ".", "filter_size", "=", "filter_size", "\n", "self", ".", "win_size", "=", "win_size", "\n", "# all these will be set along the execution", "\n", "self", ".", "W_emb", "=", "None", "\n", "self", ".", "W_cnn", "=", "None", "\n", "self", ".", "b_cnn", "=", "None", "\n", "self", ".", "W_mlp", "=", "None", "\n", "self", ".", "b_mlp", "=", "None", "\n", "self", ".", "V_mlp", "=", "None", "\n", "self", ".", "a_mlp", "=", "None", "\n", "self", ".", "spec", "=", "(", "tokenizer", ",", "eval_measures", ",", "emb_size", ",", "early_stopping", ",", "epochs", ",", "use_meta_features", ",", "\n", "batch_size", ",", "seed", ",", "filter_size", ",", "win_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.cnn_max_pooling.CnnMaxPooling.calc_scores": [[98, 147], ["dynet.average", "dynet.concatenate", "dynet.conv2d_bias", "dynet.max_dim", "dynet.reshape", "dynet.rectify", "pool_out_agg.append", "dynet.tanh", "dynet.logistic", "dynet.inputVector", "dynet.concatenate", "dynet.tanh", "dynet.logistic", "len", "dynet.lookup", "sorted", "len", "meta_data.items"], "methods", ["None"], ["", "def", "calc_scores", "(", "self", ",", "sentences", ",", "meta_data", "=", "None", ",", "get_probability", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        calculating the score for parallel LSTM network (in a specific state along learning phase)\n        :param sentences: list\n            list of lists of sentences (represented already as numbers and not letters)\n        :param meta_data: dict or None\n            if None, meta data is not used for calculating the score\n            if dict, meta data about each observation (SR) is pulled out from this dict\n        :param get_probability: bool, default: True\n            whether to return probability of each SR to be drawing one (based on the dy.logistic function) or just the\n            last layer (which is not too informative)\n        :return: dynet parameter. size: (2,)\n            prediction of the instance to be a drawing one according to the model (vector of 2, first place is the\n            probability to be a drawing team) - this is in case 'get_probability' set to True\n            Otherwise we get a vector of values from the last layer of the network\n        \"\"\"", "\n", "# padding with zeros in case sentences are too short", "\n", "for", "words", "in", "sentences", ":", "\n", "            ", "if", "len", "(", "words", ")", "<", "self", ".", "win_size", ":", "\n", "                ", "words", "+=", "[", "0", "]", "*", "(", "self", ".", "win_size", "-", "len", "(", "words", ")", ")", "\n", "\n", "# looping over each sentence, calculating the CNN max pooling and taking the average at the end", "\n", "", "", "pool_out_agg", "=", "[", "]", "\n", "for", "words", "in", "sentences", ":", "\n", "            ", "cnn_in", "=", "dy", ".", "concatenate", "(", "[", "dy", ".", "lookup", "(", "self", ".", "W_emb", ",", "x", ")", "for", "x", "in", "words", "]", ",", "d", "=", "1", ")", "\n", "cnn_out", "=", "dy", ".", "conv2d_bias", "(", "cnn_in", ",", "self", ".", "W_cnn", ",", "self", ".", "b_cnn", ",", "stride", "=", "(", "1", ",", "1", ")", ",", "is_valid", "=", "False", ")", "\n", "pool_out", "=", "dy", ".", "max_dim", "(", "cnn_out", ",", "d", "=", "1", ")", "\n", "pool_out", "=", "dy", ".", "reshape", "(", "pool_out", ",", "(", "self", ".", "filter_size", ",", ")", ")", "\n", "pool_out", "=", "dy", ".", "rectify", "(", "pool_out", ")", "# Relu function: max(x_i, 0)", "\n", "pool_out_agg", ".", "append", "(", "pool_out", ")", "\n", "", "pool_out_avg", "=", "dy", ".", "average", "(", "pool_out_agg", ")", "\n", "\n", "if", "meta_data", "is", "None", ":", "\n", "            ", "h", "=", "dy", ".", "tanh", "(", "(", "self", ".", "W_mlp", "*", "pool_out_avg", ")", "+", "self", ".", "b_mlp", ")", "\n", "prediction", "=", "dy", ".", "logistic", "(", "(", "self", ".", "V_mlp", "*", "h", ")", "+", "self", ".", "a_mlp", ")", "\n", "if", "get_probability", ":", "\n", "                ", "return", "prediction", "\n", "", "else", ":", "\n", "                ", "return", "pool_out_avg", "\n", "", "", "else", ":", "\n", "            ", "meta_data_ordered", "=", "[", "value", "for", "key", ",", "value", "in", "sorted", "(", "meta_data", ".", "items", "(", ")", ")", "]", "\n", "meta_data_vector", "=", "dy", ".", "inputVector", "(", "meta_data_ordered", ")", "\n", "first_layer_avg_and_meta_data", "=", "dy", ".", "concatenate", "(", "[", "pool_out_avg", ",", "meta_data_vector", "]", ")", "\n", "h", "=", "dy", ".", "tanh", "(", "(", "self", ".", "W_mlp", "*", "first_layer_avg_and_meta_data", ")", "+", "self", ".", "b_mlp", ")", "\n", "prediction", "=", "dy", ".", "logistic", "(", "(", "self", ".", "V_mlp", "*", "h", ")", "+", "self", ".", "a_mlp", ")", "\n", "if", "get_probability", ":", "\n", "                ", "return", "prediction", "\n", "", "else", ":", "\n", "                ", "return", "first_layer_avg_and_meta_data", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.cnn_max_pooling.CnnMaxPooling.fit_predict": [[148, 271], ["list", "list", "dynet.AdamTrainer", "cnn_max_pooling.CnnMaxPooling.model.add_parameters", "cnn_max_pooling.CnnMaxPooling.model.add_parameters", "cnn_max_pooling.CnnMaxPooling.model.add_parameters", "cnn_max_pooling.CnnMaxPooling.model.add_parameters", "cnn_max_pooling.CnnMaxPooling.model.add_parameters", "cnn_max_pooling.CnnMaxPooling.model.add_parameters", "range", "enumerate", "cnn_max_pooling.CnnMaxPooling.calc_eval_measures", "print", "cnn_max_pooling.CnnMaxPooling.data_prep_meta_features", "len", "cnn_max_pooling.CnnMaxPooling.get_reddit_sentences", "cnn_max_pooling.CnnMaxPooling.get_reddit_sentences", "cnn_max_pooling.CnnMaxPooling.model.add_lookup_parameters", "cnn_max_pooling.CnnMaxPooling.build_embedding_matrix", "numpy.expand_dims", "numpy.expand_dims", "numpy.expand_dims.reshape", "cnn_max_pooling.CnnMaxPooling.model.add_lookup_parameters", "time.time", "list", "print", "enumerate", "print", "cnn_max_pooling.CnnMaxPooling.calc_scores().npvalue", "test_predictions.append", "numpy.argmax", "print", "range", "dynet.renew_cg", "enumerate", "batch_loss.backward", "dynet.AdamTrainer.update", "cnn_max_pooling.CnnMaxPooling.calc_scores().npvalue", "numpy.argmax", "len", "cnn_max_pooling.CnnMaxPooling.calc_scores", "dynet.pickneglogsoftmax", "losses.append", "dynet.pickneglogsoftmax.value", "dynet.esum", "cnn_max_pooling.CnnMaxPooling.calc_scores", "len", "cnn_max_pooling.CnnMaxPooling.calc_scores", "list", "len", "time.time", "len", "train_meta_data.keys"], "methods", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.calc_eval_measures", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.data_prep_meta_features", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.get_reddit_sentences", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.get_reddit_sentences", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.build_embedding_matrix", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.cnn_max_pooling.CnnMaxPooling.calc_scores", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.cnn_max_pooling.CnnMaxPooling.calc_scores", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.cnn_max_pooling.CnnMaxPooling.calc_scores"], ["", "", "", "def", "fit_predict", "(", "self", ",", "train_data", ",", "test_data", ",", "embedding_file", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        fits a parallel LSTM model\n        :param train_data: list\n            list of sr objects to be used as train set\n        :param test_data: list\n            list of sr objects to be used as train set\n        :param embedding_file: str\n            the path to the exact embedding file to be used. This should be a txt file, each row represents\n            a word and it's embedding (separated by whitespace). Example can be taken from 'glove' pre-trained models\n            If None, we build an embedding from random normal distribution\n        :return: tuple\n            tuple with 3 variables:\n            self.eval_results, model, test_predicitons\n            1. eval_results: dictionary with evaluation measures over the test set\n            2. model: the MLP trained model which was used\n            3. test_predicitons: list with predictions to each sr in the test dataset\n        \"\"\"", "\n", "# case we wish to use meta features along modeling, we need to prepare the SRs objects for this", "\n", "if", "self", ".", "use_meta_features", ":", "\n", "            ", "train_meta_data", ",", "test_meta_data", "=", "self", ".", "data_prep_meta_features", "(", "train_data", "=", "train_data", ",", "test_data", "=", "test_data", ",", "update_objects", "=", "False", ")", "\n", "meta_data_dim", "=", "len", "(", "train_meta_data", "[", "list", "(", "train_meta_data", ".", "keys", "(", ")", ")", "[", "0", "]", "]", ")", "\n", "", "else", ":", "\n", "            ", "train_meta_data", "=", "None", "\n", "test_meta_data", "=", "None", "\n", "meta_data_dim", "=", "0", "\n", "# next we are creating the input for the algorithm. train_data_for_dynet will contain list of lists. Each inner", "\n", "# list contains the words index relevant to the specific sentence", "\n", "", "train_data_for_dynet", "=", "list", "(", "self", ".", "get_reddit_sentences", "(", "sr_objects", "=", "train_data", ")", ")", "\n", "train_data_names", "=", "[", "i", "[", "2", "]", "for", "i", "in", "train_data_for_dynet", "]", "# list of train sr names", "\n", "train_data_for_dynet", "=", "[", "(", "i", "[", "0", "]", ",", "i", "[", "1", "]", ")", "for", "i", "in", "train_data_for_dynet", "]", "\n", "\n", "# test_data_for_dynet will contain list of lists. Each inner list contains the words index relevant to the", "\n", "# specific sentence", "\n", "test_data_for_dynet", "=", "list", "(", "self", ".", "get_reddit_sentences", "(", "sr_objects", "=", "test_data", ")", ")", "\n", "test_data_names", "=", "[", "i", "[", "2", "]", "for", "i", "in", "test_data_for_dynet", "]", "# list of test sr names", "\n", "# Need to check here that the order is saved!!!!", "\n", "test_data_for_dynet", "=", "[", "(", "i", "[", "0", "]", ",", "i", "[", "1", "]", ")", "for", "i", "in", "test_data_for_dynet", "]", "\n", "\n", "# Start DyNet and define trainer", "\n", "trainer", "=", "dy", ".", "AdamTrainer", "(", "self", ".", "model", ")", "\n", "# Define the model", "\n", "# Word embeddings part", "\n", "if", "embedding_file", "is", "None", ":", "\n", "            ", "self", ".", "W_emb", "=", "self", ".", "model", ".", "add_lookup_parameters", "(", "(", "self", ".", "nwords", ",", "1", ",", "1", ",", "self", ".", "emb_size", ")", ")", "\n", "", "else", ":", "\n", "            ", "external_embedding", "=", "self", ".", "build_embedding_matrix", "(", "embedding_file", ",", "add_extra_row", "=", "False", ")", "\n", "external_embedding", "=", "np", ".", "expand_dims", "(", "external_embedding", ",", "axis", "=", "0", ")", "\n", "external_embedding", "=", "np", ".", "expand_dims", "(", "external_embedding", ",", "axis", "=", "0", ")", "\n", "external_embedding_reshaped", "=", "external_embedding", ".", "reshape", "(", "(", "self", ".", "nwords", ",", "1", ",", "1", ",", "self", ".", "emb_size", ")", ")", "\n", "self", ".", "W_emb", "=", "self", ".", "model", ".", "add_lookup_parameters", "(", "(", "self", ".", "nwords", ",", "1", ",", "1", ",", "self", ".", "emb_size", ")", ",", "init", "=", "external_embedding_reshaped", ")", "\n", "", "self", ".", "W_cnn", "=", "self", ".", "model", ".", "add_parameters", "(", "(", "1", ",", "self", ".", "win_size", ",", "self", ".", "emb_size", ",", "self", ".", "filter_size", ")", ")", "# cnn weights", "\n", "self", ".", "b_cnn", "=", "self", ".", "model", ".", "add_parameters", "(", "(", "self", ".", "filter_size", ")", ")", "# cnn bias", "\n", "\n", "# Last layer with network is an MLP one, case we are not using meta data, meta_data_dim will be zero", "\n", "# and hence not relevant", "\n", "self", ".", "W_mlp", "=", "self", ".", "model", ".", "add_parameters", "(", "(", "self", ".", "filter_size", ",", "self", ".", "filter_size", "+", "meta_data_dim", ")", ")", "\n", "self", ".", "b_mlp", "=", "self", ".", "model", ".", "add_parameters", "(", "self", ".", "filter_size", ")", "\n", "self", ".", "V_mlp", "=", "self", ".", "model", ".", "add_parameters", "(", "(", "self", ".", "ntags", ",", "self", ".", "filter_size", ")", ")", "\n", "self", ".", "a_mlp", "=", "self", ".", "model", ".", "add_parameters", "(", "1", ")", "\n", "\n", "mloss", "=", "[", "0.0", ",", "0.0", "]", "# we always save the current run loss and the prev one (for early stopping purposes", "\n", "# looping over each epoch", "\n", "for", "ITER", "in", "range", "(", "self", ".", "epochs", ")", ":", "\n", "# checking the early stopping criterion", "\n", "            ", "if", "self", ".", "early_stopping", "and", "(", "ITER", ">=", "(", "self", ".", "epochs", "*", "1.0", "/", "2", ")", ")", "and", "(", "(", "mloss", "[", "0", "]", "-", "mloss", "[", "1", "]", ")", "*", "1.0", "/", "mloss", "[", "0", "]", ")", "<=", "0.01", ":", "\n", "                ", "print", "(", "\"Early stopping has been applied since improvement was not greater than 1%\"", ")", "\n", "break", "\n", "# Perform training", "\n", "#random.seed(self.seed)", "\n", "#random.shuffle(train_data_for_dynet)", "\n", "", "start", "=", "time", ".", "time", "(", ")", "\n", "cur_mloss", "=", "0.0", "\n", "batches_starting_point", "=", "list", "(", "range", "(", "0", ",", "len", "(", "train_data_for_dynet", ")", ",", "self", ".", "batch_size", ")", ")", "\n", "# looping over each batch", "\n", "for", "cur_sp", "in", "batches_starting_point", ":", "\n", "                ", "dy", ".", "renew_cg", "(", ")", "\n", "losses", "=", "[", "]", "\n", "# looping over each SR (contains multiple sentences)", "\n", "for", "idx", ",", "(", "sentences", ",", "tag", ")", "in", "enumerate", "(", "train_data_for_dynet", "[", "cur_sp", ":", "cur_sp", "+", "self", ".", "batch_size", "]", ")", ":", "\n", "                    ", "cur_meta_data", "=", "train_meta_data", "[", "train_data_names", "[", "cur_sp", "+", "idx", "]", "]", "if", "self", ".", "use_meta_features", "else", "None", "\n", "scores", "=", "self", ".", "calc_scores", "(", "sentences", ",", "meta_data", "=", "cur_meta_data", ")", "\n", "my_loss", "=", "dy", ".", "pickneglogsoftmax", "(", "scores", ",", "tag", ")", "\n", "losses", ".", "append", "(", "my_loss", ")", "\n", "cur_mloss", "+=", "my_loss", ".", "value", "(", ")", "\n", "\n", "", "batch_loss", "=", "dy", ".", "esum", "(", "losses", ")", "/", "(", "idx", "+", "1", ")", "\n", "batch_loss", ".", "backward", "(", ")", "\n", "trainer", ".", "update", "(", ")", "\n", "\n", "# updating the mloss for early stopping purposes", "\n", "", "mloss", "[", "0", "]", "=", "mloss", "[", "1", "]", "\n", "mloss", "[", "1", "]", "=", "cur_mloss", "\n", "print", "(", "\"iter %r: train loss/sr=%.4f, time=%.2fs\"", "%", "(", "ITER", ",", "cur_mloss", "/", "len", "(", "train_data_for_dynet", ")", ",", "\n", "time", ".", "time", "(", ")", "-", "start", ")", ")", "\n", "# Perform testing validation (at the end of current epoch)", "\n", "test_correct", "=", "0.0", "\n", "for", "idx", ",", "(", "sentences", ",", "tag", ")", "in", "enumerate", "(", "test_data_for_dynet", ")", ":", "\n", "                ", "cur_meta_data", "=", "test_meta_data", "[", "test_data_names", "[", "idx", "]", "]", "if", "self", ".", "use_meta_features", "else", "None", "\n", "scores", "=", "self", ".", "calc_scores", "(", "sentences", ",", "meta_data", "=", "cur_meta_data", ")", ".", "npvalue", "(", ")", "\n", "predict", "=", "np", ".", "argmax", "(", "scores", ")", "\n", "if", "predict", "==", "tag", ":", "\n", "                    ", "test_correct", "+=", "1", "\n", "", "", "print", "(", "\"iter %r: test acc=%.4f\"", "%", "(", "ITER", ",", "test_correct", "/", "len", "(", "test_data_for_dynet", ")", ")", ")", "\n", "# Perform testing validation after all ephocs ended", "\n", "", "test_correct", "=", "0.0", "\n", "test_predictions", "=", "[", "]", "\n", "for", "idx", ",", "(", "words", ",", "tag", ")", "in", "enumerate", "(", "test_data_for_dynet", ")", ":", "\n", "            ", "cur_meta_data", "=", "test_meta_data", "[", "test_data_names", "[", "idx", "]", "]", "if", "self", ".", "use_meta_features", "else", "None", "\n", "cur_score", "=", "self", ".", "calc_scores", "(", "words", ",", "meta_data", "=", "cur_meta_data", ")", ".", "npvalue", "(", ")", "\n", "# adding the prediction of the sr to draw (to be label 1) and calculating the acc on the fly", "\n", "test_predictions", ".", "append", "(", "cur_score", "[", "1", "]", ")", "\n", "predict", "=", "np", ".", "argmax", "(", "cur_score", ")", "\n", "if", "predict", "==", "tag", ":", "\n", "                ", "test_correct", "+=", "1", "\n", "\n", "", "", "y_test", "=", "[", "a", "[", "1", "]", "for", "a", "in", "test_data_for_dynet", "]", "\n", "self", ".", "calc_eval_measures", "(", "y_true", "=", "y_test", ",", "y_pred", "=", "test_predictions", ",", "nomalize_y", "=", "True", ")", "\n", "print", "(", "\"final test acc=%.4f\"", "%", "(", "test_correct", "/", "len", "(", "y_test", ")", ")", ")", "\n", "\n", "return", "self", ".", "eval_results", ",", "self", ".", "model", ",", "test_predictions", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.cnn_max_pooling.CnnMaxPooling.predict": [[272, 307], ["list", "enumerate", "cnn_max_pooling.CnnMaxPooling.data_prep_meta_features", "cnn_max_pooling.CnnMaxPooling.get_reddit_sentences", "cnn_max_pooling.CnnMaxPooling.calc_scores().npvalue", "test_predictions.append", "cnn_max_pooling.CnnMaxPooling.calc_scores().npvalue", "test_predictions.append", "cnn_max_pooling.CnnMaxPooling.calc_scores", "cnn_max_pooling.CnnMaxPooling.calc_scores"], "methods", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.data_prep_meta_features", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.get_reddit_sentences", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.cnn_max_pooling.CnnMaxPooling.calc_scores", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.cnn_max_pooling.CnnMaxPooling.calc_scores"], ["", "def", "predict", "(", "self", ",", "train_data", ",", "test_data", ",", "get_probability", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        given a train & test datasets, yields a prediction to the test dataset.\n        this assumes we have a trained model to be used\n        :param train_data: list\n            list of sr objects to be used as train set\n        :param test_data: list\n            list of sr objects to be used as train set\n        :param get_probability: bool, default: True\n            whether to return probability of each SR to be drawing one (based on the dy.logistic function) or just the\n            last layer (which is not too informative)\n        :return: list\n            list of prediction values to the test dataset (in case get_probability=True, it will be list of values\n            which indicate the probability of each community to be drawing one in r/place)\n        \"\"\"", "\n", "if", "self", ".", "use_meta_features", ":", "\n", "            ", "_", ",", "test_meta_data", "=", "self", ".", "data_prep_meta_features", "(", "train_data", "=", "train_data", ",", "test_data", "=", "test_data", ",", "\n", "update_objects", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "test_meta_data", "=", "None", "\n", "", "test_data_for_dynet", "=", "list", "(", "self", ".", "get_reddit_sentences", "(", "sr_objects", "=", "test_data", ")", ")", "\n", "test_data_names", "=", "[", "i", "[", "2", "]", "for", "i", "in", "test_data_for_dynet", "]", "# list of test sr names", "\n", "test_data_for_dynet", "=", "[", "(", "i", "[", "0", "]", ",", "i", "[", "1", "]", ")", "for", "i", "in", "test_data_for_dynet", "]", "\n", "test_predictions", "=", "[", "]", "\n", "for", "idx", ",", "(", "words", ",", "tag", ")", "in", "enumerate", "(", "test_data_for_dynet", ")", ":", "\n", "            ", "cur_meta_data", "=", "test_meta_data", "[", "test_data_names", "[", "idx", "]", "]", "if", "self", ".", "use_meta_features", "else", "None", "\n", "if", "get_probability", ":", "\n", "                ", "cur_score", "=", "self", ".", "calc_scores", "(", "words", ",", "meta_data", "=", "cur_meta_data", ")", ".", "npvalue", "(", ")", "\n", "# adding the prediction of the sr to draw (to be label 1) and calculating the acc on the fly", "\n", "test_predictions", ".", "append", "(", "cur_score", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "                ", "cur_score", "=", "self", ".", "calc_scores", "(", "words", ",", "meta_data", "=", "cur_meta_data", ",", "get_probability", "=", "False", ")", ".", "npvalue", "(", ")", "\n", "# adding the prediction of the sr to draw (to be label 1) and calculating the acc on the fly", "\n", "test_predictions", ".", "append", "(", "cur_score", "[", "1", "]", ")", "\n", "", "", "return", "test_predictions", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.cnn_max_pooling.CnnMaxPooling.save_model": [[308, 337], ["os.path.join", "dynet.save", "copy.copy", "dict", "dict", "pickle.dump", "os.path.exists", "os.makedirs", "os.path.join", "setattr", "open", "getattr", "os.path.join", "str", "str"], "methods", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.pytorch_cnn.train.save"], ["", "def", "save_model", "(", "self", ",", "path", ",", "model_version", ",", "fold", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        saving the trained model in a specific folder which can be loaded later\n        :param path: str\n            the location to dave the model\n        :param model_version: str\n            model version (can be any string)\n        :param fold: int\n            the fold which the model refers to (out of the k-folds we build each model)\n            usually will be in the [0,5] range\n        :return: Nothing\n        \"\"\"", "\n", "full_saving_path", "=", "os", ".", "path", ".", "join", "(", "path", ",", "\"model_\"", "+", "model_version", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "full_saving_path", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "full_saving_path", ")", "\n", "# list of all the vars we need to remove since these are dynet features (they will be saved soon separately", "\n", "", "nn_vars", "=", "[", "'W_emb'", ",", "'W_cnn'", ",", "'b_cnn'", ",", "'W_mlp'", ",", "'b_mlp'", ",", "'V_mlp'", ",", "'a_mlp'", "]", "\n", "# saving these features (it will be saved in the desired directory under the model name)", "\n", "dy", ".", "save", "(", "os", ".", "path", ".", "join", "(", "full_saving_path", ",", "'model_'", "+", "model_version", "+", "'_fold'", "+", "str", "(", "fold", ")", ")", ",", "[", "getattr", "(", "self", ",", "i", ")", "for", "i", "in", "nn_vars", "]", ")", "\n", "obj_to_save", "=", "copy", ".", "copy", "(", "self", ")", "\n", "# now setting these to None, since we cannot save them as is in a pickle type", "\n", "for", "n", "in", "nn_vars", ":", "\n", "            ", "setattr", "(", "obj_to_save", ",", "n", ",", "None", ")", "\n", "", "obj_to_save", ".", "model", "=", "None", "\n", "# converting default dict into dict, since pickle can only save dict objects and not defaultdict ones", "\n", "obj_to_save", ".", "w2i", "=", "dict", "(", "obj_to_save", ".", "w2i", ")", "\n", "obj_to_save", ".", "t2i", "=", "dict", "(", "obj_to_save", ".", "t2i", ")", "\n", "pickle", ".", "dump", "(", "obj_to_save", ",", "\n", "open", "(", "os", ".", "path", ".", "join", "(", "full_saving_path", ",", "'model_'", "+", "model_version", "+", "'_fold'", "+", "str", "(", "fold", ")", "+", "\".p\"", ")", ",", "\"wb\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.cnn_max_pooling.CnnMaxPooling.load_model": [[338, 365], ["os.path.join", "pickle.load", "dynet.ParameterCollection", "dynet.load", "collections.defaultdict", "collections.defaultdict", "open", "len", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "load_model", "(", "path", ",", "model_version", ")", ":", "\n", "        ", "\"\"\"\n        loading a model object from a specific folder\n        :param path: str\n            the location to dave the model\n        :param model_version: str\n            model version (can be any string)\n        :return: dynet obj model\n            the model requested to be loaded\n        \"\"\"", "\n", "full_saving_path", "=", "os", ".", "path", ".", "join", "(", "path", ",", "model_version", ")", "\n", "new_model_obj", "=", "pickle", ".", "load", "(", "open", "(", "full_saving_path", "+", "\".p\"", ",", "\"rb\"", ")", ")", "\n", "model_to_load", "=", "dy", ".", "ParameterCollection", "(", ")", "\n", "W_emb", ",", "W_cnn", ",", "b_cnn", ",", "W_mlp", ",", "b_mlp", ",", "V_mlp", ",", "a_mlp", "=", "dy", ".", "load", "(", "full_saving_path", ",", "model_to_load", ")", "\n", "new_model_obj", ".", "W_emb", "=", "W_emb", "\n", "new_model_obj", ".", "W_cnn", "=", "W_cnn", "\n", "new_model_obj", ".", "b_cnn", "=", "b_cnn", "\n", "new_model_obj", ".", "W_mlp", "=", "W_mlp", "\n", "new_model_obj", ".", "b_mlp", "=", "b_mlp", "\n", "new_model_obj", ".", "V_mlp", "=", "V_mlp", "\n", "new_model_obj", ".", "a_mlp", "=", "a_mlp", "\n", "# converting default dict into dict, since pickle can only save dict objects and not defaultdict ones", "\n", "new_model_obj", ".", "w2i", "=", "defaultdict", "(", "lambda", ":", "len", "(", "new_model_obj", ".", "w2i", ")", ",", "new_model_obj", ".", "w2i", ")", "\n", "new_model_obj", ".", "t2i", "=", "defaultdict", "(", "lambda", ":", "len", "(", "new_model_obj", ".", "t2i", ")", ",", "new_model_obj", ".", "t2i", ")", "\n", "new_model_obj", ".", "model", "=", "model_to_load", "\n", "return", "new_model_obj", "\n", "", "", ""]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.pytorch_mlp.PytorchMLP.__init__": [[26, 44], ["torch.Module.__init__", "torch.Sequential", "torch.Linear", "torch.ReLU", "torch.Dropout", "torch.Linear", "torch.ReLU", "torch.Dropout", "torch.Linear", "torch.Softmax", "isinstance", "torch.init.kaiming_normal_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.single_lstm.SinglelLstm.__init__"], ["def", "__init__", "(", "self", ",", "num_features", ",", "dropout", "=", "0.25", ",", "n_hid", "=", "128", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "model", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "num_features", ",", "n_hid", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "#nn.BatchNorm1d(n_hid),", "\n", "nn", ".", "Dropout", "(", "dropout", ")", ",", "\n", "nn", ".", "Linear", "(", "n_hid", ",", "n_hid", "//", "4", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "#nn.BatchNorm1d(n_hid // 4),", "\n", "nn", ".", "Dropout", "(", "dropout", ")", ",", "\n", "nn", ".", "Linear", "(", "n_hid", "//", "4", ",", "2", ")", ",", "\n", "nn", ".", "Softmax", "(", ")", "\n", ")", "\n", "for", "m", "in", "self", ".", "model", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "                ", "nn", ".", "init", ".", "kaiming_normal_", "(", "m", ".", "weight", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.pytorch_mlp.PytorchMLP.forward": [[45, 47], ["pytorch_mlp.PytorchMLP.model"], "methods", ["None"], ["", "", "", "def", "forward", "(", "self", ",", "input_tensor", ")", ":", "\n", "        ", "return", "self", ".", "model", "(", "input_tensor", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.mlp.MLP.__init__": [[47, 53], ["r_place_drawing_classifier.neural_net.nn_classifier.NNClassifier.__init__"], "methods", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.single_lstm.SinglelLstm.__init__"], ["def", "__init__", "(", "self", ",", "tokenizer", ",", "eval_measures", ",", "emb_size", "=", "100", ",", "hid_size", "=", "100", ",", "early_stopping", "=", "True", ",", "\n", "epochs", "=", "10", ",", "use_meta_features", "=", "True", ",", "seed", "=", "1984", ",", "use_embed", "=", "False", ")", ":", "\n", "        ", "super", "(", "MLP", ",", "self", ")", ".", "__init__", "(", "model", "=", "None", ",", "tokenizer", "=", "tokenizer", ",", "eval_measures", "=", "eval_measures", ",", "emb_size", "=", "emb_size", ",", "\n", "hid_size", "=", "hid_size", ",", "early_stopping", "=", "early_stopping", ",", "epochs", "=", "epochs", ",", "\n", "use_meta_features", "=", "use_meta_features", ",", "seed", "=", "seed", ")", "\n", "self", ".", "use_embed", "=", "use_embed", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.mlp.MLP.fit_predict": [[54, 78], ["warnings.warn", "mlp.MLP.fit_simple_mlp", "mlp.MLP.fit_embedded_mlp"], "methods", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.mlp.MLP.fit_simple_mlp", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.mlp.MLP.fit_embedded_mlp"], ["", "def", "fit_predict", "(", "self", ",", "train_data", ",", "test_data", ",", "embedding_file", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        fits a model and predicts probability over the test set\n        :param train_data: list\n            list of sr objects to be used for training\n        :param test_data: list\n            list of sr objects to be as the test data set\n        :param embedding_file: str\n            the external embedding matrix to be used along modeling (only the explicit file location is provided here).\n            If None - external embedding is not used\n        :return: tuple\n            tuple with 3 variables:\n            self.eval_results, model, test_predicitons\n            1. eval_results: dictionary with evaluation measures over the test set\n            2. model: the MLP trained model which was used\n            3. test_predicitons: list with predictions to each sr in the test dataset\n        \"\"\"", "\n", "if", "embedding_file", "is", "not", "None", "and", "not", "self", ".", "use_embed", ":", "\n", "            ", "warnings", ".", "warn", "(", "\"Note that you provedid an external embedding file, while you configured settings not to use\"", "\n", "\"embedding phase along model building. The embedding file will not be used\"", ")", "\n", "", "if", "not", "self", ".", "use_embed", ":", "\n", "            ", "return", "self", ".", "fit_simple_mlp", "(", "train_data", "=", "train_data", ",", "test_data", "=", "test_data", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "fit_embedded_mlp", "(", "train_data", "=", "train_data", ",", "test_data", "=", "test_data", ",", "embedding_file", "=", "embedding_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.mlp.MLP.fit_simple_mlp": [[79, 167], ["random.seed", "random.shuffle", "mlp.MLP.data_prep_meta_features", "len", "dynet.Model", "dynet.SimpleSGDTrainer", "dynet.renew_cg", "dynet.Model.add_parameters", "dynet.Model.add_parameters", "dynet.Model.add_parameters", "dynet.Model.add_parameters", "dynet.vecInput", "dynet.tanh", "dynet.scalarInput", "dynet.logistic", "dynet.binary_log_loss", "range", "enumerate", "mlp.MLP.calc_eval_measures", "print", "list", "time.time", "enumerate", "print", "dynet.logistic", "enumerate", "print", "zip", "dynet.vecInput.set", "dynet.logistic.value", "test_predicitons.append", "train_meta_data[].keys", "print", "zip", "dynet.vecInput.set", "dynet.scalarInput.set", "dynet.binary_log_loss.value", "dynet.binary_log_loss.backward", "dynet.SimpleSGDTrainer.update", "zip", "dynet.vecInput.set", "dynet.logistic.value", "sorted", "len", "sorted", "sorted", "cur_sr_dict.items", "cur_sr_dict.items", "len", "time.time", "cur_sr_dict.items", "len"], "methods", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.data_prep_meta_features", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.calc_eval_measures"], ["", "", "def", "fit_simple_mlp", "(", "self", ",", "train_data", ",", "test_data", ")", ":", "\n", "        ", "\"\"\"\n        fits an MLP model over the train data and evaluates results over the test data\n        :param train_data: list\n            list of sr objects to be used for training\n        :param test_data: list\n        :return: tuple\n            tuple with 3 variables:\n            self.eval_results, model, test_predicitons\n            1. eval_results: dictionary with evaluation measures over the test set\n            2. model: the MLP trained model which was used\n            3. test_predicitons: list with predictions to each sr in the test dataset\n        \"\"\"", "\n", "random", ".", "seed", "(", "self", ".", "seed", ")", "\n", "random", ".", "shuffle", "(", "train_data", ")", "\n", "# data prep to meta features", "\n", "self", ".", "data_prep_meta_features", "(", "train_data", "=", "train_data", ",", "test_data", "=", "test_data", ",", "update_objects", "=", "True", ")", "\n", "\n", "# pulling out the meta features and the tag (of train and test)", "\n", "train_meta_data", "=", "[", "sr_obj", ".", "explanatory_features", "for", "sr_obj", "in", "train_data", "]", "\n", "test_meta_data", "=", "[", "sr_obj", ".", "explanatory_features", "for", "sr_obj", "in", "test_data", "]", "\n", "y_train", "=", "[", "sr_obj", ".", "trying_to_draw", "for", "sr_obj", "in", "train_data", "]", "\n", "y_test", "=", "[", "sr_obj", ".", "trying_to_draw", "for", "sr_obj", "in", "test_data", "]", "\n", "meta_data_dim", "=", "len", "(", "list", "(", "train_meta_data", "[", "0", "]", ".", "keys", "(", ")", ")", ")", "\n", "# Start DyNet and define trainer", "\n", "model", "=", "dy", ".", "Model", "(", ")", "\n", "trainer", "=", "dy", ".", "SimpleSGDTrainer", "(", "model", ")", "\n", "dy", ".", "renew_cg", "(", ")", "\n", "\n", "# dynet model's params", "\n", "W", "=", "model", ".", "add_parameters", "(", "(", "self", ".", "hid_size", ",", "meta_data_dim", ")", ")", "\n", "b", "=", "model", ".", "add_parameters", "(", "self", ".", "hid_size", ")", "\n", "V", "=", "model", ".", "add_parameters", "(", "(", "1", ",", "self", ".", "hid_size", ")", ")", "\n", "a", "=", "model", ".", "add_parameters", "(", "1", ")", "\n", "x", "=", "dy", ".", "vecInput", "(", "meta_data_dim", ")", "\n", "h", "=", "dy", ".", "tanh", "(", "(", "W", "*", "x", ")", "+", "b", ")", "\n", "y", "=", "dy", ".", "scalarInput", "(", "0", ")", "\n", "y_pred", "=", "dy", ".", "logistic", "(", "(", "V", "*", "h", ")", "+", "a", ")", "\n", "loss", "=", "dy", ".", "binary_log_loss", "(", "y_pred", ",", "y", ")", "\n", "mloss", "=", "[", "0.0", ",", "0.0", "]", "# we always save the current run loss and the prev one (for early stopping purposes", "\n", "\n", "# iterations over the epochs", "\n", "for", "ITER", "in", "range", "(", "self", ".", "epochs", ")", ":", "\n", "# checking the early stopping criterion", "\n", "            ", "if", "self", ".", "early_stopping", "and", "(", "ITER", ">=", "(", "self", ".", "epochs", "*", "1.0", "/", "2", ")", ")", "and", "(", "(", "mloss", "[", "0", "]", "-", "mloss", "[", "1", "]", ")", "*", "1.0", "/", "mloss", "[", "0", "]", ")", "<=", "0.01", ":", "\n", "                ", "print", "(", "\"Early stopping has been applied since improvement was not greater than 1%\"", ")", "\n", "break", "\n", "# Perform training", "\n", "", "start", "=", "time", ".", "time", "(", ")", "\n", "cur_mloss", "=", "0.0", "\n", "for", "idx", ",", "(", "cur_sr_dict", ",", "tag", ")", "in", "enumerate", "(", "zip", "(", "train_meta_data", ",", "y_train", ")", ")", ":", "\n", "# create graph for computing loss", "\n", "                ", "cur_sr_values_ordered", "=", "[", "value", "for", "key", ",", "value", "in", "sorted", "(", "cur_sr_dict", ".", "items", "(", ")", ")", "]", "\n", "x", ".", "set", "(", "cur_sr_values_ordered", ")", "\n", "tag_normalized", "=", "1", "if", "tag", "==", "1", "else", "0", "\n", "y", ".", "set", "(", "tag_normalized", ")", "\n", "# loss calc", "\n", "cur_mloss", "+=", "loss", ".", "value", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "trainer", ".", "update", "(", ")", "\n", "# updating the mloss for early stopping purposes", "\n", "", "mloss", "[", "0", "]", "=", "mloss", "[", "1", "]", "\n", "mloss", "[", "1", "]", "=", "cur_mloss", "\n", "print", "(", "\"iter %r: train loss/sr=%.4f, time=%.2fs\"", "%", "(", "ITER", ",", "cur_mloss", "/", "len", "(", "y_train", ")", ",", "time", ".", "time", "(", ")", "-", "start", ")", ")", "\n", "# Perform testing validation", "\n", "test_correct", "=", "0.0", "\n", "y_pred", "=", "dy", ".", "logistic", "(", "(", "V", "*", "h", ")", "+", "a", ")", "\n", "for", "idx", ",", "(", "cur_sr_dict", ",", "tag", ")", "in", "enumerate", "(", "zip", "(", "test_meta_data", ",", "y_test", ")", ")", ":", "\n", "                ", "cur_sr_values_ordered", "=", "[", "value", "for", "key", ",", "value", "in", "sorted", "(", "cur_sr_dict", ".", "items", "(", ")", ")", "]", "\n", "x", ".", "set", "(", "cur_sr_values_ordered", ")", "\n", "y_pred_value", "=", "y_pred", ".", "value", "(", ")", "\n", "if", "(", "y_pred_value", ">=", ".5", "and", "tag", "==", "1", ")", "or", "(", "y_pred_value", "<=", ".5", "and", "tag", "==", "-", "1", ")", ":", "\n", "                    ", "test_correct", "+=", "1", "\n", "", "", "print", "(", "\"iter %r: test acc=%.4f\"", "%", "(", "ITER", ",", "test_correct", "/", "len", "(", "y_test", ")", ")", ")", "\n", "# Perform testing validation after all batches ended", "\n", "", "test_predicitons", "=", "[", "]", "\n", "test_correct", "=", "0.0", "\n", "for", "idx", ",", "(", "cur_sr_dict", ",", "tag", ")", "in", "enumerate", "(", "zip", "(", "test_meta_data", ",", "y_test", ")", ")", ":", "\n", "            ", "cur_sr_values_ordered", "=", "[", "value", "for", "key", ",", "value", "in", "sorted", "(", "cur_sr_dict", ".", "items", "(", ")", ")", "]", "\n", "x", ".", "set", "(", "cur_sr_values_ordered", ")", "\n", "y_pred_value", "=", "y_pred", ".", "value", "(", ")", "\n", "test_predicitons", ".", "append", "(", "y_pred_value", ")", "\n", "if", "(", "y_pred_value", ">=", ".5", "and", "tag", "==", "1", ")", "or", "(", "y_pred_value", "<=", ".5", "and", "tag", "==", "-", "1", ")", ":", "\n", "                ", "test_correct", "+=", "1", "\n", "", "", "self", ".", "calc_eval_measures", "(", "y_true", "=", "y_test", ",", "y_pred", "=", "test_predicitons", ",", "nomalize_y", "=", "True", ")", "\n", "print", "(", "\"final test acc=%.4f\"", "%", "(", "test_correct", "/", "len", "(", "y_test", ")", ")", ")", "\n", "return", "self", ".", "eval_results", ",", "model", ",", "test_predicitons", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.mlp.MLP._calc_scores_embedded_mlp": [[168, 207], ["dynet.renew_cg", "dynet.average", "dynet.tanh", "dynet.logistic", "dynet.inputVector", "dynet.concatenate", "dynet.tanh", "dynet.logistic", "dynet.lookup", "dynet.average", "sorted", "meta_data.items"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_calc_scores_embedded_mlp", "(", "sentences", ",", "W_emb", ",", "W_mlp", ",", "b_mlp", ",", "V_mlp", ",", "a_mlp", ",", "meta_data", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        calculating the score for a a NN network (in a specific state along learning phase)\n        :param sentences: list\n            list of lists of sentences (represented already as numbers and not letters)\n        :param W_emb: lookup parameter (dynet obj). size: (emb_size x nwords)\n            matrix holding the word embedding values\n        :param W_mlp: model parameter (dynet obj). size: (hid_size, emb_size + meta_data_dim)\n            matrix holding weights of the mlp phase\n        :param b_mlp: model parameter (dynet obj). size: (hid_size,)\n            vector holding weights of intercept for each hidden state\n        :param V_mlp: model parameter (dynet obj). size: (2, hid_size)\n            matrix holding weights of the logisitc regression phase. 2 is there due to the fact we are in a binary\n            classification\n        :param a_mlp: model parameter (dynet obj). size: (1,)\n            intercept value for the logistic regression phase\n        :param meta_data: dict or None\n            meta data features for the model. If None - meta data is not used\n        :return: dynet parameter. size: (2,)\n            prediction of the instance to be a drawing one according to the model (vector of 2, first place is the\n            probability to be a drawing team)\n        \"\"\"", "\n", "dy", ".", "renew_cg", "(", ")", "\n", "# sentences_len = len(sentences)", "\n", "word_embs", "=", "[", "[", "dy", ".", "lookup", "(", "W_emb", ",", "w", ")", "for", "w", "in", "words", "]", "for", "words", "in", "sentences", "]", "\n", "# taking the average over all words", "\n", "first_layer_avg", "=", "dy", ".", "average", "(", "[", "dy", ".", "average", "(", "w_em", ")", "for", "w_em", "in", "word_embs", "]", ")", "\n", "# case we don't wish to use meta features for the model", "\n", "if", "meta_data", "is", "None", ":", "\n", "            ", "h", "=", "dy", ".", "tanh", "(", "(", "W_mlp", "*", "first_layer_avg", ")", "+", "b_mlp", ")", "\n", "prediction", "=", "dy", ".", "logistic", "(", "(", "V_mlp", "*", "h", ")", "+", "a_mlp", ")", "\n", "", "else", ":", "\n", "            ", "meta_data_ordered", "=", "[", "value", "for", "key", ",", "value", "in", "sorted", "(", "meta_data", ".", "items", "(", ")", ")", "]", "\n", "meta_data_vector", "=", "dy", ".", "inputVector", "(", "meta_data_ordered", ")", "\n", "first_layer_avg_and_meta_data", "=", "dy", ".", "concatenate", "(", "[", "first_layer_avg", ",", "meta_data_vector", "]", ")", "\n", "h", "=", "dy", ".", "tanh", "(", "(", "W_mlp", "*", "first_layer_avg_and_meta_data", ")", "+", "b_mlp", ")", "\n", "prediction", "=", "dy", ".", "logistic", "(", "(", "V_mlp", "*", "h", ")", "+", "a_mlp", ")", "\n", "", "return", "prediction", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.mlp.MLP.fit_embedded_mlp": [[208, 320], ["list", "list", "dynet.Model", "dynet.AdamTrainer", "dynet.Model.add_parameters", "dynet.Model.add_parameters", "dynet.Model.add_parameters", "dynet.Model.add_parameters", "range", "enumerate", "mlp.MLP.calc_eval_measures", "print", "mlp.MLP.data_prep_meta_features", "len", "mlp.MLP.get_reddit_sentences", "mlp.MLP.get_reddit_sentences", "dynet.Model.add_lookup_parameters", "mlp.MLP.build_embedding_matrix", "dynet.Model.add_lookup_parameters", "time.time", "enumerate", "print", "enumerate", "print", "mlp.MLP._calc_scores_embedded_mlp().npvalue", "test_predictions.append", "numpy.argmax", "print", "dynet.pickneglogsoftmax", "dynet.pickneglogsoftmax.value", "dynet.pickneglogsoftmax.backward", "dynet.AdamTrainer.update", "mlp.MLP._calc_scores_embedded_mlp().npvalue", "numpy.argmax", "mlp.MLP._calc_scores_embedded_mlp", "mlp.MLP._calc_scores_embedded_mlp", "len", "mlp.MLP._calc_scores_embedded_mlp", "list", "len", "time.time", "len", "train_meta_data.keys"], "methods", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.calc_eval_measures", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.data_prep_meta_features", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.get_reddit_sentences", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.get_reddit_sentences", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.build_embedding_matrix", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.mlp.MLP._calc_scores_embedded_mlp", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.mlp.MLP._calc_scores_embedded_mlp", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.mlp.MLP._calc_scores_embedded_mlp"], ["", "def", "fit_embedded_mlp", "(", "self", ",", "train_data", ",", "test_data", ",", "embedding_file", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        fits an MLP model with embedding layer\n        :param train_data: list\n            list of sr objects to be used as train set\n        :param test_data: list\n            list of sr objects to be used as train set\n        :param embedding_file: str\n            the path to the exact embedding file to be used. This should be a txt file, each row represents\n            a word and it's embedding (separated by whitespace). Example can be taken from 'glove' pre-trained models\n            If None, we build an embedding from random normal distribution\n        :return: tuple\n            tuple with 3 variables:\n            self.eval_results, model, test_predicitons\n            1. eval_results: dictionary with evaluation measures over the test set\n            2. model: the MLP trained model which was used\n            3. test_predicitons: list with predictions to each sr in the test dataset\n        \"\"\"", "\n", "# case we wish to use meta features along modeling, we need to prepare the SRs objects for this", "\n", "if", "self", ".", "use_meta_features", ":", "\n", "            ", "train_meta_data", ",", "test_meta_data", "=", "self", ".", "data_prep_meta_features", "(", "train_data", "=", "train_data", ",", "test_data", "=", "test_data", ",", "update_objects", "=", "False", ")", "\n", "meta_data_dim", "=", "len", "(", "train_meta_data", "[", "list", "(", "train_meta_data", ".", "keys", "(", ")", ")", "[", "0", "]", "]", ")", "\n", "", "else", ":", "\n", "            ", "train_meta_data", "=", "None", "\n", "test_meta_data", "=", "None", "\n", "meta_data_dim", "=", "0", "\n", "# next we are creating the input for the algorithm. train_data_for_dynet will contain list of lists. Each inner", "\n", "# list contains the words index relevant to the specific sentence", "\n", "", "train_data_for_dynet", "=", "list", "(", "self", ".", "get_reddit_sentences", "(", "sr_objects", "=", "train_data", ")", ")", "\n", "train_data_names", "=", "[", "i", "[", "2", "]", "for", "i", "in", "train_data_for_dynet", "]", "# list of train sr names", "\n", "train_data_for_dynet", "=", "[", "(", "i", "[", "0", "]", ",", "i", "[", "1", "]", ")", "for", "i", "in", "train_data_for_dynet", "]", "\n", "\n", "# test_data_for_dynet will contain list of lists. Each inner list contains the words index relevant to the", "\n", "# specific sentence", "\n", "test_data_for_dynet", "=", "list", "(", "self", ".", "get_reddit_sentences", "(", "sr_objects", "=", "test_data", ")", ")", "\n", "test_data_names", "=", "[", "i", "[", "2", "]", "for", "i", "in", "test_data_for_dynet", "]", "# list of test sr names", "\n", "# Need to check here that the order is saved!!!!", "\n", "test_data_for_dynet", "=", "[", "(", "i", "[", "0", "]", ",", "i", "[", "1", "]", ")", "for", "i", "in", "test_data_for_dynet", "]", "\n", "\n", "# Start DyNet and define trainer", "\n", "model", "=", "dy", ".", "Model", "(", ")", "\n", "trainer", "=", "dy", ".", "AdamTrainer", "(", "model", ")", "\n", "# Define the model", "\n", "# Word embeddings part", "\n", "if", "embedding_file", "is", "None", ":", "\n", "            ", "W_emb", "=", "model", ".", "add_lookup_parameters", "(", "(", "self", ".", "nwords", ",", "self", ".", "emb_size", ")", ")", "\n", "", "else", ":", "\n", "            ", "external_embedding", "=", "self", ".", "build_embedding_matrix", "(", "embedding_file", ")", "\n", "W_emb", "=", "model", ".", "add_lookup_parameters", "(", "(", "self", ".", "nwords", ",", "self", ".", "emb_size", ")", ",", "init", "=", "external_embedding", ")", "\n", "# Last layer with network is an MLP one, case we are not using meta data, meta_data_dim will be zero", "\n", "# and hence not relevant", "\n", "", "W_mlp", "=", "model", ".", "add_parameters", "(", "(", "self", ".", "hid_size", ",", "self", ".", "emb_size", "+", "meta_data_dim", ")", ")", "\n", "b_mlp", "=", "model", ".", "add_parameters", "(", "self", ".", "hid_size", ")", "\n", "V_mlp", "=", "model", ".", "add_parameters", "(", "(", "self", ".", "ntags", ",", "self", ".", "hid_size", ")", ")", "\n", "a_mlp", "=", "model", ".", "add_parameters", "(", "1", ")", "\n", "\n", "mloss", "=", "[", "0.0", ",", "0.0", "]", "# we always save the current run loss and the prev one (for early stopping purposes", "\n", "for", "ITER", "in", "range", "(", "self", ".", "epochs", ")", ":", "\n", "# checking the early stopping criterion", "\n", "            ", "if", "self", ".", "early_stopping", "and", "(", "ITER", ">=", "(", "self", ".", "epochs", "*", "1.0", "/", "2", ")", ")", "and", "(", "(", "mloss", "[", "0", "]", "-", "mloss", "[", "1", "]", ")", "*", "1.0", "/", "mloss", "[", "0", "]", ")", "<=", "0.01", ":", "\n", "                ", "print", "(", "\"Early stopping has been applied since improvement was not greater than 1%\"", ")", "\n", "break", "\n", "# Perform training", "\n", "#random.seed(self.seed)", "\n", "#random.shuffle(train_data_for_dynet)", "\n", "", "start", "=", "time", ".", "time", "(", ")", "\n", "cur_mloss", "=", "0.0", "\n", "# looping over each sentence in the train data, calculating the loss and updating the model", "\n", "for", "idx", ",", "(", "sentences", ",", "tag", ")", "in", "enumerate", "(", "train_data_for_dynet", ")", ":", "\n", "                ", "cur_meta_data", "=", "train_meta_data", "[", "train_data_names", "[", "idx", "]", "]", "if", "self", ".", "use_meta_features", "else", "None", "\n", "my_loss", "=", "dy", ".", "pickneglogsoftmax", "(", "self", ".", "_calc_scores_embedded_mlp", "(", "sentences", "=", "sentences", ",", "W_emb", "=", "W_emb", ",", "\n", "W_mlp", "=", "W_mlp", ",", "b_mlp", "=", "b_mlp", ",", "V_mlp", "=", "V_mlp", ",", "\n", "a_mlp", "=", "a_mlp", ",", "meta_data", "=", "cur_meta_data", ")", ",", "tag", ")", "\n", "cur_mloss", "+=", "my_loss", ".", "value", "(", ")", "\n", "my_loss", ".", "backward", "(", ")", "\n", "trainer", ".", "update", "(", ")", "\n", "# updating the mloss for early stopping purposes", "\n", "", "mloss", "[", "0", "]", "=", "mloss", "[", "1", "]", "\n", "mloss", "[", "1", "]", "=", "cur_mloss", "\n", "print", "(", "\"iter %r: train loss/sr=%.4f, time=%.2fs\"", "%", "(", "ITER", ",", "cur_mloss", "/", "len", "(", "train_data_for_dynet", ")", ",", "\n", "time", ".", "time", "(", ")", "-", "start", ")", ")", "\n", "# Perform testing validation", "\n", "test_correct", "=", "0.0", "\n", "for", "idx", ",", "(", "words", ",", "tag", ")", "in", "enumerate", "(", "test_data_for_dynet", ")", ":", "\n", "                ", "cur_meta_data", "=", "test_meta_data", "[", "test_data_names", "[", "idx", "]", "]", "if", "self", ".", "use_meta_features", "else", "None", "\n", "scores", "=", "self", ".", "_calc_scores_embedded_mlp", "(", "sentences", "=", "words", ",", "W_emb", "=", "W_emb", ",", "W_mlp", "=", "W_mlp", ",", "b_mlp", "=", "b_mlp", ",", "\n", "V_mlp", "=", "V_mlp", ",", "a_mlp", "=", "a_mlp", ",", "meta_data", "=", "cur_meta_data", ")", ".", "npvalue", "(", ")", "\n", "predict", "=", "np", ".", "argmax", "(", "scores", ")", "\n", "if", "predict", "==", "tag", ":", "\n", "                    ", "test_correct", "+=", "1", "\n", "", "", "print", "(", "\"iter %r: test acc=%.4f\"", "%", "(", "ITER", ",", "test_correct", "/", "len", "(", "test_data_for_dynet", ")", ")", ")", "\n", "# Perform testing validation after all batches ended", "\n", "", "test_correct", "=", "0.0", "\n", "test_predictions", "=", "[", "]", "\n", "for", "idx", ",", "(", "words", ",", "tag", ")", "in", "enumerate", "(", "test_data_for_dynet", ")", ":", "\n", "            ", "cur_meta_data", "=", "test_meta_data", "[", "test_data_names", "[", "idx", "]", "]", "if", "self", ".", "use_meta_features", "else", "None", "\n", "cur_score", "=", "self", ".", "_calc_scores_embedded_mlp", "(", "sentences", "=", "words", ",", "W_emb", "=", "W_emb", ",", "W_mlp", "=", "W_mlp", ",", "b_mlp", "=", "b_mlp", ",", "\n", "V_mlp", "=", "V_mlp", ",", "a_mlp", "=", "a_mlp", ",", "meta_data", "=", "cur_meta_data", ")", ".", "npvalue", "(", ")", "\n", "# adding the prediction of the sr to draw (to be label 1) and calculating the acc on the fly", "\n", "test_predictions", ".", "append", "(", "cur_score", "[", "1", "]", ")", "\n", "predict", "=", "np", ".", "argmax", "(", "cur_score", ")", "\n", "if", "predict", "==", "tag", ":", "\n", "                ", "test_correct", "+=", "1", "\n", "\n", "", "", "y_test", "=", "[", "a", "[", "1", "]", "for", "a", "in", "test_data_for_dynet", "]", "\n", "self", ".", "calc_eval_measures", "(", "y_true", "=", "y_test", ",", "y_pred", "=", "test_predictions", ",", "nomalize_y", "=", "True", ")", "\n", "print", "(", "\"final test acc=%.4f\"", "%", "(", "test_correct", "/", "len", "(", "y_test", ")", ")", ")", "\n", "\n", "return", "self", ".", "eval_results", ",", "model", ",", "test_predictions", "\n", "", "", ""]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.parallel_lstm.ParallelLstm.__init__": [[49, 55], ["nn_classifier.NNClassifier.__init__"], "methods", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.single_lstm.SinglelLstm.__init__"], ["def", "__init__", "(", "self", ",", "tokenizer", ",", "eval_measures", ",", "emb_size", "=", "100", ",", "hid_size", "=", "100", ",", "early_stopping", "=", "True", ",", "\n", "epochs", "=", "10", ",", "use_meta_features", "=", "True", ",", "seed", "=", "1984", ",", "use_bilstm", "=", "False", ")", ":", "\n", "        ", "super", "(", "ParallelLstm", ",", "self", ")", ".", "__init__", "(", "model", "=", "None", ",", "tokenizer", "=", "tokenizer", ",", "eval_measures", "=", "eval_measures", ",", "emb_size", "=", "emb_size", ",", "\n", "hid_size", "=", "hid_size", ",", "early_stopping", "=", "early_stopping", ",", "epochs", "=", "epochs", ",", "\n", "use_meta_features", "=", "use_meta_features", ",", "seed", "=", "seed", ")", "\n", "self", ".", "use_bilstm", "=", "use_bilstm", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.parallel_lstm.ParallelLstm._calc_scores_two_layers": [[56, 99], ["dynet.renew_cg", "first_lstm.initial_state", "dynet.average", "first_embs.append", "dynet.tanh", "dynet.logistic", "dynet.inputVector", "dynet.concatenate", "dynet.tanh", "dynet.logistic", "dynet.lookup", "first_lstm.initial_state.transduce", "sorted", "meta_data.items"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_calc_scores_two_layers", "(", "sentences", ",", "W_emb", ",", "first_lstm", ",", "W_mlp", ",", "b_mlp", ",", "V_mlp", ",", "a_mlp", ",", "meta_data", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        calculating the score for parallel LSTM network (in a specific state along learning phase)\n        :param sentences: list\n            list of lists of sentences (represented already as numbers and not letters)\n        :param first_lstm:\n\n        :param W_mlp: model parameter (dynet obj). size: (hid_size, emb_size + meta_data_dim)\n            matrix holding weights of the mlp phase\n        :param b_mlp: model parameter (dynet obj). size: (hid_size,)\n            vector holding weights of intercept for each hidden state\n        :param V_mlp: model parameter (dynet obj). size: (2, hid_size)\n            matrix holding weights of the logisitc regression phase. 2 is there due to the fact we are in a binary\n            classification\n        :param a_mlp: model parameter (dynet obj). size: (1,)\n            intercept value for the logistic regression phase\n        :param meta_data: dict or None\n            meta data features for the model. If None - meta data is not used\n        :return: dynet parameter. size: (2,)\n            prediction of the instance to be a drawing one according to the model (vector of 2, first place is the\n            probability to be a drawing team)\n        \"\"\"", "\n", "dy", ".", "renew_cg", "(", ")", "\n", "word_embs", "=", "[", "[", "dy", ".", "lookup", "(", "W_emb", ",", "w", ")", "for", "w", "in", "words", "]", "for", "words", "in", "sentences", "]", "\n", "first_init", "=", "first_lstm", ".", "initial_state", "(", ")", "\n", "first_embs", "=", "[", "]", "\n", "for", "wb", "in", "word_embs", ":", "\n", "            ", "first_embs", ".", "append", "(", "first_init", ".", "transduce", "(", "wb", ")", ")", "\n", "", "last_comp_in_first_layer", "=", "[", "i", "[", "-", "1", "]", "for", "i", "in", "first_embs", "]", "\n", "# calculating the avg over all last components of the LSTMs", "\n", "# if wanted to take the maximum, one can use dy.emax instead of dy.average (but it is not too recommended)", "\n", "first_layer_avg", "=", "dy", ".", "average", "(", "last_comp_in_first_layer", ")", "\n", "if", "meta_data", "is", "None", ":", "\n", "            ", "h", "=", "dy", ".", "tanh", "(", "(", "W_mlp", "*", "first_layer_avg", ")", "+", "b_mlp", ")", "\n", "prediction", "=", "dy", ".", "logistic", "(", "(", "V_mlp", "*", "h", ")", "+", "a_mlp", ")", "\n", "", "else", ":", "\n", "            ", "meta_data_ordered", "=", "[", "value", "for", "key", ",", "value", "in", "sorted", "(", "meta_data", ".", "items", "(", ")", ")", "]", "\n", "meta_data_vector", "=", "dy", ".", "inputVector", "(", "meta_data_ordered", ")", "\n", "first_layer_avg_and_meta_data", "=", "dy", ".", "concatenate", "(", "[", "first_layer_avg", ",", "meta_data_vector", "]", ")", "\n", "h", "=", "dy", ".", "tanh", "(", "(", "W_mlp", "*", "first_layer_avg_and_meta_data", ")", "+", "b_mlp", ")", "\n", "prediction", "=", "dy", ".", "logistic", "(", "(", "V_mlp", "*", "h", ")", "+", "a_mlp", ")", "\n", "", "return", "prediction", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.parallel_lstm.ParallelLstm.fit_predict": [[100, 213], ["list", "list", "dynet.Model", "dynet.AdamTrainer", "dynet.LSTMBuilder", "dynet.Model.add_parameters", "dynet.Model.add_parameters", "dynet.Model.add_parameters", "dynet.Model.add_parameters", "range", "enumerate", "parallel_lstm.ParallelLstm.calc_eval_measures", "print", "parallel_lstm.ParallelLstm.data_prep_meta_features", "len", "parallel_lstm.ParallelLstm.get_reddit_sentences", "parallel_lstm.ParallelLstm.get_reddit_sentences", "dynet.Model.add_lookup_parameters", "parallel_lstm.ParallelLstm.build_embedding_matrix", "dynet.Model.add_lookup_parameters", "time.time", "enumerate", "print", "enumerate", "print", "parallel_lstm.ParallelLstm._calc_scores_two_layers().npvalue", "test_predicitons.append", "numpy.argmax", "print", "dynet.pickneglogsoftmax", "dynet.pickneglogsoftmax.value", "dynet.pickneglogsoftmax.backward", "dynet.AdamTrainer.update", "parallel_lstm.ParallelLstm._calc_scores_two_layers().npvalue", "numpy.argmax", "parallel_lstm.ParallelLstm._calc_scores_two_layers", "parallel_lstm.ParallelLstm._calc_scores_two_layers", "len", "parallel_lstm.ParallelLstm._calc_scores_two_layers", "list", "len", "time.time", "len", "train_meta_data.keys"], "methods", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.calc_eval_measures", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.data_prep_meta_features", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.get_reddit_sentences", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.get_reddit_sentences", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.build_embedding_matrix", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.parallel_lstm.ParallelLstm._calc_scores_two_layers", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.parallel_lstm.ParallelLstm._calc_scores_two_layers", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.parallel_lstm.ParallelLstm._calc_scores_two_layers"], ["", "def", "fit_predict", "(", "self", ",", "train_data", ",", "test_data", ",", "embedding_file", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        fits a parallel LSTM model\n        :param train_data: list\n            list of sr objects to be used as train set\n        :param test_data: list\n            list of sr objects to be used as train set\n        :param embedding_file: str\n            the path to the exact embedding file to be used. This should be a txt file, each row represents\n            a word and it's embedding (separated by whitespace). Example can be taken from 'glove' pre-trained models\n            If None, we build an embedding from random normal distribution\n        :return: tuple\n            tuple with 3 variables:\n            self.eval_results, model, test_predicitons\n            1. eval_results: dictionary with evaluation measures over the test set\n            2. model: the MLP trained model which was used\n            3. test_predicitons: list with predictions to each sr in the test dataset\n        \"\"\"", "\n", "# case we wish to use meta features along modeling, we need to prepare the SRs objects for this", "\n", "if", "self", ".", "use_meta_features", ":", "\n", "            ", "train_meta_data", ",", "test_meta_data", "=", "self", ".", "data_prep_meta_features", "(", "train_data", "=", "train_data", ",", "test_data", "=", "test_data", ",", "update_objects", "=", "False", ")", "\n", "meta_data_dim", "=", "len", "(", "train_meta_data", "[", "list", "(", "train_meta_data", ".", "keys", "(", ")", ")", "[", "0", "]", "]", ")", "\n", "", "else", ":", "\n", "            ", "train_meta_data", "=", "None", "\n", "test_meta_data", "=", "None", "\n", "meta_data_dim", "=", "0", "\n", "# next we are creating the input for the algorithm. train_data_for_dynet will contain list of lists. Each inner", "\n", "# list contains the words index relevant to the specific sentence", "\n", "", "train_data_for_dynet", "=", "list", "(", "self", ".", "get_reddit_sentences", "(", "sr_objects", "=", "train_data", ")", ")", "\n", "train_data_names", "=", "[", "i", "[", "2", "]", "for", "i", "in", "train_data_for_dynet", "]", "# list of train sr names", "\n", "train_data_for_dynet", "=", "[", "(", "i", "[", "0", "]", ",", "i", "[", "1", "]", ")", "for", "i", "in", "train_data_for_dynet", "]", "\n", "\n", "# test_data_for_dynet will contain list of lists. Each inner list contains the words index relevant to the", "\n", "# specific sentence", "\n", "test_data_for_dynet", "=", "list", "(", "self", ".", "get_reddit_sentences", "(", "sr_objects", "=", "test_data", ")", ")", "\n", "test_data_names", "=", "[", "i", "[", "2", "]", "for", "i", "in", "test_data_for_dynet", "]", "# list of test sr names", "\n", "# Need to check here that the order is saved!!!!", "\n", "test_data_for_dynet", "=", "[", "(", "i", "[", "0", "]", ",", "i", "[", "1", "]", ")", "for", "i", "in", "test_data_for_dynet", "]", "\n", "\n", "# Start DyNet and define trainer", "\n", "model", "=", "dy", ".", "Model", "(", ")", "\n", "trainer", "=", "dy", ".", "AdamTrainer", "(", "model", ")", "\n", "# Define the model", "\n", "# Word embeddings part", "\n", "if", "embedding_file", "is", "None", ":", "\n", "            ", "W_emb", "=", "model", ".", "add_lookup_parameters", "(", "(", "self", ".", "nwords", ",", "self", ".", "emb_size", ")", ")", "\n", "", "else", ":", "\n", "            ", "external_embedding", "=", "self", ".", "build_embedding_matrix", "(", "embedding_file", ")", "\n", "W_emb", "=", "model", ".", "add_lookup_parameters", "(", "(", "self", ".", "nwords", ",", "self", ".", "emb_size", ")", ",", "init", "=", "external_embedding", ")", "\n", "", "first_lstm", "=", "dy", ".", "LSTMBuilder", "(", "1", ",", "self", ".", "emb_size", ",", "self", ".", "hid_size", ",", "model", ")", "# Forward LSTM", "\n", "# Last layer with network is an MLP one, case we are not using meta data, meta_data_dim will be zero", "\n", "# and hence not relevant", "\n", "W_mlp", "=", "model", ".", "add_parameters", "(", "(", "self", ".", "hid_size", ",", "self", ".", "hid_size", "+", "meta_data_dim", ")", ")", "\n", "b_mlp", "=", "model", ".", "add_parameters", "(", "self", ".", "hid_size", ")", "\n", "V_mlp", "=", "model", ".", "add_parameters", "(", "(", "self", ".", "ntags", ",", "self", ".", "hid_size", ")", ")", "\n", "a_mlp", "=", "model", ".", "add_parameters", "(", "1", ")", "\n", "\n", "mloss", "=", "[", "0.0", ",", "0.0", "]", "# we always save the current run loss and the prev one (for early stopping purposes", "\n", "for", "ITER", "in", "range", "(", "self", ".", "epochs", ")", ":", "\n", "# checking the early stopping criterion", "\n", "            ", "if", "self", ".", "early_stopping", "and", "(", "ITER", ">=", "(", "self", ".", "epochs", "*", "1.0", "/", "2", ")", ")", "and", "(", "(", "mloss", "[", "0", "]", "-", "mloss", "[", "1", "]", ")", "*", "1.0", "/", "mloss", "[", "0", "]", ")", "<=", "0.01", ":", "\n", "                ", "print", "(", "\"Early stopping has been applied since improvement was not greater than 1%\"", ")", "\n", "break", "\n", "# Perform training", "\n", "", "start", "=", "time", ".", "time", "(", ")", "\n", "cur_mloss", "=", "0.0", "\n", "for", "idx", ",", "(", "sentences", ",", "tag", ")", "in", "enumerate", "(", "train_data_for_dynet", ")", ":", "\n", "                ", "cur_meta_data", "=", "train_meta_data", "[", "train_data_names", "[", "idx", "]", "]", "if", "self", ".", "use_meta_features", "else", "None", "\n", "my_loss", "=", "dy", ".", "pickneglogsoftmax", "(", "self", ".", "_calc_scores_two_layers", "(", "sentences", "=", "sentences", ",", "W_emb", "=", "W_emb", ",", "\n", "first_lstm", "=", "first_lstm", ",", "W_mlp", "=", "W_mlp", ",", "b_mlp", "=", "b_mlp", ",", "\n", "V_mlp", "=", "V_mlp", ",", "a_mlp", "=", "a_mlp", ",", "\n", "meta_data", "=", "cur_meta_data", ")", ",", "tag", ")", "\n", "cur_mloss", "+=", "my_loss", ".", "value", "(", ")", "\n", "my_loss", ".", "backward", "(", ")", "\n", "trainer", ".", "update", "(", ")", "\n", "# updating the mloss for early stopping purposes", "\n", "", "mloss", "[", "0", "]", "=", "mloss", "[", "1", "]", "\n", "mloss", "[", "1", "]", "=", "cur_mloss", "\n", "print", "(", "\"iter %r: train loss/sr=%.4f, time=%.2fs\"", "%", "(", "ITER", ",", "cur_mloss", "/", "len", "(", "train_data_for_dynet", ")", ",", "\n", "time", ".", "time", "(", ")", "-", "start", ")", ")", "\n", "# Perform testing validation (at the end of current epoch)", "\n", "test_correct", "=", "0.0", "\n", "for", "idx", ",", "(", "words", ",", "tag", ")", "in", "enumerate", "(", "test_data_for_dynet", ")", ":", "\n", "                ", "cur_meta_data", "=", "test_meta_data", "[", "test_data_names", "[", "idx", "]", "]", "if", "self", ".", "use_meta_features", "else", "None", "\n", "scores", "=", "self", ".", "_calc_scores_two_layers", "(", "sentences", "=", "words", ",", "W_emb", "=", "W_emb", ",", "first_lstm", "=", "first_lstm", ",", "\n", "W_mlp", "=", "W_mlp", ",", "b_mlp", "=", "b_mlp", ",", "V_mlp", "=", "V_mlp", ",", "a_mlp", "=", "a_mlp", ",", "\n", "meta_data", "=", "cur_meta_data", ")", ".", "npvalue", "(", ")", "\n", "predict", "=", "np", ".", "argmax", "(", "scores", ")", "\n", "if", "predict", "==", "tag", ":", "\n", "                    ", "test_correct", "+=", "1", "\n", "", "", "print", "(", "\"iter %r: test acc=%.4f\"", "%", "(", "ITER", ",", "test_correct", "/", "len", "(", "test_data_for_dynet", ")", ")", ")", "\n", "# Perform testing validation after all ephocs ended", "\n", "", "test_correct", "=", "0.0", "\n", "test_predicitons", "=", "[", "]", "\n", "for", "idx", ",", "(", "words", ",", "tag", ")", "in", "enumerate", "(", "test_data_for_dynet", ")", ":", "\n", "            ", "cur_meta_data", "=", "test_meta_data", "[", "test_data_names", "[", "idx", "]", "]", "if", "self", ".", "use_meta_features", "else", "None", "\n", "cur_score", "=", "self", ".", "_calc_scores_two_layers", "(", "sentences", "=", "words", ",", "W_emb", "=", "W_emb", ",", "first_lstm", "=", "first_lstm", ",", "\n", "W_mlp", "=", "W_mlp", ",", "b_mlp", "=", "b_mlp", ",", "V_mlp", "=", "V_mlp", ",", "a_mlp", "=", "a_mlp", ",", "\n", "meta_data", "=", "cur_meta_data", ")", ".", "npvalue", "(", ")", "\n", "# adding the prediction of the sr to draw (to be label 1) and calculating the acc on the fly", "\n", "test_predicitons", ".", "append", "(", "cur_score", "[", "1", "]", ")", "\n", "predict", "=", "np", ".", "argmax", "(", "cur_score", ")", "\n", "if", "predict", "==", "tag", ":", "\n", "                ", "test_correct", "+=", "1", "\n", "\n", "", "", "y_test", "=", "[", "a", "[", "1", "]", "for", "a", "in", "test_data_for_dynet", "]", "\n", "self", ".", "calc_eval_measures", "(", "y_true", "=", "y_test", ",", "y_pred", "=", "test_predicitons", ",", "nomalize_y", "=", "True", ")", "\n", "print", "(", "\"final test acc=%.4f\"", "%", "(", "test_correct", "/", "len", "(", "y_test", ")", ")", ")", "\n", "\n", "return", "self", ".", "eval_results", ",", "model", ",", "test_predicitons", "\n", "", "", ""]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.single_lstm.SinglelLstm.__init__": [[49, 55], ["nn_classifier.NNClassifier.__init__"], "methods", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.single_lstm.SinglelLstm.__init__"], ["def", "__init__", "(", "self", ",", "tokenizer", ",", "eval_measures", ",", "emb_size", "=", "100", ",", "hid_size", "=", "100", ",", "early_stopping", "=", "True", ",", "\n", "epochs", "=", "10", ",", "use_meta_features", "=", "True", ",", "seed", "=", "1984", ",", "use_bilstm", "=", "False", ")", ":", "\n", "        ", "super", "(", "SinglelLstm", ",", "self", ")", ".", "__init__", "(", "model", "=", "None", ",", "tokenizer", "=", "tokenizer", ",", "eval_measures", "=", "eval_measures", ",", "emb_size", "=", "emb_size", ",", "\n", "hid_size", "=", "hid_size", ",", "early_stopping", "=", "early_stopping", ",", "epochs", "=", "epochs", ",", "\n", "use_meta_features", "=", "use_meta_features", ",", "seed", "=", "seed", ")", "\n", "self", ".", "use_bilstm", "=", "use_bilstm", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.single_lstm.SinglelLstm._calc_scores_single_layer_lstm": [[57, 96], ["dynet.renew_cg", "fwdLSTM.initial_state", "fwdLSTM.initial_state.transduce", "dynet.lookup", "bwdLSTM.initial_state", "bwdLSTM.initial_state.transduce", "dynet.softmax", "reversed", "dynet.concatenate", "dynet.concatenate"], "methods", ["None"], ["", "def", "_calc_scores_single_layer_lstm", "(", "self", ",", "words", ",", "W_emb", ",", "fwdLSTM", ",", "bwdLSTM", ",", "W_sm", ",", "b_sm", ",", "normalize_results", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        calculating the score for an LSTM network (in a specific state along learning phase)\n        :param words: list\n            list of words representing a sentence (represented already as numbers and not letters)\n        :param W_emb: lookup parameter (dynet obj). size: (emb_size x nwords)\n            matrix holding the word embedding values\n        :param fwdLSTM:\n\n        :param bwdLSTM:\n\n        :param W_sm: model parameter (dynet obj). size: (hid_size, emb_size + meta_data_dim)\n            matrix holding weights of the mlp phase\n        :param b_sm: model parameter (dynet obj). size: (hid_size,)\n            vector holding weights of intercept for each hidden state\n        :param normalize_results:\n\n        :return: dynet parameter. size: (2,)\n            prediction of the instance to be a drawing one according to the model (vector of 2, first place is the\n            probability to be a drawing team)\n        \"\"\"", "\n", "dy", ".", "renew_cg", "(", ")", "\n", "# embed the words", "\n", "word_embs", "=", "[", "dy", ".", "lookup", "(", "W_emb", ",", "x", ")", "for", "x", "in", "words", "]", "\n", "fwd_init", "=", "fwdLSTM", ".", "initial_state", "(", ")", "\n", "fwd_embs", "=", "fwd_init", ".", "transduce", "(", "word_embs", ")", "\n", "# case we wish to use bi directional LSTM model", "\n", "if", "self", ".", "use_bilstm", ":", "\n", "            ", "bwd_init", "=", "bwdLSTM", ".", "initial_state", "(", ")", "\n", "bwd_embs", "=", "bwd_init", ".", "transduce", "(", "reversed", "(", "word_embs", ")", ")", "\n", "score_not_normalized", "=", "W_sm", "*", "dy", ".", "concatenate", "(", "[", "fwd_embs", "[", "-", "1", "]", ",", "bwd_embs", "[", "-", "1", "]", "]", ")", "+", "b_sm", "\n", "", "else", ":", "\n", "            ", "score_not_normalized", "=", "W_sm", "*", "dy", ".", "concatenate", "(", "[", "fwd_embs", "[", "-", "1", "]", "]", ")", "+", "b_sm", "\n", "\n", "# case we wish to normalize results (by default we do want to)", "\n", "", "if", "normalize_results", ":", "\n", "            ", "return", "dy", ".", "softmax", "(", "score_not_normalized", ")", "\n", "", "else", ":", "\n", "            ", "return", "score_not_normalized", "\n", "\n"]], "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.single_lstm.SinglelLstm.fit_predict": [[97, 165], ["dynet.Model", "dynet.AdamTrainer", "dynet.LSTMBuilder", "dynet.LSTMBuilder", "dynet.Model.add_parameters", "range", "dynet.Model.add_lookup_parameters", "single_lstm.SinglelLstm.build_embedding_matrix", "dynet.Model.add_lookup_parameters", "dynet.Model.add_parameters", "dynet.Model.add_parameters", "random.seed", "random.shuffle", "time.time", "enumerate", "print", "print", "single_lstm.SinglelLstm._calc_scores_single_layer_lstm().npvalue", "all_scores.append", "dynet.pickneglogsoftmax", "dynet.pickneglogsoftmax.value", "dynet.pickneglogsoftmax.backward", "dynet.AdamTrainer.update", "single_lstm.SinglelLstm._calc_scores_single_layer_lstm().npvalue", "numpy.argmax", "single_lstm.SinglelLstm._calc_scores_single_layer_lstm", "single_lstm.SinglelLstm._calc_scores_single_layer_lstm", "single_lstm.SinglelLstm._calc_scores_single_layer_lstm", "len", "time.time", "len"], "methods", ["home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.nn_classifier.NNClassifier.build_embedding_matrix", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.single_lstm.SinglelLstm._calc_scores_single_layer_lstm", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.single_lstm.SinglelLstm._calc_scores_single_layer_lstm", "home.repos.pwc.inspect_result.naslabbgu_rplace-engagement-prediction.neural_net.single_lstm.SinglelLstm._calc_scores_single_layer_lstm"], ["", "", "def", "fit_predict", "(", "self", ",", "train_data", ",", "test_data", ",", "embedding_file", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        fits a single LSTM model\n        :param train_data: list\n            list of sr objects to be used as train set\n        :param test_data: list\n            list of sr objects to be used as train set\n        :param embedding_file: str\n            the path to the exact embedding file to be used. This should be a txt file, each row represents\n            a word and it's embedding (separated by whitespace). Example can be taken from 'glove' pre-trained models\n            If None, we build an embedding from random normal distribution\n        :return: tuple\n            tuple with 3 variables:\n            self.eval_results, model, test_predicitons\n            1. eval_results: dictionary with evaluation measures over the test set\n            2. model: the MLP trained model which was used\n            3. test_predicitons: list with predictions to each sr in the test dataset\n        \"\"\"", "\n", "# Start DyNet and define trainer", "\n", "model", "=", "dy", ".", "Model", "(", ")", "\n", "trainer", "=", "dy", ".", "AdamTrainer", "(", "model", ")", "\n", "\n", "# Define the model", "\n", "# Word embeddings part", "\n", "if", "embedding_file", "is", "None", ":", "\n", "            ", "W_emb", "=", "model", ".", "add_lookup_parameters", "(", "(", "self", ".", "nwords", ",", "self", ".", "emb_size", ")", ")", "\n", "", "else", ":", "\n", "            ", "external_embedding", "=", "self", ".", "build_embedding_matrix", "(", "embedding_file", ")", "\n", "W_emb", "=", "model", ".", "add_lookup_parameters", "(", "(", "self", ".", "nwords", ",", "self", ".", "emb_size", ")", ",", "init", "=", "external_embedding", ")", "\n", "", "fwdLSTM", "=", "dy", ".", "LSTMBuilder", "(", "1", ",", "self", ".", "emb_size", ",", "self", ".", "hid_size", ",", "model", ")", "# Forward LSTM", "\n", "bwdLSTM", "=", "dy", ".", "LSTMBuilder", "(", "1", ",", "self", ".", "emb_size", ",", "self", ".", "hid_size", ",", "model", ")", "# Backward LSTM", "\n", "if", "self", ".", "use_bilstm", ":", "\n", "            ", "W_sm", "=", "model", ".", "add_parameters", "(", "(", "self", ".", "ntags", ",", "2", "*", "self", ".", "hid_size", ")", ")", "# Softmax weights", "\n", "", "else", ":", "\n", "            ", "W_sm", "=", "model", ".", "add_parameters", "(", "(", "self", ".", "ntags", ",", "self", ".", "hid_size", ")", ")", "# Softmax weights", "\n", "", "b_sm", "=", "model", ".", "add_parameters", "(", "self", ".", "ntags", ")", "# Softmax bias", "\n", "for", "ITER", "in", "range", "(", "self", ".", "epochs", ")", ":", "\n", "# Perform training", "\n", "            ", "random", ".", "seed", "(", "self", ".", "seed", ")", "\n", "random", ".", "shuffle", "(", "train_data", ")", "\n", "train_loss", "=", "0.0", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "for", "idx", ",", "(", "words", ",", "tag", ")", "in", "enumerate", "(", "train_data", ")", ":", "\n", "                ", "my_loss", "=", "dy", ".", "pickneglogsoftmax", "(", "self", ".", "_calc_scores_single_layer_lstm", "(", "words", "=", "words", ",", "W_emb", "=", "W_emb", ",", "\n", "fwdLSTM", "=", "fwdLSTM", ",", "bwdLSTM", "=", "bwdLSTM", ",", "\n", "W_sm", "=", "W_sm", ",", "b_sm", "=", "b_sm", ",", "\n", "normalize_results", "=", "True", ")", ",", "tag", ")", "\n", "train_loss", "+=", "my_loss", ".", "value", "(", ")", "\n", "my_loss", ".", "backward", "(", ")", "\n", "trainer", ".", "update", "(", ")", "\n", "", "print", "(", "\"iter %r: train loss/sent=%.4f, time=%.2fs\"", "%", "(", "ITER", ",", "train_loss", "/", "len", "(", "train_data", ")", ",", "time", ".", "time", "(", ")", "-", "start", ")", ")", "\n", "# Perform testing validation", "\n", "test_correct", "=", "0.0", "\n", "for", "words", ",", "tag", "in", "test_data", ":", "\n", "                ", "scores", "=", "self", ".", "_calc_scores_single_layer_lstm", "(", "words", "=", "words", ",", "W_emb", "=", "W_emb", ",", "fwdLSTM", "=", "fwdLSTM", ",", "\n", "bwdLSTM", "=", "bwdLSTM", ",", "W_sm", "=", "W_sm", ",", "b_sm", "=", "b_sm", ")", ".", "npvalue", "(", ")", "\n", "predict", "=", "np", ".", "argmax", "(", "scores", ")", "\n", "if", "predict", "==", "tag", ":", "\n", "                    ", "test_correct", "+=", "1", "\n", "", "", "print", "(", "\"iter %r: test acc=%.4f\"", "%", "(", "ITER", ",", "test_correct", "/", "len", "(", "test_data", ")", ")", ")", "\n", "# Perform testing validation after all batches ended", "\n", "", "all_scores", "=", "[", "]", "\n", "for", "words", ",", "tag", "in", "test_data", ":", "\n", "            ", "cur_score", "=", "self", ".", "_calc_scores_single_layer_lstm", "(", "words", "=", "words", ",", "W_emb", "=", "W_emb", ",", "fwdLSTM", "=", "fwdLSTM", ",", "bwdLSTM", "=", "bwdLSTM", ",", "\n", "W_sm", "=", "W_sm", ",", "b_sm", "=", "b_sm", ",", "normalize_results", "=", "True", ")", ".", "npvalue", "(", ")", "\n", "all_scores", ".", "append", "(", "cur_score", ")", "\n", "", "return", "all_scores", "\n", "", "", ""]]}