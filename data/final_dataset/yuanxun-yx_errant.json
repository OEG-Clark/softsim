{"home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.alignment.Alignment.__init__": [[13, 21], ["alignment.Alignment.align", "alignment.Alignment.get_cheapest_align_seq"], "methods", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.annotator.Annotator.align", "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.alignment.Alignment.get_cheapest_align_seq"], ["def", "__init__", "(", "self", ",", "orig", ",", "cor", ",", "lev", "=", "False", ")", ":", "\n", "# Set orig and cor", "\n", "        ", "self", ".", "orig", "=", "orig", "\n", "self", ".", "cor", "=", "cor", "\n", "# Align orig and cor and get the cost and op matrices", "\n", "self", ".", "cost_matrix", ",", "self", ".", "op_matrix", "=", "self", ".", "align", "(", "lev", ")", "\n", "# Get the cheapest align sequence from the op matrix", "\n", "self", ".", "align_seq", "=", "self", ".", "get_cheapest_align_seq", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.alignment.Alignment.align": [[24, 82], ["len", "len", "range", "range", "range", "range", "range", "range", "range", "range", "float", "costs.index", "min", "alignment.Alignment.get_sub_cost", "str", "sorted", "sorted"], "methods", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.alignment.Alignment.get_sub_cost"], ["", "def", "align", "(", "self", ",", "lev", ")", ":", "\n", "# Sentence lengths", "\n", "        ", "o_len", "=", "len", "(", "self", ".", "orig", ")", "\n", "c_len", "=", "len", "(", "self", ".", "cor", ")", "\n", "# Lower case token IDs (for transpositions)", "\n", "o_low", "=", "[", "o", ".", "lower", "for", "o", "in", "self", ".", "orig", "]", "\n", "c_low", "=", "[", "c", ".", "lower", "for", "c", "in", "self", ".", "cor", "]", "\n", "# Create the cost_matrix and the op_matrix", "\n", "cost_matrix", "=", "[", "[", "0.0", "for", "j", "in", "range", "(", "c_len", "+", "1", ")", "]", "for", "i", "in", "range", "(", "o_len", "+", "1", ")", "]", "\n", "op_matrix", "=", "[", "[", "\"O\"", "for", "j", "in", "range", "(", "c_len", "+", "1", ")", "]", "for", "i", "in", "range", "(", "o_len", "+", "1", ")", "]", "\n", "# Fill in the edges", "\n", "for", "i", "in", "range", "(", "1", ",", "o_len", "+", "1", ")", ":", "\n", "            ", "cost_matrix", "[", "i", "]", "[", "0", "]", "=", "cost_matrix", "[", "i", "-", "1", "]", "[", "0", "]", "+", "1", "\n", "op_matrix", "[", "i", "]", "[", "0", "]", "=", "\"D\"", "\n", "", "for", "j", "in", "range", "(", "1", ",", "c_len", "+", "1", ")", ":", "\n", "            ", "cost_matrix", "[", "0", "]", "[", "j", "]", "=", "cost_matrix", "[", "0", "]", "[", "j", "-", "1", "]", "+", "1", "\n", "op_matrix", "[", "0", "]", "[", "j", "]", "=", "\"I\"", "\n", "\n", "# Loop through the cost_matrix", "\n", "", "for", "i", "in", "range", "(", "o_len", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "c_len", ")", ":", "\n", "# Matches", "\n", "                ", "if", "self", ".", "orig", "[", "i", "]", ".", "orth", "==", "self", ".", "cor", "[", "j", "]", ".", "orth", ":", "\n", "                    ", "cost_matrix", "[", "i", "+", "1", "]", "[", "j", "+", "1", "]", "=", "cost_matrix", "[", "i", "]", "[", "j", "]", "\n", "op_matrix", "[", "i", "+", "1", "]", "[", "j", "+", "1", "]", "=", "\"M\"", "\n", "# Non-matches", "\n", "", "else", ":", "\n", "                    ", "del_cost", "=", "cost_matrix", "[", "i", "]", "[", "j", "+", "1", "]", "+", "1", "\n", "ins_cost", "=", "cost_matrix", "[", "i", "+", "1", "]", "[", "j", "]", "+", "1", "\n", "trans_cost", "=", "float", "(", "\"inf\"", ")", "\n", "# Standard Levenshtein (S = 1)", "\n", "if", "lev", ":", "sub_cost", "=", "cost_matrix", "[", "i", "]", "[", "j", "]", "+", "1", "\n", "# Linguistic Damerau-Levenshtein", "\n", "else", ":", "\n", "# Custom substitution", "\n", "                        ", "sub_cost", "=", "cost_matrix", "[", "i", "]", "[", "j", "]", "+", "self", ".", "get_sub_cost", "(", "self", ".", "orig", "[", "i", "]", ",", "self", ".", "cor", "[", "j", "]", ")", "\n", "# Transpositions require >=2 tokens", "\n", "# Traverse the diagonal while there is not a Match.", "\n", "k", "=", "1", "\n", "while", "i", "-", "k", ">=", "0", "and", "j", "-", "k", ">=", "0", "and", "cost_matrix", "[", "i", "-", "k", "+", "1", "]", "[", "j", "-", "k", "+", "1", "]", "!=", "cost_matrix", "[", "i", "-", "k", "]", "[", "j", "-", "k", "]", ":", "\n", "                            ", "if", "sorted", "(", "o_low", "[", "i", "-", "k", ":", "i", "+", "1", "]", ")", "==", "sorted", "(", "c_low", "[", "j", "-", "k", ":", "j", "+", "1", "]", ")", ":", "\n", "                                ", "trans_cost", "=", "cost_matrix", "[", "i", "-", "k", "]", "[", "j", "-", "k", "]", "+", "k", "\n", "break", "\n", "", "k", "+=", "1", "\n", "# Costs", "\n", "", "", "costs", "=", "[", "trans_cost", ",", "sub_cost", ",", "ins_cost", ",", "del_cost", "]", "\n", "# Get the index of the cheapest (first cheapest if tied)", "\n", "l", "=", "costs", ".", "index", "(", "min", "(", "costs", ")", ")", "\n", "# Save the cost and the op in the matrices", "\n", "cost_matrix", "[", "i", "+", "1", "]", "[", "j", "+", "1", "]", "=", "costs", "[", "l", "]", "\n", "if", "l", "==", "0", ":", "op_matrix", "[", "i", "+", "1", "]", "[", "j", "+", "1", "]", "=", "\"T\"", "+", "str", "(", "k", "+", "1", ")", "\n", "elif", "l", "==", "1", ":", "op_matrix", "[", "i", "+", "1", "]", "[", "j", "+", "1", "]", "=", "\"S\"", "\n", "elif", "l", "==", "2", ":", "op_matrix", "[", "i", "+", "1", "]", "[", "j", "+", "1", "]", "=", "\"I\"", "\n", "else", ":", "op_matrix", "[", "i", "+", "1", "]", "[", "j", "+", "1", "]", "=", "\"D\"", "\n", "# Return the matrices", "\n", "", "", "", "return", "cost_matrix", ",", "op_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.alignment.Alignment.get_sub_cost": [[86, 100], ["Levenshtein.ratio"], "methods", ["None"], ["", "def", "get_sub_cost", "(", "self", ",", "o", ",", "c", ")", ":", "\n", "# Short circuit if the only difference is case", "\n", "        ", "if", "o", ".", "lower", "==", "c", ".", "lower", ":", "return", "0", "\n", "# Lemma cost", "\n", "if", "o", ".", "lemma", "==", "c", ".", "lemma", ":", "lemma_cost", "=", "0", "\n", "else", ":", "lemma_cost", "=", "0.499", "\n", "# POS cost", "\n", "if", "o", ".", "pos", "==", "c", ".", "pos", ":", "pos_cost", "=", "0", "\n", "elif", "o", ".", "pos", "in", "self", ".", "_open_pos", "and", "c", ".", "pos", "in", "self", ".", "_open_pos", ":", "pos_cost", "=", "0.25", "\n", "else", ":", "pos_cost", "=", "0.5", "\n", "# Char cost", "\n", "char_cost", "=", "1", "-", "Levenshtein", ".", "ratio", "(", "o", ".", "text", ",", "c", ".", "text", ")", "\n", "# Combine the costs", "\n", "return", "lemma_cost", "+", "pos_cost", "+", "char_cost", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.alignment.Alignment.get_cheapest_align_seq": [[103, 134], ["align_seq.reverse", "len", "len", "align_seq.append", "align_seq.append", "align_seq.append", "int", "align_seq.append"], "methods", ["None"], ["", "def", "get_cheapest_align_seq", "(", "self", ")", ":", "\n", "        ", "i", "=", "len", "(", "self", ".", "op_matrix", ")", "-", "1", "\n", "j", "=", "len", "(", "self", ".", "op_matrix", "[", "0", "]", ")", "-", "1", "\n", "align_seq", "=", "[", "]", "\n", "# Work backwards from bottom right until we hit top left", "\n", "while", "i", "+", "j", "!=", "0", ":", "\n", "# Get the edit operation in the current cell", "\n", "            ", "op", "=", "self", ".", "op_matrix", "[", "i", "]", "[", "j", "]", "\n", "# Matches and substitutions", "\n", "if", "op", "in", "{", "\"M\"", ",", "\"S\"", "}", ":", "\n", "                ", "align_seq", ".", "append", "(", "(", "op", ",", "i", "-", "1", ",", "i", ",", "j", "-", "1", ",", "j", ")", ")", "\n", "i", "-=", "1", "\n", "j", "-=", "1", "\n", "# Deletions", "\n", "", "elif", "op", "==", "\"D\"", ":", "\n", "                ", "align_seq", ".", "append", "(", "(", "op", ",", "i", "-", "1", ",", "i", ",", "j", ",", "j", ")", ")", "\n", "i", "-=", "1", "\n", "# Insertions", "\n", "", "elif", "op", "==", "\"I\"", ":", "\n", "                ", "align_seq", ".", "append", "(", "(", "op", ",", "i", ",", "i", ",", "j", "-", "1", ",", "j", ")", ")", "\n", "j", "-=", "1", "\n", "# Transpositions", "\n", "", "else", ":", "\n", "# Get the size of the transposition", "\n", "                ", "k", "=", "int", "(", "op", "[", "1", ":", "]", ")", "\n", "align_seq", ".", "append", "(", "(", "op", ",", "i", "-", "k", ",", "i", ",", "j", "-", "k", ",", "j", ")", ")", "\n", "i", "-=", "k", "\n", "j", "-=", "k", "\n", "# Reverse the list to go from left to right and return", "\n", "", "", "align_seq", ".", "reverse", "(", ")", "\n", "return", "align_seq", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.alignment.Alignment.get_all_split_edits": [[136, 142], ["edits.append", "errant.edit.Edit"], "methods", ["None"], ["", "def", "get_all_split_edits", "(", "self", ")", ":", "\n", "        ", "edits", "=", "[", "]", "\n", "for", "align", "in", "self", ".", "align_seq", ":", "\n", "            ", "if", "align", "[", "0", "]", "!=", "\"M\"", ":", "\n", "                ", "edits", ".", "append", "(", "Edit", "(", "self", ".", "orig", ",", "self", ".", "cor", ",", "align", "[", "1", ":", "]", ")", ")", "\n", "", "", "return", "edits", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.alignment.Alignment.get_all_merge_edits": [[144, 152], ["itertools.groupby", "alignment.Alignment.merge_edits", "edits.append", "list", "errant.edit.Edit"], "methods", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.merge_edits"], ["", "def", "get_all_merge_edits", "(", "self", ")", ":", "\n", "        ", "edits", "=", "[", "]", "\n", "for", "op", ",", "group", "in", "groupby", "(", "self", ".", "align_seq", ",", "\n", "lambda", "x", ":", "True", "if", "x", "[", "0", "]", "==", "\"M\"", "else", "False", ")", ":", "\n", "            ", "if", "not", "op", ":", "\n", "                ", "merged", "=", "self", ".", "merge_edits", "(", "list", "(", "group", ")", ")", "\n", "edits", ".", "append", "(", "Edit", "(", "self", ".", "orig", ",", "self", ".", "cor", ",", "merged", "[", "0", "]", "[", "1", ":", "]", ")", ")", "\n", "", "", "return", "edits", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.alignment.Alignment.get_all_equal_edits": [[154, 161], ["itertools.groupby", "alignment.Alignment.merge_edits", "edits.append", "list", "errant.edit.Edit"], "methods", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.merge_edits"], ["", "def", "get_all_equal_edits", "(", "self", ")", ":", "\n", "        ", "edits", "=", "[", "]", "\n", "for", "op", ",", "group", "in", "groupby", "(", "self", ".", "align_seq", ",", "lambda", "x", ":", "x", "[", "0", "]", ")", ":", "\n", "            ", "if", "op", "!=", "\"M\"", ":", "\n", "                ", "merged", "=", "self", ".", "merge_edits", "(", "list", "(", "group", ")", ")", "\n", "edits", ".", "append", "(", "Edit", "(", "self", ".", "orig", ",", "self", ".", "cor", ",", "merged", "[", "0", "]", "[", "1", ":", "]", ")", ")", "\n", "", "", "return", "edits", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.alignment.Alignment.merge_edits": [[163, 166], ["None"], "methods", ["None"], ["", "def", "merge_edits", "(", "self", ",", "seq", ")", ":", "\n", "        ", "if", "seq", ":", "return", "[", "(", "\"X\"", ",", "seq", "[", "0", "]", "[", "1", "]", ",", "seq", "[", "-", "1", "]", "[", "2", "]", ",", "seq", "[", "0", "]", "[", "3", "]", ",", "seq", "[", "-", "1", "]", "[", "4", "]", ")", "]", "\n", "else", ":", "return", "seq", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.alignment.Alignment.__str__": [[168, 175], ["str", "str", "str"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "orig", "=", "\" \"", ".", "join", "(", "[", "\"Orig:\"", "]", "+", "[", "tok", ".", "text", "for", "tok", "in", "self", ".", "orig", "]", ")", "\n", "cor", "=", "\" \"", ".", "join", "(", "[", "\"Cor:\"", "]", "+", "[", "tok", ".", "text", "for", "tok", "in", "self", ".", "cor", "]", ")", "\n", "cost_matrix", "=", "\"\\n\"", ".", "join", "(", "[", "\"Cost Matrix:\"", "]", "+", "[", "str", "(", "row", ")", "for", "row", "in", "self", ".", "cost_matrix", "]", ")", "\n", "op_matrix", "=", "\"\\n\"", ".", "join", "(", "[", "\"Operation Matrix:\"", "]", "+", "[", "str", "(", "row", ")", "for", "row", "in", "self", ".", "op_matrix", "]", ")", "\n", "seq", "=", "\"Best alignment: \"", "+", "str", "(", "[", "a", "[", "0", "]", "for", "a", "in", "self", ".", "align_seq", "]", ")", "\n", "return", "\"\\n\"", ".", "join", "(", "[", "orig", ",", "cor", ",", "cost_matrix", ",", "op_matrix", ",", "seq", "]", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.annotator.Annotator.__init__": [[12, 18], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "lang", ",", "nlp", "=", "None", ",", "merger", "=", "None", ",", "classifier", "=", "None", ",", "sorter", "=", "None", ")", ":", "\n", "        ", "self", ".", "lang", "=", "lang", "\n", "self", ".", "nlp", "=", "nlp", "\n", "self", ".", "merger", "=", "merger", "\n", "self", ".", "classifier", "=", "classifier", "\n", "self", ".", "sorter", "=", "sorter", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.annotator.Annotator.parse": [[22, 30], ["annotator.Annotator.nlp", "spacy.tokens.Doc", "annotator.Annotator.nlp.tagger", "annotator.Annotator.nlp.parser", "spacy.tokens.Doc.split"], "methods", ["None"], ["", "def", "parse", "(", "self", ",", "text", ",", "tokenise", "=", "False", ")", ":", "\n", "        ", "if", "tokenise", ":", "\n", "            ", "text", "=", "self", ".", "nlp", "(", "text", ")", "\n", "", "else", ":", "\n", "            ", "text", "=", "Doc", "(", "self", ".", "nlp", ".", "vocab", ",", "text", ".", "split", "(", ")", ")", "\n", "self", ".", "nlp", ".", "tagger", "(", "text", ")", "\n", "self", ".", "nlp", ".", "parser", "(", "text", ")", "\n", "", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.annotator.Annotator.align": [[35, 37], ["errant.alignment.Alignment"], "methods", ["None"], ["", "def", "align", "(", "self", ",", "orig", ",", "cor", ",", "lev", "=", "False", ")", ":", "\n", "        ", "return", "Alignment", "(", "orig", ",", "cor", ",", "lev", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.annotator.Annotator.merge": [[41, 59], ["annotator.Annotator.merger.get_rule_edits", "alignment.get_all_split_edits", "alignment.get_all_merge_edits", "alignment.get_all_equal_edits", "Exception"], "methods", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.get_rule_edits", "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.alignment.Alignment.get_all_split_edits", "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.alignment.Alignment.get_all_merge_edits", "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.alignment.Alignment.get_all_equal_edits"], ["", "def", "merge", "(", "self", ",", "alignment", ",", "merging", "=", "\"rules\"", ")", ":", "\n", "# rules: Rule-based merging", "\n", "        ", "if", "merging", "==", "\"rules\"", ":", "\n", "            ", "edits", "=", "self", ".", "merger", ".", "get_rule_edits", "(", "alignment", ")", "\n", "# all-split: Don't merge anything", "\n", "", "elif", "merging", "==", "\"all-split\"", ":", "\n", "            ", "edits", "=", "alignment", ".", "get_all_split_edits", "(", ")", "\n", "# all-merge: Merge all adjacent non-match ops", "\n", "", "elif", "merging", "==", "\"all-merge\"", ":", "\n", "            ", "edits", "=", "alignment", ".", "get_all_merge_edits", "(", ")", "\n", "# all-equal: Merge all edits of the same operation type", "\n", "", "elif", "merging", "==", "\"all-equal\"", ":", "\n", "            ", "edits", "=", "alignment", ".", "get_all_equal_edits", "(", ")", "\n", "# Unknown", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Unknown merging strategy. Choose from: \"", "\n", "\"rules, all-split, all-merge, all-equal.\"", ")", "\n", "", "return", "edits", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.annotator.Annotator.classify": [[62, 64], ["annotator.Annotator.classifier.classify"], "methods", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.en.classifier.classify"], ["", "def", "classify", "(", "self", ",", "edit", ")", ":", "\n", "        ", "return", "self", ".", "classifier", ".", "classify", "(", "edit", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.annotator.Annotator.sort": [[67, 69], ["annotator.Annotator.sorter.sort"], "methods", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.en.sorter.sort"], ["", "def", "sort", "(", "self", ",", "edit", ")", ":", "\n", "        ", "return", "self", ".", "sorter", ".", "sort", "(", "edit", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.annotator.Annotator.annotate": [[75, 82], ["annotator.Annotator.align", "annotator.Annotator.merge", "annotator.Annotator.classify", "annotator.Annotator.sort"], "methods", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.annotator.Annotator.align", "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.annotator.Annotator.merge", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.classifier.classify", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.sorter.sort"], ["", "def", "annotate", "(", "self", ",", "orig", ",", "cor", ",", "lev", "=", "False", ",", "merging", "=", "\"rules\"", ")", ":", "\n", "        ", "alignment", "=", "self", ".", "align", "(", "orig", ",", "cor", ",", "lev", ")", "\n", "edits", "=", "self", ".", "merge", "(", "alignment", ",", "merging", ")", "\n", "for", "edit", "in", "edits", ":", "\n", "            ", "edit", "=", "self", ".", "classify", "(", "edit", ")", "\n", "edit", "=", "self", ".", "sort", "(", "edit", ")", "\n", "", "return", "edits", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.annotator.Annotator.import_edit": [[89, 107], ["len", "errant.edit.Edit", "errant.edit.Edit.minimise", "annotator.Annotator.classify", "len", "errant.edit.Edit", "Exception"], "methods", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.edit.Edit.minimise", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.classifier.classify"], ["", "def", "import_edit", "(", "self", ",", "orig", ",", "cor", ",", "edit", ",", "min", "=", "True", ",", "old_cat", "=", "False", ")", ":", "\n", "# Undefined error type", "\n", "        ", "if", "len", "(", "edit", ")", "==", "4", ":", "\n", "            ", "edit", "=", "Edit", "(", "orig", ",", "cor", ",", "edit", ")", "\n", "# Existing error type", "\n", "", "elif", "len", "(", "edit", ")", "==", "5", ":", "\n", "            ", "edit", "=", "Edit", "(", "orig", ",", "cor", ",", "edit", "[", ":", "4", "]", ",", "edit", "[", "4", "]", ")", "\n", "# Unknown edit format", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Edit not of the form: \"", "\n", "\"[o_start, o_end, c_start, c_end, (cat)]\"", ")", "\n", "# Minimise edit", "\n", "", "if", "min", ":", "\n", "            ", "edit", "=", "edit", ".", "minimise", "(", ")", "\n", "# Classify edit", "\n", "", "if", "not", "old_cat", ":", "\n", "            ", "edit", "=", "self", ".", "classify", "(", "edit", ")", "\n", "", "return", "edit", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.edit.Edit.__init__": [[9, 24], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "orig", ",", "cor", ",", "edit", ",", "type", "=", "\"NA\"", ",", "importance", "=", "0", ")", ":", "\n", "# Orig offsets, spacy tokens and string", "\n", "        ", "self", ".", "o_start", "=", "edit", "[", "0", "]", "\n", "self", ".", "o_end", "=", "edit", "[", "1", "]", "\n", "self", ".", "o_toks", "=", "orig", "[", "self", ".", "o_start", ":", "self", ".", "o_end", "]", "\n", "self", ".", "o_str", "=", "self", ".", "o_toks", ".", "text", "if", "self", ".", "o_toks", "else", "\"\"", "\n", "# Cor offsets, spacy tokens and string", "\n", "self", ".", "c_start", "=", "edit", "[", "2", "]", "\n", "self", ".", "c_end", "=", "edit", "[", "3", "]", "\n", "self", ".", "c_toks", "=", "cor", "[", "self", ".", "c_start", ":", "self", ".", "c_end", "]", "\n", "self", ".", "c_str", "=", "self", ".", "c_toks", ".", "text", "if", "self", ".", "c_toks", "else", "\"\"", "\n", "# Error type", "\n", "self", ".", "type", "=", "type", "\n", "# Importance", "\n", "self", ".", "importance", "=", "importance", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.edit.Edit.minimise": [[26, 46], ["None"], "methods", ["None"], ["", "def", "minimise", "(", "self", ")", ":", "\n", "# While the first token is the same on both sides", "\n", "        ", "while", "self", ".", "o_toks", "and", "self", ".", "c_toks", "and", "self", ".", "o_toks", "[", "0", "]", ".", "text", "==", "self", ".", "c_toks", "[", "0", "]", ".", "text", ":", "\n", "# Remove that token from the span, and adjust the start offsets", "\n", "            ", "self", ".", "o_toks", "=", "self", ".", "o_toks", "[", "1", ":", "]", "\n", "self", ".", "c_toks", "=", "self", ".", "c_toks", "[", "1", ":", "]", "\n", "self", ".", "o_start", "+=", "1", "\n", "self", ".", "c_start", "+=", "1", "\n", "# Do the same for the last token", "\n", "", "while", "self", ".", "o_toks", "and", "self", ".", "c_toks", "and", "self", ".", "o_toks", "[", "-", "1", "]", ".", "text", "==", "self", ".", "c_toks", "[", "-", "1", "]", ".", "text", ":", "\n", "            ", "self", ".", "o_toks", "=", "self", ".", "o_toks", "[", ":", "-", "1", "]", "\n", "self", ".", "c_toks", "=", "self", ".", "c_toks", "[", ":", "-", "1", "]", "\n", "self", ".", "o_end", "-=", "1", "\n", "self", ".", "c_end", "-=", "1", "\n", "# Update the strings", "\n", "", "self", ".", "o_str", "=", "self", ".", "o_toks", ".", "text", "if", "self", ".", "o_toks", "else", "\"\"", "\n", "self", ".", "c_str", "=", "self", ".", "c_toks", ".", "text", "if", "self", ".", "c_toks", "else", "\"\"", "\n", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.edit.Edit.to_m2": [[49, 55], ["str", "str", "str", "str"], "methods", ["None"], ["", "def", "to_m2", "(", "self", ",", "id", "=", "0", ")", ":", "\n", "        ", "span", "=", "\" \"", ".", "join", "(", "[", "\"A\"", ",", "str", "(", "self", ".", "o_start", ")", ",", "str", "(", "self", ".", "o_end", ")", "]", ")", "\n", "cor_toks_str", "=", "\" \"", ".", "join", "(", "[", "tok", ".", "text", "for", "tok", "in", "self", ".", "c_toks", "]", ")", "\n", "return", "\"|||\"", ".", "join", "(", "[", "span", ",", "self", ".", "type", ",", "cor_toks_str", ",", "\n", "str", "(", "self", ".", "importance", ")", "if", "self", ".", "importance", "else", "\"REQUIRED\"", ",", "\n", "\"-NONE-\"", ",", "str", "(", "id", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.edit.Edit.__str__": [[57, 62], ["str", "str", "repr"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "orig", "=", "\"Orig: \"", "+", "str", "(", "[", "self", ".", "o_start", ",", "self", ".", "o_end", ",", "self", ".", "o_str", "]", ")", "\n", "cor", "=", "\"Cor: \"", "+", "str", "(", "[", "self", ".", "c_start", ",", "self", ".", "c_end", ",", "self", ".", "c_str", "]", ")", "\n", "type", "=", "\"Type: \"", "+", "repr", "(", "self", ".", "type", ")", "\n", "return", "\", \"", ".", "join", "(", "[", "orig", ",", "cor", ",", "type", "]", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.__init__.load": [[9, 29], ["importlib.import_module", "importlib.import_module", "importlib.import_module", "errant.annotator.Annotator", "Exception", "spacy.load"], "function", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.__init__.load"], ["def", "load", "(", "lang", ",", "nlp", "=", "None", ")", ":", "\n", "# Make sure the language is supported", "\n", "    ", "supported", "=", "{", "\"en\"", "}", "\n", "if", "lang", "not", "in", "supported", ":", "\n", "        ", "raise", "Exception", "(", "\"%s is an unsupported or unknown language\"", "%", "lang", ")", "\n", "\n", "# Load spacy", "\n", "", "nlp", "=", "nlp", "or", "spacy", ".", "load", "(", "lang", ",", "disable", "=", "[", "\"ner\"", "]", ")", "\n", "\n", "# Load language edit merger", "\n", "merger", "=", "import_module", "(", "\"errant.%s.merger\"", "%", "lang", ")", "\n", "\n", "# Load language edit classifier and sorter", "\n", "classifier", "=", "import_module", "(", "\"errant.%s.classifier\"", "%", "lang", ")", "\n", "sorter", "=", "import_module", "(", "\"errant.%s.sorter\"", "%", "lang", ")", "\n", "# The English classifier needs spacy", "\n", "if", "lang", "==", "\"en\"", ":", "classifier", ".", "nlp", "=", "nlp", "\n", "\n", "# Return a configured ERRANT annotator", "\n", "return", "Annotator", "(", "lang", ",", "nlp", ",", "merger", ",", "classifier", ",", "sorter", ")", "", "", ""]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.get_rule_edits": [[13, 32], ["itertools.groupby", "list", "merger.process_seq", "edits.append", "edits.append", "errant.edit.Edit", "errant.edit.Edit"], "function", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.process_seq"], ["def", "get_rule_edits", "(", "alignment", ")", ":", "\n", "    ", "edits", "=", "[", "]", "\n", "# Split alignment into groups of M, T and rest. (T has a number after it)", "\n", "for", "op", ",", "group", "in", "groupby", "(", "alignment", ".", "align_seq", ",", "\n", "lambda", "x", ":", "x", "[", "0", "]", "[", "0", "]", "if", "x", "[", "0", "]", "[", "0", "]", "in", "{", "\"M\"", ",", "\"T\"", "}", "else", "False", ")", ":", "\n", "        ", "group", "=", "list", "(", "group", ")", "\n", "# Ignore M", "\n", "if", "op", "==", "\"M\"", ":", "continue", "\n", "# T is always split", "\n", "elif", "op", "==", "\"T\"", ":", "\n", "            ", "for", "seq", "in", "group", ":", "\n", "                ", "edits", ".", "append", "(", "Edit", "(", "alignment", ".", "orig", ",", "alignment", ".", "cor", ",", "seq", "[", "1", ":", "]", ")", ")", "\n", "# Process D, I and S subsequence", "\n", "", "", "else", ":", "\n", "            ", "processed", "=", "process_seq", "(", "group", ",", "alignment", ")", "\n", "# Turn the processed sequence into edits", "\n", "for", "seq", "in", "processed", ":", "\n", "                ", "edits", ".", "append", "(", "Edit", "(", "alignment", ".", "orig", ",", "alignment", ".", "cor", ",", "seq", "[", "1", ":", "]", ")", ")", "\n", "", "", "", "return", "edits", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.process_seq": [[36, 113], ["list", "list.sort", "len", "merger.merge_edits", "itertools.combinations", "re.sub", "re.sub", "set", "merger.merge_edits", "set", "set", "range", "set.isdisjoint", "len", "merger.process_seq", "merger.process_seq", "merger.process_seq", "len", "len", "set.issubset", "merger.process_seq", "len", "len", "merger.process_seq", "merger.merge_edits", "merger.merge_edits", "merger.process_seq", "merger.is_punct", "merger.is_punct", "merger.process_seq", "merger.process_seq", "merger.merge_edits", "len", "merger.process_seq", "merger.merge_edits", "merger.process_seq", "merger.process_seq", "merger.process_seq", "merger.process_seq", "merger.process_seq", "c[].text[].isupper", "o[].text[].isupper", "len", "len", "merger.process_seq", "merger.merge_edits", "merger.char_cost", "merger.char_cost", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.en.sorter.sort", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.merge_edits", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.merge_edits", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.process_seq", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.process_seq", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.process_seq", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.process_seq", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.process_seq", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.merge_edits", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.merge_edits", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.process_seq", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.is_punct", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.is_punct", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.process_seq", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.process_seq", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.merge_edits", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.process_seq", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.merge_edits", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.process_seq", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.process_seq", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.process_seq", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.process_seq", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.process_seq", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.process_seq", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.merge_edits", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.char_cost", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.char_cost"], ["", "def", "process_seq", "(", "seq", ",", "alignment", ")", ":", "\n", "# Return single alignments", "\n", "    ", "if", "len", "(", "seq", ")", "<=", "1", ":", "return", "seq", "\n", "# Get the ops for the whole sequence", "\n", "ops", "=", "[", "op", "[", "0", "]", "for", "op", "in", "seq", "]", "\n", "# Merge all D xor I ops. (95% of human multi-token edits contain S).", "\n", "if", "set", "(", "ops", ")", "==", "{", "\"D\"", "}", "or", "set", "(", "ops", ")", "==", "{", "\"I\"", "}", ":", "return", "merge_edits", "(", "seq", ")", "\n", "\n", "content", "=", "False", "# True if edit includes a content word", "\n", "# Get indices of all start-end combinations in the seq: 012 = 01, 02, 12", "\n", "combos", "=", "list", "(", "combinations", "(", "range", "(", "0", ",", "len", "(", "seq", ")", ")", ",", "2", ")", ")", "\n", "# Sort them starting with largest spans first", "\n", "combos", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", "-", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "# Loop through combos", "\n", "for", "start", ",", "end", "in", "combos", ":", "\n", "# Ignore ranges that do NOT contain a substitution.", "\n", "        ", "if", "\"S\"", "not", "in", "ops", "[", "start", ":", "end", "+", "1", "]", ":", "continue", "\n", "# Get the tokens in orig and cor. They will now never be empty.", "\n", "o", "=", "alignment", ".", "orig", "[", "seq", "[", "start", "]", "[", "1", "]", ":", "seq", "[", "end", "]", "[", "2", "]", "]", "\n", "c", "=", "alignment", ".", "cor", "[", "seq", "[", "start", "]", "[", "3", "]", ":", "seq", "[", "end", "]", "[", "4", "]", "]", "\n", "# First token possessive suffixes", "\n", "if", "start", "==", "0", "and", "(", "o", "[", "0", "]", ".", "tag_", "==", "\"POS\"", "or", "c", "[", "0", "]", ".", "tag_", "==", "\"POS\"", ")", ":", "\n", "            ", "return", "[", "seq", "[", "0", "]", "]", "+", "process_seq", "(", "seq", "[", "1", ":", "]", ",", "alignment", ")", "\n", "# Merge possessive suffixes: [friends -> friend 's]", "\n", "", "if", "o", "[", "-", "1", "]", ".", "tag_", "==", "\"POS\"", "or", "c", "[", "-", "1", "]", ".", "tag_", "==", "\"POS\"", ":", "\n", "            ", "return", "process_seq", "(", "seq", "[", ":", "end", "-", "1", "]", ",", "alignment", ")", "+", "merge_edits", "(", "seq", "[", "end", "-", "1", ":", "end", "+", "1", "]", ")", "+", "process_seq", "(", "seq", "[", "end", "+", "1", ":", "]", ",", "alignment", ")", "\n", "# Case changes", "\n", "", "if", "o", "[", "-", "1", "]", ".", "lower", "==", "c", "[", "-", "1", "]", ".", "lower", ":", "\n", "# Merge first token I or D: [Cat -> The big cat]", "\n", "            ", "if", "start", "==", "0", "and", "(", "(", "len", "(", "o", ")", "==", "1", "and", "c", "[", "0", "]", ".", "text", "[", "0", "]", ".", "isupper", "(", ")", ")", "or", "(", "len", "(", "c", ")", "==", "1", "and", "o", "[", "0", "]", ".", "text", "[", "0", "]", ".", "isupper", "(", ")", ")", ")", ":", "\n", "                ", "return", "merge_edits", "(", "seq", "[", "start", ":", "end", "+", "1", "]", ")", "+", "process_seq", "(", "seq", "[", "end", "+", "1", ":", "]", ",", "alignment", ")", "\n", "# Merge with previous punctuation: [, we -> . We], [we -> . We]", "\n", "", "if", "(", "len", "(", "o", ")", ">", "1", "and", "is_punct", "(", "o", "[", "-", "2", "]", ")", ")", "or", "(", "len", "(", "c", ")", ">", "1", "and", "is_punct", "(", "c", "[", "-", "2", "]", ")", ")", ":", "\n", "                ", "return", "process_seq", "(", "seq", "[", ":", "end", "-", "1", "]", ",", "alignment", ")", "+", "merge_edits", "(", "seq", "[", "end", "-", "1", ":", "end", "+", "1", "]", ")", "+", "process_seq", "(", "seq", "[", "end", "+", "1", ":", "]", ",", "alignment", ")", "\n", "# Merge whitespace/hyphens: [acat -> a cat], [sub - way -> subway]", "\n", "", "", "s_str", "=", "sub", "(", "\"['-]\"", ",", "\"\"", ",", "\"\"", ".", "join", "(", "[", "tok", ".", "lower_", "for", "tok", "in", "o", "]", ")", ")", "\n", "t_str", "=", "sub", "(", "\"['-]\"", ",", "\"\"", ",", "\"\"", ".", "join", "(", "[", "tok", ".", "lower_", "for", "tok", "in", "c", "]", ")", ")", "\n", "if", "s_str", "==", "t_str", ":", "\n", "            ", "return", "process_seq", "(", "seq", "[", ":", "start", "]", ",", "alignment", ")", "+", "merge_edits", "(", "seq", "[", "start", ":", "end", "+", "1", "]", ")", "+", "process_seq", "(", "seq", "[", "end", "+", "1", ":", "]", ",", "alignment", ")", "\n", "# Merge same POS or auxiliary/infinitive/phrasal verbs:", "\n", "# [to eat -> eating], [watch -> look at]", "\n", "", "pos_set", "=", "set", "(", "[", "tok", ".", "pos", "for", "tok", "in", "o", "]", "+", "[", "tok", ".", "pos", "for", "tok", "in", "c", "]", ")", "\n", "if", "len", "(", "o", ")", "!=", "len", "(", "c", ")", "and", "(", "len", "(", "pos_set", ")", "==", "1", "or", "pos_set", ".", "issubset", "(", "{", "POS", ".", "AUX", ",", "POS", ".", "PART", ",", "POS", ".", "VERB", "}", ")", ")", ":", "\n", "            ", "return", "process_seq", "(", "seq", "[", ":", "start", "]", ",", "alignment", ")", "+", "merge_edits", "(", "seq", "[", "start", ":", "end", "+", "1", "]", ")", "+", "process_seq", "(", "seq", "[", "end", "+", "1", ":", "]", ",", "alignment", ")", "\n", "# Split rules take effect when we get to smallest chunks", "\n", "", "if", "end", "-", "start", "<", "2", ":", "\n", "# Split adjacent substitutions", "\n", "            ", "if", "len", "(", "o", ")", "==", "len", "(", "c", ")", "==", "2", ":", "\n", "                ", "return", "process_seq", "(", "seq", "[", ":", "start", "+", "1", "]", ",", "alignment", ")", "+", "process_seq", "(", "seq", "[", "start", "+", "1", ":", "]", ",", "alignment", ")", "\n", "# Split similar substitutions at sequence boundaries", "\n", "", "if", "(", "ops", "[", "start", "]", "==", "\"S\"", "and", "char_cost", "(", "o", "[", "0", "]", ",", "c", "[", "0", "]", ")", ">", "0.75", ")", "or", "(", "ops", "[", "end", "]", "==", "\"S\"", "and", "char_cost", "(", "o", "[", "-", "1", "]", ",", "c", "[", "-", "1", "]", ")", ">", "0.75", ")", ":", "\n", "                ", "return", "process_seq", "(", "seq", "[", ":", "start", "+", "1", "]", ",", "alignment", ")", "+", "process_seq", "(", "seq", "[", "start", "+", "1", ":", "]", ",", "alignment", ")", "\n", "# Split final determiners", "\n", "", "if", "end", "==", "len", "(", "seq", ")", "-", "1", "and", "(", "(", "ops", "[", "-", "1", "]", "in", "{", "\"D\"", ",", "\"S\"", "}", "and", "o", "[", "-", "1", "]", ".", "pos", "==", "POS", ".", "DET", ")", "or", "(", "ops", "[", "-", "1", "]", "in", "{", "\"I\"", ",", "\"S\"", "}", "and", "c", "[", "-", "1", "]", ".", "pos", "==", "POS", ".", "DET", ")", ")", ":", "\n", "                ", "return", "process_seq", "(", "seq", "[", ":", "-", "1", "]", ",", "alignment", ")", "+", "[", "seq", "[", "-", "1", "]", "]", "\n", "# Set content word flag", "\n", "", "", "if", "not", "pos_set", ".", "isdisjoint", "(", "open_pos", ")", ":", "content", "=", "True", "\n", "# Merge sequences that contain content words", "\n", "", "if", "content", ":", "return", "merge_edits", "(", "seq", ")", "\n", "else", ":", "return", "seq", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.is_punct": [[115, 117], ["None"], "function", ["None"], ["", "def", "is_punct", "(", "token", ")", ":", "\n", "    ", "return", "token", ".", "pos", "==", "POS", ".", "PUNCT", "or", "token", ".", "text", "in", "punctuation", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.char_cost": [[119, 121], ["Levenshtein.ratio"], "function", ["None"], ["", "def", "char_cost", "(", "a", ",", "b", ")", ":", "\n", "    ", "return", "Levenshtein", ".", "ratio", "(", "a", ".", "text", ",", "b", ".", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.merger.merge_edits": [[123, 126], ["None"], "function", ["None"], ["", "def", "merge_edits", "(", "seq", ")", ":", "\n", "    ", "if", "seq", ":", "return", "[", "(", "\"X\"", ",", "seq", "[", "0", "]", "[", "1", "]", ",", "seq", "[", "-", "1", "]", "[", "2", "]", ",", "seq", "[", "0", "]", "[", "3", "]", ",", "seq", "[", "-", "1", "]", "[", "4", "]", ")", "]", "\n", "else", ":", "return", "seq", "", "", ""]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.lancaster.LancasterStemmer.__init__": [[172, 181], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "rule_tuple", "=", "None", ",", "strip_prefix_flag", "=", "False", ")", ":", "\n", "        ", "\"\"\"Create an instance of the Lancaster stemmer.\n        \"\"\"", "\n", "# Setup an empty rule dictionary - this will be filled in later", "\n", "self", ".", "rule_dictionary", "=", "{", "}", "\n", "# Check if a user wants to strip prefix", "\n", "self", ".", "_strip_prefix", "=", "strip_prefix_flag", "\n", "# Check if a user wants to use his/her own rule tuples.", "\n", "self", ".", "_rule_tuple", "=", "rule_tuple", "if", "rule_tuple", "else", "self", ".", "default_rule_tuple", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.lancaster.LancasterStemmer.parseRules": [[182, 204], ["re.compile", "re.compile.match", "ValueError", "lancaster.LancasterStemmer.rule_dictionary[].append"], "methods", ["None"], ["", "def", "parseRules", "(", "self", ",", "rule_tuple", "=", "None", ")", ":", "\n", "        ", "\"\"\"Validate the set of rules used in this stemmer.\n\n        If this function is called as an individual method, without using stem\n        method, rule_tuple argument will be compiled into self.rule_dictionary.\n        If this function is called within stem, self._rule_tuple will be used.\n\n        \"\"\"", "\n", "# If there is no argument for the function, use class' own rule tuple.", "\n", "rule_tuple", "=", "rule_tuple", "if", "rule_tuple", "else", "self", ".", "_rule_tuple", "\n", "valid_rule", "=", "re", ".", "compile", "(", "\"^[a-z]+\\*?\\d[a-z]*[>\\.]?$\"", ")", "\n", "# Empty any old rules from the rule set before adding new ones", "\n", "self", ".", "rule_dictionary", "=", "{", "}", "\n", "\n", "for", "rule", "in", "rule_tuple", ":", "\n", "            ", "if", "not", "valid_rule", ".", "match", "(", "rule", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\"The rule {0} is invalid\"", ".", "format", "(", "rule", ")", ")", "\n", "", "first_letter", "=", "rule", "[", "0", ":", "1", "]", "\n", "if", "first_letter", "in", "self", ".", "rule_dictionary", ":", "\n", "                ", "self", ".", "rule_dictionary", "[", "first_letter", "]", ".", "append", "(", "rule", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "rule_dictionary", "[", "first_letter", "]", "=", "[", "rule", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.lancaster.LancasterStemmer.stem": [[205, 220], ["word.lower.lower.lower", "lancaster.LancasterStemmer.__doStemming", "lancaster.LancasterStemmer.__stripPrefix", "lancaster.LancasterStemmer.parseRules"], "methods", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.en.lancaster.LancasterStemmer.__doStemming", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.lancaster.LancasterStemmer.__stripPrefix", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.lancaster.LancasterStemmer.parseRules"], ["", "", "", "def", "stem", "(", "self", ",", "word", ")", ":", "\n", "        ", "\"\"\"Stem a word using the Lancaster stemmer.\n        \"\"\"", "\n", "# Lower-case the word, since all the rules are lower-cased", "\n", "word", "=", "word", ".", "lower", "(", ")", "\n", "word", "=", "self", ".", "__stripPrefix", "(", "word", ")", "if", "self", ".", "_strip_prefix", "else", "word", "\n", "\n", "# Save a copy of the original word", "\n", "intact_word", "=", "word", "\n", "\n", "# If rule dictionary is empty, parse rule tuple.", "\n", "if", "not", "self", ".", "rule_dictionary", ":", "\n", "            ", "self", ".", "parseRules", "(", ")", "\n", "\n", "", "return", "self", ".", "__doStemming", "(", "word", ",", "intact_word", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.lancaster.LancasterStemmer.__doStemming": [[221, 285], ["re.compile", "lancaster.LancasterStemmer.__getLastLetter", "re.compile.match", "re.compile.match.groups", "int", "lancaster.LancasterStemmer.endswith", "lancaster.LancasterStemmer.__isAcceptable", "lancaster.LancasterStemmer.__isAcceptable", "lancaster.LancasterStemmer.__applyRule", "lancaster.LancasterStemmer.__applyRule"], "methods", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.en.lancaster.LancasterStemmer.__getLastLetter", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.lancaster.LancasterStemmer.__isAcceptable", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.lancaster.LancasterStemmer.__isAcceptable", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.lancaster.LancasterStemmer.__applyRule", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.lancaster.LancasterStemmer.__applyRule"], ["", "def", "__doStemming", "(", "self", ",", "word", ",", "intact_word", ")", ":", "\n", "        ", "\"\"\"Perform the actual word stemming\n        \"\"\"", "\n", "\n", "valid_rule", "=", "re", ".", "compile", "(", "\"^([a-z]+)(\\*?)(\\d)([a-z]*)([>\\.]?)$\"", ")", "\n", "\n", "proceed", "=", "True", "\n", "\n", "while", "proceed", ":", "\n", "\n", "# Find the position of the last letter of the word to be stemmed", "\n", "            ", "last_letter_position", "=", "self", ".", "__getLastLetter", "(", "word", ")", "\n", "\n", "# Only stem the word if it has a last letter and a rule matching that last letter", "\n", "if", "(", "\n", "last_letter_position", "<", "0", "\n", "or", "word", "[", "last_letter_position", "]", "not", "in", "self", ".", "rule_dictionary", "\n", ")", ":", "\n", "                ", "proceed", "=", "False", "\n", "\n", "", "else", ":", "\n", "                ", "rule_was_applied", "=", "False", "\n", "\n", "# Go through each rule that matches the word's final letter", "\n", "for", "rule", "in", "self", ".", "rule_dictionary", "[", "word", "[", "last_letter_position", "]", "]", ":", "\n", "                    ", "rule_match", "=", "valid_rule", ".", "match", "(", "rule", ")", "\n", "if", "rule_match", ":", "\n", "                        ", "(", "\n", "ending_string", ",", "\n", "intact_flag", ",", "\n", "remove_total", ",", "\n", "append_string", ",", "\n", "cont_flag", ",", "\n", ")", "=", "rule_match", ".", "groups", "(", ")", "\n", "\n", "# Convert the number of chars to remove when stemming", "\n", "# from a string to an integer", "\n", "remove_total", "=", "int", "(", "remove_total", ")", "\n", "\n", "# Proceed if word's ending matches rule's word ending", "\n", "if", "word", ".", "endswith", "(", "ending_string", "[", ":", ":", "-", "1", "]", ")", ":", "\n", "                            ", "if", "intact_flag", ":", "\n", "                                ", "if", "word", "==", "intact_word", "and", "self", ".", "__isAcceptable", "(", "\n", "word", ",", "remove_total", "\n", ")", ":", "\n", "                                    ", "word", "=", "self", ".", "__applyRule", "(", "\n", "word", ",", "remove_total", ",", "append_string", "\n", ")", "\n", "rule_was_applied", "=", "True", "\n", "if", "cont_flag", "==", "\".\"", ":", "\n", "                                        ", "proceed", "=", "False", "\n", "", "break", "\n", "", "", "elif", "self", ".", "__isAcceptable", "(", "word", ",", "remove_total", ")", ":", "\n", "                                ", "word", "=", "self", ".", "__applyRule", "(", "\n", "word", ",", "remove_total", ",", "append_string", "\n", ")", "\n", "rule_was_applied", "=", "True", "\n", "if", "cont_flag", "==", "\".\"", ":", "\n", "                                    ", "proceed", "=", "False", "\n", "", "break", "\n", "# If no rules apply, the word doesn't need any more stemming", "\n", "", "", "", "", "if", "rule_was_applied", "==", "False", ":", "\n", "                    ", "proceed", "=", "False", "\n", "", "", "", "return", "word", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.lancaster.LancasterStemmer.__getLastLetter": [[286, 296], ["range", "len", "word[].isalpha"], "methods", ["None"], ["", "def", "__getLastLetter", "(", "self", ",", "word", ")", ":", "\n", "        ", "\"\"\"Get the zero-based index of the last alphabetic character in this string\n        \"\"\"", "\n", "last_letter", "=", "-", "1", "\n", "for", "position", "in", "range", "(", "len", "(", "word", ")", ")", ":", "\n", "            ", "if", "word", "[", "position", "]", ".", "isalpha", "(", ")", ":", "\n", "                ", "last_letter", "=", "position", "\n", "", "else", ":", "\n", "                ", "break", "\n", "", "", "return", "last_letter", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.lancaster.LancasterStemmer.__isAcceptable": [[297, 314], ["len", "len"], "methods", ["None"], ["", "def", "__isAcceptable", "(", "self", ",", "word", ",", "remove_total", ")", ":", "\n", "        ", "\"\"\"Determine if the word is acceptable for stemming.\n        \"\"\"", "\n", "word_is_acceptable", "=", "False", "\n", "# If the word starts with a vowel, it must be at least 2", "\n", "# characters long to be stemmed", "\n", "if", "word", "[", "0", "]", "in", "\"aeiouy\"", ":", "\n", "            ", "if", "len", "(", "word", ")", "-", "remove_total", ">=", "2", ":", "\n", "                ", "word_is_acceptable", "=", "True", "\n", "# If the word starts with a consonant, it must be at least 3", "\n", "# characters long (including one vowel) to be stemmed", "\n", "", "", "elif", "len", "(", "word", ")", "-", "remove_total", ">=", "3", ":", "\n", "            ", "if", "word", "[", "1", "]", "in", "\"aeiouy\"", ":", "\n", "                ", "word_is_acceptable", "=", "True", "\n", "", "elif", "word", "[", "2", "]", "in", "\"aeiouy\"", ":", "\n", "                ", "word_is_acceptable", "=", "True", "\n", "", "", "return", "word_is_acceptable", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.lancaster.LancasterStemmer.__applyRule": [[315, 326], ["len"], "methods", ["None"], ["", "def", "__applyRule", "(", "self", ",", "word", ",", "remove_total", ",", "append_string", ")", ":", "\n", "        ", "\"\"\"Apply the stemming rule to the word\n        \"\"\"", "\n", "# Remove letters from the end of the word", "\n", "new_word_length", "=", "len", "(", "word", ")", "-", "remove_total", "\n", "word", "=", "word", "[", "0", ":", "new_word_length", "]", "\n", "\n", "# And add new letters to the end of the truncated word", "\n", "if", "append_string", ":", "\n", "            ", "word", "+=", "append_string", "\n", "", "return", "word", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.lancaster.LancasterStemmer.__stripPrefix": [[327, 347], ["word.startswith", "len"], "methods", ["None"], ["", "def", "__stripPrefix", "(", "self", ",", "word", ")", ":", "\n", "        ", "\"\"\"Remove prefix from a word.\n\n        This function originally taken from Whoosh.\n\n        \"\"\"", "\n", "for", "prefix", "in", "(", "\n", "\"kilo\"", ",", "\n", "\"micro\"", ",", "\n", "\"milli\"", ",", "\n", "\"intra\"", ",", "\n", "\"ultra\"", ",", "\n", "\"mega\"", ",", "\n", "\"nano\"", ",", "\n", "\"pico\"", ",", "\n", "\"pseudo\"", ",", "\n", ")", ":", "\n", "            ", "if", "word", ".", "startswith", "(", "prefix", ")", ":", "\n", "                ", "return", "word", "[", "len", "(", "prefix", ")", ":", "]", "\n", "", "", "return", "word", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.lancaster.LancasterStemmer.__repr__": [[348, 350], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "\"<LancasterStemmer>\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.sorter.add_apostr_for_cont": [[18, 26], ["len", "len"], "function", ["None"], ["def", "add_apostr_for_cont", "(", "edit", ")", ":", "\n", "# Check token number", "\n", "    ", "if", "len", "(", "edit", ".", "o_toks", ")", "and", "len", "(", "edit", ".", "c_toks", ")", ":", "\n", "        ", "return", "edit", ".", "c_toks", "[", "-", "1", "]", ".", "lower_", "in", "conts", "and", "\"\"", ".", "join", "(", "[", "x", ".", "lower_", "for", "x", "in", "edit", ".", "o_toks", "]", ")", "==", "\"\"", ".", "join", "(", "[", "x", ".", "lower_", "for", "x", "in", "edit", ".", "c_toks", "]", ")", ".", "replace", "(", "\"'\"", ",", "\"\"", ")", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.sorter.decomp_cont": [[30, 41], ["len", "len", "contraction.replace"], "function", ["None"], ["", "", "def", "decomp_cont", "(", "edit", ")", ":", "\n", "# Check token number", "\n", "    ", "if", "len", "(", "edit", ".", "c_toks", ")", "==", "2", "and", "len", "(", "edit", ".", "o_toks", ")", "==", "1", ":", "\n", "# Find the expanded token's contraction form", "\n", "        ", "if", "edit", ".", "c_toks", "[", "-", "1", "]", ".", "lower_", "in", "conts_expanded", ":", "\n", "            ", "contraction", "=", "edit", ".", "c_toks", "[", "0", "]", ".", "lower_", "+", "conts_expanded", "[", "edit", ".", "c_toks", "[", "-", "1", "]", ".", "lower_", "]", "\n", "return", "contraction", ".", "replace", "(", "\"'\"", ",", "\"\"", ")", "==", "edit", ".", "o_toks", "[", "0", "]", ".", "lower_", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.sorter.sort": [[47, 62], ["sorter.add_apostr_for_cont", "sorter.decomp_cont", "edit.type.endswith", "edit.type.endswith"], "function", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.en.sorter.add_apostr_for_cont", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.sorter.decomp_cont"], ["", "", "def", "sort", "(", "edit", ")", ":", "\n", "# Apostrophe for contraction", "\n", "# 1. nt -> n't", "\n", "# 2. its -> it is", "\n", "    ", "if", "add_apostr_for_cont", "(", "edit", ")", "or", "decomp_cont", "(", "edit", ")", ":", "\n", "        ", "edit", ".", "importance", "=", "2", "\n", "# Punctuations, casing", "\n", "", "elif", "edit", ".", "type", ".", "endswith", "(", "\"PUNCT\"", ")", "or", "edit", ".", "type", ".", "endswith", "(", "\"ORTH\"", ")", ":", "\n", "        ", "edit", ".", "importance", "=", "1", "\n", "# Everything left is identified as 3", "\n", "", "else", ":", "\n", "        ", "edit", ".", "importance", "=", "3", "\n", "\n", "", "return", "edit", "\n", "", ""]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.classifier.load_word_list": [[8, 11], ["open", "set", "word.strip"], "function", ["None"], ["def", "load_word_list", "(", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ")", "as", "word_list", ":", "\n", "        ", "return", "set", "(", "[", "word", ".", "strip", "(", ")", "for", "word", "in", "word_list", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.classifier.load_pos_map": [[14, 38], ["open", "line.strip().split.strip().split", "line.strip().split.strip", "line[].strip"], "function", ["None"], ["", "", "def", "load_pos_map", "(", "path", ")", ":", "\n", "    ", "map_dict", "=", "{", "}", "\n", "with", "open", "(", "path", ")", "as", "map_file", ":", "\n", "        ", "for", "line", "in", "map_file", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "# Change ADP to PREP for readability", "\n", "if", "line", "[", "1", "]", "==", "\"ADP\"", ":", "map_dict", "[", "line", "[", "0", "]", "]", "=", "\"PREP\"", "\n", "# Change PROPN to NOUN; we don't need a prop noun tag", "\n", "elif", "line", "[", "1", "]", "==", "\"PROPN\"", ":", "map_dict", "[", "line", "[", "0", "]", "]", "=", "\"NOUN\"", "\n", "# Change CCONJ to CONJ", "\n", "elif", "line", "[", "1", "]", "==", "\"CCONJ\"", ":", "map_dict", "[", "line", "[", "0", "]", "]", "=", "\"CONJ\"", "\n", "# Otherwise", "\n", "else", ":", "map_dict", "[", "line", "[", "0", "]", "]", "=", "line", "[", "1", "]", ".", "strip", "(", ")", "\n", "# Add some spacy PTB tags not in the original mapping.", "\n", "", "map_dict", "[", "'\"\"'", "]", "=", "\"PUNCT\"", "\n", "map_dict", "[", "\"SP\"", "]", "=", "\"SPACE\"", "\n", "map_dict", "[", "\"_SP\"", "]", "=", "\"SPACE\"", "\n", "map_dict", "[", "\"BES\"", "]", "=", "\"VERB\"", "\n", "map_dict", "[", "\"HVS\"", "]", "=", "\"VERB\"", "\n", "map_dict", "[", "\"ADD\"", "]", "=", "\"X\"", "\n", "map_dict", "[", "\"GW\"", "]", "=", "\"X\"", "\n", "map_dict", "[", "\"NFP\"", "]", "=", "\"X\"", "\n", "map_dict", "[", "\"XX\"", "]", "=", "\"X\"", "\n", "", "return", "map_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.classifier.classify": [[71, 112], ["classifier.get_one_sided_type", "classifier.get_one_sided_type", "classifier.classify", "classifier.get_two_sided_type", "len", "len"], "function", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.en.classifier.get_one_sided_type", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.classifier.get_one_sided_type", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.classifier.classify", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.classifier.get_two_sided_type"], ["def", "classify", "(", "edit", ")", ":", "\n", "# Nothing to nothing is a detected but not corrected edit", "\n", "    ", "if", "not", "edit", ".", "o_toks", "and", "not", "edit", ".", "c_toks", ":", "\n", "        ", "edit", ".", "type", "=", "\"UNK\"", "\n", "# Missing", "\n", "", "elif", "not", "edit", ".", "o_toks", "and", "edit", ".", "c_toks", ":", "\n", "        ", "op", "=", "\"M:\"", "\n", "cat", "=", "get_one_sided_type", "(", "edit", ".", "c_toks", ")", "\n", "edit", ".", "type", "=", "op", "+", "cat", "\n", "# Unnecessary", "\n", "", "elif", "edit", ".", "o_toks", "and", "not", "edit", ".", "c_toks", ":", "\n", "        ", "op", "=", "\"U:\"", "\n", "cat", "=", "get_one_sided_type", "(", "edit", ".", "o_toks", ")", "\n", "edit", ".", "type", "=", "op", "+", "cat", "\n", "# Replacement and special cases", "\n", "", "else", ":", "\n", "# Same to same is a detected but not corrected edit", "\n", "        ", "if", "edit", ".", "o_str", "==", "edit", ".", "c_str", ":", "\n", "            ", "edit", ".", "type", "=", "\"UNK\"", "\n", "# Special: Ignore case change at the end of multi token edits", "\n", "# E.g. [Doctor -> The doctor], [, since -> . Since]", "\n", "# Classify the edit as if the last token wasn't there", "\n", "", "elif", "edit", ".", "o_toks", "[", "-", "1", "]", ".", "lower", "==", "edit", ".", "c_toks", "[", "-", "1", "]", ".", "lower", "and", "(", "len", "(", "edit", ".", "o_toks", ")", ">", "1", "or", "len", "(", "edit", ".", "c_toks", ")", ">", "1", ")", ":", "\n", "# Store a copy of the full orig and cor toks", "\n", "            ", "all_o_toks", "=", "edit", ".", "o_toks", "[", ":", "]", "\n", "all_c_toks", "=", "edit", ".", "c_toks", "[", ":", "]", "\n", "# Truncate the instance toks for classification", "\n", "edit", ".", "o_toks", "=", "edit", ".", "o_toks", "[", ":", "-", "1", "]", "\n", "edit", ".", "c_toks", "=", "edit", ".", "c_toks", "[", ":", "-", "1", "]", "\n", "# Classify the truncated edit", "\n", "edit", "=", "classify", "(", "edit", ")", "\n", "# Restore the full orig and cor toks", "\n", "edit", ".", "o_toks", "=", "all_o_toks", "\n", "edit", ".", "c_toks", "=", "all_c_toks", "\n", "# Replacement", "\n", "", "else", ":", "\n", "            ", "op", "=", "\"R:\"", "\n", "cat", "=", "get_two_sided_type", "(", "edit", ".", "o_toks", ",", "edit", ".", "c_toks", ")", "\n", "edit", ".", "type", "=", "op", "+", "cat", "\n", "", "", "return", "edit", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.classifier.get_edit_info": [[115, 122], ["pos.append", "dep.append"], "function", ["None"], ["", "def", "get_edit_info", "(", "toks", ")", ":", "\n", "    ", "pos", "=", "[", "]", "\n", "dep", "=", "[", "]", "\n", "for", "tok", "in", "toks", ":", "\n", "        ", "pos", ".", "append", "(", "pos_map", "[", "tok", ".", "tag_", "]", ")", "\n", "dep", ".", "append", "(", "tok", ".", "dep_", ")", "\n", "", "return", "pos", ",", "dep", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.classifier.get_one_sided_type": [[126, 156], ["classifier.get_edit_info", "set().issubset", "len", "set", "set", "len", "len", "dep_map.keys", "set", "set"], "function", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.en.classifier.get_edit_info"], ["", "def", "get_one_sided_type", "(", "toks", ")", ":", "\n", "# Special cases", "\n", "    ", "if", "len", "(", "toks", ")", "==", "1", ":", "\n", "# Possessive noun suffixes; e.g. ' -> 's", "\n", "        ", "if", "toks", "[", "0", "]", ".", "tag_", "==", "\"POS\"", ":", "\n", "            ", "return", "\"NOUN:POSS\"", "\n", "# Contractions. Rule must come after possessive", "\n", "", "if", "toks", "[", "0", "]", ".", "lower_", "in", "conts", ":", "\n", "            ", "return", "\"CONTR\"", "\n", "# Infinitival \"to\" is treated as part of a verb form", "\n", "", "if", "toks", "[", "0", "]", ".", "lower_", "==", "\"to\"", "and", "toks", "[", "0", "]", ".", "pos", "==", "POS", ".", "PART", "and", "toks", "[", "0", "]", ".", "dep_", "!=", "\"prep\"", ":", "\n", "            ", "return", "\"VERB:FORM\"", "\n", "# Extract pos tags and parse info from the toks", "\n", "", "", "pos_list", ",", "dep_list", "=", "get_edit_info", "(", "toks", ")", "\n", "# Auxiliary verbs", "\n", "if", "set", "(", "dep_list", ")", ".", "issubset", "(", "{", "\"aux\"", ",", "\"auxpass\"", "}", ")", ":", "\n", "        ", "return", "\"VERB:TENSE\"", "\n", "# POS-based tags. Ignores rare, uninformative categories", "\n", "", "if", "len", "(", "set", "(", "pos_list", ")", ")", "==", "1", "and", "pos_list", "[", "0", "]", "not", "in", "rare_pos", ":", "\n", "        ", "return", "pos_list", "[", "0", "]", "\n", "# More POS-based tags using special dependency labels", "\n", "", "if", "len", "(", "set", "(", "dep_list", ")", ")", "==", "1", "and", "dep_list", "[", "0", "]", "in", "dep_map", ".", "keys", "(", ")", ":", "\n", "        ", "return", "dep_map", "[", "dep_list", "[", "0", "]", "]", "\n", "# To-infinitives and phrasal verbs", "\n", "", "if", "set", "(", "pos_list", ")", "==", "{", "\"PART\"", ",", "\"VERB\"", "}", ":", "\n", "        ", "return", "\"VERB\"", "\n", "# Tricky cases", "\n", "", "else", ":", "\n", "        ", "return", "\"OTHER\"", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.classifier.get_two_sided_type": [[160, 458], ["classifier.get_edit_info", "classifier.get_edit_info", "classifier.only_orth_change", "classifier.exact_reordering", "set().issubset", "len", "len", "o_toks[].text.isalpha", "len", "set", "set().issubset", "o_dep[].startswith", "c_dep[].startswith", "set", "set", "o_toks[].text.isalpha", "c_toks[].text.isalpha", "Levenshtein.distance", "set", "set", "len", "dep_map.keys", "len", "len", "c_toks[].lower_.replace", "stemmer.stem", "stemmer.stem", "dep_map.keys", "set", "set", "len", "len", "len", "len", "len", "set", "Levenshtein.distance", "classifier.preceded_by_aux", "set", "max", "len", "len", "o_dep[].startswith", "c_dep[].startswith", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "o_toks[].text.startswith", "c_toks[].text.startswith", "max", "round", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.en.classifier.get_edit_info", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.classifier.get_edit_info", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.classifier.only_orth_change", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.classifier.exact_reordering", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.lancaster.LancasterStemmer.stem", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.lancaster.LancasterStemmer.stem", "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.classifier.preceded_by_aux"], ["", "", "def", "get_two_sided_type", "(", "o_toks", ",", "c_toks", ")", ":", "\n", "# Extract pos tags and parse info from the toks as lists", "\n", "    ", "o_pos", ",", "o_dep", "=", "get_edit_info", "(", "o_toks", ")", "\n", "c_pos", ",", "c_dep", "=", "get_edit_info", "(", "c_toks", ")", "\n", "\n", "# Orthography; i.e. whitespace and/or case errors.", "\n", "if", "only_orth_change", "(", "o_toks", ",", "c_toks", ")", ":", "\n", "        ", "return", "\"ORTH\"", "\n", "# Word Order; only matches exact reordering.", "\n", "", "if", "exact_reordering", "(", "o_toks", ",", "c_toks", ")", ":", "\n", "        ", "return", "\"WO\"", "\n", "\n", "# 1:1 replacements (very common)", "\n", "", "if", "len", "(", "o_toks", ")", "==", "len", "(", "c_toks", ")", "==", "1", ":", "\n", "# 1. SPECIAL CASES", "\n", "# Possessive noun suffixes; e.g. ' -> 's", "\n", "        ", "if", "o_toks", "[", "0", "]", ".", "tag_", "==", "\"POS\"", "or", "c_toks", "[", "0", "]", ".", "tag_", "==", "\"POS\"", ":", "\n", "            ", "return", "\"NOUN:POSS\"", "\n", "# Contraction. Rule must come after possessive.", "\n", "# Punctuation is also possible, for example adding apostrophe in a contraction.", "\n", "", "if", "(", "o_toks", "[", "0", "]", ".", "lower_", "in", "conts", "or", "c_toks", "[", "0", "]", ".", "lower_", "in", "conts", ")", "and", "o_pos", "==", "c_pos", ":", "\n", "            ", "if", "o_toks", "[", "0", "]", ".", "lower_", "==", "c_toks", "[", "0", "]", ".", "lower_", ".", "replace", "(", "\"'\"", ",", "\"\"", ")", ":", "\n", "                ", "return", "\"PUNCT\"", "\n", "", "else", ":", "\n", "                ", "return", "\"CONTR\"", "\n", "# Special auxiliaries in contractions (1); e.g. ca -> can, wo -> will", "\n", "# Rule was broken in V1. Turned off this fix for compatibility.", "\n", "", "", "if", "(", "o_toks", "[", "0", "]", ".", "lower_", "in", "aux_conts", "and", "c_toks", "[", "0", "]", ".", "lower_", "==", "aux_conts", "[", "o_toks", "[", "0", "]", ".", "lower_", "]", ")", "or", "(", "c_toks", "[", "0", "]", ".", "lower_", "in", "aux_conts", "and", "o_toks", "[", "0", "]", ".", "lower_", "==", "aux_conts", "[", "c_toks", "[", "0", "]", ".", "lower_", "]", ")", ":", "\n", "            ", "return", "\"CONTR\"", "\n", "# Special auxiliaries in contractions (2); e.g. ca -> could, wo -> should", "\n", "", "if", "o_toks", "[", "0", "]", ".", "lower_", "in", "aux_conts", "or", "c_toks", "[", "0", "]", ".", "lower_", "in", "aux_conts", ":", "\n", "            ", "return", "\"VERB:TENSE\"", "\n", "# Special: \"was\" and \"were\" are the only past tense SVA", "\n", "", "if", "{", "o_toks", "[", "0", "]", ".", "lower_", ",", "c_toks", "[", "0", "]", ".", "lower_", "}", "==", "{", "\"was\"", ",", "\"were\"", "}", ":", "\n", "            ", "return", "\"VERB:SVA\"", "\n", "\n", "# 2. SPELLING AND INFLECTION", "\n", "# Only check alphabetical strings on the original side", "\n", "# Spelling errors take precedence over POS errors; this rule is ordered", "\n", "", "if", "o_toks", "[", "0", "]", ".", "text", ".", "isalpha", "(", ")", ":", "\n", "# Check a GB English dict for both orig and lower case.", "\n", "# E.g. \"cat\" is in the dict, but \"Cat\" is not.", "\n", "            ", "if", "o_toks", "[", "0", "]", ".", "text", "not", "in", "spell", "and", "o_toks", "[", "0", "]", ".", "lower_", "not", "in", "spell", ":", "\n", "# Check if both sides have a common lemma", "\n", "                ", "if", "o_toks", "[", "0", "]", ".", "lemma", "==", "c_toks", "[", "0", "]", ".", "lemma", ":", "\n", "# Inflection; often count vs mass nouns or e.g. got vs getted", "\n", "                    ", "if", "o_pos", "==", "c_pos", "and", "o_pos", "[", "0", "]", "in", "{", "\"NOUN\"", ",", "\"VERB\"", "}", ":", "\n", "                        ", "return", "o_pos", "[", "0", "]", "+", "\":INFL\"", "\n", "# Unknown morphology; i.e. we cannot be more specific.", "\n", "", "else", ":", "\n", "                        ", "return", "\"MORPH\"", "\n", "# Use string similarity to detect true spelling errors.", "\n", "", "", "else", ":", "\n", "# Normalised Lev distance works better than Lev ratio", "\n", "                    ", "levdist", "=", "Levenshtein", ".", "distance", "(", "o_toks", "[", "0", "]", ".", "lower_", ",", "c_toks", "[", "0", "]", ".", "lower_", ")", "\n", "str_sim", "=", "1", "-", "levdist", "/", "max", "(", "len", "(", "o_toks", "[", "0", "]", ".", "lower_", ")", ",", "len", "(", "c_toks", "[", "0", "]", ".", "lower_", ")", ")", "\n", "# WARNING: THIS IS AN APPROXIMATION.", "\n", "# Thresholds tuned manually on FCE_train + W&I_train", "\n", "# str_sim > 0.55 is almost always a true spelling error", "\n", "if", "str_sim", ">", "0.55", ":", "\n", "                        ", "return", "\"SPELL\"", "\n", "# Special scores for shorter sequences are usually SPELL", "\n", "", "if", "str_sim", "==", "0.5", "or", "round", "(", "str_sim", ",", "3", ")", "==", "0.333", ":", "\n", "# Short strings are more likely to be spell: eles -> else", "\n", "                        ", "if", "len", "(", "o_toks", "[", "0", "]", ".", "text", ")", "<=", "4", "and", "len", "(", "c_toks", "[", "0", "]", ".", "text", ")", "<=", "4", ":", "\n", "                            ", "return", "\"SPELL\"", "\n", "# The remainder are usually word choice: amounght -> number", "\n", "# Classifying based on cor_pos alone is generally enough.", "\n", "", "", "if", "c_pos", "[", "0", "]", "not", "in", "rare_pos", ":", "\n", "                        ", "return", "c_pos", "[", "0", "]", "\n", "# Anything that remains is OTHER", "\n", "", "else", ":", "\n", "                        ", "return", "\"OTHER\"", "\n", "\n", "# 3. MORPHOLOGY", "\n", "# Only ADJ, ADV, NOUN and VERB can have inflectional changes.", "\n", "", "", "", "", "if", "o_toks", "[", "0", "]", ".", "lemma", "==", "c_toks", "[", "0", "]", ".", "lemma", "and", "o_pos", "[", "0", "]", "in", "open_pos2", "and", "c_pos", "[", "0", "]", "in", "open_pos2", ":", "\n", "# Same POS on both sides", "\n", "            ", "if", "o_pos", "==", "c_pos", ":", "\n", "# Adjective form; e.g. comparatives", "\n", "                ", "if", "o_pos", "[", "0", "]", "==", "\"ADJ\"", ":", "\n", "                    ", "return", "\"ADJ:FORM\"", "\n", "# Noun number", "\n", "", "if", "o_pos", "[", "0", "]", "==", "\"NOUN\"", ":", "\n", "                    ", "return", "\"NOUN:NUM\"", "\n", "# Verbs - various types", "\n", "", "if", "o_pos", "[", "0", "]", "==", "\"VERB\"", ":", "\n", "# NOTE: These rules are carefully ordered.", "\n", "# Use the dep parse to find some form errors.", "\n", "# Main verbs preceded by aux cannot be tense or SVA.", "\n", "                    ", "if", "preceded_by_aux", "(", "o_toks", ",", "c_toks", ")", ":", "\n", "                        ", "return", "\"VERB:FORM\"", "\n", "# Use fine PTB tags to find various errors.", "\n", "# FORM errors normally involve VBG or VBN.", "\n", "", "if", "o_toks", "[", "0", "]", ".", "tag_", "in", "{", "\"VBG\"", ",", "\"VBN\"", "}", "or", "c_toks", "[", "0", "]", ".", "tag_", "in", "{", "\"VBG\"", ",", "\"VBN\"", "}", ":", "\n", "                        ", "return", "\"VERB:FORM\"", "\n", "# Of what's left, TENSE errors normally involved VBD.", "\n", "", "if", "o_toks", "[", "0", "]", ".", "tag_", "==", "\"VBD\"", "or", "c_toks", "[", "0", "]", ".", "tag_", "==", "\"VBD\"", ":", "\n", "                        ", "return", "\"VERB:TENSE\"", "\n", "# Of what's left, SVA errors normally involve VBZ.", "\n", "", "if", "o_toks", "[", "0", "]", ".", "tag_", "==", "\"VBZ\"", "or", "c_toks", "[", "0", "]", ".", "tag_", "==", "\"VBZ\"", ":", "\n", "                        ", "return", "\"VERB:SVA\"", "\n", "# Any remaining aux verbs are called TENSE.", "\n", "", "if", "o_dep", "[", "0", "]", ".", "startswith", "(", "\"aux\"", ")", "and", "c_dep", "[", "0", "]", ".", "startswith", "(", "\"aux\"", ")", ":", "\n", "                        ", "return", "\"VERB:TENSE\"", "\n", "# Use dep labels to find some more ADJ:FORM", "\n", "", "", "", "if", "set", "(", "o_dep", "+", "c_dep", ")", ".", "issubset", "(", "{", "\"acomp\"", ",", "\"amod\"", "}", ")", ":", "\n", "                ", "return", "\"ADJ:FORM\"", "\n", "# Adj to plural noun is usually noun number; e.g. musical -> musicals.", "\n", "", "if", "o_pos", "[", "0", "]", "==", "\"ADJ\"", "and", "c_toks", "[", "0", "]", ".", "tag_", "==", "\"NNS\"", ":", "\n", "                ", "return", "\"NOUN:NUM\"", "\n", "# For remaining verb errors (rare), rely on c_pos", "\n", "", "if", "c_toks", "[", "0", "]", ".", "tag_", "in", "{", "\"VBG\"", ",", "\"VBN\"", "}", ":", "\n", "                ", "return", "\"VERB:FORM\"", "\n", "", "if", "c_toks", "[", "0", "]", ".", "tag_", "==", "\"VBD\"", ":", "\n", "                ", "return", "\"VERB:TENSE\"", "\n", "", "if", "c_toks", "[", "0", "]", ".", "tag_", "==", "\"VBZ\"", ":", "\n", "                ", "return", "\"VERB:SVA\"", "\n", "# Tricky cases that all have the same lemma.", "\n", "", "else", ":", "\n", "                ", "return", "\"MORPH\"", "\n", "# Derivational morphology.", "\n", "", "", "if", "stemmer", ".", "stem", "(", "o_toks", "[", "0", "]", ".", "text", ")", "==", "stemmer", ".", "stem", "(", "c_toks", "[", "0", "]", ".", "text", ")", "and", "o_pos", "[", "0", "]", "in", "open_pos2", "and", "c_pos", "[", "0", "]", "in", "open_pos2", ":", "\n", "            ", "return", "\"MORPH\"", "\n", "\n", "# 4. GENERAL", "\n", "# Auxiliaries with different lemmas", "\n", "", "if", "o_dep", "[", "0", "]", ".", "startswith", "(", "\"aux\"", ")", "and", "c_dep", "[", "0", "]", ".", "startswith", "(", "\"aux\"", ")", ":", "\n", "            ", "return", "\"VERB:TENSE\"", "\n", "# POS-based tags. Some of these are context sensitive mispellings.", "\n", "", "if", "o_pos", "==", "c_pos", "and", "o_pos", "[", "0", "]", "not", "in", "rare_pos", ":", "\n", "            ", "return", "o_pos", "[", "0", "]", "\n", "# Some dep labels map to POS-based tags.", "\n", "", "if", "o_dep", "==", "c_dep", "and", "o_dep", "[", "0", "]", "in", "dep_map", ".", "keys", "(", ")", ":", "\n", "            ", "return", "dep_map", "[", "o_dep", "[", "0", "]", "]", "\n", "# Phrasal verb particles.", "\n", "", "if", "set", "(", "o_pos", "+", "c_pos", ")", "==", "{", "\"PART\"", ",", "\"PREP\"", "}", "or", "set", "(", "o_dep", "+", "c_dep", ")", "==", "{", "\"prt\"", ",", "\"prep\"", "}", ":", "\n", "            ", "return", "\"PART\"", "\n", "# Can use dep labels to resolve DET + PRON combinations.", "\n", "", "if", "set", "(", "o_pos", "+", "c_pos", ")", "==", "{", "\"DET\"", ",", "\"PRON\"", "}", ":", "\n", "# DET cannot be a subject or object.", "\n", "            ", "if", "c_dep", "[", "0", "]", "in", "{", "\"nsubj\"", ",", "\"nsubjpass\"", ",", "\"dobj\"", ",", "\"pobj\"", "}", ":", "\n", "                ", "return", "\"PRON\"", "\n", "# \"poss\" indicates possessive determiner", "\n", "", "if", "c_dep", "[", "0", "]", "==", "\"poss\"", ":", "\n", "                ", "return", "\"DET\"", "\n", "# NUM and DET are usually DET; e.g. a <-> one", "\n", "", "", "if", "set", "(", "o_pos", "+", "c_pos", ")", "==", "{", "\"NUM\"", ",", "\"DET\"", "}", ":", "\n", "            ", "return", "\"DET\"", "\n", "# Special: other <-> another", "\n", "", "if", "{", "o_toks", "[", "0", "]", ".", "lower_", ",", "c_toks", "[", "0", "]", ".", "lower_", "}", "==", "{", "\"other\"", ",", "\"another\"", "}", ":", "\n", "            ", "return", "\"DET\"", "\n", "# Special: your (sincerely) -> yours (sincerely)", "\n", "", "if", "o_toks", "[", "0", "]", ".", "lower_", "==", "\"your\"", "and", "c_toks", "[", "0", "]", ".", "lower_", "==", "\"yours\"", ":", "\n", "            ", "return", "\"PRON\"", "\n", "# Special: no <-> not; this is very context sensitive", "\n", "", "if", "{", "o_toks", "[", "0", "]", ".", "lower_", ",", "c_toks", "[", "0", "]", ".", "lower_", "}", "==", "{", "\"no\"", ",", "\"not\"", "}", ":", "\n", "            ", "return", "\"OTHER\"", "\n", "\n", "# 5. STRING SIMILARITY", "\n", "# These rules are quite language specific.", "\n", "", "if", "o_toks", "[", "0", "]", ".", "text", ".", "isalpha", "(", ")", "and", "c_toks", "[", "0", "]", ".", "text", ".", "isalpha", "(", ")", ":", "\n", "# Normalised Lev distance works better than Lev ratio", "\n", "            ", "levdist", "=", "Levenshtein", ".", "distance", "(", "o_toks", "[", "0", "]", ".", "lower_", ",", "c_toks", "[", "0", "]", ".", "lower_", ")", "\n", "str_sim", "=", "1", "-", "levdist", "/", "max", "(", "len", "(", "o_toks", "[", "0", "]", ".", "lower_", ")", ",", "len", "(", "c_toks", "[", "0", "]", ".", "lower_", ")", ")", "\n", "# WARNING: THIS IS AN APPROXIMATION.", "\n", "# Thresholds tuned manually on FCE_train + W&I_train", "\n", "# A. Short sequences are likely to be SPELL or function word errors", "\n", "if", "len", "(", "o_toks", "[", "0", "]", ".", "text", ")", "==", "1", ":", "\n", "# i -> in, a -> at", "\n", "                ", "if", "len", "(", "c_toks", "[", "0", "]", ".", "text", ")", "==", "2", "and", "str_sim", "==", "0.5", ":", "\n", "                    ", "return", "\"SPELL\"", "\n", "", "", "if", "len", "(", "o_toks", "[", "0", "]", ".", "text", ")", "==", "2", ":", "\n", "# in -> is, he -> the, to -> too", "\n", "                ", "if", "2", "<=", "len", "(", "c_toks", "[", "0", "]", ".", "text", ")", "<=", "3", "and", "str_sim", ">=", "0.5", ":", "\n", "                    ", "return", "\"SPELL\"", "\n", "", "", "if", "len", "(", "o_toks", "[", "0", "]", ".", "text", ")", "==", "3", ":", "\n", "# Special: the -> that (relative pronoun)", "\n", "                ", "if", "o_toks", "[", "0", "]", ".", "lower_", "==", "\"the\"", "and", "c_toks", "[", "0", "]", ".", "lower_", "==", "\"that\"", ":", "\n", "                    ", "return", "\"PRON\"", "\n", "# Special: all -> everything", "\n", "", "if", "o_toks", "[", "0", "]", ".", "lower_", "==", "\"all\"", "and", "c_toks", "[", "0", "]", ".", "lower_", "==", "\"everything\"", ":", "\n", "                    ", "return", "\"PRON\"", "\n", "# off -> of, too -> to, out -> our, now -> know", "\n", "", "if", "2", "<=", "len", "(", "c_toks", "[", "0", "]", ".", "text", ")", "<=", "4", "and", "str_sim", ">=", "0.5", ":", "\n", "                    ", "return", "\"SPELL\"", "\n", "# B. Longer sequences are also likely to include content word errors", "\n", "", "", "if", "len", "(", "o_toks", "[", "0", "]", ".", "text", ")", "==", "4", ":", "\n", "# Special: that <-> what", "\n", "                ", "if", "{", "o_toks", "[", "0", "]", ".", "lower_", ",", "c_toks", "[", "0", "]", ".", "lower_", "}", "==", "{", "\"that\"", ",", "\"what\"", "}", ":", "\n", "                    ", "return", "\"PRON\"", "\n", "# Special: well <-> good", "\n", "", "if", "{", "o_toks", "[", "0", "]", ".", "lower_", ",", "c_toks", "[", "0", "]", ".", "lower_", "}", "==", "{", "\"good\"", ",", "\"well\"", "}", "and", "c_pos", "[", "0", "]", "not", "in", "rare_pos", ":", "\n", "                    ", "return", "c_pos", "[", "0", "]", "\n", "# knew -> new, ", "\n", "", "if", "len", "(", "c_toks", "[", "0", "]", ".", "text", ")", "==", "3", "and", "str_sim", ">", "0.5", ":", "\n", "                    ", "return", "\"SPELL\"", "\n", "# then <-> than, form -> from", "\n", "", "if", "len", "(", "c_toks", "[", "0", "]", ".", "text", ")", "==", "4", "and", "str_sim", ">=", "0.5", ":", "\n", "                    ", "return", "\"SPELL\"", "\n", "# gong -> going, hole -> whole", "\n", "", "if", "len", "(", "c_toks", "[", "0", "]", ".", "text", ")", "==", "5", "and", "str_sim", "==", "0.8", ":", "\n", "                    ", "return", "\"SPELL\"", "\n", "# high -> height, west -> western", "\n", "", "if", "len", "(", "c_toks", "[", "0", "]", ".", "text", ")", ">", "5", "and", "str_sim", ">", "0.5", "and", "c_pos", "[", "0", "]", "not", "in", "rare_pos", ":", "\n", "                    ", "return", "c_pos", "[", "0", "]", "\n", "", "", "if", "len", "(", "o_toks", "[", "0", "]", ".", "text", ")", "==", "5", ":", "\n", "# Special: after -> later", "\n", "                ", "if", "{", "o_toks", "[", "0", "]", ".", "lower_", ",", "c_toks", "[", "0", "]", ".", "lower_", "}", "==", "{", "\"after\"", ",", "\"later\"", "}", "and", "c_pos", "[", "0", "]", "not", "in", "rare_pos", ":", "\n", "                    ", "return", "c_pos", "[", "0", "]", "\n", "# where -> were, found -> fund", "\n", "", "if", "len", "(", "c_toks", "[", "0", "]", ".", "text", ")", "==", "4", "and", "str_sim", "==", "0.8", ":", "\n", "                    ", "return", "\"SPELL\"", "\n", "# thing <-> think, quite -> quiet, their <-> there", "\n", "", "if", "len", "(", "c_toks", "[", "0", "]", ".", "text", ")", "==", "5", "and", "str_sim", ">=", "0.6", ":", "\n", "                    ", "return", "\"SPELL\"", "\n", "# house -> domestic, human -> people", "\n", "", "if", "len", "(", "c_toks", "[", "0", "]", ".", "text", ")", ">", "5", "and", "c_pos", "[", "0", "]", "not", "in", "rare_pos", ":", "\n", "                    ", "return", "c_pos", "[", "0", "]", "\n", "# C. Longest sequences include MORPH errors", "\n", "", "", "if", "len", "(", "o_toks", "[", "0", "]", ".", "text", ")", ">", "5", "and", "len", "(", "c_toks", "[", "0", "]", ".", "text", ")", ">", "5", ":", "\n", "# Special: therefor -> therefore", "\n", "                ", "if", "o_toks", "[", "0", "]", ".", "lower_", "==", "\"therefor\"", "and", "c_toks", "[", "0", "]", ".", "lower_", "==", "\"therefore\"", ":", "\n", "                    ", "return", "\"SPELL\"", "\n", "# Special: though <-> thought", "\n", "", "if", "{", "o_toks", "[", "0", "]", ".", "lower_", ",", "c_toks", "[", "0", "]", ".", "lower_", "}", "==", "{", "\"though\"", ",", "\"thought\"", "}", ":", "\n", "                    ", "return", "\"SPELL\"", "\n", "# Morphology errors: stress -> stressed, health -> healthy", "\n", "", "if", "(", "o_toks", "[", "0", "]", ".", "text", ".", "startswith", "(", "c_toks", "[", "0", "]", ".", "text", ")", "or", "c_toks", "[", "0", "]", ".", "text", ".", "startswith", "(", "o_toks", "[", "0", "]", ".", "text", ")", ")", "and", "str_sim", ">=", "0.66", ":", "\n", "                    ", "return", "\"MORPH\"", "\n", "# Spelling errors: exiting -> exciting, wether -> whether", "\n", "", "if", "str_sim", ">", "0.8", ":", "\n", "                    ", "return", "\"SPELL\"", "\n", "# Content word errors: learning -> studying, transport -> travel", "\n", "", "if", "str_sim", "<", "0.55", "and", "c_pos", "[", "0", "]", "not", "in", "rare_pos", ":", "\n", "                    ", "return", "c_pos", "[", "0", "]", "\n", "# NOTE: Errors between 0.55 and 0.8 are a mix of SPELL, MORPH and POS", "\n", "# Tricky cases", "\n", "", "", "", "else", ":", "\n", "            ", "return", "\"OTHER\"", "\n", "\n", "# Multi-token replacements (uncommon)", "\n", "# All auxiliaries", "\n", "", "", "if", "set", "(", "o_dep", "+", "c_dep", ")", ".", "issubset", "(", "{", "\"aux\"", ",", "\"auxpass\"", "}", ")", ":", "\n", "        ", "return", "\"VERB:TENSE\"", "\n", "# All same POS", "\n", "", "if", "len", "(", "set", "(", "o_pos", "+", "c_pos", ")", ")", "==", "1", ":", "\n", "# Final verbs with the same lemma are tense; e.g. eat -> has eaten ", "\n", "        ", "if", "o_pos", "[", "0", "]", "==", "\"VERB\"", "and", "o_toks", "[", "-", "1", "]", ".", "lemma", "==", "c_toks", "[", "-", "1", "]", ".", "lemma", ":", "\n", "            ", "return", "\"VERB:TENSE\"", "\n", "# POS-based tags.", "\n", "", "elif", "o_pos", "[", "0", "]", "not", "in", "rare_pos", ":", "\n", "            ", "return", "o_pos", "[", "0", "]", "\n", "# All same special dep labels.", "\n", "", "", "if", "len", "(", "set", "(", "o_dep", "+", "c_dep", ")", ")", "==", "1", "and", "o_dep", "[", "0", "]", "in", "dep_map", ".", "keys", "(", ")", ":", "\n", "        ", "return", "dep_map", "[", "o_dep", "[", "0", "]", "]", "\n", "# Infinitives, gerunds, phrasal verbs.", "\n", "", "if", "set", "(", "o_pos", "+", "c_pos", ")", "==", "{", "\"PART\"", ",", "\"VERB\"", "}", ":", "\n", "# Final verbs with the same lemma are form; e.g. to eat -> eating", "\n", "        ", "if", "o_toks", "[", "-", "1", "]", ".", "lemma", "==", "c_toks", "[", "-", "1", "]", ".", "lemma", ":", "\n", "            ", "return", "\"VERB:FORM\"", "\n", "# Remaining edits are often verb; e.g. to eat -> consuming, look at -> see", "\n", "", "else", ":", "\n", "            ", "return", "\"VERB\"", "\n", "# Possessive nouns; e.g. friends -> friend 's", "\n", "", "", "if", "(", "o_pos", "==", "[", "\"NOUN\"", ",", "\"PART\"", "]", "or", "c_pos", "==", "[", "\"NOUN\"", ",", "\"PART\"", "]", ")", "and", "o_toks", "[", "0", "]", ".", "lemma", "==", "c_toks", "[", "0", "]", ".", "lemma", ":", "\n", "        ", "return", "\"NOUN:POSS\"", "\n", "# Adjective forms with \"most\" and \"more\"; e.g. more free -> freer", "\n", "", "if", "(", "o_toks", "[", "0", "]", ".", "lower_", "in", "{", "\"most\"", ",", "\"more\"", "}", "or", "c_toks", "[", "0", "]", ".", "lower_", "in", "{", "\"most\"", ",", "\"more\"", "}", ")", "and", "o_toks", "[", "-", "1", "]", ".", "lemma", "==", "c_toks", "[", "-", "1", "]", ".", "lemma", "and", "len", "(", "o_toks", ")", "<=", "2", "and", "len", "(", "c_toks", ")", "<=", "2", ":", "\n", "        ", "return", "\"ADJ:FORM\"", "\n", "\n", "# Tricky cases.", "\n", "", "else", ":", "\n", "        ", "return", "\"OTHER\"", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.classifier.only_orth_change": [[462, 468], ["None"], "function", ["None"], ["", "", "def", "only_orth_change", "(", "o_toks", ",", "c_toks", ")", ":", "\n", "    ", "o_join", "=", "\"\"", ".", "join", "(", "[", "o", ".", "lower_", "for", "o", "in", "o_toks", "]", ")", "\n", "c_join", "=", "\"\"", ".", "join", "(", "[", "c", ".", "lower_", "for", "c", "in", "c_toks", "]", ")", "\n", "if", "o_join", "==", "c_join", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.classifier.exact_reordering": [[472, 479], ["sorted", "sorted"], "function", ["None"], ["", "def", "exact_reordering", "(", "o_toks", ",", "c_toks", ")", ":", "\n", "# Sorting lets us keep duplicates.", "\n", "    ", "o_set", "=", "sorted", "(", "[", "o", ".", "lower_", "for", "o", "in", "o_toks", "]", ")", "\n", "c_set", "=", "sorted", "(", "[", "c", ".", "lower_", "for", "c", "in", "c_toks", "]", ")", "\n", "if", "o_set", "==", "c_set", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.en.classifier.preceded_by_aux": [[483, 517], ["o_tok[].dep_.startswith", "c_tok[].dep_.startswith", "o_child.dep_.startswith", "c_child.dep_.startswith"], "function", ["None"], ["", "def", "preceded_by_aux", "(", "o_tok", ",", "c_tok", ")", ":", "\n", "# If the toks are aux, we need to check if they are the first aux.", "\n", "    ", "if", "o_tok", "[", "0", "]", ".", "dep_", ".", "startswith", "(", "\"aux\"", ")", "and", "c_tok", "[", "0", "]", ".", "dep_", ".", "startswith", "(", "\"aux\"", ")", ":", "\n", "# Find the parent verb", "\n", "        ", "o_head", "=", "o_tok", "[", "0", "]", ".", "head", "\n", "c_head", "=", "c_tok", "[", "0", "]", ".", "head", "\n", "# Find the children of the parent", "\n", "o_children", "=", "o_head", ".", "children", "\n", "c_children", "=", "c_head", ".", "children", "\n", "# Check the orig children.", "\n", "for", "o_child", "in", "o_children", ":", "\n", "# Look at the first aux...", "\n", "            ", "if", "o_child", ".", "dep_", ".", "startswith", "(", "\"aux\"", ")", ":", "\n", "# Check if the string matches o_tok", "\n", "                ", "if", "o_child", ".", "text", "!=", "o_tok", "[", "0", "]", ".", "text", ":", "\n", "# If it doesn't, o_tok is not first so check cor", "\n", "                    ", "for", "c_child", "in", "c_children", ":", "\n", "# Find the first aux in cor...", "\n", "                        ", "if", "c_child", ".", "dep_", ".", "startswith", "(", "\"aux\"", ")", ":", "\n", "# If that doesn't match either, neither are first aux", "\n", "                            ", "if", "c_child", ".", "text", "!=", "c_tok", "[", "0", "]", ".", "text", ":", "\n", "                                ", "return", "True", "\n", "# Break after the first cor aux", "\n", "", "break", "\n", "# Break after the first orig aux.", "\n", "", "", "", "break", "\n", "# Otherwise, the toks are main verbs so we need to look for any aux.", "\n", "", "", "", "else", ":", "\n", "        ", "o_deps", "=", "[", "o_dep", ".", "dep_", "for", "o_dep", "in", "o_tok", "[", "0", "]", ".", "children", "]", "\n", "c_deps", "=", "[", "c_dep", ".", "dep_", "for", "c_dep", "in", "c_tok", "[", "0", "]", ".", "children", "]", "\n", "if", "\"aux\"", "in", "o_deps", "or", "\"auxpass\"", "in", "o_deps", ":", "\n", "            ", "if", "\"aux\"", "in", "c_deps", "or", "\"auxpass\"", "in", "c_deps", ":", "\n", "                ", "return", "True", "\n", "", "", "", "return", "False", "\n", "", ""]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.m2_to_m2.main": [[4, 76], ["m2_to_m2.parse_args", "print", "errant.load", "print", "open", "open", "line.strip.strip", "m2_block.append", "out_m2.write", "errant.load.parse", "m2_to_m2.simplify_edits", "sorted", "out_m2.write", "simplify_edits.items", "m2_to_m2.get_cor_and_edits", "errant.load.parse", "out_m2.write", "errant.load.annotate", "sorted", "errant.load.import_edit", "errant.load.parse", "det_edits.append", "out_m2.write", "m2_to_m2.noop_edit", "out_m2.write", "errant.load.import_edit", "out_m2.write", "annotator.import_edit.to_m2", "annotator.import_edit.to_m2", "annotator.import_edit.to_m2"], "function", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.parse_args", "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.__init__.load", "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.annotator.Annotator.parse", "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.simplify_edits", "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.m2_to_m2.get_cor_and_edits", "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.annotator.Annotator.parse", "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.annotator.Annotator.annotate", "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.annotator.Annotator.import_edit", "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.annotator.Annotator.parse", "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.parallel_to_m2.noop_edit", "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.annotator.Annotator.import_edit", "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.edit.Edit.to_m2", "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.edit.Edit.to_m2", "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.edit.Edit.to_m2"], ["def", "main", "(", ")", ":", "\n", "# Parse command line args", "\n", "    ", "args", "=", "parse_args", "(", ")", "\n", "print", "(", "\"Loading resources...\"", ")", "\n", "# Load Errant", "\n", "annotator", "=", "errant", ".", "load", "(", "\"en\"", ")", "\n", "\n", "print", "(", "\"Processing M2 file...\"", ")", "\n", "# Open the m2 file and split it into text+edits blocks. Also open out_m2.", "\n", "with", "open", "(", "args", ".", "m2_file", ")", "as", "m2", ",", "open", "(", "args", ".", "out", ",", "\"w\"", ")", "as", "out_m2", ":", "\n", "# Store the current m2_block here", "\n", "        ", "m2_block", "=", "[", "]", "\n", "# Loop through m2 lines", "\n", "for", "line", "in", "m2", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "# If the line isn't empty, add it to the m2_block", "\n", "if", "line", ":", "m2_block", ".", "append", "(", "line", ")", "\n", "# Otherwise, process the complete blocks", "\n", "else", ":", "\n", "# Write the original text to the output M2 file", "\n", "                ", "out_m2", ".", "write", "(", "m2_block", "[", "0", "]", "+", "\"\\n\"", ")", "\n", "# Parse orig with spacy", "\n", "orig", "=", "annotator", ".", "parse", "(", "m2_block", "[", "0", "]", "[", "2", ":", "]", ")", "\n", "# Simplify the edits and sort by coder id", "\n", "edit_dict", "=", "simplify_edits", "(", "m2_block", "[", "1", ":", "]", ")", "\n", "# Loop through coder ids", "\n", "for", "id", ",", "raw_edits", "in", "sorted", "(", "edit_dict", ".", "items", "(", ")", ")", ":", "\n", "# If the first edit is a noop", "\n", "                    ", "if", "raw_edits", "[", "0", "]", "[", "2", "]", "==", "\"noop\"", ":", "\n", "# Write the noop and continue", "\n", "                        ", "out_m2", ".", "write", "(", "noop_edit", "(", "id", ")", "+", "\"\\n\"", ")", "\n", "continue", "\n", "# Apply the edits to generate the corrected text", "\n", "# Also redefine the edits as orig and cor token offsets", "\n", "", "cor", ",", "gold_edits", "=", "get_cor_and_edits", "(", "m2_block", "[", "0", "]", "[", "2", ":", "]", ",", "raw_edits", ")", "\n", "# Parse cor with spacy", "\n", "cor", "=", "annotator", ".", "parse", "(", "cor", ")", "\n", "# Save detection edits here for auto", "\n", "det_edits", "=", "[", "]", "\n", "# Loop through the gold edits", "\n", "for", "gold_edit", "in", "gold_edits", ":", "\n", "# Do not minimise detection edits", "\n", "                        ", "if", "gold_edit", "[", "-", "2", "]", "in", "{", "\"Um\"", ",", "\"UNK\"", "}", ":", "\n", "                            ", "edit", "=", "annotator", ".", "import_edit", "(", "orig", ",", "cor", ",", "gold_edit", "[", ":", "-", "1", "]", ",", "\n", "min", "=", "False", ",", "old_cat", "=", "args", ".", "old_cats", ")", "\n", "# Overwrite the pseudo correction and set it in the edit", "\n", "edit", ".", "c_toks", "=", "annotator", ".", "parse", "(", "gold_edit", "[", "-", "1", "]", ")", "\n", "# Save the edit for auto", "\n", "det_edits", ".", "append", "(", "edit", ")", "\n", "# Write the edit for gold", "\n", "if", "args", ".", "gold", ":", "\n", "# Write the edit", "\n", "                                ", "out_m2", ".", "write", "(", "edit", ".", "to_m2", "(", "id", ")", "+", "\"\\n\"", ")", "\n", "# Gold annotation", "\n", "", "", "elif", "args", ".", "gold", ":", "\n", "                            ", "edit", "=", "annotator", ".", "import_edit", "(", "orig", ",", "cor", ",", "gold_edit", "[", ":", "-", "1", "]", ",", "\n", "not", "args", ".", "no_min", ",", "args", ".", "old_cats", ")", "\n", "# Write the edit", "\n", "out_m2", ".", "write", "(", "edit", ".", "to_m2", "(", "id", ")", "+", "\"\\n\"", ")", "\n", "# Auto annotations", "\n", "", "", "if", "args", ".", "auto", ":", "\n", "# Auto edits", "\n", "                        ", "edits", "=", "annotator", ".", "annotate", "(", "orig", ",", "cor", ",", "args", ".", "lev", ",", "args", ".", "merge", ")", "\n", "# Combine detection and auto edits and sort by orig offsets", "\n", "edits", "=", "sorted", "(", "det_edits", "+", "edits", ",", "key", "=", "lambda", "e", ":", "(", "e", ".", "o_start", ",", "e", ".", "o_end", ")", ")", "\n", "# Write the edits to the output M2 file", "\n", "for", "edit", "in", "edits", ":", "\n", "                            ", "out_m2", ".", "write", "(", "edit", ".", "to_m2", "(", "id", ")", "+", "\"\\n\"", ")", "\n", "# Write a newline when there are no more edits", "\n", "", "", "", "out_m2", ".", "write", "(", "\"\\n\"", ")", "\n", "# Reset the m2 block", "\n", "m2_block", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.m2_to_m2.parse_args": [[78, 122], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_mutually_exclusive_group", "parser.add_mutually_exclusive_group.add_argument", "parser.add_mutually_exclusive_group.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.parse_args"], ["", "", "", "", "def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "\"Automatically extract and/or classify edits in an m2 file.\"", ",", "\n", "formatter_class", "=", "argparse", ".", "RawTextHelpFormatter", ",", "\n", "usage", "=", "\"%(prog)s [-h] (-auto | -gold) [options] m2_file -out OUT\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"m2_file\"", ",", "\n", "help", "=", "\"The path to an m2 file.\"", ")", "\n", "type_group", "=", "parser", ".", "add_mutually_exclusive_group", "(", "required", "=", "True", ")", "\n", "type_group", ".", "add_argument", "(", "\n", "\"-auto\"", ",", "\n", "help", "=", "\"Extract edits automatically.\"", ",", "\n", "action", "=", "\"store_true\"", ")", "\n", "type_group", ".", "add_argument", "(", "\n", "\"-gold\"", ",", "\n", "help", "=", "\"Use existing edit alignments.\"", ",", "\n", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-out\"", ",", "\n", "help", "=", "\"The output filepath.\"", ",", "\n", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-no_min\"", ",", "\n", "help", "=", "\"Do not minimise edit spans (gold only).\"", ",", "\n", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-old_cats\"", ",", "\n", "help", "=", "\"Preserve old error types (gold only); i.e. turn off the classifier.\"", ",", "\n", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-lev\"", ",", "\n", "help", "=", "\"Align using standard Levenshtein.\"", ",", "\n", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-merge\"", ",", "\n", "help", "=", "\"Choose a merging strategy for automatic alignment.\\n\"", "\n", "\"rules: Use a rule-based merging strategy (default)\\n\"", "\n", "\"all-split: Merge nothing: MSSDI -> M, S, S, D, I\\n\"", "\n", "\"all-merge: Merge adjacent non-matches: MSSDI -> M, SSDI\\n\"", "\n", "\"all-equal: Merge adjacent same-type non-matches: MSSDI -> M, SS, D, I\"", ",", "\n", "choices", "=", "[", "\"rules\"", ",", "\"all-split\"", ",", "\"all-merge\"", ",", "\"all-equal\"", "]", ",", "\n", "default", "=", "\"rules\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.m2_to_m2.simplify_edits": [[125, 143], ["edit.split.split", "[].split", "int", "int", "edit_dict.keys", "edit_dict[].append"], "function", ["None"], ["", "def", "simplify_edits", "(", "edits", ")", ":", "\n", "    ", "edit_dict", "=", "{", "}", "\n", "for", "edit", "in", "edits", ":", "\n", "        ", "edit", "=", "edit", ".", "split", "(", "\"|||\"", ")", "\n", "span", "=", "edit", "[", "0", "]", "[", "2", ":", "]", ".", "split", "(", ")", "# [2:] ignore the leading \"A \"", "\n", "start", "=", "int", "(", "span", "[", "0", "]", ")", "\n", "end", "=", "int", "(", "span", "[", "1", "]", ")", "\n", "cat", "=", "edit", "[", "1", "]", "\n", "cor", "=", "edit", "[", "2", "]", "\n", "id", "=", "edit", "[", "-", "1", "]", "\n", "# Save the useful info as a list", "\n", "proc_edit", "=", "[", "start", ",", "end", ",", "cat", ",", "cor", "]", "\n", "# Save the proc_edit inside the edit_dict using coder id", "\n", "if", "id", "in", "edit_dict", ".", "keys", "(", ")", ":", "\n", "            ", "edit_dict", "[", "id", "]", ".", "append", "(", "proc_edit", ")", "\n", "", "else", ":", "\n", "            ", "edit_dict", "[", "id", "]", "=", "[", "proc_edit", "]", "\n", "", "", "return", "edit_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.m2_to_m2.get_cor_and_edits": [[148, 180], ["orig.split", "sorted", "edit[].split", "new_edits.append", "len", "len", "orig.split"], "function", ["None"], ["", "def", "get_cor_and_edits", "(", "orig", ",", "edits", ")", ":", "\n", "# Copy orig; we will apply edits to it to make cor", "\n", "    ", "cor", "=", "orig", ".", "split", "(", ")", "\n", "new_edits", "=", "[", "]", "\n", "offset", "=", "0", "\n", "# Sort the edits by offsets before processing them", "\n", "edits", "=", "sorted", "(", "edits", ",", "key", "=", "lambda", "e", ":", "(", "e", "[", "0", "]", ",", "e", "[", "1", "]", ")", ")", "\n", "# Loop through edits: [o_start, o_end, cat, cor_str]", "\n", "for", "edit", "in", "edits", ":", "\n", "        ", "o_start", "=", "edit", "[", "0", "]", "\n", "o_end", "=", "edit", "[", "1", "]", "\n", "cat", "=", "edit", "[", "2", "]", "\n", "cor_toks", "=", "edit", "[", "3", "]", ".", "split", "(", ")", "\n", "# Detection edits", "\n", "if", "cat", "in", "{", "\"Um\"", ",", "\"UNK\"", "}", ":", "\n", "# Save the pseudo correction", "\n", "            ", "det_toks", "=", "cor_toks", "[", ":", "]", "\n", "# But temporarily overwrite it to be the same as orig", "\n", "cor_toks", "=", "orig", ".", "split", "(", ")", "[", "o_start", ":", "o_end", "]", "\n", "# Apply the edits", "\n", "", "cor", "[", "o_start", "+", "offset", ":", "o_end", "+", "offset", "]", "=", "cor_toks", "\n", "# Get the cor token start and end offsets in cor", "\n", "c_start", "=", "o_start", "+", "offset", "\n", "c_end", "=", "c_start", "+", "len", "(", "cor_toks", ")", "\n", "# Keep track of how this affects orig edit offsets", "\n", "offset", "=", "offset", "-", "(", "o_end", "-", "o_start", ")", "+", "len", "(", "cor_toks", ")", "\n", "# Detection edits: Restore the pseudo correction", "\n", "if", "cat", "in", "{", "\"Um\"", ",", "\"UNK\"", "}", ":", "cor_toks", "=", "det_toks", "\n", "# Update the edit with cor span and save", "\n", "new_edit", "=", "[", "o_start", ",", "o_end", ",", "c_start", ",", "c_end", ",", "cat", ",", "\" \"", ".", "join", "(", "cor_toks", ")", "]", "\n", "new_edits", ".", "append", "(", "new_edit", ")", "\n", "", "return", "\" \"", ".", "join", "(", "cor", ")", ",", "new_edits", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.m2_to_m2.noop_edit": [[183, 185], ["str"], "function", ["None"], ["", "def", "noop_edit", "(", "id", "=", "0", ")", ":", "\n", "    ", "return", "\"A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||\"", "+", "str", "(", "id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.parallel_to_m2.main": [[5, 46], ["parallel_to_m2.parse_args", "print", "errant.load", "print", "contextlib.ExitStack", "open", "zip", "stack.enter_context", "line[].strip", "errant.load.parse", "out_m2.write", "enumerate", "out_m2.write", "open", "annotator.parse.strip", "annotator.parse.text.strip", "out_m2.write", "errant.load.parse", "errant.load.annotate", "out_m2.write", "parallel_to_m2.noop_edit", "edit.to_m2"], "function", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.parse_args", "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.__init__.load", "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.annotator.Annotator.parse", "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.annotator.Annotator.parse", "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.annotator.Annotator.annotate", "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.parallel_to_m2.noop_edit", "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.edit.Edit.to_m2"], ["def", "main", "(", ")", ":", "\n", "# Parse command line args", "\n", "    ", "args", "=", "parse_args", "(", ")", "\n", "print", "(", "\"Loading resources...\"", ")", "\n", "# Load Errant", "\n", "annotator", "=", "errant", ".", "load", "(", "\"en\"", ")", "\n", "\n", "print", "(", "\"Processing parallel files...\"", ")", "\n", "# Process an arbitrary number of files line by line simultaneously. Python 3.3+", "\n", "# See https://tinyurl.com/y4cj4gth . Also opens the output m2 file.", "\n", "with", "ExitStack", "(", ")", "as", "stack", ",", "open", "(", "args", ".", "out", ",", "\"w\"", ")", "as", "out_m2", ":", "\n", "        ", "in_files", "=", "[", "stack", ".", "enter_context", "(", "open", "(", "i", ")", ")", "for", "i", "in", "[", "args", ".", "orig", "]", "+", "args", ".", "cor", "]", "\n", "# Process each line of all input files", "\n", "for", "line", "in", "zip", "(", "*", "in_files", ")", ":", "\n", "# Get the original and all the corrected texts", "\n", "            ", "orig", "=", "line", "[", "0", "]", ".", "strip", "(", ")", "\n", "cors", "=", "line", "[", "1", ":", "]", "\n", "# Skip the line if orig is empty", "\n", "if", "not", "orig", ":", "continue", "\n", "# Parse orig with spacy", "\n", "orig", "=", "annotator", ".", "parse", "(", "orig", ",", "args", ".", "tok", ")", "\n", "# Write orig to the output m2 file", "\n", "out_m2", ".", "write", "(", "\" \"", ".", "join", "(", "[", "\"S\"", "]", "+", "[", "token", ".", "text", "for", "token", "in", "orig", "]", ")", "+", "\"\\n\"", ")", "\n", "# Loop through the corrected texts", "\n", "for", "cor_id", ",", "cor", "in", "enumerate", "(", "cors", ")", ":", "\n", "                ", "cor", "=", "cor", ".", "strip", "(", ")", "\n", "# If the texts are the same, write a noop edit", "\n", "if", "orig", ".", "text", ".", "strip", "(", ")", "==", "cor", ":", "\n", "                    ", "out_m2", ".", "write", "(", "noop_edit", "(", "cor_id", ")", "+", "\"\\n\"", ")", "\n", "# Otherwise, do extra processing", "\n", "", "else", ":", "\n", "# Parse cor with spacy", "\n", "                    ", "cor", "=", "annotator", ".", "parse", "(", "cor", ",", "args", ".", "tok", ")", "\n", "# Align the texts and extract and classify the edits", "\n", "edits", "=", "annotator", ".", "annotate", "(", "orig", ",", "cor", ",", "args", ".", "lev", ",", "args", ".", "merge", ")", "\n", "# Loop through the edits", "\n", "for", "edit", "in", "edits", ":", "\n", "# Write the edit to the output m2 file", "\n", "                        ", "out_m2", ".", "write", "(", "edit", ".", "to_m2", "(", "cor_id", ")", "+", "\"\\n\"", ")", "\n", "# Write a newline when we have processed all corrections for each line", "\n", "", "", "", "out_m2", ".", "write", "(", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.parallel_to_m2.parse_args": [[48, 86], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.parse_args"], ["", "", "", "def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "\"Align parallel text files and extract and classify the edits.\\n\"", ",", "\n", "formatter_class", "=", "argparse", ".", "RawTextHelpFormatter", ",", "\n", "usage", "=", "\"%(prog)s [-h] [options] -orig ORIG -cor COR [COR ...] -out OUT\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-orig\"", ",", "\n", "help", "=", "\"The path to the original text file.\"", ",", "\n", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-cor\"", ",", "\n", "help", "=", "\"The paths to >= 1 corrected text files.\"", ",", "\n", "nargs", "=", "\"+\"", ",", "\n", "default", "=", "[", "]", ",", "\n", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-out\"", ",", "\n", "help", "=", "\"The output filepath.\"", ",", "\n", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-tok\"", ",", "\n", "help", "=", "\"Word tokenise the text using spacy (default: False).\"", ",", "\n", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-lev\"", ",", "\n", "help", "=", "\"Align using standard Levenshtein (default: False).\"", ",", "\n", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-merge\"", ",", "\n", "help", "=", "\"Choose a merging strategy for automatic alignment.\\n\"", "\n", "\"rules: Use a rule-based merging strategy (default)\\n\"", "\n", "\"all-split: Merge nothing: MSSDI -> M, S, S, D, I\\n\"", "\n", "\"all-merge: Merge adjacent non-matches: MSSDI -> M, SSDI\\n\"", "\n", "\"all-equal: Merge adjacent same-type non-matches: MSSDI -> M, SS, D, I\"", ",", "\n", "choices", "=", "[", "\"rules\"", ",", "\"all-split\"", ",", "\"all-merge\"", ",", "\"all-equal\"", "]", ",", "\n", "default", "=", "\"rules\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.parallel_to_m2.noop_edit": [[89, 91], ["str"], "function", ["None"], ["", "def", "noop_edit", "(", "id", "=", "0", ")", ":", "\n", "    ", "return", "\"A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||\"", "+", "str", "(", "id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.main": [[6, 35], ["compare_m2.parse_args", "open().read().strip().split", "open().read().strip().split", "collections.Counter", "zip", "enumerate", "compare_m2.print_results", "len", "len", "compare_m2.simplify_edits", "compare_m2.simplify_edits", "compare_m2.process_edits", "compare_m2.process_edits", "compare_m2.evaluate_edits", "collections.Counter", "compare_m2.merge_dict", "open().read().strip", "open().read().strip", "open().read", "open().read", "open", "open"], "function", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.parse_args", "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.print_results", "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.simplify_edits", "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.simplify_edits", "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.process_edits", "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.process_edits", "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.evaluate_edits", "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.merge_dict"], ["def", "main", "(", ")", ":", "\n", "# Parse command line args", "\n", "    ", "args", "=", "parse_args", "(", ")", "\n", "# Open hypothesis and reference m2 files and split into chunks", "\n", "hyp_m2", "=", "open", "(", "args", ".", "hyp", ")", ".", "read", "(", ")", ".", "strip", "(", ")", ".", "split", "(", "\"\\n\\n\"", ")", "\n", "ref_m2", "=", "open", "(", "args", ".", "ref", ")", ".", "read", "(", ")", ".", "strip", "(", ")", ".", "split", "(", "\"\\n\\n\"", ")", "\n", "# Make sure they have the same number of sentences", "\n", "assert", "len", "(", "hyp_m2", ")", "==", "len", "(", "ref_m2", ")", "\n", "\n", "# Store global corpus level best counts here", "\n", "best_dict", "=", "Counter", "(", "{", "\"tp\"", ":", "0", ",", "\"fp\"", ":", "0", ",", "\"fn\"", ":", "0", "}", ")", "\n", "best_cats", "=", "{", "}", "\n", "# Process each sentence", "\n", "sents", "=", "zip", "(", "hyp_m2", ",", "ref_m2", ")", "\n", "for", "sent_id", ",", "sent", "in", "enumerate", "(", "sents", ")", ":", "\n", "# Simplify the edits into lists of lists", "\n", "        ", "hyp_edits", "=", "simplify_edits", "(", "sent", "[", "0", "]", ")", "\n", "ref_edits", "=", "simplify_edits", "(", "sent", "[", "1", "]", ")", "\n", "# Process the edits for detection/correction based on args", "\n", "hyp_dict", "=", "process_edits", "(", "hyp_edits", ",", "args", ")", "\n", "ref_dict", "=", "process_edits", "(", "ref_edits", ",", "args", ")", "\n", "# Evaluate edits and get best TP, FP, FN hyp+ref combo.", "\n", "count_dict", ",", "cat_dict", "=", "evaluate_edits", "(", "\n", "hyp_dict", ",", "ref_dict", ",", "best_dict", ",", "sent_id", ",", "args", ")", "\n", "# Merge these dicts with best_dict and best_cats", "\n", "best_dict", "+=", "Counter", "(", "count_dict", ")", "\n", "best_cats", "=", "merge_dict", "(", "best_cats", ",", "cat_dict", ")", "\n", "# Print results", "\n", "", "print_results", "(", "best_dict", ",", "best_cats", ",", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.parse_args": [[37, 114], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_mutually_exclusive_group", "parser.add_mutually_exclusive_group.add_argument", "parser.add_mutually_exclusive_group.add_argument", "parser.add_mutually_exclusive_group.add_argument", "parser.add_mutually_exclusive_group.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.parse_args"], ["", "def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "\"Calculate F-scores for error detection and/or correction.\\n\"", "\n", "\"Flags let you evaluate at different levels of granularity.\"", ",", "\n", "formatter_class", "=", "argparse", ".", "RawTextHelpFormatter", ",", "\n", "usage", "=", "\"%(prog)s [options] -hyp HYP -ref REF\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-hyp\"", ",", "\n", "help", "=", "\"A hypothesis M2 file.\"", ",", "\n", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-ref\"", ",", "\n", "help", "=", "\"A reference M2 file.\"", ",", "\n", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-b\"", ",", "\n", "\"--beta\"", ",", "\n", "help", "=", "\"Value of beta in F-score. (default: 0.5)\"", ",", "\n", "default", "=", "0.5", ",", "\n", "type", "=", "float", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-v\"", ",", "\n", "\"--verbose\"", ",", "\n", "help", "=", "\"Print verbose output.\"", ",", "\n", "action", "=", "\"store_true\"", ")", "\n", "eval_type", "=", "parser", ".", "add_mutually_exclusive_group", "(", ")", "\n", "eval_type", ".", "add_argument", "(", "\n", "\"-dt\"", ",", "\n", "help", "=", "\"Evaluate Detection in terms of Tokens.\"", ",", "\n", "action", "=", "\"store_true\"", ")", "\n", "eval_type", ".", "add_argument", "(", "\n", "\"-ds\"", ",", "\n", "help", "=", "\"Evaluate Detection in terms of Spans.\"", ",", "\n", "action", "=", "\"store_true\"", ")", "\n", "eval_type", ".", "add_argument", "(", "\n", "\"-cs\"", ",", "\n", "help", "=", "\"Evaluate Correction in terms of Spans. (default)\"", ",", "\n", "action", "=", "\"store_true\"", ")", "\n", "eval_type", ".", "add_argument", "(", "\n", "\"-cse\"", ",", "\n", "help", "=", "\"Evaluate Correction in terms of Spans and Error types.\"", ",", "\n", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-single\"", ",", "\n", "help", "=", "\"Only evaluate single token edits; i.e. 0:1, 1:0 or 1:1\"", ",", "\n", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-multi\"", ",", "\n", "help", "=", "\"Only evaluate multi token edits; i.e. 2+:n or n:2+\"", ",", "\n", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-filt\"", ",", "\n", "help", "=", "\"Do not evaluate the specified error types.\"", ",", "\n", "nargs", "=", "\"+\"", ",", "\n", "default", "=", "[", "]", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-cat\"", ",", "\n", "help", "=", "\"Show error category scores.\\n\"", "\n", "\"1: Only show operation tier scores; e.g. R.\\n\"", "\n", "\"2: Only show main tier scores; e.g. NOUN.\\n\"", "\n", "\"3: Show all category scores; e.g. R:NOUN.\"", ",", "\n", "choices", "=", "[", "1", ",", "2", ",", "3", "]", ",", "\n", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-gran\"", ",", "\n", "help", "=", "\"Consider only edits that have higher or equal level than the given one.\\n\"", "\n", "\"1: Trivial: punctuations (except apostrophe), casing;\\n\"", "\n", "\"2: Moderate: informal words (abbreviations), apostrophe for contraction\\n\"", "\n", "\"3: Major: grammatically incorrect\"", ",", "\n", "choices", "=", "[", "1", ",", "2", ",", "3", "]", ",", "\n", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-rule\"", ",", "\n", "help", "=", "\"Json file that contain equivalence rules.\\n\"", ",", "\n", "type", "=", "str", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.simplify_edits": [[117, 135], ["sent.split", "edit[].split", "edit[].split", "int", "int", "int", "out_edits.append", "edit[].isnumeric", "int"], "function", ["None"], ["", "def", "simplify_edits", "(", "sent", ")", ":", "\n", "    ", "out_edits", "=", "[", "]", "\n", "# Get the edit lines from an m2 block.", "\n", "edits", "=", "sent", ".", "split", "(", "\"\\n\"", ")", "[", "1", ":", "]", "\n", "# Loop through the edits", "\n", "for", "edit", "in", "edits", ":", "\n", "# Preprocessing", "\n", "        ", "edit", "=", "edit", "[", "2", ":", "]", ".", "split", "(", "\"|||\"", ")", "# Ignore \"A \" then split.", "\n", "span", "=", "edit", "[", "0", "]", ".", "split", "(", ")", "\n", "start", "=", "int", "(", "span", "[", "0", "]", ")", "\n", "end", "=", "int", "(", "span", "[", "1", "]", ")", "\n", "cat", "=", "edit", "[", "1", "]", "\n", "cor", "=", "edit", "[", "2", "]", "\n", "importance", "=", "int", "(", "edit", "[", "3", "]", ")", "if", "edit", "[", "3", "]", ".", "isnumeric", "(", ")", "else", "0", "\n", "coder", "=", "int", "(", "edit", "[", "-", "1", "]", ")", "\n", "out_edit", "=", "[", "start", ",", "end", ",", "cat", ",", "cor", ",", "importance", ",", "coder", "]", "\n", "out_edits", ".", "append", "(", "out_edit", ")", "\n", "", "return", "out_edits", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.process_edits": [[139, 211], ["len", "len", "cor.split", "coder_dict[].keys", "[].append", "range", "coder_dict[].keys", "[].append", "cor.split", "coder_dict[].keys", "[].append", "coder_dict[].keys", "[].append", "coder_dict[].keys", "[].append", "coder_dict[].keys", "[].append"], "function", ["None"], ["", "def", "process_edits", "(", "edits", ",", "args", ")", ":", "\n", "    ", "coder_dict", "=", "{", "}", "\n", "# Add an explicit noop edit if there are no edits.", "\n", "if", "not", "edits", ":", "edits", "=", "[", "[", "-", "1", ",", "-", "1", ",", "\"noop\"", ",", "\"-NONE-\"", ",", "0", ",", "0", "]", "]", "\n", "# Loop through the edits", "\n", "for", "edit", "in", "edits", ":", "\n", "# Name the edit elements for clarity", "\n", "        ", "start", "=", "edit", "[", "0", "]", "\n", "end", "=", "edit", "[", "1", "]", "\n", "cat", "=", "edit", "[", "2", "]", "\n", "cor", "=", "edit", "[", "3", "]", "\n", "importance", "=", "edit", "[", "4", "]", "\n", "coder", "=", "edit", "[", "5", "]", "\n", "# Add the coder to the coder_dict if necessary", "\n", "if", "coder", "not", "in", "coder_dict", ":", "coder_dict", "[", "coder", "]", "=", "{", "}", "\n", "\n", "# Optionally apply filters based on args", "\n", "# 1. UNK type edits are only useful for detection, not correction.", "\n", "if", "not", "args", ".", "dt", "and", "not", "args", ".", "ds", "and", "cat", "==", "\"UNK\"", ":", "continue", "\n", "# 2. Only evaluate single token edits; i.e. 0:1, 1:0 or 1:1", "\n", "if", "args", ".", "single", "and", "(", "end", "-", "start", ">=", "2", "or", "len", "(", "cor", ".", "split", "(", ")", ")", ">=", "2", ")", ":", "continue", "\n", "# 3. Only evaluate multi token edits; i.e. 2+:n or n:2+", "\n", "if", "args", ".", "multi", "and", "end", "-", "start", "<", "2", "and", "len", "(", "cor", ".", "split", "(", ")", ")", "<", "2", ":", "continue", "\n", "# 4. If there is a filter, ignore the specified error types", "\n", "if", "args", ".", "filt", "and", "cat", "in", "args", ".", "filt", ":", "continue", "\n", "# 5. If granularity is offered, ignore less important errors", "\n", "if", "args", ".", "gran", "and", "importance", "and", "importance", "<", "args", ".", "gran", ":", "continue", "\n", "\n", "# Token Based Detection", "\n", "if", "args", ".", "dt", ":", "\n", "# Preserve noop edits.", "\n", "            ", "if", "start", "==", "-", "1", ":", "\n", "                ", "if", "(", "start", ",", "start", ")", "in", "coder_dict", "[", "coder", "]", ".", "keys", "(", ")", ":", "\n", "                    ", "coder_dict", "[", "coder", "]", "[", "(", "start", ",", "start", ")", "]", ".", "append", "(", "cat", ")", "\n", "", "else", ":", "\n", "                    ", "coder_dict", "[", "coder", "]", "[", "(", "start", ",", "start", ")", "]", "=", "[", "cat", "]", "\n", "# Insertions defined as affecting the token on the right", "\n", "", "", "elif", "start", "==", "end", "and", "start", ">=", "0", ":", "\n", "                ", "if", "(", "start", ",", "start", "+", "1", ")", "in", "coder_dict", "[", "coder", "]", ".", "keys", "(", ")", ":", "\n", "                    ", "coder_dict", "[", "coder", "]", "[", "(", "start", ",", "start", "+", "1", ")", "]", ".", "append", "(", "cat", ")", "\n", "", "else", ":", "\n", "                    ", "coder_dict", "[", "coder", "]", "[", "(", "start", ",", "start", "+", "1", ")", "]", "=", "[", "cat", "]", "\n", "# Edit spans are split for each token in the range.", "\n", "", "", "else", ":", "\n", "                ", "for", "tok_id", "in", "range", "(", "start", ",", "end", ")", ":", "\n", "                    ", "if", "(", "tok_id", ",", "tok_id", "+", "1", ")", "in", "coder_dict", "[", "coder", "]", ".", "keys", "(", ")", ":", "\n", "                        ", "coder_dict", "[", "coder", "]", "[", "(", "tok_id", ",", "tok_id", "+", "1", ")", "]", ".", "append", "(", "cat", ")", "\n", "", "else", ":", "\n", "                        ", "coder_dict", "[", "coder", "]", "[", "(", "tok_id", ",", "tok_id", "+", "1", ")", "]", "=", "[", "cat", "]", "\n", "\n", "# Span Based Detection", "\n", "", "", "", "", "elif", "args", ".", "ds", ":", "\n", "            ", "if", "(", "start", ",", "end", ")", "in", "coder_dict", "[", "coder", "]", ".", "keys", "(", ")", ":", "\n", "                ", "coder_dict", "[", "coder", "]", "[", "(", "start", ",", "end", ")", "]", ".", "append", "(", "cat", ")", "\n", "", "else", ":", "\n", "                ", "coder_dict", "[", "coder", "]", "[", "(", "start", ",", "end", ")", "]", "=", "[", "cat", "]", "\n", "\n", "# Span Based Correction", "\n", "", "", "else", ":", "\n", "# With error type classification", "\n", "            ", "if", "args", ".", "cse", ":", "\n", "                ", "if", "(", "start", ",", "end", ",", "cat", ",", "cor", ")", "in", "coder_dict", "[", "coder", "]", ".", "keys", "(", ")", ":", "\n", "                    ", "coder_dict", "[", "coder", "]", "[", "(", "start", ",", "end", ",", "cat", ",", "cor", ")", "]", ".", "append", "(", "cat", ")", "\n", "", "else", ":", "\n", "                    ", "coder_dict", "[", "coder", "]", "[", "(", "start", ",", "end", ",", "cat", ",", "cor", ")", "]", "=", "[", "cat", "]", "\n", "# Without error type classification", "\n", "", "", "else", ":", "\n", "                ", "if", "(", "start", ",", "end", ",", "cor", ")", "in", "coder_dict", "[", "coder", "]", ".", "keys", "(", ")", ":", "\n", "                    ", "coder_dict", "[", "coder", "]", "[", "(", "start", ",", "end", ",", "cor", ")", "]", ".", "append", "(", "cat", ")", "\n", "", "else", ":", "\n", "                    ", "coder_dict", "[", "coder", "]", "[", "(", "start", ",", "end", ",", "cor", ")", "]", "=", "[", "cat", "]", "\n", "", "", "", "", "return", "coder_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.evaluate_edits": [[219, 282], ["hyp_dict.keys", "spacy.load", "ref_dict.keys", "print", "print", "open", "json.load", "range", "compare_m2.compareEdits", "compare_m2.computeFScore", "compare_m2.computeFScore", "len", "spacy.load.", "list", "list", "print", "print", "print", "print", "print", "print", "print", "print", "str", "sorted", "sorted", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "hyp_dict[].keys", "ref_dict[].keys", "str", "str", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.__init__.load", "home.repos.pwc.inspect_result.yuanxun-yx_errant.errant.__init__.load", "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.compareEdits", "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.computeFScore", "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.computeFScore"], ["", "def", "evaluate_edits", "(", "hyp_dict", ",", "ref_dict", ",", "best", ",", "sent_id", ",", "args", ")", ":", "\n", "# Load equivalence rules", "\n", "    ", "if", "args", ".", "rule", ":", "\n", "        ", "with", "open", "(", "args", ".", "rule", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "rules", "=", "json", ".", "load", "(", "f", ")", "\n", "# tokenize rules", "\n", "", "nlp", "=", "spacy", ".", "load", "(", "\"en\"", ")", "\n", "for", "equiv", "in", "rules", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "equiv", ")", ")", ":", "\n", "                ", "text", "=", "nlp", "(", "equiv", "[", "i", "]", ")", "\n", "equiv", "[", "i", "]", "=", "\" \"", ".", "join", "(", "[", "x", ".", "text", "for", "x", "in", "text", "]", ")", "\n", "", "", "", "else", ":", "\n", "        ", "rules", "=", "[", "]", "\n", "# Store the best sentence level scores and hyp+ref combination IDs", "\n", "# best_f is initialised as -1 cause 0 is a valid result.", "\n", "", "best_tp", ",", "best_fp", ",", "best_fn", ",", "best_f", ",", "best_hyp", ",", "best_ref", "=", "0", ",", "0", ",", "0", ",", "-", "1", ",", "0", ",", "0", "\n", "best_cat", "=", "{", "}", "\n", "# Compare each hyp and ref combination", "\n", "for", "hyp_id", "in", "hyp_dict", ".", "keys", "(", ")", ":", "\n", "        ", "for", "ref_id", "in", "ref_dict", ".", "keys", "(", ")", ":", "\n", "# Get the local counts for the current combination.", "\n", "            ", "tp", ",", "fp", ",", "fn", ",", "cat_dict", "=", "compareEdits", "(", "hyp_dict", "[", "hyp_id", "]", ",", "ref_dict", "[", "ref_id", "]", ",", "rules", ")", "\n", "# Compute the local sentence scores (for verbose output only)", "\n", "loc_p", ",", "loc_r", ",", "loc_f", "=", "computeFScore", "(", "tp", ",", "fp", ",", "fn", ",", "args", ".", "beta", ")", "\n", "# Compute the global sentence scores", "\n", "p", ",", "r", ",", "f", "=", "computeFScore", "(", "\n", "tp", "+", "best", "[", "\"tp\"", "]", ",", "fp", "+", "best", "[", "\"fp\"", "]", ",", "fn", "+", "best", "[", "\"fn\"", "]", ",", "args", ".", "beta", ")", "\n", "# Save the scores if they are better in terms of:", "\n", "# 1. Higher F-score", "\n", "# 2. Same F-score, higher TP", "\n", "# 3. Same F-score and TP, lower FP", "\n", "# 4. Same F-score, TP and FP, lower FN", "\n", "if", "(", "f", ">", "best_f", ")", "or", "(", "f", "==", "best_f", "and", "tp", ">", "best_tp", ")", "or", "(", "f", "==", "best_f", "and", "tp", "==", "best_tp", "and", "fp", "<", "best_fp", ")", "or", "(", "f", "==", "best_f", "and", "tp", "==", "best_tp", "and", "fp", "==", "best_fp", "and", "fn", "<", "best_fn", ")", ":", "\n", "                ", "best_tp", ",", "best_fp", ",", "best_fn", "=", "tp", ",", "fp", ",", "fn", "\n", "best_f", ",", "best_hyp", ",", "best_ref", "=", "f", ",", "hyp_id", ",", "ref_id", "\n", "best_cat", "=", "cat_dict", "\n", "# Verbose output", "\n", "", "if", "args", ".", "verbose", ":", "\n", "# Prepare verbose output edits.", "\n", "                ", "hyp_verb", "=", "list", "(", "sorted", "(", "hyp_dict", "[", "hyp_id", "]", ".", "keys", "(", ")", ")", ")", "\n", "ref_verb", "=", "list", "(", "sorted", "(", "ref_dict", "[", "ref_id", "]", ".", "keys", "(", ")", ")", ")", "\n", "# Ignore noop edits", "\n", "if", "not", "hyp_verb", "or", "hyp_verb", "[", "0", "]", "[", "0", "]", "==", "-", "1", ":", "hyp_verb", "=", "[", "]", "\n", "if", "not", "ref_verb", "or", "ref_verb", "[", "0", "]", "[", "0", "]", "==", "-", "1", ":", "ref_verb", "=", "[", "]", "\n", "# Print verbose info", "\n", "print", "(", "'{:-^40}'", ".", "format", "(", "\"\"", ")", ")", "\n", "print", "(", "\"SENTENCE \"", "+", "str", "(", "sent_id", ")", "+", "\" - HYP \"", "+", "str", "(", "hyp_id", ")", "+", "\" - REF \"", "+", "str", "(", "ref_id", ")", ")", "\n", "print", "(", "\"HYPOTHESIS EDITS :\"", ",", "hyp_verb", ")", "\n", "print", "(", "\"REFERENCE EDITS  :\"", ",", "ref_verb", ")", "\n", "print", "(", "\"Local TP/FP/FN   :\"", ",", "str", "(", "tp", ")", ",", "str", "(", "fp", ")", ",", "str", "(", "fn", ")", ")", "\n", "print", "(", "\"Local P/R/F\"", "+", "str", "(", "args", ".", "beta", ")", "+", "\"  :\"", ",", "str", "(", "loc_p", ")", ",", "str", "(", "loc_r", ")", ",", "str", "(", "loc_f", ")", ")", "\n", "print", "(", "\"Global TP/FP/FN  :\"", ",", "str", "(", "tp", "+", "best", "[", "\"tp\"", "]", ")", ",", "str", "(", "fp", "+", "best", "[", "\"fp\"", "]", ")", ",", "str", "(", "fn", "+", "best", "[", "\"fn\"", "]", ")", ")", "\n", "print", "(", "\"Global P/R/F\"", "+", "str", "(", "args", ".", "beta", ")", "+", "\"  :\"", ",", "str", "(", "p", ")", ",", "str", "(", "r", ")", ",", "str", "(", "f", ")", ")", "\n", "# Verbose output: display the best hyp+ref combination", "\n", "", "", "", "if", "args", ".", "verbose", ":", "\n", "        ", "print", "(", "'{:-^40}'", ".", "format", "(", "\"\"", ")", ")", "\n", "print", "(", "\"^^ HYP \"", "+", "str", "(", "best_hyp", ")", "+", "\", REF \"", "+", "str", "(", "best_ref", ")", "+", "\" chosen for sentence \"", "+", "str", "(", "sent_id", ")", ")", "\n", "# Save the best TP, FP and FNs as a dict, and return this and the best_cat dict", "\n", "", "best_dict", "=", "{", "\"tp\"", ":", "best_tp", ",", "\"fp\"", ":", "best_fp", ",", "\"fn\"", ":", "best_fn", "}", "\n", "return", "best_dict", ",", "best_cat", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.computeAlter": [[286, 300], ["edits.items"], "function", ["None"], ["", "def", "computeAlter", "(", "edits", ",", "rules", ")", ":", "\n", "    ", "edits_alt", "=", "{", "}", "\n", "if", "not", "rules", ":", "return", "edits_alt", "\n", "for", "edit", ",", "cats", "in", "edits", ".", "items", "(", ")", ":", "\n", "# noop ref edits are ignored", "\n", "        ", "if", "cats", "[", "0", "]", "==", "\"noop\"", ":", "continue", "\n", "# Valid edits", "\n", "for", "equiv", "in", "rules", ":", "\n", "            ", "if", "edit", "[", "-", "1", "]", "in", "equiv", ":", "\n", "# Add alternatives to reference map", "\n", "                ", "for", "alt", "in", "equiv", ":", "\n", "                    ", "if", "edit", "[", "-", "1", "]", "!=", "alt", ":", "\n", "                        ", "edits_alt", "[", "(", "edit", "[", "0", "]", ",", "edit", "[", "1", "]", ",", "alt", ")", "]", "=", "edit", "\n", "", "", "", "", "", "return", "edits_alt", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.compareEdits": [[306, 359], ["compare_m2.computeAlter", "compare_m2.computeAlter", "hyp_edits.items", "ref_edits.items", "computeAlter.keys", "ref_edits.keys", "computeAlter.keys", "hyp_edits.keys", "cat_dict.keys", "cat_dict.keys", "cat_dict.keys"], "function", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.computeAlter", "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.computeAlter"], ["", "def", "compareEdits", "(", "hyp_edits", ",", "ref_edits", ",", "rules", ")", ":", "\n", "    ", "tp", "=", "0", "# True Positives", "\n", "fp", "=", "0", "# False Positives", "\n", "fn", "=", "0", "# False Negatives", "\n", "cat_dict", "=", "{", "}", "# {cat: [tp, fp, fn], ...}", "\n", "\n", "# Pre-process reference edits to include equivalent alternatives", "\n", "hyp_edits_alt", "=", "computeAlter", "(", "hyp_edits", ",", "rules", ")", "\n", "ref_edits_alt", "=", "computeAlter", "(", "ref_edits", ",", "rules", ")", "\n", "\n", "for", "h_edit", ",", "h_cats", "in", "hyp_edits", ".", "items", "(", ")", ":", "\n", "# noop hyp edits cannot be TP or FP", "\n", "        ", "if", "h_cats", "[", "0", "]", "==", "\"noop\"", ":", "continue", "\n", "# change to original edit", "\n", "if", "h_edit", "in", "hyp_edits_alt", ".", "keys", "(", ")", ":", "\n", "            ", "h_edit", "=", "hyp_edits_alt", "[", "h_edit", "]", "\n", "# TRUE POSITIVES", "\n", "", "if", "h_edit", "in", "ref_edits", ".", "keys", "(", ")", ":", "\n", "# On occasion, multiple tokens at same span.", "\n", "            ", "for", "h_cat", "in", "ref_edits", "[", "h_edit", "]", ":", "# Use ref dict for TP", "\n", "                ", "tp", "+=", "1", "\n", "# Each dict value [TP, FP, FN]", "\n", "if", "h_cat", "in", "cat_dict", ".", "keys", "(", ")", ":", "\n", "                    ", "cat_dict", "[", "h_cat", "]", "[", "0", "]", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "cat_dict", "[", "h_cat", "]", "=", "[", "1", ",", "0", ",", "0", "]", "\n", "# FALSE POSITIVES", "\n", "", "", "", "else", ":", "\n", "# On occasion, multiple tokens at same span.", "\n", "            ", "for", "h_cat", "in", "h_cats", ":", "\n", "                ", "fp", "+=", "1", "\n", "# Each dict value [TP, FP, FN]", "\n", "if", "h_cat", "in", "cat_dict", ".", "keys", "(", ")", ":", "\n", "                    ", "cat_dict", "[", "h_cat", "]", "[", "1", "]", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "cat_dict", "[", "h_cat", "]", "=", "[", "0", ",", "1", ",", "0", "]", "\n", "", "", "", "", "for", "r_edit", ",", "r_cats", "in", "ref_edits", ".", "items", "(", ")", ":", "\n", "# noop ref edits cannot be FN", "\n", "        ", "if", "r_cats", "[", "0", "]", "==", "\"noop\"", ":", "continue", "\n", "# change to original edit", "\n", "if", "r_edit", "in", "ref_edits_alt", ".", "keys", "(", ")", ":", "\n", "            ", "r_edit", "=", "ref_edits_alt", "[", "r_edit", "]", "\n", "# FALSE NEGATIVES", "\n", "", "if", "r_edit", "not", "in", "hyp_edits", ".", "keys", "(", ")", ":", "\n", "# On occasion, multiple tokens at same span.", "\n", "            ", "for", "r_cat", "in", "r_cats", ":", "\n", "                ", "fn", "+=", "1", "\n", "# Each dict value [TP, FP, FN]", "\n", "if", "r_cat", "in", "cat_dict", ".", "keys", "(", ")", ":", "\n", "                    ", "cat_dict", "[", "r_cat", "]", "[", "2", "]", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "cat_dict", "[", "r_cat", "]", "=", "[", "0", ",", "0", ",", "1", "]", "\n", "", "", "", "", "return", "tp", ",", "fp", ",", "fn", ",", "cat_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.computeFScore": [[363, 368], ["round", "round", "round", "float", "float", "float"], "function", ["None"], ["", "def", "computeFScore", "(", "tp", ",", "fp", ",", "fn", ",", "beta", ")", ":", "\n", "    ", "p", "=", "float", "(", "tp", ")", "/", "(", "tp", "+", "fp", ")", "if", "fp", "else", "1.0", "\n", "r", "=", "float", "(", "tp", ")", "/", "(", "tp", "+", "fn", ")", "if", "fn", "else", "1.0", "\n", "f", "=", "float", "(", "(", "1", "+", "(", "beta", "**", "2", ")", ")", "*", "p", "*", "r", ")", "/", "(", "(", "(", "beta", "**", "2", ")", "*", "p", ")", "+", "r", ")", "if", "p", "+", "r", "else", "0.0", "\n", "return", "round", "(", "p", ",", "4", ")", ",", "round", "(", "r", ",", "4", ")", ",", "round", "(", "f", ",", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.merge_dict": [[371, 378], ["dict2.items", "dict1.keys", "zip"], "function", ["None"], ["", "def", "merge_dict", "(", "dict1", ",", "dict2", ")", ":", "\n", "    ", "for", "cat", ",", "stats", "in", "dict2", ".", "items", "(", ")", ":", "\n", "        ", "if", "cat", "in", "dict1", ".", "keys", "(", ")", ":", "\n", "            ", "dict1", "[", "cat", "]", "=", "[", "x", "+", "y", "for", "x", ",", "y", "in", "zip", "(", "dict1", "[", "cat", "]", ",", "stats", ")", "]", "\n", "", "else", ":", "\n", "            ", "dict1", "[", "cat", "]", "=", "stats", "\n", "", "", "return", "dict1", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.processCategories": [[383, 406], ["cat_dict.items", "proc_cat_dict.keys", "proc_cat_dict.keys", "zip", "zip"], "function", ["None"], ["", "def", "processCategories", "(", "cat_dict", ",", "setting", ")", ":", "\n", "# Otherwise, do some processing.", "\n", "    ", "proc_cat_dict", "=", "{", "}", "\n", "for", "cat", ",", "cnt", "in", "cat_dict", ".", "items", "(", ")", ":", "\n", "        ", "if", "cat", "==", "\"UNK\"", ":", "\n", "            ", "proc_cat_dict", "[", "cat", "]", "=", "cnt", "\n", "continue", "\n", "# M, U, R or UNK combined only.", "\n", "", "if", "setting", "==", "1", ":", "\n", "            ", "if", "cat", "[", "0", "]", "in", "proc_cat_dict", ".", "keys", "(", ")", ":", "\n", "                ", "proc_cat_dict", "[", "cat", "[", "0", "]", "]", "=", "[", "x", "+", "y", "for", "x", ",", "y", "in", "zip", "(", "proc_cat_dict", "[", "cat", "[", "0", "]", "]", ",", "cnt", ")", "]", "\n", "", "else", ":", "\n", "                ", "proc_cat_dict", "[", "cat", "[", "0", "]", "]", "=", "cnt", "\n", "# Everything without M, U or R.", "\n", "", "", "elif", "setting", "==", "2", ":", "\n", "            ", "if", "cat", "[", "2", ":", "]", "in", "proc_cat_dict", ".", "keys", "(", ")", ":", "\n", "                ", "proc_cat_dict", "[", "cat", "[", "2", ":", "]", "]", "=", "[", "x", "+", "y", "for", "x", ",", "y", "in", "zip", "(", "proc_cat_dict", "[", "cat", "[", "2", ":", "]", "]", ",", "cnt", ")", "]", "\n", "", "else", ":", "\n", "                ", "proc_cat_dict", "[", "cat", "[", "2", ":", "]", "]", "=", "cnt", "\n", "# All error category combinations", "\n", "", "", "else", ":", "\n", "            ", "return", "cat_dict", "\n", "", "", "return", "proc_cat_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.print_results": [[410, 437], ["print", "print", "print", "print", "print", "print", "compare_m2.processCategories", "print", "print", "print", "sorted", "processCategories.items", "compare_m2.computeFScore", "print", "map", "str", "cat.ljust", "str().ljust", "str().ljust", "str().ljust", "str().ljust", "str().ljust", "str", "list", "str", "str", "str", "str", "str", "compare_m2.computeFScore"], "function", ["home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.processCategories", "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.computeFScore", "home.repos.pwc.inspect_result.yuanxun-yx_errant.commands.compare_m2.computeFScore"], ["", "def", "print_results", "(", "best", ",", "best_cats", ",", "args", ")", ":", "\n", "# Prepare output title.", "\n", "    ", "if", "args", ".", "dt", ":", "title", "=", "\" Token-Based Detection \"", "\n", "elif", "args", ".", "ds", ":", "title", "=", "\" Span-Based Detection \"", "\n", "elif", "args", ".", "cse", ":", "title", "=", "\" Span-Based Correction + Classification \"", "\n", "else", ":", "title", "=", "\" Span-Based Correction \"", "\n", "\n", "# Category Scores", "\n", "if", "args", ".", "cat", ":", "\n", "        ", "best_cats", "=", "processCategories", "(", "best_cats", ",", "args", ".", "cat", ")", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "'{:=^66}'", ".", "format", "(", "title", ")", ")", "\n", "print", "(", "\"Category\"", ".", "ljust", "(", "14", ")", ",", "\"TP\"", ".", "ljust", "(", "8", ")", ",", "\"FP\"", ".", "ljust", "(", "8", ")", ",", "\"FN\"", ".", "ljust", "(", "8", ")", ",", "\n", "\"P\"", ".", "ljust", "(", "8", ")", ",", "\"R\"", ".", "ljust", "(", "8", ")", ",", "\"F\"", "+", "str", "(", "args", ".", "beta", ")", ")", "\n", "for", "cat", ",", "cnts", "in", "sorted", "(", "best_cats", ".", "items", "(", ")", ")", ":", "\n", "            ", "cat_p", ",", "cat_r", ",", "cat_f", "=", "computeFScore", "(", "cnts", "[", "0", "]", ",", "cnts", "[", "1", "]", ",", "cnts", "[", "2", "]", ",", "args", ".", "beta", ")", "\n", "print", "(", "cat", ".", "ljust", "(", "14", ")", ",", "str", "(", "cnts", "[", "0", "]", ")", ".", "ljust", "(", "8", ")", ",", "str", "(", "cnts", "[", "1", "]", ")", ".", "ljust", "(", "8", ")", ",", "\n", "str", "(", "cnts", "[", "2", "]", ")", ".", "ljust", "(", "8", ")", ",", "str", "(", "cat_p", ")", ".", "ljust", "(", "8", ")", ",", "str", "(", "cat_r", ")", ".", "ljust", "(", "8", ")", ",", "cat_f", ")", "\n", "\n", "# Print the overall results.", "\n", "", "", "print", "(", "\"\"", ")", "\n", "print", "(", "'{:=^46}'", ".", "format", "(", "title", ")", ")", "\n", "print", "(", "\"\\t\"", ".", "join", "(", "[", "\"TP\"", ",", "\"FP\"", ",", "\"FN\"", ",", "\"Prec\"", ",", "\"Rec\"", ",", "\"F\"", "+", "str", "(", "args", ".", "beta", ")", "]", ")", ")", "\n", "print", "(", "\"\\t\"", ".", "join", "(", "map", "(", "str", ",", "[", "best", "[", "\"tp\"", "]", ",", "best", "[", "\"fp\"", "]", ",", "\n", "best", "[", "\"fn\"", "]", "]", "+", "list", "(", "computeFScore", "(", "best", "[", "\"tp\"", "]", ",", "best", "[", "\"fp\"", "]", ",", "best", "[", "\"fn\"", "]", ",", "args", ".", "beta", ")", ")", ")", ")", ")", "\n", "print", "(", "'{:=^46}'", ".", "format", "(", "\"\"", ")", ")", "\n", "print", "(", "\"\"", ")", "\n", "\n"]]}