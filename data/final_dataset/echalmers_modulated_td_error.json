{"home.repos.pwc.inspect_result.echalmers_modulated_td_error.learners.dqn.TransitionMemory.__init__": [[29, 38], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "capacity", ",", "state_size", ",", "device", ")", ":", "\n", "        ", "self", ".", "index", "=", "-", "1", "\n", "self", ".", "size", "=", "0", "\n", "self", ".", "capacity", "=", "capacity", "\n", "self", ".", "states", "=", "torch", ".", "zeros", "(", "(", "capacity", ",", "state_size", ")", ",", "device", "=", "device", ")", "\n", "self", ".", "actions", "=", "torch", ".", "zeros", "(", "capacity", ",", "device", "=", "device", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "self", ".", "rewards", "=", "torch", ".", "zeros", "(", "capacity", ",", "device", "=", "device", ")", "\n", "self", ".", "new_states", "=", "torch", ".", "zeros", "(", "(", "capacity", ",", "state_size", ")", ",", "device", "=", "device", ")", "\n", "self", ".", "done", "=", "torch", ".", "zeros", "(", "capacity", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.learners.dqn.TransitionMemory.add": [[39, 49], ["torch.tensor", "torch.tensor", "torch.tensor", "min", "float"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "state", ",", "action", ",", "reward", ",", "new_state", ",", "done", ")", ":", "\n", "        ", "self", ".", "index", "=", "(", "self", ".", "index", "+", "1", ")", "%", "self", ".", "capacity", "\n", "\n", "self", ".", "states", "[", "self", ".", "index", ",", ":", "]", "=", "torch", ".", "tensor", "(", "state", ")", "\n", "self", ".", "actions", "[", "self", ".", "index", "]", "=", "action", "\n", "self", ".", "rewards", "[", "self", ".", "index", "]", "=", "reward", "\n", "self", ".", "new_states", "[", "self", ".", "index", ",", ":", "]", "=", "torch", ".", "tensor", "(", "new_state", ")", "\n", "self", ".", "done", "[", "self", ".", "index", "]", "=", "torch", ".", "tensor", "(", "float", "(", "done", ")", ")", "\n", "\n", "self", ".", "size", "=", "min", "(", "self", ".", "size", "+", "1", ",", "self", ".", "capacity", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.learners.dqn.TransitionMemory.sample": [[50, 55], ["min", "numpy.random.choice"], "methods", ["None"], ["", "def", "sample", "(", "self", ",", "n", ")", ":", "\n", "        ", "n", "=", "min", "(", "n", ",", "self", ".", "size", ")", "\n", "\n", "idx", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "size", ",", "n", ",", "replace", "=", "False", ")", "\n", "return", "self", ".", "states", "[", "idx", ",", ":", "]", ",", "self", ".", "actions", "[", "idx", "]", ",", "self", ".", "rewards", "[", "idx", "]", ",", "self", ".", "new_states", "[", "idx", ",", ":", "]", ",", "self", ".", "done", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.learners.dqn.TransitionMemory.last": [[56, 58], ["None"], "methods", ["None"], ["", "def", "last", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "states", "[", "self", ".", "index", ",", ":", "]", ",", "self", ".", "actions", "[", "self", ".", "index", "]", ",", "self", ".", "rewards", "[", "self", ".", "index", "]", ",", "self", ".", "new_states", "[", "self", ".", "index", "]", ",", "self", ".", "done", "[", "self", ".", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.learners.dqn.DQN.__init__": [[71, 118], ["torch.manual_seed", "dqn.build_network", "copy.deepcopy", "dqn.DQN.policy_net.to", "dqn.DQN.value_net.to", "torch.nn.MSELoss", "torch.optim.Adam", "dqn.TransitionMemory", "torch.tensor", "dqn.DQN.policy_net.parameters"], "methods", ["home.repos.pwc.inspect_result.echalmers_modulated_td_error.learners.dqn.build_network"], ["def", "__init__", "(", "self", ",", "n_inputs", ",", "n_outputs", ",", "batch_size", ",", "hidden_layer_sizes", ",", "replay_buffer_size", ",", "update_frequency", "=", "10", ",", "\n", "lr", "=", "1e-3", ",", "sync_frequency", "=", "5", ",", "\n", "gamma", "=", "0.95", ",", "epsilon", "=", "0.1", ",", "\n", "modulated_td_error", "=", "False", ",", "\n", "softmax_temp", "=", "1.0", ",", "\n", "seed", "=", "42", ")", ":", "\n", "        ", "\"\"\"\n        :param n_inputs: length of input vector\n        :param n_outputs: length of output vector\n        :param batch_size: number of experiences to sample from the replay buffer at each learning step\n        :param hidden_layer_sizes: list of hidden layer sizes\n        :param replay_buffer_size: number of experiences to keep in the replay buffer\n        :param update_frequency: number of steps before updating models\n        :param lr: learning rate\n        :param sync_frequency: number of steps to run between syncing the policy and value networks\n        :param gamma: discount factor\n        :param epsilon: parameter for e-greedy action sampling\n        :param modulated_td_error: if True, will use the new RL rule from the paper\n        :param softmax_temp: softmax tempurature for use in new RL rule\n        :param seed: random seed\n        \"\"\"", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "\n", "# instantiate networks", "\n", "self", ".", "policy_net", "=", "build_network", "(", "n_inputs", ",", "n_outputs", ",", "hidden_layer_sizes", ",", "activation", "=", "nn", ".", "Tanh", ")", "\n", "self", ".", "value_net", "=", "copy", ".", "deepcopy", "(", "self", ".", "policy_net", ")", "\n", "self", ".", "policy_net", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "value_net", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# instantiate loss and optimizer", "\n", "self", ".", "loss_fn", "=", "torch", ".", "nn", ".", "MSELoss", "(", ")", "\n", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "policy_net", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", "\n", "\n", "# instantiate experience memory", "\n", "self", ".", "transition_memory", "=", "TransitionMemory", "(", "capacity", "=", "replay_buffer_size", ",", "state_size", "=", "n_inputs", ",", "device", "=", "self", ".", "device", ")", "\n", "\n", "# store other params", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "update_frequency", "=", "update_frequency", "\n", "self", ".", "n_outputs", "=", "n_outputs", "\n", "self", ".", "sync_frequency", "=", "sync_frequency", "\n", "self", ".", "sync_counter", "=", "0", "\n", "self", ".", "gamma", "=", "torch", ".", "tensor", "(", "gamma", ",", "device", "=", "self", ".", "device", ")", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "update_counter", "=", "0", "\n", "self", ".", "modulated_td_error", "=", "modulated_td_error", "\n", "self", ".", "softmax_temp", "=", "softmax_temp", "\n", "\n"]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.learners.dqn.DQN.select_action": [[119, 125], ["index.item", "random.random", "numpy.random.choice", "torch.no_grad", "dqn.DQN.policy_net().max", "dqn.DQN.policy_net", "torch.tensor", "state.astype"], "methods", ["None"], ["", "def", "select_action", "(", "self", ",", "state", ")", ":", "\n", "        ", "if", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "np", ".", "random", ".", "choice", "(", "self", ".", "n_outputs", ")", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "max_q", ",", "index", "=", "self", ".", "policy_net", "(", "torch", ".", "tensor", "(", "state", ".", "astype", "(", "np", ".", "float32", ")", ",", "device", "=", "self", ".", "device", ")", ")", ".", "max", "(", "0", ")", "\n", "", "return", "index", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.learners.dqn.DQN.update": [[126, 163], ["dqn.DQN.transition_memory.add", "float", "ValueError", "dqn.DQN.transition_memory.sample", "dqn.DQN.policy_net", "dqn.DQN.clone().detach", "dqn.DQN.loss_fn", "dqn.DQN.optimizer.zero_grad", "dqn.DQN.backward", "dqn.DQN.optimizer.step", "dqn.DQN.item", "dqn.DQN.value_net.load_state_dict", "torch.no_grad", "dqn.DQN.value_net().max", "dqn.temperature_softmax", "dqn.DQN.policy_net.state_dict", "dqn.DQN.clone", "dqn.DQN.value_net", "numpy.arange", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.echalmers_modulated_td_error.learners.dqn.TransitionMemory.add", "home.repos.pwc.inspect_result.echalmers_modulated_td_error.learners.dqn.TransitionMemory.sample", "home.repos.pwc.inspect_result.echalmers_modulated_td_error.environments.n_armed_bandit.ChangingNArmedBanditEnv.step", "home.repos.pwc.inspect_result.echalmers_modulated_td_error.learners.dqn.temperature_softmax"], ["", "def", "update", "(", "self", ",", "state", ",", "action", ",", "reward", ",", "new_state", ",", "done", ")", ":", "\n", "\n", "        ", "if", "float", "(", "reward", ")", "not", "in", "[", "-", "1", ",", "0", ",", "1", "]", ":", "\n", "            ", "raise", "ValueError", "(", "'current implementation of DQN requires rewards of 1, 0, or -1'", ")", "\n", "\n", "", "self", ".", "transition_memory", ".", "add", "(", "state", ",", "action", ",", "reward", ",", "new_state", ",", "done", ")", "\n", "self", ".", "update_counter", "+=", "1", "\n", "if", "self", ".", "update_counter", "%", "self", ".", "update_frequency", "==", "0", ":", "\n", "\n", "# sync value and policy networks", "\n", "            ", "self", ".", "sync_counter", "+=", "1", "\n", "if", "self", ".", "sync_counter", "%", "self", ".", "sync_frequency", "==", "0", ":", "\n", "                ", "self", ".", "value_net", ".", "load_state_dict", "(", "self", ".", "policy_net", ".", "state_dict", "(", ")", ")", "\n", "\n", "", "s", ",", "a", ",", "r", ",", "ns", ",", "d", "=", "self", ".", "transition_memory", ".", "sample", "(", "self", ".", "batch_size", ")", "\n", "\n", "# get policy network's current value estimates", "\n", "state_action_values", "=", "self", ".", "policy_net", "(", "s", ")", "\n", "\n", "# get target value estimates, based on actual rewards and value net's predictions of next-state value", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "new_state_value", ",", "_", "=", "self", ".", "value_net", "(", "ns", ")", ".", "max", "(", "1", ")", "\n", "", "target_action_value", "=", "r", "+", "self", ".", "gamma", "*", "new_state_value", "*", "(", "1", "-", "d", ")", "\n", "target_values", "=", "state_action_values", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "if", "self", ".", "modulated_td_error", ":", "\n", "                ", "probabilities", "=", "temperature_softmax", "(", "state_action_values", ",", "dim", "=", "1", ",", "t", "=", "self", ".", "softmax_temp", ")", "\n", "target_action_value", "*=", "probabilities", "[", "np", ".", "arange", "(", "target_values", ".", "shape", "[", "0", "]", ")", ",", "a", "]", "\n", "", "target_values", "[", "np", ".", "arange", "(", "target_values", ".", "shape", "[", "0", "]", ")", ",", "a", "]", "=", "target_action_value", "\n", "# todo: fix the update - right now it only works for immediate rewards of 1 and -1", "\n", "\n", "# optimize loss", "\n", "loss", "=", "self", ".", "loss_fn", "(", "state_action_values", ",", "target_values", ")", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "return", "loss", ".", "item", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.learners.dqn.temperature_softmax": [[12, 15], ["torch.nn.Softmax", "torch.nn.Softmax."], "function", ["None"], ["def", "temperature_softmax", "(", "x", ",", "dim", "=", "0", ",", "t", "=", "1.0", ")", ":", "\n", "    ", "sm", "=", "torch", ".", "nn", ".", "Softmax", "(", "dim", "=", "dim", ")", "\n", "return", "sm", "(", "x", "/", "t", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.learners.dqn.build_network": [[17, 25], ["range", "torch.nn.Sequential", "layers.append", "list", "len", "torch.nn.Linear", "layers.append", "len", "activation"], "function", ["None"], ["", "def", "build_network", "(", "n_inputs", ",", "n_outputs", ",", "layer_sizes", ",", "activation", "=", "nn", ".", "Tanh", ")", ":", "\n", "    ", "layer_sizes", "=", "[", "n_inputs", "]", "+", "list", "(", "layer_sizes", ")", "+", "[", "n_outputs", "]", "\n", "layers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "layer_sizes", ")", "-", "1", ")", ":", "\n", "        ", "layers", ".", "append", "(", "nn", ".", "Linear", "(", "layer_sizes", "[", "i", "]", ",", "layer_sizes", "[", "i", "+", "1", "]", ")", ")", "\n", "if", "i", "<", "len", "(", "layer_sizes", ")", "-", "2", ":", "\n", "            ", "layers", ".", "append", "(", "activation", "(", ")", ")", "\n", "", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.learners.tabular.TabularLearner.__init__": [[23, 43], ["dict"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "action_list", ",", "default_value", "=", "1", ",", "alpha", "=", "0.1", ",", "gamma", "=", "0.9", ",", "epsilon", "=", "None", ",", "\n", "softmax_temperature", "=", "1", ",", "modulated_td_error", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        :param action_list: list of available actions in the environment\n        :param default_value: default value assigned to previously-untried state-action combinations\n        :param alpha: learning rate\n        :param gamma: discount factor\n        :param epsilon: parameter for e-greedy action selection\n        :param softmax_temperature: softmax temperature for use in the new RL rule. Will also be used for action\n        selection if specified and epsilon is not\n        :param modulated_td_error: if True, will use the new RL rule from the paper\n        \"\"\"", "\n", "self", ".", "Q", "=", "{", "action", ":", "dict", "(", ")", "for", "action", "in", "action_list", "}", "\n", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "default_value", "=", "default_value", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "softmax_temperature", "=", "softmax_temperature", "\n", "self", ".", "modulated_td_error", "=", "modulated_td_error", "\n", "self", ".", "sum_update", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.learners.tabular.TabularLearner.select_action": [[44, 55], ["isinstance", "max", "isinstance", "numpy.random.random", "numpy.random.choice", "numpy.random.choice", "list", "list", "tabular.TabularLearner.Q[].get", "tabular.softmax", "tuple", "tabular.TabularLearner.Q[].get", "tuple"], "methods", ["home.repos.pwc.inspect_result.echalmers_modulated_td_error.learners.tabular.softmax"], ["", "def", "select_action", "(", "self", ",", "state", ")", ":", "\n", "        ", "if", "isinstance", "(", "self", ".", "epsilon", ",", "Number", ")", ":", "\n", "            ", "if", "np", ".", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "                ", "return", "np", ".", "random", ".", "choice", "(", "list", "(", "self", ".", "Q", ")", ")", "\n", "\n", "", "", "elif", "isinstance", "(", "self", ".", "softmax_temperature", ",", "Number", ")", ":", "\n", "            ", "return", "np", ".", "random", ".", "choice", "(", "list", "(", "self", ".", "Q", ")", ",", "\n", "p", "=", "softmax", "(", "[", "self", ".", "Q", "[", "a", "]", ".", "get", "(", "tuple", "(", "state", ")", ",", "self", ".", "default_value", ")", "for", "a", "in", "self", ".", "Q", "]", ",", "t", "=", "self", ".", "softmax_temperature", ")", "\n", ")", "\n", "\n", "", "return", "max", "(", "self", ".", "Q", ",", "key", "=", "lambda", "a", ":", "self", ".", "Q", "[", "a", "]", ".", "get", "(", "tuple", "(", "state", ")", ",", "self", ".", "default_value", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.learners.tabular.TabularLearner.update": [[56, 72], ["tabular.TabularLearner.Q[].get", "tuple", "max", "tabular.softmax", "list().index", "tuple", "tuple", "tabular.TabularLearner.Q[].get", "tabular.TabularLearner.Q[].get", "tuple", "tuple", "list"], "methods", ["home.repos.pwc.inspect_result.echalmers_modulated_td_error.learners.tabular.softmax"], ["", "def", "update", "(", "self", ",", "state", ",", "action", ",", "reward", ",", "new_state", ",", "done", "=", "None", ")", ":", "\n", "        ", "current_val", "=", "self", ".", "Q", "[", "action", "]", ".", "get", "(", "tuple", "(", "state", ")", ",", "self", ".", "default_value", ")", "\n", "observed_val", "=", "reward", "+", "self", ".", "gamma", "*", "(", "max", "(", "[", "self", ".", "Q", "[", "a", "]", ".", "get", "(", "tuple", "(", "new_state", ")", ",", "self", ".", "default_value", ")", "for", "a", "in", "self", ".", "Q", "]", ")", ")", "\n", "\n", "if", "self", ".", "modulated_td_error", ":", "\n", "            ", "p_act", "=", "softmax", "(", "\n", "[", "self", ".", "Q", "[", "a", "]", ".", "get", "(", "tuple", "(", "state", ")", ",", "self", ".", "default_value", ")", "for", "a", "in", "self", ".", "Q", "]", ",", "\n", "t", "=", "self", ".", "softmax_temperature", "\n", ")", "[", "list", "(", "self", ".", "Q", ")", ".", "index", "(", "action", ")", "]", "\n", "\n", "self", ".", "Q", "[", "action", "]", "[", "tuple", "(", "state", ")", "]", "=", "current_val", "+", "self", ".", "alpha", "*", "p_act", "*", "(", "observed_val", "-", "current_val", ")", "\n", "self", ".", "sum_update", "+=", "self", ".", "alpha", "*", "p_act", "\n", "\n", "", "else", ":", "\n", "            ", "self", ".", "Q", "[", "action", "]", "[", "tuple", "(", "state", ")", "]", "=", "current_val", "+", "self", ".", "alpha", "*", "(", "observed_val", "-", "current_val", ")", "\n", "self", ".", "sum_update", "+=", "self", ".", "alpha", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.learners.tabular.softmax": [[7, 15], ["numpy.array().astype", "numpy.array", "numpy.exp", "numpy.exp().sum", "isinstance", "numpy.exp", "numpy.exp().sum().reshape", "numpy.array", "numpy.exp", "numpy.exp().sum", "numpy.exp"], "function", ["None"], ["def", "softmax", "(", "x", ":", "np", ".", "array", ",", "t", ")", ":", "\n", "    ", "x_local", "=", "np", ".", "array", "(", "x", ")", ".", "astype", "(", "float", ")", "\n", "if", "x_local", ".", "ndim", "==", "1", ":", "\n", "        ", "return", "np", ".", "exp", "(", "x_local", "/", "t", ")", "/", "np", ".", "exp", "(", "x_local", "/", "t", ")", ".", "sum", "(", ")", "\n", "", "else", ":", "\n", "        ", "if", "not", "isinstance", "(", "t", ",", "Number", ")", ":", "\n", "            ", "t", "=", "np", ".", "array", "(", "t", ")", "[", ":", ",", "None", "]", "\n", "", "return", "np", ".", "exp", "(", "x_local", "/", "t", ")", "/", "np", ".", "exp", "(", "x_local", "/", "t", ")", ".", "sum", "(", "axis", "=", "1", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.learners.abstract.ReinforcementLearner.select_action": [[6, 9], ["NotImplementedError"], "methods", ["None"], ["    ", "@", "abstractmethod", "\n", "def", "select_action", "(", "self", ",", "state", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.learners.abstract.ReinforcementLearner.update": [[10, 13], ["NotImplementedError"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "update", "(", "self", ",", "state", ",", "action", ",", "reward", ",", "new_state", ",", "done", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.util.experiment.ParamEvaluator.__init__": [[10, 14], ["copy.deepcopy"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "environment", ",", "agent_class", ",", "param_search_steps", "=", "1000", ")", ":", "\n", "        ", "self", ".", "environment", "=", "deepcopy", "(", "environment", ")", "\n", "self", ".", "agent_class", "=", "agent_class", "\n", "self", ".", "param_search_steps", "=", "param_search_steps", "\n", "\n"]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.util.experiment.ParamEvaluator.__call__": [[15, 19], ["Experiment.Experiment", "Experiment.Experiment.step", "Experiment.reward_history.sum", "Experiment.ParamEvaluator.agent_class"], "methods", ["home.repos.pwc.inspect_result.echalmers_modulated_td_error.environments.n_armed_bandit.ChangingNArmedBanditEnv.step"], ["", "def", "__call__", "(", "self", ",", "params", ")", ":", "\n", "        ", "experiment", "=", "Experiment", "(", "self", ".", "environment", ",", "self", ".", "agent_class", "(", "**", "params", ")", ")", "\n", "experiment", ".", "step", "(", "self", ".", "param_search_steps", ")", "\n", "return", "experiment", ".", "reward_history", ".", "sum", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.util.experiment.Experiment.__init__": [[39, 61], ["tuple", "copy.deepcopy", "copy.deepcopy", "numpy.zeros", "environment.reset", "experiment.optimize_params"], "methods", ["home.repos.pwc.inspect_result.echalmers_modulated_td_error.environments.n_armed_bandit.ChangingNArmedBanditEnv.reset", "home.repos.pwc.inspect_result.echalmers_modulated_td_error.util.experiment.optimize_params"], ["def", "__init__", "(", "self", ",", "environment", ",", "agent", ",", "params", "=", "None", ",", "param_search_steps", "=", "1000", ",", "sensors", "=", "tuple", "(", ")", ")", ":", "\n", "        ", "\"\"\"\n        :param environment: an environment instance\n        :param agent: a learner instance, or learner class. If a class, params must be provided\n        :param params: A dict mapping learner parameters to lists of values. All combinations will be tried, and the\n        best used in the experiment\n        :param param_search_steps: number of steps to use during parameter searches\n        :param sensors: a list of callables that accept the environment and agent as arguments. The values they return\n        will be saved and available through the .sensor_readings member\n        \"\"\"", "\n", "\n", "self", ".", "environment", "=", "deepcopy", "(", "environment", ")", "\n", "self", ".", "agent", "=", "deepcopy", "(", "agent", ")", "\n", "self", ".", "sensors", "=", "sensors", "\n", "self", ".", "sensor_readings", "=", "{", "sensor", ":", "[", "]", "for", "sensor", "in", "sensors", "}", "\n", "\n", "if", "params", "is", "not", "None", ":", "\n", "            ", "self", ".", "agent", "=", "optimize_params", "(", "environment", ",", "agent", ",", "params", ",", "param_search_steps", ")", "\n", "", "self", ".", "reward_history", "=", "np", ".", "zeros", "(", "0", ")", "\n", "self", ".", "action_history", "=", "[", "]", "\n", "\n", "self", ".", "state", "=", "environment", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.util.experiment.Experiment.step": [[62, 81], ["numpy.zeros", "range", "numpy.append", "experiment.Experiment.agent.select_action", "experiment.Experiment.environment.step", "experiment.Experiment.agent.update", "sensor"], "methods", ["home.repos.pwc.inspect_result.echalmers_modulated_td_error.learners.abstract.ReinforcementLearner.select_action", "home.repos.pwc.inspect_result.echalmers_modulated_td_error.environments.n_armed_bandit.ChangingNArmedBanditEnv.step", "home.repos.pwc.inspect_result.echalmers_modulated_td_error.learners.abstract.ReinforcementLearner.update"], ["", "def", "step", "(", "self", ",", "n", ")", ":", "\n", "        ", "this_rewards", "=", "np", ".", "zeros", "(", "n", ")", "\n", "these_actions", "=", "[", "None", "]", "*", "n", "\n", "these_readings", "=", "{", "sensor", ":", "[", "None", "]", "*", "n", "for", "sensor", "in", "self", ".", "sensors", "}", "\n", "\n", "for", "step", "in", "range", "(", "n", ")", ":", "#tqdm(range(n)):", "\n", "            ", "action", "=", "self", ".", "agent", ".", "select_action", "(", "self", ".", "state", ")", "\n", "new_state", ",", "reward", ",", "done", ",", "_", "=", "self", ".", "environment", ".", "step", "(", "action", ")", "\n", "self", ".", "agent", ".", "update", "(", "self", ".", "state", ",", "action", ",", "reward", ",", "new_state", ",", "done", ")", "\n", "this_rewards", "[", "step", "]", "=", "reward", "\n", "these_actions", "[", "step", "]", "=", "action", "\n", "\n", "self", ".", "state", "=", "new_state", "\n", "for", "sensor", "in", "these_readings", ":", "\n", "                ", "these_readings", "[", "sensor", "]", "[", "step", "]", "=", "sensor", "(", "self", ".", "environment", ",", "self", ".", "agent", ")", "\n", "\n", "", "", "self", ".", "reward_history", "=", "np", ".", "append", "(", "self", ".", "reward_history", ",", "this_rewards", ")", "\n", "for", "sensor", "in", "self", ".", "sensor_readings", ":", "\n", "            ", "self", ".", "sensor_readings", "[", "sensor", "]", "+=", "these_readings", "[", "sensor", "]", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.util.experiment.optimize_params": [[21, 32], ["dict", "len", "agent_class", "multiprocessing.Pool", "multiprocessing.Pool.map", "multiprocessing.Pool.close", "numpy.argmax", "print", "agent_class", "zip", "itertools.product", "multiprocessing.cpu_count", "experiment.ParamEvaluator", "list", "params.values"], "function", ["None"], ["", "", "def", "optimize_params", "(", "environment", ",", "agent_class", ",", "params", ",", "param_search_steps", "=", "1000", ")", ":", "\n", "    ", "combinations", "=", "[", "dict", "(", "zip", "(", "list", "(", "params", ")", ",", "combination", ")", ")", "for", "combination", "in", "itertools", ".", "product", "(", "*", "params", ".", "values", "(", ")", ")", "]", "\n", "if", "len", "(", "combinations", ")", "==", "1", ":", "\n", "        ", "return", "agent_class", "(", "**", "combinations", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "        ", "pool", "=", "multiprocessing", ".", "Pool", "(", "multiprocessing", ".", "cpu_count", "(", ")", ")", "\n", "rewards", "=", "pool", ".", "map", "(", "ParamEvaluator", "(", "environment", ",", "agent_class", ",", "param_search_steps", "=", "param_search_steps", ")", ",", "combinations", ")", "\n", "pool", ".", "close", "(", ")", "\n", "best_idx", "=", "np", ".", "argmax", "(", "rewards", ")", "\n", "print", "(", "f'{agent_class} best params:'", ",", "combinations", "[", "best_idx", "]", ")", "\n", "return", "agent_class", "(", "**", "combinations", "[", "best_idx", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.environments.card_sorting.CardSortingEnv.__init__": [[16, 51], ["sklearn.datasets.make_classification", "sklearn.preprocessing.StandardScaler().fit_transform", "gym.spaces.Box", "gym.spaces.Discrete", "card_sorting.CardSortingEnv.y.copy", "sklearn.preprocessing.StandardScaler", "card_sorting.CardSortingEnv.X.min", "card_sorting.CardSortingEnv.X.max"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "n_classes", ",", "n_features", ",", "samples_before_change", ",", "callback_on_change", "=", "None", ",", "show_current_ordering", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        :param n_classes: number of classes\n        :param n_features: number of predictor variables\n        :param samples_before_change: number of steps to run before shuffling classes\n        :param callback_on_change: a callable that will be called when classes are shuffled. It should accept this\n                                   CardSortingEnv instance as an argument.\n        :param show_current_ordering: if True, variables indicating the current ordering will be part of the state space\n        \"\"\"", "\n", "self", ".", "n_classes", "=", "n_classes", "\n", "self", ".", "n_features", "=", "n_features", "\n", "self", ".", "n_samples", "=", "100", "\n", "self", ".", "callback_on_change", "=", "callback_on_change", "\n", "self", ".", "show_current_ordering", "=", "show_current_ordering", "\n", "\n", "self", ".", "X", ",", "self", ".", "y", "=", "make_classification", "(", "\n", "n_samples", "=", "self", ".", "n_samples", ",", "\n", "n_features", "=", "n_features", ",", "\n", "n_informative", "=", "n_features", ",", "\n", "n_redundant", "=", "0", ",", "\n", "n_repeated", "=", "0", ",", "\n", "n_classes", "=", "n_classes", ",", "\n", "class_sep", "=", "5", ",", "\n", "n_clusters_per_class", "=", "1", ",", "\n", "scale", "=", "1", "/", "10", "\n", ")", "\n", "self", ".", "X", "=", "StandardScaler", "(", ")", ".", "fit_transform", "(", "self", ".", "X", ")", "\n", "self", ".", "observation_space", "=", "Box", "(", "low", "=", "self", ".", "X", ".", "min", "(", ")", ",", "high", "=", "self", ".", "X", ".", "max", "(", ")", ",", "shape", "=", "[", "n_features", "if", "not", "show_current_ordering", "else", "n_features", "+", "n_classes", "]", ")", "\n", "self", ".", "action_space", "=", "Discrete", "(", "n", "=", "n_classes", ")", "\n", "self", ".", "i", "=", "0", "\n", "self", ".", "current_ordering", "=", "0", "\n", "self", ".", "shuffled_y", "=", "self", ".", "y", ".", "copy", "(", ")", "\n", "\n", "self", ".", "change_counter", "=", "0", "\n", "self", ".", "samples_before_change", "=", "samples_before_change", "\n", "\n"]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.environments.card_sorting.CardSortingEnv.shuffle_classes": [[52, 55], ["None"], "methods", ["None"], ["", "def", "shuffle_classes", "(", "self", ")", ":", "\n", "        ", "self", ".", "current_ordering", "=", "(", "self", ".", "current_ordering", "+", "1", ")", "%", "self", ".", "n_classes", "\n", "self", ".", "shuffled_y", "=", "(", "self", ".", "y", "+", "self", ".", "current_ordering", ")", "%", "self", ".", "n_classes", "\n", "\n"]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.environments.card_sorting.CardSortingEnv._get_state_vector": [[56, 65], ["numpy.zeros", "numpy.concatenate"], "methods", ["None"], ["", "def", "_get_state_vector", "(", "self", ")", ":", "\n", "        ", "state_vec", "=", "self", ".", "X", "[", "self", ".", "i", "]", "\n", "\n", "if", "self", ".", "show_current_ordering", ":", "\n", "            ", "ordering_dummies", "=", "np", ".", "zeros", "(", "self", ".", "n_classes", ")", "\n", "ordering_dummies", "[", "self", ".", "current_ordering", "]", "=", "1", "\n", "state_vec", "=", "np", ".", "concatenate", "(", "(", "state_vec", ",", "ordering_dummies", ")", ")", "\n", "\n", "", "return", "state_vec", "\n", "\n"]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.environments.card_sorting.CardSortingEnv.reset": [[66, 71], ["card_sorting.CardSortingEnv.shuffle_classes", "card_sorting.CardSortingEnv._get_state_vector"], "methods", ["home.repos.pwc.inspect_result.echalmers_modulated_td_error.environments.card_sorting.CardSortingEnv.shuffle_classes", "home.repos.pwc.inspect_result.echalmers_modulated_td_error.environments.card_sorting.CardSortingEnv._get_state_vector"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "shuffle_classes", "(", ")", "\n", "self", ".", "i", "=", "0", "\n", "self", ".", "change_counter", "=", "0", "\n", "return", "self", ".", "_get_state_vector", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.environments.card_sorting.CardSortingEnv.step": [[72, 88], ["callable", "card_sorting.CardSortingEnv.shuffle_classes", "card_sorting.CardSortingEnv._get_state_vector", "card_sorting.CardSortingEnv._get_state_vector", "card_sorting.CardSortingEnv.callback_on_change"], "methods", ["home.repos.pwc.inspect_result.echalmers_modulated_td_error.environments.card_sorting.CardSortingEnv.shuffle_classes", "home.repos.pwc.inspect_result.echalmers_modulated_td_error.environments.card_sorting.CardSortingEnv._get_state_vector", "home.repos.pwc.inspect_result.echalmers_modulated_td_error.environments.card_sorting.CardSortingEnv._get_state_vector"], ["", "def", "step", "(", "self", ",", "action", ",", "check_action_only", "=", "False", ")", ":", "\n", "\n", "        ", "reward", "=", "1.0", "if", "action", "==", "self", ".", "shuffled_y", "[", "self", ".", "i", "]", "else", "-", "1.0", "\n", "\n", "if", "check_action_only", ":", "\n", "            ", "return", "self", ".", "_get_state_vector", "(", ")", ",", "reward", ",", "False", ",", "None", "\n", "\n", "", "self", ".", "change_counter", "=", "(", "self", ".", "change_counter", "+", "1", ")", "%", "self", ".", "samples_before_change", "\n", "if", "self", ".", "change_counter", "==", "0", ":", "\n", "            ", "if", "callable", "(", "self", ".", "callback_on_change", ")", ":", "\n", "                ", "self", ".", "callback_on_change", "(", "self", ")", "\n", "", "self", ".", "shuffle_classes", "(", ")", "\n", "\n", "", "self", ".", "i", "=", "(", "self", ".", "i", "+", "1", ")", "%", "self", ".", "n_samples", "\n", "\n", "return", "self", ".", "_get_state_vector", "(", ")", ",", "reward", ",", "False", ",", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.environments.n_armed_bandit.ChangingNArmedBanditEnv.__init__": [[10, 26], ["numpy.array", "gym.spaces.Discrete", "gym.spaces.Discrete", "len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "p", ",", "reward", "=", "1", ",", "penalty", "=", "-", "1", ",", "rotation_interval", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        :param p: length-n iterable of arm reward-probabilities\n        :param reward: scalar reward delivered on a succes\n        :param penalty: scalar penalty delivered on a failure\n        :param rotation_interval: number of pulls before probabilities rotate\n        \"\"\"", "\n", "self", ".", "p", "=", "np", ".", "array", "(", "p", ")", "\n", "self", ".", "reward", "=", "reward", "\n", "self", ".", "penalty", "=", "penalty", "\n", "self", ".", "rotation_interval", "=", "rotation_interval", "\n", "\n", "self", ".", "observation_space", "=", "Discrete", "(", "1", ")", "\n", "self", ".", "action_space", "=", "Discrete", "(", "len", "(", "p", ")", ")", "\n", "\n", "self", ".", "step_count", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.environments.n_armed_bandit.ChangingNArmedBanditEnv.reset": [[27, 30], ["numpy.roll", "numpy.array"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "p", "=", "np", ".", "roll", "(", "self", ".", "p", ",", "1", ")", "\n", "return", "np", ".", "array", "(", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.echalmers_modulated_td_error.environments.n_armed_bandit.ChangingNArmedBanditEnv.step": [[31, 39], ["n_armed_bandit.ChangingNArmedBanditEnv.reset", "numpy.array", "numpy.random.random"], "methods", ["home.repos.pwc.inspect_result.echalmers_modulated_td_error.environments.n_armed_bandit.ChangingNArmedBanditEnv.reset"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "self", ".", "step_count", "+=", "1", "\n", "if", "self", ".", "rotation_interval", "is", "not", "None", "and", "self", ".", "step_count", "%", "self", ".", "rotation_interval", "==", "0", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "\n", "", "reward", "=", "(", "np", ".", "random", ".", "random", "(", ")", "<=", "self", ".", "p", "[", "action", "]", ")", ".", "astype", "(", "int", ")", "\n", "reward", "=", "reward", "*", "(", "self", ".", "reward", "-", "self", ".", "penalty", ")", "+", "self", ".", "penalty", "\n", "return", "np", ".", "array", "(", "[", "0", "]", ")", ",", "reward", ",", "False", ",", "None", "\n", "", "", ""]]}