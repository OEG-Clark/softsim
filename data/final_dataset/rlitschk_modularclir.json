{"home.repos.pwc.inspect_result.rlitschk_modularclir.src.sft_mlm.ModelArguments.__post_init__": [[115, 119], ["ValueError"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", ":", "\n", "    ", "if", "self", ".", "config_overrides", "is", "not", "None", "and", "(", "self", ".", "config_name", "is", "not", "None", "or", "self", ".", "model_name_or_path", "is", "not", "None", ")", ":", "\n", "      ", "raise", "ValueError", "(", "\n", "\"--config_overrides can't be used in combination with --config_name or --model_name_or_path\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.src.sft_mlm.DataTrainingArguments.__post_init__": [[188, 198], ["ValueError", "sft_mlm.DataTrainingArguments.train_file.split", "sft_mlm.DataTrainingArguments.validation_file.split"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", ":", "\n", "    ", "if", "self", ".", "dataset_name", "is", "None", "and", "self", ".", "train_file", "is", "None", "and", "self", ".", "validation_file", "is", "None", ":", "\n", "      ", "raise", "ValueError", "(", "\"Need either a dataset name or a training/validation file.\"", ")", "\n", "", "else", ":", "\n", "      ", "if", "self", ".", "train_file", "is", "not", "None", ":", "\n", "        ", "extension", "=", "self", ".", "train_file", ".", "split", "(", "\".\"", ")", "[", "-", "1", "]", "\n", "assert", "extension", "in", "[", "\"csv\"", ",", "\"json\"", ",", "\"txt\"", "]", ",", "\"`train_file` should be a csv, a json or a txt file.\"", "\n", "", "if", "self", ".", "validation_file", "is", "not", "None", ":", "\n", "        ", "extension", "=", "self", ".", "validation_file", ".", "split", "(", "\".\"", ")", "[", "-", "1", "]", "\n", "assert", "extension", "in", "[", "\"csv\"", ",", "\"json\"", ",", "\"txt\"", "]", ",", "\"`validation_file` should be a csv, a json or a txt file.\"", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.src.sft_mlm.main": [[208, 657], ["print", "input", "print", "transformers.HfArgumentParser", "multiprocessing.cpu_count", "logger.info", "os.makedirs", "os.path.join", "logging.basicConfig", "training_args.get_process_log_level", "logger.setLevel", "datasets.utils.logging.set_verbosity", "transformers.utils.logging.set_verbosity", "transformers.utils.logging.enable_default_handler", "transformers.utils.logging.enable_explicit_format", "logger.warning", "logger.info", "transformers.set_seed", "AutoModelForMaskedLM.from_config.resize_token_embeddings", "transformers.DataCollatorForLanguageModeling", "lt_sft.LotteryTicketSparseFineTuner", "sys.argv[].endswith", "transformers.HfArgumentParser.parse_json_file", "transformers.HfArgumentParser.parse_args_into_dataclasses", "os.path.isdir", "transformers.trainer_utils.get_last_checkpoint", "datasets.load_dataset", "transformers.AutoConfig.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoModelForMaskedLM.from_pretrained", "logger.info", "transformers.AutoModelForMaskedLM.from_config", "len", "min", "torch.nn.Parameter", "lm_head.parameters", "torch.nn.Parameter", "torch.nn.Parameter", "AutoModelForMaskedLM.from_config.named_parameters", "lt_sft.LotteryTicketSparseFineTuner.train", "lt_sft.LotteryTicketSparseFineTuner.save_model", "min", "lt_sft.LotteryTicketSparseFineTuner.log_metrics", "lt_sft.LotteryTicketSparseFineTuner.save_metrics", "lt_sft.LotteryTicketSparseFineTuner.save_state", "logger.info", "os.path.join", "os.makedirs", "lt_sft.LotteryTicketSparseFineTuner.sft().save", "logger.info", "lt_sft.LotteryTicketSparseFineTuner.evaluate", "min", "lt_sft.LotteryTicketSparseFineTuner.log_metrics", "lt_sft.LotteryTicketSparseFineTuner.save_metrics", "lt_sft.LotteryTicketSparseFineTuner.push_to_hub", "len", "ValueError", "str", "logger.info", "datasets.load_dataset", "datasets.load_dataset", "datasets.load_dataset", "datasets.load_dataset.keys", "datasets.load_dataset", "datasets.load_dataset", "datasets.load_dataset.keys", "datasets.load_dataset", "datasets.load_dataset", "transformers.AutoConfig.from_pretrained", "logger.warning", "transformers.AutoTokenizer.from_pretrained", "ValueError", "logger.warning", "logger.warning", "AutoTokenizer.from_pretrained.", "training_args.main_process_first", "datasets.load_dataset.map", "AutoTokenizer.from_pretrained.", "training_args.main_process_first", "datasets.load_dataset.map", "len", "training_args.main_process_first", "tokenized_datasets.map.map", "ValueError", "train_dataset.select.select", "ValueError", "eval_dataset.select.select", "torch.zeros_like().copy_", "torch.zeros_like().copy_", "torch.zeros_like().copy_", "AutoModelForMaskedLM.from_config.named_parameters", "len", "len", "len", "len", "math.exp", "os.path.abspath", "logging.StreamHandler", "logging.FileHandler", "len", "logger.info", "data_args.dataset_config_name.split", "datasets.load_dataset.keys", "datasets.load_dataset", "data_args.train_file.split", "data_args.validation_file.split", "logger.info", "AutoConfig.from_pretrained.update_from_string", "bool", "sum", "range", "range", "ValueError", "n.startswith", "lt_sft.LotteryTicketSparseFineTuner.sft", "float", "bool", "os.listdir", "examples.keys", "concatenated_examples.items", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "range", "pathlib.Path.home", "len", "line.isspace", "list", "examples.keys"], "function", ["None"], ["", "def", "main", "(", ")", ":", "\n", "# See all possible arguments in src/transformers/training_args.py", "\n", "# or by passing the --help flag to this script.", "\n", "# We now keep distinct sets of args, for a cleaner separation of concerns.", "\n", "  ", "print", "(", "\"Select gpu device (CUDA_VISIBLE_DEVICES)\"", ")", "\n", "gpudevice", "=", "input", "(", "\"Specify GPU:\"", ")", "\n", "os", ".", "environ", "[", "\"CUDA_VISIBLE_DEVICES\"", "]", "=", "gpudevice", "\n", "print", "(", "f\"gpu device: {gpudevice}\"", ")", "\n", "\n", "parser", "=", "HfArgumentParser", "(", "(", "ModelArguments", ",", "DataTrainingArguments", ",", "SftArguments", ",", "TrainingArguments", ")", ")", "\n", "if", "len", "(", "sys", ".", "argv", ")", "==", "2", "and", "sys", ".", "argv", "[", "1", "]", ".", "endswith", "(", "\".json\"", ")", ":", "\n", "# If we pass only one argument to the script and it's the path to a json file,", "\n", "# let's parse it to get our arguments.", "\n", "    ", "model_args", ",", "data_args", ",", "sft_args", ",", "training_args", "=", "parser", ".", "parse_json_file", "(", "json_file", "=", "os", ".", "path", ".", "abspath", "(", "sys", ".", "argv", "[", "1", "]", ")", ")", "\n", "", "else", ":", "\n", "    ", "model_args", ",", "data_args", ",", "sft_args", ",", "training_args", "=", "parser", ".", "parse_args_into_dataclasses", "(", ")", "\n", "\n", "", "preprocessing_num_workers", "=", "multiprocessing", ".", "cpu_count", "(", ")", "\n", "data_args", ".", "preprocessing_num_workers", "=", "preprocessing_num_workers", "\n", "logger", ".", "info", "(", "f\"num pre-processing CPUs:{preprocessing_num_workers}\"", ")", "\n", "\n", "# Setup logging", "\n", "os", ".", "makedirs", "(", "training_args", ".", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "logfile", "=", "os", ".", "path", ".", "join", "(", "training_args", ".", "output_dir", ",", "\"log.txt\"", ")", "\n", "logging", ".", "basicConfig", "(", "\n", "format", "=", "\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\"", ",", "\n", "datefmt", "=", "\"%m/%d/%Y %H:%M:%S\"", ",", "\n", "handlers", "=", "[", "logging", ".", "StreamHandler", "(", "sys", ".", "stdout", ")", ",", "logging", ".", "FileHandler", "(", "filename", "=", "logfile", ")", "]", ",", "\n", ")", "\n", "\n", "log_level", "=", "training_args", ".", "get_process_log_level", "(", ")", "\n", "logger", ".", "setLevel", "(", "log_level", ")", "\n", "datasets", ".", "utils", ".", "logging", ".", "set_verbosity", "(", "log_level", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "set_verbosity", "(", "log_level", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "enable_default_handler", "(", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "enable_explicit_format", "(", ")", "\n", "\n", "# Log on each process the small summary:", "\n", "logger", ".", "warning", "(", "\n", "f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"", "\n", "+", "f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"", "\n", ")", "\n", "# Set the verbosity to info of the Transformers logger (on main process only):", "\n", "logger", ".", "info", "(", "f\"Training/evaluation parameters {training_args}\"", ")", "\n", "\n", "# Detecting last checkpoint.", "\n", "last_checkpoint", "=", "None", "\n", "if", "os", ".", "path", ".", "isdir", "(", "training_args", ".", "output_dir", ")", "and", "training_args", ".", "do_train", "and", "not", "training_args", ".", "overwrite_output_dir", ":", "\n", "    ", "last_checkpoint", "=", "get_last_checkpoint", "(", "training_args", ".", "output_dir", ")", "\n", "if", "last_checkpoint", "is", "None", "and", "len", "(", "os", ".", "listdir", "(", "training_args", ".", "output_dir", ")", ")", ">", "0", ":", "\n", "      ", "raise", "ValueError", "(", "\n", "f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"", "\n", "\"Use --overwrite_output_dir to overcome.\"", "\n", ")", "\n", "", "elif", "last_checkpoint", "is", "not", "None", "and", "training_args", ".", "resume_from_checkpoint", "is", "None", ":", "\n", "      ", "logger", ".", "info", "(", "\n", "f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"", "\n", "\"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"", "\n", ")", "\n", "\n", "# Set seed before initializing model.", "\n", "", "", "set_seed", "(", "training_args", ".", "seed", ")", "\n", "\n", "# Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)", "\n", "# or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/", "\n", "# (the dataset will be downloaded automatically from the datasets Hub", "\n", "#", "\n", "# For CSV/JSON files, this script will use the column called 'text' or the first column. You can easily tweak this", "\n", "# behavior (see below)", "\n", "#", "\n", "# In distributed training, the load_dataset function guarantee that only one local process can concurrently", "\n", "# download the dataset.", "\n", "if", "data_args", ".", "dataset_name", "is", "not", "None", ":", "\n", "    ", "if", "\"ru\"", "in", "data_args", ".", "dataset_config_name", "or", "\"fi\"", "in", "data_args", ".", "dataset_config_name", "or", "\"sw\"", "in", "data_args", ".", "dataset_config_name", "or", "\"so\"", "in", "data_args", ".", "dataset_config_name", ":", "\n", "      ", "lang", "=", "data_args", ".", "dataset_config_name", ".", "split", "(", "\".\"", ")", "[", "1", "]", "\n", "cache_dir", "=", "str", "(", "Path", ".", "home", "(", ")", "/", "'.cache'", "/", "'huggingface_wikipedia'", "/", "lang", ")", "\n", "logger", ".", "info", "(", "f\"Loading local dataset (beam_runner='DirectRunner', date='20220120'), cache_dir {cache_dir}\"", ")", "\n", "raw_datasets", "=", "load_dataset", "(", "\n", "'wikipedia'", ",", "\n", "f'20200501.{lang}'", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", "beam_runner", "=", "'DirectRunner'", ",", "\n", "date", "=", "\"20220120\"", "\n", ")", "\n", "if", "\"validation\"", "not", "in", "raw_datasets", ".", "keys", "(", ")", ":", "\n", "        ", "raw_datasets", "[", "\"validation\"", "]", "=", "load_dataset", "(", "\n", "'wikipedia'", ",", "\n", "f'20200501.{lang}'", ",", "\n", "split", "=", "f\"train[:{data_args.validation_split_percentage}%]\"", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", "beam_runner", "=", "'DirectRunner'", ",", "\n", "date", "=", "\"20220120\"", "\n", ")", "\n", "", "raw_datasets", "[", "\"train\"", "]", "=", "load_dataset", "(", "\n", "'wikipedia'", ",", "\n", "f'20200501.{lang}'", ",", "\n", "split", "=", "f\"train[{data_args.validation_split_percentage}%:]\"", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", "beam_runner", "=", "'DirectRunner'", ",", "\n", "date", "=", "\"20220120\"", "\n", ")", "\n", "", "else", ":", "\n", "# Downloading and loading a dataset from the hub.", "\n", "      ", "raw_datasets", "=", "load_dataset", "(", "\n", "data_args", ".", "dataset_name", ",", "\n", "data_args", ".", "dataset_config_name", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", "\n", ")", "\n", "", "if", "\"validation\"", "not", "in", "raw_datasets", ".", "keys", "(", ")", ":", "\n", "      ", "raw_datasets", "[", "\"validation\"", "]", "=", "load_dataset", "(", "\n", "data_args", ".", "dataset_name", ",", "\n", "data_args", ".", "dataset_config_name", ",", "\n", "split", "=", "f\"train[:{data_args.validation_split_percentage}%]\"", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", ")", "\n", "raw_datasets", "[", "\"train\"", "]", "=", "load_dataset", "(", "\n", "data_args", ".", "dataset_name", ",", "\n", "data_args", ".", "dataset_config_name", ",", "\n", "split", "=", "f\"train[{data_args.validation_split_percentage}%:]\"", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "    ", "data_files", "=", "{", "}", "\n", "if", "data_args", ".", "train_file", "is", "not", "None", ":", "\n", "      ", "data_files", "[", "\"train\"", "]", "=", "data_args", ".", "train_file", "\n", "extension", "=", "data_args", ".", "train_file", ".", "split", "(", "\".\"", ")", "[", "-", "1", "]", "\n", "", "if", "data_args", ".", "validation_file", "is", "not", "None", ":", "\n", "      ", "data_files", "[", "\"validation\"", "]", "=", "data_args", ".", "validation_file", "\n", "extension", "=", "data_args", ".", "validation_file", ".", "split", "(", "\".\"", ")", "[", "-", "1", "]", "\n", "", "if", "extension", "==", "\"txt\"", ":", "\n", "      ", "extension", "=", "\"text\"", "\n", "", "raw_datasets", "=", "load_dataset", "(", "extension", ",", "data_files", "=", "data_files", ",", "cache_dir", "=", "model_args", ".", "cache_dir", ")", "\n", "\n", "# If no validation data is there, validation_split_percentage will be used to divide the dataset.", "\n", "if", "\"validation\"", "not", "in", "raw_datasets", ".", "keys", "(", ")", ":", "\n", "      ", "raw_datasets", "[", "\"validation\"", "]", "=", "load_dataset", "(", "\n", "extension", ",", "\n", "data_files", "=", "data_files", ",", "\n", "split", "=", "f\"train[:{data_args.validation_split_percentage}%]\"", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", ")", "\n", "raw_datasets", "[", "\"train\"", "]", "=", "load_dataset", "(", "\n", "extension", ",", "\n", "data_files", "=", "data_files", ",", "\n", "split", "=", "f\"train[{data_args.validation_split_percentage}%:]\"", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", ")", "\n", "\n", "# See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at", "\n", "# https://huggingface.co/docs/datasets/loading_datasets.html.", "\n", "\n", "# Load pretrained model and tokenizer", "\n", "#", "\n", "# Distributed training:", "\n", "# The .from_pretrained methods guarantee that only one local process can concurrently", "\n", "# download model & vocab.", "\n", "", "", "config_kwargs", "=", "{", "\n", "\"cache_dir\"", ":", "model_args", ".", "cache_dir", ",", "\n", "\"revision\"", ":", "model_args", ".", "model_revision", ",", "\n", "\"use_auth_token\"", ":", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", "}", "\n", "if", "model_args", ".", "config_name", ":", "\n", "    ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "model_args", ".", "config_name", ",", "**", "config_kwargs", ")", "\n", "", "elif", "model_args", ".", "model_name_or_path", ":", "\n", "    ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "model_args", ".", "model_name_or_path", ",", "**", "config_kwargs", ")", "\n", "", "else", ":", "\n", "    ", "config", "=", "CONFIG_MAPPING", "[", "model_args", ".", "model_type", "]", "(", ")", "\n", "logger", ".", "warning", "(", "\"You are instantiating a new config instance from scratch.\"", ")", "\n", "if", "model_args", ".", "config_overrides", "is", "not", "None", ":", "\n", "      ", "logger", ".", "info", "(", "f\"Overriding config: {model_args.config_overrides}\"", ")", "\n", "config", ".", "update_from_string", "(", "model_args", ".", "config_overrides", ")", "\n", "\n", "", "", "tokenizer_kwargs", "=", "{", "\n", "\"cache_dir\"", ":", "model_args", ".", "cache_dir", ",", "\n", "\"use_fast\"", ":", "model_args", ".", "use_fast_tokenizer", ",", "\n", "\"revision\"", ":", "model_args", ".", "model_revision", ",", "\n", "\"use_auth_token\"", ":", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", "}", "\n", "if", "model_args", ".", "tokenizer_name", ":", "\n", "    ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "model_args", ".", "tokenizer_name", ",", "**", "tokenizer_kwargs", ")", "\n", "", "elif", "model_args", ".", "model_name_or_path", ":", "\n", "    ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "model_args", ".", "model_name_or_path", ",", "**", "tokenizer_kwargs", ")", "\n", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"", "\n", "\"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"", "\n", ")", "\n", "\n", "", "if", "model_args", ".", "model_name_or_path", ":", "\n", "    ", "model", "=", "AutoModelForMaskedLM", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "\n", "from_tf", "=", "bool", "(", "\".ckpt\"", "in", "model_args", ".", "model_name_or_path", ")", ",", "\n", "config", "=", "config", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", "revision", "=", "model_args", ".", "model_revision", ",", "\n", "use_auth_token", "=", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", ")", "\n", "", "else", ":", "\n", "    ", "logger", ".", "info", "(", "\"Training new model from scratch\"", ")", "\n", "model", "=", "AutoModelForMaskedLM", ".", "from_config", "(", "config", ")", "\n", "\n", "", "model", ".", "resize_token_embeddings", "(", "len", "(", "tokenizer", ")", ")", "\n", "\n", "# Preprocessing the datasets.", "\n", "\n", "# First we tokenize all the texts.", "\n", "if", "training_args", ".", "do_train", ":", "\n", "    ", "column_names", "=", "raw_datasets", "[", "\"train\"", "]", ".", "column_names", "\n", "", "else", ":", "\n", "    ", "column_names", "=", "raw_datasets", "[", "\"validation\"", "]", ".", "column_names", "\n", "", "text_column_name", "=", "\"text\"", "if", "\"text\"", "in", "column_names", "else", "column_names", "[", "0", "]", "\n", "\n", "if", "data_args", ".", "max_seq_length", "is", "None", ":", "\n", "    ", "max_seq_length", "=", "tokenizer", ".", "model_max_length", "\n", "if", "max_seq_length", ">", "1024", ":", "\n", "      ", "logger", ".", "warning", "(", "\n", "f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"", "\n", "\"Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.\"", "\n", ")", "\n", "max_seq_length", "=", "1024", "\n", "", "", "else", ":", "\n", "    ", "if", "data_args", ".", "max_seq_length", ">", "tokenizer", ".", "model_max_length", ":", "\n", "      ", "logger", ".", "warning", "(", "\n", "f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the\"", "\n", "f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"", "\n", ")", "\n", "", "max_seq_length", "=", "min", "(", "data_args", ".", "max_seq_length", ",", "tokenizer", ".", "model_max_length", ")", "\n", "\n", "", "if", "data_args", ".", "line_by_line", ":", "\n", "# When using line_by_line, we just tokenize each nonempty line.", "\n", "    ", "padding", "=", "\"max_length\"", "if", "data_args", ".", "pad_to_max_length", "else", "False", "\n", "\n", "def", "tokenize_function", "(", "examples", ")", ":", "\n", "# Remove empty lines", "\n", "      ", "examples", "[", "text_column_name", "]", "=", "[", "\n", "line", "for", "line", "in", "examples", "[", "text_column_name", "]", "if", "len", "(", "line", ")", ">", "0", "and", "not", "line", ".", "isspace", "(", ")", "\n", "]", "\n", "return", "tokenizer", "(", "\n", "examples", "[", "text_column_name", "]", ",", "\n", "padding", "=", "padding", ",", "\n", "truncation", "=", "True", ",", "\n", "max_length", "=", "max_seq_length", ",", "\n", "# We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it", "\n", "# receives the `special_tokens_mask`.", "\n", "return_special_tokens_mask", "=", "True", ",", "\n", ")", "\n", "\n", "", "with", "training_args", ".", "main_process_first", "(", "desc", "=", "\"dataset map tokenization\"", ")", ":", "\n", "      ", "tokenized_datasets", "=", "raw_datasets", ".", "map", "(", "\n", "tokenize_function", ",", "\n", "batched", "=", "True", ",", "\n", "num_proc", "=", "data_args", ".", "preprocessing_num_workers", ",", "\n", "remove_columns", "=", "[", "text_column_name", "]", ",", "\n", "load_from_cache_file", "=", "True", ",", "# not data_args.overwrite_cache,", "\n", "desc", "=", "\"Running tokenizer on dataset line_by_line\"", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "# Otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.", "\n", "# We use `return_special_tokens_mask=True` because DataCollatorForLanguageModeling (see below) is more", "\n", "# efficient when it receives the `special_tokens_mask`.", "\n", "    ", "def", "tokenize_function", "(", "examples", ")", ":", "\n", "      ", "return", "tokenizer", "(", "examples", "[", "text_column_name", "]", ",", "return_special_tokens_mask", "=", "True", ")", "\n", "\n", "", "with", "training_args", ".", "main_process_first", "(", "desc", "=", "\"dataset map tokenization\"", ")", ":", "\n", "      ", "tokenized_datasets", "=", "raw_datasets", ".", "map", "(", "\n", "tokenize_function", ",", "\n", "batched", "=", "True", ",", "\n", "num_proc", "=", "data_args", ".", "preprocessing_num_workers", ",", "\n", "remove_columns", "=", "column_names", ",", "\n", "load_from_cache_file", "=", "True", ",", "# not data_args.overwrite_cache,", "\n", "desc", "=", "\"Running tokenizer on every text in dataset\"", ",", "\n", ")", "\n", "\n", "# Main data processing function that will concatenate all texts from our dataset and generate chunks of", "\n", "# max_seq_length.", "\n", "", "def", "group_texts", "(", "examples", ")", ":", "\n", "# Concatenate all texts.", "\n", "      ", "concatenated_examples", "=", "{", "k", ":", "sum", "(", "examples", "[", "k", "]", ",", "[", "]", ")", "for", "k", "in", "examples", ".", "keys", "(", ")", "}", "\n", "total_length", "=", "len", "(", "concatenated_examples", "[", "list", "(", "examples", ".", "keys", "(", ")", ")", "[", "0", "]", "]", ")", "\n", "# We drop the small remainder, we could add padding if the model supported it instead of this drop, you can", "\n", "# customize this part to your needs.", "\n", "total_length", "=", "(", "total_length", "//", "max_seq_length", ")", "*", "max_seq_length", "\n", "\n", "# Split by chunks of max_len.", "\n", "result", "=", "{", "\n", "k", ":", "[", "t", "[", "i", ":", "i", "+", "max_seq_length", "]", "for", "i", "in", "range", "(", "0", ",", "total_length", ",", "max_seq_length", ")", "]", "\n", "for", "k", ",", "t", "in", "concatenated_examples", ".", "items", "(", ")", "\n", "}", "\n", "return", "result", "\n", "\n", "# Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a", "\n", "# remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value", "\n", "# might be slower to preprocess.", "\n", "#", "\n", "# To speed up this part, we use multiprocessing. See the documentation of the map method for more information:", "\n", "# https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map", "\n", "\n", "", "with", "training_args", ".", "main_process_first", "(", "desc", "=", "\"grouping texts together\"", ")", ":", "\n", "      ", "tokenized_datasets", "=", "tokenized_datasets", ".", "map", "(", "\n", "group_texts", ",", "\n", "batched", "=", "True", ",", "\n", "num_proc", "=", "data_args", ".", "preprocessing_num_workers", ",", "\n", "load_from_cache_file", "=", "True", ",", "# not data_args.overwrite_cache,", "\n", "desc", "=", "f\"Grouping texts in chunks of {max_seq_length}\"", ",", "\n", ")", "\n", "\n", "\n", "# import gc", "\n", "# gc.collect()", "\n", "# torch.cuda.empty_cache()", "\n", "# print(\"cleaned cache\")", "\n", "\n", "", "", "if", "training_args", ".", "do_train", ":", "\n", "    ", "if", "\"train\"", "not", "in", "tokenized_datasets", ":", "\n", "      ", "raise", "ValueError", "(", "\"--do_train requires a train dataset\"", ")", "\n", "", "train_dataset", "=", "tokenized_datasets", "[", "\"train\"", "]", "\n", "if", "data_args", ".", "max_train_samples", "is", "not", "None", ":", "\n", "      ", "train_dataset", "=", "train_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_train_samples", ")", ")", "\n", "\n", "", "", "if", "training_args", ".", "do_eval", ":", "\n", "    ", "if", "\"validation\"", "not", "in", "tokenized_datasets", ":", "\n", "      ", "raise", "ValueError", "(", "\"--do_eval requires a validation dataset\"", ")", "\n", "", "eval_dataset", "=", "tokenized_datasets", "[", "\"validation\"", "]", "\n", "if", "data_args", ".", "max_eval_samples", "is", "not", "None", ":", "\n", "      ", "eval_dataset", "=", "eval_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_eval_samples", ")", ")", "\n", "\n", "# Data collator", "\n", "# This one will take care of randomly masking the tokens.", "\n", "", "", "pad_to_multiple_of_8", "=", "data_args", ".", "line_by_line", "and", "training_args", ".", "fp16", "and", "not", "data_args", ".", "pad_to_max_length", "\n", "data_collator", "=", "DataCollatorForLanguageModeling", "(", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "mlm_probability", "=", "data_args", ".", "mlm_probability", ",", "\n", "pad_to_multiple_of", "=", "8", "if", "pad_to_multiple_of_8", "else", "None", ",", "\n", ")", "\n", "\n", "if", "model", ".", "base_model_prefix", "==", "'bert'", ":", "\n", "    ", "embeddings", "=", "model", ".", "base_model", ".", "embeddings", ".", "word_embeddings", ".", "weight", "\n", "lm_head", "=", "model", ".", "cls", "\n", "decoder", "=", "model", ".", "cls", ".", "predictions", ".", "decoder", "\n", "", "elif", "model", ".", "base_model_prefix", "==", "'roberta'", ":", "\n", "    ", "embeddings", "=", "model", ".", "base_model", ".", "embeddings", ".", "word_embeddings", ".", "weight", "\n", "lm_head", "=", "model", ".", "lm_head", "\n", "decoder", "=", "model", ".", "lm_head", ".", "decoder", "\n", "", "elif", "model", ".", "base_model_prefix", "==", "'transformer'", ":", "# XLM", "\n", "    ", "assert", "'xlm-mlm-100-1280'", "in", "model", ".", "name_or_path", "\n", "embeddings", "=", "model", ".", "base_model", ".", "embeddings", ".", "weight", "\n", "lm_head", "=", "model", ".", "pred_layer", "\n", "decoder", "=", "lm_head", "\n", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "f'Unsupported model type {model.base_model_prefix}'", ")", "\n", "\n", "", "if", "sft_args", ".", "freeze_head", ":", "\n", "    ", "decoder", ".", "weight", "=", "nn", ".", "Parameter", "(", "\n", "torch", ".", "zeros_like", "(", "embeddings", ")", ".", "copy_", "(", "embeddings", ")", "\n", ")", "\n", "for", "param", "in", "lm_head", ".", "parameters", "(", ")", ":", "\n", "      ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "", "if", "sft_args", ".", "untie_embeddings", ":", "\n", "    ", "decoder", ".", "weight", "=", "nn", ".", "Parameter", "(", "\n", "torch", ".", "zeros_like", "(", "embeddings", ")", ".", "copy_", "(", "embeddings", ")", "\n", ")", "\n", "\n", "", "if", "sft_args", ".", "freeze_decoder", ":", "\n", "    ", "decoder", ".", "weight", "=", "nn", ".", "Parameter", "(", "\n", "torch", ".", "zeros_like", "(", "embeddings", ")", ".", "copy_", "(", "embeddings", ")", "\n", ")", "\n", "decoder", ".", "weight", ".", "requires_grad", "=", "False", "\n", "\n", "", "if", "sft_args", ".", "freeze_embeddings", ":", "\n", "    ", "embeddings", ".", "requires_grad", "=", "False", "\n", "\n", "", "if", "sft_args", ".", "freeze_layer_norm", ":", "\n", "    ", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "      ", "if", "'LayerNorm'", "in", "n", ":", "\n", "        ", "p", ".", "requires_grad", "=", "False", "\n", "\n", "", "", "", "maskable_params", "=", "[", "\n", "n", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "\n", "if", "n", ".", "startswith", "(", "model", ".", "base_model_prefix", ")", "and", "p", ".", "requires_grad", "\n", "]", "\n", "\n", "# Initialize our Trainer", "\n", "trainer", "=", "LotteryTicketSparseFineTuner", "(", "\n", "sft_args", "=", "sft_args", ",", "\n", "maskable_params", "=", "maskable_params", ",", "\n", "model", "=", "model", ",", "\n", "args", "=", "training_args", ",", "\n", "train_dataset", "=", "train_dataset", "if", "training_args", ".", "do_train", "else", "None", ",", "\n", "eval_dataset", "=", "eval_dataset", "if", "training_args", ".", "do_eval", "else", "None", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "data_collator", "=", "data_collator", ",", "\n", ")", "\n", "\n", "# Training", "\n", "if", "training_args", ".", "do_train", ":", "\n", "    ", "checkpoint", "=", "None", "\n", "if", "training_args", ".", "resume_from_checkpoint", "is", "not", "None", ":", "\n", "      ", "checkpoint", "=", "training_args", ".", "resume_from_checkpoint", "\n", "", "elif", "last_checkpoint", "is", "not", "None", ":", "\n", "      ", "checkpoint", "=", "last_checkpoint", "\n", "", "train_result", "=", "trainer", ".", "train", "(", "resume_from_checkpoint", "=", "checkpoint", ")", "\n", "trainer", ".", "save_model", "(", ")", "# Saves the tokenizer too for easy upload", "\n", "metrics", "=", "train_result", ".", "metrics", "\n", "\n", "max_train_samples", "=", "(", "\n", "data_args", ".", "max_train_samples", "if", "data_args", ".", "max_train_samples", "is", "not", "None", "else", "len", "(", "train_dataset", ")", "\n", ")", "\n", "metrics", "[", "\"train_samples\"", "]", "=", "min", "(", "max_train_samples", ",", "len", "(", "train_dataset", ")", ")", "\n", "\n", "trainer", ".", "log_metrics", "(", "\"train\"", ",", "metrics", ")", "\n", "trainer", ".", "save_metrics", "(", "\"train\"", ",", "metrics", ")", "\n", "trainer", ".", "save_state", "(", ")", "\n", "logger", ".", "info", "(", "\"Save SFT parameters\"", ")", "\n", "sft_output_dir", "=", "os", ".", "path", ".", "join", "(", "training_args", ".", "output_dir", ",", "\"language_mask\"", ")", "\n", "os", ".", "makedirs", "(", "sft_output_dir", ")", "\n", "trainer", ".", "sft", "(", ")", ".", "save", "(", "sft_output_dir", ")", "\n", "\n", "# Evaluation", "\n", "", "if", "training_args", ".", "do_eval", ":", "\n", "    ", "logger", ".", "info", "(", "\"*** Evaluate ***\"", ")", "\n", "\n", "metrics", "=", "trainer", ".", "evaluate", "(", ")", "\n", "\n", "max_eval_samples", "=", "data_args", ".", "max_eval_samples", "if", "data_args", ".", "max_eval_samples", "is", "not", "None", "else", "len", "(", "eval_dataset", ")", "\n", "metrics", "[", "\"eval_samples\"", "]", "=", "min", "(", "max_eval_samples", ",", "len", "(", "eval_dataset", ")", ")", "\n", "try", ":", "\n", "      ", "perplexity", "=", "math", ".", "exp", "(", "metrics", "[", "\"eval_loss\"", "]", ")", "\n", "", "except", "OverflowError", ":", "\n", "      ", "perplexity", "=", "float", "(", "\"inf\"", ")", "\n", "", "metrics", "[", "\"perplexity\"", "]", "=", "perplexity", "\n", "\n", "trainer", ".", "log_metrics", "(", "\"eval\"", ",", "metrics", ")", "\n", "trainer", ".", "save_metrics", "(", "\"eval\"", ",", "metrics", ")", "\n", "\n", "", "if", "training_args", ".", "push_to_hub", ":", "\n", "    ", "kwargs", "=", "{", "\"finetuned_from\"", ":", "model_args", ".", "model_name_or_path", ",", "\"tasks\"", ":", "\"fill-mask\"", "}", "\n", "if", "data_args", ".", "dataset_name", "is", "not", "None", ":", "\n", "      ", "kwargs", "[", "\"dataset_tags\"", "]", "=", "data_args", ".", "dataset_name", "\n", "if", "data_args", ".", "dataset_config_name", "is", "not", "None", ":", "\n", "        ", "kwargs", "[", "\"dataset_args\"", "]", "=", "data_args", ".", "dataset_config_name", "\n", "kwargs", "[", "\"dataset\"", "]", "=", "f\"{data_args.dataset_name} {data_args.dataset_config_name}\"", "\n", "", "else", ":", "\n", "        ", "kwargs", "[", "\"dataset\"", "]", "=", "data_args", ".", "dataset_name", "\n", "\n", "", "", "trainer", ".", "push_to_hub", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.src.sft_mlm._mp_fn": [[659, 662], ["sft_mlm.main"], "function", ["home.repos.pwc.inspect_result.rlitschk_modularclir.helper.prepare_msmarco.main"], ["", "", "def", "_mp_fn", "(", "index", ")", ":", "\n", "# For xla_spawn (TPUs)", "\n", "  ", "main", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.src.adapter_retrieval.main": [[175, 409], ["print", "input", "transformers.HfArgumentParser", "transformers.HfArgumentParser.parse_args_into_dataclasses", "os.makedirs", "os.path.join", "logging.FileHandler", "logging.basicConfig", "training_args.get_process_log_level", "logger.setLevel", "datasets.utils.logging.set_verbosity", "transformers.utils.logging.add_handler", "transformers.utils.logging.add_handler", "transformers.utils.logging.set_verbosity", "transformers.utils.logging.set_verbosity", "transformers.utils.logging.enable_default_handler", "transformers.utils.logging.enable_default_handler", "transformers.utils.logging.enable_explicit_format", "transformers.utils.logging.enable_explicit_format", "logger.warning", "logger.info", "transformers.set_seed", "transformers.AutoConfig.from_pretrained", "transformers.AutoModelForSequenceClassification.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "min", "multiprocessing.cpu_count", "list", "transformers.AdamW", "transformers.get_linear_schedule_with_warmup", "transformers.get_linear_schedule_with_warmup", "model.to.to", "trainer_class", "os.path.isdir", "transformers.trainer_utils.get_last_checkpoint", "model.to.train_adapter", "AutoTokenizer.from_pretrained.", "torch.tensor", "torch.unsqueeze", "random.sample", "len", "model.to.named_parameters", "trainer_class.train", "min", "trainer_class.save_model", "trainer_class.log_metrics", "trainer_class.save_metrics", "trainer_class.save_state", "logger.info", "trainer_class.evaluate", "min", "trainer_class.log_metrics", "trainer_class.save_metrics", "ValueError", "datasets.load_dataset", "datasets.load_dataset", "datasets.load_dataset", "transformers.AdapterConfig.load", "transformers.AdapterConfig.load", "model.to.load_adapter", "model.to.set_active_adapters", "model.to.set_active_adapters", "ValueError", "e[].strip", "e[].strip", "train_dataset.select.select", "range", "logger.info", "eval_dataset.select.select", "predict_dataset.select.select", "len", "len", "len", "len", "logging.StreamHandler", "len", "logger.info", "model.to.load_adapter", "model.to.add_adapter", "transformers.Stack", "range", "len", "range", "range", "bool", "os.listdir", "any", "any"], "function", ["None"], ["", "def", "main", "(", ")", ":", "\n", "    ", "print", "(", "\"Select gpu device (CUDA_VISIBLE_DEVICES)\"", ")", "\n", "gpudevice", "=", "input", "(", "\"Specify GPU: \"", ")", "\n", "os", ".", "environ", "[", "\"CUDA_VISIBLE_DEVICES\"", "]", "=", "gpudevice", "\n", "\n", "# See all possible arguments in src/transformers/training_args.py", "\n", "# or by passing the --help flag to this script.", "\n", "# We now keep distinct sets of args, for a cleaner separation of concerns.", "\n", "parser", "=", "HfArgumentParser", "(", "(", "ModelArguments", ",", "DataTrainingArguments", ",", "TrainingArguments", ",", "MultiLingAdapterArguments", ")", ")", "\n", "model_args", ",", "data_args", ",", "training_args", ",", "adapter_args", "=", "parser", ".", "parse_args_into_dataclasses", "(", ")", "\n", "\n", "# Setup logging", "\n", "os", ".", "makedirs", "(", "training_args", ".", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "logfile", "=", "os", ".", "path", ".", "join", "(", "training_args", ".", "output_dir", ",", "\"log.txt\"", ")", "\n", "filehandler", "=", "logging", ".", "FileHandler", "(", "filename", "=", "logfile", ")", "\n", "logging", ".", "basicConfig", "(", "\n", "format", "=", "\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\"", ",", "\n", "datefmt", "=", "\"%m/%d/%Y %H:%M:%S\"", ",", "\n", "handlers", "=", "[", "logging", ".", "StreamHandler", "(", "sys", ".", "stdout", ")", ",", "filehandler", "]", "\n", ")", "\n", "\n", "log_level", "=", "training_args", ".", "get_process_log_level", "(", ")", "\n", "logger", ".", "setLevel", "(", "log_level", ")", "\n", "datasets", ".", "utils", ".", "logging", ".", "set_verbosity", "(", "log_level", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "add_handler", "(", "filehandler", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "set_verbosity", "(", "log_level", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "enable_default_handler", "(", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "enable_explicit_format", "(", ")", "\n", "\n", "# Log on each process the small summary:", "\n", "logger", ".", "warning", "(", "\n", "f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"", "\n", "+", "f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"", "\n", ")", "\n", "logger", ".", "info", "(", "f\"Training/evaluation parameters {training_args}\"", ")", "\n", "\n", "# Detecting last checkpoint.", "\n", "last_checkpoint", "=", "None", "\n", "if", "os", ".", "path", ".", "isdir", "(", "training_args", ".", "output_dir", ")", "and", "training_args", ".", "do_train", "and", "not", "training_args", ".", "overwrite_output_dir", ":", "\n", "        ", "last_checkpoint", "=", "get_last_checkpoint", "(", "training_args", ".", "output_dir", ")", "\n", "if", "last_checkpoint", "is", "None", "and", "len", "(", "os", ".", "listdir", "(", "training_args", ".", "output_dir", ")", ")", ">", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"", "\n", "\"Use --overwrite_output_dir to overcome.\"", "\n", ")", "\n", "", "elif", "last_checkpoint", "is", "not", "None", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"", "\n", "\"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"", "\n", ")", "\n", "\n", "# Set seed before initializing model.", "\n", "", "", "set_seed", "(", "training_args", ".", "seed", ")", "\n", "\n", "# In distributed training, the load_dataset function guarantees that only one local process can concurrently", "\n", "# download the dataset.", "\n", "# Downloading and loading prepared ms-marco dataset from disk.", "\n", "if", "training_args", ".", "do_train", ":", "\n", "        ", "train_dataset", "=", "load_dataset", "(", "\"json\"", ",", "data_files", "=", "data_args", ".", "train_file", ",", "cache_dir", "=", "model_args", ".", "cache_dir", ")", "[", "'train'", "]", "\n", "# label_list = train_dataset.features[\"label\"].names", "\n", "\n", "", "if", "training_args", ".", "do_eval", ":", "\n", "        ", "eval_dataset", "=", "load_dataset", "(", "\"json\"", ",", "data_files", "=", "data_args", ".", "validation_file", ",", "cache_dir", "=", "model_args", ".", "cache_dir", ")", "[", "'train'", "]", "\n", "# label_list = eval_dataset.features[\"label\"].names", "\n", "\n", "", "if", "training_args", ".", "do_predict", ":", "\n", "        ", "predict_dataset", "=", "load_dataset", "(", "\"json\"", ",", "data_files", "=", "data_args", ".", "test_file", ",", "cache_dir", "=", "model_args", ".", "cache_dir", ")", "[", "'train'", "]", "\n", "# label_list = predict_dataset.features[\"label\"].names", "\n", "\n", "# Labels", "\n", "", "num_labels", "=", "1", "\n", "max_length", "=", "data_args", ".", "max_seq_length", "# 512", "\n", "\n", "# Load pretrained model and tokenizer", "\n", "# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently", "\n", "# download model & vocab.", "\n", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "model_args", ".", "model_name_or_path", ")", "\n", "config", ".", "num_labels", "=", "num_labels", "\n", "model", "=", "AutoModelForSequenceClassification", ".", "from_pretrained", "(", "model_args", ".", "model_name_or_path", ",", "config", "=", "config", ")", "\n", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "model_args", ".", "model_name_or_path", ")", "\n", "\n", "# Setup adapters", "\n", "if", "adapter_args", ".", "train_adapter", ":", "\n", "        ", "task_name", "=", "\"retrieval\"", "\n", "# check if adapter already exists, otherwise add it", "\n", "if", "task_name", "not", "in", "model", ".", "config", ".", "adapters", ":", "\n", "# resolve the adapter config", "\n", "            ", "adapter_config", "=", "AdapterConfig", ".", "load", "(", "\n", "adapter_args", ".", "adapter_config", ",", "\n", "non_linearity", "=", "adapter_args", ".", "adapter_non_linearity", ",", "\n", "reduction_factor", "=", "adapter_args", ".", "adapter_reduction_factor", ",", "\n", ")", "\n", "# load a pre-trained from Hub if specified", "\n", "if", "adapter_args", ".", "load_adapter", ":", "\n", "                ", "model", ".", "load_adapter", "(", "\n", "adapter_args", ".", "load_adapter", ",", "\n", "config", "=", "adapter_config", ",", "\n", "load_as", "=", "task_name", ",", "\n", ")", "\n", "# otherwise, add a fresh adapter", "\n", "", "else", ":", "\n", "                ", "model", ".", "add_adapter", "(", "task_name", ",", "config", "=", "adapter_config", ")", "\n", "# optionally load a pre-trained language adapter", "\n", "", "", "if", "adapter_args", ".", "load_lang_adapter", ":", "\n", "# resolve the language adapter config", "\n", "            ", "lang_adapter_config", "=", "AdapterConfig", ".", "load", "(", "\n", "adapter_args", ".", "lang_adapter_config", ",", "\n", "non_linearity", "=", "adapter_args", ".", "lang_adapter_non_linearity", ",", "\n", "reduction_factor", "=", "adapter_args", ".", "lang_adapter_reduction_factor", ",", "\n", ")", "\n", "# load the language adapter from Hub", "\n", "lang_adapter_name", "=", "model", ".", "load_adapter", "(", "\n", "adapter_args", ".", "load_lang_adapter", ",", "\n", "config", "=", "lang_adapter_config", ",", "\n", "load_as", "=", "adapter_args", ".", "language", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "lang_adapter_name", "=", "None", "\n", "# Freeze all model weights except of those of this adapter", "\n", "", "model", ".", "train_adapter", "(", "[", "task_name", "]", ")", "\n", "# Set the adapters to be used in every forward pass", "\n", "if", "lang_adapter_name", ":", "\n", "            ", "model", ".", "set_active_adapters", "(", "ac", ".", "Stack", "(", "lang_adapter_name", ",", "task_name", ")", ")", "\n", "", "else", ":", "\n", "            ", "model", ".", "set_active_adapters", "(", "[", "task_name", "]", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "adapter_args", ".", "load_adapter", "or", "adapter_args", ".", "load_lang_adapter", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Adapters can only be loaded in adapters training mode.\"", "\n", "\"Use --train_adapter to enable adapter training\"", "\n", ")", "\n", "\n", "# Preprocessing the datasets", "\n", "# Padding strategy", "\n", "# We will pad later, dynamically at batch creation, to the max sequence length in each batch", "\n", "", "", "data_args", ".", "pad_to_max_length", "=", "False", "\n", "max_length", "=", "min", "(", "tokenizer", ".", "max_len_sentences_pair", ",", "data_args", ".", "max_seq_length", ")", "\n", "\n", "def", "data_collator", "(", "examples", ")", ":", "\n", "        ", "questions", "=", "[", "e", "[", "'query'", "]", ".", "strip", "(", ")", "for", "e", "in", "examples", "]", "\n", "passages", "=", "[", "e", "[", "'passage'", "]", ".", "strip", "(", ")", "for", "e", "in", "examples", "]", "\n", "tokenized_examples", "=", "tokenizer", "(", "*", "[", "questions", ",", "passages", "]", ",", "padding", "=", "True", ",", "return_tensors", "=", "\"pt\"", ",", "truncation", "=", "'longest_first'", ",", "max_length", "=", "max_length", ")", "\n", "tokenized_examples", "[", "'labels'", "]", "=", "torch", ".", "tensor", "(", "[", "e", "[", "'label'", "]", "for", "e", "in", "examples", "]", ",", "dtype", "=", "torch", ".", "float", "if", "config", ".", "num_labels", "==", "1", "else", "torch", ".", "long", ")", "\n", "tokenized_examples", "[", "'labels'", "]", "=", "torch", ".", "unsqueeze", "(", "tokenized_examples", "[", "'labels'", "]", ",", "1", ")", "\n", "return", "tokenized_examples", "\n", "\n", "", "preprocessing_num_workers", "=", "multiprocessing", ".", "cpu_count", "(", ")", "\n", "data_args", ".", "preprocessing_num_workers", "=", "preprocessing_num_workers", "\n", "\n", "if", "training_args", ".", "do_train", ":", "\n", "        ", "if", "data_args", ".", "max_train_samples", "is", "not", "None", ":", "\n", "            ", "train_dataset", "=", "train_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_train_samples", ")", ")", "\n", "# Log a few random samples from the training set:", "\n", "", "for", "index", "in", "random", ".", "sample", "(", "range", "(", "len", "(", "train_dataset", ")", ")", ",", "3", ")", ":", "\n", "            ", "logger", ".", "info", "(", "f\"Sample {index} of the training set: {train_dataset[index]}.\"", ")", "\n", "\n", "", "", "if", "training_args", ".", "do_eval", ":", "\n", "        ", "if", "data_args", ".", "max_eval_samples", "is", "not", "None", ":", "\n", "            ", "eval_dataset", "=", "eval_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_eval_samples", ")", ")", "\n", "\n", "", "", "if", "training_args", ".", "do_predict", ":", "\n", "        ", "if", "data_args", ".", "max_predict_samples", "is", "not", "None", ":", "\n", "            ", "predict_dataset", "=", "predict_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_predict_samples", ")", ")", "\n", "\n", "\n", "# Taken from Reimers et al. (CrossEncoder.py), num_train_steps = 625000, batch_size = 32", "\n", "", "", "num_train_steps", "=", "len", "(", "train_dataset", ")", "//", "training_args", ".", "train_batch_size", "\n", "weight_decay", "=", "0.01", "\n", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "weight_decay", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "optimizer", "=", "AdamW", "(", "optimizer_grouped_parameters", ",", "**", "{", "'lr'", ":", "training_args", ".", "learning_rate", "}", ")", "\n", "scheduler", "=", "transformers", ".", "get_linear_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", "=", "training_args", ".", "warmup_steps", ",", "num_training_steps", "=", "num_train_steps", ")", "\n", "\n", "# Initialize our Trainer", "\n", "trainer_class", "=", "AdapterTrainer", "if", "adapter_args", ".", "train_adapter", "else", "Trainer", "\n", "\n", "# needed because our data_collator needs access to those columns", "\n", "training_args", ".", "remove_unused_columns", "=", "False", "\n", "\n", "# this hack is necessary to trigger BCEWithLogitsLoss() internally, ", "\n", "# otherwise scalar outputs are treated as regression and evaluated with MSE", "\n", "model", ".", "config", ".", "problem_type", "=", "\"multi_label_classification\"", "\n", "model", "=", "model", ".", "to", "(", "'cuda'", ")", "\n", "\n", "trainer", "=", "trainer_class", "(", "\n", "model", "=", "model", ",", "\n", "args", "=", "training_args", ",", "\n", "train_dataset", "=", "train_dataset", "if", "training_args", ".", "do_train", "else", "None", ",", "\n", "eval_dataset", "=", "eval_dataset", "if", "training_args", ".", "do_eval", "else", "None", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "data_collator", "=", "data_collator", ",", "\n", "optimizers", "=", "(", "optimizer", ",", "scheduler", ")", "\n", ")", "\n", "\n", "# Training", "\n", "if", "training_args", ".", "do_train", ":", "\n", "        ", "checkpoint", "=", "None", "\n", "if", "training_args", ".", "resume_from_checkpoint", "is", "not", "None", ":", "\n", "            ", "checkpoint", "=", "training_args", ".", "resume_from_checkpoint", "\n", "", "elif", "last_checkpoint", "is", "not", "None", ":", "\n", "            ", "checkpoint", "=", "last_checkpoint", "\n", "", "train_result", "=", "trainer", ".", "train", "(", "resume_from_checkpoint", "=", "checkpoint", ")", "\n", "metrics", "=", "train_result", ".", "metrics", "\n", "max_train_samples", "=", "(", "\n", "data_args", ".", "max_train_samples", "if", "data_args", ".", "max_train_samples", "is", "not", "None", "else", "len", "(", "train_dataset", ")", "\n", ")", "\n", "metrics", "[", "\"train_samples\"", "]", "=", "min", "(", "max_train_samples", ",", "len", "(", "train_dataset", ")", ")", "\n", "\n", "trainer", ".", "save_model", "(", ")", "# Saves the tokenizer too for easy upload", "\n", "\n", "trainer", ".", "log_metrics", "(", "\"train\"", ",", "metrics", ")", "\n", "trainer", ".", "save_metrics", "(", "\"train\"", ",", "metrics", ")", "\n", "trainer", ".", "save_state", "(", ")", "\n", "\n", "# Evaluation", "\n", "", "if", "training_args", ".", "do_eval", ":", "\n", "        ", "logger", ".", "info", "(", "\"*** Evaluate ***\"", ")", "\n", "metrics", "=", "trainer", ".", "evaluate", "(", "eval_dataset", "=", "eval_dataset", ")", "\n", "\n", "max_eval_samples", "=", "data_args", ".", "max_eval_samples", "if", "data_args", ".", "max_eval_samples", "is", "not", "None", "else", "len", "(", "eval_dataset", ")", "\n", "metrics", "[", "\"eval_samples\"", "]", "=", "min", "(", "max_eval_samples", ",", "len", "(", "eval_dataset", ")", ")", "\n", "\n", "trainer", ".", "log_metrics", "(", "\"eval\"", ",", "metrics", ")", "\n", "trainer", ".", "save_metrics", "(", "\"eval\"", ",", "metrics", ")", "\n", "\n", "# Prediction", "\n", "", "if", "training_args", ".", "do_predict", ":", "\n", "# TODO: implement scoring", "\n", "# cf. https://github.com/huggingface/transformers/blob/master/examples/pytorch/token-classification/run_ner.py", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.src.monobert_eval._evaluate_single_baseline": [[30, 87], ["helper.evaluate.logger.info", "helper.evaluate.logger.info", "sentence_transformers.CrossEncoder", "helper.evaluate.logger.info", "helper.evaluate.print_results", "helper.evaluate.add_filehandler", "helper.evaluate.logger.setLevel", "map_results.append", "duration_results.append", "os.path.join", "helper.evaluate.rerank_and_eval", "helper.evaluate.map2str", "str"], "function", ["home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.print_results", "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.add_filehandler", "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.rerank_and_eval", "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.map2str"], ["", "def", "_evaluate_single_baseline", "(", "own_model", ",", "mode", ")", ":", "\n", "  ", "if", "args", ".", "path_logging", ":", "\n", "    ", "add_filehandler", "(", "\"evaluate_baseline.txt\"", ")", "\n", "logger", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "", "logger", ".", "info", "(", "f\"Save average precision values to {REF_PRECISION_VALS_DIR}\"", ")", "\n", "\n", "if", "own_model", ":", "\n", "    ", "model_name_path", "=", "BASELINE_DIR", "\n", "", "else", ":", "\n", "    ", "model_name_path", "=", "'amberoad/bert-multilingual-passage-reranking-msmarco'", "\n", "\n", "", "logger", ".", "info", "(", "model_name_path", ")", "\n", "reranker", "=", "CrossEncoder", "(", "model_name_path", ",", "max_length", "=", "512", ")", "\n", "\n", "if", "mode", "==", "\"mono\"", ":", "\n", "    ", "prerankers", "=", "[", "\"bm25\"", "]", "# \"unigram\", \"fasttext\"", "\n", "lang_pairs", "=", "monolingual_lang_pairs", "\n", "\n", "", "elif", "mode", "==", "\"lowres\"", ":", "\n", "    ", "prerankers", "=", "[", "\"fbnmt+bm25\"", "]", "# \"marianmt+bm25\"", "\n", "lang_pairs", "=", "low_res_lang_pairs", "\n", "\n", "", "else", ":", "\n", "    ", "assert", "mode", "==", "\"clir\"", "\n", "prerankers", "=", "[", "\"distil_mbert\"", "]", "# \"procb\", \"distil_xlmr\", \"distil_muse\", \"muse\", \"labse\", \"laser\"", "\n", "lang_pairs", "=", "crosslingual_lang_pairs", "\n", "\n", "", "header", "=", "\"\\t\"", ".", "join", "(", "[", "l1", "+", "l2", "for", "l1", ",", "l2", "in", "lang_pairs", "]", ")", "\n", "map_results", "=", "[", "header", "]", "\n", "duration_results", "=", "[", "header", "]", "\n", "\n", "for", "preranker_str", "in", "prerankers", ":", "\n", "    ", "preranker_model", "=", "preranker_str", "\n", "\n", "langpair2map", "=", "{", "}", "\n", "langpair2duration", "=", "{", "}", "\n", "for", "qlang", ",", "dlang", "in", "lang_pairs", ":", "\n", "      ", "save_precision_values_dir", "=", "os", ".", "path", ".", "join", "(", "REF_PRECISION_VALS_DIR", ",", "f\"{qlang}-{dlang}_preranker={preranker_str}\"", ")", "\n", "eval_result", "=", "rerank_and_eval", "(", "\n", "qlang", "=", "qlang", ",", "\n", "dlang", "=", "dlang", ",", "\n", "reranker", "=", "reranker", ",", "\n", "preranker", "=", "preranker_model", ",", "\n", "prerank_dir", "=", "args", ".", "prerank_dir", ",", "\n", "save_precision_values_dir", "=", "save_precision_values_dir", ",", "\n", "path_query_translations", "=", "args", ".", "path_query_translations", "\n", ")", "\n", "langpair2map", "[", "qlang", "+", "dlang", "]", "=", "map2str", "(", "eval_map", "=", "eval_result", "[", "\"MAP\"", "]", ",", "pvalue", "=", "eval_result", "[", "\"pvalue\"", "]", ")", "\n", "langpair2duration", "[", "qlang", "+", "dlang", "]", "=", "str", "(", "eval_result", "[", "\"duration_ms\"", "]", ")", "\n", "\n", "", "map_results", ".", "append", "(", "\"\\t\"", ".", "join", "(", "[", "preranker_str", "]", "+", "[", "langpair2map", "[", "q", "+", "d", "]", "for", "q", ",", "d", "in", "lang_pairs", "]", ")", ")", "\n", "duration_results", ".", "append", "(", "\"\\t\"", ".", "join", "(", "[", "preranker_str", "]", "+", "[", "langpair2duration", "[", "q", "+", "d", "]", "for", "q", ",", "d", "in", "lang_pairs", "]", ")", ")", "\n", "\n", "", "logger", ".", "info", "(", "model_name_path", ")", "\n", "print_results", "(", "durations", "=", "duration_results", ",", "map_results", "=", "map_results", ")", "\n", "\n", "return", "map_results", ",", "duration_results", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.src.monobert_eval.evaluate_baselines": [[89, 97], ["helper.evaluate.print_results", "monobert_eval._evaluate_single_baseline", "all_results.extend", "all_durations.extend"], "function", ["home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.print_results", "home.repos.pwc.inspect_result.rlitschk_modularclir.src.monobert_eval._evaluate_single_baseline"], ["", "def", "evaluate_baselines", "(", ")", ":", "\n", "  ", "all_results", "=", "[", "]", "\n", "all_durations", "=", "[", "]", "\n", "for", "mode", "in", "[", "args", ".", "mode", "]", ":", "\n", "    ", "map_results", ",", "duration_results", "=", "_evaluate_single_baseline", "(", "own_model", "=", "True", ",", "mode", "=", "mode", ")", "\n", "all_results", ".", "extend", "(", "[", "mode", "+", "\"\\t\"", "+", "line", "for", "line", "in", "map_results", "[", "1", ":", "]", "]", ")", "\n", "all_durations", ".", "extend", "(", "[", "mode", "+", "\"\\t\"", "+", "line", "for", "line", "in", "duration_results", "[", "1", ":", "]", "]", ")", "\n", "", "print_results", "(", "durations", "=", "all_durations", ",", "map_results", "=", "all_results", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.src.monobert_eval.main": [[99, 101], ["monobert_eval.evaluate_baselines"], "function", ["home.repos.pwc.inspect_result.rlitschk_modularclir.src.monobert_eval.evaluate_baselines"], ["", "def", "main", "(", ")", ":", "\n", "  ", "evaluate_baselines", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.src.adapter_mlm.ModelArguments.__post_init__": [[121, 125], ["ValueError"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "config_overrides", "is", "not", "None", "and", "(", "self", ".", "config_name", "is", "not", "None", "or", "self", ".", "model_name_or_path", "is", "not", "None", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"--config_overrides can't be used in combination with --config_name or --model_name_or_path\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.src.adapter_mlm.DataTrainingArguments.__post_init__": [[194, 204], ["ValueError", "adapter_mlm.DataTrainingArguments.train_file.split", "adapter_mlm.DataTrainingArguments.validation_file.split"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "dataset_name", "is", "None", "and", "self", ".", "train_file", "is", "None", "and", "self", ".", "validation_file", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Need either a dataset name or a training/validation file.\"", ")", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "train_file", "is", "not", "None", ":", "\n", "                ", "extension", "=", "self", ".", "train_file", ".", "split", "(", "\".\"", ")", "[", "-", "1", "]", "\n", "assert", "extension", "in", "[", "\"csv\"", ",", "\"json\"", ",", "\"txt\"", "]", ",", "\"`train_file` should be a csv, a json or a txt file.\"", "\n", "", "if", "self", ".", "validation_file", "is", "not", "None", ":", "\n", "                ", "extension", "=", "self", ".", "validation_file", ".", "split", "(", "\".\"", ")", "[", "-", "1", "]", "\n", "assert", "extension", "in", "[", "\"csv\"", ",", "\"json\"", ",", "\"txt\"", "]", ",", "\"`validation_file` should be a csv, a json or a txt file.\"", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.src.adapter_mlm.main": [[206, 651], ["print", "input", "print", "transformers.HfArgumentParser", "multiprocessing.cpu_count", "logger.info", "os.makedirs", "os.path.join", "logging.FileHandler", "logging.basicConfig", "training_args.get_process_log_level", "logger.setLevel", "datasets.utils.logging.set_verbosity", "transformers.utils.logging.add_handler", "transformers.utils.logging.add_handler", "transformers.utils.logging.set_verbosity", "transformers.utils.logging.set_verbosity", "transformers.utils.logging.enable_default_handler", "transformers.utils.logging.enable_default_handler", "transformers.utils.logging.enable_explicit_format", "transformers.utils.logging.enable_explicit_format", "logger.warning", "logger.info", "transformers.set_seed", "AutoModelForMaskedLM.from_config.resize_token_embeddings", "transformers.DataCollatorForLanguageModeling", "trainer_class", "sys.argv[].endswith", "transformers.HfArgumentParser.parse_json_file", "transformers.HfArgumentParser.parse_args_into_dataclasses", "os.path.isdir", "transformers.trainer_utils.get_last_checkpoint", "datasets.load_dataset", "transformers.AutoConfig.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoModelForMaskedLM.from_pretrained", "logger.info", "transformers.AutoModelForMaskedLM.from_config", "len", "AutoModelForMaskedLM.from_config.train_adapter", "min", "trainer_class.train", "trainer_class.save_model", "min", "trainer_class.log_metrics", "trainer_class.save_metrics", "trainer_class.save_state", "logger.info", "trainer_class.evaluate", "min", "trainer_class.log_metrics", "trainer_class.save_metrics", "trainer_class.push_to_hub", "trainer_class.create_model_card", "len", "ValueError", "data_args.dataset_config_name.split", "str", "os.makedirs", "logger.info", "datasets.load_dataset", "datasets.load_dataset", "datasets.load_dataset.keys", "datasets.load_dataset", "datasets.load_dataset", "transformers.AutoConfig.from_pretrained", "logger.warning", "transformers.AutoTokenizer.from_pretrained", "ValueError", "logger.info", "transformers.adapters.configuration.AdapterConfig.load", "transformers.adapters.configuration.AdapterConfig.load", "AutoModelForMaskedLM.from_config.load_adapter", "AutoModelForMaskedLM.from_config.set_active_adapters", "AutoModelForMaskedLM.from_config.set_active_adapters", "ValueError", "logger.warning", "logger.warning", "AutoTokenizer.from_pretrained.", "training_args.main_process_first", "datasets.load_dataset.map", "AutoTokenizer.from_pretrained.", "training_args.main_process_first", "datasets.load_dataset.map", "len", "training_args.main_process_first", "tokenized_datasets.map.map", "ValueError", "train_dataset.select.select", "ValueError", "eval_dataset.select.select", "len", "len", "len", "len", "math.exp", "os.path.abspath", "logging.StreamHandler", "len", "logger.info", "datasets.load_dataset.keys", "datasets.load_dataset", "datasets.load_dataset.keys", "datasets.load_dataset", "data_args.train_file.split", "data_args.validation_file.split", "logger.info", "AutoConfig.from_pretrained.update_from_string", "bool", "AutoModelForMaskedLM.from_config.load_adapter", "AutoModelForMaskedLM.from_config.add_adapter", "transformers.Stack", "sum", "range", "range", "float", "bool", "os.listdir", "datasets.load_dataset", "examples.keys", "concatenated_examples.items", "range", "pathlib.Path.home", "len", "line.isspace", "list", "examples.keys"], "function", ["None"], ["", "", "", "", "def", "main", "(", ")", ":", "\n", "# See all possible arguments in src/transformers/training_args.py", "\n", "# or by passing the --help flag to this script.", "\n", "# We now keep distinct sets of args, for a cleaner separation of concerns.", "\n", "\n", "    ", "print", "(", "\"Select gpu device (CUDA_VISIBLE_DEVICES)\"", ")", "\n", "gpudevice", "=", "input", "(", "\"Specify GPU: \"", ")", "\n", "os", ".", "environ", "[", "\"CUDA_VISIBLE_DEVICES\"", "]", "=", "gpudevice", "\n", "print", "(", "f\"gpu device: {gpudevice}\"", ")", "\n", "\n", "parser", "=", "HfArgumentParser", "(", "(", "ModelArguments", ",", "DataTrainingArguments", ",", "TrainingArguments", ",", "MultiLingAdapterArguments", ")", ")", "\n", "if", "len", "(", "sys", ".", "argv", ")", "==", "2", "and", "sys", ".", "argv", "[", "1", "]", ".", "endswith", "(", "\".json\"", ")", ":", "\n", "# If we pass only one argument to the script and it's the path to a json file,", "\n", "# let's parse it to get our arguments.", "\n", "        ", "model_args", ",", "data_args", ",", "training_args", ",", "adapter_args", "=", "parser", ".", "parse_json_file", "(", "\n", "json_file", "=", "os", ".", "path", ".", "abspath", "(", "sys", ".", "argv", "[", "1", "]", ")", "\n", ")", "\n", "", "else", ":", "\n", "        ", "model_args", ",", "data_args", ",", "training_args", ",", "adapter_args", "=", "parser", ".", "parse_args_into_dataclasses", "(", ")", "\n", "\n", "", "preprocessing_num_workers", "=", "multiprocessing", ".", "cpu_count", "(", ")", "\n", "data_args", ".", "preprocessing_num_workers", "=", "preprocessing_num_workers", "\n", "logger", ".", "info", "(", "f\"num pre-processing CPUs:{preprocessing_num_workers}\"", ")", "\n", "\n", "\n", "# Setup logging", "\n", "os", ".", "makedirs", "(", "training_args", ".", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "logfile", "=", "os", ".", "path", ".", "join", "(", "training_args", ".", "output_dir", ",", "\"log.txt\"", ")", "\n", "filehandler", "=", "logging", ".", "FileHandler", "(", "filename", "=", "logfile", ")", "\n", "logging", ".", "basicConfig", "(", "\n", "format", "=", "\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\"", ",", "\n", "datefmt", "=", "\"%m/%d/%Y %H:%M:%S\"", ",", "\n", "handlers", "=", "[", "logging", ".", "StreamHandler", "(", "sys", ".", "stdout", ")", ",", "filehandler", "]", ",", "\n", ")", "\n", "\n", "log_level", "=", "training_args", ".", "get_process_log_level", "(", ")", "\n", "logger", ".", "setLevel", "(", "log_level", ")", "\n", "datasets", ".", "utils", ".", "logging", ".", "set_verbosity", "(", "log_level", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "add_handler", "(", "filehandler", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "set_verbosity", "(", "log_level", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "enable_default_handler", "(", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "enable_explicit_format", "(", ")", "\n", "\n", "# Log on each process the small summary:", "\n", "logger", ".", "warning", "(", "\n", "f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"", "\n", "+", "f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"", "\n", ")", "\n", "# Set the verbosity to info of the Transformers logger (on main process only):", "\n", "logger", ".", "info", "(", "f\"Training/evaluation parameters {training_args}\"", ")", "\n", "\n", "# Detecting last checkpoint.", "\n", "last_checkpoint", "=", "None", "\n", "if", "os", ".", "path", ".", "isdir", "(", "training_args", ".", "output_dir", ")", "and", "training_args", ".", "do_train", "and", "not", "training_args", ".", "overwrite_output_dir", ":", "\n", "        ", "last_checkpoint", "=", "get_last_checkpoint", "(", "training_args", ".", "output_dir", ")", "\n", "if", "last_checkpoint", "is", "None", "and", "len", "(", "os", ".", "listdir", "(", "training_args", ".", "output_dir", ")", ")", ">", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"", "\n", "\"Use --overwrite_output_dir to overcome.\"", "\n", ")", "\n", "", "elif", "last_checkpoint", "is", "not", "None", "and", "training_args", ".", "resume_from_checkpoint", "is", "None", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"", "\n", "\"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"", "\n", ")", "\n", "\n", "# Set seed before initializing model.", "\n", "", "", "set_seed", "(", "training_args", ".", "seed", ")", "\n", "\n", "# Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)", "\n", "# or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/", "\n", "# (the dataset will be downloaded automatically from the datasets Hub", "\n", "#", "\n", "# For CSV/JSON files, this script will use the column called 'text' or the first column. You can easily tweak this", "\n", "# behavior (see below)", "\n", "#", "\n", "# In distributed training, the load_dataset function guarantee that only one local process can concurrently", "\n", "# download the dataset.", "\n", "if", "data_args", ".", "dataset_name", "is", "not", "None", ":", "\n", "        ", "lang", "=", "data_args", ".", "dataset_config_name", ".", "split", "(", "\".\"", ")", "[", "1", "]", "\n", "if", "\"ru\"", "in", "data_args", ".", "dataset_config_name", "or", "\"fi\"", "in", "data_args", ".", "dataset_config_name", "or", "\"sw\"", "in", "data_args", ".", "dataset_config_name", "or", "\"so\"", "in", "data_args", ".", "dataset_config_name", ":", "\n", "            ", "cache_dir", "=", "str", "(", "Path", ".", "home", "(", ")", "/", "'.cache'", "/", "'huggingface_wikipedia'", "/", "lang", ")", "\n", "os", ".", "makedirs", "(", "cache_dir", ",", "exist_ok", "=", "True", ")", "\n", "logger", ".", "info", "(", "f\"Loading local dataset (beam_runner='DirectRunner', date='20220120'), cache_dir {cache_dir}\"", ")", "\n", "raw_datasets", "=", "load_dataset", "(", "\n", "'wikipedia'", ",", "\n", "f'20200501.{lang}'", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", "beam_runner", "=", "'DirectRunner'", ",", "\n", "date", "=", "\"20220120\"", ",", "\n", ")", "\n", "if", "\"validation\"", "not", "in", "raw_datasets", ".", "keys", "(", ")", ":", "\n", "                ", "raw_datasets", "[", "\"validation\"", "]", "=", "load_dataset", "(", "\n", "'wikipedia'", ",", "\n", "f'20200501.{lang}'", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", "split", "=", "f\"train[:{data_args.validation_split_percentage}%]\"", ",", "\n", "beam_runner", "=", "'DirectRunner'", ",", "\n", "date", "=", "\"20220120\"", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "# Downloading and loading a dataset from the hub.", "\n", "            ", "raw_datasets", "=", "load_dataset", "(", "\n", "data_args", ".", "dataset_name", ",", "data_args", ".", "dataset_config_name", ",", "cache_dir", "=", "model_args", ".", "cache_dir", "\n", ")", "\n", "if", "\"validation\"", "not", "in", "raw_datasets", ".", "keys", "(", ")", ":", "\n", "                ", "raw_datasets", "[", "\"validation\"", "]", "=", "load_dataset", "(", "\n", "data_args", ".", "dataset_name", ",", "\n", "data_args", ".", "dataset_config_name", ",", "\n", "split", "=", "f\"train[:{data_args.validation_split_percentage}%]\"", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", ")", "\n", "if", "training_args", ".", "do_train", ":", "\n", "                    ", "raw_datasets", "[", "\"train\"", "]", "=", "load_dataset", "(", "\n", "data_args", ".", "dataset_name", ",", "\n", "data_args", ".", "dataset_config_name", ",", "\n", "split", "=", "f\"train[{data_args.validation_split_percentage}%:]\"", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", ")", "\n", "", "else", ":", "\n", "# avoid unnecessary compute if we don't use train split", "\n", "                    ", "del", "raw_datasets", "[", "\"train\"", "]", "\n", "", "", "", "", "else", ":", "\n", "        ", "data_files", "=", "{", "}", "\n", "if", "data_args", ".", "train_file", "is", "not", "None", ":", "\n", "            ", "data_files", "[", "\"train\"", "]", "=", "data_args", ".", "train_file", "\n", "extension", "=", "data_args", ".", "train_file", ".", "split", "(", "\".\"", ")", "[", "-", "1", "]", "\n", "", "if", "data_args", ".", "validation_file", "is", "not", "None", ":", "\n", "            ", "data_files", "[", "\"validation\"", "]", "=", "data_args", ".", "validation_file", "\n", "extension", "=", "data_args", ".", "validation_file", ".", "split", "(", "\".\"", ")", "[", "-", "1", "]", "\n", "", "if", "extension", "==", "\"txt\"", ":", "\n", "            ", "extension", "=", "\"text\"", "\n", "", "raw_datasets", "=", "load_dataset", "(", "extension", ",", "data_files", "=", "data_files", ",", "cache_dir", "=", "model_args", ".", "cache_dir", ")", "\n", "\n", "# If no validation data is there, validation_split_percentage will be used to divide the dataset.", "\n", "if", "\"validation\"", "not", "in", "raw_datasets", ".", "keys", "(", ")", ":", "\n", "            ", "raw_datasets", "[", "\"validation\"", "]", "=", "load_dataset", "(", "\n", "extension", ",", "\n", "data_files", "=", "data_files", ",", "\n", "split", "=", "f\"train[:{data_args.validation_split_percentage}%]\"", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", ")", "\n", "raw_datasets", "[", "\"train\"", "]", "=", "load_dataset", "(", "\n", "extension", ",", "\n", "data_files", "=", "data_files", ",", "\n", "split", "=", "f\"train[{data_args.validation_split_percentage}%:]\"", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", ")", "\n", "\n", "# See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at", "\n", "# https://huggingface.co/docs/datasets/loading_datasets.html.", "\n", "\n", "# Load pretrained model and tokenizer", "\n", "#", "\n", "# Distributed training:", "\n", "# The .from_pretrained methods guarantee that only one local process can concurrently", "\n", "# download model & vocab.", "\n", "", "", "config_kwargs", "=", "{", "\n", "\"cache_dir\"", ":", "model_args", ".", "cache_dir", ",", "\n", "\"revision\"", ":", "model_args", ".", "model_revision", ",", "\n", "\"use_auth_token\"", ":", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", "}", "\n", "if", "model_args", ".", "config_name", ":", "\n", "        ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "model_args", ".", "config_name", ",", "**", "config_kwargs", ")", "\n", "", "elif", "model_args", ".", "model_name_or_path", ":", "\n", "        ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "model_args", ".", "model_name_or_path", ",", "**", "config_kwargs", ")", "\n", "", "else", ":", "\n", "        ", "config", "=", "CONFIG_MAPPING", "[", "model_args", ".", "model_type", "]", "(", ")", "\n", "logger", ".", "warning", "(", "\"You are instantiating a new config instance from scratch.\"", ")", "\n", "if", "model_args", ".", "config_overrides", "is", "not", "None", ":", "\n", "            ", "logger", ".", "info", "(", "f\"Overriding config: {model_args.config_overrides}\"", ")", "\n", "config", ".", "update_from_string", "(", "model_args", ".", "config_overrides", ")", "\n", "\n", "", "", "tokenizer_kwargs", "=", "{", "\n", "\"cache_dir\"", ":", "model_args", ".", "cache_dir", ",", "\n", "\"use_fast\"", ":", "model_args", ".", "use_fast_tokenizer", ",", "\n", "\"revision\"", ":", "model_args", ".", "model_revision", ",", "\n", "\"use_auth_token\"", ":", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", "}", "\n", "if", "model_args", ".", "tokenizer_name", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "model_args", ".", "tokenizer_name", ",", "**", "tokenizer_kwargs", ")", "\n", "", "elif", "model_args", ".", "model_name_or_path", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "model_args", ".", "model_name_or_path", ",", "**", "tokenizer_kwargs", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"", "\n", "\"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"", "\n", ")", "\n", "\n", "", "if", "model_args", ".", "model_name_or_path", ":", "\n", "        ", "model", "=", "AutoModelForMaskedLM", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "\n", "from_tf", "=", "bool", "(", "\".ckpt\"", "in", "model_args", ".", "model_name_or_path", ")", ",", "\n", "config", "=", "config", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", "revision", "=", "model_args", ".", "model_revision", ",", "\n", "use_auth_token", "=", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "\"Training new model from scratch\"", ")", "\n", "model", "=", "AutoModelForMaskedLM", ".", "from_config", "(", "config", ")", "\n", "\n", "", "model", ".", "resize_token_embeddings", "(", "len", "(", "tokenizer", ")", ")", "\n", "\n", "# Setup adapters", "\n", "if", "adapter_args", ".", "train_adapter", ":", "\n", "        ", "adapter_name", "=", "\"mlm_adapter\"", "#data_args.dataset_name or \"mlm\"", "\n", "# check if adapter already exists, otherwise add it", "\n", "if", "adapter_name", "not", "in", "model", ".", "config", ".", "adapters", ":", "\n", "            ", "logger", ".", "info", "(", "f\"Loading task adapter: {adapter_args.adapter_config}\"", ")", "\n", "# resolve the adapter config", "\n", "adapter_config", "=", "AdapterConfig", ".", "load", "(", "\n", "adapter_args", ".", "adapter_config", ",", "\n", "non_linearity", "=", "adapter_args", ".", "adapter_non_linearity", ",", "\n", "reduction_factor", "=", "adapter_args", ".", "adapter_reduction_factor", ",", "\n", ")", "\n", "# load a pre-trained from Hub if specified", "\n", "if", "adapter_args", ".", "load_adapter", ":", "\n", "                ", "model", ".", "load_adapter", "(", "\n", "adapter_args", ".", "load_adapter", ",", "\n", "config", "=", "adapter_config", ",", "\n", "load_as", "=", "adapter_name", ",", "\n", ")", "\n", "# otherwise, add a fresh adapter", "\n", "", "else", ":", "\n", "                ", "model", ".", "add_adapter", "(", "adapter_name", ",", "config", "=", "adapter_config", ")", "\n", "# optionally load a pre-trained language adapter", "\n", "", "", "if", "adapter_args", ".", "load_lang_adapter", ":", "\n", "# resolve the language adapter config", "\n", "            ", "lang_adapter_config", "=", "AdapterConfig", ".", "load", "(", "\n", "adapter_args", ".", "lang_adapter_config", ",", "\n", "non_linearity", "=", "adapter_args", ".", "lang_adapter_non_linearity", ",", "\n", "reduction_factor", "=", "adapter_args", ".", "lang_adapter_reduction_factor", ",", "\n", ")", "\n", "# load the language adapter from Hub", "\n", "lang_adapter_name", "=", "model", ".", "load_adapter", "(", "\n", "adapter_args", ".", "load_lang_adapter", ",", "\n", "config", "=", "lang_adapter_config", ",", "\n", "load_as", "=", "adapter_args", ".", "language", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "lang_adapter_name", "=", "None", "\n", "# Freeze all model weights except of those of this adapter", "\n", "", "model", ".", "train_adapter", "(", "[", "adapter_name", "]", ")", "\n", "# Set the adapters to be used in every forward pass", "\n", "if", "lang_adapter_name", ":", "\n", "            ", "model", ".", "set_active_adapters", "(", "ac", ".", "Stack", "(", "lang_adapter_name", ",", "adapter_name", ")", ")", "\n", "", "else", ":", "\n", "            ", "model", ".", "set_active_adapters", "(", "adapter_name", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "adapter_args", ".", "load_adapter", "or", "adapter_args", ".", "load_lang_adapter", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Adapters can only be loaded in adapters training mode.\"", "\n", "\"Use --train_adapter to enable adapter training\"", "\n", ")", "\n", "\n", "# Preprocessing the datasets.", "\n", "# First we tokenize all the texts.", "\n", "", "", "if", "training_args", ".", "do_train", ":", "\n", "        ", "column_names", "=", "raw_datasets", "[", "\"train\"", "]", ".", "column_names", "\n", "", "else", ":", "\n", "        ", "column_names", "=", "raw_datasets", "[", "\"validation\"", "]", ".", "column_names", "\n", "", "text_column_name", "=", "\"text\"", "if", "\"text\"", "in", "column_names", "else", "column_names", "[", "0", "]", "\n", "\n", "if", "data_args", ".", "max_seq_length", "is", "None", ":", "\n", "        ", "max_seq_length", "=", "tokenizer", ".", "model_max_length", "\n", "if", "max_seq_length", ">", "1024", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"", "\n", "\"Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.\"", "\n", ")", "\n", "max_seq_length", "=", "1024", "\n", "", "", "else", ":", "\n", "        ", "if", "data_args", ".", "max_seq_length", ">", "tokenizer", ".", "model_max_length", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the\"", "\n", "f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"", "\n", ")", "\n", "", "max_seq_length", "=", "min", "(", "data_args", ".", "max_seq_length", ",", "tokenizer", ".", "model_max_length", ")", "\n", "\n", "", "if", "data_args", ".", "line_by_line", ":", "\n", "# When using line_by_line, we just tokenize each nonempty line.", "\n", "        ", "padding", "=", "\"max_length\"", "if", "data_args", ".", "pad_to_max_length", "else", "False", "\n", "\n", "def", "tokenize_function", "(", "examples", ")", ":", "\n", "# Remove empty lines", "\n", "            ", "examples", "[", "text_column_name", "]", "=", "[", "\n", "line", "for", "line", "in", "examples", "[", "text_column_name", "]", "if", "len", "(", "line", ")", ">", "0", "and", "not", "line", ".", "isspace", "(", ")", "\n", "]", "\n", "return", "tokenizer", "(", "\n", "examples", "[", "text_column_name", "]", ",", "\n", "padding", "=", "padding", ",", "\n", "truncation", "=", "True", ",", "\n", "max_length", "=", "max_seq_length", ",", "\n", "# We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it", "\n", "# receives the `special_tokens_mask`.", "\n", "return_special_tokens_mask", "=", "True", ",", "\n", ")", "\n", "\n", "", "with", "training_args", ".", "main_process_first", "(", "desc", "=", "\"dataset map tokenization\"", ")", ":", "\n", "            ", "tokenized_datasets", "=", "raw_datasets", ".", "map", "(", "\n", "tokenize_function", ",", "\n", "batched", "=", "True", ",", "\n", "num_proc", "=", "data_args", ".", "preprocessing_num_workers", ",", "\n", "remove_columns", "=", "[", "text_column_name", "]", ",", "\n", "load_from_cache_file", "=", "not", "data_args", ".", "overwrite_cache", ",", "\n", "desc", "=", "\"Running tokenizer on dataset line_by_line\"", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "# Otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.", "\n", "# We use `return_special_tokens_mask=True` because DataCollatorForLanguageModeling (see below) is more", "\n", "# efficient when it receives the `special_tokens_mask`.", "\n", "        ", "def", "tokenize_function", "(", "examples", ")", ":", "\n", "            ", "return", "tokenizer", "(", "examples", "[", "text_column_name", "]", ",", "return_special_tokens_mask", "=", "True", ")", "\n", "\n", "", "with", "training_args", ".", "main_process_first", "(", "desc", "=", "\"dataset map tokenization\"", ")", ":", "\n", "            ", "tokenized_datasets", "=", "raw_datasets", ".", "map", "(", "\n", "tokenize_function", ",", "\n", "batched", "=", "True", ",", "\n", "num_proc", "=", "data_args", ".", "preprocessing_num_workers", ",", "\n", "remove_columns", "=", "column_names", ",", "\n", "load_from_cache_file", "=", "not", "data_args", ".", "overwrite_cache", ",", "\n", "desc", "=", "\"Running tokenizer on every text in dataset\"", ",", "\n", ")", "\n", "\n", "# Main data processing function that will concatenate all texts from our dataset and generate chunks of", "\n", "# max_seq_length.", "\n", "", "def", "group_texts", "(", "examples", ")", ":", "\n", "# Concatenate all texts.", "\n", "            ", "concatenated_examples", "=", "{", "k", ":", "sum", "(", "examples", "[", "k", "]", ",", "[", "]", ")", "for", "k", "in", "examples", ".", "keys", "(", ")", "}", "\n", "total_length", "=", "len", "(", "concatenated_examples", "[", "list", "(", "examples", ".", "keys", "(", ")", ")", "[", "0", "]", "]", ")", "\n", "# We drop the small remainder, we could add padding if the model supported it instead of this drop, you can", "\n", "# customize this part to your needs.", "\n", "if", "total_length", ">=", "max_seq_length", ":", "\n", "                ", "total_length", "=", "(", "total_length", "//", "max_seq_length", ")", "*", "max_seq_length", "\n", "# Split by chunks of max_len.", "\n", "", "result", "=", "{", "\n", "k", ":", "[", "t", "[", "i", ":", "i", "+", "max_seq_length", "]", "for", "i", "in", "range", "(", "0", ",", "total_length", ",", "max_seq_length", ")", "]", "\n", "for", "k", ",", "t", "in", "concatenated_examples", ".", "items", "(", ")", "\n", "}", "\n", "return", "result", "\n", "\n", "# Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a", "\n", "# remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value", "\n", "# might be slower to preprocess.", "\n", "#", "\n", "# To speed up this part, we use multiprocessing. See the documentation of the map method for more information:", "\n", "# https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map", "\n", "\n", "", "with", "training_args", ".", "main_process_first", "(", "desc", "=", "\"grouping texts together\"", ")", ":", "\n", "            ", "tokenized_datasets", "=", "tokenized_datasets", ".", "map", "(", "\n", "group_texts", ",", "\n", "batched", "=", "True", ",", "\n", "num_proc", "=", "data_args", ".", "preprocessing_num_workers", ",", "\n", "load_from_cache_file", "=", "not", "data_args", ".", "overwrite_cache", ",", "\n", "desc", "=", "f\"Grouping texts in chunks of {max_seq_length}\"", ",", "\n", ")", "\n", "\n", "", "", "if", "training_args", ".", "do_train", ":", "\n", "        ", "if", "\"train\"", "not", "in", "tokenized_datasets", ":", "\n", "            ", "raise", "ValueError", "(", "\"--do_train requires a train dataset\"", ")", "\n", "", "train_dataset", "=", "tokenized_datasets", "[", "\"train\"", "]", "\n", "if", "data_args", ".", "max_train_samples", "is", "not", "None", ":", "\n", "            ", "train_dataset", "=", "train_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_train_samples", ")", ")", "\n", "\n", "", "", "if", "training_args", ".", "do_eval", ":", "\n", "        ", "if", "\"validation\"", "not", "in", "tokenized_datasets", ":", "\n", "            ", "raise", "ValueError", "(", "\"--do_eval requires a validation dataset\"", ")", "\n", "", "eval_dataset", "=", "tokenized_datasets", "[", "\"validation\"", "]", "\n", "if", "data_args", ".", "max_eval_samples", "is", "not", "None", ":", "\n", "            ", "eval_dataset", "=", "eval_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_eval_samples", ")", ")", "\n", "\n", "# Data collator", "\n", "# This one will take care of randomly masking the tokens.", "\n", "", "", "pad_to_multiple_of_8", "=", "data_args", ".", "line_by_line", "and", "training_args", ".", "fp16", "and", "not", "data_args", ".", "pad_to_max_length", "\n", "data_collator", "=", "DataCollatorForLanguageModeling", "(", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "mlm_probability", "=", "data_args", ".", "mlm_probability", ",", "\n", "pad_to_multiple_of", "=", "8", "if", "pad_to_multiple_of_8", "else", "None", ",", "\n", ")", "\n", "\n", "# Initialize our Trainer", "\n", "trainer_class", "=", "AdapterTrainer", "if", "adapter_args", ".", "train_adapter", "else", "Trainer", "\n", "trainer", "=", "trainer_class", "(", "\n", "model", "=", "model", ",", "\n", "args", "=", "training_args", ",", "\n", "train_dataset", "=", "train_dataset", "if", "training_args", ".", "do_train", "else", "None", ",", "\n", "eval_dataset", "=", "eval_dataset", "if", "training_args", ".", "do_eval", "else", "None", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "data_collator", "=", "data_collator", ",", "\n", ")", "\n", "\n", "# Training", "\n", "if", "training_args", ".", "do_train", ":", "\n", "        ", "checkpoint", "=", "None", "\n", "if", "training_args", ".", "resume_from_checkpoint", "is", "not", "None", ":", "\n", "            ", "checkpoint", "=", "training_args", ".", "resume_from_checkpoint", "\n", "", "elif", "last_checkpoint", "is", "not", "None", ":", "\n", "            ", "checkpoint", "=", "last_checkpoint", "\n", "", "train_result", "=", "trainer", ".", "train", "(", "resume_from_checkpoint", "=", "checkpoint", ")", "\n", "trainer", ".", "save_model", "(", ")", "# Saves the tokenizer too for easy upload", "\n", "metrics", "=", "train_result", ".", "metrics", "\n", "\n", "max_train_samples", "=", "(", "\n", "data_args", ".", "max_train_samples", "if", "data_args", ".", "max_train_samples", "is", "not", "None", "else", "len", "(", "train_dataset", ")", "\n", ")", "\n", "metrics", "[", "\"train_samples\"", "]", "=", "min", "(", "max_train_samples", ",", "len", "(", "train_dataset", ")", ")", "\n", "\n", "trainer", ".", "log_metrics", "(", "\"train\"", ",", "metrics", ")", "\n", "trainer", ".", "save_metrics", "(", "\"train\"", ",", "metrics", ")", "\n", "trainer", ".", "save_state", "(", ")", "\n", "\n", "# Evaluation", "\n", "", "if", "training_args", ".", "do_eval", ":", "\n", "        ", "logger", ".", "info", "(", "\"*** Evaluate ***\"", ")", "\n", "\n", "metrics", "=", "trainer", ".", "evaluate", "(", ")", "\n", "\n", "max_eval_samples", "=", "data_args", ".", "max_eval_samples", "if", "data_args", ".", "max_eval_samples", "is", "not", "None", "else", "len", "(", "eval_dataset", ")", "\n", "metrics", "[", "\"eval_samples\"", "]", "=", "min", "(", "max_eval_samples", ",", "len", "(", "eval_dataset", ")", ")", "\n", "try", ":", "\n", "            ", "perplexity", "=", "math", ".", "exp", "(", "metrics", "[", "\"eval_loss\"", "]", ")", "\n", "", "except", "OverflowError", ":", "\n", "            ", "perplexity", "=", "float", "(", "\"inf\"", ")", "\n", "", "metrics", "[", "\"perplexity\"", "]", "=", "perplexity", "\n", "\n", "trainer", ".", "log_metrics", "(", "\"eval\"", ",", "metrics", ")", "\n", "trainer", ".", "save_metrics", "(", "\"eval\"", ",", "metrics", ")", "\n", "\n", "", "kwargs", "=", "{", "\"finetuned_from\"", ":", "model_args", ".", "model_name_or_path", ",", "\"tasks\"", ":", "\"fill-mask\"", "}", "\n", "if", "data_args", ".", "dataset_name", "is", "not", "None", ":", "\n", "        ", "kwargs", "[", "\"dataset_tags\"", "]", "=", "data_args", ".", "dataset_name", "\n", "if", "data_args", ".", "dataset_config_name", "is", "not", "None", ":", "\n", "            ", "kwargs", "[", "\"dataset_args\"", "]", "=", "data_args", ".", "dataset_config_name", "\n", "kwargs", "[", "\"dataset\"", "]", "=", "f\"{data_args.dataset_name} {data_args.dataset_config_name}\"", "\n", "", "else", ":", "\n", "            ", "kwargs", "[", "\"dataset\"", "]", "=", "data_args", ".", "dataset_name", "\n", "\n", "", "", "if", "training_args", ".", "push_to_hub", ":", "\n", "        ", "trainer", ".", "push_to_hub", "(", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "        ", "trainer", ".", "create_model_card", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.src.adapter_mlm._mp_fn": [[653, 656], ["adapter_mlm.main"], "function", ["home.repos.pwc.inspect_result.rlitschk_modularclir.helper.prepare_msmarco.main"], ["", "", "def", "_mp_fn", "(", "index", ")", ":", "\n", "# For xla_spawn (TPUs)", "\n", "    ", "main", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.src.sft_retrieval.main": [[176, 378], ["print", "input", "print", "transformers.HfArgumentParser", "transformers.HfArgumentParser.parse_args_into_dataclasses", "os.makedirs", "os.path.join", "logging.FileHandler", "logging.basicConfig", "training_args.get_process_log_level", "logger.setLevel", "datasets.utils.logging.set_verbosity", "transformers.utils.logging.add_handler", "transformers.utils.logging.set_verbosity", "transformers.utils.logging.enable_default_handler", "transformers.utils.logging.enable_explicit_format", "logger.warning", "logger.info", "transformers.set_seed", "transformers.AutoConfig.from_pretrained", "transformers.AutoModelForSequenceClassification.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "min", "multiprocessing.cpu_count", "list", "transformers.AdamW", "transformers.get_linear_schedule_with_warmup", "trainer_cls", "os.path.isdir", "transformers.trainer_utils.get_last_checkpoint", "sft.SFT", "logger.info", "sft.SFT.apply", "sft.SFT", "logger.info", "sft.SFT.apply", "AutoTokenizer.from_pretrained.", "torch.tensor", "torch.unsqueeze", "random.sample", "len", "AutoModelForSequenceClassification.from_pretrained.named_parameters", "trainer_cls.train", "min", "trainer_cls.save_model", "trainer_cls.log_metrics", "trainer_cls.save_metrics", "trainer_cls.save_state", "logger.info", "os.path.join", "os.makedirs", "trainer_cls.sft().save", "logger.info", "trainer_cls.evaluate", "min", "trainer_cls.log_metrics", "trainer_cls.save_metrics", "ValueError", "datasets.load_dataset", "datasets.load_dataset", "datasets.load_dataset", "e[].strip", "e[].strip", "train_dataset.select.select", "range", "logger.info", "eval_dataset.select.select", "predict_dataset.select.select", "AutoModelForSequenceClassification.from_pretrained.named_parameters", "len", "len", "len", "len", "logging.StreamHandler", "len", "logger.info", "range", "len", "range", "range", "n.startswith", "trainer_cls.sft", "bool", "os.listdir", "any", "any"], "function", ["None"], ["", "def", "main", "(", ")", ":", "\n", "  ", "print", "(", "\"Select gpu device (CUDA_VISIBLE_DEVICES)\"", ")", "\n", "gpudevice", "=", "input", "(", "\"enter here: \"", ")", "\n", "os", ".", "environ", "[", "\"CUDA_VISIBLE_DEVICES\"", "]", "=", "gpudevice", "\n", "print", "(", "f\"Specify GPU: {gpudevice}\"", ")", "\n", "\n", "# See all possible arguments in src/transformers/training_args.py", "\n", "# or by passing the --help flag to this script.", "\n", "# We now keep distinct sets of args, for a cleaner separation of concerns.", "\n", "parser", "=", "HfArgumentParser", "(", "(", "ModelArguments", ",", "DataTrainingArguments", ",", "TrainingArguments", ",", "SftArguments", ")", ")", "\n", "model_args", ",", "data_args", ",", "training_args", ",", "sft_args", "=", "parser", ".", "parse_args_into_dataclasses", "(", ")", "\n", "\n", "# Setup logging", "\n", "os", ".", "makedirs", "(", "training_args", ".", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "logfile", "=", "os", ".", "path", ".", "join", "(", "training_args", ".", "output_dir", ",", "\"log.txt\"", ")", "\n", "filehandler", "=", "logging", ".", "FileHandler", "(", "filename", "=", "logfile", ")", "\n", "logging", ".", "basicConfig", "(", "\n", "format", "=", "\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\"", ",", "\n", "datefmt", "=", "\"%m/%d/%Y %H:%M:%S\"", ",", "\n", "handlers", "=", "[", "logging", ".", "StreamHandler", "(", "sys", ".", "stdout", ")", ",", "filehandler", "]", "\n", ")", "\n", "\n", "log_level", "=", "training_args", ".", "get_process_log_level", "(", ")", "\n", "logger", ".", "setLevel", "(", "log_level", ")", "\n", "datasets", ".", "utils", ".", "logging", ".", "set_verbosity", "(", "log_level", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "add_handler", "(", "filehandler", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "set_verbosity", "(", "log_level", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "enable_default_handler", "(", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "enable_explicit_format", "(", ")", "\n", "\n", "# Log on each process the small summary:", "\n", "logger", ".", "warning", "(", "\n", "f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"", "\n", "+", "f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"", "\n", ")", "\n", "logger", ".", "info", "(", "f\"Training/evaluation parameters {training_args}\"", ")", "\n", "\n", "# Detecting last checkpoint.", "\n", "last_checkpoint", "=", "None", "\n", "if", "os", ".", "path", ".", "isdir", "(", "training_args", ".", "output_dir", ")", "and", "training_args", ".", "do_train", "and", "not", "training_args", ".", "overwrite_output_dir", ":", "\n", "    ", "last_checkpoint", "=", "get_last_checkpoint", "(", "training_args", ".", "output_dir", ")", "\n", "if", "last_checkpoint", "is", "None", "and", "len", "(", "os", ".", "listdir", "(", "training_args", ".", "output_dir", ")", ")", ">", "0", ":", "\n", "      ", "raise", "ValueError", "(", "\n", "f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"", "\n", "\"Use --overwrite_output_dir to overcome.\"", "\n", ")", "\n", "", "elif", "last_checkpoint", "is", "not", "None", ":", "\n", "      ", "logger", ".", "info", "(", "\n", "f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"", "\n", "\"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"", "\n", ")", "\n", "\n", "# Set seed before initializing model.", "\n", "", "", "set_seed", "(", "training_args", ".", "seed", ")", "\n", "\n", "# In distributed training, the load_dataset function guarantees that only one local process can concurrently", "\n", "# download the dataset.", "\n", "# Downloading and loading prepared ms-marco dataset from disk.", "\n", "if", "training_args", ".", "do_train", ":", "\n", "    ", "train_dataset", "=", "load_dataset", "(", "\"json\"", ",", "data_files", "=", "data_args", ".", "train_file", ",", "cache_dir", "=", "model_args", ".", "cache_dir", ")", "[", "'train'", "]", "\n", "\n", "", "if", "training_args", ".", "do_eval", ":", "\n", "    ", "eval_dataset", "=", "load_dataset", "(", "\"json\"", ",", "data_files", "=", "data_args", ".", "validation_file", ",", "cache_dir", "=", "model_args", ".", "cache_dir", ")", "[", "'train'", "]", "\n", "\n", "", "if", "training_args", ".", "do_predict", ":", "\n", "    ", "predict_dataset", "=", "load_dataset", "(", "\"json\"", ",", "data_files", "=", "data_args", ".", "test_file", ",", "cache_dir", "=", "model_args", ".", "cache_dir", ")", "[", "'train'", "]", "\n", "\n", "# Labels", "\n", "", "num_labels", "=", "1", "\n", "max_length", "=", "data_args", ".", "max_seq_length", "# 512", "\n", "\n", "# Load pretrained model and tokenizer", "\n", "# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently", "\n", "# download model & vocab.", "\n", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "model_args", ".", "model_name_or_path", ")", "\n", "config", ".", "num_labels", "=", "num_labels", "\n", "model", "=", "AutoModelForSequenceClassification", ".", "from_pretrained", "(", "model_args", ".", "model_name_or_path", ",", "config", "=", "config", ")", "\n", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "model_args", ".", "model_name_or_path", ")", "\n", "\n", "# during training of task-sfts the flag sft_args.task_ft should be None, because that's what we want to train.", "\n", "if", "sft_args", ".", "task_ft", "is", "not", "None", ":", "\n", "    ", "task_ft", "=", "SFT", "(", "sft_args", ".", "task_ft", ")", "\n", "logger", ".", "info", "(", "f'Applying task fine-tuning {sft_args.task_ft}'", ")", "\n", "task_ft", ".", "apply", "(", "model", ",", "with_abs", "=", "True", ")", "\n", "\n", "", "if", "sft_args", ".", "lang_ft", "is", "not", "None", ":", "\n", "    ", "lang_ft", "=", "SFT", "(", "sft_args", ".", "lang_ft", ")", "\n", "logger", ".", "info", "(", "f'Applying language fine-tuning {sft_args.lang_ft}'", ")", "\n", "lang_ft", ".", "apply", "(", "model", ",", "with_abs", "=", "False", ")", "\n", "\n", "\n", "# Preprocessing the datasets", "\n", "# Padding strategy", "\n", "# We will pad later, dynamically at batch creation, to the max sequence length in each batch", "\n", "", "data_args", ".", "pad_to_max_length", "=", "False", "\n", "max_length", "=", "min", "(", "tokenizer", ".", "max_len_sentences_pair", ",", "data_args", ".", "max_seq_length", ")", "\n", "\n", "def", "data_collator", "(", "examples", ")", ":", "\n", "    ", "questions", "=", "[", "e", "[", "'query'", "]", ".", "strip", "(", ")", "for", "e", "in", "examples", "]", "\n", "passages", "=", "[", "e", "[", "'passage'", "]", ".", "strip", "(", ")", "for", "e", "in", "examples", "]", "\n", "tokenized_examples", "=", "tokenizer", "(", "*", "[", "questions", ",", "passages", "]", ",", "padding", "=", "True", ",", "return_tensors", "=", "\"pt\"", ",", "truncation", "=", "'longest_first'", ",", "max_length", "=", "max_length", ")", "\n", "tokenized_examples", "[", "'labels'", "]", "=", "torch", ".", "tensor", "(", "[", "e", "[", "'label'", "]", "for", "e", "in", "examples", "]", ",", "dtype", "=", "torch", ".", "float", "if", "config", ".", "num_labels", "==", "1", "else", "torch", ".", "long", ")", "\n", "tokenized_examples", "[", "'labels'", "]", "=", "torch", ".", "unsqueeze", "(", "tokenized_examples", "[", "'labels'", "]", ",", "1", ")", "\n", "return", "tokenized_examples", "\n", "\n", "", "preprocessing_num_workers", "=", "multiprocessing", ".", "cpu_count", "(", ")", "\n", "data_args", ".", "preprocessing_num_workers", "=", "preprocessing_num_workers", "\n", "\n", "if", "training_args", ".", "do_train", ":", "\n", "    ", "if", "data_args", ".", "max_train_samples", "is", "not", "None", ":", "\n", "      ", "train_dataset", "=", "train_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_train_samples", ")", ")", "\n", "# Log a few random samples from the training set:", "\n", "", "for", "index", "in", "random", ".", "sample", "(", "range", "(", "len", "(", "train_dataset", ")", ")", ",", "3", ")", ":", "\n", "      ", "logger", ".", "info", "(", "f\"Sample {index} of the training set: {train_dataset[index]}.\"", ")", "\n", "\n", "", "", "if", "training_args", ".", "do_eval", ":", "\n", "    ", "if", "data_args", ".", "max_eval_samples", "is", "not", "None", ":", "\n", "      ", "eval_dataset", "=", "eval_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_eval_samples", ")", ")", "\n", "\n", "", "", "if", "training_args", ".", "do_predict", ":", "\n", "    ", "if", "data_args", ".", "max_predict_samples", "is", "not", "None", ":", "\n", "      ", "predict_dataset", "=", "predict_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_predict_samples", ")", ")", "\n", "\n", "\n", "# Taken from Reimers et al. (CrossEncoder.py), num_train_steps = 625000, batch_size = 32", "\n", "", "", "num_train_steps", "=", "len", "(", "train_dataset", ")", "//", "training_args", ".", "train_batch_size", "\n", "weight_decay", "=", "0.01", "\n", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "weight_decay", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "maskable_params", "=", "[", "\n", "n", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "\n", "if", "n", ".", "startswith", "(", "model", ".", "base_model_prefix", ")", "and", "p", ".", "requires_grad", "\n", "]", "\n", "optimizer", "=", "AdamW", "(", "optimizer_grouped_parameters", ",", "**", "{", "'lr'", ":", "training_args", ".", "learning_rate", "}", ")", "\n", "scheduler", "=", "transformers", ".", "get_linear_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", "=", "training_args", ".", "warmup_steps", ",", "num_training_steps", "=", "num_train_steps", ")", "\n", "\n", "# needed because our data_collator needs access to those columns", "\n", "training_args", ".", "remove_unused_columns", "=", "False", "\n", "\n", "# this hack is necessary to trigger BCEWithLogitsLoss() internally, ", "\n", "# otherwise scalar outputs are treated as regression and evaluated with MSE", "\n", "model", ".", "config", ".", "problem_type", "=", "\"multi_label_classification\"", "\n", "\n", "# Initialize our Trainer", "\n", "trainer_cls", "=", "LotteryTicketSparseFineTuner", "\n", "trainer", "=", "trainer_cls", "(", "\n", "sft_args", "=", "sft_args", ",", "\n", "maskable_params", "=", "maskable_params", ",", "\n", "model", "=", "model", ",", "\n", "args", "=", "training_args", ",", "\n", "train_dataset", "=", "train_dataset", "if", "training_args", ".", "do_train", "else", "None", ",", "\n", "eval_dataset", "=", "eval_dataset", "if", "training_args", ".", "do_eval", "else", "None", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "data_collator", "=", "data_collator", ",", "\n", "optimizers", "=", "(", "optimizer", ",", "scheduler", ")", ",", "\n", ")", "\n", "\n", "# Training", "\n", "if", "training_args", ".", "do_train", ":", "\n", "    ", "checkpoint", "=", "None", "\n", "if", "training_args", ".", "resume_from_checkpoint", "is", "not", "None", ":", "\n", "      ", "checkpoint", "=", "training_args", ".", "resume_from_checkpoint", "\n", "", "elif", "last_checkpoint", "is", "not", "None", ":", "\n", "      ", "checkpoint", "=", "last_checkpoint", "\n", "", "train_result", "=", "trainer", ".", "train", "(", "resume_from_checkpoint", "=", "checkpoint", ")", "\n", "metrics", "=", "train_result", ".", "metrics", "\n", "max_train_samples", "=", "(", "\n", "data_args", ".", "max_train_samples", "if", "data_args", ".", "max_train_samples", "is", "not", "None", "else", "len", "(", "train_dataset", ")", "\n", ")", "\n", "metrics", "[", "\"train_samples\"", "]", "=", "min", "(", "max_train_samples", ",", "len", "(", "train_dataset", ")", ")", "\n", "\n", "trainer", ".", "save_model", "(", ")", "# Saves the tokenizer too for easy upload", "\n", "\n", "trainer", ".", "log_metrics", "(", "\"train\"", ",", "metrics", ")", "\n", "trainer", ".", "save_metrics", "(", "\"train\"", ",", "metrics", ")", "\n", "trainer", ".", "save_state", "(", ")", "\n", "\n", "logger", ".", "info", "(", "\"Save SFT parameters\"", ")", "\n", "sft_output_dir", "=", "os", ".", "path", ".", "join", "(", "training_args", ".", "output_dir", ",", "\"ranking_mask\"", ")", "\n", "os", ".", "makedirs", "(", "sft_output_dir", ")", "\n", "trainer", ".", "sft", "(", ")", ".", "save", "(", "sft_output_dir", ")", "\n", "\n", "# Evaluation", "\n", "", "if", "training_args", ".", "do_eval", ":", "\n", "    ", "logger", ".", "info", "(", "\"*** Evaluate ***\"", ")", "\n", "metrics", "=", "trainer", ".", "evaluate", "(", "eval_dataset", "=", "eval_dataset", ")", "\n", "\n", "max_eval_samples", "=", "data_args", ".", "max_eval_samples", "if", "data_args", ".", "max_eval_samples", "is", "not", "None", "else", "len", "(", "eval_dataset", ")", "\n", "metrics", "[", "\"eval_samples\"", "]", "=", "min", "(", "max_eval_samples", ",", "len", "(", "eval_dataset", ")", ")", "\n", "\n", "trainer", ".", "log_metrics", "(", "\"eval\"", ",", "metrics", ")", "\n", "trainer", ".", "save_metrics", "(", "\"eval\"", ",", "metrics", ")", "\n", "\n", "# Prediction", "\n", "", "if", "training_args", ".", "do_predict", ":", "\n", "# TODO: implement scoring", "\n", "# cf. https://github.com/huggingface/transformers/blob/master/examples/pytorch/token-classification/run_ner.py", "\n", "    ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.src.adapter_eval._evaluate_adapter_setting": [[55, 212], ["helper.evaluate.add_filehandler", "logger.info", "logger.info", "logger.info", "task_rf2results.keys", "os.path.join", "sentence_transformers.CrossEncoder", "sentence_transformers.CrossEncoder.model.cpu", "list", "str", "list", "sentence_transformers.CrossEncoder.model.load_adapter", "helper.config.get_preranker", "langpair2skip_layers2duration.keys", "config_to_str", "map_results.append", "duration_results.append", "type", "range", "range", "os.path.join", "os.path.join", "os.path.join", "sentence_transformers.CrossEncoder.model.load_adapter", "config_to_str", "logger.info", "os.path.join", "logger.info", "os.path.join", "logger.info", "helper.evaluate.rerank_and_eval", "str", "helper.evaluate.map2str", "config_to_str", "LA_path.replace.replace", "sentence_transformers.CrossEncoder.model.bert.delete_invertible_adapter", "sentence_transformers.CrossEncoder.model.set_active_adapters", "transformers.adapters.composition.Stack", "sentence_transformers.CrossEncoder.model.set_active_adapters"], "function", ["home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.add_filehandler", "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.config.get_preranker", "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.rerank_and_eval", "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.map2str"], ["def", "_evaluate_adapter_setting", "(", "eval_adapterdrop", "=", "False", ",", "ablation_config", "=", "None", ",", "name", "=", "\"\"", ")", ":", "\n", "  ", "fName", "=", "\"evaluate_AdapterDrop.txt\"", "if", "eval_adapterdrop", "else", "\"evaluate_adapter_ablation_tmp.txt\"", "\n", "add_filehandler", "(", "os", ".", "path", ".", "join", "(", "ADAPTER_DIR", ",", "fName", ")", ")", "\n", "logger", ".", "info", "(", "f\"loading precomputed average precision (AP) values for t-test from {REF_PRECISION_VALS_DIR}\"", ")", "\n", "config_to_str", "=", "lambda", "sl_config", ":", "f\"{sl_config[0]}-{sl_config[-1]}\"", "if", "type", "(", "sl_config", ")", "==", "list", "else", "str", "(", "sl_config", ")", "\n", "\n", "if", "eval_adapterdrop", ":", "\n", "    ", "apply_task_adapter", "=", "True", "\n", "apply_lang_adapter", "=", "True", "\n", "apply_invertible_adapters", "=", "True", "\n", "# AdapterDrop ablation settings (skip_layer_configs): ['None', '1-2', '1-4', '1-6', '1-8', '1-10', '1-12']", "\n", "# Example: https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/adapterdrop/drop_at_inference.py", "\n", "skip_layer_configs", "=", "[", "list", "(", "range", "(", "1", ",", "k", ")", ")", "for", "k", "in", "range", "(", "1", ",", "12", "+", "2", ",", "2", ")", "]", "\n", "skip_layer_configs", "[", "0", "]", "=", "None", "\n", "task_reduction_factors", "=", "[", "'1'", ",", "'2'", ",", "'4'", ",", "'8'", ",", "'16'", ",", "'32'", "]", "\n", "language_config", "=", "'dlang'", "\n", "# 'pfeiffer' is trained without invertible adapters, 'pfeiffer+inv' trained with invertible adapters", "\n", "# We observe that training with 'pfeiffer+inv', i.e., with invertible adapters during MLM pretraining of language", "\n", "# adapters, and then turning them off during inference (apply_invertible_adapters = False) improves results.", "\n", "adapter_config", "=", "'pfeiffer+inv'", "\n", "", "else", ":", "\n", "    ", "if", "ablation_config", ":", "\n", "      ", "skip_layer_configs", "=", "ablation_config", "[", "'skip_layer_configs'", "]", "\n", "apply_task_adapter", "=", "ablation_config", "[", "'apply_task_adapter'", "]", "\n", "apply_lang_adapter", "=", "ablation_config", "[", "'apply_lang_adapter'", "]", "\n", "apply_invertible_adapters", "=", "ablation_config", "[", "'apply_invertible_adapters'", "]", "\n", "language_config", "=", "ablation_config", "[", "'setting'", "]", "\n", "task_reduction_factors", "=", "ablation_config", "[", "'task_reduction_factors'", "]", "\n", "adapter_config", "=", "ablation_config", "[", "'config'", "]", "\n", "", "else", ":", "\n", "      ", "skip_layer_configs", "=", "[", "None", "]", "\n", "apply_task_adapter", "=", "True", "\n", "apply_lang_adapter", "=", "True", "\n", "apply_invertible_adapters", "=", "False", "\n", "language_config", "=", "'dlang'", "\n", "task_reduction_factors", "=", "[", "'16'", "]", "\n", "adapter_config", "=", "'pfeiffer+inv'", "\n", "\n", "", "", "logger", ".", "info", "(", "f\"task adapters: {apply_task_adapter}\"", "\n", "f\"\\tlanguage adapters: {apply_lang_adapter}\"", "\n", "f\"\\tinvertible adapters:{apply_invertible_adapters}\"", "\n", "f\"\\tskip layers: {[config_to_str(c) for c in skip_layer_configs]}\"", "\n", "f\"\\tadapter-config: {adapter_config}\"", ")", "\n", "logger", ".", "info", "(", "f\"Setting: {language_config}\\tlang_reduction_factor: {lang_rf}\\ttask reduction factor: {task_reduction_factors}\"", ")", "\n", "\n", "task_rf2results", "=", "{", "}", "\n", "task_rf2durations", "=", "{", "}", "\n", "\n", "for", "task_rf", "in", "task_reduction_factors", ":", "\n", "    ", "reranker", "=", "CrossEncoder", "(", "'bert-base-multilingual-uncased'", ",", "max_length", "=", "max_seq_len", ")", "\n", "\n", "# Load Ranking Adapter", "\n", "if", "apply_task_adapter", ":", "\n", "      ", "if", "adapter_config", "==", "'pfeiffer+inv'", ":", "\n", "        ", "RA_path", "=", "os", ".", "path", ".", "join", "(", "ADAPTER_DIR", ",", "f\"ir/rf_{lang_rf}_{task_rf}/checkpoint-625000/retrieval\"", ")", "\n", "", "else", ":", "\n", "        ", "RA_path", "=", "os", ".", "path", ".", "join", "(", "ADAPTER_DIR", ",", "f\"ir_-INV/rf_{lang_rf}_{task_rf}/checkpoint-625000/retrieval\"", ")", "\n", "", "reranker", ".", "model", ".", "load_adapter", "(", "\n", "RA_path", ",", "\n", "load_as", "=", "'ir'", ",", "\n", "with_head", "=", "True", "\n", ")", "\n", "\n", "", "langpair2skip_layers2map", "=", "{", "}", "\n", "langpair2skip_layers2duration", "=", "{", "}", "\n", "for", "qlang", ",", "dlang", "in", "lang_pairs", ":", "\n", "      ", "preranker_model", "=", "get_preranker", "(", "qlang", ",", "dlang", ")", "\n", "\n", "# Load Language Adapter", "\n", "if", "apply_lang_adapter", ":", "\n", "# Load Language Adapter, for so-en and sw-en we load translated queries, hence load en adapter", "\n", "# LA_lang = dlang if language_config == 'dlang' or qlang in ('so', 'sw') else qlang", "\n", "        ", "LA_lang", "=", "dlang", "if", "language_config", "==", "'dlang'", "or", "qlang", "in", "(", "'so'", ",", "'sw'", ")", "else", "qlang", "\n", "LA_path", "=", "os", ".", "path", ".", "join", "(", "ADAPTER_DIR", ",", "f\"mlm/rf_{lang_rf}/{LA_lang}/checkpoint-225000/mlm_adapter\"", ")", "\n", "# maybe adjust for architecture where invertible adapters are turned off already during training (not part of paper)", "\n", "if", "adapter_config", "==", "'pfeiffer'", ":", "\n", "          ", "LA_path", "=", "LA_path", ".", "replace", "(", "\"mlm/\"", ",", "\"mlm_-INV/\"", ")", "\n", "\n", "", "reranker", ".", "model", ".", "load_adapter", "(", "\n", "LA_path", ",", "\n", "load_as", "=", "\"mlm\"", ",", "\n", "with_head", "=", "False", "\n", ")", "\n", "if", "not", "apply_invertible_adapters", ":", "\n", "          ", "reranker", ".", "model", ".", "bert", ".", "delete_invertible_adapter", "(", "'mlm'", ")", "\n", "\n", "", "", "skip_layers2map", "=", "{", "}", "\n", "skip_layers2duration", "=", "{", "}", "\n", "for", "skip_layers", "in", "skip_layer_configs", ":", "\n", "        ", "skip_layers_str", "=", "config_to_str", "(", "skip_layers", ")", "\n", "logger", ".", "info", "(", "f\"Skip layers: {skip_layers_str}\\t{qlang}->{dlang}\\ttask_rf: {task_rf}\\tlang_rf: {lang_rf}\"", ")", "\n", "\n", "# Set activate Adapters", "\n", "if", "apply_task_adapter", "and", "apply_lang_adapter", ":", "\n", "          ", "reranker", ".", "model", ".", "set_active_adapters", "(", "ac", ".", "Stack", "(", "\"mlm\"", ",", "\"ir\"", ")", ",", "skip_layers", "=", "skip_layers", ")", "\n", "", "elif", "apply_task_adapter", "and", "not", "apply_lang_adapter", ":", "\n", "          ", "reranker", ".", "model", ".", "set_active_adapters", "(", "\"ir\"", ",", "skip_layers", "=", "skip_layers", ")", "\n", "", "elif", "not", "apply_task_adapter", "and", "apply_lang_adapter", ":", "\n", "          ", "raise", "NotImplementedError", "\n", "\n", "", "save_precision_values_dir", "=", "os", ".", "path", ".", "join", "(", "\n", "ADAPTER_DIR", ",", "\n", "f\"ir/rf_2_{task_rf}/precision_values\"", "\n", "f\"{qlang}-{dlang}_+LANG-INV_setting={language_config}_preranker={preranker_model}\"", "\n", ")", "\n", "logger", ".", "info", "(", "f\"saving precision values to: {save_precision_values_dir}\"", ")", "\n", "\n", "load_precision_values_dir", "=", "os", ".", "path", ".", "join", "(", "REF_PRECISION_VALS_DIR", ",", "f\"{qlang}-{dlang}_preranker={preranker_model}\"", ")", "\n", "logger", ".", "info", "(", "f\"loading precision values from: {load_precision_values_dir}\"", ")", "\n", "\n", "# Re-rank and evaluate", "\n", "eval_result", "=", "rerank_and_eval", "(", "\n", "qlang", "=", "qlang", ",", "\n", "dlang", "=", "dlang", ",", "\n", "reranker", "=", "reranker", ",", "\n", "preranker", "=", "preranker_model", ",", "\n", "prerank_dir", "=", "PRERANK_DIR", ",", "\n", "load_precision_values_dir", "=", "load_precision_values_dir", ",", "\n", "save_precision_values_dir", "=", "save_precision_values_dir", ",", "\n", "path_query_translations", "=", "args", ".", "path_query_translations", "\n", ")", "\n", "skip_layers2duration", "[", "skip_layers_str", "]", "=", "str", "(", "eval_result", "[", "\"duration_ms\"", "]", ")", "\n", "skip_layers2map", "[", "skip_layers_str", "]", "=", "map2str", "(", "eval_map", "=", "eval_result", "[", "\"MAP\"", "]", ",", "pvalue", "=", "eval_result", "[", "\"pvalue\"", "]", ")", "\n", "\n", "", "langpair2skip_layers2map", "[", "qlang", "+", "dlang", "]", "=", "skip_layers2map", "\n", "langpair2skip_layers2duration", "[", "qlang", "+", "dlang", "]", "=", "skip_layers2duration", "\n", "\n", "", "task_rf2results", "[", "task_rf", "]", "=", "langpair2skip_layers2map", "\n", "task_rf2durations", "[", "task_rf", "]", "=", "langpair2skip_layers2duration", "\n", "\n", "# Move adapters to cpu to avoid memory issues", "\n", "reranker", ".", "model", ".", "cpu", "(", ")", "\n", "\n", "", "header", "=", "\"task_rf\\tskip_layers\\t\"", "+", "name", "if", "name", "else", "\"\"", "+", "\"\\t\"", ".", "join", "(", "[", "f\"{q}{d}\"", "for", "q", ",", "d", "in", "lang_pairs", "]", ")", "\n", "map_results", "=", "[", "header", "]", "\n", "duration_results", "=", "[", "header", "]", "\n", "\n", "name_str", "=", "f\"'{name}\\t\"", "if", "name", "else", "\"\"", "\n", "for", "task_rf", "in", "task_rf2results", ".", "keys", "(", ")", ":", "\n", "    ", "langpair2skip_layers2map", "=", "task_rf2results", "[", "task_rf", "]", "\n", "langpair2skip_layers2duration", "=", "task_rf2durations", "[", "task_rf", "]", "\n", "tmp_lang_pairs", "=", "list", "(", "langpair2skip_layers2duration", ".", "keys", "(", ")", ")", "\n", "skip_layer_configs", "=", "langpair2skip_layers2map", "[", "tmp_lang_pairs", "[", "0", "]", "]", "\n", "for", "skip_layers", "in", "skip_layer_configs", ":", "\n", "      ", "skip_layers_str", "=", "config_to_str", "(", "skip_layers", ")", "\n", "map_results", ".", "append", "(", "\n", "f\"{task_rf}\\t{skip_layers_str}\\t{name_str}\"", "+", "\"\\t\"", ".", "join", "(", "\n", "[", "langpair2skip_layers2map", "[", "lp", "]", "[", "skip_layers_str", "]", "for", "lp", "in", "tmp_lang_pairs", "]", "\n", ")", "\n", ")", "\n", "duration_results", ".", "append", "(", "\n", "f\"{task_rf}\\t{skip_layers_str}\\t{name_str}\"", "+", "\"\\t\"", ".", "join", "(", "\n", "[", "langpair2skip_layers2duration", "[", "lp", "]", "[", "skip_layers_str", "]", "for", "lp", "in", "tmp_lang_pairs", "]", "\n", ")", "\n", ")", "\n", "\n", "", "", "return", "map_results", ",", "duration_results", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.src.adapter_eval.evaluate_adapters": [[214, 291], ["helper.evaluate.add_filehandler", "logger.info", "sentence_transformers.CrossEncoder", "os.path.join", "os.path.join", "sentence_transformers.CrossEncoder.model.load_adapter", "logger.info", "map_results.append", "duration_results.append", "helper.config.get_preranker", "os.path.join", "logger.info", "sentence_transformers.CrossEncoder.model.load_adapter", "sentence_transformers.CrossEncoder.model.set_active_adapters", "logger.info", "logger.info", "logger.info", "os.path.join.replace", "os.path.join", "os.path.join", "logger.info", "helper.evaluate.rerank_and_eval", "logger.info", "helper.evaluate.map2str", "str", "map_results.append", "duration_results.append", "str", "str", "transformers.adapters.composition.Stack", "langpair2map.keys", "langpair2map.keys", "langpair2map.keys"], "function", ["home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.add_filehandler", "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.config.get_preranker", "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.rerank_and_eval", "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.map2str"], ["", "def", "evaluate_adapters", "(", ")", ":", "\n", "  ", "fName", "=", "f\"evaluate_adapters.txt\"", "\n", "add_filehandler", "(", "os", ".", "path", ".", "join", "(", "ADAPTER_DIR", ",", "fName", ")", ")", "\n", "logger", ".", "info", "(", "f\"loading precomputed average precision (AP) values for t-test from {REF_PRECISION_VALS_DIR}\"", ")", "\n", "\n", "reranker", "=", "CrossEncoder", "(", "'bert-base-multilingual-uncased'", ",", "max_length", "=", "max_seq_len", ")", "\n", "map_results", "=", "[", "]", "\n", "duration_results", "=", "[", "]", "\n", "checkpoints", "=", "[", "625000", "]", "# 100000", "\n", "task_reduction_factors", "=", "args", ".", "task_rf", "\n", "language_configs", "=", "args", ".", "language_configs", "\n", "\n", "for", "task_rf", "in", "task_reduction_factors", ":", "\n", "    ", "for", "checkpoint", "in", "checkpoints", ":", "\n", "      ", "for", "language_config", "in", "language_configs", ":", "\n", "# Load Ranking Adapter", "\n", "        ", "RA_name", "=", "f\"RA_rf={task_rf}\"", "\n", "RA_path", "=", "os", ".", "path", ".", "join", "(", "ADAPTER_DIR", ",", "f\"ir/rf_{lang_rf}_{task_rf}/checkpoint-625000/retrieval\"", ")", "\n", "\n", "reranker", ".", "model", ".", "load_adapter", "(", "\n", "RA_path", ",", "\n", "load_as", "=", "RA_name", ",", "\n", "with_head", "=", "True", "\n", ")", "\n", "langpair2map", "=", "{", "}", "\n", "langpair2duration", "=", "{", "}", "\n", "logger", ".", "info", "(", "f\"Evaluating language pairs: {lang_pairs}\"", ")", "\n", "for", "qlang", ",", "dlang", "in", "lang_pairs", ":", "\n", "          ", "preranker_model", "=", "get_preranker", "(", "qlang", ",", "dlang", ")", "\n", "\n", "# Load Language Adapter, for so-en and sw-en we load translated queries, hence load en adapter", "\n", "LA_lang", "=", "dlang", "if", "language_config", "==", "'dlang'", "or", "qlang", "in", "(", "'so'", ",", "'sw'", ")", "else", "qlang", "\n", "LA_path", "=", "os", ".", "path", ".", "join", "(", "ADAPTER_DIR", ",", "f\"mlm/rf_{lang_rf}/{LA_lang}/checkpoint-225000/mlm_adapter\"", ")", "\n", "LA_name", "=", "f\"LA_rf={lang_rf}\"", "\n", "logger", ".", "info", "(", "f\"Loading language adapter {LA_path}\"", ")", "\n", "\n", "reranker", ".", "model", ".", "load_adapter", "(", "\n", "LA_path", ",", "\n", "load_as", "=", "LA_name", ",", "\n", "with_head", "=", "False", "\n", ")", "\n", "reranker", ".", "model", ".", "set_active_adapters", "(", "ac", ".", "Stack", "(", "LA_name", ",", "RA_name", ")", ")", "\n", "logger", ".", "info", "(", "\"language adapter:\\t\"", "+", "LA_path", ")", "\n", "logger", ".", "info", "(", "\"task adapter:\\t\"", "+", "RA_path", ")", "\n", "logger", ".", "info", "(", "\"setting:\\t\"", "+", "language_config", ")", "\n", "\n", "path_save_precision_values_dir", "=", "RA_path", ".", "replace", "(", "\"retrieval\"", ",", "\"precision_values\"", ")", "\n", "load_precision_values_dir", "=", "os", ".", "path", ".", "join", "(", "REF_PRECISION_VALS_DIR", ",", "f\"{qlang}-{dlang}_preranker={preranker_model}\"", ")", "\n", "save_precision_values_dir", "=", "os", ".", "path", ".", "join", "(", "\n", "path_save_precision_values_dir", ",", "f\"{qlang}-{dlang}_setting={language_config}_preranker={preranker_model}\"", "\n", ")", "\n", "logger", ".", "info", "(", "f\"saving precision values to: {save_precision_values_dir}\"", ")", "\n", "eval_result", "=", "rerank_and_eval", "(", "\n", "qlang", "=", "qlang", ",", "\n", "dlang", "=", "dlang", ",", "\n", "reranker", "=", "reranker", ",", "\n", "preranker", "=", "preranker_model", ",", "\n", "prerank_dir", "=", "PRERANK_DIR", ",", "\n", "load_precision_values_dir", "=", "load_precision_values_dir", ",", "\n", "save_precision_values_dir", "=", "save_precision_values_dir", ",", "\n", "path_query_translations", "=", "args", ".", "path_query_translations", "\n", ")", "\n", "logger", ".", "info", "(", "\"------\"", ")", "\n", "\n", "langpair2map", "[", "qlang", "+", "dlang", "]", "=", "map2str", "(", "eval_map", "=", "eval_result", "[", "'MAP'", "]", ",", "pvalue", "=", "eval_result", "[", "'pvalue'", "]", ")", "\n", "langpair2duration", "[", "qlang", "+", "dlang", "]", "=", "str", "(", "eval_result", "[", "'duration_ms'", "]", ")", "\n", "\n", "", "if", "not", "map_results", ":", "\n", "          ", "header", "=", "\"task_rf\\tcheckpoint\\tlanguage_config\\t\"", "+", "\"\\t\"", ".", "join", "(", "langpair2map", ".", "keys", "(", ")", ")", "\n", "map_results", ".", "append", "(", "header", ")", "\n", "duration_results", ".", "append", "(", "header", ")", "\n", "\n", "", "desc", "=", "[", "str", "(", "task_rf", ")", ",", "str", "(", "checkpoint", ")", ",", "language_config", "]", "\n", "map_results", ".", "append", "(", "\"\\t\"", ".", "join", "(", "desc", "+", "[", "langpair2map", "[", "lp", "]", "for", "lp", "in", "langpair2map", ".", "keys", "(", ")", "]", ")", ")", "\n", "duration_results", ".", "append", "(", "\"\\t\"", ".", "join", "(", "desc", "+", "[", "langpair2duration", "[", "lp", "]", "for", "lp", "in", "langpair2map", ".", "keys", "(", ")", "]", ")", ")", "\n", "\n", "", "", "", "return", "map_results", ",", "duration_results", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.src.adapter_eval.evaluate_split_adapters": [[293, 387], ["helper.evaluate.add_filehandler", "logger.info", "task_rf2results.items", "os.path.join", "ckpt2results.items", "sentence_transformers.CrossEncoder", "os.path.join", "sentence_transformers.CrossEncoder.model.load_adapter", "set", "sentence_transformers.CrossEncoder.model.cpu", "map_results.append", "duration_results.append", "helper.config.get_preranker", "logger.info", "os.path.join", "logger.info", "helper.evaluate.rerank_and_eval", "logger.info", "helper.evaluate.map2str", "str", "sentence_transformers.CrossEncoder.model.load_adapter", "set.add", "sentence_transformers.CrossEncoder.model.load_adapter", "set.add", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.add_filehandler", "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.config.get_preranker", "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.rerank_and_eval", "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.map2str"], ["", "def", "evaluate_split_adapters", "(", ")", ":", "\n", "  ", "fName", "=", "\"evaluate_split-adapters.txt\"", "\n", "add_filehandler", "(", "os", ".", "path", ".", "join", "(", "ADAPTER_DIR", ",", "fName", ")", ")", "\n", "\n", "# Best setting for split-adapter: [LA_rf=2, RA_rf=32]", "\n", "language_config", "=", "'split-adapter'", "\n", "task_reduction_factors", "=", "args", ".", "task_rf", "\n", "checkpoints", "=", "[", "625000", "]", "# 100000", "\n", "\n", "logger", ".", "info", "(", "f\"Setting: {language_config}\\t\"", "\n", "f\"lang_reduction_factor: {lang_rf}\\t\"", "\n", "f\"task reduction factor: {task_reduction_factors}\"", ")", "\n", "\n", "task_rf2results", "=", "{", "}", "\n", "task_rf2durations", "=", "{", "}", "\n", "for", "task_rf", "in", "task_reduction_factors", ":", "\n", "\n", "    ", "ckpt2lp2maps", "=", "{", "}", "\n", "ckpt2lp2durations", "=", "{", "}", "\n", "for", "checkpoint", "in", "checkpoints", ":", "\n", "\n", "      ", "reranker", "=", "CrossEncoder", "(", "\"bert-base-multilingual-uncased\"", ",", "max_length", "=", "max_seq_len", ")", "\n", "TA_path", "=", "os", ".", "path", ".", "join", "(", "ADAPTER_DIR", ",", "f\"ir/rf_{lang_rf}_{task_rf}/checkpoint-625000/retrieval\"", ")", "\n", "task_adapter_name", "=", "'ir'", "\n", "reranker", ".", "model", ".", "load_adapter", "(", "\n", "TA_path", ",", "\n", "load_as", "=", "task_adapter_name", ",", "\n", "with_head", "=", "True", "\n", ")", "\n", "\n", "loaded_adapters", "=", "set", "(", ")", "\n", "langpair2map", "=", "{", "}", "\n", "langpair2duration", "=", "{", "}", "\n", "for", "qlang", ",", "dlang", "in", "lang_pairs", ":", "\n", "        ", "preranker_model", "=", "get_preranker", "(", "qlang", ",", "dlang", ")", "\n", "\n", "logger", ".", "info", "(", "\"------------------\"", ")", "\n", "if", "qlang", "not", "in", "loaded_adapters", ":", "\n", "          ", "reranker", ".", "model", ".", "load_adapter", "(", "\n", "os", ".", "path", ".", "join", "(", "ADAPTER_DIR", ",", "f\"mlm/rf_{lang_rf}/{qlang}/checkpoint-225000/mlm_adapter\"", ")", ",", "\n", "load_as", "=", "qlang", ",", "\n", "with_head", "=", "False", "\n", ")", "\n", "loaded_adapters", ".", "add", "(", "qlang", ")", "\n", "", "if", "dlang", "not", "in", "loaded_adapters", ":", "\n", "          ", "reranker", ".", "model", ".", "load_adapter", "(", "\n", "os", ".", "path", ".", "join", "(", "ADAPTER_DIR", ",", "f\"mlm/rf_{lang_rf}/{dlang}/checkpoint-225000/mlm_adapter\"", ")", ",", "\n", "load_as", "=", "dlang", ",", "\n", "with_head", "=", "False", "\n", ")", "\n", "loaded_adapters", ".", "add", "(", "dlang", ")", "\n", "\n", "", "load_precision_values_dir", "=", "REF_PRECISION_VALS_DIR", "+", "f\"{qlang}-{dlang}_preranker={preranker_model}\"", "\n", "save_precision_values_dir", "=", "os", ".", "path", ".", "join", "(", "\n", "ADAPTER_DIR", ",", "\n", "f\"ir_625k/rf_2_{task_rf}/precision_values\"", ",", "\n", "f\"{qlang}-{dlang}_setting={language_config}_preranker={preranker_model}\"", "\n", ")", "\n", "logger", ".", "info", "(", "f\"saving precision values to: {save_precision_values_dir}\"", ")", "\n", "\n", "eval_result", "=", "rerank_and_eval", "(", "\n", "qlang", "=", "qlang", ",", "\n", "dlang", "=", "dlang", ",", "\n", "reranker", "=", "reranker", ",", "\n", "preranker", "=", "preranker_model", ",", "\n", "prerank_dir", "=", "PRERANK_DIR", ",", "\n", "task_adapter_name", "=", "task_adapter_name", ",", "\n", "run_split_adapters", "=", "True", ",", "\n", "load_precision_values_dir", "=", "load_precision_values_dir", ",", "\n", "save_precision_values_dir", "=", "save_precision_values_dir", ",", "\n", "# Split adapters currently not supported for low-resource languages ", "\n", "# path_query_translations=args.path_query_translations ", "\n", ")", "\n", "logger", ".", "info", "(", "f\"{qlang}->{dlang}\\tckpt: {checkpoint}\\ttask-rf: {task_rf}\\tlang-rf: {lang_rf}\"", ")", "\n", "langpair2map", "[", "qlang", "+", "dlang", "]", "=", "map2str", "(", "eval_map", "=", "eval_result", "[", "'MAP'", "]", ",", "pvalue", "=", "eval_result", "[", "'pvalue'", "]", ")", "\n", "langpair2duration", "[", "qlang", "+", "dlang", "]", "=", "str", "(", "eval_result", "[", "\"duration_ms\"", "]", ")", "\n", "\n", "", "ckpt2lp2maps", "[", "checkpoint", "]", "=", "langpair2map", "\n", "ckpt2lp2durations", "[", "checkpoint", "]", "=", "langpair2duration", "\n", "reranker", ".", "model", ".", "cpu", "(", ")", "\n", "\n", "", "task_rf2results", "[", "task_rf", "]", "=", "ckpt2lp2maps", "\n", "task_rf2durations", "[", "task_rf", "]", "=", "ckpt2lp2durations", "\n", "\n", "", "header", "=", "\"\\t\"", ".", "join", "(", "[", "'task_rf'", ",", "'checkpoint'", ",", "'language_config'", "]", "+", "[", "f\"{q}{d}\"", "for", "q", ",", "d", "in", "lang_pairs", "]", ")", "\n", "map_results", "=", "[", "header", "]", "\n", "duration_results", "=", "[", "header", "]", "\n", "for", "task_rf", ",", "ckpt2results", "in", "task_rf2results", ".", "items", "(", ")", ":", "\n", "    ", "for", "ckpt", ",", "lp2map", "in", "ckpt2results", ".", "items", "(", ")", ":", "\n", "      ", "config", "=", "f\"{task_rf}\\t{ckpt}\\tsplit\\t\"", "\n", "map_results", ".", "append", "(", "config", "+", "\"\\t\"", ".", "join", "(", "[", "lp2map", "[", "qlang", "+", "dlang", "]", "for", "qlang", ",", "dlang", "in", "lang_pairs", "]", ")", ")", "\n", "duration_results", ".", "append", "(", "config", "+", "\"\\t\"", ".", "join", "(", "[", "task_rf2durations", "[", "task_rf", "]", "[", "ckpt", "]", "[", "qlang", "+", "dlang", "]", "for", "qlang", ",", "dlang", "in", "lang_pairs", "]", ")", ")", "\n", "\n", "", "", "return", "map_results", ",", "duration_results", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.src.adapter_eval.evaluate_adapter_ablation": [[389, 457], ["name.upper.upper", "logger.info", "logger.info", "adapter_eval._evaluate_adapter_setting", "all_results.extend", "all_durations.extend", "logger.info", "logger.info", "pprint.pformat"], "function", ["home.repos.pwc.inspect_result.rlitschk_modularclir.src.adapter_eval._evaluate_adapter_setting"], ["", "def", "evaluate_adapter_ablation", "(", "selected_configs", ")", ":", "\n", "  ", "base_config", "=", "{", "\n", "# static parameters of best configuration", "\n", "\"setting\"", ":", "\"dlang\"", ",", "\n", "\"skip_layer_configs\"", ":", "[", "None", "]", ",", "\n", "\"task_reduction_factors\"", ":", "[", "'16'", "]", ",", "\n", "\"apply_task_adapter\"", ":", "True", "\n", "}", "\n", "\n", "all_configs", "=", "{", "\n", "\"+RA+LA\"", ":", "{", "\n", "# base setup: invertible and language adapters enabled during training and inference (+RA +LA), same as just applying dlang", "\n", "\"apply_lang_adapter\"", ":", "True", ",", "\n", "\"apply_invertible_adapters\"", ":", "True", ",", "\n", "\"config\"", ":", "\"pfeiffer+inv\"", ",", "\n", "**", "base_config", "\n", "}", ",", "\"+RA-LA-INV\"", ":", "{", "\n", "# turn off language adapters and invertible adapters during inference (+RA -LA -INV)", "\n", "\"apply_lang_adapter\"", ":", "False", ",", "\n", "\"apply_invertible_adapters\"", ":", "False", ",", "\n", "\"config\"", ":", "\"pfeiffer+inv\"", ",", "\n", "**", "base_config", "\n", "}", ",", "\"+RA+LA-INV\"", ":", "{", "\n", "# turn off invertible adapters during inference (+RA +LA -INV)", "\n", "\"apply_lang_adapter\"", ":", "True", ",", "\n", "\"apply_invertible_adapters\"", ":", "False", ",", "\n", "\"config\"", ":", "\"pfeiffer+inv\"", ",", "\n", "**", "base_config", "\n", "}", ",", "\n", "# The following configs are the same as above, but invertible adapters are turned off already during training", "\n", "# \"[pfeiffer]\": {", "\n", "#   # base setup: language adapters enabled during training and inference", "\n", "#   \"apply_lang_adapter\": True,", "\n", "#   \"apply_invertible_adapters\": True,", "\n", "#   \"config\": \"pfeiffer\",", "\n", "#   **base_config", "\n", "# },\"[pfeiffer]-LANG-INV\": {", "\n", "#   # turn off language adapters and invertible adapters during inference and training", "\n", "#   \"apply_lang_adapter\": False,", "\n", "#   \"apply_invertible_adapters\": False,", "\n", "#   \"config\": \"pfeiffer\",", "\n", "#   **base_config", "\n", "# }, \"[pfeiffer]-INV\": {", "\n", "#   # turn off invertible adapters during inference and training", "\n", "#   \"apply_lang_adapter\": True,", "\n", "#   \"apply_invertible_adapters\": False,", "\n", "#   \"config\": \"pfeiffer\",", "\n", "#   **base_config", "\n", "# }", "\n", "}", "\n", "\n", "all_results", "=", "[", "]", "\n", "all_durations", "=", "[", "]", "\n", "for", "name", "in", "selected_configs", ":", "\n", "    ", "name", "=", "name", ".", "upper", "(", ")", "\n", "config", "=", "all_configs", "[", "name", "]", "\n", "logger", ".", "info", "(", "f\"Running {name}\"", ")", "\n", "logger", ".", "info", "(", "pformat", "(", "config", ",", "indent", "=", "4", ")", ")", "\n", "map_results", ",", "duration_results", "=", "_evaluate_adapter_setting", "(", "\n", "eval_adapterdrop", "=", "False", ",", "ablation_config", "=", "config", ",", "name", "=", "name", "\n", ")", "\n", "k", "=", "0", "if", "not", "all_results", "else", "1", "# maybe remove header", "\n", "all_results", ".", "extend", "(", "map_results", "[", "k", ":", "]", ")", "\n", "all_durations", ".", "extend", "(", "duration_results", "[", "k", ":", "]", ")", "\n", "logger", ".", "info", "(", "f\"Done with {name}\"", ")", "\n", "logger", ".", "info", "(", "f\"---------\"", ")", "\n", "\n", "", "return", "all_results", ",", "all_durations", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.src.adapter_eval.evaluate_adapterdrop": [[459, 461], ["adapter_eval._evaluate_adapter_setting"], "function", ["home.repos.pwc.inspect_result.rlitschk_modularclir.src.adapter_eval._evaluate_adapter_setting"], ["", "def", "evaluate_adapterdrop", "(", ")", ":", "\n", "  ", "return", "_evaluate_adapter_setting", "(", "eval_adapterdrop", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.src.adapter_eval.main": [[463, 502], ["bool", "logger.info", "helper.evaluate.print_results", "args.language_configs.pop", "args.language_configs.pop", "adapter_eval.evaluate_adapters", "map_results.extend", "duration_results.extend", "adapter_eval.evaluate_split_adapters", "map_results.extend", "duration_results.extend", "adapter_eval.evaluate_adapter_ablation", "map_results.extend", "duration_results.extend", "c.lower", "ablation_configs.append", "args.language_configs.index", "args.language_configs.index"], "function", ["home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.print_results", "home.repos.pwc.inspect_result.rlitschk_modularclir.src.adapter_eval.evaluate_adapters", "home.repos.pwc.inspect_result.rlitschk_modularclir.src.adapter_eval.evaluate_split_adapters", "home.repos.pwc.inspect_result.rlitschk_modularclir.src.adapter_eval.evaluate_adapter_ablation"], ["", "def", "main", "(", ")", ":", "\n", "  ", "run_qdlang", "=", "'qlang'", "in", "args", ".", "language_configs", "or", "'dlang'", "in", "args", ".", "language_configs", "\n", "\n", "ablation_configs", "=", "[", "]", "\n", "for", "c", "in", "args", ".", "language_configs", ":", "\n", "    ", "if", "c", ".", "lower", "(", ")", "in", "[", "'+ra+la-inv'", ",", "'+ra-la-inv'", "]", ":", "\n", "      ", "ablation_configs", ".", "append", "(", "c", ")", "\n", "", "", "for", "c", "in", "ablation_configs", ":", "\n", "    ", "args", ".", "language_configs", ".", "pop", "(", "args", ".", "language_configs", ".", "index", "(", "c", ")", ")", "\n", "", "run_ablation", "=", "bool", "(", "ablation_configs", ")", "\n", "\n", "run_split", "=", "'split'", "in", "args", ".", "language_configs", "\n", "if", "run_split", ":", "\n", "    ", "args", ".", "language_configs", ".", "pop", "(", "args", ".", "language_configs", ".", "index", "(", "'split'", ")", ")", "\n", "\n", "", "map_results", "=", "[", "]", "\n", "duration_results", "=", "[", "]", "\n", "\n", "if", "run_qdlang", ":", "\n", "    ", "qdlang_map_results", ",", "qdlang_duration_results", "=", "evaluate_adapters", "(", ")", "\n", "k", "=", "0", "if", "not", "map_results", "else", "1", "# maybe remove header", "\n", "map_results", ".", "extend", "(", "qdlang_map_results", "[", "k", ":", "]", ")", "\n", "duration_results", ".", "extend", "(", "qdlang_duration_results", "[", "k", ":", "]", ")", "\n", "\n", "", "if", "run_split", ":", "\n", "    ", "split_map_results", ",", "split_duration_resulst", "=", "evaluate_split_adapters", "(", ")", "\n", "k", "=", "0", "if", "not", "map_results", "else", "1", "# maybe remove header", "\n", "map_results", ".", "extend", "(", "split_map_results", "[", "k", ":", "]", ")", "\n", "duration_results", ".", "extend", "(", "split_duration_resulst", "[", "k", ":", "]", ")", "\n", "\n", "", "if", "run_ablation", ":", "\n", "    ", "ablation_map_results", ",", "ablation_duration_results", "=", "evaluate_adapter_ablation", "(", "ablation_configs", ")", "\n", "k", "=", "0", "if", "not", "map_results", "else", "1", "# maybe remove header", "\n", "map_results", ".", "extend", "(", "ablation_map_results", "[", "k", ":", "]", ")", "\n", "duration_results", ".", "extend", "(", "ablation_duration_results", "[", "k", ":", "]", ")", "\n", "\n", "# evaluate_adapterdrop()", "\n", "", "logger", ".", "info", "(", "\"Done evaluating all configurations\"", ")", "\n", "print_results", "(", "duration_results", ",", "map_results", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.src.sft_eval.evaluate_sfts": [[46, 125], ["helper.evaluate.add_filehandler", "logger.info", "sentence_transformers.CrossEncoder", "helper.evaluate.print_results", "os.path.join", "os.path.join", "sft.SFT", "sft.SFT.apply", "sft.SFT.revert", "map_results.append", "duration_results.append", "logger.info", "helper.config.get_preranker", "os.path.join", "os.path.join", "logger.info", "logger.info", "logger.info", "helper.evaluate.rerank_and_eval", "helper.evaluate.map2str", "str", "logger.info", "map_results.append", "duration_results.append", "str", "os.path.join", "sft.SFT", "sft.SFT.apply", "active_lang_sfts.append", "os.path.join", "sft.SFT", "sft.SFT.apply", "active_lang_sfts.append", "lang_sft.revert", "langpair2map.keys", "langpair2map.keys", "langpair2map.keys"], "function", ["home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.add_filehandler", "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.print_results", "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.config.get_preranker", "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.rerank_and_eval", "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.map2str"], ["def", "evaluate_sfts", "(", ")", ":", "\n", "  ", "add_filehandler", "(", "os", ".", "path", ".", "join", "(", "SFT_DIR", ",", "f\"evaluate_sfts.txt\"", ")", ")", "\n", "logger", ".", "info", "(", "f\"loading precomputed average precision (AP) values for t-test from {REF_PRECISION_VALS_DIR}\"", ")", "\n", "\n", "reranker", "=", "CrossEncoder", "(", "'bert-base-multilingual-uncased'", ",", "max_length", "=", "max_seq_len", ")", "\n", "map_results", "=", "[", "]", "\n", "duration_results", "=", "[", "]", "\n", "for", "task_rf", "in", "task_reduction_factors", ":", "\n", "    ", "for", "steps", "in", "[", "'625k'", "]", ":", "# '100k'", "\n", "\n", "# Apply Ranking Mask (RM)", "\n", "      ", "RM_path", "=", "os", ".", "path", ".", "join", "(", "SFT_DIR", ",", "f\"ir_{steps}/rf_{lang_rf}_{task_rf}/checkpoint-625000\"", ")", "\n", "ranking_sft", "=", "SFT", "(", "RM_path", ")", "\n", "ranking_sft", ".", "apply", "(", "reranker", ".", "model", ",", "with_abs", "=", "True", ")", "\n", "\n", "for", "language_config", "in", "language_configs", ":", "\n", "        ", "langpair2map", "=", "{", "}", "\n", "langpair2duration", "=", "{", "}", "\n", "for", "qlang", ",", "dlang", "in", "lang_pairs", ":", "\n", "          ", "logger", ".", "info", "(", "\"------------------\"", ")", "\n", "active_lang_sfts", "=", "[", "]", "\n", "\n", "# Apply Language Masks (LM)", "\n", "if", "language_config", "==", "'qlang'", "or", "language_config", "==", "'both'", ":", "\n", "# For Swahili/Somali to English ({swc,so}->en) we use FacebookMT to translate queries to EN and prerank", "\n", "# with BM25 ('fbmt+bm25'), so we need to the English SFT here.", "\n", "            ", "sft_language", "=", "qlang", "if", "qlang", "not", "in", "(", "'sw'", ",", "'so'", ")", "else", "'en'", "\n", "LM_path", "=", "os", ".", "path", ".", "join", "(", "SFT_DIR", ",", "f\"mlm/rf_{lang_rf}/{sft_language}\"", ")", "\n", "sft", "=", "SFT", "(", "LM_path", ")", "\n", "sft", ".", "apply", "(", "reranker", ".", "model", ",", "with_abs", "=", "False", ")", "\n", "active_lang_sfts", ".", "append", "(", "sft", ")", "\n", "", "if", "language_config", "==", "'dlang'", "or", "language_config", "==", "'both'", ":", "\n", "            ", "LM_path", "=", "os", ".", "path", ".", "join", "(", "SFT_DIR", ",", "f\"mlm/rf_{lang_rf}/{dlang}\"", ")", "\n", "sft", "=", "SFT", "(", "LM_path", ")", "\n", "sft", ".", "apply", "(", "reranker", ".", "model", ",", "with_abs", "=", "False", ")", "\n", "active_lang_sfts", ".", "append", "(", "sft", ")", "\n", "\n", "", "preranker_model", "=", "get_preranker", "(", "qlang", ",", "dlang", ")", "\n", "load_precision_values_dir", "=", "os", ".", "path", ".", "join", "(", "\n", "REF_PRECISION_VALS_DIR", ",", "f\"{qlang}-{dlang}_preranker={preranker_model}\"", "\n", ")", "\n", "save_precision_values_dir", "=", "os", ".", "path", ".", "join", "(", "\n", "RM_path", ",", "f\"precision_values/{qlang}-{dlang}_setting={language_config}_preranker={preranker_model}\"", "\n", ")", "\n", "\n", "logger", ".", "info", "(", "f\"Load precision values from: {load_precision_values_dir}\"", ")", "\n", "logger", ".", "info", "(", "f\"Save precision values to: {save_precision_values_dir}\"", ")", "\n", "logger", ".", "info", "(", "f\"preranker: {preranker_model}\"", ")", "\n", "eval_result", "=", "rerank_and_eval", "(", "\n", "qlang", "=", "qlang", ",", "\n", "dlang", "=", "dlang", ",", "\n", "reranker", "=", "reranker", ",", "\n", "preranker", "=", "preranker_model", ",", "\n", "prerank_dir", "=", "PRERANK_DIR", ",", "\n", "load_precision_values_dir", "=", "load_precision_values_dir", ",", "\n", "save_precision_values_dir", "=", "save_precision_values_dir", ",", "\n", "path_query_translations", "=", "args", ".", "path_query_translations", "\n", ")", "\n", "langpair2map", "[", "qlang", "+", "dlang", "]", "=", "map2str", "(", "eval_map", "=", "eval_result", "[", "\"MAP\"", "]", ",", "pvalue", "=", "eval_result", "[", "\"pvalue\"", "]", ")", "\n", "langpair2duration", "[", "qlang", "+", "dlang", "]", "=", "str", "(", "eval_result", "[", "\"duration_ms\"", "]", ")", "\n", "logger", ".", "info", "(", "f\"{qlang}->{dlang}\\tsetting: {language_config}\\tsteps: {steps}\\ttask-rf: {task_rf}\\tlang-rf: 2\"", ")", "\n", "\n", "for", "lang_sft", "in", "active_lang_sfts", ":", "\n", "            ", "lang_sft", ".", "revert", "(", "reranker", ".", "model", ")", "\n", "\n", "", "", "if", "not", "map_results", ":", "\n", "          ", "header", "=", "\"task_rf\\tcheckpoint\\tsetting\\t\"", "\n", "map_results", ".", "append", "(", "header", "+", "\"\\t\"", ".", "join", "(", "langpair2map", ".", "keys", "(", ")", ")", ")", "\n", "duration_results", ".", "append", "(", "header", ")", "\n", "\n", "", "desc", "=", "[", "str", "(", "task_rf", ")", ",", "steps", ",", "language_config", "]", "\n", "map_results", ".", "append", "(", "\"\\t\"", ".", "join", "(", "desc", "+", "[", "langpair2map", "[", "lp", "]", "for", "lp", "in", "langpair2map", ".", "keys", "(", ")", "]", ")", ")", "\n", "duration_results", ".", "append", "(", "\"\\t\"", ".", "join", "(", "desc", "+", "[", "langpair2duration", "[", "lp", "]", "for", "lp", "in", "langpair2map", ".", "keys", "(", ")", "]", ")", ")", "\n", "\n", "", "ranking_sft", ".", "revert", "(", "reranker", ".", "model", ")", "\n", "\n", "", "", "print_results", "(", "durations", "=", "duration_results", ",", "map_results", "=", "map_results", ")", "\n", "\n", "return", "map_results", ",", "duration_results", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.src.sft_eval.main": [[127, 129], ["sft_eval.evaluate_sfts"], "function", ["home.repos.pwc.inspect_result.rlitschk_modularclir.src.sft_eval.evaluate_sfts"], ["", "def", "main", "(", ")", ":", "\n", "  ", "evaluate_sfts", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.config.get_preranker": [[10, 17], ["None"], "function", ["None"], ["def", "get_preranker", "(", "qlang", ":", "str", ",", "dlang", ":", "str", ")", ":", "\n", "  ", "if", "qlang", "==", "dlang", ":", "\n", "    ", "return", "preranker_mono", "\n", "", "if", "qlang", "==", "'sw'", "or", "qlang", "==", "'so'", ":", "\n", "    ", "return", "preranker_lowres", "\n", "", "else", ":", "\n", "    ", "return", "preranker_cross", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.config.get_language_pairs": [[38, 47], ["os.path.exists", "len", "os.listdir"], "function", ["None"], ["def", "get_language_pairs", "(", "mode", ":", "str", ",", "path_query_translations", ":", "str", ")", ":", "\n", "  ", "if", "mode", "==", "\"mono\"", ":", "\n", "    ", "return", "monolingual_lang_pairs", "\n", "", "elif", "mode", "==", "\"clir\"", ":", "\n", "    ", "return", "crosslingual_lang_pairs", "\n", "", "else", ":", "\n", "    ", "assert", "os", ".", "path", ".", "exists", "(", "path_query_translations", ")", "\n", "assert", "len", "(", "os", ".", "listdir", "(", "path_query_translations", ")", ")", ">", "0", "\n", "return", "low_res_lang_pairs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.prepare_msmarco.get_corpus": [[29, 47], ["print", "os.path.join", "os.path.exists", "os.path.join", "open", "os.path.exists", "logging.info", "sentence_transformers.util.http_get", "tarfile.open", "tar.extractall", "line.strip().split", "line.strip"], "function", ["None"], ["def", "get_corpus", "(", "cache_dir", ":", "str", ")", "->", "dict", ":", "\n", "#### Read the corpus files, that contain all the passages. Store them in the corpus dict", "\n", "  ", "print", "(", "\"loading corpus\"", ")", "\n", "corpus", "=", "{", "}", "\n", "collection_filepath", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "'collection.tsv'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "collection_filepath", ")", ":", "\n", "    ", "tar_filepath", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "'collection.tar.gz'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "tar_filepath", ")", ":", "\n", "      ", "logging", ".", "info", "(", "\"Download collection.tar.gz\"", ")", "\n", "util", ".", "http_get", "(", "'https://msmarco.blob.core.windows.net/msmarcoranking/collection.tar.gz'", ",", "tar_filepath", ")", "\n", "\n", "", "with", "tarfile", ".", "open", "(", "tar_filepath", ",", "\"r:gz\"", ")", "as", "tar", ":", "\n", "      ", "tar", ".", "extractall", "(", "path", "=", "cache_dir", ")", "\n", "", "", "with", "open", "(", "collection_filepath", ",", "'r'", ",", "encoding", "=", "'utf8'", ")", "as", "fIn", ":", "\n", "    ", "for", "line", "in", "fIn", ":", "\n", "      ", "pid", ",", "passage", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "corpus", "[", "pid", "]", "=", "passage", "\n", "", "", "return", "corpus", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.prepare_msmarco.get_queries": [[49, 70], ["print", "os.path.join", "len", "os.path.exists", "os.path.join", "open", "os.path.exists", "logging.info", "sentence_transformers.util.http_get", "tarfile.open", "tar.extractall", "line.strip().split", "line.strip"], "function", ["None"], ["", "def", "get_queries", "(", "cache_dir", ":", "str", ")", "->", "dict", ":", "\n", "### Read the train queries, store in queries dict (extended by us, we read all queries, needed for cross-check dev-set)", "\n", "  ", "queries", "=", "{", "}", "\n", "for", "split", "in", "[", "\"train\"", ",", "\"dev\"", ",", "\"eval\"", "]", ":", "\n", "    ", "fName", "=", "f'queries.{split}.tsv'", "\n", "print", "(", "f\"loading {fName}\"", ")", "\n", "queries_filepath", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "fName", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "queries_filepath", ")", ":", "\n", "      ", "tar_filepath", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "'queries.tar.gz'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "tar_filepath", ")", ":", "\n", "        ", "logging", ".", "info", "(", "\"Download queries.tar.gz\"", ")", "\n", "util", ".", "http_get", "(", "'https://msmarco.blob.core.windows.net/msmarcoranking/queries.tar.gz'", ",", "tar_filepath", ")", "\n", "\n", "", "with", "tarfile", ".", "open", "(", "tar_filepath", ",", "\"r:gz\"", ")", "as", "tar", ":", "\n", "        ", "tar", ".", "extractall", "(", "path", "=", "cache_dir", ")", "\n", "", "", "with", "open", "(", "queries_filepath", ",", "'r'", ",", "encoding", "=", "'utf8'", ")", "as", "fIn", ":", "\n", "      ", "for", "line", "in", "fIn", ":", "\n", "        ", "qid", ",", "query", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "queries", "[", "qid", "]", "=", "query", "\n", "", "", "", "assert", "len", "(", "queries", ")", "==", "1010916", "\n", "return", "queries", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.prepare_msmarco.get_dev_samples": [[72, 177], ["os.path.join", "print", "os.path.join", "collections.defaultdict", "os.path.join", "os.path.exists", "logging.info", "sentence_transformers.util.http_get", "gzip.open", "tqdm.tqdm", "os.path.exists", "os.path.join", "open", "tqdm.tqdm", "filename.replace", "os.path.exists", "os.path.join", "open", "tqdm.tqdm", "line.strip().split", "os.path.join.replace", "os.path.exists", "logging.info", "sentence_transformers.util.http_get", "tarfile.open", "tar.extractall", "line.strip().split", "qid2relevant_pids[].append", "os.path.exists", "logging.info", "sentence_transformers.util.http_get", "tarfile.open", "tar.extractall", "line.strip().split", "dev_lines.append", "os.path.basename", "[].add", "dev_lines.append", "line.strip", "len", "set", "set", "len", "[].add", "dev_lines.append", "os.path.basename", "line.strip", "os.path.basename", "line.strip"], "function", ["None"], ["", "def", "get_dev_samples", "(", "\n", "queries", ":", "dict", ",", "\n", "corpus", ":", "dict", ",", "\n", "sbert_splits", ":", "bool", ",", "\n", "cache_dir", ":", "str", ",", "\n", ")", "->", "Tuple", "[", "dict", ",", "list", "]", ":", "\n", "  ", "\"\"\"\n  sbert split:\n  - We use 200 random queries from the train set for evaluation during training\n  - Each query has at least one relevant and up to 200 irrelevant (negative) passages\n\n  - msmarco-qidpidtriples.rnd-shuf.train-eval.tsv.gz and msmarco-qidpidtriples.rnd-shuf.train.tsv.gz is a randomly\n  shuffled version of qidpidtriples.train.full.2.tsv.gz from the MS Marco website\n  - We extracted in the train-eval split 500 random queries that can be used for evaluation during training\n\n  Otherwise:\n  - loads full ms-marco dev dataset\n\n  :param queries: query-id to query mapping\n  :param corpus: passage-id to passage mapping\n  :param sbert_splits: whether to use the data prepared by Reimers et al. or original (but much larger) msmarco data\n  :return: mapping dev_samples from each query to all positive/negative passages and jsonl formatted lines\n  \"\"\"", "\n", "\n", "if", "not", "sbert_splits", ":", "\n", "# Load full development set from ms-marco: loads top100dev and qrels.dev.small.tsv (containing only queries from top1000 dev)", "\n", "    ", "download_url", "=", "'https://msmarco.blob.core.windows.net/msmarcoranking/top1000.dev.tar.gz'", "\n", "filename", "=", "'top1000.dev.tar.gz'", "\n", "\n", "# Load labels file: {qrels.dev.small.tsv} inside {collectionandqueries.tar.gz} (7437 lines)", "\n", "qrels_filepath", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "\"qrels.dev.small.tsv\"", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "qrels_filepath", ".", "replace", "(", "\".tar.gz\"", ",", "\"/qrels.dev.small.tsv\"", ")", ")", ":", "\n", "      ", "container_filepath", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "\"collectionandqueries.tar.gz\"", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "container_filepath", ")", ":", "\n", "        ", "logging", ".", "info", "(", "\"Download \"", "+", "os", ".", "path", ".", "basename", "(", "container_filepath", ")", ")", "\n", "util", ".", "http_get", "(", "'https://msmarco.blob.core.windows.net/msmarcoranking/collectionandqueries.tar.gz'", ",", "container_filepath", ")", "\n", "", "with", "tarfile", ".", "open", "(", "container_filepath", ",", "\"r:gz\"", ")", "as", "tar", ":", "\n", "        ", "tar", ".", "extractall", "(", "path", "=", "cache_dir", ")", "\n", "\n", "# Process labels file", "\n", "", "", "qid2relevant_pids", "=", "defaultdict", "(", "list", ")", "\n", "with", "open", "(", "qrels_filepath", ",", "encoding", "=", "\"UTF-8\"", ")", "as", "fIn", ":", "\n", "      ", "for", "line", "in", "tqdm", ".", "tqdm", "(", "fIn", ",", "total", "=", "7437", ",", "desc", "=", "\"Loading relevance labels\"", ")", ":", "\n", "# We ignore columns 1 and 3 as they are there for TREC formating.", "\n", "        ", "qid", ",", "_", ",", "pid", ",", "_", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "assert", "qid", "in", "queries", "# query id should be included in set of all queries", "\n", "assert", "pid", "in", "corpus", "\n", "qid2relevant_pids", "[", "qid", "]", ".", "append", "(", "pid", ")", "\n", "\n", "# load {top1000.dev} (6668967 lines)", "\n", "", "", "top1000dev", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ".", "replace", "(", "\".tar.gz\"", ",", "\"\"", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "top1000dev", ")", ":", "\n", "      ", "train_eval_filepath", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "train_eval_filepath", ")", ":", "\n", "        ", "logging", ".", "info", "(", "\"Download \"", "+", "os", ".", "path", ".", "basename", "(", "train_eval_filepath", ")", ")", "\n", "util", ".", "http_get", "(", "download_url", ",", "train_eval_filepath", ")", "\n", "", "with", "tarfile", ".", "open", "(", "train_eval_filepath", ",", "\"r:gz\"", ")", "as", "tar", ":", "\n", "        ", "tar", ".", "extractall", "(", "path", "=", "cache_dir", ")", "\n", "\n", "", "", "dev_lines", "=", "[", "]", "\n", "dev_samples", "=", "{", "}", "\n", "with", "open", "(", "top1000dev", ",", "encoding", "=", "\"UTF-8\"", ")", "as", "fIn", ":", "\n", "      ", "for", "line", "in", "tqdm", ".", "tqdm", "(", "fIn", ",", "total", "=", "6668967", ",", "desc", "=", "\"Creating .jsonl dev dataset\"", ")", ":", "\n", "        ", "qid", ",", "pid", ",", "query", ",", "passage", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "label", "=", "1", "if", "pid", "in", "qid2relevant_pids", "[", "qid", "]", "else", "0", "\n", "assert", "qid", "in", "queries", "\n", "assert", "pid", "in", "corpus", "\n", "sample", "=", "{", "\"qid\"", ":", "qid", ",", "\"query\"", ":", "query", ",", "\"pid\"", ":", "pid", ",", "\"passage\"", ":", "passage", ",", "\"label\"", ":", "label", "}", "\n", "dev_lines", ".", "append", "(", "sample", ")", "\n", "", "", "return", "dev_samples", ",", "dev_lines", "\n", "\n", "", "else", ":", "\n", "# Take distinct subset of training triples for dev set", "\n", "    ", "num_dev_queries", "=", "200", "\n", "num_max_dev_negatives", "=", "200", "\n", "download_url", "=", "'https://sbert.net/datasets/msmarco-qidpidtriples.rnd-shuf.train-eval.tsv.gz'", "\n", "filename", "=", "'msmarco-qidpidtriples.rnd-shuf.train-eval.tsv.gz'", "\n", "\n", "# train_eval_filepath = os.path.join(cache_dir, filename)", "\n", "", "train_eval_filepath", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "train_eval_filepath", ")", ":", "\n", "    ", "logging", ".", "info", "(", "\"Download \"", "+", "os", ".", "path", ".", "basename", "(", "train_eval_filepath", ")", ")", "\n", "util", ".", "http_get", "(", "download_url", ",", "train_eval_filepath", ")", "\n", "\n", "", "dev_samples", "=", "{", "}", "\n", "dev_lines", "=", "[", "]", "\n", "print", "(", "f\"loading {train_eval_filepath}\"", ")", "\n", "with", "gzip", ".", "open", "(", "train_eval_filepath", ",", "'rt'", ")", "as", "fIn", ":", "\n", "    ", "for", "line", "in", "tqdm", ".", "tqdm", "(", "fIn", ",", "total", "=", "20000000", ")", ":", "\n", "      ", "qid", ",", "pos_id", ",", "neg_id", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "query", "=", "queries", "[", "qid", "]", "\n", "\n", "if", "qid", "not", "in", "dev_samples", "and", "len", "(", "dev_samples", ")", "<", "num_dev_queries", ":", "\n", "        ", "dev_samples", "[", "qid", "]", "=", "{", "'query'", ":", "queries", "[", "qid", "]", ",", "'positive'", ":", "set", "(", ")", ",", "'negative'", ":", "set", "(", ")", "}", "\n", "\n", "", "if", "qid", "in", "dev_samples", ":", "\n", "        ", "positive", "=", "corpus", "[", "pos_id", "]", "\n", "dev_samples", "[", "qid", "]", "[", "'positive'", "]", ".", "add", "(", "positive", ")", "\n", "dev_lines", ".", "append", "(", "{", "\"qid\"", ":", "qid", ",", "\"query\"", ":", "query", ",", "\"pid\"", ":", "pos_id", ",", "\"passage\"", ":", "positive", ",", "\"label\"", ":", "1", "}", ")", "\n", "\n", "if", "len", "(", "dev_samples", "[", "qid", "]", "[", "'negative'", "]", ")", "<", "num_max_dev_negatives", ":", "\n", "          ", "negative", "=", "corpus", "[", "neg_id", "]", "\n", "dev_samples", "[", "qid", "]", "[", "'negative'", "]", ".", "add", "(", "negative", ")", "\n", "dev_lines", ".", "append", "(", "{", "\"qid\"", ":", "qid", ",", "\"query\"", ":", "query", ",", "\"pid\"", ":", "neg_id", ",", "\"passage\"", ":", "negative", ",", "\"label\"", ":", "0", "}", ")", "\n", "", "", "", "", "return", "dev_samples", ",", "dev_lines", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.prepare_msmarco.get_train_samples": [[179, 257], ["os.path.join", "os.path.exists", "logging.info", "sentence_transformers.util.http_get", "gzip.open", "tqdm.tqdm", "line.strip().split", "os.path.basename", "train_samples.append", "train_lines.append", "train_samples.append", "train_samples.append", "train_lines.append", "train_lines.append", "line.strip", "sentence_transformers.InputExample", "sentence_transformers.InputExample", "sentence_transformers.InputExample"], "function", ["None"], ["", "def", "get_train_samples", "(", "\n", "corpus", ":", "dict", ",", "\n", "queries", ":", "dict", ",", "\n", "dev_samples", ":", "dict", ",", "\n", "sbert_splits", ":", "bool", ",", "\n", "cache_dir", ":", "str", ",", "\n", ")", "->", "Tuple", "[", "list", ",", "list", "]", ":", "\n", "  ", "\"\"\"\n  We train the network with as a binary label task\n  Given [query, passage] is the label 0 = irrelevant or 1 = relevant?\n  We use a positive-to-negative ratio: For 1 positive sample (label 1) we include 4 negative samples (label 0)\n  in our training setup. For the negative samples, we use the triplets provided by MS Marco that\n  specify (query, positive sample, negative sample).\n  :param corpus: pid2passage dictionary\n  :param queries: qid2query dictionary\n  :param dev_samples: used to ensure no query overlap between dev and train\n  :param sbert_splits: whether to use the data prepared by Reimers et al. or original (but much larger) msmarco data\n  :param cache_dir: download directory and save directory for cache files\n  :return:\n  \"\"\"", "\n", "train_samples", "=", "[", "]", "\n", "train_lines", "=", "[", "]", "\n", "\n", "if", "sbert_splits", ":", "\n", "# number of negative passages for each positive passage", "\n", "    ", "pos_neg_ratio", "=", "4", "\n", "# Maximal number of training samples we want to use", "\n", "max_train_samples", "=", "2e7", "\n", "train_file", "=", "'msmarco-qidpidtriples.rnd-shuf.train.tsv.gz'", "\n", "base_url", "=", "'https://sbert.net/datasets/msmarco-qidpidtriples.rnd-shuf.train.tsv.gz'", "\n", "total", "=", "None", "\n", "", "else", ":", "\n", "    ", "pos_neg_ratio", "=", "1", "\n", "max_train_samples", "=", "-", "1", "# all", "\n", "train_file", "=", "'qidpidtriples.train.full.2.tsv.gz'", "\n", "base_url", "=", "'https://msmarco.blob.core.windows.net/msmarcoranking/qidpidtriples.train.full.2.tsv.gz'", "\n", "total", "=", "397768673", "\n", "\n", "", "train_filepath", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "train_file", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "train_filepath", ")", ":", "\n", "    ", "logging", ".", "info", "(", "\"Download \"", "+", "os", ".", "path", ".", "basename", "(", "train_filepath", ")", ")", "\n", "util", ".", "http_get", "(", "base_url", ",", "train_filepath", ")", "\n", "\n", "", "cnt", "=", "0", "\n", "with", "gzip", ".", "open", "(", "train_filepath", ",", "'rt'", ")", "as", "fIn", ":", "\n", "    ", "for", "line", "in", "tqdm", ".", "tqdm", "(", "fIn", ",", "unit_scale", "=", "True", ",", "total", "=", "total", ")", ":", "\n", "      ", "qid", ",", "pos_id", ",", "neg_id", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "assert", "pos_id", "in", "corpus", "\n", "assert", "neg_id", "in", "corpus", "\n", "assert", "qid", "in", "corpus", "\n", "\n", "if", "qid", "in", "dev_samples", ":", "\n", "        ", "continue", "\n", "\n", "", "query", "=", "queries", "[", "qid", "]", "\n", "\n", "if", "sbert_splits", ":", "\n", "        ", "if", "(", "cnt", "%", "(", "pos_neg_ratio", "+", "1", ")", ")", "==", "0", ":", "\n", "          ", "passage", "=", "corpus", "[", "pos_id", "]", "\n", "label", "=", "1", "\n", "pid", "=", "pos_id", "\n", "", "else", ":", "\n", "          ", "passage", "=", "corpus", "[", "neg_id", "]", "\n", "label", "=", "0", "\n", "pid", "=", "neg_id", "\n", "\n", "", "train_samples", ".", "append", "(", "InputExample", "(", "texts", "=", "[", "query", ",", "passage", "]", ",", "label", "=", "label", ")", ")", "\n", "train_lines", ".", "append", "(", "{", "\"qid\"", ":", "qid", ",", "\"query\"", ":", "query", ",", "\"pid\"", ":", "pid", ",", "\"passage\"", ":", "passage", ",", "\"label\"", ":", "label", "}", ")", "\n", "cnt", "+=", "1", "\n", "if", "cnt", ">=", "max_train_samples", ":", "\n", "          ", "break", "\n", "", "", "else", ":", "\n", "        ", "train_samples", ".", "append", "(", "InputExample", "(", "texts", "=", "[", "query", ",", "corpus", "[", "pos_id", "]", "]", ",", "label", "=", "1", ")", ")", "\n", "train_samples", ".", "append", "(", "InputExample", "(", "texts", "=", "[", "query", ",", "corpus", "[", "neg_id", "]", "]", ",", "label", "=", "0", ")", ")", "\n", "train_lines", ".", "append", "(", "{", "\"qid\"", ":", "qid", ",", "\"query\"", ":", "query", ",", "\"pid\"", ":", "pos_id", ",", "\"passage\"", ":", "corpus", "[", "pos_id", "]", ",", "\"label\"", ":", "1", "}", ")", "\n", "train_lines", ".", "append", "(", "{", "\"qid\"", ":", "qid", ",", "\"query\"", ":", "query", ",", "\"pid\"", ":", "neg_id", ",", "\"passage\"", ":", "corpus", "[", "neg_id", "]", ",", "\"label\"", ":", "0", "}", ")", "\n", "\n", "", "", "", "return", "train_samples", ",", "train_lines", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.prepare_msmarco.maybe_save_jsonl": [[259, 267], ["os.path.split", "os.makedirs", "print", "os.path.exists", "open", "tqdm.tqdm", "f.write", "json.dumps"], "function", ["None"], ["", "def", "maybe_save_jsonl", "(", "lines", ":", "List", "[", "dict", "]", ",", "filepath", ":", "str", ",", "overwrite", ":", "bool", "=", "False", ")", ":", "\n", "  ", "if", "not", "os", ".", "path", ".", "exists", "(", "filepath", ")", "or", "overwrite", ":", "\n", "    ", "path", ",", "file", "=", "os", ".", "path", ".", "split", "(", "filepath", ")", "\n", "os", ".", "makedirs", "(", "path", ",", "exist_ok", "=", "True", ")", "\n", "print", "(", "f\"Saving {filepath}\"", ")", "\n", "with", "open", "(", "filepath", ",", "\"w\"", ")", "as", "f", ":", "\n", "      ", "for", "line", "in", "tqdm", ".", "tqdm", "(", "lines", ")", ":", "\n", "        ", "f", ".", "write", "(", "json", ".", "dumps", "(", "line", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.prepare_msmarco.main": [[269, 286], ["os.makedirs", "prepare_msmarco.get_corpus", "prepare_msmarco.get_queries", "prepare_msmarco.get_dev_samples", "prepare_msmarco.maybe_save_jsonl", "prepare_msmarco.get_train_samples", "prepare_msmarco.maybe_save_jsonl"], "function", ["home.repos.pwc.inspect_result.rlitschk_modularclir.helper.prepare_msmarco.get_corpus", "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.prepare_msmarco.get_queries", "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.prepare_msmarco.get_dev_samples", "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.prepare_msmarco.maybe_save_jsonl", "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.prepare_msmarco.get_train_samples", "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.prepare_msmarco.maybe_save_jsonl"], ["", "", "", "", "def", "main", "(", ")", ":", "\n", "  ", "sbert_spits", "=", "True", "\n", "cache_dir", "=", "\"data/ms-marco/\"", "\n", "target_folder", "=", "args", ".", "output_dir", "\n", "os", ".", "makedirs", "(", "target_folder", ",", "exist_ok", "=", "True", ")", "\n", "\n", "# load ms-marco", "\n", "corpus", "=", "get_corpus", "(", "cache_dir", ")", "\n", "queries", "=", "get_queries", "(", "cache_dir", ")", "\n", "\n", "# load dev split from reimers", "\n", "dev_samples", ",", "dev_lines", "=", "get_dev_samples", "(", "queries", ",", "corpus", ",", "sbert_spits", ",", "cache_dir", ")", "\n", "maybe_save_jsonl", "(", "dev_lines", ",", "target_folder", "+", "\"dev%s.jsonl\"", "%", "(", "\"_sbert\"", "if", "sbert_spits", "else", "\"\"", ")", ")", "\n", "\n", "# load train split from reimers", "\n", "train_samples", ",", "train_lines", "=", "get_train_samples", "(", "corpus", ",", "queries", ",", "dev_samples", ",", "sbert_spits", ",", "cache_dir", ")", "\n", "maybe_save_jsonl", "(", "train_lines", ",", "target_folder", "+", "\"train%s.jsonl\"", "%", "(", "\"_sbert\"", "if", "sbert_spits", "else", "\"\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.map2str": [[38, 50], ["str", "round", "len"], "function", ["None"], ["def", "map2str", "(", "eval_map", ":", "float", ",", "pvalue", ":", "float", ")", ":", "\n", "  ", "\"\"\"\n  Uniform string formatting of mean average precision (MAP) with significance marker (*) if MAP is significant.\n  :param eval_map: mean average precision value\n  :param pvalue: \n  :return: \n  \"\"\"", "\n", "map_str", "=", "str", "(", "round", "(", "eval_map", ",", "map_ndigits", ")", ")", "\n", "n_zeros", "=", "(", "map_ndigits", "+", "2", ")", "-", "len", "(", "map_str", ")", "\n", "significance_marker", "=", "\"*\"", "if", "0", "<", "pvalue", "<=", "0.05", "else", "\"\"", "\n", "map_str", "=", "map_str", "+", "n_zeros", "*", "'0'", "+", "significance_marker", "\n", "return", "map_str", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.add_filehandler": [[52, 55], ["None"], "function", ["None"], ["", "def", "add_filehandler", "(", "logfile", ":", "str", ")", ":", "\n", "  ", "\"\"\"Utility function for logging\"\"\"", "\n", "pass", "\n", "# fh = logging.FileHandler(filename=logfile)", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate._get_rerank_dir": [[61, 98], ["preranking_model.lower.lower", "logger.info", "os.path.join", "os.path.exists", "os.path.join", "len", "os.path.join", "os.listdir", "os.path.join", "os.path.join", "os.path.join", "os.path.join"], "function", ["None"], ["", "def", "_get_rerank_dir", "(", "dlang", ":", "str", ",", "preranking_model", ":", "str", ",", "qlang", ":", "str", ",", "prerank_dir", ":", "str", ")", ":", "\n", "  ", "\"\"\"\n  Monolingual:\n  bm25, fbmt+bm25, marianmt+bm25, unigram, fasttext\n  \n  Cross-lingual: \n  procb, distil_mbert, ...\n  \n  Given a model (@model) and language pair of query language (@qlang) and document language (@dlang) return directory\n  containing all ranking files. Each ranking file corresponds to a list of document ids for a given query id (=filename). \n  \"\"\"", "\n", "preranking_model", "=", "preranking_model", ".", "lower", "(", ")", "\n", "\n", "# Path to monolingual ranking files (or those that have been translated)", "\n", "if", "preranking_model", "==", "\"bm25\"", ":", "\n", "    ", "assert", "qlang", "==", "dlang", "\n", "rerank_dir", "=", "os", ".", "path", ".", "join", "(", "prerank_dir", ",", "\"mono/bm25/%s-%s/\"", "%", "(", "qlang", ",", "dlang", ")", ")", "\n", "", "elif", "preranking_model", "==", "\"fbmt+bm25\"", ":", "\n", "    ", "rerank_dir", "=", "os", ".", "path", ".", "join", "(", "prerank_dir", ",", "\"xling/fbnm25+bm25/%s-%s/\"", "%", "(", "qlang", ",", "dlang", ")", ")", "\n", "", "elif", "preranking_model", "==", "\"marianmt+bm25\"", ":", "\n", "    ", "rerank_dir", "=", "os", ".", "path", ".", "join", "(", "prerank_dir", ",", "\"mono/MarianMT+bm25/%s-%s/\"", "%", "(", "qlang", ",", "dlang", ")", ")", "\n", "", "elif", "preranking_model", "==", "\"unigram\"", ":", "\n", "    ", "assert", "qlang", "==", "dlang", "\n", "rerank_dir", "=", "os", ".", "path", ".", "join", "(", "prerank_dir", ",", "\"mono/qlm/%s-%s/\"", "%", "(", "qlang", ",", "dlang", ")", ")", "\n", "", "elif", "preranking_model", "==", "\"fasttext\"", ":", "\n", "    ", "assert", "qlang", "==", "dlang", "\n", "rerank_dir", "=", "os", ".", "path", ".", "join", "(", "prerank_dir", ",", "\"mono/fasttext/IDF-SUM/%s-%s/raw/\"", "%", "(", "qlang", ",", "dlang", ")", ")", "\n", "\n", "# Path to cross-lingual ranking files", "\n", "", "elif", "preranking_model", "==", "\"procb\"", ":", "\n", "    ", "rerank_dir", "=", "os", ".", "path", ".", "join", "(", "prerank_dir", ",", "\"xling/clwe/procb/IDF-SUM/%s-%s/\"", "%", "(", "qlang", ",", "dlang", ")", ")", "\n", "", "else", ":", "\n", "    ", "rerank_dir", "=", "os", ".", "path", ".", "join", "(", "prerank_dir", ",", "\"xling/%s/%s-%s/\"", "%", "(", "preranking_model", ",", "qlang", ",", "dlang", ")", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"rerank dir: %s\"", "%", "rerank_dir", ")", "\n", "assert", "os", ".", "path", ".", "exists", "(", "rerank_dir", ")", "and", "len", "(", "os", ".", "listdir", "(", "rerank_dir", ")", ")", ">", "0", ",", "f\"Directory empty or does not exist: {rerank_dir}\"", "\n", "return", "rerank_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.mean_avg_precision": [[100, 172], ["query2ranking.items", "kwargs.get", "float", "os.path.exists", "print", "os.makedirs", "query2ranking.items", "numpy.mean", "os.makedirs", "os.path.join", "[].tolist", "enumerate", "all_precisions.extend", "numpy.array", "open", "f.writelines", "open", "scipy.stats.ttest_ind", "precisions.append", "len", "print", "numpy.mean", "average_precision_values.append", "open", "f.writelines", "os.path.join", "float", "type", "ranked_docs.tolist.tolist", "line.strip", "f.readlines", "numpy.where", "str", "str", "str"], "function", ["None"], ["", "def", "mean_avg_precision", "(", "\n", "query2ranking", ":", "Dict", "[", "Q_ID", ",", "Union", "[", "List", "[", "DOC_ID", "]", ",", "np", ".", "array", "]", "]", ",", "\n", "relass", ":", "Dict", "[", "Q_ID", ",", "List", "[", "DOC_ID", "]", "]", ",", "\n", "**", "kwargs", "\n", ")", "->", "MAP_PVAL", ":", "\n", "  ", "\"\"\"\n  Evaluates results for queries in terms of Mean Average Precision (MAP). Evaluation gold standard is\n  loaded from the relevance assessments.\n\n  :param query2ranking: (actual) ranking for each query\n  :param relass: gold standard (expected) ranking for each query\n  :return: tuple(MAP, p-value)\n  \"\"\"", "\n", "# collect AP values for MAP", "\n", "average_precision_values", "=", "[", "]", "\n", "\n", "# collect all precision values for significance test", "\n", "all_precisions", "=", "[", "]", "\n", "\n", "for", "query_id", ",", "ranking", "in", "query2ranking", ".", "items", "(", ")", ":", "\n", "    ", "if", "query_id", "in", "relass", ":", "# len(relevant_docs) > 0:", "\n", "      ", "relevant_docs", "=", "relass", "[", "query_id", "]", "\n", "\n", "# get ranking for j'th query", "\n", "is_relevant", "=", "[", "document", "in", "relevant_docs", "for", "document", "in", "ranking", "]", "\n", "ranks_of_relevant_docs", "=", "np", ".", "where", "(", "is_relevant", ")", "[", "0", "]", ".", "tolist", "(", ")", "\n", "\n", "precisions", "=", "[", "]", "\n", "# +1 because of mismatch betw. one based rank and zero based indexing", "\n", "for", "k", ",", "rank", "in", "enumerate", "(", "ranks_of_relevant_docs", ",", "1", ")", ":", "\n", "        ", "precision_at_k", "=", "k", "/", "(", "rank", "+", "1", ")", "\n", "precisions", ".", "append", "(", "precision_at_k", ")", "\n", "", "all_precisions", ".", "extend", "(", "precisions", ")", "\n", "\n", "if", "len", "(", "precisions", ")", "==", "0", ":", "\n", "        ", "print", "(", "\"Warning: query %s without relevant documents in corpus: %s (skipped)\"", "%", "(", "query_id", ",", "relevant_docs", ")", ")", "\n", "", "else", ":", "\n", "        ", "ap", "=", "np", ".", "mean", "(", "precisions", ")", "\n", "average_precision_values", ".", "append", "(", "ap", ")", "\n", "\n", "", "", "", "save_rankings_dir", "=", "kwargs", ".", "get", "(", "'save_rankings_dir'", ",", "None", ")", "\n", "if", "save_rankings_dir", ":", "\n", "    ", "print", "(", "\"saving rankings to %s\"", "%", "save_rankings_dir", ")", "\n", "os", ".", "makedirs", "(", "save_rankings_dir", ",", "exist_ok", "=", "True", ")", "\n", "for", "qid", ",", "ranked_docs", "in", "query2ranking", ".", "items", "(", ")", ":", "\n", "      ", "with", "open", "(", "save_rankings_dir", "+", "str", "(", "qid", ")", "+", "\".tsv\"", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "if", "type", "(", "ranked_docs", ")", "!=", "list", ":", "\n", "          ", "ranked_docs", "=", "ranked_docs", ".", "tolist", "(", ")", "\n", "", "f", ".", "writelines", "(", "[", "str", "(", "did_or_score", ")", "+", "\"\\n\"", "for", "did_or_score", "in", "ranked_docs", "]", ")", "\n", "\n", "", "", "", "mean_average_precision", "=", "float", "(", "np", ".", "mean", "(", "np", ".", "array", "(", "average_precision_values", ")", ")", ")", "\n", "\n", "if", "'save_precision_values_dir'", "in", "kwargs", ":", "\n", "    ", "tgt_dir", "=", "kwargs", "[", "'save_precision_values_dir'", "]", "\n", "os", ".", "makedirs", "(", "tgt_dir", ",", "exist_ok", "=", "True", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "tgt_dir", ",", "\"precision_values.txt\"", ")", ",", "'w'", ")", "as", "f", ":", "\n", "      ", "f", ".", "writelines", "(", "[", "str", "(", "pvalue", ")", "+", "\"\\n\"", "for", "pvalue", "in", "all_precisions", "]", ")", "\n", "\n", "# Significance test ", "\n", "# against reference model (proc-B)", "\n", "", "", "file", "=", "\"\"", "\n", "if", "'load_precision_values_dir'", "in", "kwargs", ":", "\n", "    ", "file", "=", "os", ".", "path", ".", "join", "(", "kwargs", "[", "'load_precision_values_dir'", "]", ",", "\"precision_values.txt\"", ")", "\n", "\n", "# run signifance test", "\n", "", "pvalue", "=", "-", "1.0", "\n", "if", "os", ".", "path", ".", "exists", "(", "file", ")", ":", "\n", "    ", "with", "open", "(", "file", ",", "\"r\"", ")", "as", "f", ":", "\n", "      ", "reference_precision_values", "=", "[", "float", "(", "line", ".", "strip", "(", ")", ")", "for", "line", "in", "f", ".", "readlines", "(", ")", "]", "\n", "", "pvalue", "=", "ttest_ind", "(", "reference_precision_values", ",", "all_precisions", ")", "[", "1", "]", "\n", "\n", "", "return", "mean_average_precision", ",", "pvalue", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.print_results": [[174, 188], ["logger.info", "logger.info", "print", "print", "logger.info", "logger.info", "print", "print"], "function", ["None"], ["", "def", "print_results", "(", "durations", ":", "List", "[", "str", "]", ",", "map_results", ":", "List", "[", "str", "]", ")", ":", "\n", "  ", "\"\"\"Utility function for logging\"\"\"", "\n", "logger", ".", "info", "(", "\"------------- Mean Average Precision -------------\"", ")", "\n", "for", "line", "in", "map_results", ":", "\n", "    ", "logger", ".", "info", "(", "line", ")", "\n", "", "logger", ".", "info", "(", "\"------------- Query Latency (ms) -------------\"", ")", "\n", "for", "line", "in", "durations", ":", "\n", "    ", "logger", ".", "info", "(", "line", ")", "\n", "", "print", "(", "\"\\n------------- Mean Average Precision -------------\"", ")", "\n", "for", "line", "in", "map_results", ":", "\n", "    ", "print", "(", "line", ")", "\n", "", "print", "(", "\"\\n------------- Query Latency (ms) -------------\"", ")", "\n", "for", "line", "in", "durations", ":", "\n", "    ", "print", "(", "line", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.rerank_and_eval": [[190, 302], ["round", "evaluate.mean_avg_precision", "logger.info", "kwargs.get", "evaluate._get_rerank_dir", "clef_dataloader.load_clef_rerank", "evaluate._get_rerank_dir", "clef_dataloader.load_clef_rerank", "torch.no_grad", "tqdm.tqdm", "sum", "len", "preranker.split", "open", "pickle.load", "int", "zip", "evaluate.mean_avg_precision", "os.path.join", "len", "kwargs.get", "time.perf_counter_ns", "sorted", "durations.append", "str", "[].index", "ac.Stack", "docs_batch.append", "scored_doc_ids.append", "time.perf_counter_ns", "ac.Split", "doc_scores.extend", "len", "zip", "len", "len", "[].tolist", "reranker.predict().tolist", "doc_ids.index", "len", "reranker.tokenizer", "reranker.predict", "reranker.predict", "range"], "function", ["home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.mean_avg_precision", "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate._get_rerank_dir", "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate._get_rerank_dir", "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.evaluate.mean_avg_precision"], ["", "", "def", "rerank_and_eval", "(", "\n", "qlang", ":", "str", ",", "\n", "dlang", ":", "str", ",", "\n", "reranker", ":", "CrossEncoder", ",", "\n", "preranker", ":", "str", ",", "\n", "prerank_dir", ":", "str", ",", "\n", "eval_preranker", ":", "bool", "=", "True", ",", "\n", "**", "kwargs", "\n", ")", "->", "Dict", ":", "\n", "  ", "\"\"\"\n  Re-rank and evaluate:\n  1. Load for all queries pre-ranking files (qid2topk_rerank, {query-id: [all document-ids ranked])\n  2. Load only documents that need to be scored by the re-ranker (documents in the top-k results lists for any query)\n  3. Re-rank the top-k documents for each query (scored_doc_ids, doc_scores)\n  4. Concatenate re-ranked top-k results with all other documents (qid2reranked, {query-id: [top-k reranked] + [all document-ids ranked][top-k:])\n  5. Evaluate pre-ranking (refMAP) and re-ranking (MAP).\n  \n  result = {\n    \"MAP\": performance of re-ranker, \n    \"refMAP\": performance of pre-ranker, \n    \"duration_ms\": milliseconds/query, \n    \"pvalue\": pvalue\n  }\n  \n  :param qlang: query language\n  :param dlang: document language\n  :param reranker: instance of @CrossEncoder (sentence-transformers library), used to re-rank initial ranking of :param preranker\n  :param preranker: any of the following: bm25, fbmt+bm25, marianmt+bm25, unigram, procb, distil_mbert\n  :param prerank_dir: Directory containing prerankings for all language pairs and models, each file corresponds to the full ranking of a single query\n  :param eval_preranker: additionally compute Mean Average Precision of pre-ranker only (\"refMAP\")\n  :param kwargs: (optional) save_precision_values_dir: str, 'load_precision_values_dir: str, 'run_split_adapters: bool \n  :return: results summary in terms of efficiency (query latency, milliseconds) and effectiveness (MAP)\n  \"\"\"", "\n", "if", "qlang", "in", "[", "\"sw\"", ",", "\"so\"", "]", ":", "\n", "    ", "path_query_translations", "=", "kwargs", ".", "get", "(", "\"path_query_translations\"", ",", "None", ")", "\n", "assert", "path_query_translations", ",", "\"Path for query translation files not specified, run bm25_eval.py first.\"", "\n", "rerank_dir", "=", "_get_rerank_dir", "(", "dlang", ",", "preranker", ",", "qlang", ",", "prerank_dir", ")", "\n", "doc_ids", ",", "documents", ",", "_", ",", "_", ",", "relass", ",", "qid2topk_rerank", "=", "load_clef_rerank", "(", "\n", "qlang", "=", "\"en\"", ",", "dlang", "=", "dlang", ",", "rerank_dir", "=", "rerank_dir", ",", "topk", "=", "topk", ")", "\n", "\n", "mt_system", "=", "preranker", ".", "split", "(", "\"+\"", ")", "[", "0", "]", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "path_query_translations", ",", "f\"{qlang}_to_en_translated_queries_{mt_system}.pkl\"", ")", ",", "\"rb\"", ")", "as", "f", ":", "\n", "      ", "query_ids", ",", "queries", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "query_ids", "=", "[", "int", "(", "_id", ")", "for", "_id", "in", "query_ids", "]", "\n", "\n", "", "else", ":", "\n", "    ", "rerank_dir", "=", "_get_rerank_dir", "(", "dlang", ",", "preranker", ",", "qlang", ",", "prerank_dir", ")", "\n", "doc_ids", ",", "documents", ",", "queries", ",", "query_ids", ",", "relass", ",", "qid2topk_rerank", "=", "load_clef_rerank", "(", "\n", "qlang", "=", "qlang", ",", "dlang", "=", "dlang", ",", "rerank_dir", "=", "rerank_dir", ",", "topk", "=", "topk", ")", "\n", "\n", "", "qid2reranked", "=", "{", "}", "\n", "durations", "=", "[", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "    ", "for", "qid", ",", "query", "in", "tqdm", "(", "zip", "(", "query_ids", ",", "queries", ")", ",", "total", "=", "len", "(", "queries", ")", ")", ":", "\n", "      ", "if", "qid", "in", "relass", ":", "\n", "        ", "doc_scores", "=", "[", "]", "\n", "scored_doc_ids", "=", "[", "]", "\n", "\n", "if", "kwargs", ".", "get", "(", "\"run_split_adapters\"", ",", "False", ")", ":", "\n", "          ", "import", "transformers", ".", "adapters", ".", "composition", "as", "ac", "\n", "task_adapter_name", "=", "kwargs", "[", "\"task_adapter_name\"", "]", "\n", "# determine split index IDX by feeding a dummy document, all tokens before IDX are fed to the query language ", "\n", "# adapter, all tokens after IDX are fed to the document language adapter", "\n", "split_index", "=", "reranker", ".", "tokenizer", "(", "\n", "[", "[", "query", ",", "\"Dummy document \"", "*", "50", "]", "for", "_", "in", "range", "(", "10", ")", "]", ",", "padding", "=", "True", ",", "truncation", "=", "'longest_first'", ",", "max_length", "=", "512", "\n", ")", "[", "\"token_type_ids\"", "]", "[", "0", "]", ".", "index", "(", "1", ")", "\n", "reranker", ".", "model", ".", "active_adapters", "=", "ac", ".", "Stack", "(", "ac", ".", "Split", "(", "qlang", ",", "dlang", ",", "split_index", "=", "split_index", ")", ",", "task_adapter_name", ")", "\n", "\n", "", "start", "=", "time", ".", "perf_counter_ns", "(", ")", "\n", "\n", "docs_batch", "=", "[", "]", "\n", "for", "did", "in", "qid2topk_rerank", "[", "qid", "]", ":", "\n", "          ", "docs_batch", ".", "append", "(", "documents", "[", "doc_ids", ".", "index", "(", "did", ")", "]", ")", "\n", "scored_doc_ids", ".", "append", "(", "did", ")", "\n", "if", "len", "(", "docs_batch", ")", "%", "batch_size", "==", "0", "and", "len", "(", "docs_batch", ")", ">", "0", ":", "\n", "            ", "cross_inp", "=", "[", "[", "query", ",", "doc", "]", "for", "doc", "in", "docs_batch", "]", "\n", "\n", "# Cross-Encoder that predict more than 1 score, we use the last and apply softmax", "\n", "if", "reranker", ".", "config", ".", "num_labels", ">", "1", ":", "\n", "              ", "scores", "=", "reranker", ".", "predict", "(", "cross_inp", ",", "apply_softmax", "=", "True", ")", "[", ":", ",", "1", "]", ".", "tolist", "(", ")", "\n", "", "else", ":", "\n", "              ", "scores", "=", "reranker", ".", "predict", "(", "cross_inp", ")", ".", "tolist", "(", ")", "\n", "\n", "", "doc_scores", ".", "extend", "(", "scores", ")", "\n", "docs_batch", "=", "[", "]", "\n", "\n", "", "if", "len", "(", "scored_doc_ids", ")", "==", "topk", ":", "\n", "            ", "break", "\n", "\n", "", "", "tmp_ranking", "=", "[", "(", "tmpdid", ",", "score", ")", "for", "tmpdid", ",", "score", "in", "zip", "(", "scored_doc_ids", ",", "doc_scores", ")", "]", "\n", "tmp_ranking", "=", "sorted", "(", "tmp_ranking", ",", "key", "=", "lambda", "elem", ":", "-", "elem", "[", "1", "]", ")", "\n", "ranking", "=", "[", "elem", "[", "0", "]", "for", "elem", "in", "tmp_ranking", "]", "+", "qid2topk_rerank", "[", "qid", "]", "[", "topk", ":", "]", "\n", "\n", "duration", "=", "time", ".", "perf_counter_ns", "(", ")", "-", "start", "\n", "durations", ".", "append", "(", "(", "duration", ",", "len", "(", "cross_inp", ")", ")", ")", "\n", "qid2reranked", "[", "qid", "]", "=", "ranking", "\n", "\n", "# Compute query latency", "\n", "# Skip first call of .predict() as it takes much longer than usual (not representative)", "\n", "", "", "", "durations", "=", "durations", "[", "1", ":", "]", "\n", "nanoseconds_per_query", "=", "sum", "(", "[", "duration", "/", "size", "for", "duration", ",", "size", "in", "durations", "]", ")", "/", "len", "(", "durations", ")", "\n", "milliseconds_per_query", "=", "round", "(", "nanoseconds_per_query", "/", "1000000", ",", "2", ")", "\n", "\n", "# Compute statistical signifance", "\n", "MAP", ",", "pvalue", "=", "mean_avg_precision", "(", "qid2reranked", ",", "relass", ",", "**", "kwargs", ")", "\n", "refMAP", "=", "mean_avg_precision", "(", "qid2topk_rerank", ",", "relass", ")", "[", "0", "]", "if", "eval_preranker", "else", "-", "1", "\n", "logger", ".", "info", "(", "f\"{qlang}->{dlang}\\t\"", "\n", "f\"reranker (MAP): {str(MAP)}\\t\"", "\n", "f\"preranker (MAP): {refMAP}\\t\"", "\n", "f\"duration/query (ms): {milliseconds_per_query}\\t\"", "\n", "f\"pvalue: {pvalue}\"", ")", "\n", "return", "{", "\"MAP\"", ":", "MAP", ",", "\"refMAP\"", ":", "refMAP", ",", "\"duration_ms\"", ":", "milliseconds_per_query", ",", "\"pvalue\"", ":", "pvalue", "}", "\n", "", ""]], "home.repos.pwc.inspect_result.rlitschk_modularclir.helper.prepare_clef.serialize_jsonl_files": [[14, 31], ["os.path.join", "os.makedirs", "print", "clef_dataloader.load_documents", "print", "os.path.join", "print", "json.dumps", "zip", "open", "f.writelines", "codecs.open", "f.writelines"], "function", ["None"], ["def", "serialize_jsonl_files", "(", "year", ",", "tgt_dir", ")", ":", "\n", "  ", "for", "lang", "in", "[", "\"en\"", ",", "\"de\"", ",", "\"it\"", ",", "\"fi\"", ",", "\"ru\"", "]", ":", "\n", "    ", "lang_dir", "=", "os", ".", "path", ".", "join", "(", "tgt_dir", ",", "lang", ")", "\n", "os", ".", "makedirs", "(", "lang_dir", ",", "exist_ok", "=", "True", ")", "\n", "print", "(", "f\"indexing {lang} ({lang_dir})\"", ")", "\n", "doc_ids", ",", "documents", "=", "load_documents", "(", "language", "=", "lang", ",", "year", "=", "year", ")", "\n", "print", "(", "\"documents loaded\"", ")", "\n", "jsonl", "=", "[", "json", ".", "dumps", "(", "{", "\"id\"", ":", "_id", ",", "\"contents\"", ":", "doc", "}", ",", "ensure_ascii", "=", "False", ")", "+", "\"\\n\"", "for", "_id", ",", "doc", "in", "zip", "(", "doc_ids", ",", "documents", ")", "]", "\n", "tgt_file", "=", "os", ".", "path", ".", "join", "(", "lang_dir", ",", "f\"corpus_CLEF{year}_{lang}.jsonl\"", ")", "\n", "if", "lang", "!=", "\"ru\"", ":", "\n", "      ", "with", "open", "(", "tgt_file", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "writelines", "(", "jsonl", ")", "\n", "", "", "else", ":", "\n", "# encoding = 'UTF-8' if \"russian\" == lang else 'ISO-8859-1'", "\n", "      ", "with", "codecs", ".", "open", "(", "tgt_file", ",", "encoding", "=", "'UTF-8'", ",", "mode", "=", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "writelines", "(", "jsonl", ")", "\n", "", "", "print", "(", "f\"done\"", ")", "\n", "\n"]]}