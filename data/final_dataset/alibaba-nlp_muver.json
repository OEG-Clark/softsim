{"home.repos.pwc.inspect_result.alibaba-nlp_muver.utils.tools.grid_search_hyperparamters": [[9, 23], ["args.__dict__.items", "itertools.product", "copy.deepcopy", "zip", "new_args.append", "isinstance", "search_key.append", "search_value.append", "copy.deepcopy.__setattr__"], "function", ["None"], ["def", "grid_search_hyperparamters", "(", "args", ")", ":", "\n", "    ", "search_key", ",", "search_value", "=", "[", "]", ",", "[", "]", "\n", "for", "key", ",", "value", "in", "args", ".", "__dict__", ".", "items", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "value", ",", "list", ")", "and", "key", "!=", "'test_set'", ":", "\n", "            ", "search_key", ".", "append", "(", "key", ")", "\n", "search_value", ".", "append", "(", "value", ")", "\n", "\n", "", "", "new_args", "=", "[", "]", "\n", "for", "one_search_value", "in", "product", "(", "*", "search_value", ")", ":", "\n", "        ", "arg", "=", "deepcopy", "(", "args", ")", "\n", "for", "key", ",", "value", "in", "zip", "(", "search_key", ",", "one_search_value", ")", ":", "\n", "            ", "arg", ".", "__setattr__", "(", "key", ",", "value", ")", "\n", "", "new_args", ".", "append", "(", "arg", ")", "\n", "", "return", "new_args", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.utils.tools.set_random_seed": [[24, 30], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.cuda.manual_seed_all"], "function", ["None"], ["", "def", "set_random_seed", "(", "seed", ")", ":", "\n", "# Set Random Seed", "\n", "    ", "random", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "seed", ")", "", "", ""]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.utils.multigpu.GatherLayer.forward": [[18, 24], ["ctx.save_for_backward", "torch.all_gather", "torch.all_gather", "tuple", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "range", "torch.get_world_size", "torch.get_world_size"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "input", ")", ":", "\n", "        ", "ctx", ".", "save_for_backward", "(", "input", ")", "\n", "output", "=", "[", "torch", ".", "zeros_like", "(", "input", ")", "for", "_", "in", "range", "(", "dist", ".", "get_world_size", "(", ")", ")", "]", "\n", "dist", ".", "all_gather", "(", "output", ",", "input", ")", "\n", "return", "tuple", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.utils.multigpu.GatherLayer.backward": [[25, 31], ["torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.get_rank", "torch.get_rank"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "*", "grads", ")", ":", "\n", "        ", "(", "input", ",", ")", "=", "ctx", ".", "saved_tensors", "\n", "grad_out", "=", "torch", ".", "zeros_like", "(", "input", ")", "\n", "grad_out", "[", ":", "]", "=", "grads", "[", "dist", ".", "get_rank", "(", ")", "]", "\n", "return", "grad_out", "", "", "", ""]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.utils.logger.LoggerWithDepth.__init__": [[17, 34], ["os.path.join", "logger.LoggerWithDepth.write_description_to_folder", "os.path.join", "os.path.exists", "Exception", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir", "os.path.join", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.alibaba-nlp_muver.utils.logger.LoggerWithDepth.write_description_to_folder"], ["    ", "def", "__init__", "(", "self", ",", "env_name", ",", "config", ",", "root_dir", "=", "'runtime_log'", ",", "overwrite", "=", "True", ")", ":", "\n", "        ", "if", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "root_dir", ",", "env_name", ")", ")", "and", "not", "overwrite", ":", "\n", "            ", "raise", "Exception", "(", "\"Logging Directory {} Has Already Exists. Change to another name or set OVERWRITE to True\"", ".", "format", "(", "os", ".", "path", ".", "join", "(", "root_dir", ",", "env_name", ")", ")", ")", "\n", "\n", "", "self", ".", "env_name", "=", "env_name", "\n", "self", ".", "root_dir", "=", "root_dir", "\n", "self", ".", "log_dir", "=", "os", ".", "path", ".", "join", "(", "root_dir", ",", "env_name", ")", "\n", "self", ".", "overwrite", "=", "overwrite", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "root_dir", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "root_dir", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "log_dir", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "self", ".", "log_dir", ")", "\n", "\n", "# Save Hyperparameters", "\n", "", "self", ".", "write_description_to_folder", "(", "os", ".", "path", ".", "join", "(", "self", ".", "log_dir", ",", "'description.txt'", ")", ",", "config", ")", "\n", "self", ".", "best_checkpoint_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "log_dir", ",", "'pytorch_model.bin'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.utils.logger.LoggerWithDepth.setup_sublogger": [[35, 64], ["os.path.join", "os.path.exists", "logging.Formatter", "logging.getLogger", "logging.FileHandler", "logging.FileHandler.setFormatter", "logger.LoggerWithDepth.writer.addHandler", "logger.LoggerWithDepth.writer.setLevel", "torch.utils.tensorboard.SummaryWriter", "os.path.join", "os.path.join", "Exception", "os.mkdir", "os.path.join"], "methods", ["None"], ["", "def", "setup_sublogger", "(", "self", ",", "sub_name", ",", "sub_config", ")", ":", "\n", "        ", "self", ".", "sub_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "log_dir", ",", "sub_name", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "self", ".", "sub_dir", ")", ":", "\n", "            ", "raise", "Exception", "(", "\"Logging Directory {} Has Already Exists. Change to another sub name or set OVERWRITE to True\"", ".", "format", "(", "self", ".", "sub_dir", ")", ")", "\n", "", "else", ":", "\n", "            ", "os", ".", "mkdir", "(", "self", ".", "sub_dir", ")", "\n", "\n", "# Setup File/Stream Writer", "\n", "", "log_format", "=", "logging", ".", "Formatter", "(", "\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\"", ")", "\n", "\n", "self", ".", "writer", "=", "logging", ".", "getLogger", "(", ")", "\n", "fileHandler", "=", "logging", ".", "FileHandler", "(", "os", ".", "path", ".", "join", "(", "self", ".", "sub_dir", ",", "\"training.log\"", ")", ")", "\n", "fileHandler", ".", "setFormatter", "(", "log_format", ")", "\n", "self", ".", "writer", ".", "addHandler", "(", "fileHandler", ")", "\n", "\n", "'''\n        consoleHandler = logging.StreamHandler()\n        consoleHandler.setFormatter(log_format)\n        self.writer.addHandler(consoleHandler)\n        '''", "\n", "self", ".", "writer", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "\n", "# Setup tensorboard Writer", "\n", "self", ".", "painter", "=", "SummaryWriter", "(", "self", ".", "sub_dir", ")", "\n", "tb_dir", "=", "self", ".", "painter", ".", "log_dir", "\n", "\n", "# Checkpoint", "\n", "self", ".", "checkpoint_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "sub_dir", ",", "'pytorch_model.bin'", ")", "\n", "self", ".", "lastest_checkpoint_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "sub_dir", ",", "'latest_model.bin'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.utils.logger.LoggerWithDepth.write_description_to_folder": [[66, 71], ["codecs.open", "desc_f.write", "config.items", "desc_f.write"], "methods", ["None"], ["", "def", "write_description_to_folder", "(", "self", ",", "file_name", ",", "config", ")", ":", "\n", "        ", "with", "codecs", ".", "open", "(", "file_name", ",", "'w'", ")", "as", "desc_f", ":", "\n", "            ", "desc_f", ".", "write", "(", "\"- Training Parameters: \\n\"", ")", "\n", "for", "key", ",", "value", "in", "config", ".", "items", "(", ")", ":", "\n", "                ", "desc_f", ".", "write", "(", "\"  - {}: {}\\n\"", ".", "format", "(", "key", ",", "value", ")", ")", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.SubworldBatchSampler.__init__": [[31, 34], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "batch_size", ",", "subworld_idx", ")", ":", "\n", "        ", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "subworld_idx", "=", "subworld_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.SubworldBatchSampler.__iter__": [[35, 54], ["data_loader.SubworldBatchSampler.subworld_idx.items", "list", "torch.randperm", "data_loader.SubworldBatchSampler.subworld_idx.keys", "len", "numpy.random.choice", "len", "len", "len", "list.remove"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "for", "world_name", ",", "world_value", "in", "self", ".", "subworld_idx", ".", "items", "(", ")", ":", "\n", "            ", "world_value", "[", "'perm_idx'", "]", "=", "torch", ".", "randperm", "(", "len", "(", "world_value", "[", "'idx'", "]", ")", ")", "\n", "world_value", "[", "'pointer'", "]", "=", "0", "\n", "", "world_names", "=", "list", "(", "self", ".", "subworld_idx", ".", "keys", "(", ")", ")", "\n", "\n", "while", "len", "(", "world_names", ")", ">", "0", ":", "\n", "            ", "world_name", "=", "np", ".", "random", ".", "choice", "(", "world_names", ")", "\n", "world_value", "=", "self", ".", "subworld_idx", "[", "world_name", "]", "\n", "start_pointer", "=", "world_value", "[", "'pointer'", "]", "\n", "sample_perm_idx", "=", "world_value", "[", "'perm_idx'", "]", "[", "start_pointer", ":", "start_pointer", "+", "self", ".", "batch_size", "]", "\n", "sample_idx", "=", "[", "world_value", "[", "'idx'", "]", "[", "idx", "]", "for", "idx", "in", "sample_perm_idx", "]", "\n", "\n", "if", "len", "(", "sample_idx", ")", ">", "0", ":", "\n", "                ", "yield", "sample_idx", "\n", "\n", "", "if", "len", "(", "sample_idx", ")", "<", "self", ".", "batch_size", ":", "\n", "                ", "world_names", ".", "remove", "(", "world_name", ")", "\n", "", "world_value", "[", "'pointer'", "]", "+=", "self", ".", "batch_size", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.SubworldBatchSampler.__len__": [[55, 57], ["sum", "data_loader.SubworldBatchSampler.subworld_idx.items", "len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "sum", "(", "[", "len", "(", "value", ")", "//", "self", ".", "batch_size", "+", "1", "for", "_", ",", "value", "in", "self", ".", "subworld_idx", ".", "items", "(", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.SubWorldDistributedSampler.__init__": [[59, 66], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "batch_size", ",", "subworld_idx", ",", "num_replicas", ",", "rank", ")", ":", "\n", "        ", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "subworld_idx", "=", "subworld_idx", "\n", "self", ".", "num_replicas", "=", "num_replicas", "\n", "self", ".", "rank", "=", "rank", "\n", "\n", "self", ".", "epoch", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.SubWorldDistributedSampler.__iter__": [[67, 103], ["torch.Generator", "torch.Generator.manual_seed", "data_loader.SubWorldDistributedSampler.subworld_idx.items", "list", "torch.randperm().tolist", "data_loader.SubWorldDistributedSampler.subworld_idx.keys", "len", "torch.randint().tolist", "len", "list.remove", "len", "list.remove", "torch.randperm", "len", "print", "len", "torch.randint", "len", "len", "len"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "g", "=", "torch", ".", "Generator", "(", ")", "\n", "g", ".", "manual_seed", "(", "self", ".", "epoch", ")", "\n", "\n", "for", "world_name", ",", "world_value", "in", "self", ".", "subworld_idx", ".", "items", "(", ")", ":", "\n", "            ", "world_value", "[", "'perm_idx'", "]", "=", "torch", ".", "randperm", "(", "len", "(", "world_value", "[", "'idx'", "]", ")", ",", "generator", "=", "g", ")", ".", "tolist", "(", ")", "\n", "world_value", "[", "'pointer'", "]", "=", "0", "\n", "", "world_names", "=", "list", "(", "self", ".", "subworld_idx", ".", "keys", "(", ")", ")", "\n", "\n", "while", "len", "(", "world_names", ")", ">", "0", ":", "\n", "            ", "world_idx", "=", "torch", ".", "randint", "(", "len", "(", "world_names", ")", ",", "size", "=", "(", "1", ",", ")", ",", "generator", "=", "g", ")", ".", "tolist", "(", ")", "[", "0", "]", "\n", "world_name", "=", "world_names", "[", "world_idx", "]", "\n", "\n", "world_value", "=", "self", ".", "subworld_idx", "[", "world_name", "]", "\n", "start_pointer", "=", "world_value", "[", "'pointer'", "]", "\n", "sample_perm_idx", "=", "world_value", "[", "'perm_idx'", "]", "[", "start_pointer", ":", "start_pointer", "+", "self", ".", "batch_size", "]", "\n", "\n", "if", "len", "(", "sample_perm_idx", ")", "==", "0", ":", "\n", "                ", "world_names", ".", "remove", "(", "world_name", ")", "\n", "continue", "\n", "\n", "", "if", "len", "(", "sample_perm_idx", ")", "<", "self", ".", "batch_size", ":", "\n", "                ", "world_names", ".", "remove", "(", "world_name", ")", "\n", "sample_perm_idx", "=", "sample_perm_idx", "+", "world_value", "[", "'perm_idx'", "]", "[", ":", "self", ".", "batch_size", "-", "len", "(", "sample_perm_idx", ")", "]", "\n", "#print(self.rank, sample_perm_idx)", "\n", "", "sample_perm_idx", "=", "sample_perm_idx", "[", "self", ".", "rank", ":", ":", "self", ".", "num_replicas", "]", "\n", "\n", "try", ":", "\n", "                ", "sample_idx", "=", "[", "world_value", "[", "'idx'", "]", "[", "idx", "]", "for", "idx", "in", "sample_perm_idx", "]", "\n", "assert", "len", "(", "sample_idx", ")", "==", "self", ".", "batch_size", "//", "self", ".", "num_replicas", "\n", "", "except", ":", "\n", "                ", "print", "(", "world_name", ",", "sample_perm_idx", ",", "sample_idx", ",", "len", "(", "world_value", "[", "'idx'", "]", ")", ")", "\n", "", "yield", "sample_idx", "\n", "world_value", "[", "'pointer'", "]", "+=", "self", ".", "batch_size", "\n", "\n", "", "self", ".", "epoch", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.EncodeDataset.__init__": [[108, 129], ["os.path.join", "os.path.exists", "os.path.exists", "torch.load", "print", "os.path.join", "data_loader.EncodeDataset.load_entity_description", "torch.save", "os.path.join", "os.path.join", "os.path.exists", "os.mkdir", "os.path.join", "len"], "methods", ["home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.EncodeDataset.load_entity_description"], ["    ", "def", "__init__", "(", "self", ",", "document_path", ",", "world", ",", "tokenizer", ",", "max_seq_len", ",", "max_sentence_num", ",", "all_sentences", "=", "False", ")", ":", "\n", "        ", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "max_seq_len", "=", "max_seq_len", "\n", "self", ".", "max_sentence_num", "=", "max_sentence_num", "\n", "self", ".", "all_sentences", "=", "all_sentences", "\n", "self", ".", "world", "=", "world", "\n", "\n", "self", ".", "seq_lens", "=", "{", "}", "\n", "\n", "preprocess_path", "=", "os", ".", "path", ".", "join", "(", "document_path", ",", "'preprocess_multiview'", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "preprocess_path", ")", "and", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "preprocess_path", ",", "world", "+", "'.pt'", ")", ")", ":", "\n", "            ", "self", ".", "samples", ",", "self", ".", "entity_title_to_id", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "preprocess_path", ",", "world", "+", "'.pt'", ")", ")", "\n", "print", "(", "\"World/{}: Load {} samples\"", ".", "format", "(", "world", ",", "len", "(", "self", ".", "samples", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "if", "not", "os", ".", "path", ".", "exists", "(", "preprocess_path", ")", ":", "\n", "                ", "os", ".", "mkdir", "(", "preprocess_path", ")", "\n", "\n", "", "document_path", "=", "os", ".", "path", ".", "join", "(", "document_path", ",", "world", "+", "'.json'", ")", "\n", "self", ".", "samples", ",", "self", ".", "entity_title_to_id", "=", "self", ".", "load_entity_description", "(", "document_path", ",", "tokenizer", ",", "world", ")", "\n", "torch", ".", "save", "(", "[", "self", ".", "samples", ",", "self", ".", "entity_title_to_id", "]", ",", "os", ".", "path", ".", "join", "(", "preprocess_path", ",", "world", "+", "'.pt'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.EncodeDataset.__len__": [[130, 132], ["len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.EncodeDataset.get_nth_title": [[133, 135], ["None"], "methods", ["None"], ["", "def", "get_nth_title", "(", "self", ",", "idx", ")", ":", "\n", "        ", "return", "self", ".", "samples", "[", "idx", "]", "[", "'title'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.EncodeDataset.load_entity_description": [[136, 158], ["sum", "print", "open", "enumerate", "tqdm.tqdm.tqdm", "json.loads", "data_loader.EncodeDataset.tokenize_split_description", "entity_desc.append", "sentence_nums.get", "open", "len", "len"], "methods", ["home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.EncodeDataset.tokenize_split_description"], ["", "def", "load_entity_description", "(", "self", ",", "document_path", ",", "tokenizer", ",", "world", ")", ":", "\n", "        ", "entity_desc", "=", "[", "]", "\n", "entity_title_to_id", "=", "{", "}", "\n", "\n", "num_lines", "=", "sum", "(", "1", "for", "line", "in", "open", "(", "document_path", ",", "'r'", ")", ")", "\n", "print", "(", "\"World/{}: preprocessing {} samples\"", ".", "format", "(", "world", ",", "num_lines", ")", ")", "\n", "\n", "sentence_nums", "=", "{", "}", "\n", "with", "open", "(", "document_path", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "for", "idx", ",", "line", "in", "enumerate", "(", "tqdm", "(", "f", ",", "total", "=", "num_lines", ")", ")", ":", "\n", "                ", "info", "=", "json", ".", "loads", "(", "line", ")", "\n", "token_ids", "=", "self", ".", "tokenize_split_description", "(", "info", "[", "'title'", "]", ",", "info", "[", "'text'", "]", ",", "tokenizer", ")", "\n", "entity_desc", ".", "append", "(", "{", "\n", "\"token_ids\"", ":", "token_ids", ",", "\n", "\"title\"", ":", "info", "[", "'title'", "]", "\n", "}", ")", "\n", "num", "=", "sentence_nums", ".", "get", "(", "len", "(", "token_ids", ")", ",", "0", ")", "\n", "sentence_nums", "[", "len", "(", "token_ids", ")", "]", "=", "num", "+", "1", "\n", "entity_title_to_id", "[", "info", "[", "'title'", "]", "]", "=", "idx", "\n", "\n", "#print(sorted(sentence_nums.items(), key = lambda x: x[0]))", "\n", "", "", "return", "entity_desc", ",", "entity_title_to_id", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.EncodeDataset.tokenize_description": [[159, 169], ["tokenizer.convert_tokens_to_ids", "tokenizer.tokenize", "len", "len", "tokenizer.tokenize", "len"], "methods", ["None"], ["", "def", "tokenize_description", "(", "self", ",", "title", ",", "desc", ",", "tokenizer", ")", ":", "\n", "        ", "encode_text", "=", "[", "CLS_TAG", "]", "+", "tokenizer", ".", "tokenize", "(", "title", ")", "+", "[", "ENTITY_TAG", "]", "+", "tokenizer", ".", "tokenize", "(", "desc", ")", "\n", "encode_text", "=", "encode_text", "[", ":", "self", ".", "max_cand_len", "-", "1", "]", "+", "[", "SEP_TAG", "]", "\n", "\n", "tokens", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "encode_text", ")", "\n", "if", "len", "(", "tokens", ")", "<", "self", ".", "max_cand_len", ":", "\n", "            ", "tokens", "+=", "[", "0", "]", "*", "(", "self", ".", "max_cand_len", "-", "len", "(", "tokens", ")", ")", "\n", "\n", "", "assert", "(", "len", "(", "tokens", ")", "==", "self", ".", "max_cand_len", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.EncodeDataset.tokenize_split_description": [[170, 195], ["nltk.sent_tokenize", "tokenizer.tokenize", "desc.replace", "tokenizer.tokenize", "tokenizer.convert_tokens_to_ids", "multi_sent.append", "len", "len", "len", "len"], "methods", ["None"], ["", "def", "tokenize_split_description", "(", "self", ",", "title", ",", "desc", ",", "tokenizer", ")", ":", "\n", "#if not is_split_by_sentence:", "\n", "#    encode_text = [CLS_TAG] + tokenizer.tokenize(title) + [ENTITY_TAG] + tokenizer.tokenize(desc)", "\n", "#    encode_text = encode_text[:self.max_cand_len - 1] + [SEP_TAG]", "\n", "#else:", "\n", "        ", "title_text", "=", "tokenizer", ".", "tokenize", "(", "title", ")", "+", "[", "ENTITY_TAG", "]", "\n", "\n", "multi_sent", "=", "[", "]", "\n", "pre_text", "=", "[", "]", "\n", "for", "sent", "in", "nltk", ".", "sent_tokenize", "(", "desc", ".", "replace", "(", "' .'", ",", "'.'", ")", ")", ":", "\n", "            ", "text", "=", "tokenizer", ".", "tokenize", "(", "sent", ")", "\n", "pre_text", "+=", "text", "\n", "if", "len", "(", "pre_text", ")", "<=", "5", ":", "\n", "                ", "continue", "\n", "", "whole_text", "=", "title_text", "+", "pre_text", "\n", "whole_text", "=", "[", "CLS_TAG", "]", "+", "whole_text", "[", ":", "self", ".", "max_seq_len", "-", "2", "]", "+", "[", "SEP_TAG", "]", "\n", "tokens", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "whole_text", ")", "\n", "pre_text", "=", "[", "]", "\n", "\n", "if", "len", "(", "tokens", ")", "<", "self", ".", "max_seq_len", ":", "\n", "                ", "tokens", "+=", "[", "0", "]", "*", "(", "self", ".", "max_seq_len", "-", "len", "(", "tokens", ")", ")", "\n", "", "assert", "len", "(", "tokens", ")", "==", "self", ".", "max_seq_len", "\n", "multi_sent", ".", "append", "(", "tokens", ")", "\n", "\n", "", "return", "multi_sent", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.EncodeDataset.__getitem__": [[196, 227], ["len", "len", "len", "range", "len"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "if", "self", ".", "all_sentences", ":", "\n", "            ", "entity_ids", "=", "self", ".", "samples", "[", "idx", "]", "[", "'token_ids'", "]", "\n", "", "else", ":", "\n", "            ", "entity_ids", "=", "self", ".", "samples", "[", "idx", "]", "[", "'token_ids'", "]", "[", ":", "self", ".", "max_sentence_num", "]", "\n", "if", "len", "(", "entity_ids", ")", "<=", "self", ".", "max_sentence_num", ":", "\n", "                ", "entity_ids", "+=", "[", "[", "0", "]", "*", "self", ".", "max_seq_len", "for", "_", "in", "range", "(", "self", ".", "max_sentence_num", "-", "len", "(", "entity_ids", ")", ")", "]", "\n", "\n", "", "assert", "len", "(", "entity_ids", ")", "==", "self", ".", "max_sentence_num", "\n", "'''\n            if len(self.samples[idx]['token_ids']) <= self.max_sentence_num:\n                entity_ids = self.samples[idx]['token_ids']\n            else:\n                #sentence_idx = np.random.choice(len(self.samples[idx]['token_ids']), self.max_sentence_num)\n                entity_ids = []\n                sentence_idx = []\n                for _ in range(self.max_sentence_num):\n                    s_idx = np.random.randint(len(self.samples[idx]['token_ids']))\n                    while s_idx in sentence_idx:\n                        s_idx = np.random.randint(len(self.samples[idx]['token_ids']))\n                    sentence_idx.append(s_idx)\n                    entity_ids.append(self.samples[idx]['token_ids'][s_idx])\n                #print(\"random_select_sentence: \", self.samples[idx]['title'], sentence_idx, len(self.samples[idx]['token_ids']))\n            if len(entity_ids) < self.max_sentence_num:\n                entity_ids += [[0] * self.max_seq_len for _ in range(self.max_sentence_num - len(entity_ids))]\n            '''", "\n", "assert", "len", "(", "entity_ids", ")", "==", "self", ".", "max_sentence_num", "\n", "", "return", "{", "\n", "'token_ids'", ":", "entity_ids", ",", "\n", "'title'", ":", "self", ".", "samples", "[", "idx", "]", "[", "'title'", "]", ",", "\n", "'title_ids'", ":", "idx", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.ZeshelDataset.__init__": [[240, 265], ["data_loader.ZeshelDataset.load_training_samples", "data_loader.ZeshelDataset.get_subworld_idx", "data_loader.EncodeDataset"], "methods", ["home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.ZeshelDataset.load_training_samples", "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.ZeshelDataset.get_subworld_idx"], ["    ", "def", "__init__", "(", "self", ",", "\n", "mode", ",", "desc_path", ",", "dataset_path", ",", "tokenizer", ",", "\n", "max_cand_len", "=", "30", ",", "max_seq_len", "=", "128", ",", "max_sentence_num", "=", "10", ",", "all_sentences", "=", "False", ",", "\n", ")", ":", "\n", "        ", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "max_cand_len", "=", "max_cand_len", "\n", "self", ".", "max_sentence_num", "=", "max_sentence_num", "\n", "self", ".", "max_seq_len", "=", "max_seq_len", "\n", "self", ".", "all_sentences", "=", "all_sentences", "\n", "\n", "self", ".", "entity_desc", "=", "{", "\n", "world", "[", "0", "]", ":", "EncodeDataset", "(", "\n", "document_path", "=", "desc_path", ",", "\n", "world", "=", "world", "[", "0", "]", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "max_seq_len", "=", "self", ".", "max_cand_len", ",", "\n", "max_sentence_num", "=", "self", ".", "max_sentence_num", ",", "\n", "all_sentences", "=", "self", ".", "all_sentences", "\n", ")", "\n", "for", "world", "in", "WORLDS", "[", "mode", "]", "\n", "}", "\n", "\n", "self", ".", "load_training_samples", "(", "dataset_path", ",", "mode", ",", "max_seq_len", ")", "\n", "self", ".", "subworld_idx", "=", "self", ".", "get_subworld_idx", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.ZeshelDataset.get_subworld_idx": [[266, 274], ["enumerate", "[].append"], "methods", ["None"], ["", "def", "get_subworld_idx", "(", "self", ")", ":", "\n", "        ", "worlds_sample_idx", "=", "{", "world", "[", "0", "]", ":", "{", "'idx'", ":", "[", "]", ",", "'num'", ":", "0", "}", "for", "world", "in", "WORLDS", "[", "self", ".", "mode", "]", "}", "\n", "for", "idx", ",", "sample", "in", "enumerate", "(", "self", ".", "samples", ")", ":", "\n", "            ", "world", "=", "sample", "[", "'world'", "]", "\n", "worlds_sample_idx", "[", "world", "]", "[", "'idx'", "]", ".", "append", "(", "idx", ")", "\n", "worlds_sample_idx", "[", "world", "]", "[", "'num'", "]", "+=", "1", "\n", "\n", "", "return", "worlds_sample_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.ZeshelDataset.load_training_samples": [[275, 296], ["os.path.join", "os.path.exists", "os.path.join", "sum", "print", "open", "print", "open", "tqdm.tqdm.tqdm", "open", "json.loads", "data_loader.ZeshelDataset.tokenize_context", "data_loader.ZeshelDataset.samples.append", "f.write", "len", "open", "json.loads", "json.dumps"], "methods", ["home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.ZeshelDataset.tokenize_context"], ["", "def", "load_training_samples", "(", "self", ",", "dataset_path", ",", "mode", ",", "max_seq_len", ")", ":", "\n", "        ", "token_path", "=", "os", ".", "path", ".", "join", "(", "dataset_path", ",", "\"{}_token.jsonl\"", ".", "format", "(", "mode", ")", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "token_path", ")", ":", "\n", "            ", "with", "open", "(", "token_path", ",", "'r'", ")", "as", "f", ":", "\n", "                ", "self", ".", "samples", "=", "[", "json", ".", "loads", "(", "line", ")", "for", "line", "in", "f", "]", "\n", "print", "(", "\"Set/{}: Load {} samples\"", ".", "format", "(", "mode", ",", "len", "(", "self", ".", "samples", ")", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "data_path", "=", "os", ".", "path", ".", "join", "(", "dataset_path", ",", "\"{}.jsonl\"", ".", "format", "(", "mode", ")", ")", "\n", "num_lines", "=", "sum", "(", "1", "for", "line", "in", "open", "(", "data_path", ",", "'r'", ")", ")", "\n", "\n", "self", ".", "samples", "=", "[", "]", "\n", "print", "(", "\"Set/{}: preprocessing {} samples\"", ".", "format", "(", "mode", ",", "num_lines", ")", ")", "\n", "\n", "with", "open", "(", "data_path", ",", "'r'", ")", "as", "sample_f", ":", "\n", "                ", "for", "sample_line", "in", "tqdm", "(", "sample_f", ",", "total", "=", "num_lines", ")", ":", "\n", "                    ", "sample", "=", "self", ".", "tokenize_context", "(", "json", ".", "loads", "(", "sample_line", ")", ",", "max_seq_len", ")", "\n", "self", ".", "samples", ".", "append", "(", "sample", ")", "\n", "\n", "", "", "with", "open", "(", "token_path", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "for", "sample", "in", "self", ".", "samples", ":", "\n", "                    ", "f", ".", "write", "(", "json", ".", "dumps", "(", "sample", ")", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.ZeshelDataset.__len__": [[297, 299], ["len"], "methods", ["None"], ["", "", "", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.ZeshelDataset.__getitem__": [[300, 318], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "sample", "=", "self", ".", "samples", "[", "idx", "]", "\n", "context_ids", "=", "sample", "[", "'ids'", "]", "\n", "label", ",", "world", "=", "sample", "[", "'label'", "]", ",", "sample", "[", "'world'", "]", "\n", "\n", "if", "self", ".", "all_sentences", ":", "\n", "            ", "return", "{", "\n", "\"label_ids\"", ":", "[", "-", "1", "]", ",", "\n", "\"context_ids\"", ":", "context_ids", ",", "\n", "\"world\"", ":", "world", ",", "\n", "\"label_world_idx\"", ":", "label", "\n", "}", "\n", "", "else", ":", "\n", "            ", "return", "{", "\n", "\"label_ids\"", ":", "self", ".", "entity_desc", "[", "world", "]", "[", "label", "]", "[", "'token_ids'", "]", ",", "\n", "\"context_ids\"", ":", "context_ids", ",", "\n", "\"world\"", ":", "world", ",", "\n", "\"label_world_idx\"", ":", "label", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.ZeshelDataset.concat_context_entity_ids": [[320, 331], ["len", "len", "context_ids.index"], "methods", ["None"], ["", "", "def", "concat_context_entity_ids", "(", "self", ",", "context_ids", ",", "candidate_idx", ",", "world", ")", ":", "\n", "        ", "if", "0", "in", "context_ids", ":", "\n", "            ", "context_ids", "=", "context_ids", "[", ":", "context_ids", ".", "index", "(", "0", ")", "]", "\n", "\n", "", "entity_token_ids", "=", "self", ".", "entity_desc", "[", "world", "]", "[", "candidate_idx", "]", "[", "'token_ids'", "]", "\n", "input_ids", "=", "context_ids", "+", "entity_token_ids", "[", "1", ":", "]", "\n", "padding", "=", "[", "0", "]", "*", "(", "self", ".", "max_cand_len", "+", "self", ".", "max_seq_len", "-", "len", "(", "input_ids", ")", ")", "\n", "input_ids", "+=", "padding", "\n", "assert", "len", "(", "input_ids", ")", "==", "self", ".", "max_cand_len", "+", "self", ".", "max_seq_len", "\n", "\n", "return", "input_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.ZeshelDataset.tokenize_context": [[332, 376], ["data_loader.ZeshelDataset.tokenizer.tokenize", "data_loader.ZeshelDataset.tokenizer.tokenize", "len", "len", "data_loader.ZeshelDataset.tokenizer.convert_tokens_to_ids", "data_loader.ZeshelDataset.tokenizer.tokenize", "len", "len", "len", "len", "len"], "methods", ["None"], ["", "def", "tokenize_context", "(", "\n", "self", ",", "\n", "sample", ",", "\n", "max_seq_len", "\n", ")", ":", "\n", "        ", "'''\n        https://github.com/facebookresearch/BLINK/blob/master/blink/biencoder/data_process.py\n        '''", "\n", "mention_tokens", "=", "[", "]", "\n", "if", "sample", "[", "'mention'", "]", "and", "len", "(", "sample", "[", "'mention'", "]", ")", ">", "0", ":", "\n", "            ", "mention_tokens", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "sample", "[", "'mention'", "]", ")", "\n", "mention_tokens", "=", "[", "MENTION_START_TAG", "]", "+", "mention_tokens", "+", "[", "MENTION_END_TAG", "]", "\n", "\n", "", "context_left", "=", "sample", "[", "\"context_left\"", "]", "\n", "context_right", "=", "sample", "[", "\"context_right\"", "]", "\n", "context_left", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "context_left", ")", "\n", "context_right", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "context_right", ")", "\n", "\n", "left_quota", "=", "(", "max_seq_len", "-", "len", "(", "mention_tokens", ")", ")", "//", "2", "-", "1", "\n", "right_quota", "=", "max_seq_len", "-", "len", "(", "mention_tokens", ")", "-", "left_quota", "-", "2", "\n", "left_add", "=", "len", "(", "context_left", ")", "\n", "right_add", "=", "len", "(", "context_right", ")", "\n", "if", "left_add", "<=", "left_quota", ":", "\n", "            ", "if", "right_add", ">", "right_quota", ":", "\n", "                ", "right_quota", "+=", "left_quota", "-", "left_add", "\n", "", "", "else", ":", "\n", "            ", "if", "right_add", "<=", "right_quota", ":", "\n", "                ", "left_quota", "+=", "right_quota", "-", "right_add", "\n", "\n", "", "", "context_tokens", "=", "(", "\n", "context_left", "[", "-", "left_quota", ":", "]", "+", "mention_tokens", "+", "context_right", "[", ":", "right_quota", "]", "\n", ")", "\n", "\n", "context_tokens", "=", "[", "\"[CLS]\"", "]", "+", "context_tokens", "+", "[", "\"[SEP]\"", "]", "\n", "input_ids", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "context_tokens", ")", "\n", "padding", "=", "[", "0", "]", "*", "(", "max_seq_len", "-", "len", "(", "input_ids", ")", ")", "\n", "input_ids", "+=", "padding", "\n", "assert", "len", "(", "input_ids", ")", "==", "max_seq_len", "\n", "\n", "return", "{", "\n", "\"tokens\"", ":", "context_tokens", ",", "\n", "\"ids\"", ":", "input_ids", ",", "\n", "\"label\"", ":", "sample", "[", "'label_id'", "]", ",", "\n", "\"world\"", ":", "sample", "[", "'world'", "]", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.bi_collate_fn": [[229, 237], ["torch.tensor", "torch.tensor"], "function", ["None"], ["", "", "def", "bi_collate_fn", "(", "batch", ")", ":", "\n", "    ", "token_ids", "=", "torch", ".", "tensor", "(", "[", "sample", "[", "'token_ids'", "]", "for", "sample", "in", "batch", "]", ")", "# sentence_num * max_seq_len", "\n", "title", "=", "[", "sample", "[", "'title'", "]", "for", "sample", "in", "batch", "]", "\n", "title_ids", "=", "torch", ".", "tensor", "(", "[", "sample", "[", "'title_ids'", "]", "for", "sample", "in", "batch", "]", ")", "\n", "return", "{", "\n", "'token_ids'", ":", "token_ids", ",", "\n", "'title'", ":", "title", ",", "\n", "'title_ids'", ":", "title_ids", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.cross_collate_fn": [[378, 390], ["torch.tensor", "torch.tensor", "torch.tensor"], "function", ["None"], ["", "", "def", "cross_collate_fn", "(", "batch", ")", ":", "\n", "    ", "world", "=", "[", "sample", "[", "'world'", "]", "for", "sample", "in", "batch", "]", "\n", "label_world_idx", "=", "torch", ".", "tensor", "(", "[", "sample", "[", "'label_world_idx'", "]", "for", "sample", "in", "batch", "]", ")", "\n", "label_ids", "=", "torch", ".", "tensor", "(", "[", "sample", "[", "'label_ids'", "]", "for", "sample", "in", "batch", "]", ")", "\n", "context_ids", "=", "torch", ".", "tensor", "(", "[", "sample", "[", "'context_ids'", "]", "for", "sample", "in", "batch", "]", ")", "\n", "#label_split = torch.tensor([sample['label_split'] for sample in batch])", "\n", "\n", "return", "{", "\n", "'context_ids'", ":", "context_ids", ",", "\n", "'label_ids'", ":", "label_ids", ",", "\n", "'world'", ":", "world", ",", "\n", "'label_world_idx'", ":", "label_world_idx", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.train.argument_parser": [[38, 75], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argument_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--pretrained_model'", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--dataset_path'", ",", "type", "=", "str", ",", "default", "=", "'data/zeshel'", ")", "\n", "parser", ".", "add_argument", "(", "'--bi_ckpt_path'", ",", "type", "=", "str", ",", "nargs", "=", "'+'", ",", "default", "=", "None", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--epoch'", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "'--train_batch_size'", ",", "type", "=", "int", ",", "default", "=", "12", ")", "\n", "parser", ".", "add_argument", "(", "'--max_cand_len'", ",", "type", "=", "int", ",", "default", "=", "40", ")", "\n", "parser", ".", "add_argument", "(", "'--max_seq_len'", ",", "type", "=", "int", ",", "default", "=", "128", ")", "\n", "parser", ".", "add_argument", "(", "'--max_sentence_num'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--learning_rate'", ",", "type", "=", "float", ",", "nargs", "=", "'+'", ",", "default", "=", "1e-5", ")", "\n", "parser", ".", "add_argument", "(", "'--weight_decay'", ",", "type", "=", "float", ",", "nargs", "=", "'+'", ",", "default", "=", "0.01", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup_ratio'", ",", "type", "=", "float", ",", "nargs", "=", "'+'", ",", "default", "=", "0.1", ")", "\n", "parser", ".", "add_argument", "(", "'--max_grad_norm'", ",", "type", "=", "float", ",", "default", "=", "1.0", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--merge_layers'", ",", "type", "=", "int", ",", "default", "=", "3", ")", "\n", "parser", ".", "add_argument", "(", "'--top_k'", ",", "type", "=", "float", ",", "default", "=", "0.4", ")", "\n", "#parser.add_argument('--alpha', type=float, default=0.5)", "\n", "#parser.add_argument('--beta', type=float, nargs='+', default=50)", "\n", "\n", "parser", ".", "add_argument", "(", "'--eval_batch_size'", ",", "type", "=", "int", ",", "default", "=", "8", ")", "\n", "parser", ".", "add_argument", "(", "'--logging_interval'", ",", "type", "=", "int", ",", "default", "=", "50", ")", "\n", "parser", ".", "add_argument", "(", "'--eval_interval'", ",", "type", "=", "int", ",", "default", "=", "2000", ")", "\n", "parser", ".", "add_argument", "(", "'--accumulate_score'", ",", "action", "=", "\"store_true\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--do_train'", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "'--do_eval'", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "'--do_test'", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "'--view_expansion'", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "'--test_mode'", ",", "type", "=", "str", ",", "default", "=", "'test'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--data_parallel\"", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "type", "=", "int", ",", "default", "=", "10000", ")", "\n", "parser", ".", "add_argument", "(", "\"--name\"", ",", "type", "=", "str", ",", "default", "=", "'test'", ")", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.train.main": [[77, 253], ["muver.utils.tools.set_random_seed", "muver.utils.tools.grid_search_hyperparamters", "muver.utils.logger.LoggerWithDepth", "model.BiEncoder().cuda", "model.NCE_Random", "torch.init_process_group", "torch.cuda.set_device", "torch.cuda.set_device", "torch.cuda.set_device", "torch.cuda.set_device", "time.strftime", "muver.utils.logger.LoggerWithDepth.setup_sublogger", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load.items", "torch.nn.parallel.DistributedDataParallel.load_state_dict", "torch.nn.parallel.DistributedDataParallel", "torch.utils.data.DataLoader", "transformers.AdamW", "int", "transformers.get_linear_schedule_with_warmup", "time.localtime", "model.BiEncoder", "data_loader.SubWorldDistributedSampler", "data_loader.SubworldBatchSampler", "muver.utils.logger.LoggerWithDepth.writer.info", "tqdm.tqdm", "range", "muver.utils.logger.LoggerWithDepth.write_description_to_folder", "torch.nn.parallel.DistributedDataParallel.load_state_dict", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "zeshel_evaluate.evaluate_bi_model", "len", "os.path.join", "torch.load", "torch.load", "torch.load", "torch.load", "os.path.join", "os.path.join", "os.path.dirname", "os.path.join", "os.path.join", "open", "f.write", "open", "torch.nn.parallel.DistributedDataParallel.train", "ctx_ids.cuda.cuda", "ent_ids.cuda.cuda", "torch.nn.parallel.DistributedDataParallel", "model.NCE_Random.", "loss.backward", "tr_loss.append", "torch.save", "torch.save", "torch.save", "torch.save", "os.path.abspath", "json.dumps", "f.write", "torch.nn.parallel.DistributedDataParallel.named_parameters", "torch.nn.parallel.DistributedDataParallel.named_parameters", "any", "transformers.AdamW.step", "transformers.get_linear_schedule_with_warmup.step", "torch.nn.parallel.DistributedDataParallel.zero_grad", "pbar.set_description", "pbar.update", "loss.item", "muver.utils.logger.LoggerWithDepth.writer.info", "torch.nn.parallel.DistributedDataParallel.state_dict", "os.path.join", "any", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "zeshel_evaluate.evaluate_bi_model", "json.dumps", "torch.nn.parallel.DistributedDataParallel.parameters", "loss.item", "bi_acc.item", "muver.utils.logger.LoggerWithDepth.writer.info", "torch.save", "torch.save", "torch.save", "torch.save", "sum", "len", "torch.nn.parallel.DistributedDataParallel.state_dict", "torch.save", "torch.save", "torch.save", "torch.save", "torch.nn.parallel.DistributedDataParallel.state_dict"], "function", ["home.repos.pwc.inspect_result.alibaba-nlp_muver.utils.tools.set_random_seed", "home.repos.pwc.inspect_result.alibaba-nlp_muver.utils.tools.grid_search_hyperparamters", "home.repos.pwc.inspect_result.alibaba-nlp_muver.utils.logger.LoggerWithDepth.setup_sublogger", "home.repos.pwc.inspect_result.alibaba-nlp_muver.utils.logger.LoggerWithDepth.write_description_to_folder", "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.zeshel_evaluate.evaluate_bi_model", "home.repos.pwc.inspect_result.alibaba-nlp_muver.utils.multigpu.GatherLayer.backward", "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.zeshel_evaluate.evaluate_bi_model"], ["", "def", "main", "(", "local_rank", ",", "args", ",", "train_dataset", ",", "valid_dataset", ",", "test_dataset", ",", "tokenizer", ")", ":", "\n", "    ", "args", ".", "local_rank", "=", "local_rank", "\n", "if", "args", ".", "do_train", "and", "args", ".", "local_rank", "in", "[", "0", ",", "-", "1", "]", ":", "\n", "        ", "logger", "=", "LoggerWithDepth", "(", "\n", "env_name", "=", "args", ".", "name", ",", "\n", "config", "=", "args", ".", "__dict__", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "logger", "=", "None", "\n", "\n", "# Set Training Device", "\n", "", "if", "args", ".", "data_parallel", ":", "\n", "\n", "        ", "if", "args", ".", "n_gpu", "==", "1", ":", "\n", "            ", "args", ".", "data_parallel", "=", "False", "\n", "", "else", ":", "\n", "            ", "dist", ".", "init_process_group", "(", "\"nccl\"", ",", "rank", "=", "args", ".", "local_rank", ",", "world_size", "=", "args", ".", "n_gpu", ")", "\n", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "\n", "", "", "args", ".", "device", "=", "\"cuda\"", "if", "not", "args", ".", "no_cuda", "else", "\"cpu\"", "\n", "set_random_seed", "(", "args", ".", "seed", ")", "\n", "\n", "grid_arguments", "=", "grid_search_hyperparamters", "(", "args", ")", "\n", "for", "grid_args", "in", "grid_arguments", ":", "\n", "        ", "if", "args", ".", "do_train", "and", "args", ".", "local_rank", "in", "[", "0", ",", "-", "1", "]", ":", "\n", "            ", "sub_name", "=", "time", ".", "strftime", "(", "'%Y-%m-%d-%H-%M-%S'", ",", "time", ".", "localtime", "(", ")", ")", "\n", "logger", ".", "setup_sublogger", "(", "sub_name", ",", "grid_args", ".", "__dict__", ")", "\n", "\n", "# Load Model and Tokenizer", "\n", "", "bi_model", "=", "BiEncoder", "(", "args", ".", "pretrained_model", ")", ".", "cuda", "(", ")", "\n", "\n", "criterion", "=", "NCE_Random", "(", "args", ".", "n_gpu", ")", "\n", "\n", "# Load From checkpoint", "\n", "if", "grid_args", ".", "bi_ckpt_path", "is", "not", "None", ":", "\n", "            ", "state_dict", "=", "torch", ".", "load", "(", "grid_args", ".", "bi_ckpt_path", ",", "map_location", "=", "'cpu'", ")", "\n", "new_state_dict", "=", "{", "}", "\n", "for", "param_name", ",", "param_value", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "                ", "if", "param_name", "[", ":", "7", "]", "==", "'module.'", ":", "\n", "                    ", "new_state_dict", "[", "param_name", "[", "7", ":", "]", "]", "=", "param_value", "\n", "", "else", ":", "\n", "                    ", "new_state_dict", "[", "param_name", "]", "=", "param_value", "\n", "", "", "bi_model", ".", "load_state_dict", "(", "new_state_dict", ")", "\n", "\n", "", "if", "args", ".", "n_gpu", ">", "1", ":", "\n", "            ", "bi_model", "=", "DDP", "(", "bi_model", ",", "device_ids", "=", "[", "args", ".", "local_rank", "]", ",", "find_unused_parameters", "=", "True", ")", "\n", "\n", "# Load Data", "\n", "", "if", "args", ".", "do_train", ":", "\n", "            ", "train_batch_size", "=", "grid_args", ".", "train_batch_size", "//", "grid_args", ".", "gradient_accumulation", "\n", "\n", "if", "args", ".", "data_parallel", ":", "\n", "                ", "sampler", "=", "SubWorldDistributedSampler", "(", "batch_size", "=", "grid_args", ".", "train_batch_size", ",", "subworld_idx", "=", "train_dataset", ".", "subworld_idx", ",", "num_replicas", "=", "args", ".", "n_gpu", ",", "rank", "=", "args", ".", "local_rank", ")", "\n", "", "else", ":", "\n", "                ", "sampler", "=", "SubworldBatchSampler", "(", "batch_size", "=", "grid_args", ".", "train_batch_size", ",", "subworld_idx", "=", "train_dataset", ".", "subworld_idx", ")", "\n", "", "train_dataloader", "=", "DataLoader", "(", "dataset", "=", "train_dataset", ",", "batch_sampler", "=", "sampler", ",", "collate_fn", "=", "cross_collate_fn", ")", "\n", "\n", "# optimizer & scheduler", "\n", "no_decay", "=", "[", "\"bias\"", ",", "\"LayerNorm.bias\"", ",", "\"LayerNorm.weight\"", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "bi_model", ".", "named_parameters", "(", ")", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "grid_args", ".", "weight_decay", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "bi_model", ".", "named_parameters", "(", ")", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", ",", "\n", "]", "\n", "optimizer", "=", "AdamW", "(", "optimizer_grouped_parameters", ",", "lr", "=", "grid_args", ".", "learning_rate", ")", "\n", "\n", "total_steps", "=", "len", "(", "train_dataset", ")", "*", "args", ".", "epoch", "//", "train_batch_size", "\n", "warmup_steps", "=", "int", "(", "grid_args", ".", "warmup_ratio", "*", "total_steps", ")", "\n", "scheduler", "=", "get_linear_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", "=", "warmup_steps", ",", "num_training_steps", "=", "total_steps", ")", "\n", "if", "args", ".", "local_rank", "in", "[", "0", ",", "-", "1", "]", ":", "\n", "                ", "logger", ".", "writer", ".", "info", "(", "\"Optimization steps = {},  Warmup steps = {}\"", ".", "format", "(", "total_steps", ",", "warmup_steps", ")", ")", "\n", "\n", "# Train", "\n", "", "", "if", "args", ".", "do_train", ":", "\n", "            ", "step", ",", "max_score", "=", "0", ",", "0", "\n", "with", "tqdm", "(", "total", "=", "total_steps", ")", "as", "pbar", ":", "\n", "                ", "for", "e", "in", "range", "(", "args", ".", "epoch", ")", ":", "\n", "                    ", "tr_loss", "=", "[", "]", "\n", "for", "batch", "in", "train_dataloader", ":", "\n", "                        ", "bi_model", ".", "train", "(", ")", "\n", "step", "+=", "1", "\n", "\n", "world", "=", "batch", "[", "'world'", "]", "\n", "for", "w", "in", "world", "[", "1", ":", "]", ":", "\n", "                            ", "assert", "world", "[", "0", "]", "==", "w", "\n", "\n", "", "ctx_ids", ",", "ent_ids", "=", "batch", "[", "'context_ids'", "]", ",", "batch", "[", "'label_ids'", "]", "\n", "\n", "ctx_ids", "=", "ctx_ids", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "ent_ids", "=", "ent_ids", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "ctx_output", ",", "ent_output", "=", "bi_model", "(", "\n", "ctx_ids", "=", "ctx_ids", ",", "\n", "ent_ids", "=", "ent_ids", ",", "\n", "num_gpus", "=", "args", ".", "n_gpu", "\n", ")", "\n", "loss", ",", "bi_acc", ",", "bi_score", "=", "criterion", "(", "ctx_output", ",", "ent_output", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "#if args.n_gpu > 1:", "\n", "#    dist.all_reduce(loss.div_(args.n_gpu))", "\n", "if", "step", "%", "grid_args", ".", "gradient_accumulation", "==", "0", ":", "\n", "                            ", "if", "args", ".", "max_grad_norm", ">", "0", ":", "\n", "                                ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "bi_model", ".", "parameters", "(", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "scheduler", ".", "step", "(", ")", "\n", "bi_model", ".", "zero_grad", "(", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "in", "[", "0", ",", "-", "1", "]", ":", "\n", "                            ", "pbar", ".", "set_description", "(", "\"epoch: {}, loss: {}, acc: {}\"", ".", "format", "(", "\n", "e", "+", "1", ",", "loss", ".", "item", "(", ")", ",", "bi_acc", ".", "item", "(", ")", "\n", ")", ")", "\n", "pbar", ".", "update", "(", ")", "\n", "\n", "", "tr_loss", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "if", "step", "%", "args", ".", "logging_interval", "==", "0", "and", "args", ".", "local_rank", "in", "[", "0", ",", "-", "1", "]", ":", "\n", "                            ", "logger", ".", "writer", ".", "info", "(", "\"Step {}: Average Loss = {}\"", ".", "format", "(", "step", ",", "sum", "(", "tr_loss", ")", "/", "len", "(", "tr_loss", ")", ")", ")", "\n", "tr_loss", "=", "[", "]", "\n", "\n", "", "if", "step", "%", "args", ".", "eval_interval", "==", "0", "and", "args", ".", "do_eval", ":", "\n", "                            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                                ", "score", ",", "_", "=", "evaluate_bi_model", "(", "\n", "bi_model", ",", "tokenizer", ",", "valid_dataset", ",", "\n", "mode", "=", "'valid'", ",", "\n", "device", "=", "args", ".", "device", ",", "\n", "local_rank", "=", "args", ".", "local_rank", ",", "\n", "n_gpu", "=", "args", ".", "n_gpu", ")", "\n", "\n", "if", "args", ".", "local_rank", "in", "[", "0", ",", "-", "1", "]", ":", "\n", "                                    ", "logger", ".", "writer", ".", "info", "(", "score", ")", "\n", "torch", ".", "save", "(", "bi_model", ".", "state_dict", "(", ")", ",", "logger", ".", "lastest_checkpoint_path", ")", "\n", "\n", "if", "max_score", "<", "score", ":", "\n", "                                        ", "torch", ".", "save", "(", "bi_model", ".", "state_dict", "(", ")", ",", "logger", ".", "checkpoint_path", ")", "\n", "max_score", "=", "score", "\n", "\n", "\n", "", "", "", "", "", "if", "args", ".", "local_rank", "in", "[", "0", ",", "-", "1", "]", ":", "\n", "                        ", "torch", ".", "save", "(", "bi_model", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "logger", ".", "sub_dir", ",", "'epoch_{}.bin'", ".", "format", "(", "e", ")", ")", ")", "\n", "\n", "", "", "", "grid_args", ".", "best_evaluation_score", "=", "max_score", "\n", "if", "args", ".", "local_rank", "in", "[", "0", ",", "-", "1", "]", ":", "\n", "                ", "logger", ".", "write_description_to_folder", "(", "os", ".", "path", ".", "join", "(", "logger", ".", "sub_dir", ",", "'description.txt'", ")", ",", "grid_args", ".", "__dict__", ")", "\n", "", "del", "optimizer", "\n", "\n", "", "if", "args", ".", "do_test", ":", "\n", "            ", "if", "args", ".", "do_train", "and", "args", ".", "local_rank", "in", "[", "0", ",", "-", "1", "]", ":", "\n", "                ", "bi_model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "logger", ".", "checkpoint_path", ",", "map_location", "=", "'cpu'", ")", ")", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "score", ",", "candidates", "=", "evaluate_bi_model", "(", "\n", "bi_model", ",", "tokenizer", ",", "test_dataset", ",", "\n", "mode", "=", "args", ".", "test_mode", ",", "\n", "device", "=", "args", ".", "device", ",", "\n", "local_rank", "=", "args", ".", "local_rank", ",", "\n", "n_gpu", "=", "args", ".", "n_gpu", ",", "\n", "encode_batch_size", "=", "args", ".", "eval_batch_size", ",", "\n", "view_expansion", "=", "args", ".", "view_expansion", ",", "\n", "is_accumulate_score", "=", "args", ".", "accumulate_score", ",", "\n", "merge_layers", "=", "args", ".", "merge_layers", ",", "\n", "top_k", "=", "args", ".", "top_k", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "                ", "if", "logger", "is", "not", "None", ":", "\n", "                    ", "result_path", "=", "os", ".", "path", ".", "join", "(", "logger", ".", "sub_dir", ",", "'score.json'", ")", "\n", "candidate_path", "=", "os", ".", "path", ".", "join", "(", "logger", ".", "sub_dir", ",", "'candidates.json'", ")", "\n", "", "else", ":", "\n", "                    ", "dir_path", "=", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "abspath", "(", "grid_args", ".", "bi_ckpt_path", ")", ")", "\n", "result_path", "=", "os", ".", "path", ".", "join", "(", "dir_path", ",", "'{}_score.json'", ".", "format", "(", "args", ".", "test_mode", ")", ")", "\n", "candidate_path", "=", "os", ".", "path", ".", "join", "(", "dir_path", ",", "'{}_candidates.json'", ".", "format", "(", "args", ".", "test_mode", ")", ")", "\n", "\n", "", "with", "open", "(", "result_path", ",", "'w'", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "json", ".", "dumps", "(", "score", ")", ")", "\n", "\n", "", "with", "open", "(", "candidate_path", ",", "'w'", ")", "as", "f", ":", "\n", "                    ", "for", "candidate", "in", "candidates", ":", "\n", "                        ", "f", ".", "write", "(", "json", ".", "dumps", "(", "candidate", ")", "+", "'\\n'", ")", "\n", "\n", "", "", "", "", "del", "bi_model", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.model.CrossEncoder.__init__": [[20, 25], ["torch.Module.__init__", "transformers.BertModel.from_pretrained", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.model.NCE_Random.__init__"], ["    ", "def", "__init__", "(", "self", ",", "pretrained_model", ")", ":", "\n", "        ", "super", "(", "CrossEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "cross_encoder", "=", "BertModel", ".", "from_pretrained", "(", "pretrained_model", ",", "output_attentions", "=", "True", ",", "return_dict", "=", "True", ")", "\n", "self", ".", "additional_linear", "=", "nn", ".", "Linear", "(", "self", ".", "cross_encoder", ".", "config", ".", "hidden_size", ",", "1", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "0.1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.model.CrossEncoder.to_bert_input": [[26, 33], ["torch.zeros_like().long", "torch.zeros_like().long", "torch.zeros_like().long", "torch.zeros_like().long", "torch.zeros_like().long", "torch.zeros_like().long", "torch.zeros_like().long", "torch.zeros_like().long", "torch.zeros_like().long", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like"], "methods", ["None"], ["", "def", "to_bert_input", "(", "self", ",", "input_ids", ")", ":", "\n", "        ", "attention_mask", "=", "1", "-", "(", "input_ids", "==", "0", ")", ".", "long", "(", ")", "\n", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", ".", "long", "(", ")", "\n", "return", "{", "\n", "'input_ids'", ":", "input_ids", ",", "\n", "'attention_mask'", ":", "attention_mask", ",", "\n", "'token_type_ids'", ":", "token_type_ids", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.model.CrossEncoder.forward": [[35, 59], ["ctx_ids.size", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "model.CrossEncoder.cross_encoder", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "sum", "model.CrossEncoder.to_bert_input"], "methods", ["home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.model.BiEncoder.to_bert_input"], ["", "def", "forward", "(", "self", ",", "ctx_ids", ",", "ctx_mask", "=", "None", ",", "target", "=", "None", ",", "is_return_score", "=", "False", ")", ":", "\n", "        ", "'''\n        - ctx_ids: batch_size/top_k * seq_len\n        - split_len: batch_size/top_k * [split_num]\n\n        Only support for batch_size = 1 / top_k = 1\n        '''", "\n", "\n", "batch_size", "=", "1", "\n", "top_k", ",", "seq_len", "=", "ctx_ids", ".", "size", "(", ")", "\n", "\n", "ctx_repr", "=", "self", ".", "cross_encoder", "(", "**", "self", ".", "to_bert_input", "(", "ctx_ids", ")", ")", ".", "last_hidden_state", "# (batch_size/top_k) * seq_len * hidden_siate", "\n", "\n", "\n", "scores", "=", "torch", ".", "cat", "(", "scores", ",", "0", ")", ".", "unsqueeze", "(", "0", ")", "# 1 * top_k * 9", "\n", "max_score", "=", "torch", ".", "max", "(", "scores", ",", "-", "1", ")", ".", "values", "\n", "if", "target", "is", "not", "None", ":", "\n", "\n", "            ", "loss", "=", "F", ".", "cross_entropy", "(", "max_score", ",", "target", ",", "reduction", "=", "\"mean\"", ")", "\n", "predict", "=", "torch", ".", "max", "(", "max_score", ",", "-", "1", ")", ".", "indices", "\n", "acc", "=", "sum", "(", "predict", "==", "target", ")", "*", "1.0", "\n", "return", "loss", ",", "acc", "\n", "", "else", ":", "\n", "            ", "return", "max_score", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.model.BiEncoder.__init__": [[62, 66], ["torch.Module.__init__", "transformers.BertModel.from_pretrained", "transformers.BertModel.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.model.NCE_Random.__init__"], ["    ", "def", "__init__", "(", "self", ",", "pretrained_model", ")", ":", "\n", "        ", "super", "(", "BiEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "ctx_encoder", "=", "BertModel", ".", "from_pretrained", "(", "pretrained_model", ",", "output_attentions", "=", "True", ",", "return_dict", "=", "True", ")", "\n", "self", ".", "ent_encoder", "=", "BertModel", ".", "from_pretrained", "(", "pretrained_model", ",", "output_attentions", "=", "True", ",", "return_dict", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.model.BiEncoder.to_bert_input": [[67, 74], ["torch.zeros_like().long", "torch.zeros_like().long", "torch.zeros_like().long", "torch.zeros_like().long", "torch.zeros_like().long", "torch.zeros_like().long", "torch.zeros_like().long", "torch.zeros_like().long", "torch.zeros_like().long", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like"], "methods", ["None"], ["", "def", "to_bert_input", "(", "self", ",", "input_ids", ")", ":", "\n", "        ", "attention_mask", "=", "1", "-", "(", "input_ids", "==", "0", ")", ".", "long", "(", ")", "\n", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", ".", "long", "(", ")", "\n", "return", "{", "\n", "'input_ids'", ":", "input_ids", ",", "\n", "'attention_mask'", ":", "attention_mask", ",", "\n", "'token_type_ids'", ":", "token_type_ids", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.model.BiEncoder.encode_candidates": [[76, 197], ["torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "len", "ent_ids.view.view.unsqueeze", "model.BiEncoder.ent_encoder", "torch.cat().unsqueeze.append", "torch.cat().unsqueeze.append", "torch.cat().unsqueeze.append", "ent_ids.view.view.view().tolist", "int", "zip", "ent_ids.view.view.size", "len", "ent_ids.view.view.size", "ent_ids.view.view.view", "ent_ids.view.view.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "range", "max", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "enumerate", "range", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "ent_ids.view.view.size", "model.BiEncoder.to_bert_input", "ent_ids.view.view.view", "len", "len", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "range", "sum", "len", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sort().indices[].tolist", "torch.sort().indices[].tolist", "torch.sort().indices[].tolist", "torch.sort().indices[].tolist", "torch.sort().indices[].tolist", "torch.sort().indices[].tolist", "torch.sort().indices[].tolist", "torch.sort().indices[].tolist", "torch.sort().indices[].tolist", "range", "torch.stack.append", "torch.stack.append", "torch.stack.append", "len", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "s.index", "len", "len", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss", "len", "len", "model.BiEncoder.encode_candidates.batch_sentences"], "methods", ["home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.model.BiEncoder.to_bert_input"], ["", "def", "encode_candidates", "(", "self", ",", "ent_ids", ",", "ent_mask", "=", "None", ",", "interval", "=", "500", ",", "mode", "=", "'train'", ",", "view_expansion", "=", "False", ",", "top_k", "=", "0.4", ",", "merge_layers", "=", "3", ")", ":", "\n", "        ", "sentence_num", "=", "0", "\n", "if", "len", "(", "ent_ids", ".", "size", "(", ")", ")", "==", "1", ":", "# only one sentence", "\n", "            ", "ent_ids", "=", "ent_ids", ".", "unsqueeze", "(", "0", ")", "\n", "batch_size", ",", "sentence_num", "=", "1", ",", "1", "\n", "", "elif", "len", "(", "ent_ids", ".", "size", "(", ")", ")", "==", "3", ":", "\n", "            ", "batch_size", ",", "sentence_num", ",", "ent_seq_len", "=", "ent_ids", ".", "size", "(", ")", "\n", "ent_ids", "=", "ent_ids", ".", "view", "(", "-", "1", ",", "ent_seq_len", ")", "\n", "", "else", ":", "\n", "            ", "batch_size", ",", "ent_seq_len", "=", "ent_ids", ".", "size", "(", ")", "\n", "sentence_num", "=", "1", "\n", "\n", "", "start_ids", "=", "0", "\n", "candidate_output", "=", "[", "]", "\n", "while", "start_ids", "<", "sentence_num", "*", "batch_size", ":", "\n", "            ", "model_output", "=", "self", ".", "ent_encoder", "(", "**", "self", ".", "to_bert_input", "(", "ent_ids", "[", "start_ids", ":", "start_ids", "+", "interval", "]", ")", ")", "\n", "candidate_output", ".", "append", "(", "model_output", ".", "last_hidden_state", "[", ":", ",", "0", ",", ":", "]", ")", "#batch_size * seq_len * hidden_size", "\n", "start_ids", "+=", "interval", "\n", "", "candidate_output", "=", "torch", ".", "cat", "(", "candidate_output", ",", "0", ")", ".", "view", "(", "batch_size", ",", "sentence_num", ",", "-", "1", ")", "\n", "\n", "if", "view_expansion", ":", "\n", "            ", "ori_views", "=", "candidate_output", "\n", "ori_ent_ids", "=", "ent_ids", ".", "view", "(", "batch_size", ",", "sentence_num", ",", "-", "1", ")", ".", "tolist", "(", ")", "\n", "new_pools", "=", "[", "]", "\n", "\n", "def", "merge_sequence", "(", "seq", ",", "ent_ids", ")", ":", "\n", "                ", "s", "=", "ent_ids", "[", "seq", "[", "0", "]", "]", "\n", "sentence", "=", "s", "[", ":", "s", ".", "index", "(", "102", ")", "]", "\n", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "seq", ")", ")", ":", "\n", "                    ", "s", "=", "ent_ids", "[", "seq", "[", "i", "]", "]", "\n", "mid_sentence", "=", "s", "[", "s", ".", "index", "(", "3", ")", "+", "1", ":", "s", ".", "index", "(", "102", ")", "]", "\n", "if", "0", "in", "mid_sentence", ":", "\n", "                        ", "mid_sentence", "=", "mid_sentence", "[", ":", "mid_sentence", ".", "index", "(", "0", ")", "]", "\n", "", "sentence", "+=", "mid_sentence", "\n", "", "if", "len", "(", "sentence", ")", ">", "511", ":", "\n", "                    ", "sentence", "=", "sentence", "[", ":", "511", "]", "\n", "", "sentence", "+=", "[", "102", "]", "\n", "return", "sentence", "\n", "\n", "", "def", "batch_sentences", "(", "sentences", ")", ":", "\n", "                ", "max_len", "=", "max", "(", "[", "len", "(", "s", ")", "for", "s", "in", "sentences", "]", ")", "\n", "s_tensor", "=", "torch", ".", "zeros", "(", "(", "len", "(", "sentences", ")", ",", "max_len", ")", ",", "dtype", "=", "torch", ".", "int64", ")", "\n", "for", "i", ",", "s", "in", "enumerate", "(", "sentences", ")", ":", "\n", "                    ", "s_tensor", "[", "i", ",", ":", "len", "(", "s", ")", "]", "=", "torch", ".", "tensor", "(", "s", ")", "\n", "", "return", "s_tensor", "\n", "\n", "", "top_k", "=", "[", "top_k", "for", "_", "in", "range", "(", "merge_layers", ")", "]", "\n", "target_sentence_num", "=", "int", "(", "sum", "(", "top_k", ")", "*", "sentence_num", ")", "\n", "for", "ori_view", ",", "ori_ent_id", "in", "zip", "(", "ori_views", ",", "ori_ent_ids", ")", ":", "\n", "                ", "new_pool", "=", "[", "]", "\n", "views", ",", "seq_ids", "=", "ori_view", ",", "[", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "ori_ent_id", ")", ")", "]", "\n", "\n", "for", "layer", "in", "range", "(", "len", "(", "top_k", ")", ")", ":", "\n", "                    ", "new_views", ",", "new_seq_ids", "=", "[", "]", ",", "[", "]", "\n", "dis", "=", "torch", ".", "sum", "(", "F", ".", "mse_loss", "(", "views", "[", ":", "-", "1", "]", ",", "views", "[", "1", ":", "]", ",", "reduction", "=", "'none'", ")", ",", "-", "1", ")", "\n", "merge_ids", "=", "torch", ".", "sort", "(", "dis", ",", "descending", "=", "True", ")", ".", "indices", "[", ":", "int", "(", "views", ".", "size", "(", "0", ")", "*", "top_k", "[", "layer", "]", ")", "]", ".", "tolist", "(", ")", "\n", "\n", "new_sentences", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "seq_ids", ")", ")", ":", "\n", "                        ", "if", "i", "in", "merge_ids", "and", "ori_ent_id", "[", "i", "]", "[", "0", "]", "!=", "0", ":", "\n", "                            ", "seq_ids_a", ",", "seq_ids_b", "=", "seq_ids", "[", "i", "]", ",", "seq_ids", "[", "i", "+", "1", "]", "\n", "seq_ids_merge", "=", "[", "]", "+", "seq_ids_a", "\n", "for", "ids", ",", "ids_b", "in", "enumerate", "(", "seq_ids_b", ")", ":", "\n", "                                ", "if", "ids_b", ">", "seq_ids_a", "[", "-", "1", "]", "and", "ori_ent_id", "[", "ids_b", "]", "[", "0", "]", "!=", "0", ":", "\n", "                                    ", "seq_ids_merge", "+=", "[", "ids_b", "]", "\n", "\n", "", "", "if", "len", "(", "seq_ids_merge", ")", "!=", "len", "(", "seq_ids_a", ")", ":", "\n", "                                ", "new_sentence", "=", "merge_sequence", "(", "seq_ids_merge", ",", "ori_ent_id", ")", "\n", "new_seq_ids", ".", "append", "(", "seq_ids_merge", ")", "\n", "\n", "new_sentences", ".", "append", "(", "new_sentence", ")", "\n", "#new_repr = self.ent_encoder(**self.to_bert_input(new_sentence.unsqueeze(0).cuda())).last_hidden_state[:, 0, :]", "\n", "#new_pool.append(new_repr)", "\n", "new_views", ".", "append", "(", "None", ")", "\n", "", "else", ":", "\n", "                                ", "new_seq_ids", ".", "append", "(", "seq_ids", "[", "i", "]", ")", "\n", "new_views", ".", "append", "(", "views", "[", "i", "]", ")", "\n", "", "", "else", ":", "\n", "                            ", "new_seq_ids", ".", "append", "(", "seq_ids", "[", "i", "]", ")", "\n", "new_views", ".", "append", "(", "views", "[", "i", "]", ")", "\n", "\n", "", "", "if", "len", "(", "new_sentences", ")", ">", "0", ":", "\n", "                        ", "new_sentences", "=", "batch_sentences", "(", "new_sentences", ")", "\n", "pool", "=", "[", "]", "\n", "start_ids", "=", "0", "\n", "while", "start_ids", "<", "new_sentences", ".", "size", "(", "0", ")", ":", "\n", "                            ", "new_repr", "=", "self", ".", "ent_encoder", "(", "**", "self", ".", "to_bert_input", "(", "new_sentences", "[", "start_ids", ":", "start_ids", "+", "50", "]", ".", "cuda", "(", ")", ")", ")", ".", "last_hidden_state", "[", ":", ",", "0", ",", ":", "]", "\n", "pool", "+=", "new_repr", "\n", "start_ids", "+=", "50", "\n", "", "new_repr", "=", "pool", "\n", "new_pool", "+=", "new_repr", "\n", "view_idx", "=", "0", "\n", "for", "i", ",", "new_view", "in", "enumerate", "(", "new_views", ")", ":", "\n", "                            ", "if", "new_view", "is", "None", ":", "\n", "                                ", "new_views", "[", "i", "]", "=", "new_repr", "[", "view_idx", "]", "\n", "view_idx", "+=", "1", "\n", "", "", "assert", "view_idx", "==", "len", "(", "new_repr", ")", "\n", "", "views", ",", "seq_ids", "=", "torch", ".", "stack", "(", "new_views", ",", "0", ")", ",", "new_seq_ids", "\n", "\n", "", "if", "mode", "==", "'train'", ":", "\n", "                    ", "if", "len", "(", "new_pool", ")", "==", "0", ":", "\n", "                        ", "new_pool", "=", "ori_view", "[", "-", "1", "]", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "target_sentence_num", ",", "1", ")", "\n", "", "else", ":", "\n", "                        ", "new_pool", "=", "torch", ".", "stack", "(", "new_pool", ",", "0", ")", "\n", "if", "len", "(", "new_pool", ")", "<", "target_sentence_num", ":", "\n", "                            ", "new_pool", "=", "torch", ".", "cat", "(", "[", "new_pool", ",", "ori_view", "[", "-", "1", "]", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "target_sentence_num", "-", "len", "(", "new_pool", ")", ",", "1", ")", "]", ",", "0", ")", "\n", "\n", "", "", "", "if", "mode", "==", "'train'", ":", "\n", "                    ", "new_pools", ".", "append", "(", "new_pool", ")", "\n", "", "else", ":", "\n", "                    ", "new_pools", "+=", "new_pool", "\n", "", "", "if", "mode", "==", "'train'", ":", "\n", "                ", "new_pools", "=", "torch", ".", "stack", "(", "new_pools", ",", "0", ")", "\n", "#print(new_pools.shape, ori_views.shape)", "\n", "candidate_output", "=", "torch", ".", "cat", "(", "[", "ori_views", ",", "new_pools", "]", ",", "1", ")", "\n", "", "else", ":", "\n", "                ", "if", "len", "(", "new_pools", ")", ">", "0", ":", "\n", "                    ", "new_pools", "=", "torch", ".", "stack", "(", "new_pools", ",", "0", ")", "\n", "candidate_output", "=", "torch", ".", "cat", "(", "[", "candidate_output", ".", "squeeze", "(", "0", ")", ",", "new_pools", "]", ",", "0", ")", ".", "unsqueeze", "(", "0", ")", "\n", "", "", "", "return", "candidate_output", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.model.BiEncoder.encode_context": [[198, 208], ["model.BiEncoder.ctx_encoder", "len", "ctx_ids.unsqueeze.unsqueeze.unsqueeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "ctx_ids.unsqueeze.unsqueeze.size", "model.BiEncoder.to_bert_input", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "ctx_mask.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.model.BiEncoder.to_bert_input"], ["", "def", "encode_context", "(", "self", ",", "ctx_ids", ",", "ctx_mask", "=", "None", ")", ":", "\n", "        ", "if", "len", "(", "ctx_ids", ".", "size", "(", ")", ")", "==", "1", ":", "# only one sentence", "\n", "            ", "ctx_ids", "=", "ctx_ids", ".", "unsqueeze", "(", "0", ")", "\n", "", "model_output", "=", "self", ".", "ctx_encoder", "(", "**", "self", ".", "to_bert_input", "(", "ctx_ids", ")", ")", "\n", "context_output", "=", "model_output", ".", "last_hidden_state", "\n", "if", "ctx_mask", "is", "None", ":", "\n", "            ", "context_output", "=", "context_output", "[", ":", ",", "0", ",", ":", "]", "\n", "", "else", ":", "\n", "            ", "context_output", "=", "torch", ".", "bmm", "(", "ctx_mask", ".", "unsqueeze", "(", "1", ")", ",", "context_output", ")", ".", "squeeze", "(", "1", ")", "\n", "", "return", "context_output", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.model.BiEncoder.score_candidates": [[209, 217], ["model.BiEncoder.encode_context().cpu().detach", "zip", "ctx_repr.to.to.to", "res.append", "model.BiEncoder.encode_context().cpu", "ctx_repr.to.to.unsqueeze().mm().squeeze", "model.BiEncoder.encode_context", "ctx_repr.to.to.unsqueeze().mm", "ctx_repr.to.to.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.model.BiEncoder.encode_context"], ["", "def", "score_candidates", "(", "self", ",", "ctx_ids", ",", "ctx_world", ",", "ctx_mask", "=", "None", ",", "candidate_pool", "=", "None", ")", ":", "\n", "# candidate_pool: (entity_num * 9) * hidden_state", "\n", "        ", "ctx_output", "=", "self", ".", "encode_context", "(", "ctx_ids", ",", "ctx_mask", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", "\n", "res", "=", "[", "]", "\n", "for", "world", ",", "ctx_repr", "in", "zip", "(", "ctx_world", ",", "ctx_output", ")", ":", "\n", "            ", "ctx_repr", "=", "ctx_repr", ".", "to", "(", "candidate_pool", "[", "world", "]", ".", "device", ")", "\n", "res", ".", "append", "(", "ctx_repr", ".", "unsqueeze", "(", "0", ")", ".", "mm", "(", "candidate_pool", "[", "world", "]", ".", "T", ")", ".", "squeeze", "(", "0", ")", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.model.BiEncoder.forward": [[218, 235], ["ent_ids.size", "model.BiEncoder.encode_context", "model.BiEncoder.encode_candidates", "model.BiEncoder.contiguous", "model.BiEncoder.contiguous"], "methods", ["home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.model.BiEncoder.encode_context", "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.model.BiEncoder.encode_candidates"], ["", "def", "forward", "(", "self", ",", "ctx_ids", ",", "ent_ids", ",", "num_gpus", "=", "0", ")", ":", "\n", "        ", "'''\n        if num_gpus > 1:\n            ctx_ids = ctx_ids.to(\"cuda:0\")\n            ent_gpus = num_gpus - 1\n            per_gpu_batch = ent_ids // ent_gpus\n            \n            ent_ids = ent_ids.to(\"cuda:1\")\n        elif num_gpus == 1:\n            ctx_ids = ctx_ids.cuda()\n            ent_ids = ent_ids.cuda()\n        '''", "\n", "batch_size", ",", "sentence_num", ",", "ent_seq_len", "=", "ent_ids", ".", "size", "(", ")", "\n", "\n", "ctx_output", "=", "self", ".", "encode_context", "(", "ctx_ids", ")", "# batch_size * hidden_size", "\n", "ent_output", "=", "self", ".", "encode_candidates", "(", "ent_ids", ",", "view_expansion", "=", "False", ")", "# (batch_size * sentence_num) * hidden_size", "\n", "return", "ctx_output", ".", "contiguous", "(", ")", ",", "ent_output", ".", "contiguous", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.model.NCE_Random.__init__": [[237, 240], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.model.NCE_Random.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_gpus", ")", ":", "\n", "        ", "super", "(", "NCE_Random", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_gpus", "=", "num_gpus", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.model.NCE_Random.forward": [[241, 260], ["torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "score.permute.permute.permute", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "score.permute.permute.size", "muver.utils.multigpu.GatherLayer.apply", "muver.utils.multigpu.GatherLayer.apply", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "sum", "score.permute.permute.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "ctx_output", ",", "ent_output", ")", ":", "\n", "        ", "batch_size", "=", "ctx_output", ".", "size", "(", "0", ")", "\n", "sentence_num", "=", "ent_output", ".", "size", "(", "0", ")", "//", "batch_size", "\n", "\n", "if", "self", ".", "num_gpus", ">", "1", ":", "\n", "            ", "ctx_output", "=", "torch", ".", "cat", "(", "GatherLayer", ".", "apply", "(", "ctx_output", ")", ",", "dim", "=", "0", ")", "\n", "ent_output", "=", "torch", ".", "cat", "(", "GatherLayer", ".", "apply", "(", "ent_output", ")", ",", "dim", "=", "0", ")", "\n", "\n", "#score = ent_output.mm(ctx_output.T).view(batch_size * self.num_gpus, sentence_num, batch_size * self.num_gpus) #(batch_size * sentence_num) * batch_size(context)", "\n", "", "score", "=", "torch", ".", "matmul", "(", "ent_output", ",", "ctx_output", ".", "T", ")", "\n", "score", "=", "score", ".", "permute", "(", "2", ",", "0", ",", "1", ")", "# batch_size(context) * batch_size(entity) * sentence_num", "\n", "max_score", "=", "torch", ".", "max", "(", "score", ",", "-", "1", ")", ".", "values", "\n", "target", "=", "torch", ".", "arange", "(", "score", ".", "size", "(", "0", ")", ")", ".", "to", "(", "ctx_output", ".", "device", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "max_score", ",", "target", ",", "reduction", "=", "\"mean\"", ")", "\n", "\n", "predict", "=", "torch", ".", "max", "(", "max_score", ",", "-", "1", ")", ".", "indices", "\n", "acc", "=", "sum", "(", "predict", "==", "target", ")", "*", "1.0", "/", "score", ".", "size", "(", "0", ")", "\n", "\n", "return", "loss", ",", "acc", ",", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.zeshel_evaluate.pretty_visualize": [[25, 34], ["scores.items", "prettytable.PrettyTable", "prettytable.PrettyTable.add_rows", "print", "rows.append", "round"], "function", ["None"], ["def", "pretty_visualize", "(", "scores", ",", "top_k", ")", ":", "\n", "    ", "rows", "=", "[", "]", "\n", "for", "world", ",", "score", "in", "scores", ".", "items", "(", ")", ":", "\n", "        ", "rows", ".", "append", "(", "[", "world", "]", "+", "[", "round", "(", "s", "*", "1.0", "/", "score", "[", "1", "]", ",", "4", ")", "for", "s", "in", "score", "[", "0", "]", "]", ")", "\n", "\n", "", "table", "=", "PrettyTable", "(", ")", "\n", "table", ".", "field_names", "=", "[", "\"World\"", "]", "+", "[", "\"R@{}\"", ".", "format", "(", "k", ")", "for", "k", "in", "top_k", "]", "\n", "table", ".", "add_rows", "(", "rows", ")", "\n", "print", "(", "table", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.zeshel_evaluate.evaluate_bi_model": [[35, 167], ["model.eval", "isinstance", "dataset.entity_desc.items", "torch.save", "torch.save", "range", "world_entity_titles.items", "torch.utils.data.DataLoader", "tqdm.tqdm", "print", "zeshel_evaluate.pretty_visualize", "torch.utils.data.DataLoader", "tqdm.tqdm", "torch.cat", "torch.cat", "torch.distributed.barrier", "torch.distributed.barrier", "torch.load", "torch.load", "torch.cat().to", "torch.cat().to", "enumerate", "test_module.score_candidates", "zip", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.SequentialSampler", "test_module.encode_candidates().squeeze().detach().to", "entity_pool.append", "world_entity_titles.get", "world_entity_pool.get", "world_entity_pool.get.append", "ids_range.get", "ids_range.get.append", "len", "torch.softmax", "torch.softmax", "torch.sort().indices.cpu", "torch.sort().indices.cpu", "torch.sort().values.cpu", "torch.sort().values.cpu", "dataset.entity_desc[].get_nth_title", "enumerate", "candidates.append", "test_module.encode_candidates().squeeze().detach().to.size", "torch.cat", "torch.cat", "len", "batch[].to", "test_module.encode_candidates().squeeze().detach", "len", "len", "sorted", "len", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "predict_title_dict.keys", "predict_title_dict.get", "predict_title.append", "test_module.encode_candidates().squeeze", "test_module.encode_candidates", "sample[].cuda"], "function", ["home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.zeshel_evaluate.pretty_visualize", "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.model.BiEncoder.score_candidates", "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.data_loader.EncodeDataset.get_nth_title", "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.model.BiEncoder.encode_candidates"], ["", "def", "evaluate_bi_model", "(", "model", ",", "tokenizer", ",", "dataset", ",", "mode", ",", "encode_batch_size", "=", "16", ",", "device", "=", "\"cpu\"", ",", "local_rank", "=", "-", "1", ",", "n_gpu", "=", "1", ",", "\n", "view_expansion", "=", "False", ",", "top_k", "=", "0.4", ",", "merge_layers", "=", "3", ",", "is_accumulate_score", "=", "False", ")", ":", "\n", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "if", "isinstance", "(", "model", ",", "torch", ".", "nn", ".", "parallel", ".", "DistributedDataParallel", ")", ":", "\n", "        ", "test_module", "=", "model", ".", "module", "\n", "", "else", ":", "\n", "        ", "test_module", "=", "model", "\n", "\n", "", "world_entity_pool", ",", "world_entity_titles", "=", "{", "}", ",", "{", "}", "\n", "for", "world", ",", "world_dataset", "in", "dataset", ".", "entity_desc", ".", "items", "(", ")", ":", "\n", "        ", "entity_pool", ",", "entity_title", "=", "[", "]", ",", "[", "]", "\n", "if", "n_gpu", ">", "1", ":", "\n", "            ", "sampler", "=", "DistributedSampler", "(", "world_dataset", ")", "\n", "", "else", ":", "\n", "            ", "sampler", "=", "SequentialSampler", "(", "world_dataset", ")", "\n", "", "encode_dataloader", "=", "DataLoader", "(", "dataset", "=", "world_dataset", ",", "batch_size", "=", "1", ",", "collate_fn", "=", "bi_collate_fn", ",", "shuffle", "=", "False", ",", "sampler", "=", "sampler", ")", "\n", "\n", "disable", "=", "True", "if", "local_rank", "not", "in", "[", "-", "1", ",", "0", "]", "else", "False", "\n", "for", "sample", "in", "tqdm", "(", "encode_dataloader", ",", "disable", "=", "disable", ")", ":", "\n", "            ", "candidate_encode", "=", "test_module", ".", "encode_candidates", "(", "\n", "ent_ids", "=", "sample", "[", "'token_ids'", "]", ".", "cuda", "(", ")", ",", "\n", "view_expansion", "=", "view_expansion", ",", "\n", "top_k", "=", "top_k", ",", "\n", "merge_layers", "=", "merge_layers", ",", "\n", "mode", "=", "'test'", "\n", ")", ".", "squeeze", "(", "0", ")", ".", "detach", "(", ")", ".", "to", "(", "\"cpu\"", ")", "# not support for encode_batch_size > 1", "\n", "entity_pool", ".", "append", "(", "candidate_encode", ")", "\n", "entity_title", "+=", "[", "sample", "[", "'title'", "]", "[", "0", "]", "]", "*", "candidate_encode", ".", "size", "(", "0", ")", "\n", "\n", "", "world_entity_pool", "[", "world", "]", "=", "torch", ".", "cat", "(", "entity_pool", ",", "0", ")", "\n", "world_entity_titles", "[", "world", "]", "=", "entity_title", "\n", "\n", "", "torch", ".", "save", "(", "[", "world_entity_pool", ",", "world_entity_titles", "]", ",", "'entity_{}.pt'", ".", "format", "(", "local_rank", ")", ")", "\n", "\n", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "\n", "", "if", "local_rank", "not", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "        ", "return", "None", ",", "None", "\n", "\n", "", "world_entity_pool", ",", "world_entity_titles", "=", "{", "}", ",", "{", "}", "\n", "for", "i", "in", "range", "(", "n_gpu", ")", ":", "\n", "        ", "sub_entity_pool", ",", "sub_entity_titles", "=", "torch", ".", "load", "(", "'entity_{}.pt'", ".", "format", "(", "i", ")", ",", "map_location", "=", "'cpu'", ")", "\n", "for", "world_name", ",", "world_num", "in", "WORLDS", "[", "mode", "]", ":", "\n", "            ", "titles", "=", "world_entity_titles", ".", "get", "(", "world_name", ",", "[", "]", ")", "\n", "pool", "=", "world_entity_pool", ".", "get", "(", "world_name", ",", "[", "]", ")", "\n", "\n", "sub_titles", "=", "sub_entity_titles", "[", "world_name", "]", "\n", "sub_pool", "=", "sub_entity_pool", "[", "world_name", "]", "\n", "if", "world_num", "%", "n_gpu", "and", "world_num", "%", "n_gpu", "-", "1", "<", "i", ":", "\n", "                ", "end_idx", "=", "len", "(", "sub_titles", ")", "-", "2", "\n", "while", "sub_titles", "[", "end_idx", "]", "==", "sub_titles", "[", "-", "1", "]", ":", "\n", "                    ", "end_idx", "-=", "1", "\n", "\n", "", "sub_titles", "=", "sub_titles", "[", ":", "end_idx", "+", "1", "]", "\n", "sub_pool", "=", "sub_pool", "[", ":", "end_idx", "+", "1", ",", ":", "]", "\n", "\n", "", "titles", "+=", "sub_titles", "\n", "world_entity_titles", "[", "world_name", "]", "=", "titles", "\n", "\n", "pool", ".", "append", "(", "sub_pool", ")", "\n", "world_entity_pool", "[", "world_name", "]", "=", "pool", "\n", "\n", "", "", "for", "key", ",", "_", "in", "WORLDS", "[", "mode", "]", ":", "\n", "        ", "pool", "=", "world_entity_pool", "[", "key", "]", "\n", "pool", "=", "torch", ".", "cat", "(", "pool", ",", "0", ")", ".", "to", "(", "\"cuda:0\"", ")", "\n", "world_entity_pool", "[", "key", "]", "=", "pool", "\n", "#print(world_entity_pool[key].shape)", "\n", "\n", "", "world_entity_ids_range", "=", "{", "}", "\n", "for", "key", ",", "titles", "in", "world_entity_titles", ".", "items", "(", ")", ":", "\n", "        ", "ids_range", "=", "{", "}", "\n", "for", "ids", ",", "title", "in", "enumerate", "(", "titles", ")", ":", "\n", "            ", "title_range", "=", "ids_range", ".", "get", "(", "title", ",", "[", "]", ")", "\n", "title_range", ".", "append", "(", "ids", ")", "\n", "ids_range", "[", "title", "]", "=", "title_range", "\n", "", "world_entity_ids_range", "[", "key", "]", "=", "ids_range", "\n", "\n", "", "top_k", "=", "[", "1", ",", "2", ",", "4", ",", "8", ",", "16", ",", "32", ",", "50", ",", "64", "]", "\n", "score_metrics", "=", "{", "world_name", ":", "[", "[", "0", "]", "*", "len", "(", "top_k", ")", ",", "0", "]", "for", "world_name", ",", "_", "in", "WORLDS", "[", "mode", "]", "}", "\n", "score_metrics", "[", "'total'", "]", "=", "[", "[", "0", "]", "*", "len", "(", "top_k", ")", ",", "0", "]", "\n", "candidates", "=", "[", "]", "\n", "# Then Encode the entities and Compare", "\n", "dataloader", "=", "DataLoader", "(", "dataset", "=", "dataset", ",", "batch_size", "=", "encode_batch_size", ",", "collate_fn", "=", "cross_collate_fn", ",", "shuffle", "=", "False", ")", "\n", "for", "batch", "in", "tqdm", "(", "dataloader", ")", ":", "\n", "        ", "worlds", ",", "labels", "=", "batch", "[", "'world'", "]", ",", "batch", "[", "'label_world_idx'", "]", "\n", "predict_scores", "=", "test_module", ".", "score_candidates", "(", "\n", "ctx_ids", "=", "batch", "[", "'context_ids'", "]", ".", "to", "(", "\"cuda:0\"", ")", ",", "\n", "ctx_world", "=", "batch", "[", "'world'", "]", ",", "\n", "candidate_pool", "=", "world_entity_pool", "\n", ")", "# [candidates_num] * batch_size", "\n", "\n", "for", "predict_score", ",", "world", ",", "label", "in", "zip", "(", "predict_scores", ",", "worlds", ",", "labels", ")", ":", "\n", "            ", "predict_score", "=", "torch", ".", "softmax", "(", "predict_score", ",", "-", "1", ")", "\n", "predict_ids", "=", "torch", ".", "sort", "(", "predict_score", ",", "-", "1", ",", "descending", "=", "True", ")", ".", "indices", ".", "cpu", "(", ")", "\n", "scores", "=", "torch", ".", "sort", "(", "predict_score", ",", "-", "1", ",", "descending", "=", "True", ")", ".", "values", ".", "cpu", "(", ")", "\n", "label_title", "=", "dataset", ".", "entity_desc", "[", "world", "]", ".", "get_nth_title", "(", "label", ")", "\n", "accumulate_score", "=", "is_accumulate_score", "\n", "if", "accumulate_score", ":", "\n", "                ", "predict_title_dict", "=", "{", "}", "\n", "\n", "ids", "=", "0", "\n", "while", "len", "(", "predict_title_dict", ".", "keys", "(", ")", ")", "<", "200", ":", "#top_k[-1]:", "\n", "                    ", "title", "=", "world_entity_titles", "[", "world", "]", "[", "predict_ids", "[", "ids", "]", "]", "\n", "title_score", "=", "predict_title_dict", ".", "get", "(", "title", ",", "0", ")", "+", "scores", "[", "ids", "]", "\n", "predict_title_dict", "[", "title", "]", "=", "title_score", "\n", "\n", "ids", "+=", "1", "\n", "", "predict_title", "=", "sorted", "(", "predict_title_dict", ",", "key", "=", "predict_title_dict", ".", "get", ")", "[", ":", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "predict_title", "=", "[", "]", "\n", "ids", "=", "0", "\n", "while", "len", "(", "predict_title", ")", "<", "64", ":", "\n", "                    ", "title", "=", "world_entity_titles", "[", "world", "]", "[", "predict_ids", "[", "ids", "]", "]", "\n", "if", "title", "not", "in", "predict_title", ":", "\n", "                        ", "predict_title", ".", "append", "(", "title", ")", "\n", "", "ids", "+=", "1", "\n", "\n", "", "", "for", "k_idx", ",", "k", "in", "enumerate", "(", "top_k", ")", ":", "\n", "                ", "if", "label_title", "in", "predict_title", "[", ":", "k", "]", ":", "\n", "                    ", "score_metrics", "[", "world", "]", "[", "0", "]", "[", "k_idx", "]", "+=", "1", "\n", "score_metrics", "[", "'total'", "]", "[", "0", "]", "[", "k_idx", "]", "+=", "1", "\n", "", "", "score_metrics", "[", "world", "]", "[", "1", "]", "+=", "1", "\n", "score_metrics", "[", "'total'", "]", "[", "1", "]", "+=", "1", "\n", "\n", "candidates", ".", "append", "(", "[", "{", "'title'", ":", "title", "}", "for", "title", "in", "predict_title", "]", ")", "\n", "", "", "print", "(", "score_metrics", ")", "\n", "pretty_visualize", "(", "score_metrics", ",", "top_k", ")", "\n", "\n", "return", "score_metrics", "[", "'total'", "]", "[", "0", "]", "[", "-", "1", "]", ",", "candidates", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba-nlp_muver.multi_view.zeshel_evaluate.evaluate_cross_model": [[168, 201], ["model.eval", "isinstance", "torch.utils.data.DataLoader", "tqdm.tqdm", "batch[].to", "model", "zip", "ctx_ids.squeeze.squeeze", "torch.max", "torch.max"], "function", ["None"], ["", "def", "evaluate_cross_model", "(", "model", ",", "tokenizer", ",", "dataset", ",", "mode", ",", "encode_batch_size", "=", "1", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "if", "isinstance", "(", "model", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "        ", "test_module", "=", "model", ".", "module", "\n", "", "else", ":", "\n", "        ", "test_module", "=", "model", "\n", "\n", "", "dataloader", "=", "DataLoader", "(", "dataset", "=", "dataset", ",", "batch_size", "=", "encode_batch_size", ",", "num_workers", "=", "1", ",", "shuffle", "=", "False", ",", "collate_fn", "=", "collate_fn", ")", "\n", "#normalized_correct, normalized_total, unnormalized_total = 0, 0, 0", "\n", "score_metrics", "=", "{", "world_name", ":", "[", "0", ",", "0", ",", "0", "]", "for", "world_name", ",", "_", "in", "WORLDS", "[", "mode", "]", "}", "\n", "score_metrics", "[", "'total'", "]", "=", "[", "0", ",", "0", ",", "0", "]", "\n", "for", "batch", "in", "tqdm", "(", "dataloader", ")", ":", "\n", "        ", "ctx_ids", "=", "batch", "[", "'candidate_ids'", "]", ".", "to", "(", "device", ")", "\n", "split_len", "=", "batch", "[", "'split_len'", "]", "\n", "if", "encode_batch_size", "==", "1", ":", "\n", "            ", "ctx_ids", "=", "ctx_ids", ".", "squeeze", "(", "0", ")", "\n", "split_len", "=", "split_len", "[", "0", "]", "\n", "\n", "", "score", ",", "_", "=", "model", "(", "ctx_ids", ",", "split_len", "=", "split_len", ",", "target", "=", "None", ")", "# batch_size * top_k", "\n", "predict_idx", "=", "torch", ".", "max", "(", "score", ",", "-", "1", ")", ".", "indices", "\n", "\n", "for", "idx", ",", "t", ",", "w", "in", "zip", "(", "predict_idx", ",", "batch", "[", "'label'", "]", ",", "batch", "[", "'world'", "]", ")", ":", "\n", "            ", "if", "idx", "==", "t", ":", "\n", "                ", "score_metrics", "[", "w", "]", "[", "0", "]", "+=", "1", "\n", "score_metrics", "[", "'total'", "]", "[", "0", "]", "+=", "1", "\n", "", "if", "t", "!=", "-", "1", ":", "\n", "                ", "score_metrics", "[", "w", "]", "[", "1", "]", "+=", "1", "\n", "score_metrics", "[", "'total'", "]", "[", "1", "]", "+=", "1", "\n", "\n", "", "score_metrics", "[", "w", "]", "[", "2", "]", "+=", "1", "\n", "score_metrics", "[", "'total'", "]", "[", "2", "]", "+=", "1", "\n", "\n", "", "", "return", "score_metrics", "\n", "\n"]]}