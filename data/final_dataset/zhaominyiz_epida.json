{"home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_sst_epida_eda.Collect_FN.__init__": [[103, 113], ["super().__init__", "transformers.BertTokenizer.from_pretrained", "transformers.XLNetTokenizer.from_pretrained", "transformers.XLMRobertaTokenizer.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.EPDADataSet.__init__"], ["    ", "def", "__init__", "(", "self", ",", "with_label_hard", ",", "with_GT_labels", "=", "False", ")", ":", "\n", "        ", "super", "(", "Collect_FN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "MODEL_NAME", "==", "'Bert'", ":", "\n", "            ", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ",", "local_files_only", "=", "True", ")", "\n", "", "elif", "MODEL_NAME", "==", "'XLNet'", ":", "\n", "            ", "self", ".", "tokenizer", "=", "XLNetTokenizer", ".", "from_pretrained", "(", "'xlnet-base-cased'", ",", "local_files_only", "=", "True", ")", "\n", "", "elif", "MODEL_NAME", "==", "'XLM-R'", ":", "\n", "            ", "self", ".", "tokenizer", "=", "XLMRobertaTokenizer", ".", "from_pretrained", "(", "'xlm-roberta-base'", ",", "local_files_only", "=", "True", ")", "\n", "", "self", ".", "with_label", "=", "with_label_hard", "\n", "self", ".", "with_GT_labels", "=", "with_GT_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_sst_epida_eda.Collect_FN.__call__": [[114, 134], ["train_sst_epida_eda.Collect_FN.tokenizer", "map", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "zip", "map", "zip", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "batchs", ")", ":", "\n", "# print(batchs)", "\n", "        ", "if", "(", "self", ".", "with_label", "and", "self", ".", "with_GT_labels", "==", "False", ")", ":", "\n", "            ", "sentences", ",", "labels", "=", "map", "(", "list", ",", "zip", "(", "*", "batchs", ")", ")", "\n", "", "elif", "(", "self", ".", "with_label", "==", "True", "and", "self", ".", "with_GT_labels", "==", "True", ")", ":", "\n", "            ", "sentences", ",", "labels", ",", "GT_labels", "=", "map", "(", "list", ",", "zip", "(", "*", "batchs", ")", ")", "\n", "", "else", ":", "\n", "            ", "sentences", "=", "batchs", "\n", "", "encoding", "=", "self", ".", "tokenizer", "(", "sentences", ",", "return_tensors", "=", "'pt'", ",", "padding", "=", "True", ",", "truncation", "=", "True", ")", "\n", "# input_ids = encoding['input_ids']", "\n", "# attention_mask = encoding['attention_mask']", "\n", "# ans = {'input_ids': input_ids, 'attention_mask': attention_mask}", "\n", "if", "(", "self", ".", "with_label", ")", ":", "\n", "            ", "labels", "=", "torch", ".", "tensor", "(", "labels", ")", ".", "long", "(", ")", "\n", "encoding", "[", "'labels'", "]", "=", "labels", "\n", "", "if", "(", "self", ".", "with_GT_labels", ")", ":", "\n", "            ", "GT_labels", "=", "torch", ".", "tensor", "(", "GT_labels", ")", ".", "long", "(", ")", "\n", "encoding", "[", "'GT_labels'", "]", "=", "GT_labels", "\n", "", "encoding", "[", "'sentences'", "]", "=", "sentences", "\n", "return", "encoding", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_sst_epida_eda.EPDADataSet.__init__": [[137, 181], ["print", "open().readlines", "float", "range", "enumerate", "print", "print", "line.split", "int", "train_sst_epida_eda.EPDADataSet.get_only_chars", "Xs.append", "Ys.append", "sum", "len", "len", "open", "len", "float"], "methods", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_only_chars"], ["    ", "def", "__init__", "(", "self", ",", "input_dir", ",", "max_len", "=", "30", ",", "num_classes", "=", "2", ")", ":", "\n", "        ", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "dir", "=", "input_dir", "\n", "print", "(", "\"Start to read: \"", ",", "input_dir", ",", "flush", "=", "True", ")", "\n", "# pre-load", "\n", "lines", "=", "open", "(", "input_dir", ",", "'r'", ")", ".", "readlines", "(", ")", "\n", "Xs", ",", "Ys", "=", "[", "]", ",", "[", "]", "\n", "count", "=", "[", "0", "]", "*", "num_classes", "\n", "for", "line", "in", "lines", ":", "\n", "            ", "y", ",", "x", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "y", "=", "int", "(", "y", ")", "\n", "\n", "x", "=", "x", "[", ":", "-", "1", "]", "\n", "# if 'train' in input_dir:", "\n", "#     if y==0 or y==2:", "\n", "#         continue", "\n", "# if count[y] >= int(434*int(data_split)/10*2) and 'train' in input_dir:", "\n", "#     continue", "\n", "count", "[", "y", "]", "+=", "1", "\n", "if", "len", "(", "x", ")", "<=", "2", ":", "\n", "                ", "continue", "\n", "", "x", "=", "self", ".", "get_only_chars", "(", "x", ")", "\n", "Xs", ".", "append", "(", "x", ")", "\n", "Ys", ".", "append", "(", "y", ")", "\n", "", "weight_per_class", "=", "[", "0.", "]", "*", "num_classes", "\n", "N", "=", "float", "(", "sum", "(", "count", ")", ")", "\n", "for", "i", "in", "range", "(", "num_classes", ")", ":", "\n", "            ", "weight_per_class", "[", "i", "]", "=", "N", "/", "float", "(", "count", "[", "i", "]", ")", "\n", "", "weight", "=", "[", "0", "]", "*", "len", "(", "Ys", ")", "\n", "for", "idx", ",", "val", "in", "enumerate", "(", "Ys", ")", ":", "\n", "            ", "weight", "[", "idx", "]", "=", "weight_per_class", "[", "val", "]", "\n", "", "self", ".", "weights", "=", "weight_per_class", "\n", "print", "(", "weight_per_class", ",", "count", ")", "\n", "# os._exit(233)", "\n", "# if not 'test' in input_dir:", "\n", "#     Xs,Ys = self.upsample_balance(Xs,Ys)", "\n", "#     print(\"Balance dataset Over.\")", "\n", "self", ".", "Xs", "=", "Xs", "\n", "self", ".", "Ys", "=", "Ys", "\n", "\n", "self", ".", "O_Xs", "=", "self", ".", "Xs", "\n", "self", ".", "O_Ys", "=", "self", ".", "Ys", "\n", "print", "(", "\"Load Over, Find: \"", ",", "len", "(", "self", ".", "Xs", ")", ",", "\" datas.\"", ",", "flush", "=", "True", ")", "\n", "", "def", "get_only_chars", "(", "self", ",", "line", ")", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_sst_epida_eda.EPDADataSet.get_only_chars": [[181, 202], ["line.replace.replace.lower", "line.replace.replace.replace", "line.replace.replace.replace", "line.replace.replace.replace", "line.replace.replace.replace", "line.replace.replace.replace", "re.sub"], "methods", ["None"], ["", "def", "get_only_chars", "(", "self", ",", "line", ")", ":", "\n", "\n", "        ", "clean_line", "=", "\"\"", "\n", "\n", "line", "=", "line", ".", "lower", "(", ")", "\n", "line", "=", "line", ".", "replace", "(", "\" 's\"", ",", "\" is\"", ")", "\n", "line", "=", "line", ".", "replace", "(", "\"-\"", ",", "\" \"", ")", "#replace hyphens with spaces", "\n", "line", "=", "line", ".", "replace", "(", "\"\\t\"", ",", "\" \"", ")", "\n", "line", "=", "line", ".", "replace", "(", "\"\\n\"", ",", "\" \"", ")", "\n", "line", "=", "line", ".", "replace", "(", "\"'\"", ",", "\"\"", ")", "\n", "\n", "for", "char", "in", "line", ":", "\n", "            ", "if", "char", "in", "'qwertyuiopasdfghjklzxcvbnm '", ":", "\n", "                ", "clean_line", "+=", "char", "\n", "", "else", ":", "\n", "                ", "clean_line", "+=", "' '", "\n", "\n", "", "", "clean_line", "=", "re", ".", "sub", "(", "' +'", ",", "' '", ",", "clean_line", ")", "#delete extra spaces", "\n", "if", "clean_line", "[", "0", "]", "==", "' '", ":", "\n", "            ", "clean_line", "=", "clean_line", "[", "1", ":", "]", "\n", "", "return", "clean_line", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_sst_epida_eda.EPDADataSet.__getitem__": [[203, 206], ["len"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "assert", "idx", "<", "len", "(", "self", ".", "Xs", ")", "\n", "return", "self", ".", "Xs", "[", "idx", "]", ",", "self", ".", "Ys", "[", "idx", "]", "\n", "", "def", "__len__", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_sst_epida_eda.EPDADataSet.__len__": [[206, 208], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "Xs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_sst_epida_eda.EPDADataSet.update": [[209, 218], ["print", "print", "len", "len"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "Xs", ",", "Ys", ")", ":", "\n", "        ", "print", "(", "\"Start Update Dataset, Find \"", ",", "len", "(", "self", ".", "Xs", ")", ",", "'datas.'", ",", "flush", "=", "True", ")", "\n", "# if not 'test' in self.dir:", "\n", "#     Xs,Ys = self.upsample_balance(Xs,Ys)", "\n", "#     print(\"Balance dataset Over.\")", "\n", "self", ".", "Xs", "=", "Xs", "\n", "self", ".", "Ys", "=", "Ys", "\n", "print", "(", "\"Update Dataset Finish, Find \"", ",", "len", "(", "self", ".", "Xs", ")", ",", "'datas.'", ",", "flush", "=", "True", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_sst_epida_eda.EPDADataSet.reset": [[219, 223], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "O_Xs", "is", "not", "None", ":", "\n", "            ", "self", ".", "Xs", "=", "self", ".", "O_Xs", "\n", "self", ".", "Ys", "=", "self", ".", "O_Ys", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_sst_epida_eda.EPDADataSet.upsample_balance": [[224, 249], ["numpy.array", "numpy.max", "zip", "enumerate", "range", "sentence_each_class[].append", "zip", "range", "range", "append_cur_class.append", "ans_sentences.append", "ans_labels.append", "len"], "methods", ["None"], ["", "", "def", "upsample_balance", "(", "self", ",", "sentences", ",", "labels", ")", ":", "\n", "        ", "sample_number_per_class", "=", "[", "0", "]", "*", "self", ".", "num_classes", "\n", "for", "y", "in", "labels", ":", "\n", "            ", "sample_number_per_class", "[", "y", "]", "+=", "1", "\n", "", "sample_number_per_class", "=", "np", ".", "array", "(", "sample_number_per_class", ")", "\n", "max_number", "=", "np", ".", "max", "(", "sample_number_per_class", ")", "\n", "fill_number_each_class", "=", "max_number", "-", "sample_number_per_class", "\n", "# print(\"??\",sample_number_per_class,fill_number_each_class)", "\n", "sentence_each_class", "=", "[", "[", "]", "for", "i", "in", "range", "(", "self", ".", "num_classes", ")", "]", "\n", "for", "s", ",", "l", "in", "zip", "(", "sentences", ",", "labels", ")", ":", "\n", "            ", "sentence_each_class", "[", "l", "]", ".", "append", "(", "s", ")", "\n", "", "for", "class_index", ",", "(", "sentences_cur_class", ",", "fill_num_cur_class", ")", "in", "enumerate", "(", "\n", "zip", "(", "sentence_each_class", ",", "fill_number_each_class", ")", ")", ":", "\n", "            ", "append_cur_class", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "fill_num_cur_class", ")", ":", "\n", "                ", "append_cur_class", ".", "append", "(", "sentences_cur_class", "[", "i", "%", "len", "(", "sentences_cur_class", ")", "]", ")", "\n", "", "sentence_each_class", "[", "class_index", "]", "=", "sentences_cur_class", "+", "append_cur_class", "\n", "", "ans_sentences", "=", "[", "]", "\n", "ans_labels", "=", "[", "]", "\n", "for", "class_index", "in", "range", "(", "self", ".", "num_classes", ")", ":", "\n", "            ", "for", "s", "in", "sentence_each_class", "[", "class_index", "]", ":", "\n", "                ", "ans_sentences", ".", "append", "(", "s", ")", "\n", "ans_labels", ".", "append", "(", "class_index", ")", "\n", "", "", "return", "ans_sentences", ",", "ans_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_sst_epida_eda.get_world_size": [[67, 73], ["torch.get_world_size", "torch.is_available", "torch.is_initialized"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.get_world_size"], ["def", "get_world_size", "(", ")", ":", "\n", "    ", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "        ", "return", "1", "\n", "", "if", "not", "dist", ".", "is_initialized", "(", ")", ":", "\n", "        ", "return", "1", "\n", "", "return", "dist", ".", "get_world_size", "(", ")", "\n", "", "def", "move_to_device", "(", "batch", ",", "rank", "=", "None", ")", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_sst_epida_eda.move_to_device": [[73, 86], ["batch[].to"], "function", ["None"], ["", "def", "move_to_device", "(", "batch", ",", "rank", "=", "None", ")", ":", "\n", "    ", "ans", "=", "{", "}", "\n", "if", "(", "rank", "is", "None", ")", ":", "\n", "        ", "device", "=", "'cuda'", "\n", "", "else", ":", "\n", "        ", "device", "=", "'cuda:{}'", ".", "format", "(", "rank", ")", "\n", "", "for", "key", "in", "batch", ":", "\n", "        ", "try", ":", "\n", "            ", "ans", "[", "key", "]", "=", "batch", "[", "key", "]", ".", "to", "(", "device", "=", "device", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "# print(str(e))", "\n", "            ", "ans", "[", "key", "]", "=", "batch", "[", "key", "]", "\n", "", "", "return", "ans", "\n", "", "def", "reduce_loss_dict", "(", "loss_dict", ")", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_sst_epida_eda.reduce_loss_dict": [[86, 102], ["train_sst_epida_eda.get_world_size", "torch.no_grad", "torch.no_grad", "torch.no_grad", "sorted", "torch.stack", "torch.stack", "torch.stack", "torch.reduce", "loss_dict.keys", "loss_names.append", "torch.stack.append", "torch.get_rank", "zip"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.get_world_size"], ["", "def", "reduce_loss_dict", "(", "loss_dict", ")", ":", "\n", "    ", "world_size", "=", "get_world_size", "(", ")", "\n", "if", "world_size", "<", "2", ":", "\n", "        ", "return", "loss_dict", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "loss_names", "=", "[", "]", "\n", "all_losses", "=", "[", "]", "\n", "for", "k", "in", "sorted", "(", "loss_dict", ".", "keys", "(", ")", ")", ":", "\n", "            ", "loss_names", ".", "append", "(", "k", ")", "\n", "all_losses", ".", "append", "(", "loss_dict", "[", "k", "]", ")", "\n", "", "all_losses", "=", "torch", ".", "stack", "(", "all_losses", ",", "dim", "=", "0", ")", "\n", "dist", ".", "reduce", "(", "all_losses", ",", "dst", "=", "0", ")", "\n", "if", "dist", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "            ", "all_losses", "/=", "world_size", "\n", "", "reduced_losses", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "zip", "(", "loss_names", ",", "all_losses", ")", "}", "\n", "", "return", "reduced_losses", "\n", "", "class", "Collect_FN", "(", ")", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_sst_epida_eda.do_aug": [[250, 288], ["range", "len", "eda.Translator", "aug_fn", "range", "aug_fn", "nlpaug.ContextualWordEmbsAug", "len", "Ys.append", "get_embed_fn", "Xs.append", "torch.zeros", "torch.zeros", "torch.zeros", "label_tensor.long.long", "Ys.append", "nlpaug.BackTranslationAug", "torch.CrossEntropyLoss"], "function", ["None"], ["", "", "def", "do_aug", "(", "inputs", ",", "labels", ",", "aug_method", ",", "get_embed_fn", ",", "model", "=", "None", ",", "num_aug", "=", "1", ")", ":", "\n", "    ", "if", "aug_method", "==", "'EDA'", ":", "\n", "        ", "aug_fn", "=", "eda_4", "\n", "", "elif", "aug_method", "==", "'EEDA'", ":", "\n", "        ", "aug_fn", "=", "eda", "\n", "", "elif", "aug_method", "==", "'EPDA'", ":", "\n", "        ", "aug_fn", "=", "epda_bert", "\n", "# print(len(inputs),'vs',len(labels))", "\n", "", "Xs", ",", "Ys", "=", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "        ", "if", "aug_method", "==", "'EPDA'", ":", "\n", "            ", "translator", "=", "Translator", "(", ")", "\n", "if", "EPDA_ENGINE", "==", "'CWE'", ":", "\n", "# insert, substitute", "\n", "                ", "nlp_auger", "=", "naw", ".", "ContextualWordEmbsAug", "(", "action", "=", "'insert'", ",", "device", "=", "'cuda'", ")", "\n", "", "elif", "EPDA_ENGINE", "==", "'BT'", ":", "\n", "                ", "nlp_auger", "=", "naw", ".", "BackTranslationAug", "(", "device", "=", "'cuda'", ")", "\n", "", "else", ":", "\n", "                ", "nlp_auger", "=", "None", "\n", "", "augedtxts", ",", "_", "=", "aug_fn", "(", "txt", "=", "inputs", "[", "i", "]", ",", "label", "=", "labels", "[", "i", "]", ",", "num_aug", "=", "NUM_AUG", ",", "model", "=", "model", ",", "translator", "=", "translator", ",", "\n", "engine", "=", "EPDA_ENGINE", ",", "alpha", "=", "ALPHA", ",", "mix_up", "=", "MIX_UP", ",", "get_embed_fn", "=", "get_embed_fn", ",", "loss_fn", "=", "nn", ".", "CrossEntropyLoss", "(", ")", ",", "\n", "nlp_auger", "=", "nlp_auger", ",", "alpha_epda", "=", "0.5", ")", "\n", "Xs", "+=", "augedtxts", "\n", "# \u518d\u586b\u4e00\u5806\u540c\u6837\u7684", "\n", "for", "j", "in", "range", "(", "len", "(", "augedtxts", ")", ")", ":", "\n", "                ", "Ys", ".", "append", "(", "labels", "[", "i", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "txts", "=", "aug_fn", "(", "inputs", "[", "i", "]", ",", "num_aug", "=", "NUM_AUG", ")", "\n", "for", "txt", "in", "txts", ":", "\n", "                ", "embed", "=", "get_embed_fn", "(", "txt", ")", "\n", "# print(\"Size\",embed.size())", "\n", "Xs", ".", "append", "(", "embed", ")", "\n", "label_tensor", "=", "torch", ".", "zeros", "(", "1", ")", "\n", "label_tensor", "[", "0", "]", "=", "labels", "[", "i", "]", "\n", "label_tensor", "=", "label_tensor", ".", "long", "(", ")", "\n", "Ys", ".", "append", "(", "label_tensor", ")", "\n", "\n", "", "", "", "return", "Xs", ",", "Ys", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_sst_epida_eda.setup_seed": [[289, 295], ["torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "numpy.random.seed", "random.seed"], "function", ["None"], ["", "def", "setup_seed", "(", "seed", ")", ":", "\n", "    ", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", "=", "True", "\n", "", "def", "get_model", "(", ")", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_sst_epida_eda.get_model": [[295, 310], ["transformers.BertForSequenceClassification.from_pretrained().cuda", "transformers.XLNetForSequenceClassification.from_pretrained().cuda", "transformers.BertForSequenceClassification.from_pretrained", "transformers.XLMRobertaForSequenceClassification.from_pretrained().cuda", "transformers.XLNetForSequenceClassification.from_pretrained", "transformers.XLMRobertaForSequenceClassification.from_pretrained"], "function", ["None"], ["", "def", "get_model", "(", ")", ":", "\n", "    ", "if", "MODEL_NAME", "==", "'Bert'", ":", "\n", "        ", "model", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "'bert-base-uncased'", ",", "\n", "num_labels", "=", "num_classes", ",", "\n", "gradient_checkpointing", "=", "True", ",", "\n", ")", ".", "cuda", "(", ")", "\n", "", "elif", "MODEL_NAME", "==", "'XLNet'", ":", "\n", "        ", "model", "=", "XLNetForSequenceClassification", ".", "from_pretrained", "(", "'xlnet-base-cased'", ",", "\n", "num_labels", "=", "num_classes", ",", "\n", ")", ".", "cuda", "(", ")", "\n", "", "elif", "MODEL_NAME", "==", "'XLM-R'", ":", "\n", "        ", "model", "=", "XLMRobertaForSequenceClassification", ".", "from_pretrained", "(", "'xlm-roberta-base'", ",", "\n", "num_labels", "=", "num_classes", ",", "\n", ")", ".", "cuda", "(", ")", "\n", "", "return", "model", "\n", "", "def", "train", "(", "train_data_loader", ",", "test_data_loader", ",", "dev_data_loader", ",", "devtest_data_loader", ",", "model", ")", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_sst_epida_eda.train": [[310, 470], ["torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "train_data_loader.dataset.reset", "torch.CrossEntropyLoss", "print", "range", "model.parameters", "model.train", "enumerate", "int", "print", "open().readlines", "train_sst_epida_eda.do_aug", "print", "train_data_loader.dataset.update", "print", "print", "model.train", "train_sst_epida_eda.move_to_device", "torch.optim.Adam.zero_grad", "model", "nn.CrossEntropyLoss.", "loss_fn.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.optim.Adam.step", "line.split", "int", "train_data_loader.dataset.get_only_chars", "Xs.append", "Ys.append", "len", "len", "model.parameters", "print", "model.eval", "enumerate", "sklearn.metrics.accuracy_score", "enumerate", "sklearn.metrics.accuracy_score", "print", "print", "enumerate", "sklearn.metrics.accuracy_score", "print", "open", "transformers.XLMRobertaTokenizer.from_pretrained", "train_sst_epida_eda.move_to_device", "torch.argmax().detach().cpu.size", "torch.softmax", "torch.softmax", "torch.softmax", "torch.argmax().detach().cpu", "torch.argmax().detach().cpu", "torch.argmax().detach().cpu", "range", "train_sst_epida_eda.move_to_device", "torch.argmax().detach().cpu.size", "torch.softmax", "torch.softmax", "torch.softmax", "torch.argmax().detach().cpu", "torch.argmax().detach().cpu", "torch.argmax().detach().cpu", "range", "train_sst_epida_eda.move_to_device", "torch.argmax().detach().cpu.size", "torch.softmax", "torch.softmax", "torch.softmax", "torch.argmax().detach().cpu", "torch.argmax().detach().cpu", "torch.argmax().detach().cpu", "range", "torch.save", "torch.save", "torch.save", "len", "model", "pred_y.append", "gt_y.append", "model", "pred_y.append", "gt_y.append", "model", "pred_y.append", "gt_y.append", "torch.argmax().detach", "torch.argmax().detach", "torch.argmax().detach", "torch.argmax().detach", "torch.argmax().detach", "torch.argmax().detach", "torch.argmax().detach", "torch.argmax().detach", "torch.argmax().detach", "abs", "torch.optim.Adam.state_dict", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "train_dataset.split"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.EPDADataSet.reset", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.train", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.do_aug", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.EPDADataSet.update", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.train", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.move_to_device", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_only_chars", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.move_to_device", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.move_to_device", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.move_to_device"], ["", "def", "train", "(", "train_data_loader", ",", "test_data_loader", ",", "dev_data_loader", ",", "devtest_data_loader", ",", "model", ")", ":", "\n", "    ", "EPOCHES", "=", "BASIC_EPOCH", "\n", "if", "NEED_AUG", ":", "\n", "        ", "EPOCHES", "*=", "2", "\n", "", "max_acc", "=", "0.0", "\n", "max_dev", ",", "max_devtest", "=", "0.0", ",", "0.0", "\n", "final_score", "=", "0.0", "\n", "trained_iter", "=", "0", "\n", "UPDATED", "=", "False", "\n", "if", "NEED_AUG", "==", "False", ":", "\n", "        ", "UPDATED", "=", "True", "\n", "\n", "# T_max = EPOCHES * (len(train_dataset)//BATCH_SIZE+1)", "\n", "# \u5b9a\u4e49\u4f18\u5316\u5668\u548c\u8c03\u5ea6\u5668", "\n", "# optimizer = AdamW(model.parameters(), lr=LR, eps=1e-8, weight_decay=1e-3)", "\n", "", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "LR", ")", "\n", "# scheduler = transformers.get_linear_schedule_with_warmup(optimizer,EPOCHES,T_max)", "\n", "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(", "\n", "#     optimizer, 'max', factor=0.2, min_lr=min_lr", "\n", "# )", "\n", "# reset the dataset.", "\n", "train_data_loader", ".", "dataset", ".", "reset", "(", ")", "\n", "# print(\"WEIGHTS,\",train_data_loader.dataset.weights)", "\n", "# ,alpha=torch.Tensor(train_data_loader.dataset.weights)", "\n", "# loss_fn = FocalLoss(class_num=num_classes)", "\n", "# loss_fn = nn.CrossEntropyLoss(weight=torch.Tensor(train_data_loader.dataset.weights).cuda())", "\n", "loss_fn", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "print", "(", "\"Pretraining To Generate Samples\"", ")", "\n", "for", "epoch", "in", "range", "(", "EPOCHES", ")", ":", "\n", "        ", "if", "epoch", "==", "int", "(", "EPOCHES", "//", "4", "*", "3", ")", ":", "\n", "            ", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "                ", "param_group", "[", "'lr'", "]", "=", "min_lr", "/", "2", "\n", "", "", "model", ".", "train", "(", ")", "\n", "if", "(", "NEED_AUG", "and", "ONLINE", "==", "False", "and", "epoch", "==", "BASIC_EPOCH", ")", "or", "(", "NEED_AUG", "and", "ONLINE", "and", "epoch", "%", "5", "==", "0", "and", "epoch", ">=", "BASIC_EPOCH", ")", ":", "\n", "            ", "print", "(", "\"Start to update Dataset\"", ")", "\n", "# model = torch.load(\"./release/\"+train_dataset.split('/')[-1]+\".pth\").cuda()", "\n", "input_dir", "=", "train_data_loader", ".", "dataset", ".", "dir", "\n", "lines", "=", "open", "(", "input_dir", ",", "'r'", ")", ".", "readlines", "(", ")", "\n", "Xs", ",", "Ys", "=", "[", "]", ",", "[", "]", "\n", "count", "=", "[", "0", "]", "*", "num_classes", "\n", "for", "line", "in", "lines", ":", "\n", "                ", "y", ",", "x", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "y", "=", "int", "(", "y", ")", "\n", "# if count[y] >= int(434*int(data_split)*2):", "\n", "#     continue", "\n", "count", "[", "y", "]", "+=", "1", "\n", "\n", "x", "=", "x", "[", ":", "-", "1", "]", "\n", "x", "=", "train_data_loader", ".", "dataset", ".", "get_only_chars", "(", "x", ")", "\n", "Xs", ".", "append", "(", "x", ")", "\n", "Ys", ".", "append", "(", "y", ")", "\n", "", "inputs", ",", "label", "=", "do_aug", "(", "Xs", ",", "Ys", ",", "AUG_METHOD", ",", "get_embed_fn", "=", "XLMRobertaTokenizer", ".", "from_pretrained", "(", "'xlm-roberta-base'", ")", ",", "\n", "model", "=", "model", ",", "num_aug", "=", "NUM_AUG", ")", "\n", "# print(\"??\",inputs,\"vs\",label)", "\n", "# os._exit(233)", "\n", "print", "(", "'Before'", ",", "len", "(", "train_data_loader", ")", ")", "\n", "train_data_loader", ".", "dataset", ".", "update", "(", "inputs", ",", "label", ")", "\n", "print", "(", "\"< Update Done.\"", ")", "\n", "print", "(", "'After'", ",", "len", "(", "train_data_loader", ")", ")", "\n", "UPDATED", "=", "True", "\n", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "                ", "param_group", "[", "'lr'", "]", "=", "min_lr", "\n", "", "model", ".", "train", "(", ")", "\n", "# finish the rest", "\n", "# if not ONLINE:", "\n", "#     max_acc = 0.0", "\n", "#     model = get_model()", "\n", "#     optimizer = torch.optim.Adam(model.parameters(), lr=LR)", "\n", "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(", "\n", "#         optimizer, 'max', factor=0.2, min_lr=min_lr", "\n", "#     )", "\n", "\n", "", "for", "i", ",", "batch", "in", "enumerate", "(", "train_data_loader", ")", ":", "\n", "# print(batch)", "\n", "            ", "batch", "=", "move_to_device", "(", "batch", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "# print(batch)", "\n", "# input_ids: [16,128]   label_id:[16]", "\n", "# print(batch['sentences'])", "\n", "# os._exit(233)", "\n", "del", "batch", "[", "'sentences'", "]", "\n", "# print(batch)", "\n", "# os._exit(233)", "\n", "output", "=", "model", "(", "**", "batch", ")", "\n", "# loss = output.loss", "\n", "loss", "=", "loss_fn", "(", "output", ".", "logits", ",", "batch", "[", "'labels'", "]", ")", "\n", "# loss_dict_reduced = reduce_loss_dict({'loss_all': loss})", "\n", "# losses_reduced = sum(loss for loss in loss_dict_reduced.values())", "\n", "# meters.update(loss = losses_reduced, **loss_dict_reduced)", "\n", "loss", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "1.0", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "trained_iter", "+=", "1", "\n", "if", "trained_iter", "%", "(", "len", "(", "test_data_loader", ")", "//", "5", ")", "==", "0", ":", "\n", "                ", "print", "(", "'Start Dev.'", ")", "\n", "# start eval", "\n", "model", ".", "eval", "(", ")", "\n", "pred_y", ",", "gt_y", "=", "[", "]", ",", "[", "]", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "devtest_data_loader", ")", ":", "\n", "                    ", "label", "=", "batch", "[", "'labels'", "]", "\n", "del", "batch", "[", "'sentences'", "]", "\n", "batch", "=", "move_to_device", "(", "batch", ")", "\n", "outputs", "=", "model", "(", "**", "batch", ")", ".", "logits", "\n", "b", ",", "_", "=", "outputs", ".", "size", "(", ")", "\n", "outputs", "=", "torch", ".", "softmax", "(", "outputs", ",", "1", ")", "\n", "# confidence_mat = torch.ones(outputs.size())", "\n", "\n", "outputs", "=", "torch", ".", "argmax", "(", "outputs", ",", "1", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", "\n", "for", "j", "in", "range", "(", "b", ")", ":", "\n", "                        ", "pred_y", ".", "append", "(", "outputs", "[", "j", "]", ")", "\n", "gt_y", ".", "append", "(", "label", "[", "j", "]", ")", "\n", "", "", "score_devtest", "=", "accuracy_score", "(", "gt_y", ",", "pred_y", ")", "\n", "pred_y", ",", "gt_y", "=", "[", "]", ",", "[", "]", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "dev_data_loader", ")", ":", "\n", "                    ", "label", "=", "batch", "[", "'labels'", "]", "\n", "del", "batch", "[", "'sentences'", "]", "\n", "batch", "=", "move_to_device", "(", "batch", ")", "\n", "outputs", "=", "model", "(", "**", "batch", ")", ".", "logits", "\n", "b", ",", "_", "=", "outputs", ".", "size", "(", ")", "\n", "outputs", "=", "torch", ".", "softmax", "(", "outputs", ",", "1", ")", "\n", "# confidence_mat = torch.ones(outputs.size())", "\n", "\n", "outputs", "=", "torch", ".", "argmax", "(", "outputs", ",", "1", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", "\n", "for", "j", "in", "range", "(", "b", ")", ":", "\n", "                        ", "pred_y", ".", "append", "(", "outputs", "[", "j", "]", ")", "\n", "gt_y", ".", "append", "(", "label", "[", "j", "]", ")", "\n", "", "", "score_dev", "=", "accuracy_score", "(", "gt_y", ",", "pred_y", ")", "\n", "score", "=", "score_devtest", "\n", "# scheduler.step(score)", "\n", "print", "(", "optimizer", ".", "state_dict", "(", ")", "[", "'param_groups'", "]", "[", "0", "]", "[", "'lr'", "]", ")", "\n", "# if UPDATED:", "\n", "print", "(", "\"--Current Dev\"", ",", "score_dev", ",", "\"Devtest\"", ",", "score_devtest", ",", "'Max Dev'", ",", "max_dev", ",", "'Max DevTest'", ",", "max_devtest", ",", "'Epoch'", ",", "epoch", ",", "flush", "=", "True", ")", "\n", "# print(\"Report\",classification_report(gt_y,pred_y))", "\n", "pred_y", ",", "gt_y", "=", "[", "]", ",", "[", "]", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "test_data_loader", ")", ":", "\n", "\n", "                    ", "label", "=", "batch", "[", "'labels'", "]", "\n", "del", "batch", "[", "'sentences'", "]", "\n", "batch", "=", "move_to_device", "(", "batch", ")", "\n", "outputs", "=", "model", "(", "**", "batch", ")", ".", "logits", "\n", "b", ",", "_", "=", "outputs", ".", "size", "(", ")", "\n", "outputs", "=", "torch", ".", "softmax", "(", "outputs", ",", "1", ")", "\n", "\n", "outputs", "=", "torch", ".", "argmax", "(", "outputs", ",", "1", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", "\n", "for", "j", "in", "range", "(", "b", ")", ":", "\n", "                        ", "pred_y", ".", "append", "(", "outputs", "[", "j", "]", ")", "\n", "gt_y", ".", "append", "(", "label", "[", "j", "]", ")", "\n", "\n", "", "", "test_score", "=", "accuracy_score", "(", "gt_y", ",", "pred_y", ")", "\n", "print", "(", "\"Test Acc,\"", ",", "test_score", ")", "\n", "if", "(", "score", ">", "max_acc", ")", "or", "(", "abs", "(", "score", "-", "max_acc", ")", "<", "0.001", "and", "score_dev", ">", "max_dev", ")", ":", "\n", "                    ", "max_acc", "=", "score", "\n", "max_dev", "=", "score_dev", "\n", "max_devtest", "=", "score_devtest", "\n", "final_score", "=", "test_score", "\n", "torch", ".", "save", "(", "model", ",", "\"./release/\"", "+", "train_dataset", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", "+", "\".pth\"", ")", "\n", "# model = torch.load(\"./release/\"+train_dataset.split('/')[-1]+\".pth\").cuda()", "\n", "# model.eval()", "\n", "# os._exit(233)", "\n", "", "", "", "", "return", "final_score", "\n", "", "def", "compute_model", "(", "train_dir", ",", "test_dir", ")", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_sst_epida_eda.compute_model": [[470, 491], ["train_sst_epida_eda.EPDADataSet", "train_sst_epida_eda.EPDADataSet", "train_sst_epida_eda.EPDADataSet", "train_sst_epida_eda.EPDADataSet", "train_sst_epida_eda.Collect_FN", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "range", "print", "train_sst_epida_eda.setup_seed", "train_sst_epida_eda.get_model", "train_sst_epida_eda.train", "acc_scores.append", "print", "sum", "len"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.setup_seed", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.get_model", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.train"], ["", "def", "compute_model", "(", "train_dir", ",", "test_dir", ")", ":", "\n", "    ", "acc_scores", "=", "[", "]", "\n", "train_dataset", "=", "EPDADataSet", "(", "train_dir", ",", "num_classes", "=", "num_classes", ")", "\n", "test_dataset", "=", "EPDADataSet", "(", "test_dir", ",", "num_classes", "=", "num_classes", ")", "\n", "dev_dataset", "=", "EPDADataSet", "(", "dev_dir", ",", "num_classes", "=", "num_classes", ")", "\n", "devtest_dataset", "=", "EPDADataSet", "(", "devtest_dir", ",", "num_classes", "=", "num_classes", ")", "\n", "collate_fn", "=", "Collect_FN", "(", "True", ")", "\n", "# train_sampler = torch.utils.data.sampler.WeightedRandomSampler(train_dataset.weights, BATCH_SIZE)", "\n", "# print(max(train_dataset.weights),min(train_dataset.weights))", "\n", "train_data_loader", "=", "DataLoader", "(", "dataset", "=", "train_dataset", ",", "batch_size", "=", "BATCH_SIZE", ",", "collate_fn", "=", "collate_fn", ",", "shuffle", "=", "True", ")", "\n", "test_data_loader", "=", "DataLoader", "(", "dataset", "=", "test_dataset", ",", "batch_size", "=", "64", ",", "shuffle", "=", "False", ",", "collate_fn", "=", "collate_fn", ")", "\n", "dev_data_loader", "=", "DataLoader", "(", "dataset", "=", "dev_dataset", ",", "batch_size", "=", "64", ",", "shuffle", "=", "False", ",", "collate_fn", "=", "collate_fn", ")", "\n", "devtest_data_loader", "=", "DataLoader", "(", "dataset", "=", "devtest_dataset", ",", "batch_size", "=", "64", ",", "shuffle", "=", "False", ",", "collate_fn", "=", "collate_fn", ")", "\n", "# Test them for 5 times", "\n", "for", "i", "in", "range", "(", "1", ")", ":", "\n", "        ", "setup_seed", "(", "i", ")", "\n", "model", "=", "get_model", "(", ")", "\n", "acc", "=", "train", "(", "train_data_loader", ",", "test_data_loader", ",", "dev_data_loader", ",", "devtest_data_loader", ",", "model", ")", "\n", "acc_scores", ".", "append", "(", "acc", ")", "\n", "print", "(", "\"[IMPORTANT] i=\"", ",", "i", ",", "\"Current Acc\"", ",", "acc", ",", "\"Average Acc Score: \"", ",", "sum", "(", "acc_scores", ")", "/", "len", "(", "acc_scores", ")", ",", "flush", "=", "True", ")", "\n", "", "print", "(", "\"> Done.\"", ",", "flush", "=", "True", ")", "\n", "", "if", "__name__", "==", "\"__main__\"", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.nlp_aug.get_only_chars": [[30, 54], ["line.lower.replace", "line.lower.replace", "line.lower.replace", "line.lower.replace", "line.lower.replace", "line.lower.lower", "re.sub", "print"], "function", ["None"], ["def", "get_only_chars", "(", "line", ")", ":", "\n", "\n", "    ", "clean_line", "=", "\"\"", "\n", "\n", "line", "=", "line", ".", "replace", "(", "\"\u2019\"", ",", "\"\"", ")", "\n", "line", "=", "line", ".", "replace", "(", "\"'\"", ",", "\"\"", ")", "\n", "line", "=", "line", ".", "replace", "(", "\"-\"", ",", "\" \"", ")", "#replace hyphens with spaces", "\n", "line", "=", "line", ".", "replace", "(", "\"\\t\"", ",", "\" \"", ")", "\n", "line", "=", "line", ".", "replace", "(", "\"\\n\"", ",", "\" \"", ")", "\n", "line", "=", "line", ".", "lower", "(", ")", "\n", "\n", "for", "char", "in", "line", ":", "\n", "        ", "if", "char", "in", "'qwertyuiopasdfghjklzxcvbnm '", ":", "\n", "            ", "clean_line", "+=", "char", "\n", "", "else", ":", "\n", "            ", "clean_line", "+=", "' '", "\n", "\n", "", "", "clean_line", "=", "re", ".", "sub", "(", "' +'", ",", "' '", ",", "clean_line", ")", "#delete extra spaces", "\n", "try", ":", "\n", "        ", "if", "clean_line", "[", "0", "]", "==", "' '", ":", "\n", "            ", "clean_line", "=", "clean_line", "[", "1", ":", "]", "\n", "", "", "except", ":", "\n", "        ", "print", "(", "\"FIND ERROR\"", ",", "line", ")", "\n", "", "return", "clean_line", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.nlp_aug.synonym_replacement": [[65, 85], ["words.copy", "list", "random.shuffle", "sentence.split", "set", "nlp_aug.get_synonyms", "len", "random.choice", "list"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_synonyms"], ["def", "synonym_replacement", "(", "words", ",", "n", ")", ":", "\n", "\t", "new_words", "=", "words", ".", "copy", "(", ")", "\n", "random_word_list", "=", "list", "(", "set", "(", "[", "word", "for", "word", "in", "words", "if", "word", "not", "in", "stop_words", "]", ")", ")", "\n", "random", ".", "shuffle", "(", "random_word_list", ")", "\n", "num_replaced", "=", "0", "\n", "for", "random_word", "in", "random_word_list", ":", "\n", "\t\t", "synonyms", "=", "get_synonyms", "(", "random_word", ")", "\n", "if", "len", "(", "synonyms", ")", ">=", "1", ":", "\n", "\t\t\t", "synonym", "=", "random", ".", "choice", "(", "list", "(", "synonyms", ")", ")", "\n", "new_words", "=", "[", "synonym", "if", "word", "==", "random_word", "else", "word", "for", "word", "in", "new_words", "]", "\n", "#print(\"replaced\", random_word, \"with\", synonym)", "\n", "num_replaced", "+=", "1", "\n", "", "if", "num_replaced", ">=", "n", ":", "#only replace up to n words", "\n", "\t\t\t", "break", "\n", "\n", "#this is stupid but we need it, trust me", "\n", "", "", "sentence", "=", "' '", ".", "join", "(", "new_words", ")", "\n", "new_words", "=", "sentence", ".", "split", "(", "' '", ")", "\n", "\n", "return", "new_words", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.nlp_aug.get_synonyms": [[86, 96], ["set", "nltk.corpus.wordnet.synsets", "list", "syn.lemmas", "set.remove", "l.name().replace().replace().lower", "set.add", "l.name().replace().replace", "l.name().replace", "l.name"], "function", ["None"], ["", "def", "get_synonyms", "(", "word", ")", ":", "\n", "\t", "synonyms", "=", "set", "(", ")", "\n", "for", "syn", "in", "wordnet", ".", "synsets", "(", "word", ")", ":", "\n", "\t\t", "for", "l", "in", "syn", ".", "lemmas", "(", ")", ":", "\n", "\t\t\t", "synonym", "=", "l", ".", "name", "(", ")", ".", "replace", "(", "\"_\"", ",", "\" \"", ")", ".", "replace", "(", "\"-\"", ",", "\" \"", ")", ".", "lower", "(", ")", "\n", "synonym", "=", "\"\"", ".", "join", "(", "[", "char", "for", "char", "in", "synonym", "if", "char", "in", "' qwertyuiopasdfghjklzxcvbnm'", "]", ")", "\n", "synonyms", ".", "add", "(", "synonym", ")", "\n", "", "", "if", "word", "in", "synonyms", ":", "\n", "\t\t", "synonyms", ".", "remove", "(", "word", ")", "\n", "", "return", "list", "(", "synonyms", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.nlp_aug.random_deletion": [[102, 121], ["len", "random.uniform", "len", "random.randint", "new_words.append", "len"], "function", ["None"], ["", "def", "random_deletion", "(", "words", ",", "p", ")", ":", "\n", "\n", "#obviously, if there's only one word, don't delete it", "\n", "\t", "if", "len", "(", "words", ")", "==", "1", ":", "\n", "\t\t", "return", "words", "\n", "\n", "#randomly delete words with probability p", "\n", "", "new_words", "=", "[", "]", "\n", "for", "word", "in", "words", ":", "\n", "\t\t", "r", "=", "random", ".", "uniform", "(", "0", ",", "1", ")", "\n", "if", "r", ">", "p", ":", "\n", "\t\t\t", "new_words", ".", "append", "(", "word", ")", "\n", "\n", "#if you end up deleting all words, just return a random word", "\n", "", "", "if", "len", "(", "new_words", ")", "==", "0", ":", "\n", "\t\t", "rand_int", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "words", ")", "-", "1", ")", "\n", "return", "[", "words", "[", "rand_int", "]", "]", "\n", "\n", "", "return", "new_words", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.nlp_aug.random_swap": [[127, 132], ["words.copy", "range", "nlp_aug.swap_word"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.swap_word"], ["", "def", "random_swap", "(", "words", ",", "n", ")", ":", "\n", "\t", "new_words", "=", "words", ".", "copy", "(", ")", "\n", "for", "_", "in", "range", "(", "n", ")", ":", "\n", "\t\t", "new_words", "=", "swap_word", "(", "new_words", ")", "\n", "", "return", "new_words", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.nlp_aug.swap_word": [[133, 144], ["random.randint", "random.randint", "len", "len"], "function", ["None"], ["", "def", "swap_word", "(", "new_words", ")", ":", "\n", "\t", "random_idx_1", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "new_words", ")", "-", "1", ")", "\n", "random_idx_2", "=", "random_idx_1", "\n", "counter", "=", "0", "\n", "while", "random_idx_2", "==", "random_idx_1", ":", "\n", "\t\t", "random_idx_2", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "new_words", ")", "-", "1", ")", "\n", "counter", "+=", "1", "\n", "if", "counter", ">", "3", ":", "\n", "\t\t\t", "return", "new_words", "\n", "", "", "new_words", "[", "random_idx_1", "]", ",", "new_words", "[", "random_idx_2", "]", "=", "new_words", "[", "random_idx_2", "]", ",", "new_words", "[", "random_idx_1", "]", "\n", "return", "new_words", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.nlp_aug.random_addition": [[150, 155], ["words.copy", "range", "nlp_aug.add_word"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.add_word"], ["", "def", "random_addition", "(", "words", ",", "n", ")", ":", "\n", "\t", "new_words", "=", "words", ".", "copy", "(", ")", "\n", "for", "_", "in", "range", "(", "n", ")", ":", "\n", "\t\t", "add_word", "(", "new_words", ")", "\n", "", "return", "new_words", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.nlp_aug.add_word": [[156, 168], ["random.randint", "new_words.insert", "len", "nlp_aug.get_synonyms", "len", "random.randint", "len"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_synonyms"], ["", "def", "add_word", "(", "new_words", ")", ":", "\n", "\t", "synonyms", "=", "[", "]", "\n", "counter", "=", "0", "\n", "while", "len", "(", "synonyms", ")", "<", "1", ":", "\n", "\t\t", "random_word", "=", "new_words", "[", "random", ".", "randint", "(", "0", ",", "len", "(", "new_words", ")", "-", "1", ")", "]", "\n", "synonyms", "=", "get_synonyms", "(", "random_word", ")", "\n", "counter", "+=", "1", "\n", "if", "counter", ">=", "10", ":", "\n", "\t\t\t", "return", "\n", "", "", "random_synonym", "=", "synonyms", "[", "0", "]", "\n", "random_idx", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "new_words", ")", "-", "1", ")", "\n", "new_words", ".", "insert", "(", "random_idx", ",", "random_synonym", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.nlp_aug.eda_4": [[173, 220], ["nlp_aug.get_only_chars", "get_only_chars.split", "len", "max", "max", "max", "range", "range", "range", "range", "random.shuffle", "augmented_sentences.append", "int", "int", "int", "nlp_aug.synonym_replacement", "augmented_sentences.append", "nlp_aug.random_addition", "augmented_sentences.append", "nlp_aug.random_swap", "augmented_sentences.append", "nlp_aug.random_deletion", "augmented_sentences.append", "nlp_aug.get_only_chars"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_only_chars", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.synonym_replacement", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.nlp_aug.random_addition", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.random_swap", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.random_deletion", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_only_chars"], ["", "def", "eda_4", "(", "sentence", ",", "alpha_sr", "=", "0.3", ",", "alpha_ri", "=", "0.2", ",", "alpha_rs", "=", "0.1", ",", "p_rd", "=", "0.15", ",", "num_aug", "=", "9", ")", ":", "\n", "\n", "\t", "sentence", "=", "get_only_chars", "(", "sentence", ")", "\n", "words", "=", "sentence", ".", "split", "(", "' '", ")", "\n", "words", "=", "[", "word", "for", "word", "in", "words", "if", "word", "is", "not", "''", "]", "\n", "num_words", "=", "len", "(", "words", ")", "\n", "\n", "augmented_sentences", "=", "[", "]", "\n", "num_new_per_technique", "=", "10", "\n", "n_sr", "=", "max", "(", "1", ",", "int", "(", "alpha_sr", "*", "num_words", ")", ")", "\n", "n_ri", "=", "max", "(", "1", ",", "int", "(", "alpha_ri", "*", "num_words", ")", ")", "\n", "n_rs", "=", "max", "(", "1", ",", "int", "(", "alpha_rs", "*", "num_words", ")", ")", "\n", "\n", "#sr", "\n", "for", "_", "in", "range", "(", "num_new_per_technique", ")", ":", "\n", "\t\t", "a_words", "=", "synonym_replacement", "(", "words", ",", "n_sr", ")", "\n", "augmented_sentences", ".", "append", "(", "' '", ".", "join", "(", "a_words", ")", ")", "\n", "\n", "#ri", "\n", "", "for", "_", "in", "range", "(", "num_new_per_technique", ")", ":", "\n", "\t\t", "a_words", "=", "random_addition", "(", "words", ",", "n_ri", ")", "\n", "augmented_sentences", ".", "append", "(", "' '", ".", "join", "(", "a_words", ")", ")", "\n", "\n", "#rs", "\n", "", "for", "_", "in", "range", "(", "num_new_per_technique", ")", ":", "\n", "\t\t", "a_words", "=", "random_swap", "(", "words", ",", "n_rs", ")", "\n", "augmented_sentences", ".", "append", "(", "' '", ".", "join", "(", "a_words", ")", ")", "\n", "\n", "#rd", "\n", "", "for", "_", "in", "range", "(", "num_new_per_technique", ")", ":", "\n", "\t\t", "a_words", "=", "random_deletion", "(", "words", ",", "p_rd", ")", "\n", "augmented_sentences", ".", "append", "(", "' '", ".", "join", "(", "a_words", ")", ")", "\n", "\n", "", "augmented_sentences", "=", "[", "get_only_chars", "(", "sentence", ")", "for", "sentence", "in", "augmented_sentences", "]", "\n", "shuffle", "(", "augmented_sentences", ")", "\n", "\n", "#trim so that we have the desired number of augmented sentences", "\n", "# if num_aug >= 1:", "\n", "# \taugmented_sentences = augmented_sentences[:num_aug]", "\n", "# else:", "\n", "# \tkeep_prob = num_aug / len(augmented_sentences)", "\n", "# \taugmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]", "\n", "\n", "# append the original sentence", "\n", "augmented_sentences", ".", "append", "(", "sentence", ")", "\n", "\n", "return", "augmented_sentences", "[", ":", "num_aug", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.nlp_aug.SR": [[221, 240], ["nlp_aug.get_only_chars", "get_only_chars.split", "len", "max", "range", "random.shuffle", "augmented_sentences.append", "int", "nlp_aug.synonym_replacement", "augmented_sentences.append", "nlp_aug.get_only_chars"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_only_chars", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.synonym_replacement", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_only_chars"], ["", "def", "SR", "(", "sentence", ",", "alpha_sr", ",", "n_aug", "=", "9", ")", ":", "\n", "\n", "\t", "sentence", "=", "get_only_chars", "(", "sentence", ")", "\n", "words", "=", "sentence", ".", "split", "(", "' '", ")", "\n", "num_words", "=", "len", "(", "words", ")", "\n", "\n", "augmented_sentences", "=", "[", "]", "\n", "n_sr", "=", "max", "(", "1", ",", "int", "(", "alpha_sr", "*", "num_words", ")", ")", "\n", "\n", "for", "_", "in", "range", "(", "n_aug", ")", ":", "\n", "\t\t", "a_words", "=", "synonym_replacement", "(", "words", ",", "n_sr", ")", "\n", "augmented_sentences", ".", "append", "(", "' '", ".", "join", "(", "a_words", ")", ")", "\n", "\n", "", "augmented_sentences", "=", "[", "get_only_chars", "(", "sentence", ")", "for", "sentence", "in", "augmented_sentences", "]", "\n", "shuffle", "(", "augmented_sentences", ")", "\n", "\n", "augmented_sentences", ".", "append", "(", "sentence", ")", "\n", "\n", "return", "augmented_sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.nlp_aug.RI": [[241, 260], ["nlp_aug.get_only_chars", "get_only_chars.split", "len", "max", "range", "random.shuffle", "augmented_sentences.append", "int", "nlp_aug.random_addition", "augmented_sentences.append", "nlp_aug.get_only_chars"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_only_chars", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.nlp_aug.random_addition", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_only_chars"], ["", "def", "RI", "(", "sentence", ",", "alpha_ri", ",", "n_aug", "=", "9", ")", ":", "\n", "\n", "\t", "sentence", "=", "get_only_chars", "(", "sentence", ")", "\n", "words", "=", "sentence", ".", "split", "(", "' '", ")", "\n", "num_words", "=", "len", "(", "words", ")", "\n", "\n", "augmented_sentences", "=", "[", "]", "\n", "n_ri", "=", "max", "(", "1", ",", "int", "(", "alpha_ri", "*", "num_words", ")", ")", "\n", "\n", "for", "_", "in", "range", "(", "n_aug", ")", ":", "\n", "\t\t", "a_words", "=", "random_addition", "(", "words", ",", "n_ri", ")", "\n", "augmented_sentences", ".", "append", "(", "' '", ".", "join", "(", "a_words", ")", ")", "\n", "\n", "", "augmented_sentences", "=", "[", "get_only_chars", "(", "sentence", ")", "for", "sentence", "in", "augmented_sentences", "]", "\n", "shuffle", "(", "augmented_sentences", ")", "\n", "\n", "augmented_sentences", ".", "append", "(", "sentence", ")", "\n", "\n", "return", "augmented_sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.nlp_aug.RS": [[261, 280], ["nlp_aug.get_only_chars", "get_only_chars.split", "len", "max", "range", "random.shuffle", "augmented_sentences.append", "int", "nlp_aug.random_swap", "augmented_sentences.append", "nlp_aug.get_only_chars"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_only_chars", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.random_swap", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_only_chars"], ["", "def", "RS", "(", "sentence", ",", "alpha_rs", ",", "n_aug", "=", "9", ")", ":", "\n", "\n", "\t", "sentence", "=", "get_only_chars", "(", "sentence", ")", "\n", "words", "=", "sentence", ".", "split", "(", "' '", ")", "\n", "num_words", "=", "len", "(", "words", ")", "\n", "\n", "augmented_sentences", "=", "[", "]", "\n", "n_rs", "=", "max", "(", "1", ",", "int", "(", "alpha_rs", "*", "num_words", ")", ")", "\n", "\n", "for", "_", "in", "range", "(", "n_aug", ")", ":", "\n", "\t\t", "a_words", "=", "random_swap", "(", "words", ",", "n_rs", ")", "\n", "augmented_sentences", ".", "append", "(", "' '", ".", "join", "(", "a_words", ")", ")", "\n", "\n", "", "augmented_sentences", "=", "[", "get_only_chars", "(", "sentence", ")", "for", "sentence", "in", "augmented_sentences", "]", "\n", "shuffle", "(", "augmented_sentences", ")", "\n", "\n", "augmented_sentences", ".", "append", "(", "sentence", ")", "\n", "\n", "return", "augmented_sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.nlp_aug.RD": [[281, 300], ["nlp_aug.get_only_chars", "get_only_chars.split", "len", "range", "random.shuffle", "augmented_sentences.append", "nlp_aug.random_deletion", "augmented_sentences.append", "nlp_aug.get_only_chars"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_only_chars", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.random_deletion", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_only_chars"], ["", "def", "RD", "(", "sentence", ",", "alpha_rd", ",", "n_aug", "=", "9", ")", ":", "\n", "\n", "\t", "sentence", "=", "get_only_chars", "(", "sentence", ")", "\n", "words", "=", "sentence", ".", "split", "(", "' '", ")", "\n", "words", "=", "[", "word", "for", "word", "in", "words", "if", "word", "is", "not", "''", "]", "\n", "num_words", "=", "len", "(", "words", ")", "\n", "\n", "augmented_sentences", "=", "[", "]", "\n", "\n", "for", "_", "in", "range", "(", "n_aug", ")", ":", "\n", "\t\t", "a_words", "=", "random_deletion", "(", "words", ",", "alpha_rd", ")", "\n", "augmented_sentences", ".", "append", "(", "' '", ".", "join", "(", "a_words", ")", ")", "\n", "\n", "", "augmented_sentences", "=", "[", "get_only_chars", "(", "sentence", ")", "for", "sentence", "in", "augmented_sentences", "]", "\n", "shuffle", "(", "augmented_sentences", ")", "\n", "\n", "augmented_sentences", ".", "append", "(", "sentence", ")", "\n", "\n", "return", "augmented_sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.model.CNN.__init__": [[10, 35], ["torch.Module.__init__", "print", "range", "print", "torch.Linear", "torch.Linear", "torch.Linear", "len", "len", "len", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "setattr", "sum", "print", "torch.Linear", "torch.Linear", "torch.Linear", "sum"], "methods", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.EPDADataSet.__init__"], ["    ", "def", "__init__", "(", "self", ",", "max_len", "=", "30", ",", "word_dim", "=", "300", ",", "class_size", "=", "2", ",", "size", "=", "'normal'", ")", ":", "\n", "        ", "super", "(", "CNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "MAX_SENT_LEN", "=", "max_len", "\n", "self", ".", "WORD_DIM", "=", "word_dim", "\n", "self", ".", "CLASS_SIZE", "=", "class_size", "\n", "print", "(", "\"size=\"", ",", "size", ")", "\n", "if", "size", "==", "'normal'", ":", "\n", "            ", "print", "(", "\"Init Normal\"", ")", "\n", "self", ".", "FILTERS", "=", "[", "2", ",", "3", ",", "4", "]", "\n", "self", ".", "FILTER_NUM", "=", "[", "100", ",", "100", ",", "100", "]", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "sum", "(", "self", ".", "FILTER_NUM", ")", ",", "self", ".", "CLASS_SIZE", ")", "\n", "", "elif", "size", "==", "'tiny'", ":", "\n", "            ", "print", "(", "\"Tiny Size\"", ")", "\n", "self", ".", "FILTERS", "=", "[", "3", "]", "\n", "self", ".", "FILTER_NUM", "=", "[", "20", "]", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "sum", "(", "self", ".", "FILTER_NUM", ")", ",", "self", ".", "CLASS_SIZE", ")", "\n", "", "self", ".", "DROPOUT_PROB", "=", "0.5", "\n", "self", ".", "IN_CHANNEL", "=", "1", "\n", "\n", "assert", "(", "len", "(", "self", ".", "FILTERS", ")", "==", "len", "(", "self", ".", "FILTER_NUM", ")", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "FILTERS", ")", ")", ":", "\n", "            ", "conv", "=", "nn", ".", "Conv1d", "(", "self", ".", "IN_CHANNEL", ",", "self", ".", "FILTER_NUM", "[", "i", "]", ",", "self", ".", "WORD_DIM", "*", "self", ".", "FILTERS", "[", "i", "]", ",", "stride", "=", "self", ".", "WORD_DIM", ")", "\n", "setattr", "(", "self", ",", "f'conv_{i}'", ",", "conv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.model.CNN.get_conv": [[37, 39], ["getattr"], "methods", ["None"], ["", "", "def", "get_conv", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ",", "f'conv_{i}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.model.CNN.forward": [[40, 54], ["inp.view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.dropout", "torch.dropout", "torch.dropout", "model.CNN.fc", "torch.max_pool1d().view", "torch.max_pool1d().view", "torch.max_pool1d().view", "range", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "len", "torch.relu", "torch.relu", "torch.relu", "model.CNN.get_conv"], "methods", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.model.CNN.get_conv"], ["", "def", "forward", "(", "self", ",", "inp", ")", ":", "\n", "# [B 1 C]", "\n", "        ", "x", "=", "inp", ".", "view", "(", "-", "1", ",", "1", ",", "self", ".", "WORD_DIM", "*", "self", ".", "MAX_SENT_LEN", ")", "\n", "# print(x.size())", "\n", "conv_results", "=", "[", "\n", "F", ".", "max_pool1d", "(", "F", ".", "relu", "(", "self", ".", "get_conv", "(", "i", ")", "(", "x", ")", ")", ",", "self", ".", "MAX_SENT_LEN", "-", "self", ".", "FILTERS", "[", "i", "]", "+", "1", ")", "\n", ".", "view", "(", "-", "1", ",", "self", ".", "FILTER_NUM", "[", "i", "]", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "FILTERS", ")", ")", "]", "\n", "\n", "x", "=", "torch", ".", "cat", "(", "conv_results", ",", "1", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "DROPOUT_PROB", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "fc", "(", "x", ")", "\n", "# x = torch.softmax(x,1)", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.model.BLSTMATT.__init__": [[57, 65], ["torch.Module.__init__", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.EPDADataSet.__init__"], ["    ", "def", "__init__", "(", "self", ",", "max_len", "=", "30", ",", "word_dim", "=", "300", ",", "class_size", "=", "2", ")", ":", "\n", "        ", "super", "(", "BLSTMATT", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hidden_dim", "=", "50", "\n", "self", ".", "emb_dim", "=", "word_dims", "\n", "self", ".", "dropout", "=", "0.3", "\n", "self", ".", "encoder", "=", "nn", ".", "LSTM", "(", "self", ".", "emb_dim", ",", "self", ".", "hidden_dim", ",", "num_layers", "=", "2", ",", "bidirectional", "=", "True", ",", "dropout", "=", "self", ".", "dropout", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "self", ".", "hidden_dim", ",", "class_size", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "self", ".", "dropout", ")", "\n", "#self.hidden = nn.Parameters(self.batch_size, self.hidden_dim)", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.model.BLSTMATT.attnetwork": [[67, 78], ["final_hidden.squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.softmax", "torch.softmax", "torch.softmax", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "final_hidden.squeeze.unsqueeze", "encoder_out.transpose", "torch.softmax.unsqueeze"], "methods", ["None"], ["", "def", "attnetwork", "(", "self", ",", "encoder_out", ",", "final_hidden", ")", ":", "\n", "        ", "hidden", "=", "final_hidden", ".", "squeeze", "(", "0", ")", "\n", "#M = torch.tanh(encoder_out)", "\n", "attn_weights", "=", "torch", ".", "bmm", "(", "encoder_out", ",", "hidden", ".", "unsqueeze", "(", "2", ")", ")", ".", "squeeze", "(", "2", ")", "\n", "soft_attn_weights", "=", "F", ".", "softmax", "(", "attn_weights", ",", "1", ")", "\n", "new_hidden", "=", "torch", ".", "bmm", "(", "encoder_out", ".", "transpose", "(", "1", ",", "2", ")", ",", "soft_attn_weights", ".", "unsqueeze", "(", "2", ")", ")", ".", "squeeze", "(", "2", ")", "\n", "#print (wt.shape, new_hidden.shape)", "\n", "#new_hidden = torch.tanh(new_hidden)", "\n", "#print ('UP:', new_hidden, new_hidden.shape)", "\n", "\n", "return", "new_hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.model.BLSTMATT.forward": [[79, 91], ["model.BLSTMATT.dropout", "model.BLSTMATT.encoder", "fbout.permute.permute.permute", "model.BLSTMATT.attnetwork", "model.BLSTMATT.fc"], "methods", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.model.BLSTMATT.attnetwork"], ["", "def", "forward", "(", "self", ",", "sequence", ")", ":", "\n", "# emb_input = self.embedding(sequence)    ", "\n", "        ", "inputx", "=", "self", ".", "dropout", "(", "sequence", ")", "\n", "output", ",", "(", "hn", ",", "cn", ")", "=", "self", ".", "encoder", "(", "inputx", ")", "\n", "fbout", "=", "output", "[", ":", ",", ":", ",", ":", "self", ".", "hidden_dim", "]", "+", "output", "[", ":", ",", ":", ",", "self", ".", "hidden_dim", ":", "]", "#sum bidir outputs F+B", "\n", "fbout", "=", "fbout", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", "fbhn", "=", "(", "hn", "[", "-", "2", ",", ":", ",", ":", "]", "+", "hn", "[", "-", "1", ",", ":", ",", ":", "]", ")", ".", "unsqueeze", "(", "0", ")", "\n", "#print (fbhn.shape, fbout.shape)", "\n", "attn_out", "=", "self", ".", "attnetwork", "(", "fbout", ",", "fbhn", ")", "\n", "#attn1_out = self.attnetwork1(output, hn)", "\n", "logits", "=", "self", ".", "fc", "(", "attn_out", ")", "\n", "return", "logits", "\n", "", "", ""]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.Collect_FN.__init__": [[117, 125], ["super().__init__", "transformers.BertTokenizer.from_pretrained", "transformers.XLNetTokenizer.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.EPDADataSet.__init__"], ["    ", "def", "__init__", "(", "self", ",", "with_label_hard", ",", "with_GT_labels", "=", "False", ")", ":", "\n", "        ", "super", "(", "Collect_FN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "MODEL_NAME", "==", "'Bert'", ":", "\n", "            ", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ",", "local_files_only", "=", "True", ")", "\n", "", "elif", "MODEL_NAME", "==", "'XLNet'", ":", "\n", "            ", "self", ".", "tokenizer", "=", "XLNetTokenizer", ".", "from_pretrained", "(", "'xlnet-base-cased'", ",", "local_files_only", "=", "True", ")", "\n", "", "self", ".", "with_label", "=", "with_label_hard", "\n", "self", ".", "with_GT_labels", "=", "with_GT_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.Collect_FN.__call__": [[126, 146], ["train_irony_epida_eda.Collect_FN.tokenizer", "map", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "zip", "map", "zip", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "batchs", ")", ":", "\n", "# print(batchs)", "\n", "        ", "if", "(", "self", ".", "with_label", "and", "self", ".", "with_GT_labels", "==", "False", ")", ":", "\n", "            ", "sentences", ",", "labels", "=", "map", "(", "list", ",", "zip", "(", "*", "batchs", ")", ")", "\n", "", "elif", "(", "self", ".", "with_label", "==", "True", "and", "self", ".", "with_GT_labels", "==", "True", ")", ":", "\n", "            ", "sentences", ",", "labels", ",", "GT_labels", "=", "map", "(", "list", ",", "zip", "(", "*", "batchs", ")", ")", "\n", "", "else", ":", "\n", "            ", "sentences", "=", "batchs", "\n", "", "encoding", "=", "self", ".", "tokenizer", "(", "sentences", ",", "return_tensors", "=", "'pt'", ",", "padding", "=", "True", ",", "truncation", "=", "True", ")", "\n", "# input_ids = encoding['input_ids']", "\n", "# attention_mask = encoding['attention_mask']", "\n", "# ans = {'input_ids': input_ids, 'attention_mask': attention_mask}", "\n", "if", "(", "self", ".", "with_label", ")", ":", "\n", "            ", "labels", "=", "torch", ".", "tensor", "(", "labels", ")", ".", "long", "(", ")", "\n", "encoding", "[", "'labels'", "]", "=", "labels", "\n", "", "if", "(", "self", ".", "with_GT_labels", ")", ":", "\n", "            ", "GT_labels", "=", "torch", ".", "tensor", "(", "GT_labels", ")", ".", "long", "(", ")", "\n", "encoding", "[", "'GT_labels'", "]", "=", "GT_labels", "\n", "", "encoding", "[", "'sentences'", "]", "=", "sentences", "\n", "return", "encoding", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.EPDADataSet.__init__": [[149, 193], ["print", "open().readlines", "print", "line.split", "int", "train_irony_epida_eda.EPDADataSet.get_only_chars", "Xs.append", "Ys.append", "len", "open", "len"], "methods", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_only_chars"], ["    ", "def", "__init__", "(", "self", ",", "input_dir", ",", "max_len", "=", "30", ",", "num_classes", "=", "2", ")", ":", "\n", "        ", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "dir", "=", "input_dir", "\n", "print", "(", "\"Start to read: \"", ",", "input_dir", ",", "flush", "=", "True", ")", "\n", "#\u5148\u9884\u8bfb\u4e00\u4e0b", "\n", "lines", "=", "open", "(", "input_dir", ",", "'r'", ")", ".", "readlines", "(", ")", "\n", "Xs", ",", "Ys", "=", "[", "]", ",", "[", "]", "\n", "count", "=", "[", "0", "]", "*", "num_classes", "\n", "for", "line", "in", "lines", ":", "\n", "            ", "y", ",", "x", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "y", "=", "int", "(", "y", ")", "\n", "# \u6700\u540e\u540e\u4e00\u4e2a\\n\u7684", "\n", "x", "=", "x", "[", ":", "-", "1", "]", "\n", "# if 'train' in input_dir:", "\n", "#     if y==0 or y==2:", "\n", "#         continue", "\n", "# if count[y] >= int(434*int(data_split)/10*2) and 'train' in input_dir:", "\n", "#     continue", "\n", "count", "[", "y", "]", "+=", "1", "\n", "if", "len", "(", "x", ")", "<=", "2", ":", "\n", "                ", "continue", "\n", "", "x", "=", "self", ".", "get_only_chars", "(", "x", ")", "\n", "Xs", ".", "append", "(", "x", ")", "\n", "Ys", ".", "append", "(", "y", ")", "\n", "# weight_per_class = [0.] * num_classes", "\n", "# N = float(sum(count))                                                   ", "\n", "# for i in range(num_classes):", "\n", "#     weight_per_class[i] = N/float(count[i])                     ", "\n", "# weight = [0] * len(Ys)", "\n", "# for idx, val in enumerate(Ys):", "\n", "#     weight[idx] = weight_per_class[val]", "\n", "# self.weights = weight_per_class", "\n", "# print(weight_per_class,count)", "\n", "# os._exit(233)", "\n", "# if not 'test' in input_dir:", "\n", "#     Xs,Ys = self.upsample_balance(Xs,Ys)", "\n", "#     print(\"Balance dataset Over.\")", "\n", "", "self", ".", "Xs", "=", "Xs", "\n", "self", ".", "Ys", "=", "Ys", "\n", "\n", "self", ".", "O_Xs", "=", "self", ".", "Xs", "\n", "self", ".", "O_Ys", "=", "self", ".", "Ys", "\n", "print", "(", "\"Load Over, Find: \"", ",", "len", "(", "self", ".", "Xs", ")", ",", "\" datas.\"", ",", "flush", "=", "True", ")", "\n", "", "def", "get_only_chars", "(", "self", ",", "line", ")", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.EPDADataSet.get_only_chars": [[193, 214], ["line.replace.replace.lower", "line.replace.replace.replace", "line.replace.replace.replace", "line.replace.replace.replace", "line.replace.replace.replace", "line.replace.replace.replace", "re.sub"], "methods", ["None"], ["", "def", "get_only_chars", "(", "self", ",", "line", ")", ":", "\n", "\n", "        ", "clean_line", "=", "\"\"", "\n", "\n", "line", "=", "line", ".", "lower", "(", ")", "\n", "line", "=", "line", ".", "replace", "(", "\" 's\"", ",", "\" is\"", ")", "\n", "line", "=", "line", ".", "replace", "(", "\"-\"", ",", "\" \"", ")", "#replace hyphens with spaces", "\n", "line", "=", "line", ".", "replace", "(", "\"\\t\"", ",", "\" \"", ")", "\n", "line", "=", "line", ".", "replace", "(", "\"\\n\"", ",", "\" \"", ")", "\n", "line", "=", "line", ".", "replace", "(", "\"'\"", ",", "\"\"", ")", "\n", "\n", "for", "char", "in", "line", ":", "\n", "            ", "if", "char", "in", "'qwertyuiopasdfghjklzxcvbnm '", ":", "\n", "                ", "clean_line", "+=", "char", "\n", "", "else", ":", "\n", "                ", "clean_line", "+=", "' '", "\n", "\n", "", "", "clean_line", "=", "re", ".", "sub", "(", "' +'", ",", "' '", ",", "clean_line", ")", "#delete extra spaces", "\n", "if", "clean_line", "[", "0", "]", "==", "' '", ":", "\n", "            ", "clean_line", "=", "clean_line", "[", "1", ":", "]", "\n", "", "return", "clean_line", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.EPDADataSet.__getitem__": [[215, 218], ["len"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "assert", "idx", "<", "len", "(", "self", ".", "Xs", ")", "\n", "return", "self", ".", "Xs", "[", "idx", "]", ",", "self", ".", "Ys", "[", "idx", "]", "\n", "", "def", "__len__", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.EPDADataSet.__len__": [[218, 220], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "Xs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.EPDADataSet.update": [[221, 230], ["print", "print", "len", "len"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "Xs", ",", "Ys", ")", ":", "\n", "        ", "print", "(", "\"Start Update Dataset, Find \"", ",", "len", "(", "self", ".", "Xs", ")", ",", "'datas.'", ",", "flush", "=", "True", ")", "\n", "# if not 'test' in self.dir:", "\n", "#     Xs,Ys = self.upsample_balance(Xs,Ys)", "\n", "#     print(\"Balance dataset Over.\")", "\n", "self", ".", "Xs", "=", "Xs", "\n", "self", ".", "Ys", "=", "Ys", "\n", "print", "(", "\"Update Dataset Finish, Find \"", ",", "len", "(", "self", ".", "Xs", ")", ",", "'datas.'", ",", "flush", "=", "True", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.EPDADataSet.reset": [[231, 235], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "O_Xs", "is", "not", "None", ":", "\n", "            ", "self", ".", "Xs", "=", "self", ".", "O_Xs", "\n", "self", ".", "Ys", "=", "self", ".", "O_Ys", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.EPDADataSet.upsample_balance": [[236, 261], ["numpy.array", "numpy.max", "zip", "enumerate", "range", "sentence_each_class[].append", "zip", "range", "range", "append_cur_class.append", "ans_sentences.append", "ans_labels.append", "len"], "methods", ["None"], ["", "", "def", "upsample_balance", "(", "self", ",", "sentences", ",", "labels", ")", ":", "\n", "        ", "sample_number_per_class", "=", "[", "0", "]", "*", "self", ".", "num_classes", "\n", "for", "y", "in", "labels", ":", "\n", "            ", "sample_number_per_class", "[", "y", "]", "+=", "1", "\n", "", "sample_number_per_class", "=", "np", ".", "array", "(", "sample_number_per_class", ")", "\n", "max_number", "=", "np", ".", "max", "(", "sample_number_per_class", ")", "\n", "fill_number_each_class", "=", "max_number", "-", "sample_number_per_class", "\n", "# print(\"??\",sample_number_per_class,fill_number_each_class)", "\n", "sentence_each_class", "=", "[", "[", "]", "for", "i", "in", "range", "(", "self", ".", "num_classes", ")", "]", "\n", "for", "s", ",", "l", "in", "zip", "(", "sentences", ",", "labels", ")", ":", "\n", "            ", "sentence_each_class", "[", "l", "]", ".", "append", "(", "s", ")", "\n", "", "for", "class_index", ",", "(", "sentences_cur_class", ",", "fill_num_cur_class", ")", "in", "enumerate", "(", "\n", "zip", "(", "sentence_each_class", ",", "fill_number_each_class", ")", ")", ":", "\n", "            ", "append_cur_class", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "fill_num_cur_class", ")", ":", "\n", "                ", "append_cur_class", ".", "append", "(", "sentences_cur_class", "[", "i", "%", "len", "(", "sentences_cur_class", ")", "]", ")", "\n", "", "sentence_each_class", "[", "class_index", "]", "=", "sentences_cur_class", "+", "append_cur_class", "\n", "", "ans_sentences", "=", "[", "]", "\n", "ans_labels", "=", "[", "]", "\n", "for", "class_index", "in", "range", "(", "self", ".", "num_classes", ")", ":", "\n", "            ", "for", "s", "in", "sentence_each_class", "[", "class_index", "]", ":", "\n", "                ", "ans_sentences", ".", "append", "(", "s", ")", "\n", "ans_labels", ".", "append", "(", "class_index", ")", "\n", "", "", "return", "ans_sentences", ",", "ans_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.get_world_size": [[81, 87], ["torch.get_world_size", "torch.is_available", "torch.is_initialized"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.get_world_size"], ["", "def", "get_world_size", "(", ")", ":", "\n", "    ", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "        ", "return", "1", "\n", "", "if", "not", "dist", ".", "is_initialized", "(", ")", ":", "\n", "        ", "return", "1", "\n", "", "return", "dist", ".", "get_world_size", "(", ")", "\n", "", "def", "move_to_device", "(", "batch", ",", "rank", "=", "None", ")", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.move_to_device": [[87, 100], ["batch[].to"], "function", ["None"], ["", "def", "move_to_device", "(", "batch", ",", "rank", "=", "None", ")", ":", "\n", "    ", "ans", "=", "{", "}", "\n", "if", "(", "rank", "is", "None", ")", ":", "\n", "        ", "device", "=", "'cuda'", "\n", "", "else", ":", "\n", "        ", "device", "=", "'cuda:{}'", ".", "format", "(", "rank", ")", "\n", "", "for", "key", "in", "batch", ":", "\n", "        ", "try", ":", "\n", "            ", "ans", "[", "key", "]", "=", "batch", "[", "key", "]", ".", "to", "(", "device", "=", "device", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "# print(str(e))", "\n", "            ", "ans", "[", "key", "]", "=", "batch", "[", "key", "]", "\n", "", "", "return", "ans", "\n", "", "def", "reduce_loss_dict", "(", "loss_dict", ")", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.reduce_loss_dict": [[100, 116], ["train_irony_epida_eda.get_world_size", "torch.no_grad", "torch.no_grad", "torch.no_grad", "sorted", "torch.stack", "torch.stack", "torch.stack", "torch.reduce", "loss_dict.keys", "loss_names.append", "torch.stack.append", "torch.get_rank", "zip"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.get_world_size"], ["", "def", "reduce_loss_dict", "(", "loss_dict", ")", ":", "\n", "    ", "world_size", "=", "get_world_size", "(", ")", "\n", "if", "world_size", "<", "2", ":", "\n", "        ", "return", "loss_dict", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "loss_names", "=", "[", "]", "\n", "all_losses", "=", "[", "]", "\n", "for", "k", "in", "sorted", "(", "loss_dict", ".", "keys", "(", ")", ")", ":", "\n", "            ", "loss_names", ".", "append", "(", "k", ")", "\n", "all_losses", ".", "append", "(", "loss_dict", "[", "k", "]", ")", "\n", "", "all_losses", "=", "torch", ".", "stack", "(", "all_losses", ",", "dim", "=", "0", ")", "\n", "dist", ".", "reduce", "(", "all_losses", ",", "dst", "=", "0", ")", "\n", "if", "dist", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "            ", "all_losses", "/=", "world_size", "\n", "", "reduced_losses", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "zip", "(", "loss_names", ",", "all_losses", ")", "}", "\n", "", "return", "reduced_losses", "\n", "", "class", "Collect_FN", "(", ")", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.do_aug": [[262, 302], ["range", "len", "aug_fn", "range", "aug_fn", "nlpaug.ContextualWordEmbsAug", "len", "Ys.append", "get_embed_fn", "Xs.append", "torch.zeros", "torch.zeros", "torch.zeros", "label_tensor.long.long", "Ys.append", "nlpaug.BackTranslationAug", "torch.CrossEntropyLoss"], "function", ["None"], ["", "", "def", "do_aug", "(", "inputs", ",", "labels", ",", "aug_method", ",", "get_embed_fn", ",", "model", "=", "None", ",", "num_aug", "=", "1", ")", ":", "\n", "    ", "if", "aug_method", "==", "'EDA'", ":", "\n", "        ", "aug_fn", "=", "eda_4", "\n", "", "elif", "aug_method", "==", "'EEDA'", ":", "\n", "        ", "aug_fn", "=", "eda", "\n", "", "elif", "aug_method", "==", "'EPDA'", ":", "\n", "        ", "aug_fn", "=", "epda_bert", "\n", "# print(len(inputs),'vs',len(labels))", "\n", "", "Xs", ",", "Ys", "=", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "        ", "if", "aug_method", "==", "'EPDA'", ":", "\n", "            ", "translator", "=", "None", "\n", "if", "EPDA_ENGINE", "==", "'CWE'", ":", "\n", "                ", "nlp_auger", "=", "naw", ".", "ContextualWordEmbsAug", "(", "action", "=", "'insert'", ",", "device", "=", "'cuda'", ")", "\n", "", "elif", "EPDA_ENGINE", "==", "'BT'", ":", "\n", "                ", "nlp_auger", "=", "naw", ".", "BackTranslationAug", "(", "device", "=", "'cuda'", ")", "\n", "", "else", ":", "\n", "                ", "nlp_auger", "=", "None", "\n", "# if labels[i]==0 or labels[i]==2:", "\n", "#     # print(\"Dont aug\")", "\n", "#     augedtxts = [inputs[i]]", "\n", "# else:", "\n", "", "augedtxts", ",", "_", "=", "aug_fn", "(", "txt", "=", "inputs", "[", "i", "]", ",", "label", "=", "labels", "[", "i", "]", ",", "num_aug", "=", "NUM_AUG", ",", "model", "=", "model", ",", "translator", "=", "translator", ",", "\n", "engine", "=", "EPDA_ENGINE", ",", "alpha", "=", "ALPHA", ",", "mix_up", "=", "MIX_UP", ",", "get_embed_fn", "=", "get_embed_fn", ",", "loss_fn", "=", "nn", ".", "CrossEntropyLoss", "(", ")", ",", "nlp_auger", "=", "nlp_auger", ")", "\n", "Xs", "+=", "augedtxts", "\n", "# \u518d\u586b\u4e00\u5806\u540c\u6837\u7684", "\n", "for", "j", "in", "range", "(", "len", "(", "augedtxts", ")", ")", ":", "\n", "                ", "Ys", ".", "append", "(", "labels", "[", "i", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "txts", "=", "aug_fn", "(", "inputs", "[", "i", "]", ",", "num_aug", "=", "NUM_AUG", ")", "\n", "for", "txt", "in", "txts", ":", "\n", "                ", "embed", "=", "get_embed_fn", "(", "txt", ")", "\n", "# print(\"Size\",embed.size())", "\n", "Xs", ".", "append", "(", "embed", ")", "\n", "label_tensor", "=", "torch", ".", "zeros", "(", "1", ")", "\n", "label_tensor", "[", "0", "]", "=", "labels", "[", "i", "]", "\n", "label_tensor", "=", "label_tensor", ".", "long", "(", ")", "\n", "Ys", ".", "append", "(", "label_tensor", ")", "\n", "# \u4e00\u4e2a\u5c31\u597d\u4e86", "\n", "", "", "", "return", "Xs", ",", "Ys", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.setup_seed": [[303, 309], ["torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "numpy.random.seed", "random.seed"], "function", ["None"], ["", "def", "setup_seed", "(", "seed", ")", ":", "\n", "    ", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", "=", "True", "\n", "", "def", "get_model", "(", ")", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.get_model": [[309, 320], ["transformers.BertForSequenceClassification.from_pretrained().cuda", "transformers.XLNetForSequenceClassification.from_pretrained().cuda", "transformers.BertForSequenceClassification.from_pretrained", "transformers.XLNetForSequenceClassification.from_pretrained"], "function", ["None"], ["", "def", "get_model", "(", ")", ":", "\n", "    ", "if", "MODEL_NAME", "==", "'Bert'", ":", "\n", "        ", "model", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "'bert-base-uncased'", ",", "\n", "num_labels", "=", "num_classes", ",", "\n", "gradient_checkpointing", "=", "True", ",", "\n", ")", ".", "cuda", "(", ")", "\n", "", "elif", "MODEL_NAME", "==", "'XLNet'", ":", "\n", "        ", "model", "=", "XLNetForSequenceClassification", ".", "from_pretrained", "(", "'xlnet-base-cased'", ",", "\n", "num_labels", "=", "num_classes", ",", "\n", ")", ".", "cuda", "(", ")", "\n", "", "return", "model", "\n", "", "def", "train", "(", "train_data_loader", ",", "test_data_loader", ",", "model", ")", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.train": [[320, 420], ["transformers.AdamW", "train_data_loader.dataset.reset", "torch.CrossEntropyLoss", "range", "print", "model.parameters", "model.train", "enumerate", "print", "open().readlines", "train_irony_epida_eda.do_aug", "print", "train_data_loader.dataset.update", "print", "print", "train_irony_epida_eda.move_to_device", "transformers.AdamW.zero_grad", "model", "nn.CrossEntropyLoss.", "loss_fn.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "transformers.AdamW.step", "len", "line.split", "int", "train_data_loader.dataset.get_only_chars", "Xs.append", "Ys.append", "len", "len", "model.parameters", "model.eval", "enumerate", "sklearn.metrics.f1_score", "print", "print", "open", "train_irony_epida_eda.move_to_device", "torch.argmax().detach().cpu.size", "torch.softmax", "torch.softmax", "torch.softmax", "torch.argmax().detach().cpu", "torch.argmax().detach().cpu", "torch.argmax().detach().cpu", "range", "sklearn.metrics.classification_report", "max", "model", "pred_y.append", "gt_y.append", "torch.argmax().detach", "torch.argmax().detach", "torch.argmax().detach", "torch.argmax", "torch.argmax", "torch.argmax"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.EPDADataSet.reset", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.train", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.do_aug", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.EPDADataSet.update", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.move_to_device", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_only_chars", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.move_to_device"], ["", "def", "train", "(", "train_data_loader", ",", "test_data_loader", ",", "model", ")", ":", "\n", "    ", "EPOCHES", "=", "BASIC_EPOCH", "\n", "if", "NEED_AUG", ":", "\n", "# EPOCHES += int(BASIC_EPOCH*NUM_AUG//(NUM_AUG+1))", "\n", "        ", "EPOCHES", "*=", "2", "\n", "print", "(", "\"Update EPOCHES to\"", ",", "EPOCHES", ")", "\n", "\n", "", "max_f1_score", "=", "0.0", "\n", "trained_iter", "=", "0", "\n", "UPDATED", "=", "False", "\n", "T_max", "=", "EPOCHES", "*", "(", "len", "(", "train_dataset", ")", "//", "BATCH_SIZE", "+", "1", ")", "\n", "\n", "optimizer", "=", "AdamW", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "LR", ",", "eps", "=", "1e-8", ",", "weight_decay", "=", "1e-3", ")", "\n", "# scheduler = transformers.get_linear_schedule_with_warmup(optimizer,EPOCHES,T_max)", "\n", "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [int(EPOCHES)*0.6,int(EPOCHES)*0.9], gamma=0.5, last_epoch=-1)", "\n", "# reset the dataset.", "\n", "train_data_loader", ".", "dataset", ".", "reset", "(", ")", "\n", "\n", "loss_fn", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "for", "epoch", "in", "range", "(", "EPOCHES", ")", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "if", "(", "NEED_AUG", "and", "ONLINE", "==", "False", "and", "epoch", "==", "BASIC_EPOCH", ")", "or", "(", "NEED_AUG", "and", "ONLINE", "and", "epoch", "%", "5", "==", "0", "and", "epoch", ">=", "BASIC_EPOCH", ")", ":", "\n", "            ", "print", "(", "\"Start to update Dataset\"", ")", "\n", "input_dir", "=", "train_data_loader", ".", "dataset", ".", "dir", "\n", "lines", "=", "open", "(", "input_dir", ",", "'r'", ")", ".", "readlines", "(", ")", "\n", "Xs", ",", "Ys", "=", "[", "]", ",", "[", "]", "\n", "count", "=", "[", "0", "]", "*", "num_classes", "\n", "for", "line", "in", "lines", ":", "\n", "                ", "y", ",", "x", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "y", "=", "int", "(", "y", ")", "\n", "# if count[y] >= int(434*int(data_split)*2):", "\n", "#     continue", "\n", "count", "[", "y", "]", "+=", "1", "\n", "x", "=", "x", "[", ":", "-", "1", "]", "\n", "x", "=", "train_data_loader", ".", "dataset", ".", "get_only_chars", "(", "x", ")", "\n", "Xs", ".", "append", "(", "x", ")", "\n", "Ys", ".", "append", "(", "y", ")", "\n", "", "inputs", ",", "label", "=", "do_aug", "(", "Xs", ",", "Ys", ",", "AUG_METHOD", ",", "get_embed_fn", "=", "TOKENIZER", ",", "model", "=", "model", ",", "num_aug", "=", "NUM_AUG", ")", "\n", "\n", "print", "(", "'Before'", ",", "len", "(", "train_data_loader", ")", ")", "\n", "train_data_loader", ".", "dataset", ".", "update", "(", "inputs", ",", "label", ")", "\n", "print", "(", "\"< Update Done.\"", ")", "\n", "print", "(", "'After'", ",", "len", "(", "train_data_loader", ")", ")", "\n", "UPDATED", "=", "True", "\n", "# \u5b8c\u6210\u5269\u4e0b\u7684\u8bad\u7ec3", "\n", "if", "not", "ONLINE", ":", "\n", "                ", "max_f1_score", "=", "0.0", "\n", "#     model = get_model()", "\n", "#     optimizer = AdamW(model.parameters(), lr=LR, eps=1e-8, weight_decay=1e-3)", "\n", "", "", "for", "i", ",", "batch", "in", "enumerate", "(", "train_data_loader", ")", ":", "\n", "# print(batch)", "\n", "            ", "batch", "=", "move_to_device", "(", "batch", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "# print(batch)", "\n", "# input_ids: [16,128]   label_id:[16]", "\n", "# print(batch['sentences'])", "\n", "# os._exit(233)", "\n", "del", "batch", "[", "'sentences'", "]", "\n", "# print(batch)", "\n", "# os._exit(233)", "\n", "output", "=", "model", "(", "**", "batch", ")", "\n", "# loss = output.loss", "\n", "loss", "=", "loss_fn", "(", "output", ".", "logits", ",", "batch", "[", "'labels'", "]", ")", "\n", "# loss_dict_reduced = reduce_loss_dict({'loss_all': loss})", "\n", "# losses_reduced = sum(loss for loss in loss_dict_reduced.values())", "\n", "# meters.update(loss = losses_reduced, **loss_dict_reduced)", "\n", "loss", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "1.0", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "trained_iter", "+=", "1", "\n", "if", "(", "trained_iter", "%", "1", "==", "0", "and", "'offense'", "not", "in", "train_dataset", ")", "or", "trained_iter", "%", "5", "==", "0", ":", "\n", "\n", "                ", "model", ".", "eval", "(", ")", "\n", "pred_y", ",", "gt_y", "=", "[", "]", ",", "[", "]", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "test_data_loader", ")", ":", "\n", "\n", "                    ", "label", "=", "batch", "[", "'labels'", "]", "\n", "del", "batch", "[", "'sentences'", "]", "\n", "batch", "=", "move_to_device", "(", "batch", ")", "\n", "outputs", "=", "model", "(", "**", "batch", ")", ".", "logits", "\n", "b", ",", "_", "=", "outputs", ".", "size", "(", ")", "\n", "outputs", "=", "torch", ".", "softmax", "(", "outputs", ",", "1", ")", "\n", "# confidence_mat = torch.ones(outputs.size())", "\n", "\n", "outputs", "=", "torch", ".", "argmax", "(", "outputs", ",", "1", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", "\n", "for", "j", "in", "range", "(", "b", ")", ":", "\n", "                        ", "pred_y", ".", "append", "(", "outputs", "[", "j", "]", ")", "\n", "gt_y", ".", "append", "(", "label", "[", "j", "]", ")", "\n", "# print(outputs[j],'vs',label[j])", "\n", "# print(gt_y[:10],'vs',pred_y[:10])", "\n", "# if num_classes==2:", "\n", "#     score = f1_score(gt_y, pred_y, average='binary')", "\n", "# else:", "\n", "", "", "score", "=", "f1_score", "(", "gt_y", ",", "pred_y", ",", "average", "=", "'macro'", ")", "\n", "print", "(", "\"--F1 Score\"", ",", "score", ",", "flush", "=", "True", ")", "\n", "print", "(", "\"Report\"", ",", "classification_report", "(", "gt_y", ",", "pred_y", ")", ")", "\n", "if", "UPDATED", "or", "(", "not", "NEED_AUG", ")", ":", "\n", "                    ", "max_f1_score", "=", "max", "(", "max_f1_score", ",", "score", ")", "\n", "# os._exit(233)", "\n", "", "", "", "", "return", "max_f1_score", "\n", "", "def", "compute_model", "(", "train_dir", ",", "test_dir", ")", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.compute_model": [[420, 437], ["train_irony_epida_eda.EPDADataSet", "train_irony_epida_eda.EPDADataSet", "train_irony_epida_eda.Collect_FN", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "range", "print", "train_irony_epida_eda.setup_seed", "train_irony_epida_eda.get_model", "train_irony_epida_eda.train", "f1_scores.append", "print", "sum", "len"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.setup_seed", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.get_model", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.train"], ["", "def", "compute_model", "(", "train_dir", ",", "test_dir", ")", ":", "\n", "    ", "f1_scores", "=", "[", "]", "\n", "train_dataset", "=", "EPDADataSet", "(", "train_dir", ",", "num_classes", "=", "num_classes", ")", "\n", "test_dataset", "=", "EPDADataSet", "(", "test_dir", ",", "num_classes", "=", "num_classes", ")", "\n", "collate_fn", "=", "Collect_FN", "(", "True", ")", "\n", "# train_sampler = torch.utils.data.sampler.WeightedRandomSampler(train_dataset.weights, BATCH_SIZE)", "\n", "# print(max(train_dataset.weights),min(train_dataset.weights))", "\n", "train_data_loader", "=", "DataLoader", "(", "dataset", "=", "train_dataset", ",", "batch_size", "=", "BATCH_SIZE", ",", "collate_fn", "=", "collate_fn", ",", "shuffle", "=", "True", ")", "\n", "test_data_loader", "=", "DataLoader", "(", "dataset", "=", "test_dataset", ",", "batch_size", "=", "64", ",", "shuffle", "=", "False", ",", "collate_fn", "=", "collate_fn", ")", "\n", "# Test them for 5 times", "\n", "for", "i", "in", "range", "(", "5", ")", ":", "\n", "        ", "setup_seed", "(", "i", "+", "1", ")", "\n", "model", "=", "get_model", "(", ")", "\n", "f1", "=", "train", "(", "train_data_loader", ",", "test_data_loader", ",", "model", ")", "\n", "f1_scores", ".", "append", "(", "f1", ")", "\n", "print", "(", "\"[IMPORTANT] i=\"", ",", "i", ",", "\"Current F1 Score\"", ",", "f1", ",", "\"Average F1 Score: \"", ",", "sum", "(", "f1_scores", ")", "/", "len", "(", "f1_scores", ")", ",", "flush", "=", "True", ")", "\n", "", "print", "(", "\"> Done.\"", ",", "flush", "=", "True", ")", "\n", "", "if", "__name__", "==", "\"__main__\"", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_only_chars": [[51, 70], ["line.lower.replace", "line.lower.replace", "line.lower.replace", "line.lower.replace", "line.lower.lower", "re.sub"], "function", ["None"], ["def", "get_only_chars", "(", "line", ")", ":", "\n", "    ", "clean_line", "=", "\"\"", "\n", "\n", "line", "=", "line", ".", "replace", "(", "\"\u2019\"", ",", "\"\"", ")", "\n", "line", "=", "line", ".", "replace", "(", "\"'\"", ",", "\"\"", ")", "\n", "line", "=", "line", ".", "replace", "(", "\"\\t\"", ",", "\" \"", ")", "\n", "line", "=", "line", ".", "replace", "(", "\"\\n\"", ",", "\" \"", ")", "\n", "line", "=", "line", ".", "lower", "(", ")", "\n", "\n", "for", "char", "in", "line", ":", "\n", "        ", "if", "char", "in", "'qwertyuiopasdfghjklzxcvbnm$- '", ":", "\n", "            ", "clean_line", "+=", "char", "\n", "", "else", ":", "\n", "            ", "clean_line", "+=", "' '", "\n", "\n", "", "", "clean_line", "=", "re", ".", "sub", "(", "' +'", ",", "' '", ",", "clean_line", ")", "# delete extra spaces", "\n", "if", "clean_line", "[", "0", "]", "==", "' '", ":", "\n", "        ", "clean_line", "=", "clean_line", "[", "1", ":", "]", "\n", "", "return", "clean_line", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.synonym_replacement": [[84, 115], ["words.copy", "list", "random.shuffle", "sentence.split", "set", "eda.get_synonyms", "eda.get_synonyms_adjusted", "len", "random.choice", "list", "temp.append", "temp.append"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_synonyms", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_synonyms_adjusted"], ["def", "synonym_replacement", "(", "words", ",", "n", ",", "adjusted", ")", ":", "\n", "    ", "new_words", "=", "words", ".", "copy", "(", ")", "\n", "random_word_list", "=", "list", "(", "set", "(", "[", "word", "for", "word", "in", "words", "if", "word", "not", "in", "stop_words", "and", "word", "!=", "'$t$'", "]", ")", ")", "\n", "random", ".", "shuffle", "(", "random_word_list", ")", "\n", "num_replaced", "=", "0", "\n", "for", "random_word", "in", "random_word_list", ":", "\n", "        ", "if", "not", "adjusted", ":", "\n", "            ", "synonyms", "=", "get_synonyms", "(", "random_word", ")", "\n", "", "else", ":", "\n", "            ", "synonyms", "=", "get_synonyms_adjusted", "(", "words", ",", "random_word", ")", "\n", "", "if", "len", "(", "synonyms", ")", ">=", "1", ":", "\n", "            ", "synonym", "=", "random", ".", "choice", "(", "list", "(", "synonyms", ")", ")", "\n", "temp", "=", "[", "]", "\n", "replaced", "=", "False", "\n", "for", "word", "in", "new_words", ":", "\n", "                ", "if", "word", "==", "random_word", "and", "not", "replaced", ":", "\n", "                    ", "temp", ".", "append", "(", "synonym", ")", "\n", "replaced", "=", "True", "\n", "", "else", ":", "\n", "                    ", "temp", ".", "append", "(", "word", ")", "\n", "#print(\"replaced\", random_word, \"with\", synonym)", "\n", "", "", "num_replaced", "+=", "1", "\n", "new_words", "=", "temp", "\n", "", "if", "num_replaced", ">=", "n", ":", "# only replace up to n words", "\n", "            ", "break", "\n", "\n", "# this is stupid but we need it, trust me", "\n", "", "", "sentence", "=", "' '", ".", "join", "(", "new_words", ")", "\n", "new_words", "=", "sentence", ".", "split", "(", "' '", ")", "\n", "\n", "return", "new_words", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_synonyms": [[117, 127], ["set", "nltk.corpus.wordnet.synsets", "list", "syn.lemmas", "set.remove", "l.name().replace().replace().lower", "set.add", "l.name().replace().replace", "l.name().replace", "l.name"], "function", ["None"], ["", "def", "get_synonyms", "(", "word", ")", ":", "\n", "    ", "synonyms", "=", "set", "(", ")", "\n", "for", "syn", "in", "wordnet", ".", "synsets", "(", "word", ")", ":", "\n", "        ", "for", "l", "in", "syn", ".", "lemmas", "(", ")", ":", "\n", "            ", "synonym", "=", "l", ".", "name", "(", ")", ".", "replace", "(", "\"_\"", ",", "\" \"", ")", ".", "replace", "(", "\"-\"", ",", "\" \"", ")", ".", "lower", "(", ")", "\n", "synonym", "=", "\"\"", ".", "join", "(", "[", "char", "for", "char", "in", "synonym", "if", "char", "in", "' qwertyuiopasdfghjklzxcvbnm'", "]", ")", "\n", "synonyms", ".", "add", "(", "synonym", ")", "\n", "", "", "if", "word", "in", "synonyms", ":", "\n", "        ", "synonyms", ".", "remove", "(", "word", ")", "\n", "", "return", "list", "(", "synonyms", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_synonyms_adjusted": [[129, 144], ["nltk.pos_tag", "nltk.pos_tag", "pywsd.simple_lesk.lemma_names", "syn.lower", "synonyms.append", "synonyms.remove", "eda.get_wordnet_pos", "pywsd.simple_lesk", "eda.get_wordnet_pos"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_wordnet_pos", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_wordnet_pos"], ["", "def", "get_synonyms_adjusted", "(", "words", ",", "random_word", ")", ":", "\n", "    ", "pos_tags", "=", "nltk", ".", "pos_tag", "(", "words", ")", "\n", "for", "word", ",", "func", "in", "pos_tags", ":", "\n", "        ", "if", "word", "==", "random_word", "and", "not", "get_wordnet_pos", "(", "func", ")", ":", "\n", "            ", "return", "[", "]", "\n", "", "elif", "word", "==", "random_word", ":", "\n", "            ", "meaning", "=", "simple_lesk", "(", "' '", ".", "join", "(", "words", ")", ",", "random_word", ",", "pos", "=", "get_wordnet_pos", "(", "func", ")", ")", "\n", "", "", "synonyms", "=", "[", "]", "\n", "if", "meaning", ":", "\n", "        ", "for", "syn", "in", "meaning", ".", "lemma_names", "(", ")", ":", "\n", "            ", "synonym", "=", "syn", ".", "lower", "(", ")", "\n", "synonyms", ".", "append", "(", "synonym", ")", "\n", "", "if", "random_word", "in", "synonyms", ":", "\n", "            ", "synonyms", ".", "remove", "(", "random_word", ")", "\n", "", "", "return", "synonyms", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_wordnet_pos": [[146, 157], ["treebank_tag.startswith", "treebank_tag.startswith", "treebank_tag.startswith", "treebank_tag.startswith"], "function", ["None"], ["", "def", "get_wordnet_pos", "(", "treebank_tag", ")", ":", "\n", "    ", "if", "treebank_tag", ".", "startswith", "(", "'J'", ")", ":", "\n", "        ", "return", "wordnet", ".", "ADJ", "\n", "", "elif", "treebank_tag", ".", "startswith", "(", "'V'", ")", ":", "\n", "        ", "return", "wordnet", ".", "VERB", "\n", "", "elif", "treebank_tag", ".", "startswith", "(", "'N'", ")", ":", "\n", "        ", "return", "wordnet", ".", "NOUN", "\n", "", "elif", "treebank_tag", ".", "startswith", "(", "'R'", ")", ":", "\n", "        ", "return", "wordnet", ".", "ADV", "\n", "", "else", ":", "\n", "        ", "return", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.random_deletion": [[164, 182], ["len", "random.uniform", "len", "random.randint", "new_words.append", "len"], "function", ["None"], ["", "", "def", "random_deletion", "(", "words", ",", "p", ")", ":", "\n", "# obviously, if there's only one word, don't delete it", "\n", "    ", "if", "len", "(", "words", ")", "==", "1", ":", "\n", "        ", "return", "words", "\n", "\n", "# randomly delete words with probability p", "\n", "", "new_words", "=", "[", "]", "\n", "for", "word", "in", "words", ":", "\n", "        ", "r", "=", "random", ".", "uniform", "(", "0", ",", "1", ")", "\n", "if", "r", ">", "p", "or", "'$t$'", "in", "word", ":", "\n", "            ", "new_words", ".", "append", "(", "word", ")", "\n", "\n", "# if you end up deleting all words, just return a random word", "\n", "", "", "if", "len", "(", "new_words", ")", "==", "0", ":", "\n", "        ", "rand_int", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "words", ")", "-", "1", ")", "\n", "return", "[", "words", "[", "rand_int", "]", "]", "\n", "\n", "", "return", "new_words", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.random_swap": [[189, 194], ["words.copy", "range", "eda.swap_word"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.swap_word"], ["", "def", "random_swap", "(", "words", ",", "n", ")", ":", "\n", "    ", "new_words", "=", "words", ".", "copy", "(", ")", "\n", "for", "_", "in", "range", "(", "n", ")", ":", "\n", "        ", "new_words", "=", "swap_word", "(", "new_words", ")", "\n", "", "return", "new_words", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.swap_word": [[196, 207], ["random.randint", "random.randint", "len", "len"], "function", ["None"], ["", "def", "swap_word", "(", "new_words", ")", ":", "\n", "    ", "random_idx_1", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "new_words", ")", "-", "1", ")", "\n", "random_idx_2", "=", "random_idx_1", "\n", "counter", "=", "0", "\n", "while", "random_idx_2", "==", "random_idx_1", ":", "\n", "        ", "random_idx_2", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "new_words", ")", "-", "1", ")", "\n", "counter", "+=", "1", "\n", "if", "counter", ">", "3", ":", "\n", "            ", "return", "new_words", "\n", "", "", "new_words", "[", "random_idx_1", "]", ",", "new_words", "[", "random_idx_2", "]", "=", "new_words", "[", "random_idx_2", "]", ",", "new_words", "[", "random_idx_1", "]", "\n", "return", "new_words", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.random_insertion": [[214, 219], ["words.copy", "range", "eda.add_word"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.add_word"], ["", "def", "random_insertion", "(", "words", ",", "n", ",", "adjusted", "=", "False", ")", ":", "\n", "    ", "new_words", "=", "words", ".", "copy", "(", ")", "\n", "for", "_", "in", "range", "(", "n", ")", ":", "\n", "        ", "add_word", "(", "new_words", ")", "\n", "", "return", "new_words", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.add_word": [[221, 233], ["random.randint", "new_words.insert", "len", "eda.get_synonyms", "eda.get_synonyms_adjusted", "len", "random.randint", "len"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_synonyms", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.get_synonyms_adjusted"], ["", "def", "add_word", "(", "new_words", ",", "adjusted", "=", "False", ")", ":", "\n", "    ", "synonyms", "=", "[", "]", "\n", "counter", "=", "0", "\n", "while", "len", "(", "synonyms", ")", "<", "1", ":", "\n", "        ", "random_word", "=", "new_words", "[", "random", ".", "randint", "(", "0", ",", "len", "(", "new_words", ")", "-", "1", ")", "]", "\n", "synonyms", "=", "get_synonyms", "(", "random_word", ")", "if", "not", "adjusted", "else", "get_synonyms_adjusted", "(", "new_words", ",", "random_word", ")", "\n", "counter", "+=", "1", "\n", "if", "counter", ">=", "10", ":", "\n", "            ", "return", "\n", "", "", "random_synonym", "=", "synonyms", "[", "0", "]", "\n", "random_idx", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "new_words", ")", "-", "1", ")", "\n", "new_words", ".", "insert", "(", "random_idx", ",", "random_synonym", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.eda": [[239, 275], ["sentence.split", "len", "max", "max", "max", "range", "range", "range", "random.shuffle", "augmented_sentences.append", "int", "int", "int", "eda.synonym_replacement", "augmented_sentences.append", "eda.random_insertion", "augmented_sentences.append", "range", "eda.random_deletion", "augmented_sentences.append", "eda.random_swap", "augmented_sentences.append"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.synonym_replacement", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.random_insertion", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.random_deletion", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.random_swap"], ["", "def", "eda", "(", "sentence", ",", "alpha_sr", "=", "1", ",", "alpha_ri", "=", "1", ",", "alpha_rs", "=", "1", ",", "p_rd", "=", "0", ",", "percentage", "=", ".2", ",", "adjusted", "=", "True", ",", "num_aug", "=", "1", ")", ":", "\n", "# sentence = get_only_chars(sentence)", "\n", "    ", "words", "=", "sentence", ".", "split", "(", "' '", ")", "\n", "words", "=", "[", "word", "for", "word", "in", "words", "if", "word", "is", "not", "''", "]", "\n", "words", "=", "[", "word", "for", "word", "in", "words", "if", "word", "is", "not", "''", "]", "\n", "num_words", "=", "len", "(", "words", ")", "\n", "\n", "augmented_sentences", "=", "[", "]", "\n", "n_sr", "=", "max", "(", "1", ",", "int", "(", "percentage", "*", "num_words", ")", ")", "\n", "n_ri", "=", "max", "(", "1", ",", "int", "(", "percentage", "*", "num_words", ")", ")", "\n", "n_rs", "=", "max", "(", "1", ",", "int", "(", "percentage", "*", "num_words", ")", ")", "\n", "\n", "# sr", "\n", "for", "_", "in", "range", "(", "alpha_sr", ")", ":", "\n", "        ", "a_words", "=", "synonym_replacement", "(", "words", ",", "n_sr", ",", "adjusted", ")", "\n", "augmented_sentences", ".", "append", "(", "' '", ".", "join", "(", "a_words", ")", ")", "\n", "\n", "# ri", "\n", "", "for", "_", "in", "range", "(", "alpha_ri", ")", ":", "\n", "        ", "a_words", "=", "random_insertion", "(", "words", ",", "n_ri", ",", "adjusted", ")", "\n", "augmented_sentences", ".", "append", "(", "' '", ".", "join", "(", "a_words", ")", ")", "\n", "# rs", "\n", "", "if", "not", "adjusted", ":", "\n", "        ", "for", "_", "in", "range", "(", "alpha_rs", ")", ":", "\n", "            ", "a_words", "=", "random_swap", "(", "words", ",", "n_rs", ")", "\n", "augmented_sentences", ".", "append", "(", "' '", ".", "join", "(", "a_words", ")", ")", "\n", "# rd", "\n", "", "", "for", "_", "in", "range", "(", "p_rd", ")", ":", "\n", "        ", "a_words", "=", "random_deletion", "(", "words", ",", "percentage", ")", "\n", "augmented_sentences", ".", "append", "(", "' '", ".", "join", "(", "a_words", ")", ")", "\n", "\n", "", "augmented_sentences", "=", "[", "sentence", "for", "sentence", "in", "augmented_sentences", "]", "\n", "shuffle", "(", "augmented_sentences", ")", "\n", "augmented_sentences", "=", "augmented_sentences", "[", ":", "num_aug", "]", "\n", "augmented_sentences", ".", "append", "(", "sentence", ")", "\n", "return", "augmented_sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.JointH": [[279, 290], ["range", "range", "a.size", "b.size", "torch.log2", "torch.log2"], "function", ["None"], ["", "def", "JointH", "(", "a", ",", "b", ")", ":", "\n", "    ", "s", "=", "0.0", "\n", "for", "i", "in", "range", "(", "a", ".", "size", "(", ")", "[", "0", "]", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "b", ".", "size", "(", ")", "[", "0", "]", ")", ":", "\n", "            ", "_a", "=", "a", "[", "i", "]", "\n", "_b", "=", "b", "[", "j", "]", "\n", "p", "=", "_a", "*", "_b", "\n", "if", "p", "<=", "1e-10", ":", "\n", "                ", "continue", "\n", "", "s", "+=", "p", "*", "torch", ".", "log2", "(", "p", ")", "\n", "", "", "return", "s", "*", "-", "1.0", "\n", "", "def", "mutal_info", "(", "a", ",", "b", ")", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.mutal_info": [[290, 292], ["eda.JointH", "eda.H", "eda.H"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.JointH", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.H", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.H"], ["", "def", "mutal_info", "(", "a", ",", "b", ")", ":", "\n", "    ", "return", "H", "(", "a", ")", "+", "H", "(", "b", ")", "-", "JointH", "(", "a", ",", "b", ")", "\n", "", "EPS", "=", "1e-10", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.MI": [[293, 303], ["P.sum().view().expand", "P.sum().view().expand", "z.size", "P.sum", "P.sum().view", "P.sum().view", "z.unsqueeze", "zt.unsqueeze", "P.t", "P.sum", "P.sum", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log"], "function", ["None"], ["def", "MI", "(", "z", ",", "zt", ")", ":", "\n", "    ", "C", "=", "z", ".", "size", "(", ")", "[", "1", "]", "\n", "# actually they are not independent", "\n", "P", "=", "(", "z", ".", "unsqueeze", "(", "2", ")", "*", "zt", ".", "unsqueeze", "(", "1", ")", ")", ".", "sum", "(", "dim", "=", "0", ")", "\n", "P", "=", "(", "(", "P", "+", "P", ".", "t", "(", ")", ")", "/", "2", ")", "/", "P", ".", "sum", "(", ")", "\n", "P", "[", "(", "P", "<", "EPS", ")", ".", "data", "]", "=", "EPS", "\n", "Pi", "=", "P", ".", "sum", "(", "dim", "=", "1", ")", ".", "view", "(", "C", ",", "1", ")", ".", "expand", "(", "C", ",", "C", ")", "\n", "Pj", "=", "P", ".", "sum", "(", "dim", "=", "0", ")", ".", "view", "(", "1", ",", "C", ")", ".", "expand", "(", "C", ",", "C", ")", "\n", "# revise by 1.0", "\n", "return", "1.0", "-", "(", "P", "*", "(", "-", "torch", ".", "log", "(", "Pi", ")", "-", "torch", ".", "log", "(", "Pj", ")", "+", "torch", ".", "log", "(", "P", ")", ")", ")", ".", "sum", "(", ")", "\n", "", "def", "CEM", "(", "z", ",", "zt", ")", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.CEM": [[303, 305], ["eda.MI", "eda.H"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.MI", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.H"], ["", "def", "CEM", "(", "z", ",", "zt", ")", ":", "\n", "    ", "return", "MI", "(", "z", ",", "zt", ")", "-", "H", "(", "z", ")", "\n", "", "def", "H", "(", "P", ")", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.H": [[305, 308], ["torch.log", "torch.log"], "function", ["None"], ["", "def", "H", "(", "P", ")", ":", "\n", "    ", "P", "[", "(", "P", "<", "EPS", ")", ".", "data", "]", "=", "EPS", "\n", "return", "-", "(", "P", "*", "torch", ".", "log", "(", "P", ")", ")", ".", "sum", "(", ")", "\n", "", "def", "REM", "(", "z", ",", "zt", ")", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.REM": [[308, 312], ["torch.sum", "torch.sum", "torch.log", "torch.log"], "function", ["None"], ["", "def", "REM", "(", "z", ",", "zt", ")", ":", "\n", "    ", "EPS", "=", "1e-10", "\n", "zt", "[", "(", "zt", "<", "EPS", ")", ".", "data", "]", "=", "EPS", "\n", "return", "-", "torch", ".", "sum", "(", "z", "*", "torch", ".", "log", "(", "zt", ")", ")", "\n", "", "def", "gradmutualgain", "(", "output", ",", "label", ",", "one_hot", ",", "softmaxed", ",", "softmaxed_y", ",", "loss_fn", "=", "None", ")", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.gradmutualgain": [[312, 317], ["eda.REM", "softmaxed.unsqueeze", "one_hot.unsqueeze", "eda.CEM", "softmaxed.unsqueeze", "softmaxed_y.unsqueeze"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.REM", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.CEM"], ["", "def", "gradmutualgain", "(", "output", ",", "label", ",", "one_hot", ",", "softmaxed", ",", "softmaxed_y", ",", "loss_fn", "=", "None", ")", ":", "\n", "    ", "up", "=", "REM", "(", "softmaxed", ".", "unsqueeze", "(", "0", ")", ",", "one_hot", ".", "unsqueeze", "(", "0", ")", ")", "\n", "# make all the less than zero > 0", "\n", "down", "=", "1.0", "+", "CEM", "(", "softmaxed", ".", "unsqueeze", "(", "0", ")", ",", "softmaxed_y", ".", "unsqueeze", "(", "0", ")", ")", "\n", "return", "up", ",", "down", "\n", "# PPL_model = kenlm.Model('lms/trec.arpa')", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.PPLSim": [[318, 322], ["PPL_model.perplexity", "torch.cosine_similarity", "torch.cosine_similarity", "out_x.unsqueeze", "out_y.unsqueeze"], "function", ["None"], ["", "def", "PPLSim", "(", "sentence", ",", "out_x", ",", "out_y", ")", ":", "\n", "    ", "up", "=", "PPL_model", ".", "perplexity", "(", "sentence", ")", "\n", "down", "=", "torch", ".", "cosine_similarity", "(", "out_x", ".", "unsqueeze", "(", "0", ")", ",", "out_y", ".", "unsqueeze", "(", "0", ")", ")", "[", "0", "]", "\n", "return", "up", ",", "down", "\n", "", "stop_words", "=", "[", "'i'", ",", "'me'", ",", "'my'", ",", "'myself'", ",", "'we'", ",", "'our'", ",", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.tackle": [[344, 363], ["input_text.lower.lower", "no_punct.split.split", "nltk.word_tokenize", "nltk.word_tokenize", "ans.append"], "function", ["None"], ["def", "tackle", "(", "input_text", ")", ":", "\n", "    ", "input_text", "=", "input_text", ".", "lower", "(", ")", "\n", "input_text", "=", "' '", ".", "join", "(", "nltk", ".", "word_tokenize", "(", "input_text", ")", ")", "\n", "punctuations", "=", "'''!()-[]{}|;:'\"\\,<>./?@#$%^&*_~`'''", "\n", "\n", "# To take input from the user", "\n", "# my_str = input(\"Enter a string: \")", "\n", "\n", "# remove punctuation from the string", "\n", "no_punct", "=", "\"\"", "\n", "for", "char", "in", "input_text", ":", "\n", "        ", "if", "char", "not", "in", "punctuations", ":", "\n", "            ", "no_punct", "=", "no_punct", "+", "char", "\n", "", "", "no_punct", "=", "no_punct", ".", "split", "(", ")", "\n", "ans", "=", "[", "]", "\n", "for", "word", "in", "no_punct", ":", "\n", "        ", "if", "word", "not", "in", "stop_words", ":", "\n", "            ", "ans", ".", "append", "(", "word", ")", "\n", "", "", "return", "' '", ".", "join", "(", "ans", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.epda": [[364, 451], ["model.eval", "list", "torch.stack().cuda", "torch.stack().cuda", "model().detach().cpu", "range", "numpy.array", "numpy.array", "range", "numpy.array", "numpy.argsort().tolist", "model.train", "eda.eda", "set", "len", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax.size", "torch.zeros", "torch.zeros", "eda.gradmutualgain", "np.array.append", "np.array.append", "np.array.append", "range", "nlp_aug.eda_4", "nlp_auger.augment", "print", "get_embed_fn", "newtxts.append", "torch.stack", "torch.stack", "model().detach", "numpy.min", "numpy.max", "numpy.min", "numpy.min", "numpy.max", "numpy.min", "math.isnan", "numpy.argsort", "len", "open", "open.write", "open.close", "newtxts.append", "newscores.append", "newtxts.append", "newscores.append", "random.random", "eda.tackle", "open", "open.write", "open.close", "model", "translator", "str", "str"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.train", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.eda", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.gradmutualgain", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.nlp_aug.eda_4", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.tackle"], ["", "def", "epda", "(", "txt", ",", "label", ",", "num_aug", "=", "1", ",", "get_embed_fn", "=", "None", ",", "loss_fn", "=", "None", ",", "model", "=", "None", ",", "translator", "=", "None", ",", "engine", "=", "'EDA'", ",", "alpha", "=", "0.1", ",", "mix_up", "=", "False", ",", "nlp_auger", "=", "None", ",", "alpha_epda", "=", "0.5", ")", ":", "\n", "    ", "NEED_SAV", "=", "False", "\n", "model", ".", "eval", "(", ")", "\n", "# print(\"txt=\",txt)", "\n", "# print(\"PLZ input sth\")", "\n", "# txt = input()", "\n", "if", "engine", "==", "'EEDA'", ":", "\n", "        ", "txts", "=", "eda", "(", "txt", ",", "num_aug", "=", "2", "*", "num_aug", ")", "\n", "", "elif", "engine", "==", "'EDA'", ":", "\n", "        ", "txts", "=", "eda_4", "(", "txt", ",", "alpha_sr", "=", "alpha", ",", "alpha_ri", "=", "alpha", ",", "alpha_rs", "=", "alpha", ",", "p_rd", "=", "alpha", ",", "num_aug", "=", "3", "*", "num_aug", ")", "\n", "", "else", ":", "\n", "        ", "txts", "=", "nlp_auger", ".", "augment", "(", "txt", ",", "n", "=", "num_aug", "*", "5", ")", "\n", "", "try", ":", "\n", "        ", "if", "translator", "is", "not", "None", "and", "random", ".", "random", "(", ")", "<=", "0.0", ":", "\n", "            ", "txts", "+=", "[", "tackle", "(", "translator", "(", "txt", ",", "num_aug", "=", "1", ")", "[", "0", "]", ")", "]", "\n", "", "", "except", ":", "\n", "        ", "print", "(", "\"Error! Don't Back Translate!\"", ")", "\n", "", "txts", "=", "list", "(", "set", "(", "txts", ")", ")", "\n", "# print(\"??\",len(txts))", "\n", "# print(\"txts:=\",txts)", "\n", "txts", "=", "[", "txt", "]", "+", "txts", "\n", "oldtxts", "=", "txts", "\n", "if", "get_embed_fn", "is", "not", "None", ":", "\n", "        ", "newtxts", "=", "[", "]", "\n", "for", "txt", "in", "txts", ":", "\n", "            ", "out", "=", "get_embed_fn", "(", "txt", ")", "\n", "newtxts", ".", "append", "(", "out", ")", "\n", "", "txts", "=", "newtxts", "\n", "", "inputs", "=", "torch", ".", "stack", "(", "newtxts", ")", ".", "cuda", "(", ")", "\n", "outputs", "=", "model", "(", "inputs", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", "\n", "ups", ",", "downs", ",", "scores", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "# alpha_epda = 0.4", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "newtxts", ")", ")", ":", "\n", "# a = torch.softmax(outputs[0],0)", "\n", "        ", "b", "=", "torch", ".", "softmax", "(", "outputs", "[", "i", "]", ",", "0", ")", "\n", "c", "=", "torch", ".", "softmax", "(", "outputs", "[", "0", "]", ",", "0", ")", "\n", "C", "=", "b", ".", "size", "(", "0", ")", "\n", "a", "=", "torch", ".", "zeros", "(", "C", ")", "\n", "a", "[", "label", "]", "=", "1.0", "\n", "# print('a',a,'b',b)", "\n", "# _up,_down = PPLSim(oldtxts[i],a,b)", "\n", "_up", ",", "_down", "=", "gradmutualgain", "(", "outputs", "[", "i", "]", ",", "label", ",", "a", ",", "b", ",", "c", ",", "loss_fn", ")", "\n", "ups", ".", "append", "(", "_up", ")", "\n", "downs", ".", "append", "(", "_down", ")", "\n", "", "ups", "=", "np", ".", "array", "(", "ups", ")", "\n", "downs", "=", "np", ".", "array", "(", "downs", ")", "\n", "ups", "=", "(", "ups", "-", "np", ".", "min", "(", "ups", ")", ")", "/", "(", "np", ".", "max", "(", "ups", ")", "-", "np", ".", "min", "(", "ups", ")", ")", "\n", "downs", "=", "(", "downs", "-", "np", ".", "min", "(", "downs", ")", ")", "/", "(", "np", ".", "max", "(", "downs", ")", "-", "np", ".", "min", "(", "downs", ")", ")", "\n", "for", "i", "in", "range", "(", "downs", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "_up", ",", "_down", "=", "ups", "[", "i", "]", ",", "downs", "[", "i", "]", "\n", "score", "=", "alpha_epda", "*", "_up", "+", "(", "1.0", "-", "alpha_epda", ")", "*", "_down", "\n", "# print(\"s = \",oldtxts[i],\"gm = \",_up,\"mim=\",_down)", "\n", "# print(\"??\",type(score),score)", "\n", "if", "score", "==", "np", ".", "nan", "or", "math", ".", "isnan", "(", "score", ")", ":", "\n", "# print(\"Oh no!\")", "\n", "            ", "score", "=", "1.0", "\n", "# print(\"up=\",_up,'down=',_down)", "\n", "", "scores", ".", "append", "(", "score", ")", "\n", "# print(oldtxts[i+1],'score:',_up,_down,score)", "\n", "", "scores", "=", "np", ".", "array", "(", "scores", ")", "\n", "sortargs", "=", "np", ".", "argsort", "(", "-", "scores", ")", ".", "tolist", "(", ")", "\n", "# print(scores,sortargs)", "\n", "model", ".", "train", "(", ")", "\n", "newtxts", "=", "[", "]", "\n", "newscores", "=", "[", "]", "\n", "if", "not", "mix_up", "or", "i", "+", "num_aug", ">=", "len", "(", "sortargs", ")", ":", "\n", "        ", "if", "NEED_SAV", ":", "\n", "            ", "f", "=", "open", "(", "\"data/epda.txt\"", ",", "'a'", ")", "\n", "f", ".", "write", "(", "str", "(", "label", ")", "+", "'\\t'", "+", "oldtxts", "[", "0", "]", "+", "'\\n'", ")", "\n", "f", ".", "close", "(", ")", "\n", "", "for", "idx", "in", "sortargs", "[", ":", "num_aug", "]", ":", "\n", "            ", "if", "NEED_SAV", ":", "\n", "                ", "f", "=", "open", "(", "\"data/epda.txt\"", ",", "'a'", ")", "\n", "f", ".", "write", "(", "str", "(", "label", ")", "+", "'\\t'", "+", "oldtxts", "[", "idx", "+", "1", "]", "+", "'\\n'", ")", "\n", "f", ".", "close", "(", ")", "\n", "", "newtxts", ".", "append", "(", "txts", "[", "idx", "]", ")", "\n", "newscores", ".", "append", "(", "scores", "[", "idx", "]", ")", "\n", "", "", "else", ":", "\n", "        ", "for", "i", "in", "range", "(", "num_aug", ")", ":", "\n", "# print(\"LEN\",len(sortargs),i+num_aug)", "\n", "            ", "idx_1", ",", "idx_2", "=", "sortargs", "[", "i", "]", ",", "sortargs", "[", "i", "+", "num_aug", "]", "\n", "newtxts", ".", "append", "(", "txts", "[", "idx_1", "]", ")", "\n", "newscores", ".", "append", "(", "scores", "[", "idx_1", "]", ")", "\n", "# newtxts.append(txts[0])", "\n", "# newscores.append(2.0)", "\n", "# print(\"Final\",newscores)", "\n", "", "", "return", "newtxts", ",", "newscores", "\n", "", "def", "move_to_device", "(", "batch", ",", "rank", "=", "None", ")", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.move_to_device": [[451, 464], ["batch[].to"], "function", ["None"], ["", "def", "move_to_device", "(", "batch", ",", "rank", "=", "None", ")", ":", "\n", "    ", "ans", "=", "{", "}", "\n", "if", "(", "rank", "is", "None", ")", ":", "\n", "        ", "device", "=", "'cuda'", "\n", "", "else", ":", "\n", "        ", "device", "=", "'cuda:{}'", ".", "format", "(", "rank", ")", "\n", "", "for", "key", "in", "batch", ":", "\n", "        ", "try", ":", "\n", "            ", "ans", "[", "key", "]", "=", "batch", "[", "key", "]", ".", "to", "(", "device", "=", "device", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "# print(str(e))", "\n", "            ", "ans", "[", "key", "]", "=", "batch", "[", "key", "]", "\n", "", "", "return", "ans", "\n", "", "def", "epda_bert", "(", "txt", ",", "label", ",", "num_aug", "=", "1", ",", "get_embed_fn", "=", "None", ",", "loss_fn", "=", "None", ",", "model", "=", "None", ",", "translator", "=", "None", ",", "engine", "=", "'EDA'", ",", "alpha", "=", "0.1", ",", "mix_up", "=", "False", ",", "nlp_auger", "=", "None", ",", "alpha_epda", "=", "0.5", ")", ":", "\n"]], "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.epda_bert": [[464, 555], ["model.eval", "list", "get_embed_fn", "range", "torch.tensor().long", "torch.tensor().long", "eda.move_to_device", "range", "numpy.array", "numpy.array", "range", "numpy.array", "numpy.argsort().tolist", "model.train", "newtxts.append", "newscores.append", "eda.eda", "set", "len", "torch.tensor().long.append", "model", "model().detach().cpu.logits.detach().cpu", "len", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax.size", "torch.zeros", "torch.zeros", "eda.gradmutualgain", "np.array.append", "np.array.append", "np.array.append", "range", "nlp_aug.eda_4", "nlp_auger.augment", "print", "torch.tensor", "torch.tensor", "model().detach().cpu", "numpy.min", "numpy.max", "numpy.min", "numpy.min", "numpy.max", "numpy.min", "math.isnan", "numpy.argsort", "len", "open", "open.write", "open.close", "newtxts.append", "newscores.append", "random.random", "eda.tackle", "model().detach().cpu.logits.detach", "open", "open.write", "open.close", "random.random", "newtxts.append", "newscores.append", "newtxts.append", "newscores.append", "model().detach", "math.isnan", "translator", "model", "str", "str"], "function", ["home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.move_to_device", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.train_irony_epida_eda.train", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.eda", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.gradmutualgain", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.nlp_aug.eda_4", "home.repos.pwc.inspect_result.zhaominyiz_epida.None.eda.tackle"], ["", "def", "epda_bert", "(", "txt", ",", "label", ",", "num_aug", "=", "1", ",", "get_embed_fn", "=", "None", ",", "loss_fn", "=", "None", ",", "model", "=", "None", ",", "translator", "=", "None", ",", "engine", "=", "'EDA'", ",", "alpha", "=", "0.1", ",", "mix_up", "=", "False", ",", "nlp_auger", "=", "None", ",", "alpha_epda", "=", "0.5", ")", ":", "\n", "    ", "NEED_SAV", "=", "False", "\n", "model", ".", "eval", "(", ")", "\n", "# print(\"txt=\",txt)", "\n", "# print(\"PLZ input sth\")", "\n", "# txt = input()", "\n", "# label = 1", "\n", "if", "engine", "==", "'EEDA'", ":", "\n", "        ", "txts", "=", "eda", "(", "txt", ",", "num_aug", "=", "2", "*", "num_aug", ")", "\n", "", "elif", "engine", "==", "'EDA'", ":", "\n", "        ", "txts", "=", "eda_4", "(", "txt", ",", "alpha_sr", "=", "alpha", ",", "alpha_ri", "=", "alpha", ",", "alpha_rs", "=", "alpha", ",", "p_rd", "=", "alpha", ",", "num_aug", "=", "50", "*", "num_aug", ")", "\n", "", "else", ":", "\n", "        ", "txts", "=", "nlp_auger", ".", "augment", "(", "txt", ",", "n", "=", "num_aug", "*", "5", ")", "\n", "", "try", ":", "\n", "        ", "if", "translator", "is", "not", "None", "and", "random", ".", "random", "(", ")", "<=", "0.0", ":", "\n", "            ", "txts", "+=", "[", "tackle", "(", "translator", "(", "txt", ",", "num_aug", "=", "1", ")", "[", "0", "]", ")", "]", "\n", "", "", "except", ":", "\n", "        ", "print", "(", "\"Error! Don't Back Translate!\"", ")", "\n", "", "txts", "=", "list", "(", "set", "(", "txts", ")", ")", "\n", "txts", "=", "[", "txt", "]", "+", "txts", "\n", "oldtxts", "=", "txts", "\n", "encoding", "=", "get_embed_fn", "(", "txts", ",", "return_tensors", "=", "'pt'", ",", "padding", "=", "True", ",", "truncation", "=", "True", ")", "\n", "labels", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "txts", ")", ")", ":", "\n", "        ", "labels", ".", "append", "(", "label", ")", "\n", "", "labels", "=", "torch", ".", "tensor", "(", "labels", ")", ".", "long", "(", ")", "\n", "encoding", "[", "'labels'", "]", "=", "labels", "\n", "encoding", "=", "move_to_device", "(", "encoding", ")", "\n", "\n", "try", ":", "\n", "        ", "outputs", "=", "model", "(", "**", "encoding", ")", "\n", "outputs", "=", "outputs", ".", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", "\n", "", "except", ":", "\n", "        ", "outputs", "=", "model", "(", "encoding", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", "\n", "", "ups", ",", "downs", ",", "scores", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "oldtxts", ")", ")", ":", "\n", "        ", "softmaxed", "=", "torch", ".", "softmax", "(", "outputs", "[", "i", "]", ",", "0", ")", "\n", "c", "=", "torch", ".", "softmax", "(", "outputs", "[", "0", "]", ",", "0", ")", "\n", "C", "=", "softmaxed", ".", "size", "(", "0", ")", "\n", "one_hot", "=", "torch", ".", "zeros", "(", "C", ")", "\n", "one_hot", "[", "label", "]", "=", "1.0", "\n", "# print(outputs[i],label,a,b)", "\n", "_up", ",", "_down", "=", "gradmutualgain", "(", "outputs", "[", "i", "]", ",", "label", ",", "one_hot", ",", "softmaxed", ",", "c", ",", "loss_fn", ")", "\n", "ups", ".", "append", "(", "_up", ")", "\n", "downs", ".", "append", "(", "_down", ")", "\n", "", "ups", "=", "np", ".", "array", "(", "ups", ")", "\n", "downs", "=", "np", ".", "array", "(", "downs", ")", "\n", "ups", "=", "(", "ups", "-", "np", ".", "min", "(", "ups", ")", ")", "/", "(", "np", ".", "max", "(", "ups", ")", "-", "np", ".", "min", "(", "ups", ")", ")", "\n", "downs", "=", "(", "downs", "-", "np", ".", "min", "(", "downs", ")", ")", "/", "(", "np", ".", "max", "(", "downs", ")", "-", "np", ".", "min", "(", "downs", ")", ")", "\n", "for", "i", "in", "range", "(", "downs", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "_up", ",", "_down", "=", "ups", "[", "i", "]", ",", "downs", "[", "i", "]", "\n", "score", "=", "alpha_epda", "*", "_up", "+", "(", "1.0", "-", "alpha_epda", ")", "*", "_down", "\n", "if", "score", "==", "np", ".", "nan", "or", "math", ".", "isnan", "(", "score", ")", ":", "\n", "            ", "score", "=", "1.0", "\n", "", "scores", ".", "append", "(", "score", ")", "\n", "", "scores", "=", "np", ".", "array", "(", "scores", ")", "\n", "sortargs", "=", "np", ".", "argsort", "(", "-", "scores", ")", ".", "tolist", "(", ")", "\n", "model", ".", "train", "(", ")", "\n", "newtxts", "=", "[", "]", "\n", "newscores", "=", "[", "]", "\n", "if", "not", "mix_up", "or", "i", "+", "num_aug", ">=", "len", "(", "sortargs", ")", ":", "\n", "        ", "if", "NEED_SAV", ":", "\n", "            ", "f", "=", "open", "(", "\"data/epda.txt\"", ",", "'a'", ")", "\n", "f", ".", "write", "(", "str", "(", "label", ")", "+", "'\\t'", "+", "oldtxts", "[", "0", "]", "+", "'\\n'", ")", "\n", "f", ".", "close", "(", ")", "\n", "", "for", "idx", "in", "sortargs", "[", ":", "num_aug", "]", ":", "\n", "            ", "if", "NEED_SAV", ":", "\n", "                ", "f", "=", "open", "(", "\"data/epda.txt\"", ",", "'a'", ")", "\n", "f", ".", "write", "(", "str", "(", "label", ")", "+", "'\\t'", "+", "oldtxts", "[", "idx", "+", "1", "]", "+", "'\\n'", ")", "\n", "f", ".", "close", "(", ")", "\n", "", "newtxts", ".", "append", "(", "oldtxts", "[", "idx", "]", ")", "\n", "newscores", ".", "append", "(", "scores", "[", "idx", "]", ")", "\n", "", "", "else", ":", "\n", "        ", "for", "i", "in", "range", "(", "num_aug", ")", ":", "\n", "# print(\"LEN\",len(sortargs),i+num_aug)", "\n", "            ", "idx_1", ",", "idx_2", "=", "sortargs", "[", "i", "]", ",", "sortargs", "[", "i", "+", "num_aug", "]", "\n", "# MIX UP", "\n", "if", "random", ".", "random", "(", ")", "<=", "0.2", ":", "\n", "                ", "score_1", ",", "score_2", "=", "scores", "[", "idx_1", "]", ",", "scores", "[", "idx_2", "]", "\n", "lamda", "=", "score_1", "/", "(", "score_1", "+", "score_2", ")", "\n", "if", "lamda", "==", "np", ".", "nan", "or", "math", ".", "isnan", "(", "lamda", ")", ":", "\n", "                    ", "lamda", "=", "0.5", "\n", "", "state", "=", "oldtxts", "[", "idx_1", "+", "1", "]", "*", "lamda", "+", "(", "1.0", "-", "lamda", ")", "*", "oldtxts", "[", "idx_2", "+", "1", "]", "\n", "newtxts", ".", "append", "(", "state", ")", "\n", "newscores", ".", "append", "(", "2.0", ")", "\n", "", "else", ":", "\n", "                ", "newtxts", ".", "append", "(", "oldtxts", "[", "idx_1", "+", "1", "]", ")", "\n", "newscores", ".", "append", "(", "scores", "[", "idx_1", "]", ")", "\n", "", "", "", "newtxts", ".", "append", "(", "oldtxts", "[", "0", "]", ")", "\n", "newscores", ".", "append", "(", "2.0", ")", "\n", "return", "newtxts", ",", "newscores", "\n", "", ""]]}